{
    "paper_title": "SciCoQA: Quality Assurance for Scientific Paper--Code Alignment",
    "authors": [
        "Tim Baumgärtner",
        "Iryna Gurevych"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\\% of real-world paper-code discrepancies."
        },
        {
            "title": "Start",
            "content": "SCICOQA: Quality Assurance for Scientific PaperCode Alignment Tim Baumgärtner and Iryna Gurevych Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science, TU Darmstadt and National Research Center for Applied Cybersecurity ATHENE Code: https://github.com/ukplab/scicoqa Data: https://hf.co/datasets/ukplab/scicoqa 6 2 0 2 9 ] . [ 1 0 1 9 2 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present SCICOQA, dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SCICOQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SCICOQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7% of realworld paper-code discrepancies."
        },
        {
            "title": "Introduction",
            "content": "The reproducibility crisis in AI and across science casts doubt on the reliability of research (Baker, 2016; Hutson, 2018). To address this, the computational sciences have long recognized that paper alone is insufficient and that publishing code, data, and instructions is prerequisite to ensure experimental findings are reproducible (Buckheit and Donoho, 1995; Peng, 2011; Pineau et al., 2021). However, the availability of code does not guarantee reproducibility nor consistency with the scientific text (as showcased in Fig. 1). In practice, implementation details can diverge from their descriptions, introducing performance variations that go unreported (Henderson et al., 2018). These Data also available at: https://tudatalib.ulb. tu-darmstadt.de/handle/tudatalib/4994 Figure 1: Example from SCICOQA, showing specific model implementation in the paper, and its implementation in the code (simplified for readability). The papers description and the codes implementation mismatch, creating paper-code discrepancy. discrepancies manifest in troubling ways: from mathiness, where equations simulate technical depth while actual gains stem from undocumented tricks (Lipton and Steinhardt, 2019), to evaluation metrics that differ in implementation (Post, 2018), rendering scientific comparisons invalid. Reproducibility issues and paper-code inconsistencies are typically only detected during reproduction efforts and post-publication, resulting in waste of resources and eroding trust in science. While checklists during peer review can raise awareness of authors to provide (more) reproducible code (Pineau et al., 2021; Dodge et al., 2019), ideally, reviewers check the correctness of the implementation and ensure reproducibility. However, with the rapid growth of submission numbers, reviewers are already under severe time pressure (Rogers and Augenstein, 2020) and conducting intricate code reviews is time-consuming (Collberg and Proebsting, 2016). Furthermore, the reliance on manual review is becoming increasingly impractical as science begins to scale via agentic systems which develop ideas, generate and execute code, and produce scientific articles autonomously (Lu et al., 2024; Tang et al., 2025; Weng et al., 2025b), trend recently validated by peer-reviewed acceptance to an ICLR workshop (Yamada et al., 2025). While this direction has the potential to accelerate scientific discovery, it makes oversight increasingly challenging as humans cannot verify the rapidly expanding volume of output (Bowman et al., 2022). Moreover, the faithfulness of these systems is not guaranteed; LLM agents suffer from limited context (Liu et al., 2024b), experience compounding errors (Dziri et al., 2023; Mirzadeh et al., 2025), and struggle with independent self-correction (Tyen et al., 2024; Wu et al., 2024). Consequently, ensuring the reliability of science at this scale demands automated tools capable of verifying that the code faithfully implements the methods reported in the scientific paper. To this end, we introduce the SCICOQA dataset for novel, real-world quality assurance task: detecting discrepancies between scientific papers and code in the computational sciences. Benchmarking LLMs on this task allows us to measure their potential for automated quality assurance workflows in science. To construct our dataset, we leverage issues reported in code repositories and papers dedicated to reproducing existing work, such as those from reproducibility challenges and conference tracks. While these sources are realistic and of high quality, they are sparse and limited to CS and AI. Therefore, we scale the data to various computational science domains (e.g., Physics and Quantitative Biology) by generating modifications in codebases, thereby creating discrepancies between the paper and its implementation. Finally, we benchmark open-weight and proprietary LLMs to determine whether current systems can be deployed for detecting papercode discrepancies. Our evaluation reveals that the best-performing models are precise in identifying discrepancies; however, their recall is too low to guarantee that the code faithfully implements the paper, highlighting the need to develop more effective models and systems for this critical task."
        },
        {
            "title": "2 Related Work",
            "content": "Error Detection in Science Recently, LLMs have been employed to detect errors in scientific papers. In an early work, Liu and Shah (2023) created short CS papers with logical errors and prompted LLMs to identify the issues. They found that while models could produce fluent reviews, they struggled to identify these fundamental errors without explicit instructions in the prompt. However, their reliance on manually crafted papers limits scalability. Therefore, Dycke and Gurevych (2026) and Xi et al. (2025) propose dedicated pipelines that generate modifications to papers that invalidate their key claims and evaluate whether the soundness score of generated reviews decreases or whether LLMs can spot these errors. Both find that LLMs struggle to detect most introduced errors. Besides constructing erroneous papers, recent research has also focused on identifying real-world issues. Specifically, Son et al. (2025) and Zhang and Abernethy (2025) use the authors retraction notes from WithdrarXiv (Rao et al., 2024) and PubPeer to obtain data of errors in scientific publication. While realistic, these retraction notes are often unspecific or describe errors on high level only (e.g., \"Theorem 2 is incorrect\"). Recently, Bianchi et al. (2025) developed system to detect inconsistencies within the text of AI papers (e.g., incorrect calculations in tables, errors in equations, and imprecise definitions) and then manually verify the detections. They found their system to be relatively precise, but struggled with recall, i.e., detecting all issues in paper. They also noted an increasing trend of errors in scientific publications over the years, highlighting the importance of quality assurance in science. While these works share similar spirit to ours, i.e., identifying errors in science, they are limited by solely analyzing the paper. SCICOQA extends beyond the scientific text and considers the code as well, enabling the detection of cross-modal error types, such as implementations that deviate from the paper description or omissions in the paper or code that are crucial for understanding and reproducibility. Furthermore, SCICOQA utilizes constructed errors to scale up the quantity and diversity in the computational science domain, while also providing challenging real-world discrepancies between paper and code. Comment-Code Inconsistency Our task is related to Code-Comment Inconsistency (CCI) detection, where natural language comments that no longer match piece of code, e.g., after adding feature to function, need to be identified. Early CCI research relied on rule-based systems to identify outdated comments (Ratol and Robillard, 2017). Subsequently, bi-encoders (Rabbi and Siddik, 2020) and cross-encoders (Steiner and Zhang, 2022) have been employed. Panthaplackel et al. (2021) further extended this to dynamic settings, learning to detect inconsistency arising from code updates. Recently, LLMs have been deployed for Figure 2: Overview of the data collection process of SCICOQA. We source real-world data from Reproducibility papers and GitHub issues. For the former, paper-code discrepancies are extracted from the paper with GPT-5, for the latter, issues are pre-filtered using Qwen3. Next, all candidates are manually filtered to remove any that do not fit our discrepancy definition. Finally, all paper-code discrepancies are verified with GPT-5. For synthetic data, we generate discrepancies using GPT-5 for AI and other computational domains. both detection and rectification (Dau et al., 2024; Rong et al., 2025). These methods deal with local and small inputs, typically taking single function and an inline comment or docstring as input to detect the CCI. SCICOQA is crucially much more ambitious: it requires performing global alignment, by reasoning over dense scientific paper and long, multi-file code repository to find inconsistencies. Science Automation Automation in science is rapidly accelerating. Early work targeted specific components of the research cycle, including ideation (Si et al., 2025; Baek et al., 2025), literature review (Wang et al., 2024; Liang et al., 2025), coding (Tian et al., 2024; Gandhi et al., 2025), and writing (Liang et al., 2024; Weng et al., 2025a). recent trend involves generating entire code repositories from paper (Starace et al., 2025; Seo et al., 2025; Xiang et al., 2025) or automating the scientific method end-to-end, giving rise to \"AI Scientists.\" However, for both these tasks, validating whether the generated code faithfully reflects the paper is challenging. For example, PaperBench (Starace et al., 2025) relies on high-quality, but expensive, manual rubrics specific to each paper to verify implementation nuances. Others use generic LLM-judges without ground truth validation (Tang et al., 2025), evaluate on whether the code runs (Weng et al., 2025b; Xiang et al., 2025), or assign reproducibility scores based on execution outcomes (Siegel et al., 2024; Hu et al., 2025; Ye et al., 2025). Critically, these metrics can be misleading; generated codebase may execute perfectly and achieve high performance while implementing method that differs fundamentally from the papers scientific description (Beel et al., 2025). SCICOQA addresses this gap by providing ground truth dataset of paper-code discrepancies, enabling the development of robust quality assurance models that can verify the faithfulness of (generated) papers to (generated) code."
        },
        {
            "title": "3 SCICOQA",
            "content": "The objective of the SCICOQA task is to identify discrepancies between paper and its code. We first define what constitutes discrepancy: Paper-Code Discrepancy We define papercode discrepancy as semantic conflict between the scientific method described in the publication and its actual implementation in the codebase, such that the code does not faithfully reproduce the reported method. This mismatch must be meaningful, implying fundamental alteration to the scientific logic, experimental protocol, or mathematical formulation described in the text. These discrepancies manifest as three distinct types: differences, where the code implements logic distinct from the papers description (e.g., L1 vs. L2 normalization) paper omissions, where the code includes critical components missing from the text, or code omissions, where step described in the paper is absent from the repository. We distinguish these discrepancies from engineering artifacts: We exclude bugs, as these are independent of the papers scientific description. Similarly, mismatches in default hyperparameters are not considered discrepancies if the code supports the papers settings via configuration files or CLI arguments. Finally, we exclude trivial implementation details that are standard engineering practices typically omitted from scientific descriptions (e.g., adding noise to denominator for numerical stability). The paper-code discrepancies, according to our definition, can occur from distinct authorship (where the paper writer differs from the engineer implementing the code), simplifications made in the text for readability, or code updates, such as experimental variations, that were not propagated back to the manuscript."
        },
        {
            "title": "3.1 Data Collection",
            "content": "Fig. 2 provides an overview of our data collection. To obtain real-world instances of papercode discrepancies, we draw from GitHub issues and reproducibility papers. Furthermore, synthetic discrepancies are generated in real scientific codebases. GitHub issues We first identify repositories that reference research paper in their homepage or description field on GitHub, restricted to projects published between 2020 and 2025 and repositories with at least one issue (see for details).1 For each, we crawl all associated issues, yielding 1, 890 repositories with total of 10, 636 issues. To process this large volume efficiently, we automatically classify issues with Qwen3 4B Thinking (Yang et al., 2025), prompting the model to determine whether an issue reports papercode discrepancy or not, resulting in 232 candidates (prompt in B.1). All candidates are subsequently manually filtered to ensure they meet our definition of paper-code discrepancy, yielding 59 discrepancies. Reproducibility papers We additionally collect reproducibility reports from the ML Reproducibility Challenge (Pineau et al., 2019; Sinha et al., 2020, 2022, 2023) which invites participants to reproduce papers from leading ML, CV, and NLP conferences (e.g., NeurIPS, ICML, ICLR, CVPR, 1The crawled repositories are not always the official implementation by the authors, but sometimes also reproductions. They have been published, for example, when there is no official implementation or the reimplementation uses different framework. We still consider these repositories as researchers might choose them to base their experiments on. ICCV, EMNLP, ACL). We also include papers from the reproducibility tracks of SIGIR and ECIR from 20202025. We retain only reports that (1) reproduce single, open-access paper, and (2) reproduce papers with an open-source implementation, resulting in 171 reproducibility papers. From each report, we extract mentions of papercode discrepancies using GPT-5 resulting in 132 candidates (prompt in B.3). Each extracted discrepancy is manually verified to confirm adherence to our definition, of which 65 withstand our filter. Validation and Phrasing Finally, all manually filtered discrepancies are validated by GPT-5, which checks the discrepancys existence given the original paper, codebase, and the GitHub issue or the description extracted from the reproducibility paper (prompt in B.2 and B.4). While the manual verification step ensures adherence to the discrepancy definition, this step acts as high-precision filter. By retaining only discrepancies that can be verified given the explicit evidence, we prioritize the precision and objectivity of the ground truth. In total, we obtain 81 discrepancies from 67 papers, where 37 discrepancies originate from GitHub issues and 44 from reproducibility papers. During this final verification step, GPT-5 is also tasked with generating standardized description of discrepancies. We prompt the model to output 3-8 sentences and state (1) what the paper describes, (2) what the code implements, and (3) where the difference is. We use these generated descriptions as the ground truth to ensure the same format and level of verbosity for paper-code discrepancies. Synthetic Data To scale the data in size and beyond the CS/AI domain, we employ synthetic data generation. Specifically, based on our previous GitHub crawl, we sample repositories linked to an arXiv paper and those with permissive code licenses (i.e., MIT, Apache 2.0, CC-BY, BSD), as we will be redistributing their code partially. We randomly select 102 repositories where the papers arXiv subject is CS (balanced over Machine Learning, cs.LG, Computer Vision, cs.CV, and Natural Language Processing, cs.CL), and 102 for non-CS papers.2 Next, we prompt GPT-5 with the paper and code to generate five code diffs according to our discrepancy definition (prompt in B.5). We then sample up to three of these changes, ensuring 2We manually checked the repositories and removed codebases from our sample that were not suitable, such as repositories for survey papers or with very few files. Figure 3: Analysis of the real (blue), synthetic (orange), and combined (green) discrepancy data. The y-axis shows the proportion of the data. The arXiv subjects stand for Computer Science (CS), Electrical Engineering and Systems Science (EESS), Physics, Statistics (Stats), Quantitative Biology (Q-Bio), and Mathematics (Math). The Discrepancy Category chart shows only the distribution over computer science papers. that they do not manipulate the same file and that the generated diff can be found by exact match in the original code. This yields total of 530 discrepancies across the 204 papers, of which 280 discrepancies are from the CS domain, and 250 are from other computational domains, such as Electrical Engineering and Systems Science (112 discrepancies), Physics (49), or Statistics (39). Example data, including discrepancy type and category annotations, are shown in Table 7. Examples from the synthetic subset, including the code changes, are shown in Table 8. We release the SCICOQA dataset publicly under CC-BY-4.0."
        },
        {
            "title": "3.2 Data Analysis",
            "content": "Fig. 3 analyzes several dimensions of the SCICOQA dataset. While our real-world data is mostly from computer science (specifically from AI and its subdomains such as Machine Learning, Computer Vision, and Natural Language Processing), the synthetic data contains papers and code from Electrical Engineering and System Science, Physics, Statistics, Quantitative Biology, and Mathematics, making the data diverse across computational sciences. We further analyze the discrepancy type (as defined in 3), by annotating each discrepancy in the real-world set as Difference, Paper Omission, or Code Omission. For the synthetic data, we provide discrepancy type definitions to GPT-5 in the prompt and let it generate the label along with the discrepancy. We find that 53% of discrepancies in the real data are Differences, which is also the majority class in the synthetic data, accounting for 80% of the data. Beyond the discrepancy type, we analyze the affected component within the research pipeline. We introduce taxonomy of six discrepancy categories: Algorithm: changes to step order, operations, or core logic; Model: architectural or initialization changes; Loss: alterations to loss definitions or terms; Evaluation: modifications to evaluation logic, metrics, or scripts; Data: dataset usage, preprocessing, augmentation, or filtering; and Training: changes to the learning process, schedule, or optimization. We apply this taxonomy to real-world data and the CS subset of synthetic data, excluding other scientific domains (e.g., Physics) where these ML-specific concepts may not always apply cleanly. Similar to the discrepancy type annotation, we manually label the real instances and automatically generate labels for the synthetic ones. In the real data, discrepancies in the Algorithm and Loss dominate, with 26% each, whereas the synthetic data also has 26% of discrepancies in the Algorithm, and Model discrepancies are the second-highest category, at 20%. We further visualize the number of tokens in the prompt. Papers have median of 14, 350, codebases 38, 978, and the combined prompt 57, 008 tokens. Further, 70 out of 266 papers have more than 100k tokens combined with their codebase, making SCICOQA challenging task to measure the models long-context abilities. We analyze the data by programming languages (E) and the synthetic code (D) further in the appendix."
        },
        {
            "title": "4 Experiments",
            "content": "Given paper and its code, we prompt the model to generate list of discrepancies between the two (prompt in B.6). We then parse the model output into individual discrepancies. The generation prompt contains the same instructions as those used to construct the ground-truth discrepancies. For implementation details, we refer to C. We further ablate the discrepancy prediction experiment by providing only the code as input. This allows us to quantify the contribution of the paper, distinguishing between discrepancies that require cross-modal reasoning and those that can be inferred from the code (e.g., through comments or readmes), or the models parametric knowledge of the paper. Evaluation We employ LLM-as-a-Judge (Zheng et al., 2023) to evaluate whether the predicted discrepancy matches the reference. Inspired by Wei et al. (2025), we use reasoning model, specifically GPT-OSS 20B (OpenAI, 2025b) (prompt in B.7), because it is open-weight, enabling reproducibility, and due to its favorable speed-performance tradeoff. To verify the evaluation setup, we annotate the predictions from 20 discrepancies, where 10 originate from GitHub and 10 from reproducibility papers. To streamline our evaluation and spend most time on annotating relevant generations, we compute the top three most similar predicted discrepancies per model using EmbeddingGemma (Vera et al., 2025). We then manually assess whether each prediction matches the reference discrepancy. In total, this yields 1, 039 annotations, of which 143 predictions match the reference. On this data, the LLM judge achieves an F1 score of 87.5 1.1, showing strong alignment with our annotations.3 While the evaluation is binary classification (reference and predicted discrepancies match or do not match), we refer to the performance as recall, as the predictions may contain unannotated, but valid, discrepancies, and we only evaluate whether the annotated ones are detected. Models We evaluate several state-of-the-art model families, including commercial and open models, as well as reasoning, instruction-tuned, and code-specific variants, specifically: GPT-5, Gemini 2.5, GPT-OSS, Qwen3, DeepSeek R1, Nemotron, and Mistral (exact model variants in Table 3)."
        },
        {
            "title": "5 Results",
            "content": "Fig. 4 reports the recall of the top-performing models (full results in Table 5). Overall, GPT-5 performs best, achieving recall of 45.7%, but leaving 3We observe slight variance in predictions despite fixed seed in vLLM, we therefore report the mean and standard deviation over five runs. considerable room for improvement. On the synthetic data, 71.3% are detected by the model. Crucially, we observe strong correlation (r = 0.94) between recall on real and synthetic data (see F.1), validating the synthetic subset as reliable proxy for real-world discrepancies. While general scaling laws can be observed (i.e., models trained with more compute and data perform better), GPT-OSS 20B is an exception on our task, outperforming its 120B variant. We attribute this, on the one hand, to the self-preference bias of the judge, rating its own outputs better (Panickssery et al., 2024), and on the other hand, to the models verbosity. GPTOSS 20B makes, on average, 5.1 predictions per paper, while the 120B model only makes 4.5, giving the smaller variant higher chance to match the discrepancy at the expense of precision. Additionally, we find that GPT-5 Mini outperforms GPT-5 Codex, despite the latter being the larger model (Codex is based on GPT-5). While Codex is generally superior in code generation, for SCICOQA code and natural language understanding are both crucial, and we conjecture that the general instruction-following and reasoning abilities of GPT-5 and GPT-5 Mini are more helpful than specialized coding knowledge. Origin & Type Analyzing detection rates reveals clear hierarchy: Code Omissions are the easiest to detect, followed by Differences, while Paper Omissions are the most challenging. This order also explains the performance gap between data sources. Discrepancies from GitHub are easier to detect because they primarily consist of Differences (68%) and Code Omissions (21.6%). In contrast, discrepancies from reproducibility papers are more challenging as they are often dominated by Paper Omissions (50%). This likely stems from the asymmetry between modalities: the paper acts as specification, where all described components must exist in the code. Code Omissions and Differences benefit from this explicit grounding. Conversely, the code contains many implementation details not required in the paper; thus, detecting Paper Omissions requires the harder task of distinguishing deviations from permissible engineering artifacts without textual reference. Context Length We further analyze the performance by the number of input tokens. We split the dataset by prompt length into five approximately equally sized buckets. We find consistent pattern where the longer the input, the lower the perforFigure 4: Results of the top 8 best performing models (sorted by average recall on the real and synthetic data) on the discrepancy dataset by different analyses. From left to right, we analyze the origin of the discrepancy, the type of discrepancy, the number of tokens in the input prompt, and the publication year of the paper. mance becomes, as similarly observed in various studies (Hsieh et al., 2024; Zhang et al., 2024). This sensitivity to input length also suggests an explanation for the performance between domains in the synthetic data: models struggle more with CS papers, which is likely driven by the larger repository size of CS vs non-CS repositories (Median: 50k vs 28k tokens). This increased volume creates larger search space, making the task more challenging. While many long context evaluations rely on artificially constructed needles that need to be retrieved from long context, i.e., the haystack (Kamradt, 2023), SCICOQA can offer natural evaluation setup similar to Liu et al. (2024a) for long-context evaluation of LLMs.4 Publication Year Lastly, we split the data by publication year. Among the top models, the one with the most recent knowledge cutoff is the Gemini 2.5 family, which was trained using data up to January 2025 (other models have cutoffs in 2024). Therefore, the 2025 split of our data can be considered not part of the pre-training data. Most models (except the Gemini 2.5 Flash) perform the worst on the most recent data. This demonstrates that models benefit from having the paper and code included in their pre-training data, aligning with previous findings on data contamination (OpenAI, 2023; Zhou et al., 2023). This also emphasizes the importance of our synthetic data generation pipeline, which enables us to continuously update our dataset with uncontaminated data that is not part of the pre-training data of future models. Open-Models Beyond the top-performing models, we find the other open-weight models (Qwen3, Nemotron, DeepSeek R1, Mistral) severely limited. Nemotron 49B and Qwen3 Coder 30B perform best, but only achieve an average recall on the combined data of 23.6% and 23.1%, respectively. In summary, SCICOQA remains significant challenge for state-of-the-art LLMs. We find the recall of all models to be limited; the best-performing model can only detect 45.7% of all discrepancies in the real-world data. Consequently, these systems cannot yet be relied upon to ensure the faithfulness of scientific publications with their codebases."
        },
        {
            "title": "5.1 Code Only Ablation",
            "content": "Figure 5: Performance of top 8 models when given paper and code, and only the code, split by data origin: Real (R), Synthetic (S), and combined (R+S). 4The synthetic data generation can be used to inject discrepancies at controlled places in the codebase. To test the multimodality of our data, we remove the paper from the input, leaving only the code. from its pre-training. We show the results of the Model GPT-5 Gemini GPT-OSS Model GPT-5 Gemini GPT-OSS False Positives OCR Error Paper Ambiguity Paper Misunderstanding Code Misunderstanding 3rd Party Code True Positives Unique Minor Total Precision 8 1 2 2 0 2 35 15 6 2 1 1 0 0 0 26 10 5 21 1 0 6 12 1 43 24 12 43 81.4 28 92. 64 67.2 Table 1: Analysis of unlabeled discrepancy predictions of GPT-5, Gemini 2.5 Pro, and GPT-OSS 20B of 12 NLP papers from the real subset of SCICOQA. This makes it more difficult to identify discrepancies, but not impossible since the repositories also frequently contain abstracts, summaries, or details of the paper in their readme files. Furthermore, scientific papers are typically part of the pre-training data; therefore, while the model does not directly receive the paper as input, it may be able to recall it top models in this experiment in Fig. 5 (full results in Table 6). When removing the paper from the input, all models perform worse, confirming that the paper is necessary to perform the task. We observe an average drop of 19.4 percentage points, relative drop of 53.4% on the real data. For the synthetic data, the drop is less pronounced; on average, the models performance drops by 15.1 percentage points (relative drop of 30.3%)."
        },
        {
            "title": "5.2 Validation of Unlabeled Discrepancies",
            "content": "To better understand the precision of the models, we analyze GPT-5s, Gemini 2.5 Pros, and GPTOSS 20Bs generations. We select the predictions for 12 NLP papers that were not matched to ground truth, yielding total of 135 discrepancies (results in Table 1). We select NLP papers to leverage our own domain expertise, as validating discrepancies is challenging and requires detailed understanding of the paper and its implementation. Overall, we find that Gemini 2.5 Pro does not make significant errors, achieving precision of 92.9%. Its two errors were due to preprocessing (an omitted term in formula during OCR) and an ambiguity within the paper, nuance even noted in the models generation. GPT-5 followed (81.4%), primarily failing due to paper ambiguities or misinterpretations, or incorrect assumptions about third-party library functions. Finally, GPT-OSS True Positives False Positives Precision Recall F1 38 8 82.6 56.7 67.3 29 93.5 43.3 59.2 40 20 66.7 59.7 63.0 Table 2: Performance of GPT-5, Gemini 2.5 Pro, and GPT-OSS 20B on 67 pooled discrepancies from the annotation of unlabeled predictions and discrepancies from SCICOQA from 12 NLP papers. 20B struggles most (67.2%), frequently misunderstanding the code logic (e.g., it fails to align variables that are named differently in the paper and code or misinterprets conditional execution paths). We further construct pooled ground truth for the 12 analyzed papers. We aggregate the discrepancies originally annotated in SCICOQA with the verified ones identified by the models (excluding minor ones, which are mostly discrepancies related to mismatching settings that can typically be resolved via configuration or the CLI). This results in comprehensive set of 67 distinct paper-code discrepancies (results in Table 2). We find that the number of discrepancies uniquely identified by the models is high (see also Table 1), and recall remains low (43-60%), confirming our main experiments findings that model recall is limited. GPT-5 achieves the best precision-recall tradeoff, obtaining an F1 score of 67.3%."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced SCICOQA, dataset of 611 discrepancies designed to evaluate the alignment between scientific paper and its code. Our analysis reveals critical gap: while models like GPT-5 demonstrate high precision, they suffer from insufficient recall, detecting only 45.7% of real-world discrepancies. We find that models particularly struggle with paper omissions, where code logic is absent from the paper text, and fail to maintain performance on recent, uncontaminated publications. Consequently, while current LLMs show promise as assistants, they cannot yet serve as autonomous arbiters of scientific validity. As research scales toward AI Scientists, SCICOQA provides the essential ground truth to ensure these agents remain faithful to the scientific method. To further advance this field, we encourage the broader adoption of reproducibility tracks to systematically capture these discrepancies and expand future data collection."
        },
        {
            "title": "Limitations",
            "content": "Domains Our real-world data is predominantly skewed towards Computer Science and Artificial Intelligence. While we mitigate this by including synthetic discrepancies from Physics, Quantitative Biology, and Electrical Engineering, the distribution of errors in these fields may differ from our synthetic approximations. Consequently, model performance on non-CS domains should be interpreted with this synthetic nature in mind. Discrepancy Definition Our discrepancy definition focuses on meaningful mismatches that impact reproducibility, explicitly excluding simple bugs, hyperparameter configuration mismatches, or documentation nits. While this focuses the task on scientific validity, it means SCICOQA does not cover the full spectrum of software engineering defects that may exist in research code. Dataset Size With 611 discrepancies, our dataset is relatively small compared to large-scale pretraining corpora. This size is deliberate trade-off to ensure high quality: real-world discrepancies are naturally sparse, and we employed rigorous manual verification process to guarantee that every entry constitutes meaningful mismatch rather than trivial error or noise."
        },
        {
            "title": "Ethical Considerations",
            "content": "Data Release We constructed the synthetic portion of SCICOQA using repositories with permissive licenses (MIT, Apache 2.0, BSD, CC-BY) to ensure respectful redistribution of code. For real-world discrepancies derived from GitHub issues and reproducibility reports, we leverage the publicly available data to create our ground truth data by contextualizing and rephrasing the original issue. None of our data contains any personal information; however, since we work with scientific publications, these are closely associated with the authors of the respective papers. We acknowledge that highlighting discrepancies in specific authors work may be perceived negatively. We emphasize that these discrepancies are treated as scientific artifacts for improving community reproducibility standards, rather than criticisms of individual researchers. there is risk of over-reliance; given the low recall rates demonstrated in our experiments (45.7% for GPT-5), automated tools should not yet be used as the sole arbiter of papers validity. Relying blindly on these systems could lead to false sense of security regarding the reproducibility and validity of paper."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Jan Buchmann, Bhavyajeet Singh and Vatsal Venkatkrishna for their helpful feedback throughout the paper-writing process. This work has been funded by the LOEWE Distinguished Chair Ubiquitous Knowledge Processing, LOEWE initiative, Hesse, Germany (Grant Number: LOEWE/4a//519/05/00.002(0002)/81) and by the German Federal Ministry of Research, Technology and Space and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE."
        },
        {
            "title": "References",
            "content": "Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2025. ResearchAgent: Iterative research idea generation over scientific literature with large language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 67096738, Albuquerque, New Mexico. Association for Computational Linguistics. Monya Baker. 2016. 1,500 scientists lift the lid on reproducibility. Nature, 533(7604):452454. Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, and 80 others. 2025. NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model. CoRR, abs/2508.14444. Joeran Beel, Min-Yen Kan, and Moritz Baumgart. 2025. Evaluating Sakanas AI Scientist: Bold Claims, Mixed Results, and Promising Future? SIGIR Forum, 59(1):120. Automation Risks The benchmarked models in this work are intended to assist in quality assurance of scientific papers and their codebases. However, Federico Bianchi, Yongchan Kwon, Zachary Izzo, Linjun Zhang, and James Zou. 2025. To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis. CoRR, abs/2512.05925. Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosiute, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, and 27 others. 2022. Measuring Progress on Scalable Oversight for Large Language Models. CoRR, abs/2211.03540. Jonathan B. Buckheit and David L. Donoho. 1995. WaveLab and Reproducible Research, pages 5581. Springer New York, New York, NY. Christian Collberg and Todd A. Proebsting. 2016. Repeatability in computer systems research. Communications of the ACM, 59(3):6269. Anh Dau, Jin L.c. Guo, and Nghi Bui. 2024. DocChecker: Bootstrapping code large language model for detecting and resolving code-comment inconsistencies. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 187194, St. Julians, Malta. Association for Computational Linguistics. DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. CoRR, abs/2501.12948. DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, and 21 others. 2024. DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence. CoRR, abs/2406.11931. Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. 2019. Show your work: Improved reporting of experimental results. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2185 2194, Hong Kong, China. Association for Computational Linguistics. Nils Dycke and Iryna Gurevych. 2026. Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: New Counterfactual Evaluation Framework. Transactions of the Association for Computational Linguistics. To appear. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Xiang Ren, Allyson Ettinger, Zaïd Harchaoui, and Yejin Choi. 2023. Faith and Fate: Limits of Transformers on Compositionality. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Shubham Gandhi, Dhruv Shah, Manasi Patwardhan, Lovekesh Vig, and Gautam Shroff. 2025. ResearchCodeAgent: An LLM Multi-agent System for Automated Codification of Research Methodologies. In AI for Research and Scalable, Efficient Systems, pages 337, Singapore. Springer Nature Singapore. Gemini Team. 2025. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. CoRR, abs/2507.06261. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018. Deep Reinforcement Learning That Matters. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 3207 3214. AAAI Press. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024. RULER: Whats the Real Context Size of Your Long-Context Language Models? In First Conference on Language Modeling. Chuxuan Hu, Liyun Zhang, Yeji Lim, Aum Wadhwani, Austin Peters, and Daniel Kang. 2025. REPRObench: Can agentic AI systems assess the reproducibility of social science research? In Findings of the Association for Computational Linguistics: ACL 2025, pages 2361623626, Vienna, Austria. Association for Computational Linguistics. Matthew Hutson. 2018. Artificial intelligence faces reproducibility crisis. Science, 359(6377):725726. Greg Kamradt. 2023. Needle In Haystack - Pressure Testing LLMs. https://github.com/ gkamradt/LLMTest_NeedleInAHaystack. GitHub repository. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, pages 611626. ACM. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Scott Smith, Yian Yin, and 1 others. 2024. Can large language models provide useful feedback on research papers? large-scale empirical analysis. NEJM AI, 1(8):AIoa2400196. Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song, Zehao Lin, Yebin Yang, Simin Niu, Hanyu Wang, Bo Tang, Feiyu Xiong, Keming Mao, and Zhiyu Li. 2025. SurveyX: Academic Survey Automation via Large Language Models. CoRR, abs/2502.14776. Zachary C. Lipton and Jacob Steinhardt. 2019. Troubling Trends in Machine Learning Scholarship. ACM Queue, 17(1):80. Jiawei Liu, Jia Le Tian, Vijay Daita, Yuxiang Wei, Yifeng Ding, Yuhan Katherine Wang, Jun Yang, and Lingming Zhang. 2024a. RepoQA: Evaluating Long Context Code Understanding. CoRR, abs/2406.06025. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Ryan Liu and Nihar B. Shah. 2023. ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing. CoRR, abs/2306.00622. Llama-Nemotron Team. 2025. Llama-Nemotron: Efficient Reasoning Models. CoRR, abs/2505.00949. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob N. Foerster, Jeff Clune, and David Ha. 2024. The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. CoRR, abs/2408.06292. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2025. GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Mistral AI and All Hands AI. 2025. Devstral: Finetuning Language Models for Coding Agent Applications. CoRR, abs/2509.25193. OpenAI. 2023. GPT-4 Technical Report. CoRR, abs/2303.08774. OpenAI. 2025a. GPT-5 System Card. System card, OpenAI. OpenAI. 2025b. GPT-OSS-120B & GPT-OSS-20B Model Card. CoRR, abs/2508.10925. Arjun Panickssery, Samuel R. Bowman, and Shi Feng. 2024. LLM Evaluators Recognize and Favor Their In Advances in Neural InforOwn Generations. mation Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Sheena Panthaplackel, Junyi Jessy Li, Milos Gligoric, and Raymond J. Mooney. 2021. Deep Just-In-Time Inconsistency Detection Between Comments and Source Code. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 427 435. AAAI Press. Roger D. Peng. 2011. Reproducible Research in Computational Science. Science, 334(6060):12261227. Joelle Pineau, Koustuv Sinha, Genevieve Fried, Rosemary Nan Ke, and Hugo Larochelle. 2019. ICLR Reproducibility Challenge 2019. ReScience C, 5(2):5. Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Lariviere, Alina Beygelzimer, Florence dAlche Buc, Emily Fox, and Hugo Larochelle. 2021. Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program). Journal of Machine Learning Research, 22(164):120. Matt Post. 2018. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186 191, Brussels, Belgium. Association for Computational Linguistics. Fazle Rabbi and Md. Saeed Siddik. 2020. Detecting Code Comment Inconsistency using Siamese Recurrent Network. In ICPC 20: 28th International Conference on Program Comprehension, Seoul, Republic of Korea, July 13-15, 2020, pages 371375. ACM. Delip Rao, Jonathan Young, Thomas Dietterich, and Chris Callison-Burch. 2024. WithdrarXiv: Large-Scale Dataset for Retraction Study. CoRR, abs/2412.03775. Abhinav Rastogi, Albert Q. Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo, Karmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, Léonard Blier, Lucile Saulnier, Matthieu Dinot, Maxime Darrin, Neha Gupta, Roman Soletskyi, Sagar Vaze, Teven Le Scao, Yihan Wang, and 80 others. 2025. Magistral. CoRR, abs/2506.10910. Inderjot Kaur Ratol and Martin P. Robillard. 2017. Detecting fragile comments. In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering, ASE 2017, Urbana, IL, USA, October 30 - November 03, 2017, pages 112122. IEEE Computer Society. Anna Rogers and Isabelle Augenstein. 2020. What can we do to improve peer review in NLP? In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 12561262, Online. Association for Computational Linguistics. Guoping Rong, Yongda Yu, Song Liu, Xin Tan, Tianyi Zhang, Haifeng Shen, and Jidong Hu. 2025. Code Comment Inconsistency Detection and Rectification Using Large Language Model. In 47th IEEE/ACM International Conference on Software Engineering, ICSE 2025, Ottawa, ON, Canada, April 26 - May 6, 2025, pages 18321843. IEEE. Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. 2025. Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning. CoRR, abs/2504.17192. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2025. Can LLMs Generate Novel Research Ideas? LargeScale Human Study with 100+ NLP Researchers. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Zachary S. Siegel, Sayash Kapoor, Nitya Nadgir, Benedikt Stroebl, and Arvind Narayanan. 2024. CORE-Bench: Fostering the Credibility of Published Research Through Computational Reproducibility Agent Benchmark. Transactions on Machine Learning Research, 2024. Koustuv Sinha, Maurits Bleeker, Samarth Bhargav, Jessica Zosa Forde, Sharath Chandra Raparthy, Jesse Dodge, Joelle Pineau, and Robert Stojnic. 2023. ML Reproducibility Challenge 2022. ReScience C, 9(2). Koustuv Sinha, Jesse Dodge, Sasha Luccioni, Jessica Zosa Forde, Sharath Chandra Raparthy, Joelle Pineau, and Robert Stojnic. 2022. ML Reproducibility Challenge 2021. ReScience C, 8(2). Koustuv Sinha, Joelle Pineau, Jessica Forde, Rosemary Nan Ke, and Hugo Larochelle. 2020. NeurIPS ReScience C, 2019 Reproducibility Challenge. 6(2):11. Guijin Son, Jiwoo Hong, Honglu Fan, Heejeong Nam, Hyunwoo Ko, Seungwon Lim, Jinyeop Song, Jinha Choi, Gonçalo Paulo, Youngjae Yu, and Stella Biderman. 2025. When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research. CoRR, abs/2505.11855. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. 2025. PaperBench: Evaluating AIs Ability to Replicate AI Research. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net. Theo Steiner and Rui Zhang. 2022. Code Comment Inconsistency Detection with BERT and Longformer. CoRR, abs/2207.14444. Jiabin Tang, Lianghao Xia, Zhonghang Li, and Chao Huang. 2025. AI-Researcher: Autonomous Scientific Innovation. CoRR, abs/2505.18705. Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Shengzhu Yin, and 10 others. 2024. SciCode: Research Coding Benchmark Curated by Scientists. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. 2024. LLMs cannot find reasoning errors, but can correct them given the error location. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1389413908, Bangkok, Thailand. Association for Computational Linguistics. Henrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram Panyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang Chen, Daniel Cer, Alice Lisak, Min Choi, Lucas Gonzalez, Omar Sanseviero, Glenn Cameron, Ian Ballantyne, Kat Black, Kaifeng Chen, and 70 others. 2025. EmbeddingGemma: Powerful and Lightweight Text Representations. CoRR, abs/2509.20354. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, and Yue Zhang. 2024. AutoSurvey: Large Language Models Can Automatically Write Surveys. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Anjiang Wei, Jiannan Cao, Ran Li, Hongyu Chen, Yuhui Zhang, Ziheng Wang, Yuan Liu, Thiago S. F. X. Teixeira, Diyi Yang, Ke Wang, and Alex Aiken. 2025. EquiBench: Benchmarking large language models reasoning about program semantics via equivalence checking. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3385633869, Suzhou, China. Association for Computational Linguistics. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. 2025a. CycleResearcher: Improving Automated Research via Automated Review. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Yixuan Weng, Minjun Zhu, Qiujie Xie, Qiyao Sun, Zhen Lin, Sifan Liu, and Yue Zhang. 2025b. DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively. CoRR, abs/2509.26603. Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, and Meng Jiang. 2024. Large language models can self-correct with key condition verification. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1284612867, Miami, Florida, USA. Association for Computational Linguistics. Sarina Xi, Vishisht Rao, Justin Payan, and Nihar B. Shah. 2025. FLAWS: Benchmark for Error Identification and Localization in Scientific Papers. CoRR, abs/2511.21843. Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, and Yulan He. 2025. SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers. In Second Conference on Language Modeling. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob N. Foerster, Jeff Clune, and David Ha. 2025. The AI Scientist-v2: WorkshopLevel Automated Scientific Discovery via Agentic Tree Search. CoRR, abs/2504.08066. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 39 others. 2025. Qwen3 Technical Report. CoRR, abs/2505.09388. Christine Ye, Sihan Yuan, Suchetha Cooray, Steven Dillmann, Ian L. V. Roque, Dalya Baron, Philipp Frank, Sergio Martin-Alvarez, Nolan Koblischke, Frank J. Qu, Diyi Yang, Risa Wechsler, and Ioana Ciuca. 2025. ReplicationBench: Can AI Agents Replicate Astrophysics Research Papers? CoRR, abs/2510.24591. Tianmai M. Zhang and Neil F. Abernethy. 2025. Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation. In NeurIPS 2025 AI for Science Workshop. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024. Bench: Extending long context evaluation beyond In Proceedings of the 62nd Annual 100K tokens. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15262 15277, Bangkok, Thailand. Association for Computational Linguistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Advances in Neural Information Processing Systems, volume 36, pages 4659546623. Curran Associates, Inc. Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. Dont Make Your LLM an Evaluation Benchmark Cheater. CoRR, abs/2311.01964."
        },
        {
            "title": "A GitHub Crawl Details",
            "content": "We use the GitHub API5 to search for code repositories that provide code for research paper. To obtain only repositories for single paper, we search the description and homepage fields 5https://docs.github.com/en/rest? apiVersion=2022-11-28 of the repositories, which are commonly used to indicate which paper the code is intended to implement. Specifically, we search for the following urls: arxiv.org, openreview.net, aclanthology.org, doi.org/10.1145. We further limit our search to only include code repositories that have been published since 2020."
        },
        {
            "title": "B Prompts",
            "content": "B.1 GitHub Issue Discrepancy Extraction GitHub Issue Discrepancy Extraction You are an assistant that analyzes GitHub issues of scientific codebases. Your primary goal is to determine if the code repository contains any inconsistency, discrepancy, or mismatch between what is described in the paper and implemented in the code. For this, you analyze GitHub issue and determine whether it reports concrete discrepancy. **concrete discrepancy** means the issue clearly describes mismatch between what is stated in the paper (e.g., formulas, algorithms, hyperparameters, methods, logic, or processes) and what is implemented in the repositorys code. Important: Only label issues as discrepancy if they point to _specific, concrete difference_ between paper and code. Do not label general reproducibility problems, missing details, or unrelated bugs as discrepancies. ## Not discrepancy issue Label as **Not discrepancy issue** if: - The issue is about anything other than difference between the paper and the code. - The issue describes reproducibility problems (e.g., different results) but does **not** identify concrete paper-code mismatch. - The issue is about missing information needed to reproduce results without pointing to mismatch. - The issue is about bugs or errors unrelated to the papers described methods or experiments. ## Discrepancy issue Label as **Discrepancy issue** if: - The issue explicitly reports mismatch between the paper and the code implementation. - The mismatch can involve hyperparameters, formulas, algorithms, logic, processes, or other settings described in the paper. ## Response Format After analyzing the issue in detail and applying the definitions above, provide your final classification in the structure defined below: ### Final Answer yaml issue_label: <the issue label: \"Not discrepancy issue\" or \"Discrepancy issue\"> ## Issue {issue} B.2 GitHub Discrepancy Verification GitHub Issue Discrepancy Verification Your task is to verify paper-code discrepancy described in GitHub issue. Your goal is to verify whether the discrepancy is valid or not. Follow these steps to verify the discrepancy: 1. Analyze the issue and ensure you understand exactly the claimed discrepancy between the paper and the code. 2. Analyze the paper and the code and understand in detail the relevant paper sections and code files. 3. Using your understanding of paper, code, and the discrepancy in the issue, analyze whether the discrepancy is valid or not. discrepancy is valid, if the claimed discrepancy actually exists and there is difference between the paper description and the code. 4. Provide your final judgement whether the reported discrepancy is valid or not, and if so summary and the relevant paper sections and code files in the format below: yaml is_valid_discrepancy: <yes or no> is_valid_discrepancy_reason: <provide short explanation for your judgement> discrepancy_summary: <if valid provide the following description, else this should be empty: summary of the discrepancy between the paper and the code in 3-8 sentences. Your description should contain three parts focusing on the discrepancy: 1) summarize what is described in the paper, 2) summarize what is implemented in the code, and 3) summarize the difference. Do not speculate about the impact.> relevant_paper_sections: - <verbatim quote any parts from the paper that are relevant to the discrepancy> - <if there are multiple relevant parts, paste each of them.> relevant_code_files: - <name any code files that are relevant to the discrepancy by providing the file name> - <if there are multiple relevant code files, paste each of them.> ## Issue {issue} ## Paper {paper} ## Code {code} B.3 Reproducibility Paper Discrepancy"
        },
        {
            "title": "Extraction",
            "content": "Reproducibility Paper Discrepancy Extraction You are an assistant that analyzes reproducibility reports of scientific papers. Your primary goal is to detect whether the report identifies any discrepancies between the original paper and the original code repository. ## What counts as discrepancy - concrete papercode discrepancy means the report clearly describes mismatch between what is stated in the original paper (e.g., formulas, algorithms, logic, methods, processes, or other settings) and what is implemented in the original code repository. - Each distinct mismatch should be reported as separate item. - Hyperparameter mismatches (e.g., learning rate, batch size, dropout rate) do not count as discrepancies, since these are typically configurable in code repositories. - If the report only describes differences between reproduced results and original results, without identifying papercode mismatch, do not list it. - If the report speculates about possible mismatch (uncertain or ambiguous wording), still list it, but mark it with confindence: low. ## What does not count as discrepancy - General reproducibility problems (e.g., we could not match the reported results). - Missing information in the paper (e.g., the authors did not specify X). - Missing implementation in the original code repository (e.g., the authors did not provide the code for X). - Bugs or errors in the code that are unrelated to what the paper describes. - Differences between reproduced implementation/results and the original paper. ## Output format Summarize all discrepancies found in the following structure, providing description of the discrepancy, evidence supporting the discrepancy as verbatim quotes from the reproducibility report, and confidence estimate of the authors on the reported discrepancy. yaml concrete_paper_code_discrepancies: - description: \"<38 sentence descriptive summary of the first discrepancy>\" evidence: - \"<paste any evidence (e.g. paragraph describing the discrepancy) from the reproducibility report that support the discrepancy.>\" - \"<If there are multiple evidence, paste each of them.>\" confidence: <Estimate of the confidence the authors have on the reported discrepancy. One of: low, medium, high>- ... If no discrepancies are reported, return: yaml concrete_paper_code_discrepancies: [] Reproducibility Report: {paper} B.4 Reproducibility Paper Discrepancy"
        },
        {
            "title": "Verification",
            "content": "Reproducibility Paper Discrepancy Verification Your task is to verify paper-code discrepancy described in reproducibility report. You will be provided with summary of the discrepancy according to the report, and potentially multiple quotes from the reproducibility report that support the discrepancy. Your goal is to verify whether the discrepancy is valid or not. Follow these steps to verify the discrepancy: 1. Analyze the description and evidence from the reproducibility report and ensure you understand exactly the claimed discrepancy between the paper and the code. 2. Analyze the paper and the code and understand in detail the relevant paper sections and code files. 3. Using your understanding of paper, code, and the reported discrepancy, analyze whether the discrepancy is valid or not. discrepancy is valid, if the claimed discrepancy actually exists, i.e. the described difference between paper and code exists. 4. Provide your final judgement whether the reported discrepancy is valid or not, and if so summary and the relevant paper sections and code files in the format below: yaml is_valid_discrepancy: <yes or no> is_valid_discrepancy_reason: <provide short explanation for your judgement> discrepancy_summary: <if valid provide the following description, else this should be empty: summary of the discrepancy between the paper and the code in 3-8 sentences. Your description should contain three parts focusing on the discrepancy: 1) summarize what is described in the paper, 2) summarize what is implemented in the code, and 3) summarize the difference. Do not speculate about the impact.> relevant_paper_sections: - <verbatim quote any parts from the paper that are relevant to the discrepancy> - <if there are multiple relevant parts, paste each of them.> relevant_code_files: - <name any code files that are relevant to the discrepancy by providing the file name> - <if there are multiple relevant code files, paste each of them.> ## Reproducibility Report with Discrepancy {discrepancy_in_report} ## Paper {paper} ## Code {code} B.5 Synthetic Discrepancy Generation Synthetic Discrepancy Generation Your task is to generate 5 realistic papercode discrepancies by introducing small, conceptually meaningful modifications to the codebase of the provided research paper. Follow these steps to perform the edit: 1. Carefully read and understand both the research paper and the entire code repository provided below. Your goal is to identify the key ideas, methods, and components described in the paper and how they correspond to the implementation in the code. 2. Your changes must adhere to the following constraints: - **Small**: The changes must affect few lines of code or short function. It may affect multiple files, but only to the extent necessary to create coherent and realistic discrepancy. - **Relevance**: The changes must relate directly to core scientific or algorithmic idea of the paper and would likely have an impact on reproducibility of results and validity of claims. - **Significance**: The changes must introduce conceptual discrepancy not simple hyperparameter, or formatting change, or change that could be fixed via simple (command line) argument. - **Scope**: The changes should not all rely on already implemented features of the code base, but also implement new features or introduce modifications. Balance between relying on existing features and implementing new features. - **No Comments**: Do not add comments to the changed code which would easily allow to identify the discrepancy. - **No Bugs**: The introduced discrepancies should not be bugs, i.e., code that could be detected as erroneous by inspecting the code itself. The discrepancy must be related to both the paper and the code to be identified. - **No Implications**: The discrepancies must not rely on anything the paper only implies or assumes, but the paper must clearly conflict with the code after the change, or omit an important concept that is implemented in the changed code. 3. Your changes can be one of the following types. You can create multiple discrepancies of the same or different types. - **Paper Omission**: Modify the code such that it implements concept or idea that is not described in the paper. - **Code Omission**: Modify the code such that it drops specific concept or idea that is described in the paper. - **Difference**: Modify the code such that there is difference between the paper and the code, i.e., the paper describes one thing and the code implements another, e.g., by changing the order of steps, operations, or core logic. 4. Decide which paper-code discrepancies are most appropriate for the given paper, choosing from the following categories. Note, you can create multiple discrepancies of the same or different types. - **Loss**: changes to loss definition or terms - **Algorithm**: changes in the order of steps, operations, or core logic - **Training**: changes to the learning process, schedule, or optimization - **Evaluation**: changes to evaluation logic, metrics, or scripts - **Model**: architectural or initialization changes - **Data**: dataset usage, preprocessing, augmentation, or filtering - **Other**: other types of discrepancies that are not covered by the above categories but appropriate for the given paper. 5. Generate 5 discrepancies in the following strict format: md # Discrepancy 1 - Type: <choose one from: Paper Omission, Code Omission, Difference> - Category: <choose one from: Loss, Algorithm, Training, Evaluation, Model, Data, Other> - Description: <a summary of the discrepancy between the paper and the code in 3-8 sentences. When referring to the code, do not mention original or modified, but assume the code is published with the modification and discrepancy. Your description should contain three parts focusing on the discrepancy: 1) summarize what is described in the paper, 2) summarize what is implemented in the modified code, and 3) summarize the difference. Do not speculate about the impact.> ## Code Changes <ORIGINAL CODE: <relative/path/to/file.py> (paste the relevant lines of the original code exactly as they appear) ======= (paste the lines of the modified code containing your change that replace the original code) >DISCREPANCY If multiple files are affected, repeat the diff block for each. ## Relevant Paper Sections - <verbatim quote any parts from the paper that are relevant to the discrepancy.> - <if there are multiple relevant parts, paste each of them.> # Discrepancy 2 ... # Discrepancy 3 ... ## Paper {paper} ## Code {code} B.6 Discrepancy Prediction Discrepancy Prediction You are an expert in analyzing scientific papers and their code implementations. Your task is to carefully identify concrete discrepancies between what is described in paper and what is actually implemented in the code. ## What counts as discrepancy - concrete papercode discrepancy means mismatch between what is stated in the original paper (e.g., formulas, algorithms, logic, methods, processes, or other settings) and what is implemented in the original code repository. - Each distinct mismatch should be reported as separate item. ## What does not count as discrepancy - Missing information in the paper like hyperparameters (e.g., the authors did not specify X). - Hyperparameter mismatches (e.g., learning rate, batch size, dropout rate), since these are typically configurable in code repository. - Missing implementation in the original code repository (e.g., the authors did not provide the code for X). - Bugs or errors in the code that are unrelated to what the paper describes. ## Output format Provide your findings in the following YAML structure: yaml discrepancies: - <a summary of the discrepancy between the paper and the code in 3-8 sentences. Your description should contain three parts focusing on the discrepancy: 1) summarize what is described in the paper, 2) summarize what is implemented in the code, and 3) summarize the difference. Do not speculate about the impact.> - <if there are multiple discrepancies, put each of them in separate item.> ## Paper {paper} ## Code B.7 Discrepancy Evaluation Discrepancy Evaluation Your task is to evaluate whether reference paper - code discrepancies matches predicted paper - code discrepancy. Follow these steps: 1. Analyze which part of the paper or code each discrepancy is describing. Extract the core claims and issues from the reference and predicted discrepancies. 2. Analyze whether the core claims are about the same issue, i.e. if they describe the same or different paper-code discrepancies. The two discrepancies might use different wording or one might be more detailed than the other. Focus on whether the issue is the same, even if minor details are different. However, if they describe different issues (even about the same topic or part of the paper or code) they do not match. 3. Provide brief explanation of your reasoning. ## Reference Paper-Code Discrepancy {reference discrepancy} ## Predicted Paper-Code Discrepancy {predicted discrepancy} ## Answer Format Provide your answer in the following format: yaml core_claim_reference: <core claim from reference discrepancy > core_claim_predicted: <core claim from predicted discrepancy > reasoning: <explanation of why the core claims concern the same issue > match: <yes no >"
        },
        {
            "title": "C Implementation Details",
            "content": "C."
        },
        {
            "title": "Issue Processing",
            "content": "For the initial GitHub issue classification with Qwen3 4B Thinking, we used low temperature of 0.2 to minimize generation variance and promote strict adherence to the classification schema. For the verification of discrepancies from GitHub issues with GPT-5 we checked whether an issue description contained screenshots of the paper or code. If so, we replaced these with their text equivalent. C.2 Versioning To ensure that the paper and code still contain the discrepancy, we provide links to versioned publications for arXiv, using the version at the time of the reporting of the discrepancy, i.e., publication of the reproducibility report or creation of the GitHub issue. Similarly, for the codebases, we use the commit history to obtain the version of the code at the time of the reporting of the discrepancy. Our dataset contains the versioned links to both the paper (for arxiv) and codebase. For the synthetic data, we take version of the codebase at 31-10-2025. C.3 Paper Processing We provide as input the paper text in markdown. We use MistralOCR6 to convert the PDF to markdown. We exclude figures, although the captions are included. We further remove the references section from all papers. C.4 Code Processing To provide the code in the prompt, we obtain all files from the GitHub repository at target date using the commit history, which we set to the date when the original GitHub issue was raised or the reproducibility paper was published. For the synthetic data, we set the cutoff at 31-10-2025. Having versioned and static code repository is crucial to ensure reproducibility of our experiments and to make sure paper-code discrepancies have not been fixed in our dataset. We filter the files to include only files that contain relevant content, mainly excluding dataset files. Specifically, we include files with the following extensions: .c, .cc, .cpp, .cu, .h, .hpp, .java, .jl, .m, .matlab, .Makefile, .md, .pl, .ps1, .py, .r, .sh, config.txt, .rs, readme.txt, requirements_dev.txt, requirements-dev.txt, requirements.dev.txt, requirements.txt, .scala, .yaml, .yml. We obtain this list by manually inspecting the file extensions in the repositories of our dataset. We further process all jupyter notebook files to Python files by removing their output. For the prompt, we further include list of all files in the repository to the model. C.5 Context Processing The markdown paper and processed code repository are inserted into the prompt. We fill the prompt up to 90% of the context size to leave space for the reasoning (if supported by the model) and output. If the paper and code would exceed the context window, we truncate entire files from the end of the code prompt until it fits into the context window. 6https://mistral.ai/news/mistral-ocr C.6 Model and Decoding Configuration We deploy the open-weight models with FP16, except for the GPT-OSS models, which were only published in MXFP4. We utilize the Ollama and vLLM libraries (Kwon et al., 2023) on 1-4 A100 80GB GPUs, depending on the models VRAM requirements; the commercial models are inferred via their respective APIs. We generally set the temperature to 1.0, and for the GPT-5, GPT-OSS, and Gemini models, set high reasoning effort/budget to maximize performance. For the Gemini models, we set high reasoning budget of 24k tokens; for the GPT-5 models and GPT-OSS, we set high reasoning budget. Further details about each model can be obtained from the respective model card linked in Table 3."
        },
        {
            "title": "D Synthetic Code Analysis",
            "content": "We conduct quantitative analysis of the synthetic data to validate that our generation pipeline produces minimal changes, leaving the main code intact. Figure 6 presents the analysis of the generated code diffs. Code Changes We find the generated discrepancies in the code to be highly targeted. The vast majority affect only single file (1.08 0.29 on average), with maximum of three files. Similarly, the number of distinct generated diffs is low, with mean of 1.24 0.59 snippets per discrepancy. This confirms that the generated errors are specific deviations in logic rather than large-scale refactors. Similarly, the number of line edits (i.e., added or removed lines) further supports the subtlety of the dataset. The distribution of line counts is heavily skewed towards small edits: On average, 2.31 2.78 lines are added and 3.17 4.51 lines are removed. The slightly higher count for removed lines suggests the discrepancies often simplify logic (e.g., removing normalization step) rather than adding complex boilerplate. Code Similarity To quantify the similarity between the original and modified code, we calculate the Jaccard similarity. We measure the similarity between the entire generated code (which often contains few more lines that are not changed), and only code lines that were modified. Comparing the entire code block generated by the model against the original yields high mean similarity of 0.74 0.20. This indicates that the surrounding context and structure remain largely identiFigure 6: Quantitative analysis of synthetic code modifications. We show the distribution of number of changed files, number of generated diff snippets, added (green) vs. removed (orange) lines in those snippets, and Jaccard similarity between the original and modified code. Model Paper Model Card deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct deepseek-ai/DeepSeek-R1-Distill-Qwen-32B deepseek-ai/DeepSeek-R1-Distill-Llama-8B gemini-2.5-flash gemini-2.5-flash-lite gemini-2.5-pro openai/gpt-oss-120b openai/gpt-oss-20b gpt-5-2025-08-07 gpt-5-codex gpt-5-mini-2025-08-07 gpt-5-nano-2025-08-07 DeepSeek Coder 16B v2 DeepSeek R1 32B DeepSeek R1 8B Gemini 2.5 Flash Gemini 2.5 Flash Lite Gemini 2.5 Pro GPT OSS 120B GPT OSS 20B GPT-5 GPT-5 Codex GPT-5 Mini GPT-5 Nano Devstral 24B Small Magistral 24B Small Nemotron Nano 9B v2 Nemotron Super 49B v1.5 Llama-Nemotron Team (2025) Qwen3 30B Coder Qwen3 30B Instruct Qwen3 30B Thinking Qwen3 4B Instruct Qwen3 4B Thinking DeepSeek-AI (2024) DeepSeek-AI (2025) DeepSeek-AI (2025) Gemini Team (2025) Gemini Team (2025) Gemini Team (2025) OpenAI (2025b) OpenAI (2025b) OpenAI (2025a) OpenAI (2025a) OpenAI (2025a) OpenAI (2025a) Mistral AI and All Hands AI (2025) mistralai/Devstral-Small-2507 mistralai/Magistral-Small-2509 Rastogi et al. (2025) nvidia/NVIDIA-Nemotron-Nano-9B-v2 Basant et al. (2025) nvidia/Llama-3_3-Nemotron-Super-49B-v1_5 Qwen/Qwen3-Coder-30B-A3B-Instruct Qwen/Qwen3-30B-A3B-Instruct-2507 Qwen/Qwen3-30B-A3B-Thinking-2507 Qwen/Qwen3-4B-Instruct-2507 Qwen/Qwen3-4B-Thinking-2507 Yang et al. (2025) Yang et al. (2025) Yang et al. (2025) Yang et al. (2025) Yang et al. (2025) Context Window Knowledge Cutoff Release Date Nov 2023 Jun 2024 131k Jul 2024 Jan 2025 131k Jan 2025 Jul 2024 131k Jan 2025 Aug 2025 1,047k Jan 2025 Aug 2025 1,047k Jan 2025 Aug 2025 1,047k Jun 2024 Aug 2025 131k Jun 2024 Aug 2025 131k Sep 2024 Aug 2025 272k 272k Sep 2024 Aug 2025 272k May 2024 Aug 2025 272k May 2024 Aug 2025 131k Mar 2025 Jul 2025 Sep 2025 Jun 2025 131k Apr 2025 Aug 2025 131k Jul 2025 131k Dec 2023 Jul 2025 262k Mar 2025 Jul 2025 262k Mar 2025 Jul 2025 262k Mar 2025 Jul 2025 262k Mar 2025 Jul 2025 262k Mar 2025 Table 3: Model versions including their context window, knowledge cutoff (date of most recent pre-training data), and release date. cal. Comparing only the specific lines that differ (added vs. removed) yields mean similarity of 0.50 0.29. This moderate overlap suggests the changes are often modifications of existing statements (e.g., changing an operator or variable) rather than complete replacements. Overall, the data confirms that the synthetic discrepancies are precise and minimal, strictly adhering to the small and conceptually meaningful constraints of the synthetic data generation prompt (B.5)."
        },
        {
            "title": "E Analysis by Programming Language",
            "content": "although their overall presence is relatively small. E.1 Results by Programming Language In the synthetic dataset, the known injection points allow us to analyze detection performance across different programming languages. Since the dataset is skewed towards Python, drawing conclusions for specific low-resource languages is difficult due to the small sample size. Therefore, we group all non-Python instances into unified subset of 46 samples for analysis (\"Not Python\"). The resulting recall scores are presented in Table 4 Figure 7 shows the distribution of programming languages in the SCICOQA dataset. Since the real data originates from the CS/AI domain, we observe Python as the dominant language, along with small portion of C/C++, Matlab, and CUDA files. Besides Python, other languages are present in the synthetic data, including Java, Scala, Julia, and R, Python vs. Non-Python Gap We observe distinct behaviors across models regarding language robustness. Gemini 2.5 Pro and Flash models exhibit the largest performance drops on non-Python languages, with gaps of 10.2% and 11.6%, respectively. However, other models are more stable, for example, GPT-5 or GPT-OSS 20B perform on par Figure 7: Distribution of programming languages in SCICOQA. The columns of the plot show the distribution by number of Repos, Files, Lines, and Tokens for each respective programming language. The rows show the distribution for the real, synthetic, and combined data. soning capabilities on our synthetic injections are reliably better at identifying real-world discrepancies. While this trend might be influenced by the general capabilities of the models the strong correlation confirms that the synthetic data does not introduce noise or bias that alters the model rankings. Consequently, the synthetic subset serves as reliable proxy for evaluating performance, justifying its use for scaling the benchmark to scientific domains where real-world examples are scarce. or better on the non-python discrepancies. Language-Specific Performance We highlight MATLAB as positive outlier, where top models like GPT-5 and GPT-5 Mini achieve 90.0% recall (significantly higher than their Python performance). We hypothesize that MATLABs mathematical syntax aligns more closely with the equations presented in scientific papers, facilitating easier visual and logical alignment for the models. Meanwhile, performance on systems-level languages like C/C++ and CUDA remains volatile and generally lower, though sample sizes limit definitive conclusions."
        },
        {
            "title": "F Extended Results",
            "content": "F.1 Real vs Synthetic Recall To assess the validity of our synthetic data generation pipeline, we analyze the relationship between model performance on real-world discrepancies versus synthetic discrepancies. Fig. 8 visualizes the recall and relationship between the subsets. We observe high positive Pearson correlation of = 0.94. This indicates that the relative ranking of models remains consistent regardless of the data origin; models that demonstrate stronger reaModel Python Not Python Julia MATLAB YAML C/C++ CUDA # Discrepancies 483 GPT-5 GPT-5 Mini Gemini 2.5 Pro GPT-5 Codex GPT-OSS 20B GPT-OSS 120B Gemini 2.5 Flash GPT-5 Nano Nemotron Super 49B v1.5 Gemini 2.5 Flash Lite Qwen3 30B Coder Qwen3 30B Inst. Nemotron Nano 9B v2 DeepSeek R1 32B Qwen3 30B Think. Qwen3 4B Inst. Qwen3 4B Think. DeepSeek Coder 16B V2 DeepSeek R1 8B Devstral 24B Small Magistral 24B Small 70.4 65.4 49.3 49.1 46.6 45.3 42.0 28.8 24.0 25.3 23.4 24.0 16.8 14.1 16.1 16.8 11.4 9.3 6.8 13.3 11.0 46 69.6 67.4 39.1 43.5 50.0 39.1 30.4 30.4 28.3 26.1 23.9 17.4 26.1 19.6 15.2 8.7 6.5 4.3 2.2 2.2 8.7 66.7 75.0 33.3 25.0 50.0 33.3 33.3 25.0 25.0 16.7 33.3 25.0 25.0 8.3 8.3 8.3 0.0 8.3 0.0 0.0 8.3 10 90.0 90.0 60.0 70.0 60.0 80.0 40.0 30.0 20.0 40.0 20.0 10.0 40.0 20.0 30.0 10.0 0.0 10.0 10.0 0.0 0.0 9 75.0 66.7 50.0 50.0 66.7 41.7 33.3 58.3 41.7 50.0 41.7 33.3 50.0 25.0 16.7 0.0 41.7 0.0 0.0 0.0 25.0 60.0 60.0 40.0 40.0 60.0 40.0 40.0 40.0 40.0 40.0 0.0 0.0 0.0 20.0 20.0 0.0 0.0 0.0 0.0 0.0 0.0 4 25.0 0.0 0.0 25.0 25.0 0.0 0.0 25.0 50.0 0.0 0.0 0.0 25.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 3 100.0 100.0 33.3 66.7 33.3 33.3 66.7 0.0 33.3 0.0 33.3 33.3 0.0 66.7 0.0 33.3 0.0 0.0 0.0 33.3 0. Shell Python+YAML AVG 3 66.7 33.3 33.3 33.3 0.0 0.0 0.0 0.0 0.0 0.0 33.3 0.0 0.0 0.0 0.0 33.3 0.0 0.0 0.0 0.0 0.0 1 100.0 100.0 0.0 100.0 0.0 0.0 0.0 0.0 0.0 100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 70.4 65.7 48.3 48.7 46.8 44.7 40.9 28.9 24.3 25.5 23.4 23.4 17.5 14.5 16.0 16.0 10.9 8.9 6.4 12.3 10.8 Table 4: Discrepancy recall by programming language on the synthetic SCICOQA data. Figure 8: Correlation between model recall on the synthetic (x-axis) and real (x-axis) subsets of SCICOQA. Each point represents one of the 21 evaluated models. The dashed line visualizes the Pearson correlation. F.2 Detailed Results per Model Model # Preds. Recall All Real+Synthetic Real GH Synthetic Type RP All CS Other CO PO Diff. # Real/Synthetic 81/530 81/530 81/0 37/ 44/0 0/530 0/280 0/250 55/12 51/ 424/43 GPT-5 GPT-5 Mini Gemini 2.5 Pro GPT-5 Codex GPT-OSS 20B GPT-OSS 120B Gemini 2.5 Flash GPT-5 Nano Nemotron Super 49B v1.5 Gemini 2.5 Flash Lite Qwen3 30B Coder Qwen3 30B Inst. Nemotron Nano 9B v2 DeepSeek R1 32B Qwen3 30B Think. Qwen3 4B Inst. Qwen3 4B Think. DeepSeek Coder 16B V2 DeepSeek R1 8B Devstral 24B Small Magistral 24B Small 4.1 4.7 3.1 2.0 5.1 4.5 3.5 3.4 4.6 2.5 3.8 3.3 7.1 4.0 1.5 2.6 1.4 3.3 3.6 4.2 3.5 67.1 62.8 47.1 46.2 46.2 43.5 40.3 27.7 23.6 23.9 23.1 22.4 16.5 15.5 15.5 14.7 10.6 7.7 6.1 11.9 10.8 45.7 44.4 39.5 29.6 42.0 35.8 35.8 19.8 18.5 13.6 21.0 16.0 9.9 22.2 12.3 6.2 8.6 0.0 3.7 9.9 11.1 62.2 59.5 51.4 35.1 48.6 45.9 48.6 24.3 21.6 27.0 24.3 21.6 10.8 32.4 21.6 13.5 8.1 0.0 8.1 13.5 18. 31.8 31.8 29.5 25.0 36.4 27.3 25.0 15.9 15.9 2.3 18.2 11.4 9.1 13.6 4.5 0.0 9.1 0.0 0.0 6.8 4.5 70.4 65.7 48.3 48.7 46.8 44.7 40.9 28.9 24.3 25.5 23.4 23.4 17.5 14.5 16.0 16.0 10.9 8.9 6.4 12.3 10.8 65.7 59.6 43.2 44.6 41.1 41.1 34.3 24.6 23.9 19.6 22.9 20.0 14.6 12.5 16.1 14.6 9.3 9.6 7.5 10.4 10.4 75.6 72.4 54.0 53.2 53.2 48.8 48.4 33.6 24.8 32.0 24.0 27.2 20.8 16.8 16.0 17.6 12.8 8.0 5.2 14.4 11.2 70.1 70.1 44.8 62.7 61.2 49.3 49.3 37.3 34.3 31.3 37.3 35.8 11.9 25.4 23.9 28.4 11.9 16.4 9.0 22.4 29.9 50.6 46.8 33.8 29.9 37.7 33.8 29.9 15.6 11.7 13.0 9.1 11.7 18.2 11.7 3.9 10.4 10.4 2.6 3.9 6.5 7. 69.4 64.5 49.7 46.5 45.4 44.3 40.7 28.3 24.0 24.6 23.3 22.3 16.9 14.8 16.3 13.5 10.5 7.3 6.0 11.3 8.6 Table 5: Detailed recall performance of all evaluated models in the SCICOQA dataset. # Preds. refers to the average number of predictions the model makes per paper, GH stands for discrepancies originating from GitHub, RP originating from reproducibility papers. The synthetic data is split by the Computer Science (CS) domain and Others. The discrepancies are further split by Type, specifically Code Omissions (CO), Paper Omissions (PO), and Differences (Diff ). The # Real/Synthetic row indicates how many samples per column are from the real or synthetic data. F.3 Code Only Ablation Results"
        },
        {
            "title": "Synthetic",
            "content": "Real+Synthetic"
        },
        {
            "title": "Model",
            "content": "P+C P+C P+C GPT-5 GPT-5 Mini Gemini 2.5 Pro GPT-5 Codex GPT-OSS 20B GPT-OSS 120B Gemini 2.5 Flash GPT-5 Nano"
        },
        {
            "title": "Average",
            "content": "45.7 44.4 39.5 29.6 42.0 35.8 35.8 19.8 36.6 27.2 18.5 14.8 12.3 16.0 23.5 16.0 8.6 17.1 18.5 25.9 24.7 17.3 25.9 12.3 19.8 11.1 19. 70.4 65.7 48.3 48.7 46.8 44.7 40.9 28.9 49.3 51.7 38.7 34.5 28.9 33.4 37.2 29.6 19.4 34.2 18.7 27.0 13.8 19.8 13.4 7.5 11.3 9.4 15. 67.1 62.8 47.1 46.2 46.2 43.5 40.3 27.7 47.6 48.4 36.0 31.9 26.7 31.1 35.4 27.8 18.0 31.9 18.7 26.8 15.2 19.5 15.1 8.2 12.4 9.7 15. Table 6: Discrepancy recall depending on the input: The P+C column refers to the performance when providing paper and code, for only providing the code, and shows the absolute difference between the two."
        },
        {
            "title": "G Example Discrepancies in SCICOQA",
            "content": "Type Category Description Paper Omission Algorithm The paper conditions layers using FiLM by mapping the loss-parameter vector λ through two MLPs to obtain σ and µ and then applying the affine transform = σ + µ; there is no mention of any output activation on σ or µ, and in the style transfer details it explicitly states that σ and µ are computed via affine maps. In the repository, the FiLMBlock applies sigmoid activation to both µ and σ before using them, thereby constraining both to the (0,1) range. Thus, the code restricts the FiLM coefficients to be positive and bounded, whereas the papers description implies unconstrained affine coefficients produced by linear layers. This is clear deviation between the implementation and the method described in the paper. Paper Omission Model Code Omission Data Code Omission Training Difference Eval Difference Loss The paper states that, following GNNExplainers setup, three-layer GNN is first trained and then explained, but it does not mention any batch normalization in this backbone network. In the public code for node-classification tasks (synthetic datasets), the GCN model enables batch normalization by default (args.bn=True) and applies tf.keras.layers.BatchNormalization after all GCN layers except the last, i.e., after the first and second layers in 3-layer GCN. This BatchNorm usage is not described in the papers model description or experimental setup, making it an undocumented architectural difference between the paper and the released code. The paper describes training classifiers using combination of original MNIST data and CGNgenerated counterfactual images. This is explicitly stated in Section 3.2 and reinforced by the MNIST ablation (Appendix A.3) and Table 2 which presents an Original + CGN setting. In contrast, the released MNIST training code (mnists/train_classifier.py) only supports training on single dataset per run; the provided datasets and README usage instruct training on the counterfactual dataset alone (e.g., wildlife_MNIST_counterfactual) without combining with original data. Thus, the code implements CF-only training for MNIST classifiers rather than the papers combined original+CF setup, constituting mismatch in the MNIST training protocol. The papers FL formulation and Algorithm 1 state that each worker performs τ local updates per round and accumulates the resulting gradients g_k^(t,b) into an accumulated stochastic gradient g_k^(t)=(cid:80)_{b=0}^{τ 1} g_k(θ_k^{(t,b)}), which is then used by LBGM for projection and communication. In the code, the federated worker loop processes only one minibatch and immediately breaks, with the lines that would accumulate gradients across batches commented out; an assertion on τ is present but no actual multi-step accumulation is performed. Consequently, the implemented training uses only single local update per round (effectively τ =1) rather than accumulating τ local updates as described in the paper. The paper specifies accuracy-based confidence for question as the mean accuracy across sampled responses, i.e., the fraction of correct generations among the samples. In the code, however, the function calculate_confidence takes the list of per-sample correctness indicators (greedy_scores) and returns max(greedy_scores), which reduces confidence to binary value: 1 if any sample is correct, 0 otherwise. Moreover, while the code also computes greedy_scores_avg (the intended mean), it does not use that for the confidence fed into the downstream datasets; instead, the binary max value is stored as \"confidence\" and used for reward/align data. Thus, the implemented confidence deviates from the papers definition by using maximization rather than averaging. The paper defines the second penalty P2 to enforce the admissibility constraints (ϕ, ϕ^c) ADM(c) by penalizing only negative values of ξ(x,y) = c(x,y) ϕ(x) ϕ^c(y), specifically using min(ξ(x,y), 0)^2. In the code, P2 is implemented in admissable_penalty as torch.mean(torch.clamp(full_xi_vals, min=0)**2), which penalizes positive ξ values (since negative values are clamped to zero). Therefore, the implementation reverses the intended sign: the paper penalizes violations where ξ < 0, while the code penalizes cases where ξ > 0. This directly contradicts the papers definition for P2. Table 7: Examples from the SCICOQA dataset, including discrepancy type and category annotations. All samples are from real data."
        },
        {
            "title": "H Example Synthetic Discrepancies in SCICOQA",
            "content": "Discrepancy Code Change https://arxiv.org/abs/1906. Paper: 09436 Domain: Machine Learning (stat.ML) The paper emphasizes that classification in FDA is performed using Euclidean distances in the Fisher subspace (and shows its equivalence to LDA under Euclidean metrics). The code normalizes both projected samples and class centroids to unit norm prior to classification. This switches the effective decision metric from Euclidean distance to an angular/cosine-like similarity, diverging from the Euclidean geometry prescribed in the paper. https://arxiv.org/abs/2409. Paper: 13224 Domain: General Relativity and Quantum Cosmology (gr-qc) The paper defines squared coherence as the magnitude-squared of the complex cross-spectrum normalized by the product of the auto-spectra. In the code, coherence is computed using only the square of the real part of the cross-spectrum (cospectrum) divided by the product of the auto-spectra. Thus, while the papers definition uses Sxy2, the implementation effectively uses Re(Sxy)2, ignoring the quadrature (imaginary) component. This changes the coherence measure away from magnitude-squared coherence to co-coherence, which is not described in the paper. https://arxiv.org/abs/1911. Paper: 10194 Domain: Computer Vision (cs.CV) The paper defines the offset vector in pixel space and groups pixels by moving each pixel by its predicted offset and assigning it to the nearest predicted center. The code upscales predictions to the input resolution but does not scale the offsets accordingly during this upsampling, meaning the offsets are left in lower-resolution coordinate system when grouped at full resolution. This contradicts the papers grouping formulation that assumes offsets and coordinates are in the same resolution. Code: https://github.com/concavegit/kfda File: kfda/kfda.py centroids_ = m_classes @ self.weights_ + centroids_ = centroids_ / (np.linalg.norm( centroids_, axis=1, keepdims=True) + 1e-12) self.clf_ = NearestCentroid().fit(centroids_, self .classes_) # ... - return pairwise_kernels(X, self.X_, metric=self. kernel, **self.kwds) @ self.weights_ + = pairwise_kernels(X, self.X_, metric=self. kernel, **self.kwds) @ self.weights_ + Z_norm = np.linalg.norm(Z, axis=1, keepdims=True ) + 1e- + return / Z_norm # ... new_centroids = y_onehot.T @ projections / y_onehot.T.sum(1) + new_centroids = new_centroids / (np.linalg.norm( new_centroids, axis=1, keepdims=True) + 1e-12) concatenated_classes = np.concatenate([self. classes_, new_classes]) concatenated_centroids = np.concatenate([self.clf_ .centroids_, new_centroids]) Code: https://github.com/nz-gravity/sgvb_psd File: src/sgvb_psd/postproc/plot_coherence.py def compute_coherence(pxx, pyy, pxy): - coh_q = np.real(np.abs(pxy) ** 2) / np.real( pxx) / np.real(pyy) coh_q = (np.real(pxy) ** 2) / (np.real(pxx) * np.real(pyy)) coh_uniform = None + File: src/sgvb_psd/postproc/plot_triangle_psd_coherence.py def compute_coherence(pxx, pyy, pxy): - coh_q = np.real(np.abs(pxy) ** 2) / np.real( pxx) / np.real(pyy) coh_q = (np.real(pxy) ** 2) / (np.real(pxx) * np.real(pyy)) coh_uniform = None + Code: https://github.com/bowenc0221/panoptic-deeplab File: segmentation/model/meta_arch/panoptic_deeplab.py result = OrderedDict() for key in pred.keys(): out = F.interpolate(pred[key], size= input_shape, mode=bilinear, align_corners= True) if offset in key: scale = (input_shape[0] - 1) // (pred[key ].shape[2] - 1) out *= scale result[key] = out - - - return result Table 8: Examples from the synthetic SCICOQA data. The Code Change column shows the code snippet that has been modified to create paper-code discrepancy. Discrepancy Code Change https://arxiv.org/abs/2302. Paper: 12835 Domain: Image and Video Processing (eess.IV) The paper specifies that training uses only data fidelity term plus enforcement of the no-slip condition on the wall, explicitly noting no physics residuals. In the code, an additional divergence-free penalty term is added by computing spatial derivatives of the velocity outputs with respect to the input coordinates and penalizing their sum (divergence), scaled by configurable weight. This introduces an explicit incompressibility prior not described in the paper. https://arxiv.org/abs/2412. Paper: 04595 Domain: Numerical Analysis (math.NA) The paper derives single FourierChebyshev representation for the long-range part and does not duplicate the zero wavenumber contribution. In the code, the zero in-plane Fourier mode is already computed via dedicated zeroth-order 1D path, but the spectral path is also modified to include the kx = ky = 0 contribution. This leads to double counting of the zeroth Fourier mode in the long-range part, which the papers formulation does not include. Code: https://github.com/saitta-s/INRs-4DFlowMRI File: fit_inr.py def closure(): + + + + + + + + + + + + optimizer.zero_grad() X.requires_grad_(True) outputs = model(X) obs_loss = loss_fn(outputs[:-n_wall, :], Y0) wall_loss = loss_fn(outputs[-n_wall:, :], wall_vel[wall_idx, :]) if cfg.div_weight > 0: = outputs[:, 0] = outputs[:, 1] = outputs[:, 2] grads_u = torch.autograd.grad(u.sum(), X, create_graph=True, retain_graph=True)[0] grads_v = torch.autograd.grad(v.sum(), X, create_graph=True, retain_graph=True)[0] grads_w = torch.autograd.grad(w.sum(), X, create_graph=True, retain_graph=True)[0] div = grads_u[:, 0] + grads_v[:, 1] + grads_w[:, 2] div_loss = (div ** 2).mean() loss = obs_loss + wall_loss + cfg. div_weight * div_loss else: loss = obs_loss + wall_loss loss.backward() return loss Code: https://github.com/HPMolSim/FastSpecSoG.jl File: src/FFCT/interpolate.jl for in 1:size(H_r, 2) k_yj = k_y[j] for in 1:size(H_r, 1) k_xi = k_x[i] k2 = k_xi^2 + k_yj^2 if !(k2 $approx$ zero(T)) phase = phase_x[i] * phase_y[j] cheb_ij = cheb_mat[i, j] for in 1:size(H_r, 3) r_zk = r_z[k] H_r[i, j, k] += * phase * cheb_ij(abs(z - r_zk)) end phase = phase_x[i] * phase_y[j] cheb_ij = cheb_mat[i, j] for in 1:size(H_r, 3) r_zk = r_z[k] H_r[i, j, k] += * phase * cheb_ij( abs(z - r_zk)) - - - - - - - + + + + + https://arxiv.org/abs/2402. Paper: 18396 Domain: Biomolecules (q-bio.BM) The paper describes that the confidence model reasons about local interactions by feeding only residues whose Cα lies within 20 Å of any predicted ligand atom. The code instead crops the receptor using centroid-based distance (distance from Cα to the ligand centroid), not the minimum distance to any ligand atom. This changes which residues are kept and contradicts the papers any atom criterion. Code: confidence-bootstrapping File: utils/utils.py def crop_beyond(complex_graph, cutoff, all_atoms): https://github.com/LDeng0205/ + - + ligand_pos = complex_graph[ligand].pos center = ligand_pos.mean(dim=0, keepdim=True) receptor_pos = complex_graph[receptor].pos residues_to_keep = torch.any(torch.sum(( ligand_pos.unsqueeze(0) - receptor_pos. unsqueeze(1)) ** 2, -1) < cutoff ** 2, dim=1) residues_to_keep = torch.sum((receptor_pos - center) ** 2, -1) < cutoff ** 2 Table 8: Examples from the synthetic SCICOQA data. The Code Change column shows the code snippet that has been modified to create paper-code discrepancy. (Continued)"
        }
    ],
    "affiliations": [
        "National Research Center for Applied Cybersecurity ATHENE",
        "Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science, TU Darmstadt"
    ]
}