{
    "paper_title": "Learning to Discover Regulatory Elements for Gene Expression Prediction",
    "authors": [
        "Xingyu Su",
        "Haiyang Yu",
        "Degui Zhi",
        "Shuiwang Ji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3. The source code is released as part of the AIRS library (https://github.com/divelab/AIRS/)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . - [ 1 1 9 9 3 1 . 2 0 5 2 : r Published as conference paper at ICLR"
        },
        {
            "title": "LEARNING TO DISCOVER REGULATORY ELEMENTS\nFOR GENE EXPRESSION PREDICTION",
            "content": "Xingyu Su1*, Haiyang Yu1*, Degui Zhi2 & Shuiwang Ji1 1Texas A&M University, 2The University of Texas Health Science Center at Houston {xingyu.su,haiyang,sji}@tamu.edu,{degui.zhi}@uth.tmc.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "We consider the problem of predicting gene expressions from DNA sequences. key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3. The source code is released as part of the AIRS library (https://github.com/divelab/AIRS/)."
        },
        {
            "title": "INTRODUCTION",
            "content": "Gene expression serves as fundamental process that dictates cellular function and variability, providing insights into the mechanisms underlying development (Pratapa et al., 2020), disease (Cookson et al., 2009; Emilsson et al., 2008), and responses to external factors (Schubert et al., 2018). Despite the critical importance of gene expression, predicting it from genomic sequences remains challenging task due to the complexity and variability of regulatory elements involved. Recent advances in deep learning techniques (Avsec et al., 2021; Gu & Dao, 2023; Nguyen et al., 2024; Badia-i Mompel et al., 2023) have shown remarkable capabilities and performance in modeling long sequential data like language and DNA sequence. By capturing intricate dependencies within genomic data, these techniques provide deeper understanding of how regulatory elements contribute to transcription (Aristizabal et al., 2020). To predict gene expression, DNA language models are usually applied to encode long DNA sequences with subsequent predictor to estimate the gene expression values (Avsec et al., 2021; Nguyen et al., 2024; Gu & Dao, 2023; Schiff et al., 2024). However, those language models are typically designed to encode DNA sequences alone, overlooking the specific environments like different cell types, which leads to suboptimal performance. Instead of predicting the gene expression only using DNA sequence, which is invariant across cell types, more biological relevant formulation is to predict gene expression levels using both DNA sequence and epigenomic signals. For example, GraphReg (Karbalayghareh et al., 2022) uses epigenomic signals as input data to predict gene expression values. However, it does not integrate DNA sequences and epigenomic signals in unified manner to improve gene expression prediction. EPInformer (Lin et al., 2024) uses statistical methods to identify the epigenomic signal peaks, and focuses on regulatory elements identified by those peaks. Although obtaining better results, EPInformer still neglects the complex relationship between DNA sequences, epigenomic signals and regulatory elements, which is essential for improving prediction accuracy. *Equal contribution. Corresponding author. 1 Published as conference paper at ICLR 2025 The task of predicting gene expression levels given the DNA sequences and epigenomic signals presents several challenges. First, epigenomic signals can be measured by variety of experimental techniques, including ChIP-seq, DNase-seq, Hi-C, each with their own biases and limitations (Consortium et al., 2012; Bernstein et al., 2010; Moore et al., 2020). Additionally, the regulatory elements influencing target gene expression are often sparse and may involve long-range interactions, making them challenging to identify and integrate into predictive models. These complexities highlight the need for models that can effectively discover the actively interacted regulatory elements with the target gene on long DNA sequences. In response to these challenges, we propose Seq2Exp (Sequence to Expression), novel framework designed to improve gene expression prediction by selectively extracting relevant sub-sequences from both DNA sequences and epigenomic signals. Since DNA sequences and epigenomic signals capture different aspects of biological information, their integration offers deeper insights. For example, Hi-C/HiChIP data reveals the physical interaction frequency between distal DNA regions, and DNase-seq reflects the functional activity of regulatory elements. Effectively incorporating these signals along with DNA sequences can be highly beneficial for addressing the above challenges for gene expression prediction task. Specifically, in this work, we suggest the causal relationship between genomic data and gene expression to guide the learning process as depicted in Figure 1. Inspired by the causal relationship, we decompose the mask learning process into two components: one based on DNA sequences and the other on epigenomic signals. The proposed Seq2Exp first employs generator module to learn token-level mask based on both DNA sequences and epigenomic signals, to extract DNA sub-sequences. Then, the predictor module is applied on these extracted subsequences to predict gene expression. With information bottleneck, Seq2Exp can effectively filter out non-causal parts by constraining the mask size, ensuring that only the most relevant regions are extracted. Overall, the incorporation of the DNA sequences and epigenomic signals systematically discovers regions that are likely to influence gene expression. We summarize our contributions here: We propose framework articulating the causal relationship between epigenomic signals, DNA sequences, target gene expression and related regulatory elements. Based on the causal relationships, our framework is proposed to combine the mask probability distribution from DNA sequences and epigenomic signals, and filtering out non-causal region via information bottleneck. The proposed Seq2Exp achieves SOTA performances compared to previous gene expression prediction baselines, and demonstrates the extracted regulatory elements serve as better subsequences compared to statistical peaks calling methods of epigenomic signals such as MACS3."
        },
        {
            "title": "2.1 TASK DESCRIPTION",
            "content": "Let seq = [x1, , xL] denote the DNA sequence with length L, where each token xi R41 is one-hot vector representing nucleotide from the set {A, C, G, T}. For this DNA sequence, the corresponding epigenomic signals are denoted as sig = [s1, , sL], where si Rd1 represents different signals. By using both the DNA sequence and epigenomic signals, the task aims to predict the target gene expression denoted as R. To achieve this target, we propose our framework to extract the active regulatory elements by learning token-level binary mask = [m1, , mL], where mi {0, 1} or soft mask where mi [0, 1]. Specifically, in our implementation, each example contains one target gene. We first identify the transcription start site (TSS) of the target gene, then select input sequences seq and sig consist of = 200, 000 base pairs, centered on the TSS. Then, the entire sequences provide sufficient contextual information for accurate prediction of the target gene expression value . 2.2 RELATED WORKS DNA language model has been proposed recently to apply language machine learning models to long DNA sequences (Nguyen et al., 2024; Gu & Dao, 2023; Schiff et al., 2024) and solve vari2 Published as conference paper at ICLR ous downstream tasks. Two notable methods in this area are HyenaDNA (Nguyen et al., 2024) and Caduceus (Schiff et al., 2024). HyenaDNA utilizes the Hyena operator (Poli et al., 2023) to process long DNA sequences. Caduceus introduces bidirectional Mamba (Gu & Dao, 2023) for DNA sequences, providing linear complexity for long sequence modeling while also considering the reverse complement of the DNA sequences. These methods offer powerful approach for modeling long sequence data, such as DNA, and can be fine-tuned for tasks like gene expression prediction. However, they usually only considers DNA sequences as input, and do not explicitly consider the additional epigenomic signals during the prediction. Since these signals often carry meaningful information, such as physical interaction frequency and functional activity, incorporating them into the model could further enhance its performance on the gene expression prediction task. Gene expression prediction is one of the fundamental tasks in bioinformatics (Segal et al., 2002). Numerous studies (Agarwal & Shendure, 2020; Karbalayghareh et al., 2022; Avsec et al., 2021; Lin et al., 2024) have attempted to predict gene expression values directly from DNA sequences. Enformer (Avsec et al., 2021), for instance, tries to only encode DNA sequences as input and employs convolutional and transformer blocks to predict 5,313 human genomic tracks and 1,643 mouse tracks. In contrast, GraphReg (Karbalayghareh et al., 2022), incorporates graph attention network to account for Hi-C/HiChIP interactions between DNA sub-sequences, improving gene expression predictions by considering physical interaction frequencies. However, both methods either rely on epigenomic signals or DNA sequences as input data, without integrating both data types. Recently, EPInformer (Lin et al., 2024) has advanced this approach by integrating both DNA sequences and epigenomic signals for gene expression prediction. EPInformer first identifies enhancer regions from the DNA sequences based on DNase-seq signals, treating epigenomic signals as enhancer features, and then use promoter-enhancer interactions for gene expression prediction. Despite this progress, EPInformer selects enhancer regions solely based on epigenomic signal peaks, overlooking the complex relationships between DNA sequences, epigenomic signals, and predicted gene expression values. This highlights the need for machine learning methods capable of learning to extract relevant regions in more comprehensive manner. 2.3 BACKGROUND OF INFORMATION BOTTLENECK To effectively extract active regulatory elements from DNA sequences, it is important to understand the concept of the information bottleneck. The information bottleneck method is widely used technique in machine learning on tasks for images (Alemi et al., 2016; Chen et al., 2018), language data (Belinkov et al., 2020; Lei et al., 2016; Paranjape et al., 2020; Bastings et al., 2019; Jain et al., 2020) or graph data (Wu et al., 2020; Miao et al., 2022). Its goal is to maximize the mutual information between compressed representations and the target variable , expressed as I(Z; ), while controlling the information extracted from the input X. Note that in the proposed method, represents the target gene expression. straightforward approach would be to set = X, but this retains the full complexity of X, which makes the optimization process challenging, especially with the long and noisy nature of DNA sequences. To address this, researchers impose constraint on the information transferred from to Z, ensuring that I(X; Z) Ic, where Ic is an information constraint that allows us to capture only the most critical compressed representations. The information bottleneck objective becomes maximizing: = I(Z; ) βI(X; Z), (1) where β is hyperparameter that balances the trade-off between compression and relevance. However, directly optimizing this objective is challenging. To overcome this, Chen et al. (2018) proposes to maximize lower bound approximation, which leads to minimizing the following expression: 1 (cid:88) i= Epθ(Zxi)[ log qϕ(yiZ)] + βKL[pθ(Zxi), r(Z)], (2) where pθ(Zxi) is parametric approximation of Z, qϕ(yiZ) is variational approximation of the true distribution p(yiZ), and r(Z) approximates the marginal distribution p(Z). 3 Published as conference paper at ICLR 2025 Figure 1: Causal relationships between epigenomic signals, sequence, gene expression and related regulatory elements."
        },
        {
            "title": "3 PROPOSED METHODS",
            "content": "In this section, we present our framework Seq2Exp. We first present our motivation for predicting gene expression with learnable extraction of effective regulatory elements. We illustrate the causal relationship among regulatory elements, epigenomic signals and DNA sequences as shown in Figure 1. Motivated by this structural causal model (SCM) (Pearl, 2009; Pearl et al., 2000; Wu et al., 2022), our framework provides learnable approach to effectively extract effective regulatory elements, considering both DNA sequences and epigenomic signals, through an information bottleneck mechanism. 3.1 CAUSAL RELATIONSHIP AMONG REGULATORY ELEMENTS, DNA SEQUENCE AND EPIGENOMIC SIGNALS The interactions between target gene and regulatory elements are complex, particularly when multiple potential regulatory elements are involved. Meanwhile, long sequences and distal interactions require large search region, further complicating the discovery of effective regulatory elements that influence target gene expression. In this study, we take use of epigenomic signals sig from laboratory experiments as well as the DNA sequence seq for target gene expression , and formulate their relationships with the proposed three categories of regulatory elements. Rg: Regulatory elements that have the potential to interact with target gene. However, they might not influence target gene expression if they are inactive in specific cell type or are distant from the target gene. Rm: Regulatory elements discovered from measurement. Typically, the region with strong measured epigenomic signals, such as peaks in DNase-seq, are more likely to influence the gene expression. However, there are usually multiple genes within sequence and the association of Rm with target gene remains unknown. Rag: Regulatory elements actively interacted with target gene. It is identified as the causal component for the final target gene expression . The causal relationship between these variables is depicted in Figure 1. Note that each variable corresponds to distribution and link represents causal connection. The flow of this SCM illustrates the perspective of data generation. seq Rg. The DNA sequence consists of Rg and other non-causal parts. Rag . The causal part Rag directly influences the final gene expression. For example, an active enhancer interacting with gene can significantly impact its expression. Rg Rag Rm. The key causal component Rag is shared by both Rg and Rm. It can be detected through epigenomic signals in laboratory experiments and also participates in interactions with the target gene. 4 Published as conference paper at ICLR 2025 Rm sig. sig usually contains strong observable signals, such as peaks in DNase-seq, whereas regions without such signals often provide limited useful information."
        },
        {
            "title": "3.2 TASK OBJECTIVE",
            "content": "Based on information bottleneck, Equation 2 describes how to learn compressed representations rather than selecting specific sub-sequences. To directly select regulatory elements, we define the latent representations as = X, where is binary variable controlling the selection of each DNA base or soft mask indicating the importance of each DNA base. We assume that each selection is independent given the input sequence X, i.e., p(M X) = (cid:81) p(miX). Following the method of Paranjape et al. (2020), the objective becomes:"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Epθ(mixi)[ log qϕ(yimi xi)] + βKL[pθ(mixi), r(mi)], (3) where the first term is the task-specific loss, such as mean square error in DNA gene expression prediction, and the second term imposes constraint on the learned mask m, aligning it with the predefined distribution r(m) without conditioning on any specific sequence x. In our case, we use this second term to enforce sparsity in the learned regulatory elements. 3.3 DECOMPOSITION OF SEQUENCES AND SIGNALS By using information bottleneck shown in Equation 3, our primary focus is on estimating pθ(M X), i.e., learning the mask from the input sequences. Given that the input consists of both DNA sequences and epigenomic signals, we need to estimate pθ(M {Xseq, Xsig}). Assumption 1 (Conditional Independence of Sequences and Signals). We assume that, conditioned on the selection of regulatory elements , the DNA sequences and epigenomic signals are conditional independent to each other, i.e., p(Xsig, XseqM ) = p(XsigM )p(XseqM ) (4) Assumption 1 is based on the causal relationships outlined in Section 3.1. The selected subsequences of full given sequence, represented by X, can be viewed as the optimal regulatory elements (Rag) for specific gene in particular cell type. From data generation perspective, both the regulatory elements detected through measurements (Rm) and those interacting with the gene (Rg) originate from the optimal regulatory elements (Rag). Therefore, given the optimal regulatory elements, the distributions p(XsigM ) and p(XseqM ) should be independent of each other. Proposition 1. Based on Assumption 1, the estimation of pθ(M X) can be decomposed into terms involving Xseq and Xsig. Specifically, we have pθ(M X) pθ1(M Xseq)pθ2(M Xsig), where pθ1 (M Xseq) and pθ2(M Xsig) represent the contributions from the DNA sequence and the epigenomic signals, respectively. (5) The detailed proof of this decomposition is provided in Appendix A.1. Proposition 1 allows us to factorize the estimation of pθ(M X) into two independent components, corresponding to the DNA sequence Xseq and the epigenomic signals Xsig. As result, we can independently estimate pθ1 (M Xseq) and pθ2 (M Xsig), which simplifies the overall estimation process. This decomposition is based on the assumption that, conditioned on the selection of regulatory elements m, the DNA sequences and epigenomic signals are independent, thus enabling more efficient and targeted modeling of each component. 3.4 MASK DISTRIBUTION With the conditional independence property shown in Proposition 1, the estimation of the mask can be decomposed into two components: one based on DNA sequences pθ1 (M Xseq) and the other on epigenomic signals pθ2(M Xsig). We assume that both components follow the Beta distribution, as described in Assumption 2. The sampled values from the Beta distribution represent the probability of selecting specific base pairs from DNA sequence. 5 Published as conference paper at ICLR 2025 Assumption 2 (Mask Distribution). We assume that the soft mask ms follows the Beta distribution, i.e., ms Beta(α, β). Unlike the binary hard mask , the soft mask ms takes values between 0 and 1, making it more suitable for the Beta distribution. The hard mask can then be obtained by applying threshold to the soft mask. For the implementation, we apply both hard mask version and soft mask version. There are several reasons for choosing the Beta distribution. First, the Beta distribution typically quantifies success rates (DeGroot & Schervish, 2013; Gelman et al., 2013). The input parameters α and β represent the weights for selecting and not selecting the base pair, respectively. Therefore, when α > β, the base pair is more likely to be selected, and vice versa. Second, as both α and β increase simultaneously, the selection process will exhibit lower variance, indicating more confidence in the selection. Third, the product of two Beta distributions, when properly normalized, results in another Beta distribution. This ensures that the distributions within the framework remain in the same family, simplifying subsequent mathematical calculations and providing consistent fitting objectives for the models. Based on these properties of the Beta distribution, we assume that both pθ1 (msXseq) and pθ2 (msXsig) follow Beta distributions, but with different parameters α and β. Proposition 2. Given pθ1 (msXseq) Beta(α1, β1) and pθ2(msXsig) Beta(α2, β2), the product of these distributions also follows Beta distribution, with parameters: pθ1(msXseq)pθ2 (msXsig) Beta(α1 + α2 1, β1 + β2 1) (6) The proof of Proposition 2 is provided in Appendix A.2. When combining the probability distributions learned from the DNA sequence and signals, Proposition 2 ensures that the resulting distribution remains within the same family. And the final mask ms is then obtained through the combined Beta distribution. Specifically, deep learning models are applied in our framework to learn these two distributions by predicting the parameters α and β. 3.5 SPARSE OBJECTIVE In this part, we focus on the mask prior distribution r(m). From the objective in Equation 3, the KL divergence between pθ(mixi) and r(mi) needs to be computed. To simplify this calculation, we assume the prior distribution of the soft mask r(ms) follows the Beta distribution as well. Therefore, we have r(ms) Beta(α3, β3), where α3 and β3 are related to the sparsity of mask. The expectation of the Beta distribution is E[ms] = µ = α3 α3 + β3 , (7) where µ approximately represents the proportion of regulatory elements within the DNA sequences. Therefore, by setting the hyperparameters α3 and β3, the sparsity of the mask is taken into consideration, acting as bottleneck to filter out non-causal parts."
        },
        {
            "title": "4 MODEL DESIGNS",
            "content": "4.1 ARCHITECTURE As shown in Figure 2, our proposed model generate the mask distribution pθ(M X) from the DNA sequences and epigenomic signals = {Xseq, Xsig}, and an predictor, qϕ(Y X), provides gene expression values from the masked sequences = X. Those two modules will be trained together. Generator. As outlined in Section 3.4, we aim to generate mask to identify the critical regions within the DNA sequences. To achieve this, we first learn soft mask ms, which is probabilistic representation of each base pairs relevance, where ms [0, 1]. The soft mask is modeled using the Beta distribution, whose parametersα1, α2, β1, and β2are estimated from the combination of DNA sequences and epigenomic signals, as detailed in Proposition 2. 6 Published as conference paper at ICLR 2025 Figure 2: Pipeline of proposed architectures. The input data contains the DNA sequence seq and epigenomic signals sig. deep learning model fθ is then applied to seq to learn the corresponding parameters for the Beta distribution α1, β1, while α2, β2 are obtained from sig in non-parameterized manner. By combining these two beta distributions, p(msX) is obtained and used to generate the mask for actively interacted regulatory elements. The selected elements are then fed into the predictor model gϕ to provide the final target gene expression. For the parameters derived from the DNA sequences, the neural network fθ is used to predict α1 and β1. Specifically, we have α1, β1 = fθ(Xseq), where the network fθ outputs the L-dimensional parameters α1 and β1, with being the length of the input DNA sequence. Each position in the sequence is associated with pair of values α1 and β1, which parameterize the Beta distribution for that base pair. (8) For the parameters related to epigenomic signals, we use the intuition that higher signal values increase the likelihood of selecting the corresponding base pair. To capture this, we directly use the epigenomic signal values as the parameter α2, which influences the selection weight for each base pair. The parameter β2, representing selection threshold, is set as fixed constant. Specifically, we define α2 = Xsig; β2 = Cβ. By the above modeling procedure, we simplify the modeling process, making the learning of α2 and β2 non-parametric while maintaining the influence of signal strength without introducing additional learnable parameters. (9) After estimating the parameters, based on Proposition 2, the soft mask ms is sampled from the combined Beta distribution, pθ1(msXseq)p(msXsig) Beta(α1 + α2 1, β1 + β2 1), which represents the probability of selecting each base pair in the sequence. This probabilistic formulation allows us to model the selection process effectively. Finally, for the hard mask version, threshold is applied to the soft mask to generate the hard mask, = I(ms Cm), where Cm is the threshold (e.g., 0.5). The hard mask provides binary decision for selecting or ignoring specific base pairs. Through this approach, we model the mask generation process by leveraging both DNA sequences and epigenomic signals, combining parametric and non-parametric methods for more efficient region selection. Predictor. After obtaining the mask , we apply it to the input sequences to extract the relevant sub-sequences, represented as X. The extracted sub-sequences are then fed into secondary neural network, denoted by gϕ, to estimate the probability distribution of the target gene expression . The conditional distribution is expressed as qϕ(Y X), where ϕ represents the parameters of the network, and refers to the masked input sequences. To incorporate epigenomic signals alongside the DNA sequences, the input is formed by concatenating the one-hot encoded DNA sequence embeddings with the epigenomic signal values. This combined input allows the model to leverage both DNA sequence information and epigenomic signals, enhancing the models predictive capability during the estimation process. 4.2 OPTIMIZATION To optimize the loss function introduced in Equation 3, it is essential that every step remains differentiable to allow for gradient-based optimization during training. After obtaining the parameters 7 Published as conference paper at ICLR 2025 of the Beta distribution through the neural network pθ, we generate the soft mask ms by sampling from this distribution. To maintain differentiability, we treat the Beta distribution as special case of the Dirichlet distribution (Figurnov et al., 2018; Bishop, 2006). Using the reparameterization trick, we achieve differentiable sampling from the Dirichlet distribution by first sampling from the Gamma distribution and then normalizing the results (Figurnov et al., 2018). This method ensures that the sampling process remains differentiable with respect to the parameters α and β, allowing for efficient backpropagation and optimization. During inference, instead of sampling from the Beta distribution, we directly use the expected value of the Beta distribution as the soft mask ms for each base pair. The expected value of Beta distribution with parameters α and β is given by E[ms] = α α+β , which provides deterministic and efficient way to generate the soft mask without introducing randomness during inference, thus stabilizing the models predictions. For the soft mask version, we multiply the soft mask value. And for the hard mask version, when the soft mask ms is obtained, we need to convert it into hard binary mask to make final selections for each base pair. To retain differentiability in this process, we apply the straight-through estimator (STE) commonly used in Gumbel-Softmax (Jang et al., 2016). The STE allows us to make the forward pass non-differentiable by applying hard threshold (e.g., setting values greater than 0.5 to 1 and others to 0), while during the backward pass, the gradient is propagated through the soft mask as if it were continuous. This approach ensures that the model can learn effectively while using discrete decisions during the forward pass, preserving differentiability in the overall optimization process."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 SETTINGS 5.1.1 DATASETS AND INPUT FEATURES In this study, we aim to predict gene expression by modeling CAGE values, as it serves as key indicators of gene expression levels. Since gene expression varies across different cell types, we focus on two well-studied cell types: K562 and GM12878, both commonly used in biological research. The CAGE data are sourced from the ENCODE project (Consortium et al., 2012), and we follow the methodology of Lin et al. (2024) to predict gene expression values for 18,377 protein-coding genes. For the input data, we utilize the HG38 human reference genome to provide the reference DNA sequences. Additionally, the model incorporates several types of epigenomic signals: DNase-seq data is used to capture chromatin accessibility by identifying regions of the genome that are open and accessible to transcription factors and other regulatory proteins. The signals are extracted from bigWig files, providing genome-wide distributions of chromatin accessibility. H3K27ac ChIP-seq data is used to detect histone modifications, specifically the acetylation of lysine 27 on histone H3, which is often associated with active enhancers and promoters. This data is also extracted from bigWig files to provide genome-wide information on histone modification patterns. Hi-C data is processed to calculate the contact frequencies between each base pair and the target transcription start site (TSS), following the ABC pipeline method as described by Fulco et al. (2019). Furthermore, we incorporate additional features such as mRNA half-life and promoter activity from previous studies (Lin et al., 2024), which improve the models prediction accuracy on gene expression levels. The detailed information about these signals can be found in Appendix A.3. detailed description of data acquisition, preprocessing, and extraction, including downloading, filtering, and alignment, is provided in Appendix A.3. 5.1.2 BASELINES To benchmark our models performance, we compare it against several well-established baselines in gene expression prediction: Published as conference paper at ICLR 2025 Enformer (Avsec et al., 2021): widely used deep learning model for gene expression prediction, designed to capture long-range interactions across DNA sequences. Enformer employs the CNN and transformer architecture to model the DNA sequence to get the gene expression. HyenaDNA (Nguyen et al., 2024): cutting-edge method for modeling long DNA sequences, building on the Hyena (Poli et al., 2023) operator, which introduces novel way to handle longrange dependencies efficiently. HyenaDNA is designed to maintain high accuracy while significantly reducing computational complexity compared to traditional transformer-based models. Mamba (Gu & Dao, 2023): long-sequence modeling approach based on the state space model (SSM), offering linear computational complexity. Mamba is specifically tailored for efficiently handling long sequences, making it highly scalable while retaining strong predictive performance. Caduceus (Schiff et al., 2024): The state-of-the-art model for long genomic sequence modeling, built upon the Mamba architecture. Caduceus is optimized for learning rich representations of genomic sequences. In our study, we utilize Caduceus-Ph. classification layer is appended to evaluate its performance on our specific task. EPInformer (Lin et al., 2024): recently developed model extends the Activity-By-Contact (ABC) model (Fulco et al., 2019) for gene expression prediction. EPInformer utilizes DNase-seq peak data to define potential regulatory regions and applies an attention mechanism to aggregate enhancer signals. By leveraging both epigenomic and spatial information, EPInformer effectively models the enhancer information for gene expression prediction. Note that the size of the field of view of Enformer, HyenaDNA, Mamba, Caduceus are longrange DNA sequence, while the EPInformer is the extracted potential enhancer candidates based on DNase-seq measurement following ABC model [5]. Moreover, our proposed Seq2Exp also has the view of long-range DNA sequence, since the generator will take the long-range DNA sequence to extract the relevant sub-sequences for the prediction. 5.1.3 EVALUATION METRICS We employ the following evaluation metrics to assess the performance of our model and baselines on the gene expression regression task: Mean Squared Error (MSE) measures the average squared difference between the predicted and target gene expression values, capturing large deviations strongly. Mean Absolute Error (MAE) evaluates the absolute differences between predicted and actual values, providing more direct measure of average prediction error. Pearson Correlation is used to assess the linear correlation between the predicted and actual gene expression values, reflecting how well the model captures the relative ordering of gene expression. While MSE and MAE focus on the absolute errors in predictions, Pearson Correlation assess the models ability to capture relative ranking and overall trends in the data. 5.1.4 IMPLEMENTATION DETAILS We evaluate model performance using cross-chromosome validation strategy. The model is trained on all chromosomes except those designated for validation and testing. Specifically, chromosomes 3 and 21 are used as the validation set, and chromosomes 22 and are reserved for the test set. The inclusion of chromosome is particularly challenging due to its unique biological characteristics compared to autosomes, thus providing more stringent test of the models robustness. Both generator pθ and predictor qϕ are based on Caduceus architecture (Schiff et al., 2024), an advanced long-sequence model considering the bi-direction and RC-equivariance for DNA. Specifically, we train for 50,000 steps on 4-layer Caduceus architecture from scratch with hidden dimension of 128, and more hyperparameters can be found in the Appendix A.4 All experiments were conducted on system equipped with an NVIDIA A100 80GB PCIe GPU. The input sequences consist of 200,000 base pairs, centered around the promoter regions of the target genes, providing sufficient contextual information for accurate gene expression prediction. 5.2 RESULTS OF GENE EXPRESSION PREDICTION Table 1 presents the gene expression results based on CAGE values. Enformer, HyenaDNA, Mamba, and Caduceus are all DNA sequence-based methods, relying solely on DNA sequences without incorporating epigenomic signals. Among these, Caduceus achieves the best performance. We 9 Published as conference paper at ICLR 2025 Table 1: Performance on Gene Expression CAGE Prediction. The top performance over all the methods are highlighted in bold. Underline indicates that the best performance over all the baselines. K562 MSE MAE 0.2920 0.2265 0.2241 0.2197 0.4056 0.3497 0.3416 0.3327 Enformer HyenaDNA Mamba Caduceus Caduceus w/ signals 0. 0.3187 EPInformer 0.2140 0.3291 Seq2Exp-hard Seq2Exp-soft 0.1863 0. 0.3074 0.3054 Pearson MSE MAE Pearson GM12878 0.7961 0.8425 0.8412 0.8475 0. 0.8473 0.8682 0.8723 0.2889 0.2217 0.2145 0.2124 0.4185 0.3562 0.3446 0.3436 0.1942 0. 0.1975 0.3246 0.1890 0.1873 0.3199 0.3137 0.8327 0.8729 0.8788 0.8819 0. 0.8907 0.8916 0.8951 Table 2: Comparison with MACS3 on Gene Expression CAGE Prediction. MSE MAE K562 Pearson Mask Ratio MSE MAE GM Pearson Mask Ratio Seq2Exp-hard Seq2Exp-retrain MACS3 0.1863 0.1979 0.2195 0.3074 0.3168 0.3455 0.8682 0.8623 0.8435 6.88% 10.00% 13.61% 0.1890 0.1887 0.2340 0.3199 0.3177 0.3654 0.8916 0.8941 0.8634 6.32% 10.00% 15.95% further evaluate Caduceus by incorporating epigenomic signals, concatenated with the one-hot DNA sequence embeddings as input features. EPInformer, which explicitly extracts enhancer regions based on DNase-seq measurements, outperforms other baselines. This highlights that selecting key regions based on epigenomic signals yields better results. Finally, our proposed model, Seq2Exp, achieves the best performance overall. By using the Caduceus sequence model as both the generator and predictor, and incorporating epigenomic signals as additional features to the predictor, Seq2Exp explicitly learns the positions of regulatory elements from both DNA sequences and epigenomic signals, resulting in superior performance. We propose two versions of Seq2Exp. Seq2Exp-hard is to have binary mask, and Seq2Exp-soft takes use of soft mask values to denote the importance, resulting in an even better performances regarding the CAGE prediction task. 5.3 COMPARISON WITH PEAK DETECTION METHOD Table 2 compares the performance of Seq2Exp with regions identified through peak calling by MACS3 (Zhang et al., 2008) on DNase-seq epigenomic signals. While DNase-seq is crucial technique for identifying the positions of regulatory elements, statistical peak-calling methods, such as MACS3, can be considered simple approach for measuring these elements. Our results show that Seq2Exp significantly outperforms MACS3 in terms of predictive performance. Seq2Exp-hard utilizes hard binary mask. Seq2Exp-retrain builds on soft mask, and explicitly select the top 10% of base pairs and retrain the predictor model using only the selected ones. Both models outperform MACS3, suggesting the ability of discovering regulatory elements."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced Seq2Exp, framework for gene expression prediction that learns critical regulatory elements from both DNA sequences and epigenomic signals. By generating binary mask to identify relevant sub-sequences, Seq2Exp reduces input complexity and focuses on key regions for prediction. Our experiments demonstrate its effectiveness in identifying important regulatory elements and improving predictive performances, though current evaluations are limited to two cell types and several epigenomic signals. For the future direction, expanding the framework to more cell types and integrating diverse epigenomic data will be important for validating its generalizability. Beyond gene expression, applying this approach to other tasks related to regulatory element discovery and sequence analysis presents exciting research opportunities. Developing pretraining models focused on regulatory element extraction could also advance the field, enhancing generalization across genomic tasks. 10 Published as conference paper at ICLR"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "Research reported in this publication was supported in part by the National Institute on Aging of the National Institutes of Health under Award Number U01AG070112 and ARPA-H under Award Number 1AY1AX000053. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies."
        },
        {
            "title": "REFERENCES",
            "content": "Vikram Agarwal and Jay Shendure. Predicting mrna abundance directly from genomic sequence using deep convolutional neural networks. Cell reports, 31(7), 2020. Alexander Alemi, Ian Fischer, Joshua Dillon, and Kevin Murphy. Deep variational information bottleneck. arXiv preprint arXiv:1612.00410, 2016. Robin Andersson, Claudia Gebhard, Irene Miguel-Escalada, Ilka Hoof, Jette Bornholdt, Mette Boyd, Yun Chen, Xiaobei Zhao, Christian Schmidl, Takahiro Suzuki, et al. An atlas of active enhancers across human cell types and tissues. Nature, 507(7493):455461, 2014. Maria Aristizabal, Ina Anreiter, Thorhildur Halldorsdottir, Candice Odgers, Thomas McDade, Anna Goldenberg, Sara Mostafavi, Michael Kobor, Elisabeth Binder, Marla Sokolowski, et al. Biological embedding of experience: primer on epigenetics. Proceedings of the National Academy of Sciences, 117(38):2326123269, 2020. ˇZiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph Ledsam, Agnieszka Grabska-Barwinska, Kyle Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18 (10):11961203, 2021. Pau Badia-i Mompel, Lorna Wessels, Sophia Muller-Dott, Remi Trimbour, Ricardo Ramirez Flores, Ricard Argelaguet, and Julio Saez-Rodriguez. Gene regulatory network inference in the era of single-cell multi-omics. Nature Reviews Genetics, 24(11):739754, 2023. Joost Bastings, Wilker Aziz, and Ivan Titov. Interpretable neural predictions with differentiable binary variables. In 57th Annual Meeting of the Association for Computational Linguistics, pp. 29632977. ACL Anthology, 2019. Yonatan Belinkov, James Henderson, et al. Variational information bottleneck for effective lowresource fine-tuning. In International Conference on Learning Representations, 2020. Bradley Bernstein, John Stamatoyannopoulos, Joseph Costello, Bing Ren, Aleksandar Milosavljevic, Alexander Meissner, Manolis Kellis, Marco Marra, Arthur Beaudet, Joseph Ecker, et al. The nih roadmap epigenomics mapping consortium. Nature biotechnology, 28(10): 10451048, 2010. C.M. Bishop. Pattern Recognition and Machine Learning. tics. Springer, 2006. kTNoQgAACAAJ. ISBN 9780387310732. Information Science and StatisURL https://books.google.com/books?id= Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An In International conference on mainformation-theoretic perspective on model interpretation. chine learning, pp. 883892. PMLR, 2018. ENCODE Project Consortium et al. An integrated encyclopedia of dna elements in the human genome. Nature, 489(7414):57, 2012. William Cookson, Liming Liang, Goncalo Abecasis, Miriam Moffatt, and Mark Lathrop. Mapping complex disease traits with global gene expression. Nature Reviews Genetics, 10(3):184194, 2009. M.H. DeGroot and M.J. Schervish. Probability and Statistics. Pearson custom library. Pearson Education, 2013. ISBN 9781292025049. URL https://books.google.com/books?id=hIPkngEACAAJ. Published as conference paper at ICLR 2025 Job Dekker, Andrew Belmont, Mitchell Guttman, Victor Leshyk, John Lis, Stavros Lomvardas, Leonid Mirny, Clodagh Oshea, Peter Park, Bing Ren, et al. The 4d nucleome project. Nature, 549(7671):219226, 2017. Valur Emilsson, Gudmar Thorleifsson, Bin Zhang, Amy Leonardson, Florian Zink, Jun Zhu, Sonia Carlson, Agnar Helgason, Bragi Walters, Steinunn Gunnarsdottir, et al. Genetics of gene expression and its effect on disease. Nature, 452(7186):423428, 2008. Mikhail Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients. Advances in neural information processing systems, 31, 2018. Charles Fulco, Joseph Nasser, Thouis Jones, Glen Munson, Drew Bergman, Vidya Subramanian, Sharon Grossman, Rockwell Anyoha, Benjamin Doughty, Tejal Patwardhan, et al. Activity-by-contact model of enhancerpromoter regulation from thousands of crispr perturbations. Nature genetics, 51(12):16641669, 2019. A. Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, A. Vehtari, and D.B. Rubin. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis, 2013. ISBN 9781439840955. URL https://books.google.com/books?id=ZXL6AQAAQBAJ. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, and Byron Wallace. Learning to faithfully rationalize by construction. In Proceedings of the Association for Computational Linguistics (ACL), 2020. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Alireza Karbalayghareh, Merve Sahin, and Christina Leslie. Chromatin interactionaware gene regulatory modeling with graph attention networks. Genome Research, 32(5):930944, 2022. Tao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 107117, 2016. Jiecong Lin, Ruibang Luo, and Luca Pinello. Epinformer: scalable deep learning framework for gene expression prediction by integrating promoter-enhancer sequences with multimodal epigenomic data. bioRxiv, pp. 202408, 2024. Siqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In International Conference on Machine Learning, pp. 1552415543. PMLR, 2022. Jill Moore, Michael Purcaro, Henry Pratt, Charles Epstein, Noam Shoresh, Jessika Adrian, Trupti Kawli, Carrie Davis, Alexander Dobin, et al. Expanded encyclopaedias of dna elements in the human and mouse genomes. Nature, 583(7818):699710, 2020. Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. Advances in neural information processing systems, 36, 2024. Bhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi, and Luke Zettlemoyer. An information bottleneck approach for controlling conciseness in rationale extraction. arXiv preprint arXiv:2005.00652, 2020. Judea Pearl. Causal inference in statistics: An overview. 2009. Judea Pearl et al. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress, 19(2):3, 2000. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 2804328078. PMLR, 2023. Published as conference paper at ICLR 2025 Aditya Pratapa, Amogh Jalihal, Jeffrey Law, Aditya Bharadwaj, and TM Murali. Benchmarking algorithms for gene regulatory network inference from single-cell transcriptomic data. Nature methods, 17(2):147154, 2020. Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. arXiv preprint long-range dna sequence modeling. Caduceus: Bi-directional equivariant arXiv:2403.03234, 2024. Michael Schubert, Bertram Klinger, Martina Klunemann, Anja Sieber, Florian Uhlitz, Sascha Sauer, Mathew Garnett, Nils Bluthgen, and Julio Saez-Rodriguez. Perturbation-response genes reveal signaling footprints in cancer gene expression. Nature communications, 9(1):20, 2018. Eran Segal, Yoseph Barash, Itamar Simon, Nir Friedman, and Daphne Koller. From promoter sequence to expression: probabilistic framework. In Proceedings of the sixth annual international conference on Computational biology, pp. 263272, 2002. Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. Graph information bottleneck. Advances in Neural Information Processing Systems, 33:2043720448, 2020. Ying-Xin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. arXiv preprint arXiv:2201.12872, 2022. Yong Zhang, Tao Liu, Clifford Meyer, Jerˆome Eeckhoute, David Johnson, Bradley Bernstein, Chad Nusbaum, Richard Myers, Myles Brown, Wei Li, et al. Model-based analysis of chip-seq (macs). Genome biology, 9:19, 2008. 13 Published as conference paper at ICLR"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DERIVATION OF SEQUENCE AND SIGNAL DECOMPOSITION For the mask distribution pθ(mX), we aim to decompose it. For simplicity, we omit the parameter θ in the following derivation. By applying Bayes theorem, we obtain p(mX) = p(mXseq, Xsig) = p(Xseq, Xsigm)p(m) p(Xseq, Xsig) p(Xseqm)p(Xsigm)p(m), (10) where p(Xseq, Xsigm) = p(Xseqm)p(Xsigm) is based on Assumption 1, and p(Xseq, Xsig) represents the input data, which is independent of the learning process. Applying Bayes theorem again to p(Xseqm) and p(Xsigm), we have p(mX) p(Xseqm)p(Xsigm)p(m) = p(mXsig)p(Xsig) p(m) p(mXseq)p(Xseq) p(m) p(mXseq)p(mXsig) p(m) , p(m) (11) where we can safely omit p(Xseq) and p(Xsig). For the marginal distribution p(m), we make it to be prior distribution with constant predefined parameters, allowing us to omit it as well. Thus, we derive p(mX) p(mXseq)p(mXsig), which corresponds to Proposition 1. A.2 BETA DISTRIBUTION PRODUCT The probability density function for Beta distribution is given by Beta(x; α, β) xα1(1 x)β1. Given that both p(msXseq) and p(msXsig) follow Beta distribution, we have p(msXseq) xα11(1 x)β11, p(msXsig) xα21(1 x)β21. Multiplying these distributions yields p(msXseq)p(msXsig) xα1+α22(1 x)β1+β22 Beta(ms; α1 + α2 1, β1 + β2 1). (12) (13) (14) (15) Note that the parameters of Beta distribution must lie within the range (0, ), thus we require α1 + α2 > 1 and β1 + β2 > 1 to ensure valid distribution. A.3 DATA PROCESSING The gene expression is different for different cell types. In this work, we consider the well-studied cell type K562 and GM12878. CAGE. Cap Analysis of Gene Expression (CAGE) is the primary target for prediction in this work. Each gene is assigned CAGE value to quantify its expression level. CAGE is high-throughput sequencing technique primarily used to map transcription start sites (TSS) and quantify gene expression. It provides comprehensive profile of promoter usage and alternative TSS across different genes, quantifying the number of RNA molecules initiating at each TSS, thereby reflecting gene transcriptional activity. Published as conference paper at ICLR 2025 Table 3: Hyperparameter values and their search space (final choices are highlighted in bold)."
        },
        {
            "title": "Values",
            "content": "# Layers of Generator # Layers of Predictor Hidden dimensions α3, β3 # training steps Batch size Learning rate Scheduler strategy Initial warmup learning rate Min learning rate Warmup steps Validation model selection criterion 4 4 128 [1, 9], [10,90], [10, 190], [10, 10], [10, 1.11] 50000, 85000 8 1e 3, 5e-4, 1e 4, 5e 5 CosineLR with Linear Warmup 1e-5 1e-4 5,000 validation MSE In this study, we use CAGE data from the FANTOM5 project (Andersson et al., 2014) (K562: CNhs11250; GM12878: CNhs12333). We follow the procedures outlined in Agarwal & Shendure (2020) and Lin et al. (2024) to derive the target values for each gene. DNase-seq. DNase-seq (DNase hypersensitive site sequencing) is technique used to identify regions of open chromatin within the genome. It pinpoints areas that are less compacted by nucleosomes, typically corresponding to promoters, enhancers, and transcription factor binding sites. The value derived from DNase-seq represents the frequency of DNase cleavage at specific sites, with higher values indicating regions that are more accessible to regulatory elements. We obtained the DNase-seq data from the ENCODE project (Consortium et al., 2012) (K562: ENCFF414OGC; GM12878: ENCFF960FMM). We directly downloaded the data in bigWig format, as it provides genome-wide distribution of DNase-seq values. H3K27ac. H3K27ac refers to the acetylation of lysine 27 on histone H3, post-translational modification associated with active enhancers and promoters. High levels of H3K27ac in genomic region indicate that it is likely an active enhancer or promoter, playing significant role in gene expression regulation. We also obtained H3K27ac data from the ENCODE project (Consortium et al., 2012) (K562: ENCFF465GBD; GM12878: ENCFF798KYP), again in bigWig format, which provides the value distribution across the genome. Hi-C. Hi-C measures the three-dimensional (3D) organization of genomes by capturing physical interactions between different regions of DNA. This technique helps researchers understand how DNA is folded and structured within the nucleus. Hi-C data provides information about genome contacts, but due to technical limitations, it often has low resolution (typically at 5,000 base pairs), meaning we can only observe interactions between two regions of DNA of at least this length. In this work, we follow previous studies (Fulco et al., 2019), calculating the frequency of contacts between specific region (TSS) and all other regions, generating Hi-C frequency distribution across the genome. The Hi-C data were sourced from the 4D Nucleome project (Dekker et al., 2017) (K562: 4DNFITUOMFUQ; GM12878: 4DNFI1UEG1HD). mRNA half-life and promoter activity features. When predicting the CAGE values, following the implementation of Lin et al. (2024), we use the promoter activity feature and mRNA half-life features as supplementary for fair comparison and further improvement. The promoter activity feature is defined as the square root of the product of DNase-seq and H3K27ac signal values. The mRNA features include G/C contents, lengths of functional regions, intron length, and exon junction density within the coding region. Specifically, the features are The log-transformed z-score of the 5 UTR (untranslated region) length. The log-transformed z-score of the CDS (coding sequence) length. 15 Published as conference paper at ICLR 2025 Table 4: Performance on Gene Expression CAGE Prediction with Standard Deviation for Cell Type K562. MSE MAE Pearson"
        },
        {
            "title": "Enformer\nHyenaDNA\nMamba\nCaduceus",
            "content": "0.2920 0.0050 0.2265 0.0013 0.2241 0.0027 0.2197 0.0038 0.4056 0.0040 0.3497 0.0012 0.3416 0.0026 0.3327 0.0070 0.7961 0.0019 0.8425 0.0008 0.8412 0.0021 0.8475 0.0014 Caduceus w/ signals 0.1959 0.0036 0.3187 0. 0.8630 0."
        },
        {
            "title": "EPInformer",
            "content": "0.2140 0.0042 0.3291 0.0031 0.8473 0.0017 MACS3 0.2195 0.0023 0.3455 0. 0.8435 0.0013 Seq2Exp-hard Seq2Exp-soft 0.1863 0.0051 0.1856 0.0032 0.3074 0.0036 0.3054 0.0024 0.8682 0.0045 0.8723 0.0012 Table 5: Performance on Gene Expression CAGE Prediction with Standard Deviation for Cell Type GM12878. MSE MAE Pearson Enformer HyenaDNA Mamba Caduceus 0.2889 0.0009 0.2217 0.0018 0.2145 0.0021 0.2124 0.0037 0.4185 0.0013 0.3562 0.0012 0.3446 0.0022 0.3436 0. 0.8327 0.0025 0.8729 0.0010 0.8788 0.0011 0.8819 0.0009 Caduceus w/ signals 0.1942 0.0058 0.3269 0.0048 0.8928 0.0017 EPInformer 0.1975 0.0031 0.3246 0.0025 0.8907 0.0011 MACS3 0.2340 0.0028 0.3654 0. 0.8634 0.0020 Seq2Exp-hard Seq2Exp-soft 0.1890 0.0045 0.1873 0.0044 0.3199 0.0040 0.3137 0.0028 0.8916 0.0027 0.8951 0.0038 The log-transformed z-score of the 3 UTR (untranslated region) length. The GC content of the 5 UTR, expressed as the proportion of and bases. The GC content of the CDS. The GC content of the 3 UTR. The log-transformed z-score of the total intron length for gene. The exon density within the open reading frame (ORF), reflecting the number of exon junctions per unit length of the ORF. A.4 EXPERIMENT SETUP Here we present some hyperparameters values and their search space in Table 3. A.5 EXPERIMENT RESULTS Based on Table 1 and Table 2, here we present the experimental results with standard deviation in Table 4 and Table 5. The results are from five runs of different random seeds: {2,22,222,2222,22222}."
        }
    ],
    "affiliations": [
        "Texas A&M University",
        "The University of Texas Health Science Center at Houston"
    ]
}