{
    "paper_title": "Data-Efficient RLVR via Off-Policy Influence Guidance",
    "authors": [
        "Erle Zhu",
        "Dazhi Jiang",
        "Yuan Wang",
        "Xujun Li",
        "Jiale Cheng",
        "Yuxian Gu",
        "Yilin Niu",
        "Aohan Zeng",
        "Jie Tang",
        "Minlie Huang",
        "Hongning Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data selection is a critical aspect of Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing the reasoning capabilities of large language models (LLMs). Current data selection methods are largely heuristic-based, lacking theoretical guarantees and generalizability. This work proposes a theoretically-grounded approach using influence functions to estimate the contribution of each data point to the learning objective. To overcome the prohibitive computational cost of policy rollouts required for online influence estimation, we introduce an off-policy influence estimation method that efficiently approximates data influence using pre-collected offline trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we employ sparse random projection to reduce dimensionality and improve storage and computation efficiency. Leveraging these techniques, we develop \\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy \\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that iteratively selects the most influential data for the current policy. Experiments on models up to 7B parameters demonstrate that CROPI significantly accelerates training. On a 1.5B model, it achieves a 2.66x step-level acceleration while using only 10\\% of the data per stage compared to full-dataset training. Our results highlight the substantial potential of influence-based data selection for efficient RLVR."
        },
        {
            "title": "Start",
            "content": "Data-Efficient RLVR via Off-Policy Influence Guidance Erle Zhu*, Dazhi Jiang**, Yuan Wang, Xujun Li, Jiale Cheng, Yuxian Gu Yilin Niu, Aohan Zeng, Jie Tang, Minlie Huang, Hongning Wang CoAI Group, Tsinghua University Z. AI {zel24}@mails.tsinghua.edu.cn, hw-ai@tsinghua.edu.cn 5 2 0 2 0 3 ] . [ 1 1 9 4 6 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Data selection is critical aspect of Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing the reasoning capabilities of large language models (LLMs). Current data selection methods are largely heuristicbased, lacking theoretical guarantees and generalizability. This work proposes theoreticallygrounded approach using influence functions to estimate the contribution of each data point to the learning objective. To overcome the prohibitive computational cost of policy rollouts required for online influence estimation, we introduce an off-policy influence estimation method that efficiently approximates data influence using pre-collected offline trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we employ sparse random projection to reduce dimensionality and improve storage and computation efficiency. Leveraging these techniques, we develop Curriculum RL with Off-Policy Influence guidance (CROPI), multi-stage RL framework that iteratively selects the most influential data for the current policy. Experiments on models up to 7B parameters demonstrate that CROPI significantly accelerates training. On 1.5B model, it achieves 2.66x step-level acceleration while using only 10% of the data per stage compared to fulldataset training. Our results highlight the substantial potential of influence-based data selection for efficient RLVR."
        },
        {
            "title": "Introduction",
            "content": "The advent of highly capable reasoning models, such as OpenAI series models (OpenAI, 2024) and DeepSeek R1 (Guo et al., 2025), have established Reinforcement Learning with Verifiable Rewards (RLVR) as key step for enhancing the reasoning capabilities of large language models (LLMs). Data quality is critical to model perfor- ** denotes equal contribution. Work was done when EZ & DJ intered at Z.ai. mance, making data selection for RLVR key research area. Existing data selection methods for RLVR (Wang et al., 2025; Bae et al., 2025; Li et al., 2025; Zhao et al., 2025; Sun et al., 2025) are primarily heuristic-based, often focusing on metrics like difficulty or uncertainty; but such metrics lack theoretical performance guarantees and exhibit poor generalizability across different scenarios. In this work, we propose using influence functions (Hampel, 1974; Koh and Liang, 2017) for RL data selection. This method approximates the contribution of given data point to the learning objective via variational analysis in calculus. Compared to heuristic-based approaches, influence functions offer stronger theoretical guarantees and provide more fine-grained information about data effect. However, applying influence functions to RLVR for large-scale models faces significant barrier. Unlike supervised learning (e.g., pre-training or supervised fine-tuning) where supervision is readily available, RL supervision must be generated through policy rollouts (Grosse et al., 2023; Xia et al., 2024). For LLMs, these rollouts are computationally expensive, which makes the online estimation of data influence prohibitively difficult. To address this challenge, we propose method to estimate data gradients using pre-collected offline trajectories. This approach allows for the efficient evaluation of data points influence on the online policy without requiring new, costly rollouts. Furthermore, to overcome the challenges of storing and computing the high-dimensional gradients typical of LLMs, we employ sparse random projection. This technique maps the gradients to lower-dimensional space, thereby improving storage efficiency and mitigating numerical noise. Leveraging off-policy influence estimation, we develop curriculum-based reinforcement learning framework named Curriculum RL with Off-Policy Influence guidance (CROPI). CROPI segments the RL training process into multiple stages. In each stage, it selects the subset of data with the highest estimated influence on the current policy checkpoint for subsequent training. We demonstrate CROPIs effectiveness through experiments on models of varying sizes (1.5B to 7B) and context lengths. On the 1.5B model, CROPI achieves 2.66 step-level acceleration compared to fulldataset training, while using only 10% of the data in each stage. This result highlights the substantial potential of influence-based data selection for online RLVR. In summary, our contributions are as follows: We introduce Off-Policy Influence Estimation, theoretically-grounded and rollout-free method to quantify the influence of individual data points on an online policy, eliminating the need for real-time sampling. To efficiently handle the high-dimensional gradients of LLMs, we employ Sparse Random Projection for dimensionality reduction. We empirically demonstrate that applying dropout prior to this projection mitigates numerical noise and enhances computational efficiency while preserving inner products. We propose CROPI, curriculum reinforcement learning framework that leverages our influence estimation method for multi-stage data selection. Our experiments show that CROPI substantially outperforms both fulldataset training and alternative data selection baselines."
        },
        {
            "title": "2 Preliminaries",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR). Using the language of reinforcement learning (RL), the reasoning process of LLMs can be modeled as Markov Decision Process = (S, A, P, r, γ) (Sutton et al., 1999). Let denote the vocabulary, with each generated token V. The state space = consists of all possible token sequences, and actions correspond to generating the next token, such that = V. The autoregressive generation process produces xt at each step given the prefix st, with deterministic state transition: st+1 = stxt, where denotes concatenation. The reward function is outcomebased: rt = 0 for < , and rT = Rcorrect(y), where is the solution extracted from the final sequence sT , and Rcorrect(y) {0, 1} is deterministic correctness indicator. reasoning trajectory is defined as τ = {(s0, x0), ..., (sT 1, xT 1)}, and its return is R(τ ) = (cid:80)T t=1 rt = Rcorrect(y) (with discount factor γ = 1). With Deepseek-R1 (Guo et al., 2025) emerging as the most powerful open-source reasoning model, its RL algorithm GRPO (Shao et al., 2024), has become the mainstream approach for RLVR. This algorithm builds upon PPO (Schulman et al., 2017), but estimates the advantage using groupnormalized returns, thereby eliminating the overhead of the critic model required in PPO. The optimization objective is as follows (ignoring the clipping & KL term): J(θ) = s0q(),{τk}K k=1πθold (cid:88) k=1 1 Tk Tk1 (cid:88) t=0 ρπθ k,t (cid:98)Ak,t (1) [R(τ )] k,t = [R(τ )] πθ(xk,tsk,t) πθold where ρπθ R(τk)(cid:98)Eπθold (cid:98)σπθold (xk,tsk,t) and (cid:98)Ak,t = . (cid:98)E and (cid:98)σ is the empirical mean and standard derivation of the return of trajectories {τk}K k=1 sampled from πθold given query s0. Influence Function. Influence functions (Hampel, 1974; Koh and Liang, 2017) offer gradientbased approach for data attribution, derived based on the variational analysis of the objective function. Briefly, given an objective function to be maximized and collection of data points zi, suppose the model parameters are updated from θ0 to θT . Our goal is to estimate the contribution (or influence) of each individual data point to the change in the objective function: J(θT ) = J(θ0)+(cid:80)N i=1 Influence(zi). Owing to strong theoretical guarantees and empirical success, influence function and its variants have been widely applied in pre-training and supervised fine-tuning stages of LLMs (Grosse et al., 2023; Gu et al., 2024; Xia et al., 2024; Wang et al., 2024) for data attribution and selection. However, how to leverage influence functions for data selection in RLVR for LLMs remains an open question."
        },
        {
            "title": "Influence Estimation in RLVR",
            "content": "Following the first-order influence function formula (Pruthi et al., 2020), in the RLVR context, given training prompt s0 and test query 0, we use the inner product the policy gradients of s0, 0 to measure the influence of s0 to policys perforFigure 1: Practical issues in computing influence for data point in RL training process for large-scale models. mance on 0: Inf(πθ; s0, 0) := θJ(θ; s0), θJ(θ; 0). (2) where θJ(θ; s0) = Eτ πθ [R(τ )θ log πθ(τ s0)] (3) J(θ; s0) denotes policy gradient for initial state s0, we use vanilla version for simplicity. We give short derivation for Equation 2 in Appendix A. Although the notion of influence function possesses favorable theoretical for the problem of data selection, its practical estimation for RL algorithms still faces two main challenges, namely the Rollout Issue and the Gradient-Scale Issue as illustrated in Figure 1. Rollout Issue. Unlike supervised learning whose training dynamics are predominantly shaped by labeled data and optimization algorithms (Xia et al., 2024; Gu et al., 2024), RL involves online sampling, making its training process more dynamic and less predictable. Consequently, global data selection strategies (Zhao et al., 2025; Wang et al., 2025) can hardly capture the evolving characteristics of policy learning in RLVR. Therefore, we seek to dynamically evaluate the influence of each data point s0 conditioned on the current policy πθ, thereby enabling effective online utility estimation. However, accurate influence estimation typically requires computing the policy gradient for each s0, which demands rollouts over multiple trajectories (Eq. (3)). The substantial computational costs and latency associated with these rollouts present significant barrier to real-time influence estimation in LLMs. Gradient-Scale Issue. Moreover, the high dimensionality of gradients in large-scale models poses additional storage and computational challenges. For instance, Xia et al. (2024) mitigate this issue through random projection, leveraging the JohnsonLindenstrauss Lemma (Johnson et al., 1984) to efficiently preserve inner products with high probability. However, while Xia et al. (2024) utilize LoRA (Hu et al., 2022) to reduce raw gradient dimensionality during SFT, we consider full-parameter training for better performance assurance in RLVR, resulting in massive raw gradients and high projection overhead. To address these two issues, we propose an offpolicy gradient estimation technique to eliminate the reliance on costly rollouts, and employ sparse random projection methods to achieve scalable and efficient gradient storage and computation. This dual approach facilitates practical and effective online influence estimation within RLVR, making it suitable for large-scale model training scenarios."
        },
        {
            "title": "Estimation",
            "content": "To address the Rollout Issue, following the ideas in offline RL (Levine et al., 2020) which use offline trajectories to perform RL algorithms, we use offline trajectories generated by behavior policy β to compute the gradient for policy πθ and use the off-policy gradient to estimate the influence of the prompt s0, as shown on Figure 2 (c). Given data point s0, RL policy πθ trained with groupnorm advantage estimator, behavior policy β and offline trajectories {τk}K k=1 β(s0) sampled from behavior policy β conditioned on s0. If πθ and β are KL-constrained, we can approximate the on-policy gradient with an off-policy estimator: (cid:98)gβ(θ, s0, {τk}K k=1)"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 1 τk τk1 (cid:88) t=0 θρπθ k,t (cid:98)Aβ k,t (4) and (cid:98)Aβ k,t = πθ(xk,tsk,t) β(xk,tsk,t) Where ρπθ R(τk)(cid:98)Eβ [R(τ )] (cid:98)σβ [R(τ )] k,t = . This gradient is derived from TRPO method (Schulman et al., 2015), please refer to Appendix for more details. This gradient is equivalent to removing the clipping operation from the GRPO objective and replacing πθold with β in the gradient computation. 0): (cid:99)Infβ(πθ; s0, In this work, we select β = πθ0. Since there is KL-Term in online RL objective (Shao et al., 2024; Ouyang et al., 2022), the KL-distance of πθ0 and πθ is usually constrained, making our off-policy gradient estimator relatively accurate. In our setting, we sample multiple trajectories by πθ0 for every prompt in the dataset before training, resulting in = {s(i) i=1, which is necessary process for all existing data selection methods in RLVR. We denote (cid:98)gβ(θ, s0, {τk}K k=1) as (cid:98)gβ(θ, s0) for simplicity. 0 , {τ (i) k=1}N }K Based on off-policy gradient (cid:98)gβ(θ, s0), we can measure the influence between training data s0 and validation data 0. Following Equation 2, our off-policy influence estimation can be formulated as (cid:99)Infβ(πθ; s0, 0) = (cid:98)gβ(θ, s0) (cid:98)gβ(θ, 0) (5) The experimental results indicate that the off-policy gradient can approximate the on-policy gradient to certain extent; please refer to Appendix for further details."
        },
        {
            "title": "Gradient Compression",
            "content": "To address the Gradient-Scale Issue, we propose sparse random projection, method where subset of gradient dimensions is randomly omitted before the projection is performed. Let the gradient be denoted by Rd, where is its dimensionality. standard random projection uses matrix Rkd, where and each element Pi,j is sampled from standard normal distribution, (0, 1). Our method first samples random set of indices {1, . . . , d}. It then constructs sparse random projection matrix, Psparse Rkd, where the columns of Psparse corresponding to indices in are sampled from (0, 1), while all other columns are zero vectors, i.e. Psparse[i, j] = ϵi,jIjS, ϵi,j (0, 1), where denotes indicator function. This process is equivalent to first selecting random subset of the gradients dimensions and then applying smaller projection. This can be expressed as Psparseg = Psparse[:, S]g[S], where g[S] is the subvector of with subset of elements indexed by S, and Psparse[:, S] is the submatrix of Psparse formed by the columns indexed by S. We define the number of selected dimensions as rs = S. Our empirical results show that the random dropout of the gradient dimensions before projection achieves efficient and accurate rank preservation in inner product compared to directly conducting random projection. Please refer to Section 6.1 for more details. 3.3 Final Solution In order to eliminate the bias of the gradient norm caused by different lengths and pass rates, following Xia et al. (2024), we normalize the gradient features before inner product, which is equivalant to computing the cossim. Combining the off-policy gradient estimation and sparse random projection, denote (cid:101)gβ = Psparse (cid:98)gβ, the practical computation of off-policy influence (cid:99)Infβ(πθ; s0, 0) can be formulated as (cid:102)Infβ(πθ; s0, 0): (cid:102)Infβ(πθ; s0, 0) := cossim (cid:0) (cid:101)gβ(θ, s0), (cid:101)gβ(θ, 0)(cid:1) (6) We call this computation (cid:102)Infβ(πθ; s0, 0) the Practical Off-Policy Influence estimation (POPI) for simplicity."
        },
        {
            "title": "Influence Guidance",
            "content": "In this chapter, we will illustrate how to use POPI estimator to select influential data for efficient RL training."
        },
        {
            "title": "4.1 Data Selection with POPI",
            "content": "To measure the influence of training prompt s0 to specific validation set, we need to compute POPI over batch of validation prompts. For validation set Dval = {s(i) i=1 , we denote the gradient feature of validation set as the average gradient feature over all validation data points: (cid:101)gβ(θ, Dval) := (cid:80)Nval i=1 (cid:101)gβ(θ, s(i) 0 ). Then the POPI of training prompt s0 to the validation set Dval is defined as: 0 }Nval (cid:102)Infβ(πθ; s0, Dval) := cossim((cid:101)gβ(θ, s0), (cid:101)gβ(θ, Dval)) For multiple validation sets, we need to consider the effectiveness over different validation sets. We use ranking fusion method Reciprocal Rank Fusion (RRF) (Cormack et al., 2009) which is widely use in IR field to combine the POPI scores over different validation sets. Denote the training set as Dtr, number of validation sets as Dval,j, = 1, 2, ...V , the rank of POPI score of training data s0 to j-th validation set over the whole Figure 2: The schematic of our proposed framework Curriculum RL with Off-Policy Influence Guidance (CROPI). training set as rj(s0) = rankPOPI(πθ; s0, Dval,j), then the fused rank score is defined as: UPOPI-R(πθ; s0) = (cid:88) j=1 1 rj(s0) (7) For any validation set, higher rank (i.e. small rank value) contribute to obtaining higher RRF score. In practice, given RL policy checkpoint πθ, we select subset Dsel from the whole training set using UPOPI-R scores: Dsel = argmax SD,S=αD (cid:88) s0S UPOPI-R(πθ; s0) (8)"
        },
        {
            "title": "4.2 Curriculum RL with Data Selection",
            "content": "To balance long-term planning with dynamic selection, we adopt phase-level data selection strategy with POPI, Curriculum RL with Off-Policy Influence Guidance (CROPI), which is illustrated on Figure 2. CROPI is an iterative curriculumbased RL framework designed to progressively filter and focus training on the most influential data points. At the start of each phase m, the current policy πθ(m) is evaluated on all training instances, yielding POPI-R scores UPOPI-R(πθ(m); s(i) 0 ) (Eq. 7) for each s(i) 0 Dtr. The subset D(m) comprises the highest scoring samples, specifically αDtr instances, where α is the selection ratio. This targeted selection implements dynamic curriculum, focusing subsequent policy optimization on the most impactful data points. The policy is then refined on the selected subset using the GRPO algorithm over steps, producing an improved policy πθ(m+1) for the next phase. This iterative procedure is repeated for phases, resulting in final policy πθ(M ) output by CROPI. We formulate this process in Algorithm 1 in Appendix D. This curriculum-based filtering and optimization scheme encourages the policy to focus on examples with the greatest potential benefit, thus accelerating learning process of RLVR."
        },
        {
            "title": "5.1 Setup",
            "content": "respectively. We evaluate CROPI on mathematical reasoning tasks using several open-source, instructionaligned models diversed from different model scales and context sizes: Qwen2.5-1.5B-Instruct, Qwen2.5-7B-Instruct (Qwen et al., 2025), and Deepseek-R1-Distill-Qwen-1.5B (Guo et al., 2025). these models are referred to For simplicity, 1.5B, 7B, and 1.5B-R1, Our training data consists of 47K unique problems aggregated from GSM8K-Train (Cobbe et al., 2021), MATH-Train(Hendrycks et al., 2021), and DeepScaleR-Preview-Dataset (Luo et al., 2025). We assess performance on suite of standard benchmarks, including GSM8K-Test, MATH-Test, Gaokao2023EN (MARIO-Math, 2024), OlympiadBench (He et al., 2024), AMC23 (math ai, 2024b) and AIME24 (math ai, 2024a). For influence-based data selection, small validation set (max 100 examples) is sampled from subset of these test sets. Table 1: Evaluation results of CROPI and other baseline methods across various math datasets. CROPI consistently outperforms existing data selection approaches, achieving state-of-the-art performance on target tasks at different training steps and under various experimental settings. The best results at each training step are highlighted in bold. Acc.(%) GSM8K MATH Gaokao. AMC Olympiad. AIME24 Targeted(Avg.) Untar.(Avg.) Qwen2.5-1.5B-Instruct + Full Dataset(GRPO)@500 + Full Dataset(GRPO)@1k + Full Dataset(DAPO)@500 + Full Dataset(DAPO)@1k + Learnability(GRPO)@500 + Learnability(GRPO)@1k + Pass Rate(GRPO)@500 + Pass Rate(GRPO)@1k + Influence(GRPO)@500 + Influence(GRPO)@1k + CROPI (Ours)@500 + CROPI (Ours)@1k Qwen2.5-7B-Instruct + Full Dataset(GRPO)@300 + Full Dataset(GRPO)@600 + CROPI (Ours)@300 + CROPI (Ours)@600 R1-Distill-Qwen-1.5B + Full Dataset(GRPO)@150 + Full Dataset(GRPO)@300 + CROPI (Ours)@150 + CROPI (Ours)@300 72.99 78.01 79.30 78.64 78.93 77.57 79.12 78.82 80.19 78.31 78.58 80.55 81.36 90.51 92.59 93.54 92.13 92.89 77. 77.42 79.46 77.97 79.18 55.48 58.07 59.54 53.27 53.26 59.05 59.03 57.87 58.33 58.80 58.82 58.43 59.17 75.08 76.61 77.39 77.92 78.35 72. 73.64 76.63 75.54 76.67 46.43 47.34 44.09 42.73 41.88 50.19 47.19 49.09 46.86 49.74 47.73 48.12 46.54 62.64 60.84 60.97 65.39 63.70 60. 63.77 65.15 63.44 66.88 28.13 32.50 31.25 29.38 30.21 30.63 29.17 26.88 29.17 31.25 23.96 26.25 34.38 46.88 55.00 52.50 58.75 55.62 53. 53.75 55.21 51.25 60.42 24.27 26.32 26.23 20.05 19.85 27.55 26.96 26.08 27.12 25.49 25.08 26.86 27.78 41.91 43.58 45.49 45.64 45.78 41. 41.62 44.53 43.97 43.63 4.00 1.67 6.67 0.0 0.0 5.00 2.78 3.33 8.33 5.83 8.33 4.17 9.72 8.33 16.67 13.33 14.17 17.50 20. 17.50 23.61 22.50 20.83 64.24 68.04 69.42 65.95 66.09 68.31 69.07 68.35 69.26 68.56 68.70 69.49 70.26 53.96 57.36 57.44 57.46 58.63 43. 44.16 47.12 45.29 47.94 25.71 26.96 27.06 23.04 22.99 28.34 26.52 26.34 27.87 28.08 26.28 26.35 29.60 54.76 57.92 56.74 62.07 59.66 75. 75.53 78.05 76.76 77.92 We refer to tasks in which this validation set is employed for data selection in CROPI as \"Targeted,\" while other test sets are designated as untargeted tasks (\"Untar.\"). We compared CROPI with several data selection baselines in selection ratio α = 0.1: Learnability (Bae et al., 2025), Pass Rate (Yu et al., 2025), Influence Function (Pruthi et al., 2020) for global-level data selection, and the DAPO (Yu et al., 2025) for batch-level data selection. Due to computational constraints, baseline comparisons are conducted on the 1.5B model. Full details regarding model versions, dataset construction, and hyperparameters are available in Appendix E."
        },
        {
            "title": "5.2 Main Results",
            "content": "As shown in Table 1, CROPI consistently outperforms both full-data training and all baselines across different model scales in targeted tasks, demonstrating superior sample and step efficiency. The improvements are particularly pronounced in the early stages of training. For the 1.5B model, CROPI achieves remarkable 2.66 step-level speedup compared to training on the full dataset on targeted tasks, as illustrated in Figure 3. This acceleration is achieved while using only 10% of the training data in each phase. In contrast, while other data selection baselines show some initial gains, their performance plateaus quickly because they rely on static estimation of data utility and fail to adapt to the evolving policy. Meanwhile, CROPI also exhibits strong generalization capabilities. We observe significant performance gains on \"Untargeted\" benchmarksthose not used for creating the validation setindicating that the data selected by CROPI benefits the models overall reasoning ability, not just performance on the targeted tasks. As can be seen on Table 2, due to the requirement to compute gradients over the entire dataset (after filtering out samples that are completely correct or incorrect), the data selection time for CROPI remains non-negligible without rollouts. However, even after accounting for the selection time, the slowdown factor is only about 0.81x, and CROPI still achieves 2.16x speedup overall. Moreover, there remains significant room for optimization in our gradient computation process, such as selecting partial subsets, accelerating parallelization, or training proxy scorer. Thus, the time speedup of CROPI could be further improved, highlighting its considerable potential. Figure 3: Training curves on 1.5B setting. CROPI surpasses all other baselines and achieves significant steplevel accelerate ratio 2.66 compared to full data training while using only 10% of data during each phase. Time Cost Select Train 1.5B 7B 1.5B-R1 1.2h (19k) 2.6h (18k) 3.4h (17k) 5.2h (200 steps) 9.6h (200 steps) 13.7h (100 steps) Table 2: Time costs of data selection and training for CROPI in each phase. We denote the number of prompts to process (gradient computation, projection, cossim) in data selection and optimization steps in training stage in brackets. Time cost is computed in 8-GPUs (NVIDIA H100) machine."
        },
        {
            "title": "6 Analysis",
            "content": "This chapter takes in-depth analysis in two key computational components of the CROPI framework: random projection and data selection. We also provide empirical analysis of off-policy gradient estimation in POPI in Appendix F."
        },
        {
            "title": "6.1 Analysis on Sparse Random Projection",
            "content": "As mentioned on Section 3, we use sparse random projection for the projection of full-parameter gradients. Specifically, we randomly select proportion of dimension of the gradient to perform random projection to avoid computation over the entire high-dimensional gradient. We define this proportion as sparse ratio. However, through our experiments, we surprisingly found that this dropout actually improve the preservation of the inner products between gradient feature after random projection. We sampled 50 prompts from the training set and computed the GRPO gradients for the 1.5B model. We then selected subset of gradient dimensions according to predefined sparse ratio Figure 4: Rank preservation experiments for Sparse Random Projection. and applied random projection to these selected dimensions. For all gradients, we compute the pairwise cosine similarities before and after sparse random projection. We define precision@10% as the probability that, for each projected gradient, the top 10% most similar gradients (based on cosine similarity) contain the top 10% most similar gradients as measured by the full-parameter cosine similarities. This metric reflects the degree to which the random projection preserves the ranking of similarities among gradient features. higher precision@10% indicates better preservation. Figure 4 presents the experimental results under the 1.5B Setting. We observe that when we directly apply random projection to the full gradient (i.e. sparse ratio equals 1) , the precision@10% of similarity ranking is only around 13%, comparable to random selection. However, when the sparse ratio is 0.1, the precision@10% is significantly higher, reaching nearly 80%. This is counter-intuitive phenomenon: under sparse random projection, the ranking preservation is actually better with less information. We hypothesize that this may be related to the presence of numerical noise in the gradients: random projection can amplify such noise. Sparsity, while masking some information, filters out much of the numerical noise and achieves better signal-to-noise ratio around sparse ratio of 0.1. We include more analysis in Appendix G."
        },
        {
            "title": "6.2 Analysis on data selected by CROPI",
            "content": "We analyze the training data selected by CROPI under the 1.5B setting. Specifically, we examine the top-100 and bottom-100 training samples ranked by POPI scores, given validation sets GSM8K and MATH. For each checkpoint (training steps: 0, 200, 400, 600, 800), we evaluate both the semantic simiFigure 5: Semantic similarity between top-100 and bottom-100 training prompts selected by POPI and the validation set. larity between selected samples and the validation set, as well as the models pass rate on these samples. Sematic Correlation. For semantic analysis, we compute embeddings for each prompt using BGE-large-en-V1.5 (Xiao et al., 2023). The average embedding of the validation set is compared to the embeddings of top-100 and bottom-100 samples via cosine similarity. As shown in Figure 5, data selected by POPI exhibits higher semantic similarity to the validation set compared to both randomly sampled (baseline) and bottom-100 samples. These findings suggest that POPI leverages latent relationships between gradient and semantic spaces to automatically identify training samples most relevant to the validation set. Pass Rate. To analyze the pass rate of the selected data, we contrast the performance of the top-100 prompts (selected using the MATH validation set) on the base model versus the evolving online model. As illustrated in Figure 6, we plot both the offline pass rate (pass rate on the base model) and the online pass rate (pass rate on the current training checkpoint). The offline pass rate (blue bars) shows downward trend after the initial step, dropping from 0.75 to around 0.53. This indicates that as training progresses, CROPI selects problems that are increasingly difficult for the original base model. In stark contrast, the online pass rate (green bars) exhibits strong upward trend, rising from 0.75 to 0.87. This demonstrates that while the selected problems are challenging, they fall within the current models learning frontier. The model effectively learns to solve them, reflecting CROPIs ability to dynamically select data that maximizes learning efficiency. Ultimately, the model is trained on data with high online pass rate (in the 0.6 to 0.9 range), which corresponds to Figure 6: MATH Top-100: Offline vs Online Pass Rate. This figure compares the pass rates of the top100 prompts selected using the MATH validation set. The Offline Pass Rate (blue) shows the performance of the base model on these prompts, indicating their inherent difficulty. The Online Pass Rate (green) shows the performance of the model at the current training step, demonstrating its learning progress on the curated data. the difficulty interval where performance improvement is most pronounced. In Appendix H, we provide detailed breakdown of data source, knowledge catagirues, diversity of selected data. These results demonstrate that CROPI not only achieves strong performance but also offers degree of interpretability."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper introduced principled data selection method for RLVR using influence functions. To circumvent the prohibitive cost of online rollouts for LLMs, we developed novel off-policy estimation technique that evaluates data influence using offline trajectories. Our curriculum learning framework, CROPI, leverages this method to select the most impactful data for training. Experiments demonstrate that CROPI significantly accelerates learning, achieving remarkable speedup on RL training while using substantially less data than fulldataset training. Our work validates that influencebased data selection is theoretically-grounded and highly efficient alternative to common heuristics, paving the way for more scalable and effective training of large reasoning models."
        },
        {
            "title": "Limitations",
            "content": "Theoretical Limitations. In this paper, we restrict our analysis to first-order influence estimation under the SGD optimizer. Future work could extend this framework to encompass broader range of optimizers and influence estimation approaches. While we employ off-policy gradient estimates to compute influence, we do not provide theoretical analysis of the associated errors, bias, or variance; addressing these aspects is left for future research. Limitations of Offline Trajectories. In this study, we estimate the gradients solely using trajectories from the base model. This choice results in certain training prompts having zero gradient specifically, those for which the base models predictions are entirely correct or incorrect (for these cases, the GRPO advantage is zero), and thus they lack gradient information during online gradient computation. Further investigations could consider incorporating positive (strong) or negative examples to ensure that all prompts possess non-zero gradient signals. Additionally, we do not explore the use of rollouts generated by smaller models to estimate the gradients of larger models, nor do we investigate reusing rollouts collected during the training process (e.g., via replay buffer). These directions remain open for future work. Limitations of the Experimental Setting. In terms of task scope, our experiments are limited to single-turn mathematics question answering scenarios; extension to multi-turn dialogues and other types of reasoning, agentic, or multi-modal tasks is promising direction for future studies. In terms of scale, our empirical validation predominantly focuses on models of 1.5B and 7B parameters, with training steps limited to fewer than 1000. Extension to larger-scale models and longer training durations is left to future work. Numerical error in gradient computation. As we utilize float16 format to conduct gradient-related calculation, numerical error can not be ignored and might be increased by random projection. We proposed randomly dropout operation before random projection as way to gain better rank preservation. Additional details can be found in Appendix G."
        },
        {
            "title": "References",
            "content": "Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak. 2025. Online difficulty filtering for reasoning arXiv preprint oriented reinforcement learning. arXiv:2504.03380. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, and 1 others. 2021. Training verifiers arXiv preprint to solve math word problems. arXiv:2110.14168. Gordon Cormack, Charles LA Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 758759. Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, and 1 others. 2023. Studying large language model generalization with influence functions. arXiv preprint arXiv:2308.03296. Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, and Minlie Huang. 2024. Data selection via optimal control for language models. arXiv preprint arXiv:2410.07064. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Frank Hampel. 1974. The influence curve and its role in robust estimation. Journal of the american statistical association, 69(346):383393. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, and 1 others. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Yuzheng Hu, Fan Wu, Haotian Ye, David Forsyth, James Zou, Nan Jiang, Jiaqi W. Ma, and Han Zhao. 2025. snapshot of influence: local data attribution framework for online reinforcement learning. Preprint, arXiv:2505.19281. William Johnson, Joram Lindenstrauss, and 1 others. 1984. Extensions of lipschitz mappings into hilbert space. Contemporary mathematics, 26(189-206):1. Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In International conference on machine learning, pages 18851894. PMLR. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. 2020. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. Preprint, arXiv:2005.01643. Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025. Limr: Less is more for rl scaling. arXiv preprint arXiv:2502.11886. Aleksander Madry Logan Engstrom, Axel Feldmann. 2024. Dsdm: Model-aware dataset selection with datamodels. International Conference on Machine Learning (ICML), 2024. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. 2025. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/DeepScaleRSurpassing-O1-Preview-with-a-1-5B-Model-byScaling-RL-19681902c1468005bed8ca303013a4e2. Notion Blog. MARIO-Math. 2024. Gaokao2023-math-en dataset. math ai. 2024a. Aime24 dataset. math ai. 2024b. Amc23 dataset. Accessed: 2024-06-13. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Preprint, arXiv:2501.19393. OpenAI. 2024. Learning to reason with llms. Accessed: 2024-09-12. Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. 2023. Trak: Attributing model behavior at scale. arXiv preprint arXiv:2303.14186. Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. 2020. Estimating training data influence by tracing gradient descent. Advances in Neural Information Processing Systems, 33:1992019930. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust region policy optimization. In International conference on machine learning, pages 18891897. PMLR. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256. Yifan Sun, Jingyan Shen, Yibin Wang, Tianyu Chen, Zhendong Wang, Mingyuan Zhou, and Huan Zhang. 2025. Improving data efficiency for llm reinforcement fine-tuning through difficulty-targeted online data selection and rollout replay. arXiv preprint arXiv:2506.05316. Richard Sutton, Andrew Barto, and 1 others. 1999. Reinforcement learning. Journal of Cognitive Neuroscience, 11(1):126134. Jiachen Tianhao Wang, Tong Wu, Dawn Song, Prateek Mittal, and Ruoxi Jia. 2024. Greats: Online selection of high-quality data for llm training in every iteration. Advances in Neural Information Processing Systems, 37:131197131223. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, and 1 others. 2025. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571. Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, and 2 others. 2024. Training large language models for reasoning through reverse curriculum reinforcement learning. Preprint, arXiv:2402.05808. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, and 1 others. arXiv preprint 2024. Qwen2 technical report. arXiv:2407.10671. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, and 1 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Yang Zhao, Kai Xiong, Xiao Ding, Li Du, Zhouhao Sun, Jiannan Guan, Wenbin Zhang, Bin Liu, Dong Ufo-rl: Hu, Bing Qin, and 1 others. 2025. Uncertainty-focused optimization for efficient reinforcement learning data selection. arXiv preprint arXiv:2505.12457. Derivation of First-order Influence Function In this Section, we will restate the first-order influence estimation in TracIn (Pruthi et al., 2020) in RL context. In RL context, our goal is to maximize performance function J(θ) = Es0ρ0(),τ πθ [R(τ )] (Sutton et al., 1999). In LLM, the s0 corresponds to the input query. We rewrite the RL objective under test query 0: 0)], 0) := Eτ πθ(s Consider policy πθl at optimization step l. Using first-order Taylor Expansion, the objective function Es where J(θ; 0q()[J(θ; 0)[R(τ )] max θ (10) (9) in next iteration step J(θl+1; 0) can be rewritten as: J(θl+1; 0) = J(θl; J(θl; 0) + θJ(θl; 0) + θJ(θl; 0), θl+1 θl + O(θl+1 θl2) 0), θl+1 θl (11) (12) Consider training the policy πθl using Stochastic Gradient Ascent (SGA) with batch size 1 and learning rate ηl. At iteration l, let s0 be the initial state sampled as the training data point. Since the objective in reinforcement learning is to maximize the performance function J, the update uses positive sign before the policy gradient: θl+1 = θl + ηtθJ(θl; s0) J(θl+1; 0) J(θl; 0) + ηtθJ(θl; 0), θJ(θl; s0) (13) (14) Consider training policy with training prompts, denoted by {s(i) i=1. Following the approximation of TracInCP as proposed in (Pruthi et al., 2020), we treat one epoch of training as single optimization step with large batch size , and, for simplicity, we ignore parameter updates within the epoch by assuming the parameters remain constant throughout. Let the parameters at the start and end of the m-th epoch be represented as θ(m) = θlm and θ(m+1) = θlm+1, respectively, where the actual number of update steps within the epoch is lm+1 lm. 0 }N Under this approximation, the parameter update after one epoch can be expressed as θ(m+1) θ(m) + ηt (cid:88) i=1 θJ(θ(m); s(i) 0 ), and for test sample 0, the change in the objective function is approximated by (cid:88) J(θ(m+1); 0) J(θ(m); 0) + ηt θJ(θ(m); 0), θJ(θ(m); s(i) 0 ). i=1 (15) (16) We define the first-order influence of training prompt s(i) 0 on the model checkpoint πθ(m) with respect to test sample 0 as: Inf(πθ(m); s(i) 0 , 0) := θJ(θ(m); 0), θJ(θ(m); s(i) 0 ). (17) If we have multiple test samples, denote the distribution of test query as q(), 0q(),τ πθ(s 0 q(), then the 0)[R(τ )]. With similar derivation, we can rewrite objective function becomes J(θ; q) = Es the influence of training prompt s(i) 0 with respect to test distribution as: Inf(πθ(m); s(i) 0 , q) := θJ(θ(m); q), θJ(θ(m); s(i) 0 ). (18) In practical implementations, to prevent data leakage, we allocate small subset of the training data as validation set and use the validation samples to measure the influence of the training data. Derivation of Off-Policy Gradient In order to estimate the online influence with offline trajectories, we need to first approximate the policy gradient using offline trajectories, this is equivalent to the off-policy policy gradient estimation. We first write the vanilla policy gradient in on-policy form (Sutton et al., 1999): θJ(θ) = Eτ πθ [ τ 1 (cid:88) t=0 Atθ log πθ(xtst)] (19) Since we are using initial trajectories τ β = πθ0 to estimate the gradient, using importance sampling: θJ(θ; s0) = Eτ πθ(s0)[ τ 1 (cid:88) t=0 Atθ log πθ(xtst)] = Eτ β(s0)[ρθ(τ s0) τ 1 (cid:88) t= Atθ log πθ(xtst)] where ρθ(τ s0) = πθ(τ s0) β(τ s0) (20) (21) However, for multi-step trajectory generation, the variance of the importance weight ρθ(τ s0) can become extremely large. This high variance arises due to the product of many likelihood ratios over the steps, leading to instability and poor sample efficiency in the gradient estimate. We more widely used off-policy gradient estimator from (Schulman et al., 2015, 2017) that uses importance sampling in token level: θJ(θ; s0) = θJ(πθ; s0) = θ[J(β; s0) + Esdπθ (s0),xπθ(s)[Aβ(s, x)])] sdβ (s0),xπθ(s)[Aβ(s, x)])] θ[J(β; s0) + = θ[J(β; s0) + sdβ (s0),xβ(s)[ πθ(xs) β(xs) Aβ(s, x)]] = θE sdβ (s0),xβ(s)[ πθ(xs) β(xs) Aβ(s, x)]) = sdβ (s0),xβ(s)[ πθ(xs) β(xs) Aβ(s, x)θ log πθ(xs)]) = t,stdβ (sts0),xtβ(xtst) = t,stdβ (sts0),xtβ(xtst) (cid:20) πθ(xtst) β(xtst) Aβ ρπθ (cid:104) Aβ (cid:21) θ log πθ(xtst) (cid:105) (cid:18) θ log πθ(xtst) ρπθ = Eτ β(s0) 1 τ τ 1 (cid:88) Aβ ρπθ θ log πθ(xtst) (cid:19) := πθ(xtst) β(xtst) = {τk}K k=1β(s0) = {τk}K k=1β(s0) t="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 (cid:88) k=1 1 τk 1 τk τk1 (cid:88) t=0 τk1 (cid:88) t=0 k,tAβ ρπθ k,tθ log πθ(xk,tsk,t) θρπθ k,tAβ k,t (22) Thus, our off-policy gradient estimator can be formulated as: (cid:98)gβ(θ, s0, {τk}K k=1) 1 (cid:88) k=1 1 τk τk1 (cid:88) t=0 where For group normalized advantage estimator in GRPO, we have: ρπθ k,t = (cid:98)Aβ k,t = πθ(xk,tsk,t) β(xk,tsk,t) R(τk) (cid:98)Eβ[R(τ )] (cid:98)σβ[R(τ )] θρπθ k,t (cid:98)Aβ k,t (23) (24) (25) (26) (cid:80)K . i.e. (cid:104) (R(τ ) (cid:98)Eβ[R(τ )])2(cid:105) (cid:98)Eβ denotes empirical mean under policy β, k=1[R(τk)], τk β(s0), = 1, 2, ..., K. (cid:98)σβ denotes the empirical standard value under policy β: (cid:98)σβ[R(τ )] = (cid:114) (cid:98)Eβ[R(τ )] = 1 (cid:98)Eβ The approximations employed here generally require that πθ and β are relatively close. In TRPO (Schulman et al., 2015), β = πθold is constrained to be close to πθ in terms of the Kullback-Leibler (KL) divergence. In the context of LLM RLVR, when performing GRPO (Shao et al., 2024) training, KL loss is introduced to constrain the divergence between the training policy and the initial policy. Experimental results demonstrate that the estimated KL values during RLVR training are usually maintained below 0.1, which ensures the reasonableness of the approximation to certain extent."
        },
        {
            "title": "C Related Works",
            "content": "C.1 Data Selection for RLVF Recently, how to automatically select high-quality data for RLVF has become research topic of increasing interest. According to the time granularity of data selection, existing methods can be roughly divided into three categories: global-level, batch-level and phase-level. The first is the globallevel granularity, which selects data based on the initial policy over the entire dataset (Zhao et al., 2025; Wang et al., 2025). Although this approach is easy to implement, it cannot capture dynamic checkpoint information and often requires warmup training. The second granularity is at the batchlevel, where data selection is performed within each training batch. This method can better capture the dynamics of model training; however, due to the lack of long-term planning, it often introduces high variance that can lead to unstable training. Representative methods include DAPO (Yu et al., 2025) and ODF (Bae et al., 2025). The third category is compromise, where data selection is performed every certain number of training steps, which we refer to as phase-level (or curriculum) data selection. This approach strikes balance between capturing training dynamics and enabling long-term planning, but may still face challenges such as unstable training distributions. An example of this method is (Xi et al., 2024). As for the utiltity estimator, previous works use various ways to measure the utility of the data point, mainly focusing on some heuristics around difficulty and uncertainty of the training query s0. In order to select prompts with proper difficulties, some researchers use correctness rate of various trajectories as the signal of the difficulty (Li et al., 2025; Yu et al., 2025; Bae et al., 2025; Sun et al., 2025). As an example, Bae et al. (2025) suggests filtering data points with pass rate around 0.5 to construct the batch for GRPO training. Zhao et al. (2025) estimate prompt difficulty based on the models confidence (likelihood), and select moderately difficult prompts for reinforcement learning (RL) training. Another heuristic involves selecting data according to the uncertainty arising from data perturbation. For instance, Wang et al. (2025) train with prompts whose associated historical trajectories exhibit the highest reward variance. These methods generally need to obtain prompt trajectories to assess the utility of each prompt, but online rollouts with LLMs are costly in terms of computation and time. In order to avoid online rollouts, these methods have made significant sacrifices in online estimation: either selecting data based on the initial policy, which is global-level data selection, or using the pass rate signal from the data buffer as the basis for data filtering. Such compromises largely neglect the dynamics of RL training, for instance, the changes in the pass rate of the same data point across different stages of training. C. Influence Functions for LLM Data Selection Influence function (Koh and Liang, 2017) is gradient-based data attribution method derived from variantion analysis of objective function. Influence functions and their variants have been widely applied in the Pre-Training (PT) and Supervised Fine-Tuning (SFT) stages due to their strong theoretical guarantees and empirical effectiveness (Logan Engstrom, 2024; Wang et al., 2024; Gu et al., 2024; Grosse et al., 2023). Logan Engstrom (2024) proposed the datamodel framework leveraging an efficient influence function estimation (Park et al., 2023), which is inspired by influence function, to select pretraining data. LESS (Xia et al., 2024) is the first to introduce first-order influence functions into the post-training of large language models (LLMs), thereby improving the training efficiency of supervised fine-tuning (SFT). Wang et al. (2024) further refine LESS by extending it to the batch-level; however, its application remains limited to the SFT stage. Nonetheless, leveraging influence function theory to guide data selection for RLVF remains challenging problem. This is mainly due to the fact that, in RL settings, rewards and gradients for the data typically require rollouts to obtain, which is computationally expensive for LLMs. concurrent work Hu et al. (2025) explores online data selection using influence functions in RLHF. However, their setting differs substantially from RLVF, and the definition of the target function for influence estimation is largely heuristic."
        },
        {
            "title": "D Algorithmic pseudocode of CROPI",
            "content": "We formulate the process of our proposed method CROPI in Algorithem 1. Algorithm 1 Algorithmic pseudocode of CROPI Require: Training dataset Dtr, Validation datasets {Dval,j}V j=1, Base LLM πθ0, Selection ratio α. number of phases , training steps per phase E. Ensure: Output final policy πθ(M ) Load initial policy πθ(0) πθ0 for = 0, 1, ..., 1 do for s(i) 0 Dtr, = 1, 2, ..., do Compute POPI score UPOPI-R(πθ(m); s(i) 0 ); end for Select training subset D(m) arg max S=αDtr (cid:88) s0S UPOPI-R(πθ(m); s0) Optimize the policy with GRPO πθ(m+1) GRPO(πθ(m); E) end for return πθ(M )"
        },
        {
            "title": "E Additional Experimental Details",
            "content": "E."
        },
        {
            "title": "Implementation Details",
            "content": "Base Models. We use Qwen2.5-1.5B-Instruct, Qwen2.5-7B-Instruct (Qwen et al., 2025), and Deepseek-R1-Distill-Qwen-1.5B (Guo et al., 2025), which we refer to as 1.5B, 7B, and 1.5BR1, respectively. We use the official prompt template from Qwen-Math (Yang et al., 2024) for the Qwen2.x models and the DeepSeek-R1 template (Guo et al., 2025) for 1.5B-R1. For more details on prompts, please see Appendix E.4. Datasets. Our primary training set is the intersection of GSM8K-Train (Cobbe et al., 2021), MATHTrain (Hendrycks et al., 2021), and DeepScaleRPreview-Dataset (Luo et al., 2025), containing 47K unique mathematical queries. For the 1.5B-R1 model experiments, we randomly sampled 25K queries from this set to reduce training time. Validation Sets. To enable influence-based selection, we create validation set by allocating 20% of each designated test set, with maximum cap of 100 examples per set to prevent its use in training. To mitigate data leakage, all reported test results exclude these validation examples. The validation set composition varies by model: 1.5B: GSM8K-Test, MATH-Test 7B: GSM8K-Test, MATH-Test, OlympiadBench, AIME24 1.5B-R1: GAOKAO23EN, AMC23, OlympiadBench, AIME24 The remaining test sets are used to evaluate generalization performance. We divide the test benchmarks into two categories. Targeted tasks (or \"Targeted\") refer to those whose domains are represented in the validation set. All other test sets are designated as Untargeted tasks (\"Untargeted\"). To evaluate the in-domain and out-of-distribution (OOD) generalization of CROPI, we report the average accuracies on Targeted and Untargeted tasks, respectively, under various settings. Note that reported test results exclude performance on the validation sets. E.2 Hyperparameters of CROPI We use the VeRL (Sheng et al., 2024) framework for training. For rollout engine, we use vllm (Kwon et al., 2023). For the POPI computation process, we set the sparse ratio of the random projection to 0.01. We use TRAK (Park et al., 2023) for efficient random projection on GPU. The data selection ratio is α = 0.1. The number of update steps per training phase, E, is set to 200 for the 1.5B and 7B models and 100 for the 1.5B-R1 model. The total training steps for the 1.5B, 7B, and 1.5B-R1 models were 1000, 600, and 300, respectively. E.3 Baselines in Main Results We compare CROPI against the following baselines, with all hyperparameters aligned: 1. Learnability (Bae et al., 2025): Utility is estimated as (πθ; s0) = p(1 p), where is the offline pass rate of prompt. 2. Pass Rate (Muennighoff et al., 2025; Yu et al., 2025): Utility is defined as (πθ; s0) = I0<p<1, where is the offline pass rate of prompt. 3. Influence Function (Pruthi et al., 2020): Standard first-order influence function estimation. This baseline is equivalent to CROPI with only 1 phase. For the above baselines, data selection is performed once globally using the initial policy πθ0 at selection ratio of α = 0.1. We also include DAPO (Yu et al., 2025), batch-level filtering method that removes samples with perfect or zero pass rates and replaces them from buffer of historical trajectories. E.4 Prompt Templates for Evaluation & RL Training For experiments on Qwen2.5-1.5B-Instruct (1.5B) and Qwen2.5-7B-Instruct (7B) , we use prompt template in official evaluation code repository of Qwen2.5-Math 1. And for the experiments on Deepseek-R1-Distill-Qwen-1.5B (1.5B-R1), we template used in the training of use prompt Deepseek-R1 (Guo et al., 2025). The prompt templates are shown on Table 3. We made slight modifications to the prompt for R1 to facilitate the extraction of the final answer from the boxed. E.5 Hyperparameters of RL We present the key hyperparameters used for GRPO training on Table 4. For 1.5B / 7B / 1.5B-R1 experiment settings, we use max response length of 2048 / 4096 / 8192 respectively. Additional Analysis on Off-policy"
        },
        {
            "title": "Gradient Estimator",
            "content": "We conduct supplementary experiments to evaluate our proposed off-policy gradient estimator in 4. To demonstrate this point, we aim to verify that the off-policy gradient estimator produces gradients that are sufficiently consistent with the on-policy gradients at the step 200 checkpoint of CROPI. Specifically, we collect 50 problems on which both the original Qwen2.5-1.5B-Instruct model and the step 200 checkpoint of CROPI neither achieve perfect success nor complete failure across 8 rollouts. Using Equation 1, we obtain the on-policy gradients for these 50 problems, and using Equation 4, we compute the corresponding off-policy estimated gradients. We then calculate the cosine similarity between each pair of on-policy and off-policy gradients and visualize the distribution in Figure 7. As shown, 40 out of the 50 pairs exhibit cosine similarity greater than 0.6, which provides strong evidence for the effectiveness of our proposed offpolicy gradient estimator. We further investigate the rank preservation capability of the off-policy gradient estimator. Specifically, based on the two sets of 50 gradients obtained above, we construct two 5050 cosine similarity matrices and compute the index ranking for each row. In the ideal case, the ranking orders of corresponding rows in the two matrices should be identical, indicating perfect rank consistency of the estimator. However, when evaluating the top 10% rank preservation across all Figure 7: Cosine similarity distribution between onpolicy gradients and off-policy gradients produced by our proposed estimator gradients of interest, the off-policy gradient estimator achieves only 28.80% consistency. Although this performance is significantly higher than that of random baseline, we believe there remains substantial room for improving rank preservation."
        },
        {
            "title": "Projection",
            "content": "In order to analyze the rank preservation efficiency of gradient projection method we used, we provide additional experimental results and one possible explanation for the outcomes. Following the setting of Section 6.1, we analyze the influence of the sparse ratio for the 3B model. We observe the same pattern of rank preservation curve as in the 1.5B model experiment. To be specific, we observe that when we use the full gradient before applying the random projection, the precision@10% is only slightly above the random guess. However, when the sparse ratio reaches 0.1, the precision@10% increases to more than 80%. Figure 8: Rank preservation experiments for Sparse Random Projection under 3B model setting. 1https://github.com/QwenLM/Qwen2.5-Math As mentioned in 6.1, we hypothesize that sparsiTable 3: Prompt templates used in CROPI experiments. \"<prompt>\" will be replaced by specific training prompt during training. Setting 1.5B & 7B 1.5B-R1 Prompt Templates System: Please reason step by step, and put your final answer within boxed{}. User: <prompt> System: conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here, and put your final answer within boxed{} </answer>. User: <prompt> fication reduce numerical error in projection operation, which is conducted in float 16 format. Serious numerical instability and precision loss can happen during projection. Sparsity might help filter out much of the error by masking bunch of information. In Figure 9, we plot the histogram of the element value distribution of the projected gradients. We observe that conducting random projection after sparsification cluster more element values towards zero, which might help avoid precision loss in rank preservation. We leave further exploration of this area to future work. H.1 Data Source of Selected Prompts We first analyzed the dataset origins of the top 100 training examples selected using POPI with different validation sets, as shown in Figure 10 and Figure 11. Across multiple rounds of data selection, we observe pronounced shift in the composition of training examples deemed most relevant for the GSM8K validation set. Initially, in Round 0, 71% of the top 100 selected examples are sourced from GSM8K data and 6% from MATH. However, by Round 4 (step=800), GSM8Ks representation rises to 99%, while MATH data are almost entirely excluded. This dynamic indicates that the selection algorithm increasingly prioritizes GSM8K training samples that are most advantageous for improving performance on the GSM8K validation set. Furthermore, the process simultaneously eliminates GSM8K examples that are particularly ineffective for the MATH validation set, underscoring nuanced mechanism of data curation that both refines selection towards task relevance and discards samples detrimental to cross-task generalization. Figure 9: Element values distribution of gradients produced by normal random projection and sparse random projection with sparse ratio of 0.1."
        },
        {
            "title": "H Additional Analysis on Data Selected",
            "content": "by CROPI Following the setting of Section 6.2, we put more analysis results in this section. Figure 10: The source dataset of top-100 and bottom100 training prompts selected by POPI with validation set GSM8K in different training steps. Table 4: Key Hyperparameters for GRPO Training Parameter Value Description RL Algorithm Base Model Batch Size Max Prompt Length Max Response Length Advantage Estimator Actor Learning Rate PPO Mini-batch Size KL Regularization KL Coefficient (β) KL Loss Type Entropy Coefficient Qwen2.5-1.5B-Instruct / Qwen2.5-7B-Instruct / DeepSeek-R1-DistillQwen-1.5B 128 2048 2048 / 4096 / 8192 grpo 1 106 128 True 0.001 low_var_kl 0.001 Rollout & Generation Rollout Engine Samples per Prompt (n) Tensor Parallel Size Dynamic Batching Max Batched Tokens vllm 8 4 True 16384 The pretrained model used as the starting point. The number of prompts processed in each training step. Maximum token length for the input prompt. Maximum token length for the generated response. The specific RL algorithm used for training. The learning rate for the actor models optimizer. The batch size used for each PPO optimization update. Enables KL-divergence penalty against the reference model. Weight of the KL-divergence term in the loss function. specific variant of KL loss calculation. Weight of the entropy bonus to encourage exploration. The inference engine used for generating samples. Number of candidate responses generated for each prompt. Degree of tensor model parallelism for rollouts. Enables dynamic batching for rollouts. Maximum number of tokens in dynamic vLLM batch. dation set itself. Moreover, as training progresses, the distribution of knowledge domains among the selected samples also shifts dynamically to meet the need of online policy. H.3 Semantic Diversity of Selected Prompts We also evaluated the internal diversity of the top100 and bottom-100 training samples selected by POPI, quantified by 1 Eei,ej E[cossim(ei, ej)], where denotes the set of semantic embeddings for each dataset. As shown in Figure 13, the diversity of the POPI-selected top-100 samples is noticeably lower compared to both the random baseline and the bottom-100 set. This indicates that training examples highly relevant to the validation set tend to be semantically similar, which aligns with our intuition. Such reduced diversity could potentially limit the generalization ability of the trained model. Employing multiple diverse validation sets and aggregating the selected training samples from each can effectively mitigate this issue. Our experimental results further demonstrate that models trained with CROPI exhibit performance gains even on untargeted test sets, indicating good generalization capability of the CROPI approach. Figure 11: The source dataset of top-100 and bottom100 training prompts selected by POPI with validation set MATH in different training steps. H.2 Knowledge Domains of Selected Prompts Since direct inspection of each individual problem makes it difficult to observe distinctive patterns, we adopted the categorization approach from s1 (Muennighoff et al., 2025). Utilizing GPT-4o, we classified both the validation set (randomly sampled 100 questions) and the top-100 mathematics problems selected by POPI according to the Mathematics Subject Classification (MSC) system (e.g., geometry, combinatorics, etc.) from the American Mathematical Society2. The results (Figure 12 show that the distribution of knowledge domains in the selected data closely mirrors that of the vali2https://mathscinet.ams.org/mathscinet/msc/msc2020.html Figure 12: The knowledge dimains of top-100 and training prompts selected by POPI with validation set MATH in different training steps. \"Validation\" denotes the knowledge category distribution of the validation set, while \"step=X\" refers to the distribution of knowledge categories within the top-100 selected samples at the checkpoint corresponding to step=X. validation set in gradient space, with cosine similarity values falling below zero. We put the most infuential prompts selected by POPI with MATH validation set MATH in Table 5, which reflects the training dynamics of the model in data perspective. Figure 15: The influence scores of top-100 and bottom100 training prompts selected by POPI with validation set GSM8K and MATH in different training steps."
        },
        {
            "title": "Experiments",
            "content": "We plot the main recorded metrics from RL training under different settings in Figure 16, 17, 18. As illustrated in these figures, the entropy of the data selection methods is significantly lower than that of the full data baseline during RL training, which may hinder policy exploration in subsequent training stages. Figure 13: The semantic diversity of top-100 and bottom-100 training prompts selected by POPI with validation set GSM8K and MATH in different training steps. H.4 Human Annotated Difficulty-level of Selected Prompts Since MATH includes human-annotated difficulty levels, we also recorded the difficulty levels of MATH-Train samples selected when MATH served as the validation set, as depicted in Figure 14. It can be observed that, as training progresses, CROPI increasingly favors samples with higher difficulty levels. This trend suggests continual expansion of the models capability boundaries, requiring the selection of progressively more challenging problems. Figure 14: The human annotated difficulty-levels of top 100 prompts in MATH-Train dataset H.5 Most influential questions We also visualized the influence scores of the top100 and bottom-100 training samples calculated by POPI across different rounds, as shown in Figure 15. The influence scores for the top-100 samples exhibit steadily increasing trend, eventually stabilizing around 0.5. In contrast, the bottom-100 samples display substantial divergence from the Round Rank POPI Score Prompt 0 0 1 1 1 2 2 2 3 3 4 4 4 2 3 1 2 3 1 3 1 2 3 1 3 0.2166 0.2160 0.2136 0.5534 0.5479 0. 0.5382 0.5314 0.5263 0.5716 0.5623 0. 0.5555 0.5532 0.5472 Let and satisfy mn = 4 and + = 5. What is n? If + 2y 3z = 7 and 2x + 2z = 6, determine 8x + y. 5 + + If of (5 + x)(20 x)? 20 = 7, what is the value bird discovered 5438 different ways to build nest in each of its eight tree homes. How many ways are there in base 10? How many three-eighths are there in 8 5 3 3? secret facility is rectangle measuring 200 300 meters... How many meters did the fourth guard run to reach the intruder? Determine the least possible value of (x + 2)(x + 3)(x + 4)(x + 5) + 2024 where is real number. What is the total volume and the total surface area in square feet of three cubic boxes if their edge lengths are 3 feet, 5 feet, and 6 feet, respectively? Find the sum of 5437, 657, and 67 in base 7. Suppose that and are positive numbers with xy = 1 9 , x(y + 1) = 7 9 , and y(x + 1) = 5 18 ... What is the value of (x + 1)(y + 1)? Given the plane vectors and ... calculate the angle between vectors and What is the smallest positive integer such that 5n 105 (mod 24)? . In set of 15 different-colored markers, how many ways can Jane select five markers if the order of selection does not matter? What is the greatest common divisor of 1729 and 1768? For what value of does 6 + ni = 5? Table 5: Most Influential prompts in different rounds. Round 0 means the first round of data selection, corresponding to training step of 0. Round 1 corresponds to training step 200, and so on. Figure 16: Training curves in 1.5B setting: KL Loss. Entropy Loss, Accuracy across different test sets. Figure 17: Training curves in 7B setting: KL Loss. Entropy Loss, Accuracy across different test sets. Figure 18: Training curves in 1.5B-R1 setting: KL Loss. Entropy Loss, Accuracy across different test sets."
        }
    ],
    "affiliations": [
        "CoAI Group, Tsinghua University"
    ]
}