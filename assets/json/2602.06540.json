{
    "paper_title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
    "authors": [
        "Yishan Li",
        "Wentong Chen",
        "Yukun Yan",
        "Mingwei Li",
        "Sen Mei",
        "Xiaorong Wang",
        "Kunpeng Liu",
        "Xin Cong",
        "Shuo Wang",
        "Zhong Zhang",
        "Yaxi Lu",
        "Zhenghao Liu",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 0 4 5 6 0 . 2 0 6 2 : r AgentCPM-Report AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research AgentCPM Team https://huggingface.co/openbmb/AgentCPM-Report https://huggingface.co/openbmb/AgentCPM-Report-GGUF https://github.com/OpenBMB/AgentCPM Abstract Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing significant challenge for current language models. Most existing approaches follow plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for userauthored data. In this work, we present AgentCPM-Report, lightweight yet highperforming local solution composed of framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce Multi-Stage Agentic Training strategy, consisting of coldstart, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight."
        },
        {
            "title": "1 Introduction",
            "content": "Open-ended deep research requires artificial agents to navigate vast information landscapes and synthesize their findings into coherent, insightful reports (OpenAI, 2025; Google, 2025; x.AI, 2025; Perplexity, 2025; Kimi, 2025; ByteDance, 2025). In the context of such complex inquiry, writing is far more than the mere transcription of retrieved data. Instead, it reflects the knowledge-transforming process described in cognitive psychology (Scardamalia & Bereiter, 1987): because the information landscape is initially opaque, researchers rarely execute rigid, end-to-end plan derived solely from pre-existing thoughts. Rather, writing itself functions as reasoning mechanism, progressively revealing what is not yet known. Indeed, researchers often identify gaps, contradictions, or novel directions only during the act of drafting, indicating that effective synthesis depends on tight and continual coupling between planning and writing. Despite this, existing deep research systems struggle to replicate such dynamics. Early approaches followed retrieval-then-write paradigm (Fig. 1a) (Hu et al., 2025), in which agents generated content sequentially based on retrieved evidence. While flexible, this loosely structured process frequently degenerates into incoherence over long horizons. To improve structural consistency, more recent frameworks (Fig. 1b) (Wang et al., 2024, 2025; Yan et al., 2025), such as WebWeaver (Li et al., 2025b), adopt plan-then-write paradigm. By freezing comprehensive outline prior to writing, these systems enforce global structure and stability. However, this paradigm rests on the assumption of initial information completenessan assumption that is often violated in open-ended research. By reducing the downstream writer to an executor of static blueprint, this rigid separation prevents agents from capturing emergent insights: subtle connections and refinements that surface 1 AgentCPM-Report Figure 1: Comparison of different writing paradigms. only when articulating concrete arguments. As result, such methods encounter an insight ceiling, producing reports that are structurally sound yet intellectually shallow. Another critical limitation of the plan-then-write paradigm lies in its heavy reliance on generating highquality, comprehensive outline before writing begins. This requirement places substantial demands on the models reasoning capacity and domain knowledge. Smaller models are generally weaker in these aspects than large-scale models, which has led most existing deep research systems to rely on closed-source or online large models as their backbone. This reliance introduces practical and often overlooked challenge: online deployment makes it difficult to support writing over users local or private data, as uploading such data inevitably raises security and privacy concerns. Consequently, there is growing need for fully local, on-device deep research and writing solution that does not depend on external large-scale models. These two limitations stem from the same root cause: the rigid separation between planning and writing. To address both the insight ceiling and the challenges of on-device deep research, we present AgentCPM-Report, lightweight yet high-performance local system built upon novel WARP (Writing As Reasoning Policy) framework and an 8B-parameter deep research agent. WARP is policy-level reformulation of deep research, motivated by the observation that any approach grounded in static planning inevitably incurs an insight ceiling. By modeling research as an iterative refinement loop, WARP enables planning decisions to emerge from, and adapt to, the writing process itself. Rather than adhering to fixed outline, the agent alternates between two macro-states: Evidence-based Drafting and Reasoning-driven Deepening. Crucially, WARP is formulated as dynamic policy instead of rule-based heuristic. In the Reasoning-driven Deepening state, the agent autonomously determines whether to terminate or continue deepening by evaluating the semantic density and logical coherence of the current draft. When further deepening is warranted, it decomposes high-level sections into more granular inquiries and updates the outline based on feedback from the writing process itselfclosely mirroring the human knowledge-transforming process. The dynamic nature of WARP introduces long-horizon credit assignment and vastly expanded action space, which standard training pipelines fail to handle. We design Multi-Stage Agentic Training strategy to ensure stable convergence under reasonable resource constraints.Specifically, we employ trajectory pruning mechanism to filter high-quality supervision signals, and design curriculum-based reinforcement learning pipeline that progressively optimizes local atomic actions before fine-tuning end-to-end behavior. This training strategy yields robust policy that adaptively balances research depth against computational cost, triggering recursive refinement only when it produces meaningful informational gains. Extensive experiments on DeepResearch Bench, Deep Consult, and DeepResearch Gym demonstrate the effectiveness of AgentCPM-Report, yielding substantial improvements in overall report qualityparticularly on the Insight metric. Despite relying on only an 8B-parameter agent, AgentCPM-Report matches or outperforms the leading closed-source deep research systems, including Gemini-2.5-Pro. These results indicate that specialized WARP inference paradigm enables small-size, open-source models to achieve deep research capabilities previously associated with proprietary large-scale systems. Together, they establish strong foundation for safe, privacy-preserving, and fully local deep research report generation. 2 AgentCPM-Report Figure 2: The WARP framework. The agent interleaves Evidence-Based Drafting (writing content) and Reasoning-Driven Deepening (updating the dynamic outline). This loop allows the agent to discover and bridge logical gaps that emerge only during the writing process."
        },
        {
            "title": "2 Method",
            "content": "In this section, we formally present AgentCPM-Report. First, we formulate deep research as unified sequential decision-making process. Then, we provide our WARP (Writing As Reasoning Policy) framework. Finally, we show the multi-stage agentic training strategy."
        },
        {
            "title": "2.1 Problem Formulation",
            "content": "Specifically, we formulate open-ended deep research as an iterative hierarchical decision-making process: At any interaction loop i, the agent observes global state Si = (Q, Oi, Di, Ci), comprising the user query Q, dynamic outline Oi, the current draft Di, and the context Ci retrieved at the current loop i. At the j-th step ti,j in loop i, the agent executes an action Ai,j selected from defined action space: {INITIALIZE, SEARCH, WRITE, EXPAND, TERMINATE}. This formulation unifies planning and writing: outline adjustments (Oi Oi+1) and content generation (Di Di+1) are treated as equivalent state transitions driven by the policy."
        },
        {
            "title": "2.2 The WARP Inference Diagram",
            "content": "WARP begins with coarse-to-fine initialization strategy designed to establish comprehensive research scope before diving into details. Starting from the initial state S0, the agent analyzes the query to generate broad search queries q0. Upon retrieving the background context C0, it synthesizes an initial Level-1 outline O0: O0 INITIALIZE(Q, C0). (1) In contrast to static planners Li et al. (2025b) that attempt to generate fully detailed hierarchy, our O0 is intentionally sparse, consisting only of high-level section titles and brief writing intents. This design mitigates the risk of being ungrounded. To maximize the final report quality, the agent operates under unified policy πθ that orchestrates the research trajectory. Crucially, this workflow is not linear but iterative, alternating between Drafting and Deepening. 3 AgentCPM-Report Table 1: Reward definition for Atomic Skill RL. For each metric, we indicate whether it requires references (Ref.?) or relies on the LLM-as-Judge method (LLM?)."
        },
        {
            "title": "Metric Name",
            "content": "Ref.? LLM? Description & Reward Objective"
        },
        {
            "title": "Planning",
            "content": "Basic Properties Holistic Quality Faithfulness"
        },
        {
            "title": "Retrieval",
            "content": "Relevance Recall"
        },
        {
            "title": "Writing",
            "content": "Basic Properties Holistic Quality Faithfulness Citation Precision Decision-Making Accuracy Section number and language consistency. Quality metrics such as guidance, logic, clarity, etc. Whether the content in the plan is real and reliable. Overlap score between retrieved docs and golden ones. Content length, citation number, and language consistency. Quality metrics such as relevance, coverage, depth, etc. Penalizes unsupported claims. Rewards citations that overlaps with golden ones. Whether terminate at the suitable time. Evidence-Based Drafting Given tentative outline Oi, the agent executes retrieve-then-write strategy to convert the structural plan into substantiated content. Unlike independent parallel generation, which often leads to fragmentation or redundancy, we enforce contextual consistency by conditioning retrieval queries on the accumulating narrative. For specific section k, the agent first formulates query qi,k based on user query Q, the sections intent Ok , and the draft context Di: qi,k SEARCH(Q, Ok , Di). (2) Then, the retrieval tools will acquire new content based on the query qi,k. This ensures that new information strictly extends the logical flow of previous sections. The agent then synthesizes the section content ck by grounding the text in retrieved evidence to guarantee faithfulness: Dk Dk1 WRITE(Q, Ok , Dk1 , ). (3) The objective is to achieve information integrationsynthesizing disparate sources into coherent argumentrather than mere aggregation. This phase focuses on writing, iteratively populating the outline to produce new draft Di+1 that serves as the foundation for deeper reasoning. Reasoning-Driven Deepening Initial outlines are inevitably constrained by the models pre-retrieval knowledge, which often creates an insight ceiling: the structure may cover the breadth of the topic but fail to capture its nuanced depth. To break this ceiling, the policy πθ periodically shifts from local drafting to global planning, treating the newly generated draft Di+1 as fresh observation for reasoning and diagnosis. Since Di+1 provides concrete reasoning context, the agent can detect logical gaps or superficial arguments that were invisible during initial planning. If section lacks depth, the agent generates local sub-sections to decompose the topic, updating Oi and triggering targeted drafting cycle: Oi+1 Oi EXPAND{k}(Q, Oi, Di+1). (4) The process concludes only when the agent verifies that the logical chain is complete and the content depth aligns with the querys complexity: End TERMINATE(Q, Oi, Di+1). (5)"
        },
        {
            "title": "2.3 Multi-Stage Agentic Training",
            "content": "While large-scale models have demonstrated strong capabilities within our WARP framework (see 3.3.1), training small-scale models (such as 8B) for open-ended research is non-trivial. The challenges are twofold: (1) Ambiguous Termination: even teacher models struggle to determine the optimal stopping point for research; (2) Sparse Rewards: long horizons make reward assignment difficult. To address these, we first introduce the trajectory pruning strategy in in 2.3.1. Then, we propose curriculum learning pipeline in 2.3.2. 4 AgentCPM-Report Figure 3: Overview of our multi-stage agentic training process. 2.3."
        },
        {
            "title": "Data Preparation",
            "content": "A critical challenge in training is the scarcity of expert trajectories that exhibit efficient decision-making. Teacher models often either expand indefinitely or terminate arbitrarily, giving rise to what we term the optimal stopping problem. To solve this, we introduce trajectory pruning strategy. Instead of cloning the teachers termination behavior, we force the teacher to \"over-expand\" recursively. This generates sequence of drafts with varying granularity {D1, . . . , DN }. We then retroactively identify the optimal point where the report draft Di has the highest score. We prune the trajectory at i, relabeling the action to TERMINATE. This provides supervision signal for information saturation, teaching the agent to stop based on report quality rather than arbitrary imitation. Details on our query construction and retrieval environment are provided in App. A.2 and App. A.3. 2.3."
        },
        {
            "title": "Curriculum Policy Optimization",
            "content": "Our optimization includes three stages, as shown in Figure 3: (1) SFT for Cold Start: Establishes basic instruction following and format adherence. (2) Atomic Skill RL: Uses teacher trajectories as anchors to master local execution and stabilize exploration. (3) Holistic Pipeline RL: Optimizes global report quality, enabling the agent to refine its strategy beyond the teachers limitations. Atomic Skill RL To tackle the reward assignment problem, we first decompose the global objective to atomic abilities: planning (Initialize, Expand), retrieval (Search), writing (Write), and decision-making (Terminate). Then, we design different reward functions for them, which combine execution results (e.g., basic properties, holistic quality, and faithfulness) with reference alignment, as shown in Table 1. This stage ensures the agent masters the \"how\"producing valid plans, precise searches, and coherent paragraphsbefore attempting to optimize global strategy. Holistic Pipeline RL Local correctness (e.g., valid paragraph) does not guarantee global coherence. Thus, the final stage shifts to end-to-end optimization to evaluate the final report quality, such as Comprehensiveness and Insight. Crucially, this stage empowers the agent to deviate from the teachers path. By propagating the holistic report score backward, the agent learns to trigger the deepening only when it yields significant informational gain. This effectively refines the quality-efficiency frontier, suppressing redundant expansions that the teacher might have made. 5 AgentCPM-Report"
        },
        {
            "title": "3.1 Settings",
            "content": "Implementation Details. We implement AgentCPM-Report using MiniCPM4.1-8B (Team et al., 2025) as the backbone for our deep research agent system. Training follows the curriculum described in 2.3, progressively scaling from atomic skills to holistic reporting. During the training and inference phase, we cap the report structure at three levels and limit the number of deepening steps to 12 to ensure efficiency. Detailed hyperparameters and data statistics per stage are provided in App. B.1. Benchmarks and Metrics. To ensure comprehensive evaluation, we test on three diverse benchmarks: (1) DeepResearch Bench (Du et al., 2025) (100 PhD-level scientific tasks); (2) DeepConsult (Dee, 2025) (102 business and financial analysis queries); and (3) DeepResearchGym (Coelho et al., 2025) (100 generalpurpose information-seeking tasks). We adhere to the standard evaluation protocols of each benchmark, employing Gemini-2.5-Pro, o3-mini, and GPT-4.1-mini respectively as impartial judges. Baselines. We compare AgentCPM-Report against three distinct categories of latest systems: (1) Proprietary Systems: Leading commercial deep research systems including OpenAI (OpenAI, 2025), Gemini (Google, (2) Prompt-Based Frameworks: 2025), and Claude (claude, 2025), and Doubao (ByteDance, 2025). WebWeaver (Li et al., 2025b), Enterprise DR (Prabhakar et al., 2025), and RhinoInsigh (Lei et al., 2025). (3) Trained Open Models: Recent open-source research agents including WebShaper (Tao et al., 2025), WebThinker (Li et al., 2025a) and DR Tulu (Shao et al., 2025)."
        },
        {
            "title": "3.2 Main Results",
            "content": "Our results on three benchmarks are summarized in Table 2 and Figure 4. (1) Our WARP framework has strong performance on Insight and Comprehensiveness. Across these benchmarks, our method achieves nearly the best performance in both Insight and Comprehensiveness metrics despite using the smallest model. Specifically, on the DeepResearch Bench, it achieves an Insight score of 52.64 and Comprehensiveness score of 50.54, surpassing Gemini-2.5-Pro-deepresearch (49.45 and 49.51, respectively). On the DeepResearch Gym, it gets the highest 100.0 score in the Depth, Breadth, and Insightfulness metrics. These gains stem directly from our reasoning-driven deepening. On one hand, the agent continuously extracts insights from condensed intermediate drafts, enabling deeper reasoning and synthesis. On the other hand, by revisiting intermediate outputs, it can identify missing topics and globally assess which sections require further expansion, resulting in broader and more balanced coverage. (2) The Multi-Stage Agentic Training brings stable and comprehensive improvement. The performance of AgentCPM-Report steadily improves from SFT to Atomic RL and finally to Pipeline RL across all metrics on these benchmarks. On DeepResearch Bench, the metric comprehensiveness rises from 46.24 to 50.54, Insight from 48.10 to 52.64, and Readability from 41.79 to 44.17. On DeepConsult, average score grows from 6.04 to 6.60, win rate increase from 54.17% to 57.60%, and loss rate drop from 35.54% to 28.68%. These consistent gains demonstrate that each stage of the curriculum contributes to mastering the full deep research workflow, yielding more stable and capable agent. (3) Small-scale agent systems can rival large-scale ones. Averaged across benchmarks, our deep research system demonstrates excellent performance. Our AgentCPM-Report (Pipeline RL) achieves an Overall score of 50.11 on DeepResearch Bench, surpassing Gemini-2.5-Pro-deepresearch (49.71). It also attains state-of-theart results on DeepResearch Gym, with an average score of 98.48. These results show that integrating WARP with multi-stage agentic training enables small models to reach the performance level of leading proprietary research systems. These findings suggest that, for deep research tasks, the primary bottleneck lies not in model size, but in the design of effective cognitive and planning processes that fully leverage models inherent capabilities. 6 AgentCPM-Report Table 2: Performance of agent systems on DeepResearch Bench in terms of comprehensiveness (Comp.), insight, instruction-following (Inst.), readability (Read.) and DeepConsult (Avg., Win, Tie, Lose)."
        },
        {
            "title": "DeepConsult",
            "content": "Overall Comp. Insight Inst. Read. Avg. Win Tie Lose Proprietary Deep Research Systems Doubao-research Claude-research OpenAI-deepresearch Gemini-2.5-Pro-deepresearch 44.34 45.00 46.45 49.71 Prompt-Based Frameworks WebWeaver (Qwen3-30B-A3B) WebWeaver (Claude-Sonnet-4) Enterprise DR (Gemini-2.5-Pro) RhinoInsigh (Gemini-2.5-Pro) Trained Open Models WebShaper-32B WebThinker-32B-DPO DR Tulu-8B Our Deep Research Systems AgentCPM-Report (SFT) AgentCPM-Report (Atomic RL) AgentCPM-Report (Pipeline RL) 46.77 50.58 49.86 50. 34.93 46.73 48.81 50.11 44.84 45.34 46.46 49.51 45.15 51.45 49.01 50.51 31.58 39.40 41.70 46.24 48.70 50. 40.56 42.79 43.73 49.45 45.78 50.02 50.28 51.45 26.17 35.40 41.80 48.10 51.36 52.64 47.95 47.58 49.39 50.12 49.21 50.81 50.03 51. 44.81 46.00 48.20 47.61 48.64 48.87 44.69 44.66 47.22 50.00 47.34 49.79 49.98 50.00 40.38 43.50 41.30 41.79 42.25 44. 5.42 4.60 5.00 6.70 4.57 6.96 6.82 6.82 1.63 6.04 6.06 6.60 29.95 25.00 0.00 61.27 28.65 66.86 71.57 68. 3.25 40.35 38.89 100.00 31.13 34.90 10.47 19.12 11.02 3.75 54.17 56.13 57.60 10.29 11.03 13. 29.70 36.11 0.00 7.60 36.46 22.67 9.31 20.47 93.00 35.54 32.84 28.68 Figure 4: Performance of agent systems on DeepResearch Gym."
        },
        {
            "title": "3.3 Analysis",
            "content": "3.3.1 Does WARP remain effective without training? To assess whether our framework is intrinsically effective without training, we conduct prompt-based comparison on DeepResearch Bench using larger model, Qwen3-235B-A22B-Instruct-2507 (Yang et al., 2025). We compare two policies: (1) Plan-then-write, where the model first constructs detailed outline through retrieval and then generates the report from this fixed plan; and (2) WARP, which starts from simple outline and interleaves writing with iterative deepening. Table 3: Evaluation for different generation paradigms."
        },
        {
            "title": "Paradigm",
            "content": "Overall Comp."
        },
        {
            "title": "Insight",
            "content": "Inst. Read. Plan-then-write WARP 49.90 50.72 49.35 50.33 51.60 52.79 50.13 46.46 50.32 47. As shown in Table 3, WARP consistently outperforms the Plan-then-write paradigm across all metrics, with notable gain in Insight (+1.19) and Comprehensiveness (+0.98). By using the evolving draft as reasoning context, WARP can detect underdeveloped or ambiguous content during writing and trigger targeted Deepening with additional evidence, whereas Plan-then-write remains constrained by static outline. This confirms that draft-aware deepening is key driver of insight. 7 AgentCPM-Report 3.3.2 How multi-stage training shapes agent actions and report structure? In this section, we analyze how agent behavior evolves across training stages by examining statistics of its actions and report sections. Specifically, we focus on the Write and Expand actions, which directly determine the report structure. Table 4: Evolution of action usage and hierarchical sectioning across training stages on DeepResearch Bench."
        },
        {
            "title": "Sections",
            "content": "Write Expand Level-1 Level-2 Level-3 21.24 SFT Atomic RL 36.89 Pipeline RL 39.51 4.44 8.88 8.63 6.27 6.49 6.52 10.11 14.17 15.75 4.86 16.50 17. Figure 5: The Expand steps on DeepResearch Bench. As shown in Table 4, clear behavioral shift emerges when moving from SFT to the RL-based training stages. The frequency of Expand (Deepening) actions nearly doubles (from 4.44 to around 8.8), which in turn leads to dramatic growth in fine-grained subsectioning (Level-3: from 4.86 to 17.32). Moreover, Figure 5 illustrates that RL training drives the agent to deepen more compared to SFT, ensuring at least 4 Expand steps in all cases. This trend indicates that RL training effectively equips the agent with Reasoning-Driven Deepening: Rather than adhering to shallow outline as in SFT, the agent learns to identify underdeveloped parts of draft and proactively expand them through iterative refinement, resulting in reports with substantially richer structure. 3.3.3 How does the number of deepening influence report quality? To examine whether the model has learned an appropriate stopping policy for deepening and to quantify how deepening depth affects report quality, we conduct \"Forced Expansion\" experiment. Specifically, during inference, we force AgentCPM-Report (Pipeline RL) to apply the Expand action exactly times, where ranges from 0 to 15. We then compare this forced expansion curve with the actual deepening behaviors of AgentCPM-Report at different training stages (SFT, Atomic RL, and Pipeline RL), overriding their learned termination policies. This allows us to directly evaluate report quality as function of deepening depth and to compare the models learned stopping behavior against the optimal expansion point. The results in Figure 6 reveal three consistent patterns. First, performance increases steadily with deeper expansion and begins to plateau at around nine steps, indicating diminishing returns beyond this depth. Second, both Comprehensiveness and Insight rise strongly with deepening, improving by nearly 6 points from shallow to sufficiently deep regimes, confirming the importance of iterative refinement for rich and insightful reports. Third, different training stages exhibit distinct stopping behaviors. The SFT agent typically stops within 6 steps and rarely reaches the saturation regime, whereas the Atomic RL and Pipeline RL agents shift their stopping distributions toward 615 steps, closely matching the empirically optimal depth. 8 AgentCPM-Report Figure 6: Mean Performance metrics per Deepen Steps on DeepResearch Bench. 3.3.4 How does the trajectory pruning affect the agent training? To address the optimal stopping problem, we introduce trajectory pruning strategy to construct higher-quality training data. In this section, we isolate its impact by training SFT models using the same teacher-generated trajectories, either with or without pruning. We consider two settings: (1) w/o pruning, which directly uses the raw trajectories produced by the teacher model, and (2) with pruning, which selects the best intermediate draft in one trajectory based on reward scores, retaining only the sub-trajectory before that draft. Table 5: Effect of trajectory pruning on SFT training. Trajectory Overall Comp. Insight Inst. Read. w/o pruning with pruning 45.80 46.73 44.95 46. 47.35 48.10 46.71 40.86 47.61 41.79 As shown in Table 5, models trained on pruned trajectories consistently outperform those trained on raw teacher ones across all evaluation dimensions. This indicates that trajectory pruning effectively improves the quality of supervision. More importantly, these results reveal key limitation of large teacher models: although they generate strong drafts, their termination decisions are often suboptimal. By applying reward-based selection over intermediate states, trajectory pruning filters out poorly timed stopping points and provides cleaner training signals. As result, the student model learns more accurate termination policy, which is crucial for effective Reasoning-Driven Deepening."
        },
        {
            "title": "4 Conclusion and Future Works",
            "content": "In this work, we address the limitations of existing deep research systems that rigidly separate planning from writing, an assumption that leads to an inherent insight ceiling and drives reliance on large, closed-source models. We propose AgentCPM-Report, fully local deep research system built upon WARP (Writing As Reasoning Policy), which reformulates deep research as policy-level iterative refinement process where planning decisions dynamically emerge from the writing itself. To support the long-horizon decision-making and expanded action space induced by WARP, we introduce multi-stage agentic training strategy that enables stable and efficient learning. Extensive experiments on multiple deep research benchmarks demonstrate substantial improvements in report qualityparticularly in insightshowing that, despite relying on only an 8B-parameter model, AgentCPM-Report surpasses several closed-source systems. Overall, this work establishes strong foundation for safe, privacy-preserving, and fully local deep research report generation and highlights policy design as viable alternative to model scaling for advancing deep research capabilities. Better report presentation. In most existing deep research systems, including ours, tables and figures are generated inline with paragraph-level text. However, constructing tabular layouts requires reasoning process fundamentally different from writing prose, placing heavy demands on models structural and formatting abilities. This coupling partly explains why agent systems based on smaller models often underperform large ones in presentation quality. promising direction is to decouple presentation from content generation and assign it to dedicated rendering agent, which could enable small models to achieve comparable or even 9 AgentCPM-Report superior layout quality. Moreover, current readability evaluation remains largely text-based and weakly reflects the true visual structure of rendered reports, suggesting the need for visual-modality evaluation in future work. More information sources. Our system relies on locally deployed textual knowledge base (e.g., arXiv abstracts and web summaries), which ensures stability and reproducibility but limits coverage and timeliness. It also lacks access to images, videos, domain-specific corpora, and personalized data. Future extensions will expand the knowledge base to support multi-modal content, local and personalized sources, and continuous updates, enabling richer and more realistic research scenarios."
        },
        {
            "title": "5 Contributions and Acknowledgments",
            "content": "AgentCPM-Report is the result of the collective efforts of all members of our team. Project Leads: Yishan Li, Wentong Chen Contributors: Yishan Li, Wentong Chen, Yukun Yan, Mingwei Li, Sen Mei, Xiaorong Wang, Kunpeng Liu, Xin Cong, Shuo Wang, Zhong Zhang, Yaxi Lu, Zhenghao Liu, Yankai Lin, Zhiyuan Liu, Maosong Sun Advisors: Yukun Yan, Yankai Lin, Zhiyuan Liu, Maosong Sun 10 AgentCPM-Report"
        },
        {
            "title": "References",
            "content": "Deepconsult: deep research benchmark for consulting/business queries. youdotcom-oss/ydc-deep-research-evals, 2025. GitHub repository. https://github.com/ ByteDance. Doubao deep research. https://www.doubao.com/chat/, 2025. Yuxuan Chen, Dewen Guo, Sen Mei, Xinze Li, Hao Chen, Yishan Li, Yixuan Wang, Chaoyue Tang, Ruobing Wang, Dingjun Wu, et al. Ultrarag: modular and automated toolkit for adaptive retrieval-augmented generation. arXiv preprint arXiv:2504.08761, 2025. Meet claude. Claude deep research. https://www.anthropic.com/claude, 2025. João Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie Callan, João Magalhães, Bruno Martins, et al. Deepresearchgym: free, transparent, and reproducible evaluation sandbox for deep research. arXiv preprint arXiv:2505.19253, 2025. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763, 2025. Google. Gemini deep research. https://gemini.google/overview/deep-research/, 2025. Chen Hu, Haikuo Du, Heng Wang, Lin Lin, Mingrui Chen, Peng Liu, Ruihang Miao, Tianchi Yue, Wang You, Wei Ji, et al. Step-deepresearch technical report. arXiv preprint arXiv:2512.20491, 2025. Kimi. Kimi-researcher: End-to-end rl training for emerging agentic capabilities. https://moonshotai. github.io/Kimi-Researcher/, 2025. Yu Lei, Shuzheng Si, Wei Wang, Yifei Wu, Gang Chen, Fanchao Qi, and Maosong Sun. Rhinoinsight: Improving deep research through control mechanisms for model behavior and context. arXiv preprint arXiv:2511.18743, 2025. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability, 2025a. URL https://arxiv.org/abs/2504.21776. Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, et al. Webweaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research. arXiv preprint arXiv:2509.13312, 2025b. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251, 2023. OpenAI. Deep research. https://openai.com/index/introducing-deep-research/, 2025. Perplexity. Perplexity introducing-perplexity-deep-research, 2025. deep research. https://www.perplexity.ai/hub/blog/ Akshara Prabhakar, Roshan Ram, Zixiang Chen, Silvio Savarese, Frank Wang, Caiming Xiong, Huan Wang, and Weiran Yao. Enterprise deep research: Steerable multi-agent deep research for enterprise analytics. arXiv preprint arXiv:2510.17797, 2025. Marlene Scardamalia and Carl Bereiter. Knowledge telling and knowledge transforming in written composition. Advances in applied psycholinguistics, 2:142175, 1987. Rulin Shao, Akari Asai, Shannon Zejiang Shen, Hamish Ivison, Varsha Kishore, Jingming Zhuo, Xinran Zhao, Molly Park, Samuel Finlayson, David Sontag, et al. Dr tulu: Reinforcement learning with evolving rubrics for deep research. arXiv preprint arXiv:2511.19399, 2025. Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. Assisting in writing wikipedia-like articles from scratch with large language models. arXiv preprint arXiv:2402.14207, 2024. 11 AgentCPM-Report Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061, 2025. MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, et al. Minicpm4: Ultra-efficient llms on end devices. arXiv preprint arXiv:2506.07900, 2025. Haoyu Wang, Yujia Fu, Zhu Zhang, Shuo Wang, Zirui Ren, Xiaorong Wang, Zhili Li, Chaoqun He, Bo An, Zhiyuan Liu, et al. Llm mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources. arXiv preprint arXiv:2504.05732, 2025. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, et al. Autosurvey: Large language models can automatically write surveys. Advances in neural information processing systems, 37:115119115145, 2024. x.AI. Grok 3 beta the age of reasoning agents, 2025. URL https://x.ai/news/grok-3. Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, and Huajun Chen. Omnithink: Expanding knowledge boundaries in machine writing through thinking. arXiv preprint arXiv:2501.09751, 2025. Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu Xia, Bin Wang, Bo Zhang, and Lei Bai. Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing. arXiv preprint arXiv:2503.04629, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 12 AgentCPM-Report"
        },
        {
            "title": "A Method Details",
            "content": "A.1 The Prompts in WARP Framework In our WARP framework, there are five actions in all three stages: Initialize, Search, Write, Expand(Deepen), and Terminate. In the Initialization stage, the agent generates the initial Level-1 outline with writing plans by the search and initialize actions. The prompt for the search is shown in Figure 8, and the prompt for the initialize is shown in Figure 7. Then, in the Evidence-Based Drafting stage, the agent writes the paragraphs by the search and write actions. The prompt for the write is shown in Figure 9. After that, in the ReasoningDriven Deepening stage, the agent will decision whether to expand section for more details by the expand action or to end the total process directly by the terminate action. The prompt for both actions is shown in Figure 10."
        },
        {
            "title": "Prompt for Search Keywords",
            "content": "You are professional report generation expert, skilled at creating high-quality report outlines. (cid:44) Now, you need to analyze the users question and provide simple article outline (cid:44) structure (only top-level sections). ** User Query ** [user query] ** Latest Retrieved Information ** [current information] ## Notes 1. The outline must be comprehensive, logically sound, and aligned with the users stated preferences and requirements. (cid:44) 2. The output language must match the language of the users query. ** Available Actions ** - initialize: Generate the top-level section outline along with an appropriate title. ## Action Format: <action> {\"name\": \"initialize\", \"title\": \"...\", \"sections\": [{\"title\": \"...\", \"plan\": (cid:44) \"...\"}, {\"title\": \"...\", \"plan\": \"...\"}, ...]} </action> ** Output Format ** <thought> Provide detailed reasoning process </thought> <action> Action (in JSON format) </action> Please output strictly according to the specified format. Figure 7: The LLM prompt for the initialize action."
        },
        {
            "title": "Prompt for Search Keywords",
            "content": "You are searcher within multi-agent system consisting of (cid:44) (cid:44) \"Analyst-Searcher-Writer\". You must perform retrieval based on instructions from the \"Analyst\". Carefully select the most accurate search keywords and strictly adhere to the specified output format. (cid:44) You should focus on the user's query and the current article outline to determine the most relevant keywords for searching. You can give one or less to five keywords. The content should be in the same language as the user's query. (cid:44) (cid:44) ** User Query ** 13 AgentCPM-Report [user query] ** Current Article Outline ** [current outline] ** Analyst's Instruction ** [current instruction] ## Action Example: <action> {\"name\": \"search\", \"keywords\": [keyword-1, keyword-2, ..]} </action> ** Output Format ** <thought> Your reasoning process </thought> <action> Action (in JSON format) </action> Please output strictly according to the specified format. Figure 8: The LLM prompt for the search action."
        },
        {
            "title": "Prompt for Search Keywords",
            "content": "You are writer operating within multi-agent system consisting of (cid:44) (cid:44) (cid:44) \"Analyzer-Searcher-Writer\". Based on instructions from the \"Analyzer\", the current writing status, and the most recently retrieved information, you are to compose new paragraph while ensuring logical coherence and accurate citation of facts. (cid:44) You should give paragraph with breadth and depth, ensuring it is informative and (cid:44) (cid:44) engaging. You are encouraged to incorporate examples, **tables**, code snippets, and other elements to enhance the content. But don't write other sections or chapters that are not assigned to you. (cid:44) You'd better give analytical and comparative content, not just summary of facts. (cid:44) Please attention the coherence and logical flow of the entire article and the other sections. (cid:44) You can extract the claims from the retrieved information, and design how to write (cid:44) the paragraph based on the claims in the thought process. (cid:44) **BE FAITHFUL! Make sure all your claims, especially the facts and the numbers, can be supported by the retrieved information, with your citations. Don't add any claims can't be supported by your citations. All the facts or data in your claims should can be found in the retrieved information you cited.** (cid:44) **You should ensure that the content you write is not redundant with other sections.** **And you should strictly follow the citation format like cite{bibkey} or (cid:44) cite{bibkey1, bibkey2..} for any referenced information. The content should be in the same language as the user's query.** (cid:44) (cid:44) PLEASE JUST OUTPUT THE CONTENT IN ANALYZER'S INSTRUCTION, DO NOT OUTPUT OTHER (cid:44) SECTIONS. THE OUTPUT SHOULD BE IN THE SAME LANGUAGE AS THE USER'S QUERY. ** User Query ** [user query] ** Current Article Summary ** [current survey] ** Analyzers Instruction ** 14 AgentCPM-Report [current instruction] ** Retrieved Information ** [current information] ## Action Example: <action> content </action> ** Output Format ** <thought> Your thought process </thought> <action> Your Content (in Markdown format) (include BIBKEY for citations within the (cid:44) content) </action> Please strictly follow the specified output format. Figure 9: The LLM prompt for the write action."
        },
        {
            "title": "Prompt for Search Keywords",
            "content": "You are professional report-generation expert skilled at crafting high-quality report outlines. (cid:44) Based on the the users stated preferences, you must now determine whether any (cid:44) section requires expansion into subsections. ## Important Notes: 1. Select only the single section or subsection most in need of expansion. 2. If no expansion is needed, output \"terminate\" (no operation) action. 3. If you think the expendsion is necessary to make the article more comprehensive or insightful, feel free to expand it. (cid:44) 4. Make sure the new subsections aren't redundant or overly detailed with other (cid:44) sections. If it's too detailed or redundant with other sections, just terminate it. (cid:44) 5. Make sure the new subsections are relevant and coherent with other sections. 6. You can only expand the section in 1 level and 2 level, do not expand the section in 3 level or more. (cid:44) 7. Please don't extend the section that is already extended. 8. Just extend one hierarchy level at time, the subsections you give should not have more than one hierarchy level. (cid:44) 8. The output language must match the language of the users query. ** User Query ** [user query] ** Current Full Report ** [current survey] ** Available Actions ** - extend-plan: Expand section by adding subsections (e.g., section-1 to section-1.1, section-1.2, section-1.3). (cid:44) - terminate: No operation. ## Action Format: <action> {\"name\": \"expand\", \"position\": \"section-x.y.z\", \"subsections\": [{\"title\": \"...\", \"plan\": \"...\"}, {\"title\": \"...\", \"plan\": \"...\"}, ...]} </action> (cid:44) <action> {\"name\": \"terminate\"} </action> ** Output Format ** 15 AgentCPM-Report <thought> Provide detailed reasoning process </thought> <action> Action (in JSON format) </action> Please output strictly according to the specified format. Figure 10: The LLM prompt for the expand and terminate action. A.2 User Query Construction We constructed dataset of approximately 2000 user queries with corresponding scoring checklists to support the multi-stage training process. Of these, around 700 queries are focused on specialized academic survey topics, while the remaining 1300 address general research reporting topics. For the academic survey queries, we employed reverse question construction approach: we first selected 700 surveys from ArXiv and then used large model to generate user questions based on these articles. The prompts used for this process are available in Figure 11. For general research reporting queries, we selected 1300 real questions. In addition, we constructed preference checklist inspired by DeepResearch Bench Du et al. (2025) for each user query. large model was used to generate weighted scores for different evaluation aspects based on the existing questions. The final report score for query is computed as weighted sum across these aspects. The method for checklist generation is same as DeepResearch Bench. Out of the 2000 queries, approximately 1500 were used to construct trajectory data for SFT and single-step RL, while the remaining 500 were directly used for end-to-end RL training."
        },
        {
            "title": "Prompt for Search Keywords",
            "content": "You are Instruction-writing expert. Below is Survey title. Your task is to infer (cid:44) the Instruction the user might have. Survey Title: {title} Before crafting the Query, analyze the following: 1. **Avoid using exact titles.** 2. Use domain-specific keywords, synonyms. First, output your analysis starting with \"Thought:\", simulating the Survey authors thought process. (cid:44) Then, generate one or more Queries based on the analysis, starting with \"Instruction:\". DONT OUTPUT EXPLANATIONS AFTER THE Instruction. (cid:44) Figure 11: The LLM prompt for reverse question (user query) construction. A.3 Retrieval Environment Setup We constructed local database containing approximately 2.86 million documents to serve as the agents retrieval environment. This database supports both trajectory data collection and interactive RL training. Among these documents, roughly 2.71 million are abstracts of papers from ArXiv, sourced via Kaggle1 . The remaining 150k documents come from general web pages, for which we employed Gemini 2.0-Flash to generate concise summaries while controlling for document length and quality. To ensure efficient retrieval, we built vector database. Specifically, all documents were vectorized using the embedding model MiniCPM-Embedding-Light 2 and indexed with Faiss3. The pipeline is implemented via UltraRAGChen et al. (2025), an open-source library for constructiong Retrieval-Augmented Generation (RAG) systems. 1https://www.kaggle.com/api/v1/datasets/download/Cornell-University/arxiv 2https://huggingface.co/openbmb/MiniCPM-Embedding-Light 3https://github.com/facebookresearch/faiss 16 AgentCPM-Report Table 6: The mapping between actions and the four agent abilities."
        },
        {
            "title": "Action Parameters",
            "content": "planning retrieval writing decision-making terminate {} initialize {\"title\": \"...\", \"sections\": [{\"title\": \"...\", \"plan\": \"...\"}, {\"title\": \"...\", \"plan\": \"...\",}, ...]} expand {\"position\": \"section-x.y.z\", \"content\": \"..\", \"subsections\": [{\"title\": \"..\", \"plan\": \"..\"}, ..]} search {\"keywords\": [keyword-1, keyword-2, ..,]} write {\"position\": \"section-x.y.z\", \"title\": \"...\", \"content\": \"...\"} A.4 Trajectory Data Construction Base on the user queries in A.2 and the retrieval environment in A.3, we collected 1,500 actual execution trajectories within our WARP framework. We chose Qwen3-235B-A22B-Instruct-2507 as the teacher model, and use the prompts in A.1. Despite their scale, current large language models still struggle with high-level decision-making and cannot reliably determine when to stop. To address this, we introduce an trajectory pruning strategy. The gathering process here slightly differs from the standard inference phase of the WARP framework: during each ReasoningDriven Deepening stage, we explicitly force the agent to select position for outline deepening, instead of allowing the model to autonomously decide whether to expand or terminate. This ensures that the agent continuously expands the outline and revises the report until maximum of 12 expansions is reached. All the results are scored by the survey-level reward mentioned in A.6.2, and the highest-scoring result is selected as the endpoint of the trajectory. For each user query, we collect single high-quality execution trajectory. In total, 1500 trajectories were obtained corresponding to the 1500 user queries. Among these, 1200 trajectories were used as SFT data for cold-start training, while the remaining 300 were reserved for atomic skill RL. A.5 Action Data Distribution For complete trajectory collected in Section A.4, it typically contains one initialize action, one termination action, several expand actions, and many search and write actions. These actions correspond to four core agent capabilities: planning, retrieval, writing, and decision-making. However, the natural distribution of actions is highly imbalanced with respect to training needs: the more critical and challenging abilitiessuch as planning and decision-makingare underrepresented, while easier-to-learn abilitiessuch as searching and writingdominate the trajectories. To address this issue, we introduce an action-level balanced sampling strategy that increases the sampling probability of more important and difficult actions and decreases that of easier ones, thereby providing more effective supervision for training the agents core capabilities. From the 1,500 collected trajectories, we obtained approximately 100k actions in total. When grouped by user query type, actions from academic review tasks and general report tasks followed an approximate ratio of 3:5. We used about 33k actions for cold-start training (SFT) and about 5k actions for atomic skill RL, with the remaining data discarded. A.6 Reward System In Reinforcement Learning (RL) training, the design of the reward functions is crucial, often directly impacting training efficiency and stability. In this section, we introduce our reward system from two aspects. First, in Section A.6.1, we introduce our different ability-specific reward functions for four abilities: planning, retrieval, writing, and decision-making, primarily used for single-step RL training. These functions are used for optimizing five actions: initialize, expand-plan, search, write, and terminate, as shown in Table 6. Second, in Section A.6.2, we introduce report-level reward design, which scores the overall quality of the generated report, primarily used for end-to-end RL training. A.6."
        },
        {
            "title": "Rewards for Action Optimization",
            "content": "In this section, we introduce our reward design for action optimization, aimed at improving the agents four core capabilities. For LLM as Judgement, the judgment model we use is Qwen2.5-72B-Instruct. 17 AgentCPM-Report (1) Planning capability. Both Initialize and Expand actions reflect the planning ability, and we calculate the rewards for these two actions by evaluating their generated one-level outlines (with detailed writing plans for each section). We use basic properties, holistic quality, and faithfulness to evaluate the results. Outline Basic Properties mainly evaluates whether the number of sections in the outline is reasonable, and judges the language consistency. We constrain the number of subsections in section to between 2 to 7. Meanwhile, we control the language consistency by character statistics. Outline Holistic Quality is evaluated by LLM for three aspects, following the setting of OmniThink (Xi et al., 2025). The scoring criteria are in Table 8, and the prompt Figure 12: Guidance for Content Generation Does the outline effectively guide content generation, ensuring comprehensive coverage of the topic? Hierarchical Clarity Does the outline clearly define hierarchy of topics and subtopics, with logical, diverse structure that is easy to understand? Logical Coherence Does the outline logically organize topics and subtopics, ensuring smooth and natural flow of ideas with clear logical transitions? Outline Faithfulness (Min et al., 2023) verifies whether the content in the plan is real and reliable. (2) Retrieval capbility. by evaluating its generated search keywords. We use recall score to evaluate them. Recall Score is calculated by the comparison of the retrieved documents and the golden ones. Search action reflects the retrieval ability, and we calculate the rewards for the action (3) Writing capability. The Write action reflects the writing ability, and we calculate the rewards for the action by evaluating its generated paragraphs. We use basic properties, holistic quality, faithfulness, and citation precision to evaluate them. Paragraph Basic Properties mainly evaluates whether the length and the number of citations of paragraph are reasonable, and judges the language consistency. We constrain the length of paragraph to between 100 to 2000 tokens. Then, we constrain the citation number of paragraph to between 0 to 12. Finally, we control the language consistency by character statistics. Paragraph Holistic Quality is evaluated by LLM for four aspects, according to STORM (Shao et al., 2024), with the prompt Figure 13 and the critria Table 9 : Relevance How effectively does the report maintain relevance and focus, given the dynamic nature of the discourse? Coverage Does the article provide an in-depth exploration of the topic and have good coverage? Depth How thoroughly does the report explore the initial topic and its related areas, reflecting the dynamic discourse? Novelty Does the report cover novel aspects that relate to the users initial intent but are not directly derived from it? Paragraph Faithfulness checks whether the fact claims are consistent with citations. Citation Precision is calculated by the comparison of the selected citations and the golden ones. We first check the citation hallucination phenomenon, any hallucination will make the score as 0. Then we employ the F1 Score of the citations between the generated content with the golden one. (4) Decision-making capbility. The terminate action reflects the agents high-level decision-making ability, and its reward is computed by evaluating the correctness of this decision. We use accuracy as the metric: if the agents decision matches the reference answer, the reward is 1.0; otherwise, it is 0.0. A.6."
        },
        {
            "title": "Rewards for Pipeline Optimization",
            "content": "Unlike the action-level RL phase, the process-level RL phase no longer has reference answer to constrain the agents exploration, and the training data only contains user questions. We will directly evaluate the final result of the entire process (the generated report) from several aspects, instead of scoring the intermediate actions. We hope that in this phase, the agent can explore more freely, generate more diverse results, and ultimately surpass the capabilities of the teacher model. This section will detail our evaluation of the final result from four aspects: comprehensiveness, insight, instruction-following, and readability. The judgment model we use is Qwen3-32B (Yang et al., 2025). 18 AgentCPM-Report Table 7: The training settings for different stages."
        },
        {
            "title": "Parameters",
            "content": "SFT Single-Step RL End-to-End RL user queries train samples learning rate batch size rollout number train epochs train steps 1,200 33,292 1.5e-5 32 4 300 5150 2.5e-6 8 8 200 500 500 1e-6 8"
        },
        {
            "title": "B Experiments Details",
            "content": "B.1 Training Details We adopt three-stage training pipeline consisting of cold start training (SFT), atomic skill RL (single-step RL), and holistic pipeline RL (end-to-end RL). All our experiments are run on 8 A100 GPUs, and the training settings are shown in Table 7. Cold-Start Training For cold-start, we collect approximately 33k action-level samples from 1,200 trajectories. The model is trained using SFT with learning rate of 1.5e-5 and batch size as 32 for 4 epochs, taking about 2 days to complete. Atomic Skill RL We further perform single-step RL using approximately 5150 action-level samples from 300 trajectories. We set the learning rate to 2.5e-6, batch size to 8, rollout number to 8, and train for 200 optimization steps, taking about 2 days to complete. Holistic Pipeline RL Finally, we conduct end-to-end RL on 500 user queries, optimizing the entire report generation pipeline jointly. The learning rate remains 1e-6, with batch size of 8, rollout number of 4, and total of 50 training steps, taking about 4 days to complete. B.2 Metrics Details We conducted evaluations on three benchmarks: DeepResearch Bench Du et al. (2025), DeepConsult Dee (2025), and DeepResearch Gym Coelho et al. (2025). DeepResearch Bench It consists of 100 PhD-level research tasks spanning 22 academic domains. It adopts the RACE and FACT evaluation frameworks. RACE assesses Comprehensiveness, Insight/Depth, Instruction Following, and Readability, while FACT evaluates effective citations per report and citation reliability. We evaluate it by Gemini-2.5-Pro. Deep Consult It includes 102 queries from business and consulting scenarios. Evaluation is conducted via pairwise comparisons against an OpenAI-DeepSearch baseline, reporting win, tie, and loss rates, together with average quality scores on instruction following, comprehensiveness, completeness, and writing quality. We evaluate it by o3-mini-2025-01-31. DeepResearch Gym It is built on the Researchy Questions dataset. Following WebWeaver (Li et al., 2025b), we sample 100 queries from the top 1,000 test queries and evaluate the report quality in six aspects: clarity, depth, balance, breadth, support, and insightfulness. We evaluate it by GPT-4.1-mini-20250414."
        },
        {
            "title": "Prompt for Search Keywords",
            "content": "###Task Description: 19 AgentCPM-Report"
        },
        {
            "title": "Rating",
            "content": "Score 1 Score 2 Score 3 Score 4 Score 5 Score Score 2 Score 3 Score 4 Score 5 Score 1 Score Score 3 Score 4 Score"
        },
        {
            "title": "Guide",
            "content": "The outline fails to guide content generation, omitting significant aspects of the topic or providing insufficient direction. The outline provides limited guidance, covering some key areas but lacking depth or completeness in addressing the topic. The outline provides moderate guidance for content generation, addressing most key areas but leaving some gaps or ambiguities. The outline effectively guides content generation, covering all significant aspects with clear direction, though minor refinements could enhance comprehensiveness. The outline is exemplary in guiding content generation, thoroughly addressing all aspects of the topic with clear, detailed direction and no significant gaps."
        },
        {
            "title": "Hierarchical",
            "content": "The outline exhibits no discernible hierarchical structure. Topics and subtopics are jumbled together without logical separation or clear levels, making it nearly impossible to follow or identify any organization. The outline attempts to establish hierarchy but fails to maintain logical consistency. Main topics and subtopics are frequently misclassified, and the structure is overly rigid or disjointed. Subtopics may be missing, misplaced, or redundant, making it hard to grasp the intent of the structure. The outline demonstrates basic level of logical coherence. Most topics follow general sequence, but some sections feel forced, with weak or unclear transitions. There are small jumps in logic, causing slight confusion or loss of flow at certain points. The outline displays clear, logical, and diverse hierarchical structure. Main topics are distinct, and subtopics are properly nested. While most elements are well-placed, there may be minor redundancies or opportunities to introduce more diverse formats for subtopics. Slight adjustments could achieve better precision and variety in style. The outline showcases an exceptional, flawless hierarchical structure. Each main topic is distinct, and subtopics are logically nested with absolute clarity and stylistic diversity. The outline demonstrates flexibility in structure and organization, adapting its style where appropriate for the content and logic. No further refinement is necessary."
        },
        {
            "title": "Coherence",
            "content": "The outline is highly disjointed and incoherent. Topics and subtopics appear in random, unordered manner, with no logical flow or sense of progression. Major conceptual gaps and illogical jumps are present throughout the structure. The outline shows some attempt at logical organization, but it contains frequent inconsistencies, abrupt shifts, or logical missteps. Topics and subtopics are misaligned or lack proper transitions, making the reader work hard to follow the structure. The outline demonstrates basic level of logical coherence. Most topics follow general sequence, but some sections feel forced, with weak or unclear transitions. There are small jumps in logic, causing slight confusion or loss of flow at certain points. The outline exhibits strong sense of logical flow, with ideas presented in mostly smooth and connected manner. Transitions between topics and subtopics are clear, but few minor adjustments could make the flow more seamless or natural. The logic is sound, but room for refinement exists. The outline achieves exceptional logical coherence. Each topic and subtopic follows deliberate, thoughtful progression, with clear, natural, and intuitive transitions. The reader experiences seamless flow of ideas, and no adjustments are required to improve logical consistency or flow. Table 8: The plan scoring criteria rating scale 1-5. An instruction (might include an Input inside it), response to evaluate, (cid:44) reference answer that gets score of 5, and score rubric representing evaluation criteria are given. (cid:44) 1. Identify the major and minor errors in this Response. Write detailed list of the (cid:44) errors in the response strictly based on the given score rubric, not evaluating in general. (cid:44) 2. After writing the list of errors, write score that is an integer between 1 and 5. (cid:44) You should refer to the score rubric. 20 AgentCPM-Report"
        },
        {
            "title": "Rating",
            "content": "Score 1 Score 2 Score 3 Score 4 Score 5 Score 1 Score Score 3 Score 4 Score 5 Score 1 Score 2 Score 3 Score 4 Score 5 score 1 score 2 score 3 score 4 score"
        },
        {
            "title": "Relevance",
            "content": "Very poor focus; discourse diverges significantly from the initial topic and intent with many irrelevant detours. Poor focus; some relevant information, but many sections diverge from the initial topic. Moderate focus; mostly stays on topic with occasional digressions that still provide useful information. Good focus; maintains relevance and focus throughout the discourse with minor divergences that add value. Excellent focus; consistently relevant and focused discourse, even when exploring divergent but highly pertinent aspects."
        },
        {
            "title": "Coverage",
            "content": "Severely lacking; offers little to no coverage of the topics primary aspects, resulting in very narrow perspective. Partial coverage; includes some of the topics main aspects but misses others, resulting in an incomplete portrayal Acceptable breadth; covers most main aspects, though it may stray into minor unnecessary details or overlook some relevant points. Good coverage; achieves broad coverage of the topic, hitting on all major points with minimal extraneous information. Exemplary in breadth; delivers outstanding coverage, thoroughly detailing all crucial aspects of the topic without including irrelevant information."
        },
        {
            "title": "Depth",
            "content": "Very superficial; provides only basic overview with significant gaps in exploration. Superficial; offers some detail but leaves many important aspects unexplored. Moderate depth; covers key aspects but may lack detailed exploration in some areas. Good depth; explores most aspects in detail with minor gaps. Excellent depth; thoroughly explores all relevant aspects with comprehensive detail, reflecting deep and dynamic discourse."
        },
        {
            "title": "Novelty",
            "content": "Lacks novelty; the report strictly follows the users initial intent with no additional insights. Minimal novelty; includes few new aspects but they are not significantly related to the initial intent. Moderate novelty; introduces some new aspects that are somewhat related to the initial intent. Good novelty; covers several new aspects that enhance the understanding of the initial intent. Excellent novelty; introduces numerous new aspects that are highly relevant and significantly enrich the initial intent. Table 9: The content scoring criteria rating scale 1-5. 3. The output format should look as follows: \"(write the list of errors for criteria) [RESULT] (an integer number between 1 and 5)\" (cid:44) 4. Please do not generate any other opening, closing, and explanations. 5. Please be fair, don't hesitate to give low score like 1 or 2. 6. Note that Major errors refer to actual errors that affects the task severely, may (cid:44) (cid:44) change the meaning of the output, and Minor errors refer to smaller imperfections, and purely subjective opinions about the output. ###The instruction to evaluate: {instruction} ###Response to evaluate: {response} ###Reference Answer (Score 5): {reference_answer} ###Score Rubrics: 21 AgentCPM-Report {rubric} ###Feedback: Figure 12: The Outline quality reward prompt template."
        },
        {
            "title": "Prompt for Search Keywords",
            "content": "Here is an academic survey about the topic \"[TOPIC]\": --- [SURVEY] --- <instruction> Please evaluate this survey about the topic \"[TOPIC]\" based on the criterion provided below, identify the major and minor errors in this survey, and give score from 1 to 5 according to the score description: (cid:44) (cid:44) --- Criterion Description: [Criterion Description] --- Score 1 Description: [Score 1 Description] Score 2 Description: [Score 2 Description] Score 3 Description: [Score 3 Description] Score 4 Description: [Score 4 Description] Score 5 Description: [Score 5 Description] --- Note that Major errors refer to actual errors that affect the task severely, may (cid:44) change the meaning of the output, and Minor errors refer to smaller imperfections, and purely subjective opinions about the output. (cid:44) There may be multiple errors or no errors in the output. After listing the errors, then, please score the survey with 1 to 5. Return the score without any other information at the end of the output. Figure 13: The Content quality reward prompt template."
        }
    ],
    "affiliations": [
        "OpenBMB"
    ]
}