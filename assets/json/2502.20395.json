{
    "paper_title": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts",
    "authors": [
        "Zhongyang Li",
        "Ziyue Li",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs)' powerful reasoning capabilities, deterring LMMs' performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with a mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose a novel and efficient method \"Re-Routing in Test-Time(R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in a neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighbor-search spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs' performance on challenging benchmarks of diverse tasks, without training any base-model parameters."
        },
        {
            "title": "Start",
            "content": "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Zhongyang Li 1 Ziyue Li 2 Tianyi Zhou 2 1Johns Hopkins University; 2University of Maryland, College Park zli300@jh.edu, {litzy619,tianyi}@umd.edu Project: https://github.com/tianyi-lab/R2-T2 5 2 0 2 7 2 ] . [ 1 5 9 3 0 2 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In large multimodal models (LMMs), the perception of non-language modalities (e.g., visual representations) is usually not on par with the large language models (LLMs) powerful reasoning capabilities, deterring LMMs performance on challenging downstream tasks. This weakness has been recently mitigated by replacing the vision encoder with mixture-of-experts (MoE), which provides rich, multi-granularity, and diverse representations required by diverse downstream tasks. The performance of multimodal MoE largely depends on its router, which reweights and mixes the representations of different experts for each input. However, we find that the end-to-end trained router does not always produce the optimal routing weights for every test sample. To bridge the gap, we propose novel and efficient method ReRouting in Test-Time (R2-T2) that locally optimizes the vector of routing weights in test-time by moving it toward those vectors of the correctly predicted samples in neighborhood of the test sample. We propose three R2-T2 strategies with different optimization objectives and neighborsearch spaces. R2-T2 consistently and greatly improves state-of-the-art LMMs performance on challenging benchmarks of diverse tasks, without training any base-model parameters."
        },
        {
            "title": "1 Introduction",
            "content": "Mixture-of-Experts (MoE) have achieved remarkable success in scaling up the size and capacity of large language and multimodal models (LLMs and LMMs) (Shazeer et al., 2017) without (significantly) increasing the inference cost. Specifically, it allows us to increase the total number of experts, which provides finer-grained expertise and skills, yet selecting constant number of experts for each input (Lepikhin et al., 2020). In MoE, the sparse selection of experts is achieved through router, which determines 1 Figure 1. R2-T2 applied to MoAI-7B compared against 7/8/13B VLMs on 9 benchmarks. R2-T2 significantly enhances performance of the 7B base MoE model, surpassing recent 13B VLM. the weight of each candidate expert based on the input so only experts with nonzero weights are selected (Fedus et al., 2022). MoE then aggregates the outputs of the selected experts according to their weights. Hence, the router and its produced routing weights play important roles in MoEs inference cost and output quality. As the most widely studied LMM, many vision language models (VLM) adopt an architecture composed of vision encoder and an LLM (Zhu et al., 2023), which are both pre-trained and then aligned by further finetuning so the LLM can include the vision encoders output in its input as additional tokens. The alignment is usually obtained through lightweight projection layer or Q-former (a Transformer model) converting the vision encoders output to LLM tokens. Despite the broad usage of this architecture, the capability of vision encoder is usually much more limited than the LLMs (i.e., the modality imbalance) (Schrodi et al., 2024). So the visual features cannot cover all the information required by different reasoning tasks performed by LLMs. Moreover, the alignment module may lead to R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Figure 2. An example of how R2-T2 optimizes the routing weights. Given the test sample, it finds kNN in the reference set of correctly predicted samples with similar questions. In the example, the test sample requires reasoning about positional relationships. R2-T2 identifies relevant kNN samples, adjusting the top-1 expert from ILANG (aligning visual features with language) to IAUX (aligning visual features with auxiliary computer vision features). This expert shift is crucial in correcting the initial wrong answer. an information bottleneck from the visual perception to the reasoning (Yao et al., 2024). paper, we investigate how to improve the routing weights in test-time without training any model parameters. Recent advances in LMMs replace single vision encoder with mixture of encoders (Lin et al., 2024; Lee et al., 2025; Zong et al., 2024; Shi et al., 2024), which turns out to be an effective and low-cost approach to mitigate modality imbalance and alignment bottleneck. In multimodal MoE, each expert is an encoder or mixer of sensory inputs that focuses on specific type of features, e.g., object classes, text in images, spatial relations, dense captions, segmentation, etc., so the LLM can select the information acquired by any given downstream task from the concatenated or fused features from the MoE, through router that is trained in an end-to-end manner to produce the weights of all the experts adaptive to the input task. Although multimodal MoE achieves remarkable success in enhancing the performance of existing LMMs, the choice of experts or the routing weights for individual instances are not always optimal due to the limitations of the routers design and the diversity of potential downstream tasks compared to the tasks used to train the router. The suboptimality of routing substantially limits the performance and generalization of multimodal MoE on unseen tasks. As illustrated in Figure 2, the base model initially selects sub-optimal expert (e.g., ILANG) for spatial reasoning task, leading to incorrect predictions. This has been verified on recent multimodal MoE models. As shown in Table 2, compared to the original routing weights of base models, the optimal (oracle) routing weights improve the accuracy by 10% on most evaluated LMM benchmarks. To avoid the expensive cost of re-training router on much larger dataset, in this Since routing weights encode the choices of experts with essential knowledge and key skills acquired by the input task, and motivated by the assumption that knowledge and skills are usually transferable across different tasks, we posit that the routing weights of successful tasks can provide critical clues for optimizing the routing weights of new task. Specifically, we leverage the similarity in task embedding space, which may reflect the knowledge or skill sharing between tasks, and modify the routing weight vector of test task by imitating its nearby successful tasks. While the task embedding space, optimization objective, and the number of update steps can vary and their design choices may result in different performances, this innovative mechanism of optimizing routing weights or re-routing in test-time (R2-T2) focuses on correcting the mistakes made by the routers in existing multimodal MoE, e.g., extracting object detection features for task mainly depending on the text information in an input image, and thus turns various failed cases into success. Rather than finetuning the whole model, R2-T2 is training-free and aims to maximize the potential of MoE in the reasoning tasks by LMMs. Following the above idea, we explored several novel strategies for test-time routing weight optimization. They all modify the routing weights of test task/sample based on representative set of tasks/samples on which the multimodal MoE achieves correct or high-quality outputs. While the oracle routing weights are achieved by minimizing the test samples loss, for practical approach, we propose to replace the oracle loss with surrogate, i.e., weighted 2 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts average of losses of nearby reference samples, and apply multiple steps of neighborhood gradient descent (NGD) to minimize the surrogate. In addition, we investigate kernel regression and mode finding, which do not require gradient descent. The former moves the routing weights to kernelweighted sum of nearby reference tasks routing weights in task embedding space, while the latter moves the routing weights to the nearest mode on the distribution of reference tasks routing weights. Evaluating these strategies on two recent multimodal MoE models across eight challenging benchmarks, we find that R2-T2 significantly outperforms models twice its size, as shown in Figure 1. Our analysis reveals that NGD progressively refines routing, increasing correct predictions while mitigating the original routers over-reliance on single expert. Case studies confirm that test-time re-routing enhances domain-specific reasoning, demonstrating R2-T2s ability to adapt multimodal MoE models without additional training, unlocking greater generalization and robustness. Our main contributions can be summarized below: We proposed novel problem of R2-T2 that bridges significant performance gap on multimodal MoE. We developed three practical R2-T2 strategies that shed several critical insights into expert re-routing. Our R2-T2 considerably advances the performance of multimodal MoE on several recent benchmarks of challenging tasks for LMMs."
        },
        {
            "title": "2 Related Work",
            "content": "Large Multimodel Models has emerged as powerful paradigm for integrating language and non-language modalities, such as images (Radford et al., 2021), audio (Ao et al., 2021), and video (Zellers et al., 2021), to perform complex reasoning tasks. Recent advancements have been driven by the fusion of pretrained LLMs with multimodal encoders (Peng et al., 2023; Tsimpoukelli et al., 2021; Alayrac et al., 2022), enabling the models to process and generate cross-modal content effectively. Works such as Flamingo (Alayrac et al., 2022) and BLIP-2 (Li et al., 2023a) demonstrated the potential of aligning vision and language modalities through carefully designed bridging modules. However, these models often fall short in richness or alignment with the reasoning capabilities of LLMs (Bubeck et al., 2023; Bommasani et al., 2021). To address this, techniques have been proposed, such as contrastive pretraining (Radford et al., 2021; Yuan et al., 2021) and feature fusion mechanisms (Lu et al., 2019). Yet, efficiently capturing diverse modal interactions across different tasks remains bottleneck (Baltrušaitis et al., 2018), highlighting the need for more adaptive mechanisms in multimodal reasoning. Mixture-of-Experts has become prominent architec3 tural choice to enhance the scalability and efficiency of large-scale neural networks (Shazeer et al., 2017). By dynamically selecting subset of specialized expert modules for each input (Li et al., 2023b), MoE reduces computational overhead while maintaining high expressive In the power (Shazeer et al., 2017; Zoph et al., 2022). context of LLMs, MoE has been shown to improve both training efficiency and generalization across tasks (Artetxe & Schwenk, 2019). Works such as Switch Transformers (Fedus et al., 2022) and GShard (Lepikhin et al., 2020) have demonstrated the effectiveness of MoE in scaling up model capacity without prohibitive increases in training costs. In multimodal settings, MoE has been explored to address the modality alignment problem (Goyal et al., 2021), where different experts handle distinct modalities or specific tasks. However, the optimal utilization of experts heavily relies on the effectiveness of routing mechanisms, which remains an active area of research. Routers and Routing Strategies are the cornerstone of any MoE-based architecture, responsible for determining which experts are activated for each input (Li & Zhou, 2024). Traditional routers, such as softmax gating functions (Shazeer et al., 2017), compute weighted combination of experts based on input embeddings. Despite their simplicity, these routing strategies often face challenges in achieving optimal expert assignment (Lepikhin et al., 2020; Zoph et al., 2022), particularly in unseen or highly diverse test scenarios. Recent works have proposed advanced routing strategies, including routing via reinforcement learning (Rosenbaum et al., 2017), early-exit (Li et al., 2023c), and task-specific allocation (Shi et al., 2024). However, these approaches typically focus on training-time optimization, leaving test-time adaptability largely unexplored. R2-T2 introduces an efficient method to refine routing weights dynamically during inference, ensuring better alignment with task-specific requirements and improving overall model robustness across diverse multimodal benchmarks. Test-Time Optimization has been explored by adapting models dynamically during inference to improve generalization. For example, (Wang et al., 2022) propose test-time adaptation, which fine-tunes model parameters on test data distributions using entropy minimization or self-supervised learning. Similarly, (Sun et al., 2020) introduce test-time training, where models are updated via auxiliary tasks (e.g., rotation prediction) during inference. However, these methods require modifying the base models parameters, leading to significant computational overhead and potential instability when deployed on resource-constrained systems. Unlike prior test-time optimization methods that update model weights, R2-T2 solely optimizes the routing weights of frozen MoE model without retraining any model parameters. R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Figure 3. Illustration of R2-T2 test-time re-routing mechanism with three strategies. (a) Neighborhood Gradient Descent: Optimizes using gradients derived from neighbors loss functions (rl1, rl2, and rl3 for the 3 nearest neighbors), weighted by their similarity to the test sample. (b) Kernel Regression: Estimates as weighted average of neighbors routing weights (ˆr), and further optimizes it through binary search between ˆr and initial weights to find the optimal coefficient α. (c) Mode Finding: Iteratively updates through weighted interpolation between currecnt weights and the local average in routing weight space, shifting towards the densest region."
        },
        {
            "title": "3 Test-Time Re-Routing",
            "content": "3.1 Gradient Descent MoE trains router to reweight experts for each input. However, such an end-to-end trained router may not always produce optimal weights for challenging or out-of-distribution samples at test-time, whereas sub-optimal weights can drastically degrade the performance of MoE on diverse downstream tasks. The importance of routing weights has been broadly demonstrated on eight benchmarks in our experiments: The large performance gap between the base model (using the routers routing weights) and the oracle (using the optimal routing weights) in Table 2 implies the potential merits of optimizing the routing weights in the test-time. To address this problem, Test-Time Re-Routing (R2-T2) introduces dynamic test-time re-routing mechanism that adapts the routing weights for each test sample based on similar samples in reference seta set of samples on which the MoEs outputs are correct or preferred. Specifically, given reference set of samples {(xi, yi)}n i=1 and their corresponding routing weights {ri}n i=1, on which the model makes correct prediction (i.e., (xi, ri) = yi), for new test sample x, the goal of R2-T2 is to find better routing weight vector for that leads to more accurate and higher-quality output (x, r). In the following, we will introduce three core strategies, illustrated in Figure 3, to optimize based on the neighbors of in the reference set, i.e., (x), according to similarity metric. These strategies are developed with different optimization objectives (e.g., loss surrogate, regression, mode finetuning, etc.) and neighbor-search spaces (e.g., routing weights, task embedding, etc.). While the first is gradientbased, the other two are gradient-free, offering more flexible options for different setups and computational budgets. The gradient descent method uses the gradient of an objective function L(r) to update for multiple steps until convergence or when certain stopping criteria have been fulfilled. In every step, we apply λrL(r), (1) where λ is learning rate determined by scheduler. We discuss the two choices of L(r) in the following. Oracle (upper bound) assumes that we know the ground truth label for x, which is cheating setting that can provide an upper bound of the gradient descent method. In this setting, L(r) = ℓ[f (x, r), y], (2) where ℓ[, ] is the loss function (e.g., cross-entropy or L2 loss) measuring the discrepancy between the model output (x, r) and the ground truth y. Although this is not applicable in real scenarios, it serves as performance ceiling to reveal the degradation caused by sub-optimal routing weights and evaluate the effectiveness of other methods. Neighborhood Gradient Descent (NGD) is practical approach that uses the loss functions of the nearest neighbors of in the reference set to estimate the gradient of r, i.e., L(r) = (cid:80) iN (x) K(xi, x) ℓ[f (xi, r), yi] iN (x) K(xi, x) (cid:80) (3) By incorporating loss information from the neighborhood of x, NGD enables label-free, test-time adaptation mechanism. This effectively aligns with the successful routing patterns in the reference set. This ensures that exploits the routing for relevant reference examples without requiring access to the oracle loss. 4 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts 3.2 Kernel Regression either kNN or ϵ-ball, i.e., Kernel regression predicts by the weighted average of the neighbors routing weights {ri}iN (x), i.e., ˆr (cid:80) iN (x) K(xi, x) ri (cid:80) iN (x) K(xi, x) , (4) where K(, ) is kernel function, e.g., Gaussian kernel, Matern kernel, etc. In the experiments, we found that directly setting ˆr already brings non-trivial improvement. However, ˆr does not take the router-produced initial into account and may not fully capture the nuanced dependencies required for optimal performance. To further optimize r, we conduct binary search on the straight line between and ˆr: αr + (1 α)ˆr. (5) The search goal is to find the optimal α minimizing the objective L(r), i.e., α arg min α L(αr + (1 α)ˆr). (6) This refinement step balances the kernel regression estimate with the routers original routing weights. It includes ˆr as special case (when α = 0) and can further enhance the accuracy and robustness of the models predictions. 3.3 Mode Finding (Meanshift) Mode finding aims to move towards the highest density region of the distribution p(r) for the reference routing weights {ri}n i=1. It applies the following update for multiple steps until convergence. αr + (1 α)r, (7) where α controls the step size and the weighted average routing weights defined below (different from ˆr). (cid:80) iN (r) K(ri, r) ri (cid:80) iN (r) K(ri, r) . (8) Unlike kernel regression, mode finding identifies the densest region in the routing weight space (so the kernel K(, ) and neighborhood () are applied to instead of x), representing the most consistent configurations among nearby reference samples. This makes it effective for capturing the dominating patterns in the local distribution of routing weights. 3.4 Neighborhood and Embedding Space Neighborhood The choices of neighborhood definition and the embedding space in which to apply the kernels are important to the final performance. For the former, we can use (x) arg min A2n,Ak (cid:88) iA d(xi, x), (x) {i [n] : d(xi, x) ϵ}, (9) (10) Embedding Instead of directly applying an existing kernel function K(, ) and distance metric d(, ) to the raw inputs xi and x, we can replace and xi with their embedding E(x) and E(xi), where E() is pre-trained embedding model applied to the task description of each sample. Table 1. Summary of reference and evaluation benchmarks. If the reference dataset contains more than 5,000 samples, we randomly select 5,000 to ensure balanced evaluation. Task Type General Visual Understanding KnowledgeBased Reasoning Size Evaluation 5,000 MMBench Reference VQA-V2 Visual7W 5,000 MME-P COCO-QA 5,000 CVBench2D/3D CLEVR 5,000 GQA A-OKVQA 5,000 TQA MathVista 5,000 AI2D 5,000 SQA-IMG PhysBench Optical Character Recognition ST-VQA DocVQA 5,000 5,000 TextVQA Size 2,374 2,114 2,638 1,590 2,017 3,087 2,093 5,"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setting Models We evaluate two multimodal MoE models: MoAI (Lee et al., 2025) and MoVA (Zong et al., 2024), each leveraging specialized experts for vision-language tasks. MoAI has six experts: (1) Visual Experts process auxiliary CV features (IAUX), align visuals with language (ILANG), and capture spatial relationships (ISELF); (2) Language Experts integrate external knowledge (LAUX), link language to visuals (LIMG), and maintain coherence (LSELF). Further details about MoAI experts are provided in Appendix A. MoVA includes seven experts, incorporating SAM (Zou et al., 2024) to enhance the vision encoder with specialized knowledge. Reference datasets and evaluation benchmarks Our evaluation covers three task categories: general visual understanding, knowledge-based reasoning, and optical character recognition. Table 1 summarizes the reference datasets and evaluation benchmarks, including their dataset sizes. See Appendix for details. Evaluations We adopt standard evaluation protocols for each benchmark. For MME-P, performance is assessed using two metrics:(1) Accuracy, measuring the correctness of single question per image, and (2) Accuracy+, requiring both questions per image to be answered correctly. The final score is the sum of these two metrics, with maximum of 2,000 (Fu et al., 2024). For other benchmarks, accuracy is the primary metric (Yin et al., 5 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Table 2. Comparison of three R2-T2 methods (kNN with = 5) applied to MoVA and MoAI (base models), with Accuracy (%) reported1. Oracle has access to the ground truths and provides an upper bound. NGD significantly improves base models and performs the best. Method MMBench MME-P SQA-IMG AI2D TextVQA GQA CVBench2D CVBench3D PhysBench MoVA (base model) Mode Finding Kernel Regression NGD Oracle (upper bound) MoAI (base model) Mode Finding Kernel Regression NGD Oracle (upper bound) 74.3 75.2 77.9 81.2 87. 79.3 80.8 83.7 85.2 92.1 1579.2 1587.1 1610.6 1645.3 1735. 1714.0 1725.2 1756.7 1785.5 1860.2 74.4 74.9 76.4 79.1 87. 83.5 84.1 86.2 88.3 93.8 74.9 75.8 78.5 81.8 88. 78.6 79.8 82.6 85.0 91.2 76.4 77.3 79.9 83.2 89. 67.8 66.5 71.2 73.5 79.6 64.8 65.7 68.3 71.5 76. 70.2 71.4 74.5 77.0 83.2 61.6 62.5 65.2 68.3 72. 71.2 70.0 74.6 77.9 84.0 62.3 63.2 65.9 68.9 73. 59.3 60.1 64.5 69.2 76.8 32.6 33.5 35.7 37.8 47. 39.1 40.2 42.8 44.7 54.5 1 2023). We compute the mean score across benchmarks #benchmark (Stotal + Smmp-e) , where Stotal is the sum of all as benchmark scores except MME-P, and Smmp-e is the normalized MME-P score. Baselines R2-T2 introduces test-time re-routing, problem not addressed in prior work. To assess its effectiveness, we compare it against multiple R2-T2 variants and base models. Additionally, we benchmark R2-T2 against state-of-the-art VLMs across scales, as shown in Table 3. We use fixed hyperparameters across all benchmarks without per-task tuning, determined via experiments on smallscale benchmarks independent of our evaluation datasets. See Appendix for details. 4.2 Main Results Comparsion of different R2-T2 methods Tables 2 summarizes the performance of R2-T2 methods on the MoVA and MoAI models across eight benchmarks. Among all evaluated methods, kNN Neighborhood Gradient Descent (NGD) emerges as the most effective, delivering significant improvements over the pretrained base models. For MoAI7B, R2-T2 enhances performance significantly, achieving +6.9% on MMBench, +66.1-point increase on MME-P, and +6.8% gain on TextVQA. Similarly, on MoVA-7B, it yields notable improvements of +5.9% on MMBench, +71.5 points on MME-P, and +5.7% on TextVQA. These consistent gains across diverse benchmarks highlight the ability of R2-T2 to optimize routing weights effectively, enabling better utilization of expert modules for improved model performance. Notably, kNN NGD achieves results close to the Oracle upper bound, which relies on ground truth labels during test-time and is thus infeasible in practice. Our method, without accessing labels, captures 7080% of the potential improvement, demonstrating its effectiveness. Comparison with state-of-the-art VLMs In Table 3, we compare our approach with state-of-the-art VLMs of various sizes (7B, 8B, 13B, 34B) across benchmarks. When 1Except MME-Ps score, the sum of two accuracy metrics. applied to the pretrained MoVA-7Bwhich initially lags behind larger modelsR2-T2 achieves substantial performance gains and outperforms 7/8/13/34 competitors across most benchmarks through effective test-time re-routing. In addition, applying R2-T2 to MoAI-7B results in significant performance boost, establishing it as highly competitive against larger models. Notably, for PhysBench, which contains both video and image tests, our results reflect only the image-only evaluation. R2-T2(MoAI-7B) ranks second in the image-only leaderboard of PhysBench. These results highlight the effectiveness of R2-T2 in unlocking the potential of smaller models, enabling them to match or even surpass the performance of significantly larger VLMs. Inference efficiency trade-off While R2-T2 introduces additional operations beyond the base models inference pipeline, it achieves near-oracle performance with moderate computational overhead  (Table 4)  . To ensure hardwareindependent comparison, we measure computational costs in FLOPs. The base model requires 9.9T FLOPs per case. Mode finding adds only 1.8T FLOPs, leading to 1.5% accuracy gain. Kernel regression and R2-T2 require 67 more FLOPs due to loss computations over five neighbors, yet R2T2 (kNN, NGD) achieves the highest accuracy improvement (+5.9%) while maintaining competitive efficiency. 4.3 Ablation Study We analyze how each component contributes to the performance and robustness of kNN NGD, with all studies conducted on MoAI. Results are averaged across 8 test benchmarks detailed in Section 4.1, with individual results and further analysis provided in Appendix D.1. Neighborhood selection compare two strategies: ϵ-ball (radius ϵ = 0.2 to 0.8) and kNN (k = 3 to 20), as shown in Table 5. The results demonstrate that kNN with = 5 consistently achieves better performance across most tasks, outperforming both smaller neighborhoods that may lack sufficient context and larger ones that could introduce noise. While ϵ-ball shows stable performance across different radius, it suffers from inherent limitations: fixed radius 6 Table 3. Comparison of R2-T2 (kNN, NGD) with state-of-the-art vision-language models on nine benchmarks (higher the better). R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts VLM 7B Models InstructBLIP-7B (Dai et al., 2023) Qwen-VL-7B (Bai et al., 2023) Qwen-VL-Chat-7B (Bai et al., 2023) mPLUG-Owl-7B (Ye et al., 2023) mPLUG-Owl2-7B (Ye et al., 2024) ShareGPT4V-7B (Chen et al., 2025) 8B Models Mini-Gemini-HD-8B (Li et al., 2024) LLaVA-NeXT-8B (Liu et al., 2024) Cambrian1-8B (Tong et al., 2024) 13B Models BLIP2-13B (Li et al., 2023a) InstructBLIP-13B (Dai et al., 2023) Mini-Gemini-HD-13B (Li et al., 2024) LLaVA-NeXT-13B (Liu et al., 2024) Cambrian1-13B (Tong et al., 2024) 34B Models Mini-Gemini-HD-34B (Li et al., 2024) LLaVA-NeXT-34B (Liu et al., 2024) Cambrian1-34B (Tong et al., 2024) Ours MoVA-7B R2-T2 (MoVA-7B) MoAI-7B R2-T2 (MoAI-7B) MMBench MME-P SQA-IMG AI2D TextVQA GQA CVBench2D CVBench3D PhysBench 36.0 38.2 60.6 46.6 64.5 68.8 72.7 72.1 75. 28.8 39.1 68.6 70.0 75.7 80.6 79.3 81.4 74.3 81.2 79.3 85.2 - - 1488.0 967.0 1450.0 1567.4 1606.0 1603.7 1647. 1294.0 1213.0 1597.0 1575.0 1610.4 1659.0 1633.2 1689.3 1579.2 1645.3 1714 1785.5 60.5 67.1 68.2 - 68.7 68.4 75.1 72.8 74. 61.0 63.1 71.9 73.5 79.3 77.7 81.8 85.6 74.4 79.1 83.5 88.3 - 62.3 57.7 - - 67.3 73.5 71.6 73. - - 70.1 70.0 73.6 80.5 74.9 79.7 74.9 81.8 78.6 85.0 50.1 63.8 61.5 - 58.2 65.8 70.2 64.6 68. 42.5 50.7 70.2 67.1 72.8 74.1 69.5 76.7 76.4 83.2 67.8 73.5 56.7 59.4 - 58.9 62.9 63.4 64.5 65.2 64. - - 63.7 65.4 64.3 65.8 67.1 65.8 64.8 71.5 70.2 77.0 - - - - - 60.2 62.2 62.2 72. - - 53.6 62.7 72.5 71.5 73.0 74.0 61.6 68.3 71.2 77.9 - - - - - 57.5 63.0 65.3 65. - - 67.3 65.7 71.8 79.2 74.8 79.7 62.3 68.9 59.3 69.2 23.8 - 35.6 - - 31.3 34.7 - 24. 38.6 29.9 - 40.5 - - - 30.2 32.6 37.8 39.1 44.7 Table 4. FLOPs of different methods (kNN with = 5) on MMBench using MoAI-7B as the base model. Method Inference steps FLOPs (T) per case Accuracy (%) Base Model (MoAI-7B) Mode Finding Kernel Regression R2-T2 (kNN, NGD) Oracle (upper bound) 1 10 10 10 10 9.9 10.7 61.9 67.5 11. 79.3 80.8 83.7 85.2 89.8 Table 5. Ablation study of R2-T2 (kNN, NGD) with different choices of neighborhood on MoAI. ϵ-ball Parameter Avg. kNN Parameter Avg. ϵ = 0.2 ϵ = 0.4 ϵ = 0.6 ϵ = 0. 76.5 77.9 78.9 77.7 = 3 = 5 = 10 = 20 78.6 80.7 79.4 76.6 threshold may yield too few neighbors in sparse regions or excessive neighbors in dense regions, leading to inconsistent performance. The kNN approach provides more reliable and generally superior results. This suggests that maintaining fixed number of neighbors not only ensures consistent computational cost but also provides sufficient information for effective test-time re-routing. Kernel choice is critical for determining how similarity is modeled in high-dimensional spaces, which directly affects gradient updates in NGD. In Table 6, we compare four different kernel functions. The results consistently show that the Gaussian kernel outperforms other kernel functions across all tasks, with up to 4.4% accuracy improvement over the linear kernel. Its superior performance may due to its ability to effectively capture similarity relationships in high-dimensional embedding spaces while being less affected by the curse of dimensionality (Cristianini, 2000). Embedding model directly impacts the neighborhood quality, which in turn influences the gradient updates. In Table 7, we compare four embedding models. The results show that NV-Embed-V2 achieves consistent improvements of 3.2% over Sentence-Bert, indicating its ability to provide more discriminative feature representations that better capture semantic relationships between samples. Gradient descent steps significantly affect both convergence and performance. Experiments with 5, 10, 20, and 50 steps assess the trade-off between cost and accuracy. As seen in Table 8, increasing the step count from 5 to 10 significantly improves performance (76.6 80.7), indicating that more iterations enhance optimization. Beyond 10 steps, performance saturates (80.5 at 20 steps, 80.7 at 50), suggesting diminishing returns. Thus, 10 steps offer the best balance between performance and efficiency. 4.4 Case Studies Accuracy Transition Analysis Figure 5 illustrates the transition of predictions as NGD progresses over ten steps. During Step 0 to Step 4, 17.22% of incorrect predictions are corrected, and by Step 10, total of 28.12% of incorrect predictions have been converted to correct ones. Meanwhile, only 2.31% correct predictions become incorrect throughR2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Table 6. Ablation study of R2-T2 (kNN, NGD) with kernel choices on MoAl. Table 7. Ablation study of R2-T2 (kNN, NGD) with embedding models on MoAI. Table 8. Ablation study of R2-T2 (kNN, NGD) with NGD steps on MoAI. Kernel Linear (Cortes, 1995) Polynomial (Cortes, 1995) Matern (Williams & Rasmussen, 2006) Gaussian (Williams & Rasmussen, 2006) Avg. 76.3 77.7 78.7 80.7 Embedding Model Sentence-Bert (Reimers, 2019) Stella-En-1.5B-V5 (Kusupati et al., 2022) Gte-Qwen2-7B (Li et al., 2023c) NV-Embed-V2 (Lee et al., 2024) Avg. 77.5 78.5 78.7 80.7 #Step Avg. 5 10 20 50 76.6 80.7 80.5 80.7 Figure 4. Top-1 expert transitions to correct/incorrect preditions on CVBench2D/3D after re-routing. The primary transitions to correct predictions in (a) include ILANG to LIMG, LAUX and LAUX. The primary transitions to incorrect predictions in (b) include ILANG to IAUX, LIMG and LAUX. R2-T2 considerably mitigates the mobility imbalance of the base model. versifies expert selection. Furthermore, transition patterns differ between correctly and incorrectly predicted samples. In correct cases (Figure 4 (a)), re-routing typically shifts ILANG to LIMG. In incorrect cases (Figure 4 (b)), transitions often involve ILANG to LAUX. This may indicate occasional mismatches in the re-weighted routing. Crucially, the number of cases shifting from correct to incorrect is significantly lower than those transitioning from incorrect to correct. The overall improvements outweigh potential misclassifications, validating R2-T2 as an effective optimization strategy. Example Case: Spatial Reasoning Improvement Figure 2 demonstrates how R2-T2 rectifies spatial reasoning failure. The test question asks, where is the chair located with respect to the tennis racket?. Initially, the model selects ILANG (language-aligned visual expert), which prioritizes textual alignment but fails to capture positional relationships. R2T2 addresses this by retrieving nearest neighbors from the reference set with similar spatial queries. By dynamically adjusting routing weights, R2-T2 elevates IAUX to the top-1 position. IAUX integrates features from open-world object detection (Lee et al., 2025; Minderer et al., 2023), enabling more precise interpretation of spatial layouts. Additional transition pattern cases and details are provided in Appendix D.2 and for further insights."
        },
        {
            "title": "5 Conclusions",
            "content": "We introduce R2-T2, novel test-time re-routing method that enhances multimodal Mixture-of-Experts (MoE) modFigure 5. Transition between correct and incorrect predictions on CVBench2D/3D during NGD steps of R2-T2 from Step 0 to 10. NGD keeps turning more incorrect predictions to correct. out the optimization process. As the optimization converges in later steps, the routing weight changes become smaller, reducing the number of prediction shifts. Expert Shift Patterns Figure 4 illustrates top-1 expert transitions before and after re-routing, where (a) shows transitions leading to correct predictions and (b) those leading to incorrect predictions. The original router over-relies on ILANG, limiting model adaptability. After re-routing, many samples shift from ILANG to LIMG, IAUX, and LAUX, leading to improved accuracy. This indicates that the pretrained router excessively favors ILANG, preventing optimal expert utilization. Notably, samples that were initially correct before re-routing exhibited more balanced expert distribution, whereas those initially incorrect depended heavily on ILANG. After re-routing, expert distributions in both cases become more balanced, showing that R2-T2 effectively di8 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts els without additional training. By dynamically adjusting routing weights based on reference samples, R2-T2 corrects suboptimal expert selection, improving model generalization. We propose and evaluate three strategiesNeighborhood Gradient Descent, Kernel Regression, and Mode Findingdemonstrating their effectiveness across multiple multimodal benchmarks. R2-T2 consistently outperforms the base MoE model and rivals oracle-based optimization methods, highlighting the potential of test-time adaptation for more efficient and adaptive expert utilization."
        },
        {
            "title": "References",
            "content": "Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: visual language model for fewshot learning. Advances in neural information processing systems, 35:2371623736, 2022. Ao, J., Wang, R., Zhou, L., Wang, C., Ren, S., Wu, Y., Liu, S., Ko, T., Li, Q., Zhang, Y., et al. Speecht5: Unifiedmodal encoder-decoder pre-training for spoken language processing. arXiv preprint arXiv:2110.07205, 2021. Artetxe, M. and Schwenk, H. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. Transactions of the association for computational linguistics, 7:597610, 2019. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2): 3, 2023. Baltrušaitis, T., Ahuja, C., and Morency, L.-P. Multimodal machine learning: survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence, 41 (2):423443, 2018. Biten, A. F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar, C., and Karatzas, D. Scene text visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4291 4301, 2019. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pp. 370387. Springer, 2025. Cheng, B., Misra, I., Schwing, A. G., Kirillov, A., and Girdhar, R. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12901299, 2022. Chow, W., Mao, J., Li, B., Seita, D., Guizilini, V., and Wang, Y. Physbench: Benchmarking and enhancing visionlanguage models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. Cortes, C. Support-vector networks. Machine Learning, 1995. Cristianini, N. An introduction to support vector machines and other kernel-based learning methods. Cambridge University Press, 2000. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S. Instructblip: Towards generalpurpose vision-language models with instruction tuning. arxiv 2023. arXiv preprint arXiv:2305.06500, 2, 2023. Du, Y., Li, C., Guo, R., Cui, C., Liu, W., Zhou, J., Lu, B., Yang, Y., Liu, Q., Hu, X., et al. Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144, 2021. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., Wu, Y., and Ji, R. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. URL https: //arxiv.org/abs/2306.13394. Goyal, A., Didolkar, A., Lamb, A., Badola, K., Ke, N. R., Rahaman, N., Binas, J., Blundell, C., Mozer, M., and Bengio, Y. Coordination among neural modules through shared global workspace. arXiv preprint arXiv:2103.01197, 2021. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Hudson, D. A. and Manning, C. D. Gqa: new dataset for real-world visual reasoning and compositional question 9 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6700 6709, 2019. Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 29012910, 2017. Kembhavi, A., Salvato, M., Kolve, E., Seo, M., Hajishirzi, H., and Farhadi, A. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 235251. Springer, 2016. Kembhavi, A., Seo, M., Schwenk, D., Choi, J., Farhadi, A., and Hajishirzi, H. Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pp. 4999 5007, 2017. Kusupati, A., Bhatt, G., Rege, A., Wallingford, M., Sinha, A., Ramanujan, V., Howard-Snyder, W., Chen, K., Kakade, S., Jain, P., et al. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:3023330249, 2022. Li, Z., Ren, K., Jiang, X., Shen, Y., Zhang, H., and Li, D. Simple: Specialized model-sample matching for domain generalization. In The Eleventh International Conference on Learning Representations, 2023b. Li, Z., Ren, K., Yang, Y., Jiang, X., Yang, Y., and Li, D. Towards inference efficient deep ensemble learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 87118719, 2023c. Lin, X. V., Shrivastava, A., Luo, L., Iyer, S., Lewis, M., Ghosh, G., Zettlemoyer, L., and Aghajanyan, A. Moma: Efficient early-fusion pre-training with arXiv preprint mixture of modality-aware experts. arXiv:2407.21770, 2024. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., and Lee, Y. J. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2025. Lu, J., Yang, J., Batra, D., and Parikh, D. Hierarchical question-image co-attention for visual question answering. Advances in neural information processing systems, 29, 2016. Lee, B.-K., Park, B., Won Kim, C., and Man Ro, Y. Moai: Mixture of all intelligence for large language and vision models. In European Conference on Computer Vision, pp. 273302. Springer, 2025. Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019. Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023a. Li, Y., Zhang, Y., Wang, C., Zhong, Z., Chen, Y., Chu, R., Liu, S., and Jia, J. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. Li, Z. and Zhou, T. Your mixture-of-experts llm is secretly an embedding model for free. arXiv preprint arXiv:2410.10814, 2024. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu, S.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Mathew, M., Karatzas, D., and Jawahar, C. Docvqa: In Proceedings dataset for vqa on document images. of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. Minderer, M., Gritsenko, A., and Houlsby, N. Scaling openvocabulary object detection. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7298373007. Curran Associates, Inc., 2023. 10 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Minderer, M., Gritsenko, A., and Houlsby, N. Scaling open-vocabulary object detection. Advances in Neural Information Processing Systems, 36, 2024. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., and Wei, F. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Tong, S., Brown, E., Wu, P., Woo, S., Middepogu, M., Akula, S. C., Yang, J., Yang, S., Iyer, A., Pan, X., et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. Tsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S., Vinyals, O., and Hill, F. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200212, 2021. Wang, Q., Fink, O., Van Gool, L., and Dai, D. Continual testtime domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 72017211, 2022. Reimers, N. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. Williams, C. K. and Rasmussen, C. E. Gaussian processes for machine learning, volume 2. MIT press Cambridge, MA, 2006. Rosenbaum, C., Klinger, T., and Riemer, M. Routing networks: Adaptive selection of non-linear functions for multi-task learning. arXiv preprint arXiv:1711.01239, 2017. Wu, H., Zhang, Z., Zhang, E., Chen, C., Liao, L., Wang, A., Li, C., Sun, W., Yan, Q., Zhai, G., et al. Q-bench: benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023. Schrodi, S., Hoffmann, D. T., Argus, M., Fischer, V., and Brox, T. Two effects, one trigger: On the modality gap, object bias, and information imbalance in contrastive vision-language representation learning. arXiv preprint arXiv:2404.07983, 2024. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., and Mottaghi, R. A-okvqa: benchmark for visual quesIn European tion answering using world knowledge. conference on computer vision, pp. 146162. Springer, 2022. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Shi, M., Liu, F., Wang, S., Liao, S., Radhakrishnan, S., Huang, D.-A., Yin, H., Sapra, K., Yacoob, Y., Shi, H., et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D., and Rohrbach, M. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A. A., and Hardt, M. Test-time training with self-supervision for generalization under distribution shifts, 2020. URL https://arxiv.org/abs/1909.13231. Yang, J., Ang, Y. Z., Guo, Z., Zhou, K., Zhang, W., and Liu, Z. Panoptic scene graph generation. In European Conference on Computer Vision, pp. 178196. Springer, 2022. Yao, L., Li, L., Ren, S., Wang, L., Liu, Y., Sun, X., and Hou, L. Deco: Decoupling token compression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985, 2024. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Ye, Q., Xu, H., Ye, J., Yan, M., Hu, A., Liu, H., Qian, Q., Zhang, J., and Huang, F. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1304013051, 2024. Yin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen, E. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. Yuan, X., Lin, Z., Kuen, J., Zhang, J., Wang, Y., Maire, M., Kale, A., and Faieta, B. Multimodal contrastive training for visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 69957004, 2021. Zellers, R., Lu, X., Hessel, J., Yu, Y., Park, J. S., Cao, J., Farhadi, A., and Choi, Y. Merlot: Multimodal neural 11 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts script knowledge models. Advances in neural information processing systems, 34:2363423651, 2021. Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Zhu, Y., Groth, O., Bernstein, M., and Fei-Fei, L. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 49955004, 2016. Zong, Z., Ma, B., Shen, D., Song, G., Shao, H., Jiang, D., Li, H., and Liu, Y. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024. Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and Fedus, W. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906, 2(3): 17, 2022. Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Wang, J., Wang, L., Gao, J., and Lee, Y. J. Segment everything everywhere all at once. Advances in Neural Information Processing Systems, 36, 2024. 12 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts"
        },
        {
            "title": "A MoAI Experts",
            "content": "MoAI integrates specialized computer vision models and expert modules to achieve comprehensive scene understanding: External CV Models: Four computer vision models provide complementary capabilities: (1) panoptic segmentation (Cheng et al., 2022) for object identification and localization, (2) open-world object detection (Minderer et al., 2024) for recognizing diverse objects beyond predefined categories, (3) scene graph generation (Yang et al., 2022) for understanding object relationships, and (4) optical character recognition (OCR) (Du et al., 2021) for text understanding. These models provide auxiliary information that enhances MoAIs visual perception. Cross-Modal Capabilities: The expert modules are designed to facilitate effective cross-modal interactions: Visual Experts: IAUX connects visual features with structured CV outputs through cross-attention, ILANG aligns visual representations with language semantics, while ISELF maintains spatial awareness through self-attention. Language Experts: LAUX integrates verbalized CV outputs with language understanding, LIMG grounds language in visual context, and LSELF ensures coherent text generation. The combination of specialized CV models and cross-modal experts enables MoAI to bridge the gap between detailed visual perception and high-level language understanding. This architecture is particularly effective for tasks requiring both fine-grained visual analysis and natural language reasoning."
        },
        {
            "title": "B Evaluation Benchmarks and Reference Datasets",
            "content": "We conduct evaluations using diverse set of reference datasets and task-specific benchmarks. For general visual understanding, we use four reference datasets: VQA-V2 (Goyal et al., 2017), Visual7W (Zhu et al., 2016), CLEVR (Johnson et al., 2017), and COCO-QA (Lu et al., 2016). For knowledge-based reasoning, which requires leveraging external knowledge, we include A-OKVQA (Schwenk et al., 2022), TQA (Kembhavi et al., 2017) and MathVista (Lu et al., 2023). For optical character recognition (OCR), we employ ST-VQA (Biten et al., 2019), DocVQA (Mathew et al., 2021). To ensure balanced evaluation, we randomly sample 5,000 instances from datasets exceeding this size. Correspondingly, we evaluate on task-specific benchmarks. For general visual understanding, these include MMBench (Liu et al., 2025), MME-P (Fu et al., 2024), CVBench2D/3D (Tong et al., 2024), and GQA (Hudson & Manning, 2019). For knowledge-based reasoning, we evaluate on SQA-IMG (Lu et al., 2022) AI2D (Kembhavi et al., 2016) and PhysBench (Chow et al., 2025). TextVQA (Singh et al., 2019) is evaluated for OCR. Reference Datasets VQA-V2 (Goyal et al., 2017): Focuses on open-ended visual question answering, requiring models to answer questions about images. Tasks include object recognition, attribute identification, and scene understanding. Contains 1.1M questions across 200K+ COCO images, with balanced annotations to reduce language bias. Visual7W (Zhu et al., 2016) Specializes in 7-type visual QA (what, where, when, who, why, how, and which), emphasizing grounding answers in image regions (e.g., Where is the cat? with bounding box annotations). It includes 327K QA pairs, challenging models on spatial reasoning and causal explanations. CLEVR (Johnson et al., 2017): synthetic benchmark for compositional visual reasoning. Tasks involve counting objects, comparing attributes, and logical operations (e.g., Are there more red cubes than blue spheres?). Contains 100K rendered 3D images and 853K questions, designed to test systematic generalization. COCO-QA (Lu et al., 2016): Automatically generates QA pairs from COCO image captions for basic visual understanding. Questions fall into four categories: object, number, color, and location (e.g., What color is the car?). Includes 117K QA pairs, serving as lightweight evaluation for object-centric reasoning. A-OKVQA (Schwenk et al., 2022): Requires commonsense and external knowledge for visual QA (e.g., Why is the person wearing helmet?). Distinguishes between direct perception (What is this?) and knowledge-augmented reasoning. Contains 25K questions with crowdsourced explanations. 13 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts TQA (Kembhavi et al., 2017): multimodal machine comprehension dataset designed to test reasoning over middle school science curricula. It contains 1,076 lessons with 26,260 questions, combining text, diagrams, and images. Questions require parsing complex scientific concepts and reasoning across multiple modalities, making it more challenging than traditional QA datasets. The dataset is split into training, validation, and test sets, with no content overlap, ensuring robust evaluation of models ability to integrate and reason over multimodal information. MathVista (Lu et al., 2023): multimodal math reasoning benchmark combining visual understanding (diagrams/plots) and textual problem-solving. Contains 6,141 problems testing abilities like geometric reasoning, equation parsing, and chart interpretation. Highlights the stark gap between human performance (91.6% on text-only tasks) and state-of-the-art AI models (58.9%), particularly in visual-textual integration and multi-step reasoning. ST-VQA (Biten et al., 2019): Evaluates scene text understanding in visual QA. Questions require reading text in images (e.g., What is the store name?). Includes 23K questions across diverse scenarios (signboards, documents, etc.), with strict answer normalization. DocVQA (Mathew et al., 2021): Focuses on document image understanding. Tasks include extracting information from tables, forms, and invoices (e.g., What is the invoice number?). Contains 50K questions on 12K document images, testing OCR and layout understanding. Evaluation Benchmarks MMBench (Liu et al., 2025): comprehensive benchmark for multimodal understanding and generation. Tasks span image captioning, visual entailment, and fine-grained attribute QA. Includes 2,374 pairs with hierarchical evaluation dimensions (perception, reasoning, knowledge). MME-P (Fu et al., 2024): Evaluates multimodal event understanding through paired questions (e.g., before/after event prediction). Contains 2,114 pairs covering temporal, causal, and counterfactual reasoning in video/text contexts. CVBench 2D/3D (Tong et al., 2024): unified benchmark for 2D and 3D vision tasks. 2D tasks include depth estimation and object detection (1,438 pairs), while 3D tasks focus on point cloud registration and mesh reconstruction (1,200 pairs). GQA (Hudson & Manning, 2019): Tests compositional reasoning over real-world images. Questions use functional programs (e.g., Select then compare) to ensure compositional validity. Includes 1,590 pairs with explicit scene graph grounding for error analysis. SQA-IMG (Lu et al., 2022): science QA benchmark with diagrammatic reasoning. Questions combine textbook diagrams and textual context (e.g., Which process is shown in the diagram?). Contains 2,017 pairs spanning biology, physics, and chemistry. AI2D (Kembhavi et al., 2016): Focuses on diagram interpretation for K-12 science. Tasks include diagram labeling, relation extraction, and multi-step inference (e.g., What happens after step 3?). Contains 3,087 pairs with annotated diagram primitives (arrows, labels). TextVQA (Singh et al., 2019): Requires text-aware visual QA (e.g., answering What brand? from text in images). Contains 5,734 pairs with focus on OCR-VQA integration, using real-world images with scene text. PhysBench (Chow et al., 2025): Requires physical world understanding (e.g., reasoning about object properties and dynamics). Contains 10,002 video-image-text entries(2093 image-only entries) evaluating VLMs on physical properties, relationships, scenes, and dynamics understanding."
        },
        {
            "title": "C Hyperparameter Choices",
            "content": "To ensure robust and fair evaluation, we use fixed set of hyperparameters across all benchmarks. This approach maintains consistency, prevents task-specific optimizations, and allows for an unbiased comparison of performance. The selected hyperparameters are as follows: cosine annealing schedule with learning rate ranging from 1 102 to 1 105, neighborhood selection is performed using kNN with = 5, the number of NGD steps is fixed at 10, the Gaussian kernel is used for kernel-based methods, and NV-Embed-V2 is adopted as the embedding model. These values are applied uniformly across all evaluated tasks. 14 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Hyperparameter Selection Strategy Rather than tuning hyperparameters separately for each benchmark, we determined these values through controlled experiments on Qbench (Wu et al., 2023) that do not overlap with our evaluation benchmarks. This ensures that hyperparameter selection is independent of the test sets, minimizing the risk of overfitting while maintaining general applicability. Additionally, our ablation studies (Section 4.3) confirm the effectiveness of these choices. Variations in key hyperparameters, such as NGD steps and neighborhood size, show that our selected values strike balance between performance and efficiency, supporting their suitability across diverse benchmarks."
        },
        {
            "title": "D Additional Analysis",
            "content": "D.1 Ablation Study We perform an ablation study to assess the impact of key hyperparameters on R2-T2s performance. Table 9 evaluates different learning rate schedules for Gradient Descent, comparing cosine annealing, step decay, and fixed schedules. Full results for the ablation studies discussed in Section 4.3 are presented in Tables 10-13. Comparison of different learning rate In Table 9, we investigate how different learning rate schedules affect the performance of Gradient Descent. We compare cosine annealing schedule against two fixed (1e-3 and 1e-4) and step decay schedule. The cosine annealing schedule consistently outperforms all baseline approaches across all benchmarks, achieving improvements of up to 12.7 percentage points over the fixed learning rate (1e-3) baseline. These findings suggest that carefully designed learning rate schedules are essential for maximizing the potential of R2-T2. Schedule Fixed (1e-3) Fixed (1e-4) Step Decay Cosine Table 9. Ablation study of R2-T2 (kNN, NGD) with different learning rate schedules. MMBench MME-P SQA-IMG AI2D TextVQA GQA CVBench2D CVBench3D 71.8 75.2 82.9 85.2 1671.2 1692.5 1745.4 1785.5 74.3 77.8 84.2 88.3 70.9 74.5 81.8 85.0 60.4 63.9 70.5 73.5 63.1 66.5 73.8 77. 66.8 69.9 73.5 77.9 57.2 63.3 67.2 69.2 Table 10. Ablation study of R2-T2 (kNN, NGD) with different choices of neighborhood on MoAI. Neighbors Parameter MMBench MME-P SQA-IMG AI2D TextVQA GQA CVBench2D CVBench3D ϵ-ball kNN ϵ = 0.2 ϵ = 0.4 ϵ = 0.6 ϵ = 0.8 = 3 = 5 = 10 = 20 82.4 83.9 85.4 83.7 83.2 85.2 84.0 80.7 1733.9 1758.4 1778.8 1756. 1740.9 1785.5 1761.3 1693.6 84.8 86.0 87.2 85.9 86.1 88.3 86.8 83.6 81.3 83.0 83.8 82.5 83.1 85.0 83.5 80.7 69.9 71.5 72.4 71. 71.3 73.5 72.8 70.5 73.1 74.8 75.9 74.5 75.1 77.0 75.3 73.2 67.1 68.5 69.6 68.3 75.8 77.9 76.6 73.9 66.5 67.3 68.0 67. 67.4 69.2 68.1 65.7 Table 11. Ablation study of R2-T2 (kNN, NGD) with different choices of kernels on MoAI. Kernel MMBench MME-P SQA-IMG AI2D TextVQA GQA CVBench2D CVBench3D Linear Polynomial Matern Gaussian 82.1 83.2 83.9 85. 1722.3 1745.5 1752.8 1785.5 84.2 85.1 85.8 88.3 80.8 81.9 82.5 85.0 69.5 70.4 71.2 73.5 72.8 73.9 74.6 77.0 72.7 74.5 76.3 77. 62.1 65.2 67.8 69.2 15 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Table 12. Ablation study of R2-T2 (kNN, NGD) with different embedding models on MoAI. Embedding Model MMBench MME-P SQA-IMG AI2D TextVQA GQA CVBench2D CVBench3D Sentence-Bert Stella-En-1.5B-V5 Gte-Qwen2-7B-instruct NV-Embed-V2 82.8 83.6 84.0 85.2 1748.2 1752.5 1757.0 1785.5 84.2 85.4 86.0 88.3 80.3 82.1 82.7 85.0 70.2 70.8 71.3 73. 73.8 74.3 74.8 77.0 75.6 76.3 76.1 77.9 66.0 67.5 67.0 69.2 Table 13. Ablation study of R2-T2 (kNN, NGD) with different number of NGD steps. #Step MMBench MME-P SQA-IMG AI2D TextVQA GQA CVBench2D CVBench3D 5 7 10 (ours) 20 50 81.3 83.8 85.2 85.0 85.3 1705.8 1745.2 1785.5 1777.8 1792.0 84.2 86.5 88.3 88.5 88.2 80.9 83.2 85.0 84.6 84.8 69.2 71.8 73.5 73.7 73. 73.5 75.2 77.0 76.8 77.1 72.2 76.0 77.9 77.7 77.6 66.1 67.6 69.2 69.0 69.3 16 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts D.2 Case study Case Study: Transition from ILANG to LAUX Figures 7 and 8 illustrate cases where the initial routing incorrectly prioritizes ILANG, which aligns visual features with language but lacks object-specific recognition capabilities. This results in misidentifications: in the first case, the model misinterprets the plane number, yielding 728FW instead of the correct 728TFW; in the second case, it incorrectly predicts FRENCH as the license plates state instead of the correct California. To correct these errors, R2-T2 retrieves three highly relevant reference samples using kNN based on question similarity. Each reference set contains samples with similar question structures, providing more suitable routing adjustment. After incorporating insights from these references, the routing shifts towards LAUX, which enhances object-specific recognition and scene understanding. This re-routing process enables the model to produce the correct answers 728TFW and California, demonstrating the effectiveness of R2-T2 in dynamically refining expert selection. Case Study: Transition from ILANG to IAUX We show one case for this transition in Figure 2 and analyze in Section 4.4. Figure 6 illustrates another case where the initial routing incorrectly prioritizes ILANG, which aligns visual features with language but lacks object-specific recognition capabilities. As result, the model miscounts the number of hats in the image, selecting answer (C) 2 instead of the correct (D) 1. To correct this, R2-T2 retrieves three highly relevant reference samples using kNN based on question similarity. These samples contain similar counting-related queries, allowing for more effective routing adjustment. After integrating insights from these references, the routing shifts towards IAUX, which specializes in fine-grained object recognition. This re-routing enables the model to correctly identify and count the hats, selecting the correct answer (D) 1. This case demonstrates the ability of R2-T2 to refine expert selection dynamically, improving numerical reasoning in visual question-answering tasks. Case Study: Transition from ILANG to LIMG Figures 9 and 10 illustrate cases where the initial routing incorrectly prioritizes ILANG, which aligns visual features with language but lacks fine-grained perceptual understanding. This misalignment leads to incorrect predictions: in the first case, the model incorrectly identifies DVD Player instead of the correct answer Speaker when asked which device is not illuminated; in the second case, it incorrectly answers No instead of Yes when asked if the shirt is soft and white. To correct these errors, R2-T2 retrieves three relevant reference samples using kNN based on question similarity. These samples involve similar queries related to illumination and color perception, guiding more suitable routing adjustment. After incorporating insights from these references, the routing shifts towards LIMG, which specializes in fine-grained visual details. This adjustment enables the model to correctly identify the non-illuminated device and recognize the shirts color and texture, leading to the correct answers Speaker and Yes. These cases demonstrate R2-T2 ability to dynamically refine expert selection, improving visual perception in multimodal reasoning tasks by leveraging contextual cues from reference samples. 17 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Figure 6. Example for transition from ILANG to IAUX using R2-T2. The model initially gives incorrect answer (C)2\" by relying on ILANG. After kNN retrieval with similar questions about counting hats , it re-routes to IAUX and correctly answers (D) 1\" for the number of hats in the image. Figure 7. Example of routing transition from ILANG to LAUX using R2-T2. Initially, the model selects ILANG, misidentifying the plane number. By retrieving kNN with similar queries, R2-T2 shifts the routing weights towards LAUX, leading to the correct answer. R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Figure 8. Example for transition from ILANG to LAUX using R2-T2. The model initially gives incorrect answer FRENCH\" by relying on ILANG. After kNN retrieval with similar questions, it re-routes to LAUX and correctly identifies California\" as the plates state. Figure 9. Example of routing transition from ILANG to LIMG using R2-T2. Initially, the model selects ILANG, leading to the incorrect prediction DVD Player when asked which device is not illuminated. By retrieving kNN samples with similar illumination-related queries, R2-T2 shifts the routing weights towards LIMG, enabling the correct answer Speaker. 19 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Figure 10. Example of routing transition from ILANG to LIMG using R2-T2. Initially, the model selects ILANG, leading to the incorrect prediction No when asked if the shirt is soft and white. By retrieving kNN samples with similar color-based queries, R2-T2 shifts the routing weights towards LIMG, allowing the model to correctly answer Yes. 20 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts"
        },
        {
            "title": "E Expert Transition Analysis",
            "content": "To better understand the impact of test-time re-routing, we analyze expert transitions across different prediction scenarios. Figures 11-14 illustrate how top-1 expert selections shift before and after re-routing on CVBench2D/3D. Figure 11. Top-1 expert transitions from incorrect to correct predictions on CVBench2D/3D after re-routing. For transitions to incorrect predictions, the main patterns include transitions from ILANG to IIMG, LAUX and IAUX Figure 12. Top-1 expert transitions from correct to incorrect predictions on CVBench2D/3D after re-routing. The visualization shows primary transitions from ILANG to IAUX and LIMG, demonstrating how correct predictions can shift to incorrect outcomes through these pathways. Figure 13. Top-1 expert transitions from correct to correct predictions on CVBench2D/3D after re-routing. The main transition patterns demonstrate consistent routing from ILANG through IAUX to LIMG and IAUX, showing stable pathways for maintaining correct predictions. 21 R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts Figure 14. Top-1 expert transitions from incorrect to incorrect predictions on CVBench2D/3D after re-routing. The visualization reveals persistent incorrect prediction patterns, with transitions primarily flowing from ILANG through IAUX to LIMG and IAUX, with additional ISELF routing observed."
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "University of Maryland, College Park"
    ]
}