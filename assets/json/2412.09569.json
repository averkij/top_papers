{
    "paper_title": "JuStRank: Benchmarking LLM Judges for System Ranking",
    "authors": [
        "Ariel Gera",
        "Odellia Boni",
        "Yotam Perlitz",
        "Roy Bar-Haim",
        "Lilach Eden",
        "Asaf Yehudai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias."
        },
        {
            "title": "Start",
            "content": "JuStRank: Benchmarking LLM Judges for System Ranking Ariel Gera, Odellia Boni, Yotam Perlitz, Roy Bar-Haim, Lilach Eden and Asaf Yehudai IBM Research 4 2 0 2 2 1 ] . [ 1 9 6 5 9 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Given the rapid progress of generative AI, there is pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLMbased judges compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where judge is evaluated over set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as judges positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judges quality is assessed by comparing the resulting system ranking to human-based ranking. Beyond overall judge assessment, our analysis provides fine-grained characterization of judge behavior, including their decisiveness and bias. Figure 1: Instance and system level judges make different calls: An instance-level judge (top) is used to make decisions about the quality of individual responses (which may be produced by different systems). system-level judge (bottom) is used to make decisions about the overall quality of systems. For clarity, in this illustration, we focus on pairwise decisions."
        },
        {
            "title": "Introduction",
            "content": "The evaluation of Large Language Models (LLMs) is rapidly adopting the LLM-as-a-judge paradigm (Zheng et al., 2023), where automatic evaluations with LLMs complement the use of human annotators, or even replace them altogether. LLMbased judges are increasingly relied upon to conclude which models exhibit superior performance, whether novel training and inference approaches are beneficial, and ultimately which LLM configurations offer better value proposition to users. Since relying on an inaccurate judge will likely result in sub-optimal decisions, this trend lends an urgency to evaluating the performance of the LLM judges themselves. Indeed, recent works attempt to benchmark judging capabilities, compiling leaderboards of judge performance (Lambert et al., 2024; Tan et al., 2024) as well as analyzing their sensitivities and biases (Wang et al., 2023; Thakur et al., 2024; Wei et al., 2024; Bavaresco et al., 2024; Feuer et al., 2024; Liu et al., 2024b; Lee et al., 2024a; Xu et al., 2024; Ye et al., 2024). These works all focus on the instance-level performance of judges. good instance-level judge is expected to make correct judgment about each response, regardless of the system generating it. For example, given specific pair of responses, the judge may be asked to determine which one is better (Figure 1, top). This approach is very much in line with prevailing paradigms for model alignment (e.g., RLHF, DPO; Lee et al., 2024b) and synthetic data generation (Yehudai et al., 2024); these often rely on LLM judges and reward models for making 1 Figure 2: System-level judge pipeline. Schematic of our data generation pipeline for judge system rankings. instance-level pairwise decisions on the quality of individual responses. Although judges are evaluated based on their instance-level performance, very commonly they are actually used for making system-level decisions; namely, to compare and rank different models or different configurations (Figure 1, bottom). Crucially, even very good instance-level capabilities do not guarantee accurate model ranking; and at the same time, mediocre performance on instances could still yield very accurate overall ranking (Dorner et al., 2024, 2). Thus, the systemlevel performance of judges that is, to what degree they can correctly decide between candidate systems, and produce accurate model performance rankings remains largely an open question. Furthermore, system-level evaluations can unveil an entire range of under-explored judge qualities, such as being biased towards certain models or making un-calibrated model preference judgments. In this work we aim to address this gap, and characterize the system-level evaluation capabilities and behaviors of LLM-based judges. To this end, we introduce novel judge benchmark JuStRank (Judges for System Ranking). JuStRank compares judges by their ability to correctly rank models, based on agreement with ground-truth model ranking. JuStRank encompasses collection of 48 state-of-the-art judges, including both generalpurpose LLMs and reward models. Our large-scale benchmark and analysis allow us to investigate the performance and behavior of judges when ranking systems. Our contributions are as follows: 1. We introduce JuStRank, the first large-scale benchmark of judges for ranking target systems. 2. We quantify the tendency of judge to exhibit system bias, where some models are judged unfairly (6.2). 3. We reveal an emergent quality of systemlevel judge, its decisiveness factor; decisive judges consistently amplify the gap between strong and weak target systems (6.1)."
        },
        {
            "title": "2 The Gap in Judge Benchmarking",
            "content": "In this section, we outline why existing estimations of judge performance are insufficient to decide which judge is best at choosing between target systems. (Figure 1, bottom). At present, users looking for judge for ranking models, will likely choose it according to the available instance-level judge benchmarks. Yet, from theoretical standpoint instance-level judge performance does not directly correspond to system-level judge performance (Dorner et al., 2024). More specifically, instance-level judge evaluations focus on how many errors the judge makes, and do not address the distribution of these errors across systems. For system-level judge evaluation, however, the error distribution plays key role, as judge errors may distribute unevenly across systems, impacting their induced ranking. For example, judge may exhibit an unjustifiable preference (positive bias) towards responses from particular system A. Thus, this judge will tend to give this system the wrong ranking, even if it makes very few mistakes on responses from other systems (i.e., has an overall high instance-level accuracy). Hence, more uniform distribution of errors reflecting less biased judgment is desirable quality for system-level judges, and one that may lead to more accurate ranking. Drawing on this observation, our goal here is to construct system-level benchmark for judges. As benchmark tailored for system-level evaluation, it will enable reliably estimating judges ability to rank systems; moreover, our ranking-oriented analysis can shed light on judge behaviors and biases, as they occur in real-world data."
        },
        {
            "title": "3 Task Formulation",
            "content": "In this work we study the use of LLM-based judges for determining the relative quality of systems1, over given set of user instructions (prompts). l=1, and user instructions = {ik}K Formally, we begin with set of systems = {sl}L k=1. Each system produces response for each such user instruction, denoted as = {rl k,l=1,1 , such that sl(ik) = rl k}k,l=K,L (see Figure 2). Judges = {jp}P p=1 map pair of instruction ik, and system response rl to scalar score that estimates the quality of the response. Each judge has specific realization for performing this score k) = Scorep mapping2, of the form: jp(ik, rl k,l. Once judge jp scores all responses, we can define scores matrix jp(R) RKL where jp(R)k,l = Scorep k,l. In order to quantify system-level quality, we must apply an aggregation method, = {a : RKL RL}. The aggregation method maps scores matrix jp(R) to system-level vector p,a RL where each entry, p,a , is single overall quality score for system sl by judge jp. In turn, ordering the systems scores in p,a induces ranking over the systems set S. We test the performance of judge jp as ranker by checking the correlation between the ranking induced by p,a and golden ranking for S."
        },
        {
            "title": "4 Experimental setup",
            "content": "To explore judge performance and behavior, we utilize responses from multiple systems (4.1) and run reward model judges (4.2.1) and LLM judges (4.2.2) over these responses. To obtain system rankings, we experiment with different aggregation methods (4.3) over the judge scores. Finally, the resulting rankings are compared against gold system ranking, obtained from separate dataset (4.4). 1Henceforth, we will use the term System to refer to target model or pipeline that performs task, and Judge for one that is asked to score (or compare) the quality of such systems. Generative LLMs can act as both systems and judges. 2We note that some realizations, such as the comparative realization in 4.2.2, may incorporate separate set of responses to perform the judgment. 3 4.1 System Responses Data We utilize the Arena Hard v0.1 dataset (Li et al., 2024) for diverse set of instructions and system responses. The dataset uses curated set of = 500 challenging instructions, I. As of September 2024, it includes responses from = 63 systems, S, totaling about 32K pairs of instructions and their associated system responses, R. 4.2 Generating Judgments For every judge realization, jp, we generate judgment scores matrix, jp(R), over R. In total, we examine 48 judge realizations, yielding total of 1.5M individual judge scores (63 systems 500 instances 48 judge realizations). 4.2.1 Reward Models We run multiple reward models over R. While their exact architectures vary, reward models generally produce scalar quality score for given pair of an instruction and system response. We utilize the following reward models: ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024), Eurus-RM-7b (Yuan et al., 2024), InternLM2-7breward, InternLM2-20b-reward (Cai et al., 2024), Skywork-Reward-Llama-3.1-8B-v0.2 (Liu et al., 2024a), Llama-3-OffsetBias-RM-8B (Park et al., 2024), GRM-Llama3.2-3B-ft (Yang et al., 2024), URM-LLaMa-3.1-8B (Lou et al., 2024)."
        },
        {
            "title": "4.2.2 LLM Judge Realizations\nUnlike dedicated reward models that produce a\nsingle score, generative LLMs can be prompted to\njudge in multiple ways. Thus, for every LLM we\nexamine several judge realizations.",
            "content": "Absolute judgment - Numeric score (Numeric) The LLM judge is given an instruction and system response, and is asked to provide quality score for the response between 0 and 100. Absolute judgment - Textual score (Likert) The judge is asked to provide quality score of the response on Likert (Likert, 1932) scale with 5 labels: [Very Bad, Bad, Mediocre, Good, Very Good]. We then convert the textual judgments to scores in [1 5]. judgment Absolute - Token probablities (TokenProbs) The task is framed to the judge as yes/no question: Is this good response?. We then extract the top log-probabilities for the first generated token, and specifically look at the probabilities for the tokens yes or no. The judgment Judge Model Realization Aggregation Agreement (τ ) with Gold Ranking Likert Reward Anchor Qwen2.5-72B-Instruct URM-LLaMa-3.1-8B GPT-4o-2024-11-20 Llama-3-1-405b-instruct-fp8 Numeric Likert Mistral-large-instruct-2407 Numeric GPT-4o-mini-2024-07-18 Reward ArmoRM-Llama3-8B-v0.1 Llama-3-1-70b-instruct Numeric Skywork-Llama-3.1-8B-v0.2 Reward Llama-3.1-8B-Instruct Win-Rate Mean Mean Mean BT Win-Rate Mean Win-Rate Mean TokenProbs Mean .83 .82 .82 .81 .81 .81 .80 .80 .79 .78 Table 1: Top 10 judges by ranking performance. Judges are sorted by the Kendalls Tau correlation between their overall system ranking and the gold ranking from Chatbot Arena (4.4). For every judge model, only the best-performing realization and aggregation method is shown. For the full results, refer to Appendix Table 2. score [0.0 1.0] is the sum of probabilities for yes divided by the sum of probabilities for yes and no. Median aggregation, and BT (Bradley-Terry) aggregation. Details are provided in Appendix B. judgment Comparative - Anchor model (Anchor) Here the judgment task is comparative, i.e., the judge is asked to state preference between two responses rather than an absolute quality judgment of given response. Conducting paired comparisons between system and all other systems is unfeasible; thus, we follow Li et al. (2024) and use the responses of GPT-4-0314 as anchors to which the responses of other systems are compared. Given an anchor response and system response, we ask the judge which one it prefers. The output is then converted to scores in [2, +2] (where 0 indicates tie, and +1 / +2 indicate slight/strong preference for the system response over the anchor response, respectively). In total, we collect judgments from 10 LLMs and 4 realizations3, yielding 40 LLM judges. We use the following generative LLM judges: Mixtral8x7B-Instruct-v0.1 (Jiang et al., 2024), Mixtral8x22B-Instruct-v0.1, Mistral-Large-Instruct-2407, Llama-3.1-405B-Instruct (Dubey et al., 2024), Llama-3.1-70B-Instruct, Llama-3.1-8B-Instruct, Qwen2.5-72B-Instruct, GPT-4o and GPT-4o-mini."
        },
        {
            "title": "4.3 Aggregations",
            "content": "Given the raw judgment scores of each judge, jp(R), there are multiple ways to construct ranking of the 63 target systems. We calculate rankings using Win-rate aggregation, Mean aggregation, 3Prompts for all realizations are provided in Appendix G. 4 Figure 3: Comparison to RewardBench. The plot depicts the relative performance of judges present in both JuStRank and RewardBench (Lambert et al., 2024). For comparison, we perform Min-Max normalization over the judge performance scores (accuracy for RewardBench, Kendalls Tau for our results). Results shown are for the BT aggregation method; the LLM judges use the Anchor realization, which is closest to the setting in RewardBench. Plots for the different RewardBench subsets are shown in Appendix Figure 8."
        },
        {
            "title": "4.4 Gold Ranking - Chatbot Arena Battles",
            "content": "data preference Human from Chatbot Arena (Zheng et al., 2023) serve as our groundtruth reference for the relative quality of systems. Chatbot Arena relies on human-annotated battles between system responses to produce system ranking. We use the English Hard Prompts Figure 4: LLM judge realizations. Kendalls Tau correlations (95% bootstrapping CI) between the system rankings produced by various LLM judge realizations (4.2.2) and the gold system ranking from Chatbot Arena. The plot depicts results for the BT aggregation method; for the full results, refer to App. Table 2. subset4 of their data. We chose this subset as its distribution of user instructions has been shown (Li et al., 2024) to match that of our system response data (4.1). We extract the data and ranking following the official code (see Appendix C). Given system ranking produced by judge, we quantify judge performance via the correlation between its ranking and the reference ranking from Chatbot Arena. Simply put, we assume that ranking given by good automated judge would have high agreement with the ranking compiled from human judgments. 5 JuStRank - Judge Performance Results Table 1 depicts the 10 top-performing judges on JuStRank, based on their ranking agreement (τ ) with the ground-truth human ranking from Chatbot Arena. For each judge model, the best-performing realization and aggregation method is shown. As seen in the table, there are both LLMs and reward models that reach decent agreement with the gold ranking. Moreover, several 8B-parameter reward models are on par with much larger LLMs on the task of system ranking. Thus, we see that reward models, which are explicitly trained to make instance-level decisions between pairs of responses, can excel at the system-level ranking task as well. Note that an identical correlation score with the ground-truth ranking does not indicate that the judges produce the same ranking; rather, each judge has different pattern of agreement with the ground-truth. Correlations among the judges 4Chatbot Arena Hard Prompts themselves are shown in App. Fig. 9. Comparison to Instance-Level Performance In Figure 3 we compare our system-level judge leaderboard to the instance-level benchmark RewardBench (Lambert et al., 2024). The results demonstrate that better instance-level judges are not always better system rankers, highlighting the discrepancy between the two tasks. Thus, JuStRank offers novel perspective on judge ability. However, there may be additional factors at play as well. For LLM judges, we use slightly different realization from the comparative prompts used for RewardBench. Moreover, since creators of reward models aim to do well on RewardBench, it is possible that some newer reward models are slightly overfitted to this test distribution."
        },
        {
            "title": "5.1 Effects of LLM Realizations",
            "content": "Figure 4 depicts the performance of the LLM judge models by their realization (4.2.2). The plot demonstrates that the choice of realization has considerable effect on the system ranking quality; this appears to be nearly as important as the identity of the LLM used. We confirm this finding using statistical variance analysis (Appendix D). Many works recommend asking LLMs for comparative rather than absolute judgments (Zheng et al., 2023). However, in our experiments the comparative realization (Anchor) exhibits lower performance, with the notable exception of GPT4o. The best realizations overall were Numeric and Likert, where the judge is asked to provide verbalized quality score. This is in line with findings 5 Figure 5: Predicted pairwise win-rates. Each point represents win-rate between pair of systems R(sa, sb) (App. E). The x-axis denotes the gold win-rate from Chatbot Arena, and the y-axis denotes the predicted win-rate as derived from the judge scores. The diagonal marks an exact match between the predicted and gold win-rate; the quadrants signify whether the predicted winning system is the same (green) or different (red) from the gold winning system for this pair. Note that every pair is represented twice (e.g., R(sa, sb) = 0.2, R(sb, sa) = 0.8). from Tian et al. (2023), who report better calibration with verbalized LLM confidence scores. The higher performance for both Numeric and Likert realizations compared to Anchor and TokenProbs is statistically significant (App. D). We also note that each realization induces characteristic distribution of judge scores, Dp, such that Scorep k,l Dp. Notably, the LLM judges tend to produce particular score values more often than others. Refer to Appendix for more details."
        },
        {
            "title": "Judge Behavior",
            "content": "Next, we explore more fine-grained judge behaviors, beyond the bottom-line system rankings. To that end, we focus on the judgment task of pairwise system preference, as this is the foundation of system ranking tasks. As in 5, our aim is to gain an understanding of judge performance and characteristics, by comparing judge behavior on pairwise system preference to ground-truth data. Pairwise Win-Rates For every judge jp, and for every pair of systems (sa, sb), the win-rate of sa, denoted by Rp(sa, sb), is the number of instances where it received higher score than sb, divided by the number of non-tied instances (cf. Appendix E). Thus, we calculate the pairwise win-rate for each system pair according to each judge. Note that the win-rates are calculated on the scores matrix jp(R), i.e., before applying an aggregation method. Gold Win-Rates Similarly, we extract gold pairwise win-rates, Rg, from Chatbot Arena (App. C). 59 systems appear both in our response data (4.1) and in Chatbot Arena; in total, we have both judge and gold data for 968 head-to-head comparisons between pairs of systems."
        },
        {
            "title": "6.1 Some Judges Are Particularly Decisive",
            "content": "Figure 5 depicts the relationship between predicted win-rates and gold win-rates for several judges. The quadrants in the figure indicate whether the judges pairwise preference decision is aligned with the gold preference. As can be expected, the judge predictions in Figure 5 are often centered around the ground-truth win-rates determined by humans. But strikingly, some judges exhibit unique prediction patterns, yielding win-rates that are consistently closer to the extremes (0.0 / 1.0) compared to the human data. For instance, for pairs with ground-truth win-rate of 0.8, we can see that the predicted win-rate in the judgments of Llama405B (Fig. 5, right) tends to exceed 0.9. Put simply, when faced with response from strong system, the judge is very likely to prefer it over the response of less capable system, even where human judges are less decisive. This sigmoidal win-rate prediction pattern resembles behaviors previously described for classifier calibration (Silva Filho et al., 2023), where classifiers may exhibit overconfidence in their predicted probabilities.5 Thus, following Kull et al. (2017), we quantify judges decisive (overconfident) behavior by fitting the cumulative beta distribution function to the win-rate prediction plots. 5Note, however, that the behavior in our case does not reflect judge probability scores, but rather the empirical ratio of instances where the responses {rl l=1 of system are preferred over those of another system. k}l=L 6 Figure 6: Beta distribution fit of pairwise win-rates. (a): Judge beta fit example. Each point represents the win-rate between pair of systems, R(sa, sb); the curve and α value describe fit to the beta distribution (App. F). Plots for all judges are in App. Fig. 11. (b): Decisiveness by judge realization. Cell values denote the decisiveness behaviors of different LLM judge realizations, as described by the α value for their win-rate distribution. This enables describing judge prediction behavior in terms of single fit value α = β, where α [0, ], α = 1 represents no overor underdecisiveness, and larger values represent more decisive behavior (refer to Appendix for details). Figure 6a and App. Fig. 11 depict the beta curve fit for win-rates of various judges. Figure 6b compares judge realizations in terms of their decisiveness behavior. We see that LLM judges are usually more decisive when directly asked to provide quality score, and in particular textual one (Likert); in contrast, the realization that relies on token probabilities (TokenProbs) does not give rise to such pattern, and can even result in judge indecision (i.e., α < 1). This pattern can be explained from two directions. First, the human judgments (4.4) were collected from multiple individuals, who likely have differing preferences; this may introduce some noise that could lead to less extreme win-rates in the gold data. The other factor is the judges, who may rely on certain heuristics to identify responses from strong systems (Feuer et al., 2024), leading to more extreme win-rates in the judge data. While the variance between judges (Fig. 6b) supports the latter, we cannot determine this conclusively. In practical terms, extreme win-rates can be beneficial to users, as they increase the likelihood of correct system preference decision given smaller set of responses (see Ashury Tahan et al., 2024)."
        },
        {
            "title": "6.2 Bias Towards Specific Systems",
            "content": "A major concern when using judges for system preference is judge bias judge may treat specific system unfairly, by consistently judging its responses too favorably or too harshly. We define the bias Bp sa of judge jp towards system sa by the expectation over the differences between the predicted win-rate and the gold win-rate, over all systems that sa interacts with. Formally, Bp sa = EsbS(W Rp(sa, sb) Rg(sa, sb)). In other words, if according to jp the win-rates of system sa are (on average) higher than those in the human data, we will say that jp exhibits positive bias towards it; and if they are lower than the ground-truth, jp would be said to exhibit negative bias towards it. Note that the decisiveness behavior in 6.1 directly entails general bias pattern in some judges namely, positive bias towards strong systems, and negative bias towards weak ones. Thus, we calcup, where the late decisiveness-corrected bias, sa gold win-rate Rg is replaced by Rg p, i.e., the predicted value for the gold win-rate on the beta distribution fit for judge jp (App. F). We observe some consistent trends of systemspecific bias that are common across judges. Figure 7 depicts systems for which there is high bias across judges. For instance, most judges exhibit strong positive bias towards Athene-70B, to the extent that it is often ranked by them as the #1 system. In contrast, GPT-4-0613, which is 27th in the gold ranking, receives negative bias, resulting in median rank of 38 among the judges. We also ask whether LLM judges exhibit selfbias (Xu et al., 2024), i.e., bias towards the system that uses the same underlying LLM. While we find some instances of self-bias, this is not consistent effect across judge realizations (App. Table 3)."
        },
        {
            "title": "To quantify the overall propensity of a judge for",
            "content": "7 and classifiers) on the task of correctly deciding between pairs of outputs, labeled as \"preferred\" or \"rejected\" by human annotators. RewardBench aims to identify the most suitable judges for model alignment, e.g., for use in RLHF; in contrast, the present work measures judges in terms of their ability to compare the performance of candidate systems. Another recent instance-level benchmark is JudgeBench (Tan et al., 2024), which focuses on curating challenging response pairs where the judge must discern subtle errors. Multiple works are dedicated to analyzing various biases (Ye et al., 2024) and undesirable behaviors exhibited by judges. These include positional bias (Wang et al., 2023), verbosity bias (Saito et al., 2023; Chen et al., 2024) and self-bias (Xu et al., 2024), as well as sensitivity to prompts (Wei et al., 2024), source datasets (Bavaresco et al., 2024), epistemic markers (Lee et al., 2024a) and style (Feuer et al., 2024; Liu et al., 2024b). Several popular benchmarks rely on LLM judges to produce leaderboards of state-of-the-art systems. Such benchmarks e.g., Arena Hard (Li et al., 2024) and AlpacaEval (Dubois et al., 2024) do perform system-level validation of their resulting leaderboards against other benchmark rankings (see Perlitz et al., 2024). However, such efforts are limited to validating the particular dataset and judge setup chosen for the benchmark (usually incorporating GPT-4 as the judge), rather than comparing and analyzing the performance of different judge models and implementations. Thakur et al., 2024 conduct task-specific system-level evaluation of judges, over the TriviaQA (Joshi et al., 2017) dataset. Compared to their work, the present study is on larger scale and offers novel metrics and analyses on system-level judge behaviors."
        },
        {
            "title": "8 Discussion",
            "content": "The usage of LLM-based judges is continually expanding. Moreover, many research papers proposing novel architectures, algorithms and training methods rely heavily on system-level evaluations using judges as evidence for the utility of their approach. But without evaluating the judges on such system-level tasks, how can one know whether to trust such evaluations, and their conclusions? We are the first to investigate on large scale the performance of LLM-based judges on the system ranking task. Our resulting benchmark, JuStRank, will assist users and researchers in choosing the Figure 7: System-specific judge biases. The plot depicts win-rate biases of judges towards specific systems, with respect to the ground-truth win-rates from Chatbot Arena (after correction for the beta distribution fit of each judge). This plot portrays select systems with high bias; the full heat map, including all judge realizations and all systems, is shown in App. Fig. 10b. bias, we measure the standard deviation of its bias over all systems, δ = σsS(Bp). The bias measure for each judge is presented in App. Table 4."
        },
        {
            "title": "6.3 Characterizing Judge Behaviors",
            "content": "We have shown that beyond their overall ranking capability (5), judges exhibit distinct traits in their system-level judgments in particular, they show different levels of decisiveness (6.1), and overall propensities for bias (6.2). Interestingly, each of these traits (cf. App. Table 4) is correlated to the ranking quality τ , with = 0.55 for the α decisiveness measure, and = 0.56 for the bias propensity δ. At the same time, these marked traits are by design uncorrelated with each other (r = 0.07 between α and δ). Thus, our analyses reveal global system-level judge traits, ones that remain hidden when assessing judges from an instancelevel perspective."
        },
        {
            "title": "7 Related Work",
            "content": "Applying and assessing automatic metrics for system-level evaluation has been studied for decades, in particular for natural language generation tasks (Reiter and Belz, 2009; Louis and Nenkova, 2013; Deutsch et al., 2022). In the context of LLM-based judges, however, system-level evaluation is still under-explored. For LLM-based judges and reward models, prior works have opted for an instance-level evaluation approach, curating benchmarks of task outputs with ground-truth quality annotations in order to evaluate judge performance. Most prominently, RewardBench (Lambert et al., 2024) compares dozens of judges (including reward models, generative LLMs, 8 judge best suited for their needs. Choosing judge requires many fine-grained decisions. user can decide which reward model or LLM to use as the judge; opt for relative judgments or absolute scores; try various prompts; apply different aggregations to compile ranking, etc. Furthermore, these decisions may interact in non-trivial ways (e.g., the distribution of scores judge tends to output can dictate which aggregations will work well). Indeed, our findings confirm that such decisions substantially affect system-level judgments (5), and thus are quite likely to change the model selection of an end user, or flip the conclusions of an NLP research paper. Our system-level approach has multiple additional benefits. First, it forces the evaluation of judges to be representative with respect to the distribution of systems that generate the responses. In existing instance-level benchmarks this factor is not taken into account, and likely results in less accurate judge evaluations. Second, it affords new perspective on what it means for judge to be biased; on the one hand, we discover some decisiveness trends (6.1) that may actually be useful for making correct preference decisions, and increasing the separability between systems; and on the other, we report some problematic biases that directly distort the judgment of particular systems (6.2). An important avenue for future work is to connect our findings here to the existing literature on judge biases (Ye et al., 2024), and understand to what extent both of these behaviors stem from particular LLM style attributes (Feuer et al., 2024). Given this vast and complex space, our work is admittedly only first step in understanding the behavior of judges for ranking and selecting LLMs. We encourage the community to explore these issues further: for instance, by training dedicated system-level judges, exploring judge ensembles, or studying other aggregation approaches. We believe that JuStRank can facilitate such research directions, as it can be easily extended to new judges without requiring additional human annotations. Our hope is that both practitioners and researchers can benefit from JuStRank, by making more informed choices of judges to suit their needs."
        },
        {
            "title": "9 Conclusion",
            "content": "In this work we conducted the first comprehensive evaluation of system ranking by LLM judges. We tested wide array of judges, including reward models, as well as different realizations of generative LLMs, over large collection of systems. We collected system responses over diverse set of instructions. The judges scored each response, and we compiled ranking by aggregating the judgments over all the responses. Then, the quality of the judges system ranking was compared to human-based ranking, producing the JuStRank leaderboard. JuStRank allows users to pick judges that are better aligned with the goal of choosing between different models and configurations. JuStRank demonstrates that judge ranking abilities are not directly tied to LLM size or overall quality, and that some dedicated reward models are on par with leading LLM judges. Moreover, our analysis reveals emergent judge traits decisiveness and bias that are strongly correlated with their ranking ability."
        },
        {
            "title": "Limitations",
            "content": "The gold reference data the English Hard Prompts subset of Chatbot Arena does not include user instructions or responses. Hence, we collect judgment data over Arena Hard, which contains large set of instructions and responses. This raises some questions regarding our ability to directly compare the LLM judges and human judges. However, given that Arena Hard was designed to match the distribution of user instructions in English Hard Prompts (see Li et al., 2024), we assume that these datasets are sufficiently similar. Our analyses of LLM judge realizations are, by necessity, limited to the specific realization prompts that we used. Several studies show that LLMs (Mizrahi et al., 2024) as well as LLM judges (Wei et al., 2024) are brittle with respect to prompt phrasing, and hence this may have had an impact on the results. As in multiple other works, here we treat human preference as single concept. In practice, however, preference is inherently subjective, and is composed of numerous dimensions (e.g., helpfulness, safety, style, coherence etc.). For instance, one individual may prefer succinct model responses while another would prefer more detailed answers. Thus there is no single human preference, but rather collection of preference decisions that depend on the annotation guidelines, cultural context, and human idiosyncrasies (Conitzer et al., 2024; Kirk et al., 2024). 9 Note that following Peyrard et al. (2021), as well as Chatbot Arena (Chiang et al., 2024), we generally regard the ground-truth quality of system in terms of the Bradley-Terry model; simply put, better system is system that wins more often. Thus, in this work we do not directly consider the quality difference in system responses per instance, i.e., beyond counting wins/losses. Still, some of the aggregation methods we use (e.g., mean) implicitly reflect other perspectives on system quality. All of our analyses are performed on heterogeneous datasets of user instructions to LLMs. Thus, while we study judges through the lens of generalpurpose LLM usage, we cannot draw conclusions on judge behavior that is task-specific (or in specialized domains), nor on performance in languages other than English (Gureja et al., 2024). The issue of task, domain, and language-specific judge behavior is thus an important avenue for future work."
        },
        {
            "title": "References",
            "content": "Shir Ashury Tahan, Ariel Gera, Benjamin Sznajder, Leshem Choshen, Liat Ein-Dor, and Eyal Shnarch. 2024. Label-efficient model selection for text generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 83848402, Bangkok, Thailand. Association for Computational Linguistics. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al. 2024. LLMs instead of human judges? large scale empirical study across 20 NLP evaluation tasks. arXiv:2406.18403. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. 2024. InternLM2 technical report. arXiv:2403.17297. Lichang Chen, Chen Zhu, Jiuhai Chen, Davit Soselia, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, and Bryan Catanzaro. 2024. ODIN: Disentangled reward mitigates hacking in RLHF. In Forty-first International Conference on Machine Learning. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph Gonzalez, et al. 2024. Chatbot Arena: An open platform for evaluating LLMs by human preference. In Forty-first International Conference on Machine Learning. Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley Holliday, Bob Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, et al. 2024. Social choice should guide ai alignment in dealing with diverse human feedback. arXiv:2404.10271. Daniel Deutsch, Rotem Dror, and Dan Roth. 2022. Reexamining system-level correlations of automatic summarization evaluation metrics. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 60386052, Seattle, United States. Association for Computational Linguistics. Florian Dorner, Vivian Nastl, and Moritz Hardt. 2024. Limits to scalable evaluation at the frontier: LLM as judge wont beat twice the data. arXiv:2410.13341. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The Llama 3 herd of models. arXiv:2407.21783. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled AlpacaEval: simple way to debias automatic evaluators. arXiv:2404.04475. Benjamin Feuer, Micah Goldblum, Teresa Datta, Sanjana Nambiar, Raz Besaleli, Samuel Dooley, Max Cembalest, and John Dickerson. 2024. Style over substance: Failure modes of LLM judges in alignment benchmarking. arXiv:2409.15268. Srishti Gureja, Lester James Validad Miranda, Shayekh Bin Islam, Rishabh Maheshwary, Drishti Sharma, Gusti Winata, Nathan Lambert, Sebastian Ruder, Sara Hooker, and Marzieh Fadaee. 2024. MRewardBench: Evaluating reward models in multilingual settings. arXiv:2410.15522. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv:2401.04088. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada. Association for Computational Linguistics. Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He He, Bertie Vidgen, and Scott Hale. 2024. The PRISM 10 alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. arXiv:2404.16019. Meelis Kull, Telmo Silva Filho, and Peter Flach. 2017. Beta calibration: well-founded and easily implemented improvement on logistic calibration for binary classifiers. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pages 623631. PMLR. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. RewardBench: Evaluating reward models for language modeling. arXiv:2403.13787. Dongryeol Lee, Yerin Hwang, Yongil Kim, Joonsuk Park, and Kyomin Jung. 2024a. Are LLM-judges robust to expressions of uncertainty? investigating the effect of epistemic markers on LLM-based evaluation. arXiv:2410.20774. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. 2024b. RLAIF vs. RLHF: Scaling reinforcement learning from human feedback with ai feedback. arXiv:2309.00267. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. arXiv:2406.11939. Rensis Likert. 1932. technique for the measurement of attitudes. Archives of Psychology. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024a. Skywork-reward: Bag of tricks for reward modeling in LLMs. arXiv:2410.18451. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2024b. RM-bench: Benchmarking reward models of language models with subtlety and style. arXiv:2410.16184. Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie, and Junge Zhang. 2024. Uncertainty-aware reward model: Teaching reward models to know what is unknown. arXiv:2410.00847. Annie Louis and Ani Nenkova. 2013. Automatically assessing machine summary content without gold standard. Computational Linguistics, 39(2):267 300. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. 2024. State of what art? call for multi-prompt LLM evaluation. arXiv:2401.00595. Junsoo Park, Seungyeon Jwa, Ren Meiying, Daeyoung Kim, and Sanghyuk Choi. 2024. OffsetBias: Leveraging debiased data for tuning evaluators. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 10431067, Miami, Florida, USA. Association for Computational Linguistics. Yotam Perlitz, Ariel Gera, Ofir Arviv, Asaf Yehudai, Elron Bandel, Eyal Shnarch, Michal Shmueli-Scheuer, and Leshem Choshen. 2024. Do these LLM benchmarks agree? Fixing benchmark evaluation with BenchBench. arXiv:2407.13696. Maxime Peyrard, Wei Zhao, Steffen Eger, and Robert West. 2021. Better than average: Paired evaluation of NLP systems. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 23012315, Online. Association for Computational Linguistics. Ehud Reiter and Anja Belz. 2009. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 35(4):529558. Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. 2023. Verbosity bias in preference labeling by large language models. arXiv:2310.10076. Telmo Silva Filho, Hao Song, Miquel Perello-Nieto, Raul Santos-Rodriguez, Meelis Kull, and Peter Flach. 2023. Classifier calibration: survey on how to assess and improve predicted class probabilities. Machine Learning, 112(9):32113260. Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. 2024. JudgeBench: benchmark for evaluating LLMbased judges. arXiv:2410.12784. Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. 2024. Judging the judges: Evaluating alignment and vulnerabilities in LLMs-as-judges. arXiv:2406.12624. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 54335442, Singapore. Association for Computational Linguistics. John Tukey. 1949. Comparing individual means in the analysis of variance. Biometrics, pages 99114. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Interpretable preferences and Tong Zhang. 2024. 11 via multi-objective reward modeling and mixture-ofexperts. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 10582 10592, Miami, Florida, USA. Association for Computational Linguistics. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. arXiv:2305.17926. Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, and Mei Han. 2024. Systematic evaluation of LLM-as-a-judge in LLM alignment tasks: Explainable metrics and diverse prompt templates. arXiv:2408.13006. Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Wang. 2024. Pride and prejudice: LLM amplifies self-bias in self-refinement. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1547415492, Bangkok, Thailand. Association for Computational Linguistics. Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. 2024. Regularizing hidden states enables learning generalizable reward model for LLMs. In Advances in Neural Information Processing Systems. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al. 2024. Justice or prejudice? quantifying biases in LLM-as-a-judge. arXiv:2410.02736. Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Eyal Shnarch, and Leshem Choshen. 2024. Achieving human parity in content-grounded In The Twelfth International datasets generation. Conference on Learning Representations. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. 2024. Advancing LLM reasoning generalists with preference trees. arXiv:2404.02078. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Advances in Neural Information Processing Systems, volume 36, pages 4659546623. Curran Associates, Inc."
        },
        {
            "title": "A Judge Score Distributions",
            "content": "Figure 12 depicts the score distributions, Dp, of the judges in our data. Reward model distributions The reward models exhibit continuous score distributions. As seen in Figure 12, these distributions vary in the range of scores, as well as in the shape of the distribution. Some reward model judges have narrow range of scores, e.g., 0.1 to 0.4, whereas in others it is much wider, e.g., 3000 to 5000. Similarly, some distributions are more symmetric while others have peaks at more extreme values. However, all distributions are uni-modal, with single peak. Moreover, we note that the continuous nature of these judgment scores also entails an absence of ties between the judged responses. LLM Numeric distributions As shown in Figure 12, even though the LLM judges are given wide range of possible judgment scores ([0 100]), in practice they tend to prefer specific score values. This results in many ties when comparing responses from different systems. LLM Likert distributions Similarly to the Numeric distributions, the Likert realizations put most of their probability mass on specific scores, which leads to an even greater inclination towards ties (as here they are limited to smaller range of scores). LLM TokenProbs distributions TokenProbs scores tend to be extreme, namely very close to either 0.0 or 1.0. Thus, in many cases the score gap between responses is extremely small. This can result in low judge robustness (see the error bars in Figure 4), as well as higher sensitivity to the choice of aggregation method. LLM Anchor distributions The distribution for Anchor judgments is mainly tied to the quality of the anchor system relative to the other systems. However, we see that it is also affected by the characteristics of the judge. For example, we see in Fig. 12 that Llama-3.1-8B exhibits indecision, rating most responses as comparable to those of the anchor. In addition, for some judges, the proportion of 1 scores (i.e., the response is slightly worse than the anchor) or 1 scores (the response is slightly better than the anchor) is unusually low."
        },
        {
            "title": "B Aggregation Methods",
            "content": "Given the raw judgments of each judge, jp(R), there are multiple aggregation methods, a, that con12 struct ranking over all the target systems. Here, we calculate rankings using Win-rate aggregation, BT aggregation, Mean aggregation, and Median aggregation. In the following, we provide further details on each aggregation. Mean & Median Aggregation These aggregation methods map score for each system, sl, by relying solely on the scores assigned to its responses by judge jp. In other words, the mapping of p,a by depends only on the column corresponding to system sl in jp(R). Accordingly, these aggregations can be viewed as an operation on the columns of the scores matrix jp(R). Specifically, for the Mean aggregation, p,a = 1 k,l. Similarly, Median aggregation is the median of the vector jp(R)l. k=1Scorep ΣK We note that for realizations with discrete score distributions (see A), many systems will likely share the same median score; in this case, the Median aggregation method fails to separate the systems. Hence, Table 2 contains only handful of LLM judges with Median aggregation, all using the TokenProbs realization. Win-rate Aggregation This aggregation maps each system based on its proportion of wins insystems, over other = structions ik I. 1 1 L1 ΣL ΣK k,l), where I() denotes the indicator function. averaged over all Formally, p,a k,b > Scorep I(Scorep l=1,l=b k=1 Bradley-Terry Aggregation Following Chiang et al. (2024), we use the vector of Bradley-Terry (BT) coefficients (Bradley and Terry, 1952) as system scores. For calculating the BT scores we use the implementation of the Chatbot Arena official notebook6. Whereas Chiang et al. (2024) apply this method for battles between responses with human judge, we apply it over our LLM-based judge data, i.e., each battle is comparison between the judge scores Scorep k,b for response generated by systems sa and sb. k,a, Scorep When there are no ties, e.g., for the reward model judges, this aggregation produces similar rankings to the win-rate aggregation."
        },
        {
            "title": "C Chatbot Arena Data",
            "content": "The data for the Chatbot Arena LLM leaderboard (https://lmarena.ai) consists of \"battles\" between systems over the same instructions. In these 6Arena official notebook battles, users indicate preference (or tie) between pair of responses generated by different LLMs (Zheng et al., 2023; Chiang et al., 2024). We use their public data file from August 20247, and follow the official notebook6 to extract the raw data, deduplicate it, and calculate the overall system rankings. This dataset includes the human preference judgments and names of the participating systems, but not the instructions or system responses for the battles. Here we limit the analysis to the English Hard Prompts subset of their data8 (300K battles). Notably, Arena Hard was specifically designed to match the distribution of user instructions in the English Hard Prompts subset, as described by Li et al. (2024). We follow their code to construct full system ranking based on these 300K battles, using Bradley-Terry coefficients. This yields score for each system in their data, including 59 systems that are also in our system responses data (4.1) Out of this full English Hard data, we also extract total of 113K battles that were not judged by humans as ties, and that are between pairs of systems which appear in our responses data. We then use those to calculate win-rates between pairs of systems (E), yielding total of 968 system pairwise win-rates. Note that the Chatbot Arena data does not contain battles between every possible pairing of systems, and thus we do not have winrates for all combinations of the 59 systems under consideration. In addition, we limit the analysis to system pairs with at least 10 non-tied battles."
        },
        {
            "title": "Performance",
            "content": "In 5 and Table 2 we report results of agreement with the gold ranking (τ ) for various judge pipelines. Each pipeline consists of chosen judge model, realization (4.2.2) and an aggregation method (4.3, App. B). We focus on the LLM judges and perform three-way ANOVA (analysis of variance), with the ranking correlation τ as dependent variable and the model, realization and aggregation as factors. In addition to the variance analysis estimating the effects of these factors, we perform post-hoc pairwise comparisons to ask whether certain configurations (i.e., specific realization/aggregation) outperform the others. We conduct all analyses using 7Chatbot Arena data 8Chatbot Arena Hard Prompts 13 IBM SPSS Statistics v30.0. the Accuracy is defined as follows: The ANOVA shows that both the judge model and the realization have strong influence on τ , with an effect size (Partial Eta-Squared) of η2 = 0.81 for the judge model (p < 0.001; = 36.0), η2 = 0.51 for the realization (p < 0.001; = 26.6), and η2 = 0.78 for the interaction effect between model and realization (p < 0.001; = 10.1). In contrast, the aggregation methods were not found to have significant effect on τ (η2 = 0.02; > 0.5). We also perform Tukeys HSD (Tukey, 1949) post-hoc tests to compare the means of the variables. The analysis indicates that the both the Numeric (mean τ = 0.75; στ = 0.06) and Likert (τ = 0.74; στ = 0.07) realizations are significantly better than the Anchor (τ = 0.71; στ = 0.07) and TokenProbs (τ = 0.68; στ = 0.13) realizations (all values <= 0.002). The differences between aggregation methods are not statistically significant. Pairwise Win-Rates We denote the win-rate of system sa over system sb as R(sa, sb)p where denotes the judge upon which the win-rate was calculated, and {g}, where stands for human gold data. The win-rate of system sa over system sb according to judge jp over the set of instances is calculated as the proportion of instances where the score given by jp to the response generated by sa surpasses that of system sb, where ties are excluded. Namely Rp(sa, sb) = I(Scorep 1 k,b) Where KT a,b = {ikScorep k,b}, and I() denotes the indicator function. Notice that Rp(sa, sb) = 1 Rp(sb, sa). k,a > Scorep k,a = Scorep a,b ΣK k=1 To quantify the agreement between the judge and gold win-rates we also define an Accuracy metric. This measures the proportion of pairs where the judge pairwise system preference decisions are in agreement with those of the human gold-data. In other words, we want to count the pairs that appear in the first and third quadrants in Figure 5; namely, the pairs where the judge and gold win-rate are both bigger than 0.5, or the pairs where both are lower than 0.5, representing agreement on the winning system. For that, we denote all the pairs of systems we have in the gold data as {sam, sbm}M m=1. Now 14 Accp = 1 ΣM m=1 I(I(W Rp(sam, sbm) > 0.5) = I(W Rg(sam, sbm) > 0.5)) Additionally, we define second metric, the Mean Squared Error over all win-rate pairs. SEm = 1 ΣM m=1(W Rg(sam, sbm) Rp(sam, sbm))2. The Accp scores are in high agreement with the JuStRank judge ranking quality scores τ (Pearson correlation of = 0.96 for the BT aggregation, = 0.79 for the Mean aggregation). This highlights the direct link between judges ability to rank systems and their performance on pairwise system preference."
        },
        {
            "title": "The M SEp",
            "content": "W scores have low correlation with the JuStRank judge τ scores (r = 0.19 for the BT aggregation, = 0.07 for the Mean aggregation). This can be explained by the decisiveness effect (6.1), where judges deviate substantially from the gold win-rate, but mostly toward the stronger system in the pair."
        },
        {
            "title": "F Beta Distribution Fit",
            "content": "Following Kull et al. (2017), we model the relation between judge and gold win-rates using the cumulative distribution function (CDF) of the Beta distribution. We parameterize the distribution such that both shape parameters α and β are equal (α = β). The CDF of the Beta distribution, defined over the interval [0, 1], for α = β [0, ] provides wide range of function fits: linear = fit for α = 1, sigmoidal fit for larger α values, and approaching step function as α . These attributes make it particularly suited for our data characteristics. points data set {(W Rp(sam, sbm), Rg(sam, sbm)}M m=1, where Rp(sam, sbm) [0, 1] represents the judge win-rate and Rg(sam, sbm) [0, 1] denotes the gold win-rate between system, sam and sbm. We fit the Beta CDF by optimizing the shape parameter α. The optimization objective is minimizing the sum of absolute errors (SAE) between the judge win-rate, Rp(sam, sbm), and the predicted values from the Beta CDF. In order to capture the behavior across the entire range of win-rates, we"
        },
        {
            "title": "Given",
            "content": "of weight the errors by the distance of Rp from 0.5: SAE = (cid:88) m=1 γ(W Rp(sam, sbm)) (cid:12) (cid:12) Rp(sam, sbm) (cid:12) (cid:12) FBeta(W Rg(sam, sbm); α) (cid:12) (cid:12) (cid:12) (cid:12) where FBeta(x; α) denotes the Beta CDF with shape parameters α = β, and γ is the distance of Rp from 0.5. The optimization was performed using the scipy.optimize.minimize9 function, with the parameter (α) constrained to reasonable range [0.1, 10000]. This approach efficiently identified the best-fit parameter (α). The resulting Beta CDF closely captures the empirical data distribution, as validated both quantitatively, through low SAE, and qualitatively via visual inspection. Figure 11 depicts the fitted Beta CDF curve and the observed data points, demonstrating the effectiveness of this approach for modeling the judges predicted win-rate distribution. 9SciPy Documentation for scipy.optimize.minimize"
        },
        {
            "title": "G LLM Judge Prompts",
            "content": "Below we list the prompts we use for each LLM judge realization (4.2.2). Numeric Here is user input and model response. On scale of 0 to 100, to what extent is this good response for the given input? Reply with your rating score without any preceding explanation. Input: [user instruction] Response: [system response] Rating (0-100): Likert Here is user input and model response. To what extent is this good response for the given input? Provide rating from one of the following choices: Very Bad, Bad, Mediocre, Good, Very Good. Reply using the format of [[rating]], for example: [[Mediocre]] Input: [user instruction] Response: [system response] Rating:"
        },
        {
            "title": "TokenProbs",
            "content": "Here is user input and model response. Is this good response for the given input? Answer with only yes/no. Input: [user instruction] Response: [system response] Good response? (Yes/No): 16 Anchor Here is user input and responses from two assistants, and B. Which response is better? You must output only one of the following choices as your final verdict with label: 1. Assistant is significantly better: [[A>>B]] 2. Assistant is slightly better: [[A>B]] 3. Tie, relatively the same: [[A=B]] 4. Assistant is slightly better: [[B>A]] 5. Assistant is significantly better: [[B>>A] Example output: \"My final verdict is tie: [[A=B]]\". <User Prompt> [user instruction] <The Start of Assistant As Answer> [system response] <The End of Assistant As Answer> <The Start of Assistant Bs Answer> [anchor system response] <The End of Assistant Bs Answer> Final Verdict: 17 Judge Model Realization Aggregation Agreement (τ ) w/ Gold Ranking Qwen2.5-72B-Instruct URM-LLaMa-3.1-8B GPT-4o-2024-11-20 URM-LLaMa-3.1-8B Qwen2.5-72B-Instruct URM-LLaMa-3.1-8B Qwen2.5-72B-Instruct GPT-4o-2024-11-20 Qwen2.5-72B-Instruct Llama-3-1-405b-instruct-fp8 Llama-3-1-405b-instruct-fp8 Mistral-large-instruct-2407 GPT-4o-2024-11-20 Mistral-large-instruct-2407 URM-LLaMa-3.1-8B GPT-4o-mini-2024-07-18 Llama-3-1-405b-instruct-fp8 GPT-4o-mini-2024-07-18 Mistral-large-instruct-2407 Qwen2.5-72B-Instruct ArmoRM-Llama3-8B-v0.1 Qwen2.5-72B-Instruct GPT-4o-mini-2024-07-18 Llama-3-1-70b-instruct Llama-3-1-70b-instruct Mistral-large-instruct-2407 Qwen2.5-72B-Instruct Llama-3-1-405b-instruct-fp8 Llama-3-1-70b-instruct GPT-4o-mini-2024-07-18 ArmoRM-Llama3-8B-v0.1 Llama-3-1-405b-instruct-fp8 Mistral-large-instruct-2407 Skywork-Llama-3.1-8B-v0.2 Qwen2.5-72B-Instruct Mistral-large-instruct-2407 GPT-4o-mini-2024-07-18 Skywork-Llama-3.1-8B-v0.2 Llama-3-1-405b-instruct-fp8 Skywork-Llama-3.1-8B-v0.2 Llama-3.1-8B-Instruct Qwen2.5-72B-Instruct Llama-3.1-8B-Instruct Mixtral-8x22B-instruct-v0.1 Llama-3-1-70b-instruct GPT-4o-2024-11-20 GPT-4o-mini-2024-07-18 Qwen2.5-72B-Instruct Likert Reward Anchor Reward Likert Reward Numeric Anchor Numeric Numeric Numeric Likert Anchor Numeric Reward Numeric Numeric Numeric Numeric Likert Reward Anchor Likert Numeric Numeric Likert Anchor Likert TokenProbs Likert Reward Likert Anchor Reward Anchor Likert Numeric Reward Likert Reward TokenProbs TokenProbs TokenProbs Numeric TokenProbs Numeric Likert Numeric 18 Win-Rate Mean Mean BT BT Win-Rate BT Win-Rate Win-Rate Mean Win-Rate BT BT BT Median Win-Rate BT BT Win-Rate Mean Mean Mean BT Win-Rate BT Win-Rate BT Win-Rate Win-Rate Win-Rate Median BT Win-Rate Mean Win-Rate Mean Mean Win-Rate Mean BT Mean BT Median BT Median BT Mean Mean .827 .823 .822 .819 .817 .816 .814 .814 .813 .812 .812 .811 .809 .809 .809 .807 .805 .804 .802 .801 .800 .799 .798 .798 .798 .798 .794 .793 .793 .793 .793 .787 .786 .786 .786 .782 .781 .780 .780 .778 .778 .777 .776 .776 .776 .774 .773 .773 GPT-4o-2024-11-20 GPT-4o-2024-11-20 Llama-3-OffsetBias-RM-8B Llama-3-1-70b-instruct Llama-3-OffsetBias-RM-8B Skywork-Llama-3.1-8B-v0.2 Llama-3-1-70b-instruct Mistral-large-instruct-2407 Llama-3-1-70b-instruct ArmoRM-Llama3-8B-v0.1 ArmoRM-Llama3-8B-v0.1 Llama-3-OffsetBias-RM-8B GPT-4o-mini-2024-07-18 GPT-4o-2024-11-20 Llama-3-OffsetBias-RM-8B Mixtral-8x22B-instruct-v0.1 GPT-4o-mini-2024-07-18 Qwen2.5-72B-Instruct Mistral-large-instruct-2407 Llama-3-70b-instruct Qwen2.5-72B-Instruct Llama-3-1-405b-instruct-fp8 Llama-3-1-70b-instruct GPT-4o-2024-11-20 Llama-3.1-8B-Instruct Llama-3-1-405b-instruct-fp8 Llama-3.1-8B-Instruct Llama-3-1-405b-instruct-fp8 GPT-4o-mini-2024-07-18 Mixtral-8x22B-instruct-v0.1 GPT-4o-2024-11-20 Llama-3-1-405b-instruct-fp8 Llama-3.1-8B-Instruct Llama-3-70b-instruct Llama-3-1-405b-instruct-fp8 Llama-3-1-70b-instruct Mixtral-8x22B-instruct-v0.1 Qwen2.5-72B-Instruct Internlm2-7b-reward Llama-3-1-405b-instruct-fp8 Mistral-large-instruct-2407 Internlm2-20b-reward Mistral-large-instruct-2407 Internlm2-20b-reward GPT-4o-mini-2024-07-18 Llama-3.1-8B-Instruct Llama-3-1-70b-instruct Internlm2-7b-reward Mixtral-8x22B-instruct-v0.1 Internlm2-7b-reward Internlm2-20b-reward Mixtral-8x22B-instruct-v0. Likert Numeric Reward TokenProbs Reward Reward TokenProbs Anchor Numeric Reward Reward Reward TokenProbs Likert Reward Numeric TokenProbs TokenProbs Numeric Numeric TokenProbs Anchor Likert Likert TokenProbs Anchor TokenProbs TokenProbs TokenProbs Likert Numeric TokenProbs Likert Numeric TokenProbs Likert Likert TokenProbs Reward Anchor TokenProbs Reward Anchor Reward TokenProbs Likert Likert Reward Likert Reward Reward TokenProbs 19 BT Win-Rate Win-Rate BT BT Median Mean Mean Mean BT Win-Rate Median Win-Rate Win-Rate Mean Win-Rate BT Median Mean BT Win-Rate Win-Rate Mean Mean Win-Rate Mean BT Win-Rate Mean BT Mean Median Mean Win-Rate Mean Win-Rate Win-Rate Mean Mean BT Mean Mean BT Median Median BT BT Median Mean Win-Rate BT Win-Rate .773 .771 .765 .765 .765 .764 .764 .764 .764 .763 .762 .759 .759 .758 .757 .756 .752 .752 .750 .749 .748 .748 .746 .744 .744 .744 .741 .741 .741 .738 .738 .737 .736 .733 .733 .732 .732 .732 .731 .730 .730 .728 .725 .724 .723 .723 .722 .721 .719 .717 .717 .717 Llama-3-1-70b-instruct GRM-Llama3.2-3B Internlm2-20b-reward Mixtral-8x22B-instruct-v0.1 Llama-3-1-70b-instruct GRM-Llama3.2-3B Internlm2-7b-reward GRM-Llama3.2-3B GRM-Llama3.2-3B GPT-4o-2024-11-20 Llama-3-70b-instruct Mixtral-8x22B-instruct-v0.1 GPT-4o-2024-11-20 GPT-4o-2024-11-20 Llama-3-70b-instruct Llama-3-70b-instruct GPT-4o-2024-11-20 Llama-3.1-8B-Instruct Llama-3.1-8B-Instruct Llama-3-1-70b-instruct Llama-3-70b-instruct Llama-3.1-8B-Instruct Llama-3-70b-instruct Llama-3.1-8B-Instruct GPT-4o-mini-2024-07-18 Llama-3-1-405b-instruct-fp8 Llama-3.1-8B-Instruct GPT-4o-mini-2024-07-18 Llama-3-70b-instruct Llama-3-70b-instruct Mixtral-8x22B-instruct-v0.1 Llama-3-70b-instruct GPT-4o-mini-2024-07-18 Mixtral-8x7B-instruct-v0.1 Mixtral-8x7B-instruct-v0.1 Mixtral-8x22B-instruct-v0.1 Eurus-RM-7b Eurus-RM-7b Mixtral-8x22B-instruct-v0.1 Llama-3.1-8B-Instruct Llama-3-70b-instruct Llama-3-70b-instruct Llama-3.1-8B-Instruct Eurus-RM-7b Eurus-RM-7b Mixtral-8x7B-instruct-v0.1 Mixtral-8x7B-instruct-v0.1 Mixtral-8x7B-instruct-v0.1 Mixtral-8x22B-instruct-v0.1 Mixtral-8x7B-instruct-v0.1 Mixtral-8x7B-instruct-v0.1 Mixtral-8x7B-instruct-v0.1 Anchor Reward Reward Numeric Anchor Reward Reward Reward Reward TokenProbs Numeric TokenProbs TokenProbs TokenProbs Likert TokenProbs TokenProbs Anchor Likert Anchor Likert Numeric Likert Anchor Anchor TokenProbs Numeric Anchor Anchor TokenProbs Anchor TokenProbs Anchor Numeric Anchor TokenProbs Reward Reward Anchor Anchor Anchor Anchor Numeric Reward Reward Numeric Numeric Anchor Anchor Anchor Likert Likert 20 Win-Rate Mean Win-Rate Mean Mean Win-Rate BT BT Median Median Mean BT Mean BT BT Win-Rate Win-Rate Mean Win-Rate BT Win-Rate Mean Mean BT Mean BT BT Win-Rate Mean Mean Mean BT BT BT BT Mean Median Mean BT Win-Rate Win-Rate BT Win-Rate Win-Rate BT Win-Rate Mean Win-Rate Win-Rate Mean BT Mean .716 .716 .716 .715 .714 .712 .712 .711 .706 .704 .704 .702 .701 .700 .698 .696 .696 .695 .694 .688 .681 .680 .678 .677 .675 .672 .668 .668 .667 .666 .665 .663 .659 .656 .655 .650 .643 .641 .641 .639 .638 .633 .632 .629 .628 .626 .626 .622 .612 .610 .590 .585 Mixtral-8x7B-instruct-v0.1 Mixtral-8x7B-instruct-v0.1 Mistral-large-instruct-2407 Mixtral-8x7B-instruct-v0.1 Mixtral-8x7B-instruct-v0.1 Mistral-large-instruct-2407 Mistral-large-instruct-2407 Likert TokenProbs TokenProbs TokenProbs TokenProbs TokenProbs TokenProbs Win-Rate BT Win-Rate Mean Win-Rate BT Median .543 .427 .417 .411 .371 .369 .363 Table 2: Judges by ranking performance. The judges are sorted by the Kendalls Tau correlation between their overall system ranking and the gold ranking from Chatbot Arena (4.4). Figure 8: Comparison to RewardBench. The plot depicts the relative performance of judges present in both JuStRank and RewardBench (Lambert et al., 2024). For comparison, we perform Min-Max normalization over the judge performance scores (accuracy for RewardBench, Kendalls Tau for our results). The results shown are for the BT aggregation method; the LLM judges use the Anchor realization, which is closest to the setting in RewardBench. Each panel portrays different subset of RewardBench. 21 Figure 9: Judge Correlations. Kendalls Tau correlations between the system rankings produced by the different judge realizations, using the BT aggregation method. The first row/column denotes correlations with the reference ranking from Chatbot Arena."
        },
        {
            "title": "Judge",
            "content": "Self-bias Significance p-value 0.05 GPT-4o-mini-2024-07-18 (Anchor) 0.04 GPT-4o-mini-2024-07-18 (Likert) +0.03 GPT-4o-mini-2024-07-18 (Numeric) +0.06 GPT-4o-mini-2024-07-18 (TokenProbs) 0.05 Llama-3-1-70b-instruct (Anchor) +0.16 Llama-3-1-70b-instruct (Likert) 0.00 Llama-3-1-70b-instruct (Numeric) 0.03 Llama-3-1-70b-instruct (TokenProbs) +0.09 Llama-3-70b-instruct (Anchor) +0.15 Llama-3-70b-instruct (Likert) +0.14 Llama-3-70b-instruct (Numeric) 0.01 Llama-3-70b-instruct (TokenProbs) 0.07 Llama-3.1-8B-Instruct (Anchor) 0.04 Llama-3.1-8B-Instruct (Likert) +0.02 Llama-3.1-8B-Instruct (Numeric) 0.04 Llama-3.1-8B-Instruct (TokenProbs) 0.07 Mistral-large-instruct-2407 (Anchor) +0.02 Mistral-large-instruct-2407 (Likert) +0.06 Mistral-large-instruct-2407 (Numeric) Mistral-large-instruct-2407 (TokenProbs) +0.01 > 0.5 (N.S.) 0.13 (N.S.) 7.1e 03 4.7e 04 8.4e 08 1.8e 13 > 0.5 (N.S.) > 0.5 (N.S.) 0.33 (N.S.) > 0.5 (N.S.) Table 3: Judge self-bias. The table shows the self-bias values for LLM judge realizations, i.e., the value of (6.2) where the LLM judge and system sa correspond to the same underlying LLM. the corrected bias sa For positive self-bias values we test the statistical significance using paired t-tests (one-sided, with Bonferroni correction). N.S.: non-significant (p > 0.05). 22 (a) (b) Figure 10: System-specific judge biases. The heat maps depict the win-rate biases of various judges towards specific systems (6.2), with respect to the ground-truth win-rates from Chatbot Arena. (a): Bias w.r.t. the raw ground-truth win-rates Rg; (b): Bias w.r.t. the fit value for the gold win-rate Rg on the beta distribution fit (App. F) for each judge. 23 Figure 11: Beta distribution fit of pairwise win-rates (Part 1/4) 24 Figure 11: Beta distribution fit of pairwise win-rates (Part 2/4) 25 Figure 11: Beta distribution fit of pairwise win-rates (Part 3/4) 26 Figure 11: Beta distribution fit of pairwise win-rates (Part 4/4). Each point represents the win-rate between pair of systems, R(sa, sb); the curve and α value describe fit to the beta probability distribution. Refer to Appendix for details. 27 Figure 12: Judge score distributions (Part 1/3) 28 Figure 12: Judge score distributions (Part 2/3) 29 Figure 12: Judge score distributions (Part 3/3). 30 Judge Model Realization Agreement with Gold τ Decisiveness Bias δ α TokenProbs Reward URM-LLaMa-3.1-8B Likert Qwen2.5-72B-Instruct Numeric Qwen2.5-72B-Instruct Likert Mistral-large-instruct-2407 Anchor GPT-4o-2024-11-20 Mistral-large-instruct-2407 Numeric Llama-3-1-405b-instruct-fp8 Numeric Numeric GPT-4o-mini-2024-07-18 Likert GPT-4o-mini-2024-07-18 Numeric Llama-3-1-70b-instruct Qwen2.5-72B-Instruct Anchor Llama-3-1-405b-instruct-fp8 Likert Skywork-Llama-3.1-8B-v0.2 Reward Qwen2.5-72B-Instruct Mixtral-8x22B-instruct-v0.1 Numeric Numeric GPT-4o-2024-11-20 Likert GPT-4o-2024-11-20 TokenProbs Llama-3-1-70b-instruct Reward Llama-3-OffsetBias-RM-8B Reward ArmoRM-Llama3-8B-v0.1 TokenProbs GPT-4o-mini-2024-07-18 Numeric Llama-3-70b-instruct TokenProbs Llama-3.1-8B-Instruct Mixtral-8x22B-instruct-v0.1 Likert Llama-3-1-405b-instruct-fp8 Anchor Anchor Mistral-large-instruct-2407 Likert Llama-3.1-8B-Instruct Likert Llama-3-1-70b-instruct Reward Internlm2-20b-reward Reward Internlm2-7b-reward Reward GRM-Llama3.2-3B TokenProbs Mixtral-8x22B-instruct-v0.1 TokenProbs GPT-4o-2024-11-20 Likert Llama-3-70b-instruct Anchor Llama-3-1-70b-instruct Llama-3.1-8B-Instruct Anchor Llama-3-1-405b-instruct-fp8 TokenProbs Numeric Llama-3.1-8B-Instruct TokenProbs Llama-3-70b-instruct Anchor GPT-4o-mini-2024-07-18 Numeric Mixtral-8x7B-instruct-v0.1 Mixtral-8x7B-instruct-v0.1 Anchor Mixtral-8x22B-instruct-v0.1 Anchor Anchor Llama-3-70b-instruct Reward Eurus-RM-7b Likert Mixtral-8x7B-instruct-v0.1 TokenProbs Mixtral-8x7B-instruct-v0.1 TokenProbs Mistral-large-instruct- .819 .817 .814 .811 .809 .809 .805 .804 .798 .798 .794 .787 .778 .777 .776 .774 .773 .765 .765 .763 .752 .749 .741 .738 .730 .725 .723 .722 .717 .712 .711 .702 .700 .698 .688 .677 .672 .668 .663 .659 .656 .655 .641 .633 .628 .590 .427 .369 1.84 4.76 4.09 5.47 3.07 3.01 4.33 2.91 4.61 2.69 2.93 5.22 2.46 2.69 2.12 2.15 5.49 1.26 1.39 1.84 2.10 1.27 .598 2.53 3.58 2.13 .935 3.90 1.90 2.35 2.30 1.85 2.22 2.40 2.71 .868 1.55 1.20 .775 1.41 1.27 1.17 1.50 1.82 2.49 .838 .739 1.17 .085 .079 .079 .086 .085 .082 .087 .077 .087 .087 .090 .097 .100 .082 .089 .077 .089 .070 .076 .092 .084 .084 .061 .108 .112 .111 .090 .120 .098 .113 .114 .088 .093 .122 .126 .085 .092 .104 .071 .111 .102 .102 .140 .132 .138 .110 .107 .123 Table 4: Judge characteristics. The table presents three measures for each judge realization: an overall ranking quality τ (5, Kendalls Tau correlation with the Chatbot Arena gold ranking), decisiveness score α (6.1, App. F), and its propensity for system-specific biases δ (6.2). Correlations τ shown are for the BT aggregation method; α and δ are calculated on the judge scores before aggregation. : Lower is better."
        }
    ],
    "affiliations": [
        "IBM Research"
    ]
}