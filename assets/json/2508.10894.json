{
    "paper_title": "MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data",
    "authors": [
        "Antoine Labatie",
        "Michael Vaccaro",
        "Nina Lardiere",
        "Anatol Garioud",
        "Nicolas Gonthier"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-supervised learning holds great promise for remote sensing, but standard self-supervised methods must be adapted to the unique characteristics of Earth observation data. We take a step in this direction by conducting a comprehensive benchmark of fusion strategies and reconstruction target normalization schemes for multimodal, multitemporal, and multispectral Earth observation data. Based on our findings, we propose MAESTRO, a novel adaptation of the Masked Autoencoder, featuring optimized fusion strategies and a tailored target normalization scheme that introduces a spectral prior as a self-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO sets a new state-of-the-art on tasks that strongly rely on multitemporal dynamics, while remaining highly competitive on tasks dominated by a single mono-temporal modality. Code to reproduce all our experiments is available at https://github.com/ignf/maestro."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 4 9 8 0 1 . 8 0 5 2 : r MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data Antoine Labatie 1 Michael Vaccaro 1 Nina Lardiere 1 Anatol Garioud 1 Nicolas Gonthier 1,2 1 Institut national de linformation géographique et forestière (IGN), France 2 Univ Gustave Eiffel, ENSG, IGN, LASTIG, France {firstname.lastname}@ign.fr Figure 1. Overview of MAESTRO. MAESTRO extends the Masked Autoencoder to orchestrate the complex interplay of multimodal, multitemporal, and multispectral Earth Observation data. It employs token-based early fusion across time steps and similar modalities, and token-based late fusion across dissimilar modalities. It uses joint-token fusion for multispectrality, but still relies on novel normalization of reconstruction targetsnamely, patch-group-wise within groups of highly correlated bandsto inject useful spectral prior during pretraining. Best viewed in color."
        },
        {
            "title": "Abstract",
            "content": "Self-supervised learning holds great promise for remote sensing, but standard self-supervised methods must be adapted to the unique characteristics of Earth observation data. We take step in this direction by conducting comprehensive benchmark of fusion strategies and reconstruction target normalization schemes for multimodal, multitemporal, and multispectral Earth observation data. Based on our findings, we propose MAESTRO, novel adaptation of the Masked Autoencoder, featuring optimized fusion strategies and tailored target normalization scheme that introduces spectral prior as self-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO sets new state-of-the-art on tasks that strongly rely on multitemporal dynamics, while remaining highly competitive on tasks dominated by single monotemporal modality. Code to reproduce all our experiments is available at https://github.com/ignf/maestro. 1. Introduction Self-supervised learning (SSL) has been central to recent breakthroughs in natural language processing [16, 63] and computer vision [34, 96]. It is an effective pre-training strategy, improving versatility and data efficiency when finetuning models on diverse downstream tasksespecially in the case of foundation models pre-trained on large amounts of unlabeled data. The SSL paradigm is promising for Earth Observation (EO) applications, where unlabeled data is abundant, but labeled data remains scarce and costly. However, to fully leverage the potential of SSL, it may be necessary to adapt off-the-shelf approaches to the unique characteristics of EO data. key characteristic of EO data lies in its heterogeneity across resolutions, scales, modalities, and spectral bands [65]. This heterogeneity is frequently reflected in multimodal, multitemporal, and multispectral EO datasets, where the integration of various sensors effectively addresses the spatial-temporal-spectral resolution dilemma [2]. Our goal in this work is to efficiently adapt the canonical SSL framework of masked autoencoding to the specific characteristics of EO data to learn useful and versatile representations. We adapt the Masked Autoencoder (MAE) [34], which has shown strong performance and computational efficiency on natural images. However, extending the MAE to multimodal, multitemporal, and multispectral EO data introduces several challenges. First, integrating multiple 1 modalities and temporal observations requires an effective fusion strategy. One approach, joint-token fusion, projects all data dimensions into shared token space. An alternative, token-based fusion, first clusters data dimensions, embeds them separately, and fuses the resulting tokens. Token-based fusion appears more suited for handling multimodality and multitemporality for several reasons: (i) it better captures the heterogeneity across modalities and time steps, (ii) it allows the integration of modalityand temporality-specific encodings, and (iii) it enables the use of cross-modal and cross-temporal self-supervisory signals. Yet, it remains unclear whether fusion should be applied early or late in the model. Second, handling multispectral data adds further complexity. Here, the choice between token-based and jointtoken fusion is less obvious. While joint-token fusion offers computational efficiency, it limits the ability to incorporate cross-spectral self-supervisory signals. key question, then, is whether we can preserve the computational benefits of joint-token fusion while reintroducing an effective prior on the multispectral structurepotentially through carefully designed normalization of reconstruction targets. We investigate these questions in Sec. 4.3 and, based on our findings, introduce our proposed approach, MAESTRO, in Sec. 4.4. MAESTRO extends the MAE framework by applying token-based early fusion across time steps and similar modalities, and token-based late fusion across dissimilar modalities. For multispectrality, MAESTRO employs joint-token early fusion combined with novel normalization scheme for reconstruction targets. Our main contributions are as follows: We extensively benchmark various fusion strategies and target normalization schemes for multimodal, multitemporal, and multispectral SSL in EO. We introduce novel patch-group-wise normalization method that normalizes reconstruction targets patch-wise within groups of highly correlated spectral bands. This approach injects useful spectral prior into the selfsupervised signal at negligible computational cost. Building on the above, we propose MAESTRO, tailored adaptation of the MAE framework which effectively orchestrates the use of multimodal, multitemporal, and multispectral EO databoth in terms of performance and computational cost. We validate MAESTRO on four diverse EO benchmarks, demonstrating that it achieves state-of-the-art results on tasks strongly tied to multitemporal dynamics, while remaining highly competitive on others. 2. Related Work SSL Approaches and Models for EO. SSL has emerged as promising approach for EO, due to the abundance of unlabeled remote sensing data. Initially, the development of SSL approaches for EO followed those from the natural images domain. Early work focused primarily on contrastive SSL [6, 14, 39, 42, 52, 54, 61, 73, 88, 90, 91, 94], often using convolutional neural networks [6, 39, 52, 54, 61, 73, 90, 94]. Positive pairs were typically derived from different modalities, dates, or augmented views at the same location. More recent works have increasingly shifted toward generative SSL using Transformers, well-suited to masked autoencoding. These include approaches inspired by MAE [9, 11, 13, 35, 37, 40, 48, 49, 58, 64, 71, 72, 77, 80, 83 86, 92, 93, 97, 100] or SimMIM [17, 33, 36, 55, 68, 70], or custom generative approaches [20, 21, 67, 87, 89]. Some recent works also explore hybrid contrastive/generative approaches [18, 57] or latent-space generative SSL [4, 5, 12, 45, 78, 82]. Multimodal and Multitemporal SSL for EO. Recent studies in EO have begun to address multimodal [8, 9, 14, 24, 33, 36, 37, 39, 41, 58, 61, 62, 82, 88, 90, 93, 97] or multitemporal SSL [13, 21, 40, 59, 71], but rarely both simultaneously. When multimodality is included, it is typically handled through parameter sharing or late fusion [7, 33, 37, 58, 62, 88, 93, 97]. To date, most published EO foundation models support multimodal or multitemporal inputs only via late fusion, which limits representation learning and downstream performance (see Sec. 4.3). Notable exceptions include Presto [77], Galileo [78], OmniSat [4], AnySat [5], SeaMo [46], EarthMAE [80], SkySense v1/v2 [32, 101], and SkySense++ [95]. Among these, Presto processes data pixel-wise without spatial context; OmniSat and AnySat use joint-token early fusion (via LTAE [29]) across time steps; Galileo applies token-based early fusion across all time steps and modalities. In contrast, MAESTRO adopts token-based early fusion across time steps and similar modalities only, avoiding the inefficiencies of sharing encoders across dissimilar modalities. Despite the importance of fusion design in multimodal and multitemporal SSL (see Sec. 4.3), to our knowledge only one prior work [46] has benchmarked different fusion strategiesand even then, multitemporal fusion was limited to SSL pre-training and excluded fine-tuning evaluation. Multispectral SSL for EO. Various SSL studies in EO have incorporated SAR [36, 42, 45, 49] or multispectral data [13, 15, 21, 37, 40, 54, 55, 62, 67, 68, 71, 73, 94] or both [4, 5, 79, 14, 24, 33, 39, 41, 43, 47, 58, 61, 77, 78, 80, 82, 88, 9093, 97]. However, only few have explicitly encoded spectral priors into the model architecture or the self-supervisory signal. Some prior works have used token-based multispectral fusion by assigning separate tokens to each spectral band [9, 62] or to groups of spectral bands [13, 37, 77, 78]. While these designs can help mitigate early information bottlenecks, enhance modeling capacity, and enable cross-spectral self-supervisory signals, they come at the cost of higher computational demands. 2 handful of studies have explored the normalization of reconstruction targets for multispectral data, but their approaches have been restricted to patch-wise normalization without modeling the underlying spectral structure [58, 80]. In contrast, we normalize reconstruction targets across groups of highly correlated spectral bands, effectively injecting spectral prior into the self-supervisory signal. This approach enhances spectral representation learning without increasing the token count and naturally extends to SAR inputs via analogous band-grouping strategies. 3. Approach 3.1. Architecture In this section, we describe our MAE-derived approach specifically tailored to EO data. Our approach accommodates the heterogeneous nature of EO data, notably its multimodal, multitemporal, and multispectral characteristics. We describe our approach in the context of fixed dataset D, composed of set of modalities M. For each dataset tile, every modality is associated with an input tensor of shape Im Im Tm Cm, where Im denotes the spatial size, Tm 1 the number of original time steps, and Cm the number of channels. We assume that Cm and Im remain constant across all tiles, while Tm may vary. This variability reflects the nature of satellite time series, which may span fixed duration but with location-dependent revisit intervalssuch as with Sentinel-1 or Sentinel-2. Multitemporal Discretization. Our preprocessing pipeline begins by enforcing fixed tensor shape for the model inputs. For each modality m, we define target number of discretized time steps Dm. For modality m, the original input tensor of shape Im Im Tm Cm is reduced to tensor of shape Im Im Dm Cm through two-step process: (i) temporal binning, where the sequence is reshaped into Dm bins, each covering Tm/Dm time steps (after random truncation, if needed); and (ii) time step selection within each bin. During training, time step selection within each bin is random to introduce data augmentation. During validation and testing, the selection aims to maximize the representativeness of each bin. For more information, refer to SM Sec. 6.2.1. Patchification/Unpatchification. Once the inputs from different modalities have been reduced to fixed shape, they are passed to modality-specific tokenizers. These tokenizers are shared across time steps within given modality but remain distinct across modalities. Tokenization is applied independently at each time step. We adopt the standard Vision Transformer (ViT) patchification strategy [19], where for any modality and time step, inputs are first partitioned spatially into nonoverlapping patches of size Pm. Each patch is then flattened into vector of shape 2 mCm and projected into patch embedding of dimension Ce before the encoder. All spectral bands are jointly projected into the same tokens, implementing joint-token multispectral fusion. During SSL pre-training, we apply transposed unpatchification to convert decoded tokens of shape Cd after the decoder back into reconstructed flattened patches of shape 2 mCm. For each modality m, the set of patch positions Pm has cardinality Pm = (Im/Pm)2, while the set of temporal bins Tm has cardinality Tm = Dm. Importantly, we do not assume that all modalities share the same number of spatial positions or temporal binsthat is, their spatial and temporal resolutions may differ. For example, modalities such as Sentinel-1 and Sentinel-2 may employ coarser spatial grids but finer temporal grids, whereas modalities such as aerial and SPOT 67 may use finer spatial grids with coarser temporal resolution. Maintaining these coarse or fine resolutions throughout the tokenization process helps prevent information bottlenecks in the tokenizersthe models entry blocks. Following tokenization, we add (i) two-dimensional spatial positional encodings based on ground sampling distance, as in ScaleMAE [64], and (ii) sine-cosine temporal encodings, as used in SatMAE [13]. We do not include explicit modality encodings, as modality-specific tokenizers and learnable modality-specific [mask] tokens implicitly encode the source or target modality associated with each token. For more information, refer to SM Sec. 6.2.2. Multimodal and Multitemporal Token-Based Fusion. At this stage, we obtain embedded inputs in the form of tensors of shape Im/Pm Im/Pm Dm Ce for each modality m. As in the original MAE [34], these tensors are processed in two stages: Transformer encoder processes only the visible tokens, while Transformer decoder processes the encoded tokens concatenated with [mask] tokens. However, the original MAE workflow was designed for monomodal, monotemporal data, and there remains ambiguity on how it should be extended to support multimodal and multitemporal data. This raises key questions: Should information from different modalities and time steps be fused via early fusion or late fusion? Should encoder and decoder parameters be shared across modalities or kept independent? To answer these questions, we explore five different fusion modes, as illustrated in Fig. 2: Mode shared: Late fusion across modalities and time steps. Parameters are shared across all modalities. Mode monotemp: Same as shared, but with parameters kept independent for each modality. Mode mod: Late fusion across modalities but early fusion across time steps. Parameters are kept independent for each modality. 3 Figure 2. Token-based fusion modes for handling multimodality and multitemporality. Modes shared and monotemp involve late fusion across modalities and time steps, with parameters either shared across modalities (shared) or kept independent for each modality (monotemp). Mode group involves late fusion across predefined groups of modalities but early fusion across time steps and within each group. Mode inter-group extends group by replacing the final encoder blocks with fusion blocks that enable cross-group token interactions. Mode mod is special case of group with late fusion across all modalities, but early fusion across time steps. Mode group: Late fusion across predefined groups of modalities but early fusion across time steps and within Parameters are kept independent across each group. groups. Mode inter-group: Similar to group, but with the last three encoder blocks replaced by fusion blocks enabling cross-group token interactions. Modalities from different groups are subject to intermediate fusion rather than late fusion. Note that, strictly speaking, the term late fusion applies only to fine-tuning, where pooling occurs before the prediction heads. In SSL pre-training, no such pooling occurs before the decoder. 3.2. Pretext Task Multispectral Patch Normalization. Our tokenizers jointly encode all spectral bands of each modality into shared tokens, implementing joint-token multispectral fusion. While this design improves efficiency, we still aim to embed meaningful spectral priors into the reconstruction task. To achieve this, we extend the patch-wise target normalization strategy, originally proposed in MAE [34] and later adopted in several EO-focused works [35, 58, 80, 83], with patch-group-wise target normalization strategy. Specifically, for each modality m, we consider partitioning the spectral bands into set of band groups Gm, and normalizing the targets independently for each patch and each band group. Using the same notations as previously, let xm, xrec RIm/PmIm/PmDmP 2 mCm denote the patch-level representations of unnormalized and reconstructed targets for modality m, respectively. The reconstruction lossbased on an L1 lossthen depends on the chosen normalization strategy: No normalization: Targets xm are used as-is: = (cid:80) 1 PmTm (cid:88) m,p,t (cid:13) (cid:13)xm,p,t xrec m,p,t (cid:13) (cid:13)1 ; Patch-wise normalization: Targets are normalized independently per patch across all bands [34, 35, 58, 80, 83]: ˆL = 1 PmTm (cid:80) (cid:88) m,p,t (cid:13) (cid:13)ˆxm,p,t xrec m,p,t (cid:13) (cid:13)1 m, p, : ˆxm,p,t = xm,p,t µ(xm,p,t) σ(xm,p,t) , ; Patch-group-wise normalization (see Fig. 1): Targets are normalized independently per patch and band group: ˆLgrp = 1 PmTmGm (cid:80) (cid:88) m,p,t,g (cid:13) (cid:13)ˆxgrp m,p,t,g xrec m,p,t,g (cid:13) (cid:13)1 m, p, t, : ˆxgrp m,p,t,g = xm,p,t,g µ(xm,p,t,g) σ(xm,p,t,g) , . Our patch-group-wise target normalization generalizes the original patch-wise normalization; specifically, when all bands are grouped into single set, it reduces to the original strategy used in [34, 35, 58, 80, 83]. We find that applying patch-group-wise normalization with well-chosen spectral groups Gm, composed of highly correlated bands, yields strong performance. As shown in Sec. 4.3, this target normalization allows jointtoken fusion to matchand sometimes even surpassthe performance of token-based fusion, while being significantly more computationally efficient. Masking. Another aspect that must be adapted to heterogeneous data is the masking strategy. Our goal is to maintain fixed masking ratio within the token sets processed by each encoder and decoder. However, in the case of heterogeneous modalities, it remains ambiguous how this masking should be applied across the different token sets. We choose two-stage approach: (i) applying structured masking over modality, spatial, and temporal dimensions, and (ii) performing an unstructured adjustment to meet an overall masking ratio of 75%. Full details and comparisons with masking strategies from previous works are given in SM Sec. 6.3.1. 3.3. Downstream Tasks 4.2. Datasets Classification/Segmentation Heads. Following the MAE framework, we transfer only the encoders after SSL pre-training, mitigating domain shift effects related to [mask] tokens [16, 50]. As result, the encoder outputs are feature tensors of shape Im/Pm Im/Pm Dm Ce for each modality m. We then attach classification and segmentation heads to process these encoded features (see Fig. 1): Classification heads operate as follows: (i) Concatenate tokens across all modalities m, spatial positions p, and time steps t; (ii) Apply attentive pooling [22] to aggregate the concatenated tokens; (iii) Apply dense layer with output dimension equal to the number of target classes. Segmentation heads first align all modality-specific encoded tensors to common spatial token grid of reference. Then, for each spatial position p, they operate as follows: (i) Concatenate tokens across all modalities and time steps t; (ii) Apply attentive pooling to aggregate the concatenated tokens; (iii) Apply dense layer with output dimension equal to the number of target classes. 4. Experiments 4.1. Workflow Our workflow varies depending on the evaluated models. With MAEs, we follow three-phase workflow applied sequentially on the same dataset: 1. Self-supervised pre-training: The model learns representations from unlabeled data without supervision. 2. Probing: task-specific head is trained on top of the frozen pre-trained backbone to assess the quality of the learned representations. Only the task-specific heads parameters are updated in this phase. 3. Fine-tuning: The entire modelincluding both the backbone and the task-specific headis trained end-to-end in fully supervised manner. With baseline foundation models (FMs) and supervised ViTs, we focus solely on the supervised fine-tuning phase, which is closer to an operational setting. Note that our MAE workflow involves only intra-dataset transfer learning; inter-dataset transfer is not addressed in this work. Importantly, our SSL pre-training is performed exclusively on the union of the training and validation sets, with test sets strictly excluded. This ensures that the MAE models do not gain an unfair advantage through any form of prior exposure to the fine-tuning test domains, including indirectly via input modalities. We apply our workflows to four mediumto large-scale datasets involving multimodality, multitemporality, and multispectrality: TreeSatAI-TS [1, 4] Tree species identification with 15 multi-label classes. The dataset comprises 50 381 tiles of 60 60 across Germany, with aerial imagery (RGB + NIR) at 0.2 resolution, along with Sentinel-1 and Sentinel-2 time series covering the full year closest to the aerial acquisition. PASTIS-HD [4, 30] Agricultural crop segmentation with 19 semantic classes. This dataset includes 433 tiles of 1280 1280 in France, with very high-resolution (VHR) satellite imagery (SPOT 67) resampled to 1 m, along with Sentinel-1 time series covering about 70 acquisitions in both ascending and descending orbits and Sentinel-2 time series spanning approximately one year. FLAIR#2 [26, 27] Land cover segmentation with 12 semantic classes. The dataset consists of 77 762 tiles of 102.4 102.4 in France, with aerial and elevation imagery (RGB + NIR + DSM) at 0.2 resolution, along with Sentinel-2 time series spanning full year. As in [5], we use version of FLAIR#2 without super-patches, cropping the Sentinel-2 time series to the VHR image extent (discarding 93.5% of pixels). FLAIR-HUB [28] Land cover segmentation with 15 semantic classes. This extended version of FLAIR#2 includes 241 100 tiles of 102.4 102.4 in France, with aerial and elevation imagery (RGB + NIR + DEM + DSM) at 0.2 resolution, along with Sentinel-1 and Sentinel-2 time series covering full year. Note that we do not use the full set of labels or input modalities for some datasets (see SM Sec. 6.1 for details). To study scaling laws with respect to pre-training and fine-tuning dataset size, we also create filtered versions of each dataset containing 20% and 5% of the original samples. These subsets are obtained via Furthest Point Sampling based on geographical coordinates. 4.3. Ablation studies In this section, we assess the impact of various choices of SSL strategy and model components to guide the design of our final approach. We conduct experiments on TreeSatAITS, PASTIS-HD (fold I), and FLAIR-HUB (filtered at 20%, using split 1). Our comparison includes MAE and ViT models [19], alongside several baseline FMsDINO-v2 [60], DINO-v2 sat.[74], DOFA[97], and CROMA [24]with hyperparameters carefully set after extensive tuning. By default, we use the group fusion mode, grouping together the Sentinel-1 ascending and descending modalities. For multispectral data, we apply joint-token fusion with patch-group-wise target normalization during reconstruction, based on spectral band groups exhibiting strong intra5 group and weaker inter-group correlations across Sentinel2, Sentinel-1, and aerial modalities. Full details are provided in SM Secs. 6.3 and 6.4. Multimodal and Multitemporal Fusion. We evaluate in Fig. 3 and SM Tab. 11 the different multimodal and multitemporal fusion modes outlined in Fig. 2. In terms of multimodal fusion, we observe modest benefit from early fusion among similar modalities, but significant performance drop when it is applied across dissimilar ones (group with all modalities grouped performs worst on all three datasets). Additionally, sharing encoder parameters across modalities did not consistently help when using late fusion (monotemp vs shared). Overall, these results suggest that the potential benefits of synergistic learning across modalities may not outweigh the drawbacks of reduced modality-specific specialization. This findinglikely reflecting the strong heterogeneity of EO modalitiescasts doubt on strategies aimed at building universal and modality-agnostic FMs for EO. Regarding multitemporality, we find that early multitemporal fusion (mod, group, and inter-group) consistently outperforms late multitemporal fusion (shared and monotemp) by +23% on TreeSatAI-TS (weighted F1) and PASTIS-HD (mIoU), and +1% on FLAIR-HUB (mIoU). The performance gain is especially pronounced on tasks where temporal dynamics play critical role (see SM Sec. 7.5). The benefit is also more significant for MAEs than for supervised ViTs, suggesting that effectively leveraging temporal dynamics is both essential and non-trivial. These findings highlight potentially overlooked opportunity in multitemporal SSL, which has received less attention than multimodal SSL in prior work. Notably, most existing FMs are inherently monotemporal and thus only compatible with late multitemporal fusion, resulting in significant performance gap compared to models involving early multitemporal fusion. Multispectral Fusion and Target Normalization. In Fig. 4 and SM Tab. 12, we evaluate different choices of multispectral fusion and target normalization. Here, we limit our evaluation to TreeSatAI-TS and PASTIS-HD (using fold I) since the performance on FLAIR-HUB is driven mainly by the aerial modality (see SM Sec. 7.6), which exhibits only weak multispectral nature. We first evaluate the impact of different choices of target normalization within joint-token fusion, where all spectral band groups are combined into the same tokens. Consistent with prior work [34, 35], patch-wise normalization improves performance, but our proposed patch-group-wise approach yields significantly better results. This normalization supports effective transfer learning, even when using only 20% or 5% of pre-training data, as shown in Fig. 4. Overall, our findings highlight that target normalization is essential to fully leverage larger pre-training datasets. Figure 3. Comparison of different multimodal and multitemporal fusion modes for MAEs, ViTs, and baseline FMs. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB 20%. Refer to SM Tab. 11 for exact numbers and additional results with CROMA. Strikingly, patch-group-wise normalization combined with joint-token fusion matchesand sometimes even surpassesthe performance of patch-wise normalization combined with token-based fusion, where each spectral band group forms separate token [9, 13, 35, 37, 62, 77, 78]. With token-based fusion, patch-wise normalization naturally applies within each spectral group since each group corresponds to distinct token, making the two approaches equivalent in terms of target normalization. This result is especially notable given the computational implications: while token-based fusion incurs cost that grows linearly with the number of spectral groups, patchgroup-wise normalization combined with joint-token fusion achieves similar performance at negligible overhead. To our knowledge, the underlying reason why patchwise normalization improves performance remains unclear, even for SSL methods that reconstruct RGB images or videos [34, 75]. One hypothesis is that it helps balance the pretext task by ensuring uniform difficulty and loss contribution across patches. However, patch-wise normalization may fail to provide such balance on multispectral EO data. For example, if different bands have narrow but non-overlapping histograms, patch-wise normalization tends to be equivalent to applying constant normalization with no patch dependence. Patchgroup-wise normalization addresses this by adapting normalization to both the patch and the spectral group. We hypothesize that this balancing requirement is specific to pixel-space generative SSL. In contrast, latent-space generative SSL methods typically achieve balance implic6 itlyby normalizing either before the loss [3] through layers such as LayerNorm, or within the loss via softmax [60, 82, 102] or cosine similarity [78]. Overall, our findings suggest that, for multispectral data, ensuring balanced pretext task may be more important than using complex fusion strategies. Figure 4. Comparison of different choices of multispectral fusion and target normalization for MAE-B models. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD, with varying pre-training dataset fractions. For each dataset and choice of multispectral fusion, we also indicate the pre-training cost in GFLOPs per forward pass (single batch element). Refer to SM Tab. 12 and Tab. 20 for exact numbers. Masking Strategy. The evaluation of our masking strategy is provided in SM Sec. 7.4. In brief, the impact of the masking strategy is minor, though structured masking along each axis can yield noticeable benefits during probing. 4.4. Design of MAESTRO Following the analysis in Sec. 4.3, we finalize the choice of SSL strategy and model components for our new approach. We adopt either the group or the inter-group fusion mode, grouping together the Sentinel-1 ascending and descending modalities. This choice enables early fusion across time steps and across the Sentinel-1 modality pair, while enabling either intermediate or late fusion across other modality pairs. For multispectrality, we use joint-token fusion along with patch-group-wise target normalization during reconstruction. We name our approach MAESTRO to reflect how it orchestrates the complex interplay of multimodal, multitemporal, and multispectral components in EO data within the MAE framework, as illustrated in Fig. 1. 4.5. Performance We evaluate in Tab. 1 the performance of MAESTRO alongside that of baseline FMs and supervised ViTs across the four evaluated datasets. For supervised ViTs, we adopt either the group or intergroup fusion modes, grouping together the Sentinel-1 ascending and descending modalities, as done for MAESTRO. For baseline FMs, guided by the results in Fig. 3 7 and SM Tab. 11, we select the shared fusion mode, except for CROMA, where we use the inter-croma mode (see SM Sec. 6.4.3). As noted earlier, the baseline FMs have been extensively fine-tuned via hyperparameter search to ensure fair comparison (see SM Sec. 6.4). MAESTRO vs. SOTA. MAESTRO sets new state-ofthe-art (SOTA) on two of the evaluated datasets and nearly matches the previous SOTA or best-performing baseline FMs on the remaining two. Specifically, it outperforms the prior SOTA by +2.7% (weighted F1) on TreeSatAITS and by +2.5% (mIoU) on PASTIS-HD, while trailing by only -0.2% (mIoU) on FLAIR#2 and -0.1% (mIoU) on FLAIR-HUB compared to our adaptation of DINO-v2 (see SM Sec. 6.4.1), which sets the new SOTA on these two datasets. MAESTRO vs. Supervised ViTs. MAESTRO outperforms supervised ViTs by even larger margins: +3.7% (weighted F1) on TreeSatAI-TS, +4.4% (mIoU) on PASTIS-HD, +5.7% (mIoU) on FLAIR#2, and +3.8% (mIoU) on FLAIR-HUB. Notably, MAESTRO maintains strong advantage over ViTs even on the very large-scale FLAIR-HUB dataset, which is not always observed with other SSL methods [98]. Overall, these results suggest that transfer learning from SSL pre-training is essential to achieve good fine-tuning performance. MAESTRO vs. Baseline FMs. MAESTRO significantly outperforms the baseline FMs on TreeSatAI-TS and PASTIS-HD, primarily due to the limitations of late multitemporal fusionthe only fusion strategy supported by these baseline FMswhich is particularly suboptimal for these datasets (see SM Sec. 7.5). However, this advantage does not extend to FLAIR#2 and FLAIR-HUB, where MAESTRO performs on par with, or slightly below, the best-performing baseline FMs. This is likely due to these datasets relying predominantly on the monotemporal aerial imagery (see SM Sec. 7.6), reducing the impact of the choice of multitemporal fusion. Other factors influencing this comparison may counterbalance each other. For example, baseline FMs are often pre-trained on larger, more balanced datasets [60] using more computationally intensive methods. Yet, differences in pre-training modalities or preprocessing pipelines relative to fine-tuning can limit transfer effectiveness. Interestinglyand perhaps unsurprisinglybaseline FMs perform best when the dominant modality in the finetuning dataset closely match those used during pre-training (see Tab. 1 and SM Tab. 3). Scaling by Dataset Size. In Fig. 5 and SM Tab. 14, we finally assess the performance of MAESTRO and supervised ViTs across varying pre-training and fine-tuning dataset fractions. MAESTRO consistently outperforms supervised ViTs, especially when fine-tuning data is limited. ComparTable 1. Performance comparison of MAESTRO, baseline FMs and supervised ViTs across the four evaluated datasets. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD, FLAIR#2, and FLAIR-HUB. Arrows ( / ) show the gains/losses versus the best other FM/ViT/SOTA. We have trained all models listed in this table except AnySat and previous SOTA (). MAESTRO models have been pre-trained for twice the number of epochs. Model Model size Fusion mode TreeSatAI-TS PASTIS-HD fold FLAIR#2 split 1 FLAIR-HUB split MAESTRO (ours) MAESTRO (ours) MAESTRO (ours) MAESTRO (ours) DINO-v2 [60] DINO-v2 sat. [74] DOFA [97] CROMA [24] AnySat [5] ViT ViT Previous SOTA Base Base Base Base Base Large Base Base Base Base Base group inter-group group inter-group shared shared shared inter-croma group inter-group 78.5 78.8 79.1 79.42.7 76.7 76.3 76.0 70.5 75. 75.7 75.6 68.8 68.6 68.8 69.02.5 63.8 62.6 64.00.2 63.3 64.9 65.90.1 64.8 65.8 64.4 64.0 62.9 65.0 66.5 64.6 64. 64.2 63.5 62.3 39.0 55.1 58.3 58.2 66.0 66.0 65.1 44.3 61.6 62.1 75.1 [5] 66.5 [5] 64.1 [69] 64.3 [28] ing full and reduced pre-training datasets, we see that more unlabeled data systematically improves downstream performance regardless of the amount of labeled fine-tuning data. These results suggest MAESTRO could further benefit from larger-scale unlabeled pre-training. However, fully leveraging such large-scale datasets may require additional balancing strategiesbeyond our target normalizationtailored for large pretext tasks [42, 81]. 5. Conclusion In this work, we explored how to adapt SSL approaches to the distinctive characteristics of EO data, focusing on three axes of heterogeneity: multimodality, multitemporality, and multispectrality. (i) Multimodality Early fusion offers slight benefits when modalities are similar but can hurt performance when they differ significantly, raising doubts about the viability of universal, modality-agnostic EO foundation models. (ii) Multitemporality Early fusion consistently outperforms late fusion for tasks that strongly rely on multitemporal dynamics, revealing an underexplored opportunity in multitemporal SSL. Current foundation models, being inherently monotemporal, are limited to late multitemporal fusion during fine-tuning, which can incur significant performance drops. (iii) Multispectrality Joint-token early fusion combined with proper target normalization for reconstruction matches the performance of token-based fusion while being significantly more efficient. Building on these findings, we proposed MAESTROan adaptation of the MAE that effectively orchestrates the use of multimodal, multitemporal, and multispectral data in EO. Evaluated on four EO benchmarks, MAESTRO achieves SOTA results on tasks that strongly rely on Figure 5. Scaling of MAESTRO-B and ViT-B models with different pre-training/fine-tuning dataset fractions. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB for three finetuning dataset fractions: 5%, 20%, and 100%. For each finetuning fraction, we compare three pre-training settings: pre-training on 100% of the data, pre-training on the same fraction as fine-tuning, and no pretraining. Refer to SM Tab. 14 for exact numbers. temporal dynamics, while remaining highly competitive on others. We hope that our work will contribute to the design of SSL strategies that specifically account for the distinctive characteristics of EO data."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was granted access to the HPC/AI resources provided by GENCI-IDRIS (allocations A0181013803, A0161013803, thank and AD010114597R1). SIMV/SDM colleagues at IGN, Guillaume Astruc, Loïc Landrieu, Alexandre Tuel, and Thomas Kerdreux for useful discussions. We"
        },
        {
            "title": "References",
            "content": "[1] S. Ahlswede, C. Schulz, C. Gava, P. Helber, B. Bischke, M. Förster, F. Arias, J. Hees, B. Demir, and B. Kleinschmit. TreeSatAI benchmark archive: multi-sensor, multi-label dataset for tree species classification in remote sensing. Earth System Science Data, 15(2):681695, 2023. 5, 14 [2] Firouz Al-Wassai and NV Kalyankar. Major limitations of satellite images. Journal of Global Research in Computer Science, 4(5):5159, 2013. 1 [3] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. 7 [4] Guillaume Astruc, Nicolas Gonthier, Clement Mallet, and Loic Landrieu. OmniSat: Self-supervised modality fusion In European Conference on Comfor earth observation. puter Vision (ECCV), pages 409427. Springer, 2024. 2, 5, 14 [5] Guillaume Astruc, Nicolas Gonthier, Clement Mallet, and Loic Landrieu. AnySat: An earth observation model for any resolutions, scales, and modalities. In Proceedings of the IEEE/CVF international conference on computer vision (CVPR), 2025. 2, 5, 8, 14, 18 [6] Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell, and Stefano Ermon. Geography-aware self-supervised learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1018110190, 2021. 2 [7] Favyen Bastani, Piper Wolters, Ritwik Gupta, Joe Ferdinando, and Aniruddha Kembhavi. SatlasPretrain: largescale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1677216782, 2023. [8] Hanbo Bi, Yingchao Feng, Boyuan Tong, Mengyu Wang, Haichen Yu, Yongqiang Mao, Hao Chang, Wenhui Diao, Peijin Wang, Yue Yu, et al. RingMoE: Mixture-ofmodality-experts multi-modal foundation models for universal remote sensing image interpretation. arXiv preprint arXiv:2504.03166, 2025. 2 [9] Nikolaos Ioannis Bountos, Arthur Ouaknine, Ioannis Papoutsis, and David Rolnick. FoMo: Multi-modal, multiscale and multi-task remote sensing foundation models for forest monitoring. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2785827868, 2025. 2, 6, 17 9 [10] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 17 [11] Keumgang Cha, Junghoon Seo, and Taekyung Lee. billion-scale foundation model for remote sensing images. arXiv preprint arXiv:2304.05215, 2023. 2 [12] Shabnam Choudhury, Yash Salunkhe, Sarthak Mehrotra, and Biplab Banerjee. REJEPA: novel joint-embedding predictive architecture for efficient remote sensing image retrieval. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 23732382, 2025. [13] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Marshall Burke, David Lobell, and Stefano Ermon. SatMAE: Pre-training transformers for temporal and multi-spectral satellite imagery. Advances in Neural Information Processing Systems, 35:197211, 2022. 2, 3, 6, 17, 18 [14] Muhammad Sohail Danish, Muhammad Akhtar Munir, Syed Roshaan Ali Shah, Muhammad Haris Khan, Rao Muhammad Anwer, Jorma Laaksonen, Fahad Shahbaz Khan, and Salman Khan. TerraFM: scalable foundation model for unified multisensor earth observation. arXiv preprint arXiv:2506.06281, 2025. 2 [15] Rangel Daroya, Elijah Cole, Oisin Mac Aodha, Grant Van Horn, and Subhransu Maji. WildSAT: Learning satellite image representations from wildlife observations. arXiv preprint arXiv:2412.14428, 2024. 2 [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. 1, 5 [17] Wenhui Diao, Haichen Yu, Kaiyue Kang, Tong Ling, Di Liu, Yingchao Feng, Hanbo Bi, Libo Ren, Xuexue Li, Yongqiang Mao, et al. RingMo-Aerial: An aerial remote sensing foundation model with affine transformation contrastive learning. arXiv preprint arXiv:2409.13366, 2024. 2 [18] Philipe Dias, Aristeidis Tsaris, Jordan Bowman, Abhishek Potnis, Jacob Arndt, Lexie Yang, and Dalton Lunga. OReole-FM: successes and challenges toward billion-parameter foundation models for high-resolution satellite imagery. In Proceedings of the 32nd ACM International Conference on Advances in Geographic Information Systems, pages 597600, 2024. [19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogIn International Conference on Learning nition at scale. Representations, 2021. 3, 5, 19 [20] Chuc Man Duc and Hiromichi Fukui. SatMamba: Development of foundation models for remote sensing imagery using state space models. arXiv preprint arXiv:2502.00435, 2025. 2 [21] Iris Dumeur, Silvia Valero, and Jordi Inglada. Selfsupervised spatio-temporal representation learning of satellite image time series. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, PP:118, 2024. 2 [22] Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Ángel Bautista, Vaishaal Shankar, Alexander Toshev, Joshua M. Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models. In Proceedings of the 41st International Conference on Machine Learning, pages 1237112384, 2024. [23] Fajwel Fogel, Yohann Perron, Nikola Besic, Laurent SaintAndré, Agnès Pellissier-Tanon, Martin Schwartz, Thomas Boudras, Ibrahim Fayad, Alexandre dAspremont, Loic Landrieu, et al. Open-Canopy: Towards very high resolution forest monitoring. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1395 1406, 2025. 19 [24] Anthony Fuller, Koreen Millard, and James Green. CROMA: Remote sensing representations with contrastive radar-optical masked autoencoders. Advances in Neural Information Processing Systems, 36:55065538, 2023. 2, 5, 8, 19, 20, 23 [25] Anatol Garioud, Stéphane Peillet, Eva Bookjans, Sébastien Giordano, and Boris Wattrelos. FLAIR# 1: semantic segmentation and domain adaptation dataset. arXiv preprint arXiv:2211.12979, 2022. 14 [26] Anatol Garioud, Apolline De Wit, Marc Poupée, Marion Valette, Sébastien Giordano, and Boris Wattrelos. FLAIR# 2: textural and temporal information for semantic segmentation from multi-source optical imagery. arXiv preprint arXiv:2305.14467, 2023. 5, 14 [27] Anatol Garioud, Nicolas Gonthier, Loic Landrieu, Apolline De Wit, Marion Valette, Marc Poupée, Sébastien Giordano, et al. FLAIR: country-scale land cover semantic segmentation dataset from multi-source optical imagery. Advances in Neural Information Processing Systems, 36: 1645616482, 2023. 5, 14 [28] Anatol Garioud, Sébastien Giordano, Nicolas David, and Nicolas Gonthier. FLAIR-HUB: Large-scale multimodal dataset for land cover and crop mapping. arXiv preprint arXiv:2506.07080, 2025. 5, 8, 14, [29] Vivien Sainte Fare Garnot and Loic Landrieu. Lightweight temporal self-attention for classifying satellite images time series. In Advanced Analytics and Learning on Temporal Data: 5th ECML PKDD Workshop, AALTD 2020, Ghent, Belgium, September 18, 2020, Revised Selected Papers 6, pages 171181. Springer, 2020. 2 [30] Vivien Sainte Fare Garnot and Loic Landrieu. Panoptic segmentation of satellite image time series with convolutional temporal attention networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 48724881, 2021. 5, 14 [31] Vivien Sainte Fare Garnot, Loic Landrieu, and Nesrine Chehata. Multi-modal temporal attention models for crop mapping from satellite time series. ISPRS Journal of Photogrammetry and Remote Sensing, 187:294305, 2022. 14 [32] Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, et al. SkySense: multi-modal remote sensing foundation model towards universal interpreIn Proceedings of tation for earth observation imagery. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2767227683, 2024. 2 [33] Boran Han, Shuai Zhang, Xingjian Shi, and Markus Reichstein. Bridging remote sensors with multisensor geospatial foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2785227862, 2024. 2 [34] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1597915988, 2021. 1, 3, 4, 6, 16, [35] Danfeng Hong, Bing Zhang, Xuyang Li, Yuxuan Li, Chenyu Li, Jing Yao, Naoto Yokoya, Hao Li, Pedram Ghamisi, Xiuping Jia, et al. SpectralGPT: Spectral remote sensing foundation model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2, 4, 6 [36] Huiyang Hu, Peijin Wang, Hanbo Bi, Boyuan Tong, Zhaozhi Wang, Wenhui Diao, Hao Chang, Yingchao Feng, Ziqi Zhang, Yaowei Wang, et al. RS-vHeat: Heat conduction guided efficient remote sensing foundation model. arXiv preprint arXiv:2411.17984, 2024. 2 [37] Jeremy Irvin, Lucas Tao, Joanne Zhou, Yuntao Ma, Langston Nashold, Benjamin Liu, and Andrew Ng. USat: unified self-supervised encoder for multi-sensor satellite imagery. arXiv preprint arXiv:2312.02199, 2023. 2, 6, 18 Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In Conference on Uncertainty in Artificial Intelligence (UAI), 2018. 17 [38] Pavel [39] Pallavi Jain, Bianca Schoen-Phelan, and Robert Ross. Self-supervised learning for invariant representations from IEEE Journal of Selected multi-spectral and sar images. Topics in Applied Earth Observations and Remote Sensing, 15:77977808, 2022. 2 [40] Johannes Jakubik, Sujit Roy, CE Phillips, Paolo Fraccaro, Denys Godwin, Bianca Zadrozny, Daniela Szwarcman, Carlos Gomes, Gabby Nyirjesy, Blair Edwards, et al. Foundation models for generalist geospatial artificial intelligence. arXiv preprint arXiv:2310.18660, 2023. [41] Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Maurogiovanni, Jente Bosmans, Nikolaos Dionelis, Valerio Marsocci, Niklas Kopp, et al. TerraMind: Large-scale generative multimodality for earth observation. arXiv preprint arXiv:2504.11171, 2025. 2 [42] Thomas Kerdreux, Alexandre Tuel, Quentin Febvre, Alexis Mouche, and Bertrand Chapron. Efficient self-supervised learning for earth observation via dynamic dataset curation. 10 In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR) Workshops, pages 30173027, 2025. 2, 8 [43] Jingtao Li, Yingyi Liu, Xinyu Wang, Yunning Peng, Chen Sun, Shaoyu Wang, Zhendong Sun, Tian Ke, Xiao Jiang, Tangwei Lu, et al. HyperFree: channel-adaptive and tuning-free foundation model for hyperspectral remote In Proceedings of the Computer Vision sensing imagery. and Pattern Recognition Conference, pages 2304823058, 2025. 2 [44] Shuaipeng Li, Penghao Zhao, Hailin Zhang, Xingwu Sun, Hao Wu, Dian Jiao, Weiyan Wang, Chengjun Liu, Zheng Fang, Jinbao Xue, et al. Surge phenomenon in optimal learning rate and batch size scaling. Advances in Neural Information Processing Systems, 37:132722132746, 2024. [45] Weijie Li, Wei Yang, Tianpeng Liu, Yuenan Hou, Yuxuan Li, Zhen Liu, Yongxiang Liu, and Li Liu. Predicting gradient is better: Exploring self-supervised learning for sar atr with joint-embedding predictive architecture. ISPRS Journal of Photogrammetry and Remote Sensing, 218:326 338, 2024. 2 [46] Xuyang Li, Danfeng Hong, Chenyu Li, and JoceSeaMo: multi-seasonal and multilyn Chanussot. modal remote sensing foundation model. arXiv preprint arXiv:2412.19237, 2024. 2, 17 [47] Xuyang Li, Chenyu Li, Pedram Ghamisi, and Danfeng Hong. FlexiMo: flexible remote sensing foundation model. arXiv preprint arXiv:2503.23844, 2025. 2 [48] Zhihao Li, Biao Hou, Siteng Ma, Zitong Wu, Xianpeng Guo, Bo Ren, and Licheng Jiao. Masked angle-aware autoencoder for remote sensing images. In European Conference on Computer Vision, pages 260278. Springer, 2024. 2 [49] Junyan Lin, Feng Gao, Xiaochen Shi, Junyu Dong, and Qian Du. SS-MAE: Spatialspectral masked autoencoder for multisource remote sensing image classification. IEEE Transactions on Geoscience and Remote Sensing, 61:114, 2023. 2 [50] Jihao Liu, Xin Huang, Jinliang Zheng, Yu Liu, and Hongsheng Li. MixMAE: Mixed and masked autoencoder for efficient pretraining of hierarchical vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62526261, 2023. [51] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 17 [52] Utkarsh Mall, Bharath Hariharan, and Kavita Bala. Change-aware sampling and contrastive learning for satellite images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 52615270, 2023. 2 [53] Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the sdes and scaling rules for adaptive gradient algorithms. Advances in Neural Information Processing Systems, 35:76977711, 2022. 17 [54] Oscar Manas, Alexandre Lacoste, Xavier Giró-i Nieto, David Vazquez, and Pau Rodriguez. Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 94149423, 2021. 2 [55] Matías Mendieta, Boran Han, Xingjian Shi, Yi Zhu, and Chen Chen. Towards geospatial foundation models via conIn Proceedings of the IEEE/CVF Intinual pretraining. ternational Conference on Computer Vision, pages 16806 16816, 2023. 2 [56] Daniel Morales-Brotons, Thijs Vogels, and Hadrien Hendrikx. Exponential moving average of weights in deep learning: Dynamics and benefits. Transactions on Machine Learning Research, 2024. 17, 22 [57] Dilxat Muhtar, Xueliang Zhang, Pengfeng Xiao, Zhenshi Li, and Feng Gu. CMID: unified self-supervised learning framework for remote sensing image understanding. IEEE Transactions on Geoscience and Remote Sensing, 61:117, 2023. [58] Vishal Nedungadi, Ankit Kariryaa, Stefan Oehmcke, Serge Belongie, Christian Igel, and Nico Lang. MMEarth: Exploring multi-modal pretext tasks for geospatial representation learning. In European Conference on Computer Vision, pages 164182. Springer, 2024. 2, 3, 4 [59] Mubashir Noman, Muzammal Naseer, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, and Fahad Shahbaz Khan. Rethinking transformers pre-training for multispectral satellite imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2781127819, 2024. 2 [60] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin ElNouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Ishan Russell Howes, Po-Yao Huang, Shang-Wen Li, Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. 5, 7, 8, 17, 19, 23 [61] Jonathan Prexl and Michael Schmitt. Multi-modal multiobjective contrastive learning for sentinel-1/2 imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 21362144, 2023. 2 [62] Jonathan Prexl and Michael Schmitt. SenPa-MAE: Sensor parameter aware masked autoencoder for multi-satellite self-supervised pretraining. In DAGM German Conference on Pattern Recognition, pages 317331. Springer, 2024. 2, 6 [63] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 1 [64] Colorado Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian Clipp, Kurt Keutzer, Salvatore Candido, Matt Uyttendaele, and Trevor Darrell. ScaleMAE: scale-aware masked autoencoder for multiscale In Proceedings of the geospatial representation learning. IEEE/CVF International Conference on Computer Vision, pages 40884099, 2023. 2, 3, 11 [65] Esther Rolf, Konstantin Klemmer, Caleb Robinson, and Hannah Kerner. Position: mission critical - satellite data is distinct modality in machine learning. 2024. 1 [66] Xian Shuai, Yiding Wang, Yimeng Wu, Xin Jiang, and Xiaozhe Ren. Scaling law for language models training considering batch size. arXiv preprint arXiv:2412.01505, 2024. 17 [67] Michael Smith, Luke Fleming, and James Geach. EarthPT: time series foundation model for earth observation. In NeurIPS CCAI workshop, 2023. 2 [68] Caleb Spradlin, Jordan Caraballo-Vega, Jian Li, Mark Carroll, Jie Gong, and Paul Montesano. SatVision-TOA: geospatial foundation model for coarseresolution all-sky remote sensing imagery. arXiv preprint arXiv:2411.17000, 2024. 2 [69] Jakub Straka and Ivan Gruber. Modernized training of unet for aerial semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshops, pages 776784, 2024. 8 [70] Xian Sun, Peijin Wang, Wanxuan Lu, Zicong Zhu, Xiaonan Lu, Qibin He, Junxi Li, Xuee Rong, Zhujun Yang, Hao Chang, Qinfglin He, Guang Yang, Ruiping Wang, Jiwen Lu, and Kun Fu. RingMo remote sensing foundation IEEE Transactions model with masked image modeling. on Geoscience and Remote Sensing, 61:122, 2023. [71] Daniela Szwarcman, Sujit Roy, Paolo Fraccaro, Þorsteinn Elí Gíslason, Benedikt Blumenstiel, Rinki Ghosal, Pedro Henrique de Oliveira, Joao Lucas de Sousa Almeida, Rocco Sedona, Yanghui Kang, et al. Prithvi-EO2.0: versatile multi-temporal foundation model for earth observation applications. arXiv preprint arXiv:2412.02732, 2024. 2 [72] Maofeng Tang, Andrei Cozma, Konstantinos Georgiou, and Hairong Qi. Cross-Scale MAE: tale of multiscale exploitation in remote sensing. Advances in Neural Information Processing Systems, 36:2005420066, 2023. 2 [73] Jiayuan Tian, Jie Lei, Jiaqing Zhang, Weiying Xie, and Yunsong Li. SwiMDiff: Scene-wide matching contrastive learning with diffusion constraint for remote sensing image. IEEE Transactions on Geoscience and Remote Sensing, 2024. 2 [74] Jamie Tolan, Hung-I Yang, Benjamin Nosarzewski, Guillaume Couairon, Huy Vo, John Brandt, Justine Spore, Sayantan Majumdar, Daniel Haziza, Janaki Vamaraju, et al. Very high resolution canopy height maps from rgb imagery using self-supervised vision transformer and convolutional decoder trained on aerial lidar. Remote Sensing of Environment, 300:113888, 2024. 5, 8, 19, 23 [75] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 6, [76] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 1034710357. PMLR, 2021. 16 12 [77] Gabriel Tseng, Ruben Cartuyvels, Ivan Zvonkov, Mirali Purohit, David Rolnick, and Hannah Kerner. Lightweight, pre-trained transformers for remote sensing timeseries. arXiv preprint arXiv:2304.14065, 2023. 2, 6, 18 [78] Gabriel Tseng, Anthony Fuller, Marlena Reil, Henry Herzog, Patrick Beukema, Favyen Bastani, James Green, Evan Shelhamer, Hannah Kerner, and David Rolnick. Galileo: Learning global & local features of many remote sensing modalities. In Forty-second International Conference on Machine Learning, 2025. 2, 6, 7, 18 [79] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 16 [80] Diego Velazquez, Pau Rodriguez, Sergio Alonso, Josep Gonfaus, Jordi Gonzalez, Gerardo Richarte, Javier Marin, Yoshua Bengio, and Alexandre Lacoste. EarthView: large scale remote sensing dataset for self-supervision. In Proceedings of the Winter Conference on Applications of Computer Vision, pages 12281237, 2025. 2, 3, 4, 18 [81] Huy Vo, Vasil Khalidov, Timothée Darcet, Théo Moutakanni, Nikita Smetanin, Marc Szafraniec, Hugo Touvron, Camille Couprie, Maxime Oquab, Armand Joulin, et al. Automatic data curation for self-supervised arXiv preprint learning: clustering-based approach. arXiv:2405.15613, 2024. 8 [82] Leonard Waldmann, Ando Shah, Yi Wang, Nils Lehmann, Adam Stewart, Zhitong Xiong, Xiao Xiang Zhu, Stefan Bauer, and John Chuang. Panopticon: Advancing anyIn Prosensor foundation models for earth observation. ceedings of the Computer Vision and Pattern Recognition Conference, pages 22042214, 2025. 2, [83] Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang. Advancing plain vision transformer toward remote sensing foundation model. IEEE Transactions on Geoscience and Remote Sensing, 61:115, 2022. 2, 4 [84] Di Wang, Jing Zhang, Minqiang Xu, Lin Liu, Dongsheng Wang, Erzhong Gao, Chengxi Han, Haonan Guo, Bo Du, Dacheng Tao, et al. MTP: Advancing remote sensing foundation model via multi-task pretraining. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2024. [85] Di Wang, Meiqi Hu, Yao Jin, Yuchun Miao, Jiaqi Yang, Yichu Xu, Xiaolei Qin, Jiaqi Ma, Lingyu Sun, Chenxing Li, Chuan Fu, Hongruixuan Chen, Chengxi Han, Naoto Yokoya, Jing Zhang, Minqiang Xu, Lin Liu, Lefei Zhang, Chen Wu, Bo Du, Dacheng Tao, and Liangpei Zhang. HyperSIGMA: Hyperspectral intelligence comprehension foundation model. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 118, 2025. [86] Wang, Wang, Wang, Guo, Zhong, Lan, Zhang, Liu, and Sun. Scaling efficient masked image modeling on large remote sensing dataset. arXiv preprint arXiv:2406.11933, 2024. 2 [87] Fengxiang Wang, Hongzhen Wang, Yulin Wang, Di Wang, Mingshuo Chen, Haiyan Zhao, Yangang Sun, Shuo Wang, dak. Specialized foundation models struggle to beat supervised baselines. In The Thirteenth International Conference on Learning Representations, 2025. [99] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019. 17 [100] Lixian Zhang, Yi Zhao, Runmin Dong, Jinxiao Zhang, Shuai Yuan, Shilei Cao, Mengxuan Chen, Juepeng Zheng, Weijia Li, Wei Liu, et al. A2-MAE: spatial-temporalspectral unified remote sensing pre-training method based arXiv preprint on anchor-aware masked autoencoder. arXiv:2406.08079, 2024. 2 [101] Yingying Zhang, Lixiang Ru, Kang Wu, Lei Yu, Lei Liang, Yansheng Li, and Jingdong Chen. SkySense V2: unified foundation model for multi-modal remote sensing. arXiv preprint arXiv:2507.13812, 2025. 2 [102] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. iBOT: Image BERT pretraining with online tokenizer. In International Conference on Learning Representations, 2022. 7 Long Lan, Wenjing Yang, et al. RoMA: Scaling up mambabased foundation models for remote sensing. arXiv preprint arXiv:2503.10392, 2025. 2 [88] Yi Wang, Conrad Albrecht, and Xiao Xiang Zhu. Selfsupervised vision transformers for joint sar-optical repreIn IGARSS 2022-2022 IEEE Internasentation learning. tional Geoscience and Remote Sensing Symposium, pages 139142. IEEE, 2022. [89] Yuelei Wang, Ting Zhang, Liangjin Zhao, Lin Hu, Zhechao Wang, Ziqing Niu, Peirui Cheng, Kaiqiang Chen, Xuan Zeng, Zhirui Wang, et al. RingMo-lite: remote sensing multi-task lightweight network with cnn-transformer hybrid framework. arXiv preprint arXiv:2309.09003, 2023. 2 [90] Yi Wang, Conrad Albrecht, Nassim Ait Ali Braham, Chenying Liu, Zhitong Xiong, and Xiao Xiang Zhu. Decoupling common and unique representations for multiIn European Conference modal self-supervised learning. on Computer Vision, pages 286303. Springer, 2024. 2 [91] Yi Wang, Conrad Albrecht, and Xiao Xiang Zhu. Multilabel guided soft contrastive learning for efficient earth obIEEE Transactions on Geoscience servation pretraining. and Remote Sensing, 2024. 2 [92] Yi Wang, Hugo Hernández Hernández, Conrad Albrecht, and Xiao Xiang Zhu. Feature guided masked autoencoder for self-supervised learning in remote sensing. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2024. 2 [93] Yi Wang, Zhitong Xiong, Chenying Liu, Adam Stewart, Thomas Dujardin, Nikolaos Ioannis Bountos, Angelos Zavras, Franziska Gerken, Ioannis Papoutsis, Laura LealTaixé, et al. Towards unified Copernicus foundation model for earth vision. arXiv preprint arXiv:2503.11849, 2025. 2 [94] Xinye Wanyan, Sachith Seneviratne, Shuchang Shen, and Michael Kirley. Extending global-local view alignment for self-supervised learning with remote sensing imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24432453, 2024. 2 [95] Kang Wu, Yingying Zhang, Lixiang Ru, Bo Dang, Jiangwei Lao, Lei Yu, Luo Junwei, Zifan Zhu, Yue Sun, Jiahao Zhang, Qi Zhu, Jian Wang, Ming Yang, Jingdong Chen, and Yansheng Li. semantic-enhanced multi-modal remote sensing foundation model for earth observation. Nature Machine Intelligence, pages 115, 2025. [96] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: simple framework for masked image modeling. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 96439653, 2021. 1, 17 [97] Zhitong Xiong, Yi Wang, Fahong Zhang, Adam Stewart, Ioannis Papoutsis, Bertrand Le Saux, Gustau Camps-Valls, and Xiao Xiang Zhu. Neural plasticity-inspired multimodal founarXiv preprint dation model for earth observation. arXiv:2403.15356, 2024. 2, 5, 8, 19, 23 Joëlle Hanna, Damian Borth, [98] Zongzhe Xu, Ritvik Gupta, Wenduo Cheng, Alexander Shen, Junhong Shen, Ameet Talwalkar, and Mikhail Kho13 MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data"
        },
        {
            "title": "Supplementary Material",
            "content": "The appendix is structured as follows: Sec. 6 provides our full experimental details; Sec. 7 reports the detailed results for the experiments presented in Figs. 3 to 5 as well as additional ablation studies; Sec. 8 offers qualitative analysis on inference results; Sec. 9 reports computational costs. 6. Experimental Details In this section, we provide our full experimental details. We provide details on the four evaluated datasets (Sec. 6.1), details shared across models (Sec. 6.2), details specific to MAEs/MAESTRO (Sec. 6.3), and details specific to baseline FMs (Sec. 6.4). Additionally, we provide tables reporting the exhaustive list of hyperparameter values (Sec. 6.5). 6.1. Experimental Details on the Datasets In this subsection, we provide additional details on the choice of input modalities and labels for each of the four evaluated datasets ( Secs. 6.1.1 to 6.1.4). We also report further details on the raw preprocessing steps applied to some input modalities of these datasets (Sec. 6.1.5). 6.1.1. TreeSatAI-TS TreeSat was introduced in [1], featuring aerial imagery (RGB + NIR) at 0.2 resolution, together with monotemporal Sentinel-1 and Sentinel-2 data (Sentinel-2 provided as seasonal medians). It was later extended as TreeSatAI-TS in [4], adding Sentinel-1 and Sentinel-2 time series covering the full year closest to the aerial acquisition. In this work, we retain four distinct modalities: aerial imagery, Sentinel-1 time series in both orbits (ascending and descending), and Sentinel-2 time series. We disregard the original monotemporal Sentinel-1 and Sentinel-2 data. The original labels introduced in [1] were formulated as regression targets, representing the spatial fraction of each tree species within patch. Following [4, 5], we recast this as multi-label classification task: species class is considered present if its spatial fraction exceeds 0.07. 6.1.2. PASTIS-HD PASTIS was first introduced in [30] with Sentinel-2 time series spanning approximately one year. It was later extended as PASTIS-R in [31] by adding Sentinel-1 time series covering about 70 dates in both ascending and descending orbits, and further expanded in [4] as PASTIS-HD with SPOT 67 imagery resampled to 1 resolution. We retain four distinct modalities: SPOT 67 imagery, Sentinel-1 time series in both orbits (ascending and descending), and Sentinel-2 time series. The crop segmentation labels in [30] covered both semantic and panoptic segmentation. Here, we retain only the semantic segmentation labels, omitting panoptic segmentation. Restricting to semantic segmentation allows us to incorporate spatialized fine-tuning task without introducing additional complexities related to specialized heads or loss functions. We assume that the spatialized nature of the task (e.g., semantic segmentation vs. classification) plays more decisive role than the precise segmentation type (e.g., panoptic vs. semantic) when benchmarking different SSL approaches. 6.1.3. FLAIR#2 FLAIR was introduced in [25] with 77,762 tiles of aerial and elevation imagery (RGB + NIR + DSM) at 0.2 resolution. It was extended as FLAIR#2 in [26, 27] with Sentinel-2 time series spanning full year in the form of superpatches, covering larger spatial extent than the aerial imagery (400 400 vs. 102.4 102.4 m) to provide additional spatial context. Following [5], we crop the Sentinel-2 time series to match the extent of the VHR imagery, discarding 93.5% of pixels. We also include elevation imagery at 0.2 resolution, extracted from the FLAIR-HUB extension [28] on the 77,762 FLAIR#2 tiles. In total, we retain three distinct modalities: aerial imagery (RGB + NIR), elevation imagery (DEM + DSM), and Sentinel-2 time series. We use the land cover semantic segmentation labels following the filtering procedure described in [2527]: we retain only 12 of the 18 original classes, excluding the remaining 6 classes from the loss and metric computations. 14 6.1.4. FLAIR-HUB FLAIR-HUB [28] is an extension of FLAIR#2 with 241,100 tiles of 102.4 102.4 m. Compared to FLAIR#2, FLAIRHUB enriches the elevation imagery (DEM + DSM) and treats it as separate modality, crops the Sentinel-2 imagery to the extent of the aerial imagery (as we do in our version of FLAIR#2), and adds Sentinel-1 time series spanning full year, as well as SPOT 67 imagery and historical aerial imagery. We retain five distinct modalities: aerial imagery (RGB + NIR), elevation imagery (DEM + DSM), Sentinel-1 time series in both orbits, and Sentinel-2 time series. We disregard the SPOT 67 imagery, which did not significantly improve fine-tuning performancelikely due to redundancy with the aerial modalityand the historical imagery, due to its lack of synchronicity with the labels. We use the land cover semantic segmentation labels following the filtering procedure in [28]: we retain 15 of the 18 original classes, excluding the remaining 3 from the loss and metric computations. We omit the crop semantic segmentation labels. 6.1.5. Raw preprocessing In TreeSatAI-TS, FLAIR#2, and FLAIR-HUB, the Sentinel-1 backscattering coefficient data is provided in linear scale. We apply logarithmic transformation to express it in decibel (dB) scale (up to multiplicative constant). For PASTIS-HD, the Sentinel-1 data is already in dB scale; however, we retain only the first two channels, corresponding to vertical (VV) and horizontal (VH) polarizations, and discard the third channel expressing their ratio (VV/VH). In FLAIR#2 and FLAIR-HUB, we also apply simple preprocessing step to the elevation imagery (DEM + DSM). We recast it as the DEM and rescaled elevation defined as 103 (DSM DEM), ensuring that both channels have comparable value ranges. The TreeSatAI-TS aerial imagery tiles have slightly larger spatial extent than the other modalities (304 304 pixels, corresponding to 60.8 m, instead of 300 300 pixels). We therefore apply centered crop to align them spatially with the other inputs. 6.2. Experimental Details Shared across Models In this subsection, we provide details on the preprocessing pipeline (Sec. 6.2.1), positional/temporal encodings (Sec. 6.2.2), data augmentation (Sec. 6.2.3), regularization (Sec. 6.2.4) and optimizers (Sec. 6.2.5). 6.2.1. Preprocessing Here we provide the full details on the preprocessing pipeline introduced in Sec. 3.1. The full implementation is available in ssl_models/dataset/dataset.py in our code. Dataset-specific crop hyperparameter Each dataset has specific hyperparameter controlling the spatial extent of crops within original tiles. For classification tasks, where labels apply to entire tiles, we set the crop extent equal to the original tile extent. For segmentation tasks, the crop extent can be smaller, with the same crop applied to the segmentation labels. Choosing the crop size involves balancing two competing effects: Smaller crops increase the spatial resolution of the token grid under fixed token budget. For fixed token budget (Im/Pm)2Dm, reducing the image size Im reduces the patch size Pm, mitigating the risk of an information bottleneck in the tokenizer (the models entry block). Larger crops provide more spatial context for the model. When cropping to smaller extent than the original tile, we apply for each epoch repetition factor RD equal to the ratio of the original tile area to the cropped area. This ensures that the total spatial area seen by the model in each epoch matches that of the full dataset. Practically, this means the dataset length is scaled by RD: each tile, originally mapped to single index, is now associated with RD indices, effectively repeating it that many times with different crops. The crop sampling strategy differs between training and validation/testing: Training: the crop is sampled randomly from all valid crops within the tile, subject only to the constraint that crop boundaries align with integer pixel indices for each modality m. Validation and testing: tiles are partitioned into non-overlapping crops, which are iterated over exhaustively. The same repetition factor is applied in the test epoch, ensuring test metrics cover the exact same spatial footprint as the original tiles. In practice, we use the full tile extent for TreeSatAI-TS, FLAIR#2, and FLAIR-HUB. For PASTIS-HD, however, we found it beneficial to substantially reduce the crop size. This allows us to set Pm = 2 with Dm = 16 and Dm = 4 temporal bins for Sentinel-2 and Sentinel-1, respectively. Without cropping, keeping the same token budget and number of temporal bins would have required Pm = 16 for these modalities. 15 Modality-specific hyperparameters. Each modality in the dataset has configurable set of preprocessing hyperparameters: The image size Im, which by default corresponds to the number of pixels covering the spatial extent of the crop (however, this can be adjusted for baseline FMs to match specific transferred patch size Pm); The target number of temporal bins Dm; Whether snow/cloud mask is used to guide valid time step selection within temporal bins, and the probability threshold for disregarding mask values above this threshold; constant multiplicative normalization factor applied to the modality. Preprocessing steps. The __getitem__ method of our datasets operates in three steps: (i) Determine the sampled dataset tile based on the dataset index; (ii) Sample spatial crop shared across all modalities D; (iii) Process each modality based on the sampled crop. Processing step (iii) for each modality proceeds as follows: (a) If the number of original time steps Tm is not multiple of Dm, randomly truncate the sequence to Dm Tm/Dm consecutive time steps. (b) Read the corresponding tile section defined by the crops spatial extent and the (potentially truncated) temporal extent. To minimize I/O overhead, we use lazy loading: For .tif files: read with rasterio using window reading. For .npy files: read with numpy.load using the argument mmap_mode=\"r\". For .h5 files: read with h5py.File in read mode, and index the required array sections. (c) Reshape the array into Dm temporal bins. (d) For each temporal bin, sample one time step inside the bin: If snow/cloud mask is used, filter out time steps that include any mask values above the configured threshold. An exception occurs when no valid time steps remain, in which case all time steps in the bin are retained. Select single time step from the remaining ones. During training, the selection is random to serve as data augmentation. During validation and testing, we maximize representativeness by: computing the pixel-wise median across valid time steps in the temporal bin; computing the mean absolute deviation to this median for each time step; and selecting the time step with the lowest mean absolute deviation. (e) If the image size Im does not match the number of pixels covering the spatial extent of the crop, reinterpolate the array spatially with torch.nn.functional.interpolate using mode=\"nearest\". (f) Scale the array by the configured constant normalization factor. 6.2.2. Positional/Temporal Encodings As noted in Sec. 3.1, we do not include explicit modality encodings, since we use modality-specific tokenizers and learnable modality-specific [mask] tokens that implicitly encode the source or target modality for each token. However, we do include spatial and temporal positional encodings, as detailed below. Spatial encodings. To encode spatial information, we follow the common practice of using two-dimensional sinecosine positional encodings [34, 76, 79]. As in Scale-MAE [64], we scale these positional encodings according to the ground sampling distance of each modality. Concretely, we compute the least common multiple (LCM) of the token grid sizes across modalities, i.e., lcmmD{Im/Pm}, and generate positional encodings on the high-resolution grid defined by this LCM. For each modality m, we then obtain its positional encodings by downsampling the high-resolution grid, averaging over all high-resolution grid cells corresponding to each low-resolution grid cell. Temporal encodings. To encode temporal information, we first extract the day of year and hour of day for the selected time step in each temporal bin (see Sec. 6.2.1). The day of year is normalized by 365.25 and the hour of day by 24. Applying sine and cosine transformations to these normalized values yields four temporal features per time step. In addition, for each tile, we define reference date shared across all modalities to capture temporal differences across years, rather than only within single year (as we would be limited to with just sine/cosine encodings). For each time step, we compute the difference between its acquisition date and this reference date, obtaining fifth temporal feature. This value is then duplicated four times, resulting in total of eight temporal features per time step. Aggregation. Spatial and temporal encodings are aggregated by concatenation. In the encoder with latent dimension Ce, the spatial encodings occupy Ce 8 dimensions, while the temporal encodings use 8 dimensions, leading to Ce dimensions after concatenation. The same approach is applied in the decoder with latent dimension Cd. 6.2.3. Data Augmentation We use up to three types of data augmentation: Random spatial cropping: During training, crop of the configured extent is randomly sampled within the original tile at the start of preprocessing. This is disabled for validation and testing, where tiles are instead partitioned into non-overlapping crops processed exhaustively in each epoch. When the crop extent matches the original tile extent, this augmentation is implicitly disabled, even during training. Random time step selection: During training, time steps are randomly sampled from the valid steps within each temporal bin. For validation and testing, we instead select time steps that maximize representativeness among valid steps. D4 augmentation: Throughout training, validation, and testing, synchronized D4 transformations are applied across all modalities and semantic segmenation labels just after the preprocessing pipeline. We retain this augmentation for validation and testing, as the model is assumed to have learned equivariance to these transformations during training. 6.2.4. Regularization As regularization, we apply an Exponential Moving Average (EMA) of the model weights during fine-tuning [56], similar to Stochastic Weight Averaging [38]. Concretely, an EMA of the weights is updated at each epoch with smoothing window equal to 20% of the total fine-tuning epochs. Denoting the model weights by θ and their EMA by θEMA, the EMA weights are initialized as θ and updated at each epoch as: θEMA = αθEMA + (1 α)θ, α = 1 (0.2 Nepochs)1. During validation and testing, we use the EMA weights instead of the regular model weights. 6.2.5. Optimizer In all experiments, we use the AdamW optimizer [51]. The learning rate is scaled with the square root of the batch size [44, 53, 66, 99]; that is, we set the learning rate equal to the base learning rate multiplied by the square root of the batch size. However, for given dataset and training phase (i.e., SSL pre-training, probing, or fine-tuning), we keep the batch size and learning rate fixed across experiments to avoid any confounding factors. Across all phasesSSL pre-training, probing, and fine-tuningwe use cosine decay learning rate scheduler with single cycle and warm-up period. This approach is common in recent SSL studies with Transformers in computer vision [10, 34, 60, 96]. We implement this using torch.optim.lr_scheduler.OneCycleLR with its default annealing strategy and warm-up for 20% of the total epochs. During SSL pre-training and probing, the learning rate is annealed down to 1 104 times its maximum value. During fine-tuning, it is instead annealed only to half of the maximum learning rate. This helps fully leverage the EMA strategy as weight averaging (i) naturally reduces noise [56] and (ii) benefits from greater model diversity along the training trajectory when the learning rate remains sufficiently high. 6.3. Experimental Details Specific to MAEs/MAESTRO In this subsection, we provide experimental details specific to MAEs/MAESTRO. We detail the masking strategy (Sec. 6.3.1) and the selection of band groups for patch-group-wise normalization (Sec. 6.3.2). 6.3.1. Masking Strategy We adopt masking strategy that proceeds in two stages: (i) Structured Masking: (a) Modality structure: Mask each modality with fixed probability of 0.25; (b) Spatial structure: Within each modality, mask each spatial position with fixed probability of 0.25; (c) Temporal structure: Within each modality, mask each temporal position with fixed probability of 0.25. (ii) Unstructured Masking: Adjust the masking from step (i) to match an overall 75% masking ratio: (a) If step (i) results in too few masked tokens, randomly mask additional unmasked tokens; (b) If step (i) results in too many masked tokens, randomly unmask some of them. Our masking strategy relates to several approaches proposed in previous works, as outlined below: Step (i-a), which introduces modality-structured masking, is conceptually similar to the band-masking strategy in FomoNet [9], although direct equivalence would require treating each spectral band as distinct modality. Step (i-b), which introduces spatially-structured masking, aligns with the consistent masking used in SatMAE [13] and the tube masking in VideoMAE [75]. It also resembles the approach in SeaMo [46], but with key distinction: SeaMo 17 enforces consistency across modalities at single time step, whereas we enforce consistency across time steps within single modality. Step (i-c), which introduces temporally-structured masking, corresponds to the Timesteps strategy in Presto [77]. Our combined masking strategy shares similarities with those in AnySat [5], EarthMAE [80], and Galileo [78], but still differs in important ways. AnySat and Galileo apply only spatiallyand temporally-structured masking, while EarthMAE uses only spatially-structured masking. Moreover, the way in which we integrate the structured and unstructured stages is distinct from these methods. 6.3.2. Band Groups Here, we provide details on our selection of spectral band groups for patch-group-wise normalization. We start by computing per-band histograms for the VHR, Sentinel-1, and Sentinel-2 modalities on TreeSatAI-TS, PASTISHD, and FLAIR-HUB. To reduce computational cost, toy version of FLAIR-HUB containing 250 tiles was used for histogram computation. The resulting histograms are shown in Fig. 6a, Fig. 6b, and Fig. 6c. For VHR aerial imagery, the RED, GREEN, and BLUE bands exhibit relatively similar histograms, whereas the NIR band shows distinct distribution. For Sentinel-1, the histograms of VV and VH differ markedly, in line with their differing polarization responses; VH generally exhibits lower backscatter than VV. For Sentinel-2, the ten bands cluster into three natural groups with strong intra-group similarity but higher inter-group variation. (a) TreeSatAI-TS (b) PASTIS-HD (c) FLAIR-HUB Figure 6. Per-band histograms for VHR, Sentinel-1, and Sentinel-2 modalities on the TreeSatAI-TS, PASTIS-HD, and FLAIR-HUB datasets. Based on these insights, we define the spectral band groups detailed in Tab. 2 for patch-group-wise normalization. Table 2. Spectral band groups selected for patch-group-wise normalization. Modality Group 1 Aerial SPOT S1ASC / S1DES S2 RED, GREEN, BLUE RED, GREEN, BLUE VV B02, B03, B04, B05 Group NIR Group 3 VH B06, B07, B08, B8A B11, B12 Note that the Sentinel-2 band groups follow the natural wavelength order, which does not precisely align with the order of spatial resolutions (e.g., band B05 has 20 resolution, while B08 has 10 m). This contrasts with some prior approaches to S2 band grouping [13, 37]. 6.4. Experimental Details Specific to Baseline FMs In this subsection, we describe the FMs used as baselines in our comparative evaluation of MAESTRO, along with their specific adaptations to our context. The selected models include DINO-v2 sat. [74], CROMA [24], and DOFA [97], which are pre-trained on EO data, as well as DINO-v2 [60], which is pre-trained on natural images. To optimize the performance of these baseline FMs in our experimental setup, we introduce several key adaptations: (i) Input resizing: Images are resized to match MAESTROs token grid while preserving the original patch size (see Tabs. 5 to 7). (ii) Multimodal tokenization: Multiple modalities are processed simultaneously via parallel, modality-specific tokenizers. For DINO-v2 and DINO-v2 sat., tokenizers are adapted to transfer RGB weights while still allowing the handling of non-RGB inputs. (iii) Late fusion: For DINO-v2, DINO-v2 sat., and DOFA, we incorporate late fusion across modalities and time steps using the shared and monotemp fusion modes (see Sec. 3.1). For CROMA, specific multimodal and multitemporal fusion modes are used. (iv) Temporal encoding: As these baseline FMs lack native temporal handling, temporal encodings from Sec. 6.2.2 are injected into embedded tokens after the encoder. This choice avoids disrupting the transferred representations. All baseline FMs natively use ViT backbones [19], as in MAESTRO. We also use the same classification and segmentation heads as in MAESTRO. Details on the pre-training modalities and the modalities selected for fine-tuning are provided in Tab. 3. Table 3. Pre-training modalities and modalities selected for fine-tuning for the different baseline FMs. Model Pre-training modalities DINO-v2 [60] DINO-v2 sat. [74] DOFA [97] CROMA [24] Natural RGB images (LVD-142M dataset) Maxar Vivid2 mosaic imagery Sentinel-1, Sentinel-2, Gaofen, NAIP, EnMaP Sentinel-1, Sentinel-2 (SSL4EO dataset) Fine-tuning modalities VHR S1ASC/S1DES S2 DEM/DSM 6.4.1. DINO-v2 and DINO-v2 sat. DINO-v2 [60] is FM pre-trained on natural RGB images using SSL teacherstudent distillation framework, which combines global (image-level) and local (patch-level) objectives. Due to the domain gap between natural and EO imagery, transferring DINO-v2 to EO tasks is non-trivial. To adapt it to our multimodal setting, we modify its patchification operation to support both VHR and Sentinel-2 inputs using two parallel patchification layers. For PASTIS-HD, FLAIR#2, and FLAIR-HUB, we retain the pre-trained RGB channel weights, while additional channels are initialized with near-zero values to minimize disruption, following [23]. In contrast, for TreeSatAI-TS, we found it beneficial to map the pre-trained RGB weights to the infrared colors (IRC) channels NIR, RED, and GREEN in the downstream task, while initializing the BLUE channel weights with near-zero values. We attribute this beneficial effect to two factors: (i) representations learned on natural RGB images transfer well to IRC aerial imagery, and (ii) the NIR channel plays decisive role in TreeSatAI-TS, benefiting from being mapped to the transferred weights. We experiment with two fusion modes for handling multimodality and multitemporality: (i) shared, where single encoder (initialized from DINO-v2s weights) processes all modalities and time steps independently, with shared weights across them, (ii) monotemp, where modality-specific encoders (also initialized from DINO-v2) process each modality and time step independently. Since DINO-v2 lacks native temporal handling, the temporal encodings of Sec. 6.2.2 are injected into the post-encoder token embeddings. The class token is discarded. DINO-v2 sat. [74] is based on the same architecture as DINO-v2 but pre-trained on Maxars very high-resolution RGB satellite imagery. We apply the same adaptation protocol for this model as for DINO-v2. 6.4.2. DOFA DOFA [97] is FM pre-trained on multi-source EO data using the MAE framework and dynamic channel handling via hypernetwork that generates patch embedding weights based on input wavelengths (see Table 4). While DOFA is inherently flexible, adaptations are required to support multimodality and multitemporality in our context. We implement four parallel patchification layers (initialized from the original patchification layer) to handle VHR, Sentinel-2, and Sentinel-1 in both ascending and descending orbits. As with DINO-v2 and DINO-v2 sat., we explore both 19 shared and monotemp fusion modes for handling multimodality and multitemporality. Since the original model lacks native temporal handling, the temporal encodings of Sec. 6.2.2 are again injected into the post-encoder token embeddings. Table 4. DOFAs wavelengths per modality. Modality Bands Wavelengths Aerial SPOT S1ASC S1DES S2 RED, GREEN, BLUE, NIR RED, GREEN, BLUE VV, VH VV, VH B02, B03, B04, B05, B06, B07, B08, B8A, B11, B12 0.64, 0.56, 0.48, 0.81 0.66, 0.56, 0.48 5.405, 5.405 5.405, 5.405 0.490, 0.560, 0.665, 0.705, 0.740, 0.783, 0.842, 0.865, 1.610, 2. 6.4.3. CROMA CROMA [24] is FM specifically designed for Sentinel imagery, featuring separate encoders for Sentinel-1 and Sentinel-2, coupled via cross-encoder that fuses their intermediate representations. Since we fine-tune CROMA only on its original modalities, only minimal adaptations are required. The time series for Sentinel-1 in ascending and descending orbits are concatenated and passed as single sequence to the Sentinel-1 encoder. As with other baseline FMs, temporal encodings are injected post-encoder. We consider two fusion modes for handling multimodality and multitemporality: (i) late-croma, where monotemporal Sentinel-1 and Sentinel-2 encoder outputs are concatenated across modalities and time steps before the classification/segmentation heads (cross-encoder disabled); (ii) inter-croma (default), where monotemporal Sentinel-1 and Sentinel-2 encoder outputs are grouped into pairs, fused via the cross-encoder, and finally concatenated across pairs before the classification/segmentation heads. Note that CROMA remains intrinsically monotemporal: late-croma performs only late multitemporal fusion, while intercroma performs intermediate multitemporal fusion within individual Sentinel-1/Sentinel-2 pairs, but late fusion across pairs. 6.5. Hyperparameter Tables In this subsection, we provide tables reporting the exhaustive list of our hyperparameter values. 6.5.1. Dataset-specific Hyperparameters Table 5. TreeSatAI-TSs hyperparameters. Spatial extent of original tiles: 60 Spatial extent of crops: 60 Aerial S1ASC S1DES Datasets original resolution (m) Image size (Im) MAE/MAESTRO/ViT DINO-v2 DINO-v2 sat. DOFA CROMA Patch size (Pm) MAE/MAESTRO/ViT DINO-v2 DINO-v2 sat. DOFA CROMA Number of temporal bins (Dm) Number of channels (Cm) Multiplicative normalization factor Use cloud/snow mask 0.2 300 210 240 240 20 14 16 16 1 4 255 10 6 48 24 2 16 8 4 2 5 10 6 48 2 16 8 4 2 5 MAE/MAESTROs band groups (Gm) RED, GREEN, BLUE NIR VV VH VV VH 10 6 42 48 48 24 2 14 16 16 8 16 10 5 103 (mask proba. > 0) B02, B03, B04, B05 B06, B07, B08, B8A B11, B12 20 Table 6. PASTIS-HDs hyperparameters. Spatial extent of original tiles: 1280 Spatial extent of crops: 160 Spatial resolution of token grid of reference: 20 SPOT S1ASC S1DES Datasets orig. resolution (m) Image size (Im) MAE/MAESTRO/ViT DINO-v2 DINO-v2 sat. DOFA CROMA Patch size (Pm) MAE/MAESTRO/ViT DINO-v2 DINO-v2 sat. DOFA CROMA Number of temporal bins (Dm) Number of channels (Cm) Multiplicative normalization factor Use cloud/snow mask 1 160 140 160 160 16 14 16 16 1 3 255 16 128 64 2 16 8 4 2 20 10 16 128 64 2 16 8 4 2 20 MAE/MAESTROs band groups (Gm) RED, GREEN, BLUE VV VH VV VH S2 10 16 112 128 128 2 14 16 16 8 16 10 1 104 (not avail.) B02, B03, B04, B05 B06, B07, B08, B8A B11, B12 Table 7. FLAIR#2s and FLAIR-HUBs hyperparameters. Present in FLAIR#2 Datasets original resolution (m) Image size (Im) MAE/MAESTRO/ViT DINO-v2 DINO-v2 sat. DOFA CROMA Patch size (Pm) MAE/MAESTRO/ViT DINO-v2 DINO-v2 sat. DOFA CROMA Number of temporal bins (Dm) Number of channels (Cm) Multiplicative normalization factor Use cloud/snow mask Aerial 0.2 512 448 512 512 16 14 16 16 1 4 255 Spatial extent of original tiles: 102.4 Spatial extent of crops: 102.4 Spatial resolution of token grid of reference: 3.2 DEM/DSM S1ASC S1DES 10.24 10.24 10.24 0.2 512 32 1 2 1 103 10 80 40 2 16 8 4 2 5 10 80 40 2 16 8 4 2 5 10 70 80 80 2 14 16 16 8 16 10 5 103 (mask proba. > 0) B02, B03, B04, B05 B06, B07, B08, B8A B11, B12 MAE/MAESTROs band groups (Gm) RED, GREEN, BLUE NIR DEM, DSM VV VH VV VH 21 6.5.2. Data Augmentation/Regularization Hyperparameters Table 8. Data augmentation/regularization hyperparameters. Augmentation/Regularization Phase used Hyperparameter Random spatial cropping Random time step selection D4 augmentation Pre-train/Probe/Fine-tune Pre-train/Probe/Fine-tune Pre-train/Probe/Fine-tune EMA [56] Fine-tune α = 1 (0.2 Nepochs)1 6.5.3. Optimizer Hyperparameters Table 9. Optimizer hyperparameters. Phase Hyperparameter TreeSatAI-TS PASTIS-HD FLAIR# FLAIR-HUB Optimizer LR schedule Warmup fraction Weight decay β1 β2 Base learning rate Final div. factor Batch size AdamW cosine decay 20% 1 102 0.9 0.99 3 105 1 104 AdamW cosine decay 20% 1 102 0.9 0. 3 105 1 104 AdamW cosine decay 20% 1 102 0.9 0.99 3 105 1 104 AdamW cosine decay 20% 1 102 0.9 0.99 3 105 1 104 Pre-train Dataset frac. 100% 96 96 Dataset frac. 20% 96 Dataset frac. 5% Epochs Dataset frac. 100% 100 200 Dataset frac. 20% 400 Dataset frac. 5% 72 72 72 100 200 400 72 100 144 72 72 100 100 200 Base learning rate Final div. factor Batch size 1 105 1 104 1 105 1 1 105 1 104 1 105 1 104 Probe Dataset frac. 100% 96 96 Dataset frac. 20% 96 Dataset frac. 5% Epochs Dataset frac. 100% 10 20 Dataset frac. 20% 40 Dataset frac. 5% 48 48 48 10 20 40 48 15 96 48 48 15 15 Base learning rate Final div. factor Batch size 1 105 2 1 105 2 1 105 2 1 105 2 Fine-tune Dataset frac. 100% 96 96 Dataset frac. 20% 96 Dataset frac. 5% Epochs Dataset frac. 100% 50 100 Dataset frac. 20% 200 Dataset frac. 5% 48 48 48 50 100 200 48 100 96 48 48 100 100 200 22 6.5.4. SSL Hyperparameters Hyperparameter Reconstruction loss Table 10. SSL hyperparameters. L1 w/ patch-group-wise normalization (for some ablations: L1 w/o normalization or L1 w/ patch-wise normalization) Masking ratio Probability modality-structured masking Probability temporally-structured masking Probability spatially-structured masking 75% 0.25 (if not disabled) 0.25 (if not disabled) 0.25 (if not disabled) 7. Additional Results In this section, we report the detailed results on multimodal and multitemporal fusion (Sec. 7.1), multispectral fusion and target normalization (Sec. 7.2), and scaling with respect to pre-training and fine-tuning dataset size (Sec. 7.3). We also report additional ablations on our masking strategy (Sec. 7.4), the impact of multitemporal components (Sec. 7.5), and the importance of each modality by dataset (Sec. 7.6). 7.1. Multimodal/Multitemporal Fusion The detailed numbers of the evaluation of the different multimodal and multitemporal fusion modes with MAEs, baseline FMs and ViTs can be found in Tab. 11, in addition to Fig. 3. Table 11. Evaluation of different multimodal and multitemporal fusion modes for MAEs, baseline FMs and ViTs across three datasets. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB 20%. Model Model size Fusion mode Modality groups TreeSatAI-TS PASTIS-HD fold FLAIR-HUB filt. 20% / split 1 MAE MAE MAE MAE MAE MAE MAE DINO-v2 [60] DINO-v2 [60] DINO-v2 sat. [74] DINO-v2 sat. [74] DOFA [97] DOFA [97] CROMA [24] CROMA [24] ViT ViT ViT ViT ViT ViT ViT Base Base Base Base Base Base Base Base Base Large Large Base Base Base Base Base Base Base Base Base Base Base shared monotemp mod group group group inter-group shared monotemp shared monotemp shared monotemp late-croma inter-croma shared monotemp mod group group group inter-group S1ASC, S1DES S1ASC, S1DES, S2 S1ASC, S1DES, S2, VHR S1ASC, S1DES S1ASC, S1DES S1ASC, S1DES, S2 S1ASC, S1DES, S2, VHR S1ASC, S1DES 76.1 77.0 78.4 78.5 78.1 78.2 78.8 76.7 76.4 76.3 76.6 76.1 75.9 69.8 70.5 72.6 73.0 75.4 75.7 76.0 75.9 75. 66.1 66.0 68.9 68.8 69.1 68.1 68.6 64.4 63.7 64.0 63.1 62.9 63.2 64.9 65.0 64.1 64.4 64.6 64.6 64.2 64.0 64.5 62.5 62.4 63.3 63.6 63.5 61.4 63.5 64.2 63.9 64.5 64.0 62.8 64.1 41.9 42.5 59.3 58.9 59.0 59.5 59.1 56.4 58. 7.2. Multispectral Fusion/Target Normalization The detailed numbers of the evaluation of the different choices of multispectral fusion and target normalization with MAEs can be found in Tab. 12, in addition to Fig. 4. We also report results after probing evaluation in Tab. 13. We observe slight performance drop when using token-based fusion with patch-wise normalization compared to jointtoken fusion with patch-group-wise normalization. This may be attributed to the combination of two factors: (i) certain 23 Sentinel-2 band groups may be less relevant to the downstream tasks, and (ii) our segmentation heads, which aggregate multimodal and multitemporal features through single attentive pooling layer, are shallow and may struggle to filter out irrelevant tokensthereby introducing noise into the final predictions. Table 12. Evaluation of different choices of multispectral fusion and target normalization for MAE-B models across two datasets. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD, with varying pre-training dataset fractions. Multispectral fusion Target normalization TreeSatAI-TS PASTIS-HD fold Pre-training dataset frac. 5% 20% 100% Pre-training dataset frac. 5% 20% 100% Joint-token Joint-token Joint-token No normalization Patch-wise Patch-group-wise Token-based Patch-wise 72.7 74.4 76.3 76.6 72.8 75.5 77.4 77.0 74.6 77.4 78.5 78. 64.9 66.7 67.3 66.7 65.4 67.1 68.1 67.2 65.1 67.5 68.8 68. Table 13. Probing evaluation of different choices of multispectral fusion and target normalization for MAE-B models across two datasets. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD, with varying pre-training dataset fractions. Multispectral fusion Target normalization TreeSatAI-TS PASTIS-HD fold Pre-training dataset frac. 5% 20% 100% Pre-training dataset frac. 5% 20% 100% Joint-token Joint-token Joint-token No normalization Patch-wise Patch-group-wise Token-based Patch-wise 57.2 62.9 64.8 61.9 61.1 65.1 68.6 66.2 63.2 67.7 69. 69.3 52.5 58.2 59.0 55.9 56.9 59.7 60.2 57.5 57.9 61.2 61. 61.0 7.3. Scaling by Dataset Size The detailed numbers of the performance with different pre-training/fine-tuning dataset fractions are provided in Tab. 14, in addition to Fig. 5. Across all settings, SSL pre-training with MAESTRO outperforms supervised ViTs, with particularly large gains when fine-tuning data is scarce. Comparing Pre-training fraction = 100% and Pre-training fraction = Fine-tuning fraction in the table highlights that additional pre-training data improves downstream performance regardless of the amount of labeled data available for fine-tuning. This suggests that MAESTRO could further benefit from larger-scale unlabeled pre-training. Table 14. Scaling of MAESTRO-B and ViT-B models with different pre-training/fine-tuning dataset fractions across three datasets. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB for three fine-tuning dataset fractions: 5%, 20%, and 100%. For each fine-tuning fraction, we compare three pre-training settings: no pre-training, pre-training on the same fraction as fine-tuning, and pre-training on 100% of the data. Note that when fine-tuning on 100% of the data, pre-training on the same fraction as fine-tuning and pre-training on 100% of the data become equivalent, hence the identical performance. TreeSatAI-TS PASTIS-HD fold FLAIR-HUB split 1 Fine-tuning dataset frac. 100% 5% 20% Fine-tuning dataset frac. 100% 5% 20% Fine-tuning dataset frac. 100% 5% 20% No pre-training Pre-training fraction = Fine-tuning fraction Pre-training fraction = 100% 61.8 65.5 67.8 69.0 71.8 73.8 75.7 78.5 78. 38.8 46.6 52.5 52.2 57.1 59.2 64.6 68.8 68.8 55.3 60.1 61.5 59.5 63.6 64.6 61.6 64.9 64. 24 7.4. Masking Strategy We examine the impact of various components of our masking strategy on downstream task performance. In particular, we assess the effect of including the different structured masking stepsnamely steps (i-a), (i-b), and (i-c) detailed in Sec. 6.3.1in the SSL pretext task. Results are reported in Tab. 16 and Tab. 15, corresponding to performance after finetuning and probing, respectively. In these experiments, we use the group fusion mode, grouping together the Sentinel-1 ascending and descending modalities. For multispectral data, we use joint-token fusion combined with patch-group-wise target normalization during reconstruction. We find that structured masking yields only marginal improvements in fine-tuning performance. However, temporally structured masking leads to notable boost in probing performance on PASTIS-HD. This suggests that structured masking may produce more useful representations when evaluated directly (i.e., without fine-tuning), although these benefits do not necessarily transfer fully after model fine-tuning. Table 15. Evaluation of different choices of masking for MAE-B models across three datasets. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB 20%. Masking structure Modality Spatial Temporal TreeSatAI-TS PASTIS-HD fold FLAIR-HUB filt. 20% / split 78.5 78.5 78.5 78.3 78.2 68.8 68.4 68.6 68.5 68.4 63.6 63.3 63.0 63.4 63. Table 16. Probing evaluation of different choices of masking for MAE-B models across three datasets. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB 20%. Masking structure Modality Spatial Temporal TreeSatAI-TS PASTIS-HD fold FLAIR-HUB filt. 20% / split 1 69.3 69.8 69.6 69.2 69. 61.2 61.4 61.5 57.7 57.6 56.2 56.3 56.3 56.2 56.3 7.5. Importance of Multitemporal Components We examine the impact of various multitemporal components on downstream task performance for MAE-B and ViT-B models. In these experiments, we use the group fusion mode, grouping together the Sentinel-1 ascending and descending modalities. For multispectral data, we use joint-token fusion combined with patch-group-wise target normalization during reconstruction. We report in Tab. 17 the effect of varying the number of temporal bins Dm for the Sentinel-2 and Sentinel-1 ascending and descending modalities. Additionally, we show in Tab. 18 the impact of removing date encodings and disabling the data augmentation associated with random time step selection within temporal bins. In Tab. 17, we observe that the choice of Dm for Sentinel-2 is critical when this modality drives performance, as in TreeSatAI-TS and PASTIS-HD (see Sec. 7.6). Notably, the performance gap between Dm = 16 and Dm = 1 for Sentinel-2 is much larger than that between Dm = 1 and Dm = 0. This indicates that it is the multitemporal dynamics of Sentinel-2, rather than its monotemporal component, that is essential. In contrast, varying Dm for Sentinel-1 has only minor effect, reflecting its smaller role (see Sec. 7.6). In Tab. 18, we find that removing date encodings leads to clear performance drop whenever multitemporal dynamics are important. Random time step selection during training generally improves performance, although the magnitude of its benefit varies across datasets. 25 Table 17. Importance of the number of temporal bins for MAE-B and ViT-B models across three datasets. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB 20%. Arrows ( / ) indicate the difference compared to using 16 temporal bins for the Sentinel-2 modality and 4 temporal bins for the Sentinel-1 ascending and descending modalities. Model MAE MAE MAE MAE MAE MAE ViT ViT ViT ViT ViT ViT Number of temporal bins TreeSatAI-TS S1ASC S1DES 4 4 4 4 1 0 4 4 4 4 1 4 4 4 4 1 0 4 4 4 4 1 0 S2 16 4 1 0 16 16 16 4 1 0 16 16 78.5 75.3 3.2 73.1 5.4 72.6 5.9 78.3 0.2 78.2 0. 75.7 70.6 5.1 66.1 9.6 65.8 9.9 75.8 0.1 75.7 0.0 PASTIS-HD fold FLAIR-HUB filt. 20% / split 1 68.8 64.3 4.5 49.3 19.5 44.9 23.9 68.8 0.0 69.2 0.4 64.6 61.2 3.4 46.5 18.1 41.5 23.1 64.6 0.0 64.4 0.2 63.6 62.4 1.2 61.8 1.8 61.6 2.0 63.5 0.1 63.1 0. 59.5 58.4 1.1 57.6 1.9 57.4 2.1 59.8 0.3 59.2 0.3 Table 18. Impact of removing date encodings and disabling random time step selection for MAE-B and ViT-B models across three datasets. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB 20%. Arrows ( / ) indicate the difference compared to including date encodings and enabling random time step selection. Model MAE MAE MAE ViT ViT ViT Number of temporal bins S1ASC S1DES 4 4 4 4 4 4 4 4 4 4 4 S2 16 16 16 16 16 16 Date encodings Random time step selection TreeSatAI-TS PASTIS-HD fold FLAIR-HUB filt. 20% / split 1 78.5 77.5 1.0 78.8 0.3 75.7 74.8 0.9 75.4 0.3 68.8 67.8 1.0 68.2 0.6 64.6 63.8 0.8 63.9 0.7 63.6 63.6 0.0 63.7 0.1 59.5 59.4 0.1 58.7 0. 7.6. Importance of each Modality by Dataset We present an ablation study on modality removal for MAE-B and ViT-B models. Results are reported in Tab. 19. In these experiments, we use the group fusion mode, grouping together the Sentinel-1 ascending and descending modalities. For multispectral data, we use joint-token fusion along with patch-group-wise target normalization during reconstruction. Table 19. Impact of removing modalities for MAE-B and ViT-B models across three datasets. We report the weighted F1 score (%) on TreeSatAI-TS and the mIoU (%) on PASTIS-HD and FLAIR-HUB 20%. Arrows ( / ) indicate the difference compared to including all modalities. TreeSatAI-TS PASTIS-HD fold FLAIR-HUB filt. 20% / split Aerial S1 S2 wF1 (%) Spot S2 mIoU (%) Aerial S1 S2 DEM/DSM mIoU (%) 78.5 76.8 1.7 78.2 0.3 72.6 5. 75.7 75.3 0.4 75.7 0.0 65.8 9.9 68.8 68.6 0.2 69.2 0.4 44.9 23.9 64.6 64.5 0.1 64.4 0.2 41.5 23.1 63.6 52.2 11.4 63.1 0.5 61.6 2.0 62.7 0. 59.5 47.9 11.6 59.2 0.3 57.4 2.1 58.3 1.2 Model MAE MAE MAE MAE MAE ViT ViT ViT ViT ViT 26 We observe that both MAEs and ViTs generally perform best when all modalities are included. The Sentinel-2 modality has strong influence on tasks heavily tied to multitemporal dynamics, such as tree species identification and crop segmentation. Additionally, the modality from which the ground truth is derived markedly affects performancefor example, aerial imagery for FLAIR-HUB and Sentinel-2 for PASTIS-HD. The slight performance drop observed when omitting Sentinel-1 on PASTIS-HD may stem from the combination of two factors: (i) its lower task relevance compared to Sentinel-2, and (ii) the relatively shallow segmentation heads, which aggregate multimodal and multitemporal features via single attentive pooling and may struggle to suppress irrelevant features, thereby introducing noise into the predictions. 8. Inference Results In this section, we report examples of inference results from MAESTRO and supervised ViTs on the segmentation tasks of PASTIS-HD and FLAIR-HUB. We again consider MAESTRO and ViTs with the group fusion mode, grouping together the Sentinel-1 ascending and descending modalities. For multispectral data, we use joint-token fusion along with patch-groupwise target normalization during reconstruction. Results are reported in Fig. 7 and Fig. 8 for PASTIS-HD and FLAIR-HUB, respectively. For each tile, we display the VHR imagery, the Sentinel-2 imagery, the MAESTRO prediction, the ViT prediction, and the corresponding ground truth. Examples are randomly sampled from the test set. Figure 7. Inference results from MAESTRO-B and ViT-B models on PASTIS-HD. For Sentinel-2 imagery, we report the pixel-wise median across temporal bins. White parcels correspond to areas with missing annotations (void labels). In Fig. 7, MAESTRO produces precise segmentation masks on PASTIS-HD, closely matching the boundaries of parcel and tree-covered areas. Its predictions are more spatially coherent than those of ViTs and better differentiate crop types. 27 Figure 8. Inference results from MAESTRO-B and ViT-B models on FLAIR-HUB. For Sentinel-2 imagery, we report the pixel-wise median across temporal bins. In Fig. 8, although the complex scenes in FLAIR-HUB introduce prediction ambiguities, MAESTRO still delivers sharper and more accurate object delineations than ViTs. It also performs better on classes such as brushwood and water, and more effectively separates impervious from pervious surfaces. 9. Computational Costs In this section, we quantify floating-point operations (FLOPs) (Sec. 9.1) and report the overall computational cost of the experiments shown in the paper (Sec. 9.2). 9.1. MACs/FLOPs We report FLOPs per forward pass separately for SSL pre-training (Sec. 9.1.1) and probing/fine-tuning (Sec. 9.1.2). While forward FLOPs are identical between probing and fine-tuning, probing is substantially more efficient when backward FLOPs are taken into account. Forward FLOPs (for single batch element) are computed for MAESTRO-B, MAE-B, and ViT-B models. We first calculate the number of multiply-accumulate operations (MACs), then convert to FLOPs using the relation FLOPs = 2 MACs. Following standard practice, we include only operations from model components with significant MAC contributions, excluding element-wise operations such as normalizations, nonlinearities, biases, and component-wise summations. We again consider the group fusion mode, grouping the Sentinel-1 ascending and descending modalities together. For multispectral data, we evaluate both joint-token and token-based multispectral fusion. 9.1.1. Pre-training MACs/FLOPs For each modality m, let Lm denote its sequence length. With joint-token multispectral fusion, the sequence length is Lm = (Im/Pm)2 Dm, where Im is the image size, Pm the patch size, and Dm the number of temporal bins. With tokenbased multispectral fusion, the sequence length becomes Lm = (Im/Pm)2 DmGm, where Gm is the cardinality of the set of band groups mapped to different tokens. For modality that is not grouped with others via early multimodal fusion, the MACs in the encoder and decoder are: MACsenc = (cid:0)12(1 )LmC 2 MACsdec = (cid:0)12LmC 2 mCd + 2L2 + 2(1 )Lm2Ce (cid:1) Nd, (cid:1) Ne, where is the fraction of masked tokens, Ne and Nd are the number of layers in the encoder and decoder, while Ce and Cd are the latent dimensions in the encoder and decoder. For the grouped Sentinel-1 ascending and descending modalities, MACsenc and MACsdec are computed by replacing Lm with the sum of sequence lengths of the two modalities. The dense layers that project from the encoder space to the decoder space contribute: MACsenc-to-dec = (cid:88) (1 )LmCeCd. The patchify and unpatchify operations contribute: MACspatchify = MACsunpatchify = (cid:88) (cid:88) 2 mDmCmCe, 2 mDmCmCd, where Cm is the number of channels for each modality m. Putting everything together, and using = 0.75 for the fraction of masked tokens, Ne = 12, Ce = 768 for the encoder, and Nd = 3, Cd = 512 for the decoder, we compute the forward MACs/FLOPs for MAESTRO-B and MAE-B models during pre-training, based on the default configurations in Tab. 5, Tab. 6, and Tab. 7. This computation is carried out for both joint-token and token-based multispectral fusion across the four evaluated datasets. Results are reported in Tab. 20. Table 20. MACs/FLOPs for MAESTRO-B and MAE-B models during pre-training. We report the forward MACs/FLOPs (for single batch element) for both joint-token and token-based multispectral fusion across the four evaluated datasets. Multipliers (x) indicate the FLOPs increase compared to using joint-token multispectral fusion. Multispectral fusion Joint-token Token-based TreeSatAI-TS PASTIS-HD FLAIR# FLAIR-HUB MACs (109) FLOPs (109) MACs (109) FLOPs (109) MACs (109 ) FLOPs (109) MACs (109) FLOPs (109 ) 14.3 33.7 2.4 28.7 67.4 2.4 56.1 173.6 3.1 112.1 347.2 3.1 59.1 133.9 2.3 118.2 267.8 2. 65.4 146.9 2.2 130.9 293.8 2.2 9.1.2. Probing/Fine-tuning MACs/FLOPs We reuse the same notation as in Sec. 9.1.1. As before, the sequence length is given by Lm = (Im/Pm)2 Dm for joint-token multispectral fusion, and by Lm = (Im/Pm)2 DmGm for token-based multispectral fusion. For modality that is not grouped with others via early multimodal fusion, the MACs in the encoder are: MACsenc = (cid:0)12LmC 2 + 2L mCe (cid:1) Ne. For the grouped Sentinel-1 ascending and descending modalities, MACsenc is computed by replacing Lm with the sum of sequence lengths of the two modalities. The attentive pooling operations in the classification and segmentation heads contribute: MACsattn-pool = (cid:88) LmCe. 29 The final dense projections in the classification and segmentation heads contribute: MACsproj-cls = CeCcls, MACsproj-seg = LrefCeCseg, where Lref is the sequence length of the spatial token grid of reference (see Sec. 3.3), while Ccls and Cseg are the number of semantic classes for the classification and segmentation tasks, respectively, including the classes ignored during loss and metric computations (see Sec. 6.1). Putting everything together, and using Ne = 12, Ce = 768 for the encoder, we compute the forward MACs/FLOPs for MAESTRO-B, MAE-B, and ViT-B models during probing/fine-tuning, based on the default configurations in Tab. 5, Tab. 6, and Tab. 7. This computation is performed for both joint-token and token-based multispectral fusion across the four evaluated datasets. The results are presented in Tab. 21. Table 21. MACs/FLOPs for MAESTRO-B, MAE-B, and ViT-B models during probing/fine-tuning. We report the forward MACs/FLOPs (for single batch element) for both joint-token and token-based multispectral fusion across the four evaluated datasets. Multipliers (x) indicate the FLOPs increase compared to using joint-token multispectral fusion. Multispectral fusion Joint-token Token-based TreeSatAI-TS PASTIS-HD FLAIR#2 FLAIR-HUB MACs (109) FLOPs (109) MACs (109) FLOPs (109) MACs (109 ) FLOPs (109) MACs (109) FLOPs (109 ) 39.1 95.0 2.4 78.3 190.0 2.4 163.4 549.9 3.4 326.8 1099.9 3. 167.4 403.9 2.4 334.8 807.8 2.4 185.1 440.8 2.4 370.3 881.7 2.4 9.2. Overall Computational Cost All our experiments were run on SLURM clusters, with either V100, A40, A100, or H100 GPU nodes. Training was performed in mixed precision. In total, running all the experiments shown in the paper required 17 661 V100 hours, 7 736 A40 hours, 268 A100 hours, and 268 H100 hours."
        }
    ],
    "affiliations": [
        "Institut national de linformation géographique et forestière (IGN), France",
        "Univ Gustave Eiffel, ENSG, IGN, LASTIG, France"
    ]
}