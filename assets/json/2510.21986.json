{
    "paper_title": "Sprint: Sparse-Dense Residual Fusion for Efficient Diffusion Transformers",
    "authors": [
        "Dogyun Park",
        "Moayed Haji-Ali",
        "Yanyu Li",
        "Willi Menapace",
        "Sergey Tulyakov",
        "Hyunwoo J. Kim",
        "Aliaksandr Siarohin",
        "Anil Kag"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet na\\\"ive strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT, Sparse--Dense Residual Fusion for Efficient Diffusion Transformers, a simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on a sparse subset to cut computation, and their outputs are fused through residual connections. Training follows a two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the train--inference gap. On ImageNet-1K 256x256, SPRINT achieves 9.8x training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as a simple, effective, and general solution for efficient DiT training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 6 8 9 1 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "SPRINT: SPARSE-DENSE RESIDUAL FUSION FOR EFFICIENT DIFFUSION TRANSFORMERS Dogyun Park1,2 Moayed Haji-Ali1 Yanyu Li1 Willi Menapace1 Sergey Tulyakov1 Hyunwoo J. Kim3 Aliaksandr Siarohin1 Anil Kag1 1Snap Inc. 2Korea University 3KAIST"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet naïve strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT, SparseDense Residual Fusion for Efficient Diffusion Transformers, simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on sparse subset to cut computation, and their outputs are fused through residual connections. Training follows two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the traininference gap. On ImageNet-1K 2562, SPRINT achieves 9.8 training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as simple, effective, and general solution for efficient DiT training."
        },
        {
            "title": "INTRODUCTION",
            "content": "Figure 1: Sparsedense residual fusion improves the efficiency of diffusion transformer training. SPRINT decouples the computationally heavy middle blocks of DiT into sparsedeep path and denseshallow residual path. Notably, on ImageNet-1K 2562, SPRINT achieves up to 5.6 and 9.8 lower training cost compared to vanilla models, while improving generation quality. Diffusion Transformers (DiTs) (Peebles & Xie, 2023; Esser et al., 2024b) have emerged as powerful class of generative models (OpenAI, 2024; Labs, 2024a). Yet their training cost scales quadratically with sequence length, making large-scale pretraining prohibitively expensive in compute and memory. natural way to reduce cost is to shorten sequences by dropping tokens. However, naïve token dropping (Sehwag et al., 2025) degrades representations and leads to poor generalization when models are evaluated with full-token inputs at inference. Work done during internship at Snap Inc."
        },
        {
            "title": "Preprint",
            "content": "Another direction is to guide DiTs with external supervision. For instance, REPA (Yu et al., 2024) aligns intermediate DiT features with DINOv2, accelerating convergence. However, such auxiliary losses can harm long-term performance or destabilize training (Wang et al., 2025), since pre-trained vision features are not naturally aligned with diffusions iterative denoising. Recent work (Zheng et al., 2024; Gao et al., 2023) has explored more advanced token-dropping strategies. While promising, these methods either add substantial parameters (Sehwag et al., 2025) or only support moderate drop ratios (Krause et al., 2025; Zheng et al., 2024), and break down under aggressive settings (e.g., 75%). In this work, we present training algorithm that enables high-ratio token dropping while preserving robust, semantically meaningful representations that transfer effectively to full-token fine-tuning. Our design philosophy is to train DiTs efficiently with minimal architectural changes, achieving performance on par withor better thanstrong baselines. The core idea is to exploit the complementary roles of shallow and deep layers in neural networks: shallow layers capture fine-grained local details, while deeper layers model global semantics. However, our analysis shows that in standard DiT training, deeper layers often waste computation on redundant local details that contribute little to modeling global semantics, due to the homogeneous architecture of DiTs. This redundancy significantly slows training convergence and reduces efficiency. We demonstrate that reformulating the architecture and coupling it with principled token-dropping strategy resolves this issue. Our Solution. We introduce SparseDense Residual Fusion for Efficient Diffusion Transformers (SPRINT), simple strategy that enables aggressive token dropping while preserving representation quality. Specifically, we partition the DiT into three components: encoder, middle blocks, and decoder. The encoder processes all tokens to encode local information, producing dense shallow features. Before the middle blocks, we drop most tokens (typically 75%), forcing deeper layers to focus on sparse global context with far lower compute, making sparse deep features. residual fusion mechanism then combines dense shallow features with sparse deep features, while dummy masking tokens ensure dimensional alignment, and the fused representation is passed to the decoder. Training proceeds in two stages. First, we perform long pre-training with 75% token dropping, yielding large compute savings. Then, short fine-tuning stage restores full-token processing in the middle blocks, allowing them to adapt to dense inputs and closing the traininference gap. Training uses the standard diffusion loss, and the DiT block design remains unchanged, making SPRINT easy to integrate into existing codebases. Contributions. Our work makes the following key contributions: We propose Sparse-Dense Residual Fusion (SPRINT), which fuses dense shallow and sparse deep features for efficient DiT training, supporting up to 75% token dropping and yielding large efficiency gains over prior methods (Tab. 1, Fig. 3c). We demonstrate faster convergence and improved efficiency on modern DiTs. On ImageNet1K 2562 class-conditional generation, SPRINT reduces training GFLOPs by 9.8 compared to standard SiT training while achieving similar or better quality (Fig. 1c, Tab. 3). SPRINT provides new insights into DiT representations: our denseshallow features are more noise-invariant and semantically expressive  (Fig. 6)  ; achieve higher CKNNA scores than vanilla DiT (Fig. 3b); and shallow versus deep paths specialize in local versus global semantics  (Fig. 4)  . We introduce Path-Drop Guidance (PDG), replacement for classifier-free guidance (CFG) that computes the unconditional pass using only dense shallow features. PDG nearly halves inference FLOPs while surpassing CFG in generation quality (Fig. 2, Tab. 3). We show that SPRINT is simple, architecture-agnostic, and complementary to alignment-based methods. It applies seamlessly across architectures (SiT, UViT), latent spaces (SD, FLUX VAE), and resolutions (256, 512), and provides further gains when combined with REPA (Yu et al., 2024)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Accelerating DiT training via representation alignment. Several works accelerate DiT convergence by aligning internal features with pre-trained vision transformers. REPA (Yu et al., 2024) aligns intermediate DiT activations with DINOv2 features, while Lee et al. (2025) extend this to textimage models via contrastive loss. Wang & He (2025) instead propose dispersive loss that spreads features without external alignment. However, HASTE (Wang et al., 2025) shows that alignment signals can"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: SPRINT improves visual quality over baseline with only 57% of inference TFLOPs. We present samples from two SiT-XL/2REPA models after 1M training iterations, where SPRINT is applied to one of the models. For our approach, we further incorporate the proposed Path-Drop Guidance (PDG), yielding improved FDD scores and higher visual quality compared to vanilla REPA. conflict with diffusion objectives and destabilize training. These objectives are complementary to our token-dropping scheme and can be combined to further boost performance (Tab. 2). Efficient DiT training with token dropping. Another direction reduces training cost by shortening sequences. Progressive training (Podell et al., 2024; Esser et al., 2024b) first pre-trains at 128128 before fine-tuning at 256256. MDTv2 (Gao et al., 2023) restructures DiT into an encoderdecoder, processing masked tokens with skip connections and optimizing both reconstruction and diffusion losses. MaskDiT (Zheng et al., 2024) drops random patches, replaces them with mask tokens, and trains an auxiliary decoder, which adds inference cost. MicroDiT (Sehwag et al., 2025) adds patch-mixer for high masking ratios; and TREAD (Krause et al., 2025) bypasses subsets of tokens through inner layers to optimize full denoising loss. These approaches work at moderate drop ratios ( 50%) but degrade at aggressive settings (e.g., 75%) and are difficult to pair with alignment losses. In contrast, our approach remains alignment-friendly and robust even under high drop rates."
        },
        {
            "title": "TRANSFORMERS",
            "content": "3.1 PRELIMINARIES Diffusion and flow-based generative models. Diffusion and flow-based models Ho et al. (2020); Song et al. (2020); Lipman et al. (2023); Liu et al. (2023) learn continuous transformation between simple reference distribution π1 (e.g., Gaussian noise) and target data distribution π0. Given x0 π0 and x1 π1, the transformation evolves over [0, 1] by the ODE dxt dt (1) where xt interpolates between x0 and x1, and : Rd [0, 1] Rd is the velocity field. We use xt (αtx0, σ2 I) with α0 = σ1 = 1, α1 = σ0 = 0, and adopt linear schedule Ma et al. (2024): αt = 1 t, σt = t. neural network vθ (e.g., DiT) learns by minimizing (cid:2)v(xt, t) vθ(xt, t)2(cid:3). Ex0,x1,t = v(xt, t), (2) min θ Token dropping in diffusion transformers. Given noisy image xt, DiT divides it into nonoverlapping patches, producing tokens xt RBN D, where = HW p2 , is the embedding dimension, and the image resolution. Since the attention cost in DiTs scales quadratically with , dropping tokens reduces training cost. For drop ratio r, we remove rN tokens and process only the remaining rN with DiT blocks. Although described for 2D images, this naturally extends to other modalities such as video."
        },
        {
            "title": "Preprint",
            "content": "(a) ℓ2 gradient norm of fθ (b) CKNNA(fθ, DINOv2) Figure 3: Training behavior of diffusion transformers. We empirically analyze the training dynamics of SiT-B/2 and its SPRINT variants under different token-drop ratios. (a) We measure the ℓ2 gradient norm of fθ, showing that SPRINT enables the encoder to receive stronger gradient signals from the loss. (b) SPRINT variants achieve higher and earlier CKNNA scores than SiT, indicating SPRINT learns more semantic, noise-robust representations. (c) SPRINT converges substantially faster and to lower FID than SiT, with the gap further widening at higher drop ratios (up to 75%), highlighting both the effectiveness and efficiency of our framework. (c) FID-10K 3.2 BOTTLENECK IN STANDARD DIT TRAINING Standard Diffusion Transformers (DiTs) use homogeneous architecture where every layer, from shallow to deep, processes the full set of dense tokens. This is inefficient: in deeper layers, token representations become redundant as features shift from local, high-frequency patterns to global, low-frequency semantics (Hoover et al., 2019; Voita et al., 2019). Inference-time pruning and merging methods (Rao et al., 2021; Chang et al., 2023; Bolya & Hoffman, 2023) further show that large fractions of tokens can be removed in later layers with minimal effect on output quality. Training deep layers on all tokens thus wastes compute, spending large portion of the FLOP budget on fine-grained details that contribute little to modeling global structure. We address this by introducing architectural specialization: 1. Early layers process dense tokens to robustly capture local evidence under noisy input and build rich foundation of features. 2. Deeper layers operate on sparse subset of tokens to efficiently model global semantic relationships without redundant computation. 3. Final layers reintroduce all tokens for dense prediction. Based on these principles, we reformulate the DiT architecture with densesparse fusion mechanism. 3.3 SPARSEDENSE RESIDUAL FUSION We propose SparseDense Residual Fusion for Efficient Diffusion Transformers (SPRINT), which decouples dense local details from sparse global semantics, improving efficiency by accelerating convergence and reducing compute. An overview is shown in Fig. 1. We begin with standard DiT, divided into encoder fθ (first two blocks), middle blocks gθ, and decoder hθ (final two blocks), and reformulate the computation flow as: 1. Encoder fθ processes all noisy tokens to produce feature map that retains fine-grained local noise information across all spatial locations. 2. Dense shallow path creates residual connection that directly forwards the dense feature map from fθ to the fusion block, preserving local, high-frequency detail. 3. Sparse deep path drops large fraction of tokens (e.g., 75%) before gθ, forcing the deep layers to operate on sparse subset, yielding sparse global context. 4. Fusion and decoder integrate dense local information from the shallow path with sparse global context from the deep path to predict all tokens. Formally, given input tokens xt RBN C, we first compute dense features ft = fθ(xt). fraction of tokens (the drop ratio) is removed to form drop , which is processed by the middle blocks: gdrop ). To fuse the dense and sparse paths, we restore gdrop = gθ(f drop to the original sequence length by padding the dropped positions with fixed [MASK] token (denoted M), yielding gpad RBN C. t"
        },
        {
            "title": "Preprint",
            "content": "We concatenate ft and gpad along the channel dimension, project back to the original size, and feed the fused representation to the decoder hθ. This enables the decoder to combine local details from the encoder with sparse global semantics from the middle blocks for full-token prediction. The entire model is trained end-to-end by minimizing the flow matching loss in Eq. 2 (refer to Alg. 1). Improving training efficiency with minimal modification. SPRINT improves training through two key mechanisms. First, it reduces per-iteration compute cost by restricting the expensive middle blocks gθ to sparse token set, while the dense shallow path preserves fine-grained information. Unlike prior methods, it remains stable even under aggressive drop ratios (Fig. 3c) where others fail. Second, it accelerates iteration-wise convergence by enhancing contextual and relation learning: the decoder hθ must predict all tokens despite most deep-path inputs being [MASK] tokens. This forces encoder (fθ) and middle blocks (gθ) to learn robust, context-aware features, as reflected in faster FID improvement (Fig. 3c), stronger gradient flow (Fig. 3a), and richer representations (CKNNA in Fig. 3b). These gains come with minimal architectural change: the standard DiT blocks remain intact, making SPRINT easy to integrate into existing codebases. Analysis details are in Appendix A. Denseshallow vs. sparsedeep features. The ablation in Fig. 4 highlights their complementary roles. The denseshallow path preserves local textures (e.g., feathers, skin patterns) but fails to form coherent global structure. The sparsedeep path captures global shapes (e.g., bird outline, shark body) but introduces severe texture artifacts. Fusing both yields high-quality outputs with realistic global semantics and fine local detail, showing that denseshallow features encode local evidence while sparsedeep features capture global semantics. Figure 4: Roles of denseshallow and sparsedeep features. Denseshallow features preserve local textures but lose global structure, while sparsedeep features capture global shapes but distort local details. Fusing both recovers high-quality outputs with coherent semantics and fine detail. Fine-tuning with full tokens. After efficient sparse pre-training, we transition the middle blocks to operate on the full token set for brief fine-tuning stage, addressing the potential traininference gap as demonstrated in prior works (Zhang et al., 2024; Sehwag et al., 2025; Krause et al., 2025) (refer to Alg. 2). Since pre-training typically dominates with 1M4M iterations, this fine-tuning phase is short (e.g., 100K200K iterations), yet sufficient for the deeper layers to adapt to the full data distribution, ensuring high inference quality while retaining most of the pre-training speedup. 3.4 EFFICIENT PATH-DROP GUIDANCE (PDG) SPRINTs dual-path design also enables efficient guidance during inference. Standard Classifier-Free Guidance (CFG) doubles sampling cost by requiring two forward passes per step: one conditional vθ(xt, c) and one unconditional vθ(xt, ). Auto Guidance (Karras et al., 2024a) shows that the unconditional pass can be replaced by weaker network. The SPRINT architecture inherently contains natural weaker network: the dense shallow path that bypasses the deep middle blocks. We therefore introduce Path-Drop Guidance (PDG): For the conditional estimate, we perform full forward pass. For the unconditional estimate, we bypass gθ entirely, replacing it with [MASK] tokens. Formally, the conditional and unconditional velocities are: (3) (4) where denotes the [MASK] token tensor. This provides high-quality generation while nearly halving FLOPs and latency per step, since the expensive middle blocks are executed only once. v(xt, c) = hθ(Fusion(gθ(fθ(xt, c)), fθ(xt, c)), c), v(xt, ) = hθ(Fusion(M, fθ(xt, )), ),"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Training efficiency on ImageNet 256256. Iteration-wise results of SiT-XL/2 with different token-dropping methods on SD and Flux autoencoders. We report total training TFLOPs (using Deepspeed library) and performance with/without classifier-free guidance, along with relative gains over SiT-XL/2. All metrics are evaluated with 50 sampling steps using the ODE sampler. Method AE TFLOPs (106) w/o CFG (w = 1.0) CFG (w = 1.4) FDD FID IS Pre. Rec. FDD FID IS Pre. Rec. 400K training iterations Improved SiT-XL/2 + Progressive Training + MicroDiT + Tread + SPRINT (Ours) Gain 1M training iterations Improved SiT-XL/2 + Progressive Training + MicroDiT + Tread + SPRINT (Ours) Gain 400K training iterations Improved SiT-XL/2 + Progressive Training + MicroDiT + Tread + SPRINT (Ours) Gain SD SD SD SD SD SD SD SD SD SD SD SD Flux Flux Flux Flux Flux Flux 24.4 16.8 20.8 19.7 18.7 1.32 61.2 25.8 37.5 34.5 31.5 1.94 24.6 17.0 20.9 19.8 18.8 1.31 351.1 365.5 349.9 461.1 262.6 +88.5 290.0 359.4 293.4 372.6 248.8 +41.2 358.9 375.3 420.9 470.1 268.4 +90. 12.8 12.7 11.54 16.28 9.30 +3.5 10.9 12.3 10.9 12.3 9.15 +1.75 14.8 13.5 17.8 19.9 11.4 +3.4 97.4 96.2 99.9 89.9 118.5 +24.1 113.4 102.2 113.8 112.1 129.5 +16.1 84.4 89.2 76.8 72.2 101.9 +17. 0.66 0.67 0.67 0.63 0.68 0.66 0.67 0.68 0.66 0.67 0.64 0.66 0.61 0.60 0.66 0.65 0.63 0.64 0.64 0.65 0.67 0.65 0.65 0.66 0.67 0.63 0.63 0.64 0.63 0.63 185.0 215.6 178.1 264.3 136.5 +48.5 146.0 188.1 147.6 197.7 126.1 +14.9 178.7 186.3 212.9 255.0 135.4 +43.3 3.09 3.47 3.16 4.07 2.56 +0.53 2.36 2.95 2.53 2.82 2.29 +0.07 3.95 4.02 4.45 5.18 3.77 +0. 211.6 206.4 213.7 201.2 247.1 +35.5 243.7 222.2 241.4 242.9 268.3 24.6 210.7 205.4 196.2 187.5 239.8 +29.1 0.81 0.83 0.82 0.80 0.82 0.80 0.82 0.82 0.80 0.81 0.83 0.84 0.81 0.79 0.83 0.55 0.53 0.54 0.54 0.56 0.58 0.55 0.55 0.57 0.59 0.50 0.49 0.51 0.50 0.51 3.5 STRUCTURED GROUP-WISE TOKEN SUBSAMPLING The effectiveness of token dropping depends not just on how many tokens are removed, but on which are kept. Uniform random sampling risks leaving large contiguous holes in the feature map. To avoid this, we propose structured group-wise subsampling strategy that guarantees local coverage while maintaining global irregularity. Specifically, we partition tokens into small, non-overlapping groups in their native topology (e.g., 2D for images). For images, we divide the (H/p) (W/p) grid into groups. At each training iteration, we randomly select tokens per group, giving drop ratio = 1 k/n2. We use = 2, = 1, corresponding to 75% drop ratio. This ensures that every local patch is represented while preventing the model from overfitting to fixed sampling patterns."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 EXPERIMENTAL DETAILS Training details. Our framework follows the setups of DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024). Unless stated otherwise, most of the experiments are trained on ImageNet-1K at 256 256 resolution using pretrained VAEs from Stable Diffusion (Rombach et al., 2022) and Flux (Labs, 2024b), both with 8 downsampling but encoding into 4 and 16 channels, respectively. Unless stated otherwise, models are pre-trained with 75% token drop ratio using our structured group-wise subsampling. We adopt the SiT architecture, where each block contains self-attention and feed-forward layer, and apply standard improvements: RMS Normalization for queries and keys (Touvron et al., 2023a;b), 2D RoPE for positional embeddings (Wang et al., 2024), and lognormal timestep sampling (Esser et al., 2024a). Experiments focus on SiT-B/2 and SiT-XL/2. Additional hyperparameters and training details are provided in Appendix C. Pre-training and fine-tuning algorithm is provided in Alg. 1 and 2, respectively. Evaluation details. We evaluate generation quality using standard metrics: FDD (Fréchet Distance on DINOv2 (Oquab et al., 2023) features), FID (Fréchet Inception Distance (Heusel et al., 2017)), Inception Score (IS) (Salimans et al., 2016), Precision, and Recall (Kynkäänniemi et al., 2019). Among these, FDD has been shown to be more reliable for diffusion models (Stein et al., 2023). To assess training and inference efficiency, we report total training FLOPs and inference FLOPs computed with the DeepSpeed library. We provide details in Appendix and D. Inference algorithm is provided in Alg. 3."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: SPRINT improves visual scaling. Qualitative comparison of images generated without classifier-free guidance at 400K iterations using two SiT-XL/2REPA models, with SPRINT applied to the model in the upper row. Figure 6: SPRINT improves feature semantics. We visualize the PCA features of fθ and gθ from two SiT-XL/2 models at 400K iterations, with SPRINT applied to the model in the upper row."
        },
        {
            "title": "4.2 SYSTEM-LEVEL COMPARISON",
            "content": "We compare against following methods to demonstrate the effectiveness and efficiency of our method: 1. Dense SOTA: We use SiT (Ma et al., 2024) models as our primary baseline. This represents state-of-the-art model trained with full, dense tokens, providing direct measure of the trade-off between performance and our efficiency gains. 2. Sparse SOTA: We compare against recent methods that leverage token-dropping strategy to accelerate training, e.g., MicroDiT (Sehwag et al., 2025) and Tread (Krause et al., 2025). 3. Alternative methods: We also compare against progressive training, popular strategy where model is pretrained on lower resolution and finetuned on the full resolution. more detailed description of each baseline is provided in Appendix E. We adopt the same architectural improvements for all models and report the performance during training in Tab. 1. SPRINT demonstrates superior performance and efficiency across all settings. At just 400K training iterations with the SD-VAE, our model significantly outperforms SiT-XL/2 baseline (e.g., +88.5 improvement in FDD) while using 1.32 fewer FLOPs. As training progresses to 1M iterations, SPRINT consistently improves over the baseline while becoming even more efficient, achieving 1.95 computational speedup. This result highlights SPRINTs dual acceleration: achieving higher sample quality in the same training iterations with lower computational budget. In contrast, competing token-dropping methods fail to match the performance of SiT-XL/2, even at higher computational cost than our method. These trends hold when using classifier-free guidance. Table 2: Compatibility with other architectures. We apply SPRINT to REPA and U-ViT on the SD autoencoder, reporting performance at 400K iterations with/without classifier-free guidance and relative gains over baselines. All metrics are computed with 50 ODE sampling steps. Method w/o CFG (w = 1.0) CFG (w = 1.4) FDD FID IS Pre. Rec. FDD FID IS Pre. Rec. Improved SiT-XL/2REPA + SPRINT (Ours) Gain Improved U-ViT-XL/2 + SPRINT (Ours) Gain 279.6 234.5 +45.1 335.1 271.7 +63.4 10.0 8.68 +1. 12.1 9.20 +2.9 114.0 129.6 +15.6 98.6 114.4 +15.8 0.67 0.67 0.67 0.69 0.66 0.67 0.64 0.64 146.6 125.1 +21.5 193.7 146.4 +30.1 2.42 2.38 +0.04 3.36 2.97 +0.39 237.1 259.8 +22. 200.3 236.7 +36.4 0.81 0.80 0.80 0.83 0.57 0.59 0.56 0.54 Generalization to other diffusion architectures. To demonstrate that our method is general training strategy and not limited to specific DiT architecture, we apply SPRINT to two other prominent models: REPA (Yu et al., 2024) and U-ViT (Bao et al., 2023). We integrate our dense-sparse fusion mechanism into their respective backbones and report the results after 400K training iterations in Tab. 2. The results show that SPRINT provides significant improvements in all cases. When applied to REPA, SPRINT improves the FDD by +45.1 and FID by +1.32 (w/o CFG). Similarly, for U-ViT, we observe +63.4 improvement in FDD and +2.9 improvement in FID. These experiments confirm that SPRINT is broadly applicable and effective method for accelerating the training."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Comprehensive performance comparison on ImageNet 256 256 class-conditioned generation with classifier-free guidance. / indicate whether lower or higher values are better, respectively. denotes training with batch size 1024, our reproduction with architectural improvements, and use of guidance scheduling. All metrics are evaluated with 250 sampling steps using the SDE sampler. Training and inference TFLOPs are measured with the DeepSpeed library (refer to Appendix for details.) Method Epochs #Params. Training TFLOPs (106) Inference TFLOPs FDD FID Pre. Rec. ADM (Dhariwal & Nichol, 2021) CDM (Ho et al., 2022) LDM-4 (Rombach et al., 2022) U-ViT-H (Bao et al., 2023) MDTv2-XL (Gao et al., 2023) MaskDiT (Zheng et al., 2024) DiT-XL (Peebles & Xie, 2023) FiTv2-XL (Wang et al., 2024) Tread (Krause et al., 2025) SiT-XL (Ma et al., 2024) SiT-XLREPA (Yu et al., 2024) SiT-XL SiT-XL + SPRINT + SPRINTPDG + SPRINT + SPRINTPDG SiT-XL REPA + SPRINT + SPRINTPDG + SPRINTPDG 400 2160 200 240 1080 1600 1400 400 740 1400 800 200 400 200 200 400 200 200 200 400 673M 400M 501M 742M 730M 675M 671M 675M 675M 675M 675M 675M 677M 677M 677M 677M 675M 677M 677M 677M 268.0 427.7 427.7 248.6 61.1 122.2 43.7 43.7 65.1 65.1 62.1 44.3 44.3 66.7 0.513 0.475 0.475 0.475 0.474 0.474 0.477 0.274 0.477 0.274 0.474 0.477 0.274 0.274 75. 82.4 79.5 80.5 78.5 72.5 89.8 79.5 79.0 63.1 75.4 58.4 78.8 75.6 57.1 54.7 3.94 4.88 3.60 2.29 1.58 2.28 2.27 2.26 2.09 2.06 1. 2.18 2.04 2.01 1.82 1.96 1.62 1.93 1.87 1.61 1.59 0.82 0.87 0.82 0.79 0.80 0.83 0.81 0.81 0.82 0.81 0.82 0.82 0.82 0.81 0.80 0. 0.81 0.81 0.80 0.80 0.52 0.48 0.57 0.65 0.61 0.57 0.59 0.62 0.59 0.61 0.60 0.60 0.60 0.61 0.61 0.63 0.60 0.61 0.64 0. Table 4: Effect of densesparse residuals on FID. Table 5: Effect of token-drop strategies on FID. Table 6: Effect of fθ, gθ, hθ on compute and performance. Dense Sparse FID Strategy 85.1 81.4 27.5 Random Structured (Ours) FID 30.1 27.5 fθ gθ hθ 2 3 5 8 6 2 2 3 5 FLOPs / iter 7.47G 9.33G 13.1G FID 27.5 29.1 49.2 Visual analysis. In Fig. 5, we show that SPRINT not only accelerates convergence quantitatively but also enhances the visual progression. At just 100K iterations, SPRINT produces coherent global structures (e.g., the shape of car) along with fine details, whereas REPA lags behind. Furthermore, in Fig. 6, we analyze the PCA of features from fθ and gθ, demonstrating that SPRINT learns more noise-invariant and semantically vivid representations than the SiT model across diffusion timesteps. 4.3 COMPARISON WITH STATE-OF-THE-ART MODELS Tab. 3 compares SPRINT against recent state-of-the-art diffusion transformers. We evaluate both FDD and FID on the original SiT, our reproduced SiT with architectural improvements, and the same model trained with SPRINT. Our improved SiT closely matches the original SiT performance after 400 epochs (78.5 vs. 79.5 FDD). In contrast, SiT trained with SPRINT achieves comparable performance 79.0 FDD in only 200 epochs. At 400 epochs, it outperforms the improved SiT baseline by 4.4 FDD (from 79.5 to 75.1) and 0.08 FID while using just 53% of the training FLOPs. This shows that SPRINT both accelerates convergence and substantially reduces training cost. At inference, Path-Drop Guidance (PDG) further boosts efficiency: with only 57% of the inference cost, SPRINT improves performance by 21.1 FDD (from 79.5 to 58.4) over the improved SiT. Similar trends hold when combined with REPA. SPRINT reduces FDD from 78.8 to 75.6 using only 71% of the training FLOPs. With PDG sampling at 400 epochs, it surpasses the vanilla REPA model trained for 800 epochs by 17.8 FDD and 0.21 FID, while using only 27% of the training FLOPs. Overall, SPRINT consistently improves generation quality while drastically lowering both training and inference cost, outperforming strong baselines and alignment-augmented models."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Effect of dense residuals and drop ratio on FID. Table 8: Effect of fθ and hθ depth on FID. Figure 7: Effect of guidance scale on SiT and our SPRINT. Method SiT-B/2 Dense residual SPRINT 0 FID 55. 0 25% 50% 75% 54.1 43.2 32.3 27.5 87.5% 50.2 fθ gθ hθ FID 0 1 2 3 4 8 8 8 8 8 4 3 2 1 0 79.7 61.5 27.5 44.4 81."
        },
        {
            "title": "4.4 ANALYSIS",
            "content": "We mostly use SiT-B/2 configuration at 400K training iterations (detailed in Tab. 9) in following analysis unless stated otherwise. Sparsedense residual fusion (Tab. 4). To evaluate the effectiveness of sparsedense residual fusion, we perform an ablation by disabling each of the two parallel paths during training. Removing the dense shallow path causes sharp performance drop, with FID rising from 27.5 to 85.1, underscoring its role in accurate velocity prediction. Conversely, removing the sparse deep path reduces the model to standard dense DiT with only four layers, which also degrades performance due to limited capacity. These results confirm that the parallel sparsedense design is critical for maintaining high performance under token dropping. Token sampling strategy (Tab. 5). We compare our structured group-wise sampling strategy with standard uniform random sampling. At the same 75% drop ratio, structured sampling improves FID from 30.1 to 27.5, demonstrating that preserving local coverage is crucial for effective sparse training. Effect of gθ depth (Tab. 6). We study the trade-off between performance and computation as function of middle block depth. The default configuration yields the best FID (27.3) with the lowest cost (7.47G). Shifting layers from the middle block to the encoder and decoder (e.g., 3-6-3 or 5-2-5) increases cost without benefit, and FID degrades to 29.1 and 49.2, respectively. Thus, the default configuration strikes the best balance between efficiency and performance. Effect of fθ and hθ depth (Tab. 8). We find that allocating at least two blocks to both fθ and hθ is critical for high performance. Reducing either to single block degrades results (FID 61.5 and 44.4), while removing either entirely collapses performance (FID >79). These results highlight the need for balanced, symmetric design with sufficient depth in both encoder and decoder. Drop ratio (Tab. 7). As the drop ratio increases from 0 (dense training) to 75%, model performance steadily improves, with FID decreasing from 54.1 to 27.5. This trend indicates that higher sparsity in SPRINT promotes complementary interactions between the encoder and middle blocks, leading to more robust and efficient representations. However, at an extreme drop ratio of 87.5%, FID rises to 50.2, suggesting that excessive sparsity hinders semantic modeling and limits the models representational capacity. Path-drop guidance  (Fig. 7)  . We compare FDD across guidance scales for CFG (SiT-XL/2), CFG (SPRINT), and PDG (SPRINT). PDG consistently outperforms both CFG baselines, achieving lower (better) peak FDD. Moreover, it delivers these gains at nearly half the inference cost, since the unconditional estimate bypasses the middle blocks. These results show that PDG provides superior trade-off, generating higher-quality samples while substantially reducing computational cost. Training at higher resolution (Appendix F.1, Fig. 31). Similar to the 2562 setting, we also evaluate our model against baselines at 5122 resolution with XL config. Results are provided in Tab. 10 and show that SPRINT achieves comparable efficiency and performance gains to those observed at 2562. Lower sampling steps (Appendix F.2). SPRINT remains competitive at few-step inference, consistently surpassing SiT-XL/2 in Tab. 11. At 10 steps, it reduces FID from 7.37 to 6.29 and FDD from 205.2 to 174.5, highlighting the representational strength of our method."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced SPRINT, simple and architecture-agnostic training framework for DiTs that combines denseshallow and sparsedeep features through residual fusion. By exploiting the complementary strengths of shallow and deep layers, it enables aggressive token dropping (up to 75%) while preserving representation quality, and two-stage schedule with masked pre-training and short fulltoken fine-tuning closes the traininference gap. Experiments on ImageNet-1K show that SPRINT reduces training cost by up to 9.8 while matching or surpassing the quality of strong baselines. SPRINT also enables Path-Drop Guidance, simple replacement for CFG that halves inference cost while improving sample quality. Thus, SPRINT is simple, effective, and general approach for efficient DiT training, applicable across architectures, resolutions, and alignment methods."
        },
        {
            "title": "REPRODUCIBILITY",
            "content": "We have made every effort to ensure the reproducibility of our results. Detailed hyper-parameters, training schedules, and architectural configurations are provided in the Appendix, including model definitions, pre-training and fine-tuning iterations, number of sampling steps at inference, and compute resources. Our framework follows the well-established setups of DiT (Peebles & Xie, 2023) and SiT (Ma et al., 2024), which are widely adopted in diffusion research. Although our training code cannot be released at submission time, the use of these standardized setups, along with the provided experimental details, should allow independent reproduction of our results."
        },
        {
            "title": "REFERENCES",
            "content": "Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Conference on Computer Vision and Pattern Recognition, CVPR, 2023. Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Conference on Computer Vision and Pattern Recognition, CVPR, 2023. Shuning Chang, Pichao Wang, Ming Lin, Fan Wang, David Junhao Zhang, Rong Jin, and Mike Zheng Shou. Making vision transformers efficient from token sparsification view. In Conference on Computer Vision and Pattern Recognition, CVPR, 2023. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, NeurIPS, 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In arXiv preprint arXiv:2010.11929, 2020. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, ICML, 2024a. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024b. URL https://openreview.net/forum?id=FPnUhsQJ5B. Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Advances in Neural Information Processing Systems, NeurIPS, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In arXiv preprint arXiv:2207.12598, 2022."
        },
        {
            "title": "Preprint",
            "content": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, NeurIPS, 2020. Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 2022. Benjamin Hoover, Hendrik Strobelt, and Sebastian Gehrmann. exbert: visual analysis tool to explore learned representations in transformers models. In arXiv preprint arXiv:1910.05276, 2019. Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In International Conference on Machine Learning, ICML, 2024. Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. In Advances in Neural Information Guiding diffusion model with bad version of itself. Processing Systems, NeurIPS, 2024a. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Conference on Computer Vision and Pattern Recognition, CVPR, 2024b. Felix Krause, Timy Phan, Ming Gui, Stefan Andreas Baumann, Vincent Tao Hu, and Björn Ommer. Tread: Token routing for efficient architecture-agnostic diffusion training, 2025. Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In Advances in Neural Information Processing Systems, NeurIPS, 2019. Black Forest Labs. Flux: generative model by black forest labs. https://github.com/ black-forest-labs/flux, 2024a. Accessed: 2025-05-14. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024b. Jaa-Yeon Lee, Byunghee Cha, Jeongsol Kim, and Jong Chul Ye. Aligning text to image in diffusion models is easier than you think, 2025. URL https://arxiv.org/abs/2503.08250. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In International Conference on Learning Representations, ICLR, 2023. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, ICLR, 2023. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, ECCV, 2024. OpenAI. Video generation models as world simulators: Introducing sora. https://openai. Accom/index/video-generation-models-as-world-simulators/, 2024. cessed: 2025-05-14. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. In arXiv preprint arXiv:2304.07193, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Conference on Computer Vision and Pattern Recognition, CVPR, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=di52zR8xgf."
        },
        {
            "title": "Preprint",
            "content": "Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. In Advances in Neural Information Processing Systems, NeurIPS, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition, CVPR, 2022. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, NeurIPS, 2016. Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, and Lingjuan Lyu. Stretching each dollar: Diffusion training from scratch on micro-budget. In Conference on Computer Vision and Pattern Recognition, CVPR, 2025. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In arXiv preprint arXiv:2010.02502, 2020. George Stein, Jesse Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Ross, Valentin Villecroze, Zhaoyan Liu, Anthony Caterini, Eric Taylor, and Gabriel Loaiza-Ganem. Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. In Advances in Neural Information Processing Systems, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. In arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. In arXiv preprint arXiv:2307.09288, 2023b. Elena Voita, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations in the transformer: study with machine translation and language modeling objectives. In Empirical Methods in Natural Language Processing, EMNLP, 2019. Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization, 2025. URL https://arxiv.org/abs/2506.09027. ZiDong Wang, Zeyu Lu, Di Huang, Cai Zhou, Wanli Ouyang, et al. Fitv2: Scalable and improved flexible vision transformer for diffusion model. In arXiv preprint arXiv:2410.13925, 2024. Ziqiao Wang, Wangbo Zhao, Yuhao Zhou, Zekai Li, Zhiyuan Liang, Mingjia Shi, Xuanlei Zhao, Pengfei Zhou, Kaipeng Zhang, Zhangyang Wang, Kai Wang, and Yang You. Repa works until it doesnt: Early-stopped, holistic alignment supercharges diffusion training, 2025. URL https: //arxiv.org/abs/2505.16792. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In arXiv preprint arXiv:2410.06940, 2024. Yuzhe Zhang, Jiawei Zhang, Hao Li, Zhouxia Wang, Luwei Hou, Dongqing Zou, and Liheng Bian. Diffusion-based blind text image super-resolution. In Conference on Computer Vision and Pattern Recognition, CVPR, 2024. Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. In Transactions on Machine Learning Research, TMLR, 2024."
        },
        {
            "title": "A ANALYSIS DETAILS",
            "content": "Training behavior  (Fig. 3)  . We provide the implementation details used to measure the training behavior shown in Fig. 3. We adopt the SiT-B/2 configuration from the SiT paper (Ma et al., 2024), which consists of 2 encoder blocks, 8 middle blocks, and 2 decoder blocks. In Fig. 3a, we plot the ℓ2 gradient norm of the encoder fθ with respect to the flow-matching loss L, i.e., fθ L, across pretraining iterations. This analysis highlights the improved gradient flow within the encoder blocks. Compared to the SiT baseline, SPRINT exhibits consistently stronger gradient propagation to the encoder as sparsity increases, leading to more effective parameter updates and faster convergencereflected in both higher CKNNA scores and lower FID values. In Fig. 3b, we report the Centered Kernel Nearest-Neighbor Alignment (CKNNA) (Huh et al., 2024) score, relaxed variant of Centered Kernel Alignment (CKA). CKNNA is commonly used to assess the semantic alignment (Yu et al., 2024) between diffusion models and large-scale self-supervised visual encoders such as DINOv2. Intuitively, given noisy input xt, CKNNA quantifies how well the intermediate features of diffusion model capture noise-invariant semantics by comparing them with DINOv2 features extracted from the corresponding clean image x0. Higher CKNNA scores indicate more semantically meaningful and noise-robust representations that align more closely with the features of the visual encoder. We follow the definition and implementation provided in the original work (Huh et al., 2024). Specifically, we compute the CKNNA score between the output of the encoder fθ on noisy inputs xt and the output of DINOv2 on clean inputs x0. We randomly sample 10K images from the ImageNet-1K validation set and report results with = 10. Finally, in Fig. 3c, we report FID values computed with 10K generated images. Consistent with previous findings (Yu et al., 2024), we observe strong negative correlation between the CKNNA values of intermediate diffusion features and FID scores. This suggests that higher alignment between diffusion features and high-quality visual representations leads to better generation quality. Roles of dense-shallow and sparse-deep features  (Fig. 4)  . In Fig. 4, we analyze the contribution of each path in SPRINT. To generate samples using only single path, we replace the feature representation of one path with that of the other. In other words, we duplicate the features from one path and concatenate the original and duplicated features before feeding them into the decoder. PCA visualization of diffusion features  (Fig. 6)  . In Fig. 6, we perform principal component analysis (PCA) of the intermediate features to better understand what the model has learned. PCA identifies the principal axes that capture the greatest variance in the feature space and is widely used to analyze representations learned by neural networks (Oquab et al., 2023). We compute PCA across patch embeddings and visualize the first three principal components as RGB channels. Specifically, we examine the outputs of the encoder fθ and the middle blocks gθ at different timesteps to observe how the feature representations evolve throughout the diffusion process. Additional PCA visualizations are provided in Fig. 32."
        },
        {
            "title": "B SPRINT WITH DIFFERENT DIFFUSION TRANSFORMERS",
            "content": "We provide details of the different diffusion transformers used in the main paper and describe how SPRINT is implemented on top of them. SiT (Ma et al., 2024). We closely follow the architecture of SiT. The SiT model is structurally analogous to Vision Transformer (ViT) (Dosovitskiy et al., 2020), consisting of sequence of identical transformer blocks that process patchified 1D token sequence. SiT adapts this for the diffusion task by incorporating timestep and class conditioning, which is injected into each block via AdaIN-zero layers. Because the architecture is simple, homogeneous stack of blocks, it is straightforward to decouple it into our encoder, middle, and decoder blocks when applying SPRINT. REPA (Yu et al., 2024). Representation Alignment (REPA)regularizes DiT by aligning hidden states with clean image features from pre-trained DINOv2 model. The architecture largely follows SiT, with the key modification being projection layer inserted at the 8th transformer block to perform the alignment. To integrate SPRINT with REPA, we place this projection layer at the corresponding"
        },
        {
            "title": "Preprint",
            "content": "Table 9: Hyperparameters used for SPRINT. SiT-B+SPRINT (Fig. 3, Tab. 4-8) SiT-XL+SPRINT (Tab. 1, 3) SiT-XLREPA+SPRINT (Tab. 2, 3) SiT-XL+SPRINT (Tab. 10) U-ViT-XL+SPRINT (Tab. 2) Architecture Target latent res. Patch size Total Num. Layers Num. fθ Layers Num. gθ Layers Num. hθ Layers Hidden dims Num. heads Pretraining config. Optimizer Learning rate Batch size Visual Encoder Token drop ratio PDG drop ratio Finetuning config. Training iterations Warmup iterations Optimizer Learning rate Batch size Token drop ratio PDG drop ratio Evaluation config. Sampler Sampling steps 32 32 2 12 2 8 2 384 6 AdamW 0.0001 256 75% 10% 75% 10% ODE 50 32 32 2 28 2 24 2 1152 16 AdamW 0.0001 256 75% 10% 100K 5K AdamW 0.0002 512/1024 75% 10% ODE/SDE 50/250 32 32 2 28 2 24 2 1152 AdamW 0.0001 256 DINOv2-B (λ = 0.5) 75% 10% 100K 5K AdamW 0.0002 512/1024 75% 10% ODE/SDE 50/250 64 64 2 28 2 24 2 1152 16 AdamW 0.0001 256 75% 10% 200K 5K AdamW 0.00015 1024 75% 10% SDE 250 32 32 2 28 2 24 2 1152 16 AdamW 0.0001 256 75% 10% 100K 5K AdamW 0.0002 512 75% 10% ODE 50 location within our sparse middle block, gθ. key consideration is that the hidden states in gθ operate on sparse token set of length , while the target DINOv2 features have full sequence length of . To resolve this, we simply apply the same token-dropping mask to the DINOv2 feature sequence, ensuring one-to-one correspondence for the alignment loss. Since DINOv2 also uses standard transformer architecture with positional encodings, aligning the corresponding tokens is straightforward. U-ViT (Bao et al., 2023). U-ViT extends the Vision Transformer with U-Net (Ho et al., 2020)- style architecture. Similar to U-Net, it stacks transformer blocks with long skip-connections between encoder and decoder stages, directly passing features from encoder to decoder. To apply SPRINT, we first conceptually decompose the U-ViT into our standard fθ, gθ, and hθ sections while preserving all original skip-connections. We then introduce our dense residual path between fθ and hθ and apply token dropping to the middle section, gθ. The U-Net skip-connections remain compatible with this design. The long-range skips between the encoder and decoder are unaffected. The shorter skip-connections within the sparse middle section naturally operate on the reduced set of tokens. This allows SPRINT to be integrated cleanly without disrupting the U-ViTs core component."
        },
        {
            "title": "C IMPLEMENTATION DETAILS AND HYPERPARAMETERS",
            "content": "C.1 TRAINING DETAILS We follow the model configuration of the original SiT implementation (Ma et al., 2024), with the only modification being single linear projection layer for sparsedense residual fusion. This adds only marginal number of parameters, approximately 0.3% of the original model size. We use pre-computed latent vectors from raw images via Stable Diffusion (Rombach et al., 2022) and Flux (Labs, 2024b) VAEs, and, following common practice, do not apply any data augmentation. For pretraining, we train SPRINT with batch size of 256, learning rate of 1e-4, fixed drop ratio of 75%, and an EMA decay rate of 0.9999. After pre-training, we switch the middle blocks to operate on the full token set for short fine-tuning stage for 100K iterations. We increase the batch size and the learning rate, following standard practice (Zheng et al., 2024; Krause et al., 2025). We found that applying linear learning rate warm-up from 2e-6 to 2e-4 over the first 5K iterations stabilizes the training. During the warm-up"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 SPRINT Pre-training Require: Input x0, Drop ratio r, Path-drop prob p, encoder fθ, middle blocks gθ, decoder hθ, condition Sample [0, 1] and ϵ (0, I) xt (1 t) x0 + ϵ ft fθ(xt, c) drop Drop(ft, r) gθ(f drop gdrop , c) gpad PadWithMask(gdrop gpad [MASK] with probability ht Fusion(ft, gpad ) ˆvt hθ(ht, c) Lvel ˆvt vt2 Update θ using θLvel 1: while not converged do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end while 14: return fθ, gθ, hθ ) drop gdrop xt RBN ft RBN RB(1r)N RB(1r)N gpad RBN Path-drop learning Sparsedense residual fusion ˆvt RBN Algorithm 2 SPRINT Fine-tuning Require: Input x0, Path-drop prob p, encoder fθ, middle blocks gθ, decoder hθ, condition 1: while not converged do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: end while 12: return fθ, gθ, hθ Sample [0, 1] and ϵ (0, I) xt (1 t) x0 + ϵ ft fθ(xt, c) gt gθ(ft, c) gt [MASK] with probability ht Fusion(ft, gt) ˆvt hθ(ht, c) Lvel ˆvt vt2 Update θ using θLvel xt RBN ft RBN gt RBN Path-drop learning Sparsedense residual fusion ˆvt RBN stage, we use an EMA decay rate of 0.999, which is restored to 0.9999 afterward. For both training phases, we introduce path-drop learning strategy to maximize the effectiveness of our path-drop guidance, in addition to the standard class-condition dropping. Specifically, following the practice in CFG training, we randomly drop the features of the sparsedeep path with probability of 10% and replace the dropped features with mask tokens. This random dropping is performed independently of the condition dropping in CFG. To accelerate training, we adopt mixed-precision (bf16) training and apply gradient norm clipping at 1.0 during both pretraining and finetuning. Detailed hyperparameters are summarized in Table 9. All experiments are conducted on 8 NVIDIA A100 80GB GPUs. C.2 EVALUATION DETAILS Metrics. We evaluate generation performance using several standard metrics: FDD (Stein et al., 2023) (Fréchet Distance on DINOv2), FID (Heusel et al., 2017) (Fréchet Inception Distance), IS (Salimans et al., 2016) (Inception Score), and Precision/Recall (Kynkäänniemi et al., 2019). Unless otherwise specified, we follow the evaluation protocol of (Dhariwal & Nichol, 2021) and report results using 50K generated samples. FID is the most widely used metric, measuring the feature distance between the distributions of real and generated images. It relies on the Inception-V3 network and assumes both feature distributions follow multivariate Gaussian distributions. IS also uses the Inception-V3 network but instead evaluates the quality and diversity of generated images by computing the KL-divergence between the marginal label distribution and the conditional label distribution predicted from logits."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 3 SPRINT Inference Require: encoder fθ, middle blocks gθ, decoder hθ, condition c, guidance scale w, sampling steps v(xt, ) hθ(Fusion(M, fθ(xt, )), ) , sampler 1: x1 (0, I) 2: for = to 1 do 3: if Path-drop guidance then 4: 5: 6: 7: end if 8: v(xt, c) v(xt, ) + (cid:0)v(xt, c) v(xt, )(cid:1) 9: xt 1 10: 11: end for 12: return x0 S(xt, v(xt, c)) else v(xt, ) hθ(Fusion(gθ(fθ(xt, c), c), fθ(xt, )), ) Path-drop guidance Classifier-free guidance FDD adopts the same formulation as FID but replaces Inception features with DINOv2 features, which provide stronger semantic alignment and robustness to noise. Notably, FDD has been shown to be more reliable for diffusion models (Stein et al., 2023; Karras et al., 2024b). Finally, Precision measures the fraction of generated images that are realistic, while Recall measures the fraction of the training data manifold covered by generated samples. Guidance scale. We use the following formulation for guidance sampling (Ho & Salimans, 2022): v(xt, c) = v(xt, ) + (v(xt, c) v(xt, )) , (5) where denotes the guidance scale. In standard Classifier-Free Guidance (CFG), the unconditional velocity v(xt, ) is computed using the full model path with null condition. In contrast, our Path-Drop Guidance (PDG) replaces the unconditional branch with weaker network, as defined in Eq. 4. For the results in Tables 1 and 2, we consistently use CFG scale of 1.4 with the ODE sampler across all methods. For Table 3, we adopt the SDE sampler to compare baselines. Under this setting, we use CFG scale of 1.35 to achieve the best FID and 2.0 to achieve the best FDD. For our PDG sampling, the optimal scales are 1.35 for FID and 1.9 for FDD. Moreover, for the improved baseline models we reproduced (e.g., improved SiT-XL and SiT-XLREPA), we use the optimal CFG scale of 1.35 and 2.0 for FID and FDD, respectively. For our model in Table 10, we use CFG scale of 1.35 and 1.8 for FID and FDD, respectively, and 1.25 and 1.5 for PDG scale."
        },
        {
            "title": "D COMPUTATION ANALYSIS",
            "content": "We use the SiT-XL/2 configuration for evaluating computational analysis below. FLOPs. To estimate the total training FLOPs, we measure the forward-pass FLOPs over 100 iterations with batch size of 256, average the results, and multiply by the total number of training iterations. For inference FLOPs, we sum the forward-pass FLOPs across all sampling timesteps using batch size of 32 and report the average over both timesteps and batch size. This procedure provides consistent and reproducible measure of computational cost across methods. Note that we report floating-point operations (FLOPs), not multiplyaccumulate operations (MACs), where one MAC corresponds to approximately two FLOPs. Training speed. Here, we compare the actual run-time performance of each method on Stable Diffusion VAE latents. For all token-dropping methods, we use fixed drop rate of 75%. At the ImageNet resolution of 2562, SPRINT achieves pretraining speed of 5.2 iters/sec, which is more than 2 faster than the SiT baseline (2.5 iters/sec) and clearly outperforms other tokendropping baselines, including MaskDiT (4.57 iters/sec), MicroDiT (3.9 iters/sec), and Tread (4.7 iters/sec). At the higher ImageNet resolution of 5122, SPRINT maintains its advantage, achieving 2."
        },
        {
            "title": "Preprint",
            "content": "Table 10: Comprehensive performance comparison on ImageNet 512 512 class-conditioned generation with classifier-free guidance. / indicate whether lower or higher values are better, respectively. our reproduction with architectural improvements. All metrics are evaluated with 250 sampling steps using the SDE sampler. Training and inference TFLOPs are measured with the DeepSpeed library. Method Epochs #Params. Training TFLOPs (106) Inference TFLOPs FDD FID Pre. Rec. ADM (Dhariwal & Nichol, 2021) Simple diffusion (U-Net) Simple diffusion (U-ViT-L) MaskDiT (Zheng et al., 2024) DiT-XL (Peebles & Xie, 2023) SiT-XL (Ma et al., 2024) SiT-XL + SPRINT + SPRINTPDG 400 800 800 800 600 400 400 730M 675M 675M 677M 677M 327.2 366.6 366. 184.8 184.8 1.029 0.952 0.952 0.954 0.471 53.6 46.9 2.85 4.28 4.53 2.50 3.04 2.62 2.23 2.55 0.84 0.83 0.84 0. 0.83 0.84 0.53 0.56 0.54 0.57 0.57 0.55 iters/secover 2.5 faster than the SiT baseline (0.79 iters/sec)and again surpassing MaskDiT (1.77 iters/sec), MicroDiT (1.54 iters/sec), and Tread (1.79 iters/sec). This acceleration results in substantial reductions in wall-clock training time and GPU consumption, making large-scale diffusion model training significantly more practical and resource-efficient. VRAM memory consumption. In addition to reducing computational cost, SPRINT significantly lowers GPU memory requirements during training. For example, when training with batch size of 32 and image resolution 2562 on single GPU, SPRINT requires only 19.6 GB of memory, compared to 29.6 GB for the baseline SiT-XL/2 model. At resolution 5122, our SPRINT requires 37.9 GB, whereas the baseline SiT-XL/2 model requires 77.7 GB. This represents 33.8% reduction in memory usage at 2562 and 51.2% reduction at 5122. Such efficiency enables training with larger batch sizes or higher resolutions on the same hardware, making our method more accessible for researchers with limited GPU resources. Importantly, this reduction comes without sacrificing performance, underscoring the practicality of SPRINT in resource-constrained environments."
        },
        {
            "title": "E BASELINES",
            "content": "E.1 BASELINE DETAILS ON TABLE 1 For fair system-level comparison in Tab. 1, we apply the same pretraining and finetuning strategies, along with identical transformer block configurations, fixed drop ratio of 75%, and consistent evaluation hyperparameters, across all baselines. Progressive training. We adopt the same network architecture for progressive training. The model is first pretrained on 128 128 images and then finetuned on 256 256 images, with positional embeddings resized using bilinear interpolation during the resolution transition. This approach is slightly more efficient than SPRINT in terms of computational cost per iteration, achieving 25.8 vs. 31.5 GFLOPs (109) at 1M training iterations. However, despite the efficiency advantage, progressive training lags behind SPRINT in performance and even fails to match the baseline SiT results, underscoring its limited effectiveness. MicroDiT (Sehwag et al., 2025). MicroDiT introduces deferred masking, where token dropping is applied only after several additional patch-mixing blocks. These modules allow local patch tokens to fuse information, enriching their semantic content. Following the original protocol, we modify the SiT-XL/2 model by inserting patch-mixing modules composed of six transformer blocks. As shown in Tab. 1, this modification substantially increases computational cost and the number of parameters. Nevertheless, despite the additional overhead, MicroDiT underperforms relative to SPRINT, highlighting that the deferred masking strategy and additional compute does not translate into superior efficiency or accuracy."
        },
        {
            "title": "Preprint",
            "content": "Table 11: Performance of SiT-XL/2 and SPRINT across NFEs. Results are reported at 1M training iterations using the ODE sampler with 50K generated samples. Method SiT-XL/2 + SPRINT (Ours) Gain SiT-XL/2 + SPRINT (Ours) Gain SiT-XL/2 + SPRINT (Ours) Gain SiT-XL/2 + SPRINT (Ours) Gain SiT-XL/2 + SPRINT (Ours) Gain SiT-XL/2 + SPRINT (Ours) Gain NFE FDD FID IS Pre. Rec. 200 200 150 150 100 100 50 50 25 25 10 10 132.3 120.4 +11. 133.1 121.1 +12.0 134.7 122.2 +12.4 140.6 126.5 +14.1 156.1 138.2 +17.9 222.4 191.7 +30.7 2.18 2.08 +0. 2.19 2.09 +0.1 2.22 2.10 +0.12 2.34 2.19 +0.15 2.91 2.59 +0.32 7.37 6.29 +1.08 249.9 272.2 +22. 249.6 271.5 +21.9 248.4 271.0 +22.6 244.0 267.7 +23.7 234.4 256.3 +21.9 187.3 211.3 +24.0 0.81 0.81 0.81 0.81 0.81 0.81 0.80 0.81 0.80 0.80 0.74 0.74 0.59 0.60 0.59 0.59 0.58 0.59 0.58 0.59 0.57 0.58 0.54 0.54 Tread (Krause et al., 2025). Tread introduces token-routing strategy in which randomly dropped tokens at early layers are routed directly to deeper layers. While this resembles SPRINT in that tokens bypass the middle layers, the two approaches differ fundamentally. In Tread, only the dropped tokens are bypassed, forcing the middle block to encode local noise information in order to estimate velocity. In contrast, SPRINT employs full dense residual path that delivers complete local noise information to the decoder, freeing the middle block to focus on modeling global contextual information. This design choice makes SPRINT highly effective under aggressive dropping ratios (75%), whereas Tread fails under the same setting. We follow the implementation details provided in the original Tread paper. E.2 MORE DISCUSSION ON OTHER BASELINES MaskDiT (Zheng et al., 2024). MaskDiT introduces an additional reconstruction task for masked tokens alongside the diffusion objective, encouraging the model to recover missing information and thereby improve contextual understanding. While this approach provides some efficiency gains, it requires an extra decoder module, increasing the model size from 675M to 730M and adding computational overhead. Moreover, its effectiveness is limited to moderate dropping ratios (e.g., 50%). As shown in Tab. 3, these limitations restrict its overall efficiency compared to our framework. Specifically, MaskDiT requires 1600 training epochs to reach 65.4 FDD and 2.28 FID, whereas SPRINT surpasses this in just 200 epochs with 61.8 FDD and 2.01 FID. This underscores the superior effectiveness and efficiency of SPRINT over MaskDiT. MDT (Gao et al., 2023). The Masked Diffusion Transformer (MDT) also aims to improve the contextual understanding of diffusion models through token dropping. They designed masked diffusion transformer with encoder-decoder split of the diffusion transformer, where the encoder processes masked tokens and forwards them to the decoder along with remaining tokens through additional side-interpolator model. It adds additional long shorcut connections between encoder blocks along with long full token input to all decoder blocks. MDT optimizes the reconstruction loss on masked tokens along with diffusion loss. The added complexity in the training and architectural changes is aimed for better generative performance. Similar to MaskDiT, this work also operates only with moderate token dropping ratios (e.g., [30%, 50%]). MDT does not work well with high token dropping ratio such as 75%."
        },
        {
            "title": "F ADDITIONAL QUANTITATIVE RESULTS",
            "content": "F.1 IMAGENET 512X512 EXPERIMENT In the main text, we have already demonstrate that SPRINT outperforms many existing training methods and state-of-the-art models at 2562 class conditional image generation. In this experiment, we train our models to generation images at 5122 resolution. Tab. 10 compares our method with strong baselines on ImageNet-1K class-conditional generation at 5122. We pre-train SPRINT for 1.8M iterations and finetune for 200K iterations (refer to Table 9). SPRINT achieves comparable or better generation quality while using substantially fewer training TFLOPs (106): only 184.8 at 400 epochs, versus 366.6 for SiT-XL at 600 epochs. This demonstrates much faster convergence, reaching better FID with nearly 2 lower training cost. At inference, PathDrop Guidance provides further benefits, nearly halving inference TFLOPs (0.471 vs. 0.952) while improving FDD. Overall, SPRINT consistently demonstrates efficiency compared to the baselines at 5122, by combining lower training and inference costs. Refer to Fig. 31 for qualitative results. F.2 PERFORMANCE WITH FEW-STEP GENERATION Tab. 11 compares SiT-XL/2 and SiT-XL/2 + SPRINT across lower inference steps (NFEs), an essential setting for achieving efficient and practical image generation. In real-world scenarios, reducing the number of function evaluations (NFEs) directly translates to faster sampling and lower inference cost, often at the expense of generation quality. While both models perform similarly at large NFEs (200), SPRINT consistently outperforms the baseline as the number of steps decreases. At 50 steps, SPRINT improves FID from 2.34 to 2.19 and IS from 244.0 to 267.7, and at only 10 steps it achieves much larger gain, reducing FID from 7.37 to 6.29 and improving IS from 187.3 to 211.3. These results highlight that SPRINT is more competitive under low-step inference. This demonstrates the strong representational power of fused denseshallow and sparsedeep features."
        },
        {
            "title": "G ADDITIONAL QUALITATIVE RESULTS",
            "content": "G.1 VISUAL COMPARISON ON IMAGENET 256 256 Figure 8: SPRINT improves visual quality over baseline with only 57% of inference FLOPs (additional examples). We present samples from two SiT-XL/2 + REPA models after 1M training iterations, where SPRINT is applied to one of the models. For our approach, we further incorporate the proposed Path-Drop Guidance (PDG), yielding higher visual quality compared to the REPA."
        },
        {
            "title": "Preprint",
            "content": "G.2 UNSELECTED GENERATED RESULTS BY SPRINT ON IMAGENET 256 256 Figure 9: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = park bench (706) Figure 10: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = park bench (706)"
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = hammerhead, hammerhead shark (4) Figure 12: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = hammerhead, hammerhead shark (4)"
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = magpie (18) Figure 14: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = magpie (18)"
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = bullfrog, Rana catesbeiana (30) Figure 16: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = bullfrog, Rana catesbeiana (30)"
        },
        {
            "title": "Preprint",
            "content": "Figure 17: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = tusker (101) Figure 18: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = tusker (101)"
        },
        {
            "title": "Preprint",
            "content": "Figure 19: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = beagle (162) Figure 20: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = beagle (162)"
        },
        {
            "title": "Preprint",
            "content": "Figure 21: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = coffeepot (505) Figure 22: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = coffeepot (505)"
        },
        {
            "title": "Preprint",
            "content": "Figure 23: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = computer keyboard, keypad (508) Figure 24: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = computer keyboard, keypad (508)"
        },
        {
            "title": "Preprint",
            "content": "Figure 25: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = convertible (511) Figure 26: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = convertible (511)"
        },
        {
            "title": "Preprint",
            "content": "Figure 27: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = cornet, horn, trumpet, trump (513) Figure 28: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = cornet, horn, trumpet, trump (513)"
        },
        {
            "title": "Preprint",
            "content": "Figure 29: Unselected generation results of SiT-XL/2 + SPRINTCFG. We use classifier-free guidance with = 4.0. Class label = cowboy hat, ten-gallon hat (515) Figure 30: Unselected generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 4.0. Class label = cowboy hat, ten-gallon hat (515)"
        },
        {
            "title": "Preprint",
            "content": "G.3 GENERATED RESULTS BY SPRINT ON IMAGENET 512 512 Figure 31: Generation results of SiT-XL/2 + SPRINTPDG. We use our path-drop guidance with = 3.0."
        },
        {
            "title": "Preprint",
            "content": "G.4 ADDITIONAL FEATURE PCA VISUALIZATION In the main text (Figure 6), we analyzed PCA visualizations of features from fθ and gθ, showing that SPRINT learns more noise-invariant and semantically vivid representations than the SiT baseline across diffusion timesteps. Figure 32 presents additional examples of these denseshallow and sparsedeep features learned by SPRINT, contrasted with those from standard SiT-XL/2 model trained with full tokens. Figure 32: SPRINT improves feature semantics (additional examples). We visualize PCA features of fθ and gθ from two SiT-XL/2 models at 400K iterations. The top rows show the model trained with SPRINT, while the bottom rows show the baseline. Compared to the baseline, features from SPRINT exhibit clearer semantic structure across both images."
        },
        {
            "title": "H LIMITATION AND FUTURE WORK",
            "content": "Our study is limited by the available computational resources, which prevented us from conducting experiments on large-scale text-to-image or video diffusion models. Exploring the scalability of SPRINT in such settings remains an important direction. In particular, the quadratic complexity of transformers becomes increasingly prohibitive as model size and input resolution grow. Since SPRINT is specifically designed to reduce redundant computation in deeper layers, we expect it to be especially beneficial for large-scale architectures where efficiency bottlenecks are most severe. Thus, extending SPRINT to other modalities such as video, 3D, or multi-modal generative models is an exciting direction. These domains pose even greater computational and memory challenges, particularly in video, where the temporal dimension compounds complexity, making our sparsedense residual fusion especially relevant for future research. Another promising avenue is the integration of SPRINT with recent advances in efficient attention mechanisms and scalable training strategies. Such combinations could amplify the benefits of our approach, further reducing training and inference costs while maintaining or improving performance."
        }
    ],
    "affiliations": [
        "KAIST",
        "Korea University",
        "Snap Inc."
    ]
}