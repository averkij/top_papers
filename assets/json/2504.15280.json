{
    "paper_title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs",
    "authors": [
        "Chun-Hsiao Yeh",
        "Chenyu Wang",
        "Shengbang Tong",
        "Ta-Ying Cheng",
        "Rouyu Wang",
        "Tianzhe Chu",
        "Yuexiang Zhai",
        "Yubei Chen",
        "Shenghua Gao",
        "Yi Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 0 8 2 5 1 . 4 0 5 2 : r Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs Chun-Hsiao Yeh1* Chenyu Wang2* Shengbang Tong3 Ta-Ying Cheng4 Rouyu Wang2 Tianzhe Chu6 Yuexiang Zhai1 Yubei Chen5 Shenghua Gao2,6 Yi Ma1,2, 1UC Berkeley 2TranscEngram 3NYU 4University of Oxford 5UC Davis 6HKU Figure 1. We present All-Angles Bench, rich-annotated benchmark with over 2,100 Q&A pairs from 90 diverse scenes for evaluating multi-view understanding of MLLMs. Left and Middle: An example question setup of multiple views capturing the same scene and the corresponding questions. Right: Accuracies of six notable MLLMs across different question categories."
        },
        {
            "title": "Abstract",
            "content": "Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and crossview correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, benchmark of over 2,100 human carefully annotated multi-view questionanswer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) Equal Contribution specifically test models geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our AllAngles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multiview understanding. The project and benchmark are publicly available at https://danielchyeh.github. io/All-Angles-Bench/. Figure 2. Overview of All-Angles Bench. Our benchmark targets comprehensive view of multi-view understanding, spanning six primary question types. These question types are designed to investigate several major aspects of 3D scene understanding, from creating correspondence between objects to associating relative object and camera poses. 1. Introduction Multi-view understanding is fundamental challenge in bridging machine and human-level understanding [13, 19, 58] because it underpins an agents ability to perceive the environment consistently from diverse viewpoints. By ensuring geometric coherence and cross-view consistency, agents can accurately reconstruct scene layouts and object relationships capabilities critical for effective navigation, manipulation, and interaction in the real world [43, 44]. The recent advancement in Multimodal Large Language Models (MLLMs) demonstrates strong capabilities in high-level reasoning and task planning [1, 4, 9, 17, 21, 27], and thus the feasibility of directly using MLLMs as embodied agents is an intriguing research challenge [14, 20, 25, 31, 39, 60]. However, such capacities alone are insufficient for generalist embodied agents operating in the real world, where comprehensive 3D scene understanding and robust multiview reasoning are pivotal [10, 22, 23]. Recent studies survey that MLLMs lacking multi-view scene understanding often commit agent manipulation and navigation errors such as misjudge the target distance, skip partially occluded obstacles stemming from limited awareness of multi-view geometry and object relationships [57, 65]. Since these models must navigate, manipulate, and make decisions in real world environments, it is vital to evaluate (and ultimately strengthen) their multi-view understanding capabilities. Yet, this aspect remains underexplored in details. To this end, we raise two questions: (1) Do MLLMs possess the ability to understand multiple viewpoints simultaneously? and (2) What are the key challenges in MLLMs to gain better multi-view understanding? To address these questions and in light of the lack of benchmarks to evaluate multi-view reasoning, we introduce AllAngles Bench, comprising over 2,100 carefully humanannotated question-answer pairs across 90 diverse multiview scenes in real world [18, 24]. We define six tasks counting, attribute identification, relative distance, relative direction, manipulation, and camera pose estimation with focus on evaluating MLLMs geometric understanding and its ability to align information consistently across multi-view scenes. To better evaluate whether models truly possess multi-view capabilities, we also propose paired question scheme by creating second question with the same content but with slightly changed wording/order of views. We benchmark 27 representative MLLMs (including Gemini-2.0 [45], Claude-3.7 [1], and GPT-4o [40]) against human evaluators. As revealed in Figure 1, substantial performance gap persists between current MLLMs and human evaluators. To better understand why MLLMs fall short of human-level multi-view reasoning, we conduct an in-depth analysis of commonly failed questions and tasks, and derive two key findings. First, MLLMs struggle to identify the same object across multiple views. We further test whether chain-ofthought prompting technique that has proven effective in other reasoning tasks [42, 55, 62] could address this limitation. However, our experiments reveal that these linguistic strategies do not provide consistent improvements across models for multi-view reasoning. This suggests that more fundamental domain-specific refinements to multiview awareness modules or training data are necessary for MLLMs to fully internalize cross-view consistency. Second, MLLMs often fail to establish correspondence between Figure 3. All-Angles Bench construction pipeline. (1) We collect and curate 90 diverse multi-view scenes and design six tasks that emphasize multi-view reasoning. (2) We generate initial questions via an MLLM, then refine and validate them through human annotation to ensure correctness, clarity, and domain relevance. (3) We create paired questions by systematically rephrasing or altering each view perspective while preserving their underlying visual correspondences to evaluate models cross-view consistency. final quality-control step removes inconsistent or ambiguous pairs. Note that counting and camera pose estimation tasks utilize all available views per query, whereas other tasks employ two randomly selected viewpoints. different viewpoints. We visualize how models infer scene layouts from multiple perspectives, revealing consistent inability to accurately estimate camera poses, which in turn impedes performance on tasks like relative direction and object manipulation. We hope these insights will be helpful to future research towards bringing more better multi-view capabilities in MLLMs. changes in object positions, orientations, or configurations across views; (6) Camera Pose Estimation: Evaluating the capacity to estimate viewpoint arrangements or scene layouts from multi-view inputs. Each task addresses specific dimension of multi-view reasoning, ensuring thorough assessment of MLLMs geometric understanding and their ability to align information across perspectives. 2. All-Angles Bench The ability to integrate observations of the scene layout from multiple viewpoints is critical for the geometric understanding of MLLMs, which can significantly help with capturing and anticipating interaction outcomes of real-world complex environments safely. 2.1. Overview of All-Angles Bench Most existing benchmarks to evaluate MLLMs primarily rely on single-view or egocentric data, leaving the multiview consistency and correspondence capabilities of current MLLMs largely unexamined. To address this gap, we introduce All-Angles Bench, which comprehensively evaluates MLLMs performance across six task categories in multi-view scenarios: (1) Counting: Enumerating objects across viewpoints without double-counting or overlooking occluded elements; (2) Attribute Identification: Recognizing key properties (e.g., pose, color, shape, orientation) consistently across different viewing perspectives; (3) Relative Distance: Estimating object distances when presented with multiple views; (4) Relative Direction: Testing the understanding of directional relationships between objects across different views; (5) Object Manipulation: Inferring Our All-Angles Bench is derived from curated selection of 90 diverse multi-view scenes sourced from Ego-Exo4D [18] and EgoHumans [24], totaling 2,132 questionanswer pairs. Each question is structured as multiple-choice questionnaire with three options, only one of which is correct. By collecting multi-view data from varying directions and viewpoints, we generate paired question-answers that highlight differences in appearance, occlusion, and spatial relationships across perspectives while preserving the underlying visual correspondences. The benchmark scenes encompass wide range of activities (e.g., basketball, soccer, cooking, music playing) and environments (e.g., offices, gym, repair store, kitchen, playground) to ensure broad coverage of real-world scenarios where cross-view reasoning is essential. As shown in Figure 2, each question targets one of the six task categories outlined above to provide challenging yet realistic platform for evaluating MLLMs geometric understanding and multi-view correspondence. 2.2. Benchmark Collection Process We build benchmark collection pipeline to effectively generate high quality question-answer pairs for multi-view understanding, as shown in Figure 3. To ensure the benchmark quality, all questions were manually annotated by human annotators after collecting and clipping the raw questions. Data Collection & Question Type Design. We begin by manually selecting 90 diverse multi-view scenes from EgoExo4D [18] and EgoHumans [24], covering broad spectrum of activities and environments (e.g., indoor settings, residential areas, industrial spaces) to ensure varied visual contexts. Since the focus of this benchmark is on multiview analysis, we ensure each scene includes footage captured from at least three viewpoints. We then manually design six task categories spanning fundamental aspects of multi-view understanding: from enumerating and identifying objects across multiple viewpoints (counting, attribute identification), to capturing spatial relationships (relative distance, relative direction), and analyzing how objects change across views or camera perspectives (object manipulation, camera pose estimation). Please see Appendix for further details on the specific question design. Question Creation & Human Annotation. After collecting our multi-view scenes and designing question templates for each task category, we leverage an MLLM [40] to generate initial questions grounded in the multi-view visual data. Specifically, we generate three questions per category for each multi-view scene except generating one question for camera pose estimation. We utilize all available views per query for counting and camera pose estimation tasks, whereas other tasks employ two randomly selected viewpoints. We hire eight human annotators who carefully examine each question along with the associated multiview images, removing invalid entries and refining question phrasing in Figure 3 (middle). This meticulous manual process also involves revising incorrect answer choices and finally annotating the single correct answer. For example, in attribute identification, the MLLM might inconsistently describe an object across two different camera views. In relative direction, it might offer contradictory options e.g., facing the right side of the camera view vs. facing the stove that actually reference the same orientation. Detailed instructions and guidelines for human annotator can be found in the Appendix. Paired-Question Generation & Human Quality Check. To rigorously evaluate whether MLLMs truly grasp multiview concepts, we generate paired questions by systematically rephrasing (i.e., orientation) or altering the original queries (e.g., views) while preserving their underlying visual correspondences and the question structure. For instance, an attribute identification question such as Is there man wearing yellow hoodie in View 1? Identify him in View 2. can be paired with There is man wearing yellow hoodie in View 2? Identify him in View 1, ensuring both questions reference the same individual despite different viewpoint. Likewise, for relative direction, we swap orientations (e.g., left vs. right) and reference views (View 1 Figure 4. Statistical overview of All-Angles Bench. The pie chart shows the distribution of 6 sub-tasks of multi-view understanding. The bar plot illustrates the percentage breakdown by primary and paired question-answers of each sub-task. vs. View 2). This process is similar to language manipulation in [56, 66] but requires careful verification of view-toview consistency. final human quality check ensures geometric alignment between the paired questions, resulting in 85.3% of questions having paired counterparts (counting task is not involved) thereby testing whether MLLMs genuinely understand multi-view scenarios or merely guess answers. The statistics of benchmark is shown in Figure 4. 3. MLLMs Have Multi-View Understanding? 3.1. Evaluation Setup Benchmark Models. We evaluate broad spectrum of MLLMs spanning diverse model families, parameter scales, and training paradigms. On the closed-source side, we include three of the most prominent model families Gemini-2.0 [45], Claude-3.7 [1], and GPTFor open-source models, we examine recent 4o [40]. breakthroughs from Deepseek-VL2 [52], Qwen2.5-VL [4], InternVL2.5 [8], Cambrian [46], LLaVA-OneVision [28], LLaVA-NeXT-Video [63], and OVIS [36]. In all experiments, we follow standard protocols and set the temperature to zero unless otherwise specified. Human Evaluation. We randomly select subset of 250 questions from our All-Angles Bench encompassing all six task categories for evaluation by human annotators, each of whom independently answers every question. For fair comparison, we also report performance of Gemini-2.0Flash, Claude-3.7-Sonnet, GPT-4o, Qwen2.5-VL-72B, and InternVL2.5-38B on this subset. 3.2. Results As the primary results shown in Table 1, there remains substantial performance gap between both of closedand open-source MLLMs and human-level multi-view understanding. We post several findings we observe. Methods Avg. Performance Against Human (250 Q&As) Human Level GPT-4o Gemini-2.0-Flash Claude-3.7-Sonnet 82.0 52.4 58.4 52.8 InternVL2.5-38B 60.8 Qwen2.5-VL-72B 58.4 Closed-source Models Open-source Models GPT-4o Gemini-1.5-Pro Gemini-1.5-Flash Gemini-2.0-Flash Claude-3.5-Sonnet Claude-3.7-Sonnet 47.8 47.4 46.6 52.3 48.2 50."
        },
        {
            "title": "Attribute",
            "content": "Ca m.Pose"
        },
        {
            "title": "C ounting",
            "content": "M anipul. Rel. Dir. Rel. Dist. Multiple-Choice Answer 93.3 66.7 62.2 60.0 73.3 73.3 66.8 59.8 62.9 68.4 63.2 68. 65.3 70.5 59.5 66.6 73.9 80.4 79.4 62.7 77.5 61.9 65.5 70.5 75.5 79.4 59.8 59.0 63.7 64.5 73.4 64.8 73.6 88.9 16.7 38.9 38.9 27.8 22.2 35.8 33.5 43.8 33.0 33.0 35.8 27.8 24.4 15.9 18.2 28.4 31.3 27.3 22.2 29.5 26.7 21.6 17.0 29.5 26.7 19.9 25.6 20.5 22.2 26.7 12.5 27.8 86.3 52.9 64.7 37.3 70.6 52.9 43.0 39.4 35.9 64.9 41.8 41. 39.0 39.0 42.6 47.8 48.6 56.6 52.6 45.0 55.4 49.0 53.4 49.4 56.6 53.8 33.1 30.7 38.2 39.4 45.4 42.2 46.2 72.0 40.0 48.0 38.0 42.0 44.0 42.6 45.2 43.9 41.0 41.2 40.1 42.6 46.2 34.2 36.6 41.6 45.2 39.7 37.2 43.7 42.0 34.0 43.5 44.3 46.2 33.0 27.3 37.2 44.5 45.6 32.6 45.2 79.5 53.8 56.4 56.4 64.1 61.5 38.9 38.6 33.2 41.8 43.5 46. 32.7 33.5 30.7 35.8 40.3 49.7 43.5 36.4 54.3 35.5 36.1 41.2 46.3 50.6 33.0 32.1 35.2 35.2 46.3 37.2 46.6 95.7 63.8 68.1 80.9 68.1 76.6 51.2 55.1 52.4 58.9 55.3 56.7 51.6 54.7 48.8 54.7 54.5 58.7 59.3 53.8 60.7 51.4 56.9 54.7 56.1 59.7 43.5 37.9 43.7 52.0 60.3 50.8 61.9 Table 1. Evaluation results for 27 MLLMs. We consolidate performance from both closed-source and open-source MLLM evaluations. We use deeper-gray to highlight the top result among all models in each sub-task, while light-gray marks the second-best result. Finding 1: era pose estimation poses challenges for MLLMs. Simple task for human like coarse camFinding 2: Certain open-source MLLMs surpass closed-source ones in orientation-sensitive tasks. While humans approaching near-perfect accuracy on multiple tasks in our All-Angles Bench, both openand closedsource models often struggle. For example, in camera pose estimation, human annotators achieve 88.9% accuracy when ordering multiple camera perspectives, whereas stateof-the-art MLLMs such as Gemini-2.0-Flash, Qwen2.5VL-72B, and InternVL2.5-38B trail behind over 50% margins. Many open-source MLLMs perform even worse than random guessing. Common errors include failures to reconcile viewpoint transitions and misinterpretations of geometric relationships, underscoring the persistent gap between human-level capabilities and current MLLM performance. Interestingly, Ovis2-34B [36] and Qwen2.5-VL-72B [4] outperform leading closed-source models such as Gemini2.0 [45] and Claude-3.7-Sonnet [1] on object manipulation and relative direction. We observe that Qwen2.5-VL-72B integrates robust video understanding and fine-grained visual grounding modules (as highlighted in its model report), positioning it well to capture how objects re-orient across different viewpoints. The specialized, video-focused training regimes observed in these open-source models, which emphasize frame-by-frame orientation tracking and spatial grounding crucial for handling multi-view scenes. While it is unclear whether closed-source models train with similar strategies, this findings can still be good indicator that domain-specific refinement can yield better performances in tasks tackling orientation and geometric reasoning. 3.3. MLLMs Robustness on Paired Questions While the correctness of one single question indicates how often model answers an isolated question correctly, it does not capture whether the model remains consistent when presented with semantically equivalent queries from different viewpoints or rephrasings. To investigate this, we also propose to look into the proportions of questions where the answers are inconsistent with one another. First, we classify each paired instance into three scenarios: 1) CC (Both Correct) when the model answers both the primary and paired question correctly, 2) WW (Both Wrong) when it fails both versions, and 3) IC (Inconsistent) when the model answers one version correctly but fails the other. We are particularly interested in the case of IC, as this shows the number of questions where the model answered correctly but does not in fact reflect correct multi-view understanding, as simply changing the order or rephrasing the question leads to wrong answer. As shown in Figure 5, we report the proportions of IC (inconsistent) outcomes across six leading MLLMs three open-source (Ovis2-34B, Qwen2.5-VL-72B, InternVL2.538B) and three closed-source (GPT-4o, Gemini-2.0-Flash, Claude-3.7-Sonnet). We have several observations: 1) GPT4o exhibits notably high inconsistency score IC (around 70%) on relative distance tasks, whereas the other five models generally have around 40% inconsistency in this category, 2) All models struggle with relative direction; all surpasses 40% inconsistency IC, highlighting the challenge of reasoning about orientation shifts in multi-view scenarios, 3) Gemini-2.0-Flash and Claude-3.7-Sonnet remain fairly balanced inconsistency across overall question types, while Ovis2-34B and GPT-4o vary significantly across tasks. 4. Why Do MLLMs Struggle with Multi-View Understanding? To investigate specific weaknesses of MLLMs in multiview comprehension, we evaluate each question type in our All-Angles Bench. We select the top-performing closedsource and open-source MLLMs in our benchmark and systematically identify where these models succeed or fail in understanding multi-view scenarios. 4.1. Failure of Multi-View Correspondence We first investigate the multi-view counting task since we are curious about the discrepancy between egocentric view and multi-view counting. We begin our analysis by examining counting questions especially counting on how many people in total are in the scene. We find that MLLMs typically succeed in the complete-visibility in one view scenario (i.e., when all individuals are visible within single view), but frequently fail in the partial-visibility across multiple views scenario when partial information is distributed across multiple viewpoints (e.g., Person and in View 1, and Person and in View 2). As illustrated in Figure 6, GPT-4o occasionally handles these scenarios by simply counting the number of people per view and choosing the highest count, neglecting to reconcile individuals across different perspectives and thus leading to errors. Can Reasoning Injection Improve MLLMs Ability? To investigate whether linguistic reasoning can enhance MLLMs multi-view understanding, we randomly select 55 scenes from our 90-scene All-Angles Bench, excluding those with only single person or with insufficient partialvisibility. In each chosen scene, all individuals are visible in at least one camera view (see Figure 6, left). We then create paired version of these scenes by manually cropping footage so that key information is split across multiple viewpoints (e.g., Person and in View 1, and Person and in View 2). This setup enables fair comparison of MLLMs performance under the same set of completevisibility versus partial-visibility conditions. Prompting techniques have shown promise in enhancing the reasoning and problem-solving capabilities of large models across diverse tasks. Motivated by these findings, we explore whether such linguistic prompts can also bolster the visual-spatial proficiency of MLLMs in multi-view settings. Specifically, we introduce an Identification CoT strategy, which instructs the model to (1) provide detailed description of each visible individual noting appearance, clothing, orientation, and interactions with nearby people or objects, (2) cross-reference these descriptions across all views to avoid double-counting, and (3) provide final tally of unique entities. The detailed prompt of Identification CoT could be found in Appendix. We also report two additional CoT strategies, Zero-Shot CoT and Self Consistency /w CoT which were used in [55], for comparison. We evaluate three prompting strategies Zero-Shot CoT, Self-Consistency, and Identification CoT across three leading MLLMs: GPT-4o [40], Ovis2-34B [36], and InternVL2.5-38B [9] chosen for their varying levels of counting proficiency. As GPT-4os results shown in Figure 7, both Zero-Shot CoT and Self-Consistency yield relative gains of approximately 15% each over the noprompting baseline. Notably, Identification CoT provides substantial improvement under partial-visibility conditions, suggesting that explicit entity descriptions and crossview consistency checks are pivotal for accurate reasoning when some individuals or objects are only partially visible across different views. However, when the model already possesses robust multi-view counting capabilities (e.g., InternVL2.5-38B), the benefits of additional promptFigure 5. Paired question-answers inconsistency across 6 MLLMs. We report the proportions of IC and CC + WW. Notably, GPT-4o struggles with relative distance (around 70% inconsistency). Gemini-2.0-Flash and Claude-3.7-Sonnet exhibit more balanced performance, whereas Ovis2-34B and GPT-4o vary considerably across tasks. Figure 6. Completeand Partial-visibility counting. While MLLMs often succeed when everyone is visible in one viewpoint, they sometimes fail to reconcile fragmented information across views, as shown by GPT-4o occasionally picks the largest per-view count rather than reconciling individuals across views. ing diminish and can even degrade performance, as observed with InternVL2.5-38B. This phenomenon echoes findings in [55], where CoT methods offered limited advantages for strong spatial-reasoning models such as Gemini1.5 [45]. We hypothesize that, beyond these prompt reasoning strategies, architectures or training methods specialized for multi-view scenarios incorporating domain-specific data or spatial-aware modules may be necessary to further advance MLLMs performance, rather than relying solely on enhanced prompt engineering. 4.2. Failure with Coarse Camera Estimation We also observe that MLLMs often struggle with orientation-sensitive challenges (as highlighted in Table 1), such as estimating camera poses, object directions, and tracking object trajectories across multiple viewpoints. To investigate how these shortcomings manifest, we design Figure 7. Analysis of reasoning prompt strategies. We report the effectiveness of Zero-Shot CoT, Self-Consistency, and Identification CoT across GPT-4o, Ovis2-34B, and InternVL2.5-38B under complete-view and partial-view settings. While CoT variations delivers notable gains in partial-visibility scenarios in GPT4o, its impact diminishes for models already be robust at multiview counting (e.g., InternVL2.5-38B). These results indicate that refining reasoning prompt alone is insufficient; specialized multiview training may be necessary to excel on All-Angles Bench. visualization prompt inspired by [55], wherein each objects center is mapped to 10 10 grid and camera view poses are depicted as dot with directional arrow. Specifically, we task GPT-4o [21] and Gemini-2.0-Flash [45] with inferring both object and cameras positions and orientations from multi-view images (see Appendix for prompt details). As illustrated in Figure 8 (object manipulation) and Figure 9 (camera pose estimation), many orientation-related errors stem from the models inability to reconcile viewpoint transformations. Instead of maintaining consistent object correspondences, the model frequently misaligns camera coordinates or overlooks background cues critical for geometric reasoning. This not only impacts camera pose estimation but also complicates downstream tasks such as relative direction or object manipulation where fine-grained rotational and positional cues are essential. These observations echo the findings from Section 4.1, suggesting that Figure 8. Visualization of multi-view scene reconstruction and camera pose alignment. Although GPT-4o and Gemini-2.0-Flash both display moderate proficiency in single-view scene reconstruction, they struggle when aligning two different camera perspectives. Misidentifying camera poses leads to incorrect directional reasoning such as tracking persons trajectory from View 1 to View 2 which needs multi-view consistency in current MLLMs. on video understanding and, more broadly, exploring model capabilities for embodied real-world tasks. Our work contributes to this growing area by: 1) providing timely evaluation benchmark for assessing current and future models abilities in multi-view perception fundamental capability for 3D and 4D tasks; and 2) offering an analysis of why current models struggle with multi-view understanding. Benchmarking Visual Spatial Ability. There are recently more works [5, 16, 29, 30, 37, 55] studying video and visual spatial ability of MLLMs. Our work is most relevant to VideoMME [16], VSI-Bench [55] and MV-Bench [29]. VideoMME extensively evaluates video understanding but emphasizes temporal reasoning [16]. VSI-Bench specifically targets spatial intelligence through egocentric video scenarios [55]. MV-Bench also focuses on comprehensive multi-modal temporal understanding [29]. Our work focuses on multi-view understanding, cornerstone for robust 3D and 4D reasoning in MLLMs. Unlike previous work that primarily assess single-view or temporal reasoning, we explicitly evaluate how models align geometric and semantic information across multiple viewpoints. We further provide detailed breakdown analysis that dissects model deficiencies in multi-view understanding. Figure 9. Visualization of camera pose estimation. When asked to order the camera poses in clockwise order, MLLMs fail completely despite providing detailed reasonings. domain-specific training or architectural refinements, especially those emphasizing viewpoint consistency may necessary to close the gap in multi-view understanding. 5. Related Works 6. Conclusion Multimodal Large Language Models. MLLMs [3, 12, 17, 21, 28, 32, 33, 46] have demonstrated impressive capabilities across various tasks [26, 34, 35, 38, 48, 59, 60] and applications [2, 11, 47, 49, 53, 61, 64]. In particular, an increasing number of studies [6, 7, 14, 19] are focusing In conclusion, we introduce All-Angles Bench, comprehensive benchmark to evaluate MLLMs multi-view understanding. Our evaluation of 27 representative models across over 2,100 annotated multi-view question-answer pairs in the six tasks, we reveal significant limitations in geometric consistency and cross-view correspondence, particularly in cross-view identification and camera pose estimation. These findings highlight the need for domain-specific training to enhance MLLMs multi-view reasoning, providing insights toward achieving human-level performance."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude, 2024. 2, 4, 5 [2] Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-thewild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2025. 8 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. 2023. 8 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 4, 5 [5] Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2025. 8 [6] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 8 [7] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context arXiv preprint visual language models for long videos. arXiv:2408.10188, 2024. 8 [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 4, 3 [9] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 2, 6 [10] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2025. [11] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. 8 [12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning. In NeurIPS, 2024. 8 [13] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 110, 2018. 2 [14] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. 2023. 2, 8 [15] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. [16] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 8 [17] Google. Gemini, 2023. 2, 8 [18] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. 2, 3, 4, 1 [19] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. 3d concept learning and reasoning from multi-view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 92029212, 2023. 2, 8 [20] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pages 91189147. PMLR, 2022. 2 [21] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 7, 8 [22] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Alaa Maalouf, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, et al. Conceptfusion: Open-set multimodal 3d mapping. arXiv preprint arXiv:2302.07241, 2023. [23] Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang. Scaling 3d vision-language learning for Sceneverse: grounded scene understanding. In European Conference on Computer Vision, pages 289310. Springer, 2024. 2 [24] Rawal Khirodkar, Aayush Bansal, Lingni Ma, Richard Newcombe, Minh Vo, and Kris Kitani. Ego-humans: An egoIn Proceedings of the centric 3d multi-human benchmark. IEEE/CVF International Conference on Computer Vision, pages 1980719819, 2023. 2, 3, 4, 1 [25] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 2 [26] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 8 [27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 4, 8 [29] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 8 [30] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Erran Li Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. Advances in Neural Information Processing Systems, 37: 100428100534, 2025. 8 [31] Fangchen Liu, Kuan Fang, Pieter Abbeel, and Sergey Levine. Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. 2 [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 8 [34] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024. 8 [35] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2023. 8 [36] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. 4, 5, 6 [37] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. 8 [38] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In ACL, 2022. 8 [39] Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei Herzig. Llarva: Vision-action instruction tuning enhances robot learning. arXiv preprint arXiv:2406.11815, 2024. [40] OpenAI. gpt4o, 2024. 2, 4, 6, 1, 3 [41] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. 2 [42] William Rudman, Michal Golovanesky, Amir Bar, Vedant Palit, Yann LeCun, Carsten Eickhoff, and Ritambhara Singh. Forgotten polygons: Multimodal large language models are shape-blind. arXiv preprint arXiv:2502.15969, 2025. 2 [43] Chan Hee Song, Jihyung Kil, Tai-Yu Pan, Brian Sadler, Wei-Lun Chao, and Yu Su. One step at time: Long-horizon vision-and-language navigation with milestones. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1548215491, 2022. 2 [44] Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav Sukhatme. Embodied bert: transformer model for embodied, language-guided visual task completion. arXiv preprint arXiv:2108.04927, 2021. 2 [45] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 4, 5, 7 [46] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. 4, 8 [47] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. 8 [48] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, 2024. 8 [49] Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Zhengxin Li, Lin Ma, Shenghua Gao, et al. Mllm-tool: multimodal large language model for tool agent learning. arXiv preprint arXiv:2401.10727, 2024. [63] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 4 [64] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 8 [65] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024. 2 [66] Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, and Xing Xie. Dynamic evaluation of large language models by meta probing agents. arXiv preprint arXiv:2402.14865, 2024. 4 [50] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR, 2023. 3 [51] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. [52] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 4 [53] Jingwei Xu, Zibo Zhao, Chenyu Wang, Wen Liu, Yi Ma, and Shenghua Gao. Cad-mllm: Unifying multimodalityarXiv preprint conditioned cad generation with mllm. arXiv:2411.04954, 2024. 8 [54] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 1 [55] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. 2, 6, 7, 8 [56] Yue Yang, Shuibai Zhang, Wenqi Shao, Kaipeng Zhang, Yi Bin, Yu Wang, and Ping Luo. Dynamic multimodal evaluation with flexible complexity by vision-language bootstrapping. arXiv preprint arXiv:2410.08695, 2024. 4 [57] Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, and Inst3d-lmm: Instance-aware 3d scene underJianke Zhu. standing with multi-modal instruction tuning, 2025. 2 [58] Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara Berg, and Dhruv Batra. Multi-target embodied question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63096318, 2019. 2 [59] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. 8 [60] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 2, 8 [61] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In NeurIPS, 2024. 8 [62] Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, et al. Vlm 2-bench: closer look at how well vlms implicitly link explicit matching visual cues. arXiv preprint arXiv:2502.12084, 2025. 2 Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs"
        },
        {
            "title": "Supplementary Material",
            "content": "In these supplementary materials, we provide the following: Details on the construction and annotation pipeline of AllAngles Bench (Section 7); Evaluation setup, implementation detail of CoT methods, evaluation results visualization and complete evaluation results for the tiny All-Angles Bench (Section 8); Additional visualization results and prompts (Section 9). 7. Construction and Annotation Pipeline 7.1. Dataset Collection We manually selected 83 scenes from Ego-Exo4D [18] and 7 scenes from EgoHumans [24] to ensure the diversity of scenes. Given the high density of viewpoints in some EgoHumans scenes, we carefully curated subset of more spatially dispersed views to avoid excessive redundancy. As result, we retained 4 - 5 views per scene. All multi-view images were standardized to resolution of 796 448 pixels. 7.2. Question Creation For each generated question, we recorded the following key attributes: question index, source dataset, task category, image list path, question text, and multiple-choice options. Since the questions would undergo human-in-the-loop quality review and verification process, letter-based answer choices were not generated at this stage. Among the six task categories in All-Angles Bench, five were generated using an MLLM [40]. For the Camera Pose Estimation task, however, we designed dedicated question template to structure the question generation process. The system prompt, task-specific prompts for the five generated tasks, and the camera pose estimation question template are illustrated in Figures 25, 26, and 27, respectively. 7.3. Human Annotation and Quality Check In this study, we hired eight Ph.D. students in STEM fields specializing in natural language processing or computer vision, to manually annotate all questions. To ensure consistency, we provided comprehensive annotation guideline, which was refined and structured into streamlined version, as illustrated in Figure 10. Additionally, to maintain high annotation quality, we implemented multi-stage verification process. Before the formal annotation process, annotators were required to complete set of sample questions to familiarize themselves with the standards and guidelines. To minimize errors and ambiguities, each annotation was then cross-checked by at least one other annotator, with any disagreements resolved through group discussions. Figure 12 presents an example comparing an initial annotation with its final version after cross-checking. Such ambiguous or unclear instances were flagged for review and collaboratively examined in team meetings, ensuring standardized and consistent annotation process. Furthermore, we adopted random sampling review mechanism, periodically evaluating subset of annotated data to ensure strict adherence to the guidelines. The finalized benchmark is stored in JSON format, with an example visualization provided in Figure 11. To streamline the annotation and quality control process, we developed GUI-based annotation platform, as shown in Figure 11. This platform provides an intuitive interface enabling annotators to inspect and edit annotations efficiently. Annotators can seamlessly browse multi-view input images, modify questions, and adjust answer options with ease, ensuring both accuracy and consistency in the annotations. Notably, the entire dataset collection and processing required over 300 person-hours, reflecting our meticulous attention to detail in ensuring the benchmarks high reliability and quality for the relevant research community. 8. Experiment Details 8.1. Evaluation Setup Our evaluation is conducted using the VLMEvalKit [15] framework. In order to ensure the reproducibility of our evaluation, we employ greedy decoding strategy for all models, setting the temperature to 0 unless otherwise stated. The text input follows standardized format: [Question][Options][Post-prompt], where the post-prompt instructs: Answer with the options letter from the given choices directly. To ensure that all final predictions are formatted as singleletter outputs, facilitating subsequent evaluation against the ground-truth answers and minimizing errors due to fuzzy matching, we leverage the open-source LLM, Qwen2.532B [54] to extract the predicted options accurately. The corresponding prompt is shown in Figure 24. For human-level performance evaluation on the tiny 250We first have to say thank for your efforts in annotating the All-Angles Bench benchmark. Your contribution is invaluable and plays crucial role in advancing this research. Your Task: Review, refine, and finalize multiple-choice questions (MCQs) generated by an MLLM. Ensure they are clear, accurate, and correctly answered. What to Check: 1. Clarity & Relevance The question must match the image/context and be easy to understand. 2. Grammar & Precision No errors or vague phrasing. 3. Answer Quality The correct answer must be factually accurate, and distractors should be plausible yet clearly incorrect. Example Before & After: Original Question: What if the man with black pants in camera View 1 starts walking toward the view, what would be the trajectory of the man in camera View 3? Issue: toward the view is unclear. Revised Question: What if the man with black pants in camera View 1 starts walking toward the camera position of the view, what would be the trajectory of the man in camera View 3? Fix: Provide more specific description of the subjects movement. Final Answer: (B) walking toward the right side of View 3. How to Annotate 1. Verify the questions and options for plausibility and accuracy based on the images/data provided. 2. Correct unclear wording, grammatical errors, or inaccuracies in the questions. 3. Finalize the correct answer. Common Mistakes to Avoid Vague phrasing (e.g., toward the view). Misleading or contradictory options. MLLM hallucinations (made-up facts or objects) Figure 10. The streamlined version of annotation guideline for annotators to follow. It outlines key verification steps, common pitfalls, and examples to help annotators improve question clarity, accuracy, and answer quality. question benchmark, we invited two additional Ph.D. students in STEM fields who were not involved in the annotation process to answer the questions. Each evaluator was assigned 125 questions and given unlimited time to answer with their best effort. Their combined scores serve as the human performance baseline for this tiny benchmark. To eliminate potential biases introduced by contextual cues, we exclude paired data from this subset, preventing evaluators from leveraging strong prior knowledge. Additionally, for questions involving only two views, we ensure consistency with the MLLM setup by displaying only the relevant input views rather than all available ones. To further prevent evaluators from unintentionally deriving answers from sequentially presented images, we randomly shuffle the question order, ensuring independent assessment of each query. 8.2. Implementation Details of CoT Methods Inspired by [55], we evaluate three distinct reasoning-based prompting strategies on our benchmark: Zero-Shot CoT, Self-Consistency, and our proposed Identification CoT. Below, we outline the implementation details. Notably, after generating intermediate reasoning steps and predictions using the three CoT approaches, we apply standardized post-processing step. Specifically, we leverage an additional open-source LLM to explicitly extract the final answer from the generated response, as described in Section 8.1. Zero-Shot CoT: Building on prior works [41, 51], we enhance step-by-step reasoning in the MLLM by appending the phrase, Lets think step by step, to each question in Figure 11. Left: structured JSON representation of question-answer pair. Right: snapshot of the GUI-based Annotation Platform used for reviewing and refining annotations. Best viewed zoomed in for details. adopts the same settings as Zero-Shot CoT, using temperature equals 0 and single inference pass to generate the final prediction. Figures 14 and 15 illustrate model outputs for two MLLMs GPT-4o [40] and InternVL2.5-38B [8] under the three prompting strategies. 8.3. Evaluation Results Visualization Figure 23 visualizes model performance across six task categories, where color intensity represents precision levels cooler colors indicate lower accuracy, while warmer colors denote higher accuracy. 8.4. More Evaluation Results Table 2 presents the evaluation results of 27 MLLMs, encompassing both closed-source and open-source models on the 250-question benchmark. The findings remain consistent with those in the main text, confirming that human performance significantly surpasses that of all MLLMs. 9. Visualization Results In this section, we present comprehensive visualization of the scene across all available views for convenience and consistency. While some questions do not require every view, we ensure that only the relevant ones are provided as input during inference. 9.1. Benchmark Examples Figures 16 and 17 showcase additional primary questionand-answer examples. These illustrations highlight the multi-view image inputs alongside their corresponding Figure 12. Comparison of flagged ambiguous case before modification and its finalized version after cross-checking. The initial annotation was reviewed by multiple annotators, with ambiguities resolved through discussions to ensure clarity and consistency. the post-prompt. The decoding parameter, temperature, is set to 0 to ensure deterministic inference. Self-Consistency: Following the Self Consistency approach [50], we prompt MLLMs to generate multiple independent responses for each question. To encourage diversity, we set temperature to 0.6 and conduct five independent inference runs, selecting the most frequently occurring prediction as the final answer. Identification CoT: Designed specifically for counting tasks, Identification CoT prompts the MLLM to list each target entity visible across all views, mitigating the risk of double-counting and improving accuracy. The corresponding prompt is shown in Figure 13. This method correctly identify the answer, and their reasoning processes are coherent and logically sound, demonstrating their ability to follow step-by-step inference when the task aligns well with their learned knowledge. 9.4. Human Evaluation Failure Cases Figure 22 presents three questions that human evaluators answered incorrectly. Analyzing these errors underscores the robustness of our annotation process while also highlighting the challenge and complexity of our benchmark. Additionally, the errors highlight the challenge and complexity of our benchmark, demonstrating its effectiveness in evaluating MLLMs ability to understand spatial relationships in multi-view images. 9.5. Visualization Prompt Figure 28 displays the prompt used to visualize scene reconstruction and camera pose alignment, as detailed in our paper. This prompt allows us to assess how well GPT4o and Gemini-2.0-Flash handle orientation-sensitive challenges, further validating their spatial reasoning capabilities."
        },
        {
            "title": "Identification CoT",
            "content": "Internally generate detailed Lets think step by step before answering the question! Your goals: 1. description of each visible personnote their appearance, clothing, orientation, movements, and any nearby detailed described objects or people. 2. descriptions across all views to avoid double-counting the same individual. 3. unique people in the scene. Important Instructions: Arrive at the total number of Cross-check these detailed - You should perform your step-by-step reasoning privately and not reveal it to the user. Figure 13. Our proposed Identification CoT prompt. To design for counting tasks with partial-visibility, our prompt guides the MLLM to systematically list each target entity across all views. tasks and Q&A pairs, demonstrating the diversity and complexity of our benchmark. 9.2. Pair Data Examples Figures 18 and 19 provide more examples of generated paired data. Each figure presents both the primary Q&A and the generated pair Q&A, presented side by side to illustrate their structural alignment and transformation process. This comparison emphasizes how the paired Q&A is derived from the primary one, reinforcing the datasets consistency and utility. 9.3. Reasoning Examples Figures 20 and 21 depict three reasoning cases evaluated using GPT-4o [40] and Gemini-2.0-Flash [45]. We modify the post-prompt to instruct the models not only to generate answers but also to provide detailed reasoning. Our analysis reveals distinct reasoning patterns across the three evaluated cases. In case 1, both GPT-4o and Gemini-2.0-Flash select incorrect answers, indicating challenges in understanding the underlying spatial relationships. In case 2, while GPT4o arrives at the correct answer, its reasoning process contains logical inconsistencies, suggesting that the model may have relied on heuristic shortcuts rather than fully comprehending the question. In contrast, Gemini-2.0-Flash fails to produce the correct response. While in case 3, both models Figure 14. Comparison of model outputs on the same questions under different prompting methods. (Case 1) Figure 15. Comparison of model outputs on the same questions under different prompting methods. (Case 2) Figure 16. All-Angles Bench Samples (Part I) Figure 17. All-Angles Bench Samples (Part II) Figure 18. Paired Data Samples (Part I) Figure 19. Paired Data Samples (Part II) Figure 20. Visualization of the reasoning process for two MLLMs (Part I). In this case, both MLLMs choose incorrect options due to errors in their reasoning process. Figure 21. Visualization of the reasoning process for two MLLMs (Part II). In the above case, GPT-4o selects the correct option but contain errors in its reasoning process. In the case below, both GPT-4o and Gemini-2.0-Flash follow correct reasoning process and ultimately select the right answer. Figure 22. Questions that evaluators answered incorrectly, along with detailed review of their reasoning for selecting the incorrect options. Figure 23. The visualization of all model performance across the 6 task categories in All-Angles Bench. Closed-source Models Methods Avg. 82."
        },
        {
            "title": "Human Level",
            "content": "GPT-4o Gemini-1.5-Pro Gemini-1.5-Flash Gemini-2.0-Flash Claude-3.5-Sonnet Claude-3.7-Sonnet 52.4 50.8 50.0 58.4 50.0 52.8 Open-source Models DeepSeek-VL2-Small DeepSeek-VL2 InternVL2.5-2B 45.2 InternVL2.5-4B 47.2 InternVL2.5-8B 52.4 InternVL2.5-38B 60.8 InternVL2.5-78B 54.4 48.0 51.6 Qwen2.5-VL-3B 52.4 Qwen2.5-VL-72B 58.4 Ovis2-2B 50.8 Ovis2-4B 54.0 Ovis2-8B 54.4 Ovis2-16B 60.4 Ovis2-34B 59.2 Cambrian-8B 40.4 Cambrian-13B 39.2 Cambrian-34B 47.2 LLaVA-Onevision-Qwen2-7B 53.6 LLaVA-Onevision-Qwen2-72B 57.2 LLaVA-Video-Qwen2-7B 46.8 LLaVA-Video-Qwen2-72B 54.0 e"
        },
        {
            "title": "A ttri b",
            "content": "o . 93.3 66.7 60.0 55.6 62.2 57.8 60. 64.4 62.2 60.0 73.3 77.8 62.2 62.2 68.9 73.3 57.8 73.3 68.9 66.7 71.1 57.8 48.9 68.9 64.4 68.9 62.2 60.0 88.9 16.7 22.2 22.2 38.9 61.1 38.9 5.6 16.7 16.7 27.8 16.7 38.9 38.9 22.2 22.2 38.9 38.9 5.6 50.0 11.1 22.2 27.8 16.7 16.7 11.1 5.6 11.1 ti u l. p"
        },
        {
            "title": "M a",
            "content": "Multiple-Choice Answer 86.3 72.0 52.9 35.3 27.5 64.7 52.9 37.3 41.2 39.2 54.9 70.6 52.9 45.1 51.0 43.1 52.9 43.1 51.0 47.1 58.8 52.9 33.3 31.4 43.1 43.1 43.1 43.1 43.1 40.0 44.0 50.0 48.0 22.0 38. 34.0 34.0 34.0 42.0 30.0 32.0 48.0 42.0 44.0 38.0 26.0 44.0 44.0 52.0 34.0 28.0 44.0 44.0 56.0 26.0 50.0 l. ir. l. ist. 79. 53.8 51.3 56.4 56.4 51.3 56.4 33.3 33.3 48.7 64.1 59.0 35.9 38.5 46.2 61.5 46.2 61.5 59.0 59.0 61.5 38.5 35.9 33.3 53.8 64.1 53.8 56.4 95.7 63.8 76.6 74.5 68.1 63.8 80.9 68.1 78.7 78.7 68.1 70.2 68.1 61.7 74.5 76.6 74.5 68.1 74.5 78.7 78.7 46.8 57.4 57.4 78.7 74.5 68.1 78.7 Table 2. Evaluation results for 27 MLLMs on 250 Q&A tiny benchmark. Multiple-Choice Answer Extraction Prompt Given prediction for multiple-choice question, directly extract the selected answer while skipping the reasoning process. explicitly chooses option A, B, or C, return the corresponding letter. If the prediction does not specify choice or indicates that none of the options are correct, return None. If the prediction Figure 24. The prompt used for extracting multiple-choice answers from predictions. System Prompt You are the expert for designing questions and answers for creating benchmark for multi-camera view scenarios. Here will first give you several images from different camera views which are pointing to the same scene. on the specific task description and example provide. Ensure the questions and answers are based on the objects and their relationships visible across the given camera views. Please answer the following questions based Figure 25. The system prompt used for generating five tasks with the MLLM."
        },
        {
            "title": "Task Specific Prompt",
            "content": "Counting: Counting across multi-views 1. Task Description: Could you count the amount on specific object (e.g., people, chair, cup) in the scene based on all the input camera views? Example: Question: Here are multiple camera views which are pointing to the same scene, could you count how many people in total are in the scene? Option: (A) 5, (B) 3, (C) 1 Attribute Identification: Attribute identification across multiple views 2. Task Description: Could you identify the same object / attribute across multiple camera views? Example: Question: View 1 and View 2 are two different views that represent the same scene. where camera View 1 was positioned. View 2? Option: (A) the person on the left side of View 2, (B) the person closer to the center of View 2, (C) the person sitting on the stairs In View 1, there is person in fencing who is facing to the spot Could you identify the same person in Relative Distance: Object-camera relative distance 3. Task Description: Could you measure the relative distance between the seen object and camera view position in the cross view scenario? Example: Question: View 1 and View 2 are two different camera views that represent the same scene. In which view, the basketball is closer to the spot where the camera view was positioned? Option: (A) closer to the spot where camera View 1 was positioned, (B) closer to the spot where camera View 2 was positioned, (C) distance to the spot where camera View 1 and View 2 were positioned is equal Relative Direction: Object-camera relative direction 4. Task Description: Could you measure the relative direction between the seen object and camera view position in the cross view scenario? Example: Question: View 1 and View 2 are two different camera views that represent the same scene. In View 1, the man in red t-shirt is facing to the spot where View 1 was positioned, which direction is the man facing in View 2? Option: (A) facing to the left side of the view, (B) facing to the right side of the view, (C) facing to the spot where camera View 2 was positioned Manipulation: Relative object manipulation across views 5. Task Description: Could you manipulate the seen objects (e.g., what if the person is walking to the left of the view), and design the Q&A based on the trajectory of object across views? Example: Question: What if the person wearing the gray t-shirt is walking toward the spot where camera View 1 was positioned, what would be the trajectory of the same person in View 2? Option: (A) walking toward the left side of View 2, (B) walking toward the right side of View 2, (C) remain in the same position Figure 26. The task-specific prompts used for generating five tasks with the MLLM."
        },
        {
            "title": "Camera Pose Estimation Question Template",
            "content": "Question: View 1, View 2, View 3, View 4 are four different input camera views that represent the same scene. Could you reconstruct the camera view layout from the top-bottom view, and answer the following question? Option: (A) in clockwise order, the layout is View 1, View 2, View 3, View 4, (B) in clockwise order, the layout is View 4, View 3, View 2, View 1, (C) in clockwise order, the layout is View 2, View 3, View 1, View 4 Figure 27. The question template designed for the Camera Pose Estimation task. [Task] These images capture the same scene. objects within each image, understand the spatial arrangement of the scene, and estimate the center point of each object, assuming the entire scene is represented by 10x10 grid. Your objective is to identify specific [Rule] We provide the primary categories about in this scene: tv, sofa, stair...) Focus ONLY on these categories (ignore small object or categories). Estimate the center location of each instance within the provided categories, assuming the entire scene is represented by 10x10 grid. Person A, Person B, Person C..). reflect its real position in the scene while preserving the relative spatial relationships. Combine and merge information from the images since they are pointing to the same scene, calibrating the object locations accordingly. If category contains multiple instances, include all of them (e.g., Each objects estimated location should (e.g., human, lego, [Output] Please generate 10x10 grid visualization that includes: locations of each object (e.g., humans, basketball, etc.). position The cameras viewpoint direction represented as an arrow indicating its view orientation The trajectory of the object in the question (e.g., object movement), indicating its motion path. answer and your reasons step by step in details. Please also provide your The camera view The predicted Figure 28. Visualization prompt designed to evaluate MLLMs on orientation-sensitive challenges."
        }
    ],
    "affiliations": [
        "HKU",
        "NYU",
        "TranscEngram",
        "UC Berkeley",
        "UC Davis",
        "University of Oxford"
    ]
}