{
    "paper_title": "Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection",
    "authors": [
        "Dongwon Jo",
        "Beomseok Kang",
        "Jiwon Song",
        "Jae-Joon Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head $Q$, $K$, $V$ to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to $\\times$3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference."
        },
        {
            "title": "Start",
            "content": "Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Dongwon Jo 1 Beomseok Kang 1 Jiwon Song 1 Jae-Joon Kim 1 https://github.com/dongwonjo/Token-Sparse-Attention"
        },
        {
            "title": "Abstract",
            "content": "The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, lightweight and dynamic token-level sparsification mechanism that compresses per-head Q, K, to reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracylatency trade-off, achieving up to 3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is complementary and effective strategy for scalable longcontext inference. 6 2 0 2 3 ] . [ 1 6 1 2 3 0 . 2 0 6 2 : r 1. Introduction The capability to process long contexts has become defining feature of modern Large Language Models (LLMs) (Achiam et al., 2023; Comanici et al., 2025; An1Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea. Correspondence to: JaeJoon Kim <kimjaejoon@snu.ac.kr>. Preprint. February 4, 2026. 1 Figure 1. Speedups with Token Sparse Attention. Attention acceleration ratios obtained by applying the proposed Token Sparse Attention (ours) to existing attention acceleration methods. τ denotes the sparsity level (i.e., higher the sparser). thropic, 2024), enabling applications ranging from long document summarization to multi-turn reasoning and code generation (Yi et al., 2024; Laban et al., 2023; Rando et al., 2025). However, as context lengths (L) increase, the complexity of attention mechanism grows quadratically (O(L2)) during the prefill stage, creating fundamental bottleneck in inference. While hardware-aware optimizations such as FlashAttention (Dao, 2023) significantly reduce memory I/O overhead, the inherent quadratic complexity remains the same, necessitating algorithmic innovations to scale practical context lengths beyond 100K. Recent state-of-the-art (SOTA) methods for accelerating prefill predominantly rely on sparse attention (Lai et al., 2025; Jiang et al., 2024a), where computations for low-importance regions in the attention map (QK ) are bypassed. While these methods approximate attention weights effectively, they typically impose block-level sparsity to stay compatible with the execution patterns of attention kernels. As result, less informative tokens may still be preserved when grouped with salient tokens in the same block, potentially leading to sub-optimal sparsity. few studies have explored token-level sparsity during prefill (Jo et al., 2025; Shi et al., 2024). These methods identify subset of important tokens in the early layers and permanently discard the rest in deeper layers. However, once tokens are evicted, they cannot be reconsidered even if they become significant in Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Figure 2. Dynamics of Token Importance. (a) Layer-wise overlap of top-k important tokens, showing that token importance shifts significantly across layers. (b) Head-wise token importance rankings within the same layer, illustrating that different attention heads prioritize different subsets of tokens. subsequent layers. This naturally enforces the same set of tokens to persist across layers and fails to capture more selective patterns such as layeror/and head-specific shifts in token importance. Consequently, eviction-based approaches often exhibit weaker accuracy-speedup trade-offs, falling short of fully leveraging the benefits of operating at finer (token-level) granularity. In this paper, we introduce Token Sparse Attention, novel token-level attention mechanism for accelerating prefill in long-context inference. At each head, the method selects small subset of informative tokens (L<L) and performs attention efficiently within the resulting compressed space (RLL ), allowing the selected subset to differ across heads. However, naıvely applying this selection prevents the remaining tokens (LL) from being re-visited in subsequent layers, inevitably evicting them. To address this issue, we interleave the attention output (RLd) back into the original sequence dimension (RLd). This Compress and then Decompress design enables each head to repeatedly select its own significant tokens from the full sequence (L), reaping the benefits of head-specific token selection. In addition, because the full sequence dimension is restored (LL) at every layer, our design naturally supports layerwise adaptive sparsity budgets. Consequently, our method dynamically determines the sparsity level on-the-fly during inference, providing flexibility that eviction-based approaches inherently lack. While improving performance on its own, Token Sparse Attentions core strength lies in being complementary to existing sparse attention methods rather than replacing them. This arises from its strong compatibility: irrelevant tokens can be pruned prior to any sparse computations, e.g., blocksparse or Λ-shape. Because our method essentially performs dense attention within the compressed space (RLL ), it integrates seamlessly with implementations like FlashAttention and sparse attention kernels. As result, we demonstrate heterogeneous granularity (see Figure 1) as an effective new strategy for enhancing sparse attention, e.g., combining FlexPrefill (Lai et al., 2025) (block-sparse) with ours (token-sparse) reaches 87.3% accuracy with 2.8 speedup than FlashAttention, whereas standard FlexPrefill achieves the same accuracy (87.3%) but 2.4 speedup (see Table 1). 2. Method 2.1. Motivation Prior token-sparse techniques (Jo et al., 2025; Shi et al., 2024) generally reduce the computational cost of attention by selecting subset of tokens in early layers and restricting computation to only those tokens in subsequent layers. However, this strategy implicitly assumes that token importance estimated in early layers will remain stable throughout the model during prefill, which rarely holds in practice. To illustrate this, we analyze token-importance dynamics during prefill for LLaMA-3.1-8B-Instruct model (Dubey et al., 2024), as shown in Figure 2. Layer-wise Token Importance. Figure 2(a) reports the layer-wise overlap of important tokens, where we select the top 1% tokens at each layer and measure their pairwise overlap across layers. Although adjacent layers share moderate fraction of important tokens, the overlap decreases rapidly as the layer distance increases. This indicates that token importance shifts substantially across layers as attention focus evolves during prefill. As result, permanently removing tokens based on early-layer selection can prematurely eliminate candidates for subsequent layers that might later become relevant. Head-wise Token Importance. Figure 2(b) further shows token-importance variation across attention heads at layer 18. Different heads exhibit distinct token-ranking patterns, suggesting that token relevance is not only layer-dependent but also head-specific. This heterogeneity is fundamental property of multi-head attention, where heads specialize in capturing different contextual relationships. Thus, enforcing coarse, layer-level eviction policy forces all heads to share unified token set, inevitably discarding tokens that 2 Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Figure 3. Overview of the proposed Token Sparse Attention. Stage 1 compresses Q, K, and by selecting per-head token subset SH=0, yielding compact ˆQ, ˆK, and ˆV that remain compatible with standard attention kernels. Stage 2 performs attention on the compressed tensors and scatters the resulting outputs back into the full sequence layout before adding the residual connection. individual heads may find essential. These observations indicate that token selection should retain flexibility across both layers and heads, rather than permanently committing to early-layer decisions. Motivated by this, Token Sparse Attention is designed to accommodate these dynamics through interleaved token selection, which restores the full sequence dimension after each compressed attention step. This reversible design reduces unnecessary computation while preserving future selection space. 2.2. Token Sparse Attention Motivated by the observations in Section 2.1, we aim to selectively skip irrelevant tokens during attention computation without permanently removing them from the sequence. As illustrated in Figure 3, this process consists of two primary stages which are compression for query-key-value tensors and decompression for the attention output. These stages ensure that the computational benefits of sparsity are realized while maintaining the structural integrity of the original sequence. Stage 1: Compression for QKV. In the compression stage, each attention head selects subset of token indices SH=h, yielding reduced sequence length L. Using these indices, we gather the corresponding rows from the original Q, K, and tensors to construct compressed tensors ˆQ, ˆK, and ˆV . Importantly, this selection is performed independently per head, allowing different heads to attend to different subsets of tokens. This design directly addresses the head-wise heterogeneity observed in Figure 2(b). major advantage of this compression strategy is that the resulting tensors ˆQ, ˆK, ˆV remain dense and contiguous in memory. This allows us to utilize highly optimized hardware-aware kernels such as FlashAttention or other specialized sparse attention implementations without any modification. The attention operation is performed on the compressed tensors to produce reduced output tensor ˆO, which contains the context-aware representations for the selected tokens, thereby reducing the quadratic attention cost from O(L2d) to O(L2d). Stage 2: Decompression for Attention Output. In the decompression stage following attention computation, the compressed output ˆO is scattered back into zero-initialized tensor of shape RLd using the head-specific index set SH=h. This operation ensures that the output dimensions match the original input, preventing any dimension mismatch issues in subsequent layers. The unselected positions in the output tensor remain zero, which is functionally equivalent to applying hard mask to those tokens in the attention map. Finally, the restored attention output is added to the residual connection. This step is critical because the residual connection preserves the information of the unselected tokens from the previous layer. As result, Token Sparse Attention achieves dynamic tokenlevel sparsification without sacrificing the expressive flexibility of dense attention. The model can selectively ignore irrelevant context to reduce computation, yet continuously re-evaluate token importance across layers and heads. In the next subsection, we describe how token importance is estimated and how the per-head token subsets SH are dynamically determined under sparsity budget. 2.3. Dynamic Token Coverage Token Sparse Attention adaptively selects the sparsity budget at inference time, which involves two key decisions: (1) determining how many tokens to retain and (2) identifying which tokens to keep for each attention head. To address this, we introduce Dynamic Token Coverage, token-selection policy applied before the compression stage of Token Sparse Attention. The full procedure is summarized in Algorithm 1. 3 Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Algorithm 1 Dynamic Token Coverage and Index Search (cid:17) (cid:16) Require: Q, K, Token Coverage τ , Sequence Length Ensure: Head-wise selected token indices {Sh}H h=1 1: // Compute attention scores using recent queries 2: ˆA softmax Q[last q:]K / 3: // Compute token-level scores per head 4: sh pooling(sumvertical( ˆA)) 5: // Aggregate and normalize token scores across heads h=1 sh)/((cid:80)L 6: sl ((cid:80)H h=1 sh[t]) 7: // Sort tokens by ascending importance 8: argsort(sl) 9: // Determine the number of sparse token 10: ksparse arg mink{0,...,L} 11: kkeep ksparse 12: // Head-wise top-kkeep token selection 13: {Sh}H 14: return {Sh}H h=1 {TopK(sh, kkeep)}H j=1 sl[I[j]] τ (cid:110)(cid:80)k (cid:80)H h=1 t=1 (cid:111) h=1 We begin by estimating token importance independently for each attention head. For given head h, we compute lightweight proxy of the attention map ˆA by attending small set of recent queries to all keys. The importance score sh[t] for the t-th token is derived by summing the attention weights along the vertical axis (i.e., the sequence length dimension of queries). We implement this scoring step using custom hardware-efficient kernel developed in Triton (Tillet et al., 2019) that minimizes memory I/O overhead by fusing the score calculation. How Many Tokens to Retain. Given the head-wise token scores, we aggregate them into layer-level importance distribution by summing scores across heads and normalizing over the sequence dimension. This aggregated score captures the overall contribution of each token at the current layer and serves as the basis for determining the sparsity budget. Instead of selecting tokens in descending order of importance, we sort tokens by ascending estimated importance and identify the minimal set of least important tokens whose cumulative mass exceeds predefined coverage threshold τ . key premise of our approach is that long-context attention accumulates attention noise, manifested as long tail of tokens with negligible cumulative contribution. We interpret pruning this tail as form of structural regularization that reduces distraction from irrelevant context. Which Tokens to Keep. Once the layer-wise budget is determined, we perform the final token selection independently for each attention head by selecting the top-kkeep tokens according to sh. This design satisfies the heterogeneity requirement discussed in Section 2.1, allowing each head to attend to distinct semantic features while utilizing 4 Figure 4. Sparse Layer Selection. (a) Layer-wise normalized drift measured across different tasks and context lengths. Task and correspond to retrieval task and summarization task, respectively. (b) Relationship between accuracy and drift under random 3-layer sparsification over 200 runs. the allocated compute budget optimally. 2.4. Sparse Layer Selection Token Sparse Attention and Dynamic Token Coverage determine how tokens are selected within each layer. However, we observe that applying Token Sparse Attention across all layers leads to substantial performance degradation. This raises an orthogonal question: which layers can accommodate token-level sparsification with minimal impact on model behavior? To identify layers where token representations are sufficiently stable for sparsification, we introduce metric called Inter-Layer Representation Drift, which measures the relative change in tokens representation by comparing the L2 norms of its input and output hidden states. For given layer ℓ, the drift Rℓ is defined as: Rℓ = Et (cid:20) hℓ+1,t hℓ,t2 hℓ,t2 + ϵ (cid:21) , (1) where hℓ,t denotes the hidden state (i.e., layer input) of token at layer ℓ. lower drift value indicates smaller representational changes across layers, suggesting larger stability in token representations. As illustrated in Figure 4(a), the drift exhibits consistent pattern across varying tasks and context lengths on LLaMA-3.1-8B-Instruct. To investigate whether the drift is predictive of sparsification robustness, we conduct controlled experiment shown in Figure 4(b). Specifically, we randomly sample three layers from the model and apply Token Sparse Attention only to these layers with token coverage of 0.99, while keeping other layers dense. For each sampled triplet, we compute the mean normalized drift of the selected layers and evaluate the resulting accuracy on the RULER benchmark (4K-length). This process is repeated over 200 runs. The results reveal clear correlation: layer subsets with lower average drift tend to exhibit higher accuracy, validating Rℓ as reliable indicator for sparsity robustness. Based on this observation, we use representation drift as criterion to select layers for Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Table 1. Complementary Gains from Token Sparse Attention on RULER. Adding Token Sparse Attention on top of FlashAttention and existing sparse attention methods improves speed with marginal accuracy variation across all context lengths and models. Speedup denotes the 128K attention speedup relative to the FlashAttention baseline."
        },
        {
            "title": "Method",
            "content": "4K 8K 16K 32K 64K 128K Avg."
        },
        {
            "title": "Flash Attention",
            "content": "w/ Token Sparse"
        },
        {
            "title": "Minference",
            "content": "w/ Token Sparse"
        },
        {
            "title": "FlexPrefill",
            "content": "w/ Token Sparse"
        },
        {
            "title": "Flash Attention",
            "content": "w/ Token Sparse"
        },
        {
            "title": "Minference",
            "content": "w/ Token Sparse"
        },
        {
            "title": "FlexPrefill",
            "content": "w/ Token Sparse 95.82 96.06 93.46 93.05 95.48 95.33 95.19 95.07 92.52 93. 94.79 94.84 LLaMA-3.1-8B-Instruct 92.77 92.90 92.29 92.00 92.71 92.47 91.02 91. 91.01 91.03 91.40 91.49 84.87 84.81 85.34 85.10 87.20 87.68 83.43 82. 83.19 82.92 83.05 83.07 Mistral-Nemo-12B-Instruct 92.29 92.10 91.02 91.36 93.13 93. 85.60 85.97 84.29 84.31 86.62 86.14 64.86 63.67 65.18 64.70 64.58 64. 47.98 47.97 46.52 46.67 49.30 48.70 74.15 73.68 73.63 72.18 73.75 73. 19.68 19.44 19.00 18.68 20.54 19.58 87.01 87.02 86.49 86.05 87.27 87. 67.60 67.37 66.42 66.46 68.16 67.91 1.00 1.36 1.12 1.38 2.44 2. 1.00 1.22 1.13 1.28 1.22 1.33 token-level sparsification. We define normalized drift rank for each layer and select the subset of layers eligible for Token Sparse Attention as: ˆRℓ ="
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) k=1 1[Rk Rℓ], Lsparse = {ℓ ˆRℓ δ} (2) In all our experiments, we set δ = 0.5, applying our method strictly to the layers exhibiting the most stable representations. This layer selection is performed once as preprocessing step for each model. 3. Experiments Our experiments first demonstrate the complementary gains achieved when Token Sparse Attention is combined with other attention-acceleration techniques in Section 3.2. We further compare it against token-eviction methods in Section 3.4. detailed analysis of the accuracy-efficiency trade-off is provided in Section 3.3. 3.1. Experimental Setup Models and Datasets. We conduct experiments on LLaMA3.1-8B-Instruct (Dubey et al., 2024) and Mistral-Nemo12B-Instruct (Mistral AI Team, 2025). For evaluation, we primarily use RULER (Hsieh et al., 2024) and InfiniteBench (Zhang et al., 2024), two benchmarks designed to assess long-context understanding and retrieval performance. Evaluation on LongBench (Bai et al., 2023) and Needle-in-a-Haystack (Kamradt, 2023) is available in the Appendix A.2. Implementation Details. Token Sparse Attention is integrated into FlashAttention without modifying the underlying kernel, and the token-scoring step in Section 2.3 is implemented using Triton kernel (Tillet et al., 2019). To balance efficiency and accuracy, we use token-coverage parameter of τ =0.005 for LLaMA and τ =0.008 for Mistral across all experiments. All experiments are conducted on single NVIDIA A100 80GB GPU. Baselines. To validate our methods compatibility, several attention acceleration baselines are considered. FlashAttention (Dao, 2023) serves as the dense attention baseline, providing highly optimized exact attention with improved memory efficiency. Minference (Jiang et al., 2024a) is structured sparse attention method that applies predefined sparsity patterns to the attention map. We follow its official configuration, in which Vertical-Slash sparsity pattern is applied uniformly across all attention heads. FlexPrefill (Lai et al., 2025) is context-aware block-sparse attention method that dynamically prunes attention computation during prefill. Following the original paper, we set its hyperparameters to γ=0.95. To compare our method against token-eviction methods, FastKV (Jo et al., 2025) and GemFilter (Shi et al., 2024) are primarily considered. For fair comparison, all methods are applied only during prefill, while decoding uses standard dense attention. 3.2. Accuracy Results RULER. Table 1 reports the complementary performance of several attention-acceleration methods with and without Token Sparse Attention. Across all context lengths, To5 Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Table 2. Complementary Results from Token Sparse Attention on InfiniteBench. Method En.MC En.QA En.Dia Retr.KV Retr.N Retr.P Math.F Code.D Avg. Flash-Attention w/ Token Sparse Minference w/ Token Sparse FlexPrefill w/ Token Sparse Flash-Attention w/ Token Sparse Minference w/ Token Sparse FlexPrefill w/ Token Sparse 64.19 65. 67.25 65.94 68.12 67.25 33.19 33.19 37.99 38.43 35.81 35.37 LLaMA-3.1-8B-Instruct 19.00 17.00 18.50 18.50 11.00 14.00 55.20 55.00 51.60 49.60 50.80 46. 98.47 98.47 97.46 97.46 99.15 98.98 Mistral-Nemo-12B-Instruct 5.50 5.50 5.50 5. 7.00 6.50 0.00 0.00 0.00 0.00 0.00 0.00 36.61 35.76 34.58 33. 41.19 39.32 27.64 28.01 26.80 27.61 27.37 27.60 16.87 16.81 16.76 16. 17.45 16.71 97.80 97.46 97.80 97.80 96.95 96.27 62.71 62.88 31.19 30. 65.76 65.93 24.00 25.71 22.29 21.43 22.57 22.86 1.43 0.86 6.00 5. 4.57 3.14 20.56 20.30 19.54 19.29 20.30 20.05 27.16 27.41 26.14 26. 26.65 25.63 50.86 50.88 50.16 49.70 49.53 49.23 22.93 22.80 19.77 19. 24.80 24.08 Figure 5. Accuracy-Speedup Trade-offs. (a) Accuracy-speedup Pareto frontier obtained by sweeping FlexPrefill hyperparameters, comparing with the Token Sparse Attention at token coverage of τ =0.005. (b) Accuracy-speedup trade-off achieved by applying Token Sparse Attention with varying token coverage to FlashAttention (left) and FlexPrefill (right). ken Sparse Attention largely preserves the accuracy of the underlying attention kernels across context lengths while improving the attention efficiency. For example, when applied to FlexPrefill on LLaMA-3.1-8B-Instruct, the average accuracy remains unchanged at 87.27%, matching the vanilla FlexPrefill result. similar trend is observed when composing with FlashAttention and Minference. For Mistral-Nemo12B-Instruct, the deviation introduced by Token Sparse Attention is consistently small, staying within 0.5% of the baseline. In terms of efficiency, Token Sparse Attention reliably increases the attention speedup at 128K across all methods, e.g., for LLaMA-3.1-8B-Instruct, Minference improves from 1.12 to 1.38, and FlexPrefill from 2.44 to 2.76. Overall, Token Sparse Attention follows the baseline behavior closely, providing complementary efficiency gains with negligible impact on model accuracy. InfiniteBench. Table 2 shows that Token Sparse Attention also preserves performance on InfiniteBench while remaining fully compatible with all baseline methods. For both LLaMA-3.1-8B-Instruct and Mistral-Nemo-12B-Instruct, augmenting Flash Attention, Minference, and FlexPrefill with Token Sparse Attention leads to only marginal accuracy differences, with overall results closely matching those of the corresponding baselines. The efficiency gains follow the same trend observed in Table 1. These results highlight that Token Sparse Attention acts as general acceleration mechanism, providing consistent inference efficiency improvements while maintaining baseline accuracy. 3.3. Efficiency Results Accuracy-Speedup Trade-offs. We further analyze the computational benefits of Token Sparse Attention with respect to the accuracyefficiency trade-offs. The attention speedup is estimated by the average attention latency measured across all layers. Figure 5(a) illustrates the Pareto frontier obtained by varying the hyperparameter γ, which represents the sparsity parameter, of FlexPrefill, SOTA sparse attention (block-level) baseline. While aggressively tuning γ yields higher speedups, it often comes at the cost of rapid accuracy degradation. In contrast, applying Token Sparse Attention (with τ =0.005) on top of FlexPrefill consistently pushes the Pareto frontier outward, achieving 6 Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Figure 6. Attention Speedup Details. (a) Attention speedup across different context lengths. (b) Attention latency breakdown with Token Sparse Attention at 128K context, where the overhead includes token scoring and indexing, as well as QKV compression and attention output decompression. Table 3. Attention Map Sparsity. Average attention map sparsity across different context lengths under different token coverage, τ . Sparsity 4K 8K 16K 32K 64K 128K τ =0.005 17.00% 21.11% 26.61% 28.44% 34.07% 54.44% τ =0.010 28.02% 32.98% 39.55% 41.25% 47.32% 67.36% superior speedups at comparable accuracy levels. This verifies that our method provides complementary efficiency gain that cannot be achieved by simply adjusting hyperparameter. Furthermore, we examine the effect of the token coverage parameter τ on performance. As shown in Figure 5(b), increasing τ enables more aggressive token sparsification, yielding higher attention speedups for both FlashAttention and FlexPrefill. Remarkably, this aggressive reduction does not compromise model performance. The accuracy degradation remains within 1% even at higher sparsity levels. This robustness suggests that Token Sparse Attention effectively targets and removes only the irrelevant tokens in the context, allowing users to flexibly trade off small margin of accuracy for significant gains in inference speed. Sparsity and Speedup across Sequence Lengths. Figure 6(a) shows that the attention speedup achieved by Token Sparse Attention increases consistently as the context length increases. While the gains are modest at shorter sequence lengths, substantially larger speedups are observed at 128K and 256K where attention computation dominates the overall latency. This behavior is explained by the increase in attention sparsity at longer contexts. As detailed in Table 3, the average attention sparsity within the selected layers increases steadily with sequence length for token coverage τ . Together, these results demonstrate that Token Sparse Attention becomes increasingly effective in longer-context regimes, yielding larger efficiency gains when composed with existing attention baselines. Latency and Overhead Breakdown. Figure 6(b) presents latency breakdown of Token Sparse Attention at 128K context length. Across all token coverage settings, the additional overhead introduced by Token Sparse Attention remains minimal, accounting for less than 11% of the total attention latency across all layers, even at the highest sparsity level. This overhead includes token scoring and indexing, as well as QKV compression and attention output decompression. These results confirm that Token Sparse Attention achieves significant acceleration while incurring only small and well bounded overhead, validating its practicality for long context inference. Dynamic Sparsity vs. Fixed Sparsity. We compare our Dynamic Token Coverage against fixed token sparsity baseline that keeps constant fraction of tokens per layer. To ensure fair comparison, we match the overall attention speedup at 128K by selecting fixed sparsity ratios that yield similar acceleration to dynamic sparsity. As shown in Table 4, dynamic sparsity consistently achieves higher RULER average accuracy than fixed sparsity under comparable speedups. The gap becomes more pronounced at higher sparsity, where dynamic sparsity preserves accuracy substantially better than fixed sparsity while maintaining comparable acceleration. These results indicate that allocating the sparse budget based on the attention score distribution is more effective than enforcing rigid token retention ratio, especially in long-context settings where token relevance varies significantly across inputs and layers. 3.4. Comparison with Token Eviction We compare Token Sparse Attention with representative token eviction methods under matched efficiency budget. Specifically, we set token coverage to τ = 0.01 and select the hyperparameter settings of FastKV and GemFilter that yield similar average attention speedup at 128K. As shown in Table 5, Token Sparse Attention attains the highest av7 Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection Table 4. Comparison between Dynamic and Fixed Sparsity. τ denotes the token coverage used in dynamic sparsity, while represents the fixed token sparsity ratio. Sparsity is measured as the average attention map sparsity across the selected sparse layers. Experiments are based on RULER. Table 5. Comparison against Token Eviction. RULER accuracy comparison with token eviction methods on LLaMA-3.1-8BInstruct. Speedups in the compared methods are set to similar level, e.g., 1.50 in FastKV, 1.53 in GemFilter, and 1.51 in Ours, compared to FlashAttn. Metric Sparsity Accuracy Speedup Dynamic Sparsity Fixed Sparsity Method 4K 8K 16K 32K 64K 128K Avg. τ =0. τ =0.010 s=0.3 s=0.5 FlashAttn 95.82 92. 91.02 84.87 83.43 74.15 87.01 54.44% 87.02% 1.36 67.36% 50.96% 74.95% 86.84% 1.51 86.91% 1. 85.43% 1.57 GemFilter FastKV Ours 92.50 94.25 95.82 90.87 91.30 92.71 88.78 89.78 91.46 85.01 84.57 84. 80.42 81.54 83.00 73.15 72.39 73.25 85.12 85.64 86.84 erage RULER accuracy under comparable speedups. We attribute this advantage to our layer-wise dynamic budget allocation and reversible interleaving, where tokens skipped in one attention operation remain available via the residual path. In addition, head-wise selection enables fine-grained, head-specific token sets, avoiding the rigid unified token constraint of eviction-based methods. 4. Related Works Long-Context Inference for LLMs. Handling long contexts in large language models introduces significant computational and memory challenges across different inference stages. During the prefill phase, the quadratic complexity of the attention operation leads to substantial computational overhead as the context length increases. To address these issues, system-level approaches such as FlashAttention (Dao et al., 2022; Dao, 2023; Shah et al., 2024) and FlashInfer (Ye et al., 2025) have been proposed to provide efficient attention frameworks. At the same time, algorithmic optimizations for long-context inference are actively being explored. Prefill Acceleration. One major direction for prefill acceleration is token eviction and prompt compression. Methods such as FastKV (Jo et al., 2025), GemFilter (Shi et al., 2024), and PyramidInfer (Yang et al., 2024a) select important tokens at specific layers during the prefill phase and compress the corresponding hidden states to reduce computation. In contrast, prompt compression methods (Mu et al., 2023; Jiang et al., 2024b; Chuang et al., 2024) operate at the prompt level rather than inside the model, reducing computation by shortening or transforming the input context itself. Other works focus on sparse attention, which skips attention computation in certain regimes based on attention patterns. Minference (Jiang et al., 2024a) applies sparse attention using vertical-slash pattern, while FlexPrefill (Lai et al., 2025) introduces query-aware dynamic block-sparse attention. X-Attention (Xu et al., 2025) adopts an antidiagonal scoring strategy to reduce the search cost for identifying sparse patterns, whereas SeerAttention (Gao et al., 2024) trains additional linear layers to search sparse attention patterns. Model compression techniques such as quantization are also widely used. These methods reduce memory load time by compressing model weights or further compress activations to enable low-precision computation. Representative approaches include GPTQ (Frantar et al., 2023), SmoothQuant (Xiao et al., 2023), and AWQ (Lin et al., 2024). Decoding Acceleration. During the decoding phase, the computational bottleneck shifts from quadratic attention computation to the memory and bandwidth cost by KV cache. To address these challenges, methods such as H2O (Zhang et al., 2023), SnapKV (Li et al., 2024), AdaKV (Feng et al., 2024), and HeadKV (Fu et al., 2024) adopt KV cache eviction strategies that retain only important tokens in the KV cache during decoding. In contrast, approaches like ThinK (Xu et al., 2024) reduce decoding overhead by pruning channels of key representations rather than evicting tokens. In addition, applying quantization (Hooper et al., 2024; Liu et al., 2024; Yang et al., 2024b) to keys and values to reduce KV cache loading time has also been actively explored. 5. Conclusion In this paper, we introduced Token Sparse Attention, dynamic and reversible token-level sparsification mechanism for efficient long-context inference. The key advantage of Token Sparse Attention lies in its ability to reduce attention computation without permanently removing tokens, allowing token relevance to be re-evaluated across layers and heads. Moreover, our design is fully compatible with existing dense and sparse attention kernels, enabling seamless composition with prior acceleration methods. Experimental results on long-context benchmarks demonstrate that Token Sparse Attention consistently improves the accuracy latency trade-off across models and tasks. By selectively filtering irrelevant tokens before attention computation, our method achieves substantial attention speedups with minimal accuracy degradation. Ultimately, Token Sparse Attention provides complementary and practical solution for scalable long-context inference, offering an effective way to improve efficiency while preserving model behavior. Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection"
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Chuang, Y.-N., Xing, T., Chang, C.-Y., Liu, Z., Chen, X., and Hu, X. Learning to compress prompt in natural language formats. arXiv preprint arXiv:2402.18700, 2024. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. International Conference on Learning Representations (ICLR), 2023. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Feng, Y., Lv, J., Cao, Y., Xie, X., and Zhou, S. K. Adakv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference. arXiv preprint arXiv:2407.11550, 2024. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pretrained transformers. International Conference on Learning Representations (ICLR), 2023. Fu, Y., Cai, Z., Asi, A., Xiong, W., Dong, Y., and Xiao, W. Not all heads matter: head-level kv cache compression method with integrated retrieval and reasoning. arXiv preprint arXiv:2410.19258, 2024. Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao, Y. S., Keutzer, K., and Gholami, A. Kvquant: Towards 10 million context length llm inference with kv cache quantization. Advances in Neural Information Processing Systems, 37:12701303, 2024. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., and Ginsburg, B. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Jiang, H., Li, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A. H., Li, D., Lin, C.-Y., et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024a. Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16581677, 2024b. Jo, D., Song, J., Kim, Y., and Kim, J.-J. Fastkv: Kv cache compression for fast long-context processing with tokenselective propagation. arXiv preprint arXiv:2502.01068, 2025. Kamradt, G. Needle in haystack - pressure testhttps://github.com/gkamradt/ ing llms. LLMTest_NeedleInAHaystack, 2023. Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M. W., and Keutzer, K. Squeezellm: DenseInternational Conference on and-sparse quantization. Machine Learning (ICML), 2024. Laban, P., Kryscinski, W., Agarwal, D., Fabbri, A. R., Xiong, C., Joty, S. R., and Wu, C.-S. Summedits: Measuring llm ability at factual reasoning through In Conference on Emthe lens of summarization. pirical Methods in Natural Language Processing, 2023. URL https://api.semanticscholar. org/CorpusID:266164172. Lai, X., Lu, J., Luo, Y., Ma, Y., and Zhou, X. Flexprefill: context-aware sparse attention mechanism for In The Thirteenth efficient long-sequence inference. International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=OfjIlbelrT. Gao, Y., Zeng, Z., Du, D., Cao, S., Zhou, P., Qi, J., Lai, J., So, H. K.-H., Cao, T., Yang, F., et al. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli, A., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems (NeurIPS), 2024. 9 Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection A., and Srikumar, V. (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 3258 3270, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-acl.195. URL https://aclanthology. org/2024.findings-acl.195/. Yang, J. Y., Kim, B., Bae, J., Kwon, B., Park, G., Yang, E., Kwon, S. J., and Lee, D. No token left behind: Reliable kv cache compression via importance-aware mixed precision quantization. arXiv preprint arXiv:2402.18096, 2024b. Ye, Z., Chen, L., Lai, R., Lin, W., Zhang, Y., Wang, S., Chen, T., Kasikci, B., Grover, V., Krishnamurthy, A., et al. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. Yi, Z., Ouyang, J., Xu, Z., Liu, Y., Liao, T., Luo, H., and Shen, Y. survey on recent advances in llm-based multiturn dialogue systems. arXiv preprint arXiv:2402.18013, 2024. Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M., Han, X., Thai, Z., Wang, S., Liu, Z., et al. -bench: Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1526215277, 2024. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems (NeurIPS), 2023. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6:87100, 2024. Liu, Z., Yuan, J., Jin, H., Zhong, S., Xu, Z., Braverman, V., Chen, B., and Hu, X. Kivi: tuning-free asymmetric 2bit quantization for kv cache. International Conference on Machine Learning (ICML), 2024. Mistral AI Team. Mistral-nemo. https://mistral. ai/news/ministraux, 2025. Mu, J., Li, X., and Goodman, N. Learning to compress prompts with gist tokens. Advances in Neural Information Processing Systems, 36:1932719352, 2023. Rando, S., Romani, L., Sampieri, A., Franco, L., Yang, J., Kyuragi, Y., Galasso, F., and Hashimoto, T. Longcodebench: Evaluating coding llms at 1m context windows. arXiv preprint arXiv:2505.07897, 2025. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems (NeurIPS), 37:68658 68685, 2024. Shi, Z., Ming, Y., Nguyen, X.-P., Liang, Y., and Joty, S. Discovering the gems in early layers: Accelerating longcontext llms with 1000x input token reduction. arXiv preprint arXiv:2409.17422, 2024. Tillet, P., Kung, H.-T., and Cox, D. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pp. 1019, 2019. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training In Internaquantization for large language models. tional conference on machine learning, pp. 3808738099. PMLR, 2023. Xu, R., Xiao, G., Huang, H., Guo, J., and Han, S. Xattention: Block sparse attention with antidiagonal scoring. arXiv preprint arXiv:2503.16428, 2025. Xu, Y., Jie, Z., Dong, H., Wang, L., Lu, X., Zhou, A., Saha, A., Xiong, C., and Sahoo, D. Think: Thinner key cache by query-driven pruning. arXiv preprint arXiv:2407.21018, 2024. Yang, D., Han, X., Gao, Y., Hu, Y., Zhang, S., and Zhao, H. PyramidInfer: Pyramid KV cache compression for high-throughput LLM inference. In Ku, L.-W., Martins, 10 Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection A. Appendix A.1. Additional Models and Datasets Details Models. To evaluate our approach in the long-context regime, we conduct experiments on LLaMA-3.1-8B-Instruct (Dubey et al., 2024) and Mistral-Nemo-12B-Instruct (Mistral AI Team, 2025), designed to operate with 128K context window. The checkpoints for all models are publicly available at: LLaMA-3.1-8B-Instruct: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct Mistral-Nemo-12B-Instruct: https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407 RULER. RULER is configurable synthetic benchmark designed to assess the long-context capabilities of large language models under varying sequence lengths and task difficulties. Rather than focusing on single retrieval scenario, it generalizes the Needle-in-a-Haystack setup into comprehensive benchmark suite comprising 13 tasks grouped into four categories: retrieval-style tasks, aggregation tasks (e.g., CWE and FWE), multi-hop tracing tasks (VT), and question answering. This diverse task design allows for controlled and systematic evaluation of models ability to retrieve, aggregate, and reason over long contexts beyond simple keyword matching. InfiniteBench InfiniteBench is large-scale benchmark designed to evaluate the ability of language models to understand and reason over extremely long contexts that exceed 100K tokens. It includes diverse collection of tasks spanning multiple domains, such as retrieval, reasoning, code understanding, mathematical computation, dialogue, and summarization, and covers both synthetic and realistic scenarios. A.2. Additional Experiments Needle-in-a-Haystack We provide Needle-in-a-Haystack results for LLaMA-3.1-8B-Instruct in Figure 7. Token Sparse Attention demonstrates strong accuracy preservation under long-context settings. When composed with FlexPrefill, Token Sparse Attention consistently achieves higher accuracy than FlexPrefill alone, These results suggest that reversible tokenlevel sparsification effectively removes irrelevant context while preserving the critical signal required for precise retrieval in long-context scenarios. Figure 7. Attention latency breakdown with Token Sparse Attention at 128K context, where the overhead includes token scoring and indexing, as well as QKV compression and attention output decompression. 11 Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection LongBench. Table 6 reports the LongBench results on LLaMA-3.1-8B-Instruct. LongBench covers broad spectrum of long-context tasks, including single-document and multi-document question answering, summarization, few-shot learning, synthetic reasoning, and code understanding, providing comprehensive evaluation of model robustness under long-context settings. Across all baselines, Token Sparse Attention preserves performance consistently while introducing minimal accuracy changes. Table 6. LongBench results with Token Sparse Attention on LLaMA-3.1-8B-Instruct. Single-Document QA Multi-Document QA Summarization Few-shot Learning Synthetic Code NrtvQ Qasper MF-en HotpotQ 2WikiM MuSiQue GovReport MSum MultiNews TriviaQ MSum SA T L B-P PCount PRe Avg. Method Flash-Attention 30.22 45.37 55.80 w/ Token Sparse 30.43 45.17 55.34 Minference 29.48 45.64 52.59 w/ Token Sparse 29.28 46.19 52.52 FlexPrefill 27.99 44.63 54.94 w/ Token Sparse 28.07 44.84 54.74 55.97 55. 55.05 54.62 57.31 56.63 45.00 45.56 42.93 42.18 41.82 40.24 31.26 31. 29.24 29.89 31.97 31.09 35.12 34.97 34.88 34.79 34.50 34.79 25.38 25. 24.94 25.05 24.92 25.47 27.20 72.50 91.65 44.32 9.41 99.50 62.90 56.80 49.28 27.09 72.50 91.64 44.07 7.47 98.00 63.06 56.28 48.96 26.53 71.00 91.89 44.15 6.38 98.50 62.38 50.72 47.89 26.48 71.00 91.89 44.07 5.34 94.50 62.79 51.12 47.61 27.06 70.50 90.74 43.81 4.09 82.00 63.08 60.36 47.48 27.12 69.00 90.89 43.72 3.09 85.50 62.78 60.24 47.39 A.3. Limitation While Token Sparse Attention is primarily designed for long-context inference, its efficiency gains are naturally smaller at short context lengths where attention computation does not yet dominate end-to-end latency. This behavior is common to most coverage-based sparse attention methods and does not detract from the effectiveness of our approach in the long-context regimes that are the main focus of this work. Meanwhile, applying overly aggressive token sparsification can increase the risk of excluding semantically important tokens from attention, potentially leading to accuracy degradation. However, Token Sparse Attention is designed to remove irrelevant tokens that contribute primarily to attention noise rather than informative context. By selecting an appropriate token coverage, the method enables controlled and practical accuracyefficiency trade-off, which is the primary objective in long-context regimes. Finally, while this work focuses on prefill acceleration for text generation models, Token Sparse Attention can be extended in several promising directions. Future work includes adapting the proposed mechanism to the decoding phase, as well as exploring its applicability to multimodal settings such as visionlanguage models."
        }
    ],
    "affiliations": [
        "Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea"
    ]
}