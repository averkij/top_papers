{
    "paper_title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
    "authors": [
        "Amirhosein Ghasemabadi",
        "Di Niu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision."
        },
        {
            "title": "Start",
            "content": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits Amirhosein Ghasemabadi University of Alberta, Canada ghasemab@ualberta.ca Di Niu University of Alberta, Canada dniu@ualberta.ca 6 2 0 2 ] . [ 2 8 7 5 0 2 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only 5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision. Code and models: Gnosis Github."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have achieved remarkable performance in open-ended generation and multi-step reasoning, yet they remain unreliable at assessing the correctness of their own outputs(Kalai et al., 2025; Huang et al., 2025b). They frequently produce confident but incorrect answers, failing to detect reasoning errors or hallucinations even when such failures are evident to external evaluators(Kirichenko et al., 2025; Kamoi et al., 2024). This gap between strong generation and weak selfverification limits the reliability, safety, and efficiency of LLM deployment, particularly in settings that require long-horizon reasoning or computeaware control. fundamental open question is whether LLMs can anticipate their own failures by examining the internal dynamics that govern their generation process. Prior work on LLM self-evaluation and hallucination detection largely follows three paradigms. Text-based self-critique and confidence estimation (Kadavath et al., 2022; Ulmer et al., 2024; Huang et al., 2025a) infer correctness from generated text or token probabilities, often tracking linguistic fluency rather than reasoning validity and degrading on long or compositional tasks. Multisample consistency methods (Sriramanan et al., 2024a; Pawitan and Holmes, 2025) estimate confidence from agreement across multiple generations, improving robustness at the cost of inference that scales linearly with the number of samples. External judges and reward models (Stiennon et al., 2020; Ouyang et al., 2022; Zheng et al., 2024; Wang et al., 2024b; Liu et al., 2025) train large auxiliary models to evaluate responses, providing strong signals but requiring costly supervision, additional decoding passes, and substantial inference overhead. Despite their differences, these approaches rely on signals external to the models own internal dynamics, leaving open whether correctness can be predicted directly from the generation process itself. In this paper, we demonstrate that large language models can reliably predict their own failures by leveraging signals intrinsic to the generation process. We introduce Gnosis, lightweight selfawareness mechanism that endows frozen LLMs with intrinsic self-verification, eliminating the need for external judges. By extracting reliability cues directly from model-internal dynamics during inference, Gnosis produces accurate and well-calibrated Figure 1: Overview of our Gnosis self-awareness mechanism and its performance. Left: Gnosis taps hidden states and attention maps from frozen LLM, learns to compress them into hidden/attention descriptors, and predicts scalar correctness (hallucination) score with only 5 million extra parameters and essentially zero added inference cost. Right: Gnosis outperforms 8B Skywork reward models and Gemini 2.5 Pro judge in AUROC on Math-Reasoning (AMC12 + AIME24/25 + HMMT Feb 2025), Open-Domain QA (TriviaQA), and Academic Knowledge (MMLU-Pro); scores are averaged over the frozen backbones listed in Table 1. correctness estimates with negligible computational overhead. This intrinsic capability enables early detection of failing reasoning trajectories, efficient scaling across model sizes and domains, and practical deployment of compute-aware and reliability-critical language systems. Our main contributions are: Intrinsic, Trajectory-Level Self-Awareness. We introduce Gnosis, lightweight mechanism that enables frozen LLMs to predict the correctness of their own generations by decoding signals intrinsic to the inference process. Unlike prior internal-signal methods that rely on statistical features (Geng et al., 2023; Wang et al., 2025; Zhang et al., 2025b) or singletoken indicators(Zhang et al., 2025a), Gnosis leverages the full spatiotemporal structure of internal dynamics across an entire generation trajectory. Dual-Stream Introspection from Hidden States and Attention. Gnosis jointly models hidden-state evolution and attention-routing patterns through compact, fixed-budget architecture that operates independently of sequence length, extracting rich reliability cues with negligible inference overhead. Cross-Scale Transfer and Early Failure Detection. Gnosis generalizes beyond selfjudgment: head trained on small backbone model transfers zero-shot to larger variants, and predicts failures reliably from partial reasoning and generations, enabling early termination and compute-aware control. State-of-the-Art Performance at Minimal Scale. With only 5M added parameters, Gnosis is orders of magnitude smaller than external verifiers yet outperforms billionparameter reward models and proprietary judges on math reasoning, open-domain QA, and academic benchmarks. It works reliably across diverse frozen backbones with negligible latency overhead."
        },
        {
            "title": "2 Related Work\nMethods for assessing LLM correctness and hallu-\ncination risk largely fall into four families: Text-\nbased confidence & self-critique, Internal signal-\nbased indicators and linear probes, external reward\nmodels and judge LLMs, and multi-sample self-\nconsistency methods.",
            "content": "External Reward Models and Judge LLMs. External verifiers train separate models to score response quality, factuality, or step-wise correctness. Outcome and Process Reward Models (ORM/PRM) are widely used for ranking, hallucination detection, and guiding test-time search (Stiennon et al., 2020; Ouyang et al., 2022; Zheng et al., 2024; Wang et al., 2023; Zhang et al., 2025c). Recent systems emphasize large, carefully curated datasets over architectural novelty: HelpSteer2 combines Likert ratings, pairwise preferences, and extrapolation to sharpen discrimination (Wang et al., 2024b), while Skywork-Reward-V2 scales humanAI curation to tens of millions of preference pairs and leads on RewardBench-style suites (Liu et al., 2025). These models provide strong signals but incur substantial annotation cost and add inference latency and deployment overhead by requiring large auxiliary model at serving time. Text-Based Confidence & Self-Critique. Textbased approaches aim to estimate correctness from the generated text and token probabilities. Trainingfree indicators use entropy or max probability as uncertainty proxies but struggle with confident hallucinations and out-of-distribution shifts (Geng et al., 2023; Sriramanan et al., 2024b; Pawitan and Holmes, 2025). Prompt-based calibration elicits verbalized confidence or self-critique, improving ECE but often tracking stylistic fluency more than reasoning validity and requiring extra passes (Kadavath et al., 2022; Ulmer et al., 2024). Generative and distillation-based calibrators predict correctness in single forward pass, e.g., APRICOT trains calibrator LLM (Ulmer et al., 2024), and SelfCalibration distills self-consistency signals to enable early stopping and confidence-weighted sampling (Huang et al., 2025a). These methods reduce dependence on external judges but may require fullmodel fine-tuning, add training cost, and remain brittle across domains and sequence lengths. Internal signal-based indicators and linear probes. Glass-box signals exploit logits, hidden states, and attention routing. Prior work shows hidden activations diverge between correct and hallucinated outputs (Duan et al., 2024), with factuality cues concentrated in middle/deep layers yet sensitive to domain shift (Zhang et al., 2025b). Token-wise hidden-state entropy and information density can outperform perplexity-based failure prediction (Chen et al., 2024). Trajectory/spectral views analyze how representations evolve across layers (e.g., Chain-of-Embedding, stability of latent paths) and relate angular/magnitude changes to correctness (Wang et al., 2025). Attention statistics provide lightweight reliability cues (Huang et al., 2024). complementary line trains simple probes (shallow MLPs) on final-token states (Azaria and Mitchell, 2023; Burns et al., 2022; Zhang et al., 2025a). However, these approaches consistently yield low accuracy across diverse benchmarks. By relying on fragile heuristics or single-token snapshots, they miss the generations full spatiotemporal structure, resulting in performance that falls far short of Gnosis. Multi-Sample Self-Consistency and TestTime Scaling. Multi-sample self-consistency infers confidence from agreement across sampled rationales, boosting robustness but incurring inference cost that scales with the number of samples and often saturating on long, compositional tasks (Sriramanan et al., 2024b). Recent cost-aware test-time scaling uses internal signals to prune search or adapt compute, reducing dependence on large external verifiers while retaining some benefits of multi-sample reasoning (Huang et al., 2025a; Ghasemabadi et al., 2025)."
        },
        {
            "title": "3 The Gnosis Mechanism",
            "content": "We introduce Gnosis, lightweight self-awareness mechanism designed to retrofit frozen LLMs with introspection capabilities. Gnosis operates on the intuition that models internal traces, its evolving hidden states and attention routing patterns, carry distinctive fingerprints of hallucination and reasoning errors. Unlike external judges that require separate, expensive decoding passes, Gnosis is passive observer: it compresses the backbones internal signals into compact descriptors and fuses them to predict scalar correctness score. The architecture is explicitly designed so that its inference cost is independent of the sequence length, adding negligible overhead even for very long contexts. 3.1 Problem Setup and Length-Invariant Inputs Let denote an input prompt of length Sx and ˆy the generated response of length Sy. The input to Gnosis is the concatenated sequence with total size of = Sx + Sy tokens.\" The backbone has hidden dimension D, decoder layers, and attention heads per layer. During generation, we read only the final-layer hidden states last RSD and the attention maps = {Aℓ,h}ℓ=1..L, h=1..H , where each Aℓ,h RSS is the attention map of head in layer ℓ. Gnosis learns verification function: ˆp = fϕ (cid:0)H last, A(cid:1) [0, 1], (1) where ˆp is the estimated probability that the generated answer is correct and ϕ are the parameters of Gnosis. The backbone LLM remains frozen throughout. Fixed-Budget Compression. To decouple computational cost from sequence length S, we use projection operator Π that maps variable-length traces into fixed-size tensors: Hidden States. The sequence last RSD is interpolated and adaptively pooled along the sequence dimension to fixed budget Khid Table 1: Correctness/hallucination detection across domains. For each model, columns (left to right) are: AUROC / AUPR-c / AUPR-e / BSS / ECE. Method Qwen3 1.7B-Hybrid Qwen3 4B-Thinking Qwen3 4B-Instruct OpenAI gpt-oss-20B (AUROC / AUPR-c / AUPR-e / BSS / ECE ) Domain I: Math-Reasoning(AMC12 + AIME24/25 + HMMTFeb2025) .79 .73 .82 .25 .05 .80 .80 .77 .23 .12 .83 .79 .82 .32 .74 .80 .79 .81 .32 .07 Logit Entropy (2024a) .78 .71 .82 .23 .06 .80 .79 .76 .21 .13 .82 .79 .82 .31 .08 .79 .78 .80 .30 .07 Mean Token Prob (2024a) .61 .52 .63 -.13 .17 .55 .60 .46 -.28 .23 .72 .66 .75 .11 .13 .72 .66 .75 .11 .13 Attn Eigenvalue Score (2024a) .59 .51 .63 -.17 .20 .56 .60 .55 -.26 .24 .66 .65 .59 -.02 .15 .66 .66 .60 -.02 .15 CoER (2025) CoEC (2025) .60 .53 .64 -.14 .18 .53 .57 .52 -.32 .26 .66 .66 .59 -.02 .14 .66 .66 .59 -.02 .14 SkyworkRM-Llama3.1-8B (2025) .88 .88 .88 .38 .10 .87 .93 .80 .24 .18 .83 .90 .72 .06 .22 .81 .80 .83 .29 .10 .90 .92 .89 .39 .14 .89 .94 .77 .10 .22 .83 .89 .75 -.49 .40 .81 .79 .84 -.12 .30 SkyworkRM-Qwen3-8B (2025) .91 .88 .86 .50 .11 .92 .97 .68 .46 .15 .84 .88 .66 .31 .18 .92 .98 .64 -1.09 .10 Gemini 2.5 Pro (2025) .95 .95 .94 .59 .09 .96 .98 .91 .65 .05 .93 .96 .89 .51 .08 .85 .86 .86 .38 .04 Gnosis(Ours) Logit Entropy Mean Token Prob Attn Eigenvalue Score CoER CoEC SkyworkRM-Llama3.1-8B SkyworkRM-Qwen3-8B Gemini 2.5 Pro Gnosis(Ours) Logit Entropy Mean Token Prob Attn Eigenvalue Score CoER CoEC SkyworkRM-Llama3.1-8B SkyworkRM-Qwen3-8B Gemini 2.5 Pro Gnosis(Ours) Domain II: Open-Domain QA(TriviaQA) .64 .53 .73 -.16 .19 .68 .70 .63 .02 .13 .71 .75 .64 .74 .11 .79 .85 .63 .05 .21 .63 .52 .72 -.17 .19 .67 .70 .63 .00 .14 .71 .75 .64 .71 .11 .79 .86 .62 .05 .20 .52 .40 .62 -.39 .27 .57 .61 .50 -.20 .19 .59 .65 .51 -.16 .19 .65 .81 .40 -.30 .21 .59 .44 .70 -.25 .22 .53 .57 .49 -.29 .24 .57 .62 .48 -.23 .19 .78 .86 .61 -.03 .21 .58 .42 .70 -.28 .22 .59 .61 .54 -.17 .20 .52 .57 .44 -.32 .22 .78 .87 .61 -.03 .21 .83 .74 .87 .00 .25 .75 .74 .74 -.13 .28 .69 .73 .60 -.47 .37 .79 .88 .59 -1.11 .52 .84 .73 .89 .00 .23 .73 .89 .43 -.05 .17 .67 .71 .57 -.82 .47 .82 .90 .64 -1.54 .59 .90 .79 .91 .40 .11 .84 .80 .86 .33 .14 .75 .75 .67 -.02 .23 .74 .83 .54 -.01 .20 .87 .79 .92 .34 .10 .89 .89 .88 .45 .05 .86 .87 .84 .38 .05 .83 .90 .73 .19 .17 Domain III: Academic Knowledge-Reasoning(MMLU-Pro) .73 .86 .49 -.11 .20 .74 .90 .41 -.31 .25 .70 .82 .45 -.22 .21 .61 .75 .39 -.36 .23 .73 .86 .49 -.11 .19 .74 .89 .42 -.30 .25 .70 .83 .48 -.17 .23 .65 .78 .31 -.24 .20 .61 .78 .36 -.35 .22 .51 .78 .24 -.76 .29 .59 .80 .32 -.47 .23 .52 .72 .31 -.50 .26 .55 .72 .34 -.47 .25 .59 .80 .32 -.60 .30 .52 .72 .28 -.64 .29 .61 .77 .38 -.33 .22 .55 .72 .34 -.47 .25 .60 .81 .32 -.58 .28 .52 .73 .29 -.62 .29 .60 .76 .38 -.34 .22 .65 .79 .46 .01 .10 .61 .82 .37 -.03 .10 .61 .80 .38 -.11 .15 .71 .82 .53 -.39 .33 .76 .87 .53 .01 .17 .73 .88 .43 -.05 .17 .66 .82 .45 -.57 .35 .75 .86 .57 -1.02 .50 .76 .83 .68 .12 .16 .70 .87 .49 -.01 .18 .67 .83 .37 -.28 .24 .78 .84 .70 .22 .15 .80 .90 .56 .15 .11 .82 .93 .55 .21 .05 .74 .87 .51 .10 .05 .75 .84 .51 .07 .06 Table 2: Comparison with an MLP-Prob baseline on Qwen3 1.7B. Method Math TriviaQA MMLU-Pro does not grow with and is negligible compared to the backbone; see Appendix for architectural details. (AUROC , AUPR-c , ECE ) 3.2 Hidden-State Circuit Encoder MLP-Prob (2025a) Gnosis .86 .85 .19 .71 .58 .21 .69 .79 .23 .95 .95 .09 .87 .79 .10 .80 .90 .11 (e.g.,192), yielding = Πhid(H last) RKhidD. (2) Attention Maps. Each attention map Aℓ,h RSS is downsampled via adaptive pooling to fixed grid size (e.g., = 256), giving standardized set = { Aℓ,h}ℓ,h, Aℓ,h Rkk. (3) All downstream encoders operate only on and with fixed dimensions (Khid, D) and (L, H, k, k), so the computational cost of Gnosis Standard confidence methods often rely on token probabilities (logits), which are poorly calibrated and only weakly aligned with correctness (Ghasemabadi et al., 2025). Gnosis instead learns from the backbones internal representation, extracting correctness cues directly from the finallayer latent representations. small encoder ρhid maps this latent trace into compact descriptor: zhid = ρhid( H) RDHID. (4) Local Temporal Encoder. We treat as temporal signal and apply lightweight multi-scale 1D depthwise convolution over the sequence dimension to capture local dependencies and irregularities in the hidden trajectory. Table 3: Sibling-model judgment across domains. Each triplet of columns shows AUROC / AUPR-c / ECE. Gnosis-SelfJudge: Gnosis head trained on each backbone and judging its own generations. Gnosis-RM: single Gnosis head trained on Qwen3 1.7B-Hybrid and used as reward model for the other models. Judge / Model Qwen3 1.7B-Hybrid Qwen3 4B-Thinking Qwen3 8B-Hybrid (AUROC , AUPR-c , ECE ) Domain I: Math(AMC12 + AIME24/25 + HMMTFeb2025) SkyworkRM-Qwen3-8B .90 .92 .14 .89 .94 .22 .86 .95 .23 .96 .98 .05 .97 .97 .08 Gnosis-SelfJudge .93 .97 .18 .97 .99 .07 Gnosis-Qwen1.7B as RM .95 .95 .09 Domain II: Open-Domain QA(TriviaQA) SkyworkRM-Qwen3-8B .84 .73 .23 .73 .89 .17 .72 .78 .32 .89 .89 .05 .86 .90 .09 Gnosis-SelfJudge .86 .86 .04 .84 .88 .12 Gnosis-Qwen1.7B as RM .87 .79 .10 Domain III: Academic Knowledge-Reasoning(MMLU-Pro) SkyworkRM-Qwen3-8B .76 .87 .17 .73 .88 .17 .73 .89 .19 .82 .93 .05 .83 .94 .07 Gnosis-SelfJudge .81 .93 .16 .83 .94 .15 Gnosis-Qwen1.7B as RM .80 .90 .11 Table 4: Correctness detection on Math-reasoning for Qwen3 1.7B-Hybrid backbone under two max response lengths (12k, 24k). We compare Gnosis with SkyworkRM-Qwen3-8B, highlighting Gnosiss nearconstant latency and large speedups as response length increases. Method Max len Latency(ms) AUROC SkyworkRM-Qwen3-8B 12k SkyworkRM-Qwen3-8B 24k 12k Gnosis 24k Gnosis 930 2465 25 (37) 25 (99) .90 .88 .95 .94 Global Set Encoder. To summarize the sequence into compressed representation, we then apply Set Transformerstyle encoder (Lee et al., 2019): Set Attention Blocks (SAB) followed by Poolingby-Multihead-Attention (PMA) block. This enables global interaction across all positions and aggregates the sequence into small set of summary tokens, which are flattened and linearly projected to form the final hidden descriptor zhid. Figure 2 illustrates the detailed architecture design of Hidden Circuit Encoder. Appendix A.1 details the encoder architecture, while Appendix B.3 presents full design ablations. marizes each downsampled attention map Aℓ,h Rkk into compact feature vector: vℓ,h = Φ( Aℓ,h) Rdgrid. (5) Here Φ denotes our per-map feature extractor, which outputs dgrid-dimensional summary for each attention map. Per-map Feature Extraction. We implement Φ using two complementary approaches: (i) lightweight CNN that treats each attention map as an image and learns local-to-global patterns, and (ii) an interpretable statistics-based extractor that summarizes how attention is distributed, where it concentrates, and how local or long-range it is. Concretely, the statistics include simple measures of spread and texture (e.g., entropyand frequencybased features), diagonal and near-diagonal mass to capture locality, and lightweight center-and-spread measures that describe the average location of attention and how widely it is dispersed. We ablate each variant and their hybrid in Appendix B.2  (Table 6)  . While the two extractors are individually competitive, the hybrid is the most consistent across benchmarks; we therefore adopt [Φcnn; Φstat] in the final design. Full definitions of the statistics are provided in Appendix A.2. Cross-Head and Cross-Layer Encoding. We arrange the per-head summaries into an layerhead grid: RLHdgrid, Gℓ,h,: = vℓ,h. (6) We add learned layer and head embeddings to preserve depth and head identity. We then treat the entries as grid tokens. lightweight encoder ρattn mixes information across layers and heads using few axial convolutional layers. This design is substantially cheaper than full global selfattention over the grid. Finally, we apply Poolingby-Multihead-Attention (PMA) to aggregate the grid into single descriptor: zattn = ρattn(G) RDATT. (7) 3.3 Attention Circuit Encoder The attention stream reveals layerand headlevel routing patterns that can indicate brittle reasoning or unstable focus, complementing the hidden-state descriptor. Rather than feeding raw attention weights into large network, Gnosis sumBecause this stage operates on fixed dimensions (L, H, dgrid), the Gnosis-side compute is independent of the original sequence length S. Figure 2 illustrates the detailed architecture design of Attention Circuit Encoder. Detailed architecture choices and ablations are deferred to Appendix B.2. Figure 2: Gnosis Encoder Architecture Details. Hidden Circuit (left): project and adaptively pool the hiddenstate trace, apply multi-scale dilated temporal mixing, then use lightweight attention-based pooling (SABPMA) to produce compact descriptor zhid. Attention Circuit (right): downsample each layerhead attention map to fixed kk grid, extract per-map CNN+statistics features, mix across the layerhead grid with lightweight axial processor, and pool (PMA) to obtain zattn. Appendix gives detailed description of the encoder design, and Appendix includes the complete set of architecture and design ablations. 3.4 Gated Fusion and Correctness Prediction Gnosis fuses the hidden and attention descriptors into single vector and maps it to correctness probability. We concatenate the two descriptors where yi {0, 1} indicates whether the verifier judged the i-th generation as correct. Gnosis is trained to minimize binary cross-entropy: L(ϕ) = (H last,A,y)D (cid:2) log ˆp+(1y) log(1ˆp) (cid:3), = [zhid; zattn], (8) and feed the result into small gated MLP head. The final correctness estimate is ˆp = σ(cid:0)GatedMLPϕ(z)(cid:1), (9) where GatedMLPϕ is lightweight gated MLP and σ is the sigmoid. This head lets Gnosis adaptively weight hidden versus attention features on per-example basis(e.g., leaning more on attention for reasoning traces and more on hidden states for factual recall). The architecture is intentionally small: Gnosis adds only 5M parameters, making it 1000 smaller than 8B reward models and dramatically smaller than Gemini 2.5 pro as judge. 3.5 Training key advantage of Gnosis is that it can be trained without costly-annotated data. For each backbone, we generate answers on the training sets and label correctness by comparing predictions to groundtruth answers. This yields binary classification dataset: = {(H last , Ai, yi)}N i=1, with ˆp = fϕ(H last, A). The backbone is frozen; gradients flow only through the Gnosis encoders and fusion head."
        },
        {
            "title": "4 Experiments and Results",
            "content": "We evaluate Gnosis in three practical regimes: (i) self-judgment, where each Gnosis head scores generations from its own frozen backbone; (ii) siblingmodel judgment, where small head serves as lightweight reward model for larger family members; and (iii) early correctness prediction, where Gnosis is queried on partial completions to support compute-aware control. 4.1 Experimental Setup Backbones. We apply Gnosis to five frozen LLMs: Qwen3 1.7B-Hybrid, Qwen3 4B-Thinking, Qwen3 4B-Instruct, Qwen3 8B-hybrid(Yang et al., 2025) and OpenAI gpt-oss-20B(Agarwal et al., 2025). The backbone weights and decoding settings are never updated. Training Data. We train one correctness head per backbone on mixed mathtrivia corpus to cover both multi-step reasoning and open-domain factual recall. For math, we use the English portion of DAPO-Math-17k (14k competition-style Figure 3: Early Correctness Prediction on Math-Reasoning. Gnosis (red) achieves higher accuracy and better calibration than both MLP-Prob (blue) and reward model SKYWORKRM-QWEN3-8B (yellow). Notably, after seeing 40% of the completion, Gnosis already matches the full-solution performance of the other methods. Figure 4: 2D Embeddings of Features Learned by Gnosis on Math-Reasoning. We show dimensionality-reduced embeddings of hidden-state features (left), attention features (middle), and their merged features (right), with KDE contours and marginal densities for wrong (red) and correct (blue) answers. Hidden features exhibit the clearest separation, attention features show weaker but still clear separation, and the merged space yields the sharpest overall discrimination between correct and wrong solutions. problems with numeric or symbolic answers(Yu et al., 2025)). For QA, we subsample 40k questions from 118k-item TriviaQA training set(Joshi et al., 2017) to retain broad coverage while keeping training compact. We generate two completions per math prompt to capture diverse reasoning trajectories and increase correct/incorrect label variety under the same question, and one completion per trivia prompt since answers are shorter and often less ambiguous. We extract final answer, label correctness by comparing to the ground-truth, and discard outputs without valid answers. This yields balanced, fully automated training set that requires no human annotation. Training Details and Cost. We train each head for two epochs over this mixed dataset using Adam with learning rate of 1 104. Because the backbone is frozen and all feature extractors operate at fixed budget independent of sequence length, training is lightweight. For the largest backbone (gpt-oss-20B MoE), the full pipeline, data generation and training finishes in roughly 12 hours on 2A100 80 GB GPUs, corresponding to $25 in cloud cost. Smaller backbones train faster. Benchmarks. For each benchmark, we prompt each frozen backbone to generate solution with maximum budget of 12k tokens, and retain only questionanswer pairs with valid final answer for evaluation. We evaluate Gnosis on three disjoint domains: Math-Reasoning (AMC12 2022/2023(AI-MO Team, 2024), AIME 2024/2025(Zhang and Math-AI, 2024, 2025), HMMT Feb 2025(Balunovic et al., 2025)), OpenDomain QA (18k held-out TriviaQA questions with no overlap with training), and Academic Knowledge Reasoning (MMLU-Pro(Wang et al., 2024a)). Together, these benchmarks stress multistep reasoning, hallucination detection on short factoid answers, and out-of-distribution generalization. We report detailed backbone-level outcome statistics (accuracy, hallucination, and nonresponse rates) in Appendix D. Additional benchmark details are provided in Appendix C. Metrics. We treat correctness prediction as binary classification and report AUROC and AUPR under two complementary labelings (AUPR-c: correct as positive; AUPR-e: incorrect as positive), together with calibration metrics Brier Skill Score (BSS) and Expected Calibration Error (ECE). AUROC/AUPR measure discriminative ranking under class imbalance, whereas BSS/ECE assess the quality and calibration of predicted probabilities. See Appendix for extended interpretations. Baselines. We compare against four baseline families. (1) Statistical internal scores are training-free indicators computed from the backbones own outputs, reported in Table 1 as Logit Entropy, Mean Token Prob, and Attn Eigenvalue Score(Sriramanan et al., 2024a). (2) Trajectory/spectral internal indicators summarize crosslayer hidden-state dynamics, reported as CoER and CoEC(Wang et al., 2025). (3) External judges include two open-source reward models that are state-of-the-art on public reward-model benchmarks(Malik et al., 2025), SkyworkRM-Llama3.18B and the family-aligned SkyworkRM-Qwen38B(Liu et al., 2025), as well as Gemini 2.5 Pro used as judge (the Gemini judging prompt is provided in the Appendix E); all are reported in Table 1. (4) Learnable probe(Zhang et al., 2025a) that observes only the final answer tokens hidden state is reported separately on Qwen3 1.7B in Table 2 to isolate the limitations of single-token probing. 4.2 Self-Judgment We evaluate Gnosis in the standard self-judgment setting: for each backbone, the model generates answers to the benchmark questions, and the verification method predicts the correctness of these specific generations. As shown in Tables 1 and 2, Gnosis consistently outperforms training-free baselines and large external judges across all tested domains. Superiority Over Internal Baselines and Probes. Across Math Reasoning and OpenDomain QA, Gnosis effectively solves the miscalibration of standard confidence metrics. It consistently lifts AUROC from the mid-0.7s (typical of Logit Entropy) to 0.950.96 while roughly doubling the BSS, turning negative calibration scores into strongly positive ones. Crucially, Gnosis outperforms the learned MLP-Prob final-token probe by 718 AUROC points across benchmarks. This consistent gap confirms that correctness is property of the full generation trajectory, specifically the distributed hidden-state dynamics and attention patterns, rather than state localized to the final token. Efficiency vs. Scale: With only 5M parameters and negligible overhead from its fixed-budget projection, Gnosis matches or exceeds state-of-theart Skywork 8B Reward Models (1000 larger) and the proprietary Gemini 2.5 Pro. This is notable because Gnosis adds no independent world knowledge. Rather than fact-checking with massive parametric memory, it detects the signatures of hallucination and reasoning error in the backbones internal traces. On complex Math Reasoning, Gnosis surpasses both large judges. It also outperforms Gemini on MMLU-Pro, domain it was not explicitly trained on, suggesting that it learns transferable error patterns instead of task-specific cues. Additionally, Gnosis maintains near-constant 25ms latency and achieves roughly 37 and 99 speedups over the 8B reward model when judging answers of length 12k and 24k tokens, respectively  (Table 4)  . These results show that intrinsic self-verification can be both more scalable and far cheaper than external oversight. Figure 5 compares the predicted correctness score distributions of Gnosis and the Skywork 8B Reward Model. Gnosis shows sharp, bimodal peaks near 0 (incorrect) and 1 (correct), whereas Skywork produces broader, overlapping scores that often cluster around 0.50.6. This separation aligns with Gnosiss stronger calibration (BSS) and its tendency to assign more decisive probabilities. 4.3 Cross-Scale: Zero-Shot Reward Modeling We introduce Sibling Modeling, where we train Gnosis on small, cheap backbone and deploy it to judge larger family members without fine-tuning. Table 3 highlights striking outcome: head trained on 1.7B backbone transfers effectively to 4B and 8B siblings across all evaluated domains. On Math Reasoning, for instance, it achieves 0.93 AUROC, nearly matching the 0.96 achieved by self-trained head. Notably, this transferred 1.7B head still consistently outperforms the Skywork 8B Reward Model across all tested backbones, proving that our tiny zero-shot verifier is more reliable than massive external judge. This broad transferability implies that hallucination manifests as structural invariant across model scales, offering free lunch where single small head serves as supervisor for an entire model family. We observe that this transfer is most effective when models share similar generation style; while Gnosis robustly handles differences in size, it performs best when the models also align in their formatting (e.g., transferring between thinking models rather than thinking-to-Instruct). 4.4 Early Error Detection Because Gnosis processes internal traces into fixedsize descriptors, it can evaluate partial generations natively. Crucially, this capability is emergent: although Gnosis is trained exclusively on complete trajectories, it generalizes zero-shot to partial prefixes without any additional fine-tuning. Figure 3 illustrates that on both Math Reasoning and TriviaQA, Gnosis reaches near-peak accuracy and positive BSS after observing only 40% of the generation. In contrast, external reward models and single-token learnable probe typically require the full response to stabilize. This enables aggressive compute-aware control policies: generated chains-of-thought can be terminated immediately if the internal hallucination alarm triggers, preventing wasted compute on failing paths, or the system can automatically escalate the query to stronger model upon detecting that the current backbone is incapable of answering correctly."
        },
        {
            "title": "5 Ablations and Analysis",
            "content": "We highlight the key ablation insights that motivate Gnosis design, and defer comprehensive studies to Appendix A. Hidden vs. Attention Circuits. Gnosis fuses hidden-state and an attention circuit. Appendix Table 5 shows that both streams help, but attention is most useful for long-form reasoning: on TriviaQA, hidden-only dominates and fusion adds little, indicating hidden states carry most short factual reliability; on Math Reasoning and MMLU-Pro, attention-only is strong (even slightly better than hidden-only on MMLU-Pro), and fusion yields the best overall performance, suggesting complementary structural cues that emerge in longer reasoning. We visualize this behavior on Math-Reasoning in Figure 4; analogous feature-distribution plots for the other domains are provided in the appendix Figure 7. Taken together, these results support that hidden states provide broad, robust signal across domains, while attention routing contributes more strongly on reasoning-heavy tasks and less on short factual QA; combining both is the most reliable overall. Attention Map Extractor. We ablate how to summarize each downsampled attention map with lightweight CNN, predefined fixed statistics, and their hybrid. Appendix Table 6 shows that the three variants perform similarly on Math, while on TriviaQA the CNN-based variants are stronger than statistics alone, and on MMLU-Pro the CNN+Stats hybrid is the most consistent. Based on these, we adopt the CNN+Stats design in the final model. Additional Ablations. We further validate the design path behind Gnosis with targeted ablations across both streams. On the attention stream, Appendix B.2 examines how grid mixing, identity embeddings, pooling strategy, layer selection, and map downsampling affect performance (Appendix Tables 7 and 8). On the hidden stream, we isolate architectural value by including simple pooled-MLP baseline that naively pools final-layer hidden states before an MLP (Row in Table 9), alongside broader studies of the localglobal encoder design and sizing trade-offs (Appendix Tables 9 and 10)."
        },
        {
            "title": "References",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, and 1 others. 2025. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925. AI-MO Team. 2024. Aimo validation set - amc subset. https://huggingface.co/datasets/AI-MO/ aimo-validation-amc. Amos Azaria and Tom Mitchell. 2023. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734. Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. 2025. Matharena: Evaluating llms on uncontaminated math competitions. Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827. Siyi Chen, Aoran Zhang, and He He. 2024. Lets measure information step-by-step: Llm-based evaluation metrics with hidden states. arXiv preprint arXiv:2508.05469. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Hong Duan, Yuxin Yang, and Kam-Yiu Tam. 2024. Do llms know about hallucination? an empirical investigation of llms hidden states. arXiv preprint arXiv:2402.09733. Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, and Iryna Gurevych. 2023. survey of confidence estimation and calibration in large language models. arXiv preprint arXiv:2311.08298. Amirhosein Ghasemabadi, Keith Mills, Baochun Li, and Di Niu. 2025. Guided by gut: Efficient test-time scaling with reinforced intrinsic confidence. arXiv preprint arXiv:2505.20325. Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. 2025a. Efficient test-time scaling via self-calibration. arXiv preprint arXiv:2503.00031. Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, Bing Xu, Tiejun Zhao, and Wenpeng Lu. 2024. Self-evaluation of large language model based on glass-box features. arXiv preprint arXiv:2403.04222. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and 1 others. 2025b. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. triviaqa: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, arXiv:1705.03551. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, and 1 others. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221. Adam Tauman Kalai, Ofir Nachum, Santosh Vempala, and Edwin Zhang. 2025. Why language models hallucinate. arXiv preprint arXiv:2509.04664. Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, and 1 others. 2024. Evaluating llms at detecting errors in llm responses. arXiv preprint arXiv:2404.03602. Polina Kirichenko, Mark Ibrahim, Kamalika Chaudhuri, and Samuel Bell. 2025. Abstentionbench: Reasoning llms fail on unanswerable questions. arXiv preprint arXiv:2506.09038. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. 2019. Set transformer: framework for attention-based permutation-invariant neural networks. In International conference on machine learning, pages 3744 3753. PMLR. Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, and 1 others. 2025. Skyworkreward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint arXiv:2507.01352. Saumya Malik, Valentina Pyatkin, Sander Land, Jacob Morrison, Noah Smith, Hannaneh Hajishirzi, and Nathan Lambert. 2025. Rewardbench 2: Advancing reward model evaluation. arXiv preprint arXiv:2506.01937. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Yudi Pawitan and Chris Holmes. 2025. Confidence in the reasoning of large language models. Harvard Data Science Review, 7(1). Gaurang Sriramanan, Siddhant Bharti, Vinu Sankar Sadasivan, Shoumik Saha, Priyatham Kattakinda, and Soheil Feizi. 2024a. Llm-check: Investigating detection of hallucinations in large language models. In Advances in Neural Information Processing Systems (NeurIPS). Gaurang Sriramanan, Siddhant Bharti, Vinu Sankar Sadasivan, Shoumik Saha, Priyatham Kattakinda, and Soheil Feizi. 2024b. Llm-check: Investigating detection of hallucinations in large language models. Advances in Neural Information Processing Systems, 37:3418834216. Aoran Zhang, Yuhan Chen, Jiamin Pan, Chen Zhao, Ananya Panda, Jiawei Li, and He He. 2025b. Are the hidden states hiding something? testing the limits of llm factuality self-evaluation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL). Yifan Zhang and Team Math-AI. 2024. American invitational mathematics examination (aime) 2024. Yifan Zhang and Team Math-AI. 2025. American invitational mathematics examination (aime) 2025. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025c. The lessons of developing process reward models in mathematical reasoning. arXiv preprint arXiv:2501.07301. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize with human feedback. Advances in neural information processing systems, 33:3008 3021. Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, and Seong Joon Oh. 2024. Calibrating large language models using their generations only. arXiv preprint arXiv:2403.05973. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2023. Math-shepherd: Verify and reinforce llms stepby-step without human annotations. arXiv preprint arXiv:2312.08935. Yiming Wang, Pei Zhang, Baosong Yang, Derek F. Wong, and Rui Wang. 2025. Latent space chain-ofembedding enables output-free llm self-evaluation. arXiv preprint arXiv:2410.13640. ICLR 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, and 1 others. 2024a. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290. Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. 2024b. Helpsteer2preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, and 16 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. Preprint, arXiv:2503.14476. Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, and He He. 2025a. Reasoning models know when theyre right: Probing hidden states for self-verification. arXiv preprint arXiv:2504.05419. Figure 5: Predicted Correctness Score Distributions. Gnosis (top) displays sharp, bimodal separation between correct (blue) and wrong (red) answers. In contrast, the larger Skywork model (bottom) exhibits diffuse distributions with significant overlap, reflecting higher uncertainty. Table 5: Impact of Dual-Stream Architecture across Benchmarks. Comparison of single-stream variants against the Full Gnosis model. While hidden states provide strong signal, fusing them with the attention circuit consistently yields the best performance. Model Variant Input Signals Math Reasoning TriviaQA AUROC AUPR-c AUPR-e AUROC AUPR-c AUPR-e AUROC AUPR-c AUPR-e MMLU-Pro BOTH (GNOSIS) Fused ([zhid; zattn]) ATTENTION-ONLY Attention Maps (zattn) HIDDEN-ONLY Hidden States (zhid) 0.95 0.92 0.92 0. 0.93 0.92 0.94 0.90 0.91 0.87 0.78 0.87 0. 0.66 0.77 0.92 0.86 0.92 0.80 0.80 0.78 0. 0.90 0.89 0.56 0.56 0."
        },
        {
            "title": "A Architecture Overview",
            "content": "Figure 2 illustrates the detailed internal components of the Gnosis Mechanism. The architecture consists of two parallel streams that process the frozen backbones internal traces to extract reliability signals efficiently: A.1 Hidden Circuit Encoder. This stream processes the sequence of hidden states Hlast RSD. To avoid the added cost and memory of storing intermediate states for every token, we use only the final-layer hidden state. We show this choice remains strongly predictive. To handle variable lengths while maintaining fixed compute budget, the sequence is first projected and pooled into fixed number of tokens. It then passes through Local Temporal Feature Mixing stage (Phase 1) using multi-scale dilated convolutions and Squeeze-andExcitation (SE) blocks to capture local dependencies and reweight informative channels. Finally, Global Set Encoder (Phase 2) utilizes Set Attention Block (SAB) followed by Pooling by Multihead Attention (PMA) to aggregate the sequence into compact descriptor zhid RDHID. A.2 Attention Circuit Encoder. This stream processes the collection of attention maps {Aℓ,h}ℓ=1..L, h=1..H from frozen backbone. To make computation invariant to the original context length, we downsample each map to fixed grid Aℓ,h Rkk. We then summarize each downsampled map into compact per-head descriptor vℓ,h = Φ( Aℓ,h) Rdgrid. These per-head descriptors are arranged as layerhead grid and processed by an Axial Grid Processor to model inter-layer and inter-head dependencies, followed by PMA to obtain the final attention descriptor zattn RDATT. Per-Map Feature Extraction Variants. As described in Section 3.3, we implement Φ using two alternatives: (i) lightweight CNN that treats each attention map as an image and learns local-to-global patterns, and (ii) an interpretable statistics-based extractor that computes predefined structural descriptors. We compare these variants and their hybrid in Table 6. While CNN-only and Stats-only are individually competitive, the hybrid is the most consistent across domains. Unless otherwise stated, we therefore adopt CNN+Stats as the default Gnosis configuration. The Stats-only variant remains strong, more interpretable alternative. Interpretable Attention Statistics. The statistics branch summarizes how attention is distributed, how local or long-range it is, and where it tends to concentrate on the map, using small set of predefined descriptors. Concretely, for each downsampled map Aℓ,h we compute: Entropy. We compute map entropy together with row and column entropies. Map entropy reflects the overall dispersion of attention mass (focused vs. diffuse). Row and column entropies provide axis-specific views of this dispersion, indicating whether spread is driven primarily by query positions (rows) or key positions (columns). Together, these metrics offer direct, interpretable summary of attentional diffusion and stability. Spectral Texture. We compute spectral entropy and the relative energy from the 2D Fourier spectrum of Aℓ,h. These features summarize whether attention exhibits coherent, structured patterns (lowfrequency dominance) or becomes fragmented and noisy (elevated high-frequency energy and higher spectral entropy). Locality via Diagonal Structure. We measure diagonal and near-diagonal mass through the diagonal ratio and diagonal-band energies. This provides simple proxy for locality versus longer-range routing, which often correlates with coherent step-wise reasoning behavior. Center and Spread on the Map. We compute lightweight center-and-width measures to describe the average location of attention mass and how widely it is dispersed across the grid. The two descriptors are concatenated and passed through lightweight gated MLP to produce scalar correctness logit, which is converted to probability via sigmoid. Because both encoders operate on fixed-size summaries, Gnosis runs at effectively constant cost in sequence length and can be queried on partial chains of thought."
        },
        {
            "title": "B Comprehensive Ablations and Analysis",
            "content": "B.1 Hidden vs. Attention Circuits To quantify the distinct contributions of the internal representations, we trained single-stream variants of Gnosis and compared them to the full dual-stream model on Qwen3 1.7B. As reported in Table 5, the HIDDEN-ONLY model already provides strong correctness signal across benchmarks. On Math Reasoning, both single-stream variants reach 0.92 AUROC, while fusing zhid and zattn improves performance to 0.95 AUROC. On TriviaQA, the hidden stream remains stronger than attention alone, and fusion achieves the best overall performance. On MMLU-Pro, attention-only is slightly stronger than hidden-only, while fusion matches the best result. These results confirm that attention contributes complementary structural cues that help maximize performance when combined with hidden representations. B.2 Attention Circuit Encoder Ablations To determine the optimal architecture for the attention circuit, we systematically investigated three key design components: the input feature representation for each attention map, the grid topology for mixing information across layers and heads, and the final aggregation strategy. Tables 6, 7, and 8 detail this investigation. We select the configuration highlighted in Row of Tables 7 and 8 as it achieves the best balance of accuracy and parameter efficiency. Feature Input Representation. We first assessed how best to encode individual attention maps. Table 6 compares lightweight learned CNN, predefined statistics, and their combination across benchmarks. The predefined statistics remain competitive with the CNN (e.g., identical 0.92 AUROC on Math Reasoning), Table 6: Impact of Per-Map Feature Extractor Choices across Benchmarks. Comparison of learnable CNN, predefined statistics, and their combination for the attention per-map extractor Φ. Across benchmarks, the CNN and statistics variants are competitive, while their hybrid is the most consistent overall; we adopt the CNN+Stats design in our final model. Model Variant CNN+STATS (FINAL) CNN-ONLY STATS-ONLY Math Reasoning TriviaQA AUROC AUPR-c AUPR-e AUROC AUPR-c AUPR-e AUROC AUPR-c AUPR-e MMLU-Pro 0.92 0.92 0.92 0.93 0.93 0.93 0. 0.88 0.90 0.78 0.79 0.75 0.65 0.66 0.62 0. 0.86 0.83 0.80 0.79 0.76 0.90 0.90 0.88 0. 0.56 0.53 Table 7: Attention Circuit: Components & Topology. We ablate the attention-circuit design by varying grid mixing, identity embeddings, and aggregation. Row is our default configuration. Axial Conv performs lightweight row/column mixing over the (Layer, Head) grid, offering cheaper alternative to full global self-attention. Table 8: Attention Circuit: Hyperparameters. Impact of layer selection stride and initial map downsampling size (kgrid). (Proposed: Select 1 every 5 layers; kgrid = 256). Note: These choices mainly affect inference speed/memory rather than parameter count. Higher AUROC is better. ID Configuration / Change Gnosis (Axial + PMA) #Params AUROC 1.4M 0.92 ID Variation Gnosis Grid Topology & Identity Remove Axial Conv (Linear Proj Only) Replace Axial Conv w/ Global Transformer Remove Layer/Head Embeddings 1.1M 4.5M 1.4M 0.84 0.92 0.90 Layer Selection Strategy E1 All Layers (Every Map) E2 First & Last Layers Only Aggregation Strategy Replace PMA w/ Mean Pool (Axial Mean) 0.9M 0.85 Downsampling Size (kgrid) F1 Small Grid (64) F2 Large Grid (512) AUROC 0.92 0.91 0.64 0.68 0.92 while providing more interpretable per-map representation. Combining CNN and statistics yields comparable performance overall and modest gains on MMLU-Pro (0.80 AUROC). Grid Topology and Layer/Head Identity. We next evaluated how to process the collection of extracted map features. As shown in Table 7, removing the grid mixing entirely (Row B) reduces performance, indicating that individual attention heads are not independent predictors; their interactions matter. Replacing our lightweight Axial Convolutions with heavy Global Transformer (Row C) increases parameters four-fold without improving AUROC, validating the efficiency of the axial design. Furthermore, removing the learned layer and head embeddings (Row D) degrades performance, confirming that the model relies on knowing where specific activation pattern occurred within the LLMs depth and breadth. Aggregation Strategy. Finally, we analyzed how to summarize the grid into fixed vector. Replacing our query-based Pooling by Multihead Attention (PMA) with simple mean pooling (Row in Table 7) causes sharp drop in accuracy. This suggests that Gnosis benefits from learning specific \"reliability prototypes\" (via PMA seeds) rather than uniformly averaging all attention circuits, likely because only small subset of heads carry high-fidelity correctness signals. B.3 Hidden-State Circuit Encoder Ablations To identify the optimal architecture for the hidden-state circuit, we conducted comprehensive ablation study investigating feature dimensionality, local temporal processing, and global aggregation strategies. Table 9 & Table 10 details this investigation. The final design (Row A) provides the best trade-off between reliability estimation (AUROC) and computational efficiency. Dimensionality and Sizing. We first investigated the information bottleneck size (dtok, khid). Comparing Rows and against our proposed model (Row A) reveals clear performance plateau at size 192. Reducing the size to 96 causes sharp performance drop (0.04 AUROC), likely due to information loss during the initial pooling. Conversely, scaling to 384 triples the parameter count without any accuracy Table 9: Hidden-state Circuit: Components & Baselines. We ablate the hidden-state encoder by isolating the roles of local temporal mixing (Phase 1) and global set aggregation (Phase 2). Row is our default configuration. Removing Phase 1 (BD) or weakening global aggregation (EF) consistently reduces AUROC, and simple pooled MLP baseline (G) performs substantially worse. Table 10: Hidden-state Circuit: Hyperparameters. Impact of varying Feature Dimension (dtok), Pooled Sequence Length (khid), and SAB Depth. (Gnosis used settings: dtok = 192, khid = 192, SAB = 3) ID Configuration / Change #Params AUROC Gnosis 2.6M 0.92 Phase 1: Local Processing Remove Phase 1 (Raw Seq Set Enc) Remove Gating & SE (Sum only) Remove Multi-scale (Dilations 1) Phase 2: Global Aggregation Remove SAB (CNN PMA only) Replace PMA w/ Mean Pool Architectural Baselines Pooled MLP (GlobalPool MLP) 2.4M 2.6M 2.4M 1.3M 2.1M 0.89 0.91 0.90 -0.03 -0.01 -0. 0.85 0.89 -0.07 -0.03 0.7M 0.82 -0.1 ID Variation #Params AUROC Gnosis 2.6M 0.92 Feature Dimension (dtok) H1 Small Width (96) H2 Large Width (384) Pooled Seq Length (khid) I1 Short Seq (96) I2 Long Seq (384) SAB Layers (Default: 3) J1 Fewer Layers (1) J2 More Layers (5) 0.8M 9.3M 2.6M 2.6M 1.7M 3.5M 0.88 0.92 0.89 0. 0.89 0.93 gain. We therefore fix the dimensions to 192 for efficiency. Local Temporal Encoder (Phase 1). We investigated whether explicit local feature extraction is necessary before global processing. Row shows that feeding raw pooled sequences directly to the set encoder reduces performance by 0.04 AUROC, confirming that Phase 1 acts as critical denoising stage. Inside Phase 1, we found that architectural complexity matters: removing the Squeeze-and-Excite gating (Row C) or replacing the multi-scale dilated convolutions with standard convolution (Row D) both degrade performance. This suggests the model relies on capturing multi-scale temporal signals (via dilation) and dynamic feature reweighting (via SE/Gating). Global Set Encoder (Phase 2). Finally, we analyzed the global aggregation stage. We found that simply averaging the features (Row F) or removing the global self-attention refinement (Row E) consistently hurts performance. This validates the use of the SAB+PMA stack to capture global context and learn specific reliability prototypes. Notably, our hybrid design significantly outperforms simple pooled MLP baseline (Row G)."
        },
        {
            "title": "C Additional Experimental Setup",
            "content": "C.1 Benchmarks (extended). We evaluate on three disjoint domains. For math reasoning, we use combined test set of AMC12 2022, AMC12 2023(AI-MO Team, 2024), AIME 2024(Zhang and Math-AI, 2024), AIME 2025(Zhang and Math-AI, 2025), and HMMT February 2025(Balunovic et al., 2025). These competition-style problems span wide range of difficulty and require multi-step symbolic and numeric reasoning. For open-domain QA, we use an 18k-question held-out trivia subset drawn from the same distribution as our training corpus but with no overlapping items. This benchmark emphasizes short factoid answers and directly evaluates hallucination detection and knowledge grounding. For academic knowledge reasoning, we use MMLU-Pro(Wang et al., 2024a), an out-of-distribution evaluation spanning 14 diverse domains (e.g., math, physics, law, psychology) that combines domain knowledge with multi-step reasoning, providing broad test of generalization beyond the training mix. C.2 Metrics (extended). We frame correctness prediction as binary classification and report both ranking and calibration quality. AUROC measures how well method ranks correct completions above incorrect ones (0.5 = chance, 1.0 = perfect). AUPR is reported with two complementary positive classes: AUPR-c treats correct completions as positive and summarizes how well method recovers correct answers with high precision across Table 11: Backbone outcome rates (%) across evaluation domains. For each frozen backbone, we report the fraction of instances that are correct (Accuracy), incorrect (Hallucination), or no-answer (I dont know/refusal/empty). No-answer cases are shown for completeness but are filtered out from our correctnessprediction evaluation  (Table 1)  . Backbone Domain Backbone Accuracy Backbone Hallucination Backbone dont know Qwen3 1.7B-Hybrid Qwen3 1.7B-Hybrid Qwen3 1.7B-Hybrid Math-Reasoning TriviaQA MMLU-Pro Qwen3 4B-Thinking-2507 Math-Reasoning Qwen3 4B-Thinking-2507 TriviaQA Qwen3 4B-Thinking-2507 MMLU-Pro Qwen3 4B-Instruct-2507 Math-Reasoning Qwen3 4B-Instruct-2507 Qwen3 4B-Instruct-2507 MMLU-Pro TriviaQA Qwen3 8B-Hybrid Qwen3 8B-Hybrid Qwen3 8B-Hybrid OpenAI gpt-oss-20B OpenAI gpt-oss-20B OpenAI gpt-oss-20B Math-Reasoning TriviaQA MMLU-Pro Math-Reasoning TriviaQA MMLU-Pro 44.87% 33.54% 66.09% 63.27% 53.73% 72.45% 57.51% 56.89% 71.69% 49.42% 62.22% 71.93% 52.53% 63.70% 68.74% 47.76% 55.67% 31.53% 25.31% 45.27% 24.75% 29.26% 42.98% 27.46% 14.16% 36.05% 22.29% 42.63% 27.67% 30.83% 7.37% 10.79% 2.39% 11.42% 0.99% 2.79% 13.22% 0.13% 0.85% 36.42% 1.74% 5.77% 4.84% 8.62% 0.44% recall; AUPR-e treats incorrect completions as positive and summarizes how well method detects errors/hallucinations, which is often the more safety-relevant viewpoint under class imbalance. For probability quality, we report Brier Skill Score (BSS), where BSS > 0 indicates improvement over prevalence baseline, and Expected Calibration Error (ECE), where lower values indicate better alignment between predicted correctness probabilities and empirical accuracy."
        },
        {
            "title": "D Backbone Outcome Statistics",
            "content": "To contextualize the results in Table 1, we report the raw outcome breakdown of each frozen backbone on the three evaluation domains. While Table 1 focuses on the quality of correctness prediction (AUROC/AUPR/BSS/ECE) for different judges and internal methods, this table provides the underlying base behavior of each backbone: how often it is correct, how often it hallucinates, and how often it produces no valid final answer. Importantly, non-response cases are reported here for completeness but are filtered out and not used in our correctness-prediction evaluation. Thus, the metrics in Table 1 are computed over the subset of generations with valid, parsable answer. This table therefore serves two purposes: (i) it clarifies the intrinsic difficulty and failure modes of each backbone across domains, and (ii) it provides the context needed to interpret domain-dependent shifts in AUPR and calibration metrics reported in Table 1, which are more sensitive to the underlying prevalence of correct vs. incorrect answered generations. Gemini As judge. For the LLM-as-judge baseline, we use Gemini 2.5 pro as generative reward model. Given question and model-generated answer, the judge produces brief correctness analysis and then emits scalar score in [0, 1] at the end of the response. We parse the score from the required <score> tags. Evaluating Gemini via the API on the three benchmarks reported in Table 1 cost approximately $500 in total. Figure 6 shows the prompt for using Gemini as the judge. Figure 6: Gemini judge system prompt used for the LLM-as-judge baseline. SYSTEM_PROMPT You are an objective, expert evaluator. Your task is to review Question and proposed Answer. Determine if the Answer is correct, and accurate based on the Question. Assign correctness score between 0.0 (completely wrong) and 1.0 (perfectly correct). IMPORTANT: <score>0.95</score> or <score>0.0</score>. <score> inside Output final score tags. Example: the Figure 7: 2D PCA scatter plots of features learned by Gnosis across three domains. Rows: Math-Reasoning, TriviaQA, and MMLU-Pro. Columns: hidden-state features (zhid), attention features (zattn), and their merged representation. We show PCA scatter plots with KDE contours and marginal densities for wrong (red) and correct (blue) answers. Across domains, the merged features provide the clearest overall class separation, illustrating the complementarity of hidden and attention signals."
        }
    ],
    "affiliations": [
        "University of Alberta, Canada"
    ]
}