{
    "paper_title": "Alchemist: Turning Public Text-to-Image Data into Generative Gold",
    "authors": [
        "Valerii Startsev",
        "Alexander Ustyuzhanin",
        "Alexey Kirillov",
        "Dmitry Baranchuk",
        "Sergey Kastryulin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pre-training equips text-to-image (T2I) models with broad world knowledge, but this alone is often insufficient to achieve high aesthetic quality and alignment. Consequently, supervised fine-tuning (SFT) is crucial for further refinement. However, its effectiveness highly depends on the quality of the fine-tuning dataset. Existing public SFT datasets frequently target narrow domains (e.g., anime or specific art styles), and the creation of high-quality, general-purpose SFT datasets remains a significant challenge. Current curation methods are often costly and struggle to identify truly impactful samples. This challenge is further complicated by the scarcity of public general-purpose datasets, as leading models often rely on large, proprietary, and poorly documented internal data, hindering broader research progress. This paper introduces a novel methodology for creating general-purpose SFT datasets by leveraging a pre-trained generative model as an estimator of high-impact training samples. We apply this methodology to construct and release Alchemist, a compact (3,350 samples) yet highly effective SFT dataset. Experiments demonstrate that Alchemist substantially improves the generative quality of five public T2I models while preserving diversity and style. Additionally, we release the fine-tuned models' weights to the public."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 7 9 2 9 1 . 5 0 5 2 : r Alchemist: Turning Public Text-to-Image Data into Generative Gold Valerii Startsev Yandex Research, HSE Alexander Ustyuzhanin Yandex Alexey Kirillov Yandex, MSU Dmitry Baranchuk Yandex Research Sergey Kastryulin Yandex Research https://huggingface.co/datasets/yandex/alchemist Figure 1: Images generated by Stable Diffusion 3.5 Large fine-tuned on Alchemist, demonstrating enhanced aesthetic quality and complexity while maintaining prompt adherence."
        },
        {
            "title": "Abstract",
            "content": "Pre-training equips text-to-image (T2I) models with broad world knowledge, but this alone is often insufficient to achieve high aesthetic quality and alignment. Consequently, supervised fine-tuning (SFT) is crucial for further refinement. However, its effectiveness highly depends on the quality of the fine-tuning dataset. Existing public SFT datasets frequently target narrow domains (e.g., anime or specific art styles), and the creation of high-quality, general-purpose SFT datasets remains significant challenge. Current curation methods are often costly and struggle to identify truly impactful samples. This challenge is further complicated by the scarcity of public general-purpose datasets, as leading models often rely on large, proprietary, and poorly documented internal data, hindering broader research progress. This paper introduces novel methodology for creating generalpurpose SFT datasets by leveraging pre-trained generative model as an estimator of high-impact training samples. We apply this methodology to construct and release Alchemist, compact (3,350 samples) yet highly effective SFT dataset. Experiments demonstrate that Alchemist substantially improves the generative quality of five public T2I models while preserving diversity and style. Additionally, we release the fine-tuned models weights to the public. Equal contribution. Preprint."
        },
        {
            "title": "Introduction",
            "content": "Generative text-to-image (T2I) models, such as DALL-E 3 [3], Imagen 3 [2], and Stable Diffusion 3 [9], have demonstrated remarkable advancements in synthesizing high-fidelity and diverse images from textual descriptions. These models, typically pre-trained on vast internet-scale datasets, are applied in creative industries, for content generation, and in scientific visualizations. Despite their capabilities, the continuous pursuit of enhanced generative quality and better alignment with user intent remains central research focus. Supervised Fine-Tuning (SFT) has become vital technique for adapting these pre-trained foundation models, whether to specialize them for particular domains or aesthetics, or to broadly elevate their general generative performance. However, the success of SFT is critically dependent on the quality and composition of the fine-tuning dataset. Current practices for SFT dataset curation often rely on extensive manual human selection. This process is not only costly and challenging to scale but can also be surprisingly ineffective. The specific characteristics of text-image pairs that render sample \"good\" for SFT that is, likely to maximally boost general model quality are frequently subtle, not obvious, and difficult for humans to consistently verbalize or identify. Alternative approaches, such as filtering large web datasets with simple heuristics or employing synthetic data generation, have their own limitations in efficiently targeting high-impact samples or ensuring quality and diversity without introducing new biases. These methodological challenges are compounded by significant scarcity of publicly available, general-purpose SFT datasets explicitly designed to broadly enhance T2I models. While numerous domain-specific fine-tuning datasets exist, they serve niche applications rather than general quality improvement. Furthermore, several recent state-of-the-art models (e.g., Emu [7], PixArt-α [6], Kolors [38], SANA [43], YaART [18]) report using internal datasets for their SFT stages. These datasets, however, remain closed-source and are often described with insufficient detail in publications, severely limiting the research communitys ability to replicate findings, understand their construction principles, or develop comparable open resources. This lack of accessible, well-characterized, general-purpose SFT datasets impedes broader progress in systematically improving T2I models. To address these challenges, we propose novel approach that leverages the intrinsic understanding of pre-trained generative model to more effectively guide the SFT dataset creation process. Our core idea is that pre-trained generative model can itself serve as an estimator of data quality, pinpointing samples most likely to contribute positively to the fine-tuning objective and maximize generative improvements in downstream models. To demonstrate the practical utility of this methodology, we created the Alchemist dataset and subsequently used it to fine-tune five publicly available text-toimage models, the improved weights of which we release as part of our contributions. This work aims to provide the first open general-purpose alternative to proprietary fine-tuning pipelines, enabling reproducible research and commercial applications. We present the following contributions: principled methodology for curating high-quality, general-purpose SFT datasets by leveraging pre-trained generative model to identify samples that maximize post-SFT model improvement. Alchemist, compact (3,350 samples) yet highly effective SFT dataset constructed via our methodology, significantly enhances text-to-image generation quality while maintaining output diversity and style. Open-sourced, fine-tuned weights for five publicly available text-to-image models, demonstrating performance gains over their baselines after SFT with Alchemist. The remainder of this paper details our methodology for dataset creation, presents experimental results showcasing its effectiveness, and discusses the outcomes and limitations of our research."
        },
        {
            "title": "2 Related Work",
            "content": "Supervised Fine-Tuning of Text-to-Image Models. Early text-to-image models like DALL-E [29] and Latent Diffusion Models (LDMs) [30] were primarily pre-trained on vast, uncurated web-scale datasets (e.g., LAION-5B [35]), focusing on general generative capabilities without specific SFT 2 stages. significant advancement came with Emu [7], which demonstrated that an SFT stage on smaller, high-quality, curated dataset substantially improved instruction following and aesthetic quality. Subsequently, SFT or similar refinement stages became standard in state-of-the-art models. For instance, PixArt-α [6] enhances outputs through training on data with higher aesthetic quality, boosting the training efficiency. Later works [38, 43, 18] also employ multi-stage training including fine-tuning on aesthetically filtered data. This trend highlights crucial role of SFT in achieving high-quality and controllable image generation. Supervised Fine-Tuning Datasets for Text-to-Image Models. Publicly available, general-purpose SFT datasets for text-to-image models remain limited. LAION-Aesthetics [36], derived from LAION5B [35] by filtering for predicted aesthetic scores, is widely used. However, its quality is often considered inferior compared to closed source datasets. While more recent efforts, such as LAIONAesthetics V2 [37], aim to improve upon this, meticulously verified, general-purpose public SFT dataset is largely absent. In contrast, domain-specific SFT datasets are more common, such as the Danbooru dataset [1] for anime-style generation and WikiArt dataset [41] for classical and modern art generation. These datasets achieve strong performance within their specific domains but typically at the cost of the models broader generative abilities, causing it to overfit to the narrow domain of the SFT data. The scarcity of high-quality, general-purpose public SFT datasets motivates our work. Quality Assessment of Text-to-Image Models. Evaluating text-to-image generation quality is complex. Automated metrics like Fréchet Inception Distance (FID) [14] and Inception Score (IS) [34], while common, often correlate poorly with human perception [4, 33]. Consequently, more comprehensive evaluations rely on human assessment. Studies for models like Imagen [32] and Parti [45], for example, involved human raters evaluating photorealism, text-image alignment, absence of artifacts, and compositionality. Standardized prompt sets such as DrawBench [32] and T2ICompBench [16] facilitate structured comparison. In this work we provide some automated metrics while building main conclusions based on carefully designed multi-aspect human-based evaluations."
        },
        {
            "title": "3 Dataset Formation",
            "content": "Our goal is to create general-purpose supervised fine-tuning dataset capable of significantly enhancing the generative quality of pre-trained text-to-image (T2I) models while preserving their diversity in content, composition, and style. To achieve this, we introduce multi-stage filtering pipeline designed to create small set of exceptionally high-quality samples from vast pool of uncurated internet data. core principle of our methodology involves leveraging pre-trained diffusion model as sophisticated estimator in the final filtering stage to identify text-image pairs with the highest potential to boost downstream SFT performance. This section details our pipeline, the effectiveness of which is demonstrated in Section 4. Overview. The dataset construction process starts from vast, diverse pool of O(10 billion) images aggregated from web-scraped sources. Some dataset curation pipelines for image-text models discussed in literature [11] impose text-based filtering at the initial stages, discarding samples with poorly structured, noisy or semantically misaligned captions. While this approach mitigates lowquality training pairs, we argue that it is increasingly restrictive given recent advances in multi-modal captioning models. Early text filtering eliminates potentially valuable visual content that could be re-captioned with synthetic texts. Instead, we compose our data curation pipeline as purely imagebased. The relatively high-quality set of data that passes first filtering stages is further filtered using diffusion-based sample quality estimator and then captioned with Vision-Language Model (VLM) to obtain the final SFT dataset. Figure 2 provides an overview of the dataset formation pipeline. Stage 1: Foundational Safety and Resolution Filtering. The first filtering stage addresses basic image requirements. We discarded images identified as containing Not-Safe-For-Work (NSFW) content through an automated classifier. Subsequently, we applied resolution filter, retaining only those images with an area exceeding 1024 1024 px. This step ensures that the candidate pool consists of sufficiently high-resolution and safe visual content for subsequent processing. Stage 2: Coarse-Grained Quality Assessment and Filtering. Following the foundational filters, we employed suite of lightweight binary classifiers for rapid, coarse-grained quality assessment. 3 Figure 2: Overview of the multi-stage image filtering pipeline. Beginning with web-scale collection of raw data, the pipeline sequentially filters images to isolate high-quality subset optimally suited for supervised fine-tuning of text-to-image models. These classifiers were trained to identify and remove images exhibiting severe degradation, watermarks, noticeable compression artifacts, significant motion blur, or low aesthetic appeal. The training data for these classifiers was derived from public Image Quality Assessment (IQA) [15, 17] and Image Aesthetics Assessment (IAA) [12, 24] datasets. Classification thresholds were manually calibrated to aggressively remove the worst-quality examples. The first two stages significantly reduced the dataset size, yielding approximately one billion images for further processing. Stage 3: Deduplication and Fine-Grained Quality Refinement. With more manageable dataset size, we applied more computationally intensive methods. First, to enhance visual diversity, we performed image deduplication by computing SIFT-like local features [22], clustering images by similarity, and retaining only one representative (highest preliminary quality score) per cluster. Second, for fine-grained perceptual quality assessment, we utilized the TOPIQ no-reference IQA model [5]. After empirical studies to balance quality with domain diversity (avoiding bias towards narrow domains like architecture and interiors), we set the TOPIQ threshold to > 0.71. This step isolated images with minimal distortions and artifacts while preserving broad thematic coverage, resulting in approximately 300 million high-quality images. Perceptual and Compositional Refinement. The objective here is to find subset of images with rare combination of visual characteristics such as high aesthetic quality, optimal color balance, and substantial image complexity that are hypothesized to maximize SFT quality. Existing IQA and IAA models often struggle to holistically capture this specific blend of attributes crucial for SFT. Our hypothesis is that pre-trained diffusion model, through its learned representations, inherently encodes these desired characteristics, particularly within its cross-attention mechanisms which mediate text-image alignment during generation. To leverage this, we developed scoring function based on cross-attention activations. We utilize long, multi-keyword prompt designed to evoke the target visual qualities (e.g., including terms like high quality, artistic, aesthetic, complex). For each image, we extract cross-attention activation norms corresponding to these keywords. To identify the most discriminative activations, we manually scored calibration set of 1,000 images based on the aforementioned SFT-desirable criteria, forming higher-quality and lower-quality groups. We then identified the top-K activation indices that best separated these two groups. The final score for any given image is an aggregation (summation) of its activation norms at these top-K indices (details in Algorithm 1). detailed discussion of methodological details, including prompt engineering and choice of inference timestep t, is provided in Appendix B. Using this diffusion-based scoring function, we evaluated all 300 million images from Stage 3 and selected the top-n samples. The SFT dataset size (n) is critical hyperparameter. Through ablation studies (detailed in Section 4.4), we determined that = 3,350 provides best model quality improvements with no observable loss of generative diversity. Final Re-captioning and the Alchemist Dataset. The 3,350 images curated by our pipeline, though visually exceptional, retained their original, often noisy, web captions. Effective supervised fine-tuning (SFT) necessitates appropriate textual guidance. Our preliminary studies highlighted the importance of caption style, finding that captions resembling moderately descriptive user-like prompts 4 Algorithm 1: Diffusion-based Quality Estimator Input: XHQ, XLQ: Two groups of train images of higher and lower visual quality X: Test images, = ϵθ: Pretrained text-to-image generative model P: Predefined prompt with tokens {w1, ..., wM } L: Number of cross-attention layers K: Number of top discriminative features t: Timestep for activation extraction Output: Quality scores RN 1. Extract activations: for each image Rhw in XHQ XLQ do l,m Rhlwl } l=1...L Save cross-attn maps {A(x) Compute spatial activation norms: m=1...M during noise prediction via ϵθ(x, P, t) (x) l,m = A(x) l,m,:,:2 {1, ..., L}, {1, ..., } end 2. Find (layer, token) pairs with most discriminative features: for each (l, m) pair do sl,m 0 for each (xHQ XHQ, xLQ XLQ) pair do Compute separation score: sl,m += I[N (xHQ) l,m > (xLQ) l,m ] end end Select top-K (l, m) pairs with highest sl,m: = {(l1, m1), ..., (lK, mK)} 3. Compute scores: for each image do fx = (cid:80) (l,m)K (x) l,m end return Quality scores rather than exhaustively detailed ones achieve better SFT results. Therefore, we re-captioned the entire set using proprietary image captioning model tuned to produce such user-centric descriptions. This re-captioning ensured consistent and relevant textual pairings. The resulting Alchemist dataset consists of these 3,350 refined image-text pairs, used for subsequent analysis and SFT."
        },
        {
            "title": "4 Experiments\nWe empirically evaluate the effectiveness of Alchemist as an SFT dataset for open-source Stable\nDiffusion (SD) models. Our goal is to verify whether a compact, highly curated dataset like Alchemist\ncan significantly boost image generation quality and outperform LAION-Aesthetics v2 [37] as a\nstandard publicly available SFT alternative. Below we discuss experimental setup and present results\nof fine-tuning with Alchemist in terms of human-perceived generation quality and common automated\nmetrics.\n4.1 Experimental Setup",
            "content": "Models and Datasets. We evaluate our proposed methodology across five widely-used pre-trained text-to-image models based on Stable Diffusion: SD1.52, SD2.13, SDXL1.04, SD3.5 Medium5, and SD3.5 Large6 [30, 28, 9]. For each base model, we utilize the official checkpoints and Diffusers-based [40] publicly available fine-tuning code to establish three comparison points: Baseline: The original official model weights; 2https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5 3https://huggingface.co/stabilityai/stable-diffusion-2-1 4https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0 5https://huggingface.co/stabilityai/stable-diffusion-3.5-medium 6https://huggingface.co/stabilityai/stable-diffusion-3.5-large 5 Alchemist-tuned: The baseline model fine-tuned on our proposed Alchemist dataset (comprising 3,350 samples); LAION-tuned: The baseline model fine-tuned on size-matched subset (3,350 samples) drawn from the LAION-Aesthetics v2 dataset [37], specifically selecting samples with aesthetic scores >= 6.5. This serves as control to assess the effectiveness of Alchemist compared to standard, high-aesthetics filtered dataset of equivalent size. We additionally ablate dataset size for LAION in Appendix C.2. Fine-Tuning and Hyperparameter Selection. We employed full fine-tuning approach, updating all parameters of the base models. To identify optimal settings for each (model, dataset) combination, we conducted grid search over key hyperparameters, including learning rate, EMA momentum, and the number of training steps. The specific search ranges and the final selected hyperparameters for each configuration are detailed in Appendix D.1. Checkpoint selection and early stopping decisions during this tuning process were guided by performance on dedicated validation set. This validation set consisted of 128 prompts selected from the PartiPrompts benchmark [45], following the methodology employed in SD3 [9]. Test Set for Final Evaluation. The final performance assessment of the best checkpoints selected via the validation process was conducted on separate, unseen test set. This test set comprised 500 distinct prompts also drawn from PartiPrompts [45], ensuring no overlap with the prompts used during validation or hyperparameter tuning. 4.2 Evaluation Protocol Human Side-by-Side Evaluation Our primary method for evaluating model performance relies on human perception via side-by-side (SbS) comparisons. For each comparison pair (e.g., Alchemisttuned vs. Baseline), we generated images using prompts from the validation or test sets; detailed parameters are provided in Appendix E). Three expert annotators were independently presented with the generated images. Annotators evaluated the pairs based on four criteria: Image-Text Relevance: Accuracy of the image content relative to the text prompt; Aesthetic Quality: Overall visual appeal, including composition and style; Image Complexity: Richness of detail and content within the scene; Fidelity: Presence and severity of defects, artifacts, distortions, or undesirable elements. For each criterion, annotators selected the preferred image, with the option of indicating tie. The final outcome for given prompt and criterion was determined by majority vote among the three annotators. We assess the statistical significance of the aggregate win rates using two-sided binomial test (p < 0.05). Details regarding the SbS interface and instructions are provided in Appendix F. Automated Metrics To complement human judgments, we report established automated metrics. These include FD-DINOv2, which calculates the Fréchet Distance [14] using DINOv2 [26] features, and CLIP Score [13], based on ViT-L/14 [8] image-text similarity. Additionally, we employ learned human preference predictors: ImageReward (IR) [44] and HPS-v2 [42]. All automated metrics were computed on the standard MJHQ-30K dataset [20]. 4.3 Results Human Evaluation Results. The results from human side-by-side (SbS) evaluations demonstrate how fine-tuning impacts the four specified assessment criteria. Regarding Image-Text Relevance, fine-tuning with Alchemist did not yield statistically significant differences compared to either the baseline or the LAION-tuned models across most tested architectures (p > 0.05). This indicates that the improvements observed in other aspects do not compromise prompt fidelity. Conversely, Alchemist fine-tuning demonstrated substantial and statistically significant improvements in both Aesthetic Quality and Image Complexity. Compared to the respective baseline models, Alchemist-tuned versions achieved human preference win rates up to 20% higher. Furthermore, 6 Model SD1.5-Alchemist vs baseline vs LAION-tuned SD2.1-Alchemist vs baseline vs LAION-tuned SDXL-Alchemist vs baseline vs LAION-tuned SD3.5M-Alchemist vs baseline vs LAION-tuned SD3.5L-Alchemist vs baseline vs LAION-tuned Side-by-Side Win Rate Automatic Metrics () Rel. Aes. Comp. Fidel. 0.53 0.47 0.57 0.49 0.52 0. 0.51 0.48 0.49 0.47 0.64 0.60 0.69 0.56 0.61 0.58 0.57 0. 0.62 0.57 0.78 0.73 0.81 0.72 0.78 0.78 0.67 0.73 0.72 0. 0.47 0.45 0.56 0.52 0.51 0.57 0.50 0.49 0.41 0.55 FDDINOv2 CLIP 0.277 0.279 0. 129.8 131.5 112.1 95.6 129.3 112.4 97.4 73.4 108.9 76.2 81.4 87.9 80.9 91.4 91.1 0.281 0.276 0. 0.286 0.293 0.294 0.286 0.287 0.286 0.287 0.286 0.297 IR HPS-v2 0.38 0.02 0.32 0.62 0.18 0. 0.76 0.71 0.81 1.07 0.97 0.87 1.12 1.01 1.10 0.270 0.243 0.260 0.282 0.253 0.278 0.292 0.283 0. 0.295 0.292 0.274 0.299 0.298 0.294 Table 1: Comparison of Alchemist-tuned models, baselines, and LAION-Aesthetics-tuned models. The table reports human win rates (by aspect) w.r.t. Alchemist-tuned models and automated metrics values for each model variant. Green indicates statistically significant improvement (p < 0.05), gray no statistically significant change, and red statistically significant decline. For automated metrics bold means the best value among three model variants. Alchemist consistently outperformed the size-matched LAION-Aesthetics-tuned variants on these two criteria, with win rate advantages ranging from +12% to +20% across the different base models. In terms of Fidelity, the results were mixed. While many models showed no significant change, fine-tuning with Alchemist led to marginal but statistically significant decrease in perceived fidelity for certain architectures (average win rate decrease of approximately 5% against baseline in those cases). We hypothesize this may represent tradeoff associated with generating more complex and detailed images, point further discussed in Sections 5 and 6. Qualitative Analysis To visually complement these quantitative assessments and human judgments, Figure 3 presents qualitative examples of images generated by several models, showcasing outputs from their baseline versions alongside those after fine-tuning with Alchemist. These visual comparisons directly illustrate the enhancements in aesthetic appeal, detail, and overall image complexity reported above. The examples also suggest that fine-tuning with Alchemist does not lead to noticeable decline in the diversity of content or stylistic range generated by the models. more extensive collection of qualitative results, including additional model comparisons and prompt examples, is provided in Appendix H. Automated Metric Results. These findings from human evaluations and qualitative analysis are further confirmed by automated metrics. Improvements in FD-DINOv2, CLIP Score, and the learned preference scores (ImageReward, HPS-v2) were observed for most models after fine-tuning with Alchemist, particularly when compared to the untuned baselines (see Table 1 for detailed results). The comparison against LAION-Aesthitics-tuned models on these metrics also generally favored the Alchemist variants, supporting the conclusions drawn from human preferences. 4.4 Dataset Size Ablation To assess the impact of strict filtering, we created two larger Alchemist variants (approx. 7k and 19k samples) by relaxing the selection threshold of our diffusion-based quality estimator. These datasets inherently contained samples with lower diffusion-guided quality scores than the original 3,350-sample Alchemist. We then fine-tuned all five base models on these 7k and 19k variants. As summarized in Table 4, fine-tuning on both larger datasets yielded consistently inferior performance across all models compared to the compact 3,350-sample Alchemist. An additional, dedicated hyperparameter sweep for the 7k and 19k datasets (Appendix C) confirmed this finding, as no config7 (a) Baseline. (b) Alchemist-tuned. Figure 3: Examples of images generated by five Stable Diffusion models for the prompt Mars rises on the horizon. before and after tuning on Alchemist. uration achieved quality comparable to that of the original Alchemist. These results underscore that exceptional sample quality curated by strict, diffusion-guided filtering is more critical for maximizing SFT efficacy than sheer dataset volume."
        },
        {
            "title": "5 Discussion",
            "content": "Fine-tuning with Alchemist substantially enhances aesthetic quality and image complexity across diverse Stable Diffusion models, highlighting the power of targeted SFT with compact, high-impact datasets. Our findings, however, also prompt further discussion. We observe that Alchemist fine-tuning yields varied improvements and trade-offs across models. Notably, later architectures like SD3.5 showed slight decrease in fidelity, trend less apparent in earlier models. This difference likely stems from the base models histories: newer models may have already incorporated some variation of fine-tuning after their initial pre-training. Consequently, our general-purpose SFT with Alchemist, while beneficial, might introduce characteristics that subtly conflict with these existing, highly specific optimizations, leading to the observed fidelity trade-off. Earlier models, with less such prior refinement, may more readily absorb Alchemists broad quality enhancements. We also observe an inherent link between increased image complexity and potential drop in fidelity. Guiding models towards richer scenes, strength Alchemist confers, inherently provides more opportunities for minor artifacts. This suggests that achieving high complexity and maximal fidelity may necessitate techniques beyond general SFT. Furthermore, our results confirm this SFT approach minimally impacts image-text relevance. This aspect seemingly depends more on model architecture, initial pre-training data, and dedicated alignment methods, rather than fine-tuning primarily focused on visual style. Ultimately, Alchemists quality improvements effectively bridge the performance gap between traditional SD models and cutting-edge solutions. Figure 5 reveals that Alchemist-tuned SDXL and SD3.5 Medium exhibit aesthetic quality and image complexity comparable to leading models like FLUX.1-dev [19] despite having 4 times less parameters. This underscores that data-efficient SFT on well-pre-trained foundations remains viable path to significant quality advancements. Model Side-by-Side Win Rate Rel. Aes. Comp. Fidel. SD1.5-Alchemist-3k vs Alchemist-7k vs Alchemist-19k SD2.1-Alchemist-3k vs Alchemist-7k vs Alchemist-19k SDXL-Alchemist-3k vs Alchemist-7k vs Alchemist-19k SD3.5M-Alchemist-3k vs Alchemist-7k vs Alchemist-19k SD3.5L-Alchemist-3k vs Alchemist-7k vs Alchemist-19k 0.44 0.43 0.45 0.46 0.49 0. 0.48 0.50 0.54 0.52 0.62 0.62 0.61 0.60 0.61 0.65 0.61 0. 0.52 0.68 0.64 0.67 0.62 0.76 0.66 0.73 0.58 0.81 0.47 0. 0.47 0.49 0.55 0.53 0.57 0.53 0.53 0.58 0.55 0.57 Figure 4: Comparison of models fine-tuned on Alchemist variants of different sizes. The table reports human win rates (by aspect) of Alchemist-3k-tuned models against models tuned on 7k and 19k variants of Alchemist. Green indicates statistically significant improvement (p < 0.05), gray no significant change, and red statistically significant decline."
        },
        {
            "title": "6 Limitations",
            "content": "Figure 5: Results of SbS comparison of SDXL, SD3.5 Medium before and after fine-tuning versus FLUX. Grey shaded region shows the interval of statistical insignificance. While Alchemist fine-tuning significantly enhances image aesthetics and complexity (Section 4.3), two primary limitations warrant acknowledgment. Firstly, this pursuit of visual richness can introduce marginal decrease in perceived fidelity for some models, trade-off more pronounced in highly optimized later architectures (e.g., SDXL, SD3.5) than in earlier ones (e.g., SD1.5, SD2.1) which showed clearer net quality gains without substantial defect increases. This suggests that pushing already high-performing models towards greater complexity via SFT may inherently surface minor imperfections. Secondly, our approach did not yield significant improvements in image-text relevance. This aspect appears to be more dependent on factors like model architecture, initial pre-training, and dedicated alignment techniques, rather than the visual quality-focused SFT employed here. Despite these points, Alchemist effectively achieves its primary goal of elevating key visual qualities in text-to-image models using compact, targeted dataset."
        },
        {
            "title": "7 Conclusion",
            "content": "This work introduced Alchemist, compact (3,350 samples) supervised fine-tuning (SFT) dataset, and its novel creation methodology leveraging pre-trained diffusion model as key quality estimator, followed by re-captioning with moderately descriptive, user-like prompts. Extensive experiments across five Stable Diffusion models demonstrated Alchemists effectiveness in significantly boosting aesthetic quality and image complexity, outperforming baselines and size-matched LAION-Aesthetics SFT. While image-text relevance remained largely unaffected and minor complexity-fidelity tradeoff emerged for highly optimized models, ablation studies underscored the crucial role of our strict filtering and compact dataset size for achieving superior SFT outcomes. Our principled, data-efficient approach and the public release of the Alchemist dataset and fine-tuned model weights offer valuable resources and insights for advancing text-to-image generation through high-quality SFT."
        },
        {
            "title": "References",
            "content": "[1] Anonymous, Danbooru community, and Gwern Branwen. Danbooru2021: large-scale crowdsourced & tagged anime illustration dataset. https://gwern.net/danbooru2021, January 2022. Accessed: May 10th, 2025. 9 [2] Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [4] Ali Borji. Pros and cons of gan evaluation measures. Computer vision and image understanding, 179:4165, 2019. [5] Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Topiq: top-down approach from semantics to distortions for image quality assessment. IEEE Transactions on Image Processing, 2024. [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [7] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [10] Tinglei Feng, Yingjie Zhai, Jufeng Yang, Jie Liang, Deng-Ping Fan, Jing Zhang, Ling Shao, and Dacheng Tao. Ic9600: benchmark dataset for automatic image complexity assessment. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(7):85778593, 2023. [11] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36:2709227112, 2023. [12] Shuai He, Yongchang Zhang, Rui Xie, Dongxiang Jiang, and Anlong Ming. Rethinking image aesthetics assessment: Models, datasets and benchmarks. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 942948, 2022. [13] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [15] V. Hosu, H. Lin, T. Sziranyi, and D. Saupe. Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment. IEEE TIP, 29:40414056, 2020. [16] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [17] Gu Jinjin, Cai Haoming, Chen Haoyu, Ye Xiaoxing, Jimmy Ren, and Dong Chao. Pipal: large-scale image quality assessment dataset for perceptual image restoration. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16, pages 633651. Springer, 2020. 10 [18] Sergey Kastryulin, Artem Konev, Alexander Shishenya, Eugene Lyapustin, Artem Khurshudov, Alexander Tselousov, Nikita Vinokurov, Denis Kuznedelev, Alexander Markovich, Grigoriy Livshits, et al. Yaart: Yet another art rendering technology. arXiv preprint arXiv:2404.05666, 2024. [19] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [20] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [22] David Lowe. Distinctive image features from scale-invariant keypoints. International journal of computer vision, 60:91110, 2004. [23] Message Passing Interface Forum. MPI: Message-Passing Interface Standard Version 4.0, June 2021. [24] Naila Murray, Luca Marchesotti, and Florent Perronnin. Ava: large-scale database for aesthetic visual analysis. In 2012 IEEE conference on computer vision and pattern recognition, pages 24082415. IEEE, 2012. [25] NovelAI. Novelai improvements on stable diffusion, 2023. Accessed: 2024-06-10. [26] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. [28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [29] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [31] Seyedmorteza Sadat, Otmar Hilliges, and Romann M. Weber. Eliminating oversaturation and artifacts of high guidance scales in diffusion models, 2024. [32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [33] Mehdi SM Sajjadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. Advances in neural information processing systems, 31, 2018. [34] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 11 [35] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [36] Christoph Schuhmann and team LAION. Laion-aesthetics dataset. https://github.com/ LAION-AI/laion-aesthetic, 2022. Accessed: May 10th, 2025. [37] Christoph Schuhmann and team LAION. Laion-aesthetics v2 dataset. https://github.com/ LAION-AI/laion-aesthetic, 2022. Version 2. Accessed: May 10th, 2025. [38] Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. [39] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261272, 2020. [40] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022. [41] WikiArt.org. WikiArt image collection. https://www.wikiart.org/, 2012. Accessed: May 10th, 2025. [42] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [43] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. [44] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. [45] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [46] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023."
        },
        {
            "title": "Appendices",
            "content": "Here we show additional results on generation of non-square images (Appnedix A), detail dataset collection procedure (Appendix B), provide additional ablations of data filtering and LAION-Aesthetics size selection (Appendix C), detail our experimental and inference settings (Appendices and E), and describe and provide examples of human evaluation (Appendix F). In conclusion we discuss broader impact (Appendix G) and provide addisional visualizations for qualitative assessment (Appendix H). Non-square Aspect Ratio Generation In the early era of diffusion-based text-to-image generation, models such as SD1.5 and SD2.1 were trained exclusively on square images, limiting their ability to generate images with different aspect ratios. As diffusion technology advanced, the concept of bucketed training was introduced [25]. This approach organizes training batches by resolution, where each batch contains images of identical resoluitons, but the image size varies between training iterations. This methodology enabled models to generate images across diverse aspect ratios. The Alchemist dataset comprises images with varying aspect ratios, facilitating bucketed fine-tuning. This training approach ensures the model can produce high-quality images beyond the traditional square format. In our experiments with SDXL, SD3.5 Medium, and SD3.5 Large, we implemented bucketing for the latent representations, varying the latent resolution across batches. In addition to evaluating square image generation, we present side-by-side (SbS) comparisons between Alchemist-tuned versions of SDXL, SD3.5 Medium, and SD3.5 Large against their original counterparts, which were inherently designed to support multi-aspect ratio image generation. Model Side-by-Side Win Rate Rel. Aes. Comp. Fidel. SDXL-Alchemist[h,w] vs baseline[1280,768] vs baseline[896,1152] SD3.5M-Alchemist[h,w] vs baseline[1280,768] vs baseline[896,1152] SD3.5L-Alchemist[h,w] vs baseline[1280,768] vs baseline[896,1152] 0.53 0.48 0.52 0. 0.51 0.51 0.62 0.63 0.55 0.60 0.60 0.67 0.78 0.83 0.65 0. 0.71 0.72 0.49 0.49 0.50 0.51 0.40 0.40 Table 2: We evaluated the ability of Alchemist-tuned models and baseline models to generate images with non-square aspect ratios. For each model, we produced images of resolution [h, w], where the exact values of and are specified in the baseline subscripts. The table reports human win rates (by aspect) w.r.t. Alchemist-tuned models. Green indicates statistically significant improvement (p < 0.05), gray no statistically significant change, and red statistically significant decline. The side-by-side (SbS) comparison results align with those in Table 1: fine-tuning enhances the aesthetic quality and complexity of generated images without sacrificing text coherence, though for SD3.5 Large it occasionally introduces more artifacts due to increased detail. This confirms that the dataset enables generation of images of various aspect ratios without compromising their quality compared to the baseline square size."
        },
        {
            "title": "B Dataset Collection Details",
            "content": "B.1 Timestep Selection The timestep [0.0, 1.0] in the input in Algorithm 1 is crucial parameter of our approach. When approaches 0.0, the generated image is almost fully formed, and the influence of the text prompt diminishes significantly. Conversely, as approaches 1.0, the activations become dominated by noise and lose interpretability. Through empirical analysis, we identified = 0.25 as an optimal balance point and employed this value across all binary classifiers. B.2 Diffusion-based Estimator Prompt Another critical element of Algorithm 1 is its input text prompt P. We define it as follows: \"complex. detailed. simple. bokeh effect. abstract. photorealistic. artistic. stylized. aesthetic. cinematic. instagram filters. color correction. midjourney. ugly. distorted. blurry. rendering. AI-generated. synthetic. high quality. low quality. pixelated. low illumination.\" This prompt formulation integrates both empirical findings and theoretical principles of visual appeal, specifically targeting perceptual factors that influence human judgments of image quality. The template incorporates descriptors that capture both desirable and undesirable attributes across key visual dimensions: 1. Image complexity. Our experimental analysis revealed that images with minimal visual complexity (e.g., images with monochrome backgrounds or reduced detail density) contributed negligibly to model generation quality and are being overshadowed by more intricate, information-rich counterparts. Furthermore, while the inclusion of images featuring bokeh effects demonstrated stabilizing influence on training dynamics, we observed corresponding degradation in overall model performance. Consequently, our final curation pipeline excluded both minimalistic imagery and samples exhibiting excessive bokeh distortion. 2. Art. Artistic images and real life photos inherently differ in their visual characteristics and require separate processing pipelines. Photographic quality relies on objective technical parameters that are more suited for measurements, whereas artistic quality depends on subjective stylistic choices and are often out-of-domain for the most of classifiers. For these reasons we focus on incorporating such feature in our prompt. 3. Aesthetic and Color correction. We aim to estimate the aesthetic quality of images by learning discriminative features associated with coherent color palette, sharp focus on key subjects, satisfying photo composition rules and other properties of aesthetically compelling images from those that are commonly produced by amateur photography. critical subtask in computational aesthetic evaluation involves assessing color fidelity, as significant portion of consumer-grade photographs exhibit improper white balance, inaccurate saturation, or unnatural tonal distributions due to uncalibrated capture devices and lack of skill. This aspect specifically identifies images with professional-grade color correction characterized by balanced neutral tones, highlight-to-shadow transitions, proper color palette and saturation. 4. Compression and noise. Beyond aesthetic considerations, technical image quality presents challenge for generative model training. Degradation categories, such as compression artifacts from JPEG and WebP formats, sensor-level noise and optical aberrations, affect high-frequency features learning that results with increase in image generation artifacts."
        },
        {
            "title": "C Additional Ablations",
            "content": "C.1 Filtration Approach This subsection examines the necessity of the diffusion-based estimator in our filtration pipeline. To evaluate its importance, we removed this component and implemented more rigorous filtering process using TOPIQ-IAA [5] and classifiers trained on TAD-66k [12], KonIQ-10k [15] and IC-9600 14 [10]. This approach maintained the same sample size as Alchemist while selecting for high aesthetic quality and substantial complexity. All other steps in our filtration pipeline remained unchanged. We adhere to the same image appeal considerations detailed in Appendix B. Our pipeline begins with complexity filtering using the IC-9600 classifier, where we apply lower threshold to exclude monochromatic or overly simplistic images. Next, we employ aesthetic and image quality estimators trained on TAD-66K and KonIQ-10k correspondingly. Based on our analysis, the KonIQ-based classifier aligns more closely with human judgment for high-scoring images. Consequently, we apply stricter threshold for KonIQ compared to the TAD-66K-based model, which shows less consistent performance for top-tier samples. Following data filtration using non-diffusion-based estimators, we fine-tuned the baseline Stable Diffusion models referenced in our main text. We then evaluated these fine-tuned models through side-by-side (SbS) comparisons with their corresponding Alchemist-tuned counterparts. The results of this evaluation are presented in Table 3. Model SD1.5-Alchemist Side-by-Side Win Rate Rel. Aes. Comp. Fidel. vs IC9600-TAD66k-KonIQ-sorted 0.78 0.54 0.52 0.62 SD2.1-Alchemist vs IC9600-TAD66k-KonIQ-sorted 0.80 0.53 0.59 0.68 SDXL-Alchemist vs IC9600-TAD66k-KonIQ-sorted 0.84 0.63 0.58 0.68 SD3.5M-Alchemist vs IC9600-TAD66k-KonIQ-sorted 0.94 0.46 0.46 0.79 SD3.5L-Alchemist vs IC9600-TAD66k-KonIQ-sorted 0.91 0.62 0.52 0.72 Table 3: Comparison of Alchemist-tuned models against models tuned on the dataset filtrated using TOPIQ-IAA and IC9600, TAD-66k and KonIQ-10k trained classifiers. The table reports human win rates (by aspect) w.r.t. Alchemist-tuned models. Green indicates statistically significant improvement (p < 0.05), gray no statistically significant change, and red statistically significant decline. The newly obtained models exhibit two key limitations: (1) reduced image-text coherence and (2) reduced fidelity. We attribute these effects to several factors: 1. The IC-9600-trained classifier retains excessively complex images in its top selections, whereas our diffusion-based estimator effectively identifies samples with \"moderate\" complexity - key characteristic for improving generation quality. Training on overly complex data consistently degrades output fidelity. 2. Overly strict thresholds on both TAD-66k and KonIQ-10k filters introduce significant content bias in the dataset, ultimately compromising text-to-image alignment during generation. 3. Visual analysis of the dataset, along with side-by-side (SbS) model comparisons after tuning on this data, shows an important limitation. Existing classifiers are not able to reliably tell apart average-quality images from the aesthetically outstanding samples needed for successful SFT. This ablation study shows that using TOPIQ-IAA and classifiers trained on TAD-66k, KonIQ-10k and IC9600 does not lead to Alchemist-level integral quality of fine-tuned models. C.2 LAION-Aesthetics Size In our primary analysis, we compared the 3,350-sample Alchemist dataset against an equally sized random subset of LAION-Aesthetics v2 [37] images meeting our minimum size threshold (area 15 1024 1024 px). In this subsection we validate that the sample size was not the reason for inferior performance of the LAION-based finetuning. To ablate the influence of the dataset size we select complete set of 31k samples from LAIONAesthetics v2 that pass resolution-based selection. Consistently with our previous fine-tuning experiments, we performed hyperparameter sweep to train the top-performing models for this dataset. After that, we conducted side-by-side (SbS) comparisons against Alchemist fine-tuned versions  (Table 4)  . Model Side-by-Side Win Rate Rel. Aes. Comp. Fidel. SD1.5-Alchemist vs full LAION-tuned 0.55 0. 0.77 0.54 SD2.1-Alchemist vs full LAION-tuned 0.54 0. 0.76 0.63 SDXL-Alchemist vs full LAION-tuned 0.54 0. 0.86 0.63 SD3.5M-Alchemist vs full LAION-tuned 0.55 0. 0.82 0.52 SD3.5L-Alchemist vs full LAION-tuned 0.52 0. 0.72 0.60 Table 4: Comparison of Alchemist-tuned models and models tuned on the full LAION-Aesthetics v2 dataset. The table reports human win rates (by aspect) w.r.t. Alchemist-tuned models. Green indicates statistically significant improvement (p < 0.05), gray no statistically significant change, and red statistically significant decline. Human evaluation results demonstrate that models trained on the full LAION-Aesthetics v2 dataset continue to underperform those fine-tuned with Alchemist, particularly in measures of aesthetic quality and image complexity."
        },
        {
            "title": "D Experimental Setting",
            "content": "D.1 Hyperparameter Sweep and Train Setting Model SD1.5 SD2.1 SDXL Learning Rates Iterations (thousands) EMA β [1e-5, 2.5e-5, 8e-5] [1e-5, 2.5e-5, 8e-5] [1e-5, 2.5e-5, 8e-5] [2.5, 5, 7.5, 10, 12.5] [2.5, 5, 7.5, 10, 12.5] [5, 10, 15, 20] [0.999, 0.9999] [0.999, 0.9999] [0.999, 0.9999] SD3.5 [5e-6, 2.5e-5, 8e-5] [1e-6, 5e-6, 2.5e-5] SD3.5 [20, 40, 60, 80] [20, 40, 60] [0.9999] [0.9999] Table 5: Hyperparameter grids during our sweep. The particular choices were made according to the community best practices as well as our computational and human resource constraints. We performed training hyperparameter sweep according to the Table 5 with the resulting training setup presented in the Table 6. Total batch size of 80, AdamW optimizer [21], Adam betas β1 = 0.9, β2 = 0.999 and constant learning rate scheduler were set the same for all the models. We didnt use learning rate warm-up. See Figure 6 for the training dynamics across different learning rates. We used NVIDIA A100 with 80Gb of VRAM, PyTorch 2.6.0 [27], and CUDA 12.6. We varied the number of GPUs from 4 to 8 to ensure the total batch size of 80 on the one hand, and minimize the quantity of GPUs on the other. Distributed communication is performed via Open MPI [23]. We"
        },
        {
            "title": "Learning Rate",
            "content": "Iterations Weight Decay EMA β GPUs Mixed Precision SD1.5 SD2.1 SDXL SD3.5 SD3.5 8e-5 2.5e-5 2.5e-5 5e-6 5e-6 5k 7.5k 10k 40k 20k 1e-2 1e-2 1e-2 1e-4 1e-4 0.999 0.999 0.999 0.9999 0.9999 4 4 8 8 float16 float16 float16 bfloat16 bfloat16 Table 6: Final setup for fine-tuning on the Alchemist. adopt Fully Sharded Data Parallel [46] for parameter and optimizer state sharding to reduce memory consumption and allow working with larger batch sizes. Ultimately, with the final tuning setup it takes 12 GPU-hours to train SD1.5 model, 18 GPU-hours to train SD2.1, 80 GPU-hours to train SDXL, 480 GPU-hours to train SD3.5 Medium and 576 GPU-hours to train SD3.5 Large. (a) SD1.5. (b) SDXL. (c) SD3.5 Medium. (d) SD3.5 Large. Figure 6: Training dynamics of SD models while tuning on Alchemist. We chose final checkpoints as those maximizing Aesthetics and Complexity while not reaching statistically significant decline in other aspects (if possible at all)."
        },
        {
            "title": "E Inference Parameters",
            "content": "E.1 Evaluation Setting For all models except for the SD3.5 Large we conducted all inference measurements on 1 NVIDIA A100 GPU with 40GB of VRAM, batch size 1, using PyTorch 2.6.0 [27], and CUDA 12.6. For the SD3.5 Large we used the same software, but NVIDIA A100 GPU with 80GB of VRAM. To generate images we used the parameters either recommended in the corresponding models HuggingFace repositories or default ones from the Diffusers library. These parameters are provided in the Table 7. For SDXL we used 80/20% split of denoising steps between base and refiner models."
        },
        {
            "title": "Precision",
            "content": "SD1.5 SD2.1 SDXL SD3.5 SD3.5 7.5 7.5 5.0 4.5 3.5 50 50 50 40 28 float16 float16 float16 bfloat16 bfloat16 Table 7: Inference parameters used in our work. E.2 Inference Parameter Sweep Although all experiments used default inference parameters, we additionally evaluated model performance across different guidance scales and denoising steps. Due to limitations in human evaluation resources, we employed automated assessment using the ImageReward [44] metric for this analysis. Varying guidance scale in [1.0, 2.0, 4.0, 7.5] and number of inference steps in [16, 32, 64], we show the dynamics of ImageReward in Figure 7. Consistent with our primary analysis, all metrics were computed on the MJHQ-30k benchmark dataset [20]. (a) SD1.5. (b) SD2.1. (c) SDXL. (d) SD3.5 Medium. Figure 7: ImageReward metric change depending on guidance scale and number of denoising steps before and after tuning on Alchemist. Our evaluations demonstrate that Alchemist-based tuning improves overall generation quality, evidenced by increased minimum and maximum ImageReward values. However, while the parameter heatmaps reveal ImageRewards preference for higher guidance scales, we caution against using these results as definitive optimization criteria. Prior work has established that excessive guidance 18 scales induce overexposure artifacts in generated images [31], suggesting potential limitations in this metrics alignment with human perceptual quality."
        },
        {
            "title": "F Human Evaluation",
            "content": "We evaluated text-to-image generation quality through controlled side-by-side (SbS) comparisons, where professional assessors selected the superior image for each prompt-image pair. Each comparison involved three independent annotations, with final judgments determined by majority vote. Our evaluation team consists of trained professionals employed under ethical working conditions, including competitive compensation and risk disclosure. Assessors have received detailed and finegrained instructions for each evaluation aspect and passed training and testing before accessing the main tasks. We highlight that our organizational equivalent of IRB approved the study. Annotators evaluated pair of images generated given the same validation or test prompt based on four criteria: Image-Text Relevance: Accuracy of the image content relative to the text prompt; Aesthetic Quality: Overall visual appeal, including composition and style; Image Complexity: Richness of detail and content within the scene; Fidelity: Presence and severity of defects, artifacts, distortions, or undesirable elements. We provide the platforms interface during each aspect assessment in Figures 8,910,11. From mathematical point of view, human evaluation is statistical hypothesis test. In particular, we are using two-sided binomial test and its implementation from scipy [39] library to test the null hypothesis of whether the two given models are equal in terms of image generation quality in 4 aspects independently. More precisely, for each aspect we calculate p-value as following: from scipy . stats import binomtest # cnt_wins_b aseline # cnt_ wins_e xper ime nt - number of wins for experimental model # cnt_equals - number of equals p_value - number of wins for baseline model = binomtest ( cnt_wins_base line + cnt_equals / 2 , cnt_wins_base line + cnt_equals + cnt_wins_experiment ) We reject the null hypothesis if is less than 0.05, i.e., at the 5% significance level."
        },
        {
            "title": "G Broader Impact",
            "content": "The release of our open-source SFT dataset and fine-tuned text-to-image diffusion models carries significant societal implications, both positive and challenging. By openly sharing these resources, we aim to advance research in generative AI while fostering accessibility and reproducibility. The improved aesthetic quality and image complexity offered by our models can empower artists, educators, and small-scale creators, democratizing access to high-quality visual generation tools. However, like all generative AI systems, these models present risks that must be carefully managed. The potential for misuse-such as generating deceptive imagery or deepfakes-necessitates safeguards, including provenance tracking and responsible deployment practices. The environmental impact of training and deploying such models also warrants consideration, encouraging the adoption of efficient fine-tuning techniques and shared computational resources. To maximize the benefits of this work while mitigating risks, we emphasize the importance of transparency, collaboration, and oversight. Users should disclose AI-generated content where ethically relevant, and developers should engage with diverse stakeholders-including artists and ethicists-to ensure alignment with societal values. By proactively addressing these challenges, we hope to contribute to the responsible advancement of generative AI, ensuring that its benefits are widely accessible while minimizing unintended harm. 19 Figure 8: An example of user interface for the Image-Text Relevance aspect of Human Evaluation with Side-by-Side comparisons. Figure 9: An example of user interface for the Aesthetics aspect of Human Evaluation with Side-by-Side comparisons. 21 Figure 10: An example of user interface for the Fidelity aspect of Human Evaluation with Side-bySide comparisons. 22 Figure 11: An example of user interface for the Image Complexity aspect of Human Evaluation with Side-by-Side comparisons."
        },
        {
            "title": "H More Visualizations",
            "content": "We provide more examples of images generated by models before and after fine-tune on Alchemist. The corresponding prompts are listed prior to the grids of images. Figure 12 prompts 1. \"the Beatles crossing Abbey road\" 2. \"a portrait of statue of the Egyptian god Anubis wearing aviator goggles, white t-shirt and leather jacket, flying over the city of Mars.\" 3. \"Downtown LA at sunrise. detailed ink wash.\" 4. \"a bird standing on stick\" 5. \"a tornado passing over corn field\" 6. \"a tennis court with tennis balls scattered all over it\" 7. \"a cloud in the shape of castle\" 8. \"a flower with large red petals growing on the moons surface\" 9. \"a diagram of brain function\" 10. \"a frustrated child\" 11. \"a woman with long black hair and dark skin\" 12. \"a macro photograph of brain coral\" 23 Figure 12: More examples of SD1.5 generations before and after tuning on Alchemist. Zoom in for the best view. 24 Figure 13 prompts 1. \"A blue Porsche 356 parked in front of yellow brick wall\" 2. \"a flower with cats face in the middle\" 3. \"a flower with large yellow petals\" 4. \"A photo of an Athenian vase with painting of pandas playing basketball in the style of Egyptian hieroglyphics.\" 5. \"a teddy bear on skateboard in times square\" 6. \"a red sports car on the road\" 7. \"an airplane flying into cloud that looks like monster\" 8. \"graffiti of funny dog on street wall\" 9. \"a laptop screen showing bunch of photographs\" 10. \"a view of the Kremlin on sunny day\" 11. \"a lavender backpack with triceratops stuffed animal head on top\" 12. \"Superman shaking hands with Spiderman\" Figure 13: More examples of SD2.1 generations before and after tuning on Alchemist. Zoom in for the best view. 26 Figure 14 prompts 1. \"a chimpanzee wearing bowtie and playing piano\" 2. \"a white towel\" 3. \"a cat licking large felt ball with drawing of the Eiffel Tower on it\" 4. \"a man and woman standing in the back up an old pickup truck\" 5. \"robots meditating\" 6. \"the silhouette of an elephant\" 7. \"A raccoon wearing formal clothes, wearing top hat and holding cane. The raccoon is holding garbage bag. Oil painting in the style of Vincent Van Gogh.\" 8. \"five red balls on table\" 9. \"a pumpkin with candle in it\" 10. \"A close-up of two mantis wearing karate uniforms and fighting, jumping over waterfall.\" 11. \"a yellow box to the right of blue sphere\" 12. \"a futuristic city in synthwave style\" 27 Figure 14: More examples of SDXL generations before and after tuning on Alchemist. Zoom in for the best view. 28 Figure 15 prompts 1. \"a witch riding broom\" 2. \"A heart made of cookie\" 3. \"an airplane flying into cloud that looks like monster\" 4. \"a peaceful lakeside landscape\" 5. \"a lavender backpack with triceratops stuffed animal head on top\" 6. \"a red block to the left of blue pyramid\" 7. \"a black dog sitting between bush and pair of green pants standing up with nobody inside them\" 8. \"a blue t-shirt\" 9. \"a woman with dog puppet and cat puppet\" 10. \"a yield sign\" 11. \"A photo of an astronaut riding horse in the forest. There is river in front of them with water lilies.\" 12. \"a tiger standing by some flowers\" 29 Figure 15: More examples of SD3.5 Medium generations before and after tuning on Alchemist. Zoom in for the best view. 30 Figure 16 prompts 1. \"The sunset on the beach is wonderful\" 2. \"a view of the Earth from the moon\" 3. \"A punk rock squirrel in studded leather jacket shouting into microphone while standing on lily pad\" 4. \"Gandalf saying you shall not pass\" 5. \"a prop plane flying low over the Great Wall\" 6. \"a marine iguana crossing the street\" 7. \"a large white yacht tossed about in stormy sea\" 8. \"the Parthenon in front of the Great Pyramid\" 9. \"a yellow wall\" 10. \"a chimpanzee wearing bowtie and playing piano\" 11. \"a tiny dragon landing on knights shield\" 12. \"A teddy bear wearing motorcycle helmet and cape is standing in front of Loch Awe with Kilchurn Castle behind him\" 31 Figure 16: More examples of SD3.5 Large generations before and after tuning on Alchemist. Zoom in for the best view."
        }
    ],
    "affiliations": [
        "MSU",
        "Yandex",
        "Yandex Research"
    ]
}