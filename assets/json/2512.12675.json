{
    "paper_title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling",
    "authors": [
        "Yuran Wang",
        "Bohan Zeng",
        "Chengzhuo Tong",
        "Wenxuan Liu",
        "Yang Shi",
        "Xiaochen Ma",
        "Hao Liang",
        "Yuanxing Zhang",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 5 7 6 2 1 . 2 1 5 2 : r Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling Yuran Wang1,2* Bohan Zeng1,2* Chengzhuo Tong1,2 Wenxuan Liu1 Yang Shi1,2, Xiaochen Ma1 Hao Liang1 Yuanxing Zhang2 Wentao Zhang1 1Peking University 2Kling Team, Kuaishou Technology"
        },
        {
            "title": "Abstract",
            "content": "Subject-driven image generation has advanced from singleto multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, unified understandinggeneration method that integrates composition and distinction. Scone enables the understanding expert to act as semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing opensource models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone. Figure 1. The distinction problem and challenges. (a) Problem. State-of-the-art methods have limitations in distinguishing (b) Challenge 1: target subjects specified by the instruction. semantic deficiency in generation. Reference image information from the understanding and generation experts in the unified model is used to compute semantic similarity with instruction. (c) Challenge 2: biased understanding and misaligned generation. Und. and Und.+Gen. indicate whether texture information from generation expert in the unified model is included to collaborate with understanding expert. The unified model is BAGEL [7]. 1. Introduction Image generation methods [3, 8, 36] have demonstrated exceptional capabilities, enabling the generation of desired images across diverse scenarios [35]. Subject-driven image generation has recently gained significant attention, with the focus evolving from single-subject to multi-subject generation, incorporating more input images. Existing methods [36, 37, 39, 40] can process two or more input images and combine subjects based on instructions. Moreover, methods such as [8, 44] extend this capability by accepting more than four images, showcasing potential for more complex composition tasks. However, existing works primarily focus on expanding subject combinations while neglecting the ability to dis- *Equal contribution Corresponding author: wentao.zhang@pku.edu.cn tinguish target subjects in complex contexts. As shown in Fig. 1(a), although current models can combine multiple subjects, they may fail to distinguish and generate the correct target subject when reference image contains multiple candidates, leading to problems such as subject omissions (none of the candidate subjects appear) or errors (misidentification of the target subject). Real-world images often involve interference and intricate details [19, 32], further limiting practical performance. Thus, we emphasize examining the input subjects themselves, focusing on the models ability to distinguish the target subject within complex contexts and leverage this information for generation. core challenge is extracting useful information from complex references, which remains difficult for generation models. Subject distinction relies on semantic understanding of instructions expression of references, where understanding models are more proficient [1, 17, 47]. As 1 shown in Fig. 1(b), in unified understanding-generation model consisting of an understanding expert and generation expert, the information encoded by the understanding expert is more similar to the instruction, which means more aligned with instruction than that encoded by the generation expert, revealing generation models deficiency and understanding models advantage in interacting with instructions and semantically understanding reference information. However, this semantic advantage of understanding models is not entirely reliable: understanding models often exhibit biases [14, 18, 31, 46], which become problematic when directly used to assist generation. As illustrated in Fig. 1(c), in unified model, relying only on semantic information from understanding expert still struggles to prevent irrelevant subjects from appearing, and subject errors persist even with correct semantic information due to misalignment between generation and understanding experts. with"
        },
        {
            "title": "Compared",
            "content": "generation models, unified understanding-generation models offer clear advantage for subject-driven image generation because the understanding expert captures semantic cues earlier than the generation expert [49], as illustrated in Fig. 2(a). These early-layer semantics highlight instruction-relevant regions such as candidate subjects and enable more accurate distinction in complex reference images. Moreover, to alleviate bias introduced by the understanding expert, the unified architecture allows end-to-end collaboration, as shown in Fig. 2(b). The understanding expert refines its semantic interpretation through feedback from generation, and the generation expert aligns with these cues to better preserve subject-related details. Based on these insights, we propose subjectdriven image generation method to address the aforementioned challenges, Scone (Subject-driven composition and distinction enhancement), built upon unified understanding-generation model capable of handling subject composition and distinction. Our method leverages the strong understanding capabilities of the understanding expert to overcome the limitations of the generation expert in complex contexts involving reference images and instructions. Specifically, Scone enables the understanding expert to act as semantic bridge conveying high-level semantic information to guide generation, which called understanding bridge strategy. In the first training stage, the model learns subject composition on single-candidate data (i.e. reference image contains only one candidate subIn the second stage, ject) within the unified framework. the understanding expert is trained to align visual and textual representations and filter instruction-irrelevant regions using semantic mask derived from early layer, forming robust semantic bridge. After this formation, the understanding expert provides semantic guidance to the generation expert, ensuring that subject-related information is em2 Figure 2. Our motivation. (a) visualizes the early similarity between image token hidden states from the understanding and generation experts and text token hidden states within the unified model, showing that the former attends to semantic regions while the latter is less sensitive. (b) illustrates the collaboration between the understanding and generation experts within the unified model through end-to-end training. phasized while unrelated interference is suppressed. This design enables Scone to distinguish useful reference information and achieve precise subject composition in complex multi-subject contexts. As shown in Fig. 1(a), compared to existing subject-driven image generation methods, our method more accurately distinguishes relevant reference information and generates ideal results. Furthermore, to evaluate whether existing models can genuinely distinguish subjects in reference images based on instructions and use relevant information to generate the correct target subject, we introduce new benchmark, SconeEval, which includes subject-driven image generation tasks with varying difficulty levels, including composition, distinction, and distinction & composition. This benchmark provides comprehensive evaluation of the performance of subject-driven image generation methods from both composition and distinction perspectives. Our main contributions are threefold: We propose the Scone (Subject-driven composition and distinction enhancement) model, which supports multisubject composition and excels in subject distinction in complex contexts. Experiments show Scone ranks first among open-source models on OmniContext benchmark. We introduce the understanding bridge strategy, which transforms the understanding expert into semantic bridge, enabling early multimodal alignment and attention-based semantic filtering to guide the generation expert, enhancing subject distinction and semantic fidelity without adding extra parameters. We develop SconeEval, challenging benchmark with three difficulty levels, to evaluate performance on subjectdriven image generation tasks from both composition and distinction perspectives. 2. Related work 2.1. Subject-driven image generation Early subject-driven generation rely on fine-tuned diffusion models [34, 43, 45], which introduce image conditions for flexible customization. With the rise of Diffusion Transformer [24], generation quality improve significantly. Recent methods [10, 13, 29, 40] extend singleand multisubject composition but typically assume clean references, making it difficult to extract or distinguish target subjects in complex images. Although methods like SSR-Encoder [48] aim to isolate features, their limited understanding ability and reliance on single-image captions restrict their effectiveness under complex instructions or noisy inputs. 2.2. Unified understanding-generation models To advance general-purpose agents, several methods [5 7, 15, 16, 28, 41, 42] integrate multimodal understanding and generation tasks within unified architecture. By leveraging multimodal understanding, these methods enhance the stability of image generation when handling complex instructions. Some methods [2, 37, 44] use this capability for subject-driven generation. However, when reference images contain substantial irrelevant content, existing unified models lack effective mechanisms to prevent interference, often resulting in unwanted subjects. We address this gap by using understanding semantics to better distinguish target conditions and guide cleaner, more reliable generation. 3. The Scone model (Subject-driven composition and We present Scone distinction enhancement), which supports multi-subject composition and demonstrates strong distinction capability in complex contexts through unified understandinggeneration modeling. 3.1. Motivation and preliminaries Distinction with understanding guidance via unified modeling Unified models outperform generation models because their strong understanding ability handles complex semantics and their cross-modal interactions enhance textimage alignment [30]. The understanding expert captures semantics earlier than the generation expert, providing instruction-relevant cues before texture features emerge [30, 49]. As shown in Fig. 2(a), early-layer similarity with text token hidden states indicates that the understanding expert attends to key subject regions, while the generation expert shows weaker semantic sensitivity. End-to-end understanding-generation collaboration The understanding expert may introduce semantic bias, leading to subject errors or redundancy. Unified modeling Figure 3. Understanding bridge strategy. Step 1: Understanding bridge formation. Early semantic alignment and attention masking enable the understanding expert to serve as the semantic bridge. Step 2: Understanding bridge guidance. The generation expert is optimized under the guidance of the semantic bridge, enabled by unified understanding-generation modeling. enables end-to-end collaboration, as shown in Fig. 2(b). The understanding expert refines its semantics through generation feedback, and the generation expert aligns with these cues to preserve subject-related details in complex reference images. 3.2. Unified understanding-generation modeling We adopt BAGEL [7] as the base. This Mixture-ofTransformer-Experts architecture processes understanding and generation information through dedicated experts sharing multimodal self-attention. For subject-driven generation, image tokens from the Vision Transformer (ViT) encoder and instruction tokens are handled by the understanding expert, while image tokens from the VAE model are processed by the generation expert. To improve distinction in complex contexts, the understanding expert acts as semantic bridge that provides discriminative cues for generation. We optimize the model using the original MSE loss during training, with no additional parameters. 3.3. Stage I: Composition training We first finetune BAGEL on single-candidate data, where each reference image contains single subject. The understanding expert and generation expert (including corresponding MLP connectors) are trained, while the ViT and VAE remain frozen. One epoch of base data enables both singleand multi-subject generation. refined dataset is then used for another epoch to further enhance subject consistency. Training data details are provided in Sec. 5.1. 3 3.4. Stage II: Distinction training with understanding bridge strategy We propose the understanding bridge strategy, which enables the understanding expert to act as semantic bridge that transfers high-level semantic information for generation guidance, as shown in Fig. 3. It comprises two steps: forming the semantic bridge via multimodal alignment and guiding generation through this bridge. This design improves subject identity preservation, relevance discrimination, and contextual fidelity. Multi-candidate data are introduced in this stage to teach distinction and distinction-aware composition. Step 1: Understanding bridge formation The understanding expert jointly learns visual and textual semantics to become the semantic bridge. Let hv = {hv i=1 and ht = {ht j=1 denote early-layer visual and textual hidden states, respectively. We apply L2-normalization to obtain: }Nv j}Nt ˆhv = hv hv , ˆht = ht ht j2 . We compute the cosine similarities as: = ˆHv( ˆHt), Si,j = ˆhv ˆht j. (1) (2)"
        },
        {
            "title": "The semantic relevance for each visual token is defined",
            "content": "as: si ="
        },
        {
            "title": "1\nNt",
            "content": "Nt(cid:88) j=1 Si,j. (3)"
        },
        {
            "title": "We construct a binary semantic mask M based on a",
            "content": "threshold τ , with the parameter study provided in Sec. D: Mi = (cid:40) si > τ, 0, , otherwise. (4) Rather than discarding hidden states, the mask modifies the attention logits. For logits mapping target tokens to reference image tokens in subsequent layers, we apply the mask as follows: Ak,i = Ak,i + Mi. (5) Tokens where Mi = receive zero attention, which allows target tokens to disregard irrelevant regions. This mechanism establishes the understanding expert as semantic bridge to align representations and suppress semantic interference. We train the model for 1k steps. Step 2: Understanding bridge guidance Functioning as the semantic bridge, the understanding expert guides the generation expert. We train both experts for an additional 1k steps to align generation representations with the bridge and focus on key regions identified by the understanding expert. This phase enforces semantic consistency within complex compositional scenarios. 4. The SconeEval benchmark 4.1. Overview Existing benchmarks usually offer simple contexts where the reference image contains single, prominent, and easily distinguished subject, and the instruction refers to it with basic category terms. Real-world images involve substantial interference and are less structured, and current test cases do not reflect model performance under such complexity. In terms of evaluation, existing benchmarks focus on reproducing and combining subjects, often relying on models such as DINOv2 [23] and CLIP [26] to extract features and compute similarity. Averaging similarity across subjects in multi-subject settings cannot reliably capture generation quality, especially when subject omission or redundancy occurs. To evaluate models ability to distinguish and generate the referred subject in complex visual contexts, we introduce new benchmark, SconeEval. It contains 409 test cases across character, object, and scene combinations and subject distinction, with 19 case types in Fig. 4(a) and 6 subtasks in Fig. 4(b), providing comprehensive evaluation of models ability to distinguish and utilize subject features. Unlike traditional benchmarks that emphasize visual fidelity or text alignment, SconeEval focuses on cross-modal reasoning from complex contexts involving reference images and instructions, which requires deciding whom to generate when multiple candidates appear within or across images. SconeEval includes three progressively challenging tasks, as shown in Fig. 4(c): composition, distinction, and distinction & composition. In the composition task, each reference image contains subject, and one or more images correspond to single or multiple generated subjects. In the distinction task, each reference image contains multiple subjects, and the model generates one target subject. The distinction & composition task integrates both settings, where each reference image contains multiple subjects and multiple images are used for multi-subject generation. Tasks involving distinction include cross-category and intra-category cases, indicating whether candidate subjects in reference image belong to the same category. As shown in Tab. 1, existing benchmarks mainly assess subject composition in simple contexts, whereas our benchmark addresses more realistic scenarios. 4.2. Construction pipeline Step 1: Image collection Images are collected from three sources. (1) existing benchmarks: We filter, recognize subjects, and classify images by subject category with Qwen3Figure 4. Overview of our SconeEval benchmark. Char: character, Obj: object, Sce: scene. SconeEval evaluates target subject identification and generation in complex visual contexts. It provides 409 test cases across three domains with 19 case types and 6 subtasks, covering composition, distinction, and distinction & composition tasks. Table 1. Task comparison of existing benchmark for subjectdriven image generation. Benchmark Composition Distinction Distinction & Composition DreamBench [27] DreamBench++ [25] OmniContext [37] XVerseBench [4] SconeEval (Ours) VL-30B-A3B-Instruct [33], followed by manual check to ensure that each image contains only one subject, using samples from the existing benchmarks DreamBench++ [25] and OmniContext [37]. (2) T2I (text-to-image) model synthesis and (3) Open access: To enhance category diversity, we further supplement the collection by synthesizing single-candidate images with the T2I model Flux.1-dev [12] and acquiring additional samples from open access. Finally, we construct single-candidate image pool that covers three categories, character, object, and scene, comprising 15 subcategories with at least 30 images per subcategory. Images in the single-candidate pool are grouped into sets of 1 to 4 images and split into two subsets for the following singlecandidate and multi-candidate data construction. Step 2: Multi-candidate editing This step produces multi-candidate images by adding other subjects to singlecandidate images with the image editing model QwenImage-Edit-2509 [36], illustrated in Fig. 5, and ensures through manual verification that each subject in the image is clearly recognizable. Step 3: Instruction construction We adopt two-step decoupling strategy for constructing instructions across composition, distinction, and distinction & composition tasks. This separates visual understanding from instruction generation, reducing cross-image interference and improves subject identification accuracy and linguistic coherence. Its Figure 5. Multi-candidate editing in our SconeEval benchmark construction. Edit images to create multi-candidate cases through subject addition. Task difficulty increases with the complexity of reference images and instructions. necessity is discussed in Sec. C. Instructions explicitly include the index of target subjects or distinct features to avoid ambiguity (e.g. Image 1, figure 1, the man with Step 1: Subject identification (image-togreen hair). text). Each image is processed independently with VLM model Qwen3-VL-30B-A3B-Instruct [33] to identify its most prominent subject, minimizing mutual interference. For single-candidate images, we extract direct names (e.g. woman). For multi-candidate images, we extract names with unique referential cues based on attribute, size, and position (e.g. woman on the left of the image), guided by the target subject in the corresponding single-candidate images. For scene images, we provide detailed scene descriptions to support interactions in the constructed instructions (e.g. place the bird on the shelf). Step 2: Instruction generation (text-to-text). Only the subject names or scene descriptions from Step 1 are provided to the LLM model Qwen3-30B-A3B-Instruct-2507 [33], without using 5 Table 2. Quantitative comparison of existing models on OmniContext [37] benchmark. Char. + Obj. indicates Character + Object. indicates our base model. Best scores in each group are highlighted in bold."
        },
        {
            "title": "Character Object",
            "content": "Character Object Char. + Obj. Character Object Char. + Obj. Average SINGLE MULTIPLE SCENE Gemini-2.5-Flash-Image [8] GPT-4o [22] FLUX.1 Kontext [dev] [13] UNO [39] USO [38] UniWorld-V2 [15] Qwen-Image-Edit-2509 [36] BAGEL [7] OmniGen2 [37] Echo-4o [44] Scone (Ours) 8.79 8.96 8.07 7.15 8.03 8.45 8. 7.00 8.17 8.34 8.34 9.12 8.91 7.97 6.72 7.55 8.44 8.41 7.04 7.63 8.27 8. Closed-source model 8.27 8.90 - 3.56 3.32 7.87 7.92 5.32 7.26 8.13 8.24 8.60 8."
        },
        {
            "title": "Generation model",
            "content": "- 6.46 6.10 8.22 8."
        },
        {
            "title": "Unified model",
            "content": "6.69 7.03 8.14 8.14 7.71 8.81 - 4.90 4.56 7.95 7.79 6.74 7.56 8.11 8. 7.63 8.92 - 2.72 2.77 5.36 5.23 3.94 7.02 7.07 7.06 7.65 8.40 - 4.89 5.38 7.47 7. 5.77 6.90 7.73 7.88 6.81 8.44 - 4.76 5.09 6.98 6.86 5.73 6.64 7.77 7. 8.07 8.78 - 5.14 5.35 7.59 7.60 6.03 7.28 7.95 8.01 any image content. This isolation ensures that instruction generation focuses on semantic composition, interaction, and logical fluency, improving the stability and quality of the final instructions. 4.3. Evaluation protocol Following the methods of VIEScore [11] and OmniContext [37], we use GPT-4.1 [21] to generate scores on 010 scale with detailed rationales for evaluating composition capability, including prompt following and subject consistency. The prompt for composition scoring is similar to that of OmniContext [37] benchmark, but it focuses solely on the target subjects preservation when scoring subject consistency. For distinction capability evaluation, GPT-4.1 determines if the described subject from the reference image appears in the target image and computes accuracy, precision, recall, and F1 score based on subject presence or absence. Precision and recall reveal issues such as subject redundancy and omission. The F1 score is the average of precision and recall, and the overall distinction score is the average of accuracy and F1 score. The prompt is shown in Fig. 9. To maintain an equal scale to composition score, we multiply the raw percentage scale (0-1) of the distinction score by 10 to match the [0, 10] scale. Overall score is the average of composition and distinction scores. 5. Experiments 5.1. Implementation details subject-driven generation datasets, Training data We collect large-scale pool of opensource including X2I [40], MUSAR-Gen [9], UNO-1M [39] and Echo-4oImage [44]. We further use Gemini-2.5-Flash-Image [8] to synthesize 15K samples with 3 to 4 input images to supplement missing data types. Data categories are defined to cover character, object, and scene, with specified object types and attributes. GPT-4o [20] generates prompts, instructions, and descriptions based on random attribute combinations. FLUX.1-dev [12] produces input images from the prompts, and Gemini-2.5-Flash-Image uses these images and descriptions to generate final outputs. (1) For training stage I: We select 70K base single-candidate samples randomly from the data pool. We further filter 22K refined single-candidate samples using Qwen3VL-30B-A3B-Instruct [33] by scoring subject consistency (2) For training stage II: In and instruction following. addition to using the refined single-candidate data, we construct 20K multi-candidate data from another filtered subset. For image acquisition, Qwen3-30B-A3B-Instruct2507 [33] extracts subject names and scene descriptions from single-candidate instructions, and Qwen-Image-Edit2509 [36] adds crossor intra-category subjects to create multi-candidate images. The target image is the original image. For instruction construction, original instructions are reused for cross-category data. For intra-category data, Qwen3-VL-30B-A3B-Instruct [33] identifies the new subject introduced during editing, and Qwen3-30B-A3BInstruct-2507 [33] replaces the subject name in the original instruction (e.g. the woman the woman on the left of the image). Evaluation settings We compare Scone with state-of-theart methods on the OmniContext [37] benchmark and our SconeEval benchmark. Images are sampled at 1024 1024 using each methods default configurations. To mitigate randomness, we perform 3 rounds of sampling, scoring 3 times per round, yielding 9 group results. The final score is 6 Table 3. Quantitative comparison of existing models on our SconeEval benchmark. indicates our base model. COM: Composition score. DIS: Distinction score. Best scores in each group are highlighted in bold. Method Composition Single COM Multi COM Distinction Distinction & Composition Cross Intra Cross Intra Average COM DIS COM DIS COM DIS COM DIS COM DIS Overall Gemini-2.5-Flash-Image [8] GPT-4o [22] 8.87 8.92 FLUX.1 Kontext [dev] [13] USO [38] UNO [39] UniWorld-V2 [15] Qwen-Image-Edit-2509 [36] BAGEL [7] OmniGen2 [37] Echo-4o [44] Scone (Ours) 7.92 8.03 7.53 8.41 8.54 7.14 8.00 8.58 8.52 7.94 8. - 5.19 5.38 7.16 6.85 5.55 6.59 7.73 7.40 Closed-Source Model 9.12 9.18 9.15 8. 9.00 9.45 8.50 9.01 8.27 8.83 8.87 8.49 8.17 8.99 8.85 9. 8.56 8.98 8.84 8.90 Generation Model 8.45 8.50 7.90 8.24 8.57 7.95 8.99 8.33 9. 6.20 7.14 6.76 7.44 7.32 6.11 6.51 6.53 6.77 6.86 Unified Model 6.93 6.99 7.74 7.97 6.21 6.80 7. 7.74 7.93 7.96 7.27 8.63 8.85 7.49 8.31 8.36 8.98 - 5.10 5.27 7.52 7.53 6.44 7.28 7. 8.20 - 6.25 7.02 8.03 8.13 7.38 8.30 8.72 9.25 - 5.07 5.61 7.70 7.49 6.87 7.14 8. 8.21 - 5.57 6.27 7.24 7.02 7.27 7.13 8.33 8.44 - 6.41 6.31 7.81 7.76 6.74 7.39 8. 8.21 - 6.71 6.93 7.57 7.65 7.20 7.81 8.14 8.79 8.70 8.94 - 6.56 6.62 7.69 7. 6.97 7.60 8.09 8.50 the average of these results. 5.2. Quantitative evaluation Table 4. Ablation results for stage I. Evaluated on OmniContext [37] benchmark. PF denotes prompt following and SC denotes subject consistency. Best scores are in bold. On OmniContext benchmark As shown in Tab. 2, our Scone achieves the highest average score among opensource methods, showing strong composition capability. Closed-source models GPT-4o [22] and Gemini-2.5-FlashImage [8] achieve the top two average scores, demonstrating leading performance. On SconeEval benchmark As shown in Tab. 3, our Scone achieves the highest composition, distinction and overall scores among open-source models, demonstrating competitive composition performance and strong distinction capabilities. Unified models with lower composition scores, such as OmniGen2 [37], achieve higher distinction scores than generation models like Qwen-ImageEdit-2509 [36], highlighting the advantage of understanding in subject-driven distinction. Closed-source models GPT4o [22] and Gemini-2.5-Flash-Image [8] exhibit strong composition and distinction abilities, securing the top two overall scores, consistent with the results on OmniContext. 5.3. Qualitative evaluation Results on the OmniContext benchmark in Fig. 6 show that Scone demonstrates strong compositional capability, generating harmonious and natural images while preserving subject consistency with high adaptability. Results on our SconeEval benchmark in Fig. 7 show that Scone can compose four subjects and distinguish the target subject among multiple candidates to produce ideal outputs and reduce issues such as subject redundancy, blending, and omisVersion BAGEL [7] - Training data PF SC Overall Stage I, step 1 Stage I, step 2 Refined single-candidate data (22K) Base single-candidate data (70K) 6. 7.83 7.92 5.73 8.27 8.31 6.03 7.95 8.02 sion. Results in Fig. 6 and Fig. 7 are all sampled with the same seed. 5.4. User study We conduct user study to validate the alignment of GPT4.1 scores with human evaluation. Thirty evaluators, both professionals and non-professionals, assess 409 cases from SconeEval. Each evaluator reviews 60 test samples, with 10 samples from each type of task, and compares the output of OmniGen2 [37], UniWorld-V2 [15] and our Scone. Evaluators select the best result based on instruction following, subject consistency, realism, and aesthetics. After normalization, the scores are: OmniGen2 0.27, UniWorld-V2 0.27, and Scone 0.46, confirming both the reasonableness of GPT-4.1 scores and the effectiveness of our method. 5.5. Ablation study Effects of refined data in stage As shown in Tab. 4, training stage significantly improves composition performance over BAGEL [7], showing the importance of singlecandidate data. The 70K base set provides substantial gains, and the refined 22K set further boosts overall performance of both prompt following and subject consistency, highlighting that importance of data quality. 7 Figure 6. Qualitative comparison of existing models on OmniContext [37] benchmark. Figure 7. Qualitative comparison of existing models on SconeEval benchmark. Effects of understanding bridge strategy in stage II As shown in Tab. 5, we compare three versions in stage II, (a) direct fine-tuning of both the understanding and generation experts, (b) and (c) first fine-tuning the understanding expert and then fine-tuning both experts, with differences in the application of the understanding bridge strategy. To ensure fair comparison, all three versions train for 2k steps, with the two-step strategies using 1k steps for each step. As shown in Tab. 5, the two-step strategy outperforms the direct strategy. With the bridge, the model achieves higher scores, which confirms that the strategy strengthens subject distinction and improves overall robustness. Table 5. Ablation results for stage II. Evaluated on SconeEval benchmark. COM denotes composition and DIS denotes distinction. Best scores are in bold. Stage Version Base BAGEL [7] Stage - Stage II (a) Direct (b) Two-step, w/o bridge (c) Two-step, w/ bridge (Ours) COM DIS Overall 6.74 7.94 7.64 8.15 8.21 7.20 7.78 8.23 8.70 8. 6.97 7.86 7.94 8.43 8.50 8 Figure 8. Stability measured by the standard deviation of scores on the SconeEval Benchmark. 5.6. Discussion Stability Subject-driven generation in complex contexts remains difficult because of semantic interference, multicandidate interference, and unstable subject preservation. Distractors in reference images often cause models to misidentify the target subject, which results in omissions or redundancies. Our Scones lowest standard deviation of scores on the SconeEval benchmark shown in Fig. 8 indicate its stable performance. External understanding model VS. Our end-to-end unified model External understanding modules add computation and break the continuity of optimization, increasing latency and limiting semantic adaptation to downstream generation. Our unified architecture learns task-relevant semantics and aligns them dynamically with generation, offering higher efficiency and lower overhead. Empirically, it achieves faster processing and improved accuracy without auxiliary models or multi-stage inference. 6. Conclusion and future direction We introduce Scone, unified understanding-generation framework that reveals the neglect problem in existing subject-driven methods, which is distinguishing target subjects in multi-candidate contexts. By transforming the understanding expert into semantic bridge, our model aligns semantics early and filters irrelevant content, guiding the generation expert toward accurate subject preservation and robust composition. Together with the SconeEval benchmark, our method provides comprehensive solution for evaluating and improving both composition and distinction. Future work focuses on developing more efficient mechanisms to reduce redundant image tokens, enabling scalable subject-driven generation in complex scenarios."
        },
        {
            "title": "References",
            "content": "[1] Ruichuan An, Sihan Yang, Ming Lu, Renrui Zhang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, et al. Mc-llava: Multi-concept personalized vision-language model. arXiv preprint arXiv:2411.11706, 2024. 1 [2] Ruichuan An, Sihan Yang, Renrui Zhang, Zijun Shen, Ming Lu, Gaole Dai, Hao Liang, Ziyu Guo, Shilin Yan, Yulin Luo, et al. Unictokens: Boosting personalized understand9 ing and generation via unified concept tokens. arXiv preprint arXiv:2505.14671, 2025. 3 [3] ByteDance. Seeddream 4.0, 2025. 1 [4] Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu. Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation. arXiv preprint arXiv:2506.21416, 2025. 5 [5] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 3 [6] Zhihong Chen, Xuehai Bai, Yang Shi, Chaoyou Fu, Huanyu Zhang, Haotian Wang, Xiaoyan Sun, Zhang Zhang, Liang Wang, Yuanxing Zhang, et al. Opengpt-4o-image: comprehensive dataset for advanced image generation and editing. arXiv preprint arXiv:2509.24900, 2025. [7] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 1, 3, 6, 7, 8, 12, 13 [8] Google. Introducing gemini 2.5 flash image, our state-ofthe-art image model, 2025. 1, 6, 7, 12 [9] Zinan Guo, Pengze Zhang, Yanze Wu, Chong Mou, Songtao Zhao, and Qian He. Musar: Exploring multi-subject customization from single-subject dataset via attention routing. arXiv preprint arXiv:2505.02823, 2025. 6 [10] Chanran Kim, Jeongin Lee, Shichang Joung, Bongmo Kim, Instantfamily: Masked attention arXiv preprint and Yeul-Min Baek. for zero-shot multi-id image generation. arXiv:2404.19427, 2024. 3 [11] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1226812290, 2024. 6 [12] Black Forest Labs. Flux, 2024. 5, 6 [13] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 3, 6, 7 [14] Jie Lei, Tamara Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 487507, 2023. 2 [15] Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, and Li Yuan. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025. 3, 6, 7 [16] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 3 [17] Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, and Hongsheng Li. Perceive anything: Recognize, explain, caption, and segment anything in images and videos. arXiv preprint arXiv:2506.05302, 2025. 1 [18] Wenxuan Liu, Yao Deng, Kang Chen, Xian Zhong, Zhaofei Yu, and Tiejun Huang. Sota: spike-navigated optimal transport saliency region detection in composite-bias videos. In Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence, 2025. 2 [19] Wenxuan Liu, Xian Zhong, Yihan Dai, Xuemei Jia, Zheng Wang, and ShinIchi Satoh. Motion-consistent representation learning for uav-based action recognition. IEEE Transactions on Intelligent Transportation Systems, 2025. 1 [20] OpenAI. Hello gpt-4o, 2025. 6, 12 [21] OpenAI. Introducing gpt-4.1 in the api, 2025. 6 [22] OpenAI. Introducing 4o image generation, 2025. 6, 7 [23] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4 [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [25] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned bencharXiv preprint mark for personalized image generation. arXiv:2406.16855, 2024. 5 [26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4 [27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 5 [28] Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, and Kaicheng Yu. Dualtoken: Towards unifying visual understanding and generation with dual visual vocabularies. arXiv preprint arXiv:2503.14324, 2025. 3 [29] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1494014950, 2025. 3 [30] Bingda Tang, Boyang Zheng, Sayak Paul, and Saining Xie. Exploring the deep fusion of large language models and diffusion transformers for text-to-image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2858628595, 2025. [31] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. IEEE Transactions on Circuits and Systems for Video Technology, 2025. 2 [32] Jing Tao, You Li, Banglei Guan, Yang Shang, and Qifeng Yu. Simultaneous enhancement and noise suppression under complex illumination conditions. IEEE Transactions on Instrumentation and Measurement, 73:111, 2024. 1 [33] Qwen Team. Qwen3 technical report, 2025. 5, 6, 12, 13 [34] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 3 [35] Yuran Wang, Zhijing Wan, Yansheng Qiu, and Zheng Wang. Devil is in details: Locality-aware 3d abdominal ct volume generation for self-supervised organ segmentation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1064010648, 2024. 1 [36] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 1, 5, 6, 7, 12 [37] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 1, 3, 5, 6, 7, 8, 13 [38] Shaojin Wu, Mengqi Huang, Yufeng Cheng, Wenxu Wu, Jiahe Tian, Yiming Luo, Fei Ding, and Qian He. Uso: Unified style and subject-driven generation via disentangled and reward learning. arXiv preprint arXiv:2508.18966, 2025. 6, [39] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. 1, 6, 7 [40] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 1, 3, 6 [41] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3 10 [42] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. ShowarXiv Improved native unified multimodal models. o2: preprint arXiv:2506.15564, 2025. 3 [43] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. [44] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, and Weijia Li. Echo-4o: Harnessing the power of gpt-4o synthetic images for improved image generation, 2025. 1, 3, 6, 7 [45] Bohan Zeng, Shanglin Li, Yutang Feng, Ling Yang, Juan Zhang, Hong Li, Jiaming Liu, Conghui He, Wentao Zhang, Jianzhuang Liu, et al. Ipdreamer: Appearance-controllable 3d object generation with complex image prompts. In The Thirteenth International Conference on Learning Representations, 2024. 3 [46] Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Chuchu Han, Xiaonan Huang, Changxin Gao, Yuehuan Wang, and Nong Sang. Holmes-vad: Towards unbiased and explainable video anomaly detection via multi-modal llm. arXiv preprint arXiv:2406.12235, 2024. 2 [47] Tao Zhang, Chenglin Zhu, Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Fan Yang, Mingan Lin, Yujing Qiao, Weipeng Chen, et al. Cfbench: comprehensive constraintsfollowing benchmark for llms. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3292632944, 2025. 1 [48] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80698078, 2024. 3 [49] Zhi Zhang, Srishti Yadav, Fengze Han, and Ekaterina Shutova. Cross-modal information flow in multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1978119791, 2025. 2, 3, 11 Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional details of motivation The similarity visualizations of instruction token hidden states and image token hidden states from the understanding and generation experts in Fig. 1(b) and Fig. 2(a) are based on experiments with our base model, BAGEL [7]. To better observe the regions with high similarity, we retain the top 50% of regions and generate masked images. We group the layers into four sets (0-7, 8-15, 16-23, 24-27) based on the layer function analysis from [49]. In Fig. 10, we further present representative similarity and masked images for each layer group. Observation 1 (Comparison between understanding and generation experts) The understanding expert provides more distinct semantic image information than the generation expert, with the image token hidden states from the understanding expert showing higher similarity with instruction token hidden states. It focuses on instruction-relevant regions such as candidate subjects. Observation 2 (Comparison in different layers of understanding expert) While the average similarity in the understanding expert remains higher at layers 16 and 24, the distinction between regions is most pronounced at layer 8, which provides more distinctive semantic cues for generation guidance. Therefore, we choose the most semanticdistinct layer 8 as the early layer providing the semantic mask, and the later layers influenced by the semantic mask are the group with the strong semantic distinction, i.e. 915. B. Additional details of training data B.1. Synthesized data for data pool As Sec. 5.1 describes, we use Gemini-2.5-Flash-Image [8] to synthesize 15K samples with 3 to 4 input images to fill gaps in the data pool. These samples enhance the composition capabilities of our Scone. The synthesized data examples are shown in Fig. 12 (4 case types) and Fig. 13 (9 case types). B.2. Data filtering for refined single-candidate data As Sec. 5.1 describes, refined single-candidate samples are filtered by scoring subject consistency and instruction following using the VLM model Qwen3-VL-30B-A3BInstruct. The important contents of our prompt are shown in Fig. 14(a), with special attention given to aspects like Figure 9. Prompt for distinction scoring in SconeEval benchmark. It determines whether the described subject from the reference image appears in the target image. facial identity, text, and quantities. Each sample is scored from 0 to 4, and only samples with score of 4 are selected, as shown in Fig. 14(b). B.3. Details of multi-candidate data Single-subject data Multi-candidate single-subject data are derived from single-candidate multi-subject data by reversing the order of the original reference and target images. Specifically, the original reference image becomes the new target image, and the original target image becomes the new reference image. This method reduces the cost of generating new images. For instruction construction, we use Qwen3-30B-A3B-Instruct-2507 [33] to identify subjects, provide distinct descriptions, and generate instructions based on the prompt shown in Fig. 15(a). The final dataset includes 2 case types, each containing both cross-category and intra-category candidate subjects in the reference images. Examples are provided in Fig. 15(b). Multi-subject data Multi-subject data are created from single-candidate multi-subject data by editing some of the reference images. We use GPT-4o [20] to generate prompts for subjects across different categories and add at least one subject to the reference images using Qwen-Image-Edit2509 [36]. The instruction construction involves two steps: step 1, subject identification, and step 2, subject replacement. Subject identification follows the method outlined in Sec. 4.2, with further details in Sec. C. Subject replace12 Figure 10. Representative similarity and masked images for each layer group. The similarity visualizations of instruction token hidden states and image token hidden states from understanding and generation experts are based on experiments with our base model, BAGEL [7]. The masked images are obtained by retaining the top 50% of regions for better observation. Table 6. Parameter study of threshold in stage II. Evaluated on SconeEval benchmark. COM denotes composition and DIS denotes distinction. Best scores are in bold. Threshold τ COM DIS Overall w/o bridge 0.82 0.85 0.88 8.15 8.18 8.19 8.21 8.70 8.73 8.75 8. 8.43 8.46 8.47 8.50 Figure 11. Limitation of our Scone. E. Limitation Our Scone still exhibits common limitation found in existing methods: unrealistic interaction. As shown in Fig. 11, the dog passes through the chair in the generated image from our Scone, violating physical laws. This illustrates an unrealistic interaction between the subjects and their environment in the generated images of both our Scone and OmniGen2 [37]. ment replaces the original subject description with the distinct description obtained from step 1, using Qwen3-30BA3B-Instruct-2507 [33] and the prompt in Fig. 16(a). The final dataset includes 5 case types, each containing both cross-category and intra-category candidate subjects in the reference images. Examples are shown in Fig. 16(b). C. Two-step decoupling instruction construction in SconeEval benchmark As described in Sec. 4.2, we adopt two-step decoupling instruction construction strategy, separating visual understanding from instruction generation. This reduces crossimage interference and improves instruction reasonability. As shown in Fig. 17, the direct instruction construction strategy results in unusable instructions, with errors such as incorrect reference image indices, ambiguity of target subjects, and the introduction of unrelated subjects. Our two-step decoupling strategy first uses the visionto language model Qwen3-VL-30B-A3B-Instruct identify and provide distinct description of the target subject based on the raw single-candidate reference image and the edited multi-candidate reference image, with prompt in Fig. 18(a). It then employs the language model Qwen330B-A3B-Instruct-2507 [33] to generate instructions using only the subject descriptions, with prompt in Fig. 18(b). [33] D. Parameter study of threshold in stage II We conduct parameter study of the threshold τ in training stage II, as described in Eq. (4). This parameter influences the number of reference image tokens that remain invisible to the target image tokens in the generation expert. As shown in Tab. 6, improvements with different threshold values (0.82, 0.85, 0.88) increase steadily as irrelevant token interference decreases. Moreover, performance improves with the introduction of the bridge strategy (w/o bridge vs. others). This highlights the robustness and effective semantic guidance of our understanding bridge strategy. 13 Figure 12. Examples of synthesized data with 3 input images. This includes 4 case types, such as combinations of characters interacting with each other and objects, characters with multiple objects, characters in scene, and different objects placed within scene. Figure 13. Examples of synthesized data with 4 input images. This includes 9 case types, such as combinations of multiple characters, characters interacting with objects, different objects grouped together, characters in scene, and objects placed within scenes, as well as various mixes of these elements. 14 Figure 14. Data filtering for refined single-candidate data. (a) Prompt for training data filtering. Key components of the prompt are shown. (b) Results of training data filtering. Data is scored from 0 to 4, and only samples with score of 4 are selected. Figure 15. Multi-candidate single-subject data construction. (a) Prompt for instruction construction. The prompt instructs the vision-language model to identify subjects, provide distinct descriptions, and generate instructions. (b) Example demonstration. This includes 2 case types: Character and Object, each containing both cross-category and intra-category candidate subjects in the reference images. 15 Figure 16. Multi-candidate multi-subject data construction. (a) Prompts for subject replacement. The prompt instructs the language model to replace the original subject description with new, distinct description corresponding to the new multi-candidate reference images. (b) Example demonstration. This includes 5 case types: Character+Character, Character+Object, Object+Object, Character+Scene, and Object+Scene, each containing both cross-category and intra-category candidate subjects in the reference images. 16 Figure 17. Comparison between two-step decoupling and direct strategies for instruction construction. The two-step decoupling strategy separates the process into an image-to-text step and text-to-text step, reducing cross-image interference and avoiding errors such as incorrect reference image indices, ambiguity of target subjects, and the introduction of unrelated subjects, which occur in the direct strategy. Figure 18. Prompts for instruction construction in SconeEval benchmark. (a) Prompt for subject identification. For Character or Object images, provide clear and concise description; for Scene images, describe the overall setting and key objects. (b) Prompt for instruction generation. Generate instructions based on the provided subject descriptions, emphasizing interactions between subjects and between subjects and the scene."
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology",
        "Peking University"
    ]
}