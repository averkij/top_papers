{
    "paper_title": "V-Thinker: Interactive Thinking with Images",
    "authors": [
        "Runqi Qiao",
        "Qiuna Tan",
        "Minghan Yang",
        "Guanting Dong",
        "Peiqing Yang",
        "Shiqiang Lang",
        "Enhui Wan",
        "Xiaowan Wang",
        "Yida Xu",
        "Lan Yang",
        "Chong Sun",
        "Chen Li",
        "Honggang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising \"Thinking with Images\" paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 0 6 4 4 0 . 1 1 5 2 : r V-Thinker: Interactive Thinking with Images Runqi Qiao1* , Qiuna Tan1* , Minghan Yang1, Guanting Dong1, Peiqing Yang1, Shiqiang Lang1, Enhui Wan1, Xiaowan Wang1, Yida Xu1, Lan Yang1, Chong Sun2, Chen Li2, Honggang Zhang1 1Beijing University of Posts and Telecommunications 2WeChat Vision, Tencent Inc. qrq@bupt.edu.cn qiunatan@bupt.edu.cn https://github.com/We-Math/V-Thinker"
        },
        {
            "title": "Abstract",
            "content": "Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains long-standing witness in this field. Recent advances in vision-centric reasoning explore promising Thinking with Images paradigm for LMMs, profoundly shifting from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by narrow visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, generalpurpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensionsdiversity, quality, and difficulty; and (2) Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications. 1. Introduction \"The soul never thinks without an image.\" Aristotle *Equal contribution. Work done as interns at WeChat, Tencent Inc. Corresponding author. zhhg@bupt.edu.cn; chaselli@tencent.com Figure 1. The three paradigms of vision-centric reasoning. interactions, Humans often simplify complex problem-solving via heuristic multi-modal especially in vision-centric reasoning [53, 67]. In challenges like geometric proofs, people interactively add auxiliary lines or sketches [19, 54], modeling visual relations to solve geometric problems more intuitively. In recent years, visual language models (LMMs) demonstrate exceptional performance in stepwise reasoning tasks, fostering expectations for their development into interactive, image-oriented thinking [5, 7, 15, 21, 37, 51, 70, 73]. However, despite producing lengthy and coherent chain-of-thought (CoT), these models often detach from visual grounding, leading to hallucinations [20, 28, 30]. This indicates that current visual reasoning depends more on linguistic priors than on visual perception. To address these challenges, OpenAIs o3 model first actively interacts with images during reasoning via visual tools (e.g., cropping, rotation), shifting the paradigm from vision-assisted reasoning to vision-centric thinking [39, 48]. Building on this milestone, recent efforts attempt to reproduce o3-like interactive thinking through end-to-end visual reinforcement learning [60, 81]. Moreover, Thyme [78] autonomously generates executable code to render diverse visual operations on images throughout reasoning. However, the available visual actions of them are still limited and heavily depend on precise spatial localization. To further expand the visual tool space and broaden interactive thinking patterns, foundational works such as DeepSketcher [63, 72] enable LMMs to autonomously add auxiliary lines within images, explicitly modeling logical relations between visual elements. While insightful for vision-centric reasoning, their tool designs are often tightly coupled to specific task types. Moreover, heavy reliance on \"Image2Code\" pipelines for image editing struggles to accurately depict spatial relationships between visual elements and may introduce extra noise [17, 75]. This raises fundamental research question: How can we effectively integrate image interaction into the visual chain-of-thought process? In this paper, we introduce V-Thinker, general-purpose multimodal reasoning assistant that fosters interactive vision-centric thinking via end-to-end reinforcement training. As shown in Figure 1, V-Thinker aims to simplify complex problems via autonomous interaction with images, thereby advancing the next generation of vision-centric reasoning paradigms. Specifically, V-Thinker introduces comprehensive vision-centric post-training paradigm, comprising two key components: the \"data evolution flywheel\" and \"progressive interactive training curriculum\" spanning perception to reasoning. Data Evolution Flywheel. An ideal vision-centric interactive reasoning model should generalize across real-world diverse tasks. To this end, V-Thinker targets three fundamental dimensions of interactive reasoning data synthesis: (1) Diversity. We directly generate QA pairs using knowledge concepts and visual tool systems, shifting from data expansion to genuine data creation. Then, the flywheel iteratively enlarges both the concept and visual tool sets using newly synthesized data, thereby sustaining continuous stream of diverse datasets. (2) Quality. To ensure strict quality control, we implement coordinated calibration mechanism where data checker rigorously screens textual, visual, and image-action dimensions while repairer calibrates annotations across modalities, ensuring high fidelity. (3) Difficulty. Building on the preceding stages, we further introduce progressive expansion stage that enriches the difficulty ladder by incorporating parallel and sequential extension strategies. Through three-stage data synthesis flywheel, we obtain high-quality visual interactive reasoning dataset, namely V-Interaction-400K. Visual Progressive Training Curriculum. Following the data evolution flywheel, we design two-stage curriculum that progressively builds robust perception and then aligns it with interactive reasoning capabilities: (1) Perception Alignment. We model the visual space through three key dimensions and synthesize perception-specific datasets to enhance the LMMs localization capabilities, named V-Perception40K. Furthermore, the LMM is fine-tuned on this dataset to align the localization and referencing of visual anchors. (2) Interactive Reasoning Alignment. Built upon the perceptual foundation, we implement cold-start supervised fine-tuning followed by RL within sandboxed executor environment, enabling the LMM to autonomously generate executable code that interacts with visual elements and maintains coherent reasoning chains grounded in visual evidence. To thoroughly evaluate LMMs visual-interactive reasoning capabilities, we introduce VTBench, benchmark targeting tasks that inherently demand visual interaction. Each instance is verified by domain experts and sourced from diverse public datasets. In summary, the key contributions are as follows: We formalize Interactive Thinking with Images and introduce V-Thinker, an end-to-end multimodal reasoner that bridges visual grounding and interactive thinking through code-driven visual tools. We propose Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions: Diversity, Quality, and Difficulty, and further release large-scale, highquality visual interaction dataset, V-Interaction-400K, which can also be extended to image-to-code and other vision-language tasks. We introduce Visual Progressive Training Curriculum that first aligns perception via point-level supervision using high-quality dataset V-Perception-40K, and then aligns interactive reasoning through two-stage curriculum training framework. We introduce VTBench, an expert-reviewed benchmark featuring standardized protocols. Extensive experiments show that V-Thinker consistently outperforms mainstream LMM-based baselines on both interactive and general reasoning scenarios, providing valuable insights for advancing image-interactive reasoning. Figure 2. Representative examples of V-Thinkers knowledge-driven synthesis spanning diverse reasoning domains. 2. Related Work Multimodal Reasoning. Recent advances in large language models (LLMs) and multimodal large language models (MLLMs) have significantly enhanced their reasoning capabilities across diverse tasks [2, 3, 814, 22, 23, 29, 41, 42, 47, 49, 50, 52, 58, 65]. Recent efforts such as MathVista [34], MathVision [56], MathVerse [76], We-Math [40], Dynamath [86], LogicVista [66], and VisuLogic [68] have introduced comprehensive benchmarks that systematically evaluate model performance across mathematical, logical, and visual reasoning scenarios. Methodologically, prior studies have improved visualtextual alignment [46, 57, 77], incorporated step-wise reasoning [35, 85], and explored RLbased optimization to strengthen multimodal reasoning [1, 4, 25, 33, 36, 43, 55, 69, 74, 82]. In particular, RL-based frameworks such as MM-Eureka [37] and VisionR1 [25] have introduced reinforcement learning into visual reasoning, revealing new possibilities for enhancing model reasoning depth. These works have laid foundation for the continued advancement of visual reasoning. Thinking with Images. Interactive visual reasoning is long-term research vision. Early explorations, such as LLaVA-Plus [31] and Visual Sketchpad [24], pioneered this direction by enabling models to conduct visual operations during reasoning. These studies laid an important foundation for integrating interactive perception into multimodal reasoning systems. With the growing success of reinforcement learning (RL) in visual reasoning, OpenAIs o3 [39] introduced the concept of thinking with images. Subsequent works [6, 26, 27, 62, 64, 71, 72, 80, 83, 84] have further developed this paradigm, provides fresh perspective for the field of agentic RL. In particular, DeepEyes [81] and Thyme [79] employ RL-based training to guide models in performing executable visual tools (e.g., cropping) as part of their reasoning chain. Meanwhile, DeepSketcher [72] explores implicit visual reasoning, where models leverage abstract visual cues instead of explicit pixel-level manipulation. Building on this progress, our work advances interactive thinking with images by enabling the model to autonomously generate, execute, and iteratively refine visual code during reasoning. This opens new perspective on integrating visual interaction into reasoning, paving the way toward more intuitive and human-like multimodal cognition. 3. Preliminary 3.1. Problem Formulation Interactive Thinking with Images. We envision an ideal vision-centric reasoning paradigm where reasoning unfolds through progressive interaction with the imageby perceiving, modifying, and reflecting on its visual state. Building upon this idea, V-Thinker treats reasoning as code-driven visual interaction process. At each reasoning step, the model generates textual thought rt and, when necessary, code segment ct that operates on the current image It. The environment executes ct, resulting in an updated image It+1. Formally, : (Q, I0) (cid:55) (R, A), = {(rt, ct, It+1)}T t=1, It+1 = E(It, ct) where is the task query, the reasoning trajectory, and the final answer. Through this interactive process, the model reasons by generating code to modify the image and leveraging the resulting visual feedback to guide subsequent reasoning. 3.2. Rethinking on Data Synthesis Paradigm Traditional vision-centric reasoning datasets are built upon manually defined tasks, where models act as solvers generating chain-of-thought reasoning and answers. This data synthesis paradigm limits diversity and scalability, while interactive reasoning requiring precise spatial and logical alignment remains challenging. With the growth of modern model generation capabilities, we re-examine this paradigm from new perspective: In data synthesis, can models evolve from solving problems to creating problems? To end this, we explore this paradigm from two directions. (1) Role Transformation: from \"solver\" to \"creator\" Traditional data synthesis methods employ models as solvers to distill their reasoning trajectories, based on the core assumption that models cannot directly generate high-quality multimodal QA pairs, particularly those involving complex geometric shapes and spatial relationships. However, this approach constrains the diversity and solvability of synthesized samples. In this work, we revisit the role of vision-language models in data synthesis, revealing that existing strong LMMs are capable of reaching the creator level in data synthesis. As shown in Figure 3, GPT-5 can directly generate Python code to render high-quality original images along with corresponding auxiliary line diagrams and reasoning trajectories. Figure 2 shows representative rendered image comparisons. The synthesis quality far exceeds our initial expectations. The exceptional coding capabilities enable finegrained, outstanding image editing, such as highlighting chemical elements and annotating vertical guide lines, directly creating image annotations. Therefore, we shift from the distillation paradigm, enabling innovative generation of complex problems, diagrams, and reasoning trajectories rather than passively responding to predefined tasks. (2) Knowledge-Driven Representation. Knowledge concepts serve as anchors for human knowledge, where an ideal comprehensive knowledge system can generate reasoning data covering diverse scenarios. We therefore move beyond using seed samples as references and instead employ structured knowledge systems, where knowledge concepts serve as condensed representations of reasoning semantics, capturing diverse real-world scenarios. Knowledge specifies what to reason about, while models determine how to unfold reasoning through interaction. As shown in Figure 2, by providing only knowledge concepts and their descriptions, knowledge-driven synthesis produces problems with precise spatial alignment, coherent reasoning, and diverse visual structures, broadening the construction and evolution of reasoning data. 4. Methodology Overview. This section introduces V-Thinker, generalpurpose multimodal reasoning assistant that enables interacFigure 3. The rendering process from code to image. tive vision-centric reasoning through end-to-end reinforcement learning. As shown in Figure 4, V-Thinker comprises two core components: 1. Data Evolution Flywheel (4.1): We automatically synthesize, evolve, and verify interactive reasoning datasets across three dimensions: Diversity, Quality, and Difficulty. 2. Visual Progressive Training Curriculum (4.2): We introduce two-stage training framework to achieve progressive alignment from perception understanding to visioncentric interactive reasoning patterns. Below, we delve into the specifics of our approach. 4.1. Data Evolution Flywheel Building on the knowledge-driven paradigm outlined in 3.2, we design the Data Evolution Flywheel, an automated, scalable, and verifiable framework for synthesizing interactive reasoning data. The framework is divided into three processes (as shown in Figure 4): Knowledge-driven Evolution (4.1.1) generates reasoning data through the co-evolution of knowledge and tool sets, iteratively expanding both to yield diverse problems and reasoning trajectories. Coordinated Calibration (4.1.2) verifies the correctness of generated questions, rendered images, and edited visual states, ensuring consistency across text, code execution, and visual outcomes. Progressive Expansion (4.1.3) enhances reasoning difficulty via iterative step extension and compositional knowledge integration, gradually constructing more complex and challenging reasoning chains. Figure 4. The Data Evolution Flywheel framework: Left: knowledge-driven evolution mechanism. Middle: coordinated calibration and progressive expansion stages. Right: representative synthetic QA instances generated through the flywheel. 4.1.1. Diversity: Knowledge-driven Evolution Algorithm 1 Constructive Evolution for Dataset Synthesis Prior data synthesis methods rely on curated seed images, inherently constraining diversity. We propose that foundational knowledge concepts and visual tool system serve as the most granular anchors for synthesizing interactive data, enabling orthogonal and diverse generation. Our pipeline avoids distillation and instead instructs strong LMM to create datasets from scratch. Following this guideline, given an initial knowledge system K0 derived from We-Math 2.0 [43] and curated taskcentric tool set T0, knowledge and tools jointly drive iterative data synthesis through co-evolutionary loop (Algorithm 1). At each iteration n, sampled combinations Combos(K) from the current knowledge system are provided to the strong generator (e.g. GPT5) to construct reasoning data: (DK, ˆT ) = G(Combos(K)). (1) Each generated instance contains question Q, the original code c0 that renders the problem-specific image I0, the corresponding tool predictions ˆT , and reasoning trajectory = {(rt, ct, It+1)}TR t=1, where each executable code ct renders the visual component It+1. Figure 3 illustrates the image rendering process from code to image. In parallel, tool-driven generation applies the same procedure on Combos(T ), producing data DT and predicted knowledge ˆK: (DT , ˆK) = G(Combos(T )). (2) Require: Initial knowledge set K0, tool set T0, generator G, expansion rules ΦK , ΦT , iterations Ensure: Evolved knowledge K, tools , and dataset Dinit 1: = K0, = T0, Dinit = 2: for = 1, 2, . . . , do 3: (DK , ˆT ) G(Combos(K)) Initialize from scratch Generate QA and predicted tools (DT , ˆK) G(Combos(T )) 4: Generate QA and predicted knowledge concepts Dinit Dinit DK DT ΦK ( ˆK, K) ΦT ( ˆT , ) K, 5: 6: 7: 8: 9: end for 10: Output: = K, = , Dinit Accumulate new samples Knowledge expansion Tool expansion integrated via expansion functions ΦK and ΦT : ΦK( ˆK, K), ΦT ( ˆT , ), (3) where ΦK and ΦT filter, merge, and normalize novel elements during incorporation via BGE-based hierarchical clustering. The co-evolution mechanism among synthetic data, knowledge concepts, and visual tool systems is illustrated in the Figure 4 (left). Through repeated execution over rounds, the system gradually enriches both and , ultimately yielding an initial dataset Dinit that serves as the foundation for subsequent calibration and expansion stages. Together, these complementary processes constitute coevolutionary loop in which and continuously generate new counterparts. The newly predicted elements are then 4.1.2. Quality: Coordinated Calibration After obtaining the synthesized diverse dataset Dinit, strict quality control is essential. Therefore, we introduce regulative calibration stage to ensure multi-level consistency across generated samples. This stage consists of two modules. Checker. As shown in Figure 4 (Mid), each instance is examined by data checking module that verifies (1) answer correctness, (2) validity of the rendered original image, and (3) coherence of intermediate visual states produced during reasoning. Only samples satisfying all three criteria are retained as valid candidates. Repairer. For cases where the textual answer is incorrect but the rendered image is valid and intermediate visual states remain coherent, we follow the principle that reasoning chain is fundamentally guided by its question, reconstructing the question from original and edited visual states to realign textual and visual reasoning. The reconstructed instances are re-evaluated by V, and the loop repeats until inconsistencies are resolved. Through the iterative calibration of data checking and repair, we refine Dinit into coherent and verified dataset Dverified, which serves as the foundation for subsequent progressive expansion. 4.1.3. Difficulty: Progressive Expansion To establish difficulty-stratified data partitions and deepen reasoning chain complexity, our intuition is to extend the CoTs context length to achieve difficulty escalation, thereby inversely mapping more challenging QA pairs. This naturally motivates us to introduce two complementary strategies to progressively expand the difficulty: parallel and sequential extensions, constructing more challenging QA pairs: (1) Parallel Extension. New auxiliary constructions are introduced independently of the existing ones, providing additional key observations that complement the original reasoning to reach the final answer. (2) Sequential Extension. New auxiliary constructions are closely linked to the existing reasoning, requiring prior results or original geometric entities to define subsequent operations. For example, if the original auxiliary line is DM , new line perpendicular to DM can be introduced to support further deductions. Under these two strategies, subset of verified data Dverified is sampled and provided to the expansion model, which generates extended reasoning code segments and corresponding visual states. The generated extensions are revalidated by the verification module and iteratively refined until convergence. We limit the maximum extension depth to three steps and merge all results to form the final dataset D. Figure 5 display representative sample from the synthesized dataset D. 4.2. Visual Progressive Training Curriculum For reliable vision-centric interactive reasoning, accurate perception of visual elements is crucial. While recent multimodal models demonstrate strong reasoning ability, they Figure 5. representative sample from the synthesized dataset V-Interaction-400K (D). often struggle with fine-grained spatial perception, failing to precisely localize points, intersections, and other anchors. To address this, we develop visual progressive training curriculum that aligns perception and interaction, starting with perceptual grounding through point-level supervision ( 4.2.1) and advancing to interactive reasoning via progressive alignment training ( 4.3). 4.2.1. Perception Alignment Perception Data Synthesis. critical aspect of perception training is synthesizing perception-specific data. Following the paradigm in Section 3.2, we model visual space via three dimensions: element relations, element count, and knowledge concepts. These aspects jointly define visual information, with style governed by knowledge and complexity by element count. Element Relations (PE ): We model visual elements such as points, lines, angles, and circles, based on foundational geometry principles [18], and extend this set with textual and symbolic elements for alignment with real-world scenarios. This modeling integrates both element types and their spatial relationships, such as \"point on line\" and \"point outside circle,\" capturing the core geometric interactions in visual reasoning. Element Count (PC): We sample the number of elements from normal distribution (µ = 8, σ2 = 4) to control 4.3. Interactive Reasoning Alignment Data Selection. For comprehensive training, we select data from the following sources: SFT Dataset: In the Cold-Start Fine-tuning stage, we first construct the Dperception dataset to fine-tune V-Thinker with fine-grained perception capabilities, and then align to equip it with basic interactive reasoning abilities. RL Dataset: To progressively align V-Thinker for robust interactive reasoning, we consider the following two components of the dataset: (1) Open-Source Samples: We use the We-Math 2.0 [43], MMK12 [37] and ThinkLite [59], which provide broad range of visual reasoning data, from basic to complex, ensuring comprehensive coverage of visual reasoning tasks. (2) Targeted Sampling from D: We select data from where the base model gives incorrect answers for the original image but correct answers for the edited version. total of 3,000 instances are sampled from D, ensuring mix of simpler and more complex data, aimed at improving the models reasoning ability with visual states. Cold-Start Fine-tuning To enable the model to generate code and interact with visual elements, we apply standard supervised fine-tuning objective on the dataset D, consisting of input-output pairs (xi, yi), where xi includes both the problem statement and the corresponding visual information, and yi represents the reasoning chain with Python code and the edited visual elements. The objective is to minimize the loss function: LSFT(θ) = E(xi,yi)D [log Pθ(yi xi)] (5) This phase initially enables the model to generate code that reads and interacts with images to perform reasoning tasks. Reinforcement Learning for Interactive Reasoning We apply reinforcement learning (RL) to enhance the models interactive reasoning capabilities. Building on the Thyme [79], we use its sandbox environment to execute the models generated code. The model decodes reasoning tasks into executable code, which interacts with the visual elements, and the resulting outputs are fed back into the reasoning process. For optimization, we adopt Group Relative Policy Optimization (GRPO) [45], which has been shown to be effective for diverse tasks. Given an input question and policy model πθ, GRPO enables the reference policy πref to generate group of outputs {y1, y2, . . . , yG} and optimizes the policy by maximizing: Figure 6. The overview of the perception data synthesis. task complexity, where µ represents the mean and σ2 the variance. Knowledge Concepts (PK): We sample from the knowledge system to define reasoning objectives for each task. As shown in Figure 6, each combination of these dimensions generates corresponding coordinates, which serve as visual tags representing the spatial relationships between elements. Using these coordinates, we then structure the tasks into three levels of complexity: surface-level perception, semantic-level reasoning, and integrated reasoning: Surface-level perception: Basic tasks, such as identifying the coordinates of specific point, for example, point A. Semantic-level reasoning: Tasks requiring geometric understanding, such as identifying the top-left vertex of cube. Integrated reasoning: Tasks combining perception and computation, such as finding the center of cube based on its dimensions. This hierarchical structure generates diverse questionanswer pairs, forming the dataset Dperception, which spans all complexity levels and enhances perceptual capabilities. Perception Training. The model is trained using supervised fine-tuning, with the objective of minimizing the loss function: LSFT(θ) = E(Q,A)Dperception [ log Pθ(A Q)] (4) LRL(θ) = ExD This process enhances the models ability to process visual information, particularly for tasks involving point-level localization. (cid:34) 1 (cid:80)G j=1 Tj (cid:88) Tj (cid:88) j=1 t=1 (cid:16) min δj,t A(t), clip (δj,t, 1 ϵl, 1 + ϵh) A(t)(cid:17) (cid:35) , (6) where δj,t = πθ(yj,tx,yj,<t) πref(yj,tx,yj,<t) represents the importance sampling ratio for the t-th token of the j-th output, and A(t) denotes the advantage at time step t. tags are then transformed into QA pairs with GPT-4.1, followed by expert validation to ensure consistency and accuracy. Reward Design. We follow reward function based on the Thyme framework [79], consisting of three components: accuracy (Racc), formatting (Rformat), and tool usage (Rtool). The total reward is defined as: R(τ ) = Racc(τ ) + λ1 Rformat(τ ) + λ2 IRacc(τ )>0 Rtool(τ ) (7) where IRacc(τ )>0 is an indicator function ensuring that the tool usage reward is applied only when the final answer is correct and involves at least one tool. We empirically set λ1 = 0.5 and λ2 = 0.3 to balance the contributions of formatting and tool usage. 5. VTBench To evaluate the models vision-centric interactive reasoning capabilities, we introduce VTBench, benchmark designed for tasks that inherently require interaction with visual elements. The design of VTBench follows the following principles: Specific Interactive Tasks: VTBench prioritizes tasks that inherently require interaction with visual elements, such as drawing auxiliary lines or labeling. Expert Evaluation: Each sample is assessed by team of five experts, who determine whether visual interaction is necessary for solving the problem. The sample is included if at least three experts agree on its necessity. Diversity and Expansion: To ensure broad coverage, we extend the benchmark by collecting and annotating datasets from multiple sources, including both opensource benchmarks and additional tasks from public platforms, following the same annotation procedure. 5.1. Data Collection and Annotation As shown in Figure 7, VTBench is constructed in two stages: Sample Selection: We collect samples from opensource benchmarks: MathVista [34], MathVision [56], MathVerse [76], Dynamath [86], We-Math [40], LogicVista [66], CMM-Math [32], CharXiv [61] and ZeroBench [44], as well as additional samples gathered from public platforms. Expert evaluation is then used to determine whether interaction with visual elements is required. Each sample is included if at least three out of five experts agree on its necessity. Interaction Annotation: We design interaction instructions based on the problem-solving chain of thought for each sample. The expert team manually generates interaction graphs in the interactive interface, which simultaneously captures perceptual coordinates. These interaction 5.2. Evaluation Dimensions VTBench evaluates vision-centric interactive reasoning capabilities across three hierarchical dimensions, modeling the problem-solving process from perception to adaptive interaction during reasoning (in Figure 7): Perception Instruction-Guided Interaction Interactive Reasoning Perception: Tasks that assess the models ability to recognize and interpret visual elements, such as identifying coordinates. Instruction-Guided Interaction: Tasks where the model receives explicit instructions (e.g., drawing lines or labeling) and must interact with the visual elements to fulfill these instructions. Interactive Reasoning: Tasks that require the model to solve reasoning tasks involving visual interaction, such as drawing auxiliary lines or modifying diagrams. 5.3. Data Statistics and Evaluation Metrics VTBench comprises 1,500 question-answer pairs across three task types, with 500 samples per task. It incorporates 9 open-source benchmarks across four domains (Logical Reasoning, Geometry, Algebra, Statistics), categorized into three key evaluation metrics: For Perception Task: Considering the inconsistency in coordinate systems across different models, the model is instructed to generate Python code that draws the point at the perceived location. The generated image, compared with the annotated image, is judged by LMMs. For Instruction-Guided Interaction Task: For tasks that require explicit instructions (e.g., drawing lines or labeling regions), the model is instructed to generate Python code to perform the required visual interaction. The result, compared with the annotated image, is judged by LMMs. For Interactive Reasoning Task: For reasoning tasks, the model generates answers, which are then evaluated by Large Language Models (LLMs) based on correctness. 6. Experiments 6.1. Experimental Setup Datasets. In this paper, we use two constructed datasets for supervised fine-tuning: V-Perception-40K for perceptual alignment and V-Interaction-400K for interactive alignment. In the reinforcement learning stage, 40K samples are sampled from We-Math 2.0 [43], MMK12 [37], ThinkLite [59], and V-Interaction-400K. All datasets are curated in compliance with copyright and licensing regulations. Experiments Figure 7. The construction guideline of our VTBench. Table 1. Overall performance on interactive VTBench(left) and general reasoning (right) benchmarks. Method VTBench General Reasoning Perception Instruction-Guided Interaction Interactive Reasoning MathVision Acc. We-Math Acc. VisuLogic Acc. GPT-4o InternVL3-78B InternVL3-8B LLaVA-OV-1.5-8B InternVL3-2B Qwen2.5-VL-7B V-Thinker-7B (vs Qwen2.5-VL-7B) 2.3 10.8 10.4 9.2 5.6 9.6 18.0 +8.4 3.7 16.0 5.4 9.6 0.8 8.8 34.6 +25.8 38.3 43.4 36 32.0 21.0 32.2 41.8 +9.6 43.8 43.1 29.3 25.6 23.3 23.0 29.3 +6. 68.8 64.2 58.8 56.7 41.7 61.7 62.8 +1.1 26.3 27.7 24.9 23.7 24.3 26.0 26.6 +0.6 are conducted on VTBench and three standard visual reasoning benchmarks: MathVision [56], We-Math [40], and VisuLogic [68]. Baselines. We conduct our experiments based on the Qwen2.5-VL-7B [3] model and compare our method with two categories of baselines: Closed-source models: For example, GPT-4o [38]. Open-source general models: Including models like InternVL3 series [58], LLaVA-OneVision-1.5 series [2] and Qwen2.5-VL series [3]. Our evaluation is performed using VLMEvalKit [16], with modifications made to the API-calling components due to network constraints. per node). Both SFT stages use learning rate of 1 105. The final SFT checkpoint initializes the RL stage, which is trained for one epoch with eight rollouts per iteration, learning rate of 5 107, and warm-up ratio of 0.05. For VTBench evaluation, we employ two models as judges: Qwen3-235B-A22B and Qwen3-VL-235B-A22B. Their results differ by less than 2% from GPT-4.1, validating their reliability as open-source evaluators under unified evaluation protocol. For data construction, We employ GPT-5 as the generator and adopt Qwen3-VL-232B-A22B as the data checking module V. Furthermore, GPT-4.1 is utilized as the repairer and perform progressive expansion. 6.2. Main Results Implementation Details. For training, we conduct all experiments on 64 8 H20 GPUs (i.e., 64 nodes with 8 GPUs To thoroughly analyze the effectiveness of our V-Thinker in the field of interactive reasoning, we conduct experimental analyses from both quantitative and qualitative perspectives: Figure 8. Qualitative analysis of V-Thinker-7B on vision-centric interactive reasoning tasks. Figure 9. Visualization of series of samples in rollout sampling. Table 2. Results of the ablation study. MVs: MathVision; WM: We-Math; VS: VisuLogic. Method M0 M1 M2 SFT (Per.) - SFT (Int.) RL MVs WM VS - 29.3 21.4 28. 62.8 55.5 62.6 26.6 24.8 26.1 Quantitative Analysis. As shown in Table 1, we compare V-Thinker with strong baseline LMMs on VTBench. VThinker demonstrates consistent improvements across tasks requiring vision-centric interactions, yielding the following key insights: (1) Perception Challenges Across LMMs. Despite impressive visual reasoning abilities, perceptual alignment remains critical bottleneck for advanced models. On VTBench, existing LMMs struggle with fine-grained visual interaction tasks, particularly in identifying spatial relationships and localizing precise points on images. While models like GPT-4o and Qwen2.5-VL excel at visual problemsolving, they underperform on tasks demanding direct visual interaction (e.g., Qwen2.5-VL: 8.8%). This reveals substantial gap between general visual reasoning and the specific perceptual grounding required for interactive visual reasoning. (2) Effectiveness of V-Thinker. Under identical experimental setups, V-Thinker consistently outperforms baseline LMMs across all three interactive reasoning domains, achieving an average accuracy improvement of 14.6% and maintaining over 8% gains on individual domains. Notably, it achieves over 25% performance improvement in the Instruction-Guided Interaction domain. These results underscore V-Thinkers superior interactive thinking capability. Qualitative Analysis. To further showcase V-Thinkers beyond-expected performance in interactive thinking, we analyze three aspects: visual interaction editing, rollout sampling behavior, and fully complete cases. (1) Visual Interaction Editing  (Fig. 8)  : On tasks requiring visual interaction, V-Thinker-7B accurately draws squares in diagrams, fully identifying and labeling instances. For arithmetic tasks that do not strictly require interaction, the model not only produces correct answers but also proactively annotates images to clarify intermediate steps. In realworld scenarios (e.g., meal images, multi-person photos), V-Thinker likewise generates instructive visual annotations, demonstrating stable and fine-grained visual interaction editing. (2) Rollout Sampling Analysis  (Fig. 9)  : We quantify sampling results within single rollout step for the same image. The results show that V-Thinker develops multidimensional derivative reasoning and covers broader solution space during RL stage, reflecting stronger exploration and more diverse reasoning paths. (3) Complete Cases  (Fig. 12)  : To illustrate the end-toend interactive thinking process, we present V-Thinkers visual reasoning trajectory. During reasoning, the model autonomously generates image-editing code and immediately renders edited outputs, externalizing intermediate states and simplifying the reasoning chainforming thinkedit loop. Overall, these three lines of evidence jointly demonstrate that V-Thinker exhibits superior and transferable interactive reasoning capabilities. 6.3. Results on Generalized Reasoning Domain To further validate V-Thinkers generalization capability on general reasoning tasks, Table 1 presents its performance across three widely-used benchmarks. V-Thinker-7B demonstrates consistent improvements across three benchmarks, with stable gains in vision-centric mathematics and logical reasoning. Notably, without specific in-domain data introducing, V-Thinker achieves substantial improvements Figure 10. Visualization of the evolved knowledge system through the Data Evolution Flywheel. ing that perception capability serves as crucial foundation for subsequent interactive reasoning. (3) Ablating RL training brings substantial performance degradation, particularly showing over 6% performance drop in Mathvision and WeMath. This indicates that RL stage is essential for exploring interactive reasoning patterns and enabling effective generalization to general reasoning scenarios. 6.5. Analysis of the Data Evolution Flywheel Data Evolution Flywheel Effectively Expands Knowledge Systems. Figure 10 illustrates an evolved knowledge system derived from seed concepts through the Data Evolution Flywheel, encompassing 25 distinct domains. The resulting hierarchical structure reaches maximum depth of 7 layers and 24,767 nodes, demonstrating the flywheels capability to construct diverse, large-scale, and multi-level knowledge representations. This scalable architecture establishes systematic foundation for domain-specific and cross-domain reasoning, facilitating knowledge organization and synthesis across interactive reasoning tasks. Scaling Analysis of Evolution Iterations. To investigate the relationship between evolution iterations and the expansion of knowledge systems and visual tools, Figure 11 quantifies the incremental growth of knowledge concepts and Figure 11. Scaling analysis of the iterations in the Data Evolution Flywheel. on complex multi-step reasoning tasks such as MathVision (+6.3%). These results validate that V-Thinkers interactive reasoning paradigm generalizes effectively across diverse reasoning domains. 6.4. Ablation Study. To explore the roles of different modules in V-Thinker, we perform an ablation study in general reasoning scenarios, as shown in Table 2. We have the following observations: (1) The performance of V-Thinker declines when any training phrase is removed, indicating that all progressive curriculum training stages are effective. (2) Removing perception alignment results in notable performance drop, demonstratFigure 12. Complete interactive reasoning samples of V-Thinker on open-source benchmarks. visual tools derived from synthesized data at each iteration."
        },
        {
            "title": "References",
            "content": "We observe two key findings: (1) Non-linear Growth with Evolve knowledge Expansion. As evolution iterations increase, both knowledge concepts and visual tools exhibit non-linear growth without saturation. After five iterations, the system scales to approximately 50 the initial seed size. This validates the superiority of our Data Evolution Flywheel design, which fundamentally enhances data diversity by discovering orthogonal knowledge concepts and visual tools as anchors, thereby sustaining continuous flywheel momentum. (2) Impact of Initial Seed Diversity. Richer initial knowledge concepts or tool sets yield superior evolution trajectories, underscoring the critical importance of providing diverse seed knowledge concepts as foundational anchors. 7. Conclusion In this paper, we propose V-Thinker, general-purpose multimodal reasoning assistant that enables interactive, visioncentric thinking through end-to-end reinforcement learning. Our main contributions include: (1) formalizing Interactive Thinking with Images and developing an end-to-end framework that bridges visual grounding with code-driven interactive reasoning; (2) proposing Data Evolution Flywheel that automatically synthesizes, evolves, and verifies datasets across diversity, quality, and difficulty dimensions; (3) introducing Visual Progressive Training Curriculum that aligns perception and interactive reasoning through two-stage training framework; and (4) releasing VTBench, an expert-verified benchmark for comprehensive evaluation. Extensive experiments show that V-Thinker consistently outperforms mainstream vision-language baselines, advancing the field of interactive visual reasoning and providing practical insights for future multimodal system development."
        },
        {
            "title": "Limitation",
            "content": "V-Thinker explores generalized paradigm for interactive visual reasoning, where models perform reasoning through visual interactions. However, it still has several limitations. Due to computational constraints, our current model iteration remains limited in scale and exhibits reduced capability in knowledge-intensive or domain-specific tasks. Our primary goal in this work is to establish unified framework for interactive reasoning. In future work, we will further optimize the model and enhance its robustness and generalization across diverse reasoning domains. Moreover, through V-Thinker, we believe that as model capabilities continue to advance, both the paradigm of data construction and the upper bound of model reasoning should be re-examined and redefined. We envision future models achieving increasingly natural and human-like forms of visual interaction and reasoning. [1] Inclusion AI, Fudong Wang, Jiajia Liu, Jingdong Chen, Jun Zhou, Kaixiang Ji, Lixiang Ru, Qingpei Guo, Ruobing Zheng, Tianqi Li, et al. M2-reasoning: Empowering mllms with unified general and spatial reasoning. arXiv preprint arXiv:2507.08306, 2025. 3 [2] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. 3, 9 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 9 [4] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large visionlanguage models. arXiv preprint arXiv:2504.11468, 2025. 3 [5] Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. SFT or rl? an early investigation into training r1-like reasoning large visionlanguage models. CoRR, abs/2504.11468, 2025. [6] Mengjie Deng, Guanting Dong, and Zhicheng Dou. Toolscope: An agentic framework for vision-guided and longhorizon tool use, 2025. 3 [7] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. CoRR, abs/2503.17352, 2025. 1 [8] Guanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, and Ji-Rong Wen. Toward general instructionfollowing alignment for retrieval-augmented generation. arXiv preprint arXiv:2410.09584, 2024. 3 [9] Guanting Dong, Licheng Bao, Zhongyuan Wang, Kangzhi Zhao, Xiaoxi Li, Jiajie Jin, Jinghan Yang, Hangyu Mao, Fuzheng Zhang, Kun Gai, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. Agentic entropy-balanced policy optimization, 2025. [10] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. CoRR, abs/2505.16410, 2025. [11] Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. [12] Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, and Zhicheng Dou. Agentic reinforced policy optimization. CoRR, abs/2507.19849, 2025. [13] Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen. Progressive multimodal reasoning via active retrieval. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 35793602. Association for Computational Linguistics, 2025. [14] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Ji-Rong Wen, and Zhicheng Dou. Understand what LLM needs: Dual preference alignment for retrieval-augmented generation. In Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 20252 May 2025, pages 42064225. ACM, 2025. 3 [15] Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, Ziwei Chen, and Zongyu Lin. Kimi-vl technical report. CoRR, abs/2504.07491, 2025. 1 [16] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. 9 [17] Michael Elad, Bahjat Kawar, and Gregory Vaksman. Image denoising: The deep learning revolution and beyond - survey paper. SIAM J. Imaging Sci., 16(3):15941654, 2023. 2 [18] Richard Fitzpatrick. Euclids elements of geometry. 2008. 6 [19] Vinod Goel. Sketches of thought. MIT press, 1995. 1 [20] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1437514385. IEEE, 2024. [21] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, and Ke Shen. Seed1.5-vl technical report. CoRR, abs/2505.07062, 2025. 1 [22] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 3 [23] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507, 2025. 3 [24] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. 3 [25] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 3 [26] Xiaoyuan Li, Moxin Li, Wenjie Wang, Rui Men, Yichang Zhang, Fuli Feng, Dayiheng Liu, and Junyang Lin. Mathopeval: fine-grained evaluation benchmark for visual operations of mllms in mathematical reasoning. arXiv preprint arXiv:2507.18140, 2025. [27] Xuchen Li, Xuzhao Li, Jiahui Gao, Renjie Pi, Shiyu Hu, and Wentao Zhang. Look less, reason more: Rollout-guided adaptive pixel-space reasoning. arXiv preprint arXiv:2510.01681, 2025. 3 [28] Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan L. Boyd-Graber, Haitao Mi, and Dong Yu. Self-rewarding vision-language model via reasoning decomposition. CoRR, abs/2508.19652, 2025. 2 [29] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2629626306, 2024. 3 [30] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. survey on hallucination in large vision-language models. CoRR, abs/2402.00253, 2024. 2 [31] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In European conference on computer vision, pages 126142. Springer, 2024. 3 [32] Wentao Liu, Qianjun Pan, Yi Zhang, Zhuo Liu, Ji Wu, Jie Zhou, Aimin Zhou, Qin Chen, Bo Jiang, and Liang He. Cmmmath: chinese multimodal math dataset to evaluate and enhance the mathematics reasoning of large multimodal models. arXiv preprint arXiv:2409.02834, 2024. [33] Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. 3 [34] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 3, 8 [35] Ruilin Luo, Zhuofan Zheng, Yifan Wang, Xinzhe Ni, Zicheng Lin, Songtao Jiang, Yiyao Yu, Chufan Shi, Ruihang Chu, Jin Zeng, et al. Ursa: Understanding and verifying chain-ofthought reasoning in multimodal mathematics. arXiv preprint arXiv:2501.04686, 2025. 3 [36] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. 3 [37] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. CoRR, abs/2503.07365, 2025. 1, 3, 7, 8 [38] OpenAI. Hello gpt-4o, 2024. 9 [39] OpenAI. Thinking with https://openai.com/index/thinking-with-images/, 2, 3 images. 2025. [40] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. 3, 8, 9 [41] Runqi Qiao, Lan Yang, Kaiyue Pang, and Honggang Zhang. Making visual sense of oracle bones for you and me. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1265612665, 2024. 3 [42] Runqi Qiao, Qiuna Tan, Guanting Dong, MinhuiWu MinhuiWu, Jiapeng Wang, YiFan Zhang, Zhuoma GongQue, Chong Sun, Yida Xu, Yadong Xue, Ye Tian, Zhimin Bao, Lan Yang, Chen Li, and Honggang Zhang. V-oracle: Making progressive reasoning in deciphering oracle bones for you and me. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2012420150, Vienna, Austria, 2025. Association for Computational Linguistics. 3 [43] Runqi Qiao, Qiuna Tan, Peiqing Yang, Yanzi Wang, Xiaowan Wang, Enhui Wan, Sitong Zhou, Guanting Dong, Yuchen Zeng, Yida Xu, et al. We-math 2.0: versatile mathbook system for incentivizing visual mathematical reasoning. arXiv preprint arXiv:2508.10433, 2025. 3, 5, 7, 8 [44] Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, et al. Zerobench: An impossible visual benchmark for contemporary large multimodal models. arXiv preprint arXiv:2502.09696, 2025. 8 [45] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 7 [46] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Mathllava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. 3 [47] Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, et al. Cs-bench: comprehensive benchmark for large language models towards computer science mastery. arXiv preprint arXiv:2406.08587, 2024. 3 [48] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. [49] Qiuna Tan, Runqi Qiao, Guanting Dong, YiFan Zhang, Minhui Wu, Jiapeng Wang, Miaoxuan Zhang, Yida Xu, Chong Sun, Chen Li, and Honggang Zhang. Ocr-critic: Aligning multimodal large language models perception through critical feedback. In Proceedings of the 33rd ACM International Conference on Multimedia, page 53855393, New York, NY, USA, 2025. Association for Computing Machinery. 3 [50] Core Team, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report, 2025. 3 [51] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599, 2025. 1 [52] Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, et al. Kwai keye-vl technical report. arXiv preprint arXiv:2507.01949, 2025. 3 [53] Barbara Tversky and Masaki Suwa. Thinking with sketches. 2009. [54] Barbara Tversky, Masaki Suwa, Maneesh Agrawala, Julie Heiser, Chris Stolte, Pat Hanrahan, Doantam Phan, Jeff Klingner, Marie-Paule Daniel, Paul Lee, et al. Sketches for design and design of sketches. In Human Behaviour in Design: Individuals, Teams, Tools, pages 7986. Springer, 2003. 1 [55] Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, et al. Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning. arXiv preprint arXiv:2506.01713, 2025. 3 [56] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. 3, 8, 9 [57] Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, et al. Mathcoder-vl: Bridging vision and code for enhanced multimodal mathematical reasoning. arXiv preprint arXiv:2505.10557, 2025. 3 [58] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Internvl3. 5: Advancing open-source Ye, Jie Shao, et al. multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 3, 9 [59] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025. 7, [60] Ye Wang, Qianglong Chen, Zejun Li, Siyuan Wang, Shijie Guo, Zhirui Zhang, and Zhongyu Wei. Simple o3: Towards interleaved vision-language reasoning. CoRR, abs/2508.12109, 2025. 2 [61] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. 8 [62] Jingxuan Wei, Caijun Jia, Qi Chen, Honghao He, Linzhuang Sun, Conghui He, Lijun Wu, Bihui Yu, and Cheng Tan. Geoint-r1: Formalizing multimodal geometric reasoning arXiv preprint with dynamic auxiliary constructions. arXiv:2508.03173, 2025. 3 [63] Shichao Weng, Zhiqiang Wang, Yuhua Zhou, Rui Lu, Ting Liu, Zhiyang Teng, Xiaozhang Liu, and Hanmeng Liu. Geosketch: neural-symbolic approach to geometric multimodal reasoning with auxiliary line construction and affine transformation. CoRR, abs/2509.22460, 2025. 2 [64] Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, and Klara Nahrstedt. Vtool-r1: Vlms learn to think with images via reinforcement learning on multimodal tool use. arXiv preprint arXiv:2505.19255, 2025. 3 [65] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 3 [66] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. 3, 8 [67] Manjie Xu, Guangyuan Jiang, Wei Liang, Chi Zhang, and Yixin Zhu. Interactive visual reasoning under uncertainty. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [68] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. 3, 9 [69] Jie Yang, Feipeng Ma, Zitian Wang, Dacheng Yin, Kang Rong, Fengyun Rao, and Ruimao Zhang. Wethink: Toward general-purpose vision-language reasoning via reinforcement learning. arXiv preprint arXiv:2506.07905, 2025. 3 [70] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. CoRR, abs/2503.10615, 2025. 1 Wang, Hengbo Xu, et al. From perception to cognition: survey of vision-language interactive reasoning in multimodal large language models. arXiv preprint arXiv:2509.25373, 2025. 3 [84] Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, and Ranjay Krishna. Reinforced visual perception with tools. arXiv preprint arXiv:2509.01656, 2025. 3 [85] Wenwen Zhuang, Xin Huang, Xiantao Zhang, and Jin Zeng. Math-puma: Progressive upward multimodal alignment to enhance mathematical reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2618326191, 2025. [86] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024. 3, 8 [71] Zhenlong Yuan, Xiangyan Qu, Chengxuan Qian, Rui Chen, Jing Tang, Lei Sun, Xiangxiang Chu, Dapeng Zhang, Yiwei Wang, Yujun Cai, et al. Video-star: Reinforcing openvocabulary action recognition with tools. arXiv preprint arXiv:2510.08480, 2025. 3 [72] Chi Zhang, Haibo Qiu, Qiming Zhang, Zhixiong Zeng, Lin Ma, and Jing Zhang. Deepsketcher: Internalizing visual manipulation for multimodal reasoning. arXiv preprint arXiv:2509.25866, 2025. 2, 3 [73] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-VL: learning to reason with multimodal large language models via step-wise group relative policy optimization. CoRR, abs/2503.12937, 2025. 1 [74] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. 3 [75] Kejia Zhang, Keda Tao, Jiasheng Tang, and Huan Wang. Poison as cure: Visual noise for mitigating object hallucinations in lvms. CoRR, abs/2501.19164, 2025. [76] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly In European see the diagrams in visual math problems? Conference on Computer Vision, pages 169186. Springer, 2024. 3, 8 [77] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al. Mavis: Mathematical visual instruction tuning. arXiv e-prints, pages arXiv2407, 2024. 3 [78] Yifan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Haonan Fan, Kaibing Chen, Jiankang Chen, Haojie Ding, Kaiyu Tang, Zhang Zhang, Liang Wang, Fan Yang, Tingting Gao, and Guorui Zhou. Thyme: Think beyond images. CoRR, abs/2508.11630, 2025. 2 [79] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025. 3, 7, 8 [80] Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, and Chen Wei. Pyvision: arXiv preprint Agentic vision with dynamic tooling. arXiv:2507.07998, 2025. 3 [81] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing \"thinking with images\" via reinforcement learning. CoRR, abs/2505.14362, 2025. 2, [82] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 3 [83] Chenyue Zhou, Mingxuan Wang, Yanbiao Ma, Chenxu Wu, Wanyi Chen, Zhe Qian, Xinyu Liu, Yiwei Zhang, Junhao"
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications",
        "WeChat Vision, Tencent Inc."
    ]
}