{
    "paper_title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer",
    "authors": [
        "Ruofan Liang",
        "Kai He",
        "Zan Gojcic",
        "Igor Gilitschenski",
        "Sanja Fidler",
        "Nandita Vijaykumar",
        "Zian Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Estimating scene lighting from a single image or video remains a longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, a novel data-driven approach that fine-tunes a video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on a large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce a low-rank adaptation finetuning strategy using a collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 0 8 6 3 0 . 9 0 5 2 : r LuxDiT: Lighting Estimation with Video Diffusion Transformer Ruofan Liang1,2,3 Kai He1,2,3 Zan Gojcic1 Igor Gilitschenski2,3 Sanja Fidler1,2,3 Nandita Vijaykumar2,3 Zian Wang1,2,3 1NVIDIA 2University of Toronto 3Vector Institute"
        },
        {
            "title": "Abstract",
            "content": "Estimating scene lighting from single image or video remains longstanding challenge in computer vision and graphics. Learning-based approaches are constrained by the scarcity of ground-truth HDR environment maps, which are expensive to capture and limited in diversity. While recent generative models offer strong priors for image synthesis, lighting estimation remains difficult due to its reliance on indirect visual cues, the need to infer global (non-local) context, and the recovery of high-dynamic-range outputs. We propose LuxDiT, novel data-driven approach that fine-tunes video diffusion transformer to generate HDR environment maps conditioned on visual input. Trained on large synthetic dataset with diverse lighting conditions, our model learns to infer illumination from indirect visual cues and generalizes effectively to real-world scenes. To improve semantic alignment between the input and the predicted environment map, we introduce low-rank adaptation finetuning strategy using collected dataset of HDR panoramas. Our method produces accurate lighting predictions with realistic angular high-frequency details, outperforming existing state-of-the-art techniques in both quantitative and qualitative evaluations. Project page: https://research.nvidia.com/labs/toronto-ai/LuxDiT/"
        },
        {
            "title": "1\nIn physically-based rendering, lighting plays a central role in shaping the appearanceâ€”how objects\ncast shadows, reflect, and appear integrated within a scene. From virtual object insertion and\naugmented reality to synthetic data generation, many downstream tasks rely on estimating scene\nillumination. Yet inferring lighting from casually captured images or video remains an open challenge.",
            "content": "A common representation of the scene illumination is the high-dynamic-range (HDR) environment map, which describes incoming light intensity from all directions. HDR maps can be acquired by using light probes or multi-exposure panoramas, requiring specialized setups that are impractical for everyday use [9]. To overcome this, several learning-based methods that estimate environment maps directly from casually captured LDR images or videos have been proposed [15, 16, 32, 73]. However, these methods typically depend on paired datasets of input images or videos and HDR environment maps, leading to chicken-and-egg problem: large collection of HDR environment maps is needed to train model that aims to alleviate the need for acquiring such expensive data in the first place. Recently, generative diffusion models have demonstrated strong capabilities in modeling complex image distributions. DiffusionLight [44] demonstrated that pretrained text-to-image models encode implicit knowledge of illumination, which can be cleverly extracted by inpainting virtual chrome ball into an image, generating plausible appearances under varying exposure settings. However, without task-specific fine-tuning, the inpainting priors of pre-trained diffusion models are insufficient Joint Advising Preprint. Input Image Lighting Estimation Figure 1: LuxDiT is generative lighting estimation model that predicts high-quality HDR environment maps from visual input. It produces accurate lighting while preserving scene semantics, enabling realistic virtual object insertion under diverse conditions. Virtual Object Insertion for producing reliable lighting estimates in single inference and cannot directly generate HDR outputs. As result, DiffusionLight relies on an expensive test-time ensemble strategy to improve robustness. Moreover, sampling multiple exposures through separate inference passes introduces inconsistencies and limits the dynamic range of the reconstructed illumination. In this work, we formulate lighting estimation as conditional generative task and propose LuxDiT, neural lighting predictor trained on synthetic data and adapted to real-world scenes. Conditioned on visual input, our approach fine-tunes diffusion transformer (DiT) to synthesize HDR panoramas from noise. Unlike pixel-aligned tasks, lighting estimation requires global reasoning over scene context. DiTs are particularly suited to this task: their attention-based architecture supports global context aggregation, and their generative priors facilitate reasoning from indirect cues such as shading and reflections. Training such model requires diverse lighting data. To overcome the lack of real-world HDR lighting supervision, we construct large-scale synthetic dataset with randomized geometry, materials, and lighting conditions. Training on this dataset allows the model to learn physically grounded cues for light direction and intensity. While this imparts general lighting priors, models trained purely on synthetic data often hallucinate lighting based on dataset priors, producing environment maps that are plausible but semantically mismatched with the input scene. For example, an image of an urban street may yield an environment map depicting rural landscape. To address this, we further apply low-rank adaptation (LoRA) [23] on curated set of real HDR panoramas, improving alignment between predicted lighting and scene semantics. Given single image or video, LuxDiT produces HDR environment maps with accurate direction, intensity, and scene-consistent content. It reduces lighting estimation error by 45% on Laval Outdoor sunlight direction and improves temporal consistency for video input, enabling reliable use in downstream applications such as virtual object insertion. Our main contributions are: DiT-based generative architecture that synthesizes HDR environment maps from visual input. LoRA-based fine-tuning strategy using curated HDR panoramas to improve semantic alignment between the input scene and predicted illumination. large-scale synthetic dataset with randomized geometry, materials, and lighting."
        },
        {
            "title": "2 Related Work\nLighting estimation aims to infer environment illumination from input imagery, and is critical for\nphotorealistic rendering and virtual object insertion. Early learning-based methods treat lighting\nestimation as a supervised regression problem, predicting spherical lobes [16, 32, 71, 68], parametric\nsources [65, 14], or low-resolution environment maps [15, 49, 73, 51] directly from a single image.\nThese models are trained on paired data obtained from real-world captures [15, 49, 57] or synthetic\nrendering [32, 73, 51]. However, their performance often degrades in complex, in-the-wild scenes\ndue to limited diversity in the training data.",
            "content": "2 Recent methods incorporate generative priors to address the ambiguity of scene illumination. StyleLight [54] fine-tunes StyleGAN to generate LDR and HDR panoramas from latent codes, using GAN inversion at test time. However, its performance hinges on inversion quality and often breaks semantic alignment on out-of-domain inputs. EverLight [8] regresses parametric lighting estimate and refines it with GAN to add high-frequency detail, but relies on pseudo-labeled HDR data and struggles with complex or bright lighting. DiffusionLight [44] uses diffusion model to inpaint virtual chrome ball under multiple exposures, merging them into an HDR map. While visually plausible, this multi-stage process yields distorted panoramas and limited dynamic range. Inverse rendering recovers scene properties such as geometry, material reflectance, and illumination from image observations. Lighting estimation is often treated as subcomponent of this broader task, with prior work jointly estimating lighting alongside depth, normals, and albedo. Learning-based approaches [48, 32, 58] typically leverage physics-based constraints and use re-rendering losses to supervise predictions. However, these methods often assume simplified reflectance models such as Lambertian shading, which limits their ability to handle complex lighting effects. Optimization-based methods leverage differentiable rendering [4, 70, 69, 6, 59, 41, 18, 33] to jointly optimize lighting parameters and other scene attributes through photometric losses and regularization terms. Some approaches [30] follow decomposition-then-optimization strategy: estimating geometry and albedo first, then solving for lighting via optimization. Other works also explore priors from proxy geometry [64] or pretrained general models [37, 35, 42]. The optimization-based pipelines often require dense multi-view captures or known proxy geometry, and involve expensive test-time optimization procedures. In contrast, our method directly predicts HDR illumination in feed-forward manner without requiring scene geometry or iterative inference. Diffusion model priors. Diffusion models (DMs) have emerged as powerful class of generative models in high-fidelity image [45, 2, 46, 7] and video synthesis [21, 72, 3, 62, 1]. Beyond generation, pretrained DMs have been adapted to perception tasks through task-specific finetuning on carefully curated datasets [61, 38, 19], showing strong results on spatially aligned predictions such as depth [29, 24, 28], surface normals [13, 63, 34], albedo [11, 30, 67, 34], and material properties [30, 67, 34, 42]. Adapting DMs to non-local tasks like lighting introduces new modeling challenges, as outputs such as HDR panoramas are not spatially-aligned with the input."
        },
        {
            "title": "3 Preliminaries: Diffusion Models",
            "content": "Diffusion models learn to approximate data distribution pdata(x) through iterative denoising. Following DDPM [20], forward process progressively adds Gaussian noise to data sample x0 pdata, producing noisy version at timestep [1, ] as: xt = 1 Î±tÏµ, where Ïµ (0, I) and Î±t defines the noise schedule. During training, neural network ÂµÎ¸ learns to reverse this process by minimizing: Î±tx0 + Ex0pdata(x),tpt,ÏµN (0,I) (cid:2)ÂµÎ¸(xt; c, t) 2 (cid:3) , (1) where represents optional conditioning inputs. The denoising target varies by formulation, and can be the noise Ïµ [20], the v-prediction 1 Î±tx0 [47], or the clean signal x0 itself [27]. At inference time, samples are generated by denoising an initial Gaussian sample through fixed number of reverse steps. In this paper, we build on CogVideoX [62], latent video diffusion model trained on compressed video representations. pretrained auto-encoder pair {E, D} maps RGB videos to and from latent space, such that E(x) = and D(z) x. All diffusion training and generation is performed in this lower-dimensional latent space to reduce memory and computational. Î±tÏµ"
        },
        {
            "title": "4 Method",
            "content": "We propose LuxDiT, diffusion-based generative framework for estimating high-dynamic-range (HDR) environment maps from single image or video. We tailor recent video diffusion transformer architecture [62] for lighting estimation, by jointly processing denoising targets (environment lighting) and condition tokens (LDR input images) through self-attention layers. Since single image can be treated as one-frame video, we refer to both inputs uniformly as input video in the remainder of this section. An overview of the architecture is shown in Figure 2. In the following sections, we describe the model design, data sources, and training procedure. 3 Figure 2: Method Overview. Given an input image or video I, LuxDiT predicts an environment map as two tone-mapped representations, Eldr and Elog, guided by directional map Edir. Environment maps are encoded with VAE, and the resulting latents are concatenated and jointly processed with visual input by DiT. The outputs Eldr and Elog are decoded and fused by lightweight MLP to reconstruct the final HDR panorama. 4.1 Model Design We formulate HDR environment map estimation as conditional denoising task. Given an input video RLHW 3 with frames, the model generates corresponding sequence of 360 HDR panoramas RLHeWe3. Two core challenges arise: (1) standard VAEs used in latent diffusion models are trained on LDR images and cannot faithfully encode HDR content, and (2) the output panoramas are not spatially aligned with the input, requiring flexible conditioning mechanisms. We address these challenges using dual-tonemapping HDR representation, token-based conditioning, and unified transformer architecture that jointly denoises two latent representations of lighting. HDR lighting representation. Realistic lighting involves high-intensity components such as the sun or artificial sources, with radiance values often exceeding 100 or 1,000. Representing this range in latent space is non-trivial: standard VAEs are trained on [0, 1]-normalized LDR images and cannot reconstruct such dynamic content, and retraining on HDR data is impractical due to data scarcity Inspired by prior works [26, 34], we represent each HDR panorama using two complementary tonemapped representations: Eldr = 1 + (cid:18) 1 + (cid:19) ; 2 ldr Elog = log(1 + E) log(1 + Mlog) (2) where Eldr is standard Reinhard tonemapping and Elog captures normalized log-intensity. We set Mldr = 16 and Mlog = 10,000. Both outputs are clipped to [0, 1] before VAE encoding. At inference time, the HDR environment map is reconstructed using lightweight MLP Ïˆ: Ë†E = Ïˆ (Eldr, Elog) . (3) Diffusion latents. Our model builds on transformer-based diffusion model ÂµÎ¸, adapted to predict HDR environment maps from visual input. The model operates in latent space and jointly denoises two tonemapped representations of the HDR lighting. The tonemapped inputs Eldr and Elog are encoded by the pretrained VAE into latent tensors [zldr, zlog] with shape as RlheweC. These are concatenated along the channel dimension to form the diffusion target = [zldr, zlog] Rlhewe2C. The input and output projection layers of the diffusion network ÂµÎ¸ are extended to accommodate the increased channel dimension. Conditioning visual input in DiT. Accurate lighting estimation requires the model to extract finegrained shading cues from the input image, such as shadow orientation, surface reflections, and specular highlights. Unlike pixel-aligned image-to-image translation tasks, we empirically observe that concatenating conditions to the noisy latents leads to poor performance (see Table 7), indicating the need for more flexible conditioning mechanism. To this end, we adopt fully attention-based architecture for the input video conditions. Specifically, we encode the input video RLHW 3 into latent tensor E(I) RlhwC using the pretrained VAE encoder, and flatten it into token sequence RlhwC. To help the model distinguish between condition tokens and denoising targets, we apply separate adaptive layer normalization (AdaLN) modules [43, 62] to each token type at every transformer block. 4 Directional embedding. To improve angular continuity in the predicted panoramas, we inject directional information into the model. Specifically, we construct direction map of unit vectors Edir that encodes per-pixel lighting directions in the camera coordinate system. This map is passed through the same VAE encoder E, then projected and fused into the noise tokens using channel-wise concatenation before the transformer blocks. During training, we apply random horizontal rotations to Edir to encourage rotational equi-variance and robust directional encoding. Conditioned denoising process. To put it together, at each denoising timestep t, the model receives noisy latent zt = [zldr ] and predicts the corresponding clean latents conditioned on visual input as ÂµÎ¸(zt; c, t). This transformer-based design allows the model to propagate indirect lighting cuessuch as shadows and reflectionsthrough global self-attention, enabling lighting prediction that is both scene-consistent and directionally accurate. , zlog 4.2 Data Strategy Supervised training of our model requires paired data in the form (I, Eldr, Elog), where is an LDR input and Eldr, Elog are tonemapped versions of the target HDR environment map. To overcome the scarcity of real-world HDR annotations, we leverage three complementary data sources: synthetic renderings, HDR panorama images, and LDR panoramic videos. Synthetic rendering data. To supervise lighting prediction using physically accurate visual cues, we generate synthetic data by rendering randomized 3D scenes lit by HDR environment maps. Each scene consists of (i) ground plane with randomly assigned PBR materials, (ii) 3D objects sampled from Objaverse [10], and (iii) simple geometric primitives such as spheres, cubes, and cylinders with varied materials. We render multiple frames per scene with randomized camera trajectories and environment map rotations. Despite their simplicity, these scenes exhibit diverse lighting effects, including cast shadows, specular highlights, and inter-reflections, all paired with ground-truth HDR illumination. Empirically, we find this data is critical for enabling the model to learn accurate shading cues and light-source location (see Table 7). HDR panorama images. We generate training pairs by sampling perspective crops from HDR environment maps with data augmentation. Specifically, given panorama, we randomly sample camera parameters including azimuth, elevation, field of view, and exposure scale. These parameters define virtual pinhole camera, which we use to project the panorama into an LDR perspective view I. The corresponding HDR environment map serves as the ground truth lighting target E. To support temporal training, we extend this procedure to generate multi-frame sequences by smoothly varying the camera pose over time. LDR panorama videos. To enable the generation of dynamic panorama environment maps, we also incorporate training data from LDR panoramic videos. Although ground-truth HDR environment maps are not available for this source, we use it in the form (I, Eldr, ), where Eldr is derived using tonemapping and indicates the absence of log-space intensity. The panoramic video is projected into perspective-view video using randomized camera parameters, following the same procedure as above. Despite the lack of HDR intensity, this data improves robustness and temporal consistency by exposing the model to natural image statistics, motion patterns, and diverse real-world lighting conditions. We use 2,000 panoramic videos from the WEB360 dataset [56] for training, and hold out 114 videos for evaluation. 4.3 Training Scheme We adopt two-stage training strategy to progressively build the models capacity and improve generalization. The first stage focuses on learning physically grounded lighting cues from synthetic data. The second stage adapts the model to real-world distributions through LoRA-based fine-tuning. Stage I: Synthetic supervised training. We begin by training the model on the synthetic rendering dataset described in Section 4.2. This stage enables the model to learn the fundamental relationship between image-based shading cues and HDR environment lighting. We follow the standard DDPM training objective [20] adopted by the CogVideoX base model [62]: LI(Î¸) = Ez0,ÏµN (0,I),tU (T ) (cid:2)Ïµ ÂµÎ¸(zt, c, t)2 2 (cid:3) , (4) where z0 denotes the clean latent pair [zldr, zlog], and is the conditioning latent from the input video. During training, we randomly drop either zldr or zlog with probability = 0.1 to encourage robustness to missing tonemapped representations. 5 Table 1: Comparison of our method with baselines on three benchmark datasets. The results are reported in terms of scale-invariant RMSE, angular error, and normalized RMSE. Dataset Method Scale-invariant RMSE Normalized RMSE Angular Error Diffuse Matte Mirror Diffuse Matte Mirror Diffuse Matte Mirror Laval Indoor Laval Outdoor Poly Haven StyleLight DiffusionLight Ours H-G et al. [22] NLFE DiffusionLight Ours StyleLight NLFE DiffusionLight Ours 0.135 0.124 0.112 0.300 0.112 0.083 0.068 0.138 0.159 0.113 0.077 0.315 0.325 0.297 0.437 0.234 0.224 0.190 0.336 0.326 0.270 0.196 0.552 0.597 0.586 0.587 0.431 0.414 0.396 0.620 0.571 0.519 0. 4.238 2.500 2.555 7.851 4.804 1.936 2.018 3.034 3.305 2.199 1.235 6.781 4.742 3.421 5.936 5.641 3.526 8.755 26.052 7.278 5.279 5.491 2.955 5.286 2.939 6.602 4.272 5.180 4.240 4.104 3.121 2.783 1.977 0.234 0.216 0.196 0.551 0.217 0.167 0.137 0.198 0.224 0.191 0.111 0.404 0.361 0.341 0.627 0.331 0.330 0.271 0.344 0.365 0.282 0.199 0.511 0.431 0.457 0.740 0.496 0.472 0.454 0.474 0.458 0.391 0.323 Table 2: Angular error on estimated peak luminance light direction on Laval Outdoor sunny scenes. Table 3: Quantitative comparison with video input. Peak angular error (PAE) is used to evaluate PolyHaven-Peak videos. Angular error (AE) on is used to evaluate WEB360 LDR videos. Method Peak Angular Error Mean Median Method PolyHaven-Peak WEB360 PAE Mean PAE Std AE AE Std H-G et al. [22] NLFE DiffusionLight Ours 52.8 52.9 44.4 23.7 47.8 43.5 32.1 17. DiffusionLight Ours (image) Ours (video) 19.09 5.74 5.21 10.31 3.68 1.95 6.504 5.679 5.218 0.269 0.382 0.072 Stage II: Semantic adaptation. After base training, we fine-tune the model to improve semantic alignment between the input appearance and the predicted HDR environment map. This stage uses real-world data sources, including perspective projections from HDR panoramas and LDR panoramic videos. Since HDR ground truth is not available in the latter, we supervise only the LDR-tonemapped component. To avoid overfitting and preserve the pretrained model capacity, we apply parameter-efficient LoRA fine-tuning [23], optimizing small set of injected low-rank parameters Î¸ in the transformer layers: LII(Î¸) = Ez0,ÏµN (0,I),tU (T ) (cid:2)Ïµ ÂµÎ¸+Î¸(zt, c, t)2 2 (cid:3) , (5)"
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experiment Settings Implementation details. We use the pre-trained CogVideoX [62] model as our backbone. All training is conducted on 16 NVIDIA A100 GPUs. Input resolutions are randomly sampled between 512512 and 480720, and output environment map resolutions are between 128256 and 256512. The image-based model is trained with batch size of 192 for 12,000 iterations. For video training, we use the same spatial resolutions and uniformly sample frame lengths from 9, 17, 25. The video model is trained with an average batch size of 48 for an additional 12,000 iterations. LoRA modules are applied to all attention layers with rank of 64. We fine-tune the LoRA parameters for 5,000 iterations during the adaptation stage. Please refer to supplement for implementation details. Datasets. We evaluate our method on the following three benchmark datasets, covering various indoor and outdoor scenes. 1) Laval Indoor [15]: We use the same set of 289 test HDRIs used by prior works [44, 54]; 2) Laval Outdoor [22]: We evaluate on 116 sunny HDR panoramas with concentrated sunlight selected from the original dataset; 3) Poly Haven [66]: We select 181 Poly Haven HDRIs not used during model training to evaluate performance across both indoor and outdoor scenes. Metrics. Following prior works [54, 44], we use three standard metrics for evaluating HDR lighting: scale-invariant root mean square error (si-RMSE) [17], angular error in degrees [31], and normalized RMSE (n-RMSE) [44]. For scenes with concentrated sunlight, we additionally report peak angular error (PAE) [22, 57], which measures the angular deviation of the predicted peak light direction. Baselines. For indoor scenes, we compare against DiffusionLight [44], StyleLight [54], Weber et al. [60], and EMLight [68], using metrics reported by [44] when applicable. For outdoor scenes, we compare against DiffusionLight [44], Hold-Geoffroy et al. [22], and NLFE [57]. Input Image GT StyleLight DiffusionLight Ours Input Image GT StyleLight DiffusionLight Ours Input Image GT H-G et al. DiffusionLight Ours Input Image GT H-G et al. DiffusionLight Ours Input Image GT NLFE DiffusionLight Ours Input Image GT NLFE DiffusionLight Ours Figure 3: Qualitative comparison with baseline methods on three benchmark datasets. Reference StyleLight DiffusionLight DiPIR Ours Figure 4: Qualitative comparison of virtual object insertion. Table 4: Ablation study on impact of LoRA scale at inference time. Table 5: Ablation study on impact of camera field-of-view. Table 6: Ablation study on impact of camera elevation. LoRA Scale Diffuse Matte Mirror 0.00 0.25 0.50 0.75 1. 2.98 2.09 1.52 1.22 1.17 5.02 3.69 2.66 2.05 1.92 6.07 4.67 3.56 2.88 2.72 FOV Diffuse Matte Mirror 45 50 60 70 75 2.14 2.06 1.92 1.85 1.80 2.95 2.86 2.72 2.63 2. 1.29 1.26 1.17 1.15 1.13 Elevation Diffuse Matte Mirror 30 15 +00 +15 +30 1.70 1.22 1.17 1.28 1.71 3.04 2.05 1.92 2.09 2.59 3.95 2.87 2.72 2.94 3. Table 7: Ablation study on model design choices and training data. We report the angular error with threespheres protocol. Settings Laval Indoor Poly Haven Diffuse Matte Mirror Diffuse Matte Mirror Ours (channel concat.) Ours (w/o synthetic data) Ours 7.09 4.50 2.56 10.04 5.14 3.53 11.07 6.96 5.64 7.09 1.48 1.23 10.04 2.08 1.98 11.07 2.86 2. 5.2 Evaluation of Image Lighting Estimation We follow the evaluation protocol from prior work to render spheres with three representative materials (gray-diffuse, silver-matte, and mirror), using the estimated HDR environment map from the LDR input image [15, 54, 44]. 7 Frame 0 Frame 12 Frame Frame 0 Frame 12 Frame 24 0 12 0 12 24 0 12 0 12 24 Reference DiffusionLight Reference DiffusionLight Ours image Ours video Ours image Ours video Figure 5: Qualitative comparison of video lighting estimation. Table 1 reports quantitative comparisons on three benchmarks spanning both indoor and outdoor scenes. On the Laval Indoor dataset, our method performs comparably or better than DiffusionLight across most metrics, despite not using Laval Indoor dataset during training. This dataset exhibits noticeable shift in color and intensity distribution compared to our training set, and our strong performance demonstrates robust generalization. From qualitative comparison shown in Figure 3, DiffusionLight can lose angular high-frequency details from the input image due to its distorted representation. In contrast, our estimated environment maps can recover more high-frequency details while preserving accurate lighting. On the Laval Outdoor and Poly Haven datasets with broader dynamic range, our method consistently outperforms prior state-of-the-art methods. Hold-Geoffroy et al. [22] can estimate concentrated peak light source such as sunlight; however, its results do not adapt well to the details of the input image. NLFE [57] can estimate in-context environment maps, but it often fails to estimate accurate highlights. DiffusionLight performs better than other baselines, but due to its limited dynamic range, it struggles with outdoor high-intensity light sources. To further assess directional accuracy, we evaluate the angular error of the peak luminance direction on subset of the Laval Outdoor dataset containing direct sunlight. Table 2 reports the mean and median peak angular errors. Our method reduces peak angular error by nearly 50% compared to DiffusionLight, confirming its advantage in capturing accurate light directiona critical factor for casting realistic shadows in downstream applications such as object insertion. 5.3 Evaluation of Video Lighting Estimation To evaluate lighting estimation accuracy and consistency on video input, we construct two types of test sequences: PolyHaven-Peak: We project 12 unseen Poly Haven panoramas (each with direct sunlight) into videos using smooth panning camera. This setting is used to evaluate peak angular error. WEB360: We randomly select 12 LDR panoramic videos featuring dynamic content from WEB360 and render them into perspective views with fixed horizontal camera motion. This setting evaluates temporal consistency using chromatic angular error on rendered mirror spheres. Each set contains 12 videos at resolution of 480720 and length of 25 frames. To quantify temporal consistency, we compute the standard deviation (std) of per-frame error metrics for each video clip, and average the results across the 12-video set. We compare our video inference to two baselines: our own image-based inference (applied frame-byframe) and DiffusionLight [44]. Table 3 reports the results. Our method outperforms DiffusionLight. Comparing to Ours (image), video inference achieves higher accuracy and significantly lower temporal variance, indicating more stable predictions across time. Figure 5 shows qualitative examples of video inference. Both DiffusionLight and our image-based variant exhibit visible temporal flickering. In contrast, our method produces smooth lighting transitions, successfully aligning content across frames and preserving consistent lighting behavior over time. 8 5.4 Evaluation of Virtual Object Insertion Virtual object insertion is key downstream application of lighting estimation. We evaluate our method on this task using the benchmark from [35], using 11 HDR panoramas from the Poly Haven dataset [66]. For each scene, virtual object and known ground plane are manually placed into the environment. Each test case includes an LDR background image rendered from the HDR panorama, along with posed object and ground plane. pseudo-ground-truth object insertion is generated by rendering the object using the original HDR environment map. This allows for controlled comparison against renderings produced using predicted lighting. Table 8: Quantitative evaluation of virtual object insertion. We report the percentage of images where users preferred Ours over baselines. preference > 50% indicates Ours outperforming baselines. Method RMSE SSIM Ours Preferred StyleLight DiffusionLight DiPIR Ours 0.056 0.057 0.048 0.047 0.986 0.987 0.989 0.990 60.6% 60.6% 54.5% / We report quantitative metrics in Table 8. In addition, we conduct user study to assess perceptual quality (details provided in the supplement), and report the percentage of samples where users preferred our results over baseline methods. Our method achieves visual quality comparable to DiPIR and significantly outperforms other baselines. Notably, DiPIR is specialized for object insertion and incorporates additional modules for tone mapping and appearance harmonization. In contrast, our model estimates lighting alone, yet still produces realistic composite renderings. We include qualitative results in Figure 4. 5.5 Ablation Study Model Design and Training Data We evaluate two model variants to ablate the contributions of our architectural and training design: (1) Channel concatenation: This variant fuses input and environment map (resized to match input image) latents along the channel dimension [26], and no token-wise concatenation is used. Our two-stage training is also applied. (2) Training without synthetic data: This variant skips Stage training and uses only panorama crops for fine-tuning. Table 7 reports angular errors on Laval Indoor and Poly Haven. Channel concatenation significantly underperforms, confirming the importance of token-level conditioning. Without synthetic pretraining, the model performs well in-domain (Poly Haven) but degrades out-of-domain (Laval Indoor), showing synthetic data pre-training is crucial for learning generalized lighting priors. LoRA scale. We vary the LoRA interpolation weight from 0.0 to 1.0 to ablate how fine-tuned LoRA affects the predicted lighting content. Table 4 shows that higher LoRA weights yield lower angular error on Poly Haven, validating the effectiveness of LoRA for improving semantic alignment. Camera sensitivity. We test robustness to camera variation by rendering crops from Poly Haven under varying field of view (45 to 75) and camera elevation (30 to 30). Results in Tables 5 and 6 show that while extreme viewpoints introduce mild error increases, performance remains stable, demonstrating robustness to moderate viewpoint shifts."
        },
        {
            "title": "6 Discussion\nWe introduce LuxDiT, a conditional generative model for estimating HDR scene illumination from\ncasually captured images and videos. Our approach fine-tunes a video diffusion transformer (DiT)\nto synthesize HDR environment maps, combining large-scale synthetic data for learning physically\ngrounded priors with LoRA-based adaptation on real HDR panoramas to improve semantic alignment.\nExtensive experiments demonstrate that LuxDiT produces accurate, high-frequency, and scene-\nconsistent lighting predictions from limited visual input.",
            "content": "Limitations and future work. While LuxDiT produces high-quality lighting predictions, inference remains computationally intensive due to the iterative nature of diffusion models, limiting its use in real-time applications. Future work could explore model distillation or more efficient architectures to accelerate inference. Additionally, the resolution of predicted panoramas is limited by data and training scale; generating high-resolution outputs for immersive applications will require richer, more diverse HDR supervision. Looking ahead, with recent progress in joint generative modeling [5, 36], we see LuxDiT as step toward unified inverse and forward rendering frameworks, complementing recent progress in neural forward rendering and G-buffer estimation [67, 34]. Future directions include joint modeling or co-training of lighting, geometry, and material for general-purpose scene reconstruction and appearance synthesis."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. eDiff-I: text-to-image diffusion models with ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [4] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik P.A. Lensch. NeRD: neural reflectance decomposition from image collections. In ICCV, 2021. [5] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. VideoJAM: Joint appearance-motion representations for enhanced motion generation in video models. arXiv: 2502.02492, 2025. [6] Wenzheng Chen, Joey Litalien, Jun Gao, Zian Wang, Clement Fuji Tsang, Sameh Khalis, Or Litany, and Sanja Fidler. DIB-R++: Learning to predict lighting and material with hybrid differentiable renderer. In NeurIPS, 2021. [7] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. [8] Mohammad Reza Karimi Dastjerdi, Jonathan Eisenmann, Yannick Hold-Geoffroy, and Jean-FranÃ§ois Lalonde. Everlight: Indoor-outdoor editable hdr lighting estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 74207429, October 2023. [9] Paul E. Debevec and Jitendra Malik. Recovering high dynamic range radiance maps from photographs. In Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 97, page 369378, USA, 1997. ACM Press/Addison-Wesley Publishing Co. [10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3D objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13142 13153, 2023. [11] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative models: What do they know? do they know things? lets find out!, 2024. [12] Egor Ershov, Alexey Savchik, Illya Semenkov, Nikola Banic, Alexander Belokopytov, Daria Senshina, Karlo KoÅ¡Ë‡cevic, Marko SubaÅ¡ic, and Sven LonË‡caric. The cube++ illumination estimation dataset. IEEE access, 8:227511227527, 2020. [13] Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. GeoWizard: unleashing the diffusion priors for 3D geometry estimation from single image. In ECCV, 2024. [14] Marc-AndrÃ© Gardner, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Christian GagnÃ©, and Jean-FranÃ§ois Lalonde. Deep parametric indoor lighting estimation. In ICCV, pages 71757183, 2019. [15] Marc-AndrÃ© Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen, Emiliano Gambaretto, Christian GagnÃ©, and Jean-FranÃ§ois Lalonde. Learning to predict indoor illumination from single image. arXiv preprint arXiv:1704.00090, 2017. [16] Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, and Jean-FranÃ§ois Lalonde. Fast spatiallyvarying indoor lighting estimation. In CVPR, pages 69086917, 2019. [17] Roger Grosse, Micah K. Johnson, Edward H. Adelson, and William T. Freeman. Ground truth dataset and baseline evaluations for intrinsic image algorithms. In ICCV, pages 23352342. IEEE, 2009. [18] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, light, and material decomposition from images using Monte Carlo rendering and denoising. arXiv:2206.03380, 2022. 10 [19] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, and Ying-Cong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction. arXiv preprint arXiv:2409.18124, 2024. [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv:2204.03458, 2022. [22] Yannick Hold-Geoffroy, Akshaya Athawale, and Jean-FranÃ§ois Lalonde. Deep sky modeling for single image outdoor lighting estimation. In CVPR, pages 69276935, 2019. [23] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [24] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, 2025. [25] Wenzel Jakob, SÃ©bastien Speierer, Nicolas Roussel, Merlin Nimier-David, Delio Vicini, Tizian Zeltner, Baptiste Nicolet, Miguel Crespo, Vincent Leroy, and Ziyi Zhang. Mitsuba 3 renderer, 2022. https://mitsubarenderer.org. [26] Haian Jin, Yuan Li, Fujun Luan, Yuanbo Xiangli, Sai Bi, Kai Zhang, Zexiang Xu, Jin Sun, and Noah Snavely. Neural gaffer: Relighting any object via diffusion. In Advances in Neural Information Processing Systems, 2024. [27] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proc. NeurIPS, 2022. [28] Bingxin Ke, Dominik Narnhofer, Shengyu Huang, Lei Ke, Torben Peters, Katerina Fragkiadaki, Anton Obukhov, and Konrad Schindler. Video depth without video models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [29] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [30] Peter Kocsis, Vincent Sitzmann, and Matthias NieÃŸner. Intrinsic image diffusion for single-view material estimation. In arxiv, 2023. [31] Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel, Jay Busch, and Paul Debevec. Deeplight: Learning illumination for unconstrained mobile mixed reality. In CVPR, pages 59185928, 2019. [32] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from single image. In CVPR, pages 24752484, 2020. [33] Ruofan Liang, Huiting Chen, Chunlin Li, Fan Chen, Selvakumar Panneer, and Nandita Vijaykumar. Envidr: Implicit differentiable renderer with neural environment lighting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7989, 2023. [34] Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, and Zian Wang. Diffusionrenderer: Neural inverse and forward rendering with video diffusion models. arXiv preprint arXiv: 2501.18590, 2025. [35] Ruofan Liang, Zan Gojcic, Merlin Nimier-David, David Acuna, Nandita Vijaykumar, Sanja Fidler, and Zian Wang. Photorealistic object insertion with diffusion-guided inverse rendering. In ECCV, 2024. [36] Yuanxun Lu, Jingyang Zhang, Tian Fang, Jean-Daniel Nahmias, Yanghai Tsin, Long Quan, Xun Cao, Yao Yao, and Shiwei Li. Matrix3D: Large Photogrammetry Model All-in-One, 2025. [37] Linjie Lyu, Ayush Tewari, Marc Habermann, Shunsuke Saito, Michael ZollhÃ¶fer, Thomas LeimkÃ¼hler, and Christian Theobalt. Diffusion posterior illumination for ambiguity-aware inverse rendering. ACM Transactions on Graphics, 42(6), 2023. 11 [38] Gonzalo Martin Garcia, Karim Abou Zeid, Christian Schmidt, Daan de Geus, Alexander Hermans, and Bastian Leibe. Fine-tuning image-conditional diffusion models is easier than you think. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025. [39] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta. Object 3dit: Language-guided 3d-aware image editing. Advances in Neural Information Processing Systems, 36:34973516, 2023. [40] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: representing scenes as neural radiance fields for view synthesis. arXiv preprint arXiv:2003.08934, 2020. [41] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas MÃ¼ller, and Sanja Fidler. Extracting triangular 3D models, materials, and lighting from images. arXiv:2111.12503, 2021. [42] Jacob Munkberg, Zian Wang, Ruofan Liang, Tianchang Shen, and Jon Hasselgren. VideoMat: Extracting PBR Materials from Video Diffusion Models. In Eurographics Symposium on Rendering - CGF Track, 2025. [43] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [44] Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani, Pramook Khungurn, and Supasorn Suwajanakorn. DiffusionLight: light probes for free by painting chrome ball. In ArXiv, 2023. [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. [47] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [48] Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W. Jacobs, and Jan Kautz. Neural inverse rendering of an indoor scene from single image. In ICCV, 2019. [49] Shuran Song and Thomas Funkhouser. Neural illumination: Lighting prediction for indoor environments. In CVPR, pages 69186926, 2019. [50] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 21492159, 2022. [51] Jiajun Tang, Yongjie Zhu, Haoyu Wang, Jun-Hoong Chan, Si Li, and Boxin Shi. Estimating spatiallyvarying lighting in urban scenes with disentangled representation. In ECCV, 2022. [52] Benjamin Ummenhofer, Sanskar Agrawal, Rene Sepulveda, Yixing Lao, Kai Zhang, Tianhang Cheng, Stephan Richter, Shenlong Wang, and German Ros. Objects with lighting: real-world dataset for evaluating reconstruction and rendering for object relighting. In 2024 International Conference on 3D Vision (3DV), pages 137147. IEEE, 2024. [53] Giuseppe Vecchio and Valentin Deschaintre. Matsynth: modern pbr materials dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [54] Guangcong Wang, Yinuo Yang, Chen Change Loy, and Ziwei Liu. Stylelight: Hdr panorama generation for lighting estimation and editing. In European Conference on Computer Vision (ECCV), 2022. [55] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, 2021. 12 [56] Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, and Jian Zhang. 360dvd: Controllable panorama video generation with 360-degree video diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69136923, 2024. [57] Zian Wang, Wenzheng Chen, David Acuna, Jan Kautz, and Sanja Fidler. Neural light field estimation for street scenes with differentiable virtual object insertion. In ECCV, 2022. [58] Zian Wang, Jonah Philion, Sanja Fidler, and Jan Kautz. Learning indoor inverse rendering with 3D spatially-varying lighting. In ICCV, 2021. [59] Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren, Zan Gojcic, Wenzheng Chen, and Sanja Fidler. Neural fields meet explicit geometric representations for inverse rendering of urban scenes. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2023. [60] Henrique Weber, Mathieu Garon, and Jean-FranÃ§ois Lalonde. Editable indoor lighting estimation. In European Conference on Computer Vision, pages 677692. Springer, 2022. [61] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. What matters when repurposing diffusion models for general dense perception tasks? arXiv preprint arXiv:2403.06090, 2024. [62] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [63] Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM Transactions on Graphics, 2024. [64] Hong-Xing Yu, Samir Agarwala, Charles Herrmann, Richard Szeliski, Noah Snavely, Jiajun Wu, and Deqing Sun. Accidental light probes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1252112530, 2023. [65] Ye Yu and William A. P. Smith. InverseRenderNet: learning single image inverse rendering. In CVPR, 2019. [66] Greg Zaal and et al. Poly Haven - The Public 3D Asset Library, 2025. [67] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, LingQi Yan, and MiloÅ¡ HaÅ¡an. RGBX: image decomposition and synthesis using material-and lighting-aware diffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [68] Fangneng Zhan, Changgong Zhang, Yingchen Yu, Yuan Chang, Shijian Lu, Feiying Ma, and Xuansong Xie. Emlight: Lighting estimation via spherical distribution approximation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021. [69] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse rendering with spherical Gaussians for physics-based material editing and relighting. In CVPR, 2021. [70] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect illumination for inverse rendering. In CVPR, 2022. [71] Yiqin Zhao and Tian Guo. Pointar: Efficient lighting estimation for mobile augmented reality. arXiv preprint arXiv:2004.00006, 2020. [72] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. [73] Yongjie Zhu, Yinda Zhang, Si Li, and Boxin Shi. Spatially-varying outdoor lighting estimation from intrinsics. In CVPR, 2021. 13 Supplement for LuxDiT: Lighting Estimation with Video Diffusion Transformer In the supplementary material, we discuss the broader impact of our project in Sec. A, and provide additional details for implementation and experiments in Sec. B. Sec. provides additional quantitative and qualitative results. We refer to the accompanied video for extended comparisons on video lighting estimation."
        },
        {
            "title": "A Broader Impact",
            "content": "We introduce LuxDiT, generative model for estimating high-dynamic-range (HDR) environment lighting from casually captured images and videos. Lighting estimation is core challenge in photorealistic rendering due to its non-local and indirect nature. LuxDiT produces scene-consistent HDR panoramas, enabling applications in virtual object insertion, relighting, AR/VR, and visual effects. It can also support synthetic data generation for downstream tasks in robotics and perception, where realistic illumination is critical. Similar to other generative methods, LuxDiT could be misused to produce visually convincing but deceptive content. While it does not directly generate synthetic scenes, it enables realistic virtual object insertion and may facilitate the creation of manipulated imagery that is difficult to distinguish from real footage. We encourage responsible use of LuxDiT and caution against its deployment in contexts where synthetic content could mislead viewers or undermine public trust, such as misinformation or falsified media."
        },
        {
            "title": "B Additional Details",
            "content": "B.1 HDR Reconstruction Section 4.1 describes our method for reconstructing HDR environment maps from two tone-mapped LDR images using lightweight MLP Ïˆ (Eldr, Elog). This MLP consists of 5 layers with 64 hidden units per layer and LeakyReLU activation. softplus activation is applied to the final output layer to ensure non-negative outputs. The MLP Ïˆ operates on per-pixel basis: it takes pair of LDR RGB values as input and predicts single HDR RGB value. It is trained using the same HDR environment maps as the diffusion model, with augmentations including random intensity rescaling and exposure adjustments for diversity. To simulate limited input precision, LDR inputs are randomly quantized to 8-bit RGB values. We train the MLP using Huber loss with Î´ = 1.0, which provides robustness against large HDR outliers while preserving smooth gradients. Additionally, we show the tone-mapping curves used to generate the LDR images in Fig. 6. Our dual-tone mapping strategy ensures sufficient sampling across the full dynamic range [0, 10,000], supporting accurate HDR reconstruction. Figure 6: The two tone-mapping curves used to generate the LDR images. The 128 dot points along the curve are evenly spaced along [0, 1] LDR value range. Figure 7: The histogram of the 99.9-th percentile intensity of all HDR environment maps in our training set. 14 B.2 Datasets We provide more details about the datasets used in our experiments. The data sources of HDR environment maps. We collected 2386 HDR environment maps from the following 4 data sources either publicly available or commercially available. Poly Haven0: 626 HDR environment maps with wide range of indoor and outdoor lighting. HDR Maps1: 403 HDR environment maps with diverse lighting conditions, including 294 panorama maps and 109 hemi-sphere sky maps. HDRI Skies2: 457 HDR environment maps with outdoor lighting conditions. DOSCH DESIGN3: 900 HDR environment maps mainly for outdoor lighting conditions. Figure 7 shows the histogram of the 99.9-th percentile intensity of all HDR environment maps in our training set. With over 50% of the HDR environment maps having 99.9-th percentile intensity greater than 2.93. Note that the for outdoor lighting, the highest intensity can be orders of magnitude higher than the 99.9-th percentile. Among these, Poly Haven and HDR Maps offer greater diversity across scene types. To balance the training distribution across data sources, we apply sampling weights in the ratio 3 : 2 : 2 : 1 in the order listed above. For quantitative and qualitative evaluation, we use the Laval Indoor 4 and Laval Outdoor 5 datasets, which contain calibrated HDR panoramas of real-world indoor and outdoor scenes. Synthetic rendering data. Similar to OBJect [39] and DiffusionRenderer [34], we create synthetic 3D scenes by compositing multiple 3D objects from Objaverse [10] and randomly placing them on plane with varying plane textures. We use filtered subset of Objaverse, containing 269,000 3D objects with decent geometries and material textures, to create synthetic 3D scenes. The varying plane textures are sampled from 4000 PBR textures from MatSynth6 [53]. Each composited scene contains up to 3 sampled Objaverse objects. We additionally add up to 3 random geometry primitives (sphere, cube, and cylinder) with varying material textures to provide rich shading cues for model to learn. For each scene, we randomly render 14 video clips with varying camera motions (e.g., orbiting camera and oscilating camera) and environment lightings. We use path-tracing renderer with 128 samples per pixel (spp) and the default OptiX denoiser to render the video clips with resolution of 480 720 or 512 512. The HDR rendering results are tone-mapped to LDR images using Blenders AgX tonemapping7. In total, we created 190,000 random synthetic scenes, resulting in 260,000 video clips with at least 16 frames per video clip. Figure 8: Randomly sampled example images from our synthetic rendering data. Perspective crops of HDR panorama images. We use subset of 1251 HDR panoramas with meaningful contents from Poly Haven, HDR Maps, and HDRI Skies for the training with perspective crops. Instead of pre-processing the perspective crops from the HDR panoramas, we do the perspective crops on-the-fly during the training. The projection cameras azimuth angle is randomly sampled from [0, 360] and the elevation angle is randomly sampled from 10 to 10. The cameras field of view (FOV) is randomly sampled from 45 to 80. The perspective crops are rendered with resolution of 480 720. random tone-mapping function is applied to perspective projection crops 0https://polyhaven.com/ 1https://hdrmaps.com/ 2https://hdri-skies.com/ 3https://doschdesign.com/ 4http://hdrdb.com/indoor/ 5http://hdrdb.com/outdoor/ 6https://huggingface.co/datasets/gvecchio/MatSynth 7https://www.blender.org/ 15 to generate LDR images. The tone-mappings include ACES, Filmic, AgX, and Gamma-2.4 sRGB mappings. Auto-exposure (i.e., remapping the 99-th percentile intensity to 0.9) is also randomly applied to the LDR crops. For video input, we create trajectories of projection cameras by smoothly rotating the camera angle within an angular cone of 15. Perspective crops of LDR panorama videos. Similar to the perspective crops of HDR panorama images. We on-the-fly sample perspective crops from the LDR panorama videos. Due to the lack of HDR content, we only apply random auto-exposure tone-mapping to the perspective crops. B.3 Model Details and Initialization LuxDiT is fine-tuned from the pre-trained CogVideoX-5b-I2V8. To adapt this model for our task, we replace the original text token with an image input token. This image token is generated in the same manner as the environment map noise token, but without adding noise. We reuse the models existing text-processing layers (e.g., AdaLN) to process these new image input tokens. Furthermore, we extend the input projection layer to incorporate additional conditioning channels derived from the concatenated noise token; these extended channels are initialized to zero. Similarly, the output projection layer is extended to predict dual tone-mapped environment tokens, with its newly added channels initialized from the original models weights. B.4 User Study Details for Virtual Object Insertion Following prior works [14, 16, 15, 57, 35], we conduct user study on Amazon Mechanical Turk to compare our method against baseline approaches in terms of perceptual realism for virtual object insertion. Each participant is shown pair of rendered resultsone from our method and one from baselineand asked to assess lighting realism, focusing on shadows, reflections, and overall visual integration. The specific instructions shown to participants are: Instruction: Find the inserted virtual object, look at the difference, and select the more realistic image. An AI system is trying to insert virtual object into an image in natural way. It aims to make the virtual object look as if it is part of the scene. There are two results: Trial and Trial B, and the virtual object is located in the center of each image. Please zoom in to compare the differences between the two images, and pay attention to the lighting effects such as the reflections and shadows. Which one looks more realistic? Participants are required to use monitor 24 inches or larger. Image pairs are randomly shuffled to prevent bias. Following [35], we repeat the user study three times, and recruited 11 unique participants for each experiment. We compute the percentage of images for which users preferred our method over the baseline, and report the average user preferences for three repeated experiments. In total, the study includes 11 3 11 3 = 1089 individual comparisons. B.5 Three-sphere Evaluation Protocol We adopt the three-sphere rendering setting described in StyleLight [54], with evaluation scripts provided by DiffusionLight9. For the Laval Indoor dataset, we use the same set of HDR environment maps and corresponding perspective crops as DiffusionLight. We resize and crop the input image to 480 720 for our model. For Laval Outdoor and Poly Haven environment maps, we generate perspective crops using fixed horizontal camera with 60 field of view and resolution of 480 720. For Laval Outdoor, we apply auto-exposure by scaling the 50th percentile intensity to 0.5. 8https://huggingface.co/THUDM/CogVideoX-5b-I2V 9https://github.com/DiffusionLight/DiffusionLight-evaluation"
        },
        {
            "title": "C Additional Experiments",
            "content": "C.1 Array-of-Spheres Evaluation Following prior work [60, 8], we evaluate our method using the array-of-spheres protocol, which renders grid of diffuse spheres on ground plane using the predicted environment map. We use 2,240 perspective crops from 224 Laval Indoor panoramas, provided by DiffusionLight10. All input images are resized to 512 512 to match our model input. Quantitative results are shown in Table 9 and qualitative results in Fig. 9. Table 9: Scores on indoor array-of-spheres. Method si-RMSE AE EverLight [8] StyleLight [54] Weber et al. [60] EMLight [68] DiffusionLight [44] Ours 0.091 0.123 0.081 0.099 0.090 0.089 6.36 7.09 4.13 3.99 5.25 4.90 While our method performs slightly below specialized systems like Weber et al. [60] and EMLight [68], it remains competitivedespite not being trained on Laval Indoor. Notably, it outperforms StyleLight [54] and DiffusionLight [44], demonstrating strong generalization across lighting domains. Input Image GT DiffusionLight Ours Input Image GT DiffusionLight Ours Figure 9: Visual results on array-of-spheres protocol. C.2 Lighting Estimation with the Cube++ Dataset Table 10: Scores on SpyderCube white face rendering on Cube++ dataset. We also evaluated our method on the Cube++ dataset [12], specifically designed for illumination estimation and color constancy. This dataset includes illumination information annotated by the SpyderCube 11. For our experiment, we selected 100 processed JPEG images from Cube++. We then applied both DiffusionLight and our method to estimate the illumination from each image. Subsequently, we rendered the left and right white faces of the SpyderCube under the estimated illumination, assuming purely Lambertian diffuse surfaces. To prevent information leakage from the SpyderCube in the input images, we masked out the SpyderCube from the tested images and inpainted the masked region using LaMa [50]. We then compared the rendered face colors to the colors sampled directly from the SpyderCube JPEG images. Table 10 presents the RMSE and angular errors, demonstrating that our method clearly outperforms DiffusionLight, achieving angular errors of less than 5 on both faces. Visual comparison results are further illustrated in Fig. 10. D.Light [44] 0.044 0.035 7.221 5.741 0.024 0.025 3.985 4.003 Ours RMSE Left Right Left Right Method AE 10https://github.com/DiffusionLight/image-array_of_spheres 11https://www.datacolor.com/spyder/products/spyder-cube/ 17 GT Ours D.Light GT Ours D.Light GT Ours D.Light GT Ours D.Light GT Ours D.Light Figure 10: Visual results on Cube++ dataset. We show the rendered two white cube faces, mirror ball, and matte silver ball from our method and DiffusionLight for visual comparison. C.3 Lighting Estimation from Foreground Objects Since our model is trained on object-centric synthetic rendering data, we can also apply it to estimate lighting from foreground objects. We selected 4 NeRF synthetic objects [40] and 4 real-world objects [52], aiming to estimate lighting from videos containing nine consecutive rendering views. We qualitatively compare LuxDiT with optimization-based inverse rendering methods [41, 18] that reconstruct 3D geometry and lighting from full NeRF scenes. Using the ground truth camera poses, we rotate each frames estimated lighting into the global coordinate system and average across frames to produce the final environment map. Qualitative results are shown in Fig.11. On mostly diffuse objects like lego and hotdog, our method recovers highlight directions accurately, enabling shadow rendering consistent with the input. For glossy objects like mic and ficus, our model estimates lighting nearly identical to the ground truth. While these HDR environment maps are included in our training set, the NeRF scenes differ significantly from our synthetic renderings (see Fig. 8), indicating that our model leverages shading cues and learned priors rather than direct memorization. In contrast, optimization-based baselines struggle to capture high-frequency lighting detail and often introduce noise and artifacts in lighting. We further tested our method on real-world foreground objects from the Objects-with-Lighting dataset [52], which provides ground truth distant environment lighting. Similar to the NeRF synthetic scene setup, the estimated lighting was then aligned into the global coordinate system using ground truth camera poses. We compared our approach to NeuS+Mitsuba [55, 25], the top-performing method on this dataset [52]. The metrics, using the three-sphere protocol, are presented in Table 11, with visual results in Fig. 12. While our model performs well overall, minor errors remain, e.g. color shifts in the NeRF Lego scene  (Fig. 11)  and slightly higher si-RMSE compared to NeuS+Mitsuba  (Table 11)  . We believe combining our generative model with optimization-based methods could further enhance lighting estimation, which we leave for future work. Table 11: Comparison of our method with NeuS+Mitsuba on Objects with Lighting datasets. Method Scale-invariant RMSE Normalized RMSE Angular Error Diffuse Matte Mirror Diffuse Matte Mirror Diffuse Matte Mirror NeuS+Mitsuba Ours 0.082 0.086 0.232 0. 0.424 0.482 3.145 1.262 3.383 1.594 3.526 2.000 0.180 0.153 0.545 0. 0.717 0.479 18 GT & Input NVDIFFREC [41] NVDIFFRECMC [18] Ours Figure 11: Lighting estimation from the NeRF synthetic objects. We use the estimated lighting from different methods to re-render the original NeRF Blender scenes. Figure 12: Lighting estimation from the masked real objects from Objects with Lighting. C.4 Additional Ablations C.4.1 The Choice of the HDR Fusion Model As detailed in Sec. 4.1, lightweight MLP Ïˆ is employed to merge the dual-tonemapped environment maps, Eldr and Elog, thereby reconstructing the HDR environment map Ë†E. There are also alternative fusion methods, such as using more complex CNN model to incorporate adjacent pixel information for HDR fusion, or applying rule-based approach with explicit inverse equations. To justify our choice of simple MLP, we evaluate various HDR fusion techniques, including MLP, CNN, and rule-based method. The CNN model has an identical number of layers to our MLP model, using 3 3 convolution kernels across layers. The rule-based method involves applying the inverse Reinhard map for lights with intensity below 8, linear interpolation between Reinhard and log maps for intensities ranging from 8 to 16, and exclusively the log map for intensities exceeding 16. Table 12 presents the RMSE results on testing Polyhaven HDRIs. All three methods demonstrate comparable accuracy, with the MLP approach exhibiting slight advantage. Compared to the rule-based approach, we believe the neural approach can better handle numerical inconsistency after image uint8 quantization, and the potential data range overflow (e.g., lights beyond the pre-defined maximum intensity 10000). Table 12: Comparison on different HDR fusion approaches. MLP CNN Rule RMSE 11.55 11.74 11.71 19 C.4.2 The Impact of LoRA on Synthetic Scenes Section 5.5 demonstrates the impact of varying LoRA scales (0.0 to 1.0) on the predicted lighting content of real-world images. This ablation study, conversely, investigates how our LoRA model, trained with real images, affects the lighting estimation of synthetic foreground objects. Table 13 presents the angular errors using three-sphere evaluation, and Fig. 16 provides the visual results. Table 13: Ablation study on impact of LoRA scale on synthetic foreground objects. LoRA Scale Diffuse Matte Mirror 0.00 1.594 2.068 3.405 0.25 1.737 2.311 3. 0.50 2.170 2.914 4.342 0.75 3.832 5.322 6.783 1.00 3.937 5.891 7. In contrast to the ablation performed on scene images, larger LoRA scale leads to lower lighting estimation accuracy. As Fig. 16 illustrates, increasing the LoRA scale causes foreground content to gradually appear on the estimated environment map, which is consistent with our LoRA models behavior. Nevertheless, the estimated highlights remain consistent across different LoRA scales."
        },
        {
            "title": "D Additional Results",
            "content": "We provide additional visual results in this section to further support the claims made in the main paper. Model Ablation and LoRA Scale: Figure 13 details the ablation study on our models design and the exploration of different LoRA scales. Camera Parameter Variations: Figures 14 and 15 show lighting estimation performance when varying camera field of view (FOV) and elevation angles, respectively. Three-Sphere Rendering Evaluations: Figures 17, 18, and 19 display further lighting estimation outcomes using the three-sphere rendering protocol on the Laval Indoor, Laval Outdoor, and Poly Haven datasets. Virtual Object Insertion: Figures 20 and 21 illustrate additional virtual object insertion results on Poly Haven panorama crops and Waymo driving scenes. 20 Input Image Model Ablation channel concat. LoRA 0.0 LoRA Scale Exploration LoRA 0.1 LoRA 0.2 LoRA 0.4 w/o syn. data LoRA 0.6 LoRA 0. LoRA 0.8 LoRA 1.0 Figure 13: Model design ablation and LoRA scale exploration. The Model Ablation column shows the results of our two model design variants: 1) channel concatenation and 2) training without synthetic rendering data. The LoRA Scale Exploration columns show the visual results of our model with different LoRA scales. 45 50 60 70 75 FOV GT Figure 14: Lighting estimation from input images with varying camera FOV. Elevation 30 15 0 15 30 GT Figure 15: Lighting estimation from input images with varying camera elevation. Input Image GT LoRA Scale Exploration LoRA 0.00 LoRA 0.25 LoRA 0. LoRA 0.75 LoRA 1.00 Figure 16: LoRA scale exploration on synthetic foreground scenes. 21 Input Image GT StyleLight DiffusionLight Ours Figure 17: Additional qualitative results on Laval Indoor dataset. 22 Input Image GT H-G et al. NLFE DiffusionLight Ours Figure 18: Additional qualitative results on Laval Outdoor dataset. 23 Input Image GT StyleLight NLFE DiffusionLight Ours Figure 19: Additional qualitative results on Poly Haven dataset. 24 Reference StyleLight DiffusionLight DiPIR Ours Figure 20: Additional virtual object insertion on Poly Haven perspective crops. H-G et al. [22] NLFE DiffusionLight DiPIR Ours Figure 21: Additional virtual object insertion on Waymo driving scenes."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "University of Toronto",
        "Vector Institute"
    ]
}