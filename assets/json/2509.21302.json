{
    "paper_title": "Quantized Visual Geometry Grounded Transformer",
    "authors": [
        "Weilun Feng",
        "Haotong Qin",
        "Mingqiang Wu",
        "Chuanguang Yang",
        "Yuqi Li",
        "Xiangqi Li",
        "Zhulin An",
        "Libo Huang",
        "Yulun Zhang",
        "Michele Magno",
        "Yongjun Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning-based 3D reconstruction models, represented by Visual Geometry Grounded Transformers (VGGTs), have made remarkable progress with the use of large-scale transformers. Their prohibitive computational and memory costs severely hinder real-world deployment. Post-Training Quantization (PTQ) has become a common practice for compressing and accelerating models. However, we empirically observe that PTQ faces unique obstacles when compressing billion-scale VGGTs: the data-independent special tokens induce heavy-tailed activation distributions, while the multi-view nature of 3D data makes calibration sample selection highly unstable. This paper proposes the first Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two technical contributions: First, we introduce Dual-Smoothed Fine-Grained Quantization, which integrates pre-global Hadamard rotation and post-local channel smoothing to mitigate heavy-tailed distributions and inter-channel variance robustly. Second, we design Noise-Filtered Diverse Sampling, which filters outliers via deep-layer statistics and constructs frame-aware diverse calibration clusters to ensure stable quantization ranges. Comprehensive experiments demonstrate that QuantVGGT achieves the state-of-the-art results across different benchmarks and bit-width, surpassing the previous state-of-the-art generic quantization method with a great margin. We highlight that our 4-bit QuantVGGT can deliver a 3.7$\\times$ memory reduction and 2.5$\\times$ acceleration in real-hardware inference, while maintaining reconstruction accuracy above 98\\% of its full-precision counterpart. This demonstrates the vast advantages and practicality of QuantVGGT in resource-constrained scenarios. Our code is released in https://github.com/wlfeng0509/QuantVGGT."
        },
        {
            "title": "Start",
            "content": "Preprint"
        },
        {
            "title": "QUANTIZED VISUAL GEOMETRY GROUNDED\nTRANSFORMER",
            "content": "Weilun Feng1,2, Haotong Qin3, Mingqiang Wu1,2, Chuanguang Yang1, Yuqi Li1, Xiangqi Li1,2, Zhulin An1, Libo Huang1, Yulun Zhang4, Michele Magno3, Yongjun Xu1 1Institute of Computing Technology, Chinese Academy of Sciences 2University of Chinese Academy of Sciences {fengweilun24s,yangchuanguang,lixiangqi24s,anzhulin,xyj}@ict.ac.cn {haotong.qin,michele.magno}@pbl.ee.ethz.ch, wumingqiang25@mails.ucas.ac.cn, {yuqili010602,www.huanglibo,yulun100}@gmail.com 4Shanghai Jiao Tong University 3ETH Zurich 5 2 0 2 5 2 ] . [ 1 2 0 3 1 2 . 9 0 5 2 : r Figure 1: QuantVGGT effectively quantizes VGGT (Wang et al., 2025a) to W4A4 without compromising visual quality while bringing 2.5 speedup and 3.7 compression."
        },
        {
            "title": "ABSTRACT",
            "content": "Learning-based 3D reconstruction models, represented by Visual Geometry Grounded Transformers (VGGTs), have made remarkable progress with the use of large-scale transformers. Their prohibitive computational and memory costs severely hinder real-world deployment. Post-Training Quantization (PTQ) has become common practice for compressing and accelerating models. However, we empirically observe that PTQ faces unique obstacles when compressing billion-scale VGGTs: the data-independent special tokens induce heavy-tailed activation distributions, while the multi-view nature of 3D data makes calibration sample selection highly unstable. This paper proposes the first Quantization framework for VGGTs, namely QuantVGGT. This mainly relies on two technical contributions: First, we introduce Dual-Smoothed Fine-Grained Quantization, which integrates pre-global Hadamard rotation and post-local channel smoothing to mitigate heavy-tailed distributions and inter-channel variance robustly. Second, we design Noise-Filtered Diverse Sampling, which filters outliers via deep-layer statistics and constructs frame-aware diverse calibration clusters to ensure stable quantization ranges. Comprehensive experiments demonstrate that QuantVGGT achieves the state-of-the-art results across different benchmarks and bit-width, surpassing the previous state-of-the-art generic quantization method with great margin. We highlight that our 4-bit QuantVGGT can deliver 3.7 memory reduction and 2.5 acceleration in real-hardware inference, while maintaining reconstruction accuracy above 98% of its full-precision counterpart. This demonstrates the vast advantages and practicality of QuantVGGT in resource-constrained scenarios. Our code is released in https://github. com/wlfeng0509/QuantVGGT. Equal contribution. Corresponding authors: Zhulin An, anzhulin@ict.ac.cn; Chuanguang Yang, yangchuanguang@ict.ac.cn 1 Preprint"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in learning-based 3D reconstruction have demonstrated unprecedented capabilities in recovering dense geometry and camera trajectories directly from image sequences. Traditional approaches (Mur-Artal et al., 2015; Mur-Artal & Tardos, 2017; Schonberger & Frahm, 2016; Hartley & Zisserman, 2003) are grounded in geometric priors and optimization, but their reliance on handcrafted design choices and iterative solvers often leads to limited scalability and reduced robustness in complex scenes. In contrast, large-scale deep models have shifted the paradigm toward datadriven frameworks, offering remarkable generalization ability across diverse environments (Wang et al., 2025b; Yang et al., 2025). milestone in this evolution is the Visual Geometry Grounded Transformer (VGGT) (Wang et al., 2025a). This 1.2B-parameter model unifies multiple 3D tasks, including dense depth estimation, point map regression, camera pose prediction, and point tracking within single forward pass, consistently surpassing task-specialized counterparts. Despite its success, the billion-scale parameterization of VGGT incurs prohibitive computational and memory costs, severely restricting its deployment in real-world scenarios. Model quantization (Gholami et al., 2022; Jacob et al., 2018) is an effective compression technique by converting weights and activations of model from high-precision floating-points to low-precision integers. While this technique has been widely validated in large language models (Frantar et al., 2022; Xiao et al., 2023) and 2D vision models (Yuan et al., 2022; Wu et al., 2024), the quantization of billionscale 3D reconstruction transformers such as VGGT remains largely unexplored. In our study, we identify two model-specific properties of VGGT that make its quantization particularly challenging: ❶ The presence of data-independent special tokens (camera and register tokens). Unlike regular image tokens that are encoded from input images, these tokens are pretrained and injected into image tokens to encode global context and cross-view geometry. This data-independent property causes activation distributions to deviate from typical patterns, amplifying heavy tails and producing extreme channel and token variance. These skewed statistics are unfriendly to standard quantization, leading to substantial information loss. ❷ The inherently semantic complexity of 3D data. Each input sequence involves non-identical and complex views, meaning that the underlying semantic space is both high-dimensional and highly redundant. For quantization calibration, the ideal process is to perceive the expected major data distribution. If calibration samples are rare outliers and not diverse, the estimated quantization ranges become biased and fail to generalize, causing performance degradation across unseen scenes. Thus, sample diversity and representativeness are far more critical than in 2D vision tasks. To address these challenges, we present the first systematic investigation of Post-Training Quantization (PTQ) for VGGT and propose tailored framework, QuantVGGT. Our approach introduces Dual-Smoothed Fine-Grained Quantization (DSFQ), which mitigates skewed statistics by combining (1) pre-global rotation via Hadamard transforms to disperse outliers and smooth heavy-tailed distributions, and (2) post-local smoothing step that normalizes channel-level variance in the rotated space. Additionally, to overcome calibration instability, we design Noise-Filtered Diverse Sampling (NFDS), which leverages deep-layer activation statistics to filter noisy extremes and employs frame-aware clustering aligned with VGGTs inductive biases. Together, these components yield robust, efficient, and accurate quantization of billion-scale 3D reconstruction transformers. Our contributions are summarized as follows: 1. We provide the first systematic analysis of PTQ on VGGT, highlighting quantization challenges rooted in its data-independent tokens and multi-view activation statistics. 2. We propose dual-stage smoothing scheme that globally disperses heavy-tailed distributions and locally balances channel variance, significantly reducing quantization errors. 3. We design calibration strategy that filters outliers and utilizes VGGTs inductive bias to construct frame-aware clusters, ensuring representative and stable calibration set. 4. Extensive experiments demonstrate that our approach enables effective low-bit quantization of VGGT, achieving substantial memory and inference efficiency gains while preserving reconstruction accuracy. 2 Preprint Figure 2: Overview of proposed QuantVGGT. Top: Our proposed Dual-Smoothed Fine-Grained Quantization architecture. Bottom: Our proposed Noise-Filtered Diverse Sampling strategy."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 LEARNING-BASED 3D RECONSTRUCTION Thanks to the development of deep learning technology in recent years, 3D reconstruction tasks have gradually shifted from traditional methods (Mur-Artal & Tardos, 2017; Mur-Artal et al., 2015; Schonberger & Frahm, 2016; Hartley & Zisserman, 2003) that rely heavily on prior knowledge to data-driven learning-based methods. Due to the large-scale training process, learning-based methods (Wang et al., 2025b; Yang et al., 2025) often achieve better reconstruction performance and generalization ability. DUSt3R (Wang et al., 2024) predicts the 3D point maps of scene by regressing two RGB images, laying the foundation for learning-based methods. MASt3R (Leroy et al., 2024) further refines the framework by introducing confidence-weighted losses for metric scale approximation. Current model, VGGT (Wang et al., 2025a) enables predicting camera position, dense depth, point maps, and point tracking with single forward process. With scaling-up to 1.2B parameters, VGGT achieves state-of-the-art results across various 3D tasks with even surpasses some task-specified methods. But up to billions of parameters and enormous computational complexity of VGGT severely limit its widespread deployment and application. However, the compression methods for VGGT, such as quantization, are still highly unexplored. 2.2 MODEL QUANTIZATION Model quantization (Gholami et al., 2022; Krishnamoorthi, 1806) significantly reduces the memory footprint and accelerates inference by reducing the data bit-width. Model quantization can be mainly divided into Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). QAT (Jacob et al., 2018; Qin et al., 2020b) utilizes substantial data to train quantization parameters and model weights, thus typically ensuring good performance at extremely low bits. But QAT often requires massive training resources. On the contrary, PTQ (Wei et al., 2024; Frantar et al., 2022) only requires little calibration data to fine-tune the quantization parameters, and therefore can be applied to large models. For PTQ, BRECQ (Li et al., 2021) builds the block-wise reconstruction framework, and QDrop (Wei et al., 2022) further enhances the performance by randomly dropping quantization activations. To ensure the effectiveness of PTQ on large models, GPTQ (Frantar et al., 2022) utilizes approximate second-order gradient to optimize Large Language Models. To address the impact of imbalanced distribution on quantization, SmoothQuant (Xiao et al., 2023) introduces smoothing parameter to transfer the difficulty of activation quantization to weight. QuaRot (Ashkboos et al., 2024) adopts similar rotation to smooth the distribution. Although these methods perform well on existing 2D-visual and language models, they do not generalize well to large-scale 3D models like VGGT (Wang et al., 2025a). To the best of our knowledge, our method is the first PTQ framework specially designed for VGGT, ensuring its performance even at low-bit quantization. 3 Preprint"
        },
        {
            "title": "3 METHODS",
            "content": "3.1 PRELIMINARY 3.1.1 VISUAL GEOMETRY GROUNDED TRANSFORMER Visual Geometry Grounded Transformer (VGGT) (Wang et al., 2025a) is recent architecture designed to predict all key 3D attributes from image sequences of arbitrary length. Its core components are tokenization and token registration. For any input sequence = {Ii}N i=1 of RGB frames, VGGT first tokenizes each frame using pretrained vision backbone F(), such as DINOv2 (Oquab et al., 2023), producing = {xi xi = F(Ii)}N i=1, xi Rnd, (1) where denotes the token length after patching and is the feature dimension. To enable multi-attribute reasoning, VGGT augments each frame with one camera token and four register tokens, which are responsible for aggregating different 3D attributes (e.g., camera parameters, scene geometry). Notably, VGGT introduces two distinct sets of these special tokens: one set tf R5d is reserved for the first frame, while another set to R5d is shared by all subsequent frames. Formally, the token registration process is defined as ˆX = {ˆxi ˆx1 = concat(x1, tf), ˆxk=1 = concat(xk, to)}N i=1, (2) and the resulting ˆX is then forwarded into the VGGT backbone. 3.1.2 POST-TRAINING QUANTIZATION Quantization (Gholami et al., 2022; Krishnamoorthi, 1806) aims to convert model weights and activations from floating-point representations into compact low-bit integer formats, thereby reducing both computational cost and memory footprint. Formally, given floating-point vector x, the symmetric quantization procedure can be described as: xint = clamp (cid:16) round(cid:2) (cid:3), 2N 1, 2N 1 1 (cid:17) , = max(x) 2N 11 , (3) where represents the target bit-width, round() denotes the rounding operator, and clamp() ensures that the integer values remain within the valid range [2N 1, 2N 1 1]. Among quantization paradigms, Post-Training Quantization (PTQ) (Wei et al., 2024; Frantar et al., 2022; Feng et al., 2025b) is widely applied for its efficiency. Unlike Quantization-Aware Training (Qin et al., 2020a; Feng et al., 2025a;c), PTQ does not require fine-tuning the weights. Instead, it fine-tunes the quantization parameters using only relatively small calibration dataset calib, while keeping the original full-precision weights fixed. This makes PTQ particularly attractive in real-world deployment where computational resources for fine-tuning are limited. Following the standard practice in prior works (Yuan et al., 2022; Shang et al., 2023; Xiao et al., 2023), the quantization error is typically measured by the following objective: Lquant = ExDcalib (cid:2)θf (x) θq(x)2 2 (cid:3), (4) where θf and θq denote the full-precision and quantized model functions, respectively. 3.2 DUAL-SMOOTHED FINE-GRAINED QUANTIZATION Observation 1. VGGT (Wang et al., 2025a) exhibits highly skewed numerical distributions, which are amplified by data-independent tokens (camera and register tokens), leading to substantial quantization errors. As illustrated in Fig. 3b, these data-independent tokens (first 5 tokens) amplify channel and token numerical variance: with massive outliers that are much larger than regular patch tokens, producing Preprint (a) Original distribution. (b) Registered tokens. (c) Naive rotation. (d) Dual-Smoothed. Figure 3: The motivation and effect of Dual-Smoothed Fine-Grained Quantization. (a): Salient distribution of VGGT (Wang et al., 2025a) frame block 9. (b):Saliency of registered tokens. (c): Distribution after naive rotation. (d): Distribution after our dual-smooth. We provide more analysis in Appendix Sec. D. heavy-tailed distributions. When passed into quantization, these few large elements occupy most of the quantization bins, causing severe numerical distortion (Xiao et al., 2023; Ashkboos et al., 2024). Pre-Global-Rotation. Motivated by rotation-based quantization (Ashkboos et al., 2024; Zhao et al., 2024), we apply Hadamard transformation to spread out the impact of special-token-induced outliers. Hadamard matrix {1, +1}dindin satisfies HH = I. Given activation Rndin and weight Rdoutdin , the matrix multiplication invariance is preserved as follows: XW = (XH)(WH). (5) Lemma 3.1. Due to the central limit effect, the distribution of values after Hadamard rotation tends to approximate Gaussian, thereby smoothing the heavy-tailed distribution introduced by special tokens (Tseng et al., 2024). Lemma 3.1 suggests that the Hadamard rotation disperses outlier values across channels, resulting in more uniform distribution, thereby significantly reducing their impact. Therefore, the original distribution becomes concentrated and smoother, which is more favorable for quantization. Figure 3c illustrates the smoothed distributions after the Hadamard rotation, where the extremely massive outliers are mitigated. Post-Local-Smooth. Although the Hadamard rotation mitigates global skew, the transformed distribution still exhibits considerable local variance, as shown in Fig. 3c. While the Hadamard rotation spreads outliers across channels, it only weakens the global outliers, rather than eliminating the outliers within individual channels. To further reduce quantization error, we introduce channel-wise scale to normalize the internal channel distributions: ˆci = max(XiH)α max(WiH)1α , XW = (XHdiag(ˆc)1)(diag(ˆc)HW), (6) where α balances quantization difficulty between activations and weights (typically α = 0.5). Unlike traditional scaling (Xiao et al., 2023; Wu et al., 2024), our scheme derives scale factors from the rotated distribution, ensuring robustness against extreme special-token values. This design offers two advantages: (1) the scale factor is derived from smoother distribution after the pre-rotation, avoiding extreme values that could otherwise complicate weight quantization; and (2) it ensures that the post-scaled distribution even smoother. If using pre-scale, the post-rotation would destabilized the benefits of channel-scaling. The scale factors can also be fused into neighboring layers (Xiao et al., 2023), introducing no runtime cost. Fine-Grained Quantization Granularity. The above rotate-and-scale quantization strategy reduces quantization error by addressing the inner-dimension din. However, the choice of quantization granularity also plays critical role in determining the overall error. Recent studies (Chee et al., 2023; Tseng et al., 2024) define the quantization difficulty using the concept of µ-coherent, where g, with representing the number of elements, where µ represents for any x, if max(x) µxF / the quantization difficulty. This suggests that reducing quantization granularity, when feasible, can significantly lower quantization error. From hardware perspective, as long as the quantized matrix multiplication shares the same quantization parameters across the summation operation, there is no 5 Preprint need to convert integers back to floating-point numbers, ensuring efficiency. In matrix multiplication, only the inner-channel din values are summed. Therefore, we can utilize the outer-dimension dout for weight quantization and the token dimension for activation quantization. In practice, we apply out-dimension-wise quantization to the weights and token-wise quantization to the activations. As shown in Fig. 3d, the proposed dual-smoothed fine-grained quantization further reduces the outer-dimension variance in the data distribution, significantly lowering the quantization error, with nearly no additional computational burden (see Appendix Sec. for efficiency analysis). 3.3 NOISE-FILTERED DIVERSE SAMPLING The purpose of the PTQ calibration process is to approximate the behavior of the model in the real data distribution using small calibration set Dcalib. Formally, we seek θ = arg min θq ExX (cid:2)θf (x) θq(x)2 2 (cid:3), (7) and in practice we approximate the outer expectation with samples from Dcalib. Therefore, the calibration set should be statistically representative of . Theorem 3.2 (Calibration sampling principle). Suppose can be divided into different domains = {X0, X1, }. Each sub-domain Xi has scale and can be partitioned into i( 2 and finite) disjoint sub-regions denoted as {Ri i} with corresponding scales {V K} where = 0, , xs E(X ) denotes expectation input. When satisfies p(xs for xs D, then maximizes the information reflecting in expectation. 1, , Ri i}. Considering constructed sample set = {xs ) = 1 , , R Theorem 3.2 implies that to construct an effective calibration set we should: (1) partition the data space into meaningful regions (subdomains) and (2) draw samples from each region in proportion In practical settings where Vk is unknown, robust strategy is to cluster the to its prevalence. dataset into regions and then sample uniformly inside each cluster (this approximates proportional representation under mild assumptions). (a) Layer distribution. (b) Label-Clustered. (c) Feature-Clustered. (d) Our-Clustered. Figure 4: The motivation and effect of Noise-Filtered Diverse Sampling. (a): Layer distribution of VGGT (Wang et al., 2025a). (b): Visualization of label-clustered. (c): Visualization of featureclustered. (d): Visualization of our-clustered. We provide more analysis in Appendix Sec. E. Observation 2. Activations in deeper layers tend to be distinctive, with the majority of samples being highly concentrated, while few samples are outliers as shown in Fig. 4a. For the expected distribution, we prefer representative distribution while the outliers are spiking samples with minimal density. However, when we divide the subdomains and sample within, the selected probability of outliers is increased, which disrupts our expected distribution. Therefore, we propose first to filter the noisy outliers using deep layer statistics for each candidate sample xi D: mi,j := mean (cid:0) layerj(xi)(cid:1), si,j := var (cid:0) layerj(xi)(cid:1), L, (8) where is all used layers union, is the candidate samples union, layerj() denotes the activation in j-th layer. We then compute noise-score using global robust moments: 6 Preprint µj = 1 (cid:88) mi,j, σj = (cid:88) si,j, τj = (cid:115) 1 (cid:115) 1 νj = 1 (mi,j µj)2 + ε, (cid:88) (cid:88) (si,j νj)2 + ε, (9) score(xi) = (cid:115) (cid:88) jL (cid:16) mi,j µj σj (cid:17)2 + (cid:88) jL (cid:16) si,j νj τj (cid:17)2 , where ε is small constant for numerical stability. We then filter out high-noise samples by thresholding the score: Dfiltered = {xi score(xi) }, (10) where is set by percentile (e.g. keep the lowest p% scores). This filtering keeps samples close to the typical distribution and removes outliers that would otherwise skew quantization calibration. Observation 3. Feature clusters based on raw labels are sub-optimal for visual-geometry tasks. We visualized the distribution of different samples and their corresponding labels in Fig. 4b and Fig. 4c. We found that the feature of these samples is highly concentrated and difficult to divide, and using labels directly as classification criteria achieves sub-optimal results. Geometry samples are usually complex scene containing multiple objects. Therefore, labels often do not directly represent their semantic information. However, we identify that VGGT (Wang et al., 2025a) contains strong inductive bias: it models the relative relationship between the first frame and subsequent frames. This motivates structural metric derived from frame-wise features. Given output feature Ai Rnd of sample xi with = (spatial tokens per frame and frames). We first reshape Ai into frame-wise vectors and construct compact frame-aware correlation vector ci Rf 1 by measuring the normalized similarity between the first frame and each subsequent frame: Ai (cid:101)Ai = (cid:2) ai 1 1, . . . , ai 0, ai 0, ai ai 02 ai t2 , ai ci = (cid:3) Rf ˆd, ˆd := d, = 1, . . . , 1. (11) We then cluster the set {ci}xiDfiltered using K-Means to obtain regions = {R1, . . . , RK}. According to Theorem 3.2, uniform sampling within each region yields calibration set that better reflects . Concretely: Rk = {xi Dfiltered ˆyi = k}, Dcalib = (cid:91) k=1 Ω(Rk), (12) where ˆyi is the cluster assignment and Ω() denotes uniform sampler. This Noise-Filtered Diverse Sampling pipeline reduces the influence of noisy outliers, leverages VGGTs frame-relative inductive bias to form semantically meaningful clusters as shown in Fig. 4d, and yields calibration set that better approximates the true data distribution for PTQ."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL AND EVALUATION SETTINGS Evaluation Settings. We select VGGT-1B (Wang et al., 2025a) as our base model and conduct all the quantization experiments on it. To validate the effectiveness of our proposed method, we conduct camera pose estimation experiments on Co3Dv2 (Reizenstein et al., 2021) and point map estimation 7 Preprint Table 1: Camera Pose Estimation on Co3Dv2 (Reizenstein et al., 2021). Bold: The best result. Method Bit-Width (W/A) Full Prec. 16/16 RTN BRECQ QDrop DopQ-ViT GPTQ SmoothQuant Quarot QuantVGGT RTN BRECQ QDrop DopQ-ViT GPTQ SmoothQuant Quarot QuantVGGT 8/8 8/8 8/8 8/8 8/8 8/8 8/8 8/ 4/4 4/4 4/4 4/4 4/4 4/4 4/4 4/4 frames=10 frames=20 AUC@30 AUC@15 AUC@5 AUC@3 AUC@30 AUC@15 AUC@5 AUC@3 89.5 88.1 88.3 88.8 88.9 89.1 89.1 89.4 89. 77.7 78.8 79.1 80.3 80.5 75.4 81.8 86.9 83.2 80.7 81.2 81.8 81.8 82.6 82.5 83.0 83.1 63.9 65.2 66.7 68.3 68.6 60.1 70.3 78.7 66.1 60.3 61.2 61.9 63.2 64.0 64.8 65.9 66. 31.4 34.3 35.7 38.3 38.7 25.8 40.1 57.3 54.9 46.7 48.7 49.1 51.5 52.1 53.2 54.6 54.8 16.6 20.1 22.0 23.3 23.2 12.3 23.5 43.6 90.0 88.1 88.2 88.5 88.8 89.1 89.1 89.4 89. 75.8 78.8 79.2 80.4 80.6 75.4 81.6 88.2 83.9 80.6 81.2 81.8 81.8 82.6 82.5 83.0 83.2 60.7 65.3 66.7 68.4 68.7 60.1 69.8 80.2 67.8 60.2 61.0 61.8 63.1 63.2 64.6 66.0 66. 26.5 34.1 35.6 35.5 35.6 25.5 39.4 58.9 56.9 46.5 48.8 49.1 51.4 51.5 53.1 54.7 54.9 12.8 20.0 21.9 22.0 22.1 12.1 22.6 45.1 experiments on DTU (Jensen et al., 2014). For the quantization setting, we select two of the most widely studied bit settings W8A8 (8-bit weight and 8-bit activation quantization) and W4A4, as they have better hardware adaptability and bring more acceleration and compression effects Xiao et al. (2023); Ashkboos et al. (2024). More details can be found in Appendix Sec. B. Baseline Methods. For quantization baseline methods, we adopt the commonly used Post-Training Quantization baseline Round-To-Nearest (RTN), BRECQ (Li et al., 2021), and QDrop (Wei et al., 2022). For 2D-vision transformer baseline, we select strong DopQ-ViT (Yang et al., 2024). For language transformer baseline, we select strong GPTQ (Frantar et al., 2022), SmoothQuant (Xiao et al., 2023), and Quarot (Ashkboos et al., 2024). 4.2 MAIN RESULTS 0.779 0.694 2.232 0.714 1. 1.313 16/16 Acc. N.C. Method Full Prec. Bit-Width (W/A) Table 2: Point Map Estimation on DTU (Jensen et al., 2014). RTN BRECQ QDrop DopQ-ViT GPTQ SmoothQuant Quarot QuantVGGT Comp. Mean Med. Mean Med. Mean Med. Camera Pose Estimation. We conduct camera pose estimation experiments using VGGT-1B (Wang et al., 2025a) on Co3Dv2 dataset (Reizenstein et al., 2021). Following prior works (Wang et al., 2025a), we randomly sample 10 frames for evaluation and further expand to 20 frames for more generalized evalThe results are presented uation. in Tab. 1. the relatively Under simpler W8A8 setting, most quantization methods can maintain relinatively good performance but evitably experience certain performance degradation. Quantvggt preserves 99.9% performance under W8A8, with AUC@30 of 89.4 and 89.5 for FP (Full Precision). For the more aggressive W4A4 setting, all quantization methods showed significant performance degradation, such as current SOTA method Quarot (Ashkboos et al., 2024) only achieving 81.6 AUC@30 under 20 frames. While, QuantVGGT still achieved 88.2, maintaining 98% of the models performance. QuantVGGT can achieve significant performance improvements even under extreme quantization settings compared to existing methods, demonstrating its quantization friendliness towards 3D reconstruction models. RTN BRECQ QDrop DopQ-ViT GPTQ SmoothQuant Quarot QuantVGGT 0.757 0.762 0.754 0.755 0.761 0.764 0.757 0.774 1.099 1.082 1.073 1.075 1.068 1.083 1.096 1.068 0.930 0.924 0.912 0.906 0.899 0.944 0.916 0.743 0.773 0.774 0.780 0.783 0.778 0.776 0.778 0.788 1.310 1.292 1.297 1.290 1.303 1.281 1.311 1.276 0.730 0.725 0.720 0.712 0.721 0.719 0.712 0. 0.656 0.662 0.673 0.673 0.675 0.675 0.670 0.681 2.028 2.024 2.012 2.003 1.997 1.993 2.034 1.992 1.700 1.688 1.642 1.587 1.442 1.740 1.593 1.282 0.687 0.690 0.692 0.691 0.688 0.692 0.694 0.700 2.237 2.236 2.239 2.235 2.226 2.229 2.231 2.215 1.216 1.212 1.204 1.200 1.196 1.201 1.184 1. 4/4 4/4 4/4 4/4 4/4 4/4 4/4 4/4 8/8 8/8 8/8 8/8 8/8 8/8 8/8 8/8 Point Map Estimation. To comprehensively evaluate the generalized quantization performance of VGGT, we further extend the experiment to the point map estimation task on DTU dataset (Jensen 8 Preprint et al., 2014). For evaluation, we sample keyframes every 5 images. The results are presented in Tab. 2. It is worth mentioning that the calibration dataset is all from Co3Dv2 training set, meaning that the DTU data are unknown for the calibration process. We identify that all existing quantization methods still show certain performance degradation even under W8A8. However, QuantVGGT still generalizes well on the point map estimation task, with even improved metrics compared with the FP model under W8A8. For W4A4 setting, all existing methods show notable performance degradation, like Quarot with ACC. of only 1.593. While QuantVGGT achieves ACC. of 1.282, significantly closer to the FP performance of 1.185. This demonstrates QuantVGGTs ability to adapt to large 3D models such as VGGT quantization, and can maintain strong generalization ability with an efficient PTQ process. 4.3 ABLATION STUDY To validate the effectiveness of each proposed component, we conduct an ablation study. All the experiments are conducted under W4A4 quantization setting on Co3Dv2 (Reizenstein et al., 2021). Method Table 3: Ablation study on quantization architecture. Quantization Architecture. We first validate the proposed Dual-Smoothed FineGrained Quantization (DSFQ) and present the result in Tab. 3. We denote naive quantization without any smoothing as Base. We further compare with the rotation-only (Rotation) and scale-only (Scale) methods with our proposed DSFQ. Naive quantization shows significant performance collapse with AUC@3 of only 9.7. While scale-based and rotation-based methods further smoothed data distribution and showed certain improvement, they still exhibit inevitable degradation. While our proposed DSFQ combines the advantages of both rotation and scale and utilizes fine-grained quantization granularity, greatly preserves the performance. AUC@30 AUC@15 AUC@5 AUC@3 89. 23.9 46.3 38.5 57.3+11.0 9.7 32.5 21.2 43.6+11.1 76.9 83.6 81.9 86.9+3.3 61.5 72.3 70.1 78.7+6.4 Base Rotation Scale DSFQ Full Prec. 54.9 83.2 66.1 Sampling Strategy. We then validate the proposed Noise-Filtered Diverse Sampling (NFDS) and show the result in Fig. 5. We denote the naive random sampling strategy as Random. We then compare with random sampling from outlier-filtered dataset (Filtered) and sampling from frame-based clustered dataset without filtering (Clustered) strategy. All the results are conducted under five different random seeds. We present the mean performance and its corresponding variance in the bar plot. Random selection not only fails to guarantee diversity but also results in significant variance due to the influence of outliers. The filtered data quality was improved, and the variance was significantly reduced. Our clustering method significantly enhances diversity and improves average performance, but there is still variance due to the presence of outliers. The final combined NFDS ensures both the removal of outliers and well diversity, ensuring average performance while being more stable. Figure 5: Ablation study on sample strategy. We present more ablation studies on quantization architecture and sampling strategy in Appendix Sec. and Sec. E. 4.4 EFFICIENCY ANALYSIS To verify the deployment efficiency of quantized VGGT, we report the hardware latency in Fig. 6. Compared with naive quantization without any smooth techniques, our dual-smoothed finegrained quantization only brings additional 0.2% latency cost at W4A4, while significantly preserving the quantized model performance. Our W4A4 QuantVGGT even surpass the naive W8A8 performance, and with significant performance gap of naive W4A4. This indicates that our VGGT-specialized quantization scheme greatly outperforms the existing naive quantization with little extra burden. We further report the memory optimization and the calibration costs in Appendix Sec. F. 9 Figure 6: Normalized latency and corresponding performance under 20 frames. Preprint"
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we propose the first Post-Training Quantization (PTQ) framework for VGGTs, namely QuantVGGT. Specifically, we identify the quantization-unfriendly distribution brought by dataindependent tokens and the highly unstable calibration dataset inherent in 3D multi-view data. We then propose Dual-Smoothed Fine-Grained Quantization to smooth the heavy-tailed distribution. We also design Noise-Filtered Diverse Sampling to constructs frame-aware diverse calibration clusters to ensure stable dataset. Extensive experiments demonstrate that QuantVGGT achieves state-of-theart performance under different bit-widths and greatly surpasses existing quantization methods."
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "This research strictly adheres to the ICLR Code of Ethics with no ethics-related risks: it uses public open-source models (VGGT (Wang et al., 2025a)) and focuses on algorithmic innovation for inference acceleration and compression, without involving scenarios endangering public safety, infringing privacy, or producing discrimination."
        },
        {
            "title": "7 REPRODUCIBILITY STATEMENT",
            "content": "To ensure reproducibility, experimental configurations, method details, and evaluation metrics are thoroughly described in Sec. 4.1 and Appendix Sec. B. Experimental results of comparative methods are sourced from public literature, and our experiments strictly follow the same configurations as baseline methods for fair comparison. Complete source code for reproducing results will be publicly released upon paper publication. The raw reconstruction files are attached in the supplementary materials. For the theorem used in the paper, we also provided detailed proof in Appendix Sec. A."
        },
        {
            "title": "REFERENCES",
            "content": "Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. 3, 5, 8, 14 Dejan Azinovic, Ricardo Martin-Brualla, Dan Goldman, Matthias Nießner, and Justus Thies. Neural rgb-d surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 62906301, 2022. 14 Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36:43964429, 2023. 5 Weilun Feng, Haotong Qin, Chuanguang Yang, Zhulin An, Libo Huang, Boyu Diao, Fei Wang, Renshuai Tao, Yongjun Xu, and Michele Magno. Mpq-dm: Mixed precision quantization for extremely low bit diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 1659516603, 2025a. 4 Weilun Feng, Chuanguang Yang, Haotong Qin, Xiangqi Li, Yu Wang, Zhulin An, Libo Huang, Boyu Diao, Zixiang Zhao, Yongjun Xu, et al. Q-vdit: Towards accurate quantization and distillation of video-generation diffusion transformers. arXiv preprint arXiv:2505.22167, 2025b. 4 Weilun Feng, Chuanguang Yang, Haotong Qin, Yuqi Li, Xiangqi Li, Zhulin An, Libo Huang, Boyu Diao, Fuzhen Zhuang, Michele Magno, et al. Mpq-dmv2: Flexible residual mixed precision quantization for low-bit diffusion models with temporal distillation. arXiv preprint arXiv:2507.04290, 2025c. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. 2, 3, 4, 8 10 Preprint Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael Mahoney, and Kurt Keutzer. survey of quantization methods for efficient neural network inference. In Low-Power Computer Vision, pp. 291326. Chapman and Hall/CRC, 2022. 2, 3, 4 Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 2, Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 27042713, 2018. 2, 3 Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aanæs. Large scale multiview stereopsis evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 406413, 2014. 8 Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: whitepaper. arxiv 2018. arXiv preprint arXiv:1806.08342, 1806. 3, 4 Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pp. 7191. Springer, 2024. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426, 2021. 3, 8, 14, 16 Raul Mur-Artal and Juan Tardos. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. IEEE transactions on robotics, 33(5):12551262, 2017. 2, 3 Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. Orb-slam: versatile and accurate monocular slam system. IEEE transactions on robotics, 31(5):11471163, 2015. 2, 3 Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4 Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan In Song. Forward and backward information retention for accurate binary neural networks. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2250 2259, 2020a. Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan In Song. Forward and backward information retention for accurate binary neural networks. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2250 2259, 2020b. 3 Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d catIn Proceedings of the IEEE/CVF international conference on computer egory reconstruction. vision, pp. 1090110911, 2021. 7, 8, 9, 14 Johannes Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 41044113, 2016. 2, 3 Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 19721981, 2023. 4 Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024. 5, 14 Preprint Jianyuan Wang, Christian Rupprecht, and David Novotny. Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 97739783, 2023. 14 Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 52945306, 2025a. 1, 2, 3, 4, 5, 6, 7, 8, 10, 14 Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1051010522, 2025b. 2, 3 Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2069720709, 2024. 3, 14 Lu Wei, Zhong Ma, Chaojie Yang, and Qin Yao. Advances in the neural network quantization: comprehensive review. Applied Sciences, 14(17):7445, 2024. 3, 4 Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization. arXiv preprint arXiv:2203.05740, 2022. 3, 8, 14 Junyi Wu, Haoxuan Wang, Yuzhang Shang, Mubarak Shah, and Yan Yan. Ptq4dit: Post-training quantization for diffusion transformers. arXiv preprint arXiv:2405.16005, 2024. 2, 5 Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: In International Accurate and efficient post-training quantization for large language models. Conference on Machine Learning, pp. 3808738099. PMLR, 2023. 2, 3, 4, 5, 8, Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2192421935, 2025. 2, 3 Lianwei Yang, Haisong Gong, Haokun Lin, Yichen Wu, Zhenan Sun, and Qingyi Gu. Dopq-vit: Towards distribution-friendly and outlier-aware post-training quantization for vision transformers. arXiv preprint arXiv:2408.03291, 2024. 8 Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun. Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. In European conference on computer vision, pp. 191207. Springer, 2022. 2, 4 Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, et al. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation. arXiv preprint arXiv:2406.02540, 2024. 5 12 Preprint PROOF OF THEOREM 3.2 Proof. For = E(X ), we have = max{N i}. Since is finite, is also finite. And for the scale for i, j. Therefore, of sub-region = /N . of , we have ), then we have = EjEi(V j = Consider = {x0, , xK} , to maximize the information of D, we need to maximize: max H(D) = (cid:88) (cid:88) i= j=1 p(xi ) log p(xi ). Since all samples are independent, Eq. 13 is equivalent to max (cid:88) Hi(D) = (cid:88) (cid:16) i= max (cid:88) j=1 p(xi ) log p(xi ) (cid:17) (13) . (14) Without loss of generality, we only need to discuss max Hi(D). To simplify writing, we denote p(xi ) as pj, and the above problem can be derived as: max Hi(D) = max (cid:88) j=1 pj log pj. To solve this problem, we introduce Lagrangian multiplier λ and construct Lagrangian as: (cid:88) pj 1 . j=1 L(p1, , pK , λ) = (cid:88) j=1 pj log pj + λ We can solve this by letting: Therefore, we have: pj = 0, (cid:88) j=1 pj = 1. pj = (log pj + 1) + λ = 0 = pj = eλ1. Substitute into (cid:80)K j=1 pj = 1: eλ1 = 1 = eλ1 = 1 = pj = 1 . At this point, Hi(xs) = log (maximum entropy). Given that j, = /N , when pj = = 1 , the information is maximized. Therefore, Theorem 3.2 holds. (15) (16) (17) (18) (19) Preprint Table 4: More experimental results on Co3Dv2 (Reizenstein et al., 2021). Bold: The best result. Method Bit-Width (W/A) Full Prec. 16/16 RTN BRECQ QDrop DopQ-ViT GPTQ SmoothQuant Quarot QuantVGGT 6/6 6/6 6/6 6/6 6/6 6/6 6/6 6/6 frames=10 frames=20 AUC@30 AUC@15 AUC@5 AUC@3 AUC@30 AUC@15 AUC@5 AUC@3 89.5 88.1 88.3 88.5 88.5 88.7 88.8 89.0 89. 83.2 80.1 80.4 80.8 80.7 81.0 81.3 81.8 82.7 66.1 58.1 58.7 58.9 59.4 61.3 61.4 62.5 65.2 54.9 43.7 43.9 44.1 44.8 46.3 47.4 49.4 53. 90.0 88.1 88.3 88.4 88.5 88.7 88.9 89.1 89.3 83.9 80.2 80.4 80.6 80.7 81.0 81.5 81.9 82.8 67.8 57.6 58.6 58.8 59.4 61.2 61.5 62.6 65. 56.9 43.1 43.8 43.9 44.7 46.2 47.5 49.5 54."
        },
        {
            "title": "B EXPERIMENT SETTINGS",
            "content": "For camera pose estimation, we randomly select 10 frames and 20 frames to validate the performance under varying sequence lengths following (Wang et al., 2025a; 2023). We use the standard metric AUC (Wang et al., 2023), which combines RRA (Relative Rotation Accuracy) and RTA (Relative Translation Accuracy). For point map estimation, we sample keyframes every 5 images. Consistent with prior works (Azinovic et al., 2022; Wang et al., 2024), we use Accuracy (Acc.), Completion (Comp.), and Normal Consistency (N.C.) to validate the performance. Same with prior works (Li et al., 2021; Xiao et al., 2023), we adopt channel-wise weight quantization strategy. For Fine-Grained Quantization Granularity, we further use dynamic token-wise quantization for activation. We use symmetry quantization for both weight and quantization for efficiency. For Hadamard rotation, we use random Hadamard matrix following (Ashkboos et al., 2024; Tseng et al., 2024). For the calibration process, we use block-wise quantization pipeline following (Li et al., 2021; Wei et al., 2022). For hyperparameter setting, we set α = 0.5 in Eq. 6 for channel-wise scale initialization. We set = 0.2 in Eq. 10 and cluster center = 8 in Eq. 12 for calibration dataset construction. We select 40 samples from total 400 samples pool. During calibration process, we set channel-wise scale ˆc and quantization parameters as learnable. We set the learning rate as 5e3 for ˆc and 5e2 for ."
        },
        {
            "title": "C MORE EXPERIMENTS RESULTS",
            "content": "In this section, we present more experiments of quantized VGGT (Wang et al., 2025a) on Co3Dv2 (Reizenstein et al., 2021). Besides W8A8 and W4A4 used in Tab. 1, we further conduct W6A6 experiments to validate the comprehensive performance on more bit-widths. We present the W6A6 results in Tab. 4. Compared to W8A8, the W6A6 QuantVGGT theoretically achieves better compression performance and still achieves state-of-the-art performance with almost no degradtion. Compared to W4A4, QuantVGGT can further maintain 99.7% of the models performance. This provides more choices for those who hope to bring more compression effects compared to 8-bit while ensuring the lossless performance of the model as much as possible, reflecting the generalization and practicality of QuantVGGT. MORE ANALYSIS ON DUAL-SMOOTHED FINE-GRAINED QUANTIZATION In this section, we provide more analysis on the proposed Dual-Smoothed Fine-Grained Quantization (DSFQ). We first present more visualizations of the heavy-tailed activation distribution of VGGT (Wang et al., 2025a) and the amplified salient phenomenon brought by data-independent tokens in Fig. 7. It can be seen that this salient phenomenon is reflected in different layers of VGGT, which will bring universal bottlenecks to the quantization performance. And these data-independent tokens almost always have more severe salient phenomena than other adjacent image tokens, which reflects the salient amplified effect of these special registration tokens. 14 Preprint (a) frame block7. (b) frame block8. (c) global block7. (d) global block8. (e) frame block7. (f) frame block8. (g) global block7. (h) global block8. Figure 7: More salient distribution and registered tokens saliency. Table 5: Ablation study on quantization granularity under W4A4. Compute Granularity Memory Opt. Latency Opt. AUC@30 Full Prec. Static Static Dynamic Dynamic Full Prec. Tensor-wise Token-wise Tensor-wise Token-wise 1.00 3.65 3.65 3.65 3.65 1.00 2.50 2.50 2.49 2.49 89.5 82.2 84.1 82.7 86.9+2.8 Table 6: More ablation study on quantization architecture. Method Base Scale-Rot. Rot.-Scale AUC@30 AUC@15 AUC@5 AUC@3 76.9 86.7 86.9 23.9 56.8 57.3 9.7 42.9 43.6 61.5 78.5 78. Also, to verify the trade-off between performance and latency in our fine-grained quantization, we tested the impact of different quantization granularities on latency and performance, and present the results in Tab. 5. It can be seen that dynamic quantization and token-wise quantization impose almost no burden on memory and only result in an additional latency of 0.01% when used simultaneously. However, this fine-grained quantization brings significant performance improvements. Compared to static tensor-wise quantization, dynamic token-wise quantization only brings 0.01% additional latency but improves AUC@30 from 82.2 to 86.9. Furthermore, to validate the necessity of our pre-global-rotation and post-local-smooth (Rot.-Scale), we also compared with pre-smooth and post-rotation (Scale-Rot.) and present the results in Tab. 6. In terms of performance, our Rot.-Scale approach has indeed achieved significant improvements compared to Scale-Rot.. This proves that the effect of smoothing the rotated space is more stable, while smoothing first and then rotating will weaken the influence of smoothing due to its rearrangement of outliers between channels, demonstrating the effectiveness of our method. MORE ANALYSIS ON NOISE-FILTERED DIVERSE SAMPLING In this section, we provide more analysis of our proposed Noise-Filtered Diverse Sampling (NFDS). 15 Preprint Method Full Prec. Table 7: Ablation study on calibration costs. Calibration Overload Performance GPU Memory (GB) GPU Time (Hours) AUC@30 AUC@15 - 83.9 90.0 - Naive PTQ QuantVGGT w/o filter QuantVGGT w/o cluster QuantVGGT 14.4 14.6+0.2 14.6+0.2 14.6+0.2 2.53 2.56+0.03 2.64+0.11 2.67+0. 78.8 86.2+7.4 86.0+7.2 86.9+9.6 65.2 77.1+11.9 76.5+11.3 78.7+14.9 We present quantization performance using different cluster strategies using five random seeds. We denote our NFDS as Framebased, directly using features to cluster as Feature-based, and using prior labels to cluster as Label-based. The performance comparison is presented in Fig. 8. It can be seen that the experimental results are consistent with our previous analysis. The dataset constructed based on inductive bias clustering results can bring better average performance and significantly reduce the impact of randomness. However, datasets constructed using other prior knowledge have only slight improvements compared to completely random ones. Figure 8: More ablation study on sample strategy."
        },
        {
            "title": "F CALIBRATION AND INFERENCE COMPUTATION RESOURCE",
            "content": "In this section, we report the calibration and inference computation resource and the additional burden brought by our proposed methods. We first present the inference comparison in Tab. 8 We present the detailed results in Tab. 7 and the performance are under W4A4. The filter and cluster are used in calibration dataset construction and we select 40 samples from 400 data pool to ensure the robustness. Compared to the baseline PTQ process (Li et al., 2021), QuantVGGT only brings an additional memory consumption of 0.02GB and an additional calibration time of 0.1 hour, but brings significant performance improvement. And the additional calibration time is almost only affected by the construction of the calibration dataset. But even our complete sampling strategy only brings an additional 0.14 hours of time, and the total PTQ process only takes 2.67 hours and can be performed on consumer GPUs such as RTX4090. This fully demonstrates that our PTQ algorithm is highly efficient and effective. Table 8: Efficiency comparison. Bit-width Memory Latency Opt. (W/A) Opt. 16/16 8/8 (naive) 8/8 (ours) 4/4 (ours) 1.00 1.94 1.93 3.65 1.00 2.19 2.17 2.49 THE USE OF LARGE LANGUAGE MODELS (LLMS) In this paper, Large Language Models are only used as general-purpose auxiliary tools, primarily for document-level auxiliary tasks such as grammar checking and expression refinement. LLMs did not participate in the core conceptualization, method derivation, or experimental design of this research, nor did they contribute to any core writing content. 16 Preprint"
        },
        {
            "title": "H MORE PERFORMANCE VISUALIZATION",
            "content": "Here, we present more visual comparison results between Full Precision model and W4A4 QuantVGGT. Figure 9: Visual comparison results. Figure 10: Visual comparison results."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Institute of Computing Technology, Chinese Academy of Sciences",
        "Shanghai Jiao Tong University",
        "University of Chinese Academy of Sciences"
    ]
}