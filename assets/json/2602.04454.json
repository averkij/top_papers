{
    "paper_title": "Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search",
    "authors": [
        "Tianming Liang",
        "Qirui Du",
        "Jian-Fang Hu",
        "Haichao Jiang",
        "Zicheng Lin",
        "Wei-Shi Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \\textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch."
        },
        {
            "title": "Start",
            "content": "Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search Tianming Liang 1 Qirui Du 1 Jian-Fang Hu 1 (cid:66) Haichao Jiang 1 Zicheng Lin 1 Wei-Shi Zheng 1 6 2 0 2 ] . [ 1 4 5 4 4 0 . 2 0 6 2 : r Figure 1. Through multi-turn interleaved reasoning and web search, our Seg-ReSearch is able to localize and segment any languagereferred target in videos, even those involving new concepts or up-to-date information that lies beyond the internal knowledge of MLLMs."
        },
        {
            "title": "Abstract",
            "content": "Segmentation based on language has been popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose Seg-ReSearch, novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By en1ISEE Lab, Sun Yat-sen University. Correspondence to: Tianming Liang <liangtm@mail2.sysu.edu.cn>, Jian-Fang Hu <hujf5@mail.sysu.edu.cn>. Preprint. February 5, 2026. 1 abling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by substantial margin. Code and data will be released at https://github.com/ iSEE-Laboratory/Seg-ReSearch. Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search 1. Introduction Localizing and segmenting the objects of interest in images or videos has been long-standing topic in AI community. Language, as the most natural interface, is commonly used as the reference for humans to indicate the target objects. With the recent advent of multi-modal large language models (MLLMs), this language reference is evolving from explicit descriptions to implicit reasoning instructions. However, existing research is still limited to understanding objects within the given visual context. In reality, we live in dynamic world, where user queries often involve up-to-date information or domain-specific concepts that exceed the frozen knowledge of MLLMs. For example, user may directly request to segment the new Tesla instead of the white vehicle on the left. While recent works, from LISA (Lai et al., 2024) to VideoSeg-R1 (Xu et al., 2026), have endowed segmentation models with reasoning capabilities, their potential is bounded by the static internal knowledge of MLLMs, as they lack the ability to continuously acquire necessary information from external sources. Although recent advances in text-centric domains have adopted reinforcement learning (RL) to empower LLMs with external search abilities, adapting such methods to multi-modal tasks remains challenging, primarily due to the dilemma in reward designs. Existing approaches either rely solely on ultimate outcome rewards (Li et al., 2025c; Zheng et al., 2025b; Hong et al., 2025) or impose step-wise process supervision (Zheng et al., 2025a; Liu et al., 2025d; Deng et al., 2025). However, neither approach effectively balances external search and visual reasoning. The former suffers from sparse signals, often leading the agent to bypass search steps and seek visual shortcuts. Conversely, the latter tends to over-prioritize the imitation of the search trajectory and thus neglect visual reasoning. In this work, we propose Seg-ReSearch, novel agentic Segmentation framework capable of interleaved Reasoning and external Search. As illustrated in Figure 1, SegReSearch performs task decomposition, reasoning, and interacting with search engines iteratively until it finds the target objects. This paradigm breaks the knowledge bottleneck of MLLMs, enabling segmentation systems to handle broader range of user queries. To effectively incentivize such capabilities, we introduce hierarchical reward mechanism that strikes balance between sparse outcome rewards and rigid step-wise imitation. Specifically, we decouple the process supervision into initial guidance and progressive incentives. For initial guidance, we utilize expert actions to supervise the first step, thereby providing reasonable starting point for the subsequent exploration. For intermediate steps, rather than enforcing trajectory imitation, we use format-based rewards to encourage diverse yet valid search steps, coupled with tapering bonus to prevent infinite search loops. This mechanism effectively bridges the gap between external search and visual reasoning, driving the evolution of the search behavior from initial exploration to efficient and accurate exploitation. To assess the effectiveness of Seg-ReSearch, we establish OK-VOS, novel language-instructed Video Object Segmentation benchmark that specifically requires Outside Knowledge. This benchmark is carefully annotated by human experts to ensure that each query contains up-to-date information or new concepts that lie beyond the internal knowledge of existing MLLMs. These queries may involve single or multi-hop searches, demanding complex reasoning across local visual contexts and external information. Our benchmark reveals that existing state-of-the-art (SOTA) reasoning segmentation models struggle significantly in such open-world scenarios. In contrast, our Seg-ReSearch demonstrates superior performance, even outperforming the baselines equipped with the same search tools by over 10 points. Furthermore, Seg-ReSearch also establishes new SOTA results on conventional reasoning segmentation benchmarks, including ReasonSeg (image) and ReasonVOS (video). Our main contributions are summarized as follows: Framework. We propose Seg-Research, novel trainable agentic segmentation framework capable of interleaved reasoning and external search. Reward Design. We introduce hierarchical reward mechanism to effectively address the dilemma between sparse outcome signals and rigid process supervision. Benchmark. We establish OK-VOS, human-annotated benchmark that explicitly requires external knowledge for video object segmentation. SOTA Performance. Our approach significantly outperforms SOTA reasoning segmentation models and the baselines using the same search tools. 2. Related Work Language-guided Segmentation. Traditional segmentation methods typically rely on visual prompts (Rother et al., 2004; Xu et al., 2018) or predefined categories (He et al., 2017; Cordts et al., 2016). This task was broadened by Referring Segmentation (Kazemzadeh et al., 2014; Yu et al., 2016) and Referring Video Object Segmentation (RVOS) (Seo et al., 2020; Ding et al., 2023; Liang et al., 2025a), which allow users to specify objects of interest in images or videos using natural language. To handle more implicit instructions, LISA (Lai et al., 2024) proposed Reasoning Segmentation, introducing special tokens (e.g., [SEG]) to bridge MLLMs and segmentation models. This token-based training paradigm was widely adopted in subsequent works (Ren et al., 2024; Qian et al., 2025), and further 2 Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search Figure 2. In order to identify the target objects involving new information, Seg-ReSearch conducts multi-turn interactions with the external search engine throughout the dynamic Multi-modal Chain-of-Thought (MCoT). This capability is incentivized by 3-level reward structure: IGR pilots the initial planning, TPR encourages extensive exploration, and OR ensures final task accuracy. extended to video domain (Bai et al., 2024; Yan et al., 2024). To emphasize the reasoning capability, several recent efforts (Liu et al., 2025b;c) shift towards decoupled strategy that trains the policy MLLM to generate positional prompts for an external segmentation model. This strategy facilitates reinforcement learning, incentivizing the Chain-of-Thought (CoT) capability in MLLMs (Huang et al., 2025; Xu et al., 2026). Despite these advances, current efforts remain confined to understanding the given visual context under static reasoning paradigm. To advance this task towards more realistic scenarios, we introduce Seg-ReSearch to enable dynamic reasoning with external search, and establish OKVOS that requires outside knowledge for segmentation. Reward Designs in Agentic RL. While RL has shown promise in augmenting MLLMs with agentic abilities, designing reward function that accurately aligns with human intent remains significant challenge. Existing approaches typically fall into two paradigms: (1) sparse outcome rewards that supervise the entire trajectory (Jin et al., 2025; Su et al., 2025; Li et al., 2025c; Zheng et al., 2025b; Hong et al., 2025; Zeng et al., 2025), and (2) rigid process rewards that supervise each action step (Zheng et al., 2025a; Liu et al., 2025d; Deng et al., 2025; Cheng et al., 2025; Xi et al., 2025). However, both paradigms fail to offer sufficient and flexible feedback, often leading to training instability or reward hacking. In this work, we introduce novel hierarchical reward mechanism to address this dilemma. Search Agents. LLMs are inherently limited by their fixed and outdated internal knowledge. To address this, RetrievalAugmented Generation (RAG) systems (Izacard et al., 2023; Lewis et al., 2020) are developed, which retrieve relevant passages based on the user query and incorporate them into the LLM inputs. However, such static retrieval often suffers from noisy information. Thus, recent advances have shifted towards training LLMs to perform iterative search during reasoning (Jin et al., 2025; Zheng et al., 2025a; Li et al., 2025a;b; Geng et al., 2025). While promising, nearly all these efforts focus only on general question answering. Extending this paradigm to fine-grain visual tasks, such as segmentation, is not minor tweak but critical step to enable truly open-world visual understanding. 3. Seg-ReSearch An overview of Seg-ReSearch is depicted in Figure 2. Given visual inputs (image or video) and query potentially requiring external knowledge, Seg-ReSearch engages in multiturn interactions with the search engine via dynamic MCoT, iteratively updating its reasoning until the target is identified. This complex capability is effectively incentivized by our hierarchical reward mechanism, which mitigates the dilemma between sparse outcome signals and rigid step-wise supervision. In the following, we elaborate on the video segmentation pipeline, as it inherently encompasses the processing required for static images. 3.1. Reasoning Segmentation with External Search In this section, we detail the pipeline of Seg-ReSearch. In contrast to existing reason segmentation paradigms that depend solely on the internal knowledge of MLLMs, our method is capable of retrieving necessary external knowledge by interacting with search engines. 3 Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search Interaction with the Search Engine. Given lowresolution video frames and complex object query, the policy MLLM begins by analyzing the multi-modal input and planning its subsequent steps. At each step, if external knowledge is required, the model generates search query and specifies the search tool (e.g., search_text or search_image) within the tags <search> and </search> for the search engine. The retrieved information is then wrapped inside the tags <information> and </information> and appended into the ongoing rollout sequence, serving as additional context as for subsequent generation steps. This interleaved reasoning-search process continues iteratively until the target is identified or the maximum number of search turns is reached. Answer Generation. The final response of the policy MLLM comprises two stages: keyframe selection and object localization. After the multi-turn searches, the model must select keyframe that best shows the target object and encapsulate the chosen frame index within the tags <keyframe> and </keyframe>. The corresponding highresolution keyframe is then appended to the ongoing context. Upon that, the model localizes the target and outputs formatted positional prompt (i.e., bounding box and point) within the tags <answer> and </answer>. This positional prompt, which identifies the target location on the keyframe, is passed to frozen mask generator (e.g., SAM2 (Carion et al., 2025b)) for mask prediction and propagation. For static image segmentation, we bypass the keyframe phase. 3.2. Hierarchical Reward Designs Existing agentic RL approaches often struggle to balance exploration and task performance, relying either solely on sparse outcome rewards or rigid step-wise imitation. To address this dilemma, we introduce hierarchical reward mechanism, which assesses the rollout trajectory at multiple levels, but imposes only loose constraints at each level. We find that this design can effectively achieve trade-off between process supervision and preventing reward hacking. Formally, our reward function is defined as follows: = α (RIGR + RTPR) (cid:125) (cid:123)(cid:122) Process Reward (cid:124) +ROR, (1) where α is coefficient of the process reward. Initial Guidance Reward (IGR). We use IGR to provide reasonable starting point for subsequent exploration. Formally, IGR is designed as binary reward that determines whether the first-turn generated search query ˆa0 matches any query in the expert search trajectory S: (cid:18) (cid:19) RIGR = max sS Sim(ˆa0, s) > 0.5 , (2) where I() is the indicator function and Sim(, ) is the semantic similarity computed by light Sentence Transformer. 4 Figure 3. The growth curve of TPR with increasing action turns. Here, instead of enforcing an imitation of the experts initial action, we allow the policy MLLM to initiate the task from any valid entry point. Tapering Process Reward (TPR). Instead of forcing strict alignment with ground-truth sequences, TPR incentivizes valid exploration using only format-based rewards. Specifically, the model receives bonus each time it outputs an action in the correct format. However, this naive strategy can lead to reward hacking, where the agent performs infinite, meaningless searches to accumulate rewards. To mitigate this, TPR incorporates tapering bonus design, which gives the model more freedom to explore while naturally preventing it from becoming trapped in infinite loops. Formally, TPR is defined as follows: RTPR = 1 (1 p)min(k,M ), (3) where [0, 1] serves as the base reward, is the number of actions following valid format (including search and answer turns), and controls the upper bound. As shown in Figure 3, rollout sequence that directly generates answers only receives the base reward, while additional search efforts yield convergent bonus that drives the total TPR toward 1. Outcome Reward (OR). This reward is designed to capture both the quality of keyframe selection and the accuracy of spatial localization. Formally, it comprises four terms: ROR = Riou + Rl1 + Rpoint + Rf rame. (4) Here, Riou, Rl1 and Rpoint are binary rewards that measure the accuracy of the predicted bbox and point in the selected keyframe. Specifically, Riou is assigned 1 only if the IoU between the predicted and ground-truth bounding boxes exceeds 0.5. Rl1 is assigned 1 only if their L1 distance is less than 10 pixels. Rpoint is assigned 1 only if the predicted point falls within the predicted box and its Euclidean distance to the ground-truth point is less than 100 pixels. Conversely, Rf rame is continuous reward designed to prioritize frames where the target is most prominent and least occluded. It is defined as Rf rame = Cj , where Ci denotes the area of the largest connected component in the ground-truth mask of the i-th frame, and is the index of the selected keyframe. maxi Ci Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search 3.3. Reinforcement Learning with Search Engine Rollout Formulation. The rollout sequence of SegReSearch can be formulated as multi-turn long-horizon trajectory interacting with the search engine. trajectory of -turn interactions is described as = {(at, et)}T t=1, where at represents the text sequence (comprising reasoning and search tokens) generated by the policy MLLM at turn t, and et denotes environmental feedback (i.e., the information retrieved from the search engine). Let represent the multi-modal inputs (system prompt, user instruction, and input video) and denote the entire answer sequence. The joint generation process is formulated as: Pθ(T , x) = (cid:104) (cid:89) t=1 (cid:105) πθ(at T<t, x) πθ(y , x) = (cid:104) (cid:89) t=1 (cid:104) (cid:89) t=1 πθ(at T<t, x) E(et at) (cid:105) πθ(y , x) (cid:105) πθ(at T<t, x) πθ(y , x) (5) where πθ( , x) represents the policy MLLM, and E(et at) denotes the environmental transition probability, which is typically treated as constant. Optimization. Seg-ReSearch is optimized following the paradigm of Group Relative Policy Optimization (GRPO). For brevity, let = [T , y] represent the complete output sequence. For each input x, GRPO samples group of outputs {o1, o2, . . . , oG} from the old policy πθold (oix), evaluates their corresponding rewards {R1, R2, . . . , RG}, and computes the advantage as follows: Ai = Ri mean({R1, R2, . . . , RG}) std({R1, R2, . . . , RG}) . (6) Ultimately, the policy model πθ is optimized by maximizing the following objective function: (θ) = xD,{oi}G i=1πθold (x) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:16) min πθ(oix) πθold(oix) Ai, clip( πθ(oix) πθold(oix) , 1 ϵ, 1 + ϵ)Ai (cid:17) (cid:35) , (7) where ϵ is the clipping threshold. 4. OK-VOS: VOS Benchmark Requiring"
        },
        {
            "title": "Outside Knowledge",
            "content": "Existing language-guided segmentation benchmarks (Seo et al., 2020; Kazemzadeh et al., 2014; Liang et al., 2025a; Ding et al., 2025) always assume that the user inputs already 5 provide all necessary evidence for identifying the target objects. While reasoning segmentation benchmarks (Lai et al., 2024; Bai et al., 2024; Yan et al., 2024) emphasize world knowledge, they tend to involve only basic common sense (e.g., which food is rich in Vitamin C). These simplified settings fail to reflect the real-world scenarios that often involve up-to-date information or long-tail knowledge. To bridge this gap, we establish OK-VOS, new video object segmentation benchmark that explicitly requires outside knowledge for object identification. This benchmark is fully annotated by five human experts. It contains 1,000 test samples, covering 150 videos and 500 objects. We conduct multi-round review and re-annotation process to strictly ensure that each query requires up-to-date information or long-tail facts that explicitly exceeds the internal knowledge of current LLMs. Any queries that could be correctly answered by LLMs without external web search were discarded or refined. Furthermore, we take rigorous measures to mitigate potential biases that allow the object targets to be easily identified through visual shortcuts. For example, queries like who is the 2025 Oscar winner for Best Actress are excluded if the video clip contains only one woman. To comprehensively evaluate the models capabilities in iterative search and reasoning, we categorize the samples into the following three hierarchical types. (i) One-hop: the target can be identified via single, direct retrieval. (ii) Multi-hop: identifying the target requires retrieving and connecting multiple pieces of information. (iii) Relational: the target is defined by its visual relationship to another knowledge-based anchor target, e.g., who received the ball from the winner of 2025 European Golden Ball. Finally, the benchmark contains 229 One-hop, 495 Multi-hop, and 276 Relational samples. By bridging the gap between fine-grained visual understanding and external information retrieval, OK-VOS establishes crucial and challenging testbed for open-world AI systems. 5. Experiments 5.1. Experimental Setup Implementation Details. We adopt Qwen3-VL-Instruct-4B and 8B (Bai et al., 2025) as our base MLLMs and manually annotate 100 samples for RL training. During training, we use E5 (Wang et al., 2022) as the retriever. Because our samples involve extensive up-to-date information that extends beyond the scope of public knowledge bases like Wikipedia Dumps (Karpukhin et al., 2020), we customize knowledge base based on the 100 training samples. During validation, we directly invoke the Google Search API. We set the number of retrieved passages to 3, retrieved images to 1, and the maximum search turns to 5. We empirically set α = 0.5, = 0.7, =10 and ϵ = 0.2. All input frames and retrieved images are resized to 448 448, while the selected Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search Table 1. Comparison on OK-VOS benchmark. Method Specialist RVOS models SAMWISE CVPR25 ReferDINO ICCV25 ReferEverything-1.4B ICCV25 MLLM-based methods VideoLISA-3.8B NeurIPS24 GLUS-7B CVPR25 RGA3-7B ICCV25 UniPixel-7B NeurIPS Qwen3-VL-4B* Qwen3-VL-4B*+Search Seg-ReSearch-4B (ours) Qwen3-VL-8B* Qwen3-VL-8B*+Search Seg-ReSearch-8B (ours) One-hop Multi-hop Relational Overall &F &F J &F &F 27.0 25.1 25.0 22.9 32.4 22.3 37.0 32.6 39.8 54.0 37.1 40.2 60.1 25.8 24.2 24.0 22.9 32.0 22.9 36. 32.3 39.7 53.6 36.5 39.8 59.7 28.2 26.0 26.1 23.0 32.8 21.8 37.5 32.9 39.8 54.3 37.7 40.6 60. 26.1 27.7 27.7 21.7 29.9 21.0 33.4 33.3 34.3 43.3 35.2 36.4 48.3 25.4 26.8 27.4 21.8 29.3 21.1 32. 33.0 34.2 42.8 34.7 35.9 47.9 26.8 28.6 28.1 21.5 30.5 20.9 34.1 33.6 34.4 43.9 35.7 36.9 48. 24.7 18.7 24.3 12.0 26.8 15.1 33.4 35.6 36.6 44.2 35.7 37.6 44.8 23.3 17.4 23.1 11.0 26.0 13.9 32. 34.5 35.9 43.0 34.6 36.6 43.6 26.0 20.1 25.5 13.0 27.6 16.2 34.5 36.6 37.2 45.4 36.8 38.6 46. 25.9 24.6 26.2 19.3 29.6 19.7 34.2 33.8 36.2 46.0 35.8 37.6 50.0 24.9 23.6 25.4 19.1 29.0 19.5 33. 33.3 36.0 45.3 35.1 37.0 49.4 26.9 25.6 26.9 19.5 30.2 19.8 35.0 34.3 36.4 46.7 36.4 38.2 50. keyframes are resized to 864 864. For image segmentation tasks, we resize all images to 864 864. We apply SAM2 (Ravi et al., 2025) to produce segmentation masks for ReasonSeg and ReasonVOS. For OK-VOS, which involves frequent shot changes, we employ SeC (Zhang et al., 2025). More details are present in the Supplementary. Evaluation Metrics. Our metrics follow the previous segmentation works. For video object segmentation, we use region similarity (J ), contour accuracy(F), and their average value (J &F). For static image segmentation, we use generalized IoU (gIoU) and cumulative IoU (cIoU). Baselines. We implement two baselines for fair comparisons: Qwen3-VL* employs the same MLLM and mask generator as ours, and Qwen3-VL*+Search is further equipped with the same search API and prompts. We also benchmark against existing SOTAs, including specialist RVOS models (Cuttano et al., 2025; Liang et al., 2025b; Bagchi et al., 2025) and MLLM-based segmentation approaches (Bai et al., 2024; Lin et al., 2025; Wang et al., 2025; Liu et al., 2025a). More details are provided in the Supplementary. 5.2. SOTA Comparisons We present the comparison results on the OK-VOS benchmark in Table 1. First, this task poses severe challenges to existing SOTA methods, whether relying on specialist RVOS models or MLLMs. For instance, VideoLISA-3.8B achieves only 19.3 in overall &F, and even the recent UniPixel-7B achieves only 34.2. Second, simply equipping MLLMs with the search engine is insufficient. For example, Qwen3-VL8B*+Search improves over the base model by only 1.8%. In contrast, Seg-ReSearch demonstrates substantial superiority over these competitors. Our 4B model outperforms the search-augmented baseline by nearly 10%, and our 8B Table 2. Comparison on ReasonSeg benchmark. Method Val Test gIoU cIoU gIoU cIoU LISA-7B CVPR24 RSVP-Qwen-7B ACL25 Seg-Zero-7B arXiv25 SAM-R1-7B NeurIPS25 SAM3-Agent-7B arXiv25 Qwen3-VL-8B* Seg-ReSearch-8B (ours) 53.6 58.6 62.6 64.0 65.4 70.3 73.3 52.3 48.5 62.0 55.8 50. 70.0 72.2 48.8 56.6 57.5 60.2 62.6 66.0 67.4 47.1 51.6 52.0 54.3 56.2 53.7 59.0 Table 3. Comparison on ReasonVOS benchmark. Method VideoLISA-3.8B NeurIPS24 GLUS-7B CVPR25 RGA3-7B ICCV25 OneThinker-8B arXiv25 Qwen3-VL-8B* Seg-ReSearch-8B (ours) &F 47.5 49.9 53.6 54.9 56.9 63. 45.1 47.5 51.3 51.1 53.8 60.2 49.9 52.4 56.0 58.7 60.0 66. model establishes significantly superior SOTA with 50.0 in overall &F, validating the effectiveness of the interleved reasoning and search capabilities of Seg-ReSearch. To demonstrate that Seg-ReSearch is also competitive in conventional reasoning segmentation tasks, we evaluate it on the ReasonSeg image benchmark and the ReasonVOS video benchmark. As shown in Table 2 and Table 3, SegReSearch-8B establishes new SOTA results on both benchmarks. Notably, compared to OneThinker-8B that uses the same MLLM, our Seg-ReSearch achieves substantial improvement of +8.3 &F. These results confirm the robustness of Seg-ReSearch in general reasoning segmentation. 6 Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search Table 4. &F results of various reward designs. Here, Baseline indicates the Qwen3-VL-4B*+Search. Rewards One-hop Multi-hop Relational Overall Sparse Rigid Ours 51.0 51.0 54.0 39.3 41.8 43.3 36.7 40.7 44.2 41.3 43.6 46.0 Table 5. Effects of IGR and TPR. IGR TPR &F 41.3 42.0 44.8 46.0 40.6 41.3 44.2 45.3 41.9 42.7 45.4 46.7 Table 6. Overall &F over different number of training samples. Method Seg-ReSearch w/o PR 25 43.0 41.1 50 44.3 41. 75 45.5 38.5 100 46.0 41.3 +4.7%, demonstrating the synergy between initial guidance and progressive incentives. Data Efficiency. We analyze the impact of training data size in Table 6. Seg-ReSearch demonstrates consistent performance gains with increased training samples. In contrast, removing our process rewards (w/o PR) leads to training instability and marginal gains. This confirms the stability and scalability of our hierarchical reward mechanism. 5.4. Ablation on Search Settings We empirically study the impacts of various search settings. All experiments in this section are based on Seg-ReSearch8B. Default settings are highlighted in the tables. Maximum Search Turns. We study the impacts of varing maximum action budget in Table 7. The performance jumps dramatically by +7.4% when increasing the limit from 1 to 5 turns, demonstrating the necessity of multi-turn reasoning and search. However, increasing the budget further to 10 turns results in marginal gain of 0.4%, highlighting the efficient and accurate search capability of our Seg-ReSearch. Retrieval Number. In Table 8, we analyze the impact of the retrieval number (i.e., Top-k) per search. Increasing the number of textual entries from 1 to 3 significantly boosts performance by +2.1%, implying that textual search serves as the primary source of external knowledge. Furthermore, increasing image retrieval yields an additional 0.8% gain, demonstrating the complementary benefit of multi-modal search. To strike balance between accuracy and efficiency, we adopt 3 textual and 1 visual entry as our default setting. Search Engines. We compare the performance of using different search backends. As shown in Table 9, Google search surpasses DuckDuckGo by 2.9 &F. Therefore, we use Google Search by default. Interestingly, we further explore the potential of web browsing, where we access the full content of each web page and employ an additional LLM for summarization. We find that this strategy yields substantial gain of +4.4 &F. This indicates that the potential of Seg-ReSearch can be further broadened by constructing more comprehensive retrieval system. Figure 4. Left: number of search calls over training steps. Right: overall performance of various methods. 5.3. Analysis of Reward Designs In this section, we delve into the impacts of various reward designs on the learning dynamics and final performance. In this section, we use Qwen3-VL-4B as the base model. Hierarchical Reward Design. In Table 4, we compare our hierarchical reward design against two common paradigms: sparse outcome rewards and rigid step-wise imitation. First, the outcome-based reward yields the lowest performance, indicating that sparse outcome signals fail to guide complex multi-turn reasoning. While introducing step-wise imitation improves performance, it constrains the models exploration and leads to sub-optimal policy. In contrast, our hierarchical reward design achieves the best performance. This demonstrates that our hierarchical reward design effectively balances guidance and exploration, facilitating the learning of robust, interleaved reasoning-and-search policies. Learning Dynamics of Search Ability. We compare our process reward with three variants: (1) LPR, linear reward that gives fixed bonus for each valid search; (2) BPR, binary reward awarded only if all searches are valid; and (3) w/o PR, without using any process supervision. As shown in Figure 4, LPR exhibits severe reward hacking, where the policy model rapidly learns to abuse the search and degrades &F to 41.1. Conversely, search usage for BPR and w/o PR are nearly unchanged during training, indicating that such sparse signals are insufficient to motivate multi-step searching. Our design effectively addresses this dilemma: it incentivizes valid exploration while preventing infinite loops, steadily increasing search turns to 2.5 and achieving the best performance of 46.0 &F. Effects of IGR and TPR. In Table 5, we verify the individual contribution of IGR and TPR. Interestingly, TPR serves as free lunch, as it boosts performance by +3.5% without requiring expert annotation. Furthermore, the combination of IGR and TPR leads to remarkable improvement of Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search Figure 5. Qualitative comparison between our Seg-ReSearch and the baselines: Qwen3-VL-8B* and Qwen3-VL-8B*+Search. Table 7. Impact of maximum search turns. Table 9. Overall &F over different search engines Max. Turns &F 1 5 10 42.6 50.0 50.4 42.2 49.4 49.7 43.1 50.7 51. Search Engine DuckDuckGo Google Google+Browsing &F 47.1 50.0 54.4 46.4 49.4 53. 47.8 50.7 55.0 Table 8. Impact of retrieval numbers for text and images. Text Image &F 1 3 3 1 1 3 47.9 50.0 50.8 47.3 49.4 50.2 48.6 50.7 51.4 host on that specific date. These results demonstrate the superior reasoning-search capability of Seg-ReSearch in handling complex and open-world scenarios. 6. Conclusion 5.5. Qualitative Comparison We present multi-hop example in Figure 5, where the user asks to identify the SNL host on the specific date when Michaela Benthaus traveled to space. Qwen3-VL* fails to respond, as this query involves specific information that lies beyond its internal knowledge. Although Qwen3VL*+Search is augmented with search tools, it tends to naively forward the raw user query to the search engine, leading to irrelevant results that hinder subsequent reasoning. In contrast, our Seg-ReSearch successfully identifies the target object by decomposing the task into iterative reasoning and external search: it first searches for the date of the space travel and subsequently searches for the SNL In this work, we propose Seg-ReSearch to break the knowledge bottleneck of existing segmentation systems. By interleaving reasoning with external search, Seg-ReSearch is able to handle dynamic, open-world queries exceeding the frozen knowledge of MLLMs. To this end, we design hierarchical reward mechanism that effectively balances outcome feedbacks with step-wise supervision. Furthermore, we established OK-VOS, challenging VOS benchmark that explicitly requires external knowledge. Extensive experiments demonstrate that Seg-ReSearch achieves substantial improvements over existing approaches. We hope this work serves as solid step towards building more autonomous and knowledgeable visual agents capable of continuously interacting with the dynamic world. 8 Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents significant step towards universal visual recognition and segmentation systems by enabling models to access the external world. The proposed SegReSearch holds great promise for next-generation visual intelligent agents, enabling users to interact with visual content more intuitively and acquire deeper insights beyond pixel-level perception. This technique can also be applied to robotic and educational scenarios to improve the open-world and dynamic abilities of assistant systems. On the negative side, it may introduce potential risks such as amplifying internet bias and raising privacy concerns. Nevertheless, considering the broader perspective, we believe this work is meaningful for advancing the machine learning community towards real-world artificial intelligence. Therefore, we advocate for responsible development, and believe its contributions significantly outweigh the potential risks."
        },
        {
            "title": "References",
            "content": "Bagchi, A., Bao, Z., Wang, Y.-X., Tokmakov, P., and Hebert, M. Refereverything: Towards segmenting everything we can speak of in videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2322123231, 2025. Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X. o., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. Bai, Z., He, T., Mei, H., Wang, P., Gao, Z., Chen, J., Zhang, Z., and Shou, M. Z. One token to seg them all: Language instructed reasoning segmentation in videos. Advances in Neural Information Processing Systems, 37:68336859, 2024. Carion, N., Gustafson, L., Hu, Y.-T., Debnath, S., Hu, R., Suris, D., Ryali, C., Alwala, K. V., Khedr, H., Huang, A., Lei, J., Ma, T., Guo, B., Kalla, A., Marks, M., Greer, J., Wang, M., Sun, P., Rädle, R., Afouras, T., Mavroudi, E., Xu, K., Wu, T.-H., Zhou, Y., Momeni, L., Hazra, R., Ding, S., Vaze, S., Porcher, F., Li, F., Li, S., Kamath, A., Cheng, H. K., Dollár, P., Ravi, N., Saenko, K., Zhang, P., and Feichtenhofer, C. Sam 3: Segment anything with concepts, 2025a. URL https://arxiv.org/abs/ 2511.16719. Carion, N., Gustafson, L., Hu, Y.-T., Debnath, S., Hu, R., Suris, D., Ryali, C., Alwala, K. V., Khedr, H., Huang, A., et al. Sam 3: Segment anything with concepts. arXiv preprint arXiv:2511.16719, 2025b. Cheng, M., Ouyang, J., Yu, S., Yan, R., Luo, Y., Liu, Z., Wang, D., Liu, Q., and Chen, E. Agent-r1: Training powerful llm agents with end-to-end reinforcement learning. arXiv preprint arXiv:2511.14460, 2025. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S., and Schiele, B. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pp. 32133223, 2016. Cuttano, C., Trivigno, G., Rosi, G., Masone, C., and Averta, G. Samwise: Infusing wisdom in sam2 for text-driven In Proceedings of the Computer video segmentation. Vision and Pattern Recognition Conference (CVPR), pp. 33953405, June 2025. Deng, Y., Hsu, I., Yan, J., Wang, Z., Han, R., Zhang, G., Chen, Y., Wang, W., Pfister, T., Lee, C.-Y., et al. Supervised reinforcement learning: From expert trajectories to step-wise reasoning. arXiv preprint arXiv:2510.25992, 2025. Ding, H., Liu, C., He, S., Jiang, X., and Loy, C. C. Mevis: large-scale benchmark for video segmentation with motion expressions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 26942703, 2023. Ding, H., Liu, C., He, S., Ying, K., Jiang, X., Loy, C. C., and Jiang, Y.-G. Mevis: multi-modal dataset for referring motion expression video segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. Feng, K., Zhang, M., Li, H., Fan, K., Chen, S., Jiang, Y., Zheng, D., Sun, P., Zhang, Y., Sun, H., et al. Onethinker: All-in-one reasoning model for image and video. arXiv preprint arXiv:2512.03043, 2025. Geng, X., Xia, P., Zhang, Z., Wang, X., Wang, Q., Ding, R., Wang, C., Wu, J., Zhao, Y., Li, K., et al. Webwatcher: Breaking new frontier of vision-language deep research agent. arXiv preprint arXiv:2508.05748, 2025. He, K., Gkioxari, G., Dollár, P., and Girshick, R. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 29612969, 2017. Hong, J., Zhao, C., Zhu, C., Lu, W., Xu, G., and Yu, X. Deepeyesv2: Toward agentic multimodal model. arXiv preprint arXiv:2511.05271, 2025. 9 Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search Huang, J., Xu, Z., Zhou, J., Liu, T., Xiao, Y., Ou, M., Ji, B., Li, X., and Yuan, K. SAM-r1: Leveraging SAM for reward feedback in multimodal segmentation via reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS), 2025. Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):143, 2023. Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S. O., Wang, D., Zamani, H., and Han, J. Search-r1: Training LLMs to reason and leverage search engines with reinforcement learning. In Second Conference on Language Modeling, 2025. Karpukhin, V., Oguz, B., Min, S., Lewis, P. S., Wu, L., Edunov, S., Chen, D., and Yih, W.-t. Dense passage retrieval for open-domain question answering. In EMNLP (1), pp. 67696781, 2020. Kazemzadeh, S., Ordonez, V., Matten, M., and Berg, T. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 787798, 2014. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., and Jia, J. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 95799589, 2024. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems (NeurIPS), 33:94599474, 2020. Li, X., Dong, G., Jin, J., Zhang, Y., Zhou, Y., Zhu, Y., Zhang, P., and Dou, Z. Search-o1: Agentic search-enhanced large reasoning models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 54205438, 2025a. Li, X., Jin, J., Dong, G., Qian, H., Wu, Y., Wen, J.-R., Zhu, Y., and Dou, Z. Webthinker: Empowering large reasoning models with deep research capability. In The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS), 2025b. Li, Z., Zhang, H., Han, S., Liu, S., Xie, J., Zhang, Y., Choi, Y., Zou, J., and Lu, P. In-the-flow agentic system optimization for effective planning and tool use. arXiv preprint arXiv:2510.05592, 2025c. Liang, T., Jiang, H., Yang, Y., Tan, C., Li, S., Zheng, W.-S., and Hu, J.-F. Long-rvos: comprehensive benchmark for long-term referring video object segmentation. arXiv preprint arXiv:2505.12702, 2025a. Liang, T., Lin, K.-Y., Tan, C., Zhang, J., Zheng, W.-S., and Hu, J.-F. Referdino: Referring video object segmentation with visual grounding foundations. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2000920019, 2025b. Lin, L., Yu, X., Pang, Z., and Wang, Y.-X. Glus: Globallocal reasoning unified into single large language model for video segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 86588667, 2025. Liu, Y., Ma, Z., Pu, J., Qi, Z., Wu, Y., Ying, S., and Chen, C. W. Unipixel: Unified object referring and segmentation for pixel-level visual reasoning. In Advances in Neural Information Processing Systems (NeurIPS), 2025a. Liu, Y., Peng, B., Zhong, Z., Yue, Z., Lu, F., Yu, B., and Jia, J. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025b. Liu, Y., Qu, T., Zhong, Z., Peng, B., Liu, S., Yu, B., and Jia, J. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025c. Liu, Z., Zang, Y., Zou, Y., Liang, Z., Dong, X., Cao, Y., Duan, H., Lin, D., and Wang, J. Visual agentic reinforcement fine-tuning. arXiv preprint arXiv:2505.14246, 2025d. Lu, Y., Cao, J., Wu, Y., Li, B., Tang, L., Ji, Y., Wu, C., Wu, J., and Zhu, W. Rsvp: Reasoning segmentation via visual In Proprompting and multi-modal chain-of-thought. ceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1469914716, 2025. Qian, R., Yin, X., and Dou, D. Reasoning to attend: Try to understand how< seg> token works. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2472224731, 2025. Ravi, N., Gabeur, V., Hu, Y.-T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle, R., Rolland, C., Gustafson, L., Mintun, E., Pan, J., Alwala, K. V., Carion, N., Wu, C.-Y., Girshick, R., Dollar, P., and Feichtenhofer, C. SAM 2: Segment anything in images and videos. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. 10 Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search Conference on Computer Vision (ECCV), pp. 98115. Springer, 2024. Yu, L., Poirson, P., Yang, S., Berg, A. C., and Berg, T. L. In EuroModeling context in referring expressions. pean conference on computer vision (ECCV), pp. 6985. Springer, 2016. Yu, S., JIN, C., Wang, H., Chen, Z., Jin, S., ZUO, Z., XIAOLEI, X., Sun, Z., Zhang, B., Wu, J., Zhang, H., and Sun, Q. Frame-voyager: Learning to query frames for video large language models. In The Thirteenth International Conference on Learning Representations, 2025. Zeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., and Chen, W. ACECODER: Acing coder RL via automated test-case synthesis. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1202312040, 2025. Zhang, Z., Ding, S., Dong, X., He, S., Lin, J., Tang, J., Zang, Y., Cao, Y., Lin, D., and Wang, J. Sec: Advancing complex video object segmentation via progressive concept construction. arXiv preprint arXiv:2507.15852, 2025. Zheng, X., An, K., Wang, Z., Wang, Y., and Wu, Y. Stepsearch: Igniting llms search ability via step-wise proximal policy optimization. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2180521830, 2025a. Zheng, Z., Yang, M., Hong, J., Zhao, C., Xu, G., Yang, L., Shen, C., and Yu, X. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025b. Ren, Z., Huang, Z., Wei, Y., Zhao, Y., Fu, D., Feng, J., and Jin, X. Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2637426383, 2024. Rother, C., Kolmogorov, V., and Blake, A. \"grabcut\" interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3):309314, 2004. Seo, S., Lee, J.-Y., and Han, B. Urvos: Unified referring video object segmentation network with large-scale benchmark. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XV 16, pp. 208223. Springer, 2020. Su, A., Wang, H., Ren, W., Lin, F., and Chen, W. Pixel reasoner: Incentivizing pixel space reasoning via curiositydriven reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Wang, H., Chen, Q., Yan, C., Cai, J., Jiang, X., Hu, Y., Xie, W., and Gavves, S. Object-centric video question answering with visual grounding and referring. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2227422284, 2025. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., and Zhou, M. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems (NeurIPS), 33:57765788, 2020. Xi, Z., Liao, C., Li, G., Yang, Y., Chen, W., Zhang, Z., Wang, B., Jin, S., Zhou, Y., Guan, J., et al. Agentprm: Process reward models for llm agents via step-wise promise and progress. arXiv preprint arXiv:2511.08325, 2025. Xu, N., Yang, L., Fan, Y., Yue, D., Liang, Y., Yang, J., and Huang, T. Youtube-vos: large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327, 2018. Xu, Z., Guo, Y., Lu, Y., Yang, F., and Li, J. Videoseg-r1: Reasoning video object segmentation via reinforcement In Proceedings of the AAAI Conference on learning. Artificial Intelligence (AAAI), 2026. Yan, C., Wang, H., Yan, S., Jiang, X., Hu, Y., Kang, G., Xie, W., and Gavves, E. Visa: Reasoning video object segmentation via large language models. In European 11 Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search A. More Implementation Details Following Search-R1 (Jin et al., 2025), we mask retrieved tokens during loss computation, ensuring that policy gradients are calculated solely based on MLLM-generated tokens. Training is conducted on single node with 8 NVIDIA A6000 GPUs. We use global batch size of 16 and mini-batch size of 8. The maximum response length is set to 6,144 tokens, comprising 4,096 tokens for generation and 2,048 tokens for environmental feedback. For IGR, we use lightweight sentence transformer all-MiniLM-L6-v2 (Wang et al., 2020) to compute the semantic similarity. We train Seg-ReSearch-4B for 100 epochs without KL penalty, and Seg-ReSearch-8B for 120 epochs with KL divergence coefficient of 0.01. The training set comprises 100 samples, consisting of 80 samples requiring search and 20 samples without search. For each video, we uniformly sample 6 frames as input. While we believe more sophisticated frame selection strategies (Yan et al., 2024; Yu et al., 2025) might further enhance performance, exploring them is beyond the scope of this work. The system prompt used for training is provided in the following. System Prompt for Seg-ReSearch Your role is video target identification assistant capable of web search. You will be given an object query and sequence of video frames. Each frame is preceded by its index. Your task is to locate the target with bounding box and point in your selected frame. # Tools You have access to the following tools: <tools> {\"name\": \"text_search\", \"description\": \" Search for textual information from the internet.\", \"parameters\": {\"query\": {\"type\": \"string\", \"description\": \"Search keywords or phrases\"}} {\"name\": \"image_search\", \"description\": \" Search for images from the internet.\", \"parameters\": {\"query\": {\"type\": \"string\", \"description\": \"Search keywords or phrases\"}} </tools> Each tool will return the top searched results between <information> and </information>. # Output Format: Depending on the situation, output one of the following: ## 0. If you need web search (zero to multiple times): <think> (1) Plan your reasoning steps; (2) Justify the need for web search. </think> <search> {\"name\": <tool-name>, \"query\": <string>} </search> ## 1. Think deeply based on the query and the video: <think> (1) Carefully compare all possible objects in the video and find the object that most matches the query; (2) Analyze the provided frames and select the best one where the target is most clearly visible. </think> <keyframe> [integer frame_index] </keyframe> ## 2. Once you receive the high-res keyframe: <think> (1) Describe the unique visual features of the target to prove you captured it; (2) Determine the precise 2D location of the target with bbox and the point inside the object. </think> <answer> {\"bbox_2d\": [x1,y1,x2,y2], \"point_2d\": [x,y]} </answer> # IMPORTANT NOTE At each step, 1) always look at the video frames first before you decide to search; 2) never search for information that can be obtained from the given video; 3) do not use search if you only wanna analyze the frames. Baselines: In this work, we mainly compare against the following SOTA models: (1) Specialist RVOS models: SAMWISE (Cuttano et al., 2025), ReferDINO (Liang et al., 2025b) and ReferEverything (Bagchi et al., 2025). (2) MLLM-based methods for video segmentation: VideoLISA (Bai et al., 2024), GLUS (Lin et al., 2025), RGA3 (Wang et al., 2025), UniPixel (Liu et al., 2025a), and OneThinker (Feng et al., 2025). (3) MLLM-based methods for image segmentation: LISA (Lai et al., 2024), RSVP (Lu et al., 2025), Seg-Zero (Liu et al., 2025b), SAM-R1 (Huang et al., 2025), SAM3-Agent (Carion et al., 2025a). 12 Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search B. Training Dynamics of Seg-ReSearch Figure 6. Training dynamics of Seg-ReSerach-8B. We visualize the evolution of Seg-ReSearch-8B during training in Figure 6. As shown in the top row, the outcome rewards (e.g., Riou, Rl1, Rpoint, and Rf rame) exhibit steady upward trend. The process reward rises rapidly in the early stages, coinciding with sharp decline in wrong response length to near zero. This demonstrates that our hierarchical reward design effectively guides the model to strictly adhere to the required interaction format before optimizing for task accuracy. Notably, the average number of searches gradually increases from 0.8 to stabilize around 2.5, reflecting the evolution of Seg-ReSearchs search ability from initial exploration to efficient, accurate exploitation. C. Comparison with SFT Baselines critical question is whether our performance gains come from extra data (the 100 samples) or our training paradigm. To answer this, we fine-tune existing SOTA models on the same 100 samples following their public supervised fine-turning (SFT) paradigm. As shown in Table 10, relying solely on SFT is insufficient to address our task. First of all, SFT is data-hungry, typically requiring extensive annotations to prevent overfitting. For example, it degrade the performance of GLUS-7B by 13.1% in overall &F. In addition, SFT leads to memorization, failing to teach the model how to actively use search engines to reason about unknown targets. For instance, while SFT on UniPixel-7B shows slight improvement (+2.8%), it still lags far behind our method. In contrast, our approach is data-efficient, achieving 9.8% and 12.4% performance improvements over the 4B and 8B baselines with only 100 samples. Table 10. Comparison with SFT baselines on the OK-VOS benchmark. Method One-hop Multi-hop Relational Overall &F J &F &F J &F GLUS-7B CVPR25 (cid:44) w/ SFT UniPixel-7B NeurIPS25 (cid:44) w/ SFT Qwen3-VL-4B*+Search Seg-ReSearch-4B (ours) Qwen3-VL-8B*+Search Seg-ReSearch-8B (ours) 32.4 18.8 -13.6 37.0 39.3 +2.3 39.8 54.0 +14. 40.2 60.1 +19.9 32.0 18.0 -14.0 36.6 38.8 +2.2 32.8 19.5 -13.3 37.5 39.8 +2.3 39.8 39.7 53.6 54.3 +13.9 +14. 40.6 39.8 59.7 60.5 +19.9 +19.9 29.9 18.0 -11.9 33.4 34.9 +1.5 34.3 43.3 +9.0 29.3 17.2 -12.1 32.7 34.0 +1. 34.2 42.8 +8.6 30.5 18.9 -11.6 34.1 35.8 +1.7 34.4 43.9 +9.5 36.4 48.3 +11.9 36.9 35.9 47.9 48.6 +12.0 +11. 26.8 12.0 -14.8 26.0 10.7 -15.3 27.6 13.3 -14.3 33.4 39.0 +5.6 36.6 44.2 +7.6 37.6 44.8 +7. 32.2 38.2 +6.0 35.9 43.0 +7.1 36.6 43.6 +7.0 34.5 39.9 +5.4 37.2 45.4 +8.2 38.6 46.0 +7. 29.6 16.5 -13.1 34.2 37.0 +2.8 36.2 46.0 +9.8 29.0 15.6 -13.4 33.5 36.3 +2.8 36.0 45.3 +9. 30.2 17.5 -12.7 35.0 37.8 +2.8 36.4 46.7 +10.3 37.6 50.0 +12.4 38.2 37.0 49.4 50.7 +12.4 +12.5 Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search D. More Qualitative Analysis We visualize the detailed reasoning process of Qwen3-VL-8B*+Search and our Seg-ReSearch in Table 11. In this sample, Qwen3-VL-8B*+Search fails to recognize the need for external information. Instead, it suffers from hallucination, incorrectly assuming the prominent female singer is the target. Conversely, Seg-ReSearch retrieves the specific band info via text search and verifies the singers identity via image search, successfully identifying the target in the provided video. Table 11. Visualization of the CoTs between Qwen3-VL-8B*+Search and our Seg-ReSearch. User Input: band is scheduled to release its third studio album, Boycott Heaven, in 2026. Please find the lead singer of this band in the provided video. Qwen3-VL-8B*+Search: <think> The video frames show performance with two main singers, man and woman. The woman has distinctive pink hair and is singing into microphone in multiple frames, suggesting she is the lead singer. The man is also singing but appears to be supporting vocalist. Therefore, the woman with pink hair is most likely the lead singer. </think> <keyframe> 1 </keyframe> Here is the high-res version of your selected frame: <think> The woman in the frame has distinctive pink hair, is wearing black top, and is holding microphone while singing. These are clear visual features that identify her as the lead singer. </think> <answer> {\"bbox_2d\": [201, 143, 610, 997], \"point_2d\": [455, 445]} </answer> Seg-ReSearch-8B (ours): <think> need to identify the lead singer of the band that will release Boycott Heaven in 2026. will search for information about the band and its lead singer. </think> <search> {\"name\": \"text_search\", \"query\": \"lead singer of the band releasing Boycott Heaven in 2026\"} </search> <information> Web Page 1: Nearly 20 years after The Format broke up, theyre back with Boycott Heaven. Its reunion built on the friendship between Nate Ruess and ..., Web Page 2: In 2025, the band reunited for second time and announced their third studio album, Boycott Heaven, for release in 2026... </information> <think> have found that the band is called The Format and that the lead singer is Nate Ruess. will now search for an image of Nate Ruess to identify him in the video. </think> <search> {\"name\": \"image_search\", \"query\": \"Nate Ruess\"} </search> <information> 1. (Title: Nate Ruess Spotify) </information> <think> have obtained an image of Nate Ruess, the lead singer of The Format. will now compare this image with the provided video frames to find the frame where he is most clearly visible. </think> <keyframe> 2 </keyframe> Here is the high-res version of your selected frame: 14 Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search <think> In this frame, can see man with short brown hair, wearing suit, singing into microphone. His facial features and hairstyle match the image of Nate Ruess found. </think> <answer> {\"bbox_2d\": [120, 78, 607, 998], \"point_2d\": [275, 675]} </answer>"
        }
    ],
    "affiliations": [
        "ISEE Lab, Sun Yat-sen University"
    ]
}