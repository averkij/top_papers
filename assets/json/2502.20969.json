{
    "paper_title": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval",
    "authors": [
        "Chien-Yu Lin",
        "Keisuke Kamahori",
        "Yiyu Liu",
        "Xiaoxiang Shi",
        "Madhav Kashyap",
        "Yile Gu",
        "Rulin Shao",
        "Zihao Ye",
        "Kan Zhu",
        "Stephanie Wang",
        "Arvind Krishnamurthy",
        "Rohan Kadekodi",
        "Luis Ceze",
        "Baris Kasikci"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, leading to system challenges in latency-sensitive deployments, especially when limited GPU memory is available. To address these challenges, we propose TeleRAG, an efficient inference system that reduces RAG latency with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that anticipates required data and transfers it from CPU to GPU in parallel with LLM generation. By leveraging the modularity of RAG pipelines, the inverted file index (IVF) search algorithm and similarities between queries, TeleRAG optimally overlaps data movement and computation. Experimental results show that TeleRAG reduces end-to-end RAG inference latency by up to 1.72x on average compared to state-of-the-art systems, enabling faster, more memory-efficient deployments of advanced RAG applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 9 6 9 0 2 . 2 0 5 2 : r TELERAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval Chien-Yu Lin1, Keisuke Kamahori1, Yiyu Liu2, Xiaoxiang Shi2, Madhav Kashyap1 Yile Gu1 Rulin Shao1 Zihao Ye1 Kan Zhu1 Stephanie Wang1 Arvind Krishnamurthy1 Rohan Kadekodi1 Luis Ceze1 Baris Kasikci1 1University of Washington 2Shanghai Jiao Tong University"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, leading to system challenges in latency-sensitive deployments, especially when limited GPU memory is available. To address these challenges, we propose TELERAG, an efficient inference system that reduces RAG latency with minimal GPU memory requirements. The core innovation of TELERAG is lookahead retrieval, prefetching mechanism that anticipates required data and transfers it from CPU to GPU in parallel with LLM generation. By leveraging the modularity of RAG pipelines, the inverted file index (IVF) search algorithm and similarities between queries, TELERAG optimally overlaps data movement and computation. Experimental results show that TELERAG reduces end-to-end RAG inference latency by up to 1.72 on average compared to state-of-the-art systems, enabling faster, more memory-efficient deployments of advanced RAG applications."
        },
        {
            "title": "1 Introduction",
            "content": "Retrieval-augmented generation (RAG) has emerged as powerful technique to enhance large language models (LLMs) by integrating them with external databases [13, 29, 57]. During inference, RAG retrieves relevant content from external data sources, usually indexed as vector datastores, to mitigate issues such as hallucinations [50, 63, 74] and incorporate up-to-date or private information [36, 64]. Modern RAG applications share two key characteristics. (1) RAG applications are built as modular pipelines as shown in Figure 1a, where single query undergoes multiple rounds of LLM calls and retrievals, each of which serves different Equal contribution. Work done during internship at UW. Correspondence to: Chien-Yu Lin <cyulin@cs.washington.edu>. (a) Typical pipeline stages of RAG application. (b) Different scenarios for RAG and the illustration of the proposed lookahead retrieval mechanism. It prefetches relevant data for retrieval from CPU to GPU, overlaps data transfer with the pre-retrieval stage, and accelerates retrieval with GPU-CPU cooperation. Figure 1: (a) Illustrations of RAG pipeline stages. (b) Overview of TELERAG and comparison to baseline systems. functions to improve overall output quality. For example, query transformation rounds [10, 27, 71, 100] generally occur before retrieval to refine the users query with LLMs (pre-retrieval generation). (2) RAGs datastores are typically large, supported by recent works demonstrating that increasing the size of the datastore positively affects the performance of RAG applications [15, 33, 50, 64, 77]. These characteristics create significant challenges for efficient RAG inference, especially in latency-sensitive applications such as customer chatbots [7, 18, 83], financial analysis [62, 65], and emergency medical diagnosis [31, 53]. First, the latency of RAG systems is inherently increased due to 1 multiple rounds of LLM generation and retrieval. Second, although GPU can speed up both LLM and retrieval, the combined memory demand of these processes often exceeds available GPU resources, making it expensive or infeasible to fully host both operations on GPU. Consequently, in local or smallscale deployments, which are common for RAG applications handling private or sensitive data, the retrieval datastore is frequently off-loaded to CPU memory to alleviate GPU constraints. However, while CPU off-loading resolves memory limitations, it significantly increases retrieval latency, diminishing overall system efficiency. Addressing the latency challenge, several recent works have been proposed to accelerate RAG inference. These approaches include FPGA-based retrieval acceleration [41], reuse of KV-cache [44, 60, 92], and speculative document retrieval [87, 98]. However, none of these works directly address the substantial memory demands associated with retrieval, which remains significant bottleneck for efficient RAG deployments. In this paper, we identify key opportunity in the widely adopted inverted file index (IVF) [80] retrieval method. IVF reduces retrieval latency by pre-clustering the datastore into sub-clusters, limiting the search space to relevant clusters of the retrieval query during runtime. This approach also offers way to reduces GPU memory usage, as only subset of clusters are needed for each retrieval query. For example, after identifying relevant clusters, their data can be dynamically transferred from CPU to GPU memory and accelerate the similarity searches on GPU. However, this approach incurs high CPU-GPU data transfer overhead, keeping the overall retrieval latency still high [49] (see 3). Our proposal. To tackle latency and memory bottlenecks in RAG inference, we introduce novel mechanism called lookahead retrieval. Lookahead retrieval predicts which subsets of IVF clusters will likely be accessed during the retrieval phase and proactively transfers them from CPU to GPU during the pre-retrieval generation stage. The idea behind lookahead retrieval is based on the observation that queries before and after the pre-retrieval generation stage share substantial semantic overlap. Consequently, IVF clusters relevant to the initial input query are also likely to be relevant to the refined query. To validate this intuition, we rigorously analyze IVF cluster assignments across original queries and refined queries produced by the pre-retrieval generation step, evaluating on six representative RAG pipelines and three datasets. Our analysis confirms strong similarity between these two query forms (see 3). Leveraging this insight, we propose TELERAG, an efficient inference system designed to optimize RAG latency while minimizing GPU memory consumption. TELERAG employs lookahead retrieval to preemptively load relevant IVF clusters onto the GPU, effectively hiding CPU-GPU data transfer overhead during concurrent LLM generation. As illustrated in Figure 1b, this approach significantly reduces retrieval latency without exceeding GPU memory constraints, enabling efficient execution of RAG applications in resource-limited environments. Although TELERAG significantly reduces latency by processing prefetched IVF clusters on the GPU, some relevant clusters might still be missed during prefetching. To maintain retrieval accuracy, TELERAG concurrently executes similarity searches for these missed clusters on the CPU. The results from CPU searches are seamlessly merged with GPU results, ensuring complete retrieval precision. Thanks to the high cluster similarity between the initial and refined queries, the number of missed clusters is typically small, allowing TELERAG to achieve low retrieval latency through efficient CPU-GPU coordination. key challenge in deploying lookahead retrieval is determining the optimal number of IVF clusters to prefetch. Prefetching too many clusters increases data-transfer overhead, whereas fetching too few clusters could result in higher retrieval latency due to increased CPU processing. To address this, we propose profile-guided approach coupled with an analytical model that dynamically determines the ideal prefetch amount based on pipeline characteristics and hardware configurations. Our adaptive method consistently outperforms static configurations, delivering substantial latency improvements across diverse RAG setups and hardware environments. Results summary. We evaluate TELERAG using 61GB Wikipedia-based datastore [5] across six popular RAG pipelines built with the Llama model family [82] (3B, 8B, and 13B). Remarkably, TELERAG supports retrieval from 61GB datastore alongside Llama3-8B (16GB) LLM on single RTX 4090 GPU (24GB memory), significantly outperforming the CPU retrieval baseline. Experiments conducted on an RTX 4090 and an H100 GPU [68] demonstrate speedups of up to 2.68x and 2.49x, with average speedups of 1.70x and 1.75x, respectively. These results highlight the capability of TELERAG to efficiently handle large-scale RAG inference tasks within tight GPU memory constraints, confirming its practical value for resource-limited deployments. In summary, we make the following key contributions: Analyzing the correlation of the queries between the pre-retrieval generation and retrieval stages, revealing significant overlap in their corresponding IVF clusters. Proposing lookahead retrieval, which prefetches likely IVF clusters to the GPU, and hides CPU-GPU data transfer time during pre-retrieval generation. Developing TELERAG, an efficient RAG inference system that integrates lookahead retrieval, resulting in significant acceleration of RAG with minimal GPU memory usage. This approach enables dynamic and informative responses in both singleand multi-turn dialogues [57]. Modularity in modern RAG applications. However, naively integrating document retrieval into LLM generation can cause several issues. For example, the retrieval often struggles to find relevant content and may select irrelevant or mismatched chunks [27, 61, 100], and single retrieval may not provide sufficient context, necessitating multiple retrieval rounds [29]. To solve these issues, most state-of-the-art RAG models adopt modular approach that employs multiple rounds of LLM calls and retrievals for single query [23, 26, 28, 29, 34, 88]. Typically, they have the following types of steps (shown in Figure 2): Pre-retrieval generation, used to assess whether retrieval is needed or to generate queries for retrieval. An example of pre-retrieval technique is query transformation [27, 37, 58, 61, 72, 73, 93, 100, 102], which reformulates the original query to make it clearer and more suitable for the retrieval task. Retrieval, used to identify relevant documents from the vector data store. This stage takes the output of preretrieval generation and generates data for the next stage. Post-retrieval generation, generates response based on user query and retrieved documents. It can also perform additional process such as summarization [39, 51] or reranking [81, 105] on the retrieved documents. Judgment, dynamically determines the execution flow. For example, it decides if more iteration is needed to enhance the response. Heuristics or LLMs can be used for judgement stage. By proceeding through these functions, RAG applications can deliver more precise and contextually appropriate responses, significantly enhancing the capabilities of LLMs for various applications [29, 45]."
        },
        {
            "title": "2.2 Vector Index and Inverted File (IVF)",
            "content": "The vector index is crucial component of RAG applications that retrieves similar items from large datasets of highdimensional vectors. Given the query vector RD and vector database = {y0, . . . , yN1} RD, which comprises vectors, the vector index aims to find the nearest neighbors of from the database: k-argmini=0:Nd(x, yi), where is the dimensionality of the vector determined by the embedding model, and denotes the distance function, which is typically the L2-norm or inner product [46]. Figure 2: Overview of RAG."
        },
        {
            "title": "2.1 RAG",
            "content": "RAG is technique that enhances the capabilities of LLMs by integrating them with information retrieval to generate more accurate and relevant text [13, 29, 57]. The core idea behind RAG is to augment the LLM with relevant information retrieved from large corpus of documents, improving the LLMs ability to answer questions without hallucinations [50, 63, 74] and generate contents based on up-to-date or private information [36, 64]. RAG workflow. basic RAG workflow involves several phases, including data store building, retrieval, and interactions with LLMs [23, 24, 28, 34, 70, 84, 88]. In order to build data store, raw data in various formats is cleaned, converted to plain text, and divided into chunks. These chunks are then encoded into vectors using an embedding model and stored in vector index, enabling efficient searching based on similarity. When user provides query, it is converted into vector using the same embedding model, and the most similar chunks from the indexed database are retrieved. For largescale index, approximate algorithms such as the inverted file index (IVF) [80] (see 2.2 for detail) are commonly used to accelerate the search process. The retrieved chunks, along with the original user query, are combined and given as prompt to the LLM. The LLM then generates response that relies on the information provided in the retrieved documents. 3 To improve search efficiency, the inverted file index (IVF) algorithm is widely used for large-scale vector databases due to its simplicity and effectiveness. IVF partitions the data store into clusters and restricts searches to the most relevant clusters, reducing the computational cost. To obtain the cluster assignment of each stored vector, it performs clustering algorithm such as k-means [66] and partitions the database into Nc clusters: {C1,C2, . . . ,CNc } = k-means(Y, Nc), where is the set of vectors assigned to the j-th cluster, and the cluster centroids {c1, c2, . . . , cNc} are obtained by taking the mean of each vectors in {C1,C2, . . . ,CNc }. Then, each database vector yi is assigned to the nearest cluster center: Cluster(yi) = argmin j=1:Ncd(yi, j). With the trained centroids and cluster assignment, we can perform more efficient index search. There are two steps involved in the IVF index searching. First, it will identify the nearest cluster centroids of query vector x: {c j1, j2, . . . , jL } = L-argmin j=1:C d(x, j). This step is also referred to as the coarse-grained search in IVF, as it identifies candidates at the cluster level. Second, the fine-grained search is performed in the nearest clusters and identify closest vectors of x: k-argminyiL l=1C jl d(x, yi). which involves sorting. In this way, IVF reduces search space and accelerates the retrieval process. Here, the hyperparameter from the first step is also referred as nprobe [25]. When nprobe is larger, the IVF retrieval will be more accurate as it search more clusters. However, the retrieve latency is longer as more computation and data are needed. Since the search process is highly parallelizable among each cluster and each vector, this search algorithm can be highly accelerated by GPUs. Open-source libraries offer efficient GPU implementations [46, 75]. However, IVF does not reduce the memory requirement for the index since the data for all clusters must be stored. As result, IVF still requires substantial GPU memory footprint, which becomes bottleneck when GPU memory capacity is the constraint of RAG systems. Moreover, recent work reported that increasing the size of the data store positively affects RAG application performance [15, 33, 50, 64, 77], exacerbating this issue. Figure 3: Latency breakdown of six RAG pipelines on NQ dataset [54]. responses. In each of these contexts, maintaining low latency per query is paramount for providing timely and accurate results. Batching multiple queries, common strategy for improving throughput, is not considered in this work. It is because many RAG applications involve user-specific private data [21, 30, 55, 86, 89] and serve relatively small user bases, making batching infeasible. For large-scale RAG services [1, 2, 10], although batching is feasible, it increases the latency for each individual queries [79, 97, 103] and therefore, is not ideal for the latency sensitive applications. straightforward way to reduce inference latency is to scale up GPU resources, allowing both the LLM and retrieval datastore to reside in GPU memory. However, this approach is often cost-prohibitive and could lead to significant hardware underutilization, particularly in the single-query scenarios that this work targets. Therefore, in this paper, we aim to improve single-query RAG latency without significantly increasing GPU memory usage, ensuring that both cloud-based and local deployments can satisfy stringent performance requirements without incurring excessive costs."
        },
        {
            "title": "3 Analyzing Latency of RAG Pipelines",
            "content": "In this section, we analyze state-of-the-art RAG applications and identify their underlying systems challenges in achieving low latency. To conduct the analysis, we construct 61GB vector index with the FAISS library [25], and set the IVF clusters to 4096 following common practice [11]. We use Llama-3-8B [6] as the LLM, and run our analysis on an H100 GPU with 80GB memory. 5.2 provides details about our experimental setup."
        },
        {
            "title": "3.1 End-to-end Latency of RAG Pipelines",
            "content": "In this paper, we focus on reducing inference latency for single query, critical objective in latency-sensitive RAG applications. These applications span range of real-world scenarios, such as customer chatbots [7, 18, 83], financial analysis [62,65], autonomous driving [17,94], and emergency medical diagnosis [31, 53], where users expect near-instantaneous We begin by analyzing the end-to-end latency of six representative RAG pipelines (see 5.1 for details) in two scenarios. In the first, the LLM runs on the GPU while retrieval is offloaded to the CPU, minimizing GPU memory usage. In the second, both the LLM and the vector index reside in GPU memory, enabling GPU-based retrieval for lower latency. If the GPUs 4 Figure 4: The breakdown of memory consumption at GPU and CPU for two different strategies: CPU offloading and GPU-based retrieval. The dotted line indicates the memory capacity of RTX4090 GPU, which is common used GPU for local deployment. memory is insufficient, then CPU offloading is necessary. We set the nprobe to 512, common used setting under this indexs configuration (see 5.2 for details), to measure retrieval latency. Figure 3 shows the breakdown of end-to-end latency into LLM computation and retrieval. The results are the average of 512 random data from the NQ dataset [54]. We observe that CPU-based retrieval (first scenario) is the primary bottleneck, consuming about 56% of the total execution time on average, while GPU-based retrieval (second scenario) accounts for just 12%. On average, GPU retrieval is 12.5x faster than CPU retrieval, reducing overall latency by approximately 2.2x. Thus, accelerating retrieval on GPU is crucial for improving end-to-end latency. However, GPU acceleration comes with significant memory cost. Figure 4 shows the memory requirements for both GPU and CPU. As Figure 4 shows, putting both LLM weights and the retrieval index on the GPU requires around 77GB of memory, which exceeds the capacity of consumer GPUs like the RTX 4090 with 24GB. Thus, GPU acceleration on retrieval is often not feasible when running on lower-end GPU or indexing with large datastore. Note that the memory footprint of the LLM and the index can be even larger and further exacerbate the memory constraint. In the rest of this section, we try to answer the following question: Is it possible to achieve the latency of GPU-based retrieval while using much less GPU memory?"
        },
        {
            "title": "3.2 Opportunities of GPU-accelerated Re-",
            "content": "trieval with Limited Memory straightforward approach to enable GPU retrieval with limited GPU memory is to fetch the necessary data from CPU to GPU on-demand for each query, leveraging the IVF index structure that narrows the target search clusters (2.2). While such method enables faster search on the GPU, the bottleneck of retrieval is shifted to the data fetching. To examine the performance, we reconstruct the IVF index in PyTorch [8], leveraging its high-performance GPU oper5 Figure 5: Latency breakdown of CPU-offload and runtimefetch GPU retrieval, averaged over 512 random NQ queries. Dataset HyDE [27] SubQ [58] Iter [58] IRG [78] FLARE [43] S-RAG [11] NQ [54] HotpotQA [91] TriviaQA [47] 75.7% 66.2% 93.4% 85.7% 80.3% 100.0% 78.6% 66.6% 86.9% 91.2% 82.2% 100.0% 76.8% 65.3% 86.8% 87.5% 82.9% 100.0% Table 1: IVF cluster overlapping rate between the input and output of the pre-retrieval generation. The nprobe is set to 512. Since Self-RAG does not incorporate query transform, its coverage is always 100%. ator, and implement such an on-demand fetching retrieval system. We measure the retrieval latency with an RTX 4090 GPU and compare it with the baseline of CPU-based retrieval. Figure 5 shows the breakdown of retrieval time into data fetch and search time. We show the performance for different amounts of data fetched (indicated by the nprobe parameter). We observe that the fetch time dominates the latency due to limited PCIe bandwidth between the CPU and GPU (32 GB/s). Despite the fact that GPU retrieval is significantly faster than CPU retrieval, due to the fetch overhead, the effective latency is slower than CPU retrieval (around 3% slow down on average across nprobes). Therefore, we must hide the data fetching latency in order to achieve meaningful speedup to following such approach. Hiding the data fetch cost. Hiding the data fetch cost implies that CPU-to-GPU data fetch must be done before the retrieval operation is performed. However, predicting which data to retrieve in advance is challenging since the exact data requirements only become clear at retrieval time once we have the query produced by the pre-retrieval process (2.1). Fortunately, as we show next, the original form of query before pre-retrieval can serve as valuable hint for the clusters required to be retrieved at the end of the pre-retrieval stage. This hint can guide us in predicting the clusters that should be fetched, and the data transfer can be initiated in the background along with pre-retrieval to hide its latency."
        },
        {
            "title": "3.3 Overlapping of IVF Clusters",
            "content": "While the exact data to be retrieved is only known after the pre-retrieval generation is done, we observe that there are high similarities of the IVF clusters assignments between the queries at different stages. Similarity of queries at different stages. During the preretrieval process of RAG pipelines (e.g., query transformation [27,37,58,61,72,73,93,100,102]), an LLM call is issued to refine an initial user query qin into transformed query qout, which is then used for retrieval. This process often rewrites the query into different format [27] or simplifies its complexities [58], which intuitively preserves the querys core semantic content. Hence, the embedding vectors of qin and qout are likely to be similar. This similarity, in turn, suggests that the IVF clusters to which these queries would be assigned will overlap. Therefore, qin can serve as valuable hint for predicting subsequent memory accesses. Prediction coverage. To verify this hypothesis, we evaluate the average cluster coverage rate between prefetched clusters and clusters used for retrieval in three popular questionanswer datasets (NQ [54], HotpotQA [91], and TriviaQA [47]) and six RAG pipelines. Table 1 shows the coverage when we prefetch 512 clusters. From the table, we observe that IVF cluster overlap rates are consistently high across range of datasets and pipelines. For instance, even the lowest reported values remain above 65% (for SubQ). Opportunity. This data shows an opportunity for predicting required clusters, which can make it possible to hide data transfer overhead during LLM generation. In this paper, we aim to leverage this observation to accelerate the inference latency for RAG."
        },
        {
            "title": "4 Design of TELERAG",
            "content": "Based on the high IVF cluster overlapping observation presented in 3.3, we present TELERAG, an efficient inference system for RAG that incorporates the idea of lookahead retrieval, which predicts and prefetches likely IVF cluster data to GPU and significantly accelerates the retrieval process with minimal GPU memory requirement. In this section, we describe our design overview (4.1), an analytical model on finding the optimal prefetch data amount (4.3), and the implementation details (4.4)."
        },
        {
            "title": "4.1 Overview",
            "content": "qout with blue background (Cout). Due to the semantic similarity of qin and qout, there is significant overlap between Cin and Cout (3.3), marked with purple background (Coverlap). Given this, the lookahead retrieval technique in TELERAG operates in three key steps. First, Cin is computed, and their data is transferred to GPU memory during pre-retrieval generation, beginning with those clusters whose centroids are closest to qin. This is possible because GPUs are equipped with Direct Memory Access (DMA) engines, which can handle memory transfers concurrently with compute operations. Additionally, LLM generation is typically compute-intensive task that involves minimal data transfer between the device and host memory. Hence, lookahead retrieval can run seamlessly alongside LLM generation. Second, at retrieval stage, the GPU performs fast similarity search on the accurately predicted clusters (Coverlap) after determining Cout. Since similarity search essentially involves dot-product computations with high dimension embeddings, this is highly efficient on the GPU. Third, simultaneously to the second step, the CPU processes the similarity search for remaining clusters of Cout that were not prefetched and is stored on CPU memory. Although CPUs are generally slower at performing similarity search, lookahead retrieval significantly reduces the workload assigned to the CPU, thereby accelerating the overall retrieval process. Additionally, TELERAG optimizes the sorting of vector distances, which is required for the k-argmin operation, the final step of the IVF search where the top-k indices are identified. Since each cluster could have more than thousands of embedding vectors, sorting is much faster on the GPU than on the CPU [9]. In order to perform the sorting on GPU, TELERAG passes the CPU computed distances of Cmiss to GPU first and then perform the global sorting on the combined distances of Coverlap and Cmiss. Unlike the original vector data, the distance values (such as L2-norm or inner product [46]) are scalars per data point and thus, are much smaller in size. Hence, transferring them to the GPU incurs minimal overhead, allowing the GPU to achieve end-to-end speedup on the sorting process. This GPU-based sorting optimization is typically absent in CPU-based retrieval implementations [25] as they assume there is no GPU in the systems. In summary, TELERAG optimizes retrieval by leveraging lookahead retrieval to hide the data movement time for prefetching, GPU computation for efficient similarity search and sorting, and CPU processing for non-overlapping clusters, significantly accelerating the retrieval process. Figure 6 presents the overview of TELERAG with lookahead retrieval. In Figure 6, the input to the pre-retrieval stage is defined as qin, and the output is denoted as qout. In the subsequent retrieval stage, qout serves as the input query to perform the retrieval. We highlight the IVF clusters corresponding to qin with red background (Cin), and those corresponding to"
        },
        {
            "title": "4.2 Prefetching Design",
            "content": "Prefetch target. In the early design of TELERAG, we use number of clusters as the prefetch target as it aligns with the nprobe setting of the IVF retrieval process. With this target, Despite we can configure the number clusters to prefetch 6 Figure 6: (a) The overview of TELERAGs lookahead retrieval and comparison to the baseline CPU-offloaded retrieval. (b) System design of TELERAG. It first loads the entire index data into CPU memory and identifies the clusters (Cin) for the query before the transformation (qin). It then transfers the data of Cin to GPU memory while running the pre-retrieval stage and generating the retrieval query qout. During the search, the overlapped clusters are searched on GPU, and the missed clusters are searched on CPU. The retrieval results are then merged and passed to the next stage. to adapt to each pipelines characteristics, we found this target brings an unstable inference performance. Its because the size of each cluster is often heavily skewed [44, 59], making data loading time for some clusters much longer than the others. To solve this, we use number of bytes, bp, as the prefetch target. With bp as the prefeth target, it provides an upper bound on prefetching time at bp , where stands for the CPU-GPU memory bandwidth. To perform prefetching, we first sort the clusters by the distance between their centroids and the query embedding vector. Starting with the nearest cluster, we include clusters one by one until adding another would exceed the target bp. We do not split clusters to completely fill bp, ensuring clear separation between GPU and CPU searches. Prefetch for Multi-Round RAG. As discussed in 2.1, RAG pipelines often run through multiple rounds. Because each round still processes for the same input query, the IVF clusters used in retrieval are highly similar across rounds. In TELERAG, we therefore perform full prefetch only for the first round, then fetch only the additional IVF clusters needed for subsequent rounds, until reaching the memory budget. We currently flush the prefetched data for each new query, assuming no correlation across queries. We leave more advanced memory management and identification of hot clusters between queries for future work."
        },
        {
            "title": "4.3 Finding the Optimal Prefetch Amount",
            "content": "A key challenge in TELERAG is to balance the benefit of reducing retrieval latency by prefetching data against the overhead of CPU-GPU transfers. Prefetching more clusters reduces the subsequent retrieval time but can also extend the transfer phase; if it extends beyond the LLM generation window, we lose the advantage of overlap and potentially introduce additional delay. Nevertheless, even additional delay in transferring data can be worthwhile if it substantially reduces retrieval latency. Mathematical model. We develop mathematical model to guide the choice of the optimal amount of data to prefetch. Here, we follow the notations from 4.2 where bp is bytes to prefetch and is the CPU-GPU bandwidth. Optimal amount of data to prefetch is denoted as p. To start, we let t1 represent the combined time of prefetching and pre-retrieval LLM generation, and t2 represents the retrieval time. We have: t1 = max(tLLM,tp), t2 = max(tc,tg), (1) (2) where tLLM is the time for LLM generation, tp is prefetching time, tc is the CPU retrieval time, and tg is the GPU retrieval time. The objective is to minimize t1 + t2. Since prefetching time tp is proportional to bp, we express t1 as piecewise function: t1 = tLLM, bp , if bp tLLM, if bp > tLLM, (3) From Eq. 3, if we prefetch fewer bytes than can be transferred during LLM generation, t1 is effectively just tLLM because the transfer overlaps completely with LLM execution. 7 As shown in 3.1, GPU retrieval (tg) is generally much faster than CPU retrieval (tc). Thus, we assume tc tg. As CPU has limited parallelism, tc usually grows proportionally with the number of clusters to be processed [40], such that: t2 = tc = rmiss nprobe tcc, (4) where rmiss is the miss rate (percentage of IVF clusters are not caught on GPU), nprobe is the total number of clusters to search, and tcc is the CPU time to search single cluster. Increasing bp can only decrease or maintain the current miss rate, i.e., drmiss 0. Moreover, because clusters are prefetched dbp in order of descending likelihood, we assume rmiss is either linear or concave up function2 of bp, i.e., d2rmiss db2 0. We now examine two cases: Case 1: tLLM. Here, t1 = tLLM is constant because prefetching is fully overlapped with LLM generation. Since increasing bp in this regime will not increase t1 and cannot worsen the miss rate, pushing bp to the boundary tLLM minimizes t1 + t2. Hence, = tLLM. (5) Case 2: > tLLM. In this region, t1 grows linearly (t1 + t2) = d2rmiss with bp, and we have: d2 nprobe tcc db2 db2 0. Therefore, t1 + t2 is concave up, allowing at most one minimum. At the minimum point, we have: dbp (t1 + t2) = 0 = 1 + drmiss dbp nprobe tcc = 0. (6) From this, we obtain: = nprobe tcc rmiss, (7) where rmiss is the decrement of the miss rate for this round. If is indeed larger than tLLM, it becomes the global minimum; otherwise, the solution reverts to Case 1. In summary, our analysis shows that the optimal prefetch amount can only lie at one of two points: (1) Prefetch until LLM generation completes (i.e. bp = tLLM). (2) point determined by Eq. 7. However, under typical CPU-GPU bandwidth (e.g., 55 GB/s on PCIe 5), the time spent loading additional clusters often outweighs any retrieval latency reduction from lowering rmiss. Consequently, the second scenario in Eq. 7 becomes nearly infeasible in practice. Therefore, on current hardware, prefetching exactly until LLM execution ends is generally the most effective choice. Profiling-guided approach. Although can be derived from the analysis above, it depends on knowing tLLM for each query, which cannot be obtained ahead of time. To address this, we use profiling-guided approach, leveraging the observation that, despite differences in query content, the output length (and thus generation time) for RAG pipeline often remains similar across most queries. Accordingly, for each RAG pipeline, we can measure tLLM on calibration set containing queries, and get estimated ˆb = tLLM, where tLLM = mean{tLLM,1,tLLM,2, . . . ,tLLM,n}. This estimated ˆb can then be used for incoming queries, ensuring nearoptimal prefetch amount. 4."
        },
        {
            "title": "Implementation",
            "content": "TELERAG is implemented in Python and leverages PyTorchs [8] operator ecosystem for efficient computation. The datastore index is initially constructed using FAISS [25], and its data structures, such as IVF centroids and cluster data, are converted to PyTorch tensors. These tensors are stored on disk to eliminate the need for repeated conversions. At runtime, cluster data is loaded into contiguous pinned memory region on the CPU, enabling non-blocking memory copies to the GPU. fixed-size contiguous buffer on the GPU is allocated based on the users configuration or GPU memory capacity during runtime. transferenable To PyTorchs and LLM generation, we ring _copy(non_blocking=True) API separate CUDA streams for prefetching and LLM. In this way, the data copy operations will not block the GPU to perform computation, and thus the TELERAG can do prefetching in parallel to the pre-retrieval LLM generation. concurrent CPU-GPU data utilize and use To implement the index search with GPU-CPU cooperation, for the GPU part, we use single matrix-vector multiplication that computes distances for all prefetched vectors; for the CPU part, we utilize multithreading in Python to parallelize similarity searches across clusters. We then move the distances computed from CPU to GPU, merge with distances on GPU and perform global sorting on GPU."
        },
        {
            "title": "5 Evaluation",
            "content": "We conduct extensive experiments to evaluate the effectiveness of TELERAG. In this section, we describe the necessary details on how we set up the evaluations, present experiments results and provide in-depth analysis and discussions."
        },
        {
            "title": "5.1 Datasets and RAG Pipelines",
            "content": "Datastore. We build datastore based on the wiki_dpr dataset [48], which is popular dataset contains 2.1 billions tokens from Wikipedia. Following previous works [11, 48, 64], we chunk the passages by every 100 tokens, and use 2An upward U-shaped function whose second derivative is positive. 3Original embedding without compression for the best retrieval precision. 8 Specification Value Dataset Dataset size # of chunks # of IVF cluster Embed model Embed dimension Index type Distance metric Index size wiki_dpr [48] 2.1 billion tokens 21 million 4096 Contriever [35] 768 FLAT3 Inner Product 61GB Table 2: Detailed configurations of our retrieval index. Contriever [35] to generate embedding for each chunk. The embeddings have hidden dimension of 768. Vector index. For the baseline retrieval, we build an IVF vector index using FAISS [25] from the datastore, following setups from [11]. See Table 2 for the detailed configurations of our vector index. As described in 4.4, we then convert the FAISS index to customized index in PyTorch for TELERAG. LLMs. We evaluate TELERAG on the Llama model family [82] in three different sizes (Llama-3.2-3B, Llama-3-8B, Llama-2-13B) to represent different use cases. RAG pipelines. We evaluate TELERAG with six popular RAG pipelines. Figure 7 shows the overview of each pipeline. Note that even though some pipelines lack pre-retrieval generation, the post-retrieval generation serves similar functionality for the retrieval of the next iteration. Below are brief descriptions to evaluated RAG pipelines. For iterative-based pipelines, we set the maximum iterations to 3. 1. HyDE [27] prompts LLM to generate hypothetical paragraph and perform retrieval based on the embedding of the generated paragraph. 2. SubQestion (SubQ)4 [58] prompts LLM to generate multiple sub-questions and performs retrievals for each generated sub-question. 3. Iterative (Iter)5 [58] prompts LLM to generate narrower questions first and iteratively refine them based on previous answers. At the end of each iteration, it prompts LLM to judge if the answer is good enough. 4. Iter-RetGen (IRG) [78] iteratively do retrieval and LLM generation for 3 iterations. 5. FLARE [43] iteratively issues retrievals based on the confidence (probability score) of predicted tokens for the upcoming sentence. 4Implemented in LlamaIndex as SubQuestionQueryEngine 5Implemented in LlamaIndex as MultiStepQueryEngine Figure 7: Overview of six RAG pipelines that we evaluate. 6. Self-RAG (S-RAG)6 [11] decides whether to retrieve documents and then answers for each query. critique stage selects the most confident answer. We use finetuned models based on Llama2-7B and Llama2-13B from their official implementation [12]. Evaluation Datasets. We evaluate on three commonly used question-answering datasets, NQ [54], HotpotQA [91] and TriviaQA [47]. For each dataset, we randomly sample 512 queries and report the average unless otherwise specified."
        },
        {
            "title": "5.2 Experiment Setups",
            "content": "Hardware setups. We evaluate TELERAG on two hardware environments, Desktop and Server, which equiped with represent the settings for the desktop and cloud instance use case. For Desktop, its paired with RTX 4090 GPU (24GB memory) and we consider the 3B and 8B models. For Server, its paired with RTX 4090 GPU (24GB memory) and we consider the 8B and 13B models. These sizes are common model size for RAG applications [10, 11]. We do not evaluate the 13B model on Desktop as its model size (26 GB) exceeds RTX4090s memory space. Table 3 summaries the hardware configurations. Nprobe and top-k. We evaluate latency with three nprobe values, 128, 256, and 512, which represent 3.1%, 6.3%, and 12.5% of the total clusters in our evaluated index. This range falls into the commonly used settings of IVF index [25, 106]. 6We evaluate on the short-form version which only has one iteration. 9 Setup CPU CPU memory size Desktop TR 5975 512GB GPU GPU memory size RTX4090 [67] 24GB Server EPYC 9554 1.5TB H100 [68] 80GB CPU-GPU Bus Bandwidth PCIe 4 32 (24) GB/s PCIe 5 64 (51) GB/s Table 3: Hardware specifications for our setups. In bandwidth, the number in the parentheses is the actual bandwidth we measured from our system. We use top-k number of 3 for retrieval, which returns 3 most relevant passages based on the retrieval query. RAG pipeline implementation. To collect the input and output texts for each stage and have consistent results across all pipelines, we implement the RAG pipelines with the FlashRAG framework [45]. For IRG, FLARE, and S-RAG, we use the frameworks default implementations; for the other pipelines, we re-implement them using FlashRAGs APIs. Benchmark methodology. We follow benchmarking methodology from [101] and use GPT-3.5-Turbo [69] to run through each pipeline and record the input and output text of each step. During latency evaluation, we set LLMs output length based on the recorded text. This way, we ensure fair latency comparison across different LLM models. Baseline systems. To evaluate the latency of each pipeline, we construct clean execution flow in Python that only contains LLM generation, datastore retrieval, and other necessary logical operations to fulfill each pipeline. For LLM generation, we use SGLang [101], which is state-of-the-art LLM inference engine. Prefetching budget setups. Based on the methodology we described in 4.3, we profile each RAG pipeline with 64 random samples from NQ [54] and define the prefetch budget of each pipeline. Max prefetching memory. We set maximum GPU memory limit for prefetching in each configuration. For Server with H100, we allocate up to 16GB, sufficient for high cluster hit rate at the largest tested nprobe value (512). For Desktop with RTX4090, we allocate up to 10GB and 3.75GB for the 3B and 8B models, respectively. These settings demonstrate that TELERAG can efficiently operate using only small fraction (up to 40% for RTX4090 and 20% for H100) of total GPU memory."
        },
        {
            "title": "5.3 Latency Evaluation",
            "content": "Baseline retrieval. Table 4 reports the retrieval precision and the baseline CPU retrieval latency of different nprobe. To conduct this test, we use 1024 random queries from NQ. For retrieval precision, we compare against the top-10 results Metric Precision Hardware nprobe 256 128 - 95.9% 97.6% 98.9% CPU Latency (ms) Desktop Server 89 176 126 349 251 Table 4: Retrieval precision and baseline latency of FAISS for different nprobe values. Precision is measured by comparing top-10 results against greedy search without IVF. from the greedy search. For the latency, we report the average retrieval time. Due to the limited scalability of the CPU, higher nprobe directly leads to longer retrieval latency even when using the state-of-the-art implementation from FAISS. End-to-end latency. We evaluate the end-to-end latency of all the RAG pipelines and datasets under all the hardware settings described in 5.2. Our comprehensive end-to-end speedup results are presented in Figure 8. We can observe that TELERAG improves the execution speed consistently with various configurations. Across different pipelines, datasets, and model/hardware setups, the average speedup is 1.21x, 1.41x, and 1.72x for nprobe values of 128, 256, and 512, respectively. We observe more significant speedup for larger nprobe, as increased nprobe settings incur longer retrieval latencies. In the best case, we observe around 2.6x speedup (for IterRetGen, HotpotQA, Llama-3.2-3B, on RTX4090). This is because the Iter-RetGen pipeline employs retrieval step in every iteration and has short LLM output in general, providing larger end-to-end speedup margin for accelerating the retrieval part. Another pipeline that has particularly high speedup is the SubQuestion pipeline, for which TELERAG achieves around 2.5x speedup across all the datasets with Llama-3.2-3B on RTX4090. This pipeline uses LLM to generate multiple subquestions and perform batched retrieval with 3 to 4 generated queries at one time. The CPU retrieval performance of batched retrieval is significantly worse than that of GPU, as it has limited parallelism. In TELERAG, we handle the batched retrieval efficiently by leveraging GPUs high parallel computing capacity and, therefore, achieve high speedup for the SubQuestion pipeline. Overall, the performance gain is stable across different datasets, with less than 3% of variance across datasets on average. For given GPU, TELERAG has higher speedup with smaller LLM because retrieval time will account for larger fraction of the end-to-end latency. Still, we have tangible speedup in RTX4090 with Llama-3-8B model (e.g., more than 1.5x when nprobe is 512, on average), even though prefetching memory budget is only 3.75GB (space left after holding LLM, embedding model, IVF cluster centroids and misc tensors on GPU) in this case. 10 (a) End-to-end latency speedup with Llama-3.2-3B with single RTX4090 GPU. (b) End-to-end latency speedup with Llama-3-8B with single RTX4090 GPU. (c) End-to-end latency speedup with Llama-3-8B with single H100 GPU. (d) End-to-end latency speedup with Llama-2-13B with single H100 GPU. Figure 8: End-to-end latency speedup of TELERAG and baseline on six RAG pipelines, three datasets, with H100 and RTX4090 GPU. Speedups on retrieval. Figure 9 shows the latency speedups on retrieval on NQ. We observe consistent speedups for all the nprobe numbers but notice the greatest speedups of retrieval often occur at nprobe 256, with average speedups of 7.21x and 7.41x on RTX4090 and H100. This is because, with larger nprobe, the retrieval performance of TELERAG will start to be bounded by the missed clusters on the CPU (we have fixed prefetch budget of different nprobe numbers). Nonetheless, TELERAG still has large latency improvement against the pure CPU baseline."
        },
        {
            "title": "5.4 Analysis",
            "content": "Latency breakdown and GPU compute interference. We further show the latency breakdown of running RAG pipelines with Llama-3-8B and Llama-2-13B on Server with H100, examining the case when the GPUs memory capacity is enough for good amount of prefetching. Figure 10 shows the breakdown of latency into LLM time and retrieval time. We compare the baselines retrieval latency (FAISS CPU) with TELERAGs retrieval latency in three different nprobe numbers. The first thing we observe is that, although we set the prefetching budget to the amount that the data loading time can overlap with the pre-retrieval generation time, there is slight slowdown in LLM generation time for TELERAG. This is potentially because GPUs compute performance will be impacted when there is memory-loading workload, as noticed in [103]. However, such impact is minimal, with less 11 (a) On Desktop with RTX4090 GPU. (a) With Llama-3-8B. (b) On Server with H100 GPU. Figure 9: Retrieval speedup on NQ. Pipeline HyDE SubQ Iter IRG FLARE S-RAG H100 (Llm3-8B) Budget Hit Rate 4090 (Llm3-3B) Budget Hit Rate 9 GB 8 GB 5 GB 4 GB 6 GB 3 GB 77.23% 7 GB 63.26% 7 GB 80.79% 3 GB 44.49% 2.5 GB 58.17% 3 GB 36.01% 1.25 GB 67.16% 58.38% 61.46% 29.39% 33.01% 14.98% Table 5: The prefetch budget and corresponding averaged cluster hit rate for each pipeline and hardware setup on NQ dataset. The target retrieval nprobe is 512. than 5% slowdown in general. We observe higher impacts for Iter-RetGen and Self-RAG with Llama-3-8B, which TELERAGs LLM geeration time is 28% and 20% longer than the baseline. Its because these two pipelines have relatively shorter LLM generation time, and thus, are more vulnerable to interference on GPU compute. However, with the significant amount of speedups on retrieval, TELERAG achieves consistent speedups across all the pipelines despite having this small impact on the LLM generation. Prefetch budgets and cluster hit rates. Table 5 shows the prefetch budgets we set with the profiling-guided approach on RTX4090 and H100 for NQ. It also presents the averaged cluster hit rate achieved with this prefetch budget. From the table, we can see that it can generally attain high cluster hit rate (> 50%) for the cases where it can hold larger amount of data for prefetching. For pipelines or scenarios that can hold for less than 3GB prefetching amount, we generally observe 12 (b) With Llama-2-13B. Figure 10: Latency breakdown for NQ on Server with H100 GPU. relatively lower hit rate (< 35%), limiting our benefits of reducing CPUs search workloads. However, as observed from Figure 8, TELERAG achieves from 1.2x to 1.5x end-to-end speedups for these pipelines. This is the combined benefit of reducing CPU workload and utilizing GPU to perform sorting on similarity distances, demonstrating the strong performance of TELERAG across wide range of scenarios. Analysis on the cluster hit rate. We measure cluster hit rates across wide range of prefetch amounts, examining scenarios with higher CPU-GPU bandwidth and sufficient GPU memory to accommodate larger prefetches. Our experiments are conducted on the HyDE pipeline with nprobe set to 512 and prefetching from 32 to 1024 clusters. We use nprobe as the prefetch metric (instead of bytes) to facilitate comparisons with optimal hit rates. We evaluate on NQ, HotpotQA, and TriviaQA, reporting average results in Figure 11. We have three observations from Figure 11. (1) From 0% to 50% of nprobe, accuracy remains high, illustrating that the probability of hitting relevant clusters decreases by order of prefetching. (2) Between 50% and 150% of nprobe, coverage still increases rapidly but is slower than the first range. (3) After 150% of nprobe, coverage plateaus at roughly 95%. These results suggest that prefetching the amount beyond the target nprobe is likely unnecessary, as the coverage increase is limited. mizes the execution of multiple functions in large language models by enabling parallel function calling. AI Metropolis [90] accelerates LLM-based multi-agent simulations with out-of-order execution. ALTO [76] improves the performance of complex AI systems by streaming partial results. Some open-source libraries provide graph-based representation for programming this kind of application [22, 56]. RAG is specific type of application in this broader direction, and we propose systems techniques to optimize its execution latency, focusing on the characteristics of retrieval workload. Figure 11: The cluster coverage rate for HyDE with different numbers of prefetched clusters. The target retrieval nprobe is 512. Results are the average of NQ, HotpotQA and TriviaQA."
        },
        {
            "title": "6.1 Systems for RAG",
            "content": "Several prior works have attempted to build systems for RAG. RAGCache [44] proposes caching system that stores KV caches from datastore chunks in an order-aware manner to improve the time to the first token. However, this approach only reduces prefill latency, leaving retrieval and decode times unaffected, despite the fact they typically dominate total latency [4]. Moreover, it assumes repeated use of the same document across multiple requests, limiting its scalability over large data stores. Similarly, TurboRAG [60] precomputes KV caches from the data store, but it also only optimizes prefill latency. RaLMSpec [98] proposes speculative retrieval and batched verification, Chameleon [41] proposes CPU-GPU-FPGA heterogeneous architecture for accelerating the retrieval process, and PipeRAG [42] proposes an algorithm-system codesign technique to overlap the retrieval and generation process by modifying the algorithms. However, these works focus on the paradigm of RAG that retrieves documents once every few tokens and are not applicable to modular RAG applications that are widely used now, as discussed in 2.1. Speculative RAG [87] introduces drafting by smaller LLMs to reduce RAG latency, but it does not target the retrieval latency. APIServe (InferCept) [3] proposes novel KV cache management strategy that can support the interception of LLM generation by other workloads, including retrieval. However, this work again does not focus on optimizing retrieval latency. Unlike all these prior works, we tackle system challenges of long retrieval latency and large memory requirement for modular RAG pipelines with large-scale datastores. Vector index is gaining growing attention as an essential component of RAG [16, 29], and many works have been proposed to improve the efficiency with system or algorithm techniques. ScaNN proposes an anisotropic quantization method for better efficiency [32]. DiskANN and SPANN propose memorydisk hybrid indexing systems that work beyond the limitation of memory capacity [19, 38]. Other prior works propose hardware acceleration of vector index with GPUs [46], FPGAs [40, 96], TPU [20], or ray tracing hardware [59, 104]. BANG proposes method to scale graph-based ANN beyond GPU memory [49], and Rummy allows the index to scale beyond GPU memory capacity with reordered pipelining [99]. However, these methods either require algorithm modifications or are bottlenecked by CPU-GPU bandwidth. Our proposal, focuses on the context of modular RAG applications, where queries for retrieval are usually generated by LLMs, and optimizes the latency and the GPU memory consumption without altering the algorithm of the IVF index."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduced TELERAG, an inference system that tackles the system challenges of RAG pipelins under latency-sensitive scenarios. By using lookahead retrieval, which overlaps data transfer and GPU computation for faster retrieval, profile-guided approach to determine optimal prefetching data amount, and GPU-CPU cooperation, TELERAG speeds up the end-to-end latency with minimal GPU memory requirement. Our evaluation shows that TELERAG significantly improves performance compared to existing state-of-the-art solutions."
        },
        {
            "title": "6.2 Systems for Compound LLM Applications",
            "content": "[1] Genspark. https://www.genspark.ai/. Apart from RAG, there is growing interest in compound or agentic LLM applications, where multiple LLM calls and other applications are combined to serve complex functionalities [14, 85, 95]. LLMCompiler [52] is framework that opti- [2] Perplexity. https://www.perplexity.ai/. [3] Reyna Abhyankar, Zijian He, Vikranth Srivatsa, Hao Zhang, and Yiying Zhang. Apiserve: Efficient api 13 support for large-language model inferencing. arXiv preprint arXiv:2402.01869, 2024. [4] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee. Taming throughput-latency tradeoff in llm inference with sarathi-serve. arXiv preprint arXiv:2403.02310, 2024. [5] AI@Meta. Dataset card for wiki_dp. https:// huggingface.co/datasets/facebook/wiki_dpr, 2020. [6] AI@Meta. Llama 3 model card. https: //github.com/meta-llama/llama3/blob/main/ MODEL_CARD.md, 2024. [7] Rama Akkiraju, Anbang Xu, Deepak Bora, Tan Yu, Lu An, Vishal Seth, Aaditya Shukla, Pritam Gundecha, Hridhay Mehta, Ashwin Jha, et al. Facts about building retrieval augmented generation-based chatbots. arXiv preprint arXiv:2407.07858, 2024. [8] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, C. K. Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Marcos Yukio Siraichi, Helen Suk, Shunting Zhang, Michael Suo, Phil Tillet, Xu Zhao, Eikan Wang, Keren Zhou, Richard Zou, Xiaodong Wang, Ajit Mathews, William Wen, Gregory Chanan, Peng Wu, and Soumith Chintala. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, ASPLOS 24, page 929947, New York, NY, USA, 2024. [9] Dmitri Arkhipov, Di Wu, Keqin Li, and Amelia Regan. Sorting with gpus: survey. arXiv preprint arXiv:1709.02520, 2017. [10] Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike Darcy, et al. Openscholar: Synthesizing scientific literature with retrievalaugmented lms. arXiv preprint arXiv:2411.14199, 2024. [11] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2023. [12] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Original implementation of selfrag: Learning to retrieve, generate and critique through self-reflection. https://github.com/AkariAsai/ self-rag, 2024. [13] Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau Yih. Reliable, adaptable, and attributable arXiv preprint language models with retrieval. arXiv:2403.03187, 2024. [14] Emery Berger and Ben Zorn. software should be more like plain old software. https://www.sigarch.org/ai-softwareshould-be-more-like-plain-old-software/, 2024. Ai [15] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 22062240. PMLR, 2022. [16] James Briggs, Gibbs Cullen, and Greg Kogan. Vector search in the wild. https://www.pinecone.io/ learn/series/wild/. [17] Tianhui Cai, Yifan Liu, Zewei Zhou, Haoxuan Ma, Seth Zhao, Zhiwen Wu, and Jiaqi Ma. Driving with regulation: Interpretable decision-making for autonomous vehicles with retrieval-augmented reasoning via llm. arXiv preprint arXiv:2410.04759, 2024. [18] Binoy Chemmagate. Reducing rag pipeline latency for real-time voice conversations. https://developer. vonage.com/en/blog/reducing-rag-pipelinelatency-for-real-time-voice-conversations, 2024. [19] Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu, Zengzhong Li, Mao Yang, and Jingdong Wang. Spann: Highly-efficient billion-scale approximate nearest neighbor search. arXiv preprint arXiv:2111.08566, 2021. [20] Felix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo, David Majnemer, and Sanjiv Kumar. Tpu-knn: nearIn Advances in est neighbor search at peak flop/s. 14 Neural Information Processing Systems, volume 35, pages 1548915501, 2022. apollo-247-leverages-medlm-with-rag-torevolutionize-healthcare, 2024. [21] Neo Christopher Chung, George Dyer, and Lennart Challenges of large language models arXiv preprint Brocki. for mental health counseling. arXiv:2311.13857, 2023. [22] Together Computer. GraphAI. https://github. com/receptron/graphai, 2024. [23] Databricks. Rag (retrieval augmented generahttps://docs.databricks. tion) on databricks. com/en/generative-ai/retrieval-augmentedgeneration.html, 2024. [24] Divyanshu Dixit. Advanced rag series: Generation https://div.beehiiv.com/p/ and evaluation. advanced-rag-series-generation-evaluation, 2024. [25] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. The faiss library. arXiv preprint arXiv:2401.08281, 2024. [26] Wenqi Fan, Yujuan Ding, Liang bo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. survey on rag meeting llms: Towards retrievalaugmented large language models. In Knowledge Discovery and Data Mining, 2024. [27] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17621777, 2023. [28] Yunfan Gao. Modular rag and rag flow: Part ii. https://medium.com/@yufan1602/modular-ragand-rag-flow-part-ii-77b62bf8a5d3, 2024. [29] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for arXiv preprint large language models: survey. arXiv:2312.10997, 2023. [30] Samira Ghodratnama and Mehrdad Zakershahrak. Adapting llms for efficient, personalized information retrieval: Methods and implications. In International Conference on Service-Oriented Computing, pages 17 26. Springer, 2023. [31] Abdussamad GM and Gopala Dhar. How apollo 247 leverages medlm with rag to revolutionhttps://cloud.google.com/ ize healthcare. blog/products/ai-machine-learning/how15 [32] Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, 2020. [33] Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models. arXiv preprint arXiv:2305.18466, 2023. [34] Ivan Ilin. Advanced rag techniques: an illushttps://pub.towardsai.net/ trated overview. advanced-rag-techniques-an-illustratedoverview-04d193d8fec6, 2023. [35] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021. [36] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):143, 2023. [37] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. Query expansion by prompting large language models. arXiv preprint arXiv:2305.03653, 2023. [38] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri, Ravishankar Krishnawamy, and Rohan Kadekodi. Diskann: Fast accurate billion-point nearest neighbor search on single node. Advances in Neural Information Processing Systems, 32, 2019. [39] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. arXiv preprint arXiv:2310.06839, 2023. [40] Wenqi Jiang, Shigang Li, Yu Zhu, Johannes de Fine Licht, Zhenhao He, Runbin Shi, Cedric Renggli, Shuai Zhang, Theodoros Rekatsinas, Torsten Hoefler, et al. Co-design hardware and algorithm for vector search. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 115, 2023. [41] Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, and Gustavo Alonso. Chameleon: heterogeneous and disaggregated accelerator system for retrieval-augmented language models. arXiv preprint arXiv:2310.09949, 2023. [42] Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, and Tim Kraska. Piperag: Fast retrievalaugmented generation via algorithm-system co-design. arXiv preprint arXiv:2403.05676, 2024. [43] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. 2023. [44] Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin Jin. Ragcache: Efficient knowledge caching for retrieval-augmented generation. arXiv preprint arXiv:2404.12457, 2024. [45] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: modular toolkit for efficient retrieval-augmented generation research. arXiv preprint arXiv:2405.13576, 2024. [46] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535547, 2019. [47] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, 2017. [48] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. [49] Saim Khan, Somesh Singh, Harsha Vardhan Simhadri, Jyothi Vedurada, et al. Bang: Billion-scale approximate nearest neighbor search using single gpu. arXiv preprint arXiv:2401.11324, 2024. [50] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2019. [51] Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and Jinwoo Shin. Sure: Improving open-domain question 16 answering of llms via summarized retrieval. In The Twelfth International Conference on Learning Representations, 2023. [52] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael Mahoney, Kurt Keutzer, and Amir Gholami. An llm compiler for parallel function calling. arXiv preprint arXiv:2312.04511, 2023. [53] Eyal Klang, Idit Tessler, Donald Apakama, Ethan Abbott, Benjamin Glicksberg, Monique Arnold, Akini Moses, Ankit Sakhuja, Ali Soroush, Alexander Charney, et al. Assessing retrieval-augmented large language model performance in emergency department icd-10-cm coding compared to human coders. medRxiv, 2024. [54] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. [55] Maximilian Lam, Jeff Johnson, Wenjie Xiong, Kiwan Maeng, Udit Gupta, Yang Li, Liangzhen Lai, Ilias Leontiadis, Minsoo Rhu, Hsien-Hsin S. Lee, Vijay Janapa Reddi, Gu-Yeon Wei, David Brooks, and G. Edward Suh. Gpu-based private information retrieval for ondevice machine learning inference, 2023. [56] LangChain, Inc. LangGraph. https://github.com/ langchain-ai/langgraph, 2024. [57] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. [58] Jerry Liu. LlamaIndex. https://github.com/ jerryjliu/llama_index, 11 2022. [59] Zihan Liu, Wentao Ni, Jingwen Leng, Yu Feng, Cong Guo, Quan Chen, Chao Li, Minyi Guo, and Yuhao Zhu. Juno: Optimizing high-dimensional approximate nearest neighbour search with sparsity-aware algorithm and ray-tracing core mapping. arXiv preprint arXiv:2312.01712, 2023. [60] Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, and Yaohua Tang. Turborag: Accelerating retrievalaugmented generation with precomputed kv caches for chunked text. arXiv preprint arXiv:2410.07590, 2024. [61] Xinbei Ma, Yeyun Gong, Pengcheng He, Nan Duan, et al. Query rewriting in retrieval-augmented large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [62] Melissa Malec. Rag in financial services: Use-cases, https://hatchworks.com/ impact, & solutions. blog/gen-ai/rag-for-financial-services/, 2024. [63] Alex Troy Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023. [64] Sewon Min, Suchin Gururangan, Eric Wallace, Weijia Shi, Hannaneh Hajishirzi, Noah Smith, and Luke Zettlemoyer. Silo language models: Isolating legal risk in nonparametric datastore. In The Twelfth International Conference on Learning Representations, 2023. [65] MyScale. 4 key benefits of rag algorithmic trading https://myscale.com/ in financial markets. blog/benefits-rag-algorithmic-tradingfinancial-markets/, 2024. [66] Mohammad Norouzi and David Fleet. Cartesian kIn Proceedings of the IEEE Conference on means. computer Vision and Pattern Recognition, pages 3017 3024, 2013. [67] NVIDIA. Geforce rtx 4090. https: //www.nvidia.com/en-us/geforce/graphicscards/40-series/rtx-4090/, 2024. [68] NVIDIA. h100 Nvidia gpu. https://resources.nvidia.com/en-us-tensorcore/nvidia-tensor-core-gpu-datasheet, 2024. tensor core [69] OpenAI. Gpt-3.5 turbo. https://platform.openai. com/docs/models/gpt-3-5-turbo. [70] Pathway. Adaptive rag: How we cut llm costs withhttps://pathway.com/ out sacrificing accuracy. developers/showcases/adaptive-rag, 2024. [71] Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Tongxu, and Enhong Chen. Large language model based long-tail query rewriting in taobao search. Companion Proceedings of the ACM on Web Conference 2024, 2023. [72] Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Derong Xu, Tong Xu, and Enhong Chen. Large language model based long-tail query rewriting in taobao search. In Companion Proceedings of the ACM on Web Conference 2024, 2024. [73] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 56875711, 2023. [74] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:13161331, 2023. [75] Rapidsai. Rapidsai/raft: Raft contains fundamental widely-used algorithms and primitives for data science, graph and machine learning. https://github.com/ rapidsai/raft, 2022. [76] Keshav Santhanam, Deepti Raghavan, Muhammad Shahir Rahman, Thejas Venkatesh, Neha Kunjal, Pratiksha Thaker, Philip Levis, and Matei Zaharia. Alto: An efficient network orchestrator for compound ai systems. In Proceedings of the 4th Workshop on Machine Learning and Systems, pages 117125, 2024. [77] Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Scaling retrieval-based language models with trillion-token datastore. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [78] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 92489274, 2023. [79] Susav Shrestha, Narasimha Reddy, and Zongwang Li. Espn: Memory-efficient multi-vector information retrieval. In Proceedings of the 2024 ACM SIGPLAN International Symposium on Memory Management, pages 95107, 2024. [80] Josef Sivic and Andrew Zisserman. Video google: text retrieval approach to object matching in videos. In ICCV, pages 14701477. IEEE Computer Society, 2003. [81] David Stewart and Jamie Linsdell. Say hello to precision: How rerankers and embeddings boost search. https://cohere.com/blog/say-hello-toprecision-how-rerankers-and-embeddingsboost-search, 2024. [82] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [83] Kuan Tung. Enhancing user experience by overcoming latency in the ion iq chatbot. https://www.ontinue. com/resource/enhancing-user-experience-byovercoming-latency-in-the-ion-iq-chatbot/, 2024. [84] Niithiyn Vijeaswaran, AJ Dhimine, Armando Diaz, Sebastian Bustillo, Farooq Sabir, and Marco Punio. Advanced rag patterns on amazon sagemaker. https://aws.amazon.com/blogs/machinelearning/advanced-rag-patterns-on-amazonsagemaker/, 2024. [85] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [86] Zijie J. Wang and Duen Horng Chau. Mememo: Ondevice retrieval augmentation for private and personalized text generation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 24, page 27652770, New York, NY, USA, 2024. Association for Computing Machinery. [87] Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, et al. Speculative rag: Enhancing retrieval augmented arXiv preprint generation through drafting. arXiv:2407.08223, 2024. [88] Khye Wei. Advanced rag with azure ai search https://techcommunity. and llamaindex. microsoft.com/t5/ai-azure-ai-servicesblog/advanced-rag-with-azure-ai-searchand-llamaindex/ba-p/4115007, 2024. [90] Zhiqiang Xie, Hao Kang, Ying Sheng, Tushar Krishna, Kayvon Fatahalian, and Christos Kozyrakis. Ai metropolis: Scaling large language model-based multiagent simulation with out-of-order execution. arXiv preprint arXiv:2411.03519, 2024. [91] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, In Conexplainable multi-hop question answering. ference on Empirical Methods in Natural Language Processing (EMNLP), 2018. [92] Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, and Junchen Jiang. Cacheblend: Fast large language model serving for rag with cached knowledge fusion, 2024. [93] Fanghua Ye, Meng Fang, Shenghui Li, and Emine Yilmaz. Enhancing conversational search: Large language model-aided informative query rewriting. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [94] Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, and Matthew Gadd. Rag-driver: Generalisable driving explanations with learning in multiretrieval-augmented in-context arXiv preprint modal large language model. arXiv:2402.10828, 2024. [95] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Ghodsi. The shift from models to compound https://bair.berkeley.edu/blog/ ai systems. 2024/02/18/compound-ai-systems/, 2024. [96] Chaoliang Zeng, Layong Luo, Qingsong Ning, Yaodong Han, Yuhang Jiang, Ding Tang, Zilong Wang, Kai Chen, and Chuanxiong Guo. FAERY: An FPGAaccelerated embedding-based retrieval system. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 841856, 2022. [97] Hengrui Zhang, August Ning, Rohan Baskar Prabhakar, and David Wentzlaff. Llmcompass: Enabling efficient hardware design for large language model inference. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 10801096, 2024. [89] Lukas Wutschitz, Boris Köpf, Andrew Paverd, Saravan Rajmohan, Ahmed Salem, Shruti Tople, Santiago Zanella-Béguelin, Menglin Xia, and Victor Rühle. Rethinking privacy in machine learning pipelines from an information flow control perspective, 2023. [98] Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lanting Li, Phitchaya Mangpo Phothilimthana, and Zhihao Jia. Accelerating retrieval-augmented language arXiv preprint model serving with speculation. arXiv:2401.14021, 2024. [99] Zili Zhang, Fangyue Liu, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast vector query processing for large datasets beyond GPU memory with reordered pipelining. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24), pages 2340, 2024. [100] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed Chi, Quoc Le, and Denny Zhou. Take step back: Evoking reasoning via abstraction in large language models. arXiv preprint arXiv:2310.06117, 2023. [101] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024. [102] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-tomost prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2022. [103] Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Yufei Gao, Qinyu Xu, Tian Tang, Zihao Ye, et al. Nanoflow: Towards optimal large language model serving throughput. arXiv preprint arXiv:2408.12757, 2024. [104] Yuhao Zhu. RTNN: Accelerating neighbor search using hardware ray tracing. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP 22, pages 7689, April 2022. [105] Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon. Open-source large language models are strong zero-shot query likelihood models for document ranking. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 88078817, 2023. [106] Zilliz. How to select index parameters for ivf index. https://zilliz.com/blog/select-indexparameters-ivf-index, 2020."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "University of Washington"
    ]
}