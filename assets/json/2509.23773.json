{
    "paper_title": "Knowledge Homophily in Large Language Models",
    "authors": [
        "Utkarsh Sahu",
        "Zhisheng Qi",
        "Mahantesh Halappanavar",
        "Nedim Lipka",
        "Ryan A. Rossi",
        "Franck Dernoncourt",
        "Yu Zhang",
        "Yao Ma",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering."
        },
        {
            "title": "Start",
            "content": "Utkarsh Sahu1, Zhisheng Qi1, Mahantesh Halappanavar2, Nedim Lipka3, Ryan A. Rossi3, Franck Dernoncourt3, Yu Zhang4, Yao Ma5, Yu Wang1 1University of Oregon 2Pacific Northwest National Laboratory 3Adobe Research 4Texas A&M University 5Rensselaer Polytechnic Institute {utkarsh,charq,yuwang}@uoregon.edu, hala@pnnl.gov, {lipka,ryrossi,dernonco}@adobe.com, yuzhang@tamu.edu, may13@rpi.edu 5 2 0 2 8 2 ] . [ 1 3 7 7 3 2 . 9 0 5 2 : r ABSTRACT Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering. Our code is available at https://github.com/utkarshxsahu/kgc. KEYWORDS Structured Knowledge, Large Language Model, Homophily"
        },
        {
            "title": "1 INTRODUCTION\nLarge Language Models (LLMs) have emerged as powerful neu-\nral knowledge bases by encoding vast amounts of world knowl-\nedge within their neural parameters [10, 21]. This neural-embedded\nknowledge enables LLMs to produce contextually relevant and fac-\ntually rich responses, supporting real-world applications such as\nfact checking [11] and question answering [32, 34]. To better ex-\nplore this neural knowledge base, researchers have devised knowl-\nedge checking methods to investigate the knowledge patterns of\nLLMs [1, 38] and leveraged the derived insights to guide knowledge-\nintensive tasks, including adaptive retrieval [35, 36], knowledge\nediting [25], and hallucination detection [26].",
            "content": "Despite various knowledge patterns identified previously [10, 21, 37], little attention has been given to whether LLMs knowledge exhibits structural organization. In fact, in cognitive neuroscience [12], several works have highlighted the semantic clustered patterns of the neural knowledge in human brain networks: (i) semantic clustering in memory recall, where people tend to retrieve related words together (e.g., recalling dog, cat, horse in sequence) [2, 16], and (ii) Figure 1: LLM is prompted on individual triple facts , which are aggregated to obtain entity-level knowledgeability scores. When visualized, these scores reveal the knowledge homophily pattern, where topologically close entities form distinct high-knowledge (red) and low-knowledge (blue) communities. Note that graph layout is visualized by the ForceAtlas2 algorithm [8] to preserve topological proximity. homophily brain networks, where regions with similar functions or inputs are more likely to connect [28]. Analogously, we hypothesize that LLMs also exhibit similar knowledge homophily pattern, i.e., they tend to possess similar levels of knowledge about conceptually related entities, as illustrated in Figure 1 by checking GPT-3.5s knowledge about triplets from WD50K dataset. Discovering this phenomenon provides valuable insights into the structural knowledge organization in LLMs and the design of knowledge-intensive task solutions. For example, by estimating the knowledgeability of concepts based on related concepts, one can identify factually weaker areas, thereby enabling more efficient knowledge labeling for knowledge injection and retrieval as we conducted in Section 4. Given the existence of knowledge homophily in other disciplines and its potential implications, this paper uncovers this innovative pattern and designs graph-based machine learning models to exploit neighborhood information to predict knowledgeability scores and discover less-known regions. These insights then guide efficient labeling for LLM fine-tuning and improve knowledge retrieval for multi-hop reasoning in question answering. Our contributions are: Knowledge Homophily Discovery: We demonstrate the existence of knowledge homophily in LLMs by measuring knowledge at triplet/entity levels, showing that topologically close entities tend to exhibit similar knowledgeability scores. Knowledge Homophily Application: We leverage the discovered knowledge homophily to develop GNN-based estimator that infers the entity knowledgeability, and showcase two applications enhancing knowledge injection efficiency and guiding multi-hop retrieval for question-answering."
        },
        {
            "title": "3.1 Knowledgeability Computation\nTo examine whether LLMs exhibit consistent knowledge about\nneighboring entities, we first evaluate knowledgeability at the\ntriplet level and then aggregate it to obtain an entity-level score.\nGiven triplets T = {(𝑠𝑖, 𝑑𝑖, 𝑟𝑖 )} | T |\n𝑖=1 from the knowledge graph, where\na source entity 𝑠𝑖 is connected to a destination entity 𝑑𝑖 via relation\n𝑟𝑖 , we define the knowledgeability of the LLM on triplet (𝑠𝑖, 𝑑𝑖, 𝑟𝑖 ) as\nK (𝑠𝑖, 𝑑𝑖, 𝑟𝑖 ), reflecting how well the LLM knows about this relational\nfact. For each entity 𝑠𝑖 , we denote its neighbor entity set as N (𝑠𝑖 ),\nrepresenting the entities adjacent to 𝑠𝑖 as either head or tail, and\ntheir corresponding neighbor triplet set as T (𝑠𝑖 ). The entity-level\nknowledgeability of 𝑠𝑖 , denoted as K (𝑠𝑖 ), is derived by aggregating\nknowledgeability scores over its neighboring triplets, capturing\nhow well the LLM knows about entity 𝑠𝑖 . Next, we introduce details\nof calculating triplet and entity knowledgeability.",
            "content": "Sahu, et al. Prompt 1: LLM-based Triplet Evaluation System Message: Evaluate the statement based on your knowledge and respond with True or False. Given: Triplet = (sub, rel, obj), Date 𝐷 (Temporal Version) Template: Relation pattern (e.g., son_of {SUB} is the son of {OBJ}.) Procedure: (1) Retrieve relation-based template for rel in triplet T. (2) Fill {SUB}sub, {OBJ}obj from to get statement S. (3) Append on Date 𝐷 (Temporal Version) (4) Prompt System Msg + User Msg: to the LLM. (5) Record the model output in the format of True/False"
        },
        {
            "title": "3.1.2 Calculating Entity Knowledgeability. Given the above calcu-\nlated triplet knowledgeability, we obtain knowledgeability of the\nentity 𝑣𝑖 by aggregating from all triplets involving 𝑣𝑖 [9, 22]:",
            "content": "K (𝑣𝑖 ) = (𝑣𝑖 ) 1 (𝑠, 𝑑, 𝑟 ). (1) (𝑠,𝑑,𝑟 ) (𝑣𝑖 ) Note that the above neighborhood aggregation naturally extends to temporal triplets (𝑠, 𝑑, 𝑟, 𝑡) (𝑣𝑖 ), allowing temporal information to be incorporated into the entity knowledgeability calculation."
        },
        {
            "title": "3.2 Homophily Computation and Analysis\nFurthermore, we evaluate whether topologically close entities share\nsimilar knowledgeability, i.e., the homophily of entity knowledge-\nability H (𝑣𝑖 ). Inspired by existing homophily computation [14, 31],\nwe compute knowledgeability homophily as one minus the average\nabsolute difference in knowledgeability between central node 𝑣𝑖\nand its neighbors N (𝑣 𝑗 ) in the knowledge graph:",
            "content": "H (𝑣𝑖 ) = 1 1 (𝑣𝑖 ) 𝑣𝑗 (𝑣𝑖 ) (𝑣𝑖 ) (𝑣𝑗 ) (2) where smaller difference between neighboring entities, (𝑣𝑖 ) (𝑣 𝑗 ), leads to higher homophily value (𝑣𝑖 ). We empirically quantify triplet and entity knowledgeability scores and analyze homophily patterns both quantitatively and qualitatively. We evaluate five representative LLMs: GPT-3.5, 4o, Gemini-2.5 Flash, LLaMA3.370B, and DeepSeek-V3. These models are assessed across five knowledge graphs: MVPKG [18], T-Rex [4], PharmKG [39], WD50K [5], and CoDEx-S [24]. Among them, T-Rex, WD50K, and CoDEx-S represent general Wikipedia knowledge, whereas PharmKG8K and MVPKG focus on biomedical and political science. Our graph visualizations in Figure 1/3 employ the ForceAtlas2 [8] to position topologically close nodes visually close, enabling an intuitive assessment of whether they share similar knowledgeability scores. Knowledge Homophily in Large Language Models Figure 2: (a): Distribution of node knowledgeability homophily for each dataset; (b): Average knowledge homophily across datasets and LLMs with black line showing Citeseer benchmark (0.74) for high homophily in graph analysis."
        },
        {
            "title": "3.2.1 Quantitative Analysis of Knowledge Homophily. Figure 2(a)/(b)\npresents the node homophily distribution and the average graph ho-\nmophily across several knowledge graphs. In Figure 2(a), these dis-\ntributions are all right-skewed and peak around 0.8, suggesting the\nmajority portion of nodes and their neighbors tend to share similar\nknowledgeability scores. This high homophily property enhances\ngraph machine learning in node-level prediction, such as node clas-\nsification and regression [40], and therefore inspires regression\nto predict entities’ knowledge scores in Section 4.1. Furthermore,\nincorporating temporal information into MVPKG results in a slight\nleft shift. This slightly decreasing neighbor homophily score might\nindicate that the temporal dimension introduces greater complex-\nity and finer distinctions in knowledgeability between the nodes\nand their neighbors. In addition, we compute the average graph\nhomophily by averaging across all nodes in Figure 2(b). Across\ndatasets and LLMs, the scores mostly exceed the homophily of the\nCiteseer [27, 33] benchmark (horizontal line), a classic homophily\nnetwork for node classification. This consistency across diverse\nsettings indicates that the observed homophily generally aligns\nwith the conventional notion of a “high-homophily” network.",
            "content": "Furthermore, we compare the calculated knowledge homophily against degree-matched random baseline. For each node 𝑣 instead of using its actual neighborhood (𝑣), we create random peer group (cid:98)N (𝑣) by randomly sampling the same number of entities ( (cid:98)N (𝑣) = (𝑣)) from the entire graph. Then, homophily score is calculated as the difference between the central node 𝑣 and this randomly sampled group. The true neighborhood homophily was found to be statistically significant than this random baseline (100 trials per dataset, two-tailed z-test, 𝑝 < 0.01), with extremely narrow 99% confidence intervals. The results in Figure 3(a) show that across datasets, the homophily calculated from true neighbors exceeds the random baseline, confirming that it is not statistical artifact or random fluctuation, but rather robust and intrinsic structural property of LLMs internal knowledge organization."
        },
        {
            "title": "3.2.2 Qualitative Analysis of Knowledge Homophily. Figure 3(b)\nshows a sampled T-Rex subgraph with nodes colored by entity-\nlevel knowledgeability K (𝑣). A geopolitical neighborhood (coun-\ntries) forms a compact and uniformly high-knowledgeability region\nwith small intra-neighborhood deviations, confirming the high\nknowledgeability homophily. A historical football neighborhood\n(event/players) is similarly coherent but at lower knowledgeability.\nDespite varying means, both exhibit knowledge homophily.",
            "content": "Figure 3: (a) Neighboring nodes possess similar knowledgeability scores than randomly sampled nodes, indicating that homophily is intrinsic structural property of LLMs; (b) Entities with their distinct knowledgeability levels (𝑣) indicated by node color (Red = High, Blue = Low)."
        },
        {
            "title": "4.1 Homophily-aware Knowledge Estimation\nGiven homophily is sufficient for high-utility GNN predictions [14],\nwe design a GNN-based regression model to perform message-\npassing, aggregate neighboring entity embeddings, and predict\npreviously unknown entity scores. Specifically, given a set of enti-\nties VTrain with known knowledgeability (by prompting LLMs), our\ngoal is to train a GNN to estimate the knowledgeability of unseen\nentities. At each layer, the model performs Message Passing (MP)\nand Feature Transformation (TR), followed by regression:",
            "content": "(cid:98)K𝑙 𝑖 = MP 𝑙 (cid:0) (cid:101)K𝑙 1 𝑗 𝑣𝑗 (𝑣𝑖 ) 𝑣𝑖 (cid:1), (cid:101)K𝑙 𝑖 = TR 𝑙 ( (cid:98)K𝑙 𝑖 ), = 1 VTrain (cid:12) (cid:12) (cid:101)K𝑙 (cid:12) 𝑖 K𝑖 (cid:12) 2 (cid:12) (cid:12) , 𝑣𝑖 VTrain The initial node feature matrix is defined as (cid:101)K 0 = [X1, . . . , X𝑣V ], where each node feature X𝑣𝑖 is dense text embedding obtained from pretrained language models. After training on VTrain, the model is further used to infer the knowledgeability scores for all entities in the knowledge graph, eliminating the need for resource/timeintensive knowledge probing via exhaustive LLM prompting. In the following, we will utilize these estimated knowledgeability scores of entities to either guide triplet selection for subsequent LLM fine-tuning in Section 4.2 or guide retrieval for answering reasoning-intensive multi-hop questions in Section 4.3. Due to space constraints, we only briefly describe the experimental setup and provide full details in Figure 4 in Appendix. (3) (4) Table 1: Performance comparison of fine-tuning with triplets selected by knowledgeability estimated by Random, MLP, and GNN. Best result in bold and second-best underlined. L=Llama3-8B, M=Mistral-7B. Selection Quality: percentage of triplets selected for fine-tuning that are unknown to base LLMs. Generalization Gain: percentage of additional 2% evaluation triplets identified by the fine-tuned LLMs. Detailed setting is visualized in Figure 4 in Appendix. Table 2: Multi-hop Question Answering Accuracy by GPT4as-a-Judge; M=MLP, G=GNN, BS=Beam Search, = Hop Dataset Q-Hop T-Rex PharmKG WD50K MVPKG CoDExS 2-H 3-H 2-H 3-H 2-H 3-H 2-H 3-H 2-H 3-H Base M-BS G-BS 25.1 17.2 24.4 19.1 28.6 20.4 30.9 22.6 21.4 33.8 23.1 21.7 25.8 17.3 24.9 19.4 29.9 20.5 34.2 23.7 22.2 16.6 26.0 17.5 25.4 19.5 31.1 20.9 16.0 16.2 Sahu, et al. Task Method T-Rex PharmKG WD50K MVPKG CoDExS M L Avg. t e - l n y Rand MLP GNN a Base Rand MLP GNN 54.4 36.5 44.8 81.9 38.4 48.7 84.8 57.4 37.3 54.5 87.6 79.2 49.6 50.6 72.2 71.5 45.5 63.9 61.2 41.2 46.8 68.5 66.3 33.8 51.7 44.3 47.8 67.9 68.6 39.1 57.8 72.8 77. 49.9 63.3 64.0 17.8 60.5 86.4 81.9 34.9 87.9 90.2 35.8 65.6 89.1 91.9 37.0 60.7 58.8 55.1 44.5 76.7 75.6 88.0 67.7 54.8 42.9 26.1 52.3 64.9 58.5 57.8 56.3 30.7 65.1 78.8 72.1 56.1 53.2 42.8 74.5 73.7 85.2 55.3 41.3 57."
        },
        {
            "title": "4.2 Homophily-guided Knowledge Injection\nWe leverage the homophily to estimate triplet knowledgeability\nand prioritize selecting less-known triplets for fine-tuning LLMs\nwithin a fixed budget, thereby enabling more effective knowledge\ninjection into LLMs. For each dataset, we allocate 4000 triplets\nas the knowledge-checking budget for selection and fine-tuning,\nwith an additional 2% of all triplets reserved as the test set. Within\nthe 4000 budget, 20% of triplets are sampled as anchor points, for\nwhich we directly query the base LLM to obtain ground-truth bi-\nnary knowledgeability scores (Section 3.1). These anchors provide\nlabeled data to estimate the knowledgeability of their associated\nentities, which is used to train a GNN model (Eq. (4)) and predict\nknowledgeability scores for all remaining entities. Based on these\npredictions, we prioritize triplets with lower-scored entities from\nthe remaining 80% unqueried pool to complete the 4000-triplet set\nfor fine-tuning. We benchmark this triplet selection against two\nbaselines: Random, which uniformly samples triplets, and MLP,\nwhich estimates knowledgeability without homophily, eliciting the\nknowledge homophily contribution to knowledge estimation. We\nexperiment with LLaMA3-8B(L) and Mistral-7B(M).",
            "content": "Table 1 evaluates homophily-guided knowledge injection from two perspectives: selection quality and generalization gain. For selection quality, we assess whether the chosen triplets better capture the knowledge deficiencies of LLMs. Among the 4000 triplets selected for fine-tuning, we compute the percentage that the base LLM does not recognize, following the procedure in Section 3.1. higher score indicates that more selected triplets are unknown to the LLM and thus more valuable for fine-tuning. Our GNN regressor achieves the highest proportion of unknown triplets, outperforming MLP and Random selection. This demonstrates the advantage of incorporating homophily into GNN design in enabling more effective estimation of ground-truth knowledgeability for knowledge injection. For generalization gain, we test whether fine-tuning on selected triplets improves the knowledgeability over the reserved 2% held-out set. The best performing GNN regressor confirms that its higher selection precision translates into stronger knowledge generalization. This superior generalization gain holds across different evaluation budgets from 1% to 20% in Figure 5 in Appendix."
        },
        {
            "title": "4.3 Homophily-guided Knowledge Retrieval\nWe test whether the estimated knowledgeability can guide en-\ntity retrieval to provide better context for question-answering. For\neach KG, we generate 1000 questions (500 2-hop/500 3-hop). Entity\nknowledgeability K (𝑣) is predicted with a GNN regressor trained\non 40% of entities labeled by GPT-3.5, excluding entities for generat-\ning 1000 evaluation questions. We embed both entities/relations and\nquestions using all-MiniLM-L6-v2. Starting with entity linking\nin the question, we run beam search up to the hop limit and score\neach neighbor by its knowledgeability K (𝑣) and semantic similar-\nity S(𝑟 ||𝑑, 𝑞) to the question 𝑞 where 𝑟 ||𝑑 represents its relation 𝑟\nconcatenated with the tail entity 𝑑. Baselines are as follows:\n• Baseline (Semantic Beam Search): It retrieves paths using\nbeam search guided solely by the semantic similarity S(𝑟 ||𝑑, 𝑞)\nbetween the path (relation + tail entity) and the input question.\n• Knowledge-aware Beam Search (BS): This method adjusts\nbeam search to favor less-known entities. For each expansion, the\nsemantic score S is penalized by the next entity knowledgeability,\nK (𝑢). The final score is S × (1−𝛼 ·K (𝑢)) with 𝛼 being weighting\nfactor. Entities with lower knowledgeability receive higher re-\ntrieval priority, achieving knowledge-aware search. Beam Search\n(BS) with GNN/MLP as knowledge estimator are G-BS/M-BS.",
            "content": "The reader model GPT-3.5 generates answers strictly based on the retrieved triples, and their correctness is evaluated by GPT-4. In Table 2, M/G-BS outperforms Baseline across all datasets and hops. Crucially, GNN-based beam search (G-BS) surpasses its homophilyagnostic MLP-based counterpart (M-BS), confirming the advantage of structural knowledge homophily in estimating knowledgeability. On 2-hop, G-BS has an improvement of 4.57% over the baseline, with larger gains on general KGs (e.g., T-Rex) than domain-specific ones (e.g., PharmKG). For 3-hop questions, all methods drop due to longer chains and semantic drift, but G-BS still has an improvement of 2.62% over the baseline. This superior performance of G-BS holds across different budgets for training the knowledgeability estimator in Figure 6 in Appendix."
        },
        {
            "title": "5 CONCLUSION\nMotivated by the structural knowledge organization of the human\nbrain, this work explores the homophily structure of neural knowl-\nedge stored in LLMs, showing that LLMs’ knowledge about entities\nis strongly correlated with their neighbors in a knowledge graph.\nBuilding on this insight, we design a GNN regressor to estimate\nentity-level knowledgeability scores by leveraging their local neigh-\nborhood scores. We demonstrate the utility of these scores in two\napplications: selecting less-known triplets for efficient knowledge\ninjection through fine-tuning, and improving retrieved context for\nenhancing multi-hop question answering.",
            "content": "Knowledge Homophily in Large Language Models"
        },
        {
            "title": "6 ETHICAL CONSIDERATIONS\nThe knowledge homophily, where topologically proximate enti-\nties tend to share similar knowledgeability, increases the risk of\nknowledge-extraction attacks. An adversary can exploit homophily\nby crafting queries that target cohorts of related entities (or by\ncombining entities across neighborhoods) to maximize coverage\nand force the model to reveal more information than intended. This\nraises privacy and copyright concerns because grouping semanti-\ncally related entities makes it easier for the model to reconstruct or\nexpose sensitive facts. To mitigate this risk, we propose defensive\nmeasures such as pre-filtering and sanitizing queries to remove sus-\npicious or overly targeted prompts, rate-limiting repetitive/cohort\nqueries, applying prompt-level heuristics that block requests for\nverbatim proprietary content, and deploying detector models or\nred-teaming workflows to identify adversarial extraction patterns.\nCombining these defenses with selective fine-grained access con-\ntrols and differential privacy constraints can substantially reduce\nthe attack surface introduced by knowledge homophily.",
            "content": "REFERENCES [1] Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. 2022. review on language models as knowledge bases. arXiv preprint arXiv:2204.06031 (2022). [2] Weston Bousfield and Charles Hill Sedgewick. 1944. An analysis of sequences of restricted associative responses. The Journal of General Psychology (1944). [3] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2021. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696 (2021). [4] Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-rex: large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). [5] Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, and Jens Lehmann. 2020. Message passing for hyper-relational knowledge graphs. arXiv preprint arXiv:2009.10847 (2020). [6] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2020. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913 (2020). [7] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020). [8] Mathieu Jacomy, Tommaso Venturini, Sebastien Heymann, and Mathieu Bastian. 2014. ForceAtlas2, continuous graph layout algorithm for handy network visualization designed for the Gephi software. PloS one (2014). [9] Shengbin Jia, Yang Xiang, Xiaojun Chen, and Kun Wang. 2019. Triple trustworthiness measurement for knowledge graph. In The World Wide Web Conference. 28652871. [10] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli TranJohnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221 (2022). [11] Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958 (2021). [12] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, et al. 2025. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv preprint arXiv:2504.01990 (2025). [13] Linhao Luo, Thuy-Trang Vu, Dinh Phung, and Gholamreza Haffari. 2023. Systematic assessment of factual knowledge in large language models. arXiv preprint arXiv:2310.11638 (2023). [14] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. 2021. Is homophily necessity for graph neural networks? International Conference on Learning Representations (2021). [15] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511 (2022). Jeremy Manning and Michael Kahana. 2012. Interpreting semantic clustering effects in free recall. Memory (2012). [16] [17] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in gpt. Advances in neural information processing systems 35 (2022), 1735917372. [18] Xinyi Mou, Zejun Li, Hanjia Lyu, Jiebo Luo, and Zhongyu Wei. 2024. Unifying local and global knowledge: Empowering large language models as political experts with knowledge graphs. In Proceedings of the ACM Web Conference 2024. 26032614. [19] Vishwas Mruthyunjaya, Pouya Pezeshkpour, Estevam Hruschka, and Nikita Bhutani. 2023. Rethinking language models as symbolic knowledge graphs. arXiv preprint arXiv:2308.13676 (2023). [20] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language Models as Knowledge Bases?. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 24632473. [21] Pouya Pezeshkpour. 2023. Measuring and modifying factual knowledge in large language models. In 2023 International Conference on Machine Learning and Applications (ICMLA). IEEE, 831838. [22] Thorsten Rings, Timo Bröhl, and Klaus Lehnertz. 2022. Network structure from characterization of interactions in complex systems. Scientific Reports 12, 1 (2022), 11742. [23] Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How Much Knowledge Can You Pack Into the Parameters of Language Model?. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 54185426. [24] Tara Safavi and Danai Koutra. 2020. Codex: comprehensive knowledge graph completion benchmark. arXiv preprint arXiv:2009.07810 (2020). [25] Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, and Ninghao Liu. 2024. Retrieval-enhanced knowledge editing in language models for multi-hop question answering. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. 20562066. [26] Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang. 2023. Knowledge unlearning for llms: Tasks, methods, and challenges. arXiv preprint arXiv:2311.15766 (2023). [27] Zixing Song, Xiangli Yang, Zenglin Xu, and Irwin King. 2022. Graph-based semi-supervised learning: comprehensive review. IEEE Transactions on Neural Networks and Learning Systems (2022). [28] Olaf Sporns. 2012. The human connectome: complex network. Schizophrenia Research (2012). [29] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models finetuned with human feedback. arXiv preprint arXiv:2305.14975 (2023). [30] Denny Vrandečić and Markus Krötzsch. 2014. Wikidata: free collaborative knowledgebase. Commun. ACM (2014). [31] Yu Wang and Tyler Derr. 2021. Tree decomposed graph neural network. In Proceedings of the 30th ACM international conference on information & knowledge management. 20402049. [32] Yu Wang, Nedim Lipka, Ryan Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. 2024. Knowledge graph prompting for multi-document question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 1920619214. Imbalanced graph classification via graph-of-graph neural networks. In Proceedings of the 31st ACM international conference on information & knowledge management. 20672076. [33] Yu Wang, Yuying Zhao, Neil Shah, and Tyler Derr. 2022. [34] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. HotpotQA: dataset for explainable multi-hop question answering. arXiv preprint arXiv:1809.09600 (2018). [35] Zijun Yao, Weijian Qi, Liangming Pan, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, and Juanzi Li. 2024. Seakr: Self-aware knowledge retrieval for adaptive retrieval augmented generation. arXiv preprint arXiv:2406.19215 (2024). [36] Zihan Zhang, Meng Fang, and Ling Chen. 2024. RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering. In Findings of the Association for Computational Linguistics ACL 2024. 69636975. [37] Danna Zheng, Mirella Lapata, and Jeff Pan. 2024. Large language models as reliable knowledge bases? arXiv preprint arXiv:2407.13578 (2024). [38] Shangshang Zheng, He Bai, Yizhe Zhang, Yi Su, Xiaochuan Niu, and Navdeep Jaitly. 2023. KGLens: Towards Efficient and Effective Knowledge Probing of Large Language Models with Knowledge Graphs. arXiv preprint arXiv:2312.11539 (2023). [39] Shuangjia Zheng, Jiahua Rao, Ying Song, Jixian Zhang, Xianglu Xiao, Evandro Fei Fang, Yuedong Yang, and Zhangming Niu. 2021. PharmKG: dedicated knowledge graph benchmark for bomedical data mining. Briefings in bioinformatics (2021). Jiong Zhu, Ryan Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen Ahmed, and Danai Koutra. 2021. Graph neural networks with heterophily. In Proceedings of the AAAI conference on artificial intelligence, Vol. 35. 1116811176. [40] Sahu, et al. Figure 4: Homophily-guided Knowledge Injection and Retrieval: The process begins by training GNN on subset of entities with ground-truth knowledgeability scores (Blue Nodes) obtained by querying the base LLM. The trained GNN then infers the knowledgeability scores for all remaining entities (Green Nodes). Based on these predictions, triplets associated with entities estimated to have the lowest knowledge values are selected until the budget is met. Finally, the base LLM is fine-tuned on these less-known triplets to efficiently inject new knowledge, and its improved performance is measured on held-out test set (Orange Nodes) in Figure 4(a). The estimated knowledgeability scores also guide retrieval, as illustrated in Figure 4(b). APPENDIX A.1 Experimental Setting Visualization A.1.1 Homophily-guided Knowledge Injection. Figure 4 (a) illustrates the pipeline of homophily-guided knowledge injection. In general, we leverage knowledge homophily to train GNN estimator that predicts entity knowledgeability, and then use these estimated knowledgeability scores to identify less-known triplets for fine-tuning LLMs. The global procedure is as follows: Step 1 - Fine-tuning Stage - Anchor Set Selection: The process begins with predefined triplet budget for fine-tuning LLMs. From this budget, 20% is allocated to anchor triplets, while the remaining 80% is reserved for knowledgeability estimation. Anchor triplets are constructed using an entity-centric sampling strategy: entities are randomly selected one by one, and all their associated triplets are added until the 20% quota is met. These anchor entities are used for training the GNN knowledge estimator and are shown as the blue Known Knowledge Scores nodes in Figure 4(a). Their ground-truth knowledgeability is obtained by querying the base LLM on their anchor triplets and aggregating the outcomes (see Section 3.1). With knowledge homophily, the GNN is then trained on these anchor entities to learn the relation between graph topology and knowledgeability. Step 2 - Fine-tuning Stage - Estimating Knowledgeability of Remaining Set: After training, the GNN estimator infers scores for all unlabeled entities (i.e., those with unknown knowledgeability). Entities are then ranked by their predicted knowledge value, (𝑣), where lower scores indicate higher unknown level to the LLM. Finally, triplets linked to the least knowledgeable entities are selected as the remaining 80% of fine-tuning budget. Step 3 - Evaluation Stage: The selected triplets are combined with the initial anchor set to form the fine-tuning dataset. This dataset, enriched with facts less known to the LLM, is used for fine-tuning. We evaluate the procedure in two ways. First, we measure selection quality, verifying whether the selected triplets are indeed unknown to the LLM. Second, we sample 2% of triplets (orange nodes) that are neither used in fine-tuning nor involve entities overlapped with fine-tuned triplets, to assess the generalization gain. As shown in Table 1, the fine-tuned LLM exhibits clear improvements in both selection quality and generalization performance. These results demonstrate the effectiveness of our homophily-aware knowledge injection in identifying knowledge deficiencies of LLMs and maximizing fine-tuning gains. A.1.2 Homophily-guided Knowledge Retrieval. Figure 4(b) details the operational pipeline of our homophily-guided knowledge retrieval method in Section 4, designed to enhance the quality of the retrieved context to further improve multi-hop question answering. The process begins with the creation of multi-hop question set from 2-hop and 3-hop triplet paths. To ensure fair evaluation, the entities that constitute these question paths are explicitly excluded from the GNN training data to prevent data leakage. From the remaining pool of entities, 40% are sampled to train the GNN regressor. This trained model infers the knowledge scores, (𝑣), for other entities, quantifying the awareness of LLMs of these other entities. For given multi-hop question, the retrieval process commences with entity linking to anchor the query to starting entity in the graph. From this point, GNN-based Beam Search (G-BS) is employed to explore potential reasoning paths. The core of this method is its unique scoring function, (1 𝛼 (𝑢)), which balances semantic relevance with penalty based on the knowledgeability of the next entity (K (𝑢)). By penalizing the expansions toward well-known entities, the search prioritizes retrieving facts with higher information gain, thereby providing the LLM with specific context to answer the query. Knowledge Homophily in Large Language Models Table 3: Correlation results between entity knowledgeability scores under Full and Sparse settings across datasets. Dataset Full vs Sparse75% Full vs Sparse50% Pearson Spearman Pearson Spearman CoDEx-S 0.9577 MVPKG 0.9376 PharmKG 0.9444 T-Rex 0.9177 WD50K 0.9370 0.9484 0.9357 0.9408 0.9156 0.9278 0.8490 0.8839 0.8773 0.7957 0.8275 0.8398 0.8775 0.8650 0.7834 0.8080 A.2 Additional Results A.2.1 Knowledgeability Score Robustness. To validate that the calculated entity knowledgeability score (K (𝑣)) is robust to our triplet sampling algorithm, we conducted an experiment to assess its stability under data sparsity. For each knowledge graph, we created two sparsified versions: one retaining 75% of the original triplets (Sparse75%) and another retaining 50% (Sparse50%). We then recalculated the knowledgeability scores for all entities on these sparse graphs and computed the Pearson/Spearman correlations against the scores derived from the full, original graph. The results, presented in Table 3, demonstrate the stability of our metric. For the Sparse75% condition, the Pearson correlation consistently exceeded 0.91 across all datasets, indicating very strong linear relationship. Even with half of the relational data removed in the Sparse50% condition, the scores maintained strong correlation with the originals, with Pearson values generally above 0.80. The consistently high values for both Pearson and Spearman coefficients confirm that the entity knowledgeability score is not simply byproduct of local graph density. These findings provide strong empirical evidence that our metric captures stable property of the LLMs knowledge, which can be reliably estimated even from an incomplete set of relational facts. Sensitivity Analysis: Knowledge Injection. To assess the roA.2.2 bustness of our findings and ensure our conclusions are not dependent upon specific test set size in the knowledge injection experiment, we conduct sensitivity analysis. This analysis evaluates the performance of our fine-tuned models on the CoDEx-S dataset using the Mistral-7B model, mirroring the primary experiment in Section 4.2. We constructed five randomly sampled test sets, each representing different proportion of the total dataset: 1%, 2%, 5%, 10%, and 20%. It was ensured that none of the entities used to train the GNN/MLP knowledgeability estimator appeared in any of the test sets. The results of this analysis are presented in the Figure 5. Our key findings of this analysis are: Consistent Superiority: The GNN-guided knowledge injection method consistently outperforms the MLP, Random, and Baseline approaches across all evaluation set sizes. This confirms the significant advantage of leveraging knowledge homophily. Performance Stability: Although minor fluctuations were observed, attributable to statistical variance in sampling, the performance of all methods remained relatively stable across evaluation set sizes. This finding suggests that the measured performance of the models is not merely statistical artifact of particular test set size. Figure 5: The knowledge injection performance of the finetuned Mistral models on the CoDEx-S dataset across varying test set sizes. The GNN-guided approach maintains significant performance advantage over other methods. Figure 6: The knowledge-aware retrieval performance across the varying training budgets of the underlying knowledgeability estimator. For both 2-hop (left) and 3-hop (right) QA on the CoDEx-S dataset, the GNN-based search (G-BS) consistently outperforms the homophily-agnostic MLP-based search (M-BS) and the semantic baseline. Sensitivity Analysis: Knowledge Retrieval. To evaluate the A.2.3 robustness of our knowledge-aware beam search method (G-BS and M-BS), we conducted sensitivity analysis on the amount of training data used for the knowledgeability estimators. The primary experiment in our paper utilizes GNN and MLP models trained on 40% of the available entities. This analysis investigates how performance on the multi-hop question-answering task varies when this training budget is reduced. For this sensitivity analysis we select the CoDEx-S dataset. We trained series of GNN and MLP knowledgeability estimators on progressively larger subsets of entity data: 1%, 2%, 5%, 10%, 20%, and 40%. Each resulting estimator was then integrated into the G-BS and M-BS retrieval methods, respectively, and evaluated on the fixed set of 1,000 2-hop and 3-hop questions. The performance of the Semantic Beam Search baseline is independent of this training budget and remains constant. The results are presented in the Figure 6 for 2-hop (left) and 3-hop (right) QA performance. Our key findings of this analysis are: Consistent Superiority of G-BS: The GNN-based approach (G-BS) consistently outperforms both the homophily-agnostic MLP-based method (M-BS) and the baseline across all training set sizes and for both 2-hop and 3-hop questions. This confirms that the advantage of leveraging knowledge homophily is robust, even under data-scarce conditions. Performance Scaling with Data: The performance of both GBS and M-BS improves as the training budget for the knowledgeability estimator increases. This suggests that more accurately estimated knowledge scores lead to better path retrieval for QA."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Pacific Northwest National Laboratory",
        "Rensselaer Polytechnic Institute",
        "Texas A&M University",
        "University of Oregon"
    ]
}