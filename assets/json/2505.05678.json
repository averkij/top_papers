{
    "paper_title": "InstanceGen: Image Generation with Instance-level Instructions",
    "authors": [
        "Etai Sella",
        "Yanir Kleiman",
        "Hadar Averbuch-Elor"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite rapid advancements in the capabilities of generative models, pretrained text-to-image models still struggle in capturing the semantics conveyed by complex prompts that compound multiple objects and instance-level attributes. Consequently, we are witnessing growing interests in integrating additional structural constraints, typically in the form of coarse bounding boxes, to better guide the generation process in such challenging cases. In this work, we take the idea of structural guidance a step further by making the observation that contemporary image generation models can directly provide a plausible fine-grained structural initialization. We propose a technique that couples this image-based structural guidance with LLM-based instance-level instructions, yielding output images that adhere to all parts of the text prompt, including object counts, instance-level attributes, and spatial relations between instances."
        },
        {
            "title": "Start",
            "content": "InstanceGen: Image Generation with Instance-level Instructions ETAI SELLA, Tel Aviv University, Israel and Meta AI, UK YANIR KLEIMAN, Meta AI, UK HADAR AVERBUCH-ELOR, Cornell University, USA Ours Emu Flux1-dev SDXL Bounded Attention Attention Refocusing tray with three muffins, vanilla Ã©clair, and piece of cake on it, one muffin is blueberry, the other two are chocolate. four friends dressed up for Halloween, from right to left they are dressed as surgeon, witch, police officer, and cowboy. three fish and seahorse in an aquarium, only the fish closest to the surface is clown fish. Fig. 1. We present method that incorporates instance-level instructions in image generation, capable of adhering to prompts with various object counts (), instance-level attributes (), and spatial relations (). Above we compare with several recent diffusion models (Emu [Dai et al. 2023], Flux [Labs 2023], SDXL [Podell et al. 2023]), as well as prior techniques that use bounding boxes to guide the generation (Bounded Attention [Dahary et al. 2025], Attention Refocusing [Phung et al. 2024]). Filled squares represent adherence to the prompt, and empty squares represent mismatch with the prompt. Despite rapid advancements in the capabilities of generative models, pretrained text-to-image models still struggle in capturing the semantics conveyed by complex prompts that compound multiple objects and instancelevel attributes. Consequently, we are witnessing growing interests in integrating additional structural constraints, typically in the form of coarse bounding boxes, to better guide the generation process in such challenging cases. In this work, we take the idea of structural guidance step further by Authors Contact Information: Etai Sella, Tel Aviv University, Tel Aviv, Israel and Meta AI, London, UK, etaisella@gmail.com; Yanir Kleiman, Meta AI, London, UK, yanirk@gmail.com; Hadar Averbuch-Elor, Cornell University, New York, New York, USA, hadarelor@cornell.edu. making the observation that contemporary image generation models can directly provide plausible fine-grained structural initialization. We propose technique that couples this image-based structural guidance with LLM-based instance-level instructions, yielding output images that adhere to all parts of the text prompt, including object counts, instance-level attributes, and spatial relations between instances. Additionally, we contribute CompoundPrompts, benchmark composed of complex prompts with three difficulty levels in which object instances are progressively compounded with attribute descriptions and spatial relations. Extensive experiments demonstrate that our method significantly surpasses the performance of prior models, particularly over complex multi-object and multi-attribute use cases. CCS Concepts: Computing methodologies Artificial intelligence; Computer vision. This work is licensed under Creative Commons Attribution 4.0 International License. Additional Key Words and Phrases: Diffusion Models, Image Generation 5 2 0 M 2 1 ] . [ 2 8 7 6 5 0 . 5 0 5 2 : r 2 Etai Sella, Yanir Kleiman, and Hadar Averbuch-Elor"
        },
        {
            "title": "Introduction",
            "content": "Text-to-image generation methods have shown continuous improvement in image quality and text fidelity, i.e., how closely the produced image follows the details described in the text prompt. However, current methods still struggle with text prompts which are composed of multiple objects with separate attributes associated with each object, including quantities (e.g. \"three cupcakes\"), physical attributes (e.g. \"chocolate cupcake\"), and spatial relations (e.g. \"chocolate cupcake on the right and vanilla cupcake on the left\"). Examples of typical failure cases for current methods are shown in Figure 1. The failure is often due to mismatch between each object instance and its associated attributes, where an object instance is either missing specified attribute, erroneously associated with attributes of different objects, or has the wrong count. To address these type of failures, existing work typically employs layout-based generation, where the coarse layout is either provided by the user [Chen et al. 2024; Dahary et al. 2025; Xie et al. 2023], or generated as bounding boxes by an LLM [Chen et al. 2023; Phung et al. 2024]. However, bounding box layouts generated this way tend to be inaccurate and unrealistic, providing relatively weak structural signal to the image generation model for generating an image that adequately follows the complex multi-object prompt. natural question therefore arises: Can one devise text-to-image generation technique that utilizes more fine-grained structural guidance, without requiring any additional inputs? In this work, we answer this question affirmatively by making the observation that current image generation models can provide rough initialization which is more precise and realistic than bounding boxes, and can be used as an anchor for generating an image which faithfully adhere to the target prompt. We present an inference-time text-to-image generation technique that couples this image-based structural guidance with instance-level instructions extracted using Large Language Models (LLMs), leveraging the relative strengths of image-based and text-based techniques for producing fine-grained layout augmented with instance-level instructions. For example, the instance-level instructions may indicate that certain segment should contain chocolate cupcake while another segment should contain vanilla cupcake with sprinkles. Moreover, we propose an attention-based optimization technique guided by our fine-grained structural layout, yielding final generated image that accurately represents the text prompt. Existing benchmarks for evaluating text fidelity [Bakr et al. 2023; Saharia et al. 2022; Wu et al. 2024b] are often too simplified or focused on out-of-distribution surreal scenes. Conversely, little attention is directed towards more complex prompts with multiple instance-level instructions. Therefore, to facilitate quantitative evaluation, we introduce benchmark of 540 prompts with instancelevel instructions which combine objects of different quantities, semantic attributes, and spatial relations. Our benchmark is divided to three complexity levels: object counts, instance-level attributes, and spatial relations between instances. We evaluate our method on the new benchmark and existing benchmarks [Ghosh et al. 2024; Saharia et al. 2022]. The evaluation shows that our method achieves stronger performance across multiple metrics quantifying to what extent the generated image conveys the target text prompt. In summary, our contributions in this paper are: text-to-image generation method which generates layout with per-segment instance-level instructions from text prompt, and conditions the image generation on the generated layout, with no additional training. benchmark for evaluating prompts of three complexity levels with multiple objects and instance-level attributes. State-of-the-art results that show our methods ability to tackle particularly challenging use cases which are not well addressed by current methods."
        },
        {
            "title": "2 Related Works\n2.1 Text to Image Generation",
            "content": "The emergence of diffusion models [Ho et al. 2020] has been huge leap forward for text-to-image generation, setting new standards in terms of both visual quality and text fidelity [Dhariwal and Nichol 2021]. These models generate samples by gradually denoising random Gaussian noise into coherent images through an iterative process, optionally incorporating encoded text via an attention mechanism [Vaswani 2017] to guide it. Advanced variants such as Latent Diffusion Models (LDMs) [Rombach et al. 2022] and Flow Matching [Lipman et al. 2022] approaches are now considered stateof-the-art [Esser et al. 2024; Podell et al. 2023], with notable high performance models such as SDXL [Podell et al. 2023], Flux-1 [Labs 2023] and Emu [Dai et al. 2023]. However, despite improvements in image quality, current models still struggle with following complex, detailed prompts, limitation largely attributed to the difficulty of acquiring large-scale, high-quality ground truth data for such specific prompts. Our work specifically addresses this challenge."
        },
        {
            "title": "2.2 Controllability in Text-to-Image Generation",
            "content": "Controllability in text-to-image generation is crucial, as users fundamentally seek to accurately materialize their creative visions. To enhance control, numerous approaches have introduced additional forms of guidance beyond text. Methods like ControlNet [Zhang et al. 2023], GLIGEN [Li et al. 2023] and InstanceDiffusion [Wang et al. 2024] enable various guidance types including bounding boxes, segmentation masks, and pose information, while DDPO [Black et al. 2023] and DPO [Wallace et al. 2024] improve text fidelity by using reinforcement learning with MLLM or human feedback. However, these approaches require specialized training and are not easily transferable across models. MultiDiffusion [Bar-Tal et al. 2023] offers an alternative by accepting segmentation masks and aspect ratio information without requiring training, instead incorporating an optimization task into the diffusion sampling procedure to coordinate multiple generation processes. Similarly, other training-free approaches [Chen et al. 2024; Dahary et al. 2025; Xie et al. 2023] ground specific objects to image regions by manipulating attention values during the reverse diffusion process using bounding boxes. These methods all require explicit user input beyond text, reducing their intuitiveness. Some works [Chen et al. 2023; Phung et al. 2024] address this by using LLMs to generate bounding box layouts from prompts, which then guide attention during inference. However, these LLM-generated InstanceGen: Image Generation with Instance-level Instructions 3 Fig. 2. Method Overview. Given text prompt (possibly) containing object counts, instance-level attributes, and spatial relations, our approach combines image-based and text-based components (visualized in pink and blue boxes above) for generating an output image (illustrated on the right). We first generate an initial image using pretrained text-to-image diffusion model. Given the image and attention information from the initial diffusion process, we extract an instance segmentation layout, and assign object and instance-level instructions using an LLM. We then use the layout and instructions, as well as the initial images latents, to incur losses, mask the attention, and optimize the latent in the assignment conditioned image generation stage which produces the output image. Note that for simplicity attention masking is not displayed in the figure. layouts often prove unrealistic, and the ambiguity inherent in bounding box representations compromises accuracy. Attend and Excite [Chefer et al. 2023] improves controllability by preventing attribute leakage between attention values of specific text tokens during inference, requiring no additional input but struggling with accurate object counting. CountGen [Binyamin et al. 2024] specifically targets counting capabilities by extracting and modifying layouts from initial images to match prompt requirements, but is limited in handling multiple object types or attributes and relies heavily on model-specific layout extraction. In contrast, our method leverages the base text-to-image model to extract realistic guidance, requiring no additional user input or training while handling complex prompts involving multiple objects, instance-level attributes, and spatial relationships. Similar to our approach, SLD [Wu et al. 2024a] derives corrected layouts by using an LLM and segmentation model, and blend missing instances which are generated separately into the initial image. This approach improves object counting fidelity, but has limited ability to generate fine grained instance attributes due to its reliance on image segmentation models and separation between background and foreground generation."
        },
        {
            "title": "3 Method",
            "content": "In this work, we consider the problem of generating images with multiple objects, instance level attributes and spatial arrangements, with the goal of producing visually pleasing and highly realistic images that also have high text fidelity. To this aim, we generate an initial image via text-to-image diffusion model. The produced image typically has high visual quality and plausible composition, but may significantly diverge from the provided prompt. We leverage the attention maps extracted during the image generation process to segment the diffused image and produce an initial layout image (Sec. 3.1.2). We then provide this layout in textual form as input to an LLM, which assigns instance-level instructions or attributes to each segment in the layout (Sec. 3.2). Finally, the layout coupled with the instance-level assignments are used as guidance in our conditional image generation process (Sec. 3.3). Figure 2 provides an overview of our approach. 3."
        },
        {
            "title": "Instance Layout Generation",
            "content": "3.1.1 Prompt Parsing. As precursor to our process, we break down the prompt into its core components, which include object-level quantities, instance-level attributes, and instance-level quantities. For example, the prompt three people and two dogs posing for photo, one person is waving contains two objects, people and dogs, with desired quantities of three and two respectively. The object people is associated with an instance-level attribute is waving, which has desired quantity of one, whereas the object dogs is not associated with any instance-level attributes. To extract this information, we instruct an LLM (Llama 3.3 [Dubey et al. 2024]) to analyze the prompt and return this information in the form of 4 Etai Sella, Yanir Kleiman, and Hadar Averbuch-Elor json format dictionary. An example prompt with its parsing output is provided in the supplementary material. Instance Segmentation. To guide our generation process, we 3.1.2 begin by generating an initial image using pretrained text-toimage diffusion model. The image generation model also produces cross-attention maps between the prompt and the image, which we use to guide fine-grained segmentation of the initial image. We combine the signals from the cross-attention maps with off-the-shelf segmentation models applied to the image itself. This is in contrast with prior works that segment objects using attention information exclusively [Binyamin et al. 2024; Namekata et al. 2024; Patashnik et al. 2023], an approach that makes it challenging to segment multiple instances that correspond to the same object token, and is prone to over-segmenting instances to semantic parts. To guide the image segmentation models, we extract anchor points in the form of local maxima of the aggregated attention maps, which indicate which segments correspond to specific object instances. We observe empirically that every object instance produces at least one such anchor point. Thus, we discard output segments that do not include anchor points, and we segment the image further as long as there are unassigned anchor points, using the anchor points as coordinate inputs. For the initial segmentation of instances, we use Mask R-CNN [He et al. 2017], which tends to segment complete instances but misses some instances due to its limited vocabulary. Hence, we merge Mask-RCNNs instance segmentation with SAM2 [Kirillov et al. 2023] keypoint-based segmentations. While SAM2 is prone to over-segmentation of visually distinct parts, by combining it with the output of Mask-RCNN we can avoid partial segmentation while also ensuring all instances are accounted for, regardless of their type. We provide additional details regarding this stage in the supplementary material. Attention Scores. To allow our LLM agent to make logical instance assignments downstream, we pair each segment in our layout with an attention score for each object word and instance attribute. The attention scores for given segment are calculated by averaging the cross attention map pixel values of object or instance attribute words over all pixels that belong to the segment. The segments sizes, positions, and attention scores are written in json format and used as input for the instance assignment phase. Robust Initialization via Seed Search. Once we have an initial instance segmentation, we ensure that we have enough instances to accurately convey the desired prompt. For example if the initial layout contains four segments and our prompt requires six objects then the layout can not be used to accurately convey it. We first attempt to generate an initial image that contains enough instance segments using different seed, and repeat this process up to five times. If viable layout is not obtained after five attempts, we choose existing instance masks at random and copy them to random location in the background. 3."
        },
        {
            "title": "Instance Assignment",
            "content": "The instance assignment LLM takes as input the prompt, the parsed json dictionary (Sec. 3.1.1), and the instance segmentation json (Sec. 3.1.2), and tasked with assigning each segment with instancelevel instructions. This can either be the delete instruction, implying (a) Initial Image (b) Cross Att. + Anchor Points (c) Initial Segmentation (d) Final Segmentation Fig. 3. Instance Layout Generation. (a) The initial image generated by pretrained text-to-image model. (b) The aggregated cross attention for all foreground words, with anchor points marked by green dots. (c) Initial segmentation after discarding segments without anchor points. (d) Final segmentation after adding segments for each unassigned anchor point. the segment should not be part of the foreground of the final image, or one of the desired object words, for example dogs or people. Additionally, the LLM optionally assigns one or more instance level attributes to each segment, for example is waving. In alignment with our overall goal, we instruct the LLM with primary and secondary objective. The primary objective is to output layout which correctly depict the input prompt. The secondary objective is to assign the layout in way that results in minimal visual difference between the output and initial image, since the initial image is likely to have plausible composition and high visual quality, even if it is inaccurate in terms of prompt fidelity. In practice, we instruct the LLM to first make assignments on the object level, ensuring spatial relationships and object counts are accurately addressed. Once these assignments are met, the LLM is instructed to follow similar procedure for instance level attributes. When not bounded by specific constraints, such as specific spatial arrangement, the LLM is instructed to assign each object or instancelevel attribute to the segment which maximizes its attention score until the desired amount is met, such that these assignments agree most with the initial image. The instruction prompt given to the LLM as input also contains number of curated in-context examples depicting typical scenarios. If we detect an error in the assignment output, for example if the assignment includes wrong number of objects, or an attribute was assigned to segment marked for deletion, we call the LLM again with the erroneous assignment appended to the instruction prompt as negative example. We find that this process is less errorprone when the LLM is instructed to output its reasoning for the assignment as well as the json format output. However we do not use the reasoning part of the output in our pipeline. An overview of the inputs given to the LLM in this stage is shown in Figure 4, and detailed example including the LLM output is provided in the supplementary material. InstanceGen: Image Generation with Instance-level Instructions 3.3.2 Attention Masking. As an additional measure for preventing semantic leakage between objects and attributes we mask the attention of object words and attributes in segments that they are not assigned to: ğ¶ğ‘¤,ğ‘– = (cid:40)ğ›¿ğ¶ğ‘¤,ğ‘– ğ‘šğ‘– = 1 ğ¶ğ‘¤,ğ‘– ğ‘šğ‘– = 0 (3) Fig. 4. Instance assignment inputs. The input given to the LLM for instance assignment contains in-context examples, the parsed prompt, and the visual layout of each segment."
        },
        {
            "title": "3.3 Assignment Conditioned Image Generation",
            "content": "Once assignments have been made, we omit the segments marked for deletion and are left with set of binary masks, each associated with one of the object words and optionally one or more instance attribute word. The goal of this stage is to generate an image in which each assigned region accurately depicts its associated instructions, while maintaining high visual quality and plausibility. Inspired by prior work [Binyamin et al. 2024; Chefer et al. 2023; Dahary et al. 2025], we approach this task by modifying the reverse diffusion process of the image generator in manner that encourages appropriate attention values to correlate with their associated segments. Specifically, we modify the image generation process as follows. 3.3.1 Cross Attention Losses. During inference, we optimize the intermediate image latents using the following attention based losses: Object Attention Loss. Inspired by [Binyamin et al. 2024] we incorporate weighted binary cross entropy loss between cross attention values for an object word ğ¶ğ‘¤ and segment ğ‘š associated with the word: where ğ¶ğ‘¤,ğ‘– is the cross attention value of object or attribute word ğ‘¤ at index ğ‘– that is not associated with mask ğ‘š. ğ›¿ is hyper-parameter set to ğ›¿ = 1.5 in our implementation. Additionally to further confine objects to the foreground and prevent objects from appearing in the background we use the self attention masking procedure used in [Binyamin et al. 2024]: ğ‘– ğ‘ğ‘›ğ‘‘ ğ‘— ğ‘– ğ‘ğ‘›ğ‘‘ ğ‘— ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’ 0 0 ğ‘†ğ‘–,ğ‘— where ğ‘† is self attention map, ğ‘– and ğ‘— pixel indices and and is the foreground and background masks, defined as = (cid:208)ğ‘€ ğ‘šğ‘– ğ‘–=0 and = . ğ‘†ğ‘–,ğ‘— = (4) 3.3.3 Composition Preserving Regularization. Leveraging our initial image, we regularize our attention losses with loss encouraging similarity between background latent pixels in the initial and diffused image: Lğ‘ğ‘” = 1 ğ‘– ğ‘– ğ‘§ (ğ‘¡ ) ( ğ‘§ (ğ‘¡ ) ğ‘– )2 (5) where is the background mask, ğ‘§ (ğ‘¡ ) the initial image at timestep ğ‘¡ and ğ‘§ (ğ‘¡ ) is the latents of the optimized ğ‘– image at timesetep ğ‘¡. With this regularization objective, our overall loss becomes: is the latent representation of ğ‘– = Lğ‘œğ‘ ğ‘— + ğœ†ğ‘ğ‘¡ğ‘¡ Lğ‘ğ‘¡ğ‘¡ + ğœ†ğ‘ğ‘”Lğ‘ğ‘” (6) with ğœ†ğ‘ğ‘¡ğ‘¡ set to 0.8 and ğœ†ğ‘ğ‘” set to 0.3 in our experiments. As final note, we point out that the optimizing and masking does not occur at every timestep and layer of inference, the timesteps and layer used for each components are detailed in the supplementary material. Lğ‘œğ‘ ğ‘— = ğ‘– ğœ†ğ‘– (ğ‘šğ‘– ğ‘™ğ‘œğ‘”(ğ¶ğ‘¤,ğ‘– ) + (1 ğ‘šğ‘– ) ğ‘™ğ‘œğ‘”(1 ğ¶ğ‘¤,ğ‘– )) (1)"
        },
        {
            "title": "4 Experiments",
            "content": "where is ğ¶ğ‘¤,ğ‘– the cross-attention map value for word ğ‘¤ at pixel ğ‘–, ğ‘šğ‘– is the value of the binary mask ğ‘š at pixel ğ‘–, and ğœ†ğ‘– is the weight assigned to each pixel ğ‘– where ğœ†ğ‘– = 1.5 if ğ‘šğ‘– = 1, otherwise ğœ†ğ‘– = 1. We set the weight ğœ†ğ‘– in this manner to better encourage object generation in the foreground. Attribute Attention Loss. While it is possible to treat attribute words in the same manner we treat object words and use the aforementioned Object Attention loss to restrict them to their assigned segments, we find that this does not affect object creation in the background and often has an adverse affect on the overall image quality. As such we instead use simple cross entropy loss to encourage attribute appearance in assigned segments: Lğ‘ğ‘¡ğ‘¡ = ğ‘šğ‘–ğ‘™ğ‘œğ‘”(ğ¶ğ‘¤,ğ‘– ) (2) ğ‘– where ğ¶ğ‘¤,ğ‘– is the cross attention for attribute word ğ‘¤ at pixel ğ‘–. In this section, we conduct both qualitative and quantitative experiments to assess the effectiveness of our method. In the supplementary material, we provide additional implementation details, discuss limitations, and show results and comparisons over the entire CompoundPrompts benchmark, presented next in Section 4.1."
        },
        {
            "title": "4.1 Benchmarks",
            "content": "While several benchmarks exist for evaluating text-to-image models, each addresses different aspects of model performance with certain limitations. DrawBench [Saharia et al. 2022] and GenEval [Ghosh et al. 2024] offer comprehensive evaluation of several image generation skills, but only subset of the prompts specifically target object counting, spatial relationships and attribute fidelity. We present an evaluation of our method on DrawBench in Section 4.3, and on the GenEval dataset in the supplementary material. Other datasets that address these aspects either employ oversimplified prompts 6 Etai Sella, Yanir Kleiman, and Hadar Averbuch-Elor for semantic segmentation evaluation (HRS [Bakr et al. 2023]) or lack instance-level attribute specification due to their procedurally generated nature (ConceptMix [Wu et al. 2024b]). CompoundPrompts. To address these limitations, we introduce new benchmark, CompoundPrompts, specifically designed to evaluate models capabilities in numerical accuracy, spatial fidelity, and semantic attribute consistency. CompoundPrompts provides prompts that compound precise object counting with complex spatial arrangements and detailed attribute specifications. The CompoundPrompts dataset was constructed with three difficulty tiers, with corresponding prompts for each tier. The first tier (Tier A) describes multiple instances of one or more object categories (e.g. an image of two dogs and three cats). The second tier (Tier B) enhances these base prompts by incorporating instance-level attributes for specific objects (e.g., an image of two dogs and three cats, one of the cats is sphinx while the other cats are tabbies). The third tier (Tier C) extends the prompts further by adding spatial relationships at both object and attribute levels (e.g., an image of two dogs and three cats, the cat on the far right is sphinx while the other cats are tabbies). In total, this dataset contains 540 prompts, split across three difficulty tiers. Following [Wu et al. 2024b], we pair each unique prompt with set of yes or no questions. Tier prompts are accompanied by question centered on counting (i.e. are there exactly two dogs and three cats in this image?). Tier prompts are paired with counting question and question centered on counting the instances with unique attributes (i.e. are there exactly two tabbies and single sphinx cat?). Tier prompts are accompanied by counting question, an attribute counting question and question focused on spatial fidelity (is the cat on the far right sphinx?)."
        },
        {
            "title": "4.2 Metrics",
            "content": "When evaluating on DrawBench we use the procedure used in [Phung et al. 2024]. When evaluating on the CompoundPrompts benchmark we use the evaluation procedure used in ConceptMix [Wu et al. 2024b], i.e. using an MLLM (GPT-4o) to answer the yes or no questions accompanying each prompt. For an image to get positive score, the MLLM must give positive answer to all of its accompanying questions. We denote this metric as VQA Accuracy or VQA Acc. Additionally, we use the recently introduced VQAScore [Lin et al. 2025] metric to evaluate general image-text similarity, as it has been shown to have significantly stronger agreement with human judgments. To avoid confusion with VQA Acc, we denote VQAScore as VQA Similarity or VQA Sim."
        },
        {
            "title": "4.3 Comparisons with baselines",
            "content": "We compare our method with baseline generic image generation methods (Emu [Dai et al. 2023], SDXL [Podell et al. 2023], Flux [Labs 2023]), as well as untrained methods that specifically aim to produce images of multiple objects with high fidelity (Bounded Attention [Dahary et al. 2025], Reason Your Layout [Chen et al. 2023], CountGen [Binyamin et al. 2024], SLD [Wu et al. 2024a]). We compare with Attention Refocusing [Phung et al. 2024], which uses supervised grounded text-to-image method (GLIGEN [Li et al. 2023]) that accepts bounding box layouts as an additional input. Finally, Method Counting Spatial Precision Recall F1 Accuracy 0.74 SDXL 0.71 Flux1-dev 0.89 CountGen 0.78 ReasonYourLayout 0.83 BoundedAttention 0.90 AttentionRefocusing 0.76 DPO 0.84 SLD 0.77 Emu Ours 0.87 Table 1. Quantitative evaluation on the DrawBench dataset. 0.78 0.76 0.90 0.79 0.85 0.87 0.84 0.81 0.88 0.85 0.86 0.88 0.92 0.83 0.96 0.90 0.96 0.85 0.96 0.91 0.19 0.50 0.17 0.39 0.36 0.64 0.17 0.39 0.50 0.67 Method VQA Acc VQA Sim B Avg. 0.42 0.18 0.11 0.23 SDXL 0.58 0.37 0.38 0.44 Flux1-dev CountGen 0.45 0.18 0.10 0.24 Reason-your-Layout 0.32 0.10 0.07 0.16 BoundedAttention 0.39 0.14 0.10 0.21 AttentionRefocusing 0.63 0.13 0.07 0.27 0.33 0.16 0.07 0.19 DPO 0.44 0.14 0.07 0.21 SLD 0.59 0.40 0.30 0.43 Emu 0.72 0.57 0.50 0.60 Ours 0.78 0.85 0.75 0.65 0.77 0.68 0.80 0.76 0.86 0.89 Table 2. Quantitative evaluation on the CompoundPrompts dataset. we compare with DPO [Wallace et al. 2024] which relies on human feedback during training to improve prompt fidelity. Below we detail quantitative and qualitative results of the comparison. Quantitative results. Table 1 presents quantitative comparison on the DrawBench benchmark. As illustrated in the table, our method outperforms all baselines in both counting and spatial accuracy, with significant improvement over our baseline method (Emu). In Table 2 we present the results of quantitative comparison conducted on the CompoundPrompts benchmark. This table presents VQA Accuracy results across the three prompt types in CompoundPrompts. While performance is somewhat comparable for type prompts, our method significantly outperforms competing methods on type (multiple objects with instance-level attributes) and type prompts (multiple objects with instance-level attributes and spatial arrangements). This gap suggests competing methods struggle with instance-level attributes and spatial arrangements. Furthermore, our method also outperforms all baselines in VQA Similarity, metric assessing overall composition fidelity. Qualitative results. Qualitative comparisons with baselines methods is presented in Fig. 5 and Fig. 6. Fig. 5 presents images generated for two sets of prompts from the CompoundPrompts dataset, each with the three difficulty tiers A, B, and C. Note that while most Method VQA Acc VQA Sim C Avg. w/o Lğ‘ğ‘” 0.61 0.60 0.68 w/o Lğ‘œğ‘ ğ‘— & Lğ‘ğ‘¡ğ‘¡ 0.60 0.54 0.72 0.54 0.56 w/o Attn. Masking 0.72 0.60 0.70 w/o M-RCNN 0.60 0.84 0.56 w/o Seed Search 0.65 0.76 0.72 0.60 0.69 Ours 0.54 0.54 0.32 0.50 0.56 0.84 0.89 0.87 0.88 0.90 0.90 Table 3. Quantitative ablation experiment on representative subset of the CompoundPrompts dataset. baselines can generate simple prompt in Tier such as man and woman sitting down with their two dogs, they cannot satisfy the instance-level attributes and instance-level counts which are introduced in tier correctly. In addition, some baselines struggle with long prompts and generate very similar images for the different prompts in tiers and C. As demonstrated by the second prompt (rows 4 to 6), baseline image generation methods have strong biases for some object counts, for example bias towards generating an even number of donuts. This bias is not solved by previous methods such as Bounded Attention and Reason Your Layout, whereas our method ensures exactly five donuts are present in the final image. This bias also propagates to more complex prompts in tier and C. Fig. 6 presents additional two sets of prompts from CompoundPrompts benchmark with their three difficulty tiers, and comparison with additional methods (Flux, Attention Refocusing, and CountGen). The figure also displays the questions that are used for evaluating VQA Accuracy metric. While several methods can generate object counts correctly (particularly for lower counts), they rarely satisfy all of the conditions of the prompts in tiers and C. We show additional comparison with three prompts in Fig. 1. The results for all prompts in CompoundPrompts can be found in the supplemental material."
        },
        {
            "title": "4.4 Ablations\nWe provide an ablation study on the CompoundPrompts benchmark\nin Table 3; qualitative results are provided in the supplementary ma-\nterial. Specifically, we ablate key components in both the instance\nlayout generation and assignment conditioned image generation\nphases of our framework. In the instance layout generation phase,\nwe ablate the dual segmentation approach by using SAM (â€œw/o M-\nRCNNâ€) exclusively and ablate the seed search by using the initial\nseed and relying on our instance copying procedure exclusively\nto handle undergeneration (â€œw/o Seed Searchâ€). In the assignment\nconditioned image generation phase, we ablate three key compo-\nnents: composition preserving regularization (â€œw/o Lğ‘ğ‘”â€), cross\nattention losses (â€œw/o Lğ‘œğ‘ ğ‘— & Lğ‘ğ‘¡ğ‘¡ â€) and attention masking (â€œw/o\nAttn. Maskingâ€).",
            "content": "The results of this study are presented in Table 3 and show that removing each of the ablated components causes some loss in performance most noticeably in terms of VQA Accuracy. This is especially evident on the more challenging Type and Type prompts, with attention masking having particularly strong effect on Type InstanceGen: Image Generation with Instance-level Instructions 7 A: \"a man and woman sitting down with their two dogs\" B: \"a man and woman sitting down with their two dogs, one of the dogs is golden retriever\" C: \"a man and woman sitting down with their two dogs, the dog between the man and the woman is golden retriever\" A: \"five donuts in box\" B: \"five donuts in box, two are chocolate glazed, the rest are plain\" C: \"five donuts in box, the donut on the top left and bottom right are chocolate glazed, the rest are plain\" Ours Emu BA RYL SDXL Fig. 5. Qualitative results from the CompoundPrompts benchmark. We present results for two unique prompts from the CompoundPrompts benchmark, presenting results for all three tiers (A, and C) for each prompt. performance. In our assessment, this shows that the semantic leakage which this component addresses plays major part in spatial errors on our benchmark. Another interesting observation is that while removing Lğ‘ğ‘” does not diminish VQA Acc values as much as removing other components does, its affect on VQA Sim is highly significant. This emphasizes the importance of this regularization in maintaining visual quality and plausibility. Finally, we observe that ablating the seed search component does not cause major drop off in VQA Accuracy and does not decrease VQA Similarity, which demonstrates that this component is not the primary factor driving our increased performance compared to competing baselines. Etai Sella, Yanir Kleiman, and Hadar Averbuch-Elor"
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we presented InstanceGen, new method that enhances the ability of text-to-image models to handle complex prompts compounding multiple objects, attributes, and spatial relationships without requiring additional training. Through comprehensive experiments, we demonstrated InstanceGens superior performance in counting accuracy and spatial arrangement compared to existing zero-shot approaches. Additionally, we introduced CompoundPrompts, new benchmark specifically designed to evaluate models capabilities in handling intricate, multi-component prompts. Our benchmark not only validates our methods effectiveness but also provides valuable resource for future research. As text-to-image synthesis continues to evolve, our work represents step toward enabling these models to faithfully render increasingly complex and creative concepts, bringing us closer to the goal of translating any imaginable scenario into visual reality."
        },
        {
            "title": "References",
            "content": "Eslam Mohamed Bakr, Pengzhan Sun, Xiaoqian Shen, Faizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny. 2023. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2004120053. Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023. Multidiffusion: Fusing diffusion paths for controlled image generation. (2023). Lital Binyamin, Yoad Tewel, Hilit Segev, Eran Hirsch, Royi Rassin, and Gal Chechik. 2024. Make It Count: Text-to-Image Generation with an Accurate Number of Objects. arXiv preprint arXiv:2406.10210 (2024). Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. 2023. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301 (2023). Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attendand-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG) 42, 4 (2023), 110. Minghao Chen, Iro Laina, and Andrea Vedaldi. 2024. Training-free layout control with cross-attention guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 53435353. Xiaohui Chen, Yongfei Liu, Yingxiang Yang, Jianbo Yuan, Quanzeng You, Li-Ping Liu, and Hongxia Yang. 2023. Reason out your layout: Evoking the layout master from large language models for text-to-image synthesis. arXiv preprint arXiv:2311.17126 (2023). Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. 2022. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 12901299. Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. 2025. Be yourself: Bounded attention for multi-subject text-to-image generation. In European Conference on Computer Vision. Springer, 432448. Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. 2023. Emu: Enhancing Image Generation Models Using Photogenic Needles in Haystack. arXiv:2309.15807 [cs.CV] https://arxiv.org/abs/2309.15807 Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in neural information processing systems 34 (2021), 87808794. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. 2024. Geneval: An objectfocused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems 36 (2024). Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick. 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision. 29612969. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. 2023. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 40154026. Black Forest Labs. 2023. FLUX. https://github.com/black-forest-labs/flux. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. 2023. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2251122521. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. 2025. Evaluating text-to-visual generation with imageto-text generation. In European Conference on Computer Vision. Springer, 366384. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. 2022. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747 (2022). Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, and Seung Wook Kim. 2024. Emerdiff: Emerging pixel-level semantic knowledge in diffusion models. arXiv preprint arXiv:2401.11739 (2024). OpenAI. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] https://arxiv.org/ abs/2303.08774 Nobuyuki Otsu et al. 1975. threshold selection method from gray-level histograms. Automatica 11, 285-296 (1975), 2327. Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. 2023. Localizing object-level shape variations with text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 23051 23061. William Peebles and Saining Xie. 2023. Scalable Diffusion Models with Transformers. arXiv:2212.09748 [cs.CV] https://arxiv.org/abs/2212. Quynh Phung, Songwei Ge, and Jia-Bin Huang. 2024. Grounded text-to-image synthesis with attention refocusing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 79327942. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and Robin Rombach. 2023. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. arXiv:2307.01952 [cs.CV] https: //arxiv.org/abs/2307.01952 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems 35 (2022), 3647936494. Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017). Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. 2024. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 82288238. Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. 2024. Instancediffusion: Instance-level control for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 62326242. Tsung-Han Wu, Long Lian, Joseph Gonzalez, Boyi Li, and Trevor Darrell. 2024a. Selfcorrecting llm-controlled diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 63276336. Xindi Wu, Dingli Yu, Yangsibo Huang, Olga Russakovsky, and Sanjeev Arora. 2024b. Conceptmix: compositional image generation benchmark with controllable difficulty. arXiv preprint arXiv:2408.14339 (2024). Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. 2023. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 74527461. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 38363847. Tier A: Prompt: Six meerkats standing watch in the Savannah Question (A): Do exactly six meerkats appear in this image? XXX XXX XXX XXX XXX XXX XXX InstanceGen: Image Generation with Instance-level Instructions 9 Tier B: Prompt: Six meerkats standing watch in the Savannah, only one of them is yawning Question (B): Is exactly one meerkat yawning in this image? XXX XXX XXX XXX XXX XXX XXX Tier C: Prompt: Six meerkats standing watch in the Savannah, only the second meerkat from the right is yawning Question (C): Is only the second meerkat from the right yawning in this image? XXX XXX XXX XXX XXX XXX XXX Tier A: Prompt: Three dogs and cat in backyard Question (A): Do exactly three dogs and one cat appear in this image? XXX XXX XXX XXX Tier B: Prompt: Three dogs and cat in backyard, one dog is Schnauzer and the cat is Persian Question (B): Is exactly one dog Schnauzer and one cat Persian in this image? XXX XXX XXX XXX XXX XXX XXX Tier C: Prompt: Three dogs and cat in backyard, one dog is Schnauzer and is on the far right, the cat is in the far left and is Persian Question (C): Is only the dog on the far right Schnauzer, and only the cat on the far left Persian in this image? XXX XXX XXX XXX XXX XXX Ours Emu Flux1-dev SDXL BA GLIGEN + AR CountGen RYL Fig. 6. Extended qualitative comparison. Additional results for prompts sampled from CompoundPrompts, along with VQA Acc. metric evaluation. For each tier, we consider an image correct () if it receives positive answers from an MLLM (GPT4o) when prompted with all questions below its tier. We compare with Emu [Dai et al. 2023], Flux1-dev [Labs 2023], SDXL [Podell et al. 2023], Bounded Attention (BA) [Dahary et al. 2025], Attention Refocusing (GLIGEN + AR) [Phung et al. 2024], and Reason Your Layout (RYL) [Chen et al. 2023]. 10 Etai Sella, Yanir Kleiman, and Hadar Averbuch-Elor \"four action figures on kids shelf, from left to right they are batman, superman, wolverine and spiderman\" \"three people and dog on the beach, the dog is dalmatian,\" and only the person next to it is wearing red hat\" Initial Image Instance Assignments Ours Initial Image Instance Assignments Ours \"three people with two motorcycles on rainy night, the one on the right is blue chopper, the one on the left is yellow sportsbike\" \"five birds on tree branch in autumn, one bird is cardinal, to the left of it is blue jay, the rest are ravens\" Initial Image Instance Assignments Ours Initial Image Instance Assignments Ours \"four pillows stacked on top of eachother, from bottom to top they are pink, tiger pattern, covered in blue velvet and pinstriped\" \"two guitars and saxophone in music store, the guitar on the right is white fender, the one on the left is rickenbacker bass\" Initial Image Instance Assignments Ours Initial Image Instance Assignments Ours \"four pints of beer lined up at bar, from right to left they are dark stout, bright lager, cloudy wheatbeer and an amber ale\" \"seven crayons in box, from left to right their colors are red, orange, yellow, green, blue, purple and black\" Initial Image Instance Assignments Ours Initial Image Instance Assignments Ours \"five ballerinas performing at studio, the second ballerina from the left is wearing black and smiling, the rest are wearing white\" \"a porcupine, raccoon and squirrel in the forest, the squirrel is in the middle holding nut\" Initial Image Instance Assignments Ours Initial Image Instance Assignments Ours Fig. 7. Assorted Tier prompt results: Initial Images shown here are post our robust initialization stage."
        },
        {
            "title": "InstanceGen Supplementary Material",
            "content": "} InstanceGen: Image Generation with Instance-level Instructions"
        },
        {
            "title": "A Additional Results and Information",
            "content": "We refer readers to the supplementary results folder available alongside this document for additional results. This folder contains output images for all 150 prompts available in the CompoundPrompts dataset for our method as well as competing baselines. Also available alongside this document are the instruction prompts given to the LLM in the prompt parsing stage (prompt_parser_instruction.txt) and the instance assignment stage (instance_assignment_instruction.txt). Technical Details B."
        },
        {
            "title": "InstanceGen Implementation Details",
            "content": "B.1.1 Base Text to Image model. The baseline text-to-image model used for both generating an initial image and generating the instance assignment guided output image is the latest version of Emu [Dai et al. 2023]. This particular model is DiT [Peebles and Xie 2023] based latent diffusion model, made up of 22 DiT layers and generating images in 1024 1024 resolution with attention dimensions 64 64. In all our experiments, we use this model with ğ‘‡ = 26 inference steps and classifier-free-guidance scale of 4.0. B.1.2 Prompt Parsing. As described in Section 3.1.1 of the main paper, in this step we task an LLM (Llama 3.3) with parsing prompt into its key components in terms of subject instances. Specifically, the LLM is instructed to return json format dictionary that lists all objects described in the prompt (and not describing the environment or setting), the amount of times they are required to appear and any instance attributes they might require with desired quantities for those as well. Below we present an example parser output for the prompt porcupine, one squirrel and raccoon in forest, the squirrel is holding nut. { \"prompt\": \"a porcupine, one squirrel and raccoon in forest, the squirrel is holding nut\", \"objects\": { \"porcupine\": { \"desired_quantity\": \"1\", \"instance_adjectives\": {} }, \"squirrel\": { \"desired_quantity\": \"1\", \"instance_adjectives\": { \"1\": { \"adjective\": \"holding nut\", \"desired amount\": \"1\" } } }, \"raccoon\": { \"desired_quantity\": \"1\", \"instance_adjectives\": {} } } Notice that the words nut or forest while certainly describing objects in the image are not treated as such by the parser as they either belong to an instance attribute (i.e. nut belongs to the holding nut attribute) or description of the environment (i.e. forest describing the setting and not an actual object instance). Instance Layout Generation. In this section we provide details B.1.3 for the anchor point extraction and layout segmentation. Anchor Point Extraction. To extract salient anchor points from the cross-attention maps, we aggregate the maps from various layers, timesteps, and text tokens in order to obtain single cross-attention map for each object word. Specifically, we average the attention maps over timesteps [0-25] and layers [2-20] for each text token associated with object word ğ‘¤. Then, we take the maximum over all tokens associated with the word ğ‘¤ for each pixel, resulting in single aggregated cross attention map ğ¶ğ‘¤. We extract anchor points from the cross attention map for each object word by obtaining foreground mask ğ‘€ using the Otsu threshold, ğ‘€ğ‘– = ğ¶ğ‘¤ > ğ‘‚ğ‘¡ğ‘ ğ‘¢ (ğ¶ğ‘¤) [Otsu et al. 1975]. Then we compute local maxima of ğ¶ğ‘¤ by running skimage.feature.peak_local_max and filter out the points that fall outside of the foreground mask ğ‘€ğ‘– , to obtain anchor points {ğ‘ğ‘– }ğ‘¤ associated with word ğ‘¤. Layout Segmentation. Given the initial image and the extracted anchor points, we produce the instance segmentation using combination of an object instance segmentation model, Mask R-CNN [He et al. 2017]), and the more general segmentation model, SAM [Kirillov et al. 2023]. First, we run Mask R-CNN on the initial image to obtain an initial set of instance segmentation masks ğ‘€. Then, for each anchor point ğ‘ğ‘–,ğ‘¤ we find the the smallest mask ğ‘š ğ‘€ such that ğ‘ğ‘–,ğ‘¤ resides in ğ‘š. If no such mask exists, we mark ğ‘ğ‘–,ğ‘¤ as unresolved. We discard masks that are not associated with any anchor points at this stage. Then, we extract additional segmentation masks for each unresolved point using SAM, by providing the unresolved point as an input positive coordinate. If there are significant overlaps, we merge the masks from the two phases. We end this process once each anchor point is associated with segmentation mask. Note that multiple anchor points can be associated with single mask. Segmentation Implementation Details. For the cross-attention aggregation, we use ğ¿ğ‘ğ‘”ğ‘” = [2, 20] and ğ‘‡ğ‘ğ‘”ğ‘” = [0, 25] in all of our experiments. For M-RCNN, we use Detectron with configuration COCO-InstanceSegmentation/mask_rcnn R_50_FPN_1x.yaml. This segmentation method sometimes outputs mask that is union of multiple smaller masks and covers vast region of the image. To avoid using these masks in our instance layout we filter out masks which cover more than 0.33% of the total image area. After filtering out masks that are not assigned to any anchor point, we are left with set of masks that are not guaranteed to be non-overlapping. To mitigate this, we check for overlapping between every pair of masks and assign the overlapping region to the smaller mask between the two. Next, we go over each unassigned anchor point and extract segmentation mask with SAM, using the anchor point coordinate as positive point. Specifically, we use SAM2 available here. These Etai Sella, Yanir Kleiman, and Hadar Averbuch-Elor segments are more prone to overlap as that is the desirable scenario for multiple anchor points belonging to the same instance. As such, we again go over each mask pair and look for overlaps. However, this time we check if the overlap between two masks cover 66.6% of each masks area, if it does we merge the two masks, if it doesent we assign the overlap region to the smaller mask. After extracting this initial layout, we run simple post processing steps in which we filter out masks that are smaller than 30 pixels, and create minimal margin of 2 pixels between adjacent masks. Instance Copying. As described in the main paper, if viable layout is not obtained after set amount of seed search iterations due to under-generation of instances, we copy existing instances from the mask and place them in the background in random manner to guide the image generation. We first extract background mask by excluding all detected instances and passing it through 2 steps of binary erosion with 33 kernel. Then, we select random existing instance mask and random location withing the background mask, and create new instance mask which has the same shape as the randomly selected instance, centered around the selected location in the background. If there are overlaps between the new instance mask and the originals we remove the overlapping region from the new mask. We then run the postprocessing steps used in the original layout generation stage to ensure 2 pixel margin between the new mask and the originals. We repeat this process until there are enough instance masks to facilitate the prompt. To prevent bias towards copying previously copied instance masks we sample only from the original instance masks when selecting instances for copying. json Output. After post processing, we calculate attention scores for each object word and attribute word and summarize them all into json format file. Below we present an example of this segmentation summary for layout obtained for the above prompt: { \"1\": { \"cluster_size\": \"392.0\", \"distance_from_top\": \"34.3\", \"distance_from_left\": \"55.8\", \"object_probabilities\": { \"porcupine\": \"0.11\", \"squirrel\": \"0.14\", \"raccoon\": \"0.75\" }, \"attribute_probabilities\": { \"holding nut\": \"0.068\" } }, \"2\": { \"cluster_size\": \"225.0\", \"distance_from_top\": \"42.8\", \"distance_from_left\": \"40.7\", \"object_probabilities\": { \"porcupine\": \"0.21\", \"squirrel\": \"0.53\", \"raccoon\": \"0.26\" }, \"attribute_probabilities\": { \"holding nut\": \"0.128\" } }, \"3\": { \"cluster_size\": \"745.0\", \"distance_from_top\": \"34.3\", \"distance_from_left\": \"15.1\", \"object_probabilities\": { \"porcupine\": \"0.71\", \"squirrel\": \"0.14\", \"raccoon\": \"0.15\" }, \"attribute_probabilities\": { \"holding nut\": \"0.066\" } } } Instance Assignment. As described in Section 3.2 of the main paper, in this stage we use an LLM (Llama 3.3) to assign each segment in the extracted layout an instance instruction. The LLM receives an input text for completion consisting of an instruction, in-context examples, the parsed input prompt and the layout summary from the previous stage. We instruct the LLM to work according to the following steps: (1) Figure out how many objects need to be set and how many need to be deleted. (2) Figure out if objects require spatial arrangement. (3) If an object does not require spatial arrangement, set it according to its probability (i.e. set it as the object with the highest object_probabilities value) until all required objects are set. (4) After setting objects move on to setting attributes following the same logic. (5) Go over your decisions and ensure you have set the correct amount of objects and attributes according to the parsed prompt. The LLM is instructed to output its instructions in json format and also provide paragraph detailing the reasoning for its decisions, in accordance with the previously explained steps. An example of these outputs for the previously used prompt is given below: ** REASONING ** 1. There are 3 total clusters in the layout dict,we are required to assign one cluster as 'porcupine', one cluster as 'squirrel' and onecluster as 'raccoon' meaning we do not have to delete any clusters. 2. The 'porcupine' object does not require spatial arrangment. The 'squirrel' object also does not require spatial arrangment. The 'raccoon' object also does not require spatial arrangment. 3. 'porcupine' has the lowest desired quantity (1) so we start by assiging it to the cluster with the highest 'porcupine' probability, which is cluster \"3\" with 'porcupine' probability (0.71). We then assign the 'raccoon' object to the cluster with the highest 'raccoon' probability, which is cluster \"1\" with 'raccoon' probability (0.75). This leaves us with cluster \"2\" which we mark as 'squirrel'. 4.a. The 'squirrel' object requires one instances to be set as 'holding nut'. 4.b. The 'holding nut' adjective does not require spatial arrangement. 4.c. We set cluster \"2\" as 'holding nut' because it has the highest 'holding nut' probability (0.128) out of the clusters we marked as 'squirrel'. 5. We have set one cluster as 'porcupine' (\"3\") as required. We have set one cluster as 'squirrel' (\"2\") as required. We have set one cluster as 'raccoon' (\"1\") as required. We have set one 'squirrel' cluster as 'holding nut' (\"2\") as required. ** ASSIGNMENTS ** { \"1\": { \"object\": \"raccoon\", \"attributes\": [] }, \"2\": { \"object\": \"squirrel\", \"attributes\": [ \"holding nut\" ] }, \"3\": { \"object\": \"porcupine\", \"attributes\": [] } } As detailed in the main paper if this assignment is erroneous according to the parsed prompt (for instance if the attribute holding nut was assigned to segment that has been assigned the racoon object, or if too many segments have been assigned the squirrel object word - as per the previous prompt), we append both the reasoning and assignment to the LLMs instruction text and generate another output. Assignment Conditioned Image Generation. As described in Section 3.3 of the main paper in this stage we generate an image that follows the assignemnts produced in the previous stage. The loss weights used in this stage are 1.0 for Lğ‘œğ‘ ğ‘— , 0.8 for Lğ‘ğ‘¡ğ‘¡ and 0.3 for Lğ‘ğ‘”. These losses were used to optimize the latents from timestep ğ‘¡ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ = 0 to timestep ğ‘¡ğ‘’ğ‘›ğ‘‘ = 20, with 15 optimization iterations added to timesteps ğ‘¡ = 0 and ğ‘¡ = 5, and 5 optimization iterations added to timestep ğ‘¡ = 10. We use fixed learning rate of 0.015. Self attention masking was done from timestep ğ‘¡ = 0 to ğ‘¡ = 3 on layers ğ‘™ = 10 to ğ‘™ = 21. Cross attention masking was done from timestep ğ‘¡ = 0 to timestep ğ‘¡ = 22on layers ğ‘™ = 0 to ğ‘™ = 21. From end to end this stage takes roughly 3 minutes on single NVIDIA H100 80GB GPU. InstanceGen: Image Generation with Instance-level Instructions"
        },
        {
            "title": "CompoundPrompts Breakdown",
            "content": "B.2 CompoundPrompts is made up of 60 unique prompts, each describing multiple objects in various settings. Each unique prompt has three versions, as described in the main paper, which include object counts (Tier A), instance-level attributes (Tier B), and spatial relationships (Tier C). In addition, we augment each of the three prompts with lower count version (2-3 objects), medium count version (4-5 objects), and higher count version (6-7 objects). In total we have 9 versions for each prompt or total of 540 prompts. Prompts with different counts describe the same scene, for example, the prompt an image of two dogs and three cats, the cat on the far right is sphinx corresponds to the lower count prompt an image of dog and two cats, the cat on the far right is sphinx. B.3 Experimentation Details B.3.1 Benchmarks. Evaluation on DrawBench. To evaluate on this dataset, we first obtained the counting and spatial prompts available here. Then we used the evaluation code in the Attention-Refocusing [Phung et al. 2024] github repository. Evaluation on CompoundPrompts. As detailed in the main paper (Section 4.3) we used two different metrics when evaluating on CompoundPrompts: VQA Sim and VQA Acc. To calculate VQA Sim we used the API available on the Evaluating text-to-visual generation with image-to-text generation github repository. To calculate VQA Acc we used GPT4o [OpenAI 2024] to answer the questions associated with each prompt in the CompoundPrompts benchmark and using the evaluation protocol detailed in the main paper. B.3.2 Baselines. SDXL. We used the stabilityai/stable-diffusion-xl-base-1.0 configuration for the SDXL pipeline available on huggingface. Flux 1 dev. We used the default black-forest-labs/FLUX.1-dev configuration for the Flux 1 dev pipeline also available on huggingface. GLIGEN + Attention Refocusing. We used the default inference procedure available on the attention-refocusing github repository, using GPT4o to generate layouts for the CompoundPrompts prompts using the instruction also available in the repository. The GLIGEN [Li et al. 2023] models were obtained via this link. Bounded-Attention. We used the default SDXL based configuration available on the Bounded-Attention repository. When evaluating this method we used the layouts generated for the GLIGEN + AR baseline. CountGen. We used the default inference configuration available on the CountGen github repository. Reason-your-Layout. We used the default inference configuration available on the Reason-your-Layout github repository. Layouts were generated using GPT4o using the instruction also available on the repository. Direct Preference Optimization (DPO). We used the SDXL DPO weights available on huggingface. 14 Etai Sella, Yanir Kleiman, and Hadar Averbuch-Elor tray with three muffins, two vanilla Ã©clairs, and piece of cake on it, the top left muffin is blueberry, the other two are chocolate. five birds on tree branch in autumn, one bird is cardinal, another is blue jay, the rest are ravens. Method Run Time 7 seconds SDXL 50 seconds Flux1-dev 2 minutes CountGen Reason-your-Layout 15 seconds 10 minutes BoundedAttention AttentionRefocusing 40 seconds 7 seconds DPO 2 minutes SLD 45 seconds Emu 5 minutes Ours four cupcakes and two pies on kitchen counter, the cupcake in the center has rainbow sprinkles. Ours Emu 1.6 w/o Lğ‘œğ‘ ğ‘— & Lğ‘ğ‘¡ğ‘¡ w/o Attn. Masking w/o Lğ‘ğ‘” Fig. 8. Qualitative ablations. We present compare results produced by our method when ablating three different components: cross attention losses (w/o Lğ‘œğ‘ ğ‘— & Lğ‘ğ‘¡ğ‘¡ ), attention masking (w/o Attn. Masking) and composition preserving regularization (w/o Lğ‘ğ‘”) Self-correcting LLM-controlled Diffusion (SLD). We used the default Image Correction configuration on images generated with SDXL using the GPT4o LLM. The code used to run this baseline is available on the SLD github repository. Further Discussion C.1 Qualitative Ablation Study qualitative ablation study accompanying the quantitative study presented in Section 4.4 is presented in Figure 8. The figure showcases the types of failures that occur when different components are ablated from our method. Without attention losses (middle column) our method often fails to make meaningful changes to the initial image in the areas where they are required. For instance failing to turn one of the eclairs to piece of cake (top row, middle column) or turning one of the ravens into cardinals (middle row, middle column). Without attention masking objects are often less distinct and semantic leakage is evident. This can be seen in the blue feathers that leak out into the ravens and cardinal (middle row, second column from the right) and sprinkles that appear on all four cupcakes (bottom row, second column from the right). The effect of removing the composition preserving regularization loss from our method (rightmost column) is perhaps most visually distinct, producing outputs that are much less visually pleasing and often seemingly ignore the parts of the input prompt that describe the environment (rightmost column, bottom and middle rows). C.2 Method Runtimes Running times for our method as well as competing methods that were compared against in this work are presented in Table 4. All Table 4. Running times for all methods tested in this work. running times are calculated on single NVIDIA A100 80GB GPU. As this table clearly shows, the general purpose image generation methods (SDXL, Emu, Flux1-dev) as well as DPO (which functions exactly like SDXL in inference) are fastest with SDXL and DPO being the fastest overall with run time of 7 seconds. The optimization based methods are expectedly slower with Bounded-Attention being slowest with runtime of 10 minutes. C.3 Extended Evaluation on CompoundPrompts Figure 6 in the main paper presents comparison against all competing baselines which were evaluated in this work. The figure shows the results for two unique prompts in their three complexity versions (Tiers A, and C). This figure also illustrates our evaluation process, specifically in calculating VQA Accuracy, displaying the prompts along with their accompanying questions and the answers each result received for it as evaluated by an MLLM (GPT4o [OpenAI 2024]). While several methods can generate object counts correctly (particularly for lower counts), they rarely satisfy all of the conditions of the prompts in tiers and C. In Table 5 we present the results of quantitative comparison against competing baselines on the CompoundPrompts dataset, specifically presenting the VQA Accuracy metric across all difficulty tiers and object counts. While these results, somewhat unsurprisingly, show general trend of VQA Accuracy being lower as the number of objects in the images increases, this is not always the case. SDXL, Emu and BoundedAttention for instance, achieve higher VQA Accuracy scores for images with 6-7 objects in comparison to images with 4-5 objects. The reason for this is not completely clear, but we hypothesize that this can be attributed to these specific baselines having slight tendency to overgenerate. Regardless, the results show that our method outperforms competing baselines across all difficulty tiers and object count tiers. C.4 Evaluation on GenEval In Table 6 we present the results of quantitative comparison conducted on the GenEval [Ghosh et al. 2024] benchmark. This benchmark is made up of 553 prompts aimed at evaluating various compositional image properties. Specifically, these prompts are split into six categories: Counting, Colors, Position, Color Attribution, Single Object and Two Object. Most relevant for our problem setting are the Counting, Position, Color Attribution and Two Object prompts as they describe images with more than one object. Similarly to InstanceGen: Image Generation with Instance-level Instructions 15 Method SDXL Flux1-dev CountGen ReasonYourLayout BoundedAttention AttentionRefocusing DPO SLD Emu Ours 2-3 Objects 4-5 Objects 6-7 Objects Avg. C Avg. C Avg. VQA Accuracy 0.28 0.53 0.28 0.02 0.18 0.28 0.23 0.27 0.60 0.75 0.14 0.53 0.20 0.68 0.13 0.55 0.05 0.40 0.11 0.45 0.04 0.68 0.08 0.45 0.08 0.62 0.28 0.78 0.38 0.87 Table 5. Detailed quantitative evaluation on the CompoundPrompts dataset. 0.15 0.53 0.17 0.13 0.23 0.15 0.12 0.13 0.52 0. 0.32 0.58 0.33 0.24 0.29 0.37 0.27 0.34 0.63 0.77 0.30 0.65 0.42 0.28 0.35 0.63 0.25 0.37 0.40 0.72 0.17 0.48 0.21 0.13 0.17 0.24 0.14 0.16 0.31 0.58 0.42 0.42 0.37 0.27 0.37 0.57 0.30 0.33 0.58 0.58 0.12 0.37 0.13 0.05 0.13 0.07 0.15 0.07 0.32 0.58 0.10 0.42 0.07 0.07 0.03 0.03 0.02 0.03 0.22 0. 0.07 0.20 0.07 0.00 0.05 0.02 0.07 0.03 0.17 0.37 0.21 0.27 0.19 0.11 0.18 0.21 0.15 0.15 0.34 0.44 Avg. 0.23 0.44 0.24 0.16 0.21 0.27 0.19 0.21 0.43 0.60 Method Single Object Colors Counting Position Color Attr. Two Object Overall SDXL Flux1-dev CountGen ReasonYourLayout BoundedAttention AttentionRefocusing DPO SLD Emu Ours 0.98 1.00 0.96 0.95 0.98 0.97 1.00 0.88 0.97 0.97 0.39 0.71 0.56 0.44 0.30 0.62 0.41 0.51 0.55 0.68 Table 6. Quantitative evaluation on the GenEval benchmark. 0.85 0.82 0.83 0.64 0.85 0.66 0.87 0.80 0.85 0.85 0.15 0.19 0.09 0.34 0.48 0.58 0.11 0.24 0.51 0. 0.23 0.47 0.20 0.07 0.21 0.15 0.24 0.18 0.62 0.73 0.74 0.81 0.61 0.55 0.75 0.73 0.81 0.70 0.85 0.87 0.55 0.67 0.54 0.50 0.59 0.66 0.57 0.55 0.73 0.79 DrawBench, the evaluation process in GenEval uses semantic segmentation model (Mask2Former [Cheng et al. 2022]) as well as VLM (CLIP ViT-L/14 [Radford et al. 2021]) in order to compare the count, positioning, object type and color of instances in the image against ground truth label. As the results show, our method outperforms competing baselines in all the multi-object categories apart from the Counting category, in which we are slightly outperformed by Flux1-dev even though our method outperforms our baseline method (Emu) by significant 13%. Also worth mentioning is the fact that the Counting prompts in this benchmark involve only single object appearing multiple times, this is in contrast to the other baselines used in the paper - DrawBench and CompoundPrompts in which Counting (or Type prompts in CompoundPrompts) can involve multiple types of objects in the same image. These multi object prompts are difficult for competing methods, and our method significantly outperforms competing baselines on these more complex counting prompts. C.5 Limitations In Figure 9 we present three typical failure cases of our method, demonstrating issues resulting from both the instance assignment stage (top and bottom rows) as well as the assignment conditioned image generation stage (middle row). In the first example (top row) our method fails to accurately convey the text prompt as while the object count and spatial ordering of the pillows is correct they are not stacked on top of each other as requested. In this specific scenario the LLM opted to delete the second pillow from the top (most likely due to it having lower attention score for all the attribute words) resulting in gap that it addressed by placing the top pillow further back on shelf. We believe these types of errors are caused by the LLM focusing on following its instructions to the point of neglecting basic logic, and might be solvable with better instruction prompt tuning or more powerful language models. The second example (middle) row, shows common failure where two (or more) instances merge into one. This is most common in two instances close to one another with both having been assigned the same object words or attributes. This shows that while mask segments are certainly more precise than bounding boxes, guidancebased optimization approach still leaves room for such errors. In the last example (bottom row) our method wrongfully assigns volleyball and black basketball, placing the volleyball in the back and the black basketball in front. This is caused by one of the more significant limitations of our method which is that the LLM in charge of instance assignment assumes instances are on 2D plane and as result is unable to differentiate front from back. An interesting direction for future work is to address this limitation by incorporating depth estimation outputs into the instance assignment procedure. Our method handles undergeneration of instances by seed search and an instance copying algorithm. While effective, these approaches have some limitations. Seed search is computationally demanding and is not guaranteed to produce viable layout with enough object instances. Instance copying, on the other hand, is guaranteed to produce viable layout, however this layout is usually less aligned with the original attention distribution, which sometimes results in inaccurate or low visual quality results. This could potentially be improved by incorporating attention based guidance into the instance copying mechanism, or using specifically trained model to add missing instances to layout maps (similarly to CountGen). We leave these potential improvements for future work. 16 Etai Sella, Yanir Kleiman, and Hadar Averbuch-Elor four pillows stacked on top of each other, from bottom to top they are pink, tiger pattern, covered in blue velvet and pinstriped. four cats and dog in living room, one cat is ginger another is babydoll the dog is sitting between them and is poodle. four balls in playing field, in front is volleyball, in the back is black basketball, soccer ball is on each side, one of which is golden. Initial Image Instance Assignments Ours Fig. 9. Limitations. We present three failure cases of our method. In the first example (top row) our method wrongfully creates gap in the pillow stack. In the second (middle row), two pairs of cats are merged into one. In the final example (bottom row) our wrongfully places the volleyball in the back and the black basketball in the front. See Section C.5 for an additional discussion over these failure cases."
        }
    ],
    "affiliations": [
        "Cornell University, USA",
        "Meta AI, UK",
        "Tel Aviv University, Israel"
    ]
}