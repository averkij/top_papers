{
    "paper_title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR",
    "authors": [
        "Jiaming Li",
        "Longze Chen",
        "Ze Gong",
        "Yukun Chen",
        "Lu Wang",
        "Wanwei He",
        "Run Luo",
        "Min Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose $\\textbf{PACS}$, a novel RLVR framework that achieves im$\\textbf{P}$licit $\\textbf{A}$ctor $\\textbf{C}$ritic coupling via a $\\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 2 2 5 2 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "IMPLICIT ACTOR CRITIC COUPLING VIA SUPERVISED LEARNING FRAMEWORK FOR RLVR Jiaming Li1,2 Longze Chen1,2 Ze Gong1 Yukun Chen1,2 Lu Wang3 Wanwei He1,2 Run Luo1,2 Min Yang1 1 Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences 2 University of Chinese Academy of Sciences {jm.li4, lz.chen2, ze.gong, min.yang}@siat.ac.cn 3 Ritzz-AI"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose PACS, novel RLVR framework that achieves imPlicit Actor Critic coupling via Supervised learning framework. By treating the outcome reward as predictable label, we reformulate the RLVR problem into supervised learning task over score function parameterized by the policy model and optimized using cross-entropy loss. detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have recently achieved remarkable success in complex reasoning tasks, driven by advancements, such as OpenAI-o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025). In particular, scaling test-time compute enables longer Chains of Thought (CoT), facilitating more sophisticated and structured reasoning. This progress has led to significant improvements in challenging domains, such as mathematics (Shao et al., 2024; Liu et al., 2025a; Zuo et al., 2025) and programming (Lambert et al., 2024; Cui et al., 2025; Zhao et al., 2025). crucial underlying ingredient behind these achievements is the emergence of Reinforcement Learning with Verifiable Rewards (RLVR), which empowers LLMs to progressively enhance their capabilities by reinforcing their self-generated outputs with verifiable outcome rewards. Existing RLVR methods fall into two primary categories: value-model-based approaches, such as PPO (Schulman et al., 2017) and VAPO (Yue et al., 2025), which employ large value models to estimate advantages for policy updates; and value-model-free approaches, such as GRPO (Shao et al., 2024; Guo et al., 2025) and DAPO (Yu et al., 2025), which bypass explicit value modeling by leveraging Monte Carlo estimation of group relative rewards. While both classes have demonstrated remarkable success, they face fundamental challenges stemming from the sparse nature of outcome rewards in RLVR settings, where only single reward signal is provided after the entire response Equal contribution. Ze Gong and Min Yang are corresponding authors."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Comparison between RLVR and the supervised learning reformulation, where the query and output are input, and the outcome reward is treated as predictable label. is generated. This sparsity severely hinders credit assignment and gradient propagation in Reinforcement Learning (RL)-based policy learning, which relies on attributing feedback to individual tokens or actions within the output. Value-model-based methods address this challenge by learning explicit value models, but at the cost of increased model complexity and computational overhead. In contrast, value-model-free methods avoid this complexity by forgoing explicit value prediction, but suffer from high variance in their Monte Carlo estimates, often leading to issues such as advantage collapse, training instability, and degraded performance. This challenge, rooted in the inherently sparse nature of RLVR feedback, highlights fundamental limitations of the existing RL-based paradigm. It motivates the development of alternative policy optimization strategies that can leverage direct supervision to enable more stable and efficient learning in RLVR settings. To this end, we propose novel framework that recasts the RLVR problem as supervised learning task (see Figure 1). Instead of relying on sparse outcome rewards to optimize the policy via RL, we directly interpret the outcome reward as supervision signal and train score function, parameterized by the policy itself, to predict this reward via cross-entropy loss. Through detailed gradient analysis, we show that this supervised learning formulation not only recovers the standard policy gradient update but also implicitly couples the actor (policy improvement) and critic (reward estimation) components within single update. Unlike conventional actor critic methods that maintain separate value function estimator, our approach adopts shared parameterization, allowing both roles to be updated jointly via unified learning signal. This tight coupling eliminates the temporal mismatch between reward estimation and policy updates, thereby improving training efficiency. Moreover, in contrast to value-model-free methods, which rely solely on highvariance Monte Carlo estimates of relative advantages for policy updates, our method leverages prediction-error-based learning signals, providing more stable and reliable guidance for optimization. We refer to our method as PACS (imPlicit Actor Critic coupling via Supervised learning framework). It offers principled and efficient training paradigm that unifies policy learning and reward estimation within coherent supervised learning framework. To assess the effectiveness of our method, we conduct experiments on four challenging mathematical reasoning tasks and compare it against the state-of-the-art RLVR methods, including PPO and GRPO. The results demonstrate that PACS significantly outperforms baselines. For instance, PACS achieves pass@256 rate of 59.78% on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. Ablation studies confirm the critical importance of the weighting mechanism, while training dynamics analysis reveals that PACS achieves superior explorationexploitation balance. The main contributions are summarized as follows: We propose PACS, novel RLVR approach that reframes the problem as supervised learning: motivated by the sparsity of outcome rewards, we treat them as target labels and train the policy via cross-entropy loss. We demonstrate that the proposed loss inherently captures policy gradient updates and implicitly unifies actor (policy improvement) and critic (reward estimation) through shared parameterization, enabling efficient and stable policy optimization. Extensive experiments on challenging mathematical reasoning benchmarks show that PACS outperforms state-of-the-art RLVR methods, achieving superior reasoning performance, maintaining healthy policy entropy for exploration, and exhibiting improved training efficiency."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Reasoning Models. The emergence of reasoning models, referring to LLMs explicitly trained or optimized for multi-step chain-of-thought inference, has been catalyzed by the release of OpenAI-o1 (Jaech et al., 2024), which achieves strong performance on challenging reasoning tasks. DeepSeek-R1 (Guo et al., 2025) further highlights the power of verifiable reward signals by using GRPO (Shao et al., 2024) with rule-based outcome rewards to guide effective policy optimization. Building on this foundation, QWQ (Qwen-Team, 2025) introduces two-stage RL framework: cold-start phase to bootstrap reasoning ability, followed by brief phase of general RL finetuning to expand the models problem-solving proficiency. Most recently, Gemini 2.5 Pro (Comanici et al., 2025) integrates human feedback, retrieval-augmented generation, and multimodal perception, enabling reasoning grounded in diverse information sources beyond text. Together, these advances illustrate the evolution of RL-enhanced reasoning models from single-domain systems to generalpurpose, multimodal agents. Reinforcement Learning with Verifiable Reward (RLVR). RLVR has become widely adopted strategy for enhancing the reasoning capabilities of LLMs in domains with clearly verifiable correctness, such as mathematics and programming (Guo et al., 2025; Team et al., 2025; Ma et al., 2025; Chu et al., 2025). Recent work has explored wide range of RLVR techniques, which can be broadly categorized as value-model-based or value-model-free. Value-model-based methods, such as PPO (Schulman et al., 2017), VinePPO (Kazemnejad et al., 2024), and VAPO (Yue et al., 2025), explicitly learn value function to estimate the expected cumulative reward, providing stable training signals at the cost of additional computational overhead. On the other hand, value-model-free methods, such as GRPO (Shao et al., 2024), REINFORCE++ (Hu et al., 2025), and DAPO (Yu et al., 2025), avoid explicit value modeling by relying on Monte Carlo-based advantage estimation. While these approaches reduce modeling complexity, they often suffer from high gradient variance, which undermines training stability and overall performance."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we first establish the preliminaries for RLVR, and then introduce PACS, novel RLVR framework that achieves imPlicit ActorCritic coupling through Supervised learning framework. We present the overall framework, followed by detailed gradient analysis that illustrates the effectiveness of our approach, and conclude with practical design considerations to instantiate the implementation. 3.1 PRELIMINARIES Let θ denote the parameters of an LLM, and let (Q) be distribution over queries from which query is sampled. The model generates an output according to the policy πθ(q). verifiable outcome reward R(q, o) {0, 1} is provided by either reward model or an external verifier which determines whether the models output to the query is correct (1) or incorrect (0). The objective of RLVR is to learn policy πθ that maximizes the expected reward: LRLVR(θ) = EqP (Q),oπθ(q)[R(q, o)]. Various RLVR approaches (Schulman et al., 2017; Kool et al., 2019; Ahmadian et al., 2024; Shao et al., 2024; Yu et al., 2025; Yue et al., 2025) have been proposed to optimize this objective function, each seeking to effectively learn the policy πθ under the supervision of outcome rewards. 3.2 RECASTING RLVR WITHIN SUPERVISED LEARNING FRAMEWORK In the RLVR framework, LLMs generate responses to input queries, and verifiable outcome rewards that indicate the correctness of the final outcomes are provided at the end of responses. key characteristic of this setting is that the reward is delivered only once, after the entire response is generated, reflecting the overall quality of the output. Inspired by this observation, we propose an alternative to conventional RL-based optimization: rather than policy learning through RL from sparse reward signals, we recast the problem as supervised learning task in which the model is trained to directly predict the outcome reward."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: An illustration of the PACS framework. The framework consists of three main components: (1) Reward Proxy Computation, which calculates reward proxy ˆr based on the logprobability ratio. (2) Group Computation, which computes RLOO-based advantage scores ψ from the reward proxies. (3) Cross-Entropy Loss, which converts the RLVR problem into supervised learning task, optimizing scoring function parameterized by the policy with cross-entropy loss. Concretely, instead of using the reward R(q, o) {0, 1} associated with query-output pair (q, o) to guide policy optimization via RL, we consider (q, o) as input data and treat R(q, o) as the corresponding label. The objective is to learn mapping (q, o) that accurately predicts the reward. Specifically, this problem can be cast as classification task where rewards serve as binary labels (e.g., correct vs. incorrect). To this end, we define the learning objective optimized via standard binary cross-entropy loss as follows: L(θ) = EqP (Q),oπθ(q) (cid:20) R(q, o) log (cid:16) σ(cid:0)ψ(q, o; πθ)(cid:1)(cid:17) +(cid:0)1R(q, o)(cid:1) log (cid:16) 1σ(cid:0)ψ(q, o; πθ)(cid:1)(cid:17)(cid:21) , (1) where σ(z) = 1 1+ez is the sigmoid function that maps real-valued scores to the [0, 1] interval, representing the predicted probability of correctness. The score function ψ(q, o; πθ) is parameterized in terms of the policy model πθ, which is trained to estimate the quality of the generated outputs directly. The specific form and implementation of ψ(q, o; πθ) will be discussed in detail in Section 3.4. 3.3 GRADIENT ANALYSIS OF OBJECTIVE FUNCTION To gain deeper understanding of how our objective guides learning, we analyze the gradient of the loss function L(θ) with respect to the model parameters θ. We begin by introducing the per-sample loss term: l(q, o; πθ) := R(q, o) log (cid:16) σ(cid:0)ψ(q, o; πθ)(cid:1)(cid:17) + (cid:0)1 R(q, o)(cid:1) log (cid:16) 1 σ(cid:0)ψ(q, o; πθ)(cid:1)(cid:17) (2) which corresponds to the cross-entropy between the ground-truth reward R(q, o) and the prediction σ(cid:0)ψ(q, o; πθ)(cid:1). Using this definition, the gradient of the full loss can be expressed as: θL(θ) = Eq (cid:104) θEoπθ(q) (cid:2)l(q, o; πθ)(cid:3)(cid:105) = Eq,oπθ(q) (cid:104) (cid:105) θ log πθ(oq)(cid:2)l(q, o; πθ)(cid:3) + θl(q, o; πθ) (3) We now derive the inner gradient term θl(q, o; πθ). Applying the chain rule yields: (cid:34) θl(q, o; πθ) = R(q, o) σ(cid:0)ψ(q, o; πθ)(cid:1) + (cid:0)1 R(q, o)(cid:1) σ(cid:0)ψ(q, o; πθ)(cid:1) σ(cid:0)ψ(q, o; πθ)(cid:1) 1 σ(cid:0)ψ(q, o; πθ)(cid:1) (cid:35) θψ(q, o; πθ) (4) Recalling that the derivative of the sigmoid function is σ(x) = σ(x)(1 σ(x)), we simplify the gradient term as follows: θl(q, o; πθ) = (cid:104) R(q, o) σ(cid:0)ψ(q, o; πθ)(cid:1)(cid:105) θψ(q, o; πθ) (5) Substituting Equation 5 into the expression for θL(θ), we obtain the final form of the gradient:"
        },
        {
            "title": "Preprint",
            "content": "θL(θ) = Eq,oπθ(q) (cid:20) (cid:2)l(q, o; πθ)(cid:3)θ log πθ(oq) (cid:125) (cid:123)(cid:122) (cid:124) ACTOR: policy improvement (cid:16) R(q, o) σ(cid:0)ψ(q, o; πθ)(cid:1)(cid:17) + (cid:124) (cid:123)(cid:122) CRITIC: reward estimation (cid:21) θψ(q, o; πθ) (cid:125) (6) In the above gradient expression, the first term corresponds to standard policy gradient update, weighted by the per-sample cross-entropy loss. The second term serves as reward estimation correction, adjusting the score function ψ to better align the predicted reward σ(cid:0)ψ(q, o; πθ)(cid:1) with the ground-truth outcome. Notably, this formulation unifies the ACTOR and CRITIC updates within single gradient step and shared parameter space, enabling tighter coupling and more efficient learning. Implicit Actor Critic Coupling. As shown in Equation 6, our formulation enables the learned model πθ to simultaneously fulfill two roles: ACTOR: It samples outputs conditioned on the query q, according to the policy distribution πθ(oq). CRITIC: It estimates the quality of each sampled output using the score function ψ(q, o; πθ), where the sigmoid-transformed value σ(cid:0)ψ(q, o; πθ)(cid:1) represents the predicted probability of correctness and serves as the predicted reward. In the ACTOR update, the per-sample cross-entropy loss term l(q, o; πθ) quantifies the alignment between the predicted reward σ(cid:0)ψ(q, o; πθ)(cid:1) and the ground-truth reward R(q, o). This term serves as dynamic weight in the policy gradient, modulating the magnitude of updates based on prediction accuracy. In the CRITIC update, the residual term R(q, o) σ(cid:0)ψ(q, o; πθ)(cid:1) captures the discrepancy between the predicted and actual reward. This prediction error serves as direct supervised learning signal, driving the score function ψ(q, o; πθ) to better predict the outcome reward. Notably, improvements in the CRITIC directly influence future ACTOR updates by refining the weighting term l(q, o; πθ). In general, the gradient decomposition in Equation 6 splits the gradient into two components: the policy gradient update (ACTOR) and the supervised reward estimation updates (CRITIC). Crucially, both components operate within single policy model πθ, enabling implicit coupling between ACTOR and CRITIC without the need for separate networks or alternating update schedules. This unified design facilitates efficient training while leveraging the strengths of both policy gradient and supervised learning paradigms. 3.4 SCORE FUNCTION INSTANTIATION To instantiate the score function ψ(q, o; πθ), we require mechanism to quantify how good samIn our work, we interpret and implement ψ(q, o; πθ) as pled response is for given query q. an advantage-like function, which measures the relative quality of response conditioned on the query within group. Unlike value-model-free methods that estimate advantages using groundtruth rewards, we define ψ(q, o; πθ) directly in terms of the policy model πθ, enabling direct policy optimization. To compute this advantage efficiently and with minimal implementation overhead, we adopt the REINFORCE Leave-One-Out (RLOO) estimator (Kool et al., 2019; Ahmadian et al., 2024). RLOO provides an unbiased estimate of the relative advantage by comparing each sampled output to others generated for the same query within batch. Specifically, for each query q, we sample set of candidate responses o1, o2, . . . , ok. For each output oi, the RLOO-based advantage-like score function is defined as: ψ(q, oi; πθ) = ˆr(q, oi; πθ) 1 1 (cid:88) j=i ˆr(q, oj; πθ), (7) where ˆr(q, oi; πθ) is reward proxy based on log-probability ratios, following prior work (Rafailov et al., 2023): ˆr(q, oi; πθ) = β log πθ(oiq) πref(oiq) = β oi (cid:88) (cid:16) t=1 log πθ(oi,tq, oi,<t) log πref(oi,tq, oi,<t) (cid:17) , (8)"
        },
        {
            "title": "Preprint",
            "content": "MATH 500 (pass@k) AMC 23 (pass@k) Model = 1 2 4 16 32 = 1 8 32 128 256 Base PPO GRPO PACS 56.66 65.48 66.92 67.31 67.64 71.94 73.34 74.75 75.75 77.51 78.60 80. 81.91 82.14 83.01 85.03 86.87 86.02 86.60 88.22 90.60 89.20 89.00 90.40 30.20 38.57 37.01 39.63 66.16 66.07 68.71 68.23 83.88 80.19 82.61 82. 90.42 86.81 87.72 88.23 94.85 92.09 92.39 92.70 97.10 95.53 96.44 96.18 AIME 2024 (pass@k) AIME 2025 (pass@k) Model = 8 32 64 128 256 = 8 32 64 128 256 Base PPO GRPO PACS 4.92 7.55 8.14 7.98 15.66 19.70 19.13 21.49 27.21 30.27 26.06 30.10 33.45 35.69 31.65 35.87 40.95 42.08 37.75 42.25 49.68 50.81 42.76 48. 1.73 1.40 2.62 2.94 10.62 8.99 7.07 11.32 23.45 21.11 14.07 22.91 30.69 28.07 20.18 30.50 39.09 35.52 28.37 39.07 47.38 42.51 36.58 47. Table 1: Results of Qwen2.5-3B trained with PPO, GRPO and PACS. Bold numbers indicate the best performance. Underlined numbers indicate the second best. Due to space constraints, selected results are presented here. The complete results are provided in AMC23  (Table 5)  , AIME 2024  (Table 7)  and AIME 2025  (Table 9)  in the Appendix. where πref is fixed reference policy used to regularize the learned policy, and β is scaling hyperparameter. However, in practical implementation, the reward proxy ˆr(q, oi; πθ) may grow progressively larger over time due to the fixed reference policy, leading to high variance and instability during training. To address this, we follow the strategy proposed in (Liu et al., 2025a), periodically hard-resetting the reference policy πref to recent snapshot of the online policy πθ, and reinitializing optimizer states to maintain stable training dynamics. With the RLOO-estimated advantage serving as the score function, the training objective becomes: qP (Q),{oi}G i=1πθ(q) LPACS(θ) = (cid:88) 1 i=1 [R(q, oi) log (σ(ψ(q, oi; πθ))) + (1 R(q, oi)) log (1 σ(ψ(q, oi; πθ)))] . (9) This formulation preserves the supervised learning structure of our original objective while incorporating relative comparisons between sampled outputs, yielding richer and more informative learning signals1. The overall PACS framework is illustrated in Figure 2. 3.4.1 LOSS FUNCTION ANALYSIS. To gain deeper insight into the learning dynamics, we analyze the loss function in Equation 9. When sampled output oi is correct (i.e., R(q, oi) = 1), the loss reduces to the term log(σ(ψ(q, oi; πθ))). Minimizing the loss in this case corresponds to maximizing ψ(q, oi; πθ), which in turn requires increasing the estimated reward proxy ˆr(q, oi; πθ). Since ˆr is defined as log-probability ratio, this implies increasing πθ(oiq), i.e., assigning higher probability to generating the correct response. In this way, the model learns to favor correct outputs. Conversely, when sampled output oi is incorrect (i.e., R(q, oi) = 0), the loss reduces to the term log(1 σ(ψ(q, oi; πθ))). Minimizing the loss requires decreasing ψ(q, oi; πθ), which again corresponds to lowering the reward proxy ˆr(q, oi; πθ). This is achieved by reducing πθ(oiq), discouraging the generation of incorrect responses. Thus, the model learns not only to generate correct outputs, but also to avoid producing incorrect ones. 3.4.2 HANDLING DATA IMBALANCE. To address the potential distributional imbalance in generated outputs oi for q, we adopt the class imbalance treatment methodology proposed by (King & Zeng, 2001), whereby differential weights are assigned to correct and incorrect samples respectively. Specifically, when the proportion of correct and incorrect samples in the training data exhibits imbalance, we mitigate the adverse effects of sample distribution bias on model performance by adjusting weight parameters to balance the models attention across different categorical samples. 1A rule-based reward function is employed, assigning reward of 1 to correct answers and 0 to incorrect ones."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Datasets & Models. This study uniformly adopts the DeepScaleR (Luo et al., 2025) dataset as the training corpus, which constitutes high-quality mathematical problem-solving collection comprising approximately 40,000 unique mathematical problem-answer pairs. To comprehensively evaluate the performance of methods, we conduct extensive evaluations on four representative benchmarks: MATH 500, AMC23, AIME 2024, and AIME 2025. The experiments utilize Qwen2.5-3B and Qwen2.5-7B (Qwen et al., 2025) as base models, enabling us to assess PACSs performance across various model scales. Baselines & Hyperparameters. To evaluate the effectiveness of our approach, we compare the performance of PACS with representative RL algorithms: PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024). The training objectives of these algorithms can be found in Appendix. We also report the performance of base models for reference. The train batch size is set to 1,024. The learning rate of actor model is set to 1e-6. For each query, 8 responses are sampled. For PACS, β is set to 1.0. During inference, we configure the sampling parameters with temperature of 0.6 and top-p of 0.96. For Qwen2.5-3B, we conduct training for 140 steps, while for Qwen2.5-7B, we extend the training to 300 steps. Prompt template and detailed hyperparameter configuration can be found in the Appendix. Hardware. All experiments are conducted on NVIDIA H200 GPUs. Training employs verl (Sheng et al., 2024), an efficient reinforcement learning library specifically designed for large language models. For inference, we utilize vllm (Kwon et al., 2023). Evaluation Metrics. To evaluate the models capability to generate diverse correct solutions while mitigating the randomness bias that may arise from single sampling, we employ the pass@k metric for assessment. pass@k represents the probability that at least one correct answer exists among generated candidate solutions. pass@k also enables performance evaluation under different computational budgets by adjusting the value of k. However, directly calculating pass@k with only samples per instance often results in high variance. To obtain more stable evaluation results, we generate samples for each problem (where k), count the number of correct samples c, and calculate the unbiased estimator (Chen et al., 2021): (cid:34) pass@k = ExD 1 (cid:35) . (cid:1) (cid:0)nc (cid:1) (cid:0)n (10) Our sampling strategy adapts to the scale of dataset. For the MATH 500 dataset, given its scale of 500 problems, the sampling size is set to = 32 for each problem and model performance is evaluated across {1, 2, 4, 8, 16, 32}. Conversely, for smaller test sets (AMC23, AIME 2024, and AIME 2025), which contain fewer problems but with high-quality content, larger sampling scale is employed to obtain more stable and reliable statistical results. Specifically, we generate 512 candidate solutions for each problem (n = 512) and conduct evaluation across broader range of where {1, 2, 4, 8, 16, 32, 64, 128, 256}. 4.2 MAIN RESULTS Table 1 and Table 2 compare the overall results of PACS with different baselines. Experimental results on the Qwen2.5-3B model demonstrate that PACS exhibits significant performance advantages across multiple mathematical reasoning tasks. On the MATH 500 dataset, PACS achieves optimal performance across all k-value configurations, particularly attaining pass rate of 67.31% at pass@1, representing an improvement of 10.65 points over the base model and surpassing both PPO and GRPO. In the AMC 23 task, PACS similarly demonstrates exceptional performance. Notably, on the more challenging AIME 2024 and AIME 2025, PACS maintains its leading position across most k-value settings, demonstrating the algorithms stability and effectiveness in complex mathematical reasoning tasks."
        },
        {
            "title": "Preprint",
            "content": "MATH 500 (pass@k) AMC 23 (pass@k) Model = 1 2 8 16 32 = 1 8 64 128 256 Base PPO GRPO PACS - w/o weight 61.55 76.56 78.29 78.97 77.84 73.90 80.74 82.08 84.39 83. 82.09 84.15 84.70 88.18 87.68 87.48 86.70 86.60 90.68 90.30 90.93 88.60 88.26 92.33 92.06 92.80 90.20 90.00 93.60 93.20 35.72 51.22 57.22 57.83 54.51 70.21 72.80 72.20 78.17 75. 85.53 81.92 80.72 88.22 83.77 91.19 85.71 86.14 91.95 86.89 95.89 88.67 90.69 95.51 88.97 99.18 90.92 93.34 98.51 90.62 AIME 2024 (pass@k) AIME 2025 (pass@k) Model = 1 8 32 64 256 = 1 8 32 64 256 Base PPO GRPO PACS - w/o weight 7.26 15.56 14.10 15.80 15.72 23.46 28.50 25.37 29.75 24.57 36.41 38.71 34.78 43.90 32.58 43.77 41.50 38.50 51.21 38. 52.66 43.85 41.66 56.54 46.16 62.55 46.46 45.42 59.78 53.42 3.35 9.61 8.36 13.33 10.44 16.04 24.89 21.63 29.65 22.86 28.56 32.32 33.26 42.88 31.03 34.99 34.94 38.12 49.45 36. 42.07 38.27 42.33 54.48 42.39 50.11 42.91 46.45 58.22 47.67 Table 2: Results of Qwen2.5-7B trained with PPO, GRPO and PACS. Bold numbers indicate the best performance. Underlined numbers indicate the second best. Due to space constraints, selected results are presented here. The complete results are provided in AMC23  (Table 6)  , AIME 2024  (Table 8)  and AIME 2025  (Table 10)  in the Appendix. Figure 3: Performance analysis of PACS with varying β. The 3D heatmaps show pass@k scores for different combinations of β values (0.1, 0.5, 1, 2, 10) and values on MATH-500, AMC23, AIME-2024, and AIME-2025 datasets. On the larger-scale Qwen2.5-7B model, the advantages of the PACS algorithm become even more pronounced, further validating its generalization capability across different model scales. On the MATH 500 dataset, PACS achieves superior performance across all evaluation metrics. The performance of PACS is especially remarkable on the AIME series tasks: in AIME 2024, it achieves pass rate of 59.78% at pass@256, representing improvements of 13.32 and 14.36 points compared to PPO and GRPO, respectively; in AIME 2025, the pass@256 metric shows improvements of 15.31 and 11.77 points compared to PPO and GRPO, respectively. These results conclusively demonstrate PACSs exceptional performance in handling high-difficulty mathematical reasoning problems and its significant advantages over reinforcement learning methods PPO and GRPO. 4.3 ABLATION STUDY Different β. To validate the impact of β on the models performance in Equation 8, we design and conduct ablation experiments. Models are trained with different β {0.1, 0.5, 1, 2, 10} and their performance is evaluated in Figure 3. The results demonstrate that model achieves optimal performance when β = 1, yielding higher pass@k. Further analysis reveals that the AIME 2024 and AIME 2025 datasets exhibit heightened sensitivity to variations in β. This suggests that for tasks of higher complexity and difficulty, β constitutes critical step in achieving optimal performance. In contrast, the AMC23 and MATH 500 datasets demonstrate greater robustness across different β, with minor fluctuations in model performance. This phenomenon can be attributed to the relatively simpler nature of problems in these datasets, enabling the model to maintain stable performance across broader range of hyperparameters. Different Advantage Estimators. To investigate the influence of advantage estimators on PACSs efficacy, we extend our evaluation beyond the default RLOO to include GRPO (Shao et al., 2024) and Dr. GRPO (Liu et al., 2025b) as shown in Appendix C.4. Our findings indicate that the PACS with RLOO exhibits superior overall performance, lead that becomes more significant at higher"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Training dynamics of Qwen2.5-7B on the DeepScaleR dataset across different optimization methods (PPO, GRPO, PACS, and PACS w/o weight). The plots illustrate the evolution of three key metrics during training: (a) entropy loss, (b) gradient norm, and (c) mean response length over training steps. pass@k metrics. Notably, whereas RLOO and Dr. GRPO yield comparable results on datasets such as MATH 500 and AMC23, RLOOs superiority is markedly evident on the more demanding AIME benchmark. We posit that this performance gap stems from subtle algorithmic divergence between RLOO and Dr. GRPO, which is magnified under more challenging conditions. By contrasting single sample with the cohort average, RLOOs leave-one-out mechanism yields more robust and precise signals for credit assignment. Effect of Weight Integration. To validate the effectiveness of the weight mechanism in PACS, we conduct ablation experiments by comparing the performance of PACS with and without the weight component. The results presented in Table 2 clearly demonstrate the positive impact of the weight mechanism. PACS consistently outperforms its weight-free variant (w/o weight). The benefits are particularly pronounced on challenging AIME datasets, where the weight component proves crucial for complex mathematical reasoning tasks, achieving substantial performance improvements especially at higher sampling rates (e.g., 6.36% point improvement on AIME 2024 at pass@32 of Qwen2.5-7B). 4.4 TRAINING DYNAMICS Figure 4 compares training dynamics of four methods applied to Qwen2.5-7B. PPO and GRPO maintain consistently low entropy loss, resulting in entropy collapse and conservative outputs. But PACS shows non-monotonic pattern: entropy increases to about 1,700 at step 220, then stabilizes at 1,200-1,300, reflecting superior exploration-exploitation balance. PACS w/o weight demonstrates intermediate behavior. Gradient norms reveal similar patterns. While PPO and GRPO maintain stable gradient norms throughout training, PACS peaks at 40 with sustained elevation, enabling aggressive parameter updates and broader exploration. This sustained gradient elevation in PACS, contrasting with the stable but lower gradient norms in baseline methods, demonstrates superior training efficiency. The consistently higher gradient activity indicates that PACS continues making meaningful parameter updates throughout the training process, while the stable but modest gradients in PPO and GRPO suggest more conservative optimization that may lead to premature convergence. Response lengths show PACS generating the most detailed responses. PACS achieves superior exploration capability and response quality through moderate entropy maintenance and dynamic gradient updates, providing novel optimization insights compared to other methods."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we propose PACS, novel RLVR algorithm that achieves implicit actor-critic coupling via supervised learning. By treating outcome rewards as supervised targets and parameterizing the score function with policy log probabilities, PACS addresses the sparse reward and training instability challenges in existing RL-based RLVR methods. Extensive experiments on four mathematical benchmark datasets demonstrate that PACS significantly outperforms strong baselines including PPO and GRPO, achieving superior task performance while maintaining healthier policy entropy for sustained exploration. These findings suggest that supervised learning-based implicit actor-critic coupling represents promising paradigm for advancing RLVR."
        },
        {
            "title": "REFERENCES",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ustun, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. arXiv preprint arXiv:2501.03262, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Refining credit assignment in rl training of llms. arXiv preprint arXiv:2410.01679, 2024. Gary King and Langche Zeng. Logistic regression in rare events data. Political Analysis, 9(2): 137163, 2001. doi: 10.1093/oxfordjournals.pan.a004868. Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get baseline for free! drlStructPred@ICLR, 2019. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025a."
        },
        {
            "title": "Preprint",
            "content": "Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025b. URL https: //arxiv.org/abs/2503.20783. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, and Jia Li. S2r: Teaching llms to self-verify and self-correct via reinforcement learning. arXiv preprint arXiv:2502.12853, 2025. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Qwen-Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Chen, Li, Xiao, Du, Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms, 2025. URL https://arxiv. org/abs/2501.12599, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "A BASELINES",
            "content": "A.1 PROXIMAL POLICY OPTIMIZATION ALGORITHMS (PPO) The objective function of PPO (Schulman et al., 2017) is formulated as follows: JPPO(θ) = EqP (Q),oπθold (q) 1 o (cid:88) (cid:26) t=1 min (cid:20) πθ(otq, o<t) πθold(otq, o<t) At, clip (cid:18) πθ(otq, o<t) πθold(otq, o<t) (cid:19) (cid:21)(cid:27) , 1 ε, 1 + ε At where represents the advantage function estimated using Generalized Advantage Estimation (GAE), and ε denotes the clipping hyperparameter that stabilizes the training process in PPO. A.2 GROUP RELATIVE POLICY OPTIMIZATION (GRPO) The objective function of GRPO (Shao et al., 2024) is defined as: JGRPO(θ) = qP (Q),{oi}G 1 (cid:88) i=1 1 oi oi (cid:88) (cid:26) t=1 min i=1πθold (q) (cid:20) πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) Ai,t, clip (cid:18) πθ(oi,tq, oi,<t) πθold(oi,tq, oi,<t) (cid:19) (cid:21) (cid:27) , 1 ε, 1 + ε Ai,t βDKL[πθπref] where the advantage function is computed as: Ai,t = R(q, oi) mean({R(q, o1), . . . , R(q, oG)}) std({R(q, o1), . . . , R(q, oG)}) The KL divergence term is estimated using: DKL[πθπref] = πref(oi,tq, oi,<t) πθ(oi,tq, oi,<t) log πref(oi,tq, oi,<t) πθ(oi,tq, oi,<t) 1. Following the approaches in DAPO (Yu et al., 2025) and Dr. GRPO (Liu et al., 2025b), the KL divergence term may be optionally omitted from the original GRPO objective (i.e., setting β = 0), as empirical evidence suggests it may not be essential for performance."
        },
        {
            "title": "B HYPERPARAMETERS SETTINGS",
            "content": "To ensure Experimental efficiency and effectiveness, specialized optimized frameworks are used for training and inference phases. This section details the key hyperparameters used in each phase. B.1 TEMPLATE The chat template employed in this study is developed by modifying the official model template as shown below. We incorporate the instruction Please reason step by step, and put your final answer within boxed{} into the user input to elicit the models reasoning capabilities and standardized output formatting of final answers, thereby facilitating automated evaluation. Chat Template <im_start>system You are helpful assistant.<im_end> <im_start>user {input} Please reason step by step, and put your final answer within boxed{}.<im_end> <im_start>assistant B.2 TRAINING HYPERPARAMETERS During the model training phase, the verl (Sheng et al., 2024) framework is employed, which is powerful toolkit designed for large-scale model training. The relevant hyperparameter settings during training are as follows:"
        },
        {
            "title": "Configuration",
            "content": "1,024 512 4,096 True 256 Train batch size Max prompt length Max response length Filter overlong prompts Mini batch size Micro batch size per GPU 16 for Qwen2.5-3B, 8 for Qwen2.5-7B Learning rate Rollout number Use KL loss Total training steps 1 106 for actor, 1 105 for critic in PPO 8 False 140 for Qwen2.5-3B, 300 for Qwen2.5-7B Table 3: Training hyperparameters. B."
        },
        {
            "title": "INFERENCE HYPERPARAMETERS",
            "content": "During the model inference and evaluation phase, the high-performance inference serving framework vllm (Kwon et al., 2023) is employed. Through vllm, high-throughout and low-latency text generation is achieved. Consistent inference configurations are adopted across all experiments to ensure fairness and comparability of evaluation results. Hyperparameters Configuration Enable prefix caching GPU memory utilization Max tokens Temperature Top-p Tensor parallel size True 0.9 4,096 0.6 0.95 4 Table 4: Inference hyperparameters."
        },
        {
            "title": "C DETAILED EXPERIMENTAL RESULTS",
            "content": "This section presents the comprehensive test results of each model on AMC 23, AIME 2024, and AIME 2025, along with corresponding analyses. C.1 AMC23 The experimental results clearly demonstrate the superior performance of the PACS algorithm on the AMC23. As shown in Table 5 and Table 6, PACS achieves optimal or near-optimal performance across the majority of pass@k metrics. The advantages of PACS are particularly pronounced at lower values; for instance, on the Qwen2.5-3B and Qwen2.5-7B models, its pass@1 scores reach 39.63 and 57.83 respectively, surpassing all baselines. This indicates that the proposed algorithm can more effectively enhance the models capability to solve problems in the first attempt or within few trials, which holds significant value in practical applications. As the model scale expands from 3B to 7B, the PACS algorithm demonstrates excellent scalability, with its performance advantages becoming increasingly pronounced. On the Qwen2.5-7B model, PACS not only achieves leading performance in pass@1 but also comprehensively outperforms other baselines across the broad range from = 1 to = 64, showing substantial performance improvements. For example, at pass@8, PACS (78.17) outperforms the second-best GRPO (72.20) by nearly 6 points."
        },
        {
            "title": "Preprint",
            "content": "AMC23 (pass@k) Model = 1 2 4 8 32 64 128 256 Base PPO GRPO PACS 30.20 38.57 37.01 39. 42.56 48.65 46.85 50.56 55.10 57.97 58.14 60.30 66.16 66.07 68.71 68.23 75.62 73.25 76.69 75.97 83.88 80.19 82.61 82.45 90.42 86.81 87.72 88. 94.85 92.09 92.39 92.70 97.10 95.53 96.44 96.18 Table 5: Results of Qwen2.5-3B on AMC23 trained with PPO, GRPO and PACS. Bold numbers indicate the best performance. Underlined numbers indicate the second best. AMC23 (pass@k) Model = 2 4 8 16 32 128 256 Base PPO GRPO PACS - w/o weight 35.72 51.22 57.22 57.83 54.51 49.47 60.01 63.36 65.57 62.95 61.05 67.22 68.21 71.94 69. 70.21 72.80 72.20 78.17 75.15 78.35 77.52 76.06 83.83 79.77 85.53 81.92 80.72 88.22 83.77 91.19 85.71 86.14 91.95 86.89 95.89 88.67 90.69 95.51 88.97 99.18 90.92 93.34 98.51 90. Table 6: Results of Qwen2.5-7B on AMC23 trained with PPO, GRPO and PACS. Bold numbers indicate the best performance. Underlined numbers indicate the second best. C.2 AIME 2024 On the more challenging AIME 2024 dataset, PACS again demonstrates its effectiveness. On the Qwen2.5-3B model scale, different algorithms show very similar performance with intense competition, yet PACS still achieves leading results in critical intervals. Throughout the entire pass@k curve, PACS consistently remains in the top tier. On the Qwen2.5-7B model, PACS achieves comprehensive leadership from = 1 to = 128, obtaining optimal results at nearly all evaluation points. Similar to the AMC experiments, the base model again achieves the highest score at = 256, which further confirms the inference that unaligned base models retain higher answer diversity and have the potential to enumerate correct answers through massive sampling, though this approach is highly inefficient for practical applications. AIME 2024 (pass@k) Model = 1 4 8 16 32 64 256 Base PPO GRPO PACS 4.92 7.55 8.14 7.98 7.90 10.83 11.45 12.28 11.43 14.96 14.79 16.98 15.66 19.70 18.13 21. 21.11 24.94 21.74 25.58 27.21 30.27 26.06 30.10 33.45 35.69 31.65 35.87 40.95 42.08 37.75 42.25 49.68 50.81 42.76 48.32 Table 7: Results of Qwen2.5-3B on AIME 2024 trained with PPO, GRPO and PACS. Bold numbers indicate the best performance. Underlined numbers indicate the second best. AIME 2024 (pass@k) Model = 1 2 4 16 32 64 128 256 Base PPO GRPO PACS - w/o weight 7.26 15.56 14.10 15.80 15.72 11.66 19.45 16.92 19.93 19.60 17.17 23.51 20.71 24.39 22.24 23.46 28.50 25.37 29.75 24.57 29.94 34.16 30.34 36.29 27.88 36.41 38.71 34.78 43.90 32. 43.77 41.50 38.50 51.21 38.77 52.66 43.85 41.66 56.54 46.16 62.55 46.46 45.42 59.78 53.42 Table 8: Results of Qwen2.5-7B on AIME 2024 trained with PPO, GRPO and PACS. Bold numbers indicate the best performance. Underlined numbers indicate the second best."
        },
        {
            "title": "Preprint",
            "content": "C.3 AIME 2025 To comprehensively evaluate the performance stability and generalization capability of PACS, we conduct assessments on the latest AIME 2025 benchmark. This dataset represents the current highest difficulty standard for mathematical reasoning tasks, enabling more rigorous examination of algorithm performance under extremely challenging scenarios. On 3B-scale models, PACS demonstrates impressive comprehensive advantages, achieving optimal performance across nearly all evaluation metrics. PACS obtains superior results throughout the entire evaluation spectrum from pass@1 to pass@256. On 7B models, PACS achieves complete dominance, from pass@1 of 13.33 to pass@256 of 58.22, with every metric substantially outperforming all baseline methods. It is worth emphasizing that the performance gap between PACS and the second-ranked method is highly significant across multiple key metrics. For instance, at pass@16, PACS (36.00) surpasses GRPO (27.57) by over 8 percentage points; at pass@128, the gap reached 12 percentage points (54.48 vs 42.33). AIME 2025 (pass@k) Model = 1 Base PPO GRPO PACS 1.73 1.40 2.62 2.94 3.32 2.71 3.92 4.92 4 6.14 5.07 5.29 7.63 16 32 64 128 256 10.62 8.99 7.07 11. 16.67 14.56 9.87 16.47 23.45 21.11 14.07 22.91 30.69 28.07 20.18 30.50 39.09 35.52 28.37 39.07 47.38 43.51 36.58 47.18 Table 9: Results of Qwen2.5-3B on AIME 2025 trained with PPO, GRPO and PACS. Bold numbers indicate the best performance. Underlined numbers indicate the second best. AIME 2025 (pass@k) Model = 1 2 4 16 32 64 128 256 Base PPO GRPO PACS - w/o weight 3.35 9.61 8.36 13.33 10.44 6.12 14.42 11.82 18.19 15.12 10.40 19.69 16.34 23.73 19.36 16.04 24.89 21.63 29.65 22.86 22.32 29.21 27.57 36.00 26.48 28.56 32.32 33.26 42.88 31. 34.99 34.94 38.12 49.45 36.59 42.07 38.27 42.33 54.48 42.39 50.11 42.91 46.45 58.22 47.67 Table 10: Results of Qwen2.5-7B on AIME 2025 trained with PPO, GRPO and PACS. Bold numbers indicate the best performance. Underlined numbers indicate the second best. Synthesizing results across the four benchmarksMATH 500, AMC23, AIME 2024, and AIME 2025PACS exhibits exceptional cross-task generalization capability and performance consistency. Whether on the relatively straightforward MATH 500 and AMC23 or the extremely challenging AIME 2025, PACS consistently outperforms existing mainstream methods. This conclusively demonstrates that our proposed algorithm possesses robust adaptability and resilience, enabling outstanding performance across mathematical reasoning tasks of varying difficulty levels. C.4 ABLATION STUDY To conduct comparative analysis of how different advantage estimators affect PACS performance, we benchmark our default RLOO method against two alternatives: GRPO (Shao et al., 2024) and Dr. GRPO (Liu et al., 2025b). While RLOO utilizes leave-one-out mechanism, the advantage functions for GRPO and Dr. GRPO are defined as follows. Dr. GRPO introduces simple yet significant modifications to address the biases in GRPO by removing the std normalization terms. ψGRPO(q, oi; πθ) = ˆr(q, oi; πθ) mean({ˆr(q, o; πθ)}) std({ˆr(q, o; πθ)}) , ψDr. GRPO(q, oi; πθ) = ˆr(q, oi; πθ) mean({ˆr(q, o; πθ)}), (11) (12)"
        },
        {
            "title": "Preprint",
            "content": "MATH 500 (pass@k) Model = 1 2 4 16 32 78.97 PACS 74.71 - GRPO - Dr. GRPO 79.68 84.39 80.19 84.44 88.18 84.31 87.90 90.68 87.42 90. 92.33 89.64 92.30 93.60 91.20 93.40 Table 11: Results of different advantage estimators of Qwen2.5-7B on MATH 500. Bold numbers indicate the best performance. Underlined numbers indicate the second best. AMC23 (pass@k) Model = 2 4 8 16 32 128 256 57.83 PACS - GRPO 48.66 - Dr. GRPO 58.29 65.57 57.43 66.54 71.94 63.96 72.83 78.17 69.68 78. 83.83 75.71 83.28 88.22 81.63 87.45 91.95 86.94 90.12 95.51 90.95 91.65 98.51 94.06 93.13 Table 12: Results of different advantage estimators of Qwen2.5-7B on AMC23. Bold numbers indicate the best performance. Underlined numbers indicate the second best. On the MATH 500 and AMC23 benchmarks (see Tables 11 and 12), PACS (employing RLOO) demonstrates highly comparable performance with the Dr. GRPO variant. This result aligns with expectations, as RLOO and Dr. GRPO exhibit only subtle algorithmic differences, both enhancing the foundational GRPO through intra-batch relative comparisons. On these moderately challenging tasks, such minor algorithmic distinctions do not translate into substantial performance disparities. Specifically, Dr. GRPO shows marginal superiority at lower (k=1, 2), while PACS exhibits superior performance at higher values; however, the overall performance of both methods remains remarkably similar. AIME 2024 (pass@k) Model = 1 2 8 16 32 64 128 15.80 PACS - GRPO 12.75 - Dr. GRPO 15.59 19.93 16.24 19.67 24.39 19.00 23.99 29.75 21.59 29.04 36.29 24.88 34.78 43.90 28.91 40. 51.21 33.22 46.76 56.54 37.14 53.08 59.78 39.95 59.68 Table 13: Results of different advantage estimators of Qwen2.5-7B on AIME 2024. Bold numbers indicate the best performance. Underlined numbers indicate the second best. AIME 2025 (pass@k) Model = 1 2 4 8 16 64 128 256 PACS 13.33 8.39 - GRPO - Dr. GRPO 13.93 18.19 13.01 17.15 23.73 17.69 20. 29.65 21.78 24.05 36.00 25.77 28.68 42.88 29.86 34.55 49.45 34.34 41.07 54.48 39.26 46.12 58.22 43.73 49. Table 14: Results of different advantage estimators of Qwen2.5-7B on AIME 2025. Bold numbers indicate the best performance. Underlined numbers indicate the second best. The superiority of RLOO is most pronounced on the AIME 24 and AIME 25 (Tables 13 and 14). Here, the nuanced algorithmic difference between RLOO and Dr. GRPO is amplified into clear performance divergence, with our PACS algorithm demonstrating unequivocal superiority across all values of k. We posit that this performance gap is rooted in the distinctive leave-one-out credit assignment of RLOO. High-difficulty tasks with sparse rewards and high solution variance demand robust signal for policy updates. RLOOs mechanismcontrasting each sample against the cohort averageprovides exactly this stability. The practical consequence is substantial: on the AIME"
        },
        {
            "title": "Preprint",
            "content": "2025 benchmark, for instance, PACS (58.22%) establishes lead of almost 9 percentage points over Dr. GRPO (49.56%) at pass@256."
        }
    ],
    "affiliations": [
        "Ritzz-AI",
        "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
    ]
}