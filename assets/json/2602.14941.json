{
    "paper_title": "AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories",
    "authors": [
        "Zun Wang",
        "Han Lin",
        "Jaehong Yoon",
        "Jaemin Cho",
        "Yue Zhang",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval."
        },
        {
            "title": "Start",
            "content": "AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Zun Wang 1 Han Lin 1 Jaehong Yoon 2 Jaemin Cho 3 Yue Zhang 1 Mohit Bansal 1 https://zunwang1.github.io/AnchorWeave 6 2 0 2 6 1 ] . [ 1 1 4 9 4 1 . 2 0 6 2 : r Abstract Maintaining spatial world consistency over long horizons remains central challenge for cameracontrollable video generation. Existing memorybased approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, memory-augmented video generation framework that replaces single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multianchor control, and coverage-driven retrieval. 1. Introduction Building video world models that can consistently generate and revisit the same environments over long horizons un1Department of Computer Science, University of North Carolina, Chapel Hill 2Nanyang Technological University, Singapore 3AI2. Correspondence to: Zun Wang <zunwang@cs.unc.edu>. Preprint. February 17, 2026. 1 der user control (such as keyboard directions (Bruce et al., 2024; Parker-Holder et al., 2024; Ball et al., 2025), camera viewpoints (Wang et al., 2024b; He et al., 2024; Chen et al., 2025b), or human/robot actions (Guo et al., 2025) remains central challenge in generative modeling. Recent image-tovideo diffusion and Transformer-based models (Wan et al., 2025; Kong et al., 2024; HaCohen et al., 2026) have significantly improved short-term coherence and controllability, enabling clip-by-clip autoregressive generation (Huang et al., 2025b; Zhang & Agrawala, 2025; Chen et al., 2025a) that follows user-specified camera trajectories. However, despite these advances, such models lack persistent memory mechanisms, causing them to lose consistency with previously generated content when revisiting earlier regions of the environment (Yu et al., 2025). To improve long-horizon world-consistent video generation, recent works (Zhao et al., 2025; Li et al., 2025a; Wu et al., 2025c) introduce explicit memory mechanisms that reconstruct global 3D scene from historical video clips. Specifically, they estimate camera poses and per-frame geometry (e.g., depth maps), and fuse them into unified 3D representation such as global point cloud. Generation is then conditioned on anchor videos (i.e., videos rendered from the reconstructed 3D scene under target camera trajectories) to enable spatial consistency. However, even state-of-theart 3D reconstruction models struggle to maintain accurate global geometry under dense, long-horizon views, making precise cross-view alignment difficult. Small pose or depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views (Figure 1(a) middle artifacts); when fused, these inconsistencies introduce structured noise into rendered anchor videos, leading to ghosting or drift artifacts that degrade generation quality (e.g., inconsistent or hallucinated content in Figure 1(b)). We propose AnchorWeave, 3D-informed, memory-aware video generation framework that avoids relying on globally fused scene representation. Our key insight is that cross-view misalignment artifacts can be mitigated by replacing single global 3D memory with per-frame local point cloud memories, as illustrated in Figure 1(c). Since AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 1. Global 3D reconstruction accumulates cross-view misalignment, introducing artifacts in the reconstructed geometry ((a), middle), which propagate to the generated video as hallucinations ((b), red boxes). In contrast, per-frame local geometry inherently avoids cross-view misalignment and therefore remains clean ((a), right). Conditioning on multiple retrieved local geometric anchors, AnchorWeave maintains strong spatial consistency with the historical frames ((c), white and green boxes). local point clouds do not accumulate ghosting or drift from multi-view fusion, they provide cleaner geometric signals for rendering-based conditioning. While these rendered anchors may still exhibit residual misalignment, we treat their reconciliation during generation as learnable problem and resolve it through learned multi-anchor weaving module. Specifically, given sequence of previously generated or observed frames, AnchorWeave estimates and maintains perframe local geometry as spatial memory for long-horizon generation. To synthesize new content under target camera trajectory, AnchorWeave introduces coverage-driven memory retrieval formulation that iteratively selects the local memory that maximizes additional visibility coverage along the target trajectory. The selected memories are rendered as multiple anchor videos for conditioning. multi-anchor weaving controller with shared cross-anchor attention and pose-guided fusion then weaves these anchors into unified control signal, enabling effective multi-anchor conditioning while mitigating residual geometric inconsistencies. The newly generated frames are updated to the history for subsequent generation, enabling AnchorWeave to iteratively extend the video through the updateretrievegenerate loop. We compare AnchorWeave with multiple baselines spanning different memory conditioning paradigms, including global point cloud and implicit cross-view conditioning, on RealEstate10K and DL3DV under history-conditioned video generation setting. AnchorWeave significantly improves visual quality and long-horizon scene consistency, while generalizing well to open-domain images and scenes. Ablation studies further validate the contribution of each component, including local geometric memory, coveragedriven retrieval, and the geometry-conditioned multi-anchor controller, supporting the effectiveness of conditioning on local geometry and learning to handle misalignment. 2. Related Work Camera-controllable video generation. Recent advances in camera-controllable video generation span text-to-video (T2V), image-to-video (I2V), and video-to-video (V2V) settings. Early approaches primarily inject explicit camera parameterizations (e.g., Plucker embeddings or pose tokens) 2 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories into video diffusion models for conditioning (Wang et al., 2024b; Hou et al., 2024b; Bahmani et al., 2024b;a; Sun et al., 2024; He et al., 2025b; Zheng et al., 2024; Xu et al., 2024; Watson et al., 2024; Yu et al., 2025; Li et al., 2025c; He et al., 2025a; Zhou et al., 2025a; Li et al., 2024). To improve geometric faithfulness, subsequent works introduce structured 3D guidance, such as conditioning on rendered point clouds or anchor videos (Yu et al., 2024b; Popov et al., 2025; Hou et al., 2024a; Ren et al., 2025; Zheng et al., 2025; Seo et al., 2024; Cao et al., 2025; Muller et al., 2024; Liu et al., 2024; Zhang et al., 2024; 2025c; Zhou et al., 2024; Yang et al., 2025b; Bernal-Berdun et al., 2025). Another line of research formulates camera control through trajectory tracking or motion encoding as intermediate guidance (Jin et al., 2025; Feng et al., 2024; Xiao et al., 2024; Gu et al., 2025b). In the V2V regime, some methods rely on testtime optimization or per-scene fine-tuning (You et al., 2024; Zhang et al., 2024), while others construct large-scale paired data from simulators such as Unreal Engine5, Kubric, or Animated Objaverse (Bai et al., 2025a;b; Greff et al., 2022; Van Hoorick et al., 2024; Deitke et al., 2023; Wu et al., 2025b; Gao et al., 2024; Yu et al., 2024a; Wang et al., 2024a). More recent approaches incorporate structured 3D priors for controllable V2V generation (Bian et al., 2025; YU et al., 2025). Despite substantial progress, existing cameracontrollable video models are typically memoryless: they condition on per-frame parameters, rendered anchors, or short local context, but lack an explicit, robust memory mechanism to maintain long-horizon spatial consistency under complex camera motions. In contrast, we focus on equipping camera control models with geometry-aware and retrieval-based memory design, enabling robust longhorizon consistency and controllability beyond single-shot or short-context conditioning. Memory-augmented video generation. Recent video world models extend long-horizon consistency by incorporating memory beyond the limited context window of current generators. One line of work adopts retrieval-based mechanisms, treating past frames as external memory and retrieving relevant history for future synthesis (Yu et al., 2025; Li et al., 2025b; Xiao et al., 2025; Gu et al., 2025a; Guo et al., 2024). Another line preserves spatial structure through explicit 3D memory constructed from historical inputs, such as maintaining global point clouds to support revisit consistency and camera control (Zhou et al., 2025b; Liu et al., 2025b; Zhao et al., 2025; Wu et al., 2025c;a; Li et al., 2025a). third direction leverages recurrent or state-space models to enhance long-horizon temporal modeling (Po et al., 2025; Savov et al., 2025; Zhang et al., 2025b; Dalal et al., 2025; Lee et al., 2025; Hong et al., 2024). More recently, several works improve long-term consistency by redesigning attention or key-value memory mechanisms in autoregressive or diffusion-based models (He et al., 2025c; Huang et al., 2025a; Yang et al., 2025a; Zhang et al., 2025a; Mao et al., 2025; Ye et al., 2025; Liu et al., 2025a; Sun et al., 2025; Hong et al., 2025). Among them, the closest are 3D-based approaches including SPMem (Wu et al., 2025c), Spatia (Zhao et al., 2025), MagicWorld (Li et al., 2025a) that rely on global 3D point cloud as memory. Instead of noisy global reconstruction, we represent memory as local geometric anchors and perform multi-anchor joint conditioning for more stable long-horizon consistency. 3. Methodology: AnchorWeave AnchorWeave is world-consistent video generation framework that leverages retrieved local spatial memories for long-horizon, camera-controlled generation. It builds upon standard video diffusion backbone (Sec. 3.1) augmented with explicit spatial memory. Instead of fusing observations into single global 3D memory (i.e., global point cloud), AnchorWeave maintains collection of per-frame local geometric memories, each represented as local point cloud with an associated camera pose (Sec. 3.2). Given this memory bank, relevant memories are selected for target camera trajectory using coverage-driven, 3D-aware retrieval strategy (Sec. 3.3) and rendered as anchor videos. These anchor videos are integrated into the diffusion process through Multi-anchor Weaving Controller, which jointly reasons over multiple anchors via shared attention and pose-guided fusion (Sec. 3.4). Finally, following Wu et al. (2025c) that condition on global memory, AnchorWeave operates in an iterative updateretrievegenerate loop, where newly generated frames are converted into local geometric memories to support long-horizon and consistent generation (Sec. 3.5). 3.1. Preliminary: Video Diffusion Models We use standard DiT-based latent diffusion models (LDMs) (Rombach et al., 2022) as backbone. Specifically, given an RGB video with frames, RF 3HW , pretrained video autoencoder first compresses the input into lower-dimensional latent tensor: = E(x) RF CH , where and denote the spatially downsampled resolution. To enable generative modeling, noise is injected into the latent representation through predefined scheduling strategy, such as DDPM (Ho et al., 2020) or Flow Matching (Lipman et al., 2023), producing noisy latent zt at timestep t. conditional diffusion network θ(zt, t, ctext/img) is then trained to recover the underlying clean signal by reversing this corruption process, where the model is conditioned on the diffusion timestep and auxiliary inputs, including textual descriptions ctext for text-to-video generation, and extra camera intrinsics and anchor videos ccam for video generation with camera control. Training is performed by minimizing the following objective: LLDM = Ez, ϵN (0,I), ϵ ϵθ(zt, t, ctext+cam)2 2 (1) AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 2. Coverage-driven memory retrieval pipeline. Given target camera, we first select local memories whose camera FoVs partially overlap with the target camera view to form candidate memory pool. At each retrieval step, we greedily select the memory that maximizes the newly covered visible area. Points invisible to the target camera are shown in gray . SiMj denotes memory selected at retrieval step i, and the red box indicates the retrieved memory. In Si-Mj, regions already covered by previously retrieved memories are highlighted in green and only newly covered regions retain their original RGB colors. No green regions appear in S1-Mj since 1st-steps coverage is empty. Retrieval terminates when the uncovered region is 0%, the retrieval budget is exhausted, or the remaining memory pool is empty. For clarity, coverage is computed with single frame here, while in practice is aggregated over multiple frames per chunk. where ϵ represents the sampled Gaussian perturbation and ϵθ denotes the noise estimate produced by the model. This objective is used to train all diffusion components. for target camera trajectory. We illustrate the retrieval pipeline in Figure 2 and provide the complete retrieval procedure in algorithm in Appendix 1. 3.2. Local Geometric Memory Construction We represent spatial memory as set of per-frame local point clouds rather than single global 3D model, thereby avoiding the accumulation and propagation of crossview misalignment artifacts. The construction proceeds as follows. We assume access to history context consisting of previously observed video frames, which can come from real-world inputs or earlier generated segments. These frames serve as the visual context from which spatial memory is constructed and updated throughout long-horizon generation. Using this history context, we estimate perframe local geometry and camera pose with pretrained 3D reconstruction model (e.g., TTT3R (Chen et al., 2025c)). Each frames memory is represented as local point cloud together with its camera pose, which is transformed into shared world coordinate system and stored as an independent spatial memory entry. This update can be performed incrementally only for newly generated frames in practice. 3.3. Coverage-driven Memory Retrieval With large pool of history local memories, we aim to retrieve compact subset that can effectively guide generation Retrieval pipeline. Given target camera trajectory of length for the next video segment, we divide it into /D temporal chunks, each spanning frames, and perform memory retrieval independently for each chunk. We formulate memory retrieval as visibility coverage maximization problem: the selected local memories should jointly maximize coverage along the target camera trajectory within each chunk. Specifically, as shown in the upper part of Figure 2, for each chunk, we first construct candidate local memory pool using coarse field-of-view (FoV) overlap test to filter out clearly irrelevant local memories. Among the remaining candidates, we iteratively select the local memory that provides the largest new coverage of the currently uncovered region along the chunk trajectory, shown in the bottom part of Figure 2, Retrieval stops when the visible region is fully covered, the candidate pool is exhausted, or the number of retrieved memories reaches the maximum K. This greedy strategy yields compact and complementary set of local memories per chunk, avoiding redundant memories that offer little additional guidance. Anchor-video construction and pose computation. For each chunk, we retrieve up to local memories. For each retrieved memory, we render anchor views along the target 4 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 3. Architecture of multi-anchor weaving controller. Anchors are encoded and jointly processed by shared attention block, followed by camera-pose-guided fusion to produce unified control signal injected into the backbone model. Camera 1 to represent the retrieved-to-target camera poses for the 1 to anchor videos, where each denotes the relative pose between the camera associated with retrieved local point cloud and the target camera, measuring their viewpoint proximity. Camera 0 is the relative target camera trajectory. camera trajectory within the chunk, producing anchor clips of length frames. If fewer than memories are retrieved, we pad the remaining anchor slots with fully-invisible anchor clips to keep fixed number of anchors per chunk. For the rendered anchors, we compute the retrieved-to-target relative camera pose sequence, defined as the rigid transformations between the camera pose at which the local memory was captured and the target camera poses within the chunk. Anchor clips corresponding to the same retrieval index are then concatenated across chunks, resulting in anchor videos of length with temporally aligned retrieved-totarget pose trajectories. These anchor videos and their pose sequences are used as conditioning signals for generation. 3.4. Generation with Multi-Anchor Weaving Controller Given retrieved anchor videos, we condition generation on their latents together with retrieved-to-target relative camera poses, and the target camera pose, enabling the model to weave multiple anchor conditions into coherent and spatially consistent output video. Overall architecture. As shown in Figure 3, each retrieved anchor video is first encoded into latent features using the same 3D VAE as the backbone diffusion model. These anchor latents are then fed into multi-anchor controller, composed of stack of DiT-based ControlNet blocks that operate alongside the backbone model. At each denoising layer, we enable joint multi-anchor modeling through two key designs: (1) shared multi-anchor attention module that jointly processes all anchor features, and (2) camera-poseguided fusion module that aggregates anchor features into single control signal. The resulting fused control feature is injected into the corresponding layer of the video diffusion backbone to provide geometric guidance during generation. Details of these designs are presented below. Shared multi-anchor attention. Instead of processing anchors independently, we jointly process the anchor latents using shared-attention block, as shown in Figure 3 inside the control block. Concretely, we reshape the anchor latent sequences with shape La Ca into single sequence of shape 1 (K La) Ca (La denotes the number of latent tokens per anchor and Ca is the hidden dimension), and apply joint attention over the concatenated tokens. This joint attention enables information exchange across anchors, allowing the controller to aggregate complementary geometric evidence while suppressing contradictory or noisy signals, rather than committing to any single anchor rendering. Pose-aware importance estimation and fusion. Since not all anchors contribute equally to target view, we condition anchor fusion on the retrieved-to-target relative camera poses (Figure 3 right, camera 1 to K). At each control layer, the retrieved-to-target camera poses are encoded by lightweight learnable MLP to produce per-anchor impor5 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories tance weights, which reflect the geometric proximity between the target view and the cameras associated with the retrieved local point clouds used to render the anchors. Using these importance weights, we perform weighted sum over the anchor-conditioned features to obtain single fused control feature, which is then injected into the corresponding backbone block to provide geometric guidance. Target camera pose control. We further condition the model on the target camera trajectory (denoted as camera 0 in Figure 3) to handle cases where geometric memories provide limited guidance, such as early generation stages with rapid camera motion that moves the target view beyond all anchor visibility. The target camera pose is encoded by trainable camera encoder to produce camera embeddings, which are injected into the backbone DiT blocks as explicit camera-motion control signals. This complementary signal improves camera following under long-horizon generation. Overall, the multi-anchor weaving controller explicitly conditions generation on geometry through multi-view anchor attention, pose-conditioned fusion, and target camera pose control, transforming multiple imperfect anchor renderings into coherent conditioning signal. 3.5. Long-horizon Generation with Updated Memories Each iteration of the components described in Sec. 3.2Sec. 3.4 produces one video segment. Repeating this process enables long-horizon video generation via an updateretrievegenerate loop in AnchorWeave. (1) Update (Sec. 3.2): Starting from an initial frame or short segment, we estimate per-frame local geometry and initialize the spatial memory bank. (2) Retrieve (Sec. 3.3): Given target camera trajectory, we retrieve relevant local memories using our coverage-driven memory retrieval and condition the model on both the target pose and retrieved anchors. (3) Generate (Sec. 3.4): The backbone model generates the next video segment under the control of the multi-anchor geometry-aware controller. The newly generated frames are then added to the memory bank, enabling memoryaugmented generation over arbitrarily long trajectories. 4. Experiments 4.1. Experiment Setup Implementation details. We train AnchorWeave on subset of 10K videos sampled from RealEstate10K (Zhou et al., 2018) and DL3DV (Ling et al., 2024). For each video, we estimate per-frame local geometry using TTT3R (Chen et al., 2025c) and store the resulting point clouds as local spatial memories. During training, we use chunk length of = 8, retrieving memories once every 8 frames, and retrieve at most = 4 local point clouds for conditioning. To improve robustness, we randomly retrieve memory from the candidate local memory pool as form of condition augmentation. We apply AnchorWeave to two backbones, CogVideoX-I2V-5B (Yang et al., 2024) and Wan2.2-TI2V5B (Wan et al., 2025), both of which are based on DiT architecture. The multi-anchor weaving controller is injected into the first third of the backbone layers to balance capacity and efficiency, and is applied during the first 90% of denoising steps. We apply random frame masking to anchor videos during training to encourage robustness to missing or imperfect geometric guidance. The whole model is trained using the Adam optimizer with learning rate of 2 104 for 10K training steps and batch size of 8. Evaluation setup and dataset. We evaluate the models ability to condition on historical context under partialrevisit setting. We sample videos with large camera motion (leading to substantial viewpoint changes between frames) from RealEstate10K (Zhou et al., 2018) and DL3DV (Ling et al., 2024), collecting total of 500 videos for evaluation. For each video, we sample 70 frames, uniformly select 49 frames as the target segment, and use the remaining 21 frames as historical context available for retrieval during generation. This setting enables direct quantitative comparison against ground-truth frames and qualitative comparison under the same partial-revisit protocol. In addition, we include long-horizon qualitative examples using keyboardstyle camera actions starting from single open-domain initial frame, which go beyond the revisit setting and assess long-term controllability and scene consistency. We provide more low-level implementation details in Appendix Sec. A. Evaluation metrics. Following Wu et al. (2025c), we use PSNR and SSIM to measure reconstruction fidelity and consistency, computed on paired predicted and ground-truth frames. For perceptual quality, the VBench protocol (Huang et al., 2024) is adopted, including Subject Consistency, Background Consistency, Motion Smoothness, Temporal Flickering, Aesthetic Quality, and Imaging Quality. 4.2. Comparison with Prior Methods Baselines. We compare AnchorWeave with state-of-theart camera-controllable, memory-augmented video generation methods, including Gen3C (Ren et al., 2025), TrajCrafter (YU et al., 2025), ViewCrafter (Yu et al., 2024b), SEVA (Zhou et al., 2025a), EPiC (Wang et al., 2025), Context-as-Memory (Yu et al., 2025), and SPMem (Wu et al., 2025c). Gen3C, TrajCrafter, ViewCrafter, and EPiC are designed to condition generation on single anchor video. To provide strong baselines, we construct the anchor video for these methods using our retrieval strategy with K=1, where the retrieved local memory corresponds to the local point cloud that maximizes spatial coverage for each chunk. This allows these baselines to condition on the most informative retrieved local memory, in the same spirit 6 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Table 1. Quantitative results on RealEstate10K and DL3DV under partial-revisit evaluation. The best numbers are in bold and the second best numbers are underlined. The total quality score is the average of all quality dimensions. denotes our reimplementation based on CogVideoX with the same data as ours (due to not open-sourced). denotes the maximum retrieval per chunk (mentioned in Sec. 3.3). Quality Score () Consistency Score () Method ViewCrafter (Yu et al., 2024b) Gen3C (Ren et al., 2025) TrajCrafter (YU et al., 2025) EPiC (Wang et al., 2025) Context-as-Memory (Yu et al., 2025) SPMem (Wu et al., 2025c) SEVA (Zhou et al., 2025a) Total 78.58 77.11 76.34 77.77 78.07 76.85 79.66 AnchorWeave (Ours, CogVideoX-5B, K=1) 80.07 80.30 AnchorWeave (Ours, CogVideoX-5B) AnchorWeave (Ours, Wan2.2-5B, K=1) 80.76 80.98 AnchorWeave (Ours, Wan2.2-5B) Subject Bg Consist Consist Smooth Motion Temporal Aesthetic Flicker Imaging Quality Quality PSNR SSIM 85.50 84.22 83.56 85.22 85.08 84.52 87.64 87.70 87.89 88.05 88.25 90.11 89.08 88.94 90.08 89.94 89.31 92. 92.51 92.34 92.78 92.60 90.91 93.74 91.43 94.74 92.36 91.84 94.31 96.33 96.21 96.58 96.45 91.36 90.43 88.06 90.43 90.21 89.67 91.00 93.05 93.90 93.32 94.15 46.71 45.25 45.85 45.25 46.97 45.61 48. 47.88 47.87 49.40 49.35 66.92 59.92 60.19 60.92 63.88 60.12 63.78 62.94 63.61 64.45 65.10 16.17 18.23 16.96 17.42 17.91 17.25 21.13 19.01 20.96 19.60 21.04 0.5240 0.5926 0.5767 0.5704 0.5884 0.5710 0. 0.6145 0.6727 0.6180 0.6739 Table 2. Results of conditioning on global or local Memory. Method AnchorWeave w/ global point cloud AnchorWeave w/ local point clouds (ours) PSNR SSIM 16.31 20.96 53.45 67.27 as our method. We additionally include single-anchor variant of AnchorWeave (K = 1, one retrieval per chunk) to directly compare with these baselines under identical conditioning settings. Context-as-Memory and SEVA directly take multi-view history as conditions for memory consistency, while SPMem conditions generation on global point cloud renderings together with selected key historical frames. Note that, as Context-as-Memory and SPMem are not open-sourced, we reimplement them on the CogVideoX backbone using the same training data as our approach for fair comparison. We provide additional implementation details in Appendix Sec. A. Quantitative comparison. Table 1 shows that AnchorWeave achieves the best overall performance across both reconstruction and perceptual quality metrics. While AnchorWeave and SEVA (Zhou et al., 2025a) obtain comparable consistency scores, AnchorWeave exhibits significantly better temporal quality, including smoother motion and reduced flickering, benefiting from preserving the pretrained video backbones motion prior. Compared to single-anchor baselines, our single-anchor variant (K = 1) consistently achieves higher quality and consistency, indicating that our design more effectively extracts fine-grained geometric details from anchor conditioning and corrects residual errors. Our method also consistently surpasses explicit memoryaugmented baselines, including Context-as-Memory and SPMem. Moreover, the full AnchorWeave further improves consistency over its = 1 variant, demonstrating the effectiveness of multi-anchor conditioning to improve consistency. Notably, AnchorWeave already outperforms all baseTable 3. Effect of number of retrievals. We vary the number of retrievals per chunk, resulting in at most 6K retrieved frames (i.e., 6, 12, and 24 for K=1, 2, 4, respectively). Retrieved frames are not necessarily unique across chunks and duplicates may occur. Max. # retrieved frames PSNR SSIM 6 12 19.01 20.01 20.96 0.6145 0.6435 0.6727 lines when built on the CogVideoX backbone, and replacing CogVideoX with Wan2.2 leads to further performance gains. Qualitative comparison. Figure 4 shows qualitative comparisons under the partial-revisit setting. We present one representative frame for each case. Additional qualitative comparisons under free-form, keyboard-controlled camera trajectories with historical context are provided in Appendix Sec. E. Overall, AnchorWeave maintains more consistent scene structure when revisiting, while baseline methods exhibit visible artifacts or misaligned details. Gen3C, EPiC, and TrajCrafter often produce frames with degraded visual quality, including noticeable blurring or structural distortions. ViewCrafter is prone to hallucinated content (2nd/3rd example), while SEVA (Zhou et al., 2025a) occasionally fails to preserve fine-grained details (see highlighted red-box regions). Context-as-Memory maintains content and scene consistency but lacks precise camera control, often resulting in noticeable deviations from the GT viewpoint. SPMem conditions on rendered global point clouds, which lead to severe hallucinations that remain difficult to mitigate even with additional key-frame conditioning. In contrast, AnchorWeave preserves structural details more reliably through dense, clean local memory conditioning. The single-anchor variant (K = 1) sometimes exhibits misalignment due to limited retrieval (see red boxes), whereas the full model achieves stronger consistency by weaving multiple anchors. Overall, AnchorWeave produces results that are visually closer to the ground truth. 7 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 4. Qualitative comparison with baselines on DL3DV. K=1 means one retrieval per chunk. Baseline methods suffer from spatial drift and inconsistency in details. In contrast, AnchorWeave (ours) maintains consistency for multiple cases. GT and the historical context are shown for reference. For clarity, representative misaligned regions are highlighted with red boxes for strong baselines (e.g., SEVA). 4.3. Ablations and Analyses We conduct series of ablations to analyze the design choices of AnchorWeave, including the use of global vs. local 3D memory, pose-conditioned anchor fusion, joint vs. separate attention in the controller, and the number of retrieved frames. We also provide detailed visualization of AnchorWeave pipeline (retrieve render generate) in Appendix Sec. to further illustrate the full process. Effect of global vs. local 3D memory. We compare global and local 3D memories as conditioning signals for training the controller. In the global-memory setting, we render single anchor from the fused global 3D memory, while in the local-memory setting, multiple anchors are rendered from local memories as conditions. The results in Table 2 show that local 3D memory yields improvements in both PSNR and SSIM, confirming the advantage of local geometry for memory conditioning. Effect of pose-conditioned anchor fusion. As shown in Figure 5(a), simply averaging multiple retrieved anchor views leads to visible artifacts in the generated video when the anchors exhibit different pose deviations from the target view. In contrast, pose-conditioned fusion assigns higher weights to anchors with smaller pose discrepancies, effectively suppressing misaligned anchors and improving visual quality. This result demonstrates that explicitly accounting for retrieved-to-target pose differences is crucial for robust multi-anchor conditioning. Effect of joint vs. separate attention in the controller. Figure 5(b) compares the proposed joint-attention-based controller with baseline that processes each retrieved anchor independently using separate attention blocks. We observe that separate attention leads to blurred or inconsistent structures due to the absence of cross-anchor information exchange. In contrast, joint attention allows the model to aggregate complementary evidence across anchors, resulting in sharper and more coherent geometry in generated frames. Effect of the number of retrieved frames. Table 3 studies the impact of the number of retrieved frames by varying the 8 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 5. Ablation study. (a) Pose-guided fusion suppresses misaligned anchors and reduces artifacts compared to simple averaging. (b) Joint attention outperforms separate attention, enabling coherent multi-anchor aggregation. Figure 6. Long-horizon world exploration examples with keyboard-controlled camera actions on open-domain images. AnchorWeave preserves consistent object geometry and appearance across frames (highlighted by dashed boxes of the same color). number of retrievals per chunk. Increasing the number of retrieved frames consistently improves PSNR and SSIM, indicating that additional anchors provide complementary spatial evidence for generation. This trend is also reflected in the qualitative comparisons in Figure 4, where more anchors leads to reduced misalignment and improved consistency. 4.4. Open-Domain Long-Horizon Examples We present three open-domain long-horizon exploration examples in Figure 6 using AnchorWeave with Wan2.2 backbone. The model generalizes effectively to diverse unseen scenarios, such as wooden cabin, an alley, and wooden sailing ship. While the visual backbone is limited to 81frame video, AnchorWeave enables generation beyond this limit by conditioning each new segment on accumulated memory. As result, spatial consistency is preserved across multiple segments, demonstrating the effectiveness of AnchorWeave for world-consistent video generation. We also provide additional long-horizon world exploration examples in Figure 12, highlighting long-term consistency, 360 9 panoramic scene generation, and generalization to thirdperson gaming scenarios. 5. Conclusion We present AnchorWeave, geometry-informed, memoryaware video generation framework that improves longhorizon generation by replacing brittle global 3D reconstruction with multiple clean, view-aligned local geometric memories. Through coverage-driven retrieval and adaptive multi-anchor fusion, AnchorWeave provides robust spatial guidance and significantly improves visual quality, camera controllability, and long-term consistency across diverse scenes and tasks. Our results show that conditioning on local geometric memories and learning to reconcile their inconsistencies offers reliable alternative to global geometric fusion for scalable long-horizon video generation. AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories"
        },
        {
            "title": "Acknowledgment",
            "content": "This work was supported by ONR Grant N00014-23-12356, ARO Award W911NF2110220, DARPA ECOLE Program No. HR00112390060, NSF-AI Engage Institute DRL2112635, and Microsoft Accelerating AI Academic Research (AARI) program. The views contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "Bahmani, S., Skorokhodov, I., Qian, G., Siarohin, A., Menapace, W., Tagliasacchi, A., Lindell, D. B., and Tulyakov, S. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. arXiv preprint arXiv:2411.18673, 2024a. Bahmani, S., Skorokhodov, I., Siarohin, A., Menapace, W., Qian, G., Vasilkovsky, M., Lee, H.-Y., Wang, C., Zou, J., Tagliasacchi, A., et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024b. Bai, J., Xia, M., Fu, X., Wang, X., Mu, L., Cao, J., Liu, Z., Hu, H., Bai, X., Wan, P., et al. Recammaster: Cameracontrolled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025a. Bai, J., Xia, M., Wang, X., Yuan, Z., Fu, X., Liu, Z., Hu, H., Wan, P., and Zhang, D. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. Proc. ICLR, 2025b. Ball, P. J., Bauer, J., Belletti, F., Brownfield, B., Ephrat, A., Fruchter, S., Gupta, A., Holsheimer, K., Holynski, A., Hron, J., Kaplanis, C., Limont, M., McGill, M., Oliveira, Y., ParkerHolder, J., Perbet, F., Scully, G., Shar, J., Spencer, S., Tov, O., Villegas, R., Wang, E., Yung, J., Baetu, C., Berbel, J., Bridson, D., Bruce, J., Buttimore, G., Chakera, S., Chandra, B., Collins, P., Cullum, A., Damoc, B., Dasagi, V., Gazeau, M., Gbadamosi, C., Han, W., Hirst, E., Kachra, A., Kerley, L., Kjems, K., Knoepfel, E., Koriakin, V., Lo, J., Lu, C., Mehring, Z., Moufarek, A., Nandwani, H., Oliveira, V., Pardo, F., Park, J., Pierson, A., Poole, B., Ran, H., Salimans, T., Sanchez, M., Saprykin, I., Shen, A., Sidhwani, S., Smith, D., Stanton, J., Tomlinson, H., Vijaykumar, D., Wang, L., Wingfield, P., Wong, N., Xu, K., Yew, C., Young, N., Zubov, V., Eck, D., Erhan, D., Kavukcuoglu, K., Hassabis, D., Gharamani, Z., Hadsell, R., van den Oord, A., Mosseri, I., Bolton, A., Singh, S., and Rocktaschel, T. Genie 3: new frontier for world models. 2025. Bernal-Berdun, E., Serrano, A., Masia, B., Gadelha, M., HoldGeoffroy, Y., Sun, X., and Gutierrez, D. Precisecam: Precise camera control for text-to-image generation. arXiv preprint arXiv:2501.12910, 2025. Bian, W., Huang, Z., Shi, X., Li, Y., Wang, F.-Y., and Li, H. Gs-dit: Advancing video generation with pseudo 4d gaussian fields through efficient dense 3d point tracking. arXiv preprint arXiv:2501.02690, 2025. Bruce, J., Dennis, M. D., Edwards, A., Parker-Holder, J., Shi, Y., Hughes, E., Lai, M., Mavalankar, A., Steigerwald, R., Apps, C., et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Cao, C., Zhou, J., Li, S., Liang, J., Yu, C., Wang, F., Xue, X., and Fu, Y. Uni3c: Unifying precisely 3d-enhanced camera and human motion controls for video generation. arXiv preprint arXiv:2504.14899, 2025. Chen, B., Monso, D. M., Du, Y., Simchowitz, M., Tedrake, R., and Sitzmann, V. Diffusion forcing: Next-token prediction In The Thirty-eighth Annual meets full-sequence diffusion. Conference on Neural Information Processing Systems, 2025a. Chen, T., Hu, X., Ding, Z., and Jin, C. Learning world models for interactive video generation. arXiv preprint arXiv:2505.21996, 2025b. Chen, X., Chen, Y., Xiu, Y., Geiger, A., and Chen, A. Ttt3r: 3d reconstruction as test-time training. arXiv preprint arXiv:2509.26645, 2025c. Dalal, K., Koceja, D., Xu, J., Zhao, Y., Han, S., Cheung, K. C., Kautz, J., Choi, Y., Sun, Y., and Wang, X. One-minute video generation with test-time training. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17702 17711, 2025. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E., Schmidt, L., Ehsani, K., Kembhavi, A., and Farhadi, A. Objaverse: universe of annotated 3d objects. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1314213153, 2023. Feng, W., Liu, J., Tu, P., Qi, T., Sun, M., Ma, T., Zhao, S., Zhou, S., and He, Q. I2vcontrol-camera: Precise video camera control with adjustable motion strength. arXiv preprint arXiv:2411.06525, 2024. Gao, R., Holynski, A., Henzler, P., Brussee, A., Martin-Brualla, R., Srinivasan, P., Barron, J. T., and Poole, B. Cat3d: Create In Proc. anything in 3d with multi-view diffusion models. NeurIPS, 2024. Greff, K., Belletti, F., Beyer, L., Doersch, C., Du, Y., Duckworth, D., Fleet, D. J., Gnanapragasam, D., Golemo, F., Herrmann, C., et al. Kubric: scalable dataset generator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 37493761, 2022. Gu, Y., Mao, W., and Shou, M. Z. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025a. Gu, Z., Yan, R., Lu, J., Li, P., Dou, Z., Si, C., Dong, Z., Liu, Q., Lin, C., Liu, Z., et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025b. Guo, X., Ding, C., Dou, H., Zhang, X., Tang, W., and Wu, W. Infinitydrive: Breaking time limits in driving world models. arXiv preprint arXiv:2412.01522, 2024. Guo, Y., Shi, L. X., Chen, J., and Finn, C. Ctrl-world: controllable generative world model for robot manipulation. arXiv preprint arXiv:2510.10125, 2025. HaCohen, Y., Brazowski, B., Chiprut, N., Bitterman, Y., Kvochko, A., Berkowitz, A., Shalem, D., Lifschitz, D., Moshe, D., Porat, E., et al. Ltx-2: Efficient joint audio-visual foundation model. arXiv preprint arXiv:2601.03233, 2026. He, H., Xu, Y., Guo, Y., Wetzstein, G., Dai, B., Li, H., and Yang, C. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 10 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories He, H., Xu, Y., Guo, Y., Wetzstein, G., Dai, B., Li, H., and Yang, C. Cameractrl: Enabling camera control for video diffusion models. In The Thirteenth International Conference on Learning Representations, 2025a. Li, L., Zhang, Z., Li, Y., Xu, J., Hu, W., Li, X., Cheng, W., Gu, J., Xue, T., and Shan, Y. Nvcomposer: Boosting generative novel view synthesis with multiple sparse and unposed images. arXiv preprint arXiv:2412.03517, 2024. He, H., Yang, C., Lin, S., Xu, Y., Wei, M., Gui, L., Zhao, Q., Wetzstein, G., Jiang, L., and Li, H. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025b. He, X., Peng, C., Liu, Z., Wang, B., Zhang, Y., Cui, Q., Kang, F., Jiang, B., An, M., Ren, Y., et al. Matrix-game 2.0: An open-source real-time and streaming interactive world model. arXiv preprint arXiv:2508.13009, 2025c. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hong, Y., Liu, B., Wu, M., Zhai, Y., Chang, K.-W., Li, L., Lin, K., Lin, C.-C., Wang, J., Yang, Z., et al. Slowfast-vgen: Slow-fast learning for action-driven long video generation. arXiv preprint arXiv:2410.23277, 2024. Hong, Y., Mei, Y., Ge, C., Xu, Y., Zhou, Y., Bi, S., Hold-Geoffroy, Y., Roberts, M., Fisher, M., Shechtman, E., et al. Relic: Interactive video world model with long-horizon memory. arXiv preprint arXiv:2512.04040, 2025. Hou, C., Wei, G., Zeng, Y., and Chen, Z. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024a. Hou, Y., Zheng, L., and Torr, P. Learning camera movement control from real-world drone videos. arXiv preprint arXiv:2412.09620, 2024b. Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025a. Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025b. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Jin, W., Dai, Q., Luo, C., Baek, S.-H., and Cho, S. Flovd: Optical flow meets video diffusion model for enhanced cameracontrolled video synthesis. arXiv preprint arXiv:2502.08244, 2025. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Lee, J.-H., Lin, B.-J., Sun, W.-F., and Lee, C.-Y. Enhancing memory and imagination consistency in diffusion-based world models via linear-time sequence modeling. arXiv e-prints, pp. arXiv2502, 2025. Li, G., Zheng, S., Xu, S., Chen, J., Li, B., Hu, X., Zhao, L., and Jiang, P.-T. Magicworld: Interactive geometry-driven video world exploration. arXiv preprint arXiv:2511.18886, 2025a. Li, R., Torr, P., Vedaldi, A., and Jakab, T. Vmem: Consistent interactive video scene generation with surfel-indexed view memory. arXiv preprint arXiv:2506.18903, 2025b. Li, T., Zheng, G., Jiang, R., Wu, T., Lu, Y., Lin, Y., Li, X., et al. Realcam-i2v: Real-world image-to-video generation arXiv preprint with interactive complex camera control. arXiv:2502.10059, 2025c. Ling, L., Sheng, Y., Tu, Z., Zhao, W., Xin, C., Wan, K., Yu, L., Guo, Q., Yu, Z., Lu, Y., et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2216022169, 2024. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. Liu, F., Sun, W., Wang, H., Wang, Y., Sun, H., Ye, J., Zhang, J., and Duan, Y. ReconX: reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024. Liu, K., Hu, W., Xu, J., Shan, Y., and Lu, S. Rolling forcing: Autoregressive long video diffusion in real time. arXiv preprint arXiv:2509.25161, 2025a. Liu, P., Guo, Z., Warke, M., Chintala, S., Paxton, C., Shafiullah, N. M. M., and Pinto, L. Dynamem: Online dynamic spatiosemantic memory for open world mobile manipulation. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 1334613355. IEEE, 2025b. Mao, X., Li, Z., Li, C., Xu, X., Ying, K., He, T., Pang, J., Qiao, Y., and Zhang, K. Yume-1.5: text-controlled interactive world generation model. arXiv preprint arXiv:2512.22096, 2025. Muller, N., Schwarz, K., Rossle, B., Porzi, L., Bul`o, S. R., Nießner, M., and Kontschieder, P. Multidiff: Consistent novel view synthesis from single image. In Proc. CVPR, 2024. Parker-Holder, J., Ball, P., Bruce, J., Dasagi, V., Holsheimer, K., Kaplanis, C., Moufarek, A., Scully, G., Shar, J., Shi, J., Spencer, S., Yung, J., Dennis, M., Kenjeyev, S., Long, S., Mnih, V., Chan, H., Gazeau, M., Li, B., Pardo, F., Wang, L., Zhang, L., Besse, F., Harley, T., Mitenkova, A., Wang, J., Clune, J., Hassabis, D., Hadsell, R., Bolton, A., Singh, S., and Rocktaschel, T. Genie 2: large-scale foundation world model. 2024. URL https://deepmind.google/discover/blog/ genie-2-a-large-scale-foundation-world-model/. Po, R., Nitzan, Y., Zhang, R., Chen, B., Dao, T., Shechtman, E., Wetzstein, G., and Huang, X. Long-context state-space video world models. arXiv preprint arXiv:2505.20171, 2025. Popov, S., Raj, A., Krainin, M., Li, Y., Freeman, W. T., and Rubinstein, M. Camctrl3d: Single-image scene exploration with precise 3d camera control. arXiv preprint arXiv:2501.06006, 2025. 11 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Ren, X., Shen, T., Huang, J., Ling, H., Lu, Y., Nimier-David, M., Muller, T., Keller, A., Fidler, S., and Gao, J. Gen3c: 3dinformed world-consistent video generation with precise camera control. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 61216132, 2025. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Savov, N., Kazemi, N., Zhang, D., Paudel, D. P., Wang, X., and Van Gool, L. Statespacediffuser: Bringing long context to diffusion world models. arXiv preprint arXiv:2505.22246, 2025. Seo, J., Fukuda, K., Shibuya, T., Narihira, T., Murata, N., Hu, S., Lai, C.-H., Kim, S., and Mitsufuji, Y. Genwarp: Single image to novel views with semantic-preserving generative warping. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Sun, W., Chen, S., Liu, F., Chen, Z., Duan, Y., Zhang, J., and Wang, Y. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. Sun, W., Zhang, H., Wang, H., Wu, J., Wang, Z., Wang, Z., Wang, Y., Zhang, J., Wang, T., and Guo, C. Worldplay: Towards long-term geometric consistency for real-time interactive world modeling. arXiv preprint arXiv:2512.14614, 2025. Van Hoorick, B., Wu, R., Ozguroglu, E., Sargent, K., Liu, R., Tokmakov, P., Dave, A., Zheng, C., and Vondrick, C. Generative camera dolly: Extreme monocular dynamic novel view synthesis. In European Conference on Computer Vision, pp. 313331. Springer, 2024. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Wang, C., Zhuang, P., Ngo, T. D., Menapace, W., Siarohin, A., Vasilkovsky, M., Skorokhodov, I., Tulyakov, S., Wonka, P., and Lee, H.-Y. 4real-video: Learning generalizable photo-realistic 4d video diffusion. arXiv preprint arXiv:2412.04462, 2024a. Wu, H., Wu, D., He, T., Guo, J., Ye, Y., Duan, Y., and Bian, J. Geometry forcing: Marrying video diffusion and 3d representation for consistent world modeling. arXiv preprint arXiv:2507.07982, 2025a. Wu, R., Gao, R., Poole, B., Trevithick, A., Zheng, C., Barron, J. T., and Holynski, A. Cat4d: Create anything in 4d with multi-view video diffusion models. Proc. CVPR, 2025b. Wu, T., Yang, S., Po, R., Xu, Y., Liu, Z., Lin, D., and Wetzstein, G. Video world models with long-term spatial memory, 2025c. URL https://arxiv.org/abs/2506.05284. Xiao, Z., Ouyang, W., Zhou, Y., Yang, S., Yang, L., Si, J., and Pan, X. Trajectory attention for fine-grained video motion control. arXiv preprint arXiv:2411.19324, 2024. Xiao, Z., Lan, Y., Zhou, Y., Ouyang, W., Yang, S., Zeng, Y., and Pan, X. Worldmem: Long-term consistent world simulation with memory. arXiv preprint arXiv:2504.12369, 2025. Xu, D., Nie, W., Liu, C., Liu, S., Kautz, J., Wang, Z., and Vahdat, A. Camco: Camera-controllable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. Yang, S., Huang, W., Chu, R., Xiao, Y., Zhao, Y., Wang, X., Li, M., Xie, E., Chen, Y., Lu, Y., et al. Longlive: Real-time interactive long video generation. arXiv preprint arXiv:2509.22622, 2025a. Yang, X., Xu, J., Luan, K., Zhan, X., Qiu, H., Shi, S., Li, H., Yang, S., Zhang, L., Yu, C., et al. Omnicam: Unified multimodal video generation via camera control. arXiv preprint arXiv:2504.02312, 2025b. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Textto-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Ye, D., Zhou, F., Lv, J., Ma, J., Zhang, J., Lv, J., Li, J., Deng, M., Yang, M., Fu, Q., et al. Yan: Foundational interactive video generation. arXiv preprint arXiv:2508.08601, 2025. You, M., Zhu, Z., Liu, H., and Hou, J. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. arXiv preprint arXiv:2405.15364, 2024. Yu, H., Wang, C., Zhuang, P., Menapace, W., Siarohin, A., Cao, J., Jeni, L., Tulyakov, S., and Lee, H.-Y. 4real: Towards photorealistic 4d scene generation via video diffusion models. Advances in Neural Information Processing Systems, 37:4525645280, 2024a. Wang, Z., Yuan, Z., Wang, X., Li, Y., Chen, T., Xia, M., Luo, P., and Shan, Y. Motionctrl: unified and flexible motion In ACM SIGGRAPH 2024 controller for video generation. Conference Papers, pp. 111, 2024b. Yu, J., Bai, J., Qin, Y., Liu, Q., Wang, X., Wan, P., Zhang, D., and Liu, X. Context as memory: Scene-consistent interactive long video generation with memory retrieval. In Proceedings of the SIGGRAPH Asia 2025 Conference Papers, pp. 111, 2025. Wang, Z., Cho, J., Li, J., Lin, H., Yoon, J., Zhang, Y., and Bansal, M. Epic: Efficient video camera control learning with precise anchor-video guidance. arXiv preprint arXiv:2505.21876, 2025. YU, M., Hu, W., Xing, J., and Shan, Y. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. arXiv preprint arXiv:2503.05638, 2025. Watson, D., Saxena, S., Li, L., Tagliasacchi, A., and Fleet, D. J. Controlling space and time with diffusion models. In The Thirteenth International Conference on Learning Representations, 2024. Yu, W., Xing, J., Yuan, L., Hu, W., Li, X., Huang, Z., Gao, X., Wong, T.-T., Shan, Y., and Tian, Y. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024b. AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Yu, W., Yin, S., Easterbrook, S., and Garg, A. Egosim: Egocentric exploration in virtual worlds with multi-modal conditioning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=zAyS5aRKV8. Zhang, D. J., Paiss, R., Zada, S., Karnad, N., Jacobs, D. E., Pritch, Y., Mosseri, I., Shou, M. Z., Wadhwa, N., and Ruiz, N. Recapture: Generative video camera controls for userprovided videos using masked video fine-tuning. arXiv preprint arXiv:2411.05003, 2024. Zhang, L. and Agrawala, M. Packing input frame context in nextframe prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. Zhang, L., Cai, S., Li, M., Wetzstein, G., and Agrawala, M. Frame context packing and drift prevention in next-frame-prediction video diffusion models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025a. Zhang, T., Bi, S., Hong, Y., Zhang, K., Luan, F., Yang, S., Sunkavalli, K., Freeman, W. T., and Tan, H. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025b. Zhang, Z., Chen, D., and Liao, J. I2v3d: Controllable image-to-video generation with 3d guidance. arXiv preprint arXiv:2503.09733, 2025c. Zhao, J., Wei, F., Liu, Z., Zhang, H., Xu, C., and Lu, Y. Spatia: Video generation with updatable spatial memory. arXiv preprint arXiv:2512.15716, 2025. Zheng, G., Li, T., Jiang, R., Lu, Y., Wu, T., and Li, X. Cami2v: Camera-controlled image-to-video diffusion model. arXiv preprint arXiv:2410.15957, 2024. Zheng, S., Peng, Z., Zhou, Y., Zhu, Y., Xu, H., Huang, X., and Fu, Y. Vidcraft3: Camera, object, and lighting control for image-to-video generation. arXiv preprint arXiv:2502.07531, 2025. Zhou, J. J., Gao, H., Voleti, V., Vasishta, A., Yao, C.-H., Boss, M., Torr, P., Rupprecht, C., and Jampani, V. Stable virtual camera: Generative view synthesis with diffusion models. arXiv e-prints, pp. arXiv2503, 2025a. Zhou, S., Du, Y., Yang, Y., Han, L., Chen, P., Yeung, D.-Y., and Gan, C. Learning 3d persistent embodied world models. arXiv preprint arXiv:2505.05495, 2025b. Zhou, T., Tucker, R., Flynn, J., Fyffe, G., and Snavely, N. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. Zhou, Z., An, J., and Luo, J. Latent-reframe: Enabling camera control for video diffusion model without training. arXiv preprint arXiv:2412.06029, 2024. 13 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories A. Additional Implementation Details A.1. Training/Inference Details We train AnchorWeave on subset of 10K videos sampled from RealEstate10K (Zhou et al., 2018) and DL3DV (Ling et al., 2024). For each video, we estimate per-frame local geometry using TTT3R (Chen et al., 2025c) and store the resulting point clouds as local spatial memories. During training, we use chunk length of = 8, retrieving memories once every 8 frames, and retrieve at most = 4 local point clouds for conditioning. To improve robustness, we randomly sample from the candidate local memory pool as form of memory augmentation. We also apply random frame masking to anchor videos to enhance robustness to missing or imperfect geometric guidance. We apply AnchorWeave to two DiT-based backbones: CogVideoX-I2V-5B (Yang et al., 2024) and Wan2.2-TI2V-5B (Wan et al., 2025). For Wan2.2, we train on 81-frame videos, while for CogVideoX we use 49-frame videos. During training, the backbone weights are frozen and only the newly introduced multi-anchor weaving modules are optimized. The weaving controller is injected into the first third of backbone layers to balance capacity and efficiency, and is applied during the first 80% of denoising steps. The backbone latent receives two control signals: (1) fused anchor latents and (2) target camera pose embeddings. Both control signals use control weight of 1.0 (i.e., added with equal weight to the original backbone features). Classifier-free guidance (CFG) follows the default configuration of each backbone. We train using the Adam optimizer with learning rate of 2 104 for 10K steps and batch size of 8. Training is conducted on 8H100 GPUs and takes approximately one day. A.2. Baseline Re-implementation Context-as-Memory. We reimplement Context-as-Memory on top of the CogVideoX backbone for fair comparison. We use the same training data as AnchorWeave, sampled from RealEstate10K and DL3DV. For each video, we follow Context-as-Memory to select context frames with high trajectory overlap with the target segment. Specifically, we sample 12 context frames per video as conditioning inputs. We apply LoRA to fine-tune the entire backbone for multi-context conditioning, with LoRA rank 16 and α = 32. Training is conducted for 10K steps with batch size of 8, consistent with AnchorWeave. During inference, we select context frames with the highest overall trajectory overlap as conditioning inputs. SPMem. We reimplement SPMem using CogVideoX as the backbone and train on the same dataset as AnchorWeave (RealEstate10K and DL3DV). SPMem conditions generation on both (1) rendered global point clouds via ControlNet branch and (2) keyframe latents via additional cross-attention layers. We follow similar structure: the ControlNet branch is injected into the first third of backbone layers, and an additional cross-attention layer is inserted into each backbone block to condition on keyframe latents. Keyframes are selected using the same trajectory-overlap strategy as in Context-as-Memory. Global point clouds are constructed using TTT3R and rendered for conditioning. During training, we freeze the backbone and only fine-tune the newly introduced cross-attention layers and the ControlNet branch. The model is trained for 10K steps with batch size of 8, consistent with AnchorWeave. B. Coverage-Driven Memory Retrieval Algorithm We provide the detailed retrieval procedure in Algorithm 1. In practice, we enforce the first rendered video frame to be selected as the first retrieval. This follows the classical anchor-video convention, where the initial frame serves as stable geometric reference. C. Camera Controllability Comparison with Baselines We compare camera controllability with prior methods using rotation and translation errors, following the evaluation protocol of EPiC. We randomly sample 300 videos from both DL3DV and RealEstate10K and evaluate single-image-based camera control accuracy. For all methods, only the first frame is provided as input (Basic Image-to-Video Camera Control setting) without any other information. Thus, for our method, the conditioning signal is the anchor video rendered from the first frame, i.e., AnchorWeave (no retrieval). As shown in Table 4, our method achieves the lowest rotation and translation errors among all baselines. Notably, although 14 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Algorithm 1 Coverage-driven 3D-aware memory retrieval for anchor-video construction Input: Local point clouds {Pi} with poses {Ti}; target trajectory { ˆTt}M Output: Anchor videos and retrieved-to-target relative poses t=1; chunk length D; max anchors per chunk Partition target trajectory into ordered chunks {Cm}, each with target poses for each chunk Cm do Initialize anchor set Am {Platest} Candidate filtering: Retain Pi whose pose Ti has sufficient FOV overlap with chunk Cm while Am < and coverage improves do Select Pi that maximizes additional visible coverage when rendered to Cm Am Am {Pi} end while Compute retrieved-to-target relative poses for anchors in Am end for Concatenate rendered anchors across chunks to form anchor videos and corresponding retrieved-to-target relative poses Table 4. Camera control ability comparisons. Lower is better. Method Rotation Error () Translation Error () ViewCrafter (Yu et al., 2024b) Gen3C (Ren et al., 2025) TrajCrafter (YU et al., 2025) EPiC (Wang et al., 2025) SPMem (Wu et al., 2025c) TrajCrafter (YU et al., 2025) SEVA (Zhou et al., 2025a) AnchorWeave (Ours, no retrieval) 1.34 0.97 1.08 0.84 1.21 1.08 0.78 0.61 3.21 1.98 2.57 2.01 2.99 2.57 1.96 1. our model is trained to follow multiple anchors, it still outperforms prior approaches when only single anchor is provided at test time. This is because AnchorWeave combines anchor-video-based visual guidance with explicit camera pose control. When camera motion is large, the geometry rendered from the first frame quickly moves out of view, causing methods that rely solely on anchor videos (e.g., ViewCrafter, TrajCrafter, EPiC, and Gen3C, SPMem) to lose controllability. In contrast, our method can still follow the target camera trajectory using pose guidance, resulting in significantly more accurate camera motion. D. AnchorWeave Generation Process Visualization We visualize the end-to-end AnchorWeave generation process in Figure 7 and Figure 8. We start from historical context video sampled from DL3DV, and select one frame as the initial state. The camera trajectory is then specified by keyboardstyle control sequence (move left orient up move backward orient down move right, each for 8 steps), which defines an interactive exploration of the environment. To ensure long-horizon consistency with the history, AnchorWeave performs coverage-driven retrieval at each chunk: it retrieves multiple history frames that jointly cover different visible regions under the target view, renders their corresponding geometric anchors, and weaves these anchors into the generation. As shown in both examples, the retrieved frames cover complementary parts of the camera-visible area, and AnchorWeave can consistently fuse multiple anchors, producing frames that effectively merge information from different retrieved views. E. Qualitative Comparison with Baselines under Free-Form Camera Trajectory We further compare AnchorWeave with ViewCrafter, TrajCrafter, Gen3C, SEVA, Context-as-Memory, and SPMem under free-form, keyboard-controlled camera trajectory, as shown in Figures 9 to 11. Unlike the revisit setting where the camera replays the historical trajectory, here the motion follows free-form control sequence (e.g., move left orient up move backward orient down move right), which deviates from the context trajectory and typical smooth motions observed in training data. We only show key historical context frames that spatially overlap with the commanded camera AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories trajectory (Due to page limits, we sampled 7 frames per example). ViewCrafter frequently introduces noticeable hallucinated structures when the camera moves into previously unseen regions. TrajCrafter exhibits significant rendering artifacts and structural inconsistencies. Gen3C tends to produce overly blurred outputs under large viewpoint changes. SEVA maintains reasonable rendering quality in camera-visible regions but struggles to synthesize plausible content in newly exposed areas, leading to degraded details. Context-as-Memory shows limited generalization to such out-of-distribution camera trajectories, often generating frames that implicitly follow the historical trajectory rather than the commanded motion. SPMem suffers from severe hallucinations due to conditioning on imperfect global point cloud renderings. In contrast, AnchorWeave remains stable under free camera control. It faithfully preserves geometry in visible regions via anchor conditioning, while seamlessly synthesizing plausible content in newly exposed areas beyond the historical context. F. Long-Horizon Examples Figure 12 presents six additional long-horizon world exploration examples across diverse scenes, including both first-person and third-person settings. We adopt Wan2.2 as the backbone and iteratively perform image-to-video generation with AnchorWeave for 81 frames conditioned on the last frame of the previous segment, resulting in 241 frames in total. For dynamic third-person scenes, we mask out the character to exclude it from the point-cloud memory construction to enable dynamic content generation. Long-term consistency. Strong long-term consistency is observed across many examples. For instance, the table in the first scene, the chair and bridge in the third scene, the temple in the fourth scene, and the cabin in the fifth scene remain spatially consistent even after many frames and cross-segment viewpoint changes. When the camera revisits these regions, the objects still appear in coherent positions, demonstrating the effectiveness of AnchorWeave. 360 panoramic generation. The second example demonstrates full 360-degree rotation capability. Starting from vending machine and corridor, the camera gradually rotates over many frames and eventually returns to the original viewpoint. The scene content remains consistent throughout the rotation, indicating robust global spatial coherence. Third-person character control. The sixth example showcases third-person game scenario. By masking the character and leveraging point-cloud control, we enable stable character motion aligned with the camera trajectory. Long-term consistency is also preserved: for example, after the character walks away in the first segment, the cabin and distant mountains remain visible when revisiting the area in later segments. Notably, this example is generated in zero-shot manner. Our training data consist only of static-scene datasets (DL3DV and RealEstate10K), without any third-person character or dynamic gameplay supervision. The ability to generalize to charactercentric, long-horizon scenarios therefore demonstrates the robustness of our design and its strong out-of-distribution generalization capability. 16 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 7. Context-conditioned camera control generation process of AnchorWeave. 17 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 8. Context-conditioned camera control generation process of AnchorWeave. 18 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 9. Context-conditioned camera control results of AnchorWeave compared with baselines. 19 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 10. Context-conditioned camera control results of AnchorWeave compared with baselines. 20 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 11. Context-conditioned camera control results of AnchorWeave compared with baselines. 21 AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories Figure 12. Long-horizon video generation on both open-domain static and dynamic environments."
        }
    ],
    "affiliations": [
        "AI2",
        "Nanyang Technological University, Singapore",
        "University of North Carolina, Chapel Hill"
    ]
}