{
    "paper_title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR",
    "authors": [
        "Khalil Hennara",
        "Muhammad Hreden",
        "Mohamed Motasim Hamed",
        "Ahmad Bastati",
        "Zeina Aldallal",
        "Sara Chrouf",
        "Safwan AlModhayan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Arabic document OCR remains a challenging task due to the language's cursive script, diverse fonts, diacritics, and right-to-left orientation. While modern Multimodal Large Language Models (MLLMs) have advanced document understanding for high-resource languages, their performance on Arabic remains limited. In this work, we introduce Baseer, a vision-language model fine- tuned specifically for Arabic document OCR. Leveraging a large-scale dataset combining synthetic and real-world documents, Baseer is trained using a decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving general visual features. We also present Misraj-DocOCR, a high-quality, expert-verified benchmark designed for rigorous evaluation of Arabic OCR systems. Our experiments show that Baseer significantly outperforms existing open-source and commercial solutions, achieving a WER of 0.25 and establishing a new state-of-the-art in the domain of Arabic document OCR. Our results highlight the benefits of domain-specific adaptation of general-purpose MLLMs and establish a strong baseline for high-accuracy OCR on morphologically rich languages like Arabic."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 4 7 1 8 1 . 9 0 5 2 : r Baseer: Vision-Language Model for Arabic Document-to-Markdown OCR Khalil Hennara, Muhammad Hreden, Mohamed Motasim Hamed, Ahmad Bastati, Zeina Aldallal, Sara Chrouf, and Safwan AlModhayan Khobar, Saudi Arabia hennara,hreden,hamed,bastati,aldallal,chrouf,safwan@misraj.ai Abstract Arabic document OCR remains challenging task due to the languages cursive script, diverse fonts, diacritics, and right-to-left orientation. While modern Multimodal Large Language Models (MLLMs) have advanced document understanding for high-resource languages, their performance on Arabic remains limited. In this work, we introduce Baseer, vision-language model finetuned specifically for Arabic document OCR. Leveraging large-scale dataset combining synthetic and real-world documents, Baseer is trained using decoder-only fine-tuning strategy to adapt pre-trained MLLM while preserving general visual features. We also present Misraj-DocOCR, high-quality, expert-verified benchmark designed for rigorous evaluation of Arabic OCR systems. Our experiments show that Baseer significantly outperforms existing open-source and commercial solutions, achieving WER of 0.25 and establishing new state-of-the-art in the domain of Arabic document OCR. Our results highlight the benefits of domain-specific adaptation of general-purpose MLLMs and establish strong baseline for high-accuracy OCR on morphologically rich languages like Arabic."
        },
        {
            "title": "Introduction",
            "content": "The recent and rapid advancements in Multimodal Large Language Models (MLLMs) have fundamentally reshaped the landscape of how machines perceive and process complex visual and textual data Hurst et al. [2024]; Comanici et al. [2025]; Zhu et al. [2025]; Bai et al. [2025]. Among the myriad applications of these models, Optical Character Recognition (OCR) and comprehensive document understanding continue to present significant challenges. This is particularly true for languages that are morphologically rich and structurally complex, such as Arabic. While contemporary OCR solutions have achieved remarkable performance for English and other high-resource languages Comanici et al. [2025]; Hurst et al. [2024], their efficacy does not readily generalize to Arabic documents. The inherent complexities of Arabic script, including its cursive nature, extensive ligature formation, the wide variety of fonts and styles, the critical role of diacritics, and the right-to-left text orientation, render Arabic OCR task of considerable difficulty. In parallel, progress in multimodal architectures has paved the way for unified vision-language reasoning, which enables models to concurrently extract both textual content and structural inforBaseer: (cid:81)(cid:30)(cid:10)(cid:146)(cid:11)(cid:29)(cid:46): meaning one who sees clearly and insightful. The name reflects the models ability to see and interpret documents with clarity. 1 Figure 1: An overview of the data and training pipeline for Baseer. The process begins with hybrid dataset of 500k pairs (300k synthetic and 200k real-world), which is used to fine-tune the Qwen2.5-VL-3B-Instruct model. mation from documents Li et al. [2025]; Mandal et al. [2025]. Despite these technological strides, modern multimodal frameworks have seldom been specialized for the distinct demands of Arabic OCR and document parsing. This significant gap in research and development leaves academics, practitioners, and industries without robust, dedicated tools for processing real-world Arabic documents, which are prevalent across academic, commercial, and cultural heritage domains. In this work, we introduce Baseer, vision-language model meticulously fine-tuned for Arabic document OCR. Leveraging the state-of-the-art capabilities of the Qwen2.5-VL-3B-Instruct model Bai et al. [2025], our approach adapts powerful general-purpose MLLM to the unique challenges of Arabic document analysis. To facilitate this specialization, Baseer was trained on large-scale, diverse dataset composed of both synthetically generated and authentic real-world Arabic documents. This dataset was curated to encompass the extensive variety of formats, fonts, and layouts encountered in practical applications. Furthermore, we present Misraj-DocOCR, novel benchmark specifically engineered for the evaluation of Arabic OCR systems, featuring high-quality, expert-verified annotations to ensure reliability. Our primary contributions are threefold: 1. We present the development and fine-tuning of Baseer, demonstrating that an efficient, 2 decoder-only fine-tuning strategy can achieve state-of-the-art performance in Arabic document OCR. 2. We introduce Misraj-DocOCR, new, reliable, and openly available benchmark designed to provide standardized and rigorous evaluation framework for Arabic OCR systems. 3. We conduct thorough analysis of the KITAB-pdf-to-markdown benchmark, providing revised and improved version that addresses significant inaccuracies to enhance its accuracy and utility for the research community. Through series of extensive experiments, we demonstrate that Baseer consistently outperforms existing open-source and commercial alternatives."
        },
        {
            "title": "2 Related Work",
            "content": "To contextualize our work, we situate our work at the intersection of two major research domains. First, we review the rapid advancements in Multimodal Large Language Models (MLLMs), which provide the architectural foundation for our approach. Second, we delve into the field of Optical Character Recognition (OCR) and Document Understanding, examining its evolution and highlighting the persistent challenges that motivate our research, particularly for morphologically complex languages like Arabic."
        },
        {
            "title": "2.1 Multimodal Large Language Models",
            "content": "The paradigm of Large Language Models (LLMs) has recently been extended to handle multimodal input, leading to the development of powerful models capable of joint vision-language reasoning. Research in this area has generally progressed along two main architectural paths. One approach involves the modular integration of pre-trained components, where specialized frozen vision encoder is connected to large language decoder via lightweight adapter. This design is seen in influential models like LLaVA Liu et al. [2023; 2024]; Li et al. [2024], Aya-Vision Dash et al. [2025], Idefics Laurençon et al. [2024b;a], and more compact architectures such as SmolVLM Marafioti et al. [2025]. These architectures achieve impressive zero-shot and few-shot performance on diverse range of multimodal tasks with high parameter efficiency. The second approach focuses on training massive, end-to-end vision-language models. This category includes state-of-the-art systems such as InternVL Chen et al. [2024]; Zhu et al. [2025], Gemma Team et al. [2025], PaliGemma Steiner et al. [2024], and Qwen-VL Bai et al. [2025]. These models, with parameter counts scaling up to 70 billion in the case of Qwen2.5-VL, have demonstrated remarkable general-purpose capabilities. However, their broad, generalist training often leaves them unspecialized for precision-critical, niche domains. As we will discuss, high-fidelity document OCR represents one such domain where these powerful models still exhibit significant limitations. https://huggingface.co/datasets/Misraj/Misraj-DocOCR https://huggingface.co/datasets/Misraj/KITAB_pdf_to_markdown_reviewed"
        },
        {
            "title": "2.2 OCR and Document Understanding",
            "content": "The field of Optical Character Recognition has evolved substantially from its origins in rule-based pattern matching. The integration of deep learning, especially Convolutional and Recurrent Neural Networks (CNNs and RNNs), marked significant leap, dramatically improving accuracy for text in both scanned documents and natural scenes. More recently, the focus has changed from text transcription to holistic Document Understanding. This advanced task requires not only recognizing text but also parsing the documents logical structure, including layouts, tables, and other semantic elements. This capability is crucial for applications in data extraction, document archiving, and content analysis. Leading efforts in this domain, such as Idefics3 Laurençon et al. [2024a], MonkeyOCR Li et al. [2025], SmolDocling Nassar et al. [2025], and commercial systems such as Nanonets-OCR-s Mandal et al. [2025], have established high benchmarks for performance on standard document types. More recent attempts, such as Qari Wasfy et al. [2025], have tackled Arabic OCR directly, but their scope remains limited compared to comprehensive document understanding systems. However, critical challenge remains: the generalization of these models to languages with scripts that are fundamentally different from Latin-based languages. Arabic serves as prime example of this challenge. Its inherent characteristics include cursive script, context-sensitive character shapes, optional but meaningful diacritics, right-to-left orientation, and wide variety of fonts and styles. These characteristics often cause state-of-the-art document OCR systems to degrade sharply in performance when applied to Arabic texts. To the best of our knowledge, the application of modern MLLM frameworks to the specific, challenging problem of Arabic document OCR remains largely unexplored area. Although several multilingual and multimodal models include Arabic in their training, they are not optimized for the script-specific and structural challenges posed by Arabic documents. This work aims to bridge this critical gap. By fine-tuning powerful, pre-trained vision-language model, we introduce Baseer, system specifically engineered for the complexities of Arabic documents. Our results demonstrate that this specialized approach yields substantial leap in performance, establishing new state-of-the-art for open source and proprietary systems in this vital domain."
        },
        {
            "title": "3 Data",
            "content": "This section details the construction of the dataset used for training and evaluation. To support effective document OCR, it is essential to represent textual content in format that preserves both structure and semantics. In our dataset, the text corresponding to each image is formatted in Markdown, providing clean and standardized representation of content. Tables are represented in HTML to accurately capture diverse table structures and complex layouts. Furthermore, specialized tags were introduced to mark specific elements within the text, including watermarks, page numbers, and the presence of images, enabling precise supervision for layout-aware OCR and document parsing tasks. The dataset itself was constructed as hybrid collection, combining large corpus of synthetically generated documents with carefully curated set of real-world publications. This approach ensures broad coverage of document styles, visual characteristics, and layout complexities. Each of these sources is described in detail below."
        },
        {
            "title": "3.1 Synthetic Data",
            "content": "The first component of our dataset was generated synthetically using an in-house pipeline, designed to capture the diverse formatting and layout variations commonly found in word-processing documents. The foundation for this synthetic data is corpus of markdown-formatted documents, which were downloaded and filtered from the Common Crawl archive using methodology analogous to our previously released dataset. To ensure the quality and relevance of the source material, the raw data were subjected to the following preprocessing filters: 1. Perplexity Filtering: An in-house language model based on KenLM Heafield [2011] was employed to calculate perplexity scores, retaining only the most linguistically cohesive text samples. 2. Table Sparsity Filtering: To ensure structural integrity, documents containing markdown tables with more than 25% empty cells were identified and discarded. The filtered markdown documents were then converted into image-text pairs via four-step rendering pipeline: 1. Markdown to HTML: Documents were first converted to HTML to facilitate the systematic parsing of distinct formatting tags. 2. HTML to Word: The resulting HTML was transformed into Microsoft Word documents, meticulously preserving all structural and stylistic attributes (e.g., bold, italics, headers). 3. Word to PDF: These Word documents were subsequently exported to PDF format to create standardized, page-level representation. 4. PDF to Image: Finally, each page of the PDF files was rendered as high-resolution image, forming the visual component of the training pairs. To foster model robustness, high degree of visual diversity was introduced during the rendering process by systematically varying document configurations, as detailed in Table 1. Furthermore, subset of the generated images underwent an augmentation process involving 29 distinct transformations, which are organized into eight categories  (Table 2)  . From the pool of generated images, 150,000 samples were randomly selected and divided into three equal subsets of 50,000 each. The first subset underwent single random transformation, the second was subjected to two transformations, and the third to three, ensuring progressive increase in complexity. To prevent redundancy, the original, pre-augmentation versions of these images were discarded. In total, this synthetic pipeline produced 300,000 high-quality imagetext pairs, comprising 150,000 clean rendered samples and 150,000 augmented variants designed to simulate diverse real-world document conditions. https://huggingface.co/datasets/Misraj/msdd Parameter Fonts Page Sizes Background Color Text Color Alignment Columns Font Size Margin Line Height Column Spacing Special Formatting Random highlights, colored paragraphs, RTL (95%) Values / Distribution 39 Arabic fonts A4, A5, Letter, Legal, Tabloid, A3 (incl. landscape variants) 8 light shades (75%), 5 dark shades (25%) 9 light, 16 dark Right (65%), Left (5%), Center (30%) 1 (75%), 2 (20%), 3 (5%) Even values from 822 pt 1.02.5 cm (uniform) 1.01.6 (uniform) 0.51.2 cm (uniform) Table 1: Document configuration diversity. Category Pre-print adjustments Printing mechanical deficits Human-made marks Paper aging effects Digital noise Geometric adjustments Lighting adjustments Blur effects Number of Transforms (Examples) 5 (e.g., Watermark) 5 (e.g., Dirty drum) 2 (e.g., Handwritten markup) 3 (e.g., Folding, yellowing) 4 (e.g., Salt-and-pepper noise) 2 (e.g., Perspective distortion) 5 (e.g., Low-light conditions) 3 (e.g., Motion blur) Table 2: Categories of transformations applied to the data."
        },
        {
            "title": "3.2 Open-Source Books and Magazines",
            "content": "The second component of our dataset was sourced from real-world documents, including diverse collection of books, magazines, educational documents, and academic papers. In contrast to synthetic data, these samples reflect authentic publishing environments, capturing genuine layout complexities and typographic conventions. To ensure maximum diversity, the selected pages span broad spectrum of layout complexities, identified using vision-based algorithms. Specifically, bounding boxes were first detected at the paragraph level, and their alignment and overlap were analyzed to capture challenging structures such as tables, figures, index pages, and skewed layouts. Page color distributions were also examined to include samples with embedded images, colorful backgrounds, and multi-colored text. Ground-truth text for the real-world documents was obtained using state-of-the-art visionlanguage model (VLM). To ensure high-quality labels, representative subset of the VLM outputs was manually verified by human experts for both textual accuracy and structural fidelity. This collection is particularly valuable because it contains complex elements not present in the synthetic dataset, including intricate footnotes, varied column layouts, and non-standard typography. From this source, we curated 200,000 document images paired with their corresponding ground-truth text. Collectively, the combination of these two sources results in 500,000 text-image pairs, used for training our model. 6 detailed breakdown of the dataset distribution across sources is shown in Figure 2. Figure 2: Distribution of data samples across the different sources."
        },
        {
            "title": "4 Misraj-DocOCR: An Arabic Document OCR Benchmark",
            "content": "The evaluation of Optical Character Recognition (OCR) models for Arabic text requires robust and accurate benchmarks. Our initial investigation involved assessing existing benchmarks, such as the KITAB-bench Heakl et al. [2025] pdf-to-markdown dataset. During this analysis, we identified significant shortcomings that compromise its reliability for model evaluation. primary issue discovered was the presence of numerous errors in the ground truth data. We observed multiple instances of hallucinatory text, where the ground truth contained phrases not present in the source documents likely originating from data creation or annotation tool rather than authentic content. Furthermore, our review revealed that many examples lacked corresponding page numbers, and small-font text was frequently omitted from the ground truth. These inaccuracies suggest that the dataset may not have undergone thorough verification process after the initial data extraction. For more details, see Appendix To address these deficiencies and provide more reliable resource for the research community, we undertook comprehensive correction of the KITAB-bench PDF-to-markdown dataset. This For example, one entry included the English sentence: \"Youre right - let me write it exactly as it appears in the image, maintaining the right-to-left direction:\" corrected version, with all identified errors rectified, has been made publicly available for academic use. Beyond the inaccuracies, our examination of existing resources also indicated lack of diversity in the style and type of documents. To foster more generalized and robust model development, benchmark should encompass wide variety of real-world scenarios. Therefore, we introduce Misraj-DocOCR, new, comprehensive benchmark specifically designed for evaluating Arabic Document OCR models. The primary contributions of this benchmark are: Diverse and Comprehensive Content: The benchmark consists of 400 high-quality images, curated to include wide variation of document types, layouts, and fonts, and comprising both synthetic and real-world pages. Expert-Verified Ground Truth: To ensure the highest level of accuracy, every image in the dataset has been meticulously reviewed by human experts. This verification process guarantees that both the transcribed text and the document structure are correct, eliminating the types of errors found in previous benchmarks. Open Access: Misraj-DocOCR is open-source and publicly available to all researchers. By providing this resource, we aim to facilitate further advancements and foster reproducible research in the field of Arabic OCR. We evaluate many models on this benchmark and the corrected version of KITAB-Bench, all results on the section 7."
        },
        {
            "title": "5 Methodology",
            "content": "The overall process for developing Baseer, as depicted in Figure 1, involved comprehensive data collection stage followed by targeted fine-tuning stage. The development of our model, Baseer, followed two-stage methodology designed to tailor powerful, pre-trained foundation model to our specific needs. The first stage involved the comprehensive collection and curation of high-quality dataset, the details of which are described in Section 3. The subsequent stage, which is the focus of this section, consisted of fine-tuning the selected base model to align with our data and enhance its capabilities for Arabic document processing. For the base architecture of Baseer, we selected the Qwen2.5-VL-3B-Instruct model Bai et al. [2025]. This decision was predicated on its robust and state-of-the-art performance on multimodal tasks, particularly its demonstrated proficiency with the Arabic language compared to other opensource alternatives. Despite its advanced capabilities, our preliminary analysis revealed that the base Qwen2.5-VL-3BInstruct model exhibited certain limitations relevant to our use case. These included occasional reversions to left-to-right text generation, suboptimal handling of diacritized Arabic text, and other https://huggingface.co/datasets/Misraj/KITAB_pdf_to_markdown_reviewed performance artifacts. key objective of our work was to mitigate these specific weaknesses through targeted fine-tuning. Our fine-tuning strategy involved updating all model parameters, except for the vision encoder, which remained frozen. This approach allows the model to adapt its language and reasoning capabilities to our specialized dataset while preserving the powerful, generalized visual features learned during its original pre-training. The specific hyperparameters, hardware used, and other details of the training procedure are provided in Appendix D."
        },
        {
            "title": "6 Experiments and Results",
            "content": "This section details the series of experiments conducted to systematically determine the optimal architecture and training configuration for Baseer. Our experimental process was designed to isolate variables and build upon the findings of each preceding stage."
        },
        {
            "title": "6.1 Base Model Selection",
            "content": "The initial experiment was focused on selecting the most suitable base model for our task. To this end, we conducted qualitative evaluation of several prominent open-source vision-language models. curated set of representative examples, designed to test key capabilities in Arabic document understanding, was used as the input. The outputs from each model were then subjected to rigorous manual review by our evaluation team. The models were assessed based on criteria such as text recognition accuracy, preservation of right-to-left directionality, and overall coherence. This qualitative analysis concluded that Qwen2.5VL-3B-Instruct demonstrated superior performance on Arabic-language tasks compared to the other candidates, making it the clear choice for our foundation. selection of comparative outputs from this evaluation is provided in Appendix B."
        },
        {
            "title": "6.2 Fine-Tuning Strategy Evaluation",
            "content": "After selecting the base model, our next objective was to identify the most effective fine-tuning strategy. We designed controlled experiment to compare three distinct approaches: 1. Full Fine-Tuning (Baseer-Full): All model parameters, including the vision encoder, were made trainable. 2. Decoder-Only Fine-Tuning (Baseer-Decoder): Only the parameters of the language decoder were updated, while the vision encoder remained frozen. 3. Parameter-Efficient Fine-Tuning (Baseer-LoRA): Low-Rank Adaptation (LoRA) was employed to update small subset of parameters. To ensure fair comparison, each of these strategies was tested on 50,000-sample subset of our training data for two epochs, holding all other hyperparameters constant. We evaluated the models using ChrF, which measures OCR accuracy at the character level and captures text transcription 9 quality. Table 3 summarizes the performance of the different fine-tuning strategies on the Baseer model. Model Trainable part ChrF Baseer-Full Full model Baseer-Decoder Languge-decoder Baseer-LoRA LoRA weight 84.79 89.79 85. Table 3: Performance comparison of different fine-tuning strategies on Baseer model As shown in Table 3, the results from our test set indicate that the decoder-only fine-tuning approach (Baseer-Decoder) significantly outperformed the other methods. This suggests that preserving the generalized features of the pre-trained vision encoder while adapting the language model to our specific data yields the best performance. 6."
        },
        {
            "title": "Impact of Sequence Length",
            "content": "Building on the previous finding, we adopted the decoder-only fine-tuning strategy and proceeded to investigate the effect of input sequence length on model performance. All training configurations were fixed while we experimented with three sequence length variants: 2048, 4096, and 8192 tokens. Context Length ChrF 2048 4096 8192 82. 89.79 87.52 Table 4: Performance comparison of different context lengths on Baseer model. The results of this experiment are presented in Table 4. The optimal performance was achieved with sequence length of 4096. We attribute this to the model having sufficient context to process high level of detail from the images. In contrast, the performance with sequence length of 8192 degraded. We hypothesize that this is because the images in our dataset do not typically contain enough information to fill such large context window, leading to excessive padding. This padding may dilute the relevant visual information and negatively impact the models learning process."
        },
        {
            "title": "7 Evaluation",
            "content": "We evaluate our model on our proposed Misraj-DocOCR benchmark and corrected version of KITAB-Bench PDF-to-Markdown, alongside several open-source and commercial models. Text extraction performance is assessed using Word Error Rate (WER) and Character Error Rate (CER), which measure wordand character-level transcription errors, BLEU for n-gram overlap, and ChrF, character-level F-score suited for morphologically rich languages like Arabic. Structural and layout fidelity is measured with Tree Edit Distance Similarity (TEDS), capturing hierarchical document structures, and MARS Heakl et al. [2025], which evaluates layout-aware alignment between predicted and reference renderings. Table 5 presents the results, showing that Baseer achieves state-of-the-art 10 performance across both text and structural metrics, despite being smaller than competing models."
        },
        {
            "title": "7.1 Evaluation Protocol",
            "content": "To ensure fair comparison across models, models designed for document understanding were evaluated using their respective system prompts, while Multimodal Large Language Models (MLLMs) were provided with carefully tested prompts to ensure optimal performance. All outputs were standardized using the following post-processing steps: 1. Remove HTML tags outside table structures 2. Convert Markdown tables to HTML format for consistency 3. Normalize horizontal line representations (, ***, etc. ) 4. Standardize header formatting 5. Unify formatting tags within HTML tables (<strong>, <b> <b>) 6. Remove model-specific tags (<page_number>, <watermark>) present only in our model and Nanonets This standardization is critical because different models may produce semantically equivalent but syntactically different outputs, which would unfairly penalize models based on formatting choices rather than content accuracy. Model Baseer (ours) Gemini-2.5-pro Azure AI Document Intelligence Dots.ocr Nanonets Qari Qwen2.5-VL-32B GPT-5 Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B Gemma3-12B Gemma3-4B GPT-4o-mini AIN Aya-vision WER CER BLEU CHRF TEDS MARS 76.885 70.775 62.245 59.205 52.445 42.750 51.820 54.8 40.210 40.850 38.765 29.695 36.52 11.620 17.905 87.77 89.55 82.49 78.41 67.89 64.50 62.64 61.6 53.42 54.70 44.53 31.39 47.04 2.24 9. 76.18 77.92 62.04 58.16 42.22 38.59 37.62 40.67 25.39 31.57 19.75 9.57 22.63 1.25 2.91 0.53 0.31 0.27 0.40 0.55 0.64 0.59 0.62 0.71 0.77 0.80 0.85 1.1 1.11 1.07 0.25 0.37 0.44 0.50 0.71 0.76 0.76 0.86 0.87 0.92 0.96 1.01 1.36 1.23 1.41 66 52 42 40 37 21 41 48 27 27 33 28 26 21 26 Table 5: Comparison of models across multiple evaluation metrics on Misraj-DocOCR. Best values are highlighted in bold and the second-best values are underlined. Table 5 presents comparative evaluation of different OCR and vision-language models using multiple metrics. The results indicate that Baseer achieves the best performance across most metrics, 11 particularly in WER, TEDS, and MARS. The gemini-2.5-pro model follows closely, obtaining the highest BLEU and CHRF scores, while Azure AI Document Intelligence achieves the lowest CER. Notably, Baseer consistently outperforms large commercial systems such as GPT-based models and Azure AI, underlining its robustness in both text and structure recognition. This is especially significant given that the evaluation dataset, Misraj-DocOCR, was deliberately designed to be highly diverse and challenging, with wide variation in layout and typography. The results also highlight sharp performance gap between the top-performing systems and smaller or less specialized models (e.g., Gemma3, AIN, Aya-vision), underscoring the difficulty of this benchmark. Overall, Baseer and Gemini-2.5-pro emerge as the strongest systems in this comparison. Example outputs of Baseer are provided in Appendix C. Model Dots.ocr Baseer (ours) Nanonets Qari Qwen2.5-VL-3B Qwen2.5-VL-7B Gemma3-12B Gemma3-4B Aya-vision AIN WER CER BLEU CHRF TEDS MARS 0.39 0.61 0.51 0.65 0.70 0.76 0.85 0.95 1.27 1.18 0.28 0.40 0.40 0.48 0.57 0.63 0.69 0.82 0.96 1. 59.28 55.78 51.37 44.61 40.44 36.76 27.56 12.94 5.58 2.61 83.16 80.26 77.45 71.45 66.78 62.45 52.09 31.72 16.19 3.99 43 56 33 43 31 24 55 27 26 24 63.08 68.13 55.225 57.225 48.89 43.225 53.545 29.36 21.095 13.995 Table 6: Comparison of models across multiple evaluation metrics on KITAB-BenchPDF-toMarkdown dataset. Best values are highlighted in bold and the second-best values are underlined. Table 6 reports the results on the KITAB-Bench PDF-to-Markdown dataset, which was carefully reviewed and corrected by domain experts to ensure high-quality ground truth annotations. This evaluation was conducted using only open-source models for fairness. While Dots.ocr achieves the strongest performance across most text-centric metrics (WER, CER, BLEU, and CHRF), slightly surpassing Baseer, Baseer shows clear superiority in structural understanding, attaining the highest TEDS score (56) and the best overall MARS. It is also worth noting that the KITAB-Bench subset is relatively small, consisting of only 30 samples, which makes every misprediction more impactful on the reported scores. In contrast, on the larger and more challenging Misraj-DocOCR benchmark with 400 diverse examples, Baseers advantage over both open-source and commercial systems becomes more pronounced, highlighting its robustness across varied document types and layouts."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we introduced Baseer, vision-language model tailored for Arabic Document OCR, and presented Misraj-DocOCR, high-quality benchmark designed for rigorous evaluation. By training on diverse dataset of 500,000 document-image pairs, we demonstrated that decoder-only fine-tuning is powerful strategy that enables Baseer to achieve superior performance compared to wide range of existing systems. Our detailed experimental analysis highlighted the importance of https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence 12 sequence length, fine-tuning scope, and dataset diversity in achieving robust performance. Notably, Baseer consistently achieved the best or near-best scores across Word Error Rate, Character Error Rate, and structure-aware metrics such as TEDS and MARS, surpassing both open-source and proprietary alternatives. These positive results underscore the value of domain-specific adaptation of general-purpose MLLMs, and provide new insights into how tailored data and efficient training strategies can push the boundaries of OCR for complex scripts. We believe that this work establishes strong baseline for future research and will accelerate the development of practical, high-accuracy OCR solutions for Arabic and other morphologically rich languages."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Saurabh Dash, Yiyang Nan, John Dang, Arash Ahmadian, Shivalika Singh, Madeline Smith, Bharat Venkitesh, Vlad Shmyhlo, Viraat Aryabumi, Walter Beller-Morales, et al. Aya vision: Advancing the frontier of multilingual multimodality. arXiv preprint arXiv:2505.08751, 2025. Kenneth Heafield. KenLM: Faster and smaller language model queries. In Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar F. Zaidan (eds.), Proceedings of the Sixth Workshop on Statistical Machine Translation, pp. 187197, Edinburgh, Scotland, July 2011. Association for Computational Linguistics. URL https://aclanthology.org/W11-2123/. Ahmed Heakl, Abdullah Sohail, Mukul Ranjan, Rania Hossam, Ghazi Shazan Ahmad, Mohamed El-Geish, Omar Maher, Zhiqiang Shen, Fahad Khan, and Salman Khan. Kitab-bench: comprehensive multi-domain benchmark for arabic ocr and document understanding. arXiv preprint arXiv:2502.14949, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding vision-language models: insights and future directions. arXiv preprint arXiv:2408.12637, 2024a. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? Advances in Neural Information Processing Systems, 37:8787487907, 2024b. 13 Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, and Xiang Bai. Monkeyocr: Document parsing with structure-recognition-relation triplet paradigm. arXiv preprint arXiv:2506.05218, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2629626306, 2024. Souvik Mandal, Ashish Talewar, Paras Ahuja, and Prathamesh Juvatkar. Nanonets-ocr-s: model for transforming documents into structured markdown with intelligent content recognition and semantic tagging, 2025. Andrés Marafioti, Orr Zohar, Miquel Farré, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, et al. Smolvlm: Redefining small and efficient multimodal models. arXiv preprint arXiv:2504.05299, 2025. Ahmed Nassar, Andres Marafioti, Matteo Omenetti, Maksym Lysak, Nikolaos Livathinos, Christoph Auer, Lucas Morin, Rafael Teixeira de Lima, Yusik Kim, Said Gurbuz, et al. Smoldocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion. arXiv preprint arXiv:2503.11576, 2025. Andreas Steiner, André Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, et al. Paligemma 2: family of versatile vlms for transfer. arXiv preprint arXiv:2412.03555, 2024. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Ahmed Wasfy, Omer Nacar, Abdelakreem Elkhateb, Mahmoud Reda, Omar Elshehy, Adel Ammar, and Wadii Boulila. Qari-ocr: High-fidelity arabic text recognition through multimodal large language model adaptation. arXiv preprint arXiv:2506.02295, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "Appendices",
            "content": "A KITAB-Bench-Analysis In this section, we present examples of the errors identified in the KITAB-bench. We observed that many items in the benchmark are missing page numbers, and the text in small fonts, particularly at the page footers, is often not captured correctly. We provide selection of these examples here, and readers are encouraged to visit our reviewed version at to explore the complete set of corrections and outputs. When dots are displayed in the image, it indicates that there is output. However, for better visualization, we omit lengthy output if it does not contain any errors. Example from KITAB-Bench pdf-to-markdown https://huggingface.co/datasets/Misraj/KITAB_pdf_to_markdown_reviewed 15 Example from KITAB-Bench pdf-to-markdown Example from KITAB-Bench pdf-to-markdown 16 Example from KITAB-Bench pdf-to-markdown"
        },
        {
            "title": "B Base Models Output",
            "content": "In this section, we present examples from the evaluation set that was used to select the most suitable model to build upon. While we tested wide range of models, here we only showcase few representative outputs for visualization purposes. 17 Example from models output used for selecting the base model Example from models output used for selecting the base model 18 Example from models output used for selecting the base model Example from models output used for selecting the base model"
        },
        {
            "title": "C Baseer Model Output",
            "content": "Example of Baseer output Example of Baseer output 20 Example of Baseer output Example of Baseer output Example of Baseer output"
        },
        {
            "title": "D Traning Details",
            "content": "The fine-tuning process for Baseer employed the standard next-token prediction methodology, with the system prompt and embedding tokens masked. Parameter Training Epochs Learning Rate Schedule Learning Rate Batch Size Weight Decay Warm-up Steps Optimizer Max Sequence Length GPU Value 3 Cosine decay 1e-4 640 0.01 100 AdamW 4096 8xH100 Table 7: Training Hyperparameters for Baseer Model"
        }
    ],
    "affiliations": [
        "Misraj.ai"
    ]
}