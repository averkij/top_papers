{
    "paper_title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
    "authors": [
        "Zhenheng Tang",
        "Xiang Liu",
        "Qian Wang",
        "Peijie Dong",
        "Bingsheng He",
        "Xiaowen Chu",
        "Bo Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy on tasks of common sense knowledge QA and basic arithmetic reasoning. In this blog, we present a brief review of recent advancements in LLMs related to retrieval-augmented generation, multi-step reasoning, external tools, and computational expressivity, all of which substantially enhance LLM performance. Then, we propose a lottery LLM hypothesis suggesting that for a given LLM and task, there exists a smaller lottery LLM capable of producing the same performance as the original LLM with the assistance of multi-step reasoning and external tools. Based on the review of current progress in LLMs, we discuss and summarize the essential capabilities that the lottery LLM and KV cache compression must possess, which are currently overlooked in existing methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 5 3 5 7 1 . 2 0 5 2 : r Published at ICLR Blogposts THE LOTTERY LLM HYPOTHESIS, RETHINKING WHAT ABILITIES SHOULD LLM COMPRESSION PRESERVE? Zhenheng Tang1 Xiang Liu2 Qian Wang3 Bingsheng He3 Xiaowen Chu2, Bo Li1, 1 CSE, The Hong Kong University of Science and Technology 2 DSA, The Hong Kong University of Science and Technology (Guangzhou) 3 National University of Singapore Peijie Dong"
        },
        {
            "title": "ABSTRACT",
            "content": "Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy on tasks of common sense knowledge QA and basic arithmetic reasoning. In this blog, we present brief review of recent advancements in LLMs related to retrievalaugmented generation, multi-step reasoning, external tools, and computational expressivity, all of which substantially enhance LLM performance. Then, we propose lottery LLM hypothesis suggesting that for given LLM and task, there exists smaller lottery LLM capable of producing the same performance as the original LLM with the assistance of multi-step reasoning and external tools. Based on the review of current progress in LLMs, we discuss and summarize the essential capabilities that the lottery LLM and KV cache compression must possess, which are currently overlooked in existing methods."
        },
        {
            "title": "1 CURRENT EFFORTS ON COMPRESSING LLMS AND KV CACHE",
            "content": "LLMs have demonstrated remarkable proficiency in natural language processing, enabling sophisticated interactions and understanding of human language (OpenAI, 2023). To learn the tremendous knowledge in the training datasets, the current advanced LLMs like GPT4 (OpenAI, 2023) and Llama3 (Touvron et al., 2023) have enormous parameters like 7 750 billion. Training such an LLM requires extensive computational resources, often measured in enormous GPU days using advanced NVIDIA GPUs (Touvron et al., 2023). This results in substantial electricity consumption, impacting both economic and energy costs (Samsi et al., 2023; Tang et al., 2019), and raising concerns regarding sustainable computing (Wilkins et al., 2024). Furthermore, providing inference services for LLMs necessitates numerous GPUs and incurs additional energy costs (Samsi et al., 2023; Tang et al., 2019), making it significant challenge for widespread deployment (Patel et al., 2024). Compression methods. To this end, both academic researchers and industrial engineers are trying to compress model parameters and reduce the model into smaller one while keeping its performance unchanged. The typical compression algorithm includes the pruning (Sun et al., 2024b; Frantar & Alistarh, 2023; Dong et al.) and quantization (Yao et al., 2022; Dettmers & Zettlemoyer, 2022; Dong et al., 2024) of LLM parameters, and KV cache compression (Zhang et al., 2023b; Xiao et al., 2024). However, most of the current methods that compress LLMs and KV cache only show guaranteed performance of the perplexity on some basic language tasks like Wikitext2 (Merity et al., 2016) and PTB (Marcus et al., 1993), common sense knowledge QA tasks (Hendrycks et al., 2021; Talmor et al., 2019) and the basic arithmetic reasoning tasks (Cobbe et al., 2021) in small-scale evaluation but not in the real-world industrial scenarios. Missed aspects. Some recent studies show that the LLMs may lose their advanced crucial abilities under the compressions like the long-context retrieval, long-context generation and long-document Corresponding author (xwchu@ust.hk and bli@ust.hk). Published at ICLR Blogposts 2025 reasoning and so on (JAISWAL et al., 2024). Also, the long-context understanding ability of LLMs is significantly reduced under the KV cache compression (Yuan et al., 2024). In the following sections, we examine recent advancements in retrieval-augmented generation, the utilization of external tools, and multi-step reasoning, all of which markedly enhance the performance of LLMs. Subsequently, we introduce the lottery LLM hypothesis, which posits that for specific LLM and task, smaller lottery LLM can achieve equivalent performance to the original LLM, aided by multi-step reasoning and external tools. Drawing from the review of current LLM advancements, we discuss and outline the critical capabilities that the lottery LLM and KV cache compression should encompass, which are currently neglected in existing methodologies."
        },
        {
            "title": "KNOWLEDGE RETRIEVAL",
            "content": "Redundant Knowledge. In contemporary applications, many individuals utilize LLMs as encyclopedic resources or to verify news and academic research, akin to an Internet search engine. Recent studies indicate that LLMs exhibit varying performance in knowledge retrieval, contingent upon the popularity of the information (Mallen et al., 2023a). Specifically, small subset of real-world question-answer (QA) pairs constitutes the majority of interactions, while limited number of QAs receive frequent attention, demonstrating long-tail distribution in their popularity (Mallen et al., 2023a). LLMs tend to perform better on high-popularity QAs compared to those with lower popularity. Hallucinated Knowledge. LLMs often generate unreal outputs rather than factual knowledge, which is phenomenon known as hallucination (Huang et al., 2023). This issue has garnered significant attention from researchers (Huang et al., 2023). There is ongoing debate regarding the feasibility of completely eliminating hallucinations (Farquhar et al., 2024). Some studies suggest that hallucinations are inevitable, as they are byproduct of the models reasoning and generalization abilities (Banerjee et al., 2024; Xu et al., 2024b). Retrieval Augmented Generation (RAG). Large Language Models (LLMs) exhibit robust in-context learning capabilities, enabling them to respond to queries using prompts rather than relying solely on their internal knowledge encoded within model parameters. Consequently, external knowledge sources such as scholarly articles, web pages, books, and other documents can be integrated into prompts to facilitate the retrieval of additional factual information (Yao et al.), thereby mitigating the occurrence of hallucinations (Yao et al.). This approach raises significant research questions: Is it necessary to store all knowledge within LLM parameters if RAG can accurately retrieve factual information from external knowledge bases? If not, which knowledge should be stored and which should not? Considering two extreme scenarios: Storing all knowledge in model parameters: If all knowledge is stored within model parameters, LLMs function as oracle machines, obviating the need for RAG. However, training such an LLM is nearly impossible because not all knowledge can be collected and never outdated (Xu et al., 2024b; Banerjee et al., 2024). Moreover, deploying such large model is inefficient. Storing all knowledge in external knowledge bases: If all knowledge is stored externally, LLM parameters could potentially be reduced significantly, allowing for the retrieval of factual information during inference. Nevertheless, LLMs require foundational common knowledge to perform tasks such as reasoning and accurate retrieval. This issue will be further explored in subsequent sections. Thus, compressing all knowledge into external knowledge bases is not feasible. Investigating the nature of learned knowledge and identifying which knowledge triggers the grokking phenomenon in LLMs remains an open research question (Nanda et al., 2023). Trade-off between model size and knowledge base. Some studies indicate that adaptive knowledge retrieval is promising direction to enhance the performance of LLMs and may help to find an optimal trade-off between the knowledge base and model size (Jeong et al., 2024b). The adaptive 2 Published at ICLR Blogposts 2025 RAG (Soudani et al., 2024; Jeong et al., 2024b) suggests that popular knowledge can be stored in the model parameters, while less popular knowledge can be stored in the external knowledge base. The core idea of adaptive RAG appears to be related to classic efficient data structure, Huffman coding (Moffat, 2019). Specifically, the cost of knowledge retrieval can be viewed as the prompt length (since the retrieved knowledge will be inserted into the prompts). Storing knowledge in the model parameters results in shorter prompt length because LLMs can directly respond to questions without needing to retrieve knowledge from the external knowledge base. Conversely, storing knowledge in the external knowledge base results in longer prompt length, implying higher retrieval operations and longer context lengths, which incur greater computational and storage costs during inference (Xiao et al., 2024). Therefore, the popularity of the knowledge can be seen as the appearance probability, as in Huffman coding. Storing popular knowledge in the model parameters is more efficient. Finetuning vs. retrieval. Another related question is whether finetuning should be used to enhance the performance of LLMs in specific application domains such as legal, finance, and medical fields (Hendrycks et al., 2021; Talmor et al., 2019). Finetuning may lead to the forgetting problem and additional training overheads, sparking debate on whether finetuning should be employed to improve LLM performance or if reliance on RAG can achieve the same goal (Jeong et al., 2024b). Recent studies demonstrate that RAG can significantly enhance LLM performance in specific domains such as legal (Pipitone & Alami, 2024), medical (Jeong et al., 2024a), and finance (Li et al., 2024b). Beyond the RAG. Document-based knowledge retrieval primarily assists LLMs in retrieving knowledge of triplets consisting of entity, relation, and object (Chen et al., 2024). However, the capabilities and exceptional performance of LLMs extend beyond retrieving triplet knowledge. LLMs also exhibit remarkable abilities such as solving arithmetic problems, playing chess, and coding, which are not simple triplet knowledge retrieval tasks (Chen et al., 2024). Ensuring the reasoning performance of smaller LLMs is crucial and cannot be easily addressed by document-based knowledge retrieval."
        },
        {
            "title": "3 EXTERNAL TOOLS",
            "content": "Advanced Large Language Models (LLMs) demonstrate remarkable capabilities in function calling, which involves invoking external tools to address specific tasks. These external tools may include Internet search engines (Qin et al., 2023), arithmetic calculation functions (Schick et al., 2023), system operations (Ge et al., 2023; Mei et al., 2024), game interfaces, and more. These are formulated into programming function calls (Abdelaziz et al., 2024) and conveyed to LLMs via prompts. Based on the function descriptions, LLMs determine which function to call to resolve the given problems (Abdelaziz et al., 2024). Arithmetic Function Calls. To solve arithmetic problems, LLMs are trained on arithmetic datasets (Cobbe et al., 2021). However, simple errors often occur during the arithmetic reasoning process, such as LLMs erroneously determining that 9.11 is greater than 9.9 (Choi et al., 2024). To mitigate this, some studies propose enabling LLMs to generate programs that include arithmetic operations and utilize an external Python interpreter to solve these problems (Gao et al., 2023a). Additionally, some research suggests leveraging arithmetic function calls to solve arithmetic problems (He-Yueya et al.). Experimental results indicate that arithmetic function calling can significantly enhance the performance of LLMs on arithmetic tasks (Gao et al., 2023a; Yang et al., 2023). Internet Search Engine. To augment LLM knowledge with online and dynamically updated external information, the Internet search engine is employed as an external tool (Yao et al.; Vu et al., 2023). Experimental results demonstrate that interacting with an Internet search engine, such as simple Wikipedia API, can significantly improve LLM performance on knowledge retrieval tasks (Yao et al.). LLM Operating System (OS). By conceptualizing LLM calls as system calls akin to traditional operating systems, recent studies propose developing new LLM-as-OS framework (Ge et al., 2023), which allows LLMs to invoke external tools like applications in an OS. Recent studies also propose the AIOS framework (Mei et al., 2024) to decouple LLM calls from system calls and implement various managers to enhance AIOS efficiency. The optimized agent framework from the OS perspective significantly improves both the efficiency and performance of LLM calls. 3 Published at ICLR Blogposts Logic Solver. There is ongoing debate regarding whether LLMs can perform logical reasoning akin to humans (Mirzadeh et al., 2024; Kambhampati, 2024; Valmeekam et al., 2022; Amirizaniani et al., 2024; Xu et al., 2023; Arkoudas, 2023). Recent studies suggest that to enhance the reasoning capabilities of LLMs, external logic solvers can be utilized to solve logical reasoning problems (Zhang et al., 2023a). In some frameworks, LLMs are tasked with transforming natural language sentences into logical forms, while logic solvers are responsible for solving the logical reasoning problems (Han et al., 2022; Pan et al., 2023; Wang et al., 2024). Other frameworks propose allowing LLMs to summarize sentences into premises and conclusions, then aggregate this extracted information into another prompt to enable Logic inference (Sun et al., 2024a; Wang et al., 2024; Xu et al., 2024a)."
        },
        {
            "title": "4 COMPUTATIONAL EXPRESSIVITY OF LLMS",
            "content": "Basic Transformer Architecture. Basic transformers, devoid of intermediate decoding steps, exhibit limited computational expressivity (Merrill & Sabharwal, 2023; Chiang et al., 2023), aligning with the relatively small circuit complexity class 0 (Merrill & Sabharwal, 2023). These basic transformers fall short of Turing completeness, as they are incapable of solving problems that are complete for classes larger than 0, such as simulating automata, which is 1-complete. Decoding-based Transformers. Decoding-based transformers generate output sequentially, word by word, rather than producing single answer. This approach enhances their computational expressivity compared to basic transformers, with expressivity increasing in tandem with the length of the decoding steps (Merrill & Sabharwal). This phenomenon elucidates why the Chain-of-Thought (CoT) reasoning process (Wei et al., 2022) augments the computational expressivity of LLMs (Feng et al., 2023). Some studies demonstrate that with linear steps, transformers equipped with projected-norm can theoretically simulate Turing automaton (Merrill & Sabharwal). Recent research indicates that autoregressive decoding, which facilitates the processing of arbitrarily long input strings, can simulate universal Turing machine (Schuurmans et al., 2024). Decoding with External Memory. Research suggests that external memory can enhance the computational expressivity of LLMs (Deletang et al., 2023), potentially endowing them with approximate Turing completeness (Perez et al., 2021). Recent advancements have introduced the Stack-Attention mechanism to further augment the reasoning capabilities of LLMs (Li et al., 2024a). With the integration of external memory and simple regular expression parsers, transformers can simulate the execution of universal Turing machine, specifically U15,2 (Schuurmans, 2023)."
        },
        {
            "title": "5 MULTI-STEP REASONING",
            "content": "The Chain-of-Thought (CoT) reasoning paradigm demonstrates that engaging in detailed, step-by-step reasoning can significantly enhance the performance of Large Language Models (LLMs) compared to single-step reasoning (Wei et al., 2022). This improvement arises because single-step reasoning may overlook crucial intermediate steps that are instrumental in problem-solving (Wei et al., 2022). The multi-step reasoning process, inspired by human cognitive processes, can substantially elevate the performance of LLMs (Wei et al., 2022). Single LLM Call. CoT exemplifies single LLM call, utilizing the model once. Beyond explicit prompting to initiate detailed reasoning, recent studies propose enabling LLMs to execute advanced search algorithms during the decoding process, such as Monte-Carlo Tree Search (MCTS) (Leblond et al., 2021) or Q-star search (Chakraborty et al., 2024). Additionally, some research suggests employing backtracking algorithms to allow LLMs to reconsider previous decisions, thereby enhancing final performance (Fu et al.). Multiple LLM Calls. Some approaches advocate for multiple LLM calls, which operate independently of each other, potentially yielding correct answers across these calls (Brown et al., 2024). Beyond the single CoT call, CoT-SC proposes multiple CoT-based LLM calls, selecting the optimal answer to improve final outcomes (Wang et al., b). However, these answers exhibit direct dependencies. To optimize scheduling and decomposition of the reasoning process, Tree-of-Thought (ToT) reasoning (Yao et al., 2024) and Graph-of-Thought (GoT) reasoning (Besta et al., 2024) have been introduced, structuring reasoning steps in tree-like or graph-like configurations. Some studies also suggest integrating knowledge graphs, enabling LLMs to reason within graph structures to enhance 4 Published at ICLR Blogposts 2025 reasoning capabilities (LUO et al.; Sun et al.). Structuring prompts into triplets using LLMs can further bolster reasoning abilities (Jiang et al., 2023). In the absence of centralized controller, some research proposes simulating multiple agents with LLMs to collaboratively address problems (Li et al., 2023; Hong et al., 2024; Liang et al., 2023; Du et al.). Planning and Scheduling. The essence of multi-step reasoning lies in decomposing the original problem into multiple sub-problems and addressing them sequentially. This process involves planning and scheduling. To facilitate autonomous planning and scheduling, recent studies propose employing LLMs as meta-agents to orchestrate planning and scheduling, wherein the original problem is decomposed, and the meta-agent delegates sub-problems to other LLMs based on the schedule (Hong et al., 2024; Wu et al.; Zhou et al.; Wang et al., a). With the aid of external symbolic reasoning, LLMs can also engage in planning and scheduling to resolve problems (Zhang et al., 2023a)."
        },
        {
            "title": "6 LOTTERY LLM HYPOTHESIS",
            "content": "Consider an original language model fθ parameterized by the θ Rkθ , capable of processing input of token length n, and an input problem Rmh with token length < and ground truth µ Rlh. The problem is question consisting of sequence of words. And the µ is also sequence of words representing the answer to the question q. is the dimension of the word embedding. The performance of the model is evaluated using performance measure (), expressed as (fθ(q), µ) which map its inputs as scalar value. We hypothesize the existence of smaller language model gϕ with parameters ϕ Rkϕ (kϕ < kθ) and the same input length n, which can solve the problem with performance comparable to fθ, such that: (fθ(q), µ) (Agϕ,D,R,C,M(q), µ), (1) ch Rno where represents reasoning algorithm that may involve one or multiple invocations of gϕ with various inputs, including the original problem q, documents retrieved from the external knowledge base D, or function calls retrieved from external tools using the retriever R. Each document Rndh is vector of words. While the function calls : Rni is provided function. The knowledge base is vector database storing vector-documents as key-value pairs, and denotes the external memory that stores intermediate results. All D, C, and are sets. And items in and are key-value pairs depends on the specific tasks, like vector database (Pan et al., 2024). The retriever is function that retrieves the required documents or function calls from the or based on the request. And its specific implementation can be various (Gao et al., 2023b). The reasoning algorithm is described as Algorithm 1 in Figure 1 and Figure 2, employing divide-and-conquer strategy to solve the original problem q. This dynamic divide-and-conquer methodology is versatile and applicable to numerous contemporary reasoning algorithms. Figure 1: general pseudo code of the reasoning algorithm A. Recursive and Dynamic Scheduling. Algorithm 1 can encompass tree-based reasoning methods such as Tree-of-Thought (ToT) (Zhou et al.; Yao et al., 2024), due to its recursive design that facilitates tree search and allows the branch-or-solve mechanism to be dynamically determined by LLMs. Additionally, Algorithm 1 is applicable to graph-based reasoning methods like Graph-of-Thought (GoT) (Besta et al., 2024; LUO et al.; Sun 5 Published at ICLR Blogposts 2025 et al.), as the interaction between different LLMs and the external memory can be conceptualized as combination in GoT, where outputs from various nodes are integrated to construct the graph structure. External Knowledge and Tools. During each phase of problem-solving, Algorithm 1 initially assesses whether the problem can be directly addressed using the external knowledge base or external tools C. If so, Algorithm 1 utilizes gϕ to evaluate the problem and ascertain the necessary knowledge or tools required for its resolution. Subsequently, based on the generated requests, the retriever searches for external knowledge or tool to provide the requisite results. These supplementary results are then integrated with the problem for resolution by the model gϕ. This framework facilitates the application of Retrieval Augmented Generation (RAG) and external tools, such as arithmetic calculation functions, Internet search engines, and logic solvers, to effectively address the problem q. Figure 2: The problem solving process of the multistep reasoning with external tools (the interaction with the external memory and the verification are not shown in the figure). External Memory. The external memory functions as repository for storing intermediate results throughout the reasoning process. When tackling various sub-problems, intermediate results can be stored in the external memory for reuse in subsequent steps. By interacting with the external memory, Algorithm 1 can emulate reasoning methods that utilize working memory (Wang et al., 2024). The structure of the Divide and Conquer function in Algorithm 1 is not constrained. Through careful design and programming, the recursive mechanism can execute fundamental operations such as MOV, COPY, JUMP, and WRITE and READ from the external memory, thereby simulating Turing machine (Schuurmans, 2023), as depicted in Figure 3. Most of previous model compression (Sun et al., 2024b; Frantar & Alistarh, 2023) (Yao et al., 2022; Dettmers & Zettlemoyer, 2022) and KV cache compression methods (Zhang et al., 2023b; Xiao et al., 2024) only focus on the guaranteeing the model performance on the perplexity metric (Merity et al., 2016) or some downstream tasks like the common sense knowledge (Hendrycks et al., 2021; Talmor et al., 2019) and the basic arithmetic problems (Cobbe et al., 2021). From the above analysis and the procedures of the Algorithm 1, we can see that there are some other crucial abilities that the lottery LLM and other compression methods must take for considering. We summarize the crucial abilities that the lottery LLM should have as follows. Figure 3: Simulating the Turing machine with LLMs and the external memory (Schuurmans, 2023). Ability 1: Retrieval from prompts. Obviously, the useful information in the prompts that related to address the problem is crucial for the lottery LLM. After collecting the required external results into the prompt, the LLM gϕ needs to be able to retrieve the required information from the prompt and avoid the interruption of some irrelevant information. This is related to the retrieval ability of the LLM and its measurement test is like the well-known needle-inthe-haystack(NIAH) test (Kamradt, 2023). We show that there is simple and interesting method to endow the LLM with advanced retrieval ability with preprocessing prompts, by applying the embedding to retrieve the related information about the question in problem and combine them with the question to prompt the LLM gϕ rather let the LLM gϕ to process the original long context information of problem q. The figures illustrate that preprocessing prompts markedly enhances the performance of LLMs on the NIAH test. Importantly, even when the input length surpasses the models context size (8K tokens for LLaMA3-8B-Instruct), there is no observed degradation in performance. This indicates the potential of utilizing preprocessed prompts to augment the retrieval capabilities of LLMs. 6 Published at ICLR Blogposts 2025 Figure 4: Vanilla NIAH results of LLaMA3-8B-Instruct. Figure 5: NIAH results of LLaMA3-8B-Instruct with preprocessing prompts. Ability 2: Identification of Required External Resources. To effectively determine which external resources to utilize, such as knowledge databases or external tools, the LLM gϕ must possess the capability to comprehend and correlate the problem and its associated sub-problems with the relevant resources. Consequently, gϕ should have foundational knowledge of the problem and the external resources. Additionally, it must exhibit strong ability to associate queries with the available resources. When external tools are adeptly employed, the performance of smaller LLMs can be significantly enhanced. The subsequent table presents the results of arithmetic problem-solving using various LLMs and methodologies. The PAL (Gao et al., 2023a) approach, which employs external arithmetic calculation functions, demonstrates substantial improvement in the performance of smaller LLMs. GSM8K SVAMP ASDIV ADDSUB MULTIARITH DIRECT Codex CoT UL2-20B CoT LaMDA-137B CoT Codex CoT PaLM-540B CoT Minerva 540B PAL (Gao et al., 2023a) 19.7 4.1 17.1 65.6 56.9 58.8 72.0 69.9 12.6 39.9 74.8 79.0 - 79.4 74.0 16.9 49.0 76.9 73.9 - 79.6 90.9 18.2 52.9 86.0 91.9 - 92.5 44.0 10.7 51.8 95.9 94.7 - 99. Table 1: Arithmetic problem-solving results using various LLMs and methodologies. Besides, with provided the external documents, following results (Asai et al., 2024) show that the small LLM (Llama-3-Ins8B) show the superb performance in many QA tasks (Mallen et al., 2023b; Kwiatkowski et al., 2019; Stelmakh et al., 2022) than the large LLMs (Llama-3-Ins70B and ChatGPT-4oMINI). Ability 3: Planning and Scheduling. To effectively decompose the problem into multiple subproblems and address them sequentially, the LLM gϕ must possess robust planning and scheduling capabilities. This competency is essential for the lottery LLM to tackle complex problems efficiently. Consequently, the LLM gϕ should have comprehensive understanding of both the primary problem and its constituent sub-problems. However, the intricate details of solving these sub-problems may not be necessary for the LLM gϕ, as external resources can be leveraged to resolve them. Moreover, proficient scheduling is crucial for the lottery LLM to enhance reasoning efficiency. The table below illustrates the performance of LLMs using simple inference compared to those employing strategy of decomposing the problem into sub-problems and utilizing external logic solvers, such as Logic-LM (Pan et al., 2023). The used five datasets are commonly used in the 7 Published at ICLR Blogposts LLM Method CoT without RAG Llama-3-Ins8B CoT without RAG Llama-3-Ins70B CoT without RAG ChatGPT-4oMINI With RAG Llama-3-Ins8B PopQA (acc) NQ (acc) ASQA (str-em) ASQA (hit) 24.8 31.6 32.4 59.8 44.0 54.4 53.2 54. 28.8 36.4 32.4 38.8 7.8 11.2 8.0 14.0 Table 2: QA task performance with and without RAG. logical reasoning tasks. Notably, we emphasize the results (Pan et al., 2023) (simple inference/with Logic-LM) of GPT-3.5, which, despite being less advanced than GPT-4, demonstrates comparable performance to GPT-4 (GPT-3.5 with Logic-LM compared with GPT-4 with simple inference). Thus, with advanced reasoning algorithms, the weaker LLMs can outperform the stronger LLMs in advanced tasks. Dataset PrOntoQA ProofWriter FOLIO LogicalDeduction AR-LSAT ChatGPT (gpt-3.5-turbo) GPT-3.5 (text-davinci-003) GPT-4 (gpt-4) 77.40 / 83.20 52.67 / 79.66 69.11 / 78.92 71.33 / 87.63 33.33 / 43. 51.80 / 85.00 36.16 / 71.45 54.60 / 61.27 41.33 / 62.00 22.51 / 25.54 47.40 / 61.00 35.50 / 58.33 45.09 / 62.74 40.00 / 65.67 20.34 / 26.41 Table 3: Performance of LLMs using simple inference and Logic-LM (Pan et al., 2023). Ability 4: Precise Approximation of Fundamental Operations. As discussed in the section on the computational expressivity of LLMs, achieving (approximate) Turing completeness necessitates that the LLM gϕ precisely approximates fundamental operations such as MOV, COPY, JUMP, and WRITE and READ from external memory (Schuurmans et al., 2024; Schuurmans, 2023). Although these operations may not be directly employed in problem-solving, they are essential for the lottery LLM to function as potential meta-agent (Hong et al., 2024). Ability 5: Long-Context Reasoning. In single-step reasoning, an extended context length allows the LLM gϕ to access and utilize more information for problem-solving. In multi-step reasoning, the prompt serves as form of working memory for the meta-agent, or planner (controller). Each result from solved sub-problems should be incorporated into the prompt for subsequent steps. As problem complexity increases, so does the depth of the sub-problem tree. Therefore, the LLM gϕ must possess the ability for extended contextual reasoning to support deep tree reasoning (Merrill & Sabharwal; Feng et al., 2023)."
        },
        {
            "title": "7 CONCLUSION",
            "content": "This blog aims to elucidate the potential of the lottery LLM and to summarize the essential capabilities that the lottery LLM should possess, which are currently lacking in existing methods of LLM and KV cache compression. The discussion on redundant knowledge within LLMs also highlights the trade-off between knowledge storage and reasoning capabilities. With the development of the lottery LLM, alongside external tools, knowledge bases, and robust algorithm A, there is potential for the lottery LLM to function as meta-agent akin to human cognition. Its external memory could serve as long-term memory, the prompt as short-term memory, and the LLM inference process gϕ as the fundamental cognitive process. External tools and knowledge bases can be considered as supplementary tools commonly used in daily life. Deploying the lottery LLM could significantly reduce energy and resource consumption in large-scale LLM-driven applications. Future research on LLM compression, KV cache compression, and other efficient LLM methodologies should address both efficiency and the essential capabilities of LLMs."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This work was partially supported by National Natural Science Foundation of China under Grant No. 62272122, the Guangzhou Municipal Joint Funding Project with Universities and Enterprises under Grant No. 2024A03J0616, Guangzhou Municipality Big Data Intelligence Key Lab (2023A03J0012), and Hong Kong CRF grants under Grant No. C7004-22G and C6015-23G, contract R6021-20, and RGC GRF grants under the contracts 16200221, 16207922 and 16207423, the MOE Academic Research Fund (AcRF) Tier 1 Grant in Singapore (Grant No. T1 251RES2315). 8 Published at ICLR Blogposts"
        },
        {
            "title": "REFERENCES",
            "content": "Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matt Stallone, Rameswar Panda, Yara Rizk, G. Bhargav, M. Crouse, Chulaka Gunasekara, S. Ikbal, Sachin Joshi, Hima P. Karanam, Vineet Kumar, Asim Munawar, S. Neelam, Dinesh Raghu, Udit Sharma, Adriana Meza Soria, Dheeraj Sreedhar, P. Venkateswaran, Merve Unuvar, David Cox, S. Roukos, Luis A. Lastras, and P. Kapanipathi. Granite-function calling model: Introducing function calling abilities via multi-task learning of granular tasks. In Conference on Empirical Methods in Natural Language Processing, 2024. URL https://www.semanticscholar.org/paper/ cbde6f07977255cd5b571d93b497aa2874a7a544. Maryam Amirizaniani, Elias Martin, Maryna Sivachenko, Afra Mashhadi, and Chirag Shah. Can llms reason like humans? assessing theory of mind reasoning in llms for open-ended questions. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM 24, pp. 3444, 2024. ISBN 9798400704369. Konstantine Arkoudas. Gpt-4 cant reason, 2023. URL https://arxiv.org/abs/2308. 03762. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= hSyW5go0v8. Sourav Banerjee, Ayushi Agarwal, and Saloni Singla. Llms will always hallucinate, and we need to live with this. arXiv preprint arXiv:2409.05746, 2024. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1768217690, 2024. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Souradip Chakraborty, Soumya Suvra Ghosal, Ming Yin, Dinesh Manocha, Mengdi Wang, Amrit Bedi, and Furong Huang. Transfer q-star : Principled decoding for LLM alignment. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https: //openreview.net/forum?id=5PrShrKxoX. Yuxuan Chen, Daniel Röder, Justus-Jonas Erker, Leonhard Hennig, Philippe Thomas, Sebastian Möller, and Roland Roller. Retrieval-augmented knowledge integration into language models: survey. In Sha Li, Manling Li, Michael JQ Zhang, Eunsol Choi, Mor Geva, Peter Hase, and Heng Ji (eds.), Proceedings of the 1st Workshop on Towards Knowledgeable Language Models (KnowLLM 2024), pp. 4563, Bangkok, Thailand, August 2024. Association for Computational Linguistics. David Chiang, Peter Cholak, and Anand Pillay. Tighter bounds on the expressivity of transformer encoders. In International Conference on Machine Learning, pp. 55445562. PMLR, 2023. Dami Choi, Vincent Huang, Kevin Meng, Daniel Johnson, Jacob Steinhardt, and Sarah https://transluce.org/ Scaling automatic neuron description. Schwettmann. neuron-descriptions, October 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Gregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WbxHAzkeQcn. 9 Published at ICLR Blogposts 2025 Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, 2022. Peijie Dong, Lujun Li, Zhenheng Tang, Xiang Liu, Xinglin Pan, Qiang Wang, and Xiaowen Chu. In Pruner-zero: Evolving symbolic pruning metric from scratch for large language models. Forty-first International Conference on Machine Learning. Peijie Dong, Lujun Li, Yuedong Zhong, Dayou Du, Ruibo Fan, Yuhan Chen, Zhenheng Tang, Qiang Wang, Wei Xue, Yike Guo, et al. Stbllm: Breaking the 1-bit barrier with structured binary llms. arXiv preprint arXiv:2408.01803, 2024. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: theoretical perspective. Neural Information Processing Systems, 2023. URL https://www.semanticscholar.org/paper/ c2260403fd5cb2de73491323433e48b6ec36872c. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. ArXiv, abs/2301.00774, 2023. Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. In Forty-first International Conference on Machine Learning. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1076410799. PMLR, 2329 Jul 2023a. URL https://proceedings. mlr.press/v202/gao23f.html. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023b. Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, and Yongfeng Zhang. Llm as os (llmao), agents as apps: Envisioning aios, agents and the aios-agent ecosystem. arXiv preprint arXiv:2312.03815, 2023. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022. Joy He-Yueya, Gabriel Poesia, Rose Wang, and Noah Goodman. Solving math word problems by combining language models with symbolic solvers. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS23. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob In International ConferSteinhardt. Measuring massive multitask language understanding. ence on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VtmBAGCN7o. 10 Published at ICLR Blogposts 2025 Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 2023. AJAY KUMAR JAISWAL, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, and Yinfei Yang. Compressing LLMs: The truth is rarely pure and never simple. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=B9klVS7Ddk. Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jaewoo Kang. Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models. Bioinformatics, 40(Supplement_1):i119i129, 2024a. Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 70297043, 2024b. Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. Structgpt: general framework for large language model to reason over structured data. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 92379251, 2023. Subbarao Kambhampati. Can large language models reason and plan? Annals of the New York Academy of Sciences, 2024. URL https://www.semanticscholar.org/paper/ f531d1a681ed12fd582767133318d0728316a0ae. Gregory Kamradt. Needle In Haystack - pressure testing LLMs. Github, 2023. URL https: //github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. Rémi Leblond, Jean-Baptiste Alayrac, L. Sifre, Miruna Pislar, Ioannis Antonoglou, K. Simonyan, ing beyond beam search. guage Processing, 2021. e8cc5b6204970a88cd1b2df491aa10c4333e083e. Jean-Baptiste Lespiau, Machine translation decodIn Conference on Empirical Methods in Natural LanURL https://www.semanticscholar.org/paper/ and O. Vinyals. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. Jiaoda Li, Jennifer C. White, Mrinmaya Sachan, and Ryan Cotterell. transformer with stack attention. NAACL-HLT, 2024a. URL https://www.semanticscholar.org/paper/ 5d7946b41dea3ed11dad48a53b1390d59b8c564e. Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, and Jun Huang. AlphaFin: Benchmarking financial analysis with retrieval-augmented stock-chain framework. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pp. 773783. ELRA and ICCL, May 2024b. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023. LINHAO LUO, Yuan-Fang Li, Reza Haf, and Shirui Pan. Reasoning on graphs: Faithful and interpretable large language model reasoning. In The Twelfth International Conference on Learning Representations. 11 Published at ICLR Blogposts 2025 Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 98029822, Toronto, Canada, July 2023a. Association for Computational Linguistics. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 98029822, 2023b. Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building large annotated corpus of english: The penn treebank. Computational Linguistics, 1993. URL https://aclanthology. org/J93-2004.pdf. Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent operating system. arXiv.org, 2024. URL https://www.semanticscholar.org/ paper/f89e85059a55b647c93822aefa7e985376e0ef20. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. 2016. URL https://openreview.net/forum?id=Byj72udxe. William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. In The Twelfth International Conference on Learning Representations. William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 06 2023. Iman Mirzadeh, Keivan Alizadeh-Vahid, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv.org, 2024. URL https://www.semanticscholar.org/ paper/05506581cade1a8ef6372616cec20b81a3d5c366. Alistair Moffat. Huffman coding. ACM Comput. Surv., 52(4), August 2019. ISSN 0360-0300. doi: 10.1145/3342555. URL https://doi.org/10.1145/3342555. Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and J. Steinhardt. Progress meaInternational Conference on LearnURL https://www.semanticscholar.org/paper/ sures for grokking via mechanistic interpretability. ing Representations, 2023. f680d47a51a0e470fcb228bf0110c026535ead1b. OpenAI. Gpt-4 technical report, 2023. James Jie Pan, Jianguo Wang, and Guoliang Li. Survey of vector database management systems. The VLDB Journal, 33(5):15911615, July 2024. ISSN 1066-8888. URL https://doi.org/10. 1007/s00778-024-00864-x. Liangming Pan, Alon Albalak, Xinyi Wang, and William Wang. Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 38063824, Singapore, December 2023. Association for Computational Linguistics. Pratyush Patel, Esha Choukse, Chaojie Zhang, Íñigo Goiri, Brijesh Warrier, Nithish Mahalingam, and Ricardo Bianchini. Characterizing power management opportunities for llms in the cloud. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, ASPLOS 24, pp. 207222, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703867. doi: 10.1145/3620666. 3651329. Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing-complete. Journal of Machine Learning Research, 22(75):135, 2021. URL http://jmlr.org/papers/v22/20-302. html. Published at ICLR Blogposts 2025 Nicholas Pipitone and Ghita Houir Alami. Legalbench-rag: benchmark for retrieval-augmented generation in the legal domain. arXiv preprint arXiv:2408.10343, 2024. Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Toolllm: Facilitating large M. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. International Conference on Learnlanguage models to master 16000+ real-world apis. URL https://www.semanticscholar.org/paper/ ing Representations, 2023. 0bfc804e31eecfd77f45e4ee7f4d629fffdcd628. Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words to watts: Benchmarking the energy costs of large language model inference. In 2023 IEEE High Performance Extreme Computing Conference (HPEC), pp. 19. IEEE, 2023. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Toolformer: Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Language models can teach themselves In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 6853968551. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf. to use tools. Dale Schuurmans. Memory augmented large language models are computationally uniURL https://www.semanticscholar.org/paper/ arXiv.org, 2023. versal. 7ec58d26c4dddb4bc3b6829fa0654a22cc26fdfe. Dale Schuurmans, Hanjun Dai, and Francesco Zanini. Autoregressive large language models are computationally universal. arXiv.org, 2024. URL https://www.semanticscholar.org/ paper/b72b981bd59ffb1536c7a9e92e43b882f36d3e5c. Heydar Soudani, Evangelos Kanoulas, and Faegheh Hasibi. Fine tuning vs. retrieval augmented generation for less popular knowledge. arXiv preprint arXiv:2403.01432, 2024. Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. Asqa: Factoid questions meet long-form answers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 82738288, 2022. Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, and Rui Yan. DetermLR: Augmenting LLM-based logical reasoning from indeterminacy to determinacy. In LunWei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 98289862. Association for Computational Linguistics, August 2024a. Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel Ni, Heung-Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large In The Twelfth International Conference on Learning language model on knowledge graph. Representations. Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. simple and effective pruning approach for large language models. In International Conference on Learning Representations, 2024b. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421. Zhenheng Tang, Yuxin Wang, Qiang Wang, and Xiaowen Chu. The impact of gpu dvfs on the In Proceedings of the Tenth energy and performance of deep learning: An empirical study. ACM International Conference on Future Energy Systems, e-Energy 19, pp. 315325, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450366717. doi: 10.1145/3307772.3328315. 13 Published at ICLR Blogposts 2025 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint, 2023. URL https://arxiv.org/ abs/2302.13971. Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still cant plan (a benchmark for LLMs on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. URL https:// openreview.net/forum?id=wUU-7XTL5XO. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, YunHsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. Freshllms: Refreshing large lanIn Annual Meeting of the Association for guage models with search engine augmentation. Computational Linguistics, 2023. URL https://www.semanticscholar.org/paper/ be177300487b6d0f25e6cade9a31900454b13281. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. Transactions on Machine Learning Research, a. Siyuan Wang, Zhongyu Wei, Yejin Choi, and Xiang Ren. Symbolic working memory enhances language models for complex rule application. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1758317604, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Grant Wilkins, Srinivasan Keshav, and Richard Mortier. Hybrid heterogeneous clusters can lower the energy consumption of llm inference workloads. In Proceedings of the 15th ACM International Conference on Future and Sustainable Energy Systems, pp. 506513, 2024. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. In ICLR 2024 Workshop on Large Language Model (LLM) Agents. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming In The Twelfth International Conference on Learning language models with attention sinks. Representations, 2024. URL https://openreview.net/forum?id=NG7sS51zVF. Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. Are large language models really good logical reasoners? comprehensive evaluation from deductive, inductive and abductive views. arXiv preprint arXiv:2306.09841, 2023. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong Li Lee, and Wynne Hsu. Faithful logical reasoning via symbolic chain-of-thought. In Annual Meeting of the Association for Computational Linguistics, 2024a. URL https://api.semanticscholar.org/CorpusID:270068130. Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large language models. arXiv preprint arXiv:2401.11817, 2024b. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: In A. Oh, T. NauTeaching large language model mann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7199572007. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf. to use tools via self-instruction. 14 Published at ICLR Blogposts 2025 Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In NeurIPS 2022 Foundation Models for Decision Making Workshop. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. ArXiv, abs/2206.01861, 2022. Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, and Xia Hu. KV cache compression, but what must we give in return? comprehensive benchmark of long context capable approaches. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 2024. Association for Computational Linguistics. Shizhuo Zhang, Curt Tigges, Stella Biderman, M. Raginsky, and T. Ringer. Can transformers learn to solve problems recursively? arXiv.org, 2023a. URL https://www.semanticscholar. org/paper/45c196d28d16b2a8c0a078e8b79bcb39887a8a9f. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2O: heavy-hitter oracle for efficient generative inference of large language models. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023b. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language In Forty-first agent tree search unifies reasoning, acting, and planning in language models. International Conference on Machine Learning."
        }
    ],
    "affiliations": [
        "CSE, The Hong Kong University of Science and Technology",
        "DSA, The Hong Kong University of Science and Technology (Guangzhou)",
        "National University of Singapore"
    ]
}