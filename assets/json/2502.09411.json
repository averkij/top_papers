{
    "paper_title": "ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation",
    "authors": [
        "Rotem Shalev-Arkushin",
        "Rinon Gal",
        "Amit H. Bermano",
        "Ohad Fried"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of Retrieval-Augmented Generation (RAG) with image generation models. We propose ImageRAG, a method that dynamically retrieves relevant images based on a given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models. Our project page is available at: https://rotem-shalev.github.io/ImageRAG"
        },
        {
            "title": "Start",
            "content": "ImageRAG: Dynamic Image Retrieval for Reference-Guided Image Generation Rotem Shalev-Arkushin 1 Rinon Gal 1 2 Amit H. Bermano 1 Ohad Fried 3 5 2 0 2 3 1 ] . [ 1 1 1 4 9 0 . 2 0 5 2 : r Figure 1: Using references broadens the generation capabilities of image generation models. Given text prompt, ImageRAG dynamically retrieves relevant images and provides them to base text-to-image model (T2I). ImageRAG works with different models, such as SDXL (A) or OmniGen (B, C), and different controls, e.g. text (A, B) or personalization (C)."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Diffusion models enable high-quality and diverse visual content synthesis. However, they struggle to generate rare or unseen concepts. To address this challenge, we explore the usage of RetrievalAugmented Generation (RAG) with image generation models. We propose ImageRAG, method that dynamically retrieves relevant images based on given text prompt, and uses them as context to guide the generation process. Prior approaches that used retrieved images to improve generation, trained models specifically for retrieval-based generation. In contrast, ImageRAG leverages the capabilities of existing image conditioning models, and does not require RAG-specific training. Our approach is highly adaptable and can be applied across different model types, showing significant improvement in generating rare and fine-grained concepts using different base models. Our project page is available at: https:// rotem-shalev.github.io/ImageRAG 1Tel Aviv University 2NVIDIA 3Reichman University. Correspondence roo@gmail.com>. to: Rotem Shalev-Arkushin <rotem1 Diffusion models (Ho et al., 2020) have recently revolutionized image generation, offering high-quality, diverse, and realistic visual content (Dhariwal & Nichol, 2021; Rombach et al., 2022). They enable text-to-image generation as well as wide range of tasks, from layout-based synthesis to image editing and style transfer (Avrahami et al., 2022; Hertz et al., 2022; Mokady et al., 2023; Avrahami et al., 2023b; Zhang et al., 2023; Brooks et al., 2023; Nitzan et al., 2024). Of course, these large models require great amounts of training data, substantial training durations, and extensive computational resources. As result, contemporary text-to-image (T2I) models, that are limited to the data they were trained on, struggle with generating user-specific concepts or updated content. Additionally, they have difficulty with generating rare concepts, stylized content, or fine-grained categories (e.g., specific dog breed), even if they were trained on images containing them (Samuel et al., 2024b; Haviv et al., 2024). In these cases, diffusion models tend to hallucinate, and potentially generate content unrelated to the textual prompt (see Figure 2). To tackle these problems, several approaches have been proposed for personalized (Gal et al., 2023a; Ruiz et al., 2023; Voynov et al., 2023; Arar et al., 2024), stylized (Hu et al., 2021), or rare concept generation (Li et al., 2024; Samuel et al., 2024b). ImageRAG"
        },
        {
            "title": "Model output",
            "content": "+Reference"
        },
        {
            "title": "ImageRAG",
            "content": ") S ( a ) i ( h Figure 2: Hallucinations. When models do not know the meaning of prompt, they may hallucinate and generate unrelated images (left). By applying our method to retrieve and utilize relevant references (mid), the base models can generate appropriate images (right). Most approaches, however, require training or specialized optimization techniques for each new concept. We note that similar problems exist with text generation using Large Language Models (LLMs). LLMs struggle to generate text based on real-world facts, proprietary or updated data, and tend to hallucinate when lacking sufficient knowledge (Brown, 2020; Ji et al., 2023). To solve these problems, the Natural Language Processing community has adopted Retrieval Augmented Generation (RAG) (Lewis et al., 2020). RAG is method for dynamically retrieving the most relevant information to given task from external sources and supplying it to an LLM as context input, enabling it to generate contextually accurate and task-specific responses. Investigating this idea for images, we notice previous works employing image retrieval for better image generation (Chen et al., 2022; Sheynin et al., 2022; Blattmann et al., 2022; Hu et al., 2024), are based on models trained specifically for the task, hindering wide applicability. In contrast, we propose ImageRAG, method that dynamically retrieves and provides images as references to pretrained T2I models, to enhance their generation capabilities. Our method does not require additional training over the retrieved content or specifically for RAG. Instead, we use T2I models in the same vein as the common use of LLMs, and use reference images during sampling for guided generation. Compared to the language case, in the visual domain, the context is more limited. Hence, we cannot simply provide references for all the concepts in prompt. We note that to apply RAG for image generation, we need to answer the questions of which images to use as context, how to retrieve them, and how to use the retrieved images to successfully generate required concept. In this work, we address these questions by offering novel method that dynamically chooses the most relevant and useful examples 2 given prompt, and uses them as references, to guide the model toward generating the required results. Leveraging T2I models ability to produce many concepts, we only pass concepts the models struggle to generate, focusing on the gaps. To understand what the challenging concepts are, we suggest novel method that dynamically chooses images to retrieve for given prompt using step-by-step method and Vision-Language Model (VLM). Specifically, we ask the VLM to identify missing image components and suggest concepts that could be retrieved, and use them as references to guide the generation. Our approach is not related to specific T2I model. To demonstrate it, we apply ImageRAG to two model types: T2I models designed to allow in-context learning (ICL) (Brown, 2020); and T2I models augmented with an IP-adapter imageencoder (Ye et al., 2023) that allows image prompting. In ICL, the generative model is provided with set of taskspecific examples in the prompt (as context input) and is expected to perform similar task on new input. ICL does not require additional training or large amounts of data, and offers way to adapt the model to new tasks or domains by supplying it with unseen information at inference time. IP-adapters are image-encoders that allow prompting T2I model with images, as adapters for existing models. Both types show great potential for improved image generation, with impressive results (Ye et al., 2023; Xiao et al., 2024; Gu et al., 2024; Wang et al., 2023; Najdenkoska et al., 2024; Sun et al., 2024). We demonstrate the effectiveness of our method for image generation of rare and fine-grained concepts on two models, representing the two model types: Omnigen (Xiao et al., 2024) model that was designed to allow ICL; and SDXL (Podell et al., 2024)+IP-adapter (Ye et al., 2023) models that allow image prompting through the IP-adapter image-encoder. We perform quantitative and qualitative comparisons on both models, and show that RAG enhances their rare and fine-grained concept generation capabilities. These results indicate that the image generation community should also consider using RAG for class or task-specific generation during sampling time. 2. Related Work 2.1. In-Context Learning In-context learning (ICL) has emerged as powerful paradigm in which large language models (LLMs) are capable of performing new tasks without additional fine-tuning (Brown, 2020). By providing few examples or relevant context directly in the input prompt, ICL enables models to infer the desired task and generate appropriate outputs. Despite its flexibility, ICL is limited by the finite context window of the model, making the selection of relevant and concise context critical for optimal performance. ImageRAG 2.2. Visual In-Context Learning Recently, visual ICL presented promising results (Gu et al., 2024; Wang et al., 2023; Xiao et al., 2024; Najdenkoska et al., 2024; Sun et al., 2024). Visual ICL has mostly been explored in the context of learning from analogies (Gu et al., 2024; Wang et al., 2023; Xiao et al., 2024; Nguyen et al., 2024). However, the ability of learning from single examples has also been researched with multimodal generative models that allow images as input (Xiao et al., 2024; Sun et al., 2024; Wang et al.). Such models, which allow image prompting, facilitate exploring RAG for image generation. 2.3. Retrieval Augmented Generation (RAG) RAG (Lewis et al., 2020) is method to improve the generation abilities of pretrained model without additional training, by dynamically retrieving and supplying information as context in the prompt. The most relevant information for given query is retrieved from an external database, and supplied to the model as context for improved generation that relies on the context data. While RAG has been greatly explored for text generation tasks and applied over multiple pretrained LLMs (Lewis et al., 2020; Gao et al., 2023; Ram et al., 2023; Zhang et al., 2025), it has yet to be explored for enhancing the capabilities of pretrained text-to-image models. Some previous work used nearest-neighbor image retrieval to improve image generation (Sheynin et al., 2022; Blattmann et al., 2022; Chen et al., 2022; Hu et al., 2024), however they train models specifically for retrieval-aided generation. Unlike them, our method leverages pretrained models and does not require additional training. 2.4. Text-to-Image Generation Text-to-image generation advanced greatly with the introduction of diffusion models (Ho et al., 2020), which can produce high-quality and diverse images of wide range of concepts (Dhariwal & Nichol, 2021; Rombach et al., 2022; Podell et al., 2024; Xiao et al., 2024). However, they still struggle with rare concepts and cannot generate user-specific concepts without additional training or optimization. Personalization works generate images of user-specific concept. However, they often require an optimization process for learning each new concept (Nitzan et al., 2022; Gal et al., 2023a; Ruiz et al., 2023; Arar et al., 2024; Alaluf et al., 2023; Voynov et al., 2023; Avrahami et al., 2023a; Kumari et al., 2023). To mitigate this challenge, recent works train image-encoders that allow prompting existing pretrained generative models with images (Ye et al., 2023; Gal et al., 2023b; Wei et al., 2023; Shi et al., 2024; Gal et al., 2024; Patashnik et al., 2025). Rare Concept Generation works that explored generating rare concepts without image retrieval, used few examples of each rare concept to optimize seeds that produce images similar to the references (Samuel et al., 2024a;b). However, in addition to the requirement of an optimization process per new concept, these works also do not address the questions of how to choose or find the reference images. 3. Method Our goal is to increase the robustness of T2I models, particularly with rare or unseen concepts, which they struggle to generate. To do so, we investigate retrieval-augmented generation approach, through which we dynamically select images that can provide the model with missing visual cues. Importantly, we focus on models that were not trained for RAG, and show that existing image conditioning tools can be leveraged to support RAG post-hoc. As depicted in Fig. 3, given text prompt and T2I generative model, we start by generating an image with the given prompt. Then, we query VLM with the image, and ask it to decide if the image matches the prompt. If it does not, we aim to retrieve images representing the concepts that are missing from the image, and provide them as additional context to the model to guide it toward better alignment with the prompt. In the following sections, we describe our method by answering key questions: (1) How do we know which images to retrieve? (2) How can we retrieve the required images? and (3) How can we use the retrieved images for unknown concept generation? By answering these questions, we achieve our goal of generating new concepts that the model struggles to generate on its own. 3.1. Which images to retrieve? The amount of images we can pass to model is limited, hence we need to decide which images to pass as references to guide the generation of base model. As T2I models are already capable of generating many concepts successfully, an efficient strategy would be passing only concepts they struggle to generate as references, and not all the concepts in prompt. To find the challenging concepts, we utilize VLM and apply step-by-step method, as depicted in the bottom part of Fig. 3. First, we generate an initial image with T2I model. Then, we provide the VLM with the initial prompt and image, and ask it if they match. If not, we ask the VLM to identify missing concepts and focus on content and style, since these are easy to convey through visual cues. As demonstrated in Tab. 3, empirical experiments show that image retrieval from detailed image captions yields better results than retrieval from brief, generic concept descriptions. Therefore, after identifying the missing concepts, we ask the VLM to suggest detailed image captions for images that describe each of the concepts. 3.1.1. ERROR HANDLING The VLM may sometimes fail to identify the missing concepts in an image, and will respond that it is unable to 3 ImageRAG Figure 3: Top: high-level overview of our method. Given text prompt <p>, we generate an initial image using text-to-image (T2I) model. Then, we generate retrieval-captions <cj>, retrieve images from an external database for each caption <ij>, and use them as references to the model for better generation. Bottom: the retrieval-caption generation block. We use VLM to decide if the initial image matches the given prompt. If not, we ask it to list the missing concepts, and to create caption that could be used to retrieve appropriate examples for each of these missing concepts. respond. In these rare cases, we allow up to 3 query repetitions, while increasing the query temperature in each repetition. Increasing the temperature allows for more diverse responses by encouraging the model to sample less probable words. In most cases, using our suggested stepby-step method yields better results than retrieving images directly from the given prompt (see Sec. 5.3). However, if the VLM still fails to identify the missing concepts after multiple attempts, we fall back to retrieving images directly from the prompt, as it usually means the VLM does not know what is the meaning of the prompt. The used prompts can be found in Appendix A. Next, we turn to retrieve images based on the acquired image captions. 3.2. How to retrieve the required images? Given image captions, our goal is to retrieve the images that are most similar to these captions from dataset. To retrieve images matching given image caption, we compare the caption to all the images in the dataset using text-image similarity metric and retrieve the top most similar images. Text-to-image retrieval is an active research field (Radford et al., 2021; Zhai et al., 2023; Ray et al., 2024; Vendrow et al., 2024), where no single method is perfect. Retrieval is especially hard when the dataset does not contain an exact match to the query (Biswas & Ramnath, 2024) or when the task is fine-grained retrieval, that depends on subtle details (Wei et al., 2022). Hence, common retrieval workflow is to first retrieve image candidates using pre-computed embeddings, and then re-rank the retrieved candidates using different, often more expensive but accurate, method (Vendrow et al., 2024). Following this workflow, we experimented with cosine similarity over different embeddings, and with multiple re-ranking methods of reference candidates. Although re-ranking sometimes yields better results compared to simply using cosine similarity between CLIP (Radford et al., 2021) embeddings, the difference was not significant in most of our experiments. Therefore, for simplicity, we use cosine similarity between CLIP embeddings as our similarity metric (see Tab. 4, Sec. 5.3 for more details about our experiments with different similarity metrics). 3.3. How to use the retrieved images? Putting it all together, after retrieving relevant images, all that is left to do is to use them as context so they are beneficial for the model. We experimented with two types of models; models that are trained to receive images as input in addition to text and have ICL capabilities (e.g., OmniGen (Xiao et al., 2024)), and T2I models augmented with an image encoder in post-training (e.g., SDXL (Podell et al., 2024) with IP-adapter (Ye et al., 2023)). As the first model type has ICL capabilities, we can supply the retrieved images as examples that it can learn from, by adjusting the original prompt. Although the second model type lacks true ICL capabilities, it offers image-based control functionalities, which we can leverage for applying RAG over it with our method. Hence, for both model types, we augment the input prompt to contain reference of the retrieved images as examples. Formally, given prompt p, concepts, and compatible images for each concept, we use the following template to create new prompt: According to these examples of <c1>:<img1,1>, ..., <img1,k>, ..., <cn>:<imgn,1>, ..., ImageRAG <imgn,k>, generate <p>, where ci for [1, n] is compatible image caption of the image <imgi,j>, [1, k]. This prompt allows models to learn missing concepts from the images, guiding them to generate the required result. Personalized Generation: For models that support multiple input images, we can apply our method for personalized generation as well, to generate rare concept combinations with personal concepts. In this case, we use one image for personal content, and 1+ other reference images for missing concepts. For example, given an image of specific cat, we can generate diverse images of it, ranging from mug featuring the cat to lego of it or atypical situations like the cat writing code or teaching classroom of dogs  (Fig. 4)  . Figure 4: Personalized generation example. ImageRAG can work in parallel with personalization methods and enhance their capabilities. For example, although OmniGen can generate images of subject based on an image, it struggles to generate some concepts. Using references retrieved by our method, it can generate the required result. 4. Implementation Details We use subset of LAION (Schuhmann et al., 2022) containing 350K images as the dataset from which we retrieve images. As retrieval similarity metric, we use CLIP ViTB/32. For VLM we use GPT-4o-2024-08-06 (Hurst et al., 2024) with temperature of 0 for higher consistency (unless GPT fails to find concepts, see Sec. 3.1.1). Full GPT prompts are supplied in Appendix A. As our T2I generation base models we use SDXL (Podell et al., 2024) with the ViT-H IP-adapter (Ye et al., 2023) plus version (ip-adapterplus sdxl vit-h), using 0.5 for the ip adapter scale, and OmniGen (Xiao et al., 2024) with the default parameters (2.5 guidance scale, 1.6 image guidance scale, and resolution of 1024x1024). As OmniGen only supports 3 images as context, we use up to = 3 concepts for each prompt and = 1 images per concept. For SDXL, as the IP-adapter we used is limited to 1 image, we use 1 concept and 1 image. 5 5. Experiments In this section, we describe quantitative and qualitative experiments performed to evaluate the effectiveness of our method. To assess its adaptability to different model types, we experiment with applying ImageRAG to two models, namely OmniGen (Xiao et al., 2024) and SDXL (Podell et al., 2024), each representing different model type. 5.1. Quantitative comparisons We evaluate the ability of our method to improve T2I generation of rare and fine-grained concepts by comparing the results of OmniGen and SDXL with their results when applying ImageRAG to them. As additional baselines, we compare with FLUX (Labs, 2023), Pixart-Σ (Chen et al., 2025), and GraPE (Goswami et al., 2024). The last is an iterative LLM-based image generation method which employs editing tools to insert missing objects. We use their OmniGenbased version. We use each method to generate images of each class in the following datasets: ImageNet (Deng et al., 2009), iNaturalist (Van Horn et al., 2018), CUB (Wah et al., 2011), and Aircraft (Maji et al., 2013). For iNaturalist, we use the first 1000 classes. Tab. 1 shows evaluation results of all methods using CLIP (Radford et al., 2021), SigLIP (Zhai et al., 2023) and DINO (Zhang et al.) similarities. For fairness, we use open-CLIP for evaluation, while using OpenAI CLIP for retrieval. As demonstrated in Tab. 1, both OmniGen and SDXL results improve when using our method for the generation of rare concepts and fine-grained categories. 5.2. Proprietary Data Generation common use for RAG in NLP is generation based on proprietary data (Lewis et al., 2020), where the retrieval dataset is proprietary one. similar application in image generation would be generating images based on proprietary gallery of images. It could be for personalization, where the gallery is of personal concept, e.g. images of persons dog, or it could be company brand or private collection of images that could broaden the knowledge of model. Our LAION-based experiments explored the scenario where user has access to general, large-scale set. Here, we further evaluate the performance of ImageRAG when we have access to potentially smaller, specialized dataset. Hence, we repeat the experiments with the datasets used in Tab. 1, but this time retrieve samples from within each dataset rather than from the LAION (Schuhmann et al., 2022) subset. Results are reported in Tabs. 2 and 5. We observe that although applying our method with the generic dataset of relatively small subset from LAION already improves the results, they improve even further when using the proprietary datasets for retrieval. ImageRAG Table 1: Comparisons on fine-grained image generation with text-to-image models. We use the ImageNet (Deng et al., 2009), iNaturalist (Van Horn et al., 2018), CUB (Wah et al., 2011), and Aircraft (Maji et al., 2013) datasets. For each set, we report mean ( standard error) CLIP, SigLIP text-to-image similarities, and DINO similarity between real and generated images. Middle rows feature OmniGen-based models, while the bottom features SDXL-based models. In each part, best results are bolded. CLIP ImageNet SigLIP DINO CLIP iNaturalist SigLIP DINO CLIP SigLIP DINO CLIP CUB Aircraft SigLIP DINO FLUX Pixart-Σ 0.262 0.001 0.262 0.001 0.132 0.001 0.121 0.001 0.711 0.003 0.691 0. 0.201 0.002 0.162 0.002 0.048 0.002 0.027 0.002 0.644 0.002 0.611 0.002 0.254 0.004 0.232 0.004 0.126 0.003 0.101 0.003 0.759 0.004 0.736 0. 0.243 0.007 0.160 0.008 0.116 0.006 0.054 0.006 0.725 0.011 0.634 0.011 OmniGen GraPE-O ImageRAG-O 0.247 0.002 0.251 0.002 0.264 0.001 0.122 0.001 0.123 0.001 0.134 0. 0.692 0.003 0.692 0.003 0.708 0.002 0.155 0.002 0.157 0.002 0.197 0.002 0.014 0.001 0.016 0.002 0.095 0.002 0.595 0.002 0.604 0.001 0.701 0.002 0.231 0.005 0.240 0.005 0.253 0.003 0.109 0.003 0.115 0.003 0.125 0. 0.747 0.005 0.747 0.005 0.760 0.003 0.181 0.010 0.191 0.009 0.228 0.006 0.073 0.007 0.073 0.007 0.103 0.005 0.656 0.013 0.647 0.013 0.747 0.010 SDXL 0.267 0.002 ImageRAG-SD 0.274 0. 0.136 0.001 0.141 0.001 0.700 0.003 0.709 0.002 0.259 0.002 0.243 0.002 0.096 0.002 0.118 0.001 0.698 0.003 0.724 0.002 0.315 0.001 0.314 0. 0.172 0.003 0.174 0.002 0.782 0.002 0.784 0.001 0.264 0.006 0.272 0.005 0.145 0.005 0.141 0.005 0.771 0.010 0.756 0.011 Table 2: Proprietary data usage experiment. Results for using each dataset as the retrieval-dataset (Proprietary- <model>) vs. using our subset from LAION as the retrieval-dataset (LAION-<model>). Here, indicates OmniGen based models, SD indicates SDXL based models. Best results for each model are bolded. CLIP ImageNet SigLIP DINO CLIP Aircraft SigLIP DINO LAION-O Proprietary-O 0.264 0.001 0.266 0.001 0.134 0.001 0.136 0.001 0.708 0.002 0.710 0. 0.228 0.006 0.244 0.007 0.103 0.005 0.109 0.005 0.747 0.010 0.786 0.010 LAION-SD 0.274 0.001 Proprietary-SD 0.288 0.001 0.141 0.001 0.142 0. 0.709 0.002 0.736 0.003 0.272 0.005 0.280 0.005 0.141 0.005 0.152 0.003 0.756 0.011 0.785 0.009 Table 3: Ablation studies. Rephrased prompt stands for only rephrasing the text prompt without giving additional images. Retrieve concepts stands for using the missing concepts directly instead of using more detailed image captions for retrieval, and Retrieve prompt stands for using the prompt directly for retrieval. Best results are bolded. OmniGen Rephrased prompt-O Retrieve concepts-O Retrieve prompt-O ImageRAG-O CLIP 0.247 0.002 0.248 0.002 0.258 0.002 0.258 0.002 0.264 0.001 SDXL 0.267 0.002 Rephrased prompt-SD 0.266 0.002 Retrieve concepts-SD 0.274 0.001 0.274 0.001 Retrieve prompt-SD 0.274 0.001 ImageRAG-SD ImageNet SigLIP 0.122 0.001 0.124 0.042 0.130 0.001 0.130 0.001 0.134 0.001 0.136 0.001 0.136 0.001 0.141 0.001 0.140 0.001 0.141 0.001 CUB DINO CLIP SigLIP DINO 0.692 0.003 0.696 0.003 0.694 0.003 0.691 0.003 0.708 0.002 0.700 0.003 0.705 0.003 0.702 0.003 0.702 0.003 0.709 0.002 0.231 0.005 0.230 0.005 0.240 0.004 0.246 0.004 0.253 0.003 0.315 0.001 0.309 0.003 0.312 0.002 0.314 0.001 0.314 0.001 0.109 0.003 0.107 0.004 0.113 0.003 0.120 0.003 0.125 0. 0.172 0.003 0.170 0.002 0.173 0.002 0.174 0.001 0.174 0.002 0.747 0.005 0.750 0.005 0.719 0.006 0.736 0.005 0.760 0.003 0.782 0.002 0.781 0.004 0.777 0.004 0.778 0.004 0.784 0.001 5.3. Ablation Studies To evaluate the contribution of each part of our method, we conduct an ablation study testing different components and report our results in Tab. 3. First, we want to ensure the performance gap is not based on simply interpreting rare words using an LLM. Hence, we evaluate OmniGen and SDXL over rephrased text prompts, without providing reference images. To do so, we asked GPT to rephrase the prompts, to make them easier for T2I generative model, by explicitly asking it to change rare words to their description if necessary. The full prompt can be found in Appendix A. As the results show, rephrasing was not enough for meaningful improvement in the results (Rephrased prompt in Tab. 3). Next, we investigate the importance of using detailed image captions for retrieval, rather than just listing the missing concepts or using the original prompt. We do so by evaluating our method when retrieving the concepts directly without generating compatible image captions for each missing concept (Retrieve concepts in Tab. 3), and when retrieving the prompt directly (Retrieve prompt in Tab. 3). While retrieval with each of them introduced some improvement over the initial results, retrieving detailed captions improved the results even further. Figure 5: Retrieval dataset size vs. CLIP score on ImageNet (left) and Aircraft (right). Dashed lines represent the scores of the base models. Even relatively small, unspecialized retrieval sets can already improve results. More data leads to further increased scores. However, small sets may not contain relevant retrieval examples, and their use may harm results, particularly for stronger models. Next, we investigate the effect of the retrieval-dataset size. We tested our method over ImageNet (Deng et al., 2009) and Aircraft (Maji et al., 2013) when using 1000, 10,000, 100,000, and 350,000 examples from LAION (Schuhmann et al., 2022). Fig. 5 shows that increasing the dataset size typically leads to better results. However, even using relatively small dataset can already lead to improvements. For OmniGen, 1000 examples were enough to see an improvement over the baseline model. SDXL has stronger baseline, hence more examples were needed for improvement. Finally, we investigate the effect of different similarity metrics for retrieval. We used CLIP (Radford et al., 2021), 6 ImageRAG Table 4: Similarity metric ablation study (OmniGen). Results of our method using different similarity metrics for image retrieval. Best results are bolded. CLIP ImageNet SigLIP DINO CLIP SigLIP DINO CUB GPT Re-rank BM25 Re-rank SigLIP CLIP 0.265 0.001 0.264 0.001 0.259 0.006 0.264 0.001 0.135 0.001 0.134 0.001 0.133 0.001 0.134 0.001 0.707 0.002 0.707 0.002 0.704 0.002 0.708 0. 0.255 0.004 0.253 0.003 0.243 0.004 0.253 0.003 0.125 0.003 0.123 0.003 0.116 0.003 0.125 0.002 0.762 0.004 0.763 0.004 0.761 0.004 0.760 0.003 SigLIP (Zhai et al., 2023), and re-ranking with GPT (Hurst et al., 2024) and BM25 (Robertson et al., 2009) over image captions generated by GPT for the retrieved candidates. Reranking was performed after retrieving 3 candidates from each of CLIP and SigLIP. Results are reported in Tab. 4. Although re-ranking with GPT produced slightly better results, they were not significant enough to justify the cost of applying this complex strategy vs. more straightforward CLIP metric. Hence, our other experiments use CLIP. Nevertheless, all the different metrics improved the generation abilities of the base model by providing helpful references. In both cases, we ask participants to compare two images at time: one created with our approach, and one using baseline. We ask users to choose the one they prefer in terms of adherence to the text prompt, visual quality, and overall preference. Since some prompts contain uncommon concepts, we supply users with real image of the least familiar concept in each prompt (not taken from our dataset). When running ImageRAG for the user study, we disable the decision step where GPT is asked if the initial image matches the prompt. This is done to avoid showing user the same image twice in cases where GPT deems the initial image to be good fit. As demonstrated in Fig. 6 participants favored ImageRAG over all other methods in all three criteria of text alignment, visual quality, and overall preference. Appendix supplies more information about questions asked in the study, visual comparison examples for each retrievalbased generation model  (Fig. 8)  , and more comparisons to SDXL  (Fig. 9)  and OmniGen  (Fig. 10)  with and without ImageRAG. Fig. 11 shows additional visual results of our method with more complex and creative prompts. 5.4. Qualitative comparisons 6. Limitations Although our method can help models generate concepts they are unable of generating alone, it has some limitations, depending on the data, retrieval method, and underlying model. For example, both OmniGen and SDXL struggle with text, and even when given text image as reference, they do not learn from it accurately. Moreover, our results depend on the quality of the retrieval method. For example, when using CLIP for retrieval, we cannot help with tasks it does not excel, such as counting (Paiss et al., 2023). Additionally, we rely on the used VLM to decide whether we should apply our method or not. Although GPT is powerful model and often answers correctly if an image matches text prompt, sometimes it may answer that the prompt aligns with the image even if it does not, and in these cases our method will not be applied. possible solution is to apply our method directly if the output image is not satisfactory. Finally, our ability to aid the model also depends on the dataset we retrieve images from. If the dataset only contains images of birds and the task is to generate specific dog breed, we will not be able to help. On the other hand, as presented in Tab. 2, if the retrieval dataset contains relevant information, the generation will also be more accurate. 7. Conclusion In this work, we explore how to apply RAG over pretrained text-to-image models, using simple yet effective method. We show that using relevant image references improves T2I models abilities to generate rare concepts and demonstrate how we can dynamically retrieve relevant references with simple approach utilizing VLM. We experiment with Figure 6: User study results. Users preference percentage of our method compared to other methods in terms of text alignment, visual quality, and overall preference. Fig. 7 shows qualitative examples from the ImageNet (Deng et al., 2009), CUB (Wah et al., 2011) and iNaturalist (Van Horn et al., 2018) datasets, comparing the results of OmniGen and SDXL with and without our method. To further assess the quality of our results, we conduct user study with 46 participants and total of 767 comparisons. We perform two types of studies one that evaluates SDXL and OmniGen with and without our method, and another that compares our results with other retrieval-based generation models. Specifically, we compare to models explicitly trained for the task of image generation using retrieved images: RDM (Blattmann et al., 2022), knn-diffusion (Sheynin et al., 2022), and ReImagen (Chen et al., 2022). Since these are largely proprietary models with no API, we compare to images and prompts published in their papers. 7 ImageRAG"
        },
        {
            "title": "SDXL",
            "content": "ImageRAG (OmniGen) ImageRAG (SDXL+IP)"
        },
        {
            "title": "Boston bull",
            "content": "Cab Academic gown Unicycle Geococcyx Cyanocitta cristata Figure 7: Qualitative comparisons: rare concept generation. Examples from ImageNet (Deng et al., 2009), CUB (Wah et al., 2011) and iNaturalist (Van Horn et al., 2018). The left-most image column is the retrieved reference using ImageRAG for each prompt. OmniGen and SDXL both struggle with the uncommon concepts, sometimes generating similar concepts such as bull or cow instead of the dog breed Boston bull, while in other times, they generate completely unrelated images, as in the case of Chow, or Geococcyx. When using ImageRAG both models generate the correct concept. 8 ImageRAG two models representing two model types and show how our method can be applied to multiple different models. We conclude that using image references broadens the applicability of text-to-image generation models, requiring minimal modifications to enhance generation possibilities. Parikh, D., Lischinski, D., Fried, O., and Yin, X. Spatext: Spatio-textual representation for controllable image In Proceedings of the IEEE/CVF Confergeneration. ence on Computer Vision and Pattern Recognition, pp. 1837018380, 2023b."
        },
        {
            "title": "Ethical statement",
            "content": "The development of ImageRAG introduces enhancement possibilities for image generation models, enabling rare or fine-grained concept generation. While these advancements hold promise for creative industries, personalized content creation, and scientific visualization, they also raise ethical concerns, including potential misuse for harmful content such as deepfakes and the use of private data. Therefore, transparency in data usage and adherence to privacy regulations are essential. We condemn any misuse of the proposed technology, and actively research tools to identify and prevent malicious usage of generative models (Agarwal et al., 2020; Knafo, 2022; Sinitsa & Fried, 2024)."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Gal Chechik and Daniel Arkushin for helpful discussions. This research was partially funded by ISF grant numbers 1337/22, 1574/21."
        },
        {
            "title": "References",
            "content": "Agarwal, S., Farid, H., Fried, O., and Agrawala, M. Detecting deep-fake videos from phoneme-viseme mismatches. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 660661, 2020. Alaluf, Y., Richardson, E., Metzer, G., and Cohen-Or, D. neural space-time representation for text-to-image personalization. ACM Transactions on Graphics (TOG), 42 (6):110, 2023. Arar, M., Voynov, A., Hertz, A., Avrahami, O., Fruchter, S., Pritch, Y., Cohen-Or, D., and Shamir, A. Palp: Prompt aligned personalization of text-to-image models. arXiv preprint arXiv:2401.06105, 2024. Avrahami, O., Lischinski, D., and Fried, O. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1820818218, 2022. Avrahami, O., Aberman, K., Fried, O., Cohen-Or, D., and Lischinski, D. Break-a-scene: Extracting multiple conIn SIGGRAPH Asia 2023 cepts from single image. Conference Papers, pp. 112, 2023a. Avrahami, O., Hayes, T., Gafni, O., Gupta, S., Taigman, Y., Biswas, B. and Ramnath, R. Efficient and interpretable information retrieval for product question answering with heterogeneous data. In Proceedings of the Seventh Workshop on e-Commerce and NLP@ LREC-COLING 2024, pp. 1928, 2024. Blattmann, A., Rombach, R., Oktay, K., Muller, J., and Ommer, B. Retrieval-augmented diffusion models. Advances in Neural Information Processing Systems, 35: 1530915324, 2022. Brooks, T., Holynski, A., and Efros, A. A. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Brown, T. B. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Chen, J., Ge, C., Xie, E., Wu, Y., Yao, L., Ren, X., Wang, Z., Luo, P., Lu, H., and Li, Z. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pp. 7491. Springer, 2025. Chen, W., Hu, H., Saharia, C., and Cohen, W. W. Reimagen: Retrieval-augmented text-to-image generator. In The Eleventh International Conference on Learning Representations, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2023a. Gal, R., Arar, M., Atzmon, Y., Bermano, A. H., Chechik, G., and Cohen-Or, D. Encoder-based domain tuning for fast personalization of text-to-image models. ACM Transactions on Graphics (TOG), 42(4):113, 2023b. Gal, R., Lichter, O., Richardson, E., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. Lcm-lookahead 9 ImageRAG for encoder-based text-to-image personalization. In European Conference on Computer Vision, pp. 322340. Springer, 2024. Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and Wang, H. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. Goswami, A., Modi, S. K., Deshineni, S. R., Singh, H., Singla, P., et al. Grape: generate-plan-edit framework for compositional t2i synthesis. arXiv preprint arXiv:2412.06089, 2024. Gu, Z., Yang, S., Liao, J., Huo, J., and Gao, Y. Analogist: Out-of-the-box visual in-context learning with image diffusion model. ACM Transactions on Graphics (TOG), 43 (4):115, 2024. Haviv, A., Sarfaty, S., Hacohen, U., Elkin-Koren, N., Livni, R., and Bermano, A. H. Not every image is worth thousand words: Quantifying originality in stable diffusion. arXiv preprint arXiv:2408.08184, 2024. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., and Cohen-or, D. Prompt-to-prompt image editing In The Eleventh Internawith cross-attention control. tional Conference on Learning Representations, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. Hu, H., Chan, K. C., Su, Y.-C., Chen, W., Li, Y., Sohn, K., Zhao, Y., Ben, X., Gong, B., Cohen, W., et al. Instructimagen: Image generation with multi-modal instruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 47544763, 2024. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., and Fung, P. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138, 2023. Knafo, G. Fakeout: Leveraging out-of-domain selfsupervision for multi-modal video deepfake detection. Masters thesis, Reichman University (Israel), 2022. 10 Kumari, N., Zhang, B., Zhang, R., Shechtman, E., and Zhu, J.-Y. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1931 1941, 2023. Labs, B. F. Flux. black-forest-labs/flux, 2023. https://github.com/ Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.-t., Rocktaschel, T., et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. Li, L., Zeng, H., Yang, C., Jia, H., and Xu, D. Block-wise lora: Revisiting fine-grained lora for effective personalization and stylization in text-to-image generation. arXiv preprint arXiv:2403.07500, 2024. Maji, S., Rahtu, E., Kannala, J., Blaschko, M., and Vedaldi, A. Fine-grained visual classification of aircraft. 2013. Mokady, R., Hertz, A., Aberman, K., Pritch, Y., and CohenOr, D. Null-text inversion for editing real images usIn Proceedings of the ing guided diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 60386047, 2023. Najdenkoska, I., Sinha, A., Dubey, A., Mahajan, D., Ramanathan, V., and Radenovic, F. Context diffusion: Incontext aware image generation. In European Conference on Computer Vision, pp. 375391. Springer, 2024. Nguyen, T., Li, Y., Ojha, U., and Lee, Y. J. Visual instruction inversion: Image editing via image prompting. Advances in Neural Information Processing Systems, 36, 2024. Nitzan, Y., Aberman, K., He, Q., Liba, O., Yarom, M., Gandelsman, Y., Mosseri, I., Pritch, Y., and Cohen-Or, D. Mystyle: personalized generative prior. ACM Transactions on Graphics (TOG), 41(6):110, 2022. Nitzan, Y., Wu, Z., Zhang, R., Shechtman, E., Cohen-Or, D., Park, T., and Gharbi, M. Lazy diffusion transformer for interactive image editing. arXiv preprint arXiv:2404.12382, 2024. Paiss, R., Ephrat, A., Tov, O., Zada, S., Mosseri, I., Irani, In M., and Dekel, T. Teaching clip to count to ten. Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 31703180, 2023. Patashnik, O., Gal, R., Ostashev, D., Tulyakov, S., Aberman, K., and Cohen-Or, D. Nested attention: Semanticaware attention values for concept personalization. arXiv preprint arXiv:2501.01407, 2025. ImageRAG Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Muller, J., Penna, J., and Rombach, R. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, In-context A., Leyton-Brown, K., and Shoham, Y. retrieval-augmented language models. Transactions of the Association for Computational Linguistics, 11:1316 1331, 2023. Ray, A., Radenovic, F., Dubey, A., Plummer, B., Krishna, R., and Saenko, K. Cola: benchmark for compositional text-to-image retrieval. Advances in Neural Information Processing Systems, 36, 2024. Robertson, S., Zaragoza, H., et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2250022510, 2023. Samuel, D., Ben-Ari, R., Darshan, N., Maron, H., and Chechik, G. Norm-guided latent space exploration for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024a. Samuel, D., Ben-Ari, R., Raviv, S., Darshan, N., and Chechik, G. Generating images of rare concepts using pre-trained diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 46954703, 2024b. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35: 2527825294, 2022. Sheynin, S., Ashual, O., Polyak, A., Singer, U., Gafni, O., Nachmani, E., and Taigman, Y. knn-diffusion: Image generation via large-scale retrieval. In The Eleventh International Conference on Learning Representations, 2022. Shi, J., Xiong, W., Lin, Z., and Jung, H. J. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8543 8552, 2024. Sinitsa, S. and Fried, O. Deep image fingerprint: Towards low budget synthetic image detection and model lineage analysis. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 40674076, 2024. Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Wang, Y., Rao, Y., Liu, J., Huang, T., and Wang, X. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024. Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 87698778, 2018. Vendrow, E., Pantazis, O., Shepard, A., Brostow, G., Jones, K. E., Mac Aodha, O., Beery, S., and Van Horn, G. Inquire: natural world text-to-image retrieval benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Voynov, A., Chu, Q., Cohen-Or, D., and Aberman, K. p+: Extended textual conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S. The caltech-ucsd birds-200-2011 dataset. 2011. Wang, Z., Li, A., Li, Z., and Liu, X. Genartist: Multimodal llm as an agent for unified image generation and editing. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Wang, Z., Jiang, Y., Lu, Y., He, P., Chen, W., Wang, Z., Zhou, M., et al. In-context learning unlocked for diffusion models. Advances in Neural Information Processing Systems, 36:85428562, 2023. Wei, X., Song, Y., Aodha, O., Wu, J., Peng, Y., Tang, J., Yang, J., and Belongie, S. Fine-grained image analysis IEEE Transactions on with deep learning: survey. Pattern Analysis and Machine Intelligence, 44(12):8927 8948, 2022. ImageRAG Wei, Y., Zhang, Y., Ji, Z., Bai, J., Zhang, L., and Zuo, W. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1594315953, 2023. Xiao, S., Wang, Y., Zhou, J., Yuan, H., Xing, X., Yan, R., Wang, S., Huang, T., and Liu, Z. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. Ye, H., Zhang, J., Liu, S., Han, X., and Yang, W. Ip-adapter: Text compatible image prompt adapter for text-to-image arXiv preprint arXiv:2308.06721, diffusion models. 2023. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1197511986, 2023. Zhang, H., Li, F., Liu, S., Zhang, L., Su, H., Zhu, J., Ni, L., and Shum, H.-Y. Dino: Detr with improved denoising In The anchor boxes for end-to-end object detection. Eleventh International Conference on Learning Representations. Zhang, H., Shalev-Arkushin, R., Baltatzis, V., Gillis, C., Laput, G., Kushalnagar, R., Quandt, L., Findlater, L., Bedri, A., and Lea, C. Towards ai-driven sign language generation with non-manual markers, 2025. URL https://arxiv.org/abs/2502.05661. Zhang, L., Rao, A., and Agrawala, M. Adding conditional control to text-to-image diffusion models, 2023. 12 ImageRAG A. Retrieval-Caption Generation Prompts 2018) datasets are presented in Tab. 5. Full prompts used for querying GPT in the retrieval-caption generation part of our method: C. User Study Questions In the user study, we asked users to compare pairs of images at time, by asking which one adheres better to the prompt and has better visual quality. We supplied real references (not from our dataset) for rare concepts with each pair. The questions we asked were: For each criteria, choose the better image out of and given the following text prompt: <prompt>. The less familiar concept <rare concept> is presented on the left of the image options. Better text alignment (choose or B) Better visual quality (choose or B) Overall preference (choose or B) Pair examples of using our method vs. other retrieval-based generation approaches can be found in Fig. 8. Due to lack of access to the models, all prompts and results of the other methods were taken from their papers. Pair examples of rare or fine-grained concept generation with and without ImageRAG are presented in Fig. 9 (SDXL examples), and Fig. 10 (OmniGen examples). More complex creative examples are presented in Fig. 11. Decision: Does this image match the prompt {prompt}? Consider both content and style aspects. Only answer yes or no. Missing Concepts Identification: What are the differences between this image and the required prompt? In your answer only provide missing concepts in terms of content and style, each in separate line. For example, if the prompt is An oil painting of sheep and car and the image is painting of car but not an oil painting, the missing concepts will be: oil painting style sheep Caption Generation: For each concept you suggested above, please suggest an image caption describing an image that explains this concept only. The captions should be standalone description of the images, assuming no knowledge of the given images and prompt, that can use to lookup images with automatically. In your answer only provide the image captions, each in new line with nothing else other than the caption. Rephrase request prompt: prompt used for the rephrasing ablation experiment: Please rephrase the following prompt to make it easier and clearer for the text-to-image generation model that generated the above image for this prompt. The goal is to generate an image that matches the given text prompt. If the prompt is already clear, return it as it is. Simplify and shorten long descriptions of known objects/entities but DO NOT change the original meaning of the text prompt. If the prompt contains rare words, change those words to description of their meaning. In your answer only provide the prompt and nothing else. The prompt to be rephrased: {prompt}. B. Additional Experiments Additional proprietary dataset experiments over the CUB (Wah et al., 2011) and iNaturalist (Van Horn et al., Table 5: Additional proprietary data usage experiments. Results for using each dataset as the retrieval-dataset (Proprietary-<model>) vs. using our subset from LAION as the retrieval-dataset (LAION-<model>). Here, indicates OmniGen based models, SD indicates SDXL based models. Best results for each model are bolded. CUB CLIP SigLIP DINO CLIP iNaturalist SigLIP DINO LAION-O Proprietary-O 0.253 0.003 0.269 0.003 0.125 0.002 0.136 0.002 0.760 0.003 0.773 0.004 0.197 0.002 0.212 0. 0.095 0.002 0.114 0.001 0.701 0.002 0.732 0.002 LAION-SD 0.314 0.001 Proprietary-SD 0.314 0.002 0.174 0.002 0.175 0.001 0.784 0.001 0.786 0. 0.243 0.002 0.251 0.002 0.118 0.001 0.118 0.002 0.724 0.002 0.737 0.002 13 ImageRAG Figure 8: Comparisons between ImageRAG and different methods using retrieval for generation. Prompts and results of all other methods are taken from their papers. The methods we compared to are RDM (Blattmann et al., 2022), Re-Imagen (Chen et al., 2022), and KNN-Diffusion (Sheynin et al., 2022). Figure 9: Examples of rare concept generation using ImageRAG with SDXL. Real examples are taken from the iNaturalist (Van Horn et al., 2018) dataset. Figure 10: Examples of rare concept generation using ImageRAG with OmniGen. Real examples are taken from the iNaturalist (Van Horn et al., 2018) dataset. 14 ImageRAG Figure 11: More creative generation examples."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Reichman University",
        "Tel Aviv University"
    ]
}