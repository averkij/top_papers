{
    "paper_title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles",
    "authors": [
        "Yitao Long",
        "Yuru Jiang",
        "Hongjun Liu",
        "Yilun Zhao",
        "Jingchen Sun",
        "Yiqiu Shen",
        "Chen Zhao",
        "Arman Cohan",
        "Dennis Shasha"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PuzzlePlex, a benchmark designed to assess these capabilities through a diverse set of puzzles. PuzzlePlex consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PuzzlePlex framework provides a comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized game-playing strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers a scalable and efficient alternative. PuzzlePlex enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models."
        },
        {
            "title": "Start",
            "content": "PUZZLEPLEX: Benchmarking Foundation Models on Reasoning and Planning with Puzzles Yitao Long1 Yuru Jiang2 Hongjun Liu1 Yilun Zhao3 Jingchen Sun4 Yiqiu Shen1,5 Chen Zhao1 Arman Cohan 3 Dennis Shasha1 1New York University 2Zhejiang University 3Yale University 4University at Buffalo, SUNY 5NYU Grossman School of Medicine https://github.com/yitaoLong/PuzzlePlex 5 2 0 2 7 ] A . [ 1 5 7 4 6 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This work investigates the reasoning and planning capabilities of foundation models and their scalability in complex, dynamic environments. We introduce PUZZLEPLEX, benchmark designed to assess these capabilities through diverse set of puzzles. PUZZLEPLEX consists of 15 types of puzzles, including deterministic and stochastic games of varying difficulty, as well as single-player and two-player scenarios. The PUZZLEPLEX framework provides comprehensive environment for each game, and supports extensibility to generate more challenging instances as foundation models evolve. Additionally, we implement customized gameplaying strategies for comparison. Building on this benchmark, we develop fine-grained metrics to measure performance and conduct an in-depth analysis of frontier foundation models across two settings: instruction-based and code-based. Furthermore, we systematically investigate their scaling limits. Our findings show that reasoning models outperform others in instruction-based settings, while code-based execution presents greater challenges but offers scalable and efficient alternative. PUZZLEPLEX enables targeted evaluation and guides future improvements in reasoning, planning, and generalization for foundation models."
        },
        {
            "title": "Introduction",
            "content": "The rapid progress of foundation models has led to remarkable improvements across broad spectrum of natural language processing tasks. Recently, the emergence of reasoning models such as OpenAI o-series models (OpenAI et al., 2024a) and DeepSeek-R1 (DeepSeek-AI et al., 2025a) have demonstrated remarkable advances in complex reasoning tasks through test-time compute scaling. These breakthroughs naturally prompt deeper question: How far can modern models push genuine problem-solving ability, especially in scenarios that demand sustained, structured reasoning? 1 Figure 1: Overview of four puzzles: SudoKill (twoplayer deterministic), Tidy Tower (single-player deterministic), Beat or Bomb Sto (two-player stochastic), and Ruby Risks (single-player stochastic). To explore this, we turn to puzzle solvinga domain that inherently blends logical, numerical, and spatial reasoning with long-horizon planning and strategic adaptation. Many puzzles unfold over multiple interactive steps, involving competition or dynamic environments. This makes them ideal for evaluating models ability to reason under evolving constraints, adapt to new strategies, and maintain coherence across extended interactions. To this end, we introduce PUZZLEPLEX, benchmark designed to evaluate foundation models reasoning and planning capabilities. Unlike prior benchmarks (Zhang et al., 2025; Wu et al., 2024b) that reuse common puzzlesmany potentially seen during pretrainingPUZZLEPLEX features 15 novel, curated puzzles spanning both textonly and text-image formats. As shown in Figure 1, the puzzles cover single-player and twoplayer settings and include both deterministic and stochastic environments. Each puzzle supports multiple difficulty levels and extensible generation, enabling adaptive evaluation as models improve. Their long-horizon, dynamic nature provides compact yet demanding testbed for assessing reasoning depth, planning, and strategic coherenceareas underexplored in prior short-context Benchmark Game Scenario Reward Predictability # MultiData Type Varying Evaluation SingleTwo-player Deterministic Stochastic Turn Text Text-Image Difficulty Inst. Code PUZZLEBENCH (Mittal et al., 2025) PUZZLES (Estermann et al., 2024) LOGICGAME (Gui et al., 2024a) BOARDGAMEQA (Kazemi et al., 2023) P3 (Schuster et al., 2021) PUZZLEQA (Zhao and Anderson, 2023) ENIGMAEVAL (Wang et al., 2025) VGRP-BENCH (Ren et al., 2025) SMARTPLAY (Wu et al., 2024a) PUZZLEPLEX (ours) Table 1: Comparison between PUZZLEPLEX and existing puzzle benchmarks. single-turn game ends after one move by one or more players. benchmarks (Gui et al., 2024b)."
        },
        {
            "title": "2 Related Work",
            "content": "We further design hand-crafted strategies for comparison and evaluate models under two complementary paradigms: instruction-based and codebased. In the former, models act as agents interacting via natural language; in the latter, they generate executable code that solves the puzzle. Together, these paradigms reveal both interactive reasoning and programmatic abstraction capabilities. Empirical results show that reasoning models outperform non-reasoning ones in instructionbased settings, leveraging test-time scaling and extended deliberation. However, performance drops in code-based evaluation due to challenges in program synthesis, though sampling-based methods help narrow the gap. Open-source models increasingly rival proprietary systems, and visual or legality-aware prompting further boosts results. However, models still struggle with multi-hop reasoning in some puzzles, suggesting limitations in their ability to maintain coherent reasoning over extended contexts. In summary, our contributions are: PUZZLEPLEX, the first benchmark to jointly evaluate reasoning in both interactive and executable settings across diverse puzzle types. Framework that supports textual and visual puzzles with deterministic and stochastic dynamics. Hand-crafted baselines and fine-grained metrics enabling rigorous evaluation and comparison of systems and reasoning strategies. Comprehensive empirical analysis across leading models, comparing the performance of different reasoning strategies, the scaling behavior of different systems, and the systems failure modes."
        },
        {
            "title": "2.1 Puzzles and Relevant Benchmarks",
            "content": "Puzzles can be broadly divided into rule-based and rule-less types. Rule-based puzzles, such as SUDOKU (Noever and Burdick, 2021), CROSSWORDS (Sadallah et al., 2025), and CHESS (Feng et al., 2023), have explicit rules, defined goals, and structured state transitions, requiring strategic and logical reasoning. Rule-less puzzles, including Riddles (Lin et al., 2021; Bisk et al., 2019), lack explicit action spaces or clear objectives. PUZZLEPLEX focuses on rule-based puzzles to enable objective evaluation of reasoning in competitive, dynamic settings. We exclude knowledge-heavy puzzles (e.g., GUESS MY CITY (Abdulhai et al., 2023)) that depend on external knowledge (Schuster et al., 2021; Lin et al., 2021; Todd et al., 2024), since modern models already trained extensively on factual corpora and outperform humans on such tasks. Table 1 compares PUZZLEPLEX with recent puzzle benchmarks. Most existing benchmarks focus on single-player, short-horizon puzzles (Mittal et al., 2025; Gui et al., 2024a; Zhao and Anderson, 2023), while multi-turn, competitive two-player settings are rarely explored (Wu et al., 2024a; Liu et al., 2023). Few benchmarks incorporate stochastic environments for reasoning under uncertainty, or multimodal puzzles that require joint text-image understanding (Estermann et al., 2024; Wang et al., 2025; Ren et al., 2025)."
        },
        {
            "title": "2.2 Evolution of Puzzle Solving Techniques",
            "content": "A variety of methods have been developed for solving rule-based puzzles. Classical approaches rely on algorithmic techniques such as dynamic programming (Smith, 2007), alphabeta pruning (Korf, 1990), and heuristic search (Lewis, 2007). For single-player puzzles, neurosymbolic methods"
        },
        {
            "title": "3 PUZZLEPLEX",
            "content": "We first introduce the PUZZLEPLEX framework in which puzzle templates can be instantiated, moves recorded, state information shared, and states evaluated. We next describe the puzzles included in this benchmark, the implementation of customized strategies, and the evaluation methods."
        },
        {
            "title": "3.1 Puzzle Generation Framework",
            "content": "PUZZLEPLEX has the following main components, as presented in Figure 2. Instance Generation. For each puzzle p, we distinguish between possibly parametrized puzzle template template(p) (e.g., SudoKill on 9 9 grid, template(SudoKill(9,9)), and an instance instance(p) (e.g., particular instance of SudoKill on 9 9 grid, instance(Suduoku(9,9)). generator function Gp maps templates to instances. The generated instance is also the initial state S0 of the game. That is, instance(p) = S0. The generator for each puzzle will create instances using randomness, and it will adjust the difficulty level by varying the size of the puzzle. State Transition. After receiving move generated by player (human or computer), the state transition module maps state Sn to new state Sn+1 while incorporating feedback Fn. The feedback Fn indicates the legality of the move, whether the game has terminated, and provides new position information. This process is represented as : Sn (Sn+1, Fn). Evaluation. Once the puzzle-solving process terminates, an Evaluator Ep is applied to the sequence of states S0, S1, . . . , Sn to determine the raw score(s), represented as rsp = Ep(S0, S1, . . . , Sn). The scale of the raw scores varies depending on the resolution type of each puzzle. To ensure comparability, we normalize these scores to obtain final scores ranging from 0 to 1 ( 3.5). To better keep track of state transitions and model reasoning steps, we implemented Web UI called Simulator for visual observation. An example of this interface is shown in the A.2."
        },
        {
            "title": "3.2 PUZZLEPLEX Benchmark Construction",
            "content": "All puzzles in PUZZLEPLEX are either derived from column in Communications of the ACM 1 or manually curated by the authors. While foun1https://cacm.acm.org/section/opinion/ Figure 2: Overview of the developed pipeline framework. Puzzle Generator creates puzzle instances from templates based on the puzzle name, difficulty level, and selected competing models. The Solver then generates response after receiving the puzzle instance. This response is passed to the Transition Checker, which verifies the legality of the operation output by the Solver and checks the game status. If the game ends, the Evaluator calculates and outputs the score. Otherwise, State Transition updates the state and passes the updated information back to the Solver. (Ahmed et al., 2023; Murali et al., 2022) are effective due to their combinatorial nature, often reducible to SAT or SMT formulations (Bright et al., 2020; Høfler, 2014). With deep learning advances, reinforcement learning (RL) has become the dominant paradigm (dos Santos et al., 2019; Huang et al., 2024), though combinatorial explosion still necessitates heuristics (Silver et al., 2016). Early model-based approaches fine-tuned models like GPT-2 (Radford et al., 2019) and FLAN-PaLM (Chung et al., 2022) for puzzles such as Sudoku (Noever and Burdick, 2021) and BoardgameQA (Kazemi et al., 2023). Stronger foundation models (OpenAI et al., 2024b; Anthropic, 2024) now solve puzzles through fewshot in-context learning and multi-run feedback. Among prompting methods, Chain-of-Thought (CoT) (Wei et al., 2023) consistently outperforms direct prompting, while extensions like Self-Refine (Madaan et al., 2023), Tree-of-Thought (ToT) (Yao et al., 2023), and Everything-of-Thoughts (Ding et al., 2024) further enhance reasoning for deterministic puzzles. In this work, we adopt CoT-style prompting to evaluate systematic reasoning, planning, and decision-making. We further introduce codebased execution setting, where models generate and execute code to interact directly with puzzle environmentslinking reasoning with concrete actions and improving solution correctness and generalization. 3 Model Single-Player Det. Two-Player Det. Easy Normal Easy Normal Score Custom 0.89 0.47 0.83 0. 0.59 0.36 0.60 0.34 0.70 0.15 Deepseek-R1 0.64 1.38 o4-mini 0.44 1.16 Gemini-2.5-pro 0.44 1.13 QwQ-32B 0.54 1.15 grok-3-mini 0.17 0.52 Deepseek-V3 0.34 0.78 0.40 0.98 GPT-4.1 Qwen-2.5-VL-72B 0.24 0.62 0.15 0.37 Llama-3.3-70B 0.13 0.38 Gemma-3-27B 0.05 0.20 Phi-4-multimodal 0.48 1.04 0.44 1.14 0.44 1.02 0.25 0.58 0.22 0.51 0.24 0.54 0.35 0.94 0.24 0.37 0.12 0.39 0.12 0.39 0.03 0.14 0.66 0.12 0.67 0.15 0.68 0.14 0.69 0.10 0.67 0.20 0.52 0.13 0.44 0.13 0.24 0.18 0.31 0.12 0.23 0.12 0.17 0. 0.66 0.14 0.68 0.12 0.67 0.13 0.68 0.11 0.67 0.20 0.50 0.14 0.45 0.09 0.28 0.24 0.31 0.12 0.24 0.10 0.17 0.05 0.62 0.15 0.59 0.15 0.58 0.14 0.58 0.14 0.49 0.15 0.43 0.10 0.42 0.11 0.25 0.09 0.25 0.07 0.19 0.06 0.12 0.05 Table 2: Instruction-based normalized scores (mean 95% CI) of models on single-player and two-player deterministic puzzles, separated by difficulty. dation models may have been exposed to textual descriptions of these puzzles, there are no publicly available strategies for solving them, thereby minimizing the risk of data contamination during gameplay. Additionally, we have simplified the rules of several puzzles to reduce the barrier to entry, enabling most users to engage with them immediately after learning the rules and objectives. Our 15 puzzles are categorized into four types: single-player deterministic, single-player stochastic, two-player deterministic, and twoplayer stochastic. Text-based puzzles span all four types, whereas text-image puzzles are limited to the two-player deterministic type. The distinction between deterministic and stochastic games lies in the predictability of operation outcomes. In deterministic games, the result of decision is fixed, regardless of how many times it is taken. In contrast, stochastic games produce probabilistic outcomes, where repeated execution of the same operation in the same state may lead to different results. Detailed information about the puzzles is provided in A.1, and individual puzzle descriptions are included in A.3."
        },
        {
            "title": "3.3 Customized Strategies",
            "content": "We implemented customized strategies for each puzzle, which can be categorized as follows: Brute-force Algorithm: This method is employed when the problem size allows for an exhaustive search within our specified time constraints. Search Algorithms: We employ variety of search techniques, including both uninformed and probabilistic methods. Specifically, we use Breadth-First Search (BFS) and Depth-First Search (DFS) as examples of uninformed search strategies. Monte Carlo Tree Search (MCTS) is incorporated as form of probabilistic search. Dynamic Programming (DP): Dynamic programming is applied to puzzles that exhibit overlapping subproblems and optimal substructure. Greedy Algorithm: Greedy algorithms are employed in puzzles where locally optimal choices are expected to lead to globally optimal solutions or the search space is too large for other techniques, often reflecting strategies used in realworld scenarios. Other Methods: These include other algorithms, such as backtracking and simulated annealing."
        },
        {
            "title": "3.4 Evaluation Protocols",
            "content": "To gain holistic view of models problemsolving ability under distinct modes of interaction, we design the following two evaluation protocols. Instruction-based Evaluation. Single-player deterministic puzzles are evaluated using 10 randomly generated instances with fixed seeds from 1 to 10 to ensure reproducibility. For two-player deterministic puzzles, each model pair competes on 5 instances (seeds 15), with each match repeated twice while alternating the first player to account for first-mover advantage. All evaluations are conducted at two difficulty levels: easy and normal. Stochastic puzzles are excluded from this setting due to their inherent variance and the high cost of running enough instances to achieve statistically robust conclusions. 4 Model Single-Player Det. Single-Player Sto. Two-Player Det. Two-Player Sto. Score Easy Normal Easy Normal Easy Normal Easy Normal Avg. Best Avg. Best Avg. Best Avg. Best Avg. Best Avg. Best Avg. Best Avg. Best Avg. Best Custom 0.89 0.83 0.75 0.80 0.59 0.75 0.55 0.72 0.73 Deepseek-R1 0.33 0.53 0.25 0.43 0.54 0.89 0.51 o4-mini 0.30 0.42 0.32 0.52 0.44 0.89 0.48 Gemini-2.5-pro 0.34 0.68 0.31 0.65 0.36 0.69 0.36 QwQ-32B 0.21 0.42 0.07 0.20 0.48 0.85 0.31 grok-3-mini 0.28 0.46 0.20 0.38 0.22 0.64 0.12 Deepseek-V3 0.22 0.40 0.16 0.34 0.54 0.86 0.35 0.24 0.43 0.28 0.48 0.44 0.88 0.44 GPT-4.1 Qwen-2.5-VL-72B 0.19 0.36 0.16 0.49 0.41 0.80 0.30 0.18 0.41 0.17 0.40 0.40 0.82 0.30 Llama-3.3-70B 0.22 0.43 0.22 0.41 0.35 0.79 0.32 Gemma-3-27B 0.00 0.00 0.00 0.00 0.06 0.24 0.01 Phi-4-multimodal 0.94 0.93 0.76 0.81 0.65 0.93 0.93 0.63 0.77 0.86 0.84 0.82 0.20 0.66 0.65 0.77 0.66 0.82 0.46 0.52 0.53 0.61 0.52 0.69 0.85 0.73 0.85 0.49 0.52 0.57 0.60 0.53 0.73 0.66 0.82 0.74 0.94 0.47 0.52 0.47 0.54 0.50 0.74 0.59 0.59 0.68 0.34 0.58 0.53 0.65 0.37 0.54 0.37 0.65 0.59 0.75 0.68 0.78 0.54 0.71 0.62 0.76 0.43 0.54 0.51 0.64 0.45 0.60 0.45 0.52 0.40 0.54 0.40 0.51 0.72 0.52 0.69 0.55 0.68 0.45 0.70 0.46 0.73 0.40 0.55 0.50 0.69 0.56 0.62 0.50 0.66 0.60 0.64 0.41 0.60 0.51 0.45 0.59 0.51 0.61 0.46 0.66 0.47 0.65 0.38 0.19 0.05 0.07 0.05 0.07 0.19 0.25 0.05 0.08 0.05 0.72 0.81 0.56 0.73 0.65 0. Table 3: Code-based normalized scores. Code-based Evaluation. Each foundation model is sampled 32 times per puzzle to generate code, following the prompt templates described in B.7. The resulting programs are then executed to play the games. For deterministic puzzles, we follow the same evaluation protocol as in the instructionbased setting. For single-player stochastic puzzles, each generated program is evaluated over 100 runs (seeds 1 to 100) across both difficulty levels. For two-player stochastic puzzles, each program competes in 50 runs (seeds 1 to 50), alternating player roles in each match."
        },
        {
            "title": "3.5 Evaluation Metrics",
            "content": "We employ two primary metrics to evaluate model performance: Normalized Score and Elo Score, both derived from raw scores. Raw Score. In single-player games, raw scores are either binary or continuous. Binary puzzles assign score of 1 for success and 0 for failure. Continuous-score puzzles assign values based on criteria such as move count, constraints met, or objectives achieved, and scores may fall outside the [0, 1] range. In two-player games, outcomes are categorized as win, loss, or tie, corresponding to scores of 1, 0, and 0.5, respectively. Normalized Score. For two-player games, raw scores already lie in [0, 1] and do not require normalization. For single-player games, normalization ensures comparability by rescaling scores to the [0, 1] interval. This involves determining the best and worst achievable scores under identical initialization conditions. If higher scores are better, the top-performing model is assigned 1 and others receive score/max; if lower is better, normalization uses min/score. Elo Score. To enable unified comparison across both single-player and two-player settings, we apply the Elo rating system, widely-used model comparison metric (Boubdir et al., 2023). For single-player games, we create pairwise matchups between models based on their normalized scoresthe model with the higher normalized score is considered the winner in each pairwise comparison. The implementation details are described in B.4."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "The foundation models we evaluate include GPT-4.1 (OpenAI, 2025a), o4-mini (OpenAI, 2025b), Gemini-2.5-pro (Google, 2025), grok-3mini (xAI, 2025), DeepSeek-V3 (DeepSeek-AI et al., 2025b), DeepSeek-R1 (DeepSeek-AI et al., 2025a), QwQ-32B (Qwen, 2024), Qwen-2.5-VL72B (Bai et al., 2025), Gemma-3-27B (Team et al., 2025), Llama-3.3-70B (Grattafiori et al., 2024), and Phi-4-multimodal (Microsoft et al., 2025).2 Models grok-3-mini, DeepSeek-V3, DeepSeek-R1, and QwQ-32B do not support image modalities and are therefore excluded from evaluation on text-image puzzles in the instruction-based setting. We use the chat or instruct versions of each model, as solving most puzzles involves multi-turn interactions."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 2 presents the normalized scores of all models under the instruction-based setting, while Elo scores are reported in Table 10 in Appendix. breakdown of scores for each puzzle is provided in C.1. For the code-based setting, results are shown in Table 3. 2Models marked with an asterisk (*) are proprietary. 5 Model SudoKillM SuperplyM Easy Normal Easy Normal Score vs. Text vs. Custom Score vs. Text vs. Custom Score vs. Text vs. Custom Score vs. Text vs. Custom 0.48 Gemini-2.5-pro 0.25 Gemma-3-27B 0.48 GPT-4.1 0.72 o4-mini Phi-4-multimodal 0.20 Qwen-2.5-VL-72B 0. 0.21 0.37 0.45 0.62 0.10 0.18 0.46 0.00 0.30 0.20 0.00 0.10 0.60 0.25 0.36 0.54 0.35 0.24 0.56 0.24 0.40 0.59 0.14 0.15 1.00 0.00 0.10 0.40 0.00 0.00 0.60 0.25 0.58 0.94 0.25 0. 0.54 0.23 0.53 0.83 0.09 0.12 0.20 0.05 0.30 0.98 1.00 0.30 0.56 0.25 0.58 0.94 0.27 0.32 0.57 0.17 0.56 0.83 0.24 0.16 0.32 0.25 0.20 0.90 0.30 0.30 Table 4: Normalized score on SUDOKILLM and SUPERPLYM puzzles. Colors indicate the percentage change of the text-only or custom-strategy baseline versus the multimodal model score. Reasoning models outperform non-reasoning models in the instruction-based setting. From Table 2, we observe that reasoning models consistently outperform non-reasoning ones, with all top-5 models employing reasoning strategies. This demonstrates the effectiveness of test-time scaling using extended CoT, where deeper deliberation translates to better performance in gameplay. Notably, the relatively small QwQ-32B model surpasses larger non-reasoning models such as GPT4.1 and DeepSeek-V3. Furthermore, open-source models are highly competitive with proprietary systems: for instance, DeepSeek-R1 achieves the highest normalized score of 0.62, outperforming Gemini-2.5-pro, the best-performing proprietary model, which scores 0.58. These findings indicate that open-source models are closing the performance gap. Although foundation models still lag behind our custom strategy (which scores 0.70) on average, several leading models perform comparablyor even betterin two-player deterministic puzzles, highlighting the rapid progress of foundation models. Code-based setting is more challenging and leads to notable performance drop. As shown in Table 3, model performance declines significantly in the code-based setting, where models must generate executable code to play the games autonomously. Unlike the instruction-based settingwhere models act as interactive agents with ongoing access to game states and can adjust actions dynamicallythe code-based setting demands strong program synthesis capabilities. This shift reduces the advantage of reasoning models: for example, GPT-4.1, non-reasoning model, ranks among the top-3 performers in the codebased setting, whereas no non-reasoning model appears in the top-5 for the instruction-based setting."
        },
        {
            "title": "The performance drop is especially evident in",
            "content": "single-player deterministic puzzles. DeepSeek-R1, for instance, sees its score decline from 0.64 to 0.33 in the easy level, and from 0.48 to 0.25 in the normal level. Table 16 in Appendix further reveals significant reduction in win rates against the customized strategy across all models in the codebased setting in two-player deterministic games. Although the code-based setting underperforms compared to the instruction-based setting, its lower computational cost makes it promising direction. These results underscore the greater difficulty of the code-based setting, which not only tests reasoning but also code generation and execution accuracy. However, one advantage of this setting is efficiency: code is generated once per puzzle and can be reused. In our experiments, each model generates 32 samples per puzzle. As shown in Table 3, the best scores from code-based runs can approach or even match the performance of the customized strategy."
        },
        {
            "title": "4.3 More Instruction-based Analysis",
            "content": "Mixed Effectiveness of Advanced Prompting Strategies. Table 13 in Appendix reports the performance of GPT-4.1 and o4-mini on two puzzles, TIDYTOWER and SUDOKILL, under various prompting strategies. Overall, the effectiveness of advanced prompting techniques is mixed. For instance, 1-shot prompting yields negligible improvement in both puzzles. ToT prompting helps in TIDYTOWER but shows minimal benefit in SUDOKILL. Interestingly, prompting without historyi.e., omitting previous model inputs and outputs from the current promptleads to substantial performance boost in TIDYTOWER, outperforming ToT, especially considering ToTs much higher computational cost. This suggests that current models still struggle with multi-hop reasoning and reflection. Including past reasoning steps may inadvertently mislead the 6 Figure 3: Comparison between the reasoning model Deepseek-R1 and the non-reasoning model Deepseek-V3 in terms of generated token counts versus normalized scores on single-player deterministic puzzles. model rather than help it retrospect effectively. We also evaluate legality-aware prompting strategy, where the model is explicitly provided with list of legal candidate moves. This is motivated by the observation that many model failures stem from selecting illegal actions, which lead to immediate losses or premature termination. Table 13 in Appendix shows that providing legal candidates consistently improves performance. Notably, the reasoning-focused model o4-mini benefits more from these prompting strategies than GPT-4.1. Evaluating Multimodal Integration in Strategic Reasoning. Table 4 shows that most models benefit from incorporating visual inputs, confirming the value of image-based state representations in puzzle-solving tasks. High-capacity models like o4-mini and GPT-4.1 achieve notable gains, with GPT-4.1 improving its win rate by +0.38 on SUPERPLYM (Normal). However, weaker models such as Phi-4-multimodal struggle to utilize visual information effectively, sometimes exhibiting performance drops (e.g., -0.75 on SUPERPLYM Easy). These results suggest that while visual information aids intuitive understanding, effective multimodal reasoning requires advanced fusion capabilities. The benefits are more pronounced in simpler tasks, whereas complex scenarios demand stronger cross-modal reasoning, which current models often lack."
        },
        {
            "title": "4.4 Scaling Analysis",
            "content": "Reasoning models demonstrate better scaling between token count and performance. Figure 3 compares Deepseek-R1 (reasoning) and DeepseekV3 (non-reasoning) in terms of total generated tokens (reasoning + completion) versus normalized scores on single-player deterministic puzzles. These puzzles are all single-pass and do not involve multi-round interactions, making them suitable for such analysis. The results show that for Deepseek-R1, performance generally improves with increased token generation, suggesting effective test-time scaling. In contrast, Deepseek-V3 exhibits flatter or even downward trend, indicating limited benefit from generating more tokens. Furthermore, Deepseek-R1 tends to allocate more tokens to normal-difficulty instances than to easy ones, aligning with task complexity, while Deepseek-V3 shows little variation across difficulty levels. Reasoning models show improved performance in instruction-based settings but mixed results in code generation. For each run instance, we define several termination statuses. LEGAL means the game ends normally. RULE VIOLATION occurs when models make moves that violate the rules, causing game termination. NOT FOLLOWING INSTRUCTION indicates that foundation models fail to follow instructions properly; in instruction-based settings, this means the model generates data in format that prevents the system from extracting moves; in code-based settings, it means the model fails to generate code meeting our requirements. TIMEOUT is status exclusive to code-based settings, indicating that the model-generated code exceeds our predetermined runtime limit, forcing the game to stop. SYNTAX ERROR, also specific to code-based settings, occurs when the model generates code containing syntax errors. RUNTIME ERROR, another code-based status, happens when code executes but fails during runtime due to errors such as index exceptions. 7 Model Deepseek-R1 o4-mini Gemini-2.5-pro QwQ-32B grok-3-mini Deepseek-V3 GPT-4.1 Qwen-2.5-VL-72B Llama-3.3-7B Gemma-3-27B Phi-4-multimodal Deepseek-R1 o4-mini Gemini-2.5-pro QwQ-32B grok-3-mini Deepseek-V3 GPT-4.1 Qwen-2.5-VL-72B Llama-3.3-7B Gemma-3-27B Phi-4-multimodal Legal Not Follow Instr. Timeout Rule Violation Runtime Err. Syntax Err. Status Type #Token 0.79 0.79 0.72 0.78 0.82 0.72 0.67 0.57 0.61 0.37 0.42 0.54 0.61 0.58 0.19 0.56 0.26 0.57 0.46 0.46 0.43 0.00 0.01 0.02 0.17 0.01 0.04 0.03 0.01 0.05 0.00 0.44 0.17 0.18 0.26 0.13 0.07 0.19 0.61 0.18 0.13 0.20 0.35 0. Instruction-based 0.20 0.19 0.12 0.21 0.15 0.25 0.32 0.38 0.38 0.19 0.41 Code-based 0.00 0.03 0.14 0.01 0.05 0.03 0.03 0.09 0.11 0.16 0.00 0.01 0.03 0.02 0.00 0.03 0.00 0.02 0.02 0.11 0.05 0.13 0.19 0.13 0.07 0.21 0.04 0.15 0.06 0.03 0.16 0.03 0.00 0.54 0.04 0.03 0.01 0.27 0.06 0.01 0.48 9420.78 710.39 4508.83 557.87 12124.58 1016.97 11840.75 835.78 12479.65 976.22 2013.47 95.00 1587.50 137.30 654.02 50.49 949.28 74.05 1236.40 79.88 765.48 87.38 11977.00 7694.16 1870.17 1379.59 14821.33 10064.59 10742.93 7226.60 14708.74 12639.34 1133.52 1235.67 1287.01 1022.89 607.89 286.81 741.86 347.07 918.74 545.65 587.53 656.44 Table 5: Distribution of status types and average tokens used per model in instruction-based and code-based settings. From Table 5, which shows the distribution of status types and average token usage per model in two different settings, we observe that in instruction-based settings, most reasoning models consume significantly more tokens than nonreasoning models, with typical reasoning models using more than five times the tokens of their non-reasoning counterparts (though o4-mini is an exception with more modest token usage). In code-based settings, o4-minis token usage remains similar to instruction-based settings, while other reasoning models consume substantially more tokensapproximately ten times that of nonreasoning models. Regarding status types in instruction-based settings, reasoning models generally make fewer errors, suggesting that increased reasoning tokens at test-time correlate with error reduction. However, in code-based settings, the situation differs. While existing research demonstrates that large reasoning models excel in competitive programming (OpenAI et al., 2025), our puzzle scenario yields different results. The table indicates that the best non-reasoning model, GPT-4.1, remains comparable to reasoning models, while one reasoning model, QwQ-32B, shows notably low legal rate due to high incidence of syntax errors in its code generation."
        },
        {
            "title": "5 Conclusion",
            "content": "PUZZLEPLEX is the first benchmark to compare reasoning techniques on puzzles that span text and vision modalities, deterministic and stochastic dynamics, and long-horizon interactions. It enables systematic evaluation of models through both instruction-based and code-based settings. We find that reasoning models perform best in instructionbased settings, benefiting from increased testtime computation. Open-source models such as DeepSeek-R1 match or surpass proprietary models, demonstrating rapid progress. In contrast, the code-based setting poses greater challenges due to the need for accurate program synthesis, though its lower computational cost and scalability make it promising direction. Best-of-n sampling significantly improves performance in this setting. Multimodal inputs and legality-aware prompting offer further gains in specific scenarios. However, our analysis reveals that models often struggle with multi-hop reasoninge.g., in TIDYTOWER, removing prior reasoning history improves accuracy, suggesting that current models may be misled by irrelevant context. Overall, PUZZLEPLEX offers testbed for advancing reasoning and planning in foundation models, highlighting limitations of current systems and consequent opportunities for future research."
        },
        {
            "title": "Limitations",
            "content": "Although PUZZLEPLEX spans 15 carefully curated puzzles, the overall number of puzzles remains modest, so results may be sensitive to the specific puzzle mix and random seeds. Moreover, due to rapid model evolution and budget constraints, our experiments may not include the latest model releases available after the experiment period. Finally, PUZZLEPLEX does not yet assess whether fine-tuned models can outperform existing LLMs, which could provide additional insights."
        },
        {
            "title": "Ethics Statement",
            "content": "Our study uses only rule-based puzzlesno human subjects or personally identifiable information. All puzzles are original or permissively licensed. We execute model-generated code in sandbox, follow provider terms/safety policies, and log only nonsensitive metadata."
        },
        {
            "title": "References",
            "content": "Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu, and Sergey Levine. 2023. Lmrl gym: Benchmarks for multi-turn reinforcement learning with language models. Preprint, arXiv:2311.18232. Kareem Ahmed, Kai-Wei Chang, and Guy Van den Broeck. 2023. Semantic strengthening of neurosymbolic learning. Preprint, arXiv:2302.14207. AI Anthropic. 2024. Introducing claude 3.5 sonnet. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025. Qwen2.5-vl technical report. Preprint, arXiv:2502.13923. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. Piqa: Reasoning about physical commonsense in natural language. Preprint, arXiv:1911.11641. Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. 2023. Elo uncovered: Robustness and best practices in language model evaluation. In Proceedings of the Third Workshop on Natural Language Generation, Evaluation, and Metrics (GEM), pages 339352, Singapore. Association for Computational Linguistics. Curtis Bright, Jürgen Gerhard, Ilias Kotsireas, and Vijay Ganesh. 2020. Effective Problem Solving Using SAT Solvers, page 205219. Springer International Publishing. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, and 16 others. 2022. Scaling instruction-finetuned language models. Preprint, arXiv:2210.11416. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2025b. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. 2024. Everything of thoughts: Defying the law of penrose triangle for thought generation. Preprint, arXiv:2311.04254. Thiago Freitas dos Santos, Paulo E. Santos, Leonardo A. Ferreira, Reinaldo A. C. Bianchi, and Pedro Cabalar. 2019. Heuristics, answer set programming and markov decision process for solving set of spatial puzzles. Preprint, arXiv:1903.03411. Benjamin Estermann, Luca A. Lanzendörfer, Yannick Niedermayr, and Roger Wattenhofer. 2024. Puzzles: benchmark for neural algorithmic reasoning. In Advances in Neural Information Processing Systems, volume 37, pages 127059127098. Curran Associates, Inc. Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Mguni, Yali Du, and Jun Wang. 2023. Chessgpt: Bridging policy learning and language modeling. Preprint, arXiv:2306.09200. Google. 2025. Gemini 2.5: Our most intelligent ai model. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Jiayi Gui, Yiming Liu, Jiale Cheng, Xiaotao Gu, Xiao Liu, Hongning Wang, Yuxiao Dong, Jie Tang, and Minlie Huang. 2024a. Logicgame: Benchmarking 9 rule-based reasoning abilities of large language models. Preprint, arXiv:2408.15778. Jiayi Gui, Yiming Liu, Jiale Cheng, Xiaotao Gu, Xiao Liu, Hongning Wang, Yuxiao Dong, Jie Tang, and Minlie Huang. 2024b. Logicgame: Benchmarking rule-based reasoning abilities of large language models. Preprint, arXiv:2408.15778. Andrea Høfler. 2014. Smt solver comparison. Graz, July, 17. Chenghao Huang, Yanbo Cao, Yinlong Wen, Tao Zhou, and Yanru Zhang. 2024. Pokergpt: An end-to-end lightweight solver for multi-player texas holdem via large language model. Preprint, arXiv:2401.06781. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, Vaiva Imbrasaite, and Deepak Ramachandran. 2023. Boardgameqa: dataset for natural language reasoning with contradictory information. Advances in Neural Information Processing Systems, 36:3905239074. Richard E. Korf. 1990. Real-time heuristic search. Artificial Intelligence, 42(2):189211. Rhyd Lewis. 2007. Metaheuristics can solve sudoku puzzles. Journal of Heuristics, 13:387401. Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, and Xiang Ren. 2021. Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. Preprint, arXiv:2101.00376. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, and 3 others. 2023. Agentbench: Evaluating llms as agents. Preprint, arXiv:2308.03688. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. Preprint, arXiv:2303.17651. Microsoft, :, Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi ling Chen, Qi Dai, and 57 others. 2025. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. Preprint, arXiv:2503.01743. Chinmay Mittal, Krishna Kartik, Mausam, and Parag Singla. 2025. Fcorebench: Can large language models solve challenging first-order combinatorial reasoning problems? Preprint, arXiv:2402.02611. Adithya Murali, Atharva Sehgal, Paul Krogmeier, and P. Madhusudan. 2022. Composing neural learning and symbolic reasoning with an application to visual discrimination. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-2022, page 33583365. International Joint Conferences on Artificial Intelligence Organization. David Noever and Ryerson Burdick. 2021. Puzzle solving without search or human knowledge: An unnatural language approach. Preprint, arXiv:2109.02797. OpenAI, :, Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, and 7 others. 2025. Competitive programming with large reasoning models. Preprint, arXiv:2502.06807. OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, and 244 others. 2024a. Openai o1 system card. Preprint, arXiv:2412.16720. OpenAI. 2025a. Introducing gpt-4.1 in the api. OpenAI. 2025b. Openai o3 and o4-mini system card. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, and 262 others. 2024b. Gpt-4 technical report. Preprint, arXiv:2303.08774. Qwen. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and 1 others. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Yufan Ren, Konstantinos Tertikas, Shalini Maiti, Junlin Han, Tong Zhang, Sabine Süsstrunk, and Filippos Kokkinos. 2025. Vgrp-bench: Visual grid reasoning puzzle benchmark for large vision-language models. Preprint, arXiv:2503.23064. Abdelrahman Sadallah, Daria Kotova, and Ekaterina Kochmar. 2025. Are llms good cryptic crossword solvers? Preprint, arXiv:2403.12094. Tal Schuster, Ashwin Kalyan, Oleksandr Polozov, and Adam Tauman Kalai. 2021. Programming puzzles. Preprint, arXiv:2106.05784. Dennis Shasha. 2017. Ruby risks. Communications of the ACM, 60(7):104104. 10 Dennis Shasha. 2022a. Card nim. Communications of xAI. 2025. Grok 3 beta the age of reasoning agents. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822. Zeyu Zhang, Zijian Chen, Zicheng Zhang, Yuze Sun, Yuan Tian, Ziheng Jia, Chunyi Li, Xiaohong Liu, Xiongkuo Min, and Guangtao Zhai. 2025. Puzzlebench: fully dynamic evaluation framework for large multimodal models on puzzle solving. Preprint, arXiv:2504.10885. Jingmiao Zhao and Carolyn Jane Anderson. 2023. Solving and generating npr sunday puzzles with large language models. Preprint, arXiv:2306.12255. the ACM, 65(10):9696. Dennis Shasha. 2022b. Exclusivity probes. Communications of the ACM, 65(7):96ff. Dennis Shasha. 2022c. Maximal cocktails. Communications of the ACM, 66(1):112112. Dennis Shasha. 2023. Tidy towers. Communications of the ACM, 66(10):116ff. David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, and 1 others. 2016. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489. David K. Smith. 2007. Dynamic programming and board games: survey. European Journal of Operational Research, 176(3):12991318. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, and 197 others. 2025. Gemma 3 technical report. Preprint, arXiv:2503.19786. Graham Todd, Tim Merino, Sam Earle, and Julian Togelius. 2024. Missed connections: Lateral thinking puzzles for large language models. Preprint, arXiv:2404.11730. Clinton J. Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, and Dan Hendrycks. 2025. Enigmaeval: benchmark of long multimodal reasoning challenges. Preprint, arXiv:2502.08859. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. Huggingfaces transformers: State-of-the-art natural language processing. Preprint, arXiv:1910.03771. Yue Wu, Xuan Tang, Tom M. Mitchell, and Yuanzhi Li. 2024a. Smartplay: benchmark for llms as intelligent agents. Preprint, arXiv:2310.01557. Yue Wu, Xuan Tang, Tom M. Mitchell, and Yuanzhi Li. 2024b. Smartplay: benchmark for llms as intelligent agents. Preprint, arXiv:2310.01557. 13 13 14 15 23 23 23 24 24 24 24 24 25 25 26 26 27"
        },
        {
            "title": "A PUZZLEPLEX",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Dataset Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Example of Simulator . A.3 Breakdown Description of Puzzles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Experimental Setup",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 LLMs Configuration . B.2 Costomized Model Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Implementation Details of Model Inference . . . . . . . . . . . . . . . . . . . . . . . . B.4 Implementation Details of Elo Score . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 Operation Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6 The Cost of Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.7 Code-based Prompt Template . . . . . ."
        },
        {
            "title": "C Experiment Results",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 Elo Score Results . C.2 Results of Different Prompting Strategies . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Breakdown Instruction-based Results of Puzzles . . . . . . . . . . . . . . . . . . . . . . C.4 Instruction-based vs. Code-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.5 Play Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.6 Win Probability Matrix . . . . . . . . . ."
        },
        {
            "title": "A PUZZLEPLEX",
            "content": "A.1 Dataset Overview Name SudoKill TidyTower (Shasha, 2023) CardNim (Shasha, 2022a) OptimalTouring CountMaximalCocktails (Shasha, 2022c) MaxMaximalCocktails ExclusivityParticles (Shasha, 2022b) ExclusivityProbes RubyRisks (Shasha, 2017) BeatOrBombSto. MaxTarget LargerTarget Superply SudoKill M. Superply M. Scenario Reward Data Main Reasoning Two-player Single-player Two-player Single-player Single-player Two-player Two-player Single-player Single-player Two-player Single-player Two-player Two-player Two-player Two-player Deterministic Deterministic Deterministic Deterministic Deterministic Deterministic Deterministic Stochastic Stochastic Stochastic Stochastic Stochastic Deterministic Text Text Text Text Text Text Text Text Text Text Text Text Text Logical, Spatial Spatial Numerical, Logical Numerical Logical Logical Numerical, Spatial Numerical, Spatial Numerical, Logical Logical, Numerical Logical, Numerical Logical, Numerical Numerical, Spatial Deterministic Text-Image Visual, Logical Deterministic Text-Image Visual, Numerical Table 6: Overview of Puzzle Games. 13 A.2 Example of Simulator (a) (c) (b) (d) Figure 4: Overview of Simulator. The purpose of the Simulator is to present history of the moves for given puzzle for review by people. The representation of that history will differ for each kind of puzzle and the particular steps will depend on the methods used. SuperplyM is two-player puzzle whose pedagogical goal for people is to teach arithmetic (e.g. multiplication). Play alternates between red and green players. When the red player responds correctly to hint (panels and c), the location chosen by that player turns red. When the green player responds (panels and d), the location chosen by the green player turns green. In this image, we see sequence of four moves, two by red and two by green, illustrating the history of moves taken by each method. A.3 Breakdown Description of Puzzles SudoKill Rule Sudokill is competitive two-player variant of the classic Sudoku game. Like standard Sudoku, the game is played on grid where the objective is to fill each row, column, and subgrid with the numbers from 1 to n, where is the size of the row or column, without repeating any number in the same row, column, or subgrid. In Sudokill, players take turns placing number into an unoccupied cell. The first player can choose any empty cell to start the game. After that, each player must place their number in an unoccupied cell that lies in the same row or column as the last move made by their opponent. If there are no such cells available, the player may choose any unoccupied cell on the board. move is considered invalid if it violates standard Sudoku rules (i.e., placing number that already appears in the same row, column, or subgrid), or if it is made in cell not allowed by the row/column constraint described above. The first player to make an invalid move loses the game. Example If the current grid is [6, 8, 4, 5, 1, 3, 2, 7, 9], [5, 9, 7, 6, 2, 0, 1, 8, 0], [2, 3, 1, 4, 8, 7, 6, 5, 0], [9, 1, 2, 7, 6, 4, 8, 0, 3], [4, 6, 8, 3, 0, 1, 7, 2, 5], [7, 5, 3, 2, 9, 8, 4, 1, 6], [8, 4, 5, 1, 3, 2, 9, 6, 7], [1, 0, 6, 9, 0, 5, 0, 3, 8], [3, 2, 0, 0, 7, 0, 5, 4, 0] and now is your turn and the previous move by the opponent is to fill the cell at (0, 8) with the value 9. So now the cells you can place number are [(1,8), (2,8), (8,8)] because you can only place number in the same row or column as the last move. If the current grid is [6, 8, 4, 5, 1, 3, 2, 7, 9], [5, 9, 7, 6, 2, 0, 1, 8, 0], [2, 3, 1, 4, 8, 7, 6, 5, 0], [9, 1, 2, 7, 6, 4, 8, 0, 3], [4, 6, 8, 3, 0, 1, 7, 2, 5], [7, 5, 3, 2, 9, 8, 4, 1, 6], [8, 4, 5, 1, 3, 2, 9, 6, 7], [1, 0, 6, 9, 0, 5, 0, 3, 8], [3, 2, 0, 0, 7, 0, 5, 4, 1] and now is your turn and the previous move by the opponent is to fill the cell at (0, 8) with the value 9. Now you can fill the cell (1, 8) with the value 4 to win this game because after you fill the cell (1, 8) with the value 4, the opponent can only fill the cell (2, 8) and (1, 5), but no matter which value the opponent fills in these two cells will violate the rules. Figure 5: Description of SudoKill. 15 TidyTower Rule Tidy Tower is single-player puzzle involving vertical stack of cubes, each with four colored sides arranged in fixed clockwise order: Red (R), Yellow (Y), Blue (B), and Green (G). The players objective is to transform the tower such that all cubes display the same color on their front facethis state is referred to as tidy tower. Two types of operations are allowed to manipulate the tower: Rotate: When player rotates cube at certain index, that cube and all cubes above it rotate together in clockwise direction. single rotation shifts the front-facing side of cube to the next color in the clockwise sequence. For example, rotating once changes cube with front face \"R\" to \"Y\", and so on. Rotating four times returns it to the original orientation. Rotate with Holding: player can also rotate cube while holding cube above it. This operation rotates only the selected cube and all cubes below it, while the held cube and any cubes above it remain in place. Example The initial setting is: RGBYRGBYBGBGBG. Can you make this tower tidy in eight moves or less? Solution for eight moves: RGBYRGBYBGBGBG (rotate by one position at position 1 and not hold at position 2) RRGBYRGBGRGRGR (rotate by one position at position 2 and hold at position 3) RRRBYRGBGRGRGR (rotate by two positions at position 3 and hold at position 4) RRRRYRGBGRGRGR (rotate by one at position 4 and hold at position 5) RRRRRRGBGRGRGR (rotate by one at position 6 and hold at position 9) RRRRRRRGRRGRGR (rotate by one at position 7 and hold at position 8) RRRRRRRRRRGRGR (rotate by one at position 10 and hold at position 11) RRRRRRRRRRRRGR (rotate by one at position 12 and hold at position 13) RRRRRRRRRRRRRR Done Figure 6: Description of TidyTower. CardNim Rule Card Nim is two-player turn-based game played with shared pile of stones and individual hands of number cards. At the start of the game, both players receive set of cards, each card displaying positive integer. single pile of stones is placed at the center of the board. On each turn, player must play one of their cards to remove exactly that number of stones from the pile. card can only be played if its value is less than or equal to the number of stones remaining. Once card is used, it cannot be reused. The two players take turns alternately. The objective is to be the player who removes the last stone from the pile. However, if player is unable to play any card on their turnbecause all of their remaining cards are greater than the number of remaining stonesthey lose the game immediately. Example For example, suppose there are five stones left and each of the two players you and your opponent has three cards with 1, 2,and 3, respectively. You goes first. Who wins? Your opponent wins. If you removes 2 or 3, then opponent can win immediately with 3 or 2 respectively. So, you removes 1. Now your opponent removes 3, leaving 1. Now you has only cards with numbers greater than 1 so you lose. Figure 7: Description of CardNim. 16 OptimalTouring Rule Optimal Touring is route optimization puzzle in which player must plan one-day tour across set of tourist sites. Each site is defined by the following attributes: - location represented by street and avenue coordinates. - fixed visiting time (in minutes) that must be spent at the site. - value, indicating the reward or importance of visiting the site. - visiting window specified by start and end hour (in 24-hour format), representing when the site is accessible. The players objective is to maximize the total value of visited sites while adhering to time and location constraints. The total time spent on the tour includes: - The visit time required at each site. - The travel time between consecutive sites, computed using Manhattan distance (i.e., the sum of the absolute differences in street and avenue numbers). The tour can start at any site, but each site must be visited within its allowed time window, and the cumulative time (including both visiting and travel) must respect this schedule. Once site is visited, its value is counted toward the total. Example Here is an example of data: Site Avenue Street Desired Time Value Begin Hour End Hour 1 2 3 4 5 50 8 88 0 1 96 23 69 95 48 114 190 218 101 192 3 186 3 86 199 6 9 9 6 12 17 12 12 12 If you start visit cite 5 at 5:00, then go to site 2, then the hour is after 12:00, and the value you get is 199 + 186 = 385. Figure 8: Description of OptimalTouring. CountMaximalCocktails Rule Count Maximal Cocktails is combinatorial puzzle inspired by drug treatment for orphan diseases, where the objective is to discover safe and effective drug combinations. Each drug is represented as node in an undirected graph. Pairs of drugs that should not be combined due to harmful interactions are represented as edges connecting the respective nodes. cocktail is subset of drugs that can be administered together safelymeaning no two drugs in the subset have harmful interaction. In graph theory, such subset corresponds to an independent set, where no two nodes are directly connected by an edge. The goal is to identify all maximal independent sets of the graph, referred to in this context as maximal cocktails. set is maximal if it is an independent set and no additional drug can be added to it without introducing harmful interaction (i.e., violating independence). Note that \"maximal\" does not mean \"maximum in size\"; rather, it means that the set cannot be extended further while maintaining its validity. Two difficulty settings are defined: In the easy level, only the number of maximal cocktails needs to be determined. In the normal level, the task is to list all maximal cocktails explicitly. The input includes list of drugs (nodes_list) and list of harmful interactions (edges_list), where each edge is pair of drug identifiers indicating conflict. Example Suppose the drug list is [1, 2, 3, 4] and the bad interaction list is [(1, 2)]. The maximal cocktails are [1, 3, 4] and [2, 3, 4], and the number of maximal cocktails is 2. Figure 9: Description of CountMaximalCocktails. 17 MaxMaximalCocktails Rule Max Maximal Cocktails is strategic two-player game played on graph where nodes represent drugs, and edges represent harmful interactions between drug pairs. The games core objective is to manipulate the structure of the graph by adding edges, while maintaining or increasing the number of valid drug combinationscalled maximal cocktails. maximal cocktail is defined as maximal independent set in the graph: set of drugs in which no two drugs have harmful interaction, and to which no more drugs can be added without creating conflict. At the beginning of the game, list of nodes (drugs) is provided with no edges, meaning all combinations are potentially valid. Players take turns, and on each turn, player adds an edge between two distinct nodes. The added edge represents the discovery or introduction of harmful interaction between the two corresponding drugs. The key rule is that move is only legal if it does not decrease the current number of maximal cocktails. The first player who adds an edge that causes decrease in the number of maximal cocktails loses the game. Example Suppose the node list is [1, 2, 3], and you are the first player, you can add the edge (1, 2), then the number of maximal cocktails is 2, which is larger than the number of maximal cocktails without the edge (1, 2), which is 1. So this addition is legal. But if your opponent adds the edge (2, 3) after you add the edge (1, 2), then the number of maximal cocktails is 3, which is also legal. After that, you will lose since you cannot add any edge to increase the number of maximal cocktails. Figure 10: Description of MaxMaximalCocktails. ExclusivityParticles Rule Exclusivity Particles is two-player combinatorial game played in d-dimensional binary space, often conceptualized as the vertices of d-dimensional hypercube. Each coordinate in this space is binaryeither 0 or 1representing discrete states along each dimension (e.g., spin up or down). Players take turns placing particles at positions in this space. Each particle occupies unique vertex of the hypercube. strict exclusion principle governs the game: Any two particles must differ in at least dimensions, meaning their Hamming distance must be greater than or equal to k. The Hamming distance is calculated as the number of differing coordinates between two binary vectors. The game proceeds as follows: - The first player places particle at any position in the d-dimensional binary space. - The second player then places another particle at different position that satisfies the minimum distance condition with respect to all previously placed particles. - Players alternate turns. - player loses if they cannot place new particle that maintains the required distance from all previously placed particles. Example If the dimension is 3 and the required distance is 2, and you are the first player, you could place the first particle at [0, 0, 0]. The second player could then place the second particle at [0, 1, 1]. If you place the third particle at [1, 0, 1], the second player cannot place fourth particle that satisfies the condition and would lose. Figure 11: Description of ExclusivityParticles. ExclusivityProbes Rule Exclusivity Probes is deductive search game played in d-dimensional binary space, conceptually represented as d-dimensional hypercube, where each position is binary vector of length (each dimension having value 0 or 1). There are exactly num_particles particles hidden in this space, and they obey strict exclusion principle: any two particles must differ in at least dimensions, meaning their Hamming distance must be greater than or equal to k. The Hamming distance between two positions is the number of coordinates in which they differ. The player interacts with the environment by making probes. probe is query at specific position in the hypercube. The response will be: - \"yes\" if there is particle at that exact position, - \"no\" otherwise. The objective is to identify the exact locations of all particles using as few probes as possible. Example If the dimension is 2, the number of particles is 2, and the distance is 1. You can probe the position [0, 0], and if the response is yes, we only need one more probe to find the other particle because the particles can be either at locations [0, 0] and [1, 1] or at [0, 1] and [1, 0]. If the response is no, we may need 3 more probes to find all the particles. Figure 12: Description of ExclusivityProbes. RubyRisks Rule Ruby Risks is sequential deduction game involving set of num_boxes hidden containers, each holding an unknown number of identical rubies. The total number of rubies across all boxes is known in advance and given as total_rubies. Each turn, the player submits single request: number of rubies they wish to take from the next unopened box. Boxes are opened in fixed left-to-right order, one per turn. The outcome of request depends on the number of rubies hidden in the box: - If the request is less than or equal to the number of rubies in the box, the player successfully collects that amount. - If the request is greater than the number of rubies in the box, the request fails, and the player receives nothing from that box. The game proceeds turn by turn, with each turn corresponding to new box. The goal is to maximize the total number of rubies collected across all turns. Example Suppose there are 3 boxes, and the hidden rubies in each box are: [11, 9, 10]. Total rubies = 30. Turn 1: You request 10 rubies. Feedback: 10 (successfully take 10 rubies). Turn 2: You request 8 rubies. Feedback: 8 (successfully take 8 rubies). Turn 3: You request 12 rubies. Feedback: 0 (because 12 > 10, so you get nothing from that box). Total rubies collected so far: 18. Figure 13: Description of RubyRisks. 19 BeatOrBombSto Rule Beat Or Bomb Sto. is two-player card game where players tactically choose when to compete or give up with the cards in their hands to maximize their total score across several rounds. At the beginning, each player is given set of num_cards, which may differ in composition but are balanced so that the total value of each set is the same. Card values are assigned as follows: - Numeric cards (210) are worth their face value. - Jacks, Queens, Kings, and Aces have values of 11, 12, 13, and 1, respectively. Each round proceeds as follows: - Both players simultaneously select one card from their remaining set and decide whether to compete with it or give it up. - This decision is privateneither player knows the others card or choice until both have confirmed. - Regardless of the choice, the selected card is removed from the players hand. Scoring rules: - If both players compete, the player with the higher card earns points equal to both card values combined. The other player earns nothing. - If both players give up, no points are awarded. - If one player competes while the other gives up, the competing player earns points equal to their cards value. The giving-up player earns nothing. The game continues until all cards are used. The player with the most points at the end wins. Example In one of the rounds, if you choose to play the card 5 and compete, your opponent plays the card and give up, you will get 5 points and your opponent gets nothing. Figure 14: Description of BeatOrBombSto. MaxTarget Rule Max Targetis probabilistic decision-making game where the player must choose bags of coins over fixed number of turns to maximize the total value of collected coins. The game consists of bag_count bags, each containing coins with specific known values (e.g., [1, 2], [3, 4]), but the order of the bags is randomized before gameplay begins. At the start, the player is informed of: - The list of coin values contained in each bag. - The total number of picks allowed during the game (max_guess). Each turn proceeds as follows: - The player chooses bag index. - One random coin is drawn from the chosen bag and added to the players total score. - The drawn coin is then removed from that bag. - Over time, based on the drawn coins, the player can infer which observed bag maps to which known configuration. Example If youre told the bags contain [1, 2] and [3, 4], and the total number of picks is 2. If you pick bag 0 and get coin value of 4, then in the next turn, you will know that bag 0 contains [3, 4] and bag 1 contains [1, 2], and value 4 in bag 0 is removed and remaining values are [3]. So, if you pick bag 0 again, you will get coin value of 3, which is bigger than the coin value of bag 1. So, you should pick bag 0 again to maximize your score. Figure 15: Description of MaxTarget. 20 LargerTarget Rule Larger Target is two-player competitive coin-picking game. There are bag_count bags, each containing known list of coin values, but the order of the bags is randomized at the start of the game. Players alternate turns, each making total of max_guess picks across the game. Each turn proceeds as follows: - The player selects bag index. - random coin is drawn from the chosen bag and removed from it. - The value of the drawn coin is added to the players total score. The objective is to accumulate higher total coin value than the opponent by making informed decisions about which bag to choose. Since the order of bags is shuffled, players must deduce which real bag corresponds to each index by observing the coins drawnboth by themselves and their opponent. Example If youre told the bags contain [1, 2] and [3, 4], and the total number of picks is 2. If your opponent pick bag 0 and get coin value of 3, then in your turn, you will know that bag 0 contains [3, 4] and bag 1 contains [1, 2], and value 3 in bag 0 is removed and remaining values are [4]. So, if you pick bag 0 again, you will get coin value of 4, which is bigger than the coin value of bag 1. So, you should pick bag 0 to make your score higher than your opponent. Figure 16: Description of LargerTarget. 21 Superply Rule Superply is two-player competitive path-building game played on 1-indexed grid-based board. The grid is initially filled with zeros and players take turns selecting valid positions based on system-provided mathematical hints. Each player has unique path-building objective: - Player 1 aims to build continuous path of their claimed squares (marked with value 1) from the left edge of the grid to the right. - Player 2 aims to build path (marked with value 2) from the top edge to the bottom. valid path is sequence of adjacent same-value cells, where adjacency includes both sidewise and diagonal (corner) neighbors. Each turn proceeds as follows: - The system provides hint, such as condition on the sum or product of the row and column indices (e.g., \"sum < 10\", \"product contains digit 6\"). - The player selects grid cell (row, column) that: 1) Is currently unoccupied (i.e., value is 0); 2) Satisfies the hint condition. - If the selection is valid, the cell is updated to reflect the players value (1 or 2). Otherwise, the move is skipped and the turn passes to the opponent. The game ends when player successfully builds full path satisfying their objective. The first to do so wins. Example If the hint is \"product contains digit 6,\" and the grid is as follows: [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0] If you are Player 1, you can select the position (1, 6), (6, 1), (2, 3), (3, 2) or (6, 6) because the product of the row and column indices is 6, 6, 6, 6 and 36, respectively, and they all contain the digit 6. If you choose the position (6, 6), the grid becomes: [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1] Figure 17: Description of Superply."
        },
        {
            "title": "B Experimental Setup",
            "content": "B.1 LLMs Configuration"
        },
        {
            "title": "Input Modalities",
            "content": "Deepseek-R"
        },
        {
            "title": "Deepseek",
            "content": "Deepseek-R1 2025.1 Open-source text o4-mini"
        },
        {
            "title": "OpenAI",
            "content": "o4-mini-medium 2025."
        },
        {
            "title": "Proprietary",
            "content": "text & image Gemini-2.5-pro"
        },
        {
            "title": "Google",
            "content": "gemini-2.5-pro-preview-03-25 2025."
        },
        {
            "title": "Proprietary",
            "content": "text & image QwQ-32B"
        },
        {
            "title": "Alibaba",
            "content": "grok-3-mini xAI QwQ-32B grok-3-mini 2025.3 Open-source 2025."
        },
        {
            "title": "Proprietary",
            "content": "Deepseek-V"
        },
        {
            "title": "Deepseek",
            "content": "Deepseek-V3 2024.12 Open-source text text text GPT-4."
        },
        {
            "title": "OpenAI",
            "content": "gpt-4.1-2025-04-14 2025."
        },
        {
            "title": "Proprietary",
            "content": "text & image Qwen2.5-VL-72B"
        },
        {
            "title": "Alibaba",
            "content": "Qwen2.5-VL-72B-Instruct 2025.2 Open-source text & image Llama-3.3-70B"
        },
        {
            "title": "Meta",
            "content": "Llama-3.3-70B-Instruct 2024.12 Open-source text Gemma-3-27B"
        },
        {
            "title": "Google",
            "content": "gemma-3-27b-it 2025.3 Open-source text & image Phi-4-multimodal Microsoft Phi-4-multimodal-instruct 2025.3 Open-source text & image Table 7: Details of the LLMs evaluated in PUZZLEPLEX. B.2 Costomized Model Configuration"
        },
        {
            "title": "Normal",
            "content": "SudoKill TidyTower CardNim OptimalTouring CountMaximalCocktails MaxMaximalCocktails ExclusivityParticles ExclusivityProbes RubyRisks BeatOrBombSto. MaxTarget LargerTarget Superply"
        },
        {
            "title": "Greedy\nDynamic Programming\nDynamic Programming",
            "content": "Random Dynamic Programming Random Simulated Annealing Algorithm Simulated Annealing Algorithm Brute-force Random Brute-force Random Monte-Carlo Tree Search Random Greedy Random Random Brute-force Brute-force Greedy Greedy Monte-Carlo Tree Search Greedy Greedy Greedy Searching Table 8: Overview of puzzle games and their basic strategies. For text-image puzzles, we apply strategies similar to those used in corresponding text-only puzzles. 23 B."
        },
        {
            "title": "Implementation Details of Model Inference",
            "content": "We use APIs to evaluate several models: Deepseek-R1, o4-mini, Gemini-2.5-pro, grok-3-mini, DeepseekV3, GPT-4.1, and Phi-4-multimodal. For other models, we utilize Hugging Face Transformers (Wolf et al., 2020) inference on 8 H100 and 8 A100. B."
        },
        {
            "title": "Implementation Details of Elo Score",
            "content": "Each player begins with an initial rating = 1000. After match between player and player B, player As updated rating is given by = RA + (SA EA) where RA and RB are the current Elo ratings of players and B, respectively. = 32 is the update constant we set. SA {1, 0.5, 0} is the actual result of the game (1 = win, 0.5 = draw, 0 = loss). EA is the expected score for player A, computed as EA = 1 1+10(RB RA)/400 . B.5 Operation Extraction In an instruction-based as well as code-based setting, the raw output of the LLM may not be in correct format. For each turn of an LLM, PUZZLEPLEX allows the LLM up to five attemps to generate move with the correct format (based on regular expression checker). As soon as an attempt generates correct format, the result is sent to the state transition engine of PUZZLEPLEX. If none of attempts generates move having the correct format, that LLM loses the game. B.6 The Cost of Experiments Model Instr.-based Cost ($) Code-based Cost ($) Total Cost ($) Instr.-based GPU Hrs Code-based GPU Hrs Total GPU Hrs GPT-4.1 o4-mini Gemini-2.5-pro grok-3-mini Deepseek-R1 Deepseek-V3 Phi-4-multimodal QwQ-32B Llama-3.3-70B Gemma-3-27B Qwen2.5-VL-72B Total 162.83 90.03 556.86 20.16 37.41 27.99 8.82 903.10 7.59 3.27 33.03 5.21 14.78 1.22 0.36 65. 170.42 93.30 589.89 25.37 52.19 29.21 9.18 968.56 50.82 48.17 10.17 56.30 6.76 2.02 3.89 1.96 57.58 50.19 14.06 58.26 165. 14.63 180.09 Table 9: Experiment costs estimated across both instruction-based and code-based settings using two approaches: API-based models are priced in USD, and GPU-based models are quantified in NVIDIA H100 GPU hours. B.7 Code-based Prompt Template Code Template Prompt You are about to play game called {puzzle name} against an opponent. {puzzle rules} {code input description and format} {code output description and format} {code template} {required LLM output format} Figure 18: The code template prompt in code-based setting."
        },
        {
            "title": "C Experiment Results",
            "content": "C.1 Elo Score Results"
        },
        {
            "title": "Model",
            "content": "Single-Player Det. Two-Player Det."
        },
        {
            "title": "Custom",
            "content": "1250.9 504.5 1229.9 529.1 1060.8 309.8 1081.3 347.2 1134.6 126.4 Deepseek-R1 o4-mini Gemini-2.5-pro QwQ-32B grok-3-mini Deepseek-V3 GPT-4.1 Qwen-2.5-VL-72B Llama-3.3-70B Gemma-3-27B Phi-4-multimodal 1146.6 445.8 1094.9 549.8 1082.1 494.9 1074.1 268.5 882.4 366.0 980.2 60.4 1041.8 321.8 905.2 349.7 863.3 381.6 858.8 397.7 819.8 294.0 1084.3 379.5 1120.0 649.5 1046.3 241.2 988.2 269.8 911.1 254.8 950.7 89.1 1042.5 321.4 950.7 282.0 891.9 266.0 918.3 217.5 866.0 322.9 1163.9 122.0 1165.7 96.0 1145.0 64.3 1160.0 111.5 1145.9 190.1 989.6 163.4 1024.8 137.2 762.6 117.0 826.9 108.3 797.5 114.3 757.2 144.9 1185.1 126.2 1156.4 100.3 1117.8 136.7 1122.9 84.4 1120.6 225.1 967.7 87.9 1044.7 111.1 817.2 194.8 792.1 99.6 797.7 103.2 796.5 97.6 1152.4 63.2 1140.9 74.9 1106.2 58.1 1100.1 54.7 1044.6 97.5 973.7 42.7 1037.5 53.1 841.7 73.0 835.0 52.9 831.7 55.8 801.6 55.8 Table 10: Comparison of Elo scores (mean 95% CI) across various models on single-player and two-player deterministic puzzles, categorized by difficulty. Model TidyTower OptimalTouring CountMaximalCocktails Easy Normal Easy Normal Easy Normal Custom 1481.3 1464. 1098.1 1047.6 1173.2 o4-mini Deepseek-r1 Deepseek-V3 GPT-4.1 grok-3-mini Gemini-2.5-pro QwQ-32B Qwen-2.5-VL-72B Llama-3.3-70B Gemma-3-27B Phi-4-multimodal 956.1 956.2 956.2 956.1 956.2 956.1 956.3 956.4 956.3 956.4 956.3 952.8 952.4 952.2 953.0 952.5 952.7 952.0 1012.7 951.9 951.8 952. 1350.1 1312.6 1004.8 1190.8 712.7 1311.8 1168.7 746.0 686.0 674.1 744.4 1421.3 1251.6 914.1 1190.9 794.3 1146.5 1110.3 819.8 768.3 819.0 716.5 978.4 1170.9 979.7 978.4 978.2 978.3 1097.2 1013.2 947.5 946.1 758.9 1178.2 985.8 1048.8 985.8 983.8 986.5 1039.8 902.4 1019.8 955.5 984.2 929.4 Table 11: Performance comparison in Elo ratings of large language models on three single-player deterministic puzzles, categorized by difficulty. Model CardNim SudoKill MaxMaximalCocktails ExclusivityParticles Superply Easy Normal Easy Normal Easy Normal Easy Normal Easy Normal Custom 688.9 697. 1155.1 1355.5 1274.6 o4-mini Deepseek-r1 Deepseek-V3 GPT-4.1 grok-3-mini Gemini-2.5-pro QwQ-32B Qwen-2.5-VL-72B Llama-3.3-70B Gemma-3-27B Phi-4-multimodal 1206.8 1245.8 1042.0 984.1 1340.8 1201.7 1252.2 679.4 839.8 890.8 627.6 1153.9 1265.2 1002.4 1084.2 1313.8 1221.1 1190.7 789.9 847.7 752.7 680. 1149.5 1050.3 1198.3 1071.9 1216.0 1160.3 1132.3 705.5 789.3 662.3 709.1 1048.4 1266.1 964.0 1033.7 1260.2 1090.6 1116.5 659.5 653.9 771.3 780.3 1039.1 1110.9 898.5 999.3 935.0 1063.4 1021.2 873.3 971.4 875.7 937.6 1106.5 1124.0 1025.3 1012.8 990.0 864.6 1109.0 1016.3 1081.2 847.8 936.1 886.4 1257. 1331.1 928.2 915.8 1196.9 1126.8 913.8 885.7 1069.1 1166.5 1175.4 857.0 786.8 771.4 793.2 1188.1 1146.8 846.3 941.5 1026.6 1216.1 1118.5 782.6 811.6 806.1 784.8 1236.2 1285.6 895.4 1182.9 1168.9 1133.0 1219.1 698.0 747.1 787.4 718. 1267.8 1222.4 1013.1 1173.9 1137.7 952.4 1172.4 772.9 799.3 722.3 850.1 Table 12: Performance comparison in Elo ratings of large language models on five two-player deterministic puzzles. Scores are reported in Elo ratings for both Easy and Normal difficulty settings. 25 C.2 Results of Different Prompting Strategies TidyTower SudoKill Easy Normal Easy Normal Model GPT-4.1 w/ 1-shot w/ ToT w/o history 0.00 0.00 0.60 1.00 w/ legal candidates 0.60 o4-mini w/ 1-shot w/ ToT w/o history 0.00 0.00 0. 1.00 w/ legal candidates 1.00 0.00 0.00 0.80 0. 0.00 0.00 0.00 0.70 1.00 0. 0.30 0.10 0.30 0.20 0.60 0. 0.50 0.40 0.40 0.50 0.10 0. 0.10 0.00 0.70 0.20 0.20 0. 0.30 0.40 Table 13: Performance of GPT-4.1 and o4-mini on TIDYTOWER and SUDOKILL puzzles under different prompting strategies. C.3 Breakdown Instruction-based Results of Puzzles Model Custom Deepseek-R1 o4-mini Gemini-2.5-pro QwQ-32B grok-3-mini Deepseek-V3 GPT-4.1 Qwen-2.5-VL-72B Llama-3.3-70B Gemma-3-27B Phi-4-multimodal TidyTower OptimalTouring CountMaximalCocktails Easy Normal Easy Normal Easy Normal 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.20 0.00 0.00 0. 0.67 0.91 0.93 0.91 0.81 0.10 0.62 0.79 0.21 0.14 0.09 0.14 0.49 0.75 0.92 0.82 0.46 0.27 0.42 0.75 0.11 0.07 0.06 0.00 1.00 1.00 0.40 0.40 0.80 0.40 0.40 0.40 0.50 0.30 0.30 0. 1.00 0.70 0.40 0.50 0.30 0.40 0.30 0.30 0.40 0.30 0.30 0.10 Table 14: Instruction-based normalized scores of models on single-player deterministic puzzles, separated by difficulty. Model Custom Deepseek-R1 o4-mini Gemini-2.5-pro QwQ-32B grok-3-mini Deepseek-V3 GPT-4.1 Qwen-2.5-VL-72B Llama-3.3-70B Gemma-3-27B Phi-4-multimodal CardNim SudoKill MaxMaximalCocktails ExclusivityParticles Superply Easy Normal Easy Normal Easy Normal Easy Normal Easy Normal 0.16 0.77 0.73 0.79 0.76 0.83 0.53 0.47 0.13 0.34 0.27 0.16 0.25 0.76 0.63 0.79 0.73 0.83 0.54 0.46 0.26 0.39 0.19 0.13 0.75 0.64 0.62 0.75 0.65 0.70 0.69 0.45 0.18 0.23 0.13 0. 0.86 0.74 0.59 0.64 0.61 0.79 0.57 0.40 0.15 0.17 0.24 0.14 0.83 0.57 0.51 0.50 0.59 0.41 0.45 0.47 0.44 0.46 0.36 0.33 0.64 0.54 0.65 0.60 0.57 0.42 0.52 0.44 0.61 0.40 0.38 0. 0.80 0.56 0.68 0.70 0.69 0.65 0.46 0.25 0.34 0.31 0.15 0.18 0.86 0.54 0.70 0.75 0.70 0.60 0.30 0.38 0.23 0.33 0.25 0.18 0.43 0.74 0.83 0.66 0.78 0.76 0.45 0.53 0.12 0.22 0.23 0. 0.40 0.73 0.83 0.55 0.78 0.72 0.56 0.56 0.16 0.26 0.17 0.24 Table 15: Instruction-based normalized scores of models on two-player deterministic puzzles, separated by difficulty. 26 C.4 Instruction-based vs. Code-based Model Easy Normal 0.56 (-0.25) Deepseek-R1 0.62 (-0.21) o4-mini 0.64 (-0.24) Gemini-2.5-pro 0.48 (-0.26) QwQ-32B 0.64 (-0.30) grok-3-mini 0.40 (-0.27) Deepseek-V3 0.46 (-0.25) GPT-4.1 Qwen-2.5-VL-72B 0.30 (-0.21) 0.32 (-0.16) Llama-3.3-70B 0.06 (-0.05) Gemma-3-27B 0.18 (-0.18) Phi-4-multimodal 0.50 (-0.32) 0.54 (-0.19) 0.58 (-0.23) 0.33 (-0.25) 0.56 (-0.28) 0.32 (-0.26) 0.36 (-0.27) 0.24 (-0.19) 0.07 (-0.07) 0.00 (-0.00) 0.20 (-0.20) Table 16: Win rates of foundation models against the custom strategy in the instruction-based setting on two-player deterministic puzzles. The blue values in parentheses indicate the win rate difference (instruction-based minus code-based), highlighting performance drops in the code-based setting. C.5 Play Statistics Name SudoKill TidyTower CardNim Total Play Legal Play Legal Play Percentage #Turns #Tokens (R) #Tokens (NR) #Turns #Tokens (R) #Tokens (NR) 3.26 0.11 6281.64 73.58 916.10 24.00 7.40 0. 5362.98 118.59 812.60 38.14 1.00 0.00 7636.68 334.95 1273.59 76.42 1.00 0. 7636.68 334.95 1282.83 77.25 1.79 0.02 5690.17 143.70 492.85 16.51 1.85 0. 5383.76 156.37 444.68 12.80 OptimalTouring 1.00 0.00 17370.60 615.55 1448.76 70. 1.00 0.00 16375.78 706.73 1563.17 103.28 CountMaximalCocktails 1.00 0.00 5217.48 441. 1228.92 94.87 1.00 0.00 5268.49 459.68 1256.98 96.57 MaxMaximalCocktails 1.56 0. 7286.40 150.85 717.81 21.12 1.51 0.02 7870.05 167.68 736.76 23.98 ExclusivityParticles 7.09 0.49 2823.25 39.29 445.75 6.75 Superply 10.13 0.20 2519.22 26.78 344.56 4.17 11.39 0.20 2514.00 27. 333.38 3.40 0.12 0.99 0.77 0.62 0. 0.78 0.00 0.84 Table 17: Statistics of play and legal play across puzzles in the instruction-based setting. (R) denotes reasoning models, and (NR) denotes non-reasoning models. legal play refers to game trajectory that ends with legal termination status; for detailed definition of termination status, please refer to 4.4. #Turns indicates the number of turns per player. In two-player puzzles, the total number of rounds is the sum of turns across both players. 27 C.6 Win Probability Matrix Figure 19: Win rate heatmap for 12 methods in two-player puzzles under the instruction-based setting. Each number represents the win rate of the row entry over the column entry, normalized to ignore ties. For example, for game instances that dont end in tie, GPT-4.1 beats o4-mini 28.6% of the time and o4-mini beats GPT-4.1 71.4% of the time. 28 Figure 20: Win rate heatmap for 12 methods in two-player puzzles under the code-based setting. Each number represents the win rate of the row entry over the column entry, normalized to ignore ties. For example, for game instances that dont end in tie, GPT-4.1 beats o4-mini 50.2% of the time and o4-mini beats GPT-4.1 49.8% of the time."
        }
    ],
    "affiliations": [
        "NYU Grossman School of Medicine",
        "New York University",
        "University at Buffalo, SUNY",
        "Yale University",
        "Zhejiang University"
    ]
}