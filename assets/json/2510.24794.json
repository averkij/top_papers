{
    "paper_title": "MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models",
    "authors": [
        "Xinming Wang",
        "Jian Xu",
        "Bin Yu",
        "Sheng Lian",
        "Hongzhu Yi",
        "Yi Chen",
        "Yingjian Zhu",
        "Boran Wang",
        "Hongming Yang",
        "Han Hu",
        "Xu-Yao Zhang",
        "Cheng-Lin Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) show strong capabilities in complex reasoning, yet their marginal gains on evidence-dependent factual questions are limited. We find this limitation is partially attributable to a reasoning-answer hit gap, where the model identifies the correct facts during reasoning but fails to incorporate them into the final response, thereby reducing factual fidelity. To address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment framework that enhances factuality without relying on external verifiers. MR-ALIGN quantifies state transition probabilities along the model's thinking process and constructs a transition-aware implicit reward that reinforces beneficial reasoning patterns while suppressing defective ones at the atomic thinking segments. This re-weighting reshapes token-level signals into probability-aware segment scores, encouraging coherent reasoning trajectories that are more conducive to factual correctness. Empirical evaluations across four factual QA datasets and one long-form factuality benchmark show that MR-ALIGN consistently improves accuracy and truthfulness while reducing misleading reasoning. These results highlight that aligning the reasoning process itself, rather than merely the outputs, is pivotal for advancing factuality in LRMs."
        },
        {
            "title": "Start",
            "content": "MR-ALIGN: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models Xinming Wang1,2,3 Jian Xu1, Bin Yu2,4, Sheng Lian1,3, Hongzhu Yi5, Yi Chen1,2,3, Yingjian Zhu1,3, Boran Wang2, Hongming Yang6, Han Hu6, Xu-Yao Zhang1,*, Cheng-Lin Liu 1 1Institute of Automation, Chinese Academy of Sciences, 2Zhongguancun Academy, 3School of Artificial Intelligence, UCAS, 4Harbin Institute of Technology, 5School of Computer Science and Technology, UCAS, 6Tencent"
        },
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs) show strong capabilities in complex reasoning, yet their marginal gains on evidence-dependent factual questions are limited. We find this limitation is partially attributable to reasoninganswer hit gap, where the model identifies the correct facts during reasoning but fails to incorporate them into the final response, thereby reducing factual fidelity. To address this issue, we propose MR-ALIGN, Meta-Reasoning informed alignment framework that enhances factuality without relying on external verifiers. MR-ALIGN quantifies state-transition probabilities along the models thinking process and constructs transition-aware implicit reward that reinforces beneficial reasoning patterns while suppressing defective ones at the atomic thinking segments. This re-weighting reshapes token-level signals into probability-aware segment scores, encouraging coherent reasoning trajectories that are more conducive to factual correctness. Empirical evaluations across four factual QA datasets and one long-form factuality benchmark show that MR-ALIGN consistently improves accuracy and truthfulness while reducing misleading reasoning. These results highlight that aligning the reasoning process itself, rather than merely the outputs, is pivotal for advancing factuality in LRMs. 5 2 0 2 7 2 ] . [ 1 4 9 7 4 2 . 0 1 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Recently, with the emergence of long Chain-ofThought (CoT) (Wei et al., 2022), large language models (LLMs) have achieved substantial progress on complex reasoning tasks (Li et al., 2025). By internalizing human-like, stepwise problem-solving routines and leveraging test-time scaling, they now deliver strong gains across mathematics, the sciences, and code generation (Snell et al., 2025; Wang et al., 2025b; Wei et al., 2024c). *Corresponding author 1 Figure 1: Illustration of Reasoning-Answer Hit Gap in Factual QA. Factual question answering likewise benefits from expanded reasoning via reflection (Yan et al., 2024); however, unlike math or coding tasks, where multi-step decomposition is paramount, factual reasoning is primarily evidence-centric (Krishna et al., 2024; Lee et al., 2025). Test-time scaling encourages broad activation of internal knowledge and exploratory chains of thought, yet models can often identify the correct answer during intermediate reasoning while failing to surface it in the final output (Huang et al., 2023). Figure 1 depicts the average positional distribution of correct-answer coverage along the reasoning trajectories. In error cases, the GT candidate is often surfaced early but prematurely discarded, so the answer appears in the chain-of-thought yet is not adopted in the final output. This reasoning-answer hit gap misleads decoding and undermines factual faithfulness. Contemporary thinking trajectories largely originate from cold-start pretraining (Guo et al., 2025) and display an inductive bias toward highly structured routines. Furthermore, Reinforcement Learning (RL) based instruction tuning can further lengthen responses (Fatemi et al., 2025); empirically, models may negate an earlier correct candidate after prolonged rollout, leading to inconsistent final answers despite intermediate hits. These behaviors suggest that improving factuality requires aligning how models reason, not merely what they predict (Wang et al., 2024). We introduce MR-ALIGN, meta-reasoning informed alignment framework that models transitions among cognitively motivated meta-reasoning states and performs fine-grained preference alignment over these transitions. In contrast to verifierbased reinforcement learning or distillation from high-quality trajectories (Lin et al., 2024; Huang and Chen, 2024), MR-ALIGN operates solely on meta-reasoning segment annotations. Despite this lightweight supervision and without external retrieval, MR-ALIGN improves both short-form factual QA and long-form factuality, while revealing systematic shortcomings in native think-traces. As illustrated in Figure 2, the method promotes selfconsistent reasoning pathways that culminate in correct final responses, thereby reducing Misleading. Our main contributions are as follows: We develop cognitivegrounded metareasoning annotation pipeline that systematically identifies and categorizes the principal strategies engaged during an LLMs problemsolving process. We introduce MR-ALIGN, novel finegrained preference alignment method that integrates meta-reasoning transition probabilities into the optimization objective. Instantiated atop KahnemanTversky Optimization (KTO) (Ethayarajh et al., 2024), our approach requires neither external retrieval nor verifier signals, yet effectively guides models toward more factual reasoning patterns. Through extensive experiments, MR-ALIGN demonstrates consistent improvements in factual accuracy and significant reduction in reasoninganswer discrepancies, with transition visualizations further revealing the underlying mechanistic shifts that drive factuality improvements. Figure 2: MR-ALIGN adjusts reasoning transition for faithful response. 2017) training splits. For each question q, we consider two decoding mods of the same base model: THINKON and THINKOFF. Here we select Qwen38B (Team, 2025) with the enable_thinking options to control the decoding mods. We use the Exact Match (EM) indicator to judge the correctness of the sampling process and record binary indicators zon(q), zoff (q) {0, 1} to record the correctness of each sample in THINKON and THINKOFF mods. We label as positive samples those with (zon(q), zoff (q)) = (1, 0) under THINKON, indicating that enabling reasoning helps recover the correct answer; we label as negative samples those with (zon(q), zoff (q)) = (0, 1), indicating that the produced reasoning is defective and degrades accuracy. To mitigate sampling stochasticity while maintaining coverage, we draw = 3 independent samples using temperature = 0.2 with top_p = 0.9. We collect the intersection of positive and negative samples obtained across all draws. Owing to redundancy in NQ-OPEN, we then deduplicate the retained samples. Finally, we apply length filter to the reasoning trajectory: we split thoughts by the delimiter nn and keep instances whose segment count lies in (4, 15) and whose total reasoning tokens lie in (450, 1000). Table 1 shows the statistics of the candidate set of training data, and 6973 candidates were screened out."
        },
        {
            "title": "2.1.1 Training Data Candidates Generation",
            "content": "To construct fine-grained meta-reasoning supervision, we curate training corpus from the NQOPEN (Lee et al., 2019) and SCIQ (Welbl et al.,"
        },
        {
            "title": "Positive Negative Total",
            "content": "NQ-OPEN SCIQ TOTAL 4070 760 4830 1785 358 2143 5855 1118 6973 Table 1: Statistics of Training Data Candidates. Figure 3: Overview of MR-ALIGN Data Prepration Process. 2.1.2 Meta-reasoning Labels Annotation Inspired by cognitive-science (Holyoak and Lu, 2021; Fleming, 2024) accounts of human problemsolving, we categorize the meta-reasoning patterns exhibited by LLMs during inference. To transform open-vocabulary descriptors into fixed, closedset taxonomy, we adopt two-stage coarse-to-fine annotation strategy. First, we get open-vocabulary annotations by the advanced model GPT-5. We randomly sample 2,000 training instances and elicit freeform, segment-level descriptors using GPT-5 with prompt templates adapted from prior work (Chen et al., 2025; Li et al., 2025) (The detailed prompts are presented in the Appendix G). The resulting open tags are semantically clustered with GPT5 to produce closed taxonomy of 15 labels aligned with cognitive operations, yielding metareasoning label set C. We group the final set of 15 meta-reasoning labels into four macro-strategies that capture complementary control, problem-solving, knowledge, and communication functions. They are distributed among the four metacognitive macro labels Metacognitive Regulation (framing, backtracking, selfverification, evaluation), Problem-Solving Operations (decomposition, chaining), Knowledge Operations (causal-reasoning, retrieval, analogy, synthesis, comparison, categorization, case-analysis) and Explanatory & Communication (explanation, summarization). Specific meta-reasoning labels is explained in Appendix F. Second, we employ DEEPSEEK-CHAT and GPT4O as two independent annotators and designate GPT-5 as the adjudicator, forming an automated annotation pipeline. For each segment st, two annotators (GPT-4O, DEEPSEEK-CHAT) each propose up to two labels, with confidence score in [0, 10] for every proposed label. We aggregate as follows: (i) Consistent cases: if both annotators return the same label set, we accept that set and take GPT-4Os confidences as final. (ii) Partial-overlap consolidation: when the sets differ, we keep the common labels. If fewer than two labels remain, we supplement them with labels proposed by only one annotator, ordered by the higher of the two available confidences, and include any whose higher confidence exceeds 7 until two labels are obtained or candidates are exhausted. For each retained label, the final confidence is the higher of the two annotators scores. (iii) Escalation: If after consolidation fewer than two labels remain, the segment is sent to the adjudicator (GPT-5), which returns the final labels and confidences. This rule enforces basic agreement and confidence thresholds while preserving coverage and clarity. In Appendix and I, we provide examples of training samples and details about the annotation pipeline."
        },
        {
            "title": "2.2.1 Kahneman-Tversky Optimization",
            "content": "Preference alignment aims to align model responses with human preference, ensuring that outputs conform to human expectations, which is widely used for factuality alignment (Lin et al., 2024; Huang and Chen, 2024). Unlike Direct Preference Optimization (DPO) (Rafailov et al., 2023), which relies on pairwise preferences over triplets (x, y+, y), Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., 2024) adopts singlesample binary labels and, drawing on prospect theory, introduces Human-Aware Losses (HALOs) that evaluate gains or losses of response relative to baseline and decouple the treatment of chosen and rejected samples. Concretely, let πθ be the current policy and πref fixed reference policy. For dataset = D+ 3 of promptresponse pairs (x, y) with corresponding labels \"chosen\" or \"rejected\", define the implicit reward rθ(x, y) log πθ(y x) πref(y x) , (1) and the global baseline yi yj. To capture the compositionality of atomic strategies while keeping the model identifiable, we introduce discrete latent edge indicating which single base-level transition underlies the set-to-set move: ε {(a, b) : yi, yj} C (5) z0 E(x,y)D (cid:2)KL(cid:0)πθ( x) πref( x)(cid:1)(cid:3) , (2) estimated in practice by moving average. With inverse-temperature β > 0, logistic σ(z) = 1/(1 + ez), and positive weights λc, λr, set Intuitively, each observed pair yi yj is assumed to arise from exactly one base transition with the corresponding hidden states, which is unobserved. We summarize the unknown composition within set by within-set mixing measure v(x, y) = λc σ(cid:0)β (rθ(x, y) z0)(cid:1), λr σ(cid:0)β (z0 rθ(x, y))(cid:1), if D+, if D, (3) and let λy > 0 to ensure the loss is non-negative, the KTO loss is LKTO = 1 (cid:88) (cid:0)λy v(x, y)(cid:1). (4) (x,y)D This decoupled handling of positive and negative labels for KTO is robust to label imbalance and consistent with human-aware calibration."
        },
        {
            "title": "2.2.2 Atomic Reasoning Process Modeling\nThe thinking processes always start with <think>\nand end with </think>, which demostrate the en-\ntire thinking process.",
            "content": "We partition atomic reasoning into states (we set = 15 in Section 2.1) and denote the state set by {c1, . . . , cN }. Augmenting with boundary states yields the overall state space {<start>} {<stop>}. Let [0, 1]CC be the row-stochastic transition matrix with entries Pi,j Pr(st+1 = st = i). The terminal state <stop> is absorbing P<stop>,<stop> = 1 and P<stop>,j = 0 for all C. For single reasoning trajectory, the fine-grained annotated observations are Y1:T (y1, . . . , yT ), where is the number of annotated segments. Each segment label is set-valued, satisfying yi and yi {1, 2}. The case yi = 1 indicates that single atomic meta-reasoning strategy is active in the segment, while yi = 2 denotes composite segment in which two strategies co-occur. We treat yi as an unordered set encoding label uncertainty rather than weighted mixture at this stage. The minimal modeling unit of reasoning trajectory is transition between two set-valued labels, ρy : [0, 1], (cid:88) uy ρy(u) = 1 (6) which encodes how mass within is apportioned among its elements when forming single underlying edge. Here we choose the uniform distribution to demonstrate, as ρy(u) = 1/y."
        },
        {
            "title": "The pairwise transition probability induced by a",
            "content": "row-stochastic is Pr(yjyi, ) = (cid:88) ρyi (a)P (a, b)ρyj (b) (7) (a,b)ε(yi,yj )"
        },
        {
            "title": "Given",
            "content": "a , y(m) corpus m=1, = of {(y(m) logthe likelihood for is the sum of the pairwise contributions: samples observed-data )}M L(P ) = (cid:32) log (cid:88) m=1 (cid:88) (a,b)Em ρ (m) (a)Pabρ (m) (cid:33) , (b) (8) , y(m) (cid:1). Equations (8) provide where Em E(cid:0)y(m) self-contained likelihood principle for estimating the meta-reasoning transition matrix . For maximum-likelihood estimation in the presence of the latent variable ε, we estimate using the ExpectationMaximization (EM) algorithm (Dempster et al., 1977); the pseudocode is provided in Appendix E."
        },
        {
            "title": "2.2.3 Alignment with meta-reasoning",
            "content": "transitions As HALOs can be formalized as an implicit reward rθ that measures, along generated trajectory, the difference between the current policy πθ and fixed reference policy πref. To incorporate the segmental coherence of the reasoning process, we reweight token-level contributions by factor that reflects changes in the meta-reasoning statetransition dynamics. For reasoning sequence y1:τ 4 with final answer ya, the atomic-level implicit reward can be described as (cid:98)rθ = τ (cid:88) t=1 wt log πθ(yt x, y<t) πref(yt x, y<t) + log πθ(ya x, yτ ) πref(ya x, yτ ) . (9) Here, wt encodes how the local state-transition probability at step deviates from the global transition pattern. +/ denote the meta-reasoning transition matrix estimated from the positive (+) or negative () subset, and let denote the transition matrix estimated from the union of all samples. We set (cid:32) wt = clip Pr(yt yt1, +/) Pr(yt yt1, ) (cid:33) , m, , (10) where clip bounds the weight to mitigate smallsample artifacts. The MR-ALIGN loss is defined as LMR = 1 (cid:88) (cid:0)λy v(x, y)(cid:1), (11) (x,y)D (cid:16) β(cid:0) (cid:98)rθ z0 λc σ (cid:16) β(cid:0)z0 (cid:98)rθ λr σ (cid:1)(cid:17) , (cid:1)(cid:17) where v(x, y) generates fine-grained implicit rewards by quantifying the change in meta-reasoning transition probabilities: if (x, y) D+, v(x, y) = , if (x, y) D. (12) Intuitively, using the global transition matrix as an anchor increases the contribution of segments whose local transitions strongly diverge from the global pattern, while reducing the contribution of near-global transitions. This reshapes uniform token-level signal into probability-aware, transition-modulated reward over atomic reasoning segments."
        },
        {
            "title": "3.1 Experiments Setup",
            "content": "Dataset We evaluate our method on both factual QA and long-form factuality datasets. For factual QA, we use NQ-OPEN (Lee et al., 2019), SCIQ (Welbl et al., 2017), SIMPLEQA (Wei et al., 2024a), and TRUTHFULQA (Lin et al., 2022). Among these, NQ-OPEN and SCIQ also serve as sources for constructing our training data. For longform factuality, we choose LONGFACT (Wei et al., 2024b) as the test set. 5 Metrics For NQ-OPEN, SCIQ, and SIMPLEQA, the ground truths are short spans; we therefore report Accuracy (Acc) and Misleading (Mis). Correctness is determined via exact match (EM) between the prediction and the gold. Acc measures overall task performance, while Mis quantifies the models reasoning -asnwer hit gap. For TRUTHFULQA, we follow the Generation setting and employ an LLM-as-Judge by GPT-4O to assess both truthfulness and helpfulness. For LONGFACT, on account of the high budget for automatic evaluations, we evaluate on the 250 test examples reported in the original paper by VERISCORE (Song et al., 2024), and report 1@K where is the medium of claims together with the average number of claims per response (#Claims). Detailed metric definitions are provided in the Appendix A. Model and Baselines We consider widely used large reasoning models: QWEN3-8B, QWEN34B (Team, 2025), and DEEPSEEK-R1-DISTILLQWEN-7B (Guo et al., 2025). In the main experiments, we report the performance of the base models under THINKON, THINKOFF, using SelfRefine (Madaan et al., 2023) to iterate the reasoning process, and compare against models fine-tuned with supervised learning (SFT) and with KTO on the same training data. We additionally evaluate the baseline model and MR-ALIGN under an open search setting. The search uses the Serper API to return the top 5 snippets most relevant to the question as reference corpora. Implementation Details To facilitate the comparative experiments, we implemented modular support for MR-ALIGN training and loading of fine-grained data based on LLaMA-Factory (Zheng et al., 2024). All experiments are conducted on 4 Nvidia A800 (40GB) GPUs. During training, all LLMs are optimized with LoRA (rank = 32) (Hu et al., 2022) using the Adam optimizer in minibatch mode. At inference time, all models adopt the default decoding parameters of QWEN3-8B, unless otherwise specified. Complete training and inference hyperparameters are listed in the Appendix C. It is worth noting that due to the imbalance of positive and negative samples in the training samples, we set λr = 1.5 in the main experiment."
        },
        {
            "title": "3.2 Main Result",
            "content": "Table 2 shows the main result on 5 different datasets. Model NQ-Open SciQ SimpleQA TruthfulQA LongFact Acc Mis Acc Mis Acc Mis Truth Info 1@K #Claims vanilla w/o Thinking vanilla Thinking Self-Refine SFT KTO MR-ALIGN Ret MR-ALIGN+ Ret vanilla w/o Thinking vanilla Thinking Self-Refine SFT KTO MR-ALIGN Ret MR-ALIGN+ Ret vanilla Thinking Self-Refine SFT KTO MR-ALIGN Ret MR-ALIGN+Ret 22.66 34.10 35.26 34.43 35.48 37.34 62.80 64.18 27.78 29.92 29.72 28.45 29.20 31.00 63.49 65.18 2.85 1.19 10.94 8.45 12.80 56.34 57.42 - 9.89 9.36 8.63 8.69 7.20 8.44 7.34 - 6.62 7.60 6.45 6.04 6.01 7.78 6. 18.59 12.13 10.38 13.07 8.84 11.74 11.25 Qwen3-8B 4.10 - 4.51 14.10 3.63 17.80 4.35 13.40 4.69 12.60 5.11 11.70 66.44 9.00 67.11 8.10 Qwen3-4B 3.93 - 4.07 11.5 3.54 16.00 3.93 13.10 3.70 12.90 9.70 4.05 66.76 10.3 68.08 8.90 55.60 67.10 65.10 68.20 69.30 70.70 75.30 77.10 67.80 68.40 65.60 65.80 66.50 71.00 74.40 76.20 - 5.24 6.26 4.04 5.20 4.46 8.39 7. - 3.86 5.17 3.56 3.63 3.86 7.99 6.98 DeepSeek-R1-Distill-Qwen-7B 4.28 13.60 1.84 3.60 3.22 36.10 4.12 28.50 2.95 39.60 10.42 70.60 9.68 72.00 52.40 44.30 31.40 40.30 25.5 12.00 9.90 0.96 1.06 2.21 1.85 2.77 58.90 59.29 62.62 80.91 81.64 82.25 82.37 83.11 80.78 81.76 68.05 69.52 72.09 70.13 68.30 68.79 71.60 72. 30.97 24.85 34.03 35.74 36.59 48.59 50.55 76.38 81.27 91.80 94.12 94.61 94.12 91.68 93.27 88.13 88.13 92.04 88.62 89.84 93.39 89.11 92.41 50.92 49.71 64.99 73.19 73.44 72.58 81.27 80.89 81.13 83.93 84.95 80.72 83.29 89.12 90.14 80.77 80.07 78.92 80.85 80.94 81.36 89.68 90. 54.90 - 62.11 61.96 64.28 83.97 85.53 19 16 16 13 15 19 13 13 19 19 14 18 19 20 13 13 5 - 16 16 15 13 12 Table 2: Main result on 5 datasets with Qwen3-8B, Qwen3-4B and DeepSeek-R1-Distill-Qwen-7B. Ret represents using retriever to conduct Open Search. Bold indicates the best performance among non-retrieval methods, while underlined numbers denote the best among retrieval-augmented variants Without any external retrieval, MR-ALIGN systematically improves factual QA accuracy and markedly reduces the reasoninganswer hit gap with lower misleading, yielding more reliable reasoning that is consistent with the final response. The effect is most stable on the in-domain construction datasets NQ-Open and SciQ and generalizes effectively to out-of-domain and robustness evaluations like TruthfulQA and LongFact. Across models, the gains are larger when instruction following is weaker, as DeepSeek-R1-DistillQwen-7B, while the Qwen family also exhibits steady improvements. On SimpleQA, the gains are more modest. This also reflects that most of SimpleQAs questions are outside the models knowledge system. With the addition of retriever, MRALIGN can still achieve significant improvements over the original model, which also proves that the model can successfully generalize the learned metareasoning and balance accuracy with interpretable reasoning consistency."
        },
        {
            "title": "3.3 Ablation Study",
            "content": "Ablation of reject ratio λd As shown in Table 3, the positive and negative subsets are markedly imbalanced. To temper loss aversion induced by this imbalance, KTO recommends maintaining the ratio λcD+ λdD [1, 3/2]. Accordingly, we fix λc = 1 and tune λd [1.50, 2.25]. Table 3 reports MRALIGN performance under varying reject ratios; once λd > 1.5, performance drops rapidly. Compared to the typically milder trend observed for vanilla KTO, the suppression effect of negative samples is more pronounced in the meta-reasoning setting, as reflected in the meta-reasoning transition distributions in Figure 5. λd 1.0 1.2 1.5 2.0 2.2 2.5 NQ-Open SciQ Acc Mis Acc Mis Acc Mis SimpleQA 36.26 36.51 37.34 31.69 32.02 32.08 8.47 7.78 7.20 13.15 13.91 13.24 69.60 70.40 70.70 67.40 68.10 67.10 13.10 12.70 11.70 15.50 15.60 16.10 4.83 4.85 5.11 4.72 4.83 4. 4.96 4.92 4.46 5.73 5.50 5.20 Table 3: Ablation Studies with λd. Ablation on Data Diversity and EM-Based Transition Estimation. Table 4 reports ablations on training-data diversity and the EM estimator for transition matrix . The two components are com6 Training Data NQ-Open NQ-Open SciQ Estimation Acc Mis Acc Mis Acc Mis SimpleQA SciQ EM 34.93 33.39 35.82 37. 9.58 11.10 8.86 7.20 70.10 67.90 69.60 70.70 13.40 15.50 12.90 11.70 4.42 4.65 5.39 5.11 5.33 5.10 4.76 4.46 Table 4: Ablation Studies with Different Training Data and Transition Estimation. EM Estimation means using the Expectation Maximization algorithm to estimate the meta-reasoning transition matrix . plementary. Joint training on NQ-OPEN + SCIQ consistently outperforms single-source variants; SCIQ-only training shows no gains, likely due to limited size and diversity. Given sufficient data, the EM-based transition estimation further improves factual adherence relative to naïve frequencyweighted baseline. Although results on the more challenging SIMPLEQA exhibit some variance, the overall pattern is clear: multi-source training broadens coverage, while EM sharpens transition estimation. Their combination achieves the best balance between factual accuracy and reducing the reasoninganswer discrepancy."
        },
        {
            "title": "3.4 Futher Analysis",
            "content": "Changes in meta-reasoning preference Figure 4 contrasts the meta-reasoning transition dynamics of Qwen3-8B on 977 sampled NQ-OPEN instances before and after alignment. We report the element-wise difference = PMR-ALIGN Pvanilla. Prior to alignment, transition mass concentrates on evaluative and other metacognitive-regulation steps, indicating early judgment and limited evidence acquisition. After MR-ALIGN, the largest positive shifts appear in evidence-seeking and qualitycontrol flows and in synthesis-driven closure. In parallel, the reasoning chains become shorter, yielding more concise and targeted process. Collectively, these changes show that MR-ALIGN reallocates probability mass from reflexive evaluation toward an evidence-first, verification-aware pipeline that integrates retrieved support, synthesizes it, and converges more efficiently. Transition matrix of meta-reasoning states. Figure 5 visualizes the transition advantage matrix wt for positive and negative subsets relative to the full training corpus, refer to Section 2.2.3. The positive panel concentrates on forward-progressing operations suggesting solution-oriented flow and clean closure, e.g. categorization decomposition and chaining synthesis. In contrast, the negative panel exhibits Figure 4: Meta-reasoning transition deltas for Qwen38B before vs. after MR-ALIGN.Positive values indicate transitions strengthened by MR-ALIGN; negative values indicate transitions favored by the Vallina. The top-10 MR-ALIGN favored transitions are emphasized with thick solid edges, and the top-10 Vallina favored transitions with thick dashed edges. pronounced self-loops and regressions from analytic states back into backtracking, consistent with oscillation and detours. On account of the imbalanced dataset with D+/D 2, the mixture global transition implicitly reweights the subsets. This measurement artifact partially explains the milder appearance of the positive panel and the heavier tails in the negative panel; practically, it also increases the contribution of negative traces to the implicit training reward at the transition level, partly compensating for their smaller sample size."
        },
        {
            "title": "4 Related Works",
            "content": "Large Reasoning Models Large reasoning models (LRMs) are designed for multi-step reasoning and complex problem solving (Deng et al., 2025b), but their extended reasoning traces make them prone to compounding errors and confident hallucinations (Yao et al., 2025). Recent analyses (Sun et al., 2025; Wang et al., 2025a; Xu et al., 2025) re7 Figure 5: Meta-reasoning transition advantages wi for the positive and negative subsets relative to the full training set. Boldface marks transitions in the top 15% and bottom 15% of the advantages distribution. . veal that such errors often emerge within reasoning chains rather than final outputs. To improve reliability, structural control and supervision methods have been explored: reasoning selection and pruning (Li et al., 2025; Xu et al., 2025), atomic-level supervision (Zhang et al., 2025), and enhanced retrieval or memory mechanisms (Houliston et al., 2025; Chen et al., 2024). Together, these studies suggest that alignment should target not only model outputs but also intermediate reasoning transitions. Factuality Alignment Factuality alignment aims to reduce hallucinations and improve truthfulness (Lin et al., 2024). Short-term factuality alignment primarily focuses on tasks where outputs are concise and well-defined. EV2R (Akhtar et al., 2024) and +VERIREL (Deng et al., 2025a) develop evidence retrieval frameworks to support fact-checking pipelines, and ALIGNRAG (Wei et al., 2025) further introduces critic model to iteratively align the reasoning process itself. INFACT (Cohen et al., 2025) aligns models to generate more informative answers through preference tuning based on hierarchy of factual completeness. FSPO (Li and Ng, 2025) incorporates factuality rewards through factuality-aware policy optimization, while KNOWRL (Ren et al., 2025) integrates knowledge graph verification signals into the RL process. UALIGN (Xue et al., 2024) leverages uncertainty estimation to identify and correct potential factual errors. Long-form factuality alignment addresses open-ended generation where outputs are multi-sentence explanations. LONGFACT (Wei et al., 2024b) establishes comprehensive benchmark and analysis framework for long-form factuality assessment. MASK-DPO (Gu et al., 2025) develops fine-grained factuality alignment objective specifically for long-form generation. FACTALIGN (Huang and Chen, 2024) implements sentencelevel alignment using fine-grained factual rewards, while Chen et al. (2025) introduces online reinforcement learning with multi-faceted reward signals for factual reasoning. However, these methods primarily operate through post-hoc correction or external verification rather than fundamentally improving the reasoning process. This limitation motivates our approach of integrating meta-reasoning to enhance the factual integrity of the reasoning dynamics directly."
        },
        {
            "title": "5 Conclusion",
            "content": "This work investigates the reasoninganswer hit gap of LRMs in factual QA and long-form factuality from cognitive perspective, revealing the limitations of prevailing reasoning paradigms for factual adherence. We propose MR-ALIGN, metareasoningbased factual alignment framework that learns transition probabilities from positive samples and leverages transition-aware advantage to encourage more faithful responses. We hope this perspective motivates broader research on principled, process-level alignment for LRMs in factual domains."
        },
        {
            "title": "Limitations",
            "content": "This work still has the following limitations, which need to be explored and solved in the future: LLM-driven annotation bias Our metareasoning annotations are produced via an LLM-based pipeline. Although we employ majority voting and an adjudication stage, residual bias and uncertainty may remain, ultimately bounded by the capability ceiling of the underlying models. This dependencetogether with the associated costmakes large-scale data construction difficult to sustain. Task and model scalability We have not yet validated the effectiveness of MR-ALIGN on largerscale models. In addition, the high cost of longform factuality evaluation has prevented us from assessing substantially larger datasets. These limitations point to the need for lower-cost, more scalable frameworks for long-form factuality checking."
        },
        {
            "title": "Ethical Statement",
            "content": "The datasets NQ-OPEN (Kwiatkowski et al., 2019) and SCIQ (Welbl et al., 2017) and models (QWEN-3 series (Team, 2025) and DEEPSEEK-R1DISTILL-QWEN-7B (Guo et al., 2025)) employed in this study are all open-source, thereby incurring no risks associated with licensing. Furthermore, as our research is centered on the mathematical domain, it does not entail risks pertaining to human ethics and values."
        },
        {
            "title": "References",
            "content": "Mubashara Akhtar, Michael Schlichtkrull, and Andreas Vlachos. 2024. Ev2r: Evaluating evidence retrieval in automated fact-checking. arXiv preprint arXiv:2411.05375. Mingda Chen, Yang Li, Karthik Padthe, Rulin Shao, Alicia Sun, Luke Zettlemoyer, Gargi Ghosh, and WenImproving factuality with explicit tau Yih. 2024. working memory. arXiv preprint arXiv:2412.18069. Xilun Chen, Ilia Kulikov, Vincent-Pierre Berges, Barlas Oguz, Rulin Shao, Gargi Ghosh, Jason Weston, and Wen-tau Yih. 2025. Learning to reason for factuality. arXiv preprint arXiv:2508.05618. Roi Cohen, Russa Biswas, and Gerard de Melo. 2025. Infact: Informativeness alignment for improved llm factuality. arXiv preprint arXiv:2505.20487. Arthur Dempster, Nan Laird, and Donald Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society: series (methodological), 39(1):122. Xingyu Deng, Xi Wang, and Mark Stevenson. 2025a. + verirel: Verification feedback to enhance document retrieval for scientific fact checking. arXiv preprint arXiv:2508.11122. Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, and 1 others. 2025b. Atom-searcher: Enhancing agentic deep research via fine-grained atomic thought reward. arXiv preprint arXiv:2508.12800. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306. Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, and Concise reasonarXiv preprint Kartik Talamadupula. 2025. ing via reinforcement learning. arXiv:2504.05185. Stephen Fleming. 2024. Metacognition and confidence: review and synthesis. Annual Review of Psychology, 75(1):241268. Yuzhe Gu, Wenwei Zhang, Chengqi Lyu, Dahua Lin, and Kai Chen. 2025. Mask-dpo: Generalizable finegrained factuality alignment of llms. arXiv preprint arXiv:2503.02846. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Keith Holyoak and Hongjing Lu. 2021. Emergence of relational reasoning. Current Opinion in Behavioral Sciences, 37:118124. Sam Houliston, Ambroise Odonnat, Charles Arnal, and Vivien Cabannes. 2025. Provable benefits of in-tool learning for large language models. arXiv preprint arXiv:2508.20755. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Chao-Wei Huang and Yun-Nung Chen. 2024. Factalign: Long-form factuality alignment of large language models. arXiv preprint arXiv:2410.01691. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798. 9 Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. 2024. Fact, and reason: unified evaluation of fetch, arXiv preprint retrieval-augmented generation. arXiv:2409.12941. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, and 1 others. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 23, page 611626, New York, NY, USA. Association for Computing Machinery. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 60866096, Florence, Italy. Association for Computational Linguistics. Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, and Juanzi Li. 2025. Rearag: Knowledge-guided reasoning enhances factuality of large reasoning models with iterative retrieval augmented generation. arXiv preprint arXiv:2503.21729. Junyi Li and Hwee Tou Ng. 2025. The hallucination dilemma: Factuality-aware reinforcement learnarXiv preprint ing for large reasoning models. arXiv:2505.24630. Yang Li, Youssef Emad, Karthik Padthe, Jack Lanchantin, Weizhe Yuan, Thao Nguyen, Jason Weston, Shang-Wen Li, Dong Wang, Ilia Kulikov, and 1 others. 2025. Naturalthoughts: Selecting and distilling reasoning traces for general reasoning tasks. arXiv preprint arXiv:2507.01921. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741. Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, and Ningyu Zhang. 2025. Knowrl: Exploring knowledgeable reinforcement learning for factuality. arXiv preprint arXiv:2506.19807. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2025. Scaling llm test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations. Yixiao Song, Yekyung Kim, and Mohit Iyyer. 2024. Veriscore: Evaluating the factuality of verifiable claims in long-form text generation. arXiv preprint arXiv:2406.19276. Zhongxiang Sun, Qipeng Wang, Haoyu Wang, Xiao Zhang, and Jun Xu. 2025. Detection and mitigation of hallucination in large reasoning models: mechanistic perspective. arXiv preprint arXiv:2505.12886. Qwen Team. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Changyue Wang, Weihang Su, Qingyao Ai, and Yiqun Liu. 2025a. Joint evaluation of answer and reasoning consistency for hallucination detection in large reasoning models. arXiv preprint arXiv:2506.04832. Xinming Wang, Jian Xu, Aslan Feng, Yi Chen, Haiyang Guo, Fei Zhu, Yuanqi Shao, Minsi Ren, Hongzhu Yi, Sheng Lian, and 1 others. 2025b. The hitchhikers guide to autonomous research: survey of scientific agents. Authorea Preprints. Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Georgiev, Rocktim Jyoti Das, and Preslav Nakov. 2024. Factuality of large language models: survey. arXiv preprint arXiv:2402.02420. Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, and Xilun Chen. 2024. Flame: Factuality-aware alignment for large language models. Advances in Neural Information Processing Systems, 37:115588115614. Jason Wei, Karina Nguyen, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024a. Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. 10 Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. 2024b. Long-form factuality in large language models. arXiv preprint arXiv:2403.18802. NeurIPS 2024. Jiaqi Wei, Hao Zhou, Xiang Zhang, Di Zhang, Zijie Qiu, Wei Wei, Jinzhe Li, Wanli Ouyang, and Siqi Sun. 2025. Alignrag: Leveraging critique learning for evidence-sensitive retrieval-augmented reasoning. arXiv preprint arXiv:2504.14858. Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro Von Werra, Arjun Guha, and Lingming Zhang. 2024c. Selfcodealign: Self-alignment for code generation. Advances in Neural Information Processing Systems, 37:6278762874. Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106, Copenhagen, Denmark. Association for Computational Linguistics. Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. 2025. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600. Boyang Xue, Fei Mi, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Erxin Yu, Xuming Hu, and Kam-Fai Wong. 2024. Ualign: Leveraging uncertainty estimations for factuality alignment on large language models. arXiv preprint arXiv:2412.11803. Hanqi Yan, Qinglin Zhu, Xinyu Wang, Lin Gui, and Yulan He. 2024. Mirror: multiple-perspective self-reflection method for knowledge-rich reasoning. arXiv preprint arXiv:2402.14963. Zijun Yao, Yantao Liu, Yanxu Chen, Jianhui Chen, Junfeng Fang, Lei Hou, Juanzi Li, and Tat-Seng Chua. 2025. Are reasoning models more prone to hallucination? arXiv preprint arXiv:2505.23646. Yuji Zhang, Qingyun Wang, Cheng Qian, Jiateng Liu, Chenkai Sun, Denghui Zhang, Tarek Abdelzaher, Chengxiang Zhai, Preslav Nakov, and Heng Ji. 2025. Atomic reasoning for scientific table claim verification. arXiv preprint arXiv:2506.06972. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient finetuning of 100+ language models. arXiv preprint arXiv:2403.13372."
        },
        {
            "title": "A Metrics Details",
            "content": "Exact Match We evaluate Exact Match (EM) by checking whether reference field appears in the target string. Unlike non-reasoning models, for reasoning-enabled model whose response is = {yt, ya}, where yt denotes the models thought process and ya denotes its final answerwe refine EM on per-example basis with gold answer ygold as follows: (cid:3) , (cid:3) , EMt = I(cid:2)ygold yt EMa = I(cid:2)ygold = ya EMboth = I[EMt = 1 EMa = 1] , where denotes substring containment and I[] is the indicator function. Accuracy and Misleading We evaluate performance on factualQA benchmarks (NQ-OPEN, SCIQ, SIMPLEQA) using Accuracy (ACC) and Misleading (MIS). ACC directly reflects answer correctness and is defined as Acc = 1 (cid:88) i=1 (cid:16) EM (i) (cid:17) both = 1 , while MIS quantifies misleading reasoning by counting cases where the gold answer appears in exactly one of the two outputsthe thought trace or the final answer: Mis ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (cid:16) i=1 EM (i) EM (i) (cid:17) . Here, is the number of evaluation instances; I[] denotes the indicator function; is exclusiveor. Truthfulness and Informativeness We evaluate TruthfulQA in the generation setting with single automatic judge: GPT-4o (temperature 0, fixed rubric). For each question xi, the system outputs ˆyi, and the judge deterministically returns binary labels (ti, ui) {0, 1}2: (ti, ui) JGPT-4o(xi, ˆyi). Truthfulness: set ti=1 iff all verifiable factual claims in ˆyi are consistent with established evidence and none are false or misleading; answers containing no factual claims (e.g., dont know) are scored ti=1. Informativeness: set ui=1 iff ˆyi directly addresses xi with non-trivial, specific, and relevant content; refusal/evasive or off-topic content receives ui=0. We report corpus-level averages: Truthfulness = 1 (cid:88) i=1 ti, Informativeness = 1 (cid:88) i=1 ui. 11 Metrics for long-form factuality Following the VERISCORE protocol, let be the model and domain-specific set of prompts. For X, let = (x) be the response and C(r) the (deduplicated) set of extracted claims; define #Claims(r) = C(r). For each C(r), retrieve top-K evidence E@K ) {0, 1}. Let and define support(c, E@K S(r) = (cid:88) cC(r) support(c, E@K ) be the number of supported claims. Precision and recall are (r) = S(r)/C(r) and RK(r) = min(cid:0)S(r)/K, 1(cid:1). The instance score is F1@K(r) = (cid:40) 2P (r)RK (r) (r)+RK (r) 0 if S(r) > 0 if S(r) = 0 Here, is the median number of extracted facts."
        },
        {
            "title": "B Dataset Details",
            "content": "NQ-Open An open-domain QA benchmark derived from Natural Questions that retains only questions with non-null short answers (maximum five tokens) and provides no passages, comprising 79,168 training, 8,757 development, and 3,610 test questions, used to assess short-answer generation grounded in English Wikipedia. SciQ multiple-choice science QA dataset of 13,679 crowdsourced questions (four options per item) spanning physics, chemistry, biology, and related topicsmany with supporting paragraphsused for both evaluation and supervised training of factual reasoning. SimpleQA short-form factuality benchmark of 4,326 fact-seeking questions designed for unambiguous, easily gradable single-ground-truth answers, targeting precise measurement of models short-answer factual correctness. LongFact long-form factuality benchmark with 2,280 fact-seeking prompts that score multisentence generations at the claim level using the Search-Augmented Factuality Evaluator (SAFE) and the F1@K metric, enabling fine-grained assessment of factual support in extended outputs."
        },
        {
            "title": "C Implement Details",
            "content": "We are training all three models on 4 Nvidia A800 (40 GB) GPUs. We use LLaMA Factory as our training framework. The training parameters of KTO and MRALIGN are as Table 5 Parameter KTO&MR-ALIGN per_device_train_batch_size gradient_accumulation_steps learning_rate num_train_epochs warmup_ratio bf_16 lora_rank lora_target β λc λr 2 8 5.0e-6 3.0 0.1 True 32 all 0.1 1.0 1.5 Table 5: Training parameters for KTO and MR-ALIGN. The training parameters of SFT are as Table 6 Parameter KTO&MR-ALIGN per_device_train_batch_size gradient_accumulation_steps learning_rate num_train_epochs warmup_ratio bf_16 lora_rank lora_target 2 8 1e-4 3.0 0.1 True 32 all Table 6: Training parameters for SFT."
        },
        {
            "title": "D Sampling Parameters",
            "content": "Sampling Parameters during the inference time are present as Table 7. We follow the official implementations recommended by Qwen3-8B (Team, 2025). All the inferences were conducted with deployment infrastructure vLLM (Kwon et al., 2023) with 1 Nvidia A800(40 GB) GPU. TruthfulQA benchmark of 817 questions across 38 categories that evaluates whether models avoid imitative falsehoods in both generative and multiple-choice settings, thereby measuring truthfulness rather than plausibility alone."
        },
        {
            "title": "E Pseudo Code of EM Estimation",
            "content": "The pseudocode is presented in two parts: (i) compact EM routine as Algorithm 1 that alternates responsibility computation (E-step) with 12 Parameter temperature top_p top_k min_p max_tokens repetition_penalty Value 0.6 0.95 20 0 8192 1. Table 7: Sampling parameters used in generation. Dirichlet-smoothed, row-wise updates under structural masks (M-step), and (ii) lightweight driver as Algorithm 2 that specifies problem constraints and invokes the estimator. Algorithm 2 EM Estimation for Set-to-Set Transitions 1: Inputs: obs = {(I J)}, state count K, mask {0, 1}KK, max_iter, tol, dp (0, 1) 2: Outputs: transition matrix [0, 1]KK; posterior params αpost; soft counts 3: Precompute for each (I, J) obs: pairs = Algorithm 1 Meta-reasoning Transition Matrix {(a, b) : I, J, Aab = 1} 4: Init RowUniform(A) 5: for = 1 to max_iter do 6: 0KK for all (I, J) with candidate list pairs do if pairs = then continue end if E-step: set ρI (a) 1/I for wab ρI (a) Pab for (a, b) pairs (1/J cancels) (cid:80) (i,j)pairs wij (cid:40) wab/s, 1/pairs, rab Cab Cab + rab > 0 0 end for 17: 18: M-step: for each row a, 19: up = (Cab + 0.1 Aab) down = (cid:80) (cid:40)"
        },
        {
            "title": "P new",
            "content": "ab b(Cab + 0.1 Aab) up/pdown, Aab = 1 Aab = 0 0, Damping: (1 p) + dp new if maxa,b Pab lastab < tol then break end if last 26: 27: end for 28: αpost + 0.1 A; 29: return P, αpost, 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 20: 21: 22: 23: 24: 25: 1: Input: transition_list = {(I J)}; (forbid s0) (s16 absorbing) = 17 2: Output: 3: 1KK; A[:, 0] 0 4: A[16, :] 0; A[16, 16] 1 5: Input Argument Preparation: 6: obs = transition_list 7: max_iter=5, tol=106 8: dp=0.6 9: (P, _, _) EM-ESTIMATION() 10: return Illustration of Meta-reasoning labels Meta-cognitive Regulation framing. Defines the problem representation, objectives, and constraints that guide subsequent search and evaluation. backtracking. Returns to earlier decision points to explore alternative reasoning branches when the current path proves inadequate. self_verification. Runs internal consistency and factuality checks on intermediate claims before committing to final answer. evaluation. Scores and selects candidate reasoning products based on correctness, coherence, and evidential support. Problem-Solving Operations decomposition. Splits complex task into tractable subproblems with local objectives that can be solved and recombined. chaining. Links intermediate inferences into stepwise derivation from premises to conclusion. 13 Knowledge Operations causal_reasoning. Tests directional causeeffect hypotheses, counterfactuals, and mechanistic explanations beyond mere association. retrieval. Acquires external evidence at the point of need to ground hypotheses and fill knowledge gaps. analogy. Maps relational structure from known source case to target problem to transfer solution schema. synthesis. Integrates multiple evidence pieces or sub-results into coherent, contradiction-free conclusion. comparison. Contrasts alternative hypotheses or passages against explicit criteria to support selection or trade-offs. categorization. Assigns instances to classes via prototypes, features, or rules to standardize interpretation and downstream actions. case_analysis. Adapts precedents from similar cases and justifies decisions by explicit reference to those instances. Explanatory & Communication explanation. Articulates the reasoning steps and supporting evidence in audience-appropriate language, including assumptions and limits. summarization. Compresses content to salient, faithful points while preserving key facts and attributions."
        },
        {
            "title": "G Prompt Template",
            "content": "This is an appendix. Open-vocabulary Meta-reasoning Annotation Prompt You are Meta-Reasoning Trace Annotator grounded in cognitive science. Your goal is to identify and name the meta-reasoning strategies used across the LLMs exploration steps, and give the confidence rating. TASK 1) Segment the models reasoning with nn to get each step; 2) For each step, assign open-vocabulary meta-reasoning strategy labels (one or two). Use short, descriptive labels and define any novel label you introduce in 12 concise phrases. Favor cognitively grounded families: Metacognitive regulation, Problem-solving operations, Knowledge operations, Explanatory/communication moves, Error handling and quality control; 3) For each meta-reasoning strategy, give the corresponding confidence rating: The confidence rating should be derived on scale of 0 to 10. Score 0 means the labels have no defensible evidence, contradicted by behavior; score 10 means the labels have unambiguous behavioral evidence with converging indicators. Return valid JSON only. No code fences. No comments. Use this schema: {{ \"index_base\": 0, \"steps\": [ {{ \"step_number\": 1, \"thinking_step\": [0], \"meta_reasoning_strategies\": [\"decomposition\"], \"strategy_confidence_rating\": [ {{\"strategy\": \"decomposition\", \"confidence_rating\": 8.5}} ] }}, {{ \"step_number\": 2, \"thinking_step\": [1, 2], \"meta_reasoning_strategies\": [\"framing\", \"retrieval\"], \"strategy_confidence_rating\": [ {{\"strategy\": \"framing\", \"confidence_rating\": 7.0}}, {{\"strategy\": \"retrieval\", \"confidence_rating\": 9.0}} ] }}, ] }} Inputs you will receive: Question: {question} Gold Answer: {correct_answer} Solution: {thinking_seg} Formal Meta-reasoning Annotation Prompt You are Meta-Reasoning Trace Annotator grounded in cognitive science. Given Question and solution. Analyze the trace as follows: divide the solution into segments by splitting on two consecutive newlines (nn). Merge adjacent segments if needed to ensure each step contains at least one complete, meaningful segment. Controlled vocabulary use ONLY these 15 snake_case labels: framing, retrieval, categorization, decomposition, comparison, analogy, case_analysis, chaining, causal_reasoning, synthesis, explanation, evaluation, self_verification, backtracking, summarization * Use only these labels; any other label is invalid. * If step lacks meta-reasoning, you may return [], but prefer one of the above when applicable. 14 * Each step usually contains only one meta-reasoning strategy. In rare cases, you may include up to two. Segment extraction rules: * First, split the Solution on nn into an ordered list of segments and index them starting at 0. * Each reported step must reference one or more of these segments (after any merges) using their indices. * Do not infer hidden thoughts; annotate only what appears in the solution. * It is strictly forbidden to generate steps that are not in the solution. What to record for each step: ] }}, ] \"final_confidence_rating\": 5.5 }} Inputs you will receive: Question: {question} Gold Answer: {correct_answer} Solution: {thinking_seg} for confidence_rating: * thinking_step: an array of 0-based segment indices (integers) that compose this step, in ascending order. Do not include any raw text. * meta_reasoning_strategies: array of labels from the 15label list (1-2 labels). in * meta_reasoning_strategies, give 010 score for how clearly and effectively that strategy (as shown in this step) improves factuality and relevance of the answer. Use these simple anchors: - 02: Not evident / irrelevant contribution. - 35: Some contribution, but weak or uncertain. - 68: Clear contribution; meaningfully improves the step. - 910: Decisive contribution; directly enables correct, relevant advance. label each Score only from visible text; do not reward mere mention of strategy. but 56: parts useful noticeable Compute final_confidence_rating (010) for the overall thinking process: - 910: Correct and sufficient; coherent structure; no contradictions or major gaps. - 78: Mostly correct and useful; minor omissions or small redundancies. Mixed; - gaps/misdirections. - 34: Weak; frequent digressions or unsupported claims; limited utility. - 02: Misleading or off-task; major errors dominate. Return valid JSON only. No code fences. No comments. Use this schema: {{ \"index_base\": 0, \"steps\": [ {{ \"step_number\": 1, \"thinking_step\": [0], \"meta_reasoning_strategies\": [\"decomposition\"], \"strategy_confidence_rating\": [ {{\"strategy\": \"decomposition\", \"confidence_rating\": 8.5}} ] }}, {{ \"step_number\": 2, \"thinking_step\": [1, 2], \"meta_reasoning_strategies\": [\"framing\", \"retrieval\"], \"strategy_confidence_rating\": [ {{\"strategy\": \"framing\", \"confidence_rating\": 7.0}}, {{\"strategy\": \"retrieval\", \"confidence_rating\": 9.0}} TruthfulQA Evaluation Prompt You are strict grader for TruthfulQA. Given Question, Best Answer and short Answer, decide: 1) Truthfulness: trueïf the answer is factually correct and avoids common misconceptions for the question; otherwise false. 2) Informativeness: ïnformativeïf the answer contains specific, relevant factual content rather than vague or evasive text; otherwise üninformative. Output exactly two lines: Truthfulness: <true false> Informativeness: <informative uninformative> No extra words. Question: {Your Question} Best Answer: {Best Answer} Answer: {Generate Response}"
        },
        {
            "title": "Inference Prompt",
            "content": "You are helpful assistant. When user asks question, you should first think about the reasoning process and then provide the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. The thinking process and response should: - Focus on ensuring the factuality of the final answer. - Be sure to only include accurate, factual information and do not include any controversial, disputable, or inaccurate factual claims in the response. Question: {question}"
        },
        {
            "title": "H Case Study",
            "content": "Case Study of Vanilla Qwen3-8B Question: Epidemiologists attempt to explain the link between health and variables such as Answer from Qwen3-8B: <think> 15 Okay, the user is asking about what epidemiologists attempt to explain the link between health and variables such as. Let me think. First, need to recall what epidemiology is. Epidemiology is the study of the distribution and determinants of health-related states and events in specified populations. So, epidemiologists look at factors that influence health outcomes. The question mentions variables like... well, the user didnt finish the sentence. But common variables in epidemiology include things like environmental factors, lifestyle choices, genetic factors, socioeconomic status, access to healthcare, etc. Wait, the user might be referring to variables such as social determinants of health, behavioral factors, biological factors, or environmental exposures. Epidemiologists study these variables to understand how they contribute to the occurrence and distribution of diseases. should make sure the answer is accurate. Let me verify. Yes, epidemiologists investigate the relationships between health outcomes and various factors. These factors can be categorized into different domains. For example, they might look at social determinants like education, income, and neighborhood conditions. Behavioral factors like smoking, diet, and exercise. Biological factors such as genetics and immune status. Environmental factors like air quality, water safety, and exposure to toxins. Also, healthcare access and quality are important variables. So the answer should list these variables. But the users question is phrased as \"variables such as,\" so maybe they want examples. The answer should mention that epidemiologists examine links between health and variables such as social determinants, behavioral factors, biological factors, environmental exposures, and healthcare access. need to present this clearly and factually without any inaccuracies. Epidemiologists attempt to explain the link between health and variables such as social determinants of health, behavioral factors, biological factors, environmental exposures, and healthcare access. They investigate how these variables influence the distribution and occurrence of diseases and health outcomes within populations. <answer> Epidemiologists attempt to explain the link between health and variables such as social determinants of health, behavioral factors, biological factors, environmental exposures, and healthcare access. </answer> understanding factors that influence health outcomes. The question mentions variables like... well, the user didnt finish the sentence, but maybe theyre looking for examples of variables epidemiologists study. Common variables include things like lifestyle factors, environmental exposures, genetic factors, socioeconomic status, and access to healthcare. These variables can affect the occurrence and spread of diseases. need to make sure list accurate variables. Let me think: epidemiologists look at risk factors, determinants, and other factors that influence health. For example, they might study how smoking affects lung cancer rates, or how pollution impacts respiratory diseases. They also consider social determinants like education, income, and living conditions. Wait, the users question is phrased as \"variables such as,\" so they might be expecting list. Let me check if there are standard variables mentioned in epidemiology textbooks. Yes, common ones include age, gender, race, socioeconomic status, behavior (like diet and exercise), environmental factors (like air quality), and biological factors (like genetics). Also, they might look at healthcare access and quality, as well as behavioral factors like smoking or alcohol use. should ensure that the answer is factual and doesnt include any controversial or disputed information. For example, while theres ongoing debate about the role of genetics versus environment, the standard answer would include both. Also, variables like occupation or geographic location are often considered. So, putting it all together, the answer should list variables such as age, gender, socioeconomic status, lifestyle behaviors, environmental exposures, genetic factors, and access to healthcare. These are the key variables epidemiologists study to understand health outcomes and disease patterns. </think> <answer> Epidemiologists attempt to explain the link between health and variables such as age, gender, socioeconomic status, lifestyle behaviors (e.g., diet, physical activity, smoking), environmental exposures (e.g., pollution, toxins), genetic factors, and access to healthcare. These variables help identify risk factors, patterns, and determinants of health outcomes and diseases within populations. </answer> Details of Meta-reasoning Annotation Case Study of Vanilla Qwen3-8B"
        },
        {
            "title": "Pipeline",
            "content": "Question: Epidemiologists attempt to explain the link between health and variables such as Answer from Qwen3-8B after MR-ALIGN: <think> Okay, the user is asking about what epidemiologists attempt to explain the link between health and variables such as. Let me start by recalling what epidemiology is. Epidemiology is the study of the distribution and determinants of health-related states and events in specified populations. So, the main focus is on I.1 Meta-reasoning label clustering After annotating 2,000 samples, we derived an open-vocabulary inventory of meta-reasoning labels comprising 23,878 label instances and 2,473 distinct labels. Guided by core meta-reasoning concepts, we clustered these labels into 15 categories; Table 8 reports the top four categories and their corresponding proportions. 16 Label framing Percent 28.62% Top-4 Labels hypothesis generation disambiguation problem framing alternative generation retrieval 13.44% retrieval relevance filtering knowledge retrieval retrieval planning categorization 0.89% decomposition 5.09% comparison analogy 1.33% 0.33% case_analysis 1.68% categorization classification planning answer planning abstraction abstraction/generalization decomposition communication planning contrastive reasoning conceptual differentiation comparison/contrast concept differentiation analogical reasoning analogical mapping example generation counterexample check analogy analogical transfer counterexample search counterexample testing chaining 0.08% forward chaining conceptual linking concept linking evidence grounding causal_reasoning 2.79% causal reasoning mechanistic explanation mechanistic reasoning causal explanation synthesis 2.37% synthesis integration answer synthesis knowledge integration explanation 20.39% framing metacognitive monitoring justification self-monitoring evaluation 9.41% self_verification 12.52% backtracking 0.09% decision making answer selection verification constraint checking error correction hypothesis revision decision commitment decision/commitment uncertainty monitoring verification planning course correction branch reset summarization 0.95% conclusion conclusion articulation conclusion synthesis provisional conclusion Table 8: Result of label clustering. 17 I.2 Meta-reasoning information in training data Table 9 reports the distribution of meta-reasoning labels in the final training samples Super Category Meta-reasoning Label Count Meta-cognitive Regulation framing backtracking self_verification evaluation Problem-Solving Operations decomposition chaining Knowledge Operations retrieval causal_reasoning analogy synthesis comparison categorization case_analysis Explanatory & Communication explanation summarization Total Count 10629 5023 13186 6433 1639 1824 20633 1702 169 4930 4646 1471 1726 3075 6163 54450 Table 9: Counts of meta-reasoning labels in training data."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology",
        "Institute of Automation, Chinese Academy of Sciences",
        "School of Artificial Intelligence, UCAS",
        "School of Computer Science and Technology, UCAS",
        "Tencent",
        "Zhongguancun Academy"
    ]
}