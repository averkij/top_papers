{
    "paper_title": "Towards Data-Efficient Pretraining for Atomic Property Prediction",
    "authors": [
        "Yasir Ghunaim",
        "Hasan Abed Al Kader Hammoud",
        "Bernard Ghanem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on a carefully selected, task-relevant dataset can match or even surpass large-scale pretraining, while using as little as 1/24th of the computational cost. We introduce the Chemical Similarity Index (CSI), a novel metric inspired by computer vision's Fr\\'echet Inception Distance, for molecular graphs which quantifies the alignment between upstream pretraining datasets and downstream tasks. By selecting the most relevant dataset with minimal CSI distance, we show that models pretrained on a smaller, focused dataset consistently outperform those pretrained on massive, mixed datasets such as JMP, even when those larger datasets include the relevant dataset. Counterintuitively, we also find that indiscriminately adding more data can degrade model performance when the additional data poorly aligns with the task at hand. Our findings highlight that quality often outperforms quantity in pretraining for atomic property prediction."
        },
        {
            "title": "Start",
            "content": "Towards Data-Efficient Pretraining for Atomic Property Prediction Yasir Ghunaim 1 Hasan Abed Al Kader Hammoud 1 Bernard Ghanem 1 5 2 0 2 6 1 ] . [ 1 5 8 0 1 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper challenges the recent paradigm in atomic property prediction that links progress to growing dataset sizes and computational resources. We show that pretraining on carefully selected, task-relevant dataset can match or even surpass large-scale pretraining, while using as little as 1/24th of the computational cost. We introduce the Chemical Similarity Index (CSI), novel metric inspired by computer visions Frechet Inception Distance, for molecular graphs which quantifies the alignment between upstream pretraining datasets and downstream tasks. By selecting the most relevant dataset with minimal CSI distance, we show that models pretrained on smaller, focused dataset consistently outperform those pretrained on massive, mixed datasets such as JMP, even when those larger datasets include the relevant dataset. Counterintuitively, we also find that indiscriminately adding more data can degrade model performance when the additional data poorly aligns with the task at hand. Our findings highlight that quality often outperforms quantity in pretraining for atomic property prediction. 1. Introduction Machine learning is transforming molecular modeling, driving advancements in accurate predictions and simulations of molecular behavior (Chanussot et al., 2021; Tran et al., 2023; Liao et al., 2023). These breakthroughs directly impact the acceleration of progress in crucial fields such as drug discovery (Huang et al., 2021) and global climate change mitigation (Sriram et al., 2024). The improvements in this field have been primarily attributed to innovations in model architectures (Liao et al., 2023; Gasteiger et al., 2021; Passaro & Zitnick, 2023) and the growing availability of large-scale molecular datasets. In recent years, the sizes of molecular datasets have increased dramatically - from tens of thou1King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia. Correspondence to: Yasir Ghunaim <yasir.ghunaim@kaust.edu.sa>. Code: github.com/Yasir-Ghunaim/efficient-atom Figure 1. Pretraining on High-Quality, Task-Relevant Dataset. Pretraining on carefully selected high-quality dataset achieves comparable or superior mean absolute error (MAE) across tasks while reducing computational cost by factor of 24 compared to JMP-S, which is pretrained on all upstream datasets. Lower MAE indicates better performance. sands of examples (Christensen & Von Lilienfeld, 2020; Chmiela et al., 2023; Wu et al., 2018) to hundreds of millions (Chanussot et al., 2021; Tran et al., 2023). This rapid growth in scale has also caused surge in the computational resources required for pretraining, increasing from few days on single GPU to over thousand GPU-days (Shoghi et al., 2023; Liao et al., 2023). This trend begs the question: (cid:17) Is scaling data and resources the only path forward in atomic property prediction, or can intelligent data selection achieve similar performance more efficiently? While data selection strategies for pretraining have been explored in fields like natural language processing (Penedo et al., 2024) and computer vision (Hammoud et al., 2024; Li et al., 2023), this area remains largely underexplored in atomic property prediction, where unique challenges arise. In his paper, we challenge the prevailing assumption that bigger is better by exploring whether smaller, strategically selected dataset can lead to comparable or even superior performance while substantially reducing computational demands. We introduce pretraining paradigm that shifts the focus from data and compute scaling to selecting the most relevant upstream dataset for improved downstream performance. Through simple baseline, our experiments reveal two key insights: Towards Data-Efficient Pretraining for Atomic Property Prediction (1) Competitive Performance Can Be Achieved with 24 Fewer Resources: Selecting upstream datasets based on their relevance to the downstream task achieves performance on par with or exceeding that of large-scale pretrained models like JMP (Shoghi et al., 2023) while utilizing only 1/24th of the computational resources, as shown in Figure 1. (2) Quality Outperforms Quantity: Expanding the pretraining dataset by incorporating additional data from less relevant sources can negatively impact downstream performance rather than enhance it. explore the potential of dataset selection for pretraining in atomic property prediction, we introduce the Chemical Similarity Index (CSI), simple metric inspired by the Frechet Inception Distance (FID) from computer vision. CSI measures the alignment between an upstream dataset and downstream task, enabling the selection of chemically relevant pretraining data. By focusing on these highly relevant datasets, we significantly reduce computational costs while maintaining competitive performance and, in many cases, achieving improvements. While large-scale datasets like OC20 (Chanussot et al., 2021; Tran et al., 2023) and mixed datasets like JMP (Shoghi et al., 2023) are popular choices for pretraining in molecular domains (Kolluru et al., 2022; Shoghi et al., 2023), our findings challenge their universal utility. Surprisingly, pretraining on single, carefully selected dataset guided by CSI often outperforms models trained on mixtures, even when those include the most relevant dataset. The contributions of this paper are threefold: (1) We introduce novel framework for computationally efficient pretraining of molecular machine learning models, demonstrating that strategic data selection can match or outperform models trained on much larger datasets. (2) We propose the Chemical Similarity Index (CSI), metric for assessing the similarity between upstream and downstream molecular datasets, enabling effective dataset selection. (3) We provide an extensive empirical evaluation demonstrating the effectiveness of our approach, offering practical and efficient alternative to the current trend of ever-increasing data and computational costs in molecular machine learning. 2. Related Work Pretraining for Atomic Property Prediction. Inspired by the success of pretraining in computer vision and natural language processing, pretraining for atomic property prediction has gained significant attention in recent years. Most approaches in molecular machine learning focus on selfsupervised learning (Liu et al., 2021; Jiao et al., 2023; Chen et al., 2021; Kolluru et al., 2022; Zhou et al., 2022a; Ji et al., 2024), as generating labels for molecular datasets is computationally expensive. In contrast, fewer studies explore the effectiveness of supervised transfer learning (Smith et al., 2019; 2018; Kolluru et al., 2022). However, in both selfsupervised and supervised settings, the focus has primarily been on improving feature representation, often overlooking the impact of pretraining dataset relevance on downstream performance. Recently, JMP (Shoghi et al., 2023) introduced multi-domain pretraining, enabling joint pretraining on various upstream sources concurrently. While effective, JMP pretraining requires enormous computational resources to reproduce and does not reveal how each upstream source impacts downstream performance. Our work addresses this gap by systematically studying the relationship between upstream pretraining datasets and downstream performance, enabling researchers to develop effective pretraining models even with limited computational resources. Computational Budgeting. Recent research highlights the importance of studying model performance under computationally budgeted setups. In continual learning (CL), works by Prabhu et al. (2023) and Ghunaim et al. (2023) show that simple baselines often outperform state-of-the-art methods in compute-constrained settings. TiC-CLIP (Garg et al., 2024) further demonstrates efficient rehearsal-based training for time-continuous data. For Vision Transformers, Pan et al. (2022) propose framework to dynamically control model complexity during training, achieving competitive performance under varying budgets. Li et al. (2019) formalize budgeted training, showing that budget-aware learning rate schedules, such as linear decay, are critical for robust performance across tasks like image classification and object detection. In multi-domain learning, Berriel et al. (2019) introduce Budget-Aware Adapters, which reduce computational complexity while maintaining accuracy by selecting relevant feature channels. These findings across domains emphasize the critical need for more efficient approaches that can achieve competitive performance while minimizing computational costs. Data Selection. Efficient training through data selection has been explored via two primary approaches: subset selection and dataset distillation. Subset selection aims to identify representative subset of the training data that matches or even outperforms training on the full dataset. Several methods have been proposed for vision and NLP tasks (Attendu & Corbeil, 2023; Killamsetty et al., 2021a;b; Kaushal et al., 2019; Bairi et al., 2015; Lapedriza et al., 2013). Dataset distillation, introduced by Wang et al. (Wang et al., 2018), focuses on generating smaller, synthetic subset of the dataset that preserves performance while reducing training time and storage requirements. Subsequent work has explored techniques such as meta-learning (Zhou et al., 2022b; Nguyen et al., 2021a;b), gradient matching (Zhao et al., 2021), and distribution matching (Zhao & Bilen, 2023). While most research in distillation has focused on vision tasks, few studies have extended it to graph data (Jin et al., 2022b; Liu et al., 2022; Jin et al., 2022a), though primarily targeting 2 Towards Data-Efficient Pretraining for Atomic Property Prediction Figure 2. Pipeline Overview. Our paradigm for pretraining and finetuning consists of two new components: (1) Dataset Selection Stage, where distance metric δ is employed to identify the dataset that is most similar to our downstream task dataset Dd, in this case D(1) . This selected dataset is then used for pretraining the model. (2) Limited Budget Pretraining, where we impose training budget by subsampling random samples from D(1) and training the model for epochs. This results in computational budget of = . The pretrained backbone θ(1) is subsequently finetuned on the downstream task dataset Dd to obtain the final model parameters θ d. knowledge and social graphs rather than molecular graphs. Two recent vision studies are particularly relevant to our work. First, Hammoud et al. (2024) shows that increasing pretraining data diversity enhances performance only when distribution shifts between upstream and downstream tasks are minimized. Second, Li et al. (2023) introduces method to dynamically leverage the open web, reducing the distribution gap between upstream and downstream tasks through targeted representation learning. Findings from other domains suggest that aligning upstream datasets may be crucial for effective pretraining. Comparison to Our Work. To the best of our knowledge, no prior work has specifically explored upstream dataset selection for molecular graphs, which present unique challenges due to their structural and chemical complexity. In this work, we take the first step in addressing this gap by focusing on aligning upstream and downstream distributions at the dataset level rather than subselecting at sample-wise level or creating synthetic distilled version of the dataset. 3. Formulation and Setup In this section, we present our problem setup, notion of computational budget, and the formulation of dataset similarity. We then detail how we adapt the Frechet Inception Distance (FID) to the molecular domain, yielding the Chemical Similarity Index (CSI). Our setup is illustrated in Figure 2. Throughout this work, we use the term molecular broadly to encompass both molecular and materials domains, as well as their respective datasets. 3.1. Formal Setting and Downstream , . . . , D(K) Datasets. Let Upstream , D(2) {D(1) } denote collection of upstream (pretraining) datasets, each containing molecular structures paired with relevant atomic properties (e.g., In the typical paradigm, the user energies and forces). aggregates all upstream datasets into one dataset for pretraining: (cid:91) D(i) . Du = i=1 We further define Dd as the downstream dataset, which focuses on specific prediction task (e.g., predicting an atomic property). Multi-task Pretraining. We consider neural network Φ(; θ), where θ encompasses the shared backbone parameters θb and task-specific head parameters θe (for energy prediction) and θf (for force prediction). During pretraining, the network is trained to simultaneously predict energies and forces. Formally, the multi-task pretraining objective over an upstream dataset D(i) θ(i) = arg min is given by: Lpretrain(θ; D(i) (1) ), θ where θ = {θb, θe, θf } and Lpretrain(θ; D(i) ) = α Lenergy(θb, θe; D(i) ) + β Lforces(θb, θf ; D(i) ). (2) Often, Lenergy is computed using the Mean Absolute Error (MAE), while Lforces uses the mean per-atom Euclidean (L2) distance. Coefficients α and β weight the importance of energy and force tasks, respectively, with β > α often chosen to prioritize accurate force predictions in atomistic modeling. Usually, pretraining is performed on the joint upstream dataset Du or subsampled version. Fine-Tuning. After the multi-task pretraining phase, the task-specific heads θe and θf are discarded, and new taskspecific head θh is attached to the pretrained backbone θb. The downstream objective then becomes: θ = arg min θb,θh Lfinetune(θb, θh; θ(i) , Dd), (3) where θ(i) denotes the pretrained backbone parameters from Eq. (1). Intuitively, the downstream training refines the 3 Towards Data-Efficient Pretraining for Atomic Property Prediction shared backbone parameters θb and learns the task-specific head θh to capture the target property in Dd. In this paper, we consider two additional needed definitions for this setting: (1) computational budget and (2) similarity metrics. Computational Budget. Following Hammoud et al. (2024), we define the computational budget to be the product of the number of epochs and the number of unique samples in the pretraining dataset: = . (4) Hence, the computational budget represents the total number of samples processed over training. It naturally splits into two factors: the dataset size (N ) and the number of passes through it (E). The choice of depends on the available computing resources. In our experiments, we explore multiple pretraining budget settings to examine how trading off against under fixed affects both pretraining and downstream performance. Dataset Similarity. key objective of this work is to estimate how well an upstream dataset Du aligns with downstream dataset Dd. We therefore seek distance metric δ(Du, Dd) that quantifies their relevance or similarity. In principle, smaller value δ(Du, Dd) indicates greater overlap or relevance. Thus, among multiple candidate upstream datasets {D(1) }, the one that minimizes , . . . , D(K) argmin 1iK δ(cid:0)D(i) , Dd (cid:1) should provide the most effective pretraining for Dd. In this paper, we empirically test this assumption, examining whether lower δ-values indeed correlate with improved downstream performance. This motivates using δ as principled metric to select or combine upstream datasets under fixed computational budget. 3.2. The Chemical Similarity Index (CSI) Recap of FID. Our proposed Chemical Similarity Index (CSI) draws its inspiration from the well-known Frechet Inception Distance (FID) (Heusel et al., 2017). Recall that FID is commonly used in computer vision to compare two sets of images via their feature distributions. Specifically, if one extracts features (e.g., from an Inception network) for datasets and and denotes their empirical means and covariances by µX , ΣX and µY , ΣY , then FID(X, ) = µX µY 2 + Tr (cid:16) ΣX +ΣY 2(ΣX ΣY )1/2(cid:17) . (5) 4 The central idea is to represent each sample in feature space where distances encode semantic similarity and then compare the distributions of these representations for the two datasets. Design Considerations for CSI. Adapting FID to the molecular domain involves several key design choices regarding how to best modify the metric: 1. Feature Type: The original FID metric for images uses features from the last layer before the head, relying on single feature type. In contrast, molecular graph-based models often use two types of features: node and edge features. Models like GemNet-OC (Gasteiger et al., 2022) retain distinct node and edge features before the head, while transformer-based models like EquiformerV2 (Liao et al., 2023) integrate edge information into unified node embeddings. To ensure compatibility across backbones, we focus on node embeddings at the last backbone layer, as edge information is either explicitly maintained (e.g., GemNetOC) or indirectly encoded into node representations (e.g., EquiformerV2). 2. Aggregation Strategy: FID in computer vision operates on fixed-size feature vectors, while molecular graphs have variable-sized embeddings. To adapt FID, we consider two strategies: flattening, which concatenates all node features into single vector, and mean pooling, which averages node features for graph-level representation. While mean pooling aligns with the original FID approach, it may oversmooth molecular features. In models like EquiformerV2 (Liao et al., 2023) and GemNetOC (Gasteiger et al., 2022), backbone node features are designed for further processing in the models head before pooling. Since we extract features directly from the backbone, mean pooling at this stage risks losing feature diversity. Thus, we choose flattening to preserve the full feature distribution. CSI values using mean pooling are shown in the appendix. 3. Sampling in Long-Tail Distributions: To make feature extraction computationally feasible, we subsample representative subset from each upstream and downstream dataset. However, molecular datasets often exhibit long-tail distributions, where common structures are overrepresented and rare ones underrepresented. These imbalances can skew mean and covariance estimates for our metric. For example, specific molecules may appear much more frequently than others, and in catalytic datasets, certain slabadsorbate configurations can dominate (Chanussot et al., 2021). To address this, we use class-balancing sampling scheme to ensure balanced representation of structures. Plots for long-tail distribution are in the appendix. Overall, our CSI metric leverages node embeddings with flattening for aggregation and uses class-balanced samTowards Data-Efficient Pretraining for Atomic Property Prediction Figure 3. Alignment Between Upstream and Downstream Using CSI. We assess how well the extracted representations from each upstream dataset align with downstream tasks using our CSI metric, where lower values indicate stronger alignment. ANI-1x demonstrates the closest feature alignment with downstream tasks, whereas OC20 and OC22 show the weakest alignment. pling to address distribution imbalances. We subsample 10k instances from each upstream and downstream dataset for feature extraction to maintain computational feasibility. To ensure our metric remains independent of the baselines in this study, we extract features for our metric using EquiformerV2 (Liao et al., 2023) pretrained on OC20 (Chanussot et al., 2021). For completeness, CSI values computed using the baseline JMP (Shoghi et al., 2023) are included in the appendix. we perform pretraining on upstream datasets of small molecules, including ANI-1x (Smith et al., 2020) and Transition-1x (Schreiner et al., 2022), as well as largescale catalysis datasets, OC20 (Chanussot et al., 2021) and OC22 (Tran et al., 2023). These datasets vary in domain focus and graph size, enabling us to examine how these factors impact the generalization of pretraining across downstream tasks. The ground-truth labels, energy and forces, are computed using Density Functional Theory (DFT). CSI Between Upstream and Downstream Results. In Figure 3, we present the CSI values for pairs of upstream and downstream tasks related to energy and force predictions, with additional details about the datasets and targets provided in Section 4. ANI-1x (Smith et al., 2020) consistently achieves the closest alignment across all downstream tasks, reflecting its design goal of maximizing chemical diversity. Transition-1x (Schreiner et al., 2022), which focuses on transition states, shows as the second most aligned dataset, suggesting that its emphasis on high-energy transition states leads to partial overlap with downstream distributions. In contrast, the catalysis datasets, OC20 (Chanussot et al., 2021) and OC22 (Tran et al., 2023), exhibit the weakest alignment. While OC20 and OC22 are often favored for pretraining (Shoghi et al., 2023; Kolluru et al., 2022) due to their scale and chemical diversity, our metric suggests they may not align well with the considered downstream tasks. Next, we examine whether these alignment values correlate with downstream performance. 4. Experiments We evaluate the impact of pretraining on different upstream datasets for downstream performance and investigate how well the CSI values in Figure 3 reflect the relevance of these datasets. We begin by defining the datasets, baselines, and evaluation setup. Upstream Datasets: Following JMP (Shoghi et al., 2023), 5 Downstream Datasets: For downstream evaluation, we focus on in-distribution (ID) tasks involving energy or force prediction, following the definition in JMP (Shoghi et al., 2023). We discuss out-of-distribution (OOD) tasks in Section 5. Given the large number of pairs for ID evaluation, we focus on the first molecule for force tasks and the energy target for the multi-property dataset QM9 (Wu et al., 2018). The selected targets and their corresponding datasets are: Aspirin in rMD17 (Christensen & Von Lilienfeld, 2020), Ac-Ala3-NHMe in MD22 (Chmiela et al., 2023), solvated amino acids in SPICE (Eastman et al., 2023), and U0 in QM9 (Wu et al., 2018). Baselines: We report the original performance of JMP, where JMP-S and JMP-L correspond to the small and large backbones, respectively. Additionally, we present our reproduced fine-tuning results using the official JMP checkpoints, denoted as JMP-S* and JMP-L*. For our budgeted evaluation, we present results in two categories: pretraining on single upstream dataset and pretraining on mixture of all datasets. For single-dataset experiments, we randomly sample instances from the original upstream data. For mixed pretraining, we construct the training set using two different strategies. (1) Balanced Sampling, where an equal number of samples is drawn from each of the four upstream datasets, totaling samples; and (2) Temperature-Based Sampling, which preserves the dataset proportions used in the full 120M sample set of JMP (Shoghi et al., 2023). Towards Data-Efficient Pretraining for Atomic Property Prediction Table 1. In-Distribution Evaluation for energy and force targets. We report test MAE when fine-tuning on downstream targets, as detailed in Downstream Datasets (Section 4). The top section represents models pretrained with the large-scale JMP budget, while the lower two sections show results under limited budget. JMP-S* denotes reproduced results. Upstream Data Backbone rMD17 (meV/ A) MD22 (meV/ A) SPICE (meV/ A) QM9 (meV) 240 106 Mixed (Temp) JMP-L (GemNet-OC-L) JMP-S (GemNet-OC-S) JMP-S*(GemNet-OC-S) 10 106 ANI-1x Transition-1x OC20 OC22 10 106 Mixed (Balanced) Mixed (Temp) GemNet-OC-S GemNet-OC-S 5.1 6.7 6.8 5.4 10.1 14.6 16.0 9.4 11.0 1.92 2.64 3.21 2.90 3.73 4.53 5.20 3.62 4. 4.75 5.71 5.60 5.13 7.55 8.74 10.73 7.02 7.98 2.9 3.3 3.4 2.9 3.2 4.8 5.7 3.2 3. Evaluation Setup: We pretrain the GemNet-OC-S model (Gasteiger et al., 2022) on each individual upstream dataset, as well as on mixed variants defined in the baselines. For our main experiments, we set fixed computational budget of = 10 106, achieved by training on = 2 106 samples for = 5 epochs. This budget ensures accessibility and reproducibility, with each pretraining run completing within 12 days on an A100 GPU. This represents 24 reduction in computational cost compared to the pretraining budget used in JMP (Shoghi et al., 2023). Additional budget configurations are explored in later sections and the appendix. Each pretrained model is then fine-tuned separately on each downstream task. 4.1. Does CSI Correlate with Better Performance? In Section 3, we introduced the Chemical Similarity Index (CSI) and presented CSI values. The results indicate that ANI-1x exhibits the highest similarity to all downstream datasets. This finding raises critical question: (cid:17) Can CSI reliably guide the selection of pretraining datasets to achieve optimal performance on specific downstream tasks? Table 1 summarizes the downstream performance of models pretrained on different datasets in an in-distribution setting. Notably, the ANI-1x model consistently outperforms all other individual datasets and even mixed variants. For instance, on both rMD17, SPICE and QM9 datasets, ANI-1x achieves an MAE of 5.4, 5.13 and 2.9, compared to 6.7, 5.71 and 3.3 for JMP-S. Remarkably, this performance is achieved with 24 less computational budget than JMPS. Additionally, pretraining on alternative upstream tasks results in weaker performance compared to ANI-1x, correlating with the trends observed in our CSI values. Furthermore, temperature-based mixing, which follows the JMP formulation and prioritizes high-CSI datasets (OC and OC22), performs worse. In contrast, balanced mixing improves slightly by incorporating more samples from the more relevant ANI-1x and Transition-1x but remains inferior to individual pretraining on ANI-1x. These results indicate that, under limited budget, mixing upstream datasets with varying CSI values is suboptimal and demands significantly more computational resources for convergence. Takeaway. Our experiments highlight three key insights for in-distribution downstream tasks: (1) High-quality, taskrelevant upstream datasets like ANI-1x outperform larger, mixed datasets, when included within the mix. (2) Mixing upstream datasets can match the benefits of highly relevant pretraining but requires significantly more compute and training time. (3) CSI effectively predicts downstream performance, as lower CSI values (e.g., ANI-1x) consistently correlate with better results. 4.2. What is the Effect of Computational Budget? Building on our earlier findings, we now investigate how varying the computational budget impacts downstream performance. Specifically, we ask: (cid:17) Do our findings about dataset relevance in terms of CSI hold across different budget levels? Table 2 presents the downstream performance across additional pretraining budgets of 1M , and 3M samples (5 epochs each) compared to 2M . ANI-1x remains superior to other datasets across all budgets, aligning with its low CSI. Increasing the budget from 1M to 2M samples generally improves performance; however, further scaling to 3M samples results in diminishing returns on rMD17, SPICE, and QM9, with only slight improvement on MD22. This occurs despite the pretraining loss for 3M being better than 2M , highlighting that improved pretraining performance does not always translate to better downstream results. Takeaway. Our findings are consistent across budget levels: 6 Towards Data-Efficient Pretraining for Atomic Property Prediction Table 2. Effect of Computational Budget on Performance. While fixing the number of epochs (E) to 5, we vary the number of training samples from = 1 106 upto = 3 106. Our findings are consistent across budget levels where the upstream dataset with the lowest CSI yields the best downstream performance. = 1 106 = 2 = 3 106 Upstream rMD17 MD22 SPICE QM9 rMD17 MD22 SPICE QM9 rMD17 MD22 SPICE QM9 ANI-1x Transition-1x OC20 OC22 5.7 11.7 14.8 18.1 2.91 3.92 4.67 5. 5.34 7.85 8.92 11.22 2.8 3.3 4.7 5.5 5.4 10.1 14.6 16.0 2.90 3.73 4.53 5.20 5.13 7.55 8.74 10.73 2.9 3.2 4.8 5. 5.6 10.2 16.1 17.4 2.85 3.64 4.61 5.15 5.37 7.65 10.44 11.00 2.8 3.2 5.3 5.8 Table 3. Effect of Changing the Backbone Size. We analyze the impact of using larger variant of GemNet-OC and find that, irrespective of backbone size, relevance-based upstream dataset selection consistently outperforms costly large-scale dataset mixing. Upstream Data Backbone 240 106 Mixed (Temp) JMP-L (GemNet-OC-L) JMP-L* (GemNet-OC-L) 10 106 ANI-1x Transition-1x OC20 OC GemNet-OC-L rMD17 (meV/ A) MD22 (meV/ A) SPICE (meV/ A) QM9 (meV) 5.1 5. 4.8 9.7 13.8 12.0 1.92 2.59 2.54 3.56 3.90 4.14 4.75 4.91 5.24 7.42 9.24 10.43 2.9 3. 2.6 3.0 4.6 4.0 improve or degrade downstream performance? To evaluate this, we pretrain on mixture of 2M samples from the best CSI dataset (ANI-1x) and 1M from OC22, non-aligned upstream dataset. We then compare this pretraining checkpoint with the 2M -sample baseline in Figure 4. Surprisingly, adding OC22 degraded downstream performance despite the increased pretraining budget. Takeaway. Including additional pretraining data from less relevant sources can harm downstream performance. Our findings highlight the CSI metric as practical tool for designing effective pretraining datasets. the upstream dataset with the lowest CSI yields the best downstream performance. 4.3. What is the Effect of Changing the Architecture Size? In the previous sections, we used the small variant, GemNetOC-S, as our backbone. Here, we address the question: (cid:17) Does the correlation between CSI and downstream performance hold across different architecture sizes? Table 3 reports the downstream performance when using the large variant, GemNet-OC-L, as the backbone. Consistent with the results on the small backbone, models pretrained on ANI-1x achieve the best performance, correlating well with the CSI values. Notably, we achieve state-of-the-art results with an MAE of 4.8 on Aspirin in rMD17 and 2.6 on U0 in QM9, even though our reproduced JMP-L baseline results fall short of those reported in the original JMP paper. Takeaway. Our findings hold across backbone sizes, showing the potential of relevance-based upstream dataset selection over large-scale mixing for improved performance. 4.4. Is More Data Always Better? The common pretraining paradigm assumes that larger and more diverse datasets lead to better generalization. Here, we challenge this assumption from another perspective: (cid:17) Does adding pretraining data from less relevant sources Figure 4. Impact of Adding Less Relevant Pretraining Data. Adding 1M OC22 samples to 2M -sample ANI-1x baseline worsens downstream performance despite larger pretraining budget. This highlights the importance of dataset relevance and the CSI metric for effective pretraining. 7 Towards Data-Efficient Pretraining for Atomic Property Prediction Table 4. OOD Task Performance Across Upstream Sources. We compare the CSI-predicted best upstream sources with actual downstream performance on OOD tasks (QMOF, MatBench, and QM9s ϵ). While CSI aligns well with QM9s OOD label, it mispredicts the best source for MatBench. Mixed pretraining generally improves performance, highlighting the benefits of diverse upstream sources for OOD generalization. QM9 [ϵ] QMOF MatBench [fold0 / mean] Upstream Data Backbone 240 106 Mixed (Temp) JMP-S (GemNet-OC-S) JMP-S* (GemNet-OC-S) 10 106 ANI-1x Transition-1x OC20 OC22 10 106 Mixed (Balanced) Mixed (Temp) GemNet-OC-S GemNet-OC-S (meV ) 23.1 24.0 24.5 25.3 30.8 35.6 27.3 27.9 (eV) 0.18 0.19 0.22 0.22 0.22 0. 0.21 0.21 (cm1) 26.60 / 22.77 24.77 / 21.48 30.09 / 29.60 52.22 / 38.56 37.52 / 30.88 32.78 / 27.55 26.11 / 24.87 26.63 / 25.61 Table 4 shows that ϵ in QM9 aligns with the CSI pattern, similar to ID evaluation, suggesting that CSI is effective for OOD in the label space. In QMOF, the different upstream sources achieve similar performance which lags behind the full pretraining by JMP. For MatBench (evaluated over 5 folds), OC22 achieves the best mean performance while OC20 lags behind, despite our metric predicting both to be equally suitable. Additionally, for both QMOF and MatBench, mixed pretraining variants generalize better than individual sources. This suggests that when the downstream domain differs from all upstream sources, mixing diverse upstream domains provides the best performance. While our CSI metric holds well for ID evaluation, further research is needed to improve OOD generalization across different chemical domains. One direction is to explore additional upstream sources to better capture domain variations. Another potential extension is leveraging foundation models trained beyond energy and forces, which could enhance feature extraction and improve similarity assessments. 6. Conclusion This paper challenges the prevailing trend of scaling data and computational resources in atomic property prediction by demonstrating that strategic data selection based on relevance can achieve comparable or superior performance with significantly fewer resources. We introduce the Chemical Similarity Index (CSI), simple metric that quantifies upstream/downstream datasets alignment, enabling the selection of high-quality, task-relevant pretraining data. Our experiments reveal that smaller, focused datasets often outperform larger, mixed ones, and that indiscriminately adding data can degrade performance when relevance is low. These findings highlight that quality often outweighs quantity in pretraining, offering more efficient and sustainable path forward for molecular machine learning. Figure 5. CSI Between Upstream and OOD Downstream Tasks. CSI values predict that ANI-1x is the best pretraining choice for QMOF, while OC20 and OC22 are best for MatBench. 5. Beyond In-Distribution Recall that our pretraining process is conducted on upstream tasks involving molecules and catalysts, with energy and force as targets. For downstream tasks with different labels (e.g., band gap in QMOF) or from distinct chemical domains such as materials (e.g., MatBench and QMOF), we classify these as out-of-distribution (OOD). While our main results focused on ID evaluation, here we explore our metrics applicability to OOD tasks. Specifically, we examine three cases: the Band Gap property from QMOF (Rosen et al., 2021), Phonons (the first non-energy target in JMP tables) from MatBench (Dunn et al., 2020), and ϵ from QM9, explicitly categorized as OOD in the JMP paper. In Figure 5, we present the CSI values for OOD domains, where the OOD label (ϵ) for QM9 follows the same values as in Figure 3. We observe that QMOF exhibits pattern similar to other ID domains shown in Figure 3. However, MatBench displays distinct pattern, showing strong correlation with OC20 and OC22, followed by ANI-1x and Transition-1x. Next, we analyze the correlation between CSI and downstream performance under OOD evaluation. 8 Towards Data-Efficient Pretraining for Atomic Property Prediction"
        },
        {
            "title": "Acknowledgements",
            "content": "The authors thank Mohamed Elhoseiny for his feedback. Yasir Ghunaim was supported by Saudi Aramco. The research reported in this publication was supported by funding from King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI, under award number 5940 and SDAIA-KAUST Center of Excellence in Data Science, Artificial Intelligence (SDAIAKAUST AI)."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Attendu, J.-M. and Corbeil, J.-P. Nlu on data diets: Dynamic data subset selection for nlp classification tasks. 2023. Bairi, R., Iyer, R., Ramakrishnan, G., and Bilmes, J. Summarization of multi-document topic hierarchies using submodular mixtures. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 553 563, 2015. Berriel, R., Lathuillere, S., Nabi, M., Klein, T., OliveiraSantos, T., Sebe, N., and Ricci, E. Budget-aware adapters for multi-domain learning. In ICCV, pp. 382391, 2019. Chanussot, L., Das, A., Goyal, S., Lavril, T., Shuaibi, M., Riviere, M., Tran, K., Heras-Domingo, J., Ho, C., Hu, W., et al. Open catalyst 2020 (oc20) dataset and community challenges. Acs Catalysis, 11(10):60596072, 2021. Chen, D., Gao, K., Nguyen, D. D., Chen, X., Jiang, Y., Wei, G.-W., and Pan, F. Algebraic graph-assisted bidirectional transformers for molecular property prediction. Nature communications, 12(1):3521, 2021. Chmiela, S., Vassilev-Galindo, V., Unke, O. T., Kabylda, A., Sauceda, H. E., Tkatchenko, A., and Muller, K.- R. Accurate global machine learning force fields for molecules with hundreds of atoms. Science Advances, 9 (2):eadf0873, 2023. Christensen, A. S. and Von Lilienfeld, O. A. On the role of gradients for machine learning of molecular energies and forces. Machine Learning: Science and Technology, 1(4): 045018, 2020. Dunn, A., Wang, Q., Ganose, A., Dopp, D., and Jain, A. Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm. npj Computational Materials, 6(1):138, 2020. Eastman, P., Behara, P. K., Dotson, D. L., Galvelis, R., Herr, J. E., Horton, J. T., Mao, Y., Chodera, J. D., Pritchard, B. P., Wang, Y., et al. Spice, dataset of drug-like molecules and peptides for training machine learning potentials. Scientific Data, 10(1):11, 2023. Garg, S., Ansari, H. P., Farajtabar, M., Mehta, S., Vemulapalli, R., Tuzel, O., Shankar, V., and Faghri, F. Tic-clip: Continual training of clip models. In ICLR, 2024. URL https://arxiv.org/abs/2310.16226. Gasteiger, J., Becker, F., and Gunnemann, S. Gemnet: Universal directional graph neural networks for molecules. NeurIPS, 34:67906802, 2021. Gasteiger, J., Shuaibi, M., Sriram, A., Gunnemann, S., Ulissi, Z., Zitnick, C. L., and Das, A. Gemnet-oc: developing graph neural networks for large and diverse molecular simulation datasets. arXiv preprint arXiv:2204.02782, 2022. Ghunaim, Y., Bibi, A., Alhamoud, K., Alfarra, M., Al Kader Hammoud, H. A., Prabhu, A., Torr, P. H., and Ghanem, B. Real-time evaluation in online continual learning: new hope. In CVPR, pp. 1188811897, 2023. Hammoud, H. A. A. K., Das, T., Pizzati, F., Torr, P., Bibi, A., and Ghanem, B. On pretraining data diversity for self-supervised learning. 2024. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 30, 2017. Huang, K., Fu, T., Gao, W., Zhao, Y., Roohani, Y. H., Leskovec, J., Coley, C. W., Xiao, C., Sun, J., and Zitnik, M. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. In NeurIPS Datasets and Benchmarks Track, 2021. URL https://openreview.net/forum? id=8nvgnORnoWr. Ji, X., Wang, Z., Gao, Z., Zheng, H., Zhang, L., Ke, G., and E, W. Exploring molecular pretraining model at scale. In NeurIPS, 2024. URL https://openreview.net/ forum?id=64V40K2fDv. Jiao, R., Han, J., Huang, W., Rong, Y., and Liu, Y. Energymotivated equivariant pretraining for 3d molecular graphs. In AAAI, volume 37, pp. 80968104, 2023. 9 Towards Data-Efficient Pretraining for Atomic Property Prediction Jin, W., Tang, X., Jiang, H., Li, Z., Zhang, D., Tang, J., and Yin, B. Condensing graphs via one-step gradient In Proceedings of the 28th ACM SIGKDD matching. Conference on Knowledge Discovery and Data Mining, pp. 720730, 2022a. Jin, W., Zhao, L., Zhang, S., Liu, Y., Tang, J., and Shah, N. Graph condensation for graph neural networks. In ICLR, 2022b. Kaushal, V., Iyer, R., Kothawade, S., Mahadev, R., Doctor, K., and Ramakrishnan, G. Learning from less data: unified data subset selection and active learning framework for computer vision. In WACV, pp. 12891299. IEEE, 2019. Killamsetty, K., Durga, S., Ramakrishnan, G., De, A., and Iyer, R. Grad-match: Gradient matching based data subset selection for efficient deep model training. In ICML, pp. 54645474. PMLR, 2021a. Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., and Iyer, R. Glister: Generalization based data subset selection for efficient and robust learning. In AAAI, volume 35, pp. 81108118, 2021b. Kolluru, A., Shoghi, N., Shuaibi, M., Goyal, S., Das, A., Zitnick, C. L., and Ulissi, Z. Transfer learning using attentions across atomic systems with graph neural networks (taag). The Journal of Chemical Physics, 156(18), 2022. Lapedriza, A., Pirsiavash, H., Bylinskii, Z., and Torralba, A. Are all training examples equally valuable? arXiv preprint arXiv:1311.6510, 2013. Li, A. C., Brown, E., Efros, A. A., and Pathak, D. Internet explorer: Targeted representation learning on the open web. In ICML. PMLR, 2023. Li, M., Yumer, E., and Ramanan, D. Budgeted training: Rethinking deep neural network training under resource constraints. arXiv preprint arXiv:1905.04753, 2019. Liao, Y.-L., Wood, B. M., Das, A., and Smidt, T. Equiformerv2: Improved equivariant transformer for scaling to higher-degree representations. In ICLR, 2023. Liu, M., Li, S., Chen, X., and Song, L. Graph condensation via receptive field distribution matching. arXiv preprint arXiv:2206.13697, 2022. Liu, S., Wang, H., Liu, W., Lasenby, J., Guo, H., and Tang, J. Pre-training molecular graph representation with 3d geometry. arXiv preprint arXiv:2110.07728, 2021. Nguyen, T., Chen, Z., and Lee, J. Dataset meta-learning from kernel ridge-regression. In ICLR, 2021a. Nguyen, T., Novak, R., Xiao, L., and Lee, J. Dataset distillation with infinitely wide convolutional networks. NeurIPS, 34:51865198, 2021b. Pan, X., Jin, X., He, Y., Song, S., Huang, G., et al. Budgeted training for vision transformer. In ICLR, 2022. Passaro, S. and Zitnick, C. L. Reducing so (3) convolutions In ICML, pp. to so (2) for efficient equivariant gnns. 2742027438. PMLR, 2023. Penedo, G., Kydlıˇcek, H., allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., Werra, L. V., and Wolf, T. The fineweb datasets: Decanting the web for the finest text data at In NeurIPS Datasets and Benchmarks Track, scale. 2024. URL https://openreview.net/forum? id=n6SCkn2QaG. Prabhu, A., Al Kader Hammoud, H. A., Dokania, P. K., Torr, P. H., Lim, S.-N., Ghanem, B., and Bibi, A. Computationally budgeted continual learning: What does matter? In CVPR, pp. 36983707, 2023. Rosen, A. S., Iyer, S. M., Ray, D., Yao, Z., Aspuru-Guzik, A., Gagliardi, L., Notestein, J. M., and Snurr, R. Q. Machine learning the quantum-chemical properties of metal organic frameworks for accelerated materials discovery. Matter, 4(5):15781597, 2021. Schreiner, M., Bhowmik, A., Vegge, T., Busk, J., and Winther, O. Transition1x-a dataset for building generalizable reactive machine learning potentials. Scientific Data, 9(1):779, 2022. Shoghi, N., Kolluru, A., Kitchin, J. R., Ulissi, Z. W., Zitnick, C. L., and Wood, B. M. From molecules to materials: Pretraining large generalizable models for atomic property prediction. In ICLR, 2023. Smith, J. S., Nebgen, B. T., Zubatyuk, R., Lubbers, N., Devereux, C., Barros, K., Tretiak, S., Isayev, O., and Roitberg, A. Outsmarting quantum chemistry through transfer learning. 2018. Smith, J. S., Nebgen, B. T., Zubatyuk, R., Lubbers, N., Devereux, C., Barros, K., Tretiak, S., Isayev, O., and Roitberg, A. E. Approaching coupled cluster accuracy with general-purpose neural network potential through transfer learning. Nature communications, 10(1):2903, 2019. Smith, J. S., Zubatyuk, R., Nebgen, B., Lubbers, N., Barros, K., Roitberg, A. E., Isayev, O., and Tretiak, S. The ani1ccx and ani-1x data sets, coupled-cluster and density functional theory properties for molecules. Scientific data, 7(1):134, 2020. Towards Data-Efficient Pretraining for Atomic Property Prediction Sriram, A., Choi, S., Yu, X., Brabson, L. M., Das, A., Ulissi, Z., Uyttendaele, M., Medford, A. J., and Sholl, D. S. The open dac 2023 dataset and challenges for sorbent discovery in direct air capture, 2024. Tran, R., Lan, J., Shuaibi, M., Wood, B. M., Goyal, S., Das, A., Heras-Domingo, J., Kolluru, A., Rizvi, A., Shoghi, N., et al. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysts. ACS Catalysis, 13 (5):30663084, 2023. Wang, T., Zhu, J.-Y., Torralba, A., and Efros, A. A. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018. Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing, K., and Pande, V. Moleculenet: benchmark for molecular machine learning. Chemical science, 9(2):513530, 2018. Zhao, B. and Bilen, H. Dataset condensation with distribution matching. In WACV, pp. 65146523, 2023. Zhao, B., Mopuri, K. R., and Bilen, H. Dataset condensation with gradient matching. In ICLR, 2021. Zhou, G., Gao, Z., Ding, Q., Zheng, H., Xu, H., Wei, Z., Zhang, L., and Ke, G. Uni-mol: universal 3d molecular representation learning framework. 2022a. Zhou, Y., Nezhadarya, E., and Ba, J. Dataset distillation using neural feature regression. NeurIPS, 35:98139827, 2022b. Towards Data-Efficient Pretraining for Atomic Property Prediction A. More Epochs or More Data? In this section, we explore the trade-off between increasing the number of training epochs and expanding the dataset size under fixed computational budget. Specifically, we aim to answer the following question: (cid:17) Given fixed computational budget, is it more effective to train on smaller dataset for more epochs or to train on larger dataset for fewer epochs? Setup. To investigate this question, we compare two scenarios under the same computational budget of 10M samples: (1) training on 2M samples for 5 epochs, and (2) training on 1M samples for 10 epochs. We evaluate the performance of models pretrained on ANI-1x, Transition-1x, OC20, and OC22, and fine-tune them on the downstream datasets: rMD17, MD22, SPICE, and QM9. For comparison, we also include the results of JMP-L and JMP-S, which use 120M samples for 2 epochs. Results. Table 5 presents the downstream performance for the two scenarios. Across all datasets, ANI-1x consistently achieves the best performance, regardless of whether the model is trained on 2M samples for 5 epochs or 1M samples for 10 epochs. For example, on rMD17, ANI-1x achieves test error of 5.4 in both scenarios, outperforming JMP-S (6.7) and JMP-L (5.1). Similarly, on SPICE, ANI-1x achieves test error of 5.08 (2M samples, 5 epochs) and 5.04 (1M samples, 10 epochs), compared to 5.71 for JMP-S and 4.75 for JMP-L. Interestingly, increasing the number of epochs from 5 to 10 while reducing the dataset size from 2M to 1M does not significantly degrade performance for ANI-1x. This suggests that for highly relevant datasets like ANI-1x, training on fewer samples for more epochs can be as effective as training on more samples for fewer epochs. However, for less relevant datasets like OC20 and OC22, increasing the number of epochs does not compensate for the reduced dataset size, as their performance degrades significantly. Conclusion. Our findings indicate that the choice between more epochs or more data depends on the relevance of the dataset to the downstream task. For highly relevant datasets like ANI-1x, training on fewer samples for more epochs can yield comparable or even superior performance, while for less relevant datasets, increasing the dataset size is more beneficial. This further underscores the importance of dataset quality and relevance, as quantified by CSI, in determining the optimal pretraining strategy. Table 5. Trade-off between increasing the number of samples and the number of epochs. We report the MAE for various downstream tasks while varying the pretraining sample count and epoch count simultaneously. C, , and denote the computational budget, number of samples, and number of epochs, respectively. E Upstream Data Backbone 10 106 2 106 5 10 1 106 10 ANI-1x Transition-1x OC20 OC22 ANI1x Transition1x OC20 OC22 GemNet-OC-S GemNet-OC-S rMD17 (meV/ A) MD22 (meV/ A) SPICE (meV/ A) QM9 (meV) 5.4 10.1 14.6 16.0 5.4 10.6 14.8 17. 2.90 3.73 4.53 5.20 2.88 3.79 4.67 5.24 5.13 7.55 8.74 10.73 5.04 7.50 10.16 11.06 2.9 3.2 4.8 5.7 2.9 3.1 4.9 5. 12 Towards Data-Efficient Pretraining for Atomic Property Prediction B. Additional Analysis on the Metric Design Figure 6. Impact of using mean aggregation instead of flattening on CSI values. We notice that the mean pooling incorrectly reduced the score for OC22 potentially due to over-smoothing. Figure 7. Impact of using random sampling strategy instead of class-balanced sampling. As highlighted in the long-tail analysis in Appendix C, random sampling can lead to class underrepresentation, potentially affecting the correlation between upstream and downstream tasks. Notably, both ANI-1x and Transition-1x exhibit different patterns compared to the class-balanced values reported in the main paper. Figure 8. The impact of using another backbone. We use JMP pretrained model and show that similar insights are obtained where Ani-1x is shown as the most similar. 13 Towards Data-Efficient Pretraining for Atomic Property Prediction C. Long-Tail Analysis As discussed in Section 3, molecular understanding datasets are typically imbalanced and exhibit skewed distribution. Figure 9 illustrates the effect of random sampling versus class-balanced sampling, which we employ in this study, in terms of class coverage. Notably, random sampling results in significant class underrepresentation, whereas class-balanced sampling ensures broader coverage across all classes. Figure 9. Impact of sampling strategies on subset construction for feature extraction. We sample 10K instances for each upstream task, highlighting the differences in class coverage between random and class-balanced sampling. Towards Data-Efficient Pretraining for Atomic Property Prediction D. Implementation Details For both pretraining and fine-tuning experiments, we primarily follow the JMP hyperparameters. However, due to resource constraints requiring smaller batch sizes compared to JMP, we adjusted the learning rate to ensure training stability, as detailed below. For pretraining, we use batch size of 20 and learning rate (LR) of 1e-4 for the small backbone (GemNet-OC-S). For the large backbone (GemNet-OC-L), the batch size is reduced to 12 to fit GPU memory. Additionally, when training with the OC22 dataset on the large backbone, LR of 1e-4 caused gradient instability, thus we used LR of 1e-5 for that particular run. Unless otherwise specified, each experiment is run for five epochs on the specified number of samples for each section of the paper. The best checkpoint is selected based on the performance in the validation set. To handle the large size of the upstream validation sets, validation is performed on smaller subset of 2,000 samples. For finetuning, we use the batch size specified in the JMP codebase and default learning rate (LR) of 8e-5, except for cases where adjustments were needed to stabilize training. Specifically, we use 5e-5 for QMOF, 8e-4 for MatBench when pretrained on Transition1x."
        }
    ],
    "affiliations": [
        "King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"
    ]
}