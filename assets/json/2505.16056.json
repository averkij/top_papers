{
    "paper_title": "Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models",
    "authors": [
        "Jingcong Liang",
        "Siyuan Wang",
        "Miren Tian",
        "Yitong Li",
        "Duyu Tang",
        "Zhongyu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the optimal segment-level cache hit rate under a given cache size limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found that models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency. We further showed that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models can balance between cache effectiveness and efficiency with cache sizes approximately 2x the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 6 5 0 6 1 . 5 0 5 2 : r Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models Jingcong Liang Fudan University jcliang22@m.fudan.edu.cn Siyuan Wang University of Southern California sw_641@usc.edu Miren Tian Huawei Technologies Ltd. tianmiren1@huawei.com Yitong Li Huawei Technologies Ltd. liyitong3@huawei.com Duyu Tang Huawei Technologies Ltd. tangduyu@huawei.com Zhongyu Wei Fudan University zywei@fudan.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce expert offloading that caches subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this local routing consistency varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) Segment Routing Best Performance (SRP), which evaluates how well fixed group of experts can cover the needs of segment of tokens, and (2) Segment Cache Best Hit Rate (SCH), which measures the optimal segment-level cache hit rate under given cache size limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found that models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency. We further showed that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models can balance between cache effectiveness and efficiency with cache sizes approximately 2x the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc."
        },
        {
            "title": "Introduction",
            "content": "Mixture-of-Experts (MoE) is widely adopted model architecture in many large language models (LLMs) that enables efficient model size scaling through sparse activation[10, 17, 4, 1]. MoE models replace dense feed-forward networks (FFNs) with multiple expert modules, with only subset activated during inference. However, the vanilla implementation requires all experts to be loaded into memory, restricting its application on memory-constrained devices such as mobile phones. To address this limitation, the expert offloading technique has been proposed to allow partial loading of expert modules during inference [8, 16, 53]. Specifically, expert offloading caches subset of experts in fast memory (e.g., GPU memory) based on predefined heuristics, while storing remaining experts in slower but larger-capacity storage (e.g., CPU memory or disk). During inference, especially in the decoding stage, if token activates an expert not cached in fast memory, the system either computes the expert forward results with CPU and slow memory (CPU offload) [18, 45], or unloads Preprint. Under review. cached expert according to specific rules such as Least Recently Used (LRU), and replaces it with the demanded expert (on-demand loading) [19, 57]. However, frequent CPU offloads or on-demand loading within short period can significantly degrade the efficiency of the expert offloading system and slow down inference, particularly when processing lengthy contexts with inevitable topic shifts. Prior research has focused on optimizing the design of the expert offloading system, aiming to strategically select which experts to cache in given context to maximize cache hit rates. Among them, some observed and exploited the locality of expert activations, where similar routing choices appear within consecutive segment of tokens, thereby minimizing the need for CPU offloads and on-demand loading [8, 49, 56]. This is especially beneficial during the decoding phase, where tokens are generated one after another. Nevertheless, not all MoE models exhibit such continuous routing patterns uniformly, and the degree or frequency of this phenomenon varies across models. Understanding this variance may help design MoE architectures that are friendly to expert offloading systems and vice versa. In this work, we investigate the degree of this inherent consecutive routing property, which we term local routing consistency, of different MoE-based LLMs to explore their potential effectiveness in segment-based expert routing or caching. Figure 1 illustrates how different levels of local routing consistency reflect different routing patterns. Specifically, we propose two metrics that quantitatively reflect the local routing consistency of specific model. (1) Segment Routing Best Performance (SRP) measures how effectively segment router selecting fixed group of experts for all tokens in segment can approximate the original routers decisions. SRP not only reflects local routing consistency without parameters other than segment length, but also enables analyzing activation patterns of individual experts. (2) Segment Cache Best Hit Rate (SCH) represents the highest cache hit rate by any expert offloading method that caches experts for whole segments, under cache size limit related to the number of active experts. SCH measures local routing consistency on modeland router-levels, yet is more related to the performance of real expert offloading systems as it accounts for the cache limit. Figure 1: Routing results by GRIN-MoE [26] layer 21 and Jamba-Mini-1.6 [20] layer 25 on the same input (Java code). Despite having similar model sizes and the same number of experts, GRIN-MoE exhibits more consistent routing patterns than Jamba-Mini-1.6, with multiple experts continuously activated, so expert caching with GRIN-MoE will be more feasible and effective. We conduct experiments on 20 MoE-based LLMs, covering parameter scales ranging from 3 billion to 54 billion and encompassing diverse architectures. While most models exhibit similar local routing consistency within few tokens, the variance enlarges when the segment length increases. Models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency, some of which even achieve load balance. Additionally, we investigate local routing consistency across different context domains and assess its relationship with experts domain and vocabulary preferences. The findings reveal that domain-specialized experts, if they exist, contribute more to local routing consistency, whereas vocabulary specialization has less impact. Finally, we verify the strong correlation between SRP and SCH, confirming that both metrics effectively represent local routing consistency. We conclude that most MoE models can achieve optimal balance between segment caching effectiveness and deployment efficiency with cache size approximately 2x the number of active experts. 2 Overall, our contributions are three-fold: 1. We propose local routing consistency, property of MoE models that reflects the potential efficiency of expert offloading for the model. We design two metrics to quantify local routing consistency: segment routing best performance (SRP), which provides parameter-free fine-grained analysis, and segment cache best hit rate (SCH), aligned with practical expert offloading. 2. We conduct empirical analysis across 20 MoE-based LLMs, demonstrating that models that apply MoE on every layer and without shared experts show the highest local routing consistency. We also reveal that domain-specialized experts contribute more to local routing consistency than vocabulary-specialized experts. 3. We analyze SCH under different cache sizes relative to the number of active experts. Alongside the optimal cache size derived from SRP results, we conclude that cache sizes 2x the size of active parameters achieve the best segment caching results on most models."
        },
        {
            "title": "2 Definitions",
            "content": "2.1 Preliminary: mixture of experts Transformer-based language models have most parameters on feed-forward network (FFN) layers. As the model size scales up, LLMs often replace them with sparse MoE layers to reduce computational cost during inference. typical MoE layer with experts can be parameterized by smaller FFNs F1(; θ1), . . . , FE(; θE), where each Fi : Rd Rd defines single expert. There is also router : Rd RE in the MoE layer to choose experts and give weights. For each token with hidden representation hx Rd, its output is given by [s1, . . . , sE] = Softmax(R(hx)); [w1, . . . , wE] = Topk(s1, . . . , sE); ox = (cid:88) i=1 wiFi(hx; θi) (1) where Topk preserves the largest scores and sets others to 0. Experts with null score can be effectively deactivated without calculating, thus saving computational resources. Other components in the transformer architecture may also be replaced by their MoE variant, such as Mixture-of-Attention [55] for self-attention and MixLoRA [22] for LoRA adapters. Nevertheless, we focus on MoE layers that replace FFNs, as this is the most prominent and effective design (in terms of the number of parameters). More discussions about MoE LLMs can be found in Appendix A, where we also briefly review expert offloading for MoE models. The above routing procedure is done token by token, which does not guarantee consecutive routing. Since the consistency of consecutive routing decisions can benefit expert offloading systems, it is necessary to investigate the degree of consecutive routing of different MoE-based LLMs. In the following sections, we propose two metrics to measure this local routing consistency and compare them across MoE models with different structure parameters. 2.2 Segment routing best performance (SRP) An intuitive way to measure local routing consistency is to compare the distribution of routing choices between tokens in continuous segment. However, typical metrics for distribution comparison, such as the Kullback-Leibler divergence, work with two distributions and thus are unsuitable for our demand. Instead, we measure how well simplified, segment-based router can mimic the behavior of the original token-based router. Here we define the segment routing best performance (SRP) of single expert and group of experts (e.g., experts from the same MoE layer or model). Single expert For any input sequence = [t1, . . . , tT ], we denote the activation sequence of expert on as A(e, ) = [a1, . . . , aT ], where ai {0, 1} indicates whether is activated on ti (ai = 1) or not (ai = 0). segment-based router Rm with segment length > 0 for expert will try to mimic [ap, . . . , ap+m1] for any and p, similar to expert-choice routing [59]. Let Rm (T, p) = [b1, . . . , bm] be its prediction, with the segment-prediction constraint: bi = 0, = 1, . . . , or bi = 1, = 1, . . . , (2) In other words, Rm simplicity, we write Rm decides that is either always active or always inactive on [tp, . . . , tp+m1]. For (T, p) = 1 for the two cases respectively. By treating (T, p) = 0 and Rm 3 each segment routing attempt as binary classification task with samples, and considering all possible segments of all possible inputs, we can calculate the F1 score of Rm : F1(Rm ) = 2 (cid:80) (cid:80)T m+1 p=1 (cid:80)T m+1 p= (cid:80) Rm (T, p) (e, T, p, m) (T, p) + (e, T, p, m)] [m Rm (3) i=p where (e, T, p, m) = (cid:80)p+m1 A(e, )[i] is the active frequency of in the segment of with length starting at position p. We demonstrate the detailed process to obtain this equation in Appendix B.1. We choose F1 score because we do not set up an upper bound on how many experts Rm can be selected, so hit rate (recall) can go up to 1 by selecting all experts. Moreover, missing major experts is worse than activating minor ones, as changing low-rank experts usually affects model performance less [19, 43], therefore we prefer F1 score to accuracy to emphasize this difference. Based on Equation 3, we define the segment routing best performance of under segment length as the maximum F1 score any Rm ). Furthermore, in Appendix B.2 we prove that F1(Rm gives active predictions for all segments that activates at least αm can achieve: SRP(e, m) maxRm ) is maximized if and only if Rm times, where αm [0, m] is only related to and m: F1(Rm SRP(e, m) = (cid:80) (cid:80) 2 (cid:80) (cid:80)T m+1 [m I[f (e, T, p, m) αm p=1 (e,T,p,m)αm f (e, T, p, m) ] + (e, T, p, m)] (4) Therefore, SRP(e, m) is an intrinsic property of the expert that reflects its local routing consistency, unrelated to any specific segment routing methods. Expert group For group of experts E, let Rm be segment-based router that decides whether each expert should be activated in segment of some input with length m; more specifically, Rm (T, p) that also follows the segment-prediction constraint (Equation 2). Following the same procedure in Appendix B.1, we have (e, T, p) is prediction sequence similar to Rm F1(Rm ) = 2 (cid:80) (cid:80)T m+1 p=1 (cid:80)T m+1 p=1 (cid:80) eE Rm eE [m Rm (e, T, p) (e, T, p, m) (e, T, p) + (e, T, p, m)] (cid:80) (cid:80) (5) ) is maximized if and only if Rm Again, F1(Rm pairs where the expert is activated at least αm Therefore we have gives active predictions for all expert-segment decided by and m. times in the segment, here αm SRP(E, m) max Rm F1(Rm ) = 2 (cid:80) (cid:80) (e,T,p,m)αm f (e, T, p, m) m+1 (cid:80) p=1 (cid:80) (cid:80) eE [m I[f (e, T, p, m) αm ] + (e, T, p, m)] (6) SRP(E, m) measures how well group of experts is coordinated by the original router(s) to achieve layer-level or model-level local routing consistency. Segment routing size ratio Consider the case where the expert group is the set of all experts from an MoE layer that activates top-k experts. When we use the prediction of Rm that achieves the highest F1 to route experts, for any segment of input with length starting at position p, only experts that is activated at least αm times by the original router will be chosen. Hence, the expectation of the number of routed experts for any segment is given by km = 1 + 1 (cid:88) T m+1 (cid:88) (cid:88) p=1 eE I[f (e, T, p, m) αm ] (7) That is, Rm will select km experts on average. We define the ratio between km and as the segment routing size ratio: ρ(E, m) km/k. small ρ(E, m) indicates that the local routing consistency of the experts is high enough, so that segment routing does not need to select too many experts to cover real demands under average cases. We use it as supplementary metric to distinguish cases when groups of experts have similar segment routing best performances. Note that ρ(E, m) is also available for the entire model if the total number of activated experts for each token is also fixed. 4 Note that in the above definitions, is unbounded, causing SRP(e, m), SRP(E, m) and ρ(E, m) to be uncomputable. In experiments, we choose fixed corpora (general or domain-specific) and let S, in which case we write SRPS(e, m), SRPS(E, m) and ρS(E, m) to indicate the corpora. Furthermore, we only need to consider αm that are integers, thus we can effectively enumerate from 0 to to find the corresponding αm and αm and αm respectively. 2.3 Segment cache best hit rate (SCH) The advantage of SRP as local routing consistency metric is that it only relies on the expert (or expert group E) and the segment length m, and is suitable to analyze individual experts. However, real expert offloading scenarios usually have hard cache size limit, so the best global F1 score may not be achievable. Moreover, when considering caching performance, F1 score is not as straightforward as hit rate (recall). Therefore, we propose another metric for local routing consistency, namely segment cache best hit rate (SCH), that is more related to expert offloading. We use the same notations as above, and stick to the expert group case where denotes group of experts that activates experts for each token. Instead of segment-based router Rm , here we consider segment-based cache m,ρ where is the segment length, and ρ be the segment cache size ratio; in other words, m,ρ has cache size of ρk. For any input and any of its sub-segment of length starting at position p, m,ρ will choose ρk experts to cache throughout the segment, without switching any experts. Let m,ρ (e, T, p) = 1 if an expert is cached for the segment, and m,ρ across all segments of all possible inputs is given by (e, T, p) = 0 otherwise, then the hit rate of m,ρ (cid:80) r(C m,ρ ) = (cid:80) (cid:80)T m+1 p=1 (cid:80) e=E m,ρ (e, T, p) (e, T, p, m) (cid:80) eE (e, T, p, m) (cid:80)T m+1 p= (8) We define the segment cache best hit rate of under segment length and segment cache size ratio ρ as the maximum hit rate any m,ρ r(C m,ρ ). Note that in Equation 8, the denominator is unrelated to m,ρ ), we only need to make sure that m,ρ can reach: SCH(E, m, ρ) = maxCm,ρ ; therefore, to maximize r(C m,ρ always caches the ρk experts with the highest : SCH(E, m, ρ) = (cid:80) (cid:80)T m+1 p=1 (cid:80) Topρk{f (e, T, p, m)e E} eE (e, T, p, m) (cid:80) (cid:80) (cid:80)T m+1 p=1 (9) We write SCHS(E, m, ρ) when we limit to fixed corpora S."
        },
        {
            "title": "3 SRP-based consistency analysis",
            "content": "3.1 Experiment setup Models We conduct experiments on 20 MoE-based LLMs with various model sizes: 010B: PowerMoE-3B [42], LLaMA-MoE-v1-3.5B [60], OLMoE-1B-7B-0125 [29], SwitchTransformers-Base-128 [10], LLaMA-MoE-v2-3.8B [33] and JetMoE-8B [41]; 1025B: OpenMoE-8B [48], MiniCPM-MoE-8x2B [13], Qwen1.5-MoE-A2.7B [34], DeepSeekV2-Lite [5] and DeepSeekMoE [4]; 2545B: XVERSE-MoE-A4.2B [50], Qwen3-30B-A3B [35], Yuan2.0-M32 [47], Phi-3.5-MoE [1] and GRIN-MoE [26]; 4560B: Mixtral-8x7B-v0.1 [17], Jamba-Mini-1.6 [20], NLLB-MoE-54B [31] and Qwen2-57BA14B [51]. We list their architecture and configuration details in Appendix C.1, and may use shorter names (e.g., OLMoE and Qwen3) when there is no ambiguity. Data We construct our sample corpus from RedPajama [46], including 7 data sources: C4, CommonCrawl, Books, Wikipedia, ArXiv, StackExchange, and GitHub, each representing specific domain. Appendix C.2 introduces data processing and input generation details. In the following sections, we denote the full corpus as S, and each domains corresponding subset as its data source name (e.g., Books, GitHub). 5 Method and configuration We collect every MoE layers routing decisions for every input*, and for each expert, count the number of activated tokens in every segment. To obtain the SRP, we count the number of segments with the same for each expert (group), then compute the F1 score for every α candidate, choose the α that achieves the highest F1 and finally obtain the segment routing best performance and size ratio. We run all experiments on 8 NVIDIA GeForce RTX 4090 graphics cards and load model parameters with BF16 precision. 3.2 Overall results Figure 2 illustrates all models SRP. While most models have similar SRP and ρ when = 4, the difference between models becomes significant as the segment length increases. Meanwhile, the relative distribution of SRP across models appears similar after = 16. There is gap between short-term (m = 4) and long-term (m 16) local routing consistency, where many models exhibit the short-term one but only few demonstrate the long-term one. Figure 2: Model SRP on S, compared with the segment routing size ratio. Marker size represents model size. We roughly divide the models into four groups that have similar SRP characteristics, whose SRP when = 16 are demonstrated in Table 1: Group 1 (LLaMA-MoE-v2OLMoE) has the highest SRP (> 0.5 when = 16) and ρ ( 1.25) across all segment lengths, showing strong long-term local routing consistency. Group 2 (Mixtral-8x7BLLaMA-MoE-v1) have the second highest SRP ( 0.48 when = 16), but their long-term ρ becomes high ( 2.5). Group 3 (XVERSE-MoEDeepSeekMoE) has significantly lower SRP than group 2, especially the long-term one ( 0.36 when = 16), but their ρ is bit lower 2.0. Group 4 (NLLB-MoESwitchTransformers) have the lowest SRP (< 0.31 when = 16); contrasting other models, their short-term ρ is already high, but their long-term ρ becomes lower. Table 1: Model segment routing best performance (m = 16) on in descending order and several architecture parameters. A:T: ratio between active and all experts; S:A: ratio between shared and active experts; every x: apply MoE every layers; after 1st: apply MoE after the first layer. Model SRP MoE A:T S:A Model SRP MoE A:T S:A LLaMA-MoE-v2 79.59 63.75 Yuan2.0 55.90 Qwen3 54.69 PowerMoE 51.59 Phi-3.5-MoE 50.55 GRIN-MoE 50.17 OLMoE Mixtral-8x7B 49.27 MiniCPM-MoE 48.93 47.46 JetMoE LLaMA-MoE-v1 45.28 all all all all all all all all all all all 1:4 1:16 1:16 1:5 1:8 1:8 1: 1:4 1:4 1:4 1:4 0 0 0 0 0 0 0 0 0 0 0 XVERSE-MoE Jamba-Mini DeepSeek-V2-Lite Qwen2 DeepSeekMoE all 38.03 37.91 every 2 37.79 after 1st 36.86 36.79 after 1st all NLLB-MoE (encoder) 24.84 every 4 Qwen1.5-MoE OpenMoE SwitchTF (encoder) (decoder) (decoder) 30.94 30.88 28.54 every 6 18.98 every 2 18.90 all 3:32 1:8 3:32 1:8 3: 1:64 1:15 1:16 1:128 1:3 0 1:3 1:1 1:3 0 1:1 1:2 0 *In encoder-decoder models, encoder layers only consider encoder input, and decoder layers likewise. 6 3.3 Analysis Which model architecture has the highest local routing consistency? We compare the SRP of different models with some architecture parameters in Table 1. Surprisingly, the least sparse models do not always have very high local routing consistency, even though they activate experts more frequently on average. Instead, we observe two common points that are possibly connected to high local routing consistency: (1) Apply MoE on every layer: all models in groups 1 and 2 apply the MoE structure on every layer, while many group 3 and 4 models skip some layers (e.g., Jamba). (2) No shared experts: all models in groups 1 and 2 do not include any shared experts, unlike many group 3 and 4 models (e.g., Qwen2). We conjecture that in this way, models gain higher local routing consistency by fully utilizing MoE. We also compare SRP with corpus perplexity (Appendix D.2) and other model design aspects (Appendix D.3), where we find little correlation. Table 2: Model segment routing best performance (m = 16) on S, along with load balance (LB) measured by activation frequency standard deviation of experts. Model SRP LB Model SRP LB Model SRP LB LLaMA-MoE-v2 79.59 30.46 Mixtral-8x7B Yuan2.0 Qwen3 PowerMoE Phi-3.5-MoE GRIN-MoE OLMoE 63.75 13.94 MiniCPM-MoE 55.90 4.35 54.69 12.39 51.59 50.55 50.17 4.06 XVERSE-MoE 3.48 5.24 DeepSeek-V2-Lite 37.79 1.93 Jamba-Mini JetMoE LLaMA-MoE-v1 45.28 2.67 49.27 2.53 Qwen2 36.86 6.72 48.93 2.81 DeepSeekMoE 36.79 1.56 47.46 0.99 NLLB-MoE (en) 24.84 1.67 (de) 30.94 2.06 38.03 2.39 Qwen1.5-MoE 30.88 0.55 28.54 2.59 37.91 2.91 OpenMoE 18.98 0.48 18.90 0.62 SwitchTF (en) (de) Can local routing consistency live with load balance? Load balance is key feature for efficient MoE inference [21], but is seemingly against local routing consistency. For instance, both DeepSeekAI et al. [6] and Skliar et al. [43] suggest adding bias to router outputs. Still, the former promotes little activated experts for load balance and the latter promotes recently cached experts for effective caching. To investigate their relation, we compute the standard deviation of all experts activation frequencies in model and compare it with SRP in Table 2. Many models with high SRP also have high activation frequency standard deviation. Appendix D.8 reveals that their local routing consistency, at least partly, comes from expert imbalance. However, models like Qwen3 and GRINMoE enjoy high local routing consistency and moderate load balance simultaneously, in which we found strong domain-specialized experts (see Section 4). Based on the observations, we claim that local routing consistency can coexist with load balance through domain-specialized experts."
        },
        {
            "title": "4 Local routing consistency and expert specialization",
            "content": "4.1 Domain-wise local routing consistency Figure 3: Segment routing best performance (m = 16) on each domain, relative to SRPS(E, 16). CC: CommonCrawl; BK: Books; WK: Wikipedia; AX: ArXiv; SE: StackExchange; GH: GitHub. For encoder-decoder models, light color represents the encoder and dark color represents the decoder. In Section 3, we analyze models on the full corpus S, which consists of text data from 7 different domains. However, each domain has its token distributions, which may affect the router decisions distribution. Figure 3 illustrates the relative difference between domain-wise and global SRP of each model when = 16, where we observe three different patterns among all models: (1) Models 7 like Phi-3.5-MoE, GRIN-MoE, and OLMoE have significantly higher SRP on Wikipedia, ArXiv, StackExchange and GitHub, whose SRP can be more than 10% higher than global SRP. All these domains are professional or related to code. (2) Models like Yuan2.0, Qwen3 and Qwen1.5-MoE have significant higher SRP on Wikipedia, but not on other three professional domains; compared with other domains, Wikipedia contains multilingual text, which may be the cause of such uniqueness. (3) Models like Mixtral-8x7B, MiniCPM-MoE and JetMoE have similar SRP across all domains with insignificant differences. Most of these models have mediocre to low SRP. Above all, models exhibit balanced local routing consistency across all domains or higher local routing consistency on certain domains with unique properties. 4.2 Expert specialization We argue that domain-wise local routing consistency patterns appear across models due to specialized experts in each model. To clarify this, we consider two types of expert specialization, first introduced by Muennighoff et al. [29]: (1) Domain specialization: the normalized frequency of an expert being activated on tokens from specific domain D. We compute the coefficient of variation (CV) of activation frequency across all domains as domain-free metric. (2) Vocabulary specialization: the normalized frequency of an expert being activated on specific token ID x. We follow Muennighoff et al. [29] to obtain the vocabulary specialization of each expert. We compare each models SRP, average expert specialization, and the correlation between experts specialization and SRP in Figure 4; expert distribution between specialization and SRP is also demonstrated in Appendix D.8. Figure 4: Model segment routing best performance (m = 16), compared with average expert specialization (marker size) and the correlation between expert specialization and SRP (y-axis). Domain specialization Most models show positive correlation between their experts domain specialization and SRP; among them, Qwen3, Phi-3.5-MoE, GRIN-MoE, and OLMoE show high SRP, high average domain specialization, and strong correlation between them simultaneously, indicating that these models possess many domain specialized experts that make major contribution to local routing consistency. An exception is LLaMA-MoE-v2, which is very imbalanced: it constantly activates group of experts, resulting in high SRP and low domain specialization. Nevertheless, both groups of experts contribute to local routing consistency, giving the model the highest SRP. Vocabulary specialization We consider three kinds of vocabulary specialization on the input, the models predicted output, and the ground-truth, respectively [29]. Most models demonstrate negative or insignificant correlation between input vocabulary specialization and SRP; LLaMA-MoEv2 becomes the only exception, also due to the constantly activated experts. On the other hand, SRP is slightly positively correlated to prediction or ground truth vocabulary specialization. We conjecture that such specialization happens more in later layers [29] that process high-level information related to the context topic. Above all, we can see that domain specialization plays more important role in forming local routing consistency than vocabulary specialization, especially on load-balanced models."
        },
        {
            "title": "5 SCH-based consistency analysis",
            "content": "5.1 Overall results As mentioned in Section 2.3, SRP has several flaws that hinder its application in expert offloading. This section focuses on the segment cache best hit rate (SCH), which works with size limit, to obtain more straightforward insight into expert offloading and cache management. Based on the number of activated tokens in every input segment from the corpus for each expert, defined as in Section 3.1, we calculate each model and each layers SCH on every possible cache size: At each segment, we sort experts in each layer or model by their in descending order, and compute the segment cache best hit rate at cache sizes ranging from 1 to E. Figure 5 illustrates SCH(E, m, ρ of each model under different ms and ρs. Similar to SRP, we can easily locate the four groups of models mentioned in Section 3.2 starting from = 16. We can also identify the minimum ρ of each group for some specific cache hit rate threshold (e.g., 0.6): Group 1 models have SCH(E, m, 2) 0.6 for segment lengths up to 256. They also have turning points between ρ = 1.5 and ρ = 2, after which increasing ρ improves SCH less. Group 2 models also have SCH(E, m, 2) 0.6 as increases, but their turning points appear at ρ > 2 and are not significant, since these models are not sparse enough. For group 3 models, ρ = 2 is sufficient to reach segment cache hit rate around 0.6 when = 16, but quickly becomes not enough as increases. Group 4 models have the slowest growth rate of SCH and require ρ 3 to make SCH(E, 16, ρ) reach 0.6. Models in groups 3 and 4 also lack turning points at ρ 4 as group 1 models do. Figure 5: Model SCH score under different segment length and segment cache size ratio ρ. Above all, increasing ρ from ρ = 2 improves SCH less for group 1 models, yet decreasing ρ can harm caching performance for group 2 and 3 models. As recent MoE LLMs fall into the first three groups, we claim that in general, ρ = 2 can balance cache effectiveness and efficiency. 5.2 Relation with segment routing best performance To clarify the relation between SCH(E, m, ρ) and SRP(E, m), Table 3 lists the correlation between them across all models. SRP(E, m) and SCH(E, m, ρ) are always highly positively correlated regardless of the values of and ρ. This ensures that SCH shares the same property of SRP under reasonable segment length and cache size. Furthermore, when ρ is around 1.5, the two metrics are most closely related, nearly perfectly linear, aligned with Figure 2 where most models have ρ(E, m) between 13 when 16, as well as our previous claim that ρ = 2 balances cache effectiveness and efficiency. Table 3: Correlation between SCH(E, m, ρ) and SRP(E, m) across all models. Bold font indicates the highest correlation across ρ for each m. ρ = 4 = 16 = 64 = 256 0.5 1.0 1.5 2.0 2.5 3.0 96.58 97.98 98.15 95.76 88.58 74. 97.95 99.54 99.73 98.32 95.33 91.86 96.86 99.04 99.77 98.90 96.49 93.13 95.57 98.36 99.57 99.08 96.80 93."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we investigate the property of MoE LLMs where similar experts can be continuously activated, namely local routing consistency. We propose two metrics to measure this property: segment routing best performance (SRP) and segment cache best hit rate (SCH). We found several key designs that may help improve local routing consistency of MoE LLMs, and suggest that cache size approximately 2x the number of active experts can balance cache effectiveness and efficiency. Broader Impacts and Limitations The primary limitations of our work are the computational constraints restricting MoE LLMs to under 60B parameters and the theoretical nature of SRP and SCH, which focus on pure segment routing/caching. Future work will investigate larger models like DeepSeek-V3 and validate the applicability of our findings and claims to real expert offloading systems. Nonetheless, our analytic methods and results still help design new MoE LLMs friendly to expert offloading and enable deployment on resource-constrained edge devices. While our study will likely have an indirect social impact, developers implementing local routing consistency to build more powerful LLMs must take responsibility for their products societal implications."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint, April 2024. doi: 10.48550/ARXIV.2404.14219. [2] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. survey on mixture of experts in large language models. IEEE Transactions on Knowledge and Data Engineering (TKDE) 2025, pages 120, June 2024. ISSN 2326-3865. doi: 10.1109/tkde.2025. 3554028. [3] Tianyu Chen, Shaohan Huang, Yuan Xie, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, and Furu Wei. Task-specific expert pruning for sparse mixture-of-experts. arXiv preprint, June 2022. doi: 10.48550/ARXIV.2206.00277. [4] Damai Dai, Chengqi Deng, Chenggang Zhao, R.x. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y.k. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12801297, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.70. URL https://aclanthology.org/2024.acl-long.70/. 10 [5] DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint, May 2024. doi: 10.48550/ARXIV.2405.04434. [6] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report. arXiv preprint, December 2024. doi: 10.48550/ARXIV.2412.19437. [7] Zhixu Du, Shiyu Li, Yuhao Wu, Xiangyu Jiang, Jingwei Sun, Qilin Zheng, Yongkai Wu, Ang Li, Hai Helen Li, and Yiran Chen. Sida: Sparsity-inspired data-aware serving for efficient and scalable large mixture-of-experts models. In P. Gibbons, G. Pekhimenko, and C. De Sa, editors, Proceedings of Machine Learning and Systems, volume 6, pages 224238, 2024. URL 11 https://proceedings.mlsys.org/paper_files/paper/2024/file/698cfaf72a208aef 2e78bcac55b74328-Paper-Conference.pdf. [8] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint, December 2023. doi: 10.48550/ARXIV.2312.17238. [9] Zhiyuan Fang, Zicong Hong, Yuegui Huang, Yufeng Lyu, Wuhui Chen, Yue Yu, Fan Yu, and Zibin Zheng. Accurate expert predictions in moe inference via cross-layer gate. arXiv preprint, February 2025. doi: 10.48550/ARXIV.2502.12224. [10] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23 (120):139, 2022. URL http://jmlr.org/papers/v23/21-0998.html. [11] Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Yu Han, and Hao Wang. Mixture-of-LoRAs: An efficient multitask tuning method for large language models. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1137111380, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.994/. [12] Xin He, Shunkang Zhang, Yuxin Wang, Haiyan Yin, Zihao Zeng, Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Ivor Tsang, and Ong Yew Soon. Expertflow: Optimized expert activation and token allocation for efficient mixture-of-experts inference. arXiv preprint, October 2024. doi: 10.48550/ARXIV.2410.17954. [13] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint, April 2024. doi: 10.48550/ARXIV.2404.06395. [14] Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin S. Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu, and Benjamin Lee. Towards moe deployment: Mitigating inefficiencies in mixture-of-expert (moe) inference. arXiv preprint, March 2023. doi: 10.48550 /ARXIV.2303.06182. [15] Wei Huang, Yue Liao, Jianhui Liu, Ruifei He, Haoru Tan, Shiming Zhang, Hongsheng Li, Si Liu, and Xiaojuan Qi. Mixture compressor for mixture-of-experts llms gains more. arXiv preprint, October 2024. doi: 10.48550/ARXIV.2410.06270. [16] Ranggi Hwang, Jianyu Wei, Shijie Cao, Changho Hwang, Xiaohu Tang, Ting Cao, and Mao Yang. Pre-gated moe: An algorithm-system co-design for fast and scalable mixture-of-expert inference. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 10181031, June 2024. doi: 10.1109/ISCA59077.2024.00078. [17] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts. arXiv preprint, January 2024. doi: 10.48550/ARXIV.2 401.04088. [18] Keisuke Kamahori, Yile Gu, Kan Zhu, and Baris Kasikci. Fiddler: CPU-GPU orchestration for fast inference of mixture-of-experts models. In 5th Workshop on practical ML for limited/low resource settings, 2024. URL https://openreview.net/forum?id=WX7lxohjFe. [19] Rui Kong, Yuanchun Li, Qingtian Feng, Weijun Wang, Xiaozhou Ye, Ye Ouyang, Linghe Kong, and Yunxin Liu. SwapMoE: Serving off-the-shelf MoE-based large language models with tunable memory budget. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 67106720, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.363. URL https://aclantholo gy.org/2024.acl-long.363/. [20] Barak Lenz, Opher Lieber, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M. Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Or Dagan, Orit Cohavi, Raz Alon, Roi Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shai Shalev-Shwartz, Shaked Haim Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz, Josh Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, and Yoav Shoham. Jamba: Hybrid transformer-mamba language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=JFPaD7lpBD. [21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb. [22] Dengchun Li, Yingzi Ma, Naizheng Wang, Zhengmao Ye, Zhiyuan Cheng, Yinghao Tang, Yan Zhang, Lei Duan, Jie Zuo, Cal Yang, and Mingjie Tang. Mixlora: Enhancing large language models fine-tuning with lora-based mixture of experts. arXiv preprint, April 2024. doi: 10.48550/ARXIV.2404.15159. [23] Jiamin Li, Yimin Jiang, Yibo Zhu, Cong Wang, and Hong Xu. Accelerating distributed MoE training and inference with lina. In 2023 USENIX Annual Technical Conference (USENIX ATC 23), pages 945959, Boston, MA, July 2023. USENIX Association. ISBN 978-1-939133-35-9. URL https://www.usenix.org/conference/atc23/presentation/li-jiamin. [24] Pingzhi Li, Zhenyu Zhang, Prateek Yadav, Yi-Lin Sung, Yu Cheng, Mohit Bansal, and Tianlong Chen. Merge, then compress: Demystify efficient SMoe with hints from its routing policy. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=eFWG9Cy3WK. [25] Jiacheng Liu, Peng Tang, Wenfeng Wang, Yuhang Ren, Xiaofeng Hou, Pheng-Ann Heng, Minyi Guo, and Chao Li. survey on inference optimization techniques for mixture of experts models. arXiv preprint, December 2024. doi: 10.48550/ARXIV.2412.14219. [26] Liyuan Liu, Young Jin Kim, Shuohang Wang, Chen Liang, Yelong Shen, Hao Cheng, Xiaodong Liu, Masahiro Tanaka, Xiaoxia Wu, Wenxiang Hu, Vishrav Chaudhary, Zeqi Lin, Chenruidong Zhang, Jilong Xue, Hany Awadalla, Jianfeng Gao, and Weizhu Chen. Grin: Gradient-informed moe. arXiv preprint, September 2024. doi: 10.48550/ARXIV.2409.12136. [27] Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, and Jie Fu. closer look into mixture-ofexperts in large language models. arXiv preprint, June 2024. doi: 10.48550/ARXIV.2406.1821 9. [28] Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, and Hongsheng Li. Not all experts are equal: Efficient expert pruning and skipping for mixture-ofexperts large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 61596172, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.334. URL https://aclantholo gy.org/2024.acl-long.334/. 13 [29] Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Evan Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. OLMoe: Open mixture-of-experts language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= xXTkbTBmqq. [30] Mohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https: //openreview.net/forum?id=7I199lc54z. Featured Certification. [31] NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation. arXiv preprint, July 2022. doi: 10.48550/ARXIV.2207.04672. [32] Quang Pham, Giang Do, Huy Nguyen, TrungTin Nguyen, Chenghao Liu, Mina Sartipi, Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, and Nhat Ho. Competesmoe effective training of sparse mixture of experts via competition. arXiv preprint, February 2024. doi: 10.48550/ARXIV.2402.02526. [33] Xiaoye Qu, Daize Dong, Xuyang Hu, Tong Zhu, Weigao Sun, and Yu Cheng. Llama-moe v2: Exploring sparsity of llama from perspective of mixture-of-experts with post-training. arXiv preprint, November 2024. doi: 10.48550/ARXIV.2411.15708. [34] Qwen Team. Qwen1.5-moe: Matching 7b model performance with 1/3 activated parameters\", February 2024. URL https://qwenlm.github.io/blog/qwen-moe/. [35] Qwen Team. Qwen3, April 2025. URL https://qwenlm.github.io/blog/qwen3/. [36] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixtureof-experts inference and training to power next-generation AI scale. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 1833218346. PMLR, July 2022. URL https://procee dings.mlr.press/v162/rajbhandari22a.html. [37] Jie Ren, Dong Xu, Shuangyan Yang, Jiacheng Zhao, Zhicheng Li, Christian Navasca, Chenxi Wang, Harry Xu, and Dong Li. Enabling large dynamic neural network training with learningbased memory management. In 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pages 788802, March 2024. doi: 10.1109/HPCA57654.2024 .00066. [38] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixtureIn International Conference on Learning Representations, 2017. URL of-experts layer. https://openreview.net/forum?id=B1ckMDqlg. [39] Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Y. Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. Mixture-of-experts meets instruction tuning: winning combination for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=6mLjDwYte5. [40] Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, and Chuang Gan. Moduleformer: Modularity emerges from mixture-of-experts. arXiv preprint, June 2023. doi: 10.48550/ARXIV.2306.04640. [41] Yikang Shen, Zhen Guo, Tianle Cai, and Zengyi Qin. Jetmoe: Reaching llama2 performance with 0.1m dollars. arXiv preprint, April 2024. doi: 10.48550/ARXIV.2404.07413. [42] Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, and Rameswar Panda. Power scheduler: batch size and token number agnostic learning rate scheduler. arXiv preprint, August 2024. doi: 10.48550/A RXIV.2408.13359. [43] Andrii Skliar, Ties van Rozendaal, Romain Lepert, Todor Boinovski, Mart van Baalen, Markus Nagel, Paul Whatmough, and Babak Ehteshami Bejnordi. Mixture of cache-conditional experts for efficient mobile device inference. arXiv preprint, November 2024. doi: 10.48550/ARXIV.2 412.00099. [44] Xiaoniu Song, Zihang Zhong, Rong Chen, and Haibo Chen. Promoe: Fast moe-based llm serving using proactive caching. arXiv preprint, October 2024. doi: 10.48550/ARXIV.2410.22 134. [45] Peng Tang, Jiacheng Liu, Xiaofeng Hou, Yifei Pu, Jing Wang, Pheng-Ann Heng, Chao Li, and Minyi Guo. Hobbit: mixed precision expert offloading system for fast moe inference. arXiv preprint, November 2024. doi: 10.48550/ARXIV.2411.01433. [46] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, April 2023. URL https://github.com/togethercomputer/RedPajama-Data. [47] Shaohua Wu, Jiangang Luo, Xi Chen, Lingjun Li, Xudong Zhao, Tong Yu, Chao Wang, Yue Wang, Fei Wang, Weixu Qiao, Houbo He, Zeru Zhang, Zeyu Sun, Junxiong Mao, and Chong Shen. Yuan 2.0-m32: Mixture of experts with attention router. arXiv preprint, May 2024. doi: 10.48550/ARXIV.2405.17976. [48] Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: An early effort on open mixture-of-experts language models. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/for um?id=1YDeZU8Lt5. [49] Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh Marina. Moe-infinity: Efficient moe inference on personal machines with sparsity-aware expert cache. arXiv preprint, January 2024. doi: 10.48550/ARXIV.2401.14361. [50] XVERSE Technology Inc. XVERSE-MoE-A4.2B, April 2024. URL https://huggingface. co/xverse/XVERSE-MoE-A4.2B. [51] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. arXiv preprint, July 2024. doi: 10.48550/ARXIV.2407.10671. [52] Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Yuanlin Duan, Wenqi Jia, Miao Yin, Yu Cheng, and Bo Yuan. MoE-i2: Compressing mixture of experts models through inter-expert pruning and intra-expert low-rank decomposition. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1045610466, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.612. URL https: //aclanthology.org/2024.findings-emnlp.612/. 15 [53] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. Edgemoe: Empowering sparse large language models on mobile devices. IEEE Transactions on Mobile Computing, pages 116, 2025. ISSN 1558-0660. doi: 10.1109/TMC.2025.3546466. [54] Hanfei Yu, Xingqi Cui, Hong Zhang, Hao Wang, and Hao Wang. fmoe: Fine-grained expert offloading for large mixture-of-experts serving. arXiv preprint, February 2025. doi: 10.48550 /ARXIV.2502.05370. [55] Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, and Zhang Xiong. Mixture of attention heads: Selecting attention heads per token. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 41504162, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.278. URL https://aclanthology.org/2022.emnlp-main.278/. [56] Yujie Zhang, Shivam Aggarwal, and Tulika Mitra. Daop: Data-aware offloading and predictive pre-calculation for efficient moe inference. arXiv preprint, January 2025. doi: 10.48550/ARX IV.2501.10375. [57] Shuzhang Zhong, Ling Liang, Yuan Wang, Runsheng Wang, Ru Huang, and Meng Li. Adapmoe: Adaptive sensitivity-based expert gating and management for efficient moe inference. In Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design, ICCAD 24, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400710773. doi: 10.1145/3676536.3676741. URL https://doi.org/10.1145/367653 6.3676741. [58] Zexuan Zhong, Mengzhou Xia, Danqi Chen, and Mike Lewis. Lory: Fully differentiable mixture-of-experts for autoregressive language model pre-training. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=LKEJPySnlt. [59] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M. Dai, zhifeng Chen, Quoc V. Le, and James Laudon. Mixture-of-experts with expert choice routing. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 71037114. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/2f00e cd787b432c1d36f3de9800728eb-Paper-Conference.pdf. [60] Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. LLaMA-MoE: Building mixture-of-experts from LLaMA with continual pre-training. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1591315923, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emn lp-main.890. URL https://aclanthology.org/2024.emnlp-main.890/."
        },
        {
            "title": "A Related work",
            "content": "A.1 MoE-based LLM and expert analysis Since its introduction into large neural networks, MoE has become critical strategy to build large language models up to trillions of parameters [38, 10, 36]. While some early models like SwitchTransformers [10] and NLLB [31] employ encoder-decoder structures as their backbone, due to the success of GPT-3, most recent popular MoE-based LLMs use decoder-only structures [17, 51, 6, 1], replacing their original FFN layers with MoE layers containing multiple experts (other components may be replaced too, e.g., self-attention [40, 41] and LoRA [22, 11]). Cai et al. [2] systematically introduces MoE architectures and implementation in LLMs. The popularity of MoE LLMs has triggered interest in understanding how experts are activated in such models. Many model reports and individual studies focused on the relation between expert selection and the input context. For example, Muennighoff et al. [29] reported that OLMoE shows significant difference in expert activity across different domains. Contrastively, Xue et al. [48] 16 found that the routing choice of OpenMoE is highly related to the input token rather than the input context. Other works investigate the similarity among expert activation patterns [24, 28], as well as the relation between expert output and routing choice [32, 27]. However, few of them have focused on the local activation pattern of experts. For example, Jiang et al. [17] reported that in Mixtral, experts are more likely to be activated consecutively, compared to the random case. While their results provide fundamental support for many efficient MoE inference systems Liu et al. [25], they only examined the case of 2 consecutive tokens, which may be insufficient to ensure the consistency of expert activation in longer segments. A.2 Efficient MoE inference and expert offloading The discrete nature of vanilla MoE routers and redundant parameters has caused MoE models to infer more slowly and consume more memory than dense models with the same number of activated parameters (thus with the same level of FLOPs). Many techniques have been proposed to boost the inference of MoE models, ranging from model modifications like model compression [3, 15, 52, 36] and soft routing [30, 58] to system implementations like load-balanced expert parallel [21, 14, 23] and hardware adaptation [6, 53]. Liu et al. [25] provides an in-depth summary of various inference optimization strategies of MoE models. In this paper, we mainly focus on the potential performance of expert offloading, which enables lossless inference of MoE models on memory-constrained devices by caching only some experts on (fast) memory while leaving others on slow memory or disk storage. Many such systems use pretrained external models and/or information from previous layers to prefetch experts for upcoming layers [37, 7, 12, 44]. Various expert offloading systems propose curated heuristics to manage the expert cache [43, 49, 54, 9]. Among them, some examine the locality of expert activations as empirical support for expert caching efficiency. For example, Eliseev and Mazur [8] found that Mixtral-8x7B-Instruct can activate the same expert across 2-4 consecutive tokens, similar to the observations by Jiang et al. [17]; Xue et al. [49] reported frequent expert reuse during decoding, and Zhang et al. [56] found similar routing choice between prefilling and decoding stages. However, most previous works focus on models like SwitchTransformers and Mixtral; only few have included more recent MoE LLMs. As more MoE LLMs emerge, understanding what models are more friendly to expert offloading becomes important for the development of both MoE architectures and expert offloading methods."
        },
        {
            "title": "B Proofs",
            "content": "B.1 Proof of equation 3 In Section 2.2, we consider each routing decision of Rm for segment of length as binary classification task with samples. If we merge all samples from all segments of all possible inputs into one global binary classification task, and define (e, T, p, m) = (cid:80)p+m1 A(e, )[i] as in i=p 17 Section 2.2, we will have the following prediction statistics: TP(Rm ) = m+1 (cid:88) (cid:88) (cid:88) p= i=1 I[A(e, )[p + 1] = 1 Rm (T, p)[i] = 1] m+1 (cid:88) (cid:88) = p=1 Rm (T, p) (e, T, p, m) FP(Rm ) = m+1 (cid:88) (cid:88) (cid:88) p=1 i= I[A(e, )[p + 1] = 0 Rm (T, p)[i] = 1] m+1 (cid:88) (cid:88) = p=1 Rm (T, p)[m (e, T, p, m)] FN(Rm ) = m+1 (cid:88) (cid:88) (cid:88) p=1 i=1 I[A(e, )[p + 1] = 1 Rm (T, p)[i] = 0] Therefore we have F1(Rm ) = (cid:88) = m+1 (cid:88) [1 Rm (T, p)]f (e, T, p, m) p=1 1 1/Precision(Rm ) + 1/Recall(Rm ) 1 = = = = [T (Rm ) + (Rm ) + [T (Rm ) + (Rm )]/T (Rm ) )]/T (Rm 2T (Rm ) )] + [T (Rm (cid:80)T m+1 p=1 ) + (Rm )] Rm (T, p) (e, T, p, m) (cid:105) (T, p) (cid:80)T m+1 p=1 (cid:104)(cid:80) + Rm (e, T, p, m) (cid:105) [T (Rm ) + (Rm 2 (cid:80) (cid:80)T m+1 p=1 (cid:80)T m+1 p=1 (cid:80)T m+1 p=1 (cid:104)(cid:80) 2 (cid:80) (cid:80) Rm (T, p) (e, T, p, m) (T, p) + (e, T, p, m)] [m Rm which gives Equation 3. (cid:50) B.2 Proof of equation"
        },
        {
            "title": "Assume that Rm",
            "content": "e (T0, p0) = 0 for some e, > 0, Rm , T0 and p0, then we have F1(Rm ) = = = = 2 (cid:80) (cid:80)T m+1 p=1 (cid:80)T m+1 p=1 (cid:80) Rm (T, p) (e, T, p, m) (T, p) + (e, T, p, m)] 2 (cid:80) (cid:80)T m+1 p=1 2 (cid:80) [m Rm (cid:80)T m+1 Rm p=1 (T, p) + (cid:80) Rm Rm =T0p=p0 (T, p) + (cid:80) Rm =T0p=p0 (T, p) (e, T, p, m) (cid:80)T m+1 p=1 (e, T, p, m) (T, p) (e, T, p, m) (cid:80)T m+1 p=1 (e, T, p, m) (cid:80) (cid:80) 2X mY + (10) (11) (12) (13) (14) where = (cid:88) =T0 p=p0 Rm (T, p) (e, T, p, m), = (cid:88) =T0 p=p0 18 Rm (T, p), = m+1 (cid:88) (cid:88) p=1 (e, T, p, m) be copy of Rm Let (cid:100)Rm Then the F1 score of the new segment router will be except that Rm (T, p) = 1; all other routing decisions remain the same. (cid:16) F1 (cid:103)Rm (cid:17) = (cid:80)T m+1 2 (cid:80) (cid:80)T m+1 p=1 p=1 (cid:103)Rm (cid:104) (cid:103)Rm (T, p) (e, T, p, m) (cid:105) (T, p) + (e, T, p, m) (cid:80) = = = 2[X + (e, T0, p0, m)] m(Y + 1) + (mY + Z) F1(Rm ) + 2f (e, T0, p0, m) m(Y + 1) + (mY + Z) F1(Rm ) + [2f (e, T0, p0, m)/m] (mY + Z) + (15) (cid:16) F1(Rm (cid:80)T m+1 p=1 ) = 0 for any Rm which is weighted mean of F1(Rm ) and 2f (e, T0, p0, m)/m with weights mY + and m. Note that = (cid:80) (e, T, p, m) 0, and = 0 if and only if (e, T, p, m) = 0 for all and p. If = 0, then is inactive everywhere and F1(Rm , thus SRP(e, m) = 0 and we can simply let αm = 0. Therefore, we assume that > 0, then both and mY + are (cid:17) ) if and only if 2f (e, T0, p0, m)/m F1(Rm (cid:103)Rm ). Equality is positive. Hence, F1 achieved when and only when all equalities hold. The above result indicates that, in order to increase F1(Rm (T, p) = 0 and (e, T, p, m) (m/2) F1(Rm (T, p) = 1, and for any segment satisfying Rm ), we should change the routing decision to Rm (T, p) = 0. Under the case where the number of possible inputs is finite (which is the case for most LLMs due to their limited context windows), this will eventually result in (cid:100)Rm , whose F1 cannot increase further. Such (cid:100)Rm another (cid:100)Rm with F1 (cid:16) (cid:100)Rm ): Otherwise, if there exists disagree are that activates and only activates all segments with (e, T, p, m) (m/2) F1 must be unique and maximizing F1(Rm > F1 (cid:16) (cid:17) , then the only segments where (cid:100)Rm and (cid:100)Rm (cid:17) (T, p) = 1 and (e, T, p, m) < (m/2) F1(Rm ), we should change the routing decision to Rm ), for any segment satisfying Rm (cid:100)Rm (cid:17) (cid:100)Rm (cid:17) (cid:16) (cid:16) (cid:17) the ones satisfying (m/2) (cid:100)Rm (e, T, p, m) < (m/2) F1 , where (cid:100)Rm (cid:16) (cid:100)Rm and (cid:100)Rm (T, p) = 0; however, changing (cid:100)Rm (cid:16) (cid:17) (cid:17) on these segments to 0 should not increase F1 (cid:17)(cid:109) (cid:16) (cid:16) , contradiction. Therefore, we can let αe,m = (cid:108) (cid:100)Rm , which (T, p) = 1 (cid:16) (cid:17) (cid:100)Rm , (cid:100)Rm thus F1 F1 yields Equation 4. (cid:50) (cid:100)Rm e"
        },
        {
            "title": "C Experiment setup details",
            "content": "C.1 Model architecture list Table 4 lists the detailed architecture and configuration of all models where we conduct our experiments. Some special notes: SwitchTransformers-Base-128 and NLLB-MoE-54B are encoder-decoder models that use the T5 architecture. SwitchTransformers-Base-128 has 12 encoder layers and 12 decoder layers. NLLB-MoE-54B has 24 encoder layers and 24 decoder layers. JetMoE-8B employs mixture-of-attention[39], which we keep intact in our experiments. GRIN-MoE shares the same architecture with Phi-3.5-MoE, but is trained using different methods. Jamba-Mini-1.6 employs hybrid SSM-Transformer structure, yet the MoE part is identical to vanilla transformer-based MoE. If = Rm When (e, T, p, m) = (m/2) F1(Rm (T, p) = 0 for all and p, then F1(Rm ), changing Rm ) is undefined, which we do not concern. (T, p) does not affect F1(Rm ). 19 Table 4: Model architecture and configuration, sorted by model size. Experts: T: total; A: active; S: shared (not included in total). Model # Params (B) Total Active # Layers MoE Layer # Experts S PowerMoE-3B[42] LLaMA-MoE-v1-3.5B[60] OLMoE-1B-7B-0125[29] SwitchTransformers-Base-128[10] LLaMA-MoE-v2-3.8B[33] JetMoE-8B[41] OpenMoE-8B[48] MiniCPM-MoE-8x2B[13] Qwen1.5-MoE-A2.7B[34] DeepSeek-V2-Lite[5] DeepSeekMoE[4] XVERSE-MoE-A4.2B[50] Qwen3-30B-A3B[35] Yuan2.0-M32[47] Phi-3.5-MoE[1] GRIN-MoE[26] Mixtral-8x7B-v0.1[17] Jamba-Mini-1.6[20] NLLB-MoE-54B[31] Qwen2-57B-A14B[51] 3.30 6.74 6.92 7.42 8.03 8.52 11.86 13.87 14.32 15.71 16.38 25.78 30.53 39.94 41.87 41.87 46.70 51.57 54.50 57.41 0.88 3.50 1.28 0.22 3.80 2.33 3.80 4.32 2.69 2.66 2.83 4.23 3.35 3.70 6.64 6.64 12.88 12.11 3.75 14.25 32 32 16 24 32 24 24 40 24 27 28 28 48 24 32 32 32 32 48 28 all all all every 2 all all every 6 all all after 1st after 1st all all all all all all every 2 every 4 all 40 16 64 128 8 8 32 8 60 64 64 64 128 32 16 16 8 16 128 8 4 8 1 2 2 2 2 4 6 6 6 8 2 2 2 2 2 2 8 0 0 0 0 0 0 1 0 4 2 2 2 0 0 0 0 0 0 0 8 Many models provide both base and post-trained (e.g., SFT) versions. We compared the local routing consistency between these versions for several models in Appendix D.4 and found no significant difference. Therefore, we always choose the base version in our main experiments. C.2 Data processing and input generation We concatenate samples from each domain subset of RedPajama, cutting them into input sequences of 512 tokens (the context window size of SwitchTransformers). For each domain, we sample 2,048 input sequences, resulting in 14,336 input samples in total. For SwitchTransformers, since the model is trained for masked language modeling, we randomly select 64 tokens from each input sequence, masking them in the original sequence as the encoder input and constructing the corresponding label sequence as the decoder input. For NLLB-MoE, as the model is trained for machine translation, we use the same sequence (with the English language token prepended) as both the encoder input and the decoder input. All other models do not need further data preprocessing, as they are decoder-only and trained for next token prediction."
        },
        {
            "title": "D Additional results",
            "content": "D.1 Statistical significance of local routing consistency Due to the very high correlation between SRP and SCH, we choose SCH to represent local routing consistency, and report the 95% confidence intervals when ρ = 2 in Table 5. The confidence intervals are obtained by bootstrapping 1,000 times with samples from S. D.2 SRP and corpus perplexity Figure 6 compares each models segment routing best performance with its mean log perplexity on S. Most models have corpus perplexity close to each other, and we do not find significant relation between SRP and corpus perplexity. 20 Table 5: Model SCH(E, m, 2) 95% confidence interval. The decoder of SwitchTransformer does not have valid data when = 256. Model = 4 = 16 = 64 = 256 LLaMA-MoE-v2 Yuan2.0 Qwen3 PowerMoE Phi-3.5-MoE GRIN-MoE OLMoE Mixtral-8x7B MiniCPM-MoE JetMoE LLaMA-MoE-v1 XVERSE-MoE Jamba-Mini-1.6 DeepSeek-V2-Lite Qwen2 DeepSeekMoE NLLB-MoE (en) (de) Qwen1.5-MoE OpenMoE SwitchTF (en) (de) (99.97, 99.97) (94.98, 95.08) (93.02, 93.18) (92.58, 92.67) (89.19, 89.39) (88.25, 88.45) (88.72, 88.90) (88.16, 88.24) (88.23, 88.31) (85.74, 85.82) (80.87, 80.96) (77.74, 77.87) (77.78, 77.91) (77.82, 77.96) (75.51, 75.59) (76.90, 77.05) (61.99, 62.13) (64.97, 65.18) (69.57, 69.70) (64.87, 65.09) (54.72, 54.86) (55.16, 55.33) (99.00, 99.04) (82.36, 82.53) (77.37, 77.64) (79.54, 79.69) (73.91, 74.26) (72.70, 73.05) (72.01, 72.34) (73.75, 73.87) (73.20, 73.31) (70.80, 70.89) (66.94, 67.03) (56.73, 56.92) (56.67, 56.84) (56.47, 56.68) (55.08, 55.14) (54.96, 55.17) (37.00, 37.25) (44.36, 44.67) (45.56, 45.74) (41.74, 42.06) (26.32, 26.57) (26.17, 26.47) (98.15, 98.21) (78.49, 78.69) (69.22, 69.59) (73.67, 73.89) (66.61, 67.07) (65.30, 65.76) (64.16, 64.59) (65.91, 66.07) (64.96, 65.08) (62.86, 62.98) (60.05, 60.15) (45.53, 45.75) (46.00, 46.23) (45.37, 45.63) (45.29, 45.36) (43.52, 43.78) (27.85, 28.14) (37.16, 37.49) (33.35, 33.56) (31.76, 32.15) (16.71, 17.01) (16.13, 16.44) (97.67, 97.76) (76.74, 76.98) (64.15, 64.64) (70.94, 71.21) (62.73, 63.30) (61.35, 61.90) (59.96, 60.45) (61.75, 61.95) (60.70, 60.84) (58.80, 58.95) (56.74, 56.86) (40.07, 40.33) (40.57, 40.87) (39.64, 39.95) (40.90, 40.97) (37.68, 37.97) (23.39, 23.70) (31.33, 31.68) (27.40, 27.64) (27.22, 27.67) (12.84, 13.16) Figure 6: Model SRP on S, compared with mean log perplexity on S. Marker size represents model size. D.3 SRP and model architecture Table 6 compares each models segment routing best performance with several architecture parameters listed in Table 4 but not in Table 1. On these parameters, we do not observe significant patterns related to SRP. D.4 Base vs. post-trained We selected three modelsLLaMA-MoE-v1, OLMoE, and JetMoEthat have both base and posttrained versions released, and calculated SRP(E, m) and ρ(E, m) of each version. Table 7 lists the results, from which we can see that the differences of both SRP(E, m) and ρ(E, m) between models before and after post-training are not significant enough to change the degree of local routing consistency, regardless of what type of post-training (SFT, DPO, etc.) is applied. Another related fact is that Phi-MoE-3.5 and GRIN-MoE, which share the same model architecture but are trained differently, have similar local routing consistency. Both indicate that the training method may be less important than the model architecture concerning local routing consistency. 21 Table 6: Model segment routing best performance (m = 16) on S, compared with architecture parameters listed in Table 4 but not in Table 1. Model SRP # Params (B) Total Active Active Param Ratio (%) # Layers # Experts LLaMA-MoE-v2 Yuan2.0 Qwen3 PowerMoE Phi-3.5-MoE GRIN-MoE OLMoE Mixtral-8x7B MiniCPM-MoE JetMoE LLaMA-MoE-v1 XVERSE-MoE Jamba-Mini-1.6 DeepSeek-V2-Lite Qwen2 DeepSeekMoE NLLB-MoE (en) (de) Qwen1.5-MoE OpenMoE SwitchTF (en) (de) 79.59 63.75 55.90 54.69 51.59 50.55 50. 49.27 48.93 47.46 45.28 38.03 37.91 37.79 36.86 36.79 24.84 30.94 30.88 28.54 18.98 18.90 8.03 39.94 30.53 3.30 41.87 41.87 6.92 46.70 13.87 8.52 6.74 25.78 51.57 15.71 57.41 16. 27.25 27.25 14.32 11.86 3.71 3.71 3.80 3.70 3.35 0.88 6.64 6.64 1.28 12.88 4.32 2.33 3.50 4.23 12.11 2.66 14.25 2.83 1.88 1.88 2.69 3.80 0.11 0.11 47.36 9.27 10.98 26.76 15.86 15.86 18. 27.58 31.13 27.36 51.85 16.39 23.48 16.94 24.82 17.27 6.88 6.88 18.78 32.07 3.02 3.02 32 24 48 32 32 32 16 32 40 24 32 28 32 27 28 24 24 24 24 12 12 8 32 128 40 16 16 64 8 8 8 16 64 16 64 64 64 128 128 60 32 128 128 2 2 8 8 2 2 2 2 2 4 6 2 6 8 6 2 2 4 2 1 1 Table 7: Segment routing best performance between models before and after post-training. Model = = 16 = 64 = 256 SRP ρ(E, m) SRP ρ(E, m) SRP ρ(E, m) SRP ρ(E, m) LLaMA-MoE-v1 +SFT 55.85 -0.01 1.03 -0.00 OLMoE 64.14 +SFT +0.27 +DPO +0.32 +0.29 +Instruct 0.99 +0.00 +0.00 +0.00 JetMoE +SFT +Chat 60.31 -0.18 -0.19 1. -0.00 -0.00 45.28 -0.01 50.17 +0.33 +0.41 +0.35 47.46 -0.12 -0.13 2.39 -0. 1.05 +0.01 +0.01 +0.01 2.25 +0.01 +0.01 41.53 -0.01 44.64 +0.38 +0.47 +0. 42.69 -0.10 -0.11 2.94 +0.00 1.20 +0.01 +0.01 +0.01 2. +0.01 +0.01 40.55 -0.00 41.63 +0.47 +0.58 +0.50 41.01 -0.06 -0.07 3.57 -0. 1.22 +0.02 -0.03 +0.02 3.18 +0.02 +0.03 22 D.5 SRP per segment position To determine whether the segment position can affect the segment routing best performance, we calculate SRP (E, m) on each segment position by summarizing statistics of all segments that share the same position. Figure 7 illustrates this position-wise SRP(E, m) at each possible segment position. Most models have nearly constant SRP(E, m) at every position except = 0, where many models activate specialized experts to handle the beginning of the input sequence. This stability of local routing consistency across input positions allows us to use segments from all positions to calculate SRP(E, m), and apply conclusions based on SRP(E, m) to any segment of the input (except the very first one). Figure 7: Position-wise segment routing best performance on of each model. For encoder-decoder models, dotted lines show the encoder SRP(E, m) and solid lines show the decoder ones. D.6 SRP across domains To verify whether local routing consistency is transitive across different domains, we calculate the correlation of expert segment routing best performance between pair-wise domains and demonstrate it in Figure 8. We also compute the correlation of expert activation frequency between pair-wise domains, results illustrated in Figure 9. By comparing corresponding heapmaps, we can see that local routing consistency is nearly always positively correlated, even between distant domains on which the experts activation frequencies are negatively correlated. This means that local routing consistency is transitive; domain-specialized experts with high local routing consistency in one domain tend to exhibit it in any other domain. We also found that some models (e.g., LLaMA-MoE-v2 and Qwen2) do not show significant difference between domains, which is aligned with the results in Section 4.2. Figure 8: Correlation between domain-wise expert SRP of each model. CC: CommonCrawl; BK: Books; WK: Wikipedia; AX: ArXiv; SE: StackExchange; GH: GitHub. Figure 9: Correlation between the domain-wise expert activation frequency of each model. CC: CommonCrawl; BK: Books; WK: Wikipedia; AX: ArXiv; SE: StackExchange; GH: GitHub. 23 D.7 Layer level results Figure 10 illustrates each models layer-wise SRP. Most models have peak SRPs among middle layers, while some (e.g., Yuan2.0 and MiniCPM) have another peak at the last layer. We conjecture that middle layers are less tied to input/output tokens and thus more sensitive to the general topic, and the final layers process highly abstract information that is also more related to the overall topic. Both encourage routers to select similar experts within local segment that share the same topic across tokens. PowerMoE and Qwen2 have another peak on layer 2 due to expert imbalance. Appendix D.8 gives clear view on this. Figure 10: Layer-wise SRP on of each model. Solid lines show SRP(E, m) while dotted lines show corresponding ρ(E, m). We also calculated layer-wise segment cache best hit rate, results demonstrated in Figure 11. The patterns are the same as SRP, indicating high correlation between the two metrics. Figure 11: SCH(E, m, 1) while dotted lines show corresponding SCH(E, m, 2). Layer-wise segment cache best hit rate on of each model. Solid lines show D.8 Expert level results We demonstrate expert-wise segment routing best performance against activation frequency in Figure 12. LLaMA-MoE-v2, Yuan2.0, and PowerMoE have experts with very high activation frequency. These experts naturally have very high local routing consistency and contribute to these models high model-level local routing consistency. The imbalanced experts of PowerMoE mainly belong to layer 2, which also explains the observation in Section D.7. Furthermore, Figures 13, 14, 15 and 16 compares SRP with domain and vocabulary specialzations. The plots are aligned with the conclusion of Section 4.2 that when the model exhibits domain specialization, domain-specialized experts contribute more to overall local routing consistency than vocabulary-specialized experts. 24 Figure 12: Per-expert activate frequency vs. SRP. The x-axis is stretched to show experts with very low or high activation frequency. Gray dashed lines indicate the theoretical lower bound of SRP at different activation frequencies. Green dashed lines show the expected activation frequency of experts from each model. Figure 13: Per-expert domain specialization vs. SRP Figure 14: Per-expert input vocabulary specialization vs. SRP. Encoder-decoder models are not involved due to different input formats from other decoder-only models. Figure 15: Per-expert predicted output vocabulary specialization vs. SRP. Encoder-decoder models are not involved due to different input formats from other decoder-only models. 25 Figure 16: Per-expert ground-truth output vocabulary specialization vs. segment routing best performance. Encoder-decoder models are not involved due to different input formats from other decoder-only models."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Huawei Technologies Ltd.",
        "University of Southern California"
    ]
}