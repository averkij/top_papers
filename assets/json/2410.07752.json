{
    "paper_title": "TVBench: Redesigning Video-Language Evaluation",
    "authors": [
        "Daniel Cores",
        "Michael Dorkenwald",
        "Manuel Mucientes",
        "Cees G. M. Snoek",
        "Yuki M. Asano"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 2 5 7 7 0 . 0 1 4 2 : r TVBENCH: REDESIGNING VIDEO-LANGUAGE EVALUATION Daniel Cores1, Michael Dorkenwald2, Manuel Mucientes1, Cees G. M. Snoek2, Yuki M. Asano3 1CiTIUS, University of Santiago de Compostela, 2QUVA Lab, University of Amsterdam, 3Fundamental AI Lab, University of Technology Nuremberg Benchmark available at: https://huggingface.co/datasets/FunAILab/TVBench"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As solution, we propose TVBench, novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier clearly surpassing this baseline."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision Language Models have gained popularity, benefiting from both the progress made in natural language processing and the surge of foundation models for vision tasks with strong generalization capabilities. Recently, video-language models have been introduced (Xu et al., 2021; Lin et al., 2023; Zhang et al., 2023a; Xue et al., 2023), aiming to replicate the success achieved in the image domain. To evaluate their performance, visual question answering has emerged as key task requiring both textual and visual reasoning. With the rapid model development and release cycles, having reliable and robust benchmark is crucial in measuring progress and guiding research efforts. There are two main approaches to designing question-answering benchmarks for videos: multiplechoice question answering (MCQA) (Wang et al., 2024; Li et al., 2024) and open-ended question answering (Yu et al., 2019; Xu et al., 2017). The most popular and commonly used benchmark for MCQA is MVBench (Li et al., 2024), which is the most downloaded video dataset with over 104K monthly downloads as of September 2024 and the second most downloaded across both image and video visual question-answering benchmarks on HuggingFace1, despite being released only recently. As such, its reliability as valid benchmark is of utmost importance. But how much video understanding does MVBench truly measure? Previous analysis in image question answering benchmarks (Goyal et al., 2017b) has demonstrated that poorly formulated benchmarks could bias the development of new models towards learning strong text representations while ignoring visual information. This is especially relevant for the video-language community, where benchmarks must account not only for visual but also for temporal understanding. In this work, we conduct comprehensive analysis of widely used video question-answering benchmarks, revealing that temporal information is poorly evaluated. Furthermore, in MCQA tasks, prior world knowledge, combined with overly informative questions and answer choices, often allows Equal contribution. 1https://huggingface.co/datasets?task_categories=task_ categories:visual-question-answering&sort=downloads 1 Figure 1: TVBench temporal video-language benchmark. In TVBench, state-of-the-art textonly, image-based, and most video-language models perform close to random chance, with only the latest strong temporal models, such as Tarsier, outperforming the random baseline. In contrast to MVBench, the performance of these temporal models significantly drops when videos are reversed. questions to be answered solely through text, without the need for visual input. Our results also indicate that automatic open-ended evaluation is unreliable, with significant evaluation discrepancies in results across different models for the same task. We assess the substantial shortcomings of MVBench and propose as solution new benchmark TVBench that requires temporal understanding to be solved, providing an effective evaluation tool for current video-language models: MVBench contains unlikely candidate answers easily dismissed from single frame. We provide only temporal challenging candidate answers, requiring models to leverage temporal information to answer correctly. MVBench contains QA pairs with obvious solutions due LLM biased generation. To address this, we generate questions using text templates, preventing grammatical issues that could be exploited by text-only models. MVBench contains QA pairs that can be solved by solely relying on prior world knowledge. We design questions that can be answered purely from the video content, without relying on prior world knowledge. As result, TVBench measures the temporal understanding of video-language models in contrast to MVBench, as shown in Fig. 1. In this setting, text-only and single-frame models, such as Llama3 and GPT-4o, perform at random chance level on TVBench, despite achieving competitive results on MVBench. Surprisingly, even the most recent state-of-the-art video-language models perform close to random chance on TVBench, with Tarsier and Gemini 1.5 Pro clearly outperforming the random baseline. For these models, shuffling and reversing the videos lead to significant performance drops, unlike in MVBench, further highlighting the value of TVBench as temporal video benchmark."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Traditional video evaluation benchmarks focused on specific tasks such as action recognition (Goyal et al., 2017a; Kay et al., 2017) or video description (Das et al., 2013; Xu et al., 2016; Wang et al., 2019). With the emergence of Vision Language Models (VLMs), there is growing need for more comprehensive evaluation protocols to effectively evaluate models with increasingly advanced generalization capabilities. Current video-language benchmarks focus on solving QA pairs that require certain level of multimodal understanding. There are two major trends in the QA format: openended QA and multiple-choice QA (MCQA). Open-ended question answering. Evaluating open-ended QA introduces new challenges, as traditional evaluation metrics such as ROUGE (Lin, 2004), METEOR (Banerjee & Lavie, 2005), and CIDEr (Vedantam et al., 2015) fail to analyze discrepancies of more complex and elaborated answers. Alternatively, Maaz et al. (2023) introduces novel quantitative evaluation pipeline for openended QA datasets. The proposed method relies on GPT-3.5 to determine the correctness of the predicted answer and provides matching score with the ground truth. Commonly used datasets for evaluating models in this context include MSRVTT-QA (Xu et al., 2017), MSVD-QA (Xu et al., 2 Figure 2: Spatial bias of MVBench video-language benchmark. We show different tasks of the MVBench benchmark and observe that the question can be answered without requiring any temporal understanding. For quantitative results see Tab. 1. 2017), TGIF-QA (Jang et al., 2017) and ActivityNet-QA (Yu et al., 2019). In general, any openended QA benchmarks can be evaluated following this protocol. As shown in our analysis, Large Language Model (LLM) based evaluations are prone to hallucinations, leading to unreliable conclusions. In contrast, MCQA benefits from more straightforward evaluation process, based on the accuracy score. Multiple-choice question answering. CLEVRER (Yi et al., 2019) assesses reasoning about object interaction in synthetic videos. Perception Test (Patraucean et al., 2024) was introduced to evaluate visual perception in multimodal settings, mainly in indoor scenes. (Bagad et al., 2023) uses synthetic generated data to evaluate the temporal understanding of early video-language models. EgoSchema (Mangalam et al., 2024) focuses on MCQA involving long egocentric videos. Recently, VideoHallucer (Wang et al., 2024) was introduced as first attempt to define video-language benchmark specifically designed for hallucination detection. MVBench (Li et al., 2024) aims to evaluate temporal understanding by defining 20 dynamic tasks specifically designed to require temporal reasoning throughout the entire video. However, our experiments demonstrate that many of these tasks are highly spatial and textual biased, failing to evaluate temporal understanding effectively. We propose new benchmark that requires high level of spatiotemporal understanding across different tasks in order to be solved."
        },
        {
            "title": "3 PROBLEMS IN VIDEO MCQA BENCHMARKS",
            "content": "In this section, we identify two key shortcomings in current video multiple-choice questionanswering benchmarks, as demonstrated on MVBench. First, we show that this benchmark contains strong spatial bias, meaning that questions can be answered without temporal understanding. Secondly, we demonstrate that MVBench also contains strong textual bias, as many questions can be answered without even looking at the visual input. 3.1 DOES TIME MATTER? Video benchmarks must define tasks that cannot be solved using solely spatial information to effectively evaluate the temporal understanding of model. In video MCQA, this means questions should not be answerable using spatial details from single random frame or multiple frames e.g. after shuffling them. However, if no understanding of the sequence of events and temporal local3 Table 1: Spatial bias of the MVBench video-language benchmark. Our evaluation reveals that temporal understanding is not required for solving temporal tasks of MVBench such as fine-grained action. We find that tasks can be solved with random frame, and shuffling videos does not impact the performance of current video-language models. ization is needed, the benchmark fails to assess temporal understanding, focusing only on spatial information which we define as spatial bias. We analyze this spatial bias in MVBench using different state-of-the-art image and video-language models. In Tab. 1 we focus on four different tasks i) scene transition ii) fine-grained pose iii) episodic reasoning and iv) fine-grained action. To assess whether solving these tasks requires temporal understanding, we compare the performance of image-language models, which receive only single image, with video-language models that process both original and shuffled videos. The image-language models demonstrate strong performance across all four tasks in Tab. 1, surpassing random baselines and often matching the capabilities of state-of-the-art video-language models. This is unexpected, as task names like Fine-grained Action suggest need for temporal understanding. For this fine-grained task, the image-model GPT-4o achieves 49%, which is even slightly better than the state-of-the-art model Tarsier, which scores 48.5%. Similarly, for the other three tasks. Overall, GPT-4o achieves an average accuracy across all 20 tasks of 47.8%, which is 20.5% higher than the random performance of 27.3% on MVBench. This indicates that large portion of the benchmark is affected by spatial bias. Additionally, shuffling the videos has minimal impact on the performance of video-language models, indicating that temporal information is not necessary to solve these tasks. Note, as confirmed in Sec. 5.3, the Tarsier model shows significant drop in performance when videos are shuffled for tasks that require temporal understanding. This problem goes beyond these four tasks as shown in Tab. 4, VideoChat2 and Tarsier achieve an average accuracy across all 20 MVBench tasks of 51.1% and 67.7%, respectively. Shuffling video frames causes performance drop of 0.2% and 1.2%, respectively, indicating that the spatial bias affects not only the tasks analyzed in Tab. 1 but the entire dataset. Additionally, VideoChat2 provides the same answer for both the original and shuffled videos 91% of the time, while Tarsier shows agreement on 82% of the questions. In Fig. 2 we show examples corresponding to tasks analyzed in Tab. 1. The first question suggests that temporal understanding is needed as it requires identifying moving objects at the end of the video. Yet, the question can be answered without temporal understanding as no red objects moving or stationary exist in the whole video. The second and third examples are from the Fine-grained Action and Fine-grained Pose tasks respectively. In both cases, the information provided from any frame is enough to correctly answer the question. Problem 1 The MVBench benchmark has strong spatial bias meaning questions can be answered without temporal understanding. 3.2 DOES VISION MATTER? Video benchmarks must be designed to prevent questions from being answered solely through common sense reasoning. Modern LLMs possess strong reasoning skills, which can exploit the informa4 Figure 3: Textual bias of MVBench video-language benchmark. We show different tasks of MVBench and find that questions can be answered without taking the visual part into account. tion within the question and candidate sets in MCQA video language evaluation benchmarks. This introduces substantial textual bias in the results, as models might correctly answer questions without leveraging the video content. To examine the textual bias, we evaluate the performance of text-only LLMs on MVBench, as shown in Tab. 2. Our findings show that LLMs can eliminate incompatible candidates easily, greatly outperforming random model. text-only model achieves even competitive results across four tasks with video-language models without using video input. For example, Llama 3 performs nearly on par with 44.5% compared to the video-language model Tarsier, which achieves 46.5% on the Action Count task. This goes beyond the four tasks, as Llama 3 achieves an average performance across all 20 tasks of 38.1%, which is 10.8% higher than the random chance baseline of 27.3%. We have identified three key sources of this textual bias. Bias from LLM-based QA generation. Collecting and manually annotating large datasets for training and evaluation is very costly. Automatic and semi-automatic collection and annotation processes are commonly used to address this challenge (Li et al., 2024; Mangalam et al., 2024). This includes techniques such as automatic QA pair generation with LLMs. ChatGPT plays fundamental role in QA generation for 11 of the 20 tasks in the MVBench dataset. However, this introduces unrealistic candidates and QA pairs with excessive information. Fig. 3 presents examples of QA pairs that can be resolved merely with text information. Questions 1 and 2 belong to the Action Antonym task, where an LLM is prompted to generate the antonym of the actual action shown in the video. The answers generated are either unrealistic, as one cannot remove something into something, or consistently incorrect, such as not sure. Questions 3 and 4 are from the Unexpected Action task. In this task, the LLM is prompted to generate textual questions and candidates based on the dataset annotations. However, in Question 3, the subject inquired in the question only occurs in the correct answer, while in Question 4, one of the candidates is paraphrased version of the question. Hence, the text-only model is able to identify the correct answer without visual information. Bias from unbalanced sets. Unbalanced QA sets also hinder robust evaluation process. For instance, the correct answer for the Action Count task on MVBench is 3 for 90 out of 200 questions, while 9 is only the correct answer for one question. model with similar bias might get higher results than random by chance. We have observed in our experiments that some text-only models such as GPT-4o have the same bias, predicting 3 for 88 out of 200 samples. This phenomenon makes GPT-4o perform on par with the best video model with an accuracy of 44.0% and 46.5% respectively. Overreliance on world knowledge in questions. Video benchmarks should ensure models cannot rely solely on memorized world knowledge from an LLM to guess answers without using visual input. Even with well-designed questions, models might bypass visual reasoning and rely on prior 5 Table 2: Textual bias of the MVBench video-language benchmark. Our analysis reveals that vision is not required for solving tasks from MVBench as text-only LLMs score high above the random baseline and nearly on par with video models. knowledge to answer correctly. An example of this can be seen in question 5 of Fig. 3. The question does not exhibit an obvious bias in the QA generation, but it can be correctly answered if the model has world knowledge of the TV show from which the question was derived as answer 2 are character names from the House TV show. Problem 2 The MVBench benchmark can be partially solved without visual information due to the bias from LLM QA generation, unbalanced dataset, and overreliance on world knowledge."
        },
        {
            "title": "4 OPEN-ENDED QA TO THE RESCUE?",
            "content": "Contrary to multiple-choice question answering (MCQA), open-ended question answering can be seen as an alternative to solving the aforementioned issues. Without predefined candidate answer set, the model cannot rely on textual information to eliminate implausible candidates. However, open-ended evaluation presents new challenges compared to MCQA. Following Maaz et al. (2023), LLMs have been widely used for the evaluation of open-ended question-answering in datasets such as MSVD-QA (Wu et al., 2017), MSRVTT-QA (Xu et al., 2016) and ActivityNet QA (Yu et al., 2019). Specifically, Maaz et al. (2023) proposed GPT-3.5 as the evaluator model, which makes the entire evaluation process rely on private API model. The evaluation model is prompted to determine if the predicted answer is correct given the question and the ground-truth answer. In addition, the evaluator also computes score to measure the answer quality. We conducted comparative analysis to assess the influence of the evaluation model on the results. Table 3 shows the accuracy and average score for different models on three open-ended datasets, using two evaluators: GPT-3.5 and Llama3-70B. The evaluators produced significantly different results for the same method on the same dataset, with discrepancies of more than 20 points. Specifically, Llama3 highly increases the accuracy of text-only and single-image models, while providing similar or even lower results than GPT-3.5 for video models. It is also evident that Llama3 assigns better metrics to predictions made by the same model. If both the prediction model and the evaluator contain similar biases, the hallucinations in the predictions may be classified as correct responses by the evaluator ignoring the correct answer. These findings raise doubt about the reliability of these evaluations, as different models give completely different results. Moreover, as shown in Tab. 3 open-ended QA does not solve the main issues of MCQA. The performance of text-only models is surprisingly strong; state-of-the-art LLMs can guess the answer solely from the question text for significant number of questions, even without candidate list. This includes questions such as Which hand of the person in black wears watch? or What color is the pants of person wearing black clothes?, which correct answers are Left hand and Black. The first question can be answered just with prior knowledge as people commonly wear the watch on the left hand, while in the second one, the question gives too much information, the person is wearing black clothes. Similar to the findings for MCQA on spatial bias in Sec. 3.1, when using single random frame for image-text models such as GPT-4o, performance reaches 60.6% and 46.4%, approaching the video-language models 80.3% and 61.6%, respectively. In addition, the performance of Tarsier34B does not significantly dropon average less than 3%when the input videos are shuffled, indicating the low temporal understanding required for solving the benchmarks. These results show that open-ended video-language benchmarks also exhibit strong spatial bias, not requiring temporal understanding to be solved. 6 Figure 4: Unreliability of open-ended video-language benchmarks. GPT 3.5 is commonly used as an evaluator of open-ended responses, here we use Llama 3 in text-only setting to generate answers. GPT gives confusing accuracies and scores. Smiley emoji shows truthful or unreliable evaluation from GPT 3.5. Fig. 4 shows incorrect evaluations on ActivityNet QA and MSVD QA. In these experiments, we prompt Llama3 model to answer the questions in text-only setting and perform the evaluation with GPT-3.5. In the first question, it can be seen how the model provides an answer completely unrelated to the video: Cacti, succulents, ocotillo, mesquite, creosote bush, ..., offering general information about desert plants. This is expected as the model only receives the question without visual information. The evaluation model classifies this question as correct with the highest matching score. It seems that the correct answer is disregarded in the evaluation of this example, focusing on the relationship between the questions and the predicted answer. Questions 2 and 3 contain two instances of no/yes QA pairs where the model correctly identifies whether they are correct, but assigns completely contradictory scores. Questions 4 and 5 contain evaluations in which the correct answer is man. However, it can be seen how the text-only model answers with specific names rather than more general concept. The evaluations are completely different, most probably due to context bias, as both Llama3 and the evaluation model GPT3.5 links the concept keyboard with Elton. In summary, current open-ended benchmarks are unreliable due to their use of LLMs as evaluators. This makes them unsuited for evaluating video-language models, especially as they also suffer from spatial and textual bias. In addition, they rely on closed-source LLMs for evaluation, which incurs costs to access, and becomes unreproducible when newer versions are released."
        },
        {
            "title": "5 TVBENCH: A TEMPORAL VIDEO QUESTION ANSWERING BENCHMARK",
            "content": "We propose TVBench, new benchmark specifically created to evaluate temporal understanding in video QA. We adopt multiple-choice QA approach to prevent the problems described in Sec. 4 regarding open-ended VQA evaluation. The main design principles of TVBench are derived from, and address, the problems listed in Sec. 3. These principles can be applied to any video QA benchmark to make it temporally demanding benchmark. Tab. 5 in Appendix provides an overview of the tasks, questions, and answers candidates used in our benchmark. 5.1 DEVELOPING TEMPORAL VIDEO-LANGUAGE BENCHMARK This section explains the key solutions implemented in TVBench to address the issues identified in Sec. 3 of current video MCQA evaluation benchmarks. Defining hard answer candidates. To address Problem 1, the temporal constraints in the question should be essential for arriving at the correct answer. Ignoring these constraints should result in incorrect or random responses. This involves designing time-sensitive questions and challenging candidates that cannot be easily disregarded without temporal understanding, which is essential for accurately evaluating the capabilities of current video-language models. Fig. 5 illustrates our main approach to solving Problem 1. Instead of including random, easy negative candidates, we define hard candidates that cannot be discarded without temporal information. Consequently, single image models and video models fed with shuffled variations of the original video should achieve random performance. We defined 10 temporally challenging tasks that either require repetition 7 Table 3: Unreliability of open-ended video-language benchmark evaluation. Different LLMs used for evaluation produce varying accuracies and scores, see column. Additionally, open-ended benchmarks also exhibit spatial and textual bias, similar to MCQA. MSVD-QA Benchmark Model Input GPT-3.5 text-only text-only single image video shuffle video text-only text-only single image video shuffle video text-only text-only single image video shuffle video Standard deviation between GPT3.5 and Llama3 score differences: 9. Llama3 70B GPT-4o GPT-4o Tarsier-34B Tarsier-34B Lama3 70B GPT-4o GPT-4o Tarsier-34B Tarsier-34B Lama3 70B GPT-4o GPT-4o Tarsier-34B Tarsier-34B Evaluation method Llama3-70B Score 2.8 2.4 3.8 4.1 4.1 2.6 2.3 2.7 3.4 3.4 1.9 1.8 2.9 3.4 3.4 Score Acc. 51.9 44.2 75.3 78.3 78.3 47.8 42.4 50.8 62.7 63.0 32.6 33.9 56.2 60.8 61.3 Acc. 29.1 29.0 60.6 76.7 80.3 23.9 23.3 34.2 63.1 66.4 25.3 27.1 46.4 59.9 61.6 2.6 2.5 3.6 4.0 4.2 2.4 2.3 2.7 3.5 3.7 2.6 2.5 3.2 3.6 3.7 ActivityNet-QA MSRVTT-QA Acc. +22.8 +15.2 +14.7 +1.6 -2.0 +23.9 +19.1 +16.6 -0.4 -3.4 +7.3 +6.8 +9.8 +0.9 -0.3 counting (Action Count), properties about moving objects (Object Shuffle, Object Count, Moving Direction), temporal localization (Action Localization, Unexpected Action), temporal sequential ordering (Action Sequence, Scene Transition, Egocentric Sequence) and distinguishing between temporally hard Action Antonyms such as Standing up and Sitting down. Define questions that are not overly informative. Contrary to LLM-based generation, we apply straightforward templates to mitigate the effect of text-biased QA pairs, mitigating Problem 2. Tab 5 shows how questions and candidates do not provide enough information to give the correct answer solely from text. For most tasks, we keep the same candidate set within the same task. This includes Action Count, Object Count, Object Shuffle, Action Localization, Unexpected Action, and Moving Direction, with each candidate being the correct answer for an equal number of questions, ensuring balanced dataset. This approach allows us to reduce textual bias while keeping the complexity of the visual task. For instance, the model still needs to recognize the unexpected action to localize it, even though the text does not refer to that action. There are always two candidates for Action Sequence and Scene Transition: two actions from the video and two possible sequences of scenes, respectively. Finally, the candidate set for Egocentric Sequence always contains the correct order of actions and random variations changing the order as negative candidates. Omitting questions that require prior knowledge. Solving Problem 2, mitigating the overreliance on world knowledge, requires providing questions and candidates that contain only the necessary information, specifically removing factual information that the LLM can exploit. Pre-existing knowledge should not influence the reasoning process, as this could lead to evaluating different task than visual understanding. It becomes essential for the model to focus on the provided visual information rather than relying on common knowledge to find the correct answer. We remove from our proposal tasks such as Episodic Reasoning from MVBench, that are based on QA pairs about TV shows or movies. These samples cannot be corrected without manual intervention. 5.2 DATASET SOURCE Videos in TVBench are sourced from Perception Test (Patraucean et al., 2024), CLEVRER (Yi et al., 2019), STAR (Wu et al., 2024), MoVQA (Zhang et al., 2023b), Charades-STA (Gao et al., 2017), NTU RGB+D (Liu et al., 2019), FunQA (Xie et al., 2023) and CSV (Qian et al., 2022). QA pairs are generated based on original annotations following the model provided in Apendix. for each task. Overall, TVBench comprises first and third-person perspectives, indoors and outdoors scenes, and real and synthetic data with 2,654 QA pairs among 10 different tasks. 8 Figure 5: Defining temporally hard answer candidates in TVBench. To address the spatial bias in MVBench, we redesign questions and answers to be temporally challenging. Figure 6: TVBench addresses the limitations of MVBench. In TVBench, text-only, image-based, and basic video models perform on par with random chance. Only strong temporal models, such as Tarsier, surpass the random baseline. Notably, shuffling the video leads to significant performance drop on TVBench, whereas it has little impact on MVBench. 5.3 TVBENCH EVALUATION Tab. 4 provides detailed performance breakdown of state-of-the-art text, image, and video models across the 10 TVBench tasks. We also include the average performance of these models on MVBench and TVBench, with the upward arrow indicating the improvement over random chance. summary of our results can be found in Fig.6. Does time matter? For TVBench, single-image models perform at random chance levels, suggesting random frame is insufficient for accurate question answering. Notably, GPT-4o, the top image-language model on TVBench, outperforms random chance by only 2.5%, compared to 20.5% improvement on MVBench. Shuffling videos has minimal impact on the performance of video-language models on MVBench, but significantly degrades their accuracy on TVBench, where it drops to near-random levels. For example, Tarsier-34Bs accuracy is 33.9% higher than the random baseline on MVBench when videos are shuffled, while on TVBench, it is only 4.7% higher under the same conditions. This suggests that temporal context is crucial for TVBench, where visual data alone is insufficient to outperform random chance, unlike MVBench. In addition, we analyze the effect of reversing videos instead of shuffling them. This significantly degrades model performance on TVBench, resulting in accuracy below the random baseline for even the best models. For instance, Tarsier-7B and Tarsier-34B perform worse than random by 6.1% and 4.9%, respectively. This is expected, as top models on TVBench rely on temporal information. Reversing frames misleads them with incorrect cues while shuffling disrupts the temporal structure entirely. These results demonstrate that temporal understanding is crucial for TVBench which is in contrast to MVBench where reversed and non-reversed videos obtain the same performance. Does vision matter? Tab. 4 also presents the results of state-of-the-art text-only models, which perform at random levels on TVBench, highlighting the effectiveness of our solutions for Problem 2. Notably, Llama 3 achieves the best performance, just 1.4% above random chance on TVBench, whereas it performs 10.8% better on MVBench. This indicates that LLMs cannot determine the answer solely by analyzing the question and answer candidates or by relying on prior world knowledge. Thus, visual information becomes key for solving TVBench. 9 Table 4: Results on TVBench. On TVBench, text-only and image models perform near-random. Surprisingly, several recent state-of-the-art video-language models, such as ST-LLM, also perform close to random. With TVBench we can identify temporally strong models like Gemini and Tarsier. In addition, these models drop significantly in accuracy when the videos are shuffled or reversed. TVBench MVBench TVBench Average 27.3 35.07.7 34.87.5 38.110.8 44.216.9 47.820.5 51.023.7 46.719.4 49.822.5 54.927.6 53.426.1 53.926.6 46.619.3 45.818.5 46.519.2 50.122.8 48.421.1 49.522.2 58.130.8 56.228.9 56.729.4 62.635.3 62.535.2 56.929.6 67.640.3 67.740.4 61.233.9 58.731.4 60.533.2 Average AC OC AS OS ST AL AA UA ES MD 25.0 25.0 50.0 33.3 50.0 25.0 50.0 25.0 25.0 25.0 33.3 27.2 18.2 44.9 32.0 53.5 26.9 45.9 29.2 26.5 26.7 33.10.2 28.0 21.0 48.7 33.3 53.5 25.6 50.9 25.8 28.5 22.4 33.80.5 30.2 27.0 48.7 32.9 55.1 26.9 49.1 23.3 25.5 28.0 34.71.4 27.1 23.0 56.8 36.9 49.2 25.6 52.2 29.2 25.5 19.8 34.51.2 30.8 19.6 62.1 33.3 52.4 25.0 52.5 27.5 33.0 21.6 35.82.5 25.9 21.6 51.7 33.3 48.1 23.1 44.7 34.2 25.0 22.8 33.00.3 28.2 22.3 50.6 33.3 38.4 23.1 45.6 33.3 23.0 22.8 32.11.2 25.6 27.0 54.0 32.9 56.2 23.1 48.1 33.3 24.5 22.4 34.71.4 25.0 35.1 49.2 36.0 54.4 31.0 45.6 32.4 24.0 20.3 35.32.0 25.4 32.4 55.1 36.9 43.6 30.5 47.5 31.4 23.5 20.3 34.71.4 25.6 35.1 50.8 36.9 49.2 31.5 45.6 32.4 23.5 19.4 35.01.7 32.1 25.7 52.1 33.3 52.4 23.8 53.1 27.5 20.5 21.6 34.20.9 32.6 29.1 53.4 33.3 48.6 24.4 48.1 28.3 22.0 21.6 34.10.8 34.0 29.1 51.5 33.3 49.7 24.4 49.7 28.3 22.5 21.6 34.41.1 37.3 24.3 57.6 33.3 55.1 28.1 47.8 35.0 19.5 17.2 35.52.2 36.9 27.7 51.9 33.3 52.4 25.0 46.6 34.2 19.5 19.0 34.61.3 36.8 25.0 55.5 33.3 51.9 27.5 45.9 35.0 21.0 19.0 35.11.8 27.6 32.4 64.8 35.6 77.8 44.4 58.8 31.7 27.0 19.4 41.98.6 26.1 27.7 51.7 35.1 33.0 26.9 54.1 29.2 26.5 23.7 33.40.1 25.9 29.1 58.5 35.1 56.2 34.4 55.9 28.3 25.0 23.7 37.23.9 45.812.5 22.9 58.8 70.1 35.6 64.9 46.9 75.6 32.5 25.5 25.0 24.3 19.6 41.1 36.0 38.9 24.4 29.4 30.0 24.5 15.9 28.44.9 36.02.7 21.1 38.5 54.5 38.2 53.5 30.0 55.6 27.5 24.0 16.8 53.820.5 31.7 64.2 77.1 32.1 77.8 58.1 84.4 40.0 24.5 48.3 30.2 25.0 45.1 32.9 31.9 21.9 19.1 38.3 23.0 4.3 27.26.1 30.2 35.8 59.8 34.7 52.4 35.6 55.6 35.0 23.0 18.1 38.04.7 41.58.2 30.6 52.0 60.2 36.9 69.2 40.0 53.1 28.3 16.0 28.5 46.513.2 33.5 22.3 70.4 34.5 82.6 51.6 77.8 33.3 25.0 33.6 Model Random GPT-3.5 Turbo GPT-4o Llama 3 70B Idefics3 GPT-4o VideoChat2 ST-LLM PLLaVA-7B PLLaVA-13B PLLaVA-34B Tarsier-7B Tarsier-34B VideoGPT+ Gemini 1.5 Pro Input text-only image video reverse shuffle video reverse shuffle video reverse shuffle video reverse shuffle video reverse shuffle video reverse shuffle video reverse shuffle video video"
        },
        {
            "title": "6 DISCUSSION",
            "content": "A sobering view on current models. With our new TVBench, we can accurately assess the temporal understanding of existing video-language models. Surprisingly, we find that most recent state-ofthe-art and highly popular models, such as VideoChat2, ST-LLM, PLLava, and VideGPT+, perform close to random chance on our temporal benchmark, with the largest gain being only +8.6%. Only the Tarsier and Gemini 1.5 Pro models outperform the random baseline, achieving +20.5% and +13.2%, respectively. The primary distinction between Tarsier and previous open-sourced models lies in its substantial increase in pretraining data, leveraging 3.8 million vision-text samples, of which only 1 million are images. From these results, we observe that TVBench amplifies the performance gaps between models with the strongest temporal understanding and those with weaker capabilities, as good benchmark should. Conclusion. In this work, we highlight major limitations in existing language-video benchmarks, particularly in the widely used MVBench. Key issues include inadequate temporal evaluation and tasks that do not require visual information, making it ineffective for tracking progress in this domain. To address these problems, we introduce TVBench, benchmark designed to explicitly assess the temporal understanding of video-language models. Our experiments reveal that on TVBench, text-only and visual models lacking temporal reasoning perform at random levels, and only few current models achieve moderately high scores, showing the potential for progress. TVBench thus provides reliable yardstick for evaluating future advancements in video-language models. 10 Acknowledgement This research was partially funded by the Spanish Ministerio de Ciencia Innovacion (grant number PID2023-149549NB-I00), and the Galician Consellerıa de Cultura, Educacion Universidade (grant numbers ED431C 2018/29 and ED431G2019/04). These grants are co-funded by the European Regional Development Fund (ERDF). It is also financially supported by Qualcomm Technologies Inc., the University of Amsterdam, and the Top Consortia for Knowledge and Innovation (TKIs) allowance from the Netherlands Ministry of Economic Affairs and Climate Policy."
        },
        {
            "title": "REFERENCES",
            "content": "Piyush Bagad, Makarand Tapaswi, and Cees GM Snoek. Test of time: Instilling video-language models with sense of time. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 25032516, 2023. Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. Pradipto Das, Chenliang Xu, Richard Doell, and Jason Corso. thousand frames in just few words: Lingual description of videos through latent topics and sparse object stitching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 26342641, 2013. Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pp. 52675275, 2017. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pp. 58425850, 2017a. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017b. Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatioIn Proceedings of the IEEE conference on temporal reasoning in visual question answering. computer vision and pattern recognition, pp. 27582766, 2017. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024. Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex Kot. Ntu rgb+ 120: large-scale benchmark for 3d human activity understanding. IEEE transactions on pattern analysis and machine intelligence, 42(10):26842701, 2019. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 11 Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36, 2024. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. Yicheng Qian, Weixin Luo, Dongze Lian, Xu Tang, Peilin Zhao, and Shenghua Gao. Svip: Sequence verification for procedures in videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1989019902, 2022. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 45664575, 2015. Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 45814591, 2019. Yuxuan Wang, Yueqian Wang, Dongyan Zhao, Cihang Xie, and Zilong Zheng. Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models. arXiv preprint arXiv:2406.16338, 2024. Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024. Zuxuan Wu, Ting Yao, Yanwei Fu, and Yu-Gang Jiang. Deep learning for video classification and captioning. In Frontiers of multimedia research, pp. 329. 2017. Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, and Ziwei Liu. Funqa: Towards surprising video comprehension. arXiv preprint arXiv:2306.14899, 2023. Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pp. 16451653, 2017. Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 67876800, 2021. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging In Proceedings of the IEEE conference on computer vision and pattern video and language. recognition, pp. 52885296, 2016. Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pre-trained image-text model to video-language alignment. In The Eleventh International Conference on Learning Representations, 2023. Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019. Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynetqa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 91279134, 2019. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023a. Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, Limin Wang, and Yu Qiao. Movqa: benchmark of versatile question-answering for long-form movie understanding. arXiv preprint arXiv:2312.04817, 2023b."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 REPRODUCIBILITY STATEMENT To ensure the reproducibility of our results, we will make the full dataset publicly accessible in raw form, along with comprehensive documentation detailing its structure and usage. Additionally, we will provide open-sourced evaluation code under permissive license, enabling researchers to replicate our experiments and adapt the tools for their own studies. A.2 OVERVIEW OF TVBENCH Tab. 5 provides the template used to generate the QA pairs for each task on TVBench. For the tasks Action Count, Object Count, Object Shuffle, Action Localization, Unexpected Action, and Moving Direction, we use the same set of options for every question. For Action Sequence and Scene Transition, we generate candidates by selecting an action and scene from the video to create negative candidates. The Action Antonym candidate set consists of two similar actions that are difficult to distinguish without time understanding, such as Wear jacket and Take off jacket. Finally, for Egocentric Sequence, we generate random modifications to the correct sequence of actions to create three negative candidates. The table also details the datasets from which videos for each task were sourced. Table 5: Overview of all tasks and datasets used in our TVBench benchmark. Task Action Count Object Count Action Sequence Object Shuffle Scene Transition Source Perception Question The person makes sets of repeated actions. How many distinct repeated actions did the person do? Candidates 2; 3; 4; 5 CLEVRER How many {metal} objects are moving when {the video begins}? 0; 1; 2; 3 STAR What did the person do first? Perception The person uses multiple similar objects to play an occlusion game. Where is the hidden object at the end of the game from the persons point of view? MoVQA Whats the right option for how the scenes in the video change? Action Localization Charades During which part of the video does the action {person takes blanket} occur? 2 actions that actually happened on the video Under the third object from the left. Under the second object from the left. Under the first object from the left. From {scene1} to {scene2}. From {scene2} to {scene1}. Throughout the entire video. At the end of the video. In the middle of the video. At the beginning of the video. Action Antonym NTU RGB+D What is the action being performed in the video? Wear jacket; Take off jacket Unexpected Action Egocentric Sequence Moving Direction FunQA Locate the {creativeamusingmesmerizing} part of the video. CSV What is the sequence of actions shown in the video? CLEVRER Which direction does the {gray cube} move in the video? Throughout the entire video. At the end of the video. At the beginning of the video. In the middle of the video. GT + Correct sequence changing the order of actions Down and to the right. Down and to the left. Up and to the right. Up and to the left. A.3 TVBENCH QUALITATIVE EXAMPLES This section provides qualitative examples for the different TVBench tasks. As shown in Fig.7, 8, and 9, temporal information from the input video is indispensable for correctly answering the given questions. 13 Figure 7: TVBench: Samples from our benchmark (1). Row 1-2: Action Antonym; Row 3-4: Action Count; Row 5-6: Action Localization; Row 7-8: Action Sequence. 14 Figure 8: TVBench: Samples from our benchmark (2). Row 1-2: Object Count; Row 3-4: Moving Direction; Row 5-6: Object Shuffle. 15 Figure 9: TVBench: Samples from our benchmark (3). Row 1-2: Scene Transition; Row 3-4: Unexpected Action. SPATIAL/TEXTUAL BIAS IN MVBENCH Fig. 10-21 contain several evaluation examples from MVBench that suffer from the issues analyzed in Sec. 3. This qualitative analysis complements the quantitative results discussed in Sec 6 showing that these issues affect large portion of MVBench. 16 Figure 10: Spatial bias in MVBench (1). Multiple-choice questions from various MVBench tasks can be solved using only single frame from the video. 17 Figure 11: Spatial bias in MVBench (2). Multiple-choice questions from various MVBench tasks can be solved using only single frame from the video. 18 Figure 12: Spatial bias in MVBench (3). single frame containing all characters inquired is sufficient to answer MVBench. 19 Figure 13: Spatial bias in MVBench (4). Temporal understanding is not needed as single frame suffices to solve these questions on MVBench. 20 Figure 14: Spatial bias in MVBench (5). Temporal understanding is not needed as single frame suffices to solve these questions on MVBench. 21 Figure 15: Spatial bias in MVBench (6). Temporal understanding is not needed as single frame suffices to solve these questions on MVBench. 22 Figure 16: Spatial bias in MVBench (6). single frame is sufficient to discard incorrect candidates, as the scenes described in these candidates never occurred in the video. 23 Figure 17: Textual bias in MVBench (1). Answer candidates are generated with an LLM. The Not sure candidate is never correct, while the other incorrect candidate makes no textual sense e.g. catching something against something. 24 Figure 18: Textual bias in MVBench (2): Overreliance on world knowledge. Answers can be inferred using world knowledge of TV shows. Questions cannot be answered from the video only as answers contain e.g. character names of the TV show. 25 Figure 19: Textual bias in MVBench (3). Some candidates in MVBench contain artifacts due to the automatic LLM-based generation process. In these cases, the correct answers end with 1 or contain the keyword true. Figure 20: Textual bias in MVBench (4). Since two of the candidate objects cannot be eaten, the choice is reduced to two remaining candidates. 26 Figure 21: Textual bias in MVBench (5). Just one candidate answer is provided for some QA pairs on MVBench."
        }
    ],
    "affiliations": [
        "CiTIUS, University of Santiago de Compostela",
        "Fundamental AI Lab, University of Technology Nuremberg",
        "QUVA Lab, University of Amsterdam"
    ]
}