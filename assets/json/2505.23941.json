{
    "paper_title": "Vision Language Models are Biased",
    "authors": [
        "An Vo",
        "Khai-Nguyen Nguyen",
        "Mohammad Reza Taesiri",
        "Vy Tuong Dang",
        "Anh Totti Nguyen",
        "Daeyoung Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) memorize a vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, board games, optical illusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 1 4 9 3 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Vision Language Models are Biased",
            "content": "An Vo KAIST an.vo@kaist.ac.kr Khai-Nguyen Nguyen College of William and Mary knguyen07@wm.edu Mohammad Reza Taesiri University of Alberta mtaesiri@gmail.com Vy Tuong Dang KAIST vydang@kaist.ac.kr Anh Totti Nguyen Auburn University anh.ng8@gmail.com Daeyoung Kim KAIST kimd@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) memorize vast amount of prior knowledge from the Internet that help them on downstream tasks but also may notoriously sway their outputs towards wrong or biased answers [3, 50, 11]. In this work, we test how the knowledge about popular subjects hurt the accuracy of vision language models (VLMs) on standard, objective visual tasks of counting and identification. We find that state-of-the-art VLMs are strongly biased (e.g., unable to recognize fourth stripe has been added to 3-stripe Adidas logo) scoring an average of 17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo) across 7 diverse domains from animals, logos, chess, boardgames, optical illusions, to patterned grids. Insert text (e.g., Adidas) describing the subject name into the counterfactual image further decreases VLM accuracy. The biases in VLMs are so strong that instructing them to double-check their results or rely exclusively on image details to answer improves counting accuracy by only +2 points, on average. Our work presents an interesting failure mode in VLMs and an automated framework for testing VLM biases. Code and data are available at: vlmsarebiased.github.io"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are trained on the Internet data and therefore learn vast amount of prior knowledge that (a) help them on downstream tasks but (b) sometimes sway their answers towards wrong or biased choices [3, 50]. Interestingly, LLMs also memorize visual knowledge from its colossal text-only corpus [49], e.g., the US national flag has 50 stars and 13 stripes or dogs have four legs  (Fig. 1)  . Because vision language models (VLMs) are built by pre-training LLMs either exclusively on text data (i.e., for late fusion with vision encoders) [31, 5] or on mix of text, image, and multimodal data in an early fusion manner [54], they may inherit strong biases from the text corpus when answering visual questions [29, 32, 28, 15]. Prior evidence [16, 28, 32] showing VLMs are biased were exclusively on artificial Y/N questions that often directly contain the biased statement, e.g., Is the mouse smaller than the cat? [32], which is framed to contradict their counterfactual (CF) image where the cat is smaller. Therefore, it is unclear (1) how much the image contributes to VLMs wrong answers or it is solely the textual prompt; (2) how such biases impact standard, objective visual tasks with neutral, unbiased prompts. In this work, we aim to evaluate how the knowledge of LLMs about popular subjects (e.g., dogs and the US flag) negatively impact the accuracy of VLMs on objective visual questions of object counting, identification (Q1 & Q3 in Fig. 2) and low-level visual tasks (e.g., measuring whether two Equal contribution. Equal advising. Preprint. lines are parallel; Fig. 1f). For example, we provide CF image of 3-legged chicken and ask VLMs How many legs does this animal have? (Fig. 1a). Examples of VLM failures across 7 domains of VLMBias How many legs does this animal have? Answer with number in curly brackets, e.g., {9}. How many points are there on the star in the logo of this car? Answer with number in curly brackets, e.g., {9}. How many stripes are there in this flag? Answer with number in curly brackets, e.g., {9}. How many chess pieces are there on this board? Answer with number in curly brackets, e.g., {9}. How many rows are there on this board? Answer with number in curly brackets, e.g., {9}. Are the two horizontal lines parallel? Answer in curly brackets, e.g., {Yes} or {No}. How many circles are there in cell C3? Answer with number in curly brackets, e.g., {9}. a. b. c. d. e. f. g. 2 2 4 2 2 3 3 3 3 3 Bias GT 2 3 3 4 Gemini-2.5 Pro 13 13 13 13 13 13 14 32 32 32 32 32 32 31 Sonnet-3. 9 9 9 9 9 9 10 GPT-4.1 Yes No Yes Yes Yes Yes No o3 3 2 3 3 3 3 2 o4-mini Figure 1: VLMs fail on 6 counting tasks (ae & g) and one low-level vision task (f). Leveraging state-of-the-art (SOTA) image editors, VLMs, and image processing libraries, we propose VLMBias, framework for automating the enumeration and generation of biased subjects, questions, and counterfactual images. We manually review all generated images and reject those that are deemed low-quality or debatable. We test VLMs on questions spanning seven diverse subjects in the decreasing order of popularity: (a) animals ; (e) game , (b) logos boards (see Sec. 3). For all subjects, the tasks ; and (g) patterned grids questions, which were originally are counting and object identification, except for optical illusion designed to test human low-level vision (e.g., identifying whether two circles are of the same size). ; (f) optical illusion ; (d) chess pieces ; (c) flags We test five SOTA VLMs: 3 thinking models of [37]; and 2 non-thinking models of Sonnet-3.7 [4], Gemini-2.5 Pro [13], o3 [37] o4-mini GPT-4.1 [36]. Our key findings are: 1. All five VLMs recognize the VLMBias subjects from the original, unmodified image, scoring 100% accuracy on both identification and counting questions (Fig. 2a) (Sec. 4.1). 2. VLMs struggle to count animal legs when one extra leg is added to 2-legged (birds; Fig. 1a) and 4-legged animals (1.01% and 2.50% accuracy, respectively; Sec. 4.2). 3. When logos of famous car and sportswear brands are modified to have one more or one fewer of the famous visual elements (e.g., stripes on the Adidas logo; Fig. 2b), VLMs struggle to count these elements. Their answers are extremely biased (0.44% accuracy) on CF car logos and slightly less biased on shoe logos (17.57% accuracy) (Appendix L.1). Similarly, VLMs fail to (a) detect the number of stripes and stars in the CF versions of popular flags chess on chessboard when piece is replaced or removed (Sec. 4.3); and (c) count the rows and columns of modified board (Appendix L.2); (b) count the chess pieces of famous board games (Appendix L.4). 4. On optical illusions (e.g., Ebbinghaus; Fig. 6), all VLMs accurately predict the names of the original, well-known illusions but unable to detect that the illusion graphics together with the groundtruth answers have changed, scoring around random chance (Sec. 4.4). 2 5. Unlike the above cases, we test VLMs on the patterned grid task where no prior information (e.g., famous illusions or logos) of the image exists on the Internet. In this grid, all cells follow pattern except for one cell about which we will question VLMs. VLMs perform poorly, failing to detect the subtly-changed cell and answer based on the rules implied from the surrounding cells (Sec. 4.5). 6. To confirm VLM failures to counting (Q1 & Q2) are due to their visual bias, we further test VLMs on Y/N identification questions (Fig. 2; Q3) but they also similarly struggle to answer (Sec. 4.6). In another experiment where the subject name (e.g., Adidas) is added to each CF image (e.g., 4-striped logo), VLM counting accuracy further drop by -2 to -6 points, confirming the bias learned from the text corpus influences its counting (Sec. 4.7). 7. Instructing VLMs to rely only on the visual details in the image alone to answer or to double-check the result in 2nd-turn message improves their counting accuracy by at most +2 points only, confirming the severe bias of the SOTA VLMs (Sec. 4.8)."
        },
        {
            "title": "2 Related work",
            "content": "Bias in LLMs and VLMs LLMs exhibited biases across various domains, including social [52, 22], cultural [30, 35, 1, 58], demographic [65, 27], political [6, 43], cognitive [8, 26], and biases related to specific names, numbers, or values [63, 25]. These biases often correlate with the over-represented associations between textual cues and specific classes or attributes (e.g., associating older people with forgetfulness) [40] in the pretraining data. Biases are not limited to textual data but extend into the visual domain. VLMs also exhibit gender biases [17, 60, 19, 10], stereotypical portrayals [46, 24, 45], and social biases [20, 47]. In our work, we investigate VLM bias in visual question answering (VQA), specifically, in cases where the visual cues in an counterfactual image strongly bias VLMs answers towards knowledge commonly known on the Internet, effectively ignoring the counterfactual (CF) modifications, resulting in inaccurate predictions  (Fig. 2)  . Table 1: Our VLMBias presents natural, objective counting and identification questions while prior benchmarks insert biased statements into the prompt. Benchmark PhD-ccs [32] VLind-Bench [28] HallusionBench [15] VLMBias (ours) Biased prompt Biased image CF images Generation method Adversarial injection Top leaderboard Question types DALL-E In-prompt 2,576 DALL-E 181 manual n/a n/a GPT-4o 81.2% GPT-4o 89.4% GPT-4V 31.4% Y/N Y/N Y/N 1,392 automated , In-image Title o4-mini 20.25% Counting (Q1, Q2) Y/N (Q3) Visual Hallucination Benchmarks Many prior attempts tested VLMs on visually ambiguous images [32, 23, 56], optical illusion [15, 59], CF images [28, 15] and counter-commonsense images [32, 28, 7, 66]. The most relevant benchmarks [32, 28, 15] (see Tab. 1) have three main drawbacks: (1) they primarily incorporate biased textual cues (e.g., directly mentioning entity names) in the questions to provoke LLM hallucination; (2) they use only Yes/No questions, which limit them to artificial questions instead of objective downstream tasks such as counting in our work; and (3) they do not study the effects of in-image adversarial injection. Among these, the Visual Dependent subset of HallusionBench [15] is the most similar to VLMBias. However, they still have the above limitations and rely entirely on humans to manually edit images to produce 181 CF images. In contrast, we automate the CF-image generation process and humans only review the generated images. VLMBias addresses these limitations by (1) inserting bias cues into the image keeping the prompt neutral; (2) using counting questions, which are objective and challenging to VLMs [44]; and (3) optionally, injecting extra bias cues as text into the image. Furthermore, VLMBias is fully automated on 6 out of 7 tasks: LLMs to generate ideas; Python scripts to generate abstract images; and SOTA text-to-image models (Gemini-2.0 Flash & GPT-4o) to produce photo-realistic images. 3 Figure 2: Given subject (e.g., Adidas logo), we first confirm that all VLMs have sufficient knowledge about the subject via an ID and counting sanity-check questions (a). Then, we test VLMs on the counterfactual image (b) and report its accuracy on the counting (Q1 & Q2) and an Y/N identification task (Q3). For all tasks, we test the hypothesis that the visual bias cues in the background (c) may be so strong that it cause VLMs to ignore the modified object and default to biased answers."
        },
        {
            "title": "3 The VLMBias Benchmark",
            "content": "VLMBias evaluates VLMs visual bias by presenting pair of counting question and subtly modified versions of well-known objects (e.g., changing the Adidas logo from 3-striped to 4-striped; Fig. 2c). We choose the counting task as it is generic, objective visual question that does not contain specific biased statements or subjects. We test whether the visual bias cues in the background is so strong that it will make VLMs default to biased answers and ignore the modifications. Taxonomy To test VLM biases, we choose 7 unique topics of decreasing popularity, i.e., from common animals that we create and did not exist before. For each topic, we generate images from scratch. The generated images are photo-realistic for 2 topics and abstract for the rest of 5 topics. to optical illusions, and novel visual pattern , then logos (1) Photo-realistic images are used in 2 tasks: most common subjects, natural ( text-to-image generators ( ) and man-made ( Gemini-2.0 Flash image generation and GPT-4o). animals and logos. These images cover the ). They are created and modified by SOTA optical illusions, patterned grids. These images are created using code, not text-to-image models. We divide this game patterned grid (2) Abstract images are used in 5 tasks: and category into three sub-categories: (a) well-known objects, including boards; (b) that did not exist before. optical illusions, which are less common than flags; and (c) novel game boards, chess pieces, chess pieces, flags, flags, Controls To minimize the language bias in the prompt, we use two different prompts per test image, written in neutral, descriptive terms (e.g. stylized curves for Nike swooshes). Each test image is re-scaled to pixels where {384, 768, 1152}. In each task, we ask 3 questions, e.g., our three questions (two counting & one identification) in the animal-leg task are (Fig. 2b): Q1: How many legs does this animal have? Answer with number in curly brackets, e.g., {9}. Q2: Count the legs of this animal. Answer with number in curly brackets, e.g., {9}. Q3: Is this an animal with 4 legs? Answer in curly brackets, e.g., {Yes} or {No}."
        },
        {
            "title": "3.1 Task 1: Counting animal legs when an extra leg is added",
            "content": "Pretrained on the Internet data, VLMs must have colossal prior knowledge of the count of animal legs from both textual and image data. Following this hypothesis, we generate images of well-known animals but with one extra leg (e.g., 3-legged birds or 5-legged dogs) and ask VLMs to count legs. Images We use three-step image generation process: (1) We obtain list of 100 well-known animal, we ask Gemini-2.0 Gemini-2.0 Flash to add one o4-mini; (2) For each Flash to generate their side-view images; (3) Then, we instruct animals with two or four legs using 4 extra leg to each image in Step 2. We then manually filter these images to retain one high-quality image of animals with clearly either 3 or 5 legs. The final set consists of 91 different animals: 23 three-legged birds and 68 five-legged mammals. We create three resolution variants for each animal image with size {384, 768, 1152}px. Specifically, for an original generated image with dimensions H, we resize both dimensions by the scaling factor max(W,H) to preserve the original aspect ratio. This procedure generates 91 animals 3 resolutions = 273 images in total. C"
        },
        {
            "title": "3.2 Tasks 2-5: Counting visual elements in modified familiar patterns:",
            "content": "logos, flags, chess pieces, and game boards Our primary hypothesis is that VLMs contain strong bias between brands logo and its famous visual representations (e.g., an Adidas logo must have 3 stripes; Fig. 2). Expanding beyond animal legs, we test this hypothesis across four domains where humans (and potentially VLMs) have wellestablished visual expectations: logos of famous brands ( ), ). For each domain, we create CF images by making systematic, minimal and game boards ( modifications to familiar visual elements, using the same methodology as Task 1 ( ) or Python scripts ( ) with all images rendered at three resolutions (384, 768, and 1152 pixels). ), national flag ( ), chess pieces ( , , Images For logos (Appendix D), we modify graphical features (points, prongs, circles, stripes, GPT-4o, placing them in curves) of three car brands and two shoe brands using realistic contexts (vehicles and athletic footwear) for total of 207 images. For flags (Appendix E), we systematically add or remove one element (stars or stripes) from 20 flags, creating 120 flag images. For chess pieces (Appendix F), we test pattern recognition by removing or replacing exactly one piece in western chess and xiangqi starting positions, generating 144 chessboard images. For game boards (Appendix G), we add or remove exactly one row and column across four game types (chess, xiangqi, Sudoku, Go), producing 84 images of the board of these games. o4-mini and"
        },
        {
            "title": "3.3 Task 6: Visual testing on original and modified optical illusion patterns",
            "content": "Recent VLMs show improved performance on optical illusion [62, 15] tasks, with o4-mini achieving 71.49% accuracy on IllusionVQA [48]. However, these VLMs might have merely memooptical illusions rather than truly perceiving visual information. To investigate rized the common this hypothesis, we create two scenarios: (1) original optical illusions (e.g., the Ebbinghaus illusion where two identical central circles appear different sizes due to surrounding context circles) and (2) modified versions with similar visual setups but reversed effects (e.g., where one central circle is actually larger than the other; Fig. 6). When tested on these modified illusions, VLMs often incorrectly claim the circles are equal (i.e. the answer true for the original illusion but false for the modified version), suggesting strong bias toward memorized knowledge. Images We use six classical optical illusions [33]: Müller-Lyer [34, 21], Zöllner [67, 57], Ebbinghaus [55, 2], Vertical-Horizontal [9, 18], Pogendorff [41, 14], and Ponzo [42, 61]. For five of these illusions, we generate 24 images per type (12 original and 12 modified versions with varying illusion strength). The Vertical-Horizontal illusion, which uses fixed T-shape that cannot vary in strength, we create only 12 images (6 original and 6 modified). Each image is rendered at three different resolutions: 384, 768, and 1152 pixels. This approach yielded (24 5 + 12) 3 = 396 images in total. More details in Appendix H."
        },
        {
            "title": "3.4 Counting the circles or lines in an anomaly cell among a patterned grid",
            "content": "VLMs can infer the patterns from nearby visual elements to answer visual questions [23]. We test how much the overall pattern in an image biases its answer to question about an anomaly region that does not obey the pattern. Images We generate two types of gridsdice and tally gridswith dimensions G, where ranges from 6 to 12. Cells in dice grids contain circles (Fig. 1 , Fig. 23ab), while tally grids use tally marks (Fig. 23cd). All grids follow symmetric visual pattern where the number of shapes in each cell increases from 1 at the edges towards the center, which contains (G + 1)/2 shapes, based on the cells distance from the nearest edge. For each generated grid image, we introduce an anomaly by modifying only single, strategically chosen cell (avoiding edges & corners). The modifications"
        },
        {
            "title": "VLMs are unable to see an extra leg in the puma and an extra stripe in the Adidas logo",
            "content": "(a) (b) Q1: How many legs does this animal have? Answer with number in curly brackets, e.g., {9}. Q3: Is this an animal with 4 legs? Answer in curly brackets, e.g., {Yes} or {No}. (c) (d) (e) Q1: How many visible stripes are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9}. (f) Q3: Are the logos on these shoes Adidas logos? Answer in curly brackets, e.g., {Yes} or {No}. (a) original Puma (Q1) (b) CF Puma (Q1) (c) CF Puma (Q3) (d) original Adidas (Q1) (e) CF Adidas (Q1) (f) CF Adidas (Q3) 4 4 4 4 4 4 4 4 4 4 5 Yes Yes Yes Yes Yes No Gemini-2.5 Pro Sonnet-3. 3 3 3 3 3 3 GPT-4.1 3 3 3 4 3 4 o3 Yes Yes Yes Yes Yes No o4-mini GT Figure 3: VLMs fail to detect subtle changes in counterfactuals (CF) and default to biased answers. depend on the grid type: in tally grids, we either add one extra tally mark or remove one existing tally mark from this single anomaly cell; in dice grids, we either remove single circle or replace one circle with different geometric shape (e.g., triangle, square, star) within this single anomaly cell. Each resulting image is rendered at three resolutions: 384, 768, and 1152 pixels. To create diverse scenarios for single-cell anomaly placement, for each of the 7 grid dimensions, we define two distinct settings by choosing different single cell to be anomaly, resulting in 14 unique base scenarios (7 dimensions 2 choices of single anomaly cell location). This systematic generation then yields total of 2 grid types (dice, tally) 2 modification types per grid type 14 unique base scenarios for single-cell anomaly placement 3 resolutions = 168 distinct images, each featuring one anomaly cell per grid. More details in Appendix I."
        },
        {
            "title": "4.1 Sanity check: VLMs do recognize familiar visual subjects on original, unmodified images",
            "content": "Here, we first verify that the subjects in our VLMBias are, in fact, known to VLMs in their original form. If VLMs fail to recognize the concepts in these unaltered images, there is no basis to attribute the their failures on modified images to their bias. logos, Sonnet-3.7, Experiments We evaluate five VLMs ( o3, Gemini-2.5 Pro, o4-mini) using their default settings on set of 66 unmodified images spanning our 6 out and patterned animals, of 7 VLMBias tasks ( from the sanity check since the patterns are created from scratch and do not grids). We exclude exist on the Internet. For five counting tasks (from to ), we ask two questions (identification and counting; Fig. 2a) per image for total of 132 questions. Since the optical illusion is not counting task (Fig. 1 ), we replace the counting question with question asking for the known questions and answers associated with each illusion. That is, we provide an image per illusion type to VLMs (e.g., Fig. 6) and ask VLMs to identify: (1) the name of the illusion; and (2) the question & correct answers associated with this famous illusion (see the sanity-check prompts in Appendix N). board game grids, chess pieces, GPT-4.1, flags, Results All five VLMs achieve 100% accuracy on all the questions. That is, for counting tasks, VLMs correctly recognize the subjects and the expected counts (e.g., puma has four legs and the Adidas logo has three stripes; Fig. 3a&d). For all 6 illusion types, VLMs are able to identify the 6 Figure 4: On the counterfactual images in VLMBias, five VLMs mostly output answers that match the biased choices that we predefine for each question, 75.70% of the time, on average. This biased behavior is the most severe on the leftmost 6 tasks where there are existing prior is the only task where the visual pattern is created from knowledge on the Internet. Patterned grid scratch in this work. Yet, VLMs still are biased 43.45% of the time."
        },
        {
            "title": "Mean",
            "content": "name (e.g., Ebbinghaus illusion in Fig. 6), the associated question (Are the two inner circles equal in size?) and its correct answer (Yes). The results here set the ground for the claims in subsequent sections that VLMs low accuracy on counterfactual images (17.05% accuracy; see Tab. 2) stem from their prior knowledge about the subjects. Table 2: All VLMs achieve 100% on identification and counting tasks with unmodified images, showing that they fully recognize the original version but fail on the counting questions on the modified images (i.e., counterfactuals) in VLMBias. The mean accuracy of five state-of-the-art VLMs o4-mini achieves the highest accuracy (20.25%) which however on our seven tasks is 17.05%. is still low. VLMs with thinking capabilities ( o3) only slightly outperform nonthinking models ( Gemini-2.5 Pro, Sonnet-3.7, GPT-4.1). o4-mini, Model Accuracy in counting questions (Q1 &Q2) on counterfactual images Unmodified Gemini-2.5 Pro Sonnet-3.7 GPT-4.1 o3 o4-mini Mean a. 0.00 0.00 9.52 0.92 0. 2.12 b. 1.96 2.72 9.07 7.60 9.31 6.13 c. d. 10.42 13.75 2.50 5.00 14.58 26.74 9.03 8.68 42.71 44.10 9.25 26.25 e. 2.38 1.79 0.00 2.38 4. 2.26 f. g. Task mean Task mean 49.81 54.29 48.61 50.38 51. 20.83 34.52 18.75 20.54 17.56 50.87 22.44 16.02 16.59 13.88 18.50 20.25 17.05 100.00 100.00 100.00 100.00 100. 100."
        },
        {
            "title": "4.2 VLMs fail to recognize an extra leg",
            "content": "is added to common birds and mammals Experiments We use the same experiment setup as in Sec. 4.1 but test VLMs on CF images. Specifically, we evaluate five VLMs on the animal images where an extra leg is added to (a) bird (three legs instead of two) and mammal (five legs instead of four). We ask each VLM with default settings to count legs (Q1 and Q2; Fig. 2b). Results On average, VLMs perform poorly (2.12% accuracy) at counting legs of 3-legged and 5-legged counterfactual animals (Tab. 2 ). Furthermore, 94.14% of the wrong answers match the original, well-known leg counts (Fig. 4 and Fig. 1a), demonstrating that VLMs rely mostly on memorized prior knowledge to answer rather than inspecting the legs in the image (see Fig. 3c). VLMs are slightly worse at counting the legs of birds than counting the legs of mammals (1.01% vs. 2.50%; Tab. 3 ). Bird legs (Fig. 1a) are typically thinner, which may make it harder to detect than GPT-4.1, all VLMs score 0% accuracy (Tab. 3 ). mammals legs (Fig. 3b). On birds, except for"
        },
        {
            "title": "4.3 VLMs consistently fail to detect subtle changes in familiar subjects",
            "content": "We test whether the biased behavior of VLMs when counting animal legs (Sec. 4.2) also exists in four other domains of man-made subjects: chess pieces, and logos of famous brands, national flags, game boards. Counterfactual Images: Chess pieces, flags, and game boards (a) How many xiangqi pieces are there on this board? Answer with number in curly brackets, e.g., {9}. (b): How many stripes are there in this flag? Answer with number in curly brackets, e.g., {9}. (c): How many rows are there on this puzzle? Answer with number in curly brackets, e.g., {9}. (d): How many stars are there in this flag? Answer with number in curly brackets, e.g., {9}. (e): How many rows are there on this board? Answer with number in curly brackets, e.g., {9}. (a) Xiangqi (b) US Flag (c) Sudoku (d) EU Flag (e) Chess board 31 32 28 32 32 31 Bias GT 13 13 13 13 13 9 9 9 9 13 14 Gemini-2.5 Pro 9 10 Sonnet-3.7 12 12 12 12 12 12 11 GPT-4.1 6 8 8 8 8 8 7 o4-mini Figure 5: VLMs perform poorly at counting elements on counterfactual images across domains, heavily defaulting to the biased answers. , , and Experiments We replicate the experiments in Sec. 4.2 on CF versions of . For each domain, we create CF images by making systematic modifications: (1) adding or removing single well-known element (e.g., stripe in the Adidas logo) in logos; (2) adding or removing star or national flags; (3) replacing or removing piece from standard starting chess or stripe in common xiangqi position; and (4) removing or adding row or column in well-known boardgame boards (e.g., Sudoku and Go). , and , , Table 3: VLMs perform poorly across six (out of seven) VLMBias tasks, spanning photo-realistic optical illusions, and images ( logos) and abstract images ( flag, chess pieces, animals and patterned grids). a. Animal b. Logo c. Flag d. Chess/Xiangqi Pieces e. Optical Illusions f. Patterned Grid Model Birds Mammals Mean Shoes Cars Mean Stars Stripes Mean Chess Xiangqi Mean Original Modified Mean Remove Rep/Add Mean 0.00 0.00 5.07 0.00 0. 1.01 0.00 0.00 11.03 1.23 0.25 2.50 0.00 0.00 9.52 0.92 0.18 2.12 5.80 8.15 25.36 21.01 27. 17.57 0.00 0.00 1.11 1.11 0.00 0.44 1.96 2.72 9.07 7.60 9.31 6.13 11.54 20.51 3.21 5.13 18. 11.79 8.33 1.19 1.19 4.76 7.14 4.52 10.42 13.75 2.50 5.00 14.58 9.25 17.36 7.64 11.81 56.94 55. 29.86 36.11 10.42 5.56 28.47 32.64 22.64 26.74 9.03 8.68 42.71 44.10 26.25 73.16 42.68 92.17 91.67 90. 78.02 26.52 65.91 5.05 9.09 12.12 23.74 49.81 54.29 48.61 50.38 51.26 50.87 13.10 35.71 10.12 14.88 12. 17.26 28.57 33.33 27.38 26.19 22.62 27.62 20.83 34.52 18.75 20.54 17.56 22.44 Mean Results VLMs generally demonstrate systematic failures to detect modifications across all four domains, with performance varying depending on the tasks. For logos, accuracy is significantly worse on car logos than on shoe logos (0.44% vs. 17.57%; Tab. 3 ). This might be because logo on car often appear much smaller than logo on shoe photo (Fig. 1 vs. Fig. 2b). For flags, VLMs perform better on counting stars (11.79%; Tab. 3 ) than counting stripes (4.52%; Tab. 3 ). Counting stripes may be harder because stripe is often placed right next 8 to other stripes in flag while stars are spatially separate symbols (Fig. 5b vs. d). More results on flags are in Appendix L.2. On counting chess pieces, thinking VLMs ( o4-mini) significantly outperform non-thinking models (>26% vs. <10%; Tab. 3 ), suggesting that explicit reasoning capabilities help detect anomalies (more results are in Sec. 4.3). Gemini-2.5 Pro, o3, and All VLMs perform extremely poorly (2.26% mean accuracy; Tab. 9) on counting rows and columns of counterfactual board-game board (Fig. 5ce). They score 0% accuracy on Sudoku and Go grids (Fig. 19ab), confirming fundamental inability to perform counting [44], here, in biased counterfactual scenarios (more results in Appendix L.4). Our findings across four domains collectively show that VLMs rely heavily on memorized knowledge to answer rather than performing detailed visual analysis of the counterfactual image."
        },
        {
            "title": "4.4 VLMs are biased towards the known illusions and fail to recognize the changes in the",
            "content": "counterfactual, modified versions Experiment We test five VLMs on 6 classic optical illusions, i.e., Müller-Lyer, Zöllner, Ebbinghaus, Vertical-Horizontal, Pogendorff, and Ponzo  (Fig. 21)  . Each illusion is presented in two versions: (a) its original form and (b) counterfactual, modified version where the groundtruth answer is reversed  (Fig. 6)  . For both versions per illusion, we ask VLMs the same Y/N question (see Appendix H). Results On average, over original and CF versions, all 5 VLMs perform around the random chance (mean accuracy of 50.87%; Tab. 3 ). They tend to provide answers that are true to the original versions (i.e., 78.02% mean accuracy) but false given the counterfactual versions (23.74% accuracy). 4 out of 5 VLMs perform well on the original versions of the illusions but poorly on the CF versions, exhibiting strong bias to the well-known illusions. However, Sonnet-3.7 is the oppositeit performs much better on the counterfactual versions than on the original illusions (65.91% vs. 42.68% accuracy; Tab. 3 ). On average, Sonnet-3.7 still performs only slightly above the random chance (54.29% accuracy), revealing poor low-level vision capability, consistent with recent findings [44]."
        },
        {
            "title": "4.5 VLMs are biased towards the global pattern in a grid",
            "content": "Experiments We test VLMs on counting the shapes or tally marks inside an anomaly cell where the total number of shapes or marks do not follow the patterns in the surrounding cells (Fig. 1g). Results Overall, VLMs perform poorly at 22.44% accuracy. 43.45% of all count predictions, both correct and incorrect, match the biased answers (Fig. 4 ) that correspond to the surrounding cells. In other words, when VLMs make wrong counting predictions, more than half (i.e., 56.02%) of the time, their answers match the global pattern of most cells in the grid rather than the target anomaly cell in question  (Fig. 23)  . Our results confirm striking influence of the background pattern to VLMs assessment on small local region. Here, our patterns in the grids are created from scratch and, therefore, do not represent pattern memorized from the Internet."
        },
        {
            "title": "4.6 VLMs continue to misidentify the common biased patterns when they do not exist in",
            "content": "counterfactual images Prior sections have shown that VLMs struggle to count the key elements in well-known subjects (e.g., the stripes in counterfactual, 4-striped Adadias-like logo; Fig. 2b) at 17.05% accuracy (Tab. 2). And 75% of the time, they default to the predefined bias choices. Here, we aim to confirm that VLMs are so biased that they are unable to tell the difference between the original version and the counterfactual by more direct binary, Yes/No identification question of Q3: Is this an animal with 4 legs? when the counterfactual (e.g., 5-legged puma Fig. 3c) is shown. Experiments We ask 5 VLMs the Q3 question given our sets of original and CF images. The correct answer is Yes for original cases and No for all CF cases (Fig. 3c). Results All VLMs achieve 100% accuracy on the original images, but collapse to mean of 25.11% on the counterfactual versions (Tab. 4), which is only half of random guessing. That is, VLMs consistently answer Yes, misidentifying the well-known subject even when the visual evidence contradicts the prompt (Fig. 3c&f). On top of the prior results with Q1 and Q2, the results on Q3 9 Table 4: Accuracy (%) of VLMs on question Q3 (e.g.., Is this an animal with 4 legs?) when the image is original (4 legs) or counterfactual (5 legs). VLMs mostly answer Yes even on counterfactuals, resulting in accuracy far below the 50% random baseline."
        },
        {
            "title": "Model",
            "content": "Original Counterfactual () Gemini-2.5 Pro Sonnet-3.7 GPT-4.1 o3 o4-mini"
        },
        {
            "title": "Mean",
            "content": "100.00 100.00 100.00 100.00 100.00 100.00 20.63 (-79.37) 23.08 (-76.92) 26.10 (-73.90) 26.15 (-73.85) 29.61 (-70.39) 25.11 (-74.89) Figure 6: Original vs. modified versions without (top) and with (bottom) the inimage text (Ebbinghaus illusion). provide extra evidence supporting the hypothesis that VLMs are too biased to recognize that the well-known pattern has changed in counterfactual images."
        },
        {
            "title": "4.7 Adversarial in-image text showing the name of the common subject further fools VLMs",
            "content": "Prior sections have shown that VLMs perform poorly on the objective task of counting when the background contains visual cues strongly correlate with well-known subjects. As VLM outputs may be influenced by adversarial or distracting text in the image [12], here, we test how in-image textual cues about the subjects (e.g., Ebbinghaus illusion) influence VLMs on the same counting questions. Experiments We insert the subject name (e.g., Adidas or Ebbinghaus illusion; Fig. 6) into the top of all original and CF images, extending the image vertically but keeping the original content unchanged. We repeat previous experiments asking VLMs the two counting questions (Q1 & Q2). Results All VLMs perform worse when an in-image text is added (-4.49; Tab. 5). Interestingly, the decrease is more pronounced for thinking models (Tab. 5), such as o4-mini (-6.56), o3 GPT-4.1 (-2.67). This Sonnet-3.7 (-2.81) and (-6.41), than for non-thinking ones such as result is consistent with recent findings that thinking models tend to hallucinate more [38, 64], here more biased toward the text in the image despite contradictory visuals. Table 5: Adding adversarial, in-image textual cues that state the subject name (e.g., Adidas) cause VLMs to decrease their accuracy (-4.49) on counterfactual images (b). In contrast, instructing VLMs to rely exclusively on the image details to answer questions (Debiased) or to double-check its answers (Double-Check) only slightly improves accuracy, by +1.87 and +2.70, respectively (c). c. Helpful textual prompt b. Adversarial a. Baseline"
        },
        {
            "title": "Model",
            "content": "w/ In-image text w/ Debiased Prompt w/ Double-Check Gemini-2.5 Pro Sonnet-3.7 GPT-4.1 o3 o4-mini"
        },
        {
            "title": "Mean",
            "content": "16.02 16.59 13.88 18.50 20.25 17.05 12.04 (-3.98) 13.78 (-2.81) 11.21 (-2.67) 12.09 (-6.41) 13.69 (-6.56) 19.72 (+3.70) 19.29 (+2.70) 14.38 (+0.50) 18.94 (+0.44) 22.25 (+2.00) 20.22 (+4.20) 20.86 (+4.27) 16.00 (+2.12) 21.02 (+2.52) 20.61 (+0.36) 12.56 (-4.49) 18.92 (+1.87) 19.75 (+2.70)"
        },
        {
            "title": "4.8 Helpful prompts do not ameliorate the bias issues in VLM",
            "content": "Previous results show that VLMs rely heavily on prior knowledge to answer objective counting questions. Here, we test how incorporating helpful instructions in the prompts may help VLMs become less biased. Experiments We apply two prompting strategies across all VLMBias tasks: 10 (1) Debiased Prompt: We prepend the original question (Q1 and Q2) with Do not assume from prior knowledge and answer only based on what is visible in the image. to encourage models to rely exclusively on image contents. (2) Double-Check: After VLMs answer the original question, we add follow-up prompt of Please double-check your answer and give your final answer in curly brackets, following the format above. These prompts are designed to encourage VLMs to examine the image more carefully. All experiments use the same images and default model settings as in the baseline setup. Results Both helpful prompting strategies improve VLM accuracy but only slightly over the baseline, +1.87 for Debiased and +2.70 for Double-Check (Tab. 5c). That is, explicitly instructing models to rely on image contents or verify their answer helps to some extent but does not address the core issue of bias."
        },
        {
            "title": "5 Discussion and Conclusion",
            "content": "o4-mini, Gemini-2.5 Pro, Our study shows that SOTA VLMs fail consistently in counting visual elements (e.g., stripes in logo) when they are strongly biased towards the subject (e.g., an Adidas logo has three stripes). Thinking o3) perform only slightly better than non-thinking ones ( Sonnet-3.7, models ( GPT-4.1) longer thoughts do not address the bias issue. Similarly, instructing VLMs to double check its answers or rely exclusively on the image contents only modestly increase the accuracy. Our work documents important visual biases of VLMs in an objective counting task rather than the common social biases [51] often documented in the literature. For 6 out of 7 tasks, we use an automated pipeline consisting of scripts, LLMs and text-to-image generators that generate counterfactual images. In our pipeline, humans do not manually edit the original images to create counterfactuals but only review the generated images. We release the automation scripts and evaluation code. It might be interesting to compare whether the original and counterfactual images map to similar visual representations after the vision encoders in VLMs. That is, are the VLM failures in this paper the result of visual encoders unable to capture the fine-grained modifications in the couterfactual images? Alternatively, vision encoders may be able to observe the visual changes but the LLMs in late-fusion architectures are too biased to output accurate answers. Limitations Our work has two limitations. First, VLMs with image generation capabilities (e.g., ) are still in early developmental stages and exhibit their own biases, making it non-trivial and to control generated images as expected. This limitation prevents us from exploring some other interesting domains. Second, we are unable to test VLMs that have the capability to use tools, which may substantially help on VLMBias questions. That is, it might be interesting to test how the biased visual cues in VLMBias images suppress tool-use VLMs to use their tools (e.g., zooming functions) to answer questions. Our preliminary results with the chat interface of o3 reveal that o3 often does not even use its visual thinking capability [39] to examine the counterfactual images but instead directly attempt to answer questions."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT)(RS-2025-00573160), and Innovative Human Resource Development for Local Intellectualization program through the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT)(IITP-2025-RS-2020II201489). We also thank Khang Gia Le (Independent Researcher) for feedback and discussions of the earlier results. AV was supported by Hyundai Motor Chung Mong-Koo Global Scholarship. AN was supported by the NSF Grant No. 1850117 & 2145767, and donations from NaphCare Foundation & Adobe Research."
        },
        {
            "title": "References",
            "content": "[1] Abubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language models. In Marion Fourcade, Benjamin Kuipers, Seth Lazar, and Deirdre K. Mulligan, editors, AIES 21: AAAI/ACM Conference on AI, Ethics, and Society, Virtual Event, USA, May 19-21, 2021, pages 298306. ACM, 2021. doi: 10.1145/3461702.3462624. URL https://doi.org/ 10.1145/3461702.3462624. [2] Salvatore Aglioti, Joseph FX DeSouza, and Melvyn Goodale. Size-contrast illusions deceive the eye but not the hand. Current biology, 5(6):679685, 1995. [3] Anonymous. B-score: Detecting biases in large language models using response history. In Fortysecond International Conference on Machine Learning, 2025. URL https://openreview. net/forum?id=kl7SbPfBsB. [4] Anthropic. Claude 3.7 Sonnet and Claude Code, 2025. URL https://www. https://www.anthropic.com/news/ anthropic.com/news/claude-3-7-sonnet. claude-3-7-sonnet. [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [6] Yejin Bang, Delong Chen, Nayeon Lee, and Pascale Fung. Measuring political bias in large language models: What is said and how it is said. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1114211159. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024. ACL-LONG.600. URL https://doi.org/10.18653/v1/2024.acl-long.600. [7] Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! vision-and-language benchmark of synthetic and compositional images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 26162627, 2023. [8] Jessica Maria Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, and Zexue He. Cognitive bias in decision-making with LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 12640 12653, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-emnlp.739. [9] Adolf Fick. De errone quodam optic asymmetria bulbi effecto. Marburg: Koch, 1851. [10] Kathleen Fraser and Svetlana Kiritchenko. Examining gender and racial bias in large vision language models using novel dataset of parallel images. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 690713, St. Julians, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/ 2024.eacl-long.41/. [11] Isabel Gallegos, Ryan Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen Ahmed. Bias and fairness in large language models: survey. Computational Linguistics, 50(3):10971179, 2024. [12] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. Distill, 6(3): e30, 2021. [13] Google. Google Gemini 2.5 Pro, 2025. URL https://deepmind.google/technologies/ gemini/pro/. https://deepmind.google/technologies/gemini/pro/. [14] RT Green and EM Hoyle. The poggendorff illusion as constancy phenomenon. Nature, 200 (4906):611612, 1963. 12 [15] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [16] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. HallusionBench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In CPVR, 2024. [17] Siobhan Mackenzie Hall, Fernanda Gonçalves Abrantes, Hanwen Zhu, Grace Sodunke, Aleksandar Shtedritski, and Hannah Rose Kirk. Visogender: dataset for benchmarking gender bias in image-text pronoun resolution. Advances in Neural Information Processing Systems, 36: 6368763723, 2023. [18] Kai Hamburger and Thorsten Hansen. Analysis of individual variations in the classical horizontal-vertical illusion. Attention, Perception, & Psychophysics, 72(4):10451052, 2010. [19] Yusuke Hirota, Yuta Nakashima, and Noa Garcia. Gender and racial bias in visual question answering datasets. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 12801292, 2022. [20] Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Anahita Bhiwandiwalla, and Vasudev Lal. Socialcounterfactuals: Probing and mitigating intersectional social biases in vision-language models with counterfactual examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1197511985, 2024. [21] Catherine Howe and Dale Purves. The müller-lyer illusion explained by the statistics of image source relationships. Proceedings of the National Academy of Sciences, 102(4):12341239, 2005. [22] Tiancheng Hu, Yara Kyrychenko, Steve Rathje, Nigel Collier, Sander van der Linden, and Jon Roozenbeek. Generative language models exhibit social identity biases. Nat. Comput. Sci., 5(1):6575, 2025. doi: 10.1038/S43588-024-00741-1. URL https://doi.org/10.1038/ s43588-024-00741-1. [23] Wen Huang, Hongbin Liu, Minxin Guo, and Neil Gong. Visual hallucinations of multi-modal large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 96149631. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.573. URL https://doi.org/10.18653/ v1/2024.findings-acl.573. [24] Sepehr Janghorbani and Gerard De Melo. Multi-modal bias: Introducing framework for stereotypical bias assessment beyond gender and race in visionlanguage models. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 17251735, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eacl-main.126. URL https://aclanthology.org/2023.eacl-main.126/. [25] Katherine Van Koevering and Jon M. Kleinberg. How random is random? evaluating the randomness and humaness of llms coin flips. CoRR, abs/2406.00092, 2024. doi: 10.48550/ ARXIV.2406.00092. URL https://doi.org/10.48550/arXiv.2406.00092. [26] Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. Benchmarking cognitive biases in large language models as evaluators. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 517545. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.29. URL https://doi.org/10.18653/v1/2024.findings-acl.29. 13 [27] Divyanshu Kumar, Umang Jain, Sahil Agarwal, and Prashanth Harshangi. Investigating implicit bias in large language models: large-scale study of over 50 llms, 2024. URL https: //arxiv.org/abs/2410.12864. [28] Kang-il Lee, Minbeom Kim, Seunghyun Yoon, Minsung Kim, Dongryeol Lee, Hyukhun Koh, and Kyomin Jung. VLind-Bench: Measuring language priors in large vision-language models. In NAACL Findings, 2025. [29] Nayeon Lee, Yejin Bang, Holy Lovenia, Samuel Cahyawijaya, Wenliang Dai, and Pascale Fung. Survey of social bias in vision-language models. arXiv preprint arXiv:2309.14381, 2023. [30] Cheng Li, Mengzhuo Chen, Jindong Wang, Sunayana Sitaram, and Xing Xie. Culturellm: Incorporating cultural differences into large language models. Advances in Neural Information Processing Systems, 37:8479984838, 2024. [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [32] Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, and Xirong Li. Phd: chatgpt-prompted visual hallucination evaluation dataset. arXiv preprint arXiv:2403.11116, 2024. [33] Dominique Makowski, Zen J. Lau, Tam Pham, W. Paul Boyce, and S.H. Annabel Chen. parametric framework to generate visual illusions using python. Perception, 50(11): 950965, 2021. doi: 10.1177/03010066211057347. URL https://doi.org/10.1177/ 03010066211057347. PMID: 34841973. [34] Franz Carl Müller-Lyer. Optische Urteilstäuschungen. Archiv für Anatomie und Physiologie, Physiologische Abteilung, 2:263270, 1889. Original description of the Müller-Lyer illusion. [35] Tarek Naous, Michael J. Ryan, Alan Ritter, and Wei Xu. Having beer after prayer? measuring cultural bias in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1636616393. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024. ACL-LONG.862. URL https://doi.org/10.18653/v1/2024.acl-long.862. [36] OpenAI. Introducing GPT-4.1 in the API, 2025. URL https://openai.com/index/ gpt-4-1/. https://openai.com/index/gpt-4-1/. [37] OpenAI. Introducing OpenAI o3 and o4-mini, 2025. com/index/introducing-o3-and-o4-mini/. introducing-o3-and-o4-mini/. URL https://openai. https://openai.com/index/ [38] OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, April 2025. URL https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/ o3-and-o4-mini-system-card.pdf. Comprehensive system card detailing the capabilities, safety, and evaluation results for OpenAI o3 and o4-mini models. [39] OpenAI. Thinking with images, 2025. URL https://openai.com/index/ thinking-with-images/. Accessed: 2025-05-28. [40] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R. Bowman. BBQ: hand-built bias benchmark for question answering. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 20862105. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022. FINDINGS-ACL.165. URL https://doi.org/10.18653/v1/2022.findings-acl.165. [41] Johann Christian Poggendorff. Biographisch-literarisches handwörterbuch zur geschichte der exakten wissenschaften von jc poggendorff, i-ii. Leipzig: Johann Ambrosius Barth. Ponatis:(1965). Amsterdam: BM Israël NV, 1863. [42] Mario Ponzo. Intorno ad alcune illusioni nel campo delle sensazioni tattili, sullillusione di Aristotele fenomeni analoghi. Wilhelm Engelmann, 1910. [43] Yujin Potter, Shiyang Lai, Junsol Kim, James Evans, and Dawn Song. Hidden persuaders: Llms political leaning and their influence on voters. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 42444275. Association for Computational Linguistics, 2024. URL https://aclanthology. org/2024.emnlp-main.244. [44] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. In Minsu Cho, Ivan Laptev, Du Tran, Angela Yao, and Hongbin Zha, editors, Computer Vision - ACCV 2024 - 17th Asian Conference on Computer Vision, Hanoi, Vietnam, December 8-12, 2024, Proceedings, Part V, volume 15476 of Lecture Notes in Computer Science, pages 293309. Springer, 2024. doi: 10.1007/978-981-96-0917-8 _17. URL https://doi.org/10.1007/978-981-96-0917-8_17. [45] Chahat Raj, Anjishnu Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, and Ziwei Zhu. Biasdora: Exploring hidden biased associations in vision-language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 1043910455. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024. findings-emnlp.611. [46] Gabriele Ruggeri and Debora Nozza. multi-dimensional study on bias in vision-language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 64456455, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.403. URL https://aclanthology.org/2023.findings-acl.403/. [47] Ashutosh Sathe, Prachi Jain, and Sunayana Sitaram. unified framework and dataset for assessing societal bias in vision-language models. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 12081249. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.findings-emnlp.66. [48] Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yue Dong, and Rifat Shahriyar. Illusionvqa: challenging optical illusion dataset for vision language models. arXiv preprint arXiv:2403.15952, 2024. [49] Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian RodriguezMunoz, Shivam Duggal, Phillip Isola, and Antonio Torralba. vision check-up for language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1441014419, 2024. [50] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as babysitter: On biases in language generation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 34073412, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1339. URL https://aclanthology.org/D19-1339/. [51] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. The woman worked as babysitter: On biases in language generation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 3405 3410. Association for Computational Linguistics, 2019. doi: 10.18653/V1/D19-1339. URL https://doi.org/10.18653/v1/D19-1339. [52] Jisu Shin, Hoyun Song, Huije Lee, Soyeong Jeong, and Jong Park. Ask llms directly, \"what shapes your bias?\": Measuring social bias in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1612216143. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.954. URL https://doi.org/10.18653/v1/2024.findings-acl.954. [53] Mohammad Reza Taesiri, Giang Nguyen, Sarra Habchi, Cor-Paul Bezemer, and Anh Imagenet-hard: The hardest images remaining from study of the power Nguyen. of zoom and spatial biases in image classification. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 706390d6f9208b03bc54f97ac3cfe99e-Abstract-Datasets_and_Benchmarks.html. [54] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [55] Edward Bradford Titchener. Experimental psychology: manual of laboratory practice, volume 2. Macmillan Company, 1905. [56] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, 2024. [57] GK Wallace. The effect of contrast on the zöllner illusion. Vision Research, 15(8-9):963966, 1975. [58] Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, Zhaopeng Tu, and Michael R. Lyu. Not all countries celebrate thanksgiving: On the cultural dominance In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, in large language models. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 63496384. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.345. URL https://doi.org/10.18653/v1/2024.acl-long.345. [59] Xiyang Wu, Tianrui Guan, Dianqi Li, Shuaiyi Huang, Xiaoyu Liu, Xijun Wang, Ruiqi Xian, Abhinav Shrivastava, Furong Huang, Jordan L. Boyd-Graber, Tianyi Zhou, and Dinesh Manocha. Autohallusion: Automatic generation of hallucination benchmarks for vision-language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 83958419. Association for Computational Linguistics, 2024. URL https://aclanthology. org/2024.findings-emnlp.493. [60] Yisong Xiao, Aishan Liu, QianJia Cheng, Zhenfei Yin, Siyuan Liang, Jiapeng Li, Jing Shao, Xianglong Liu, and Dacheng Tao. Genderbias-vl: Benchmarking gender bias in vision language models via counterfactual probing. CoRR, abs/2407.00600, 2024. doi: 10.48550/ARXIV.2407. 00600. URL https://doi.org/10.48550/arXiv.2407.00600. [61] Gizem Yildiz, Irene Sperandio, Christine Kettle, and Philippe Chouinard. review on various explanations of ponzo-like illusions. Psychonomic Bulletin & Review, pages 128, 2022. [62] Yichi Zhang, Jiayi Pan, Yuchen Zhou, Rui Pan, and Joyce Chai. Grounding visual illusions in language: Do vision-language models perceive illusions like humans? In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 57185728. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.348. URL https://doi.org/10.18653/v1/2023.emnlp-main.348. [63] Yiming Zhang, Avi Schwarzschild, Nicholas Carlini, Zico Kolter, and Daphne Ippolito. Forcing diffuse distributions out of language models. CoRR, abs/2404.10859, 2024. doi: 10.48550/ ARXIV.2404.10859. URL https://doi.org/10.48550/arXiv.2404.10859. 16 [64] Yujia Zhang, Yujing Li, Yuxuan Wang, Xinyi Wang, Yuxuan Wang, and Xinyi Wang. How language model hallucinations can snowball. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of PMLR, pages 5967059684, 2024. URL https://proceedings.mlr.press/v235/zhang24ay.html. Shows how step-by-step reasoning can propagate and amplify hallucinations in large language models. [65] Jiaxu Zhao, Meng Fang, Shirui Pan, Wenpeng Yin, and Mykola Pechenizkiy. GPTBIAS: comprehensive framework for evaluating bias in large language models. CoRR, abs/2312.06315, 2023. doi: 10.48550/ARXIV.2312.06315. URL https://doi.org/10.48550/arXiv.2312. 06315. [66] Kankan Zhou, Eason Lai, Wei Bin Au Yeong, Kyriakos Mouratidis, and Jing Jiang. ROME: Evaluating pre-trained vision-language models on reasoning beyond visual common sense. In Findings of the Association for Computational Linguistics: EMNLP, 2023. URL https: //openreview.net/forum?id=N6sXsHuWDE. [67] F. Zöllner. Ueber eine neue art anorthoskopischer zerrbilder. Annalen der Physik, 193(11):477 484, 1862. doi: https://doi.org/10.1002/andp.18621931108. URL https://onlinelibrary. wiley.com/doi/abs/10.1002/andp.18621931108. 17 Appendix for: Vision Language Models are Biased"
        },
        {
            "title": "Table of Contents",
            "content": "Illustrative questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Models and access details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Task 1: Counting legs with added limb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Task 2: Counting elements in modified brand logos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Task 3: Counting stripes/stars in modified national flags . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Task 4: Counting chess pieces on modified starting position . . . . . . . . . . . . . . . . . . . . . . . . . 34 Task 5: Counting rows and columns of board game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 Task 6: Visual testing with both original and modified optical illusion . . . . . . . . . . . . . . . . 40 Task 7: Counting circles or lines in an anomaly cell within patterned grid . . . . . . . . . . . 43 Qualitative results on animals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 flags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 Qualitative results on More findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 Prompts used for image generation and image editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 Questions for sanity check . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Illustrative questions",
            "content": "Table 6: Some examples of questions on animal, brand logos, and flags Topic Subtopic Q1 Q2 Q3 Animal How many legs does this animal have? Answer with number in curly brackets, e.g., {9}. Count the legs of this animal. Answer with number in curly brackets, e.g., {9}. Is this an animal with 4 legs? Answer in curly brackets, e.g., {Yes} or {No}. Logo Adidas How many visible stripes are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9}. Count the visible stripes in the logo on the left shoe. Answer with number in curly brackets, e.g., {9}. Are the logos on these shoes Adidas logos? Answer in curly brackets, e.g., {Yes} or {No}. Nike How many visible white stylized curves are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9}. Count the visible white stylized curves in the logo on the left shoe. Answer with number in curly brackets, e.g., {9}. Are the logos on these shoes Nike logos? Answer in curly brackets, e.g., {Yes} or {No}. Mercedes How many points are there on the star in the logo of this car? Answer with number in curly brackets, e.g., {9}. Audi How many overlapping circles are there in the logo of this car? Answer with number in curly brackets, e.g., {9}. Maserati How many prongs are there in the logo of this car? Answer with number in curly brackets, e.g., {9}. Count the points on the star in the logo of this car. Answer with number in curly brackets, e.g., {9}. Is the logo on this car Mercedes-Benz logo? Anin curly brackets, swer e.g., {Yes} or {No}. Count the overlapping circles in the logo of this car. Answer with number in curly brackets, e.g., {9}. Is the logo on this car Audi logo? Answer in curly brackets, e.g., {Yes} or {No}. Count the prongs in the logo of this car. Answer with number in curly brackets, e.g., {9}. Is the logo on this car Maserati logo? Answer in curly brackets, e.g., {Yes} or {No}. Flag Stars How many stars are there on this flag? Answer with number in curly brackets, e.g., {9}. Count the stars on this flag. Answer with number in curly brackets, e.g., {9}. Is this the flag of [country]? Answer in curly brackets, e.g., {Yes} or {No}. Stripes How many stripes are there on this flag? Answer with number in curly brackets, e.g., {9}. Count the stripes on this flag. Answer with number in curly brackets, e.g., {9}. Is this the flag of [country]? Answer in curly brackets, e.g., {Yes} or {No}. 19 Table 7: Some examples of questions on chesse pieces, game boards and patterned grid. Topic Subtopic Q1 Q3 Chess Pieces Chess How many chess pieces are there on this board? Answer with number in curly brackets, e.g., {9}. Count the chess pieces on this board. Answer with number in curly brackets, e.g., {9}. Is this the chess starting position? Answer in curly brackets, e.g., {Yes} or {No}. Xiangqi How many xiangqi pieces are there on this board? Answer with number in curly brackets, e.g., {9}. Count the xiangqi pieces on this board. Answer with number in curly brackets, e.g., {9}. Is this the Xiangqi starting position? Answer in curly brackets, e.g., {Yes} or {No}. Board Game Chess How many rows are there on this board? Answer with number in curly brackets, e.g., {9}. Count the rows on this board. Answer with number in curly brackets, e.g., {9}. Is this 8x8 Chessboard? Answer in curly brackets, e.g., {Yes} or {No}. Xiangqi How many horizontal lines are there on this board? Answer with number in curly brackets, e.g., {9}. Go How many horizontal lines are there on this board? Answer with number in curly brackets, e.g., {9}. Count the horizontal lines on this board. Answer with number in curly brackets, e.g., {9}. Is this 10x9 Xiangqi board? Answer in curly brackets, e.g., {Yes} or {No}. Count the horizontal lines on this board. Answer with number in curly brackets, e.g., {9}. Is this 19x19 Go board? Answer in curly brackets, e.g., {Yes} or {No}. Sudoku How many rows are there on this puzzle? Answer with number in curly brackets, e.g., {9}. Count the rows on this puzzle. Answer with number in curly brackets, e.g., {9}. Is this 9x9 Sudoku puzzle? Answer in curly brackets, e.g., {Yes} or {No}. Patterned Grid Dice How many circles are there in cell C5? Answer with number in curly brackets, e.g., {9}. Count the circles in cell C5. Answer with number in curly brackets, e.g., {9}. Does cell C5 contain 4 circles? Answer in curly brackets, e.g., {Yes} or {No}. Tally How many lines are there in cell C5? Answer with number in curly brackets, e.g., {9}. Count the lines in cell C5. Answer with number in curly brackets, e.g., {9}. Does cell C5 contain 3 lines? Answer in curly brackets, e.g., {Yes} or {No}. 20 Table 8: Some examples of questions on optical illusions. Topic Subtopic Q1 Q2 Optical Illusion Ebbinghaus Mullerlyer Are the two inner circles equal in size? Answer in curly brackets, e.g., {Yes} or {No}. Do the two inner circles have the same size? Anin curly brackets, swer e.g., {Yes} or {No}. Is this an example of the Ebbinghaus illusion? Anin curly brackets, swer e.g., {Yes} or {No}. Are the two horizontal lines equal in length? Anin curly brackets, swer e.g., {Yes} or {No}. the have Do the two horizontal lines same length? Answer in curly brackets, e.g., {Yes} or {No}. Is this an example of the Müller-Lyer illusion? Answer with Yes/No. Answer in curly brackets, e.g., {Yes} or {No}. Poggendorff Are the two diagonal line segments aligned? Answer in curly brackets, e.g., {Yes} or {No}. Do the two diagonal lines form straight line? Answer in curly brackets, e.g., {Yes} or {No}. Is this an example of the Poggendorff illusion? Answer in curly brackets, e.g., {Yes} or {No}. Ponzo Are the two horizontal lines equal in length? Anin curly brackets, swer e.g., {Yes} or {No}. the have Do the two horizontal lines same length? Answer in curly brackets, e.g., {Yes} or {No}. Is this an example of the Ponzo illusion? Answer in curly brackets, e.g., {Yes} or {No}. VerticalHorizontal Are the horizontal and vertical lines equal in length? Answer in curly brackets, e.g., {Yes} or {No}. Do the horizontal and vertical lines have the same length? Answer in curly brackets, e.g., {Yes} or {No}. Is this an example of the VerticalHorizontal illusion? Answer in curly brackets, e.g., {Yes} or {No}. Zollner Are the two horizontal lines parallel? Answer in curly brackets, e.g., {Yes} or {No}. Do the two horizontal lines run parallel? Answer in curly brackets, e.g., {Yes} or {No}. Is this an example of the Zöllner illusion? Answer in curly brackets, e.g., {Yes} or {No}."
        },
        {
            "title": "B Models and access details",
            "content": "We evaluate five state-of-the-art VLMs using their official APIs with default settings. These include three thinking models (e.g., Gemini-2.5 Pro, o3, o4-mini) and two non-thinking models (e.g., Sonnet-3.7, GPT-4.1). B.1 Gemini-2.5 Pro We access Gemini-2.5 Pro (gemini-2.5-pro-preview-05-06) via aistudio.google.com and use all default settings with temperature=1.0. B.2 Sonnet-3.7 We access the Anthropic API via console.anthropic.com to use Sonnet-3.7 (claude-3.7-sonnet) and default settings with temperature=1.0. B.3 GPT-4. We access the API for GPT-4.1 (gpt-4.1) via platform.openai.com and use all default settings with temperature=1.0. B.4 o3 We access the OpenAI API for o3 via platform.openai.com and use default settings with temperature=1.0. B.5 o4-mini We access the OpenAI API for o4-mini (o4-mini) via platform.openai.com with default settings including: temperature: 1.0 reasoning_effort: medium (default thinking mode setting) 22 Task 1: Counting legs with added limb C.1 Task design Figure 7: Data generation pipeline for Task 1: Counting legs with added limb. Pretrained on the Internet data, VLMs must have colossal prior knowledge of the count of animal legs from both textual and image data. Following this hypothesis, we generate images of usual animals with one additional leg (e.g., 3-legged birds or 5-legged dogs) and ask VLMs to count legs to evaluate if these models are biased toward their prior knowledge. Animal types: We modify the legs of 2 types of animals: birds and mammals. Modification types: Each animal is modified to have 1 additional leg. Target animals: We select 91 well-known animals, consisting of 23 two-legged birds and 68 four-legged mammals. Image resolutions: We generate each animal image and rescale them at 3 different pixel sizes {384, 768, 1152}px using the scaling factor in Sec. 3.1 to test resolution sensitivity This approach generates total of 91 animals 1 modification type 3 resolutions = 273 total images. C."
        },
        {
            "title": "Implementation and image generation",
            "content": "Implementation details Our image generation pipeline follows this sequence: 1. Use o4-mini to collect list of well-known animals with clearly visible legs 2. Generate full-body and side-view images of these animals using Gemini-2.0 Flash 3. For each animal image, use Gemini-2.0 Flash to add one extra leg to the animal. Each animal image is edited over 4 independent trials. 4. Manually inspect and filter out unsatisfactory images 5. Render each approved image at three different resolutions Quality control We manually inspect the images to ensure that each modified animal image has exactly one additional leg. For cases that fail (e.g., more than one added leg), we remove them from our dataset. Prompt We use the following prompts to test the VLMs: Q1: How many legs does this animal have? Answer with number in curly brackets, e.g., {9}. Q2: Count the legs of this animal. Answer with number in curly brackets, e.g., {9}. 23 Q3: Is this an animal with [NumModifiedLegs] legs? Answer in curly brackets, e.g., {Yes} or {No}. Ground truth calculation The ground truth answers are as follow: Birds leg counting (Q1&Q2): Correct answer: 3 (one additional leg) Expected bias: 2 Mammals leg counting (Q1&Q2): Correct answer: 5 (one additional leg) Expected bias: 4 Animal leg identification question (Q3): Correct answer: No (always, since each animal has one additional leg) Expected bias: Yes C.3 Qualitative results How many legs does this animal have? Answer with number in curly brackets, e.g., {9}. (a)-(e) How many legs does this animal have? Answer with number in curly brackets, e.g., {9}. (a) Lion (b) Stork (c) Elephant (d) Duck (e) Dog 4 4 5 4 4 4 5 2 2 2 2 2 2 3 Gemini-2.5 Pro 4 4 6 4 4 5 Sonnet-3.7 2 2 2 2 2 2 3 GPT-4. o3 4 4 6 6 4 5 o4-mini Bias GT Figure 8: VLMs are often biased toward the original number of legs to answer based on prior knowledge rather than by analyzing the image. animals have, and they tend C.4 List of animals Mammals: Four-legged animals horse, zebra, donkey, mule, cow, buffalo, yak, water buffalo, deer, elk, moose, reindeer, caribou, gazelle, giraffe, camel, dromedary camel, bactrian camel, llama, alpaca, goat, ibex, mountain goat, pronghorn, bighorn sheep, wild boar, pig, warthog, coyote, lynx, bobcat, leopard, tiger, lion, jaguar, puma, ocelot, caracal, hyena, rabbit, impala, springbok, kudu, eland, wildebeest, okapi, hippopotamus, african elephant, asian elephant, indian rhinoceros, gnu, maned wolf, arctic fox, red fox, fennec fox, red wolf, domestic dog, domestic cat, african wilddog, dingo, jackal, gray wolf, hare, cheetah, antelope, bison, sheep, serval 24 Birds: Two-legged animals ostrich, emu, rhea, cassowary, heron, stork, crane, egret, ibis, spoonbill, turkey, chicken, rooster, duck, swan, peacock, sandpiper, avocet, stilt, plover, lapwing, oystercatcher, secretary bird 25 Task 2: Counting elements in modified brand logos Figure 9: Data generation pipeline of shoe logos for Task 2: Counting elements in modified brand logos Figure 10: Data generation pipeline of car logos for Task 2: Counting elements in modified brand logos D.1 Task design Our initial evaluation show that some VLMs, such as o4-mini, can accurately count the four stripes on modified Adidas logo on white background. As such, to increase the task difficulty, we logos with the background they typically appear on. hypothesize that VLMs strongly associate Subsequently, we examine if the visual cues from the background would be strong enough to suppress counting the elements in the logos. Our task is designed as follow: Brand types: We use 2 different brand types: cars and shoes Target brands: We select 5 well-known brands with quantifiable graphical elements: Car brands: Mercedes-Benz, Maserati, and Audi (3 brands) Shoe brands: Adidas and Nike (2 brands) Background variations: Each brand logo has specific background settings: Car logo background: Car logos always appear on cars. For each logo, we collect 5 car body types 3 colors (white, grey, black) 26 Shoe logos (a), (c), (e): How many visible stripes are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9}. (b): How many visible white stylized curves are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9}. (d): How many visible black stylized curves are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9}. (a) Adidas (b) Nike (c) Adidas (d) Nike (e) Adidas 3 3 3 3 3 4 1 1 1 1 1 1 2 Gemini-2.5 Pro 3 3 3 3 3 3 4 Sonnet-3.7 2 1 1 1 1 2 GPT-4.1 o3 3 3 3 4 4 3 4 o4-mini Bias GT Figure 11: VLMs are often biased and rely on prior knowledge when answering questions about shoe logos, even with simple ones like the Nike Swoosh. Please zoom in to see the logo clearly. Shoe logo background: Shoe logos are often seen on the footwear of athletes. For each logo, we collect list of 4 relevant sports (tennis, running, basketball, soccer) 3 colors (black, red, white) Image resolutions: We generate each image and rescale them at 3 different pixel sizes {384, 768, 1152}px using the scaling factor in Sec. 3.1 to test resolution sensitivity This systematic approach generates total of [3 car brands (5 3) 3 resolutions] + [2 shoe brands (4 3) 3 resolutions] = 135 + 72 = 207 total images. D."
        },
        {
            "title": "Implementation and prompts",
            "content": "Implementation details We employ the following process to generate logo modification images: 1. Use o4-mini to suggest graphical modifications for each logo (e.g., increasing Adidas three stripes to four). We then select the most relevant suggestions for our benchmark. 2. Generate modified logo versions using 3. Create background images: GPT-4o. Background images for car logos: Use For each logo, generate and select relevant images of cars from the logo brand with o4-mini to suggest popular colors and body types of each car logo. the determined body types and colors. Manually place modified logos in typical car logo positions. Background images for shoe logos: Use o4-mini to suggest popular shoe colors and sports affiliated with each shoe logo. For each logo, generate and select relevant images of athletes wearing shoes with the modified logo for each determined color and sport. 4. Render each image at three different resolutions. Quality control To ensure high-quality images, we manually review to make sure that: (1) each generated logo has the correct number of modified elements; (2) each product is clearly visible and oriented correctly; and (3) the position of the logos on the products are natural-looking."
        },
        {
            "title": "Prompts We use the following prompts",
            "content": "1. Counting questions (Q1 & Q2): Q1 (Adidas): How many visible stripes are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9}. Q1 (Nike): How many visible [CurveColor] stylized curves are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9} Q1 (Audi): How many overlapping circles are there in the logo of this car? Answer with number in curly brackets, e.g., {9}. Q1 (Mercedes): How many points are there on the star in the logo of this car? Answer with number in curly brackets, e.g., {9}. Q1 (Maserati): How many prongs are there in the logo of this car? Answer with number in curly brackets, e.g., {9} Q2 (Adidas): Count the visible stripes in the logo of the left shoe. Answer with number in curly brackets, e.g., {9}. Q2 (Nike): Count the visible [CurveColor] stylized curves in the logo of the left shoe. Answer with number in curly brackets, e.g., {9} Q2 (Audi): Count the overlapping circles in the logo of this car. Answer with number in curly brackets, e.g., {9}. Q2 (Mercedes): Count the points on the star in the logo of this car. Answer with number in curly brackets, e.g., {9}. Q2 (Maserati): Count the prongs in the logo of this car. Answer with number in curly brackets, e.g., {9} 2. Y/N identification questions (Q3): Q3 (Adidas): Are the logos on these shoes Adidas logos? Answer in curly brackets, e.g., {Yes} or {No}. Q3 (Nike): Are the logos on these shoes Nike logos? Answer in curly brackets, e.g., {Yes} or {No}. Q3 (Audi): Is the logo on this car Audi logo? Answer in curly brackets, e.g., {Yes} or {No}. Q3 (Mercedes): Is the logo on this car Mercedes-Benz logo? Answer in curly brackets, e.g., {Yes} or {No}. Q3 (Maserati): Is the logo on this car Maserati logo? Answer in curly brackets, e.g., {Yes} or {No}. Ground truth calculation The ground truth answers are as follow: Adidas stripes counting (Q1&Q2): Correct answer: 4 Expected bias: 3 Nike stylized curves counting (Q1&Q2): Correct answer: 2 Expected bias: 1 Audi overlapping circles counting (Q1&Q2): Correct answer: 5 Expected bias: 4 Mercedes-Benz points on the star counting (Q1&Q2): Correct answer: 4 Expected bias: 3 Maserati prongs counting (Q1&Q2): Correct answer: 5 Expected bias: 3 Logo identification question (Q3): Correct answer: No (all logos are modified) Expected bias: Yes D.3 Qualitative results Car logos (a), (d): How many overlapping circles are there in the logo of this car? Answer with number in curly brackets, e.g., {9}. (b), (e): How many points are there on the star in the logo of this car? Answer with number in curly brackets, e.g., {9}. c: How many prongs are there in the logo of this car? Answer with number in curly brackets, e.g., {9}. (a) Audi (b) Mercedes (c) Maserati (d) Audi (e) Mercedes 4 4 4 4 4 4 5 3 3 3 3 3 3 4 Gemini-2.5 Pro 3 3 3 3 3 3 5 Sonnet-3.7 4 4 4 4 4 4 5 GPT-4.1 3 3 3 3 3 3 4 o4-mini Bias GT Figure 12: VLMs are completely biased and rely entirely on prior knowledge when answering questions about brand logos. Please zoom in to see the logo clearly. 29 Shoe logos (a), (c), (e): How many visible stripes are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9}. (b): How many visible white stylized curves are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9}. (d): How many visible black stylized curves are there in the logo of the left shoe? Answer with number in curly brackets, e.g., {9}. (a) Adidas (b) Nike (c) Adidas (d) Nike (e) Adidas 3 3 3 3 3 3 4 1 1 1 1 1 2 Gemini-2.5 Pro 3 3 3 3 3 3 Sonnet-3.7 2 1 1 1 1 1 2 GPT-4.1 o3 3 3 3 4 4 3 4 o4-mini Bias GT Figure 13: VLMs are often biased and rely on prior knowledge when answering questions about shoe logos, even with simple ones like the Nike Swoosh. Please zoom in to see the logo clearly. 30 Figure 14: Data generation pipeline for Task 3: Counting stripes/stars in modified national flags. Task 3: Counting stripes/stars in modified national flags E.1 Task design Flags of countries contain easily recognizable patterns. To evaluate if existing VLMs overly rely on their knowledge of these flags to count certain element, we design the task as follow: Flag types: We modify 2 commonly used elements across different flags: stars and stripes Modification types: Each flag has 2 types of modifications: Add: We add an additional element (star or stripe) to chosen flag Remove: We remove one element (star or stripe) from chosen flag Target flags: We select 20 well-known country flags with either 3+ stars or 5+ stripes (a total of 13 star-typed flags and 7 stripe-typed flags) to ensure the modified flags retain recognizable traits to test visual bias. Image resolutions: We generate each flag and rescale them at 3 different pixel sizes {384, 768, 1152}px using the scaling factor in Sec. 3.1 to test resolution sensitivity This systematic approach generates total of 20 target flags 2 modification types 3 resolutions = 120 total images. E.2 Implementation and image generation Implementation details We modify the SVG code of chosen flag to create new variants following this sequence: 1. Identify 20 well-known country flags (13 with 3+ stars, 7 with 5+ stripes) based on the suggestions from o4-mini. 2. Retrieve original SVG code from WikiCommons for each flag. 3. Use o4-mini to modify each SVG to create two variants: An Add variant with one additional element. Remove variant with one fewer element. 4. Render each modified flag at three different resolutions. Quality control We employ the following steps to ensure high-quality and consistent images: 31 Manual inspection: We manually review each generated sample to verify modification quality and visual consistency Filtering: We remove unsatisfactory samples from the benchmark and rerun the pipeline on these cases to obtain new samples. Fallback: For rare cases (3 in total) that consistently fail automated generation, we manual modify the flags to ensure they strictly follow the modification rules. Prompts We use the following prompts: 1. Counting questions (Q1 & Q2): Q1 (Star-typed flags): How many stars are there on this flag? Answer with number in curly brackets, e.g., {9}. Q1 (Stripe-typed flags): How many stripes are there on this flag? Answer with number in curly brackets, e.g., {9}. Q2 (Star-typed flags): Count the stars on this flag. Answer with number in curly brackets, e.g., {9}. Q2 (Stripe-typed flags): Count the stripes on this flag. Answer with number in curly brackets, e.g., {9}. 2. Y/N identification questions (Q3): Is this the flag of [CountryName]? Answer in curly brackets, e.g., {Yes} or {No}. Ground truth calculation We calculate the ground truth as follow: Direct counting questions (Q1 & Q2): Correct answer: The actual count of the elements (stars or stripes) on the flag after modification * For Remove modifications: Standard element count minus 1 * For Add modifications: Standard element count plus Expected bias: The standard element count Flag verification question (Q3): Correct answer: No (since the flags element has been modified) Expected bias: Yes 32 National Flag (a), (b), (e): How many stripes are there in this flag? Answer with number in curly brackets, e.g., {9}. (b), (c): How many stars are there in this flag? Answer with number in curly brackets, e.g., {9}. (a) US (b) US (c) EU (d) EU (e) Zimbabwe 13 13 13 13 13 13 12 Bias GT 13 13 13 13 12 12 12 12 12 13 14 Gemini-2.5 Pro 12 11 Sonnet-3. 13 12 12 12 13 12 13 GPT-4.1 o3 7 7 7 7 7 7 6 o4-mini Figure 15: VLMs are biased when counting the stars and stripes on national flags. 33 Task 4: Counting chess pieces on modified starting position Figure 16: Data generation pipeline for Task 4: Counting chess pieces on modified starting position F.1 Task design To evaluate if VLMs rely on expected structure or attend to actual pieces, we test their ability to count pieces on boards with subtle modifications. We design our task with careful control of visual parameters to ensure systematic evaluation: Board types: We use 2 different game boards: {chess (Western chess), xiangqi (Chinese chess)} Modification types: Each board has 2 types of modifications: Remove: We remove exactly one piece from the standard starting position. Replace: We replace exactly one piece with different piece of the same color. Target squares: We select 12 unique occupied squares per board type, maintaining the same target squares across the Remove and Replace modifications to ensure controlled comparison. Image resolutions: We generate each board at 3 different pixel sizes {384, 768, 1152}px to test resolution sensitivity. This systematic approach generates total of 2 board types 2 modification types 12 target squares 3 resolutions = 144 total images. F."
        },
        {
            "title": "Implementation and prompts",
            "content": "Implementation details Our implementation utilizes specialized libraries for each board type. For chess, we leverage the Python chess library to manipulate board states and chess.svg for rendering. For xiangqi (Chinese chess), we created custom implementation using svgwrite for rendering. The algorithm for both board types follows the same sequence: 1. Create standard board with all 32 pieces in their starting positions 2. Randomly select 12 target squares from the occupied squares 3. For each target square, create (a) Remove variant and (b) Replace variant 4. Render each modified board at three different resolutions The xiangqi implementation required special handling for: The traditional 910 board layout with the central river and two palaces Chinese character rendering for pieces, which requires detecting appropriate CJK fonts Different piece distribution (Chariots, Knights, Elephants, Advisors, General, Cannons, and Soldiers) Quality control To ensure consistent image quality across all variants, we implemente several technical measures: SVG to PNG conversion: We used direct SVG rendering with adjustable scaling factors based on target resolution Quality scaling: We applied quality multiplier (5.0 base resolution factor) to ensure clear piece visibility Prompts We use different prompts for each modification type to test VLMs visual attention: 1. Remove modifications: Q1: How many [chess/xiangqi] pieces are there on this board? Answer with number in curly brackets, e.g., {9}. Q2: Count the [chess/xiangqi] pieces on this board. Answer with number in curly brackets, e.g., {9}. 2. Replace modifications: Q1: How many [Added Piece Type] pieces are there on this board? Answer with number in curly brackets, e.g., {9}. Q2: Count the [Added Piece Type] pieces on this board? Answer with number in curly brackets, e.g., {9}. 3. Both modification types: Q3: Is this the [chess/xiangqi] starting position? Answer in curly brackets, e.g., {Yes} or {No}. For Replace modifications, [Added Piece Type] refers to the specific piece type that is added to the board through replacement, chosen from: For chess: Pawn, Knight, Bishop, Rook, Queen, or King For xiangqi: Soldier, Horse, Elephant, Chariot, Cannon, Advisor, or General For Replace modifications, we ask about the added piece type rather than total count because this more effectively tests whether VLMs rely on prior knowledge of standard piece distributions or actually inspect the board carefully. Ground truth calculation We calculate the ground truth answers for each prompt type: Total piece count (Remove modifications only): Correct answer: 31 (one fewer than the standard 32 pieces) Expected bias: 32 (the standard piece count) Added piece type count (Replace modifications only): Correct answer: The standard count for that piece type plus one For example, if Knight is replaced with Bishop in chess, the Bishop count would be 3 (standard 2 + 1 added) Expected bias: The standard count for that piece type (e.g., 2 for Bishops in chess) This tests if VLMs rely on their knowledge of standard piece counts or actually inspect the board Starting position question (Both modification types): Correct answer: Always No (since the board has been modified) Expected bias: Yes (since the board closely resembles the starting position) F.3 Qualitative results 35 Chess Pieces (a): How many chess pieces are there on this board? Answer with number in curly brackets, e.g., {9}. (b): How many Pawn pieces are there on this board? Answer with number in curly brackets, e.g., {9}. textbf(c): How many xiangqi pieces are there on this board? Answer with number in curly brackets, e.g., {9}. (d): How many General pieces are there on this board? Answer with number in curly brackets, e.g., {9}. (a) Chess (b) Chess (c) Xiangqi (d) Xiangqi 32 32 28 31 32 32 31 Bias GT 16 16 16 17 17 16 17 Gemini-2.5 Pro Sonnet-3.7 32 32 32 32 32 32 31 GPT-4. 2 2 2 2 2 2 3 o3 o4-mini Figure 17: VLMs are biased when counting the pieces on chess and xiangqi. 36 Task 5: Counting rows and columns of board game Figure 18: Data generation pipeline for Task 5: Counting rows and columns of board game G.1 Task design To evaluate VLMs over-reliance on visual bias versus actual counting, we adapted the row and column counting task from BlindTest [44] where Claude-3.5-Sonnet achieved 74.26% accuracy. Instead of simple grids, we leverage modified versions of well-known game boards to test whether VLMs rely on prior knowledge or perform actual visual counting. We design our task with careful control of visual parameters to ensure systematic evaluation: Board types: We use 4 different grid-based game boards: {Chess (88), Xiangqi (Chinese chess, 109), Sudoku (99), Go (1919)} Modification types: Each board has up to 4 types of modifications: Remove row: We remove exactly one row from the grid. Remove column: We remove exactly one column from the grid. Add row: We add exactly one row to the grid. Add column: We add exactly one column to the grid. Board-specific variations: For Chess, Xiangqi, and Sudoku boards, all four modifications (remove/add row, remove/add column) are visually distinct, with additional positional variations (first/last), resulting in 8 variants per board. Go boards have uniform grid structure, so we produce only 4 variations. Image resolutions: We generate each board at 3 different pixel sizes {384, 768, 1152}px to test resolution sensitivity. This systematic approach generates total of (8 variants 3 board types (Xiangqi/Chess/Sudoku) + 4 Go variants) 3 resolutions = 84 total images. G.2 Implementation and prompts Implementation details Our implementation utilizes specialized drawing libraries for each board type. For Chess, we use standard 88 chessboard grid generation with alternating square colors. For Xiangqi, we implement the traditional 109 board layout with river gap and palace diagonal lines. For Sudoku, we create 99 grids with bold 33 block boundaries and sample numbers. For Go, we generate uniform line grids with traditional star points. The algorithm for all board types follows the same sequence: 37 1. Create standard board with correct dimensions and visual elements 2. Apply systematic modifications (add/remove rows/columns at specific positions) 3. Maintain visual consistency of special elements 4. Render each modified board at three different resolutions The board-specific implementations required special handling for: Chess: Alternating light/dark square pattern preservation across dimension changes Xiangqi: River gap positioning and palace diagonal lines adjustment for row modifications Sudoku: Bold 33 block boundary lines based on original 99 grid structure Go: Uniform line spacing and star point positioning for various board sizes Quality control To ensure consistent image quality across all variants, we implemented several technical measures: SVG to PNG conversion: We used direct SVG rendering with adjustable scaling factors based on target resolution Quality scaling: We applied quality multiplier (5.0 base resolution factor) to ensure clear structural visibility Font and layout fidelity: Automatic detection and usage of appropriate fonts, particularly critical for Xiangqi (Chinese characters) and Sudoku (numbers) Table 9: All VLMs performance is extremely low (2.26%) across game boards, confirming that current VLMs are largely unable to perform even simple counting operations in structured visual settings"
        },
        {
            "title": "Chess",
            "content": "Go"
        },
        {
            "title": "Sudoku Xiangqi Mean",
            "content": "Gemini-2.5 Pro Sonnet-3.7 GPT-4.1 o3 o4-mini"
        },
        {
            "title": "Mean",
            "content": "2.08 0.00 0.00 0.00 16.67 3.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 6.25 6.25 0.00 8.33 0.00 4.17 2.38 1.79 0.00 2.38 4.76 2.26 Prompts We use different prompts for different question types to test VLMs visual counting versus prior knowledge: 1. Counting questions (Q1 & Q2): Q1 (Chess): How many [rows/columns] are there on this board? Answer with number in curly brackets, e.g., {9}. Q1 (Xiangqi, Go): How many [horizontal/vertical] are there on this board? Answer with number in curly brackets, e.g., {9}. Q1 (Sudoku): How many [rows/columns] are there on this puzzle? Answer with number in curly brackets, e.g., {9}. Q2 (Chess): Count the [rows/columns] on this board. Answer with number in curly brackets, e.g., {9}. Q2 (Xiangqi, Go): Count the [horizontal/vertical] lines on this board. Answer with number in curly brackets, e.g., {9}. Q2 (Sudoku): Count the [rows/columns] on this puzzle. Answer with number in curly brackets, e.g., {9}. 2. Y/N identification questions (Q3): Q3 (Chess): Is this 88 Chessboard? Answer in curly brackets, e.g., {Yes} or {No}. 38 Q3 (Xiangqi): Is this 109 Xiangqi board? Answer in curly brackets, e.g., {Yes} or {No}. Q3 (Sudoku): Is this 99 Sudoku puzzle? Answer in curly brackets, e.g., {Yes} or {No}. Q3 (Go): Is this 1919 Go board? Answer in curly brackets, e.g., {Yes} or {No}. Ground truth calculation We calculate the ground truth answers for each prompt type: Row/Column count (Q1 & Q2): Correct answer: The actual number of rows/columns after modification. For example, if one row is removed from 99 Sudoku, the row count is 8. Expected bias: The standard count for that board type (e.g., 8 for Chess rows, 10 for Xiangqi horizontal lines, 9 for Sudoku rows, 19 for Go horizontal lines) Standard layout question (Q3): Correct answer: Always No (since all boards have been modified from standard dimensions) Expected bias: Yes (since the boards closely resemble their standard counterparts) Game Boards (a): How many columns are there on this puzzle? Answer with number in curly brackets, e.g., {9}. (b), (c): How many horizontal lines are there on this board? Answer with number in curly brackets, e.g., {9}. (d): How many rows are there on this board? Answer with number in curly brackets, e.g., {9}. (a) Sudoku (b) Go (c) Xiangqi (d) Chess 9 9 9 9 9 9 8 Bias GT 13 19 19 19 19 20 Gemini-2.5 Pro Sonnet-3.7 10 10 10 10 12 10 11 GPT-4.1 6 8 8 8 8 8 7 o3 o4-mini Figure 19: VLMs are biased when counting the rows and columns on game boards. Task 6: Visual testing with both original and modified optical illusion Figure 20: Data generation pipeline for Task 6: Visual testing with both original and modified optical illusion H.1 Task design Recent VLMs show improved performance on optical illusion tasks, with o4-mini achieving 71.49% accuracy on IllusionVQA. However, these VLMs might have merely memorized the common optical illusions rather than truly perceiving visual information. To investigate this hypothesis, we test their ability to correctly identify illusion effects on both original and strategically modified versions. We design our task with careful control of visual parameters to ensure systematic evaluation: Illusion types: We use 6 different classical optical illusions: {Ebbinghaus, Müller-Lyer, Ponzo, Vertical-Horizontal, Zöllner, Poggendorff } Condition types: Each illusion has 2 conditions: Original: Standard illusion where the visual effect should occur (e.g., two identical circles appearing different sizes). Modified: Reversed version where the actual measurements contradict the typical illusion effect (e.g., circles that are genuinely different sizes). Parameter variations: We generate multiple combinations of illusion parameters: Most illusions: 12 original + 12 modified versions with varying illusion strength and difference Vertical-Horizontal: 6 original + 6 modified versions (fixed T-shape structure) Image resolutions: We generate each illusion at 3 different pixel sizes {384, 768, 1152}px to test resolution sensitivity. This systematic approach generates total of (12 original + 12 modified) 5 illusion types + (6 original + 6 modified) 1 Vertical-Horizontal illusion) 3 resolutions = 396 total images. H.2 Implementation and prompts Implementation details Our implementation adapts code from Pyllusion (https://github.com/ RealityBending/Pyllusion) to generate consistent, parametrically controlled optical illusions. We systematically vary two key parameters: illusion strength (which controls the intensity of contextual elements that create the illusion effect, representing how strongly the surrounding context biases perceptual experience) and difference (which controls the objective, actual difference between target elements being compared, where 0 means identical elements and non-zero values create genuine physical differences). The algorithm for all illusion types follows the same sequence: 1. Define parameter ranges for each illusion type (strength values, difference values). 2. Generate original versions with standard illusion parameters (diff=0 for equal elements). 3. Generate modified versions with reversed parameters (diff=0 for unequal elements). 40 4. Render each illusion variant at three different resolutions. The illusion-specific implementations required special parameter handling for: Ebbinghaus: Varying surrounding circle sizes (strength) and central circle differences (difference). Müller-Lyer: Different arrowhead angles (strength) and line length differences (difference). Ponzo: Perspective line angles (strength) and horizontal bar length differences (difference). Vertical-Horizontal: Fixed T-shape with varying line length ratios (difference). Zöllner: Background line angles (strength) and main line parallelism differences (difference). Poggendorff: Interrupting rectangle positions (strength) and diagonal line alignments (difference). Quality control To ensure consistent image quality and valid illusion effects across all variants, we implemented several technical measures: Parameter validation: Ensured all strength and difference values produce visually meaningful illusions, with diff= 0 cases design to be easily recognizable by humans to distinguish actual physical differences from perceptual biases clearly. Balanced generation: Equal numbers of diff=0 (original) and diff=0 (modified) cases per illusion type Prompts We use consistent prompts across illusion types to test VLMs visual perception versus memorized knowledge: 1. Main questions (Q1 & Q2): Q1 (Ebbinghaus): Are the two inner circles equal in size? Answer in curly brackets, e.g., {Yes} or {No}. Q1 (Müller-Lyer, Ponzo): Are the two horizontal lines equal in length? Answer in curly brackets, e.g., {Yes} or {No}. Q1 (Vertical-Horizontal): Are the horizontal and vertical lines equal in length? Answer in curly brackets, e.g., {Yes} or {No}. Q1 (Zöllner): Are the two horizontal lines parallel? Answer in curly brackets, e.g., {Yes} or {No}. Q1 (Poggendorff): Are the two diagonal line segments aligned? Answer in curly brackets, e.g., {Yes} or {No}. Q2 (Ebbinghaus): Do the two inner circles have the same size? Answer in curly brackets, e.g., {Yes} or {No}. Q2 (Müller-Lyer): Do the two horizontal lines have the same length? Answer in curly brackets, e.g., {Yes} or {No}. Q2 (Ponzo): Do the two horizontal lines have the same length? Answer in curly brackets, e.g., {Yes} or {No}. Q2 (Vertical-Horizontal): Do the horizontal and vertical lines have the same length? Answer in curly brackets, e.g., {Yes} or {No}. Q2 (Zöllner): Do the two horizontal lines run parallel? Answer in curly brackets, e.g., {Yes} or {No}. Q2 (Poggendorff): Do the two diagonal lines form straight line? Answer in curly brackets, e.g., {Yes} or {No}. 2. Y/N identification questions (Q3): Q3: Is this an example of the [Ebbinghaus/Müller-Lyer/Ponzo/VerticalHorizontal/Zöllner/Poggendorff] illusion? Answer in curly brackets, e.g., {Yes} or {No}. Ground truth calculation We calculate the ground truth answers based on the actual measurements in each image: Counting questions (Q1 & Q2): Correct answer: * Original illusions (diff=0): Elements are actually equal, so the correct answer is Yes * Modified illusions (diff=0): Elements are actually different, so the correct answer is No Expected bias: * Original illusions: VLMs might incorrectly say No expecting the illusion effect to make equal elements appear different * Modified illusions: VLMs might incorrectly say Yes expecting the illusion to make genuinely different elements appear equal Y/N identification questions (Q3): Correct answer: * Original illusions: Yes (standard examples of the specified illusion type). * Modified illusions: No (modified versions that contradict typical illusion effects). Expected bias: * Original illusions: VLMs likely correctly identify as Yes since they match memorized illusion patterns * Modified illusions: VLMs may incorrectly say Yes if they rely on visual similarity rather than recognizing the effect contradiction H.3 Qualitative results Abstract images: Optical Illusions (a) Original Müller-Lyer (b) Modified Müller-Lyer (c) Original Zöllner (d) Modified Zöllner (e) Original Ebbinghaus (f) Modified Ebbinghaus Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes No Yes No Yes Yes No No Yes Yes No Yes Yes Yes Yes No Bias GT Gemini-2.5 Pro Sonnet-3.7 GPT-4. o3 o4-mini (a), (b): Are the two horizontal lines equal in length? Answer in curly brackets, e.g., {Yes} or {No}. (c), (d): Are the two horizontal lines parallel? Answer in curly brackets, e.g., {Yes} or {No}. (e), (f): Are the two inner circles equal in size? Answer in curly brackets, e.g., {Yes} or {No}. Figure 21: VLMs show systematic biases, often relying on prior knowledge about rather than directly interpreting the image. optical illusions Task 7: Counting circles or lines in an anomaly cell within patterned grid Figure 22: Data generation pipeline for Task 7: Counting circles or lines in an anomaly cell within patterned grid I.1 Task design VLMs can infer patterns from nearby visual elements to answer visual questions [23]. To evaluate whether VLMs rely on pattern recognition over actual visual counting, we create square grids with systematic numerical patterns (represented visually by dice faces or tally marks) where exactly one cell violates the expected pattern. We hypothesize that VLMs will prioritize the inferred pattern over the actual visual information and report the expected pattern-completing value instead of the true count. We design our task with careful control of visual parameters to ensure systematic evaluation: Grid types: We use 2 different visual representation types: {dice (circular dots in dice-face patterns), tally (traditional tally mark lines)}. Modification types per grid type: For each grid type, we apply 2 distinct types of cell-level modifications: Dice grids: Remove (one dot is removed from cell) and Replace (one dot is replaced with different shape, like square or star, within cell). Tally grids: Remove (one tally line is removed from cell) and Add (one extra tally line is added to cell). Grid Dimensions: We generate grids of 7 different dimensions, ranging from 66 to 1212 cells. Unique scenarios for anomaly placement (single anomaly per grid image): To create 14 distinct base settings for placing anomalies, where each final grid image will feature only single modified cell. We proceed as follows: for each of the 7 grid dimensions, we define two separate base settings. Each of these two settings for given grid dimension involves selecting different, unique cell location to be the sole anomaly cell for images generated under that specific setting. These potential anomaly cell locations are carefully chosen to avoid edges and corners. This gives us (7 grid dimensions 2 distinct choices of single anomaly cell location per dimension) = 14 distinct base settings. For each of these 14 base settings (defined by grid dimension and the location of its single anomaly cell), we then apply all combinations of grid types and their respective modifications to generate the final images, each still containing only that one pre-determined anomaly. Image resolutions: Each generated grid image is rendered at 3 different pixel sizes {384, 768, 1152}px to assess sensitivity to image resolution. This systematic generation process yields total of 2 (grid types) 2 (modification types) 14 (unique scenarios) 3 (resolutions) = 168 distinct images. I.2 Implementation and prompts Implementation details Our implementation generates systematic pattern grids using distancefrom-edge algorithm to create naturally increasing-then-decreasing numerical patterns. For dice grids, 43 we use circular dots arranged in traditional dice-face configurations (1-6 dots per cell). For tally grids, we render authentic tally marks with proper grouping (four vertical lines crossed by diagonal fifth line). The algorithm for both grid types follows the same sequence: 1. Generate base grid with pattern-consistent cell counts using distance-from-edge calculation 2. Organize target positions across 14 groups, with each group containing both dice and tally variants 3. For each target cell, create modification variants: Dice: Remove one dot OR replace one dot with alternative shape (triangle, square, star) Tally: Remove one line OR add one extra line 4. Render each modified grid at three different resolutions with consistent visual quality The grid-specific implementations required special handling for: Dice pattern consistency: Maintaining standard dice-face arrangements (1-6 dots) while allowing single-dot modifications Tally mark authenticity: Proper grouping of marks with diagonal crosses for every fifth line Pattern calculation: Distance-from-edge algorithm ensuring natural numerical progression across grid cells Cell positioning: Strategic selection of anomaly cells away from edges to preserve pattern context Quality control To ensure consistent image quality and valid pattern recognition challenges across all variants, we implemented several technical measures: SVG to PNG conversion: We used direct SVG rendering with adjustable scaling factors based on target resolution Quality scaling: We applied quality multiplier (5.0 base resolution factor) to ensure clear shape and line visibility Prompts We use consistent prompts across both grid types to test VLMs pattern recognition versus actual visual counting: 1. Counting questions (Q1 & Q2): Q1 (Dice): How many circles are there in cell [CellID]? Answer with number in curly brackets, e.g., {9}. Q1 (Tally): How many lines are there in cell [CellID]? Answer with number in curly brackets, e.g., {9}. Q2 (Dice): Count the circles in cell [CellID]. Answer with number in curly brackets, e.g., {9}. Q2 (Tally): Count the lines in cell [CellID]. Answer with number in curly brackets, e.g., {9}. 2. Y/N identification questions (Q3): Q3 (Dice): Does cell [CellID] contain [ExpectedCount] circles? Answer in curly brackets, e.g., {Yes} or {No}. Q3 (Tally): Does cell [CellID] contain [ExpectedCount] lines? Answer in curly brackets, e.g., {Yes} or {No}. For all prompts, [CellID] refers to the specific anomaly cell using standard spreadsheet notation (e.g., C3, F7), and [ExpectedCount] represents the pattern-consistent count that would be expected based on surrounding cells. Ground truth calculation We calculate the ground truth answers based on the actual visual content in each modified cell: 44 Direct counting questions (Q1 & Q2): Correct answer: The actual count of visual elements in the target cell after modification * For Remove modifications: Standard pattern count minus 1 * For Add modifications: Standard pattern count plus 1 * For Replace modifications: Standard pattern count minus 1 (since one circle is replaced with different shape) Expected bias: The pattern-consistent count that VLMs might infer from surrounding cells, ignoring the actual modification Pattern-based verification question (Q3): Correct answer: Always No (since the target cell has been modified to break the pattern) Expected bias: Yes (if VLMs rely on pattern inference rather than direct visual inspection) I.3 Qualitative results Abstract Images: Patterned Grid (a) Dice Remove (b) Dice Replace (c) Tally Remove (d) Tally Add 3 2 3 3 3 2 Bias GT Gemini-2.5 Pro 3 2 3 2 3 2 Sonnet-3.7 2 2 3 3 2 3 2 GPT-4. o3 3 4 4 4 3 3 4 o4-mini (a), (b): How many circles are there in cell C3? Answer with number in curly brackets, e.g., {9}. (c), (d): How many lines are there in cell C3? Answer with number in curly brackets, e.g., {9}. Figure 23: All VLMs, except patterned grids. the Sonnet-3.7, fail to correctly identify the abnormal cell (C3) in both"
        },
        {
            "title": "J Qualitative results on",
            "content": "animals How many legs does this animal have? Answer with number in curly brackets, e.g., {9}. Baseline: How many legs does this animal have? Answer with number in curly brackets, e.g., {9}. w/ Double-Check: Please double-check your answer and give your final answer in curly brackets, following the format above. w/ Debiased Prompt: Do not assume from prior knowledge and answer only based on what is visible in the image. Ground truth: 3 Gemini-2.5 Pro (Baseline): {2} Gemini-2.5 Pro (w/ Double-Check): {2} Gemini-2.5 Pro (w/ Debiased Prompt): {2} Sonnet-3.7 (Baseline): The image shows rooster (male chicken) standing on grassy field. Chickens have two legs, which are clearly visible in this image. The rooster is standing upright on its two pale yellow/tan legs with its feet planted on the grass. {2} Sonnet-3.7 (w/ Double-Check): Looking at the image again, can clearly see the rooster is standing on two legs. Chickens are bipedal animals, with two legs that they use for walking and standing. In this particular photo, both of the chickens legs are visible beneath its body. {2} Sonnet-3.7 (w/ Debiased Prompt): Based solely on what is visible in the image, this chicken has {2} legs. The image clearly shows rooster standing on grassy field with its two yellow/tan-colored legs and feet visible beneath its body. GPT-4.1 (Baseline): {4} GPT-4.1 (w/ Double-Check): {4} GPT-4.1 (w/ Debiased Prompt): {4} o3 (Baseline): {2} o3 (w/ Double-Check): {2} o3 (w/ Debiased Prompt): {4} o4-mini (Baseline): {2} o4-mini (w/ Double-Check): {2} o4-mini (w/ Debiased Prompt): {4} Figure 24: VLMs fail 100% of the time, even on simple tasks like counting chicken legs, despite helpful prompts such as debiasing or double-checking."
        },
        {
            "title": "K Qualitative results on flags",
            "content": "How many stripes are there in this flag? Answer with number in curly brackets, e.g., {9}. Baseline: How many stripes are there in this flag? Answer with number in curly brackets, e.g., {9}. w/ Double-Check: Please double-check your answer and give your final answer in curly brackets, following the format above. w/ Debiased Prompt: Do not assume from prior knowledge and answer only based on what is visible in the image. Ground truth: 12 Gemini-2.5 Pro (Baseline): {13} Gemini-2.5 Pro (w/ Double-Check): {13} Gemini-2.5 Pro (w/ Debiased Prompt): {13} Sonnet-3.7 (Baseline): The image shows the flag of the United States of America. The American flag has 13 alternating red and white horizontal stripes (7 red and 6 white), representing the original 13 colonies. The flag also features blue rectangle in the upper left corner containing 50 white stars, representing the 50 states. To answer your specific question: {13} Sonnet-3.7 (w/ Double-Check): Ill double-check the American flags design. The flag of the United States consists of: - blue rectangle in the upper left corner (the union) containing 50 white stars - Horizontal stripes alternating between red and white. Looking more carefully at the image, can count 13 horizontal stripes total (7 red and 6 white). The final answer is: {13} Sonnet-3.7 (w/ Debiased Prompt): Based only on what is visible in the image, can see the American flag with its distinctive design. The flag has alternating red and white horizontal stripes, and in the image shown, can count total of {7} stripes. There are 4 red stripes and 3 white stripes visible in this particular rendering of the flag. GPT-4.1 (Baseline): {13} GPT-4.1 (w/ Double-Check): {13} GPT-4.1 (w/ Debiased Prompt): {13} o3 (Baseline): {13} o3 (w/ Double-Check): {13} o3 (w/ Debiased Prompt): {13} o4-mini (Baseline): {13} o4-mini (w/ Double-Check): {13} o4-mini (w/ Debiased Prompt): {13} Figure 25: VLMs fail and remain biased toward 13 even when one stripe is removed from the U.S. flag."
        },
        {
            "title": "L More findings",
            "content": "L.1 VLMs struggle to detect logo modifications, often relying on context rather than visual detail Experiments We replicate the experiment from Sec. 4.2 on our on modified shoe and car logo images. logo task, evaluating five VLMs Results VLM performance on car logos (0.44%; Tab. 3 ) is significantly worse than on shoe logos (17.57%; Tab. 3 ), as the emblem is small relative to the vehicle (see Fig. 1b). In contrast, shoe logos occupy more image area (see Fig. 3e) and involve only few simple curves or stripes (i.e., one extra curve for Nike, one added stripe for Adidas). These results highlight two key limitations: VLMs fail to attend to small, context-embedded visual changes and instead rely on memorization, without visually verifying the logo itself (e.g., by zooming in [53]). L.2 VLMs fail to count visual elements in modified flags Experiments We follow the procedure from Sec. 4.2 on our flag tasks. Five VLMs are prompted to count either the number of stars or the number of stripes in original and modified versions of national flags. Modifications consist of adding or removing single star or stripe, and each model uses its default settings. Results VLMs achieve higher mean accuracy on star modifications (11.79%; Tab. 3 ) than on stripe modifications (4.52%; Tab. 3 ). This pattern indicates that models are somewhat more attuned to discrete symbol changes (missing or extra stars; see Fig. 5d) than to subtle structural alterations (extra or missing stripes; see Fig. 5b), yet overall sensitivity to flag modifications is extremely limited (9.25%; Tab. 3 ). L.3 Thinking models better detect piece changes in modified chess starting positions Experiments We evaluate five VLMs on chess-piece counting task using standard starting positions for both Western chess and xiangqi. For each board type, we generate images in which exactly one piece is either removed or replaced by another piece of the same color. All models use their default settings and are prompted to report the total number of pieces or number of certain piece (e.g., Knights) on the board. Results VLMs perform significantly better on Western chess (see Fig. 1 ) than on xiangqi (see Fig. 5a) in terms of mean accuracy (29.86 % vs. 22.64%; Tab. 3 ). Thinking models ( Geminio4-mini) all exceed 26% accuracy, whereas non-thinking models ( GPT2.5 Pro, 4.1 and Sonnet-3.7) remain below 10% (Tab. 3 ). This suggests that on well-structured abstract images, models with explicit reasoning capabilities are better able to detect anomalies. o3, and L.4 VLMs cannot count rows and columns in simple board game grids"
        },
        {
            "title": "Experiments",
            "content": "Following our previous tasks, we evaluate five VLMs on counting tasks in four grid-based game boards: chess (88), Go (1919), Sudoku (99), and xiangqi (109). For chess (see Fig. 5e) and Sudoku (see Fig. 5c), models are asked to report the number of rows and columns. For Go and xiangqi (see Fig. 3f), they report the counts of horizontal and vertical lines. Results All VLMs perform extremely poorly on board game grid counting, (2.26% mean accuracy; Tab. 9). The models even failed to answer any counting questions correctly on Sudoku (see Fig. 5c) and Go (0%; Tab. 9). These findings confirm that current VLMs are unable to execute basic visual counting tasks in structured settings and instead default to overconfident but incorrect guesses. L.5 VLMs predominantly make bias-aligned errors rather than random mistakes when incorrect When VLMs fail on visual tasks, understanding the nature of their errors provides crucial insights into their failure modes. If errors were randomly distributed, it might suggest fundamental visual processing limitations. However, if errors systematically align with learned biases, this indicates that VLMs are overriding visual evidence with memorized knowledge. To investigate this pattern, we analyze the distribution of error types across all VLMBias tasks to determine whether incorrect responses reflect genuine visual confusion or bias-driven overconfidence."
        },
        {
            "title": "M Prompts used for image generation and image editing",
            "content": "Table 10: Prompts used for image generation and image editing with Gemini-2.0 Flash and GPT-4o by topic and prompt type Topic Prompt type Prompt Animal suggestions Animals Animal generation \"<Common Animal Name>\", \"num_legs\": <Typical Generate JSON list containing 100 animal objects. Each object should represent common animal and follow the structure below: { \"name\": Number of Legs> } Ensure the following for each animal: 1. the number of legs of this animal is 2 or 4. 2. the animals legs must be long enough to be seen easily from the body using side-view perspective. Prioritize animals whose legs are thin and/or long. Generate clear, full-body, side-view image of a(n) {animal} with {num_legs} legs that is walking in real-world natural background. The {num_legs}-legged animal must look photo-realistic in nature. All {num_legs} legs must be clearly visible. Animal editing Edit this image: Add 1 more leg to the {animal} so that it has {num_leg} legs in total. The {num_leg}-legged {animal} must be photo-realistic. All {num_leg} legs must be clearly visible. Flags Flag suggestions Generate JSON list of flags objects. Each object should represent well-known flags and follow the structure below: { \"name\": \"<Flag Name>\", \"original_stripes\" or \"original_stars\": <Number of Stripes or Stars (whichever applicable)> } 1. Ensure that the number of stars is more than 3, and the number of stripes is at least 5. 2. Ensure that the flag does not contain any other geometrically complex elements (depicting of animal, letters, etc.). 3. Prioritize well-known flags. Flag SVG code editing You are an expert in editing SVG image code. Modify the SVG code of the flag of {country} according to the following instruction: Instruction: \"The flag of {country} has {num_ele} {element}. Modify the SVG code so that it has num_ele + 1 {element} instead. Make sure the modified {element} are natural looking and integrate seamlessly on the new flag.\" Base SVG code: {svg_code} 1. Modify the base SVG by adding or removing the mentioned feature (stars, stripes, etc.) according to the instruction above. 2. Wrap the entire SVG in <code></code>. Do not explain anything. 50 Table 11: Prompts used for image generation and image editing with Gemini-2.0 Flash and GPT-4o by topic and prompt type Topic Prompt type Logo suggestion Logos Shoe generation Prompt Generate JSON list of subtle logo modification prompts and corresponding VLM question prompts to test visual bias. For each entry: Slightly modify the visual components of well-known car or sportswear logo. The selected logo must be geometrically simple and widely recognized. You must include generation prompt to create the altered image. Include question prompt (e.g., \"How many...\"). Include metadata: element being modified, actual count (ground truth), common expected count (bias). <In-context learning example 1> <In-context learning example 2> an Generate with {expected_bias}. {actual_count} {shoe_brand} style {modified_element} running shoe instead but of Shoe background generation Generate side-view image of an athlete wearing this pair of shoes. Keep all the fine-grained details of the shoes, particularly the {actual_count} {modified_element} on both shoes. The person is playing {sports_type}, showing their sports_type skills, and is wearing {sports_type} outfit. Zoom out bit to see their full body. Car logo generation Car background generation Generate {modified_element} instead of {expected_bias}. {car_brand} but with logo {actual_count} Generate photo-realistic front-view image of {color} {car_brand} {body_type} on the road in the middle of the day. Zoom out bit so that we can see the road."
        },
        {
            "title": "N Questions for sanity check",
            "content": "Topic Animal Logo Flags Table 12: Examples of Sanity check questions Identification questions Counting/Illusion questions What animal is this? Answer in curly brackets, e.g., {Fish}. How many legs do this animal have? Answer with number in curly brackets, e.g., {9}. What car logo is this? Answer in curly brackets, e.g., {Toyota}. How many overlapping circles are there on the logo of this car? Answer with number in curly brackets, e.g., {9}. What country flag is this? Answer in curly brackets, e.g., {Flag of Vietnam}. How many stars are there in this flag? Answer with number in curly brackets, e.g., {9}. Chess Pieces What board game is this? Answer in curly brackets, e.g., {Shogi}. How many chess pieces are there on this board? Answer with number in curly brackets, e.g., {9}. Game Boards What board game is this? Answer in curly brackets, e.g., {Shogi}. How many rows are there on this board? Answer with number in curly brackets, e.g., {9}. Optical Illusions What optical illusion is this? Answer in curly brackets, e.g., {Delboeuf illusion}. This image shows the Ebbinghaus illusion. What question does this illusion typically ask, and what is the correct answer?"
        }
    ],
    "affiliations": [
        "Auburn University",
        "College of William and Mary",
        "KAIST",
        "University of Alberta"
    ]
}