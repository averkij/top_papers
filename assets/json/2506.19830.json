{
    "paper_title": "Scaling Speculative Decoding with Lookahead Reasoning",
    "authors": [
        "Yichao Fu",
        "Rui Ge",
        "Zelei Shao",
        "Zhijie Deng",
        "Hao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning models excel by generating long chain-of-thoughts, but decoding the resulting thousands of tokens is slow. Token-level speculative decoding (SD) helps, but its benefit is capped, because the chance that an entire $\\gamma$-token guess is correct falls exponentially as $\\gamma$ grows. This means allocating more compute for longer token drafts faces an algorithmic ceiling -- making the speedup modest and hardware-agnostic. We raise this ceiling with Lookahead Reasoning, which exploits a second, step-level layer of parallelism. Our key insight is that reasoning models generate step-by-step, and each step needs only to be semantically correct, not exact token matching. In Lookahead Reasoning, a lightweight draft model proposes several future steps; the target model expands each proposal in one batched pass, and a verifier keeps semantically correct steps while letting the target regenerate any that fail. Token-level SD still operates within each reasoning step, so the two layers of parallelism multiply. We show Lookahead Reasoning lifts the peak speedup of SD both theoretically and empirically. Across GSM8K, AIME, and other benchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x while preserving answer quality, and its speedup scales better with additional GPU throughput. Our code is available at https://github.com/hao-ai-lab/LookaheadReasoning"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 0 3 8 9 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Scaling Speculative Decoding with\nLOOKAHEAD REASONING",
            "content": "Yichao Fu1 Rui Ge2 Zelei Shao3 Zhijie Deng2 Hao Zhang1 1UCSD 2Shanghai Jiao Tong University 3UIUC"
        },
        {
            "title": "Abstract",
            "content": "Reasoning models excel by generating long chain-of-thoughts, but decoding the resulting thousands of tokens is slow. Token-level speculative decoding (SD) helps, but its benefit is capped, because the chance that an entire γ-token guess is correct falls exponentially as γ grows. This means allocating more compute for longer token drafts faces an algorithmic ceiling making the speedup modest and hardware-agnostic. We raise this ceiling with LOOKAHEAD REASONING, which exploits second, step-level layer of parallelism. Our key insight is that reasoning models generate step-by-step, and each step needs only to be semantically correct, not exact token matching. In LOOKAHEAD REASONING, lightweight draft model proposes several future steps; the target model expands each proposal in one batched pass, and verifier keeps semantically correct steps while letting the target regenerate any that fail. Token-level SD still operates within each reasoning step, so the two layers of parallelism multiply. We show LOOKAHEAD REASONING lifts the peak speedup of SD both theoretically and empirically. Across GSM8K, AIME, and other benchmarks, LOOKAHEAD REASONING improves the speedup of SD from 1.4x to 2.1x while preserving answer quality, and its speedup scales better with additional GPU throughput. Our code is available at https://github. com/hao-ai-lab/LookaheadReasoning"
        },
        {
            "title": "Introduction",
            "content": "Large reasoning models (LRMs) have recently pushed the state of the art in math problem solving and program synthesis by generating explicit, long Chains of Thoughts (CoT) [1]. In these models, an answer unfolds as sequence of intermediate reasoning steps, and each step arrives token by token via autoregressive decoding. If solution needs steps and each step needs tokens, the model must generate O(N ) tokens, often running into tens of thousands of tokens and minutes of wall-clock time. For instance, OpenAIs o1 model [2] may take more than 2 minutes to solve single problem from the International Mathematical Olympiad (IMO) challenges. Speculative decoding (SD) mitigates this token-level dependency by spending additional FLOPs to shorten the critical path of generation: cheap draft model proposes γ future tokens and the expensive target model then verifies them in parallel; if every guess matches, the decoding can fast forward γ + 1 positions at once. However, in the face of LRMs with long decode, two facts limit how far this idea can scale. First, the probability of an entire γ-token sequence is correct drops almost exponentially with γ (2), so the expected number of accepted tokens quickly saturates as γ grows. Second, the verifier must still verify the target logits for all γ positions, and that cost grows linearly. This results in speedup curve that climbs with small γ, plateaus after few dozen tokens, and can even decline once the verification cost dominates. For example, in real profiling, we observe SDs speedup caps at 1.4x (Figure 2). As this ceiling is algorithmic rather than hardware-bound, this means that allocating more FLOPs in SD yields only diminishing returns, making SDs acceleration not scale with future accelerators. As LRMs produce ever-longer generations, the number of tokens SD can safely skip does not grow proportionally, so the end-to-end latency remains substantial. Preprint. Under review. Figure 1: One cycle of LOOKAHEAD REASONING. The draft model proposes γ = 3 steps { ˆs1, ˆs2, ˆs3}. The target model then generate {s1, s2, s3} based on prefixes and { ˆs1, ˆs2, ˆs3}, respectively. Verifier checks if draft and target steps are semantically equivalent (e.g., s1 ˆs1). If the first two steps are equivalent but the third is not, LOOKAHEAD REASONING outputs the verified draft steps ( ˆs1, ˆs2) followed by the targets correction (s3). This allows accepting multiple steps with only lowered latency (e.g., 2t + ) compared to the sequential target calls in autoregressive decoding (e.g., 3T ), where is draft step time and is target step time. Figure 2: Speedup vs Draft Tokens. Speedup over autoregressive decoding, comparing LOOKAHEAD REASONING combined with token-level SD (NGram-based) (red line) to SD alone (blue line). Our method is orthogonal to token-level SD and improves the maximum speedup from 1.4 to 2.1. This paper makes key observation that reasoning is naturally hierarchical: full chain-of-thought breaks into discrete steps, and each step unrolls tokens by token. To reach the correct answer, reasoning step requires only semantic correctness but not exact token matches. To illustrate, we replaced over 50% of DeepSeek-R1 32Bs reasoning steps with semantically equivalent ones from another smaller model. The impact on overall task accuracy was minimal, with deviations typically not exceeding 2% (4.1). This looser requirement exposes coarser unit for speculation: in addition to guessing the next few tokens, model can guess and verify the next few reasoning steps. These step proposals and verification are independent, so they can be batched and executed in parallel, making full use of GPUs batching capacity. At the same time, token-level speculation can still operate within each step, achieving two complementary layers of parallelism rather than one. This paper develops LOOKAHEAD REASONING based on this insight, with one operational cycle shown in Figure 1. First, lightweight draft model autoregressively generates several sequential, future reasoning steps {ˆs1, ˆs2, ˆs3}. Concurrently, the target LRM generates corresponding followup steps {s1, s2, s3}, where each si is generated based on prefix formed by the initial context concatenated with the sequence of preceding draft steps ˆs1, . . . , ˆsi1. Notably, the generations of {s1, s2, s3} are issued as batch running in parallel to exploit additional cross-request parallelism on GPUs. lightweight verifier, implemented as small LLM-as-a-Judge or an embedding model, then begins with the first speculative step to determine if the drafts original speculative step ˆs1 semantically aligns with this orcale step s1. If the step passes verification, we keep it and proceed to the verification of ˆs2 and s2, which are already available due to batched execution. If it fails, we drop it and revert to the target models standard generation. Concurrently, token-level speculation can operate independently when the target/draft model generates the content of each step. 2 LOOKAHEAD REASONING operates at step level, an axis orthogonal to token-level speculation. Because step-level speculation absorbs compute that would otherwise hit the speedup ceiling by token-level speculation, the method scales better with hardware. Additional FLOPs can draft more (or deeper) steps instead of lengthening token guesses, sidestepping diminishing returns faced by token-level-only speculative decoding. For example, on GSM8K, LOOKAHEAD REASONING lifts token-level SDs peak speedup from 1.4x to 2.1x (combined), as depicted in Figure 2. Even when available compute is limited, we prove that the peak speedup given limited compute shall be achieved only by combining both levels of speculation (3.3). key design consideration is the choice of verifier. While an ideal semantic verifier ensures no accuracy loss, practical ones balance compute cost against judgment accuracy. For instance, looser verification may boost draft acceptance (and speedup) but risks accuracy drop from erroneous steps. We finally opted for 7B LLM-As-a-Judge, striking balance between these competing factors (4.3). To sum up, our contributions can be listed as follows: (1) We develop LOOKAHEAD REASONING, novel step-level speculation dimension to scale speculative decoding, orthogonal to existing tokenlevel approaches. (2) We present theoretical analysis demonstrating significant speedups from our method, both as standalone technique and when combined with token-level SD. (3) We conduct extensive experiments showing consistent performance improvements across diverse datasets."
        },
        {
            "title": "2 Background",
            "content": "Speculative Decoding. LLMs autoregressively generate one token at time, with each next token xt+1 sampled from the distribution (xt+1 x1:t). This sequential dependency poses fundamental bottleneck to inference speed. Speculative decoding [3] mitigates this challenge using guess-and-verify strategy with two models: lightweight draft model and strong target model p. Given context x1:t, autoregressively proposes sequence of γ candidates tokens, ˆxt+1:t+γ, along with their draft probabilities Q. Subsequently, verifies these γ tokens in single parallel forward pass, yielding the target probabilities . rejection-sampling procedure then sequentially processes each proposed token ˆxt+i. If token is accepted, it is appended to the output; if rejected, the process halts, and final token is sampled from ps distribution based on the last accepted token. This allows for the acceptance of γ tokens in fewer steps than standard autoregression. The theoretical speedup achieved by this speculative decoding approach, assuming negligible verification overhead beyond the target models single pass, can be characterized by: g(γ) = 1 αγ+1 (1 α) (1 + γ) , where α represents the average acceptance rate of each token drafted by (assuming its independent for each token in the sequence). Note that the probability of an entire sequence of γ tokens being accepted typically decreases exponentially with γ, as errors accumulate. = Tq/Tp is the latency ratio of generating single token by the draft model relative to the target model, where Tq and Tp are step time of and p, respectively. Furthermore, for all 0 and γ 0, the speedup g(γ) is upper-bounded by 1/(1 α), limit approached only in the idealized case where = 0. This inherent upper bound on g(γ) signifies critical limitation: beyond an optimal point, investing more computational resources by increasing the speculative length (γ) yields diminishing or even negative returns on speedup. Therefore, this algorithmic ceiling implies that the acceleration gains from token-level speculative decoding do not scale with improvements in hardware, such as more powerful GPUs. Consequently, for reasoning models with longer CoTs, the bounded acceleration offered by token-level speculative decoding alone highlights an urgent need for more potent acceleration strategies. LLM Reasoning. Large reasoning models (LRMs) [2, 4] are increasingly pivotal for complex tasks such as math problem-solving and coding. These models often generate solutions by giving \"chainof-thought\" (CoT)a sequence of intermediate reasoning steps, denoted as s, produced step-by-step to derive final answer [1]. Each reasoning step (si) typically conditions on the previous one (si1), creating sequential dependency analogous to token-level autoregression but at higher conceptual granularity. We observe that this step-wise structure itself presents significant opportunity for acceleration. Specifically, entire reasoning steps can be speculatively proposed by draft model, 3 denoted as ˆs. Our preliminary experiments (4.1) show this potential. small 1.5B draft model can generate speculative steps ˆs that semantically align with over 50% of ground-truth steps from much larger 32B target model. Besides, this is achieved while maintaining comparable task accuracy."
        },
        {
            "title": "3 Method",
            "content": "In this section, we explain the details of LOOKAHEAD REASONING, then provide theoretical analysis that shows its performance benefits. Furthermore, since both step-level and token-level speculative generation rely on increasing concurrency, we show that in real-world settingswhere the two methods compete for limited concurrency resourcespeak performance gains can only be achieved when combining both speculative strategies together. 3.1 LOOKAHEAD REASONING: Step-Level Speculative Generation Solution The core idea of LOOKAHEAD REASONING is to perform speculation and verification on entire steps rather than individual tokens. To put it clear, we first presented synchronous version of this approach in Algorithm 1, and then conceptually illustrate an optimized asynchronous variant in Figure 1. As detailed in Algorithm 1 (sync version), one cycle of LOOKAHEAD REASONING proceeds as follows: 1. Draft Step Generation: Given token prefix x1:t, the draft model first autoregressive generates sequence of γ candidate steps, denoted as ˆs0, . . . , ˆsγ1. Each step ˆsj is generated conditioned on the prefix extended by all preceding draft steps: x1:t (cid:76)j1 k=0 ˆsk. In practice, we simply use nn as the step break as we found its common flag for step split in various reasoning models [5, 4, 6]. 2. Parallel Target Step Generation: Same as in the standard speculative decoding [3], once all γ draft steps are available, target model generates following steps s0, . . . , sγ accordingly in parallel. 3. Verification and Output Construction: The algorithm then determines the longest prefix of accepted draft steps. Verification between each draft step ˆsj and its corresponding target step sj is performed by verifier (sj, ˆsj), which assesses whether if ˆsj is an acceptable substitute for sj, i.e. whether they are semantic similar. Algorithm 1 Lookahead Reasoning(Sync Version) Input: Draft model q, Target model p, Prefix x1:t, Max lookahead steps γ, Verifier (, ) {True, False} ˆsj .= q.GenerateStep(xcurrent); xcurrent 1: Initialize empty step sequences ˆs0, . . . , ˆsγ1, s0, . . . , sγ .= x1:t 2: xcurrent 3: for = 0 to γ 1 do 4: 5: in parallel do for = 0 to γ: .= x1:t (cid:76)j1 Let 6: .= p.GenerateStep(x j) sj 7: 8: end parallel 9: .= min({j {0..γ 1} (sj, ˆsj) == False} {γ}) 10: OutputSequence .= ((cid:76)j1 Output: OutputSequence k=0 ˆsk if 1 else x1:t .= xcurrent ˆsj k=0 ˆsk) sj Generate γ draft steps sequentially Compute target steps sj in parallel based on draft prefixes Prefix before draft step Find first unaccepted step using Append verified drafts + decisive target Verifier Selection. The choice of verifier (V ) is pivotal design consideration in LOOKAHEAD REASONING. While an ideal semantic verifier ensures no accuracy loss, practical implementations face primary trade-off between judgment precision and computational overhead; Furthermore, the strictness of verification (e.g., threshold) presents secondary trade-off, potentially boosting draft acceptance and speedup at the risk of degrading task accuracy from erroneously accepted steps. We explore three common paradigms for semantic assessmentLLM-as-a-Judge [7] for nuanced evaluation, embedding-based verifier [8] for efficient similarity, and target model scoring [9]each with distinct cost-precision profiles, empirically evaluating their impact in 4.3. Asynchronous Generation (Illustrated in Figure 1. In Algorithm 1, the parallel verification steps launch only after all γ draft steps ˆsj with {0...γ 1} are produced. An optimized asynchronous implementation, conceptually depicted in Figure 1, can begin generating target step sj as soon as its required prefix (containing x1:t, ˆs0, . . . , ˆsj1) becomes available from the draft model. This async execution brings overlap for draft/target generation and verification phases, which can significantly reduce end-to-end latency compared with the synchronous version. Note that in this asynchronous mode implementation, both the draft and target models will lookahead γ steps, ensuring maximal utilization of parallel processing. This is different from the sync version that draft model generate γ drafts while target model generate γ + 1 steps in each cycle. Multi-Branch Drafting. To further increase the number of the accepted reasoning steps, we explore tree-structure generation where the draft model proposes multiple candidate steps at each speculative position. Specifically, instead of generating single candidate chain, the draft can propose set of alternative steps for each position in the draft sequence. Once step is generated, the draft then proposes child candidates in parallel for the subsequent position + 1. This branching process continues up to maximum γ steps, leading to an exponential growth in the total number of candidate sequences explores, i.e., γ. The target model p, however, still generate one single candidate continuation step for each position (based on the draft prefix). The verifier would then check if any of the proposed draft branches for that position semantically aligns with the target models step. If such match is found, that branch is accepted and other branches are discarded. This multi-branch strategy aims to boost the likelihood of speculative success, albeit at the cost of increased computational effort in the drafting phase. We discussed its trade-off in 4.3. 3.2 Theoretical Speedup Analysis for LOOKAHEAD REASONING We now analyze the potential performance gains of LOOKAHEAD REASONING. We make simplifying assumptions for clarity: negligible verification overhead, constant cost for generating steps, and single draft branch at each stage. Notation: Let γ1 be the maximum number of draft steps generated sequentially, and k1 = γ1 be the number of target steps that generate in parallel. Let be the wall-time cost for the target model to generate one step, and c1T be the cost for the draft model (0 < c1 < 1). Let α1 (0, 1) be the probability that draft step is accepted. Step-Level Speedup for Sync Version of LOOKAHEAD REASONING: The latency speedup for sync Lookahead Reasoning is fsync(k1) = 1 αk1 1 (1 α1) (1 c1 + c1 k1) . Step-Level Speedup for Async Version of LOOKAHEAD REASONING: The speedup depends on whether the draft generation is limited by the maximum depth k1 or by the relative cost c1. Let Xi be the number of consecutively accepted draft steps in stage i. The expected number of accepted steps before rejection is E[Xi] = α1/(1 α1). We define the asymptotic speedup as the ratio of steps generated by LOOKAHEAD REASONING compared to target-only baseline over the same wall-time. Two cases arise: 1. k1 1/c1: The draft model is relatively slow, and generating k1 drafts takes longer than one target step. The depth limit k1 is effectively inactive. The speedup converges to: S1 = 1 + E[Xi] 1 + c1E[Xi] = 1 c1 + (1 c1)(1 α1) 2. k1 < 1/c1: The draft model is fast enough to generate k1 steps within time . The maximum depth k1 limits the number of speculative steps per cycle. The speedup converges to: S2 = E[1 + Xi] E[(Xi + 1)/k1 + c1(Xi mod k1)] = (1 α1) + c1 1 αk1 1 (cid:2)α1 αk+1 1 k1(1 α1)αk1 (cid:3) (Detailed derivations are provided in Appendix B). Let fasync(k1) denote the step-level speedup, where fasync(k1) = S1 or S2 depending on the case. 3.3 Optimal Speculation Strategies under Concurrency Constraints Token-level speculative decoding [3] and step-level speculative decoding are orthogonal to each other. If k2 is the number of additional tokens speculated by the draft model within each step generation 5 (for both draft and target models, assuming they use internal speculation) and c2 is the ratio of execution time of the draft model and target model in speculative decoding, its speedup is given by g(k2), based on the token acceptance rate α2: g(k2) = 1 αk2 2 (1 α2) (1 + k2) Since these mechanisms operate at different granularities (inter-step vs. intra-step), their speedups multiply, yielding combined speedup h(k1, k2) = (k1) g(k2). However, these two orthogonal parallelism dimensions compete for computational resources, making it crucial to determine the optimal resource allocation strategy to achieve maximum speedup. In this work, we focus on fundamental question: is using both speculation methods superior to applying only one method? Optimality of Hybrid Approach under Budget Constraint: In real-world systems, memory and computational constraints necessitate capping the total degree of parallelism (M ) available to the target model, i.e., arallelDimg arallelDimf for step-level and token-level speculative decoding methods, respectively. This constraint transforms our earlier question into resource allocation optimization problem: given finite parallel budget (M ), should we distribute resources across both parallelism dimensions or concentrate them on single method? Consequently, our design goal becomes: max arallelDimgP arallelDimf h(k1, k2) = (k1) g(k2). (1) Where arallelDimg = k2, and arallelDimf = (cid:26)k1, min{ sync , k1}, async Its easy to see that if we set k1 = 1, then we are using purely token-level speculative decoding, whereas if we set k2 = 1, then we are using purely lookahead reasoning. Theorem (Hybrid Method Optimality for Async Algorithm LOOKAHEAD REASONING): Under the conditions of acceptance rates (0.52 < α1, α2 < 0.8), reasonably efficient draft models (c1 < 3 , c2 < 1 1 5 ), and sufficient parallelism budget 16, the maximum speedup h(k1, k2) is achieved if and only if hybrid strategy is employed, meaning both k1 2 and k2 2. 3 and (c2) below 1 These conditions are broadly representative of real-world scenarios: acceptance rates (α1, α2) in the 0.52-0.8 range are common in speculative decoding [10] and our experiments (4.1); draft model efficiency ratios (c1) below 1 5 are also common; and the parallelism budget (M 16) reflects typical GPU capabilities. This analysis demonstrates that neither pure step-level nor pure token-level speculation is optimal under fixed parallelism budget. The highest theoretical speedup is obtained by judiciously combining both strategies, leveraging parallelism across steps (k1) and within steps (k2). This motivates architectures and systems that can effectively manage and exploit both levels of speculative execution. It is empirically validated in 4.2. We provide complete proof in Appendix B.1.2. Additionally, we analyze in detail the conditions under which single methods (either token-level or step-level) outperform hybrid approaches, and conversely, when combining both methods yields superior performance (Appendix B.1.2)."
        },
        {
            "title": "4 Experiment",
            "content": "Models. We evaluate two popular open-source reasoning model series: DeepSeek-R1-Distill [4] and Qwen3 [6]. For the DeepSeek-R1-Distill series, the 1.5B version serves as the draft model and the 32B version as the target model. Similarly, for the Qwen3 series, the 1.7B model is used as the draft model and the 32B model as the target. Unless otherwise specified, Qwen2.5-7B-Instruct [11] is employed as the judgement model. deliberately designed judge prompt template allows our model to assess the semantic alignment between two sentences in just one prefill pass (Appendix A). Datasets. Our evaluation spans suite of benchmarks, aligning with previous speculative decoding research [12, 13] and reasoning model evaluations [4, 6]. For code generation, we use HumanEval [14] and LiveCodeBench [15]. Math reasoning tasks are assessed using GSM8K [16], AIME24 [17], and 6 AMC1223 [18]. For question answering, we include GPQA [19] and MT-Bench [7]. Specific to dataset sampling, we utilize 40 out of 50 problems from AMC1223, selected by Qwen2.5 Math [20], and randomly sample 100 queries from the 1.3K GSM8K test set. For LiveCodeBench, We select 268 problems collected between August 2024 and Janaury 2025, following previous research [4]. General Parameters. LLM generation settings are configured specifically for each model series. For the DeepSeek-R1-Distill series, we adhere to the official settings with temperature of 0.6, top_p of 0.95, and maximum generation length of 32K. For the Qwen3 series, the temperature is set to 0.6, top_p to 0.95, min_p to 0, top_k to 20, and the maximum generation length is 37K. These maximum generation lengths are chosen to ensure complete outputs. We use prompt-lookup decoding (n-gram) [21] as representative speculative decoding (SD) method: the max lookup tokens is set to 1 for GSM8K and 2 for other datasets. The number of speculative tokens is set to 8 for SD and the number of speculative steps is set to 6 for LOOKAHEAD REASONING by default. Testbed. Experiments are conducted on server equipped with eight NVIDIA H100 GPUs. Target models (32B) are deployed across two H100 GPUs using tensor parallelism. Draft models (1.5B/1.7B) and the default judge model (Qwen2.5-7B-Instruct) are each deployed on single H100 GPU. Our algorithm is built upon the vLLM v0.8.3. Both the baseline and our proposed method are evaluated using vLLM [22] to simulate real-world LLM serving conditions. Evaluation Metrics. For code generation tasks (HumanEval, LiveCodeBench) and mathematical reasoning benchmarks (GSM8K, AIME24, AMC1223), we use accuracy (or pass@1). In question answering, accuracy is used for GPQA, while MT-Bench scores are obtained using Qwen2.5-72BInstruct [11] as the judge. The accuracy on livecodebench is averaged over 8 samples while the accuracy on other datasets are averaged over 16 samples. We calculate the acceptance rate over the entire generation process and the accuracy of the final generated text. The evaluation procedure works as follows: at each generation step, we obtain outputs from both the draft and target models, then use verifier to determine whether to accept or reject the draft output. The accepted result is added to the history trajectory, and this iterative process repeats until the end of generation is reached. 4.1 End-to-End Performance of LOOKAHEAD REASONING We evaluated the end-to-end performance of LOOKAHEAD REASONING (LR) across diverse benchmarks using DeepSeek-R1-Distill and Qwen3 pairs. The detailed results are presented in Table 1. key finding is LRs consistent ability to preseve task accuracy. Across variety of benchmarks, LRs accuracy varies within narrow range relative to the target models autoregressive baseline, from approximately 1.0% above to 2.1% below baseline performance. This accuracy preservation contrasts with SpecReason, which exhibited more noticeable accuracy reductions on several tasks (e.g., dropping from 91.8% to 85.9% on GSM8K with Deepseek-R1, 6% decrease). This underscores LRs design principle of preserving output via robust semantic verification. Furthermore, LR achieves strong accuracy while maintaining high step acceptance rates, often above 50% and reaching up to 63%. These substantial acceptance rates empirically support our initial insight that smaller draft model can effectively predict semantically correct reasoning steps for larger target model. LR also delivers significant efficiency gains. Its step-level parallelism is orthogonal to token-level speculative decoding, and their synergy produces substantial speedups. LR alone achieves speedups ranging from 1.04x to 1.71x across various benchmarks and model pairs. When combined with n-gram SD, the total speedup is further amplified, reaching up to 2.11x. This combined approach consistently outperforms n-gram SD alone, demonstrating the added value of step-level speculation. These results, consistent across both Deepseek-R1 and Qwen3 families, underscore the generalizable acceleration benefits of LR. 4.2 Combining LOOKAHEAD REASONING with Speculative Decoding To empirically validate the orthogonality of LOOKAHEAD REASONING (LR) with speculative decoding, we conducted experiments using prompt-lookup decoding (n-gram) on the AIME dataset. Figure 3 shows the orthogonality of LR and Speculative Decoding (SD). Subplot (a) shows that while LR alone with varying draft step number reaches speedup around 1.4x, adding SD boosts this to approximately 1.9x. Similarly, subplot (b) illustrates that SD alone with varying Speculative Token Numbers peaks around 1.55x speedup, but combining it with LR again achieves up to 1.9. Collectively, these results highlight that while either method in isolation offers limited gains, their 7 Table 1: LOOKAHEAD REASONINGs Performance Across Datasets. Speedup is relative to the Autoregressive Decoding of the respective Target Model. Method Metric Dataset AIME24 AMC23 GSM8K HumanEval GPQA MT-Bench LiveCodeBench Draft: Deepseek-R1-Distill 1.5B / Target: Deepseek-R1-Distill 32B Draft Model Acc. (%) 28.5 3.9 71.6 4. 77.6 3.3 67.2 2.4 9.6 1.2 Target Mode Acc. (%) 70.8 5. 95.6 2.3 91.8 1.9 96.9 0.8 63.3 2.2 SpecReason LR(ours) SD Acc. (%) Apt. Acc. (%) Apt. Speedup Speedup SD+LR(ours) Speedup 58.3 5.7 0.39 90.6 2.6 0.69 85.9 2.2 0.93 69.2 8.1 0.47 1.36 94.1 2.1 0.58 1.48 92.8 1.8 0.63 1.71 1.53 1.82 1.50 2.00 1.39 2.11 94.5 1.5 0.43 95.5 1.8 0.44 1.27 1.32 1.54 57.0 2.8 0.08 61.2 2.8 0.35 1.14 1.48 1.63 Draft: Qwen3 1.5B / Target: Qwen3 32B Draft Model Acc. (%) 46.9 8. 84.2 4.7 91.1 1.6 85.4 1.6 38.5 1.4 Target Model Acc. (%) 80.0 3. 97.5 2.0 96.6 1.4 97.6 0.8 68.2 2.1 SpecReason LR(ours) SD Acc. (%) Apt. Acc. (%) Apt. Speedup Speedup SD+LR(ours) Speedup 68.3 5.3 0.75 90.5 3.9 0.92 94.5 1.4 0.95 80.4 4.1 0.43 1.12 96.4 2.0 0.53 1.22 96.4 1.2 0.50 1.32 1.40 1.49 1.38 1.62 1.32 1.68 92.0 2.0 0.91 97.1 0.8 0.39 1.13 1.32 1.39 66.3 2.0 0.46 68.5 2.4 0.30 1.04 1.40 1.44 6.23 1.9 8.17 1.2 8.13 1.2 0.48 1.27 1.25 1.51 7.96 1.5 8.53 1.1 8.46 1.15 0.38 1.10 1.41 1.49 14.5 1.3 48.9 1. 40.6 1.5 0.25 49.5 2.3 0.47 1.21 1.45 1.58 28.8 1.6 52.4 1. 39.7 1.9 0.65 51.7 1.7 0.40 1.08 1.25 1.32 Note: LR (ours) refers to our proposed Lookahead Reasoning method. SD denotes token-level Speculative Decoding (N-gram based). Acc. stands for Accuracy (%), and Apt. for Acceptance Rate. For MT-Bench (marked with ), the reported metric is its standard score (0-9 scale) instead of accuracy. \"\" indicates data not applicable. Speedup is relative to the autoregressive decoding of the respective target model. combination consistently yields the most significant performance improvements, aligning with our theoretical analysis in 3.2. (a) vary the draft step number for LR (b) vary the speculative token number for SD Figure 3: Orthogonality of Lookahead Reasoning and Speculative Decoding. When used alone, the speedup from both LR and SD is limited by their draft length (γ). However, their combination consistently improves the max achievable speedup. 4.3 Ablation Study Effectiveness of the Verifier. We conducted an ablation study to assess the impact of different verifier mechanisms on task accuracy, utilizing DeepSeek-R1-Distill 32B as the target model and 1.5B parameter model as the draft model on GSM8K and AIME24 datasets. We compare four verifiers: (1) Random Acceptance of drafts; (2) LLM-as-a-Judge (LLM-J) with Qwen2.5 7B/32B [11]; (3) Embedding-based Verification (Emb.) with all-mpnet-base-v2 model [8] at 0.85/0.95 similarity thresholds; and (4) Reasoning Model Scoring(Score) [9], using the target model to assign 0-9 scores with acceptance thresholds of 7/9. Results are in Table 2. LLM-J verifiers (both 7B and 32B) showed robust accuracy preservation across both datasets, with minimal performance difference observed between the two verifier sizes. On GSM8K, their 8 performance closely aligned with the original baseline, indicating no accuracy degradation. On AIME, LLM-J verifiers also maintained accuracy very close to the original, with observed deviations within approximately 1-2%. This contrasts sharply with Random Acceptance. Despite comparable or lower acceptance rates (e.g., 0.50 on GSM8K and 0.40 on AIME), Random Acceptance led to significant accuracy degradation on both GSM8K ( 3.5% lower) and AIME ( 11% lower). This underscores the necessity of robust verification mechanism. The Embedding-based verifier shows trade-off: the stricter 0.95 threshold on GSM8K preserved accuracy (92.3 1.4%) at lower acceptance rate (0.37), while the 0.85 threshold, despite higher acceptance rate (0.56), resulted in 2% accuracy drop (89.8 2.5%). This pattern was mirrored on AIME. This indicates that while semantic equivalence is promising criterion, its efficacy in preserving accuracy is highly dependent on the stringency (and precision) of the similarity judgement. Reasoning Model Scoring, which assesses draft quality via target model scores rather than direct equivalence, consistently underperformed in accuracy preservation. For instance, even employing the stricter Threshold 9 resulted in notable accuracy reductions of approximately 5.9% on GSM8K and 12.5% on AIME. The still relatively high acceptance rate on GSM8K with this threshold (e.g., 0.93) suggests that the scoring mechanism may possess limited discriminative power on simpler datasets, even at stricter thresholds. This highlights fundamental limitation: quality scores, even with high thresholds, do not reliably ensure alignment with the target models output distribution, which is critical for LOOKAHEAD REASONINGs correctness. These results reveals that verifiers grounded in semantic equivalence with the target models likely output are most effective for preserving accuracy within LOOKAHEAD REASONING. LLM-as-a-Judge excels in this, provding nuanced judgement, though with potential computational overhead. Embedding models provide lightweight alternative (e.g., all-mpnet-base-v2 is only 100M parameters), where performance is tunable via the similarity threshold, offering cost-effective solution. Table 2: Performance comparison with different verifiers on GSM8K and AIME datasets. Apt.: accept rate; Acc.: accuracy (%). Dataset Metric Orig. Rand. LLM-J (Qwen) Emb. (Th.) Score (Th.) GSM8K AIME Apt. Acc. Apt. Acc. 0. 7B 0.63 32B 0.58 0.85 0. 0.95 0.37 7 0.97 9 0. 91.8 1.9 88.3 3.7 92.8 1.8 92.3 1.2 89.8 2.5 92.3 1.4 82.1 2.4 85.9 2.2 0.40 0.47 0.46 0. 0.38 0.81 0.39 70.8 5.2 59.6 5.4 69.2 8.1 69.0 4.7 64.0 6.5 66.7 6.3 37.9 7.5 58.3 5.7 Effect of Tree Width on Performance We investigate the impact of speculation tree width on LOOKAHEAD REASONINGs accuracy, accept rate, and speedup, keeping depth γ = 2. In LOOKAHEAD REASONING, candidate sequences grow as γ. Wider trees (W > 1) can boost accept rate but escalate FLOPs and, with imperfect verifiers, risk accuracy degradation due to erroneous acceptances. We hypothesize stronger verifier mitigates this. Experiments on GSM8K and AIME24 used Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct as judges. Results are demonstrated in Table 3. Increasing consistently raised accept rate across datasets and judges (e.g., GSM8K with Qwen2.57B: accept rate 0.63 for = 1 to 0.83 for = 8). However, this rarely translated to better speedup beyond = 2. Further widening often diminished speedup (e.g., AIME24 with Qwen2.57B, = 8 yielded no speedup), likely due to the exponential overhead outweighing accept rate gains. Accuracy trends highlight verifier importance. With the Qwen2.5-7B judge, increasing led to noticeable accuracy drop, especially on AIME24 (from 69.2% at = 1 to 64.6% at = 8), supporting our hypothesis. The stronger Qwen2.5-32B judge demonstrated greater resilience: accuracy remained more stable on GSM8K, and the degradation on AIME24 was less pronounced (69.0% at = 1 to 67.3% at = 8). This indicates stronger verifier is crucial for wider trees to manage the increased risk of incorrect speculation. 5 Related Work Speculative Decoding. There are many different types of speculative decoding approaches. Drafthead methods like Medusa [23], Hydra [24], and EAGLE [25, 10, 13] integrate auxiliary heads into the target model to propose sequences. In contrast, Jacobi-based approaches such as Lookahead Decoding [12] and CLLM [26] enable parallel n-gram generation without draft models. Systemlevel efforts [27, 28] further optimize SDs runtime efficiency in serving systems. LOOKAHEAD 9 Table 3: Impact of Tree Width (W ) on Performance Metrics (Depth γ = 2) Dataset Judge W= W=2 W=4 W=8 Acc.(%) Apt. Spd. Acc.(%) Apt. Spd. Acc.(%) Apt. Spd. Acc.(%) Apt. Spd. GSM8K Qwen7B 92.8 1.8 0.63 1.48 91.2 1.8 0.73 1.49 91.1 1.7 0.77 1.47 91.5 1.8 0.83 Qwen32B 92.3 1.2 0.58 1.40 93.2 2.0 0.66 1.42 92.8 1.8 0.73 1.39 92.5 1.5 0.77 1.25 1.19 AIME24 Qwen7B 69.2 8.1 0.47 1.27 67.3 4.1 0.58 1.32 65.4 6.5 0.67 1.26 64.6 5.9 0.74 Qwen32B 69.0 4.7 0.46 1.23 69.0 6.7 0.54 1.23 68.1 6.1 0.59 1.17 67.3 7.1 0.68 Note: Acc.: Accuracy (%); Apt.: Accept Rate; Spd.: Speedup (relative to target model autoregressive decoding). Depth γ = 2. Qwen2.57B/32B-Instruct are judge models. 1.00 0.98 REASONING introduces complementary form of step-level speculation tailored for reasoning models, enabling new dimension of parallelism orthogonal to token-level methods. LLM Reasoning. Recent trends shift from scaling model size [29, 30] to scaling inference-time compute [31, 32, 33], enabling large reasoning models (LRMs) like OpenAI o3/o4 [5], KimiK1.5 [34], and DeepSeek-R1 [4] to generate longer CoT to solve more complex problems in many steps. Recent work has begun to leverage this inherent step-by-step structure to accelerate LRMs. For instance, Speculative Thinking [35] uses LRMs to guide smaller draft model, while SpecReason [9] accelerates reasoning by employing the large model to score the output of small model, thereby deciding whether to accept its generated step. However, unlike LOOKAHEAD REASONING, these methods do not pursue step-level equivalent with the original target model to accelerate reasoning."
        },
        {
            "title": "6 Limitation and Conclusion",
            "content": "This paper introduces LOOKAHEAD REASONING, novel method for accelerating large reasoning models during long CoT reasoning. LOOKAHEAD REASONING adds new step-level dimension of parallelism, complementing traditional token-level speculative decoding. Our approach uses draft model to propose future reasoning steps, which are then semantically verified by the target model. Evaluation on various datasets using two open-source reasoning models show that it can achieve up to 2.1X speedup combined with speculative decoding. This highlights LOOKAHEAD REASONINGs effectiveness in making large reasoning models faster. This work has limitations that suggest future improvements. First, using nn to split reasoning steps is simple but may miss optimal breaks; smarter segmentation is needed. Second, current verifiers trade speed for accuracy; faster, lightweight alternatives remain an open challenge."
        },
        {
            "title": "References",
            "content": "[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [2] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. [3] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [5] OpenAI. Openai o3 and o4-mini system card. April 2025. [6] Qwen Team. Qwen3 technical report. 2025. [7] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [8] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. [9] Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, and Ravi Netravali. Specreason: Fast and accurate inference-time compute via speculative reasoning. arXiv preprint arXiv:2504.07891, 2025. [10] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-2: Faster inference of language models with dynamic draft trees. arXiv preprint arXiv:2406.16858, 2024. [11] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [12] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024. [13] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-3: Scaling up inference acceleration of large language models via training-time test. arXiv preprint arXiv:2503.01840, 2025. [14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. [15] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [16] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [17] AIME. Aime problems and solutions, 2025. [18] AMC12. Amc 12 problems and solutions, 2025. [19] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [20] Qwen Team. Qwen2.5-math: The worlds leading open-sourced mathematical llms, 2024. [21] Apoorv Saxena. Prompt lookup decoding, November 2023. [22] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. 12 [23] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. [24] Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, and William Brandon. Hydra: Sequentially-dependent draft heads for medusa decoding. arXiv preprint arXiv:2402.05109, 2024. [25] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. [26] Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models. In Forty-first International Conference on Machine Learning, 2024. [27] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. Specinfer: Accelerating generative large language model serving with tree-based speculative inference and verification. arXiv preprint arXiv:2305.09781, 2023. [28] Xiaoxuan Liu, Cade Daniel, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Xiangxi Mo, Alvin Cheung, Zhijie Deng, Ion Stoica, and Hao Zhang. Optimizing speculative decoding for serving large language models using goodput. arXiv preprint arXiv:2406.14066, 2024. [29] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [30] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [31] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [32] Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024. [33] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [34] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [35] Wang Yang, Xiang Yue, Vipin Chaudhary, and Xiaotian Han. Speculative thinking: Enhancing small-model reasoning with large model guidance at inference time. arXiv preprint arXiv:2504.12329, 2025."
        },
        {
            "title": "Semantic Equivalence Analysis Prompt",
            "content": "<im_start>system You are Qwen, created by Alibaba Cloud. You are helpful assistant.<im_end> <im_start>user Evaluate whether the following two reasoning steps (s1 and s2) convey exactly the same meaning. Focus on semantic similarity rather than exact wording. Compare the main ideas, key points, overall message, logical structure, and numerical calculations/results of both reasoning steps. If the reasoning steps convey essentially the same meaning and generate same calculation results, respond with [aligned]. If the reasoning steps express different meanings, respond with [unaligned]. If it is too hard to determine, respond with [unaligned] Please directly provide the final result in [aligned] or [unaligned]. Reasoning step 1 (s1): <start_s1> {} <end_s1> Reasoning step 2 (s2): <start_s2> {} <end_s2><im_end> <im_start>assistant [ This prompt template is specifically designed for Qwen2.5 Instruct models. It guides the model to directly output either \"aligned\" or \"unaligned\" as its judgment. Consequently, user can determine semantic equivalence between two sentences (s1 and s2) by simply checking if the models initial output string begins with \"ali\" (the first three letters of \"aligned\"). Thus, only one forward pass is needed to get the result and judgement latency can be largely saved."
        },
        {
            "title": "B Detailed Speedup Analysis",
            "content": "B.1 Performace Gains Analysis B.1.1 Speedup Analysis of Async Lookahead Reasoning For the analysis that follows, we assume all sequences are of equal length and that the draft tree contains exactly one branch at each layer. Moreover, we treat the verifiers overhead as negligible. Notation. γ N: maximum number of generations the large model performs in parallel. : cost (wall-time) of one sentence generated by target-model. c1: cost of one draft-model run, measured in units of (so drafting costs c1T ). α1 (0, 1):accept rate of the drafts Xi: number of consecutive accepted drafts in stage before the first rejection. We view full generation as sequence of DRAFT STAGE, each of which proceeds as follows: 1. If the number of generations the large model performs in parallel is less than γ. The draft model sequentially generate drafts. 2. Each time when we start to generate draft step, we immediately ask the target model to generate target step. 3. After the target model finished generation, immediately ask the verifier to verify whether should we accept the draft. If the draft was reject, fall back to the target models original sentence and proceed to the next DRAFT STAGE. Since each draft is accepted independently, (Xi = k) = αk(1 α), E[Xi] = α 1 α . Theorem 1. The latency speedup for sync Lookahead Reasoning is fsync(γ) = 1 αγ (1 α) (1 + γ) . Proof. The proof follows the same reasoning as in [3]. The only difference is that our γ represents the maximum number of tokens the large model generates in parallel, whereas in their notation, it corresponds to γ + 1 Theorem 2. Let = total sentences generated by our algorithm in DRAFT STAGE total sentences generated by target-only model in DRAFT STAGE , We can see this as the latency speedup using async Lookahead Reasoning algorithm. Then: , the draft tree never saturates. The parallel dimension of the target model is 1. If γ 1 c1 1 , and as , the asymptotic speedup is S1 = 1 c1 + (1 c1)(1 α1) . 2. If γ < 1 c1 , the draft tree is depth-limited. The parallel dimension of the target model is γ, and as , the asymptotic speedup is S2 = (1 α1) + c1 1 αγ 1 (cid:2)α1 αγ+1 1 γ(1 α1)αγ (cid:3) . Proof. Over stages, we compare the total number of sentences generated by our algorithm to that produced by the baseline (target-only) approach. (cid:108) 1 c1 (cid:109) , note that since each draft costs c1T , the draft model can generate at most sentences during the time required for the target model to produce one sentence. Therefore, Case 1 When γ (cid:108) 1 (cid:109) the draft tree never saturates, and the parallel dimension of the target model is effectively Moreover, in DRAFT STAGE our algorithm spends + c1T Xi total time. Over that same interval, the baseline target-only model would have produced + c1T Xi = 1 + c1 Xi (cid:108) 1 c1 (cid:109) . sentences, while our algorithm emits 1 + Xi sentences. Thus over stages, according to the Law of Large Numbers, S1(n) = (cid:80) (cid:80) i(1 + Xi) (cid:0)1 + c1Xi (cid:1) = + (cid:80) Xi (cid:80) Xi + c1 1 + E[Xi] 1 + c1 E[Xi] = 1 c1 + (1 c1)(1 α1) . 15 Case 2 When γ < 1 c1 model would be γ To emit total of Xi + 1 sentences in stage i, we therefore proceed in , the draft-tree saturates at depth γ. So the parallel dimension of the target (cid:109) (cid:108) Xi+1 γ full intervals of length , plus final partial batch of size Xi mod γ. Hence the total wall-time for stage is (cid:109) (cid:108) Xi+1 γ + c1T (cid:0)Xi mod γ(cid:1). Over that same interval, the baseline target-only model would have generated (cid:108) Xi+1 γ (cid:0)Xi mod γ(cid:1) + c1 (cid:109) sentences, whereas our algorithm emits 1 + Xi. Therefore, as S2(n) = (cid:80)n i=1 (cid:6) Xi+1 γ + (cid:80)n (cid:7) + c1 i=1 Xi (cid:80)n i=1(Xi mod γ) (1 α1) + c1 1 αγ 1 (cid:2)α1 αγ+1 1 γ(1 α1)αγ 1 (cid:3) . We put the calculation details in the appendix. B.1.2 Optimal Speculation Strategies under Concurrency Constraints In this section we show that under fixed parallelism budget, the optimal inference speedup is always achieved by jointly applying step-level and token-level parallelism, rather than by using either in isolation. Concretely, let γ2 denote the degree of parallelism used by speculative decoding and c2 be the ratio of the per-step execution time of the draft model to that of the target model. Then token-level speculative decoding alone yields[3] g(γ2) = 1 αγ2 2 (1 α2) (1 c2 + c2 γ2) . (2) (Note: here the formula is little different than the one in [3] due to different definition.) Next, pure step-level asynchronous parallelism scheme of depth γ1 achieves speedup fasync(γ1) = S2 = S1 = 1 αγ1 1 1 ) c1 γ1 αγ1 1 α1 + c1 α1 (1 αγ1 1 , c1 + (1 c1)(1 α1) 1 (1 α1) , γ1 < 1 c1 otherwise and pure step-level synchronous parallelism scheme of depth γ1 achieves speedup fsync(γ1) = 1 αγ1 1 (1 α1) (1 c1 + c1 γ1) . When both schemes are combined, the resulting speedup factorizes: (γ1) g(γ2). (3) (4) Since in real-world systems, due to memory or compute constraints, we often need to cap the total degree of parallelism i.e.γ1γ2 in hybrid speculative setting to . In this case, we may ask that given finite parallel budget , is it better that we combine these two parallel dimensions or we only use one of them? Thus our design goal becomes max arallelDimgP arallelDimf h(γ1, γ2) = (γ1) g(γ2). (5) Where arallelDimg = γ2, and arallelDimf = (cid:26)γ1, min{ 1 sync , γ1}, async Its easy to see that if we set γ1 = 1, then we are using purely token-level speculative decoding, whereas if we set γ2 = 1, then we are using purely Lookahead reasoning. 16 Theorem 3. Under synchronous Lookahead Reasoning with concurrency budget 4, is an even number, and the mild parameter constraint (cid:110) 1 + α1 1 + c1 Then at least one of the following must hold: min , (cid:111) 1 + α2 1 + c2 > 1.157, 1 + α1 1 + c1 1 + α2 1 + c2 (1 + αM/2 )(1 c2 + c2M 2 ) 2 1 c2 + c2M (1 + αM/2 )(1 c1 + c1M 2 ) 1 1 c1 + c1M (6) (7) (8) , . Furthermore, if both (7) and (8) hold simultaneously, then combining both speculative techniques strictly outperforms using either one alone. Conversely: If (7) fails, the optimal strategy is to use only token-level speculation. If (8) fails, the optimal strategy is to use only step-level speculation. Proof. Step 1: At least one of (7) and (8) must hold Define, for = 1, 2, Di(M ) = (cid:0)1 + αM/2 (cid:1) 1 ci + ciM 1 ci + ciM 2 . Observe that both factors 1 + αM/2 and 1 ci + ciM 2 1 ci + ciM are strictly decreasing in . Hence each Di(M ) decreases as grows. In particular, Di(2) = 1 + αi 1 + ci . Since either D1(2) D2(2) or D1(2) < D2(2), it follows that either or D1(2) > D2(M ) = D2(2) > D1(M ) = 1 + α1 1 + c1 1 + α2 1 + c2 > D2(M ), > D1(M ). In other words, at least one of (7) or (8) must hold. Step 2: Token-level-only speculation is suboptimal if condition (7) holds; otherwise, it is the optimal strategy. Lemma 3 shows that both fsync(γ1) and g(γ2) are unimodal, reaching their maxima at γ 2 2. Below, we analyze whether token-level-only speculation (i.e. γ1 = 1) yields the best speedup for different ranges of the concurrency budget . 1 2 and γ Case 1: If < γ 2 In this case, when (7) holds, we have h(1, ) h(2, through formulas), then according to the monotonicity of h, we have 2 )(transform h(1, γ2) h(1, ) h(2, 2 ), 1 γ2 Therefore, the overall maximum is attained only by jointly employing both levels. However, when (7) fails, since Di(x) is strictly decreasing, we know that (1 + αx/ )(1 c2 + c2x 2 ) 1 + α1 1 + c1 < 2 1 c2 + c2x In this case, according to Lemma 5, h(γ1, γ2) h(1, γ1γ2) h(1, ), γ1, γ2 1, γ1γ2 So the overall maximum is attained when we use token-level-only technique. 17 Case 2: If γ 2 < 2γ 2 In this case, we first prove that h(1, γ 2 ) < h(2, γ 2 2 ), 1 γ2 Then we prove that 7 is always hold. From above we can see h(1, γ2) h(1, γ 2 ) < h(2, γ 2 2 ), 1 γ2 And the theorem follows. 1.From Lemma3, we can know that 2 ( ln αγ αγ 2 ) 1 αγ 2 a2(γ 2 ) = 2 2 = c2γ 2 1 c2 + c2 γ"
        },
        {
            "title": "Therefore",
            "content": "h(2, γ 2 2 )/h(1, γ 2 ) = = 1 + α1 1 + 1 + α1 1 + c1 Let = 1 αγ 2 /2 2 (0, 1), 1 c2 + c2γ 2 2 / (1 + αγ 2 1 1 + αγ 2 2 /2 )(1 c2 + c2 1 2 a2(γ 2 ) 1 1 γ 2 ) 2 (1 + αγ 2 /2 2 )(1 1 2 a2(γ 2 )) = 2 + (y 2 + 1 ) ln (1 y) Then from Lemma 6, we can see that its value was less than 1.157. Then given 6 we can have h(2, )/h(1, γ 2 ) > γ 2 2 2.From previous step, we know that D2(M ) is strictly decreasing, so D2(M ) D2(γ 2 ) < 1.157 < 1 + α1 1 + c1 So 7 is always hold. Case 3: If 2γ 2 In this case, same as in Case 2, 7 is always hold. Besides, we have h(1, γ2) h(1, γ 2 ) < h(2, γ 2 ), 1 γ2 So its obvious token-level-only speculation is suboptimal. Step 3: If (8) holds, step-level-only speculation is suboptimal. Otherwise, it is the optimal strategy. Same as the previous step. Theorem 4. Under asynchronous Lookahead Reasoning with 0.52 < α1, α2 < 0.8 and c1 < 3 , c2 < 1 1 5 and the total parallelism budget 16 and is an even number. Then under constraint min{ 1 , γ1} γ2 the overall speedup h(γ1, γ2) is maximized if and only if both step-level c1 and token-level parallelism are employed (i.e. γ1 2 and γ2 2). Proof. Step 1: Token-level-only is not optimal. Lemma 3 shows that g(γ2) is unimodal and reaching their maxima at γ 2 2. 1.M 2γ 2 This case, we have h(1, γ2) h(1, γ 2 ) h(2, γ 2 ), 1 γ2 Therefore, its obvious that token-level-only is not optimal. 18 2.γ 2 < 2γ 2 In this case, we only need to prove h(1, γ 2 ) h(2, γ 2 2 ) Same as the proof in 3, we only need to prove that 1 + α1 1 + c1 1.157 is hold, and its easy to prove. 3.M < γ 2 This case, according to Lemma 1, we can see that h(1, γ2) < h(2, γ2 2 ) is always hold, so we can conclude that tokne-level-only is not optimal. Step 2: Step-level-only is not optimal. From Lemma 2, we can know that fasync(γ1) is strictly increasing when 1 γ 1 c1 , and will stay constant after that. 1. 1 c1 This case, according to Lemma 1, h(γ1, 1) h(M, 1) < h( 2 , 2), γ1 So step-level-only is not optimal. 2. 1 c1 < < 2 1 c1 This case, according to Lemma h(γ1, 1) h( 1 c1 , 1) = h(M, 1) < h( 2 , 2), γ1 3. 2 1 This case, we will have h(γ1, 1) h( 1 c1 , 1) < h( 1 c1 , 2), 1 γ1 So step-level-only is not optimal. Lemma 1. Let where is an even number, and w(γ) = (γ) g(cid:0)M/γ(cid:1), (γ) = g(γ) = 1 αγ 1 1 ) c1 γ αγ 1 (1 α1) , 1 α1 + c1 α1(1 αγ 1 αγ 2 (1 α2)(1 c2 + c2 γ) , 0.5 < α1, α2 < 0.8, 0 < c1 < 1 3 , 0 < c2 < 1 5 and 16. Then w(2) > w(1), w( 2 ) > w(M ). Proof. w(2)/w(1) = (1 + αM/2 2 > (1 + αM/2 (1 + α1) )(1 + α1c1(1 α1)) (1 + α1) )(1 + α1c1(1 α1)) 1 c2 + c2M 1 c2 + c2M/2 1 19 We can easily found that this function is monotonically decreasing with respect to c1, α2 ,monotonically increasing with respect to α1. Therefore, 1 + 0.52 (1 + 0.88)(1 + 0.52 1/3 0.48) w(2)/w(1) > > 1 w( 2 )/w(M ) = 1 + α2 1 + c2 1 α1 + c1α1(1 αM (1 + αM/2 1 )(1 α1 + c1α1(1 αM/2 1 ) c1M αM ) c1 1 (1 α1) 2 αM/2 1 1 (1 α1) = 1 + α2 1 + c2 [1 (1 c1M 1 αM/2 1 2 ) αM/2 1 1 + αM/2 1 1 α1 (1 α1)(1 + c1(α1 + + αM/2 ) c1 2 αM/2 1 ] ) Given that αk 1 αM/2 1 , {1, 2, , M/2}, so we have c1(α1 + α2 1 + + αM/ 1 ) c1 2 αM/2 1 > 0 If then Otherwise, 1 c1M 1 αM/2 1 2 0 w( 2 )/w(M ) 1 + α2 1 + c2 > 1 w( )/w(M ) > 1 + α2 1 + c2 [1 1 αM/2 1 1 + αM/2 1 1] > 1 + 0.52 1 + 1/ (1 0.88 1 + 0.88 ) > 1 Lemma 2. The speedup function of the async Lookahead Reasoning fasync(γ) = S2 = S1 = 1 αγ 1 1 α1 + c1 α1 (1 αγ 1 ) c1 γ αγ 1 , c1 + (1 c1)(1 α1) 1 (1 α1) , γ < 1 c1 otherwise is strictly increasing when 1 γ 1 c1 , γ N+,and will stay constant after γ 1 c1 , γ N+ Proof. We first see the function as continuous function on and prove that when 1 γ < 1 c1 the speedup function is strictly increasing , Write A(γ) = 1 αγ 1 , Then fasync(γ) = A(γ)/D(γ), and by the quotient rule D(γ) = 1 α1 + c1 α1 (1 αγ 1 ) c1 γ αγ 1 (1 α1). async(γ) = A(γ) D(γ) A(γ) D(γ) D(γ)2 . We compute A(γ) = αγ 1 ln α1, D(γ) = c1 αγ 1 (cid:104) α1 ln α1 + (1 α1)(cid:0)1 + γ ln α1 (cid:1)(cid:105) . Hence the numerator of async(γ) becomes (cid:110) A(γ) D(γ) A(γ) D(γ) = αγ 1 ( ln α1) D(γ) + c1 (1 αγ 1 ) (cid:104) α1 ln α + (1 α1)(1 + γ ln α1) (cid:105)(cid:111) (cid:104) ( ln α1) + c1 γ αγ 1 (ln α1) + c1 (1 αγ (cid:105) 1 ) (1 + γ ln α1) = αγ = αγ 1 (1 α1) (cid:104) 1 (1 α1) (cid:105) ( ln α1)(1 c1γ) + c1(1 αγ 1 )"
        },
        {
            "title": "Since",
            "content": "ln α1 > 0, each term is strictly positive for all γ 1. Therefore 1 α1 > 0, 1 c1γ > 0, 1 αγ 1 > 0 A(γ) D(γ) A(γ) D(γ) > 0 = async(γ) > 0, Then we prove that S1 S2 We only need to prove that when γ = 1 c1 , we have S2 = S1 When γ = 1 c1 , S2 = = = 1 α1 + c1 α1 (1 αγ 1 αγ 1 1 α1 + c1 α1 (1 αγ 1 αγ 1 1 ) c1 γ αγ 1 (1 α1) 1 ) αγ 1 (1 α1) (1 α1)(1 αγ 1 αγ 1 1 ) + c1 α1 (1 αγ 1 ) = 1 c1 + (1 c1)(1 α1) = S1 Therefore, So the lemma follows. Lemma 3. The speedup function of token-level speculative decoding[3] and is unimodality function. 1 αγ 2 (1 α2) (1 c2 + c2 γ) where α2 > c2 Then increases on [1, ˆγ) and decreases on (ˆγ, ) for unique ˆγ R+. In practical scenarios, where γ N+, the maximum of g(γ) is attained at some integer point γ N+, γ 2 Sync Lookahead Reasoning has similar form, so it also has this property. , γ g(γ) = Proof. Step 1. Compute g(γ). Set (γ) = 1 αγ 2 , D(γ) = (1 α2)(cid:0)1 c2 + c2 γ(cid:1), so that g(γ) = (γ)/D(γ). By the quotient rule, g(γ) = (γ) D(γ) (γ) D(γ) D(γ)2 . Since g(γ) = (γ) = αγ D(γ) = (1 α2) c2, 2 ( ln α2) (1 α2) (1 c2 + c2 γ) c2 (1 αγ αγ 2 ln α2, 2 ) (1 α2) [(1 α2) (1 c2 + c2 γ)]2 . Since (1 α2) > 0, the sign of g(γ) equals the sign of (γ) := αγ 2 ( ln α2) (1 c2 + c2 γ) c2 (1 αγ 2 ). Step 2. is strictly decreasing. Differentiate : (γ) = αγ 2 (ln α2)2 (1 c2 + c2 γ). Since 1 c2 + c2γ > 0, it follows (γ) < 0 for all γ 1. Thus is strictly decreasing on [1, ). Step 3. Signs of at the ends. At γ = 1: since α2 > c2 > 0, ln α2 > 1 α2 > 0, so we have (1) > 0 (1) = α2( ln α2) c2(1 α2) 21 As γ , αγ 2 0, so (γ) = αγ 2 (cid:2)( ln α) (1 c2 + c2 γ) + c(cid:3) c2 (1 αγ 2 ) < 0. Step 4. Conclusion via the Intermediate Value Theorem. Because is continuous, strictly decreasing, (1) > 0, and limγ (γ) < 0, there exists unique ˆγ R, ˆγ 1 such that (ˆγ) = 0. Moreover, (γ) > 0 1 γ < ˆγ, (γ) < 0 γ > ˆγ. Since g(γ) and (γ)) share the same sign, it follows that g(γ) > 0 for 1 γ < ˆγ, g(γ) < 0 for γ > ˆγ, Besides, noting that g(2)/g(1) = 1 + α2 1 + c2 so in practical scenarios where γ N+,we conclude that the maximum is achieved at some integer point γ 2 as claimed. Lemma 4. Let 0.5 < α < 0.8 and define a(α, x) = ln α αx 1 αx , 1. Then: 1. For each fixed α (0.5, 0.8), the function (cid:55) a(α, x) is strictly decreasing on [1, ). 2. For each fixed 1, the function α (cid:55) a(α, x) is strictly increasing on (0.5, 0.8). 3. Consequently, for every 10 and α (0.5, 0.8), a(α, x) < a(0.8, 10) = ln(0.8) 10 0.810 1 0.810 0.26, and for all α (0.52, 0.8), a(α, 2) (cid:0)a(0.52, 2), a(0.8, 2)(cid:1) (0.48, 0.79). Proof. (i) Monotonicity in x. Fix α (0, 1) and write (x) = αx 1 αx = (x) D(x) , (x) = αx, D(x) = 1 αx."
        },
        {
            "title": "Then",
            "content": "and by the quotient rule (x) = αx(cid:0)1 + ln α(cid:1), D(x) = αx ln α, (x) = (x) D(x) (x) D(x) D(x)2 = αx(cid:2)(1 + ln α)(1 αx) x( ln α) αx(cid:3) (1 αx)2 . Since 0 < α < 1, setting = ln α < 0 we have by convexity of the exponential, αx = eu > 1 + = 1 + ln α, hence (1 + ln α)(1 αx) x( ln α) αx = 1 + ln α αx < 0. Thus (x) < 0. Because ln α > 0, it follows immediately a(α, x) = ln α (x) < 0, so a(α, x) is strictly decreasing in 1. 22 (ii) Monotonicity in α. Fix 1 and set (α) = ln α, (α) = αx 1 αx , so a(α, x) = (α) (α). Then (α) = 1 α , (α) = x2 α x1 (1 αx)2 > 0."
        },
        {
            "title": "Hence",
            "content": "a α = (α) (α) + (α) (α) = (α) α + ( ln α) (α). We claim this is > 0. Indeed,"
        },
        {
            "title": "Since",
            "content": "α + ( ln α) > 0 ( ln α) α > V. α = α x2αx1 (1 αx)2 = x2 αx (1 αx)2 , = αx 1 αx , this inequality becomes αx 1 αx ( ln α) > 1 αx. But for 0.5 < α < 0.8, the well-known bound ln α > 1 α and αx α imply x2 αx (1 αx)2 > ( ln α) ( ln α) ln α > 1 α 1 αx, so a/α > 0. Thus a(α, x) is strictly increasing in α (0.5, 0.8). (iii) Numerical bounds. By (i), a(α, x) a(α, 10) for all 10, and by (ii), a(α, 10) a(0.8, 10) = ln(0.8) a(α, 6) a(0.8, 6) = ln(0.8) a(α, 8) a(0.8, 8) = ln(0.8) 10 0.810 1 0.810 0.26. 10 0.86 1 0.86 0.48. 10 0.88 1 0.88 0.36. Also by (ii), for any α (0.52, 0.8), a(α, 2) (cid:0)a(0.52, 2), a(0.8, 2)(cid:1) (0.48, 0.8). This completes the proof. Lemma 5. Let w(γ) = fsync(γ) g(cid:0)M/γ(cid:1), here γ N+, M/γ N+ and assume 4. Then: (a) If both (7) and (8) hold, then w(γ) is unimodal on [1, ] and attains its maximum at some γ [2, 2 ]. (b) If (7) fails, then the unique maximizer is γ = 1 (token-level only).i.e. h(γ1, γ2) < h(1, ), γ1γ2 = M, γ1, γ2 N+ (c) If (8) fails, then unique maximizer is γ = (step-level only). i.e. h(γ1, γ2) < h(M, 1), γ1γ2 = M, γ1, γ2 N+ 23 Proof. We first treat this function as continuous function over R, analyze its derivative to determine its monotonicity, and then restrict its domain to N+ to obtain the desired results. Step 1: Derivative. By the product and chain rules, w(γ) = 1 γ fsync(γ) g(cid:0)M/γ(cid:1)(cid:104) γ sync(γ) fsync(γ) γ g(cid:0)M/γ(cid:1) g(cid:0)M/γ(cid:1) (cid:105) . Step 2: Log-derivatives. Define ai(x) = ln(αi) αx 1 αx , = 1, 2."
        },
        {
            "title": "Then",
            "content": "f sync(γ) fsync(γ) Hence with η = M/γ, γ = a1(γ) c1γ 1 c1 + c1γ , γ g(γ) g(γ) = a2(γ) c2γ 1 c2 + c2γ . w(γ) = fsync(γ) g(η) γ (cid:104) a1(γ) a2(η) (cid:16) c1γ 1c1+c1γ c2η 1c2+c2η (cid:17)(cid:105) . Step 3: Monotonicity. By Lemma 4, a1(γ) is strictly decreasing in γ and a2(M/γ) strictly increasing. Also c1γ 1 c1 + c1γ c2η 1 c2 + c2η = 1 1 c1 1 c1 + c1γ c2 (1 c2)γ + c2M is strictly increasing in γ. Therefore w(γ) is strictly decreasing on [1, ], and so w(γ) either strictly increasing, or strictly decreasing, or first increasing then decreasing on [1, ]. Step 4: Endpoint comparison. Now we restrict the domain to be N+ If (7) fails, then w(2) < w(1), so is either strictly decreasing on [1, ], or first increasing then decreasing and the critical points was in (1, 2). Therefore we can conclude that is strictly decreasing on [2, ]. Therefore we can conclude that theres unique maximize at γ = 1, so we have h(γ1, γ2) < h(1, ), γ1γ2 = M, γ1, γ2 N+ If (8) fails, then w( increasing then decreasing and the critical points was in ( conclude that is strictly increasing on [1, unique maximize at γ = , so we have 2 ) < w(M ), so is either strictly increasing on [1, ], or first 2 , ). Therefore we can 2 ]. Therefore we can conclude that theres h(γ1, γ2) < h(M, 1), γ1γ2 = M, γ1, γ2 N+ If both (7) and (8) holds, then w(2) w(1), w( 2 ) w(M ), so would achieve the max between [2, 2 ] So the lemma follows. Lemma 6. When 0 < < 1 (y) = 2 + (y 2 + 1 ) ln (1 y) < 1. Proof. Define (y) = 2 + (cid:0)y 2 + y1(cid:1) ln(1 y), 0 < < 1. First derivative. direct calculation gives (y) = dy (cid:104) (cid:105) (y 2 + y1) ln(1 y) 1 = (1 y2) ln(1 y) 1 . 24 Second derivative and concavity of . Differentiating again, (cid:105) (1 y2) ln(1 y) (cid:0)y1(cid:1) (y) = = (cid:104) dy 2 ln(1 y) + + dy 1 y2 = 1 y2 1 y(1 + y) + 2 ln(1 y) ."
        },
        {
            "title": "Set",
            "content": "so that (y) = (y)/y3. On (0, 1), (y) = y(1 + y) + 2 ln(1 y), (y) = dy (cid:2)y + y2 + 2 ln(1 y)(cid:3) = 1 + 2y 2 1 = 2y2 + 1 1 < 0, and (0) = 0. Hence (y) < 0 for all (0, 1), which implies (y) < 0 on (0, 1). Thus is strictly decreasing. Sign-change of . - As 0+, ln(1 y) y, so (y) (y)(cid:0)1 y2(cid:1) 1 = +O(y) > 0. - As 1, ln(1 y) while (1 1/y2) 1, so (y) . By continuity and strict decrease, there is unique (0, 1) with (y) = 0, and (y) > 0 (0 < < y), (y) < (y < < 1). Hence increases on (0, y) and decreases on (y, 1), so its maximum on (0, 1) occurs at y. Numerical evaluation. Numerically one finds 0.5693971022, (y) 1.1562281731 < 1.157. Conclusion. Therefore for all (0, 1), (y) (y) < 1.157, as claimed. Calculation of S2(n) Well prove when S2(n) = (cid:80)n i=1 (cid:6) Xi+1 γ + (cid:80)n (cid:7) + c1 i=1 Xi (cid:80)n i=1(Xi mod γ) (1 α)1 + c1 1 αγ 1 (cid:2)α1 αγ+1 1 γ(1 α1)αγ 1 (cid:3) . Step 1: Use the Law of Large Numbers According to the Law of Large Numbers S2(n) = 1 (cid:80)n i=1 (cid:6) Xi+1 γ 1 + 1 (cid:80)n (cid:7) + c1 i=1 Xi (cid:80)n i=1(Xi mod γ) E(cid:6) Xi+ γ 1 + E[Xi] (cid:7) + c1E[Xi mod γ] Here, the PMF of Xi is: And its expectation is: (Xi = k) = αk 1(1 α1) E[Xi] = α1 1 α1 Step 2: Compute (cid:104)(cid:108) Xi+1 γ (cid:109)(cid:105) Let = (cid:108) Xi+1 γ (cid:109) . 25 Key Observation The ceiling function 0: (cid:25) (cid:24) Xi + 1 γ = + 1 if Xi [mγ, (m + 1)γ 1] (cid:109) (cid:108) Xi+1 γ can be expressed in terms of integer thresholds. For Thus: Compute E[Y ] = + 1 for Xi [mγ, (m + 1)γ 1], = 0, 1, 2, . . . E[Y ] = (cid:88) m= (m + 1)P (mγ Xi (m + 1)γ 1) The probability (mγ Xi (m + 1)γ 1) is: Therefore we can get: (m+1)γ1 (cid:88) k=mγ (Xi = k) = αmγ 1 (1 αγ 1 ) (cid:20)(cid:24) Xi + 1 γ (cid:25)(cid:21) = 1 1 αγ 1 Step 3: Compute [Xi mod γ] To calculate the expectation of Xi mod γ, where Xi follows the given geometric distribution and γ is an integer greater than 4, we proceed as follows: Compute (Xi mod γ = r) For {0, 1, . . . , γ 1}, we have: (Xi mod γ = r) = (cid:88) m= (Xi = mγ + r) = (1 α1)αr 1 1 αγ 1 α1 αγ+1 1 γ(1 α1)αγ 1 (1 α1)(1 αγ 1 ) Compute E[Xi mod γ] The expectation is: E[Xi mod γ] = γ1 (cid:88) r=0 (Xi mod γ = r) = γ1 (cid:88) r=0 (1 α1)αr 1 1 αγ 1 ="
        },
        {
            "title": "D Illustration of Hybrid Approach",
            "content": "Figure 4: Illustration of hybrid approach."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "UCSD",
        "UIUC"
    ]
}