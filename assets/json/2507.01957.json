{
    "paper_title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation",
    "authors": [
        "Zhuoyang Zhang",
        "Luke J. Huang",
        "Chengyue Wu",
        "Shang Yang",
        "Kelly Peng",
        "Yao Lu",
        "Song Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation. Traditional autoregressive image generation relies on next-patch prediction, a memory-bound process that leads to high latency. Existing works have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. To achieve high parallelization while maintaining generation quality, we introduce two key techniques: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture that enables arbitrary generation ordering and degrees of parallelization. It uses learnable position query tokens to guide generation at target positions while ensuring mutual visibility among concurrently generated tokens for consistent parallel decoding. (2) Locality-aware Generation Ordering, a novel schedule that forms groups to minimize intra-group dependencies and maximize contextual support, enhancing generation quality. With these designs, we reduce the generation steps from 256 to 20 (256$\\times$256 res.) and 1024 to 48 (512$\\times$512 res.) without compromising quality on the ImageNet class-conditional generation, and achieving at least 3.4$\\times$ lower latency than previous parallelized autoregressive models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 7 5 9 1 0 . 7 0 5 2 : r Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation LOCALITY-AWARE PARALLEL DECODING FOR EFFICIENT AUTOREGRESSIVE IMAGE GENERATION Zhuoyang Zhang Luke J. Huang Chengyue Wu Yao Lu MIT NVIDIA First Intelligence https://github.com/mit-han-lab/lpd"
        },
        {
            "title": "ABSTRACT",
            "content": "We present Locality-aware Parallel Decoding (LPD) to accelerate autoregressive image generation. Traditional autoregressive image generation relies on nextpatch prediction, memory-bound process that leads to high latency. Existing works have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. To achieve high parallelization while maintaining generation quality, we introduce two key techniques: (1) Flexible Parallelized Autoregressive Modeling, novel architecture that enables arbitrary generation ordering and degrees of parallelization. It uses learnable position query tokens to guide generation at target positions while ensuring mutual visibility among concurrently generated tokens for consistent parallel decoding. (2) Locality-aware Generation Ordering, novel schedule that forms groups to minimize intra-group dependencies and maximize contextual support, enhancing generation quality. With these designs, we reduce the generation steps from 256 to 20 (256256 res.) and 1024 to 48 (512512 res.) without compromising quality on the ImageNet class-conditional generation, and achieving at least 3.4 lower latency than previous parallelized autoregressive models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Autoregressive modeling has achieved state-of-the-art results in large language models in terms of scalability and generalizability (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a;b; Grattafiori et al., 2024; Jiang et al., 2024; Yang et al., 2024; 2025; Liu et al., 2024a). Naturally, many works have applied this powerful paradigm to visual generation (Esser et al., 2021; Lee et al., 2022; Ramesh et al., 2021; Yu et al., 2022; Sun et al., 2024; Tian et al., 2024). Moreover, this autoregressive formulation of visual generation has become increasingly crucial for unified multimodal generation (OpenAI, 2025; Wang et al., 2024a; Wu et al., 2024c;a; Chen et al., 2025a; Ma et al., 2025; Jiao et al., 2025; Song et al., 2025; Chen et al., 2025b; Zhao et al., 2025; Lin et al., 2025; Deng et al., 2025; Liao et al., 2025; Xie et al., 2025) since it is highly compatible with language modeling. Prevailing autoregressive visual generation methods typically follow two paradigms: (1) next-patch prediction by flattening the image into sequence of patches (Esser et al., 2021) and (2) next-scale prediction via coarse-to-fine multi-scale representations (Tian et al., 2024). In the first formulation, generating one token per Equal Contribution. Figure 1: Performance comparison among parallelized autoregressive models on ImageNet 256256. We significantly reduce the generation steps and achieve at least 3.4x lower latency compared with previous models. 1 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Figure 2: Visualization of attention maps in the LLAMAGEN-1.4B model. There is strong spatial locality, as the attention of decoding token is concentrated on nearby spatial tokens. LLAMAGEN encodes images into 24 24 tokens, where token that is 24 positions earlier in the attention map corresponds to the token directly above it in the 2D grid. step creates memory-bound workload1, causing latency to scale with the number of steps. The second formulation substantially reduces generation steps and thus latency. However, its multi-scale token representation fundamentally differs from the universal flat token representation, making it incompatible with widely used flat vision perception foundation models (e.g., CLIP (Radford et al., 2021; Zhai et al., 2023), DINO (Caron et al., 2021; Oquab et al., 2023)) and thereby limiting interoperability with perception backbones that have been proven critical for unified multimodal systems (Wu et al., 2024c; Ma et al., 2025; Jiao et al., 2025; Song et al., 2025; Chen et al., 2025b; Zhao et al., 2025; Lin et al., 2025; Tong et al., 2024; Wu et al., 2025; 2024b). Thus, autoregressive visual generation should be (1) highly efficient: minimizing latency and maximizing throughput; (2) remain flat token representations for universality and compatibility with vision backbones and, by extension, unified multimodal models. Recent works (Wang et al., 2024b; Pang et al., 2024; Li et al., 2025a) have tried to parallelize next-patch prediction by shifting to multi-patch prediction to accelerate the process, but only achieved limited parallelization. Non-autoregressive mask-prediction models like MASKGIT (Chang et al., 2022) enable multi-patch prediction but require full attention for bidirectional context, making them less efficient than autoregressive methods. To address the challenges, we introduce Locality-aware Parallel Decoding (LPD), framework that consists of novel flexible parallelized autoregressive modeling architecture and novel localityaware generation order schedule. We design new modeling architecture as conventional decoder-only autoregressive models struggle with flexible generation order and parallelization, limiting efficiency. In contrast, ours enables arbitrary generation order and degrees of parallelization. This is achieved by using learnable position query tokens to guide the model in generating tokens at target positions. Moreover, the generation is parallel-aware, as we leverage specialized attention mechanism to ensure mutual visibility among tokens generated concurrently. Notably, our design also inherits the KV caching mechanism, avoiding redundant computation. Furthermore, we observe strong spatial locality in image generation attention where tokens predominantly attend to nearby regions as shown in Figure 2. This indicates high dependency among nearby tokens, meaning that spatially closer tokens provide stronger conditioning. Recent works (Wang et al., 2024b; Besnier et al., 2025) also identify that minimizing mutual dependency among simultaneously generated tokens is essential to maintain sample consistency. With these insights, we introduce locality-aware generation order schedule that selects parallel decoding groups to maximize contextual support while minimizing intra-group dependencies, enabling higher degrees of parallelization. We examine the effectiveness of our proposed method on ImageNet class-conditional image generation. Our results reveal that we reduce the generation steps of traditional raster-order autoregressive generation from 256 to 20 (256256 res.) and 1024 to 48 (512512 res.) without compromising quality, and achieving at least 3.4 lower latency (Figure 1) than previous parallelized autoregressive models. Thanks to the design of flexible autoregressive modeling, our models are also capable of zero-shot image editing including class-conditional editing, inpainting and outpainting. 1A memory-bound workload refers to the scenario where the efficiency is limited by memory access speed rather than computation speed. In this context, each generation step requires loading the entire model parameters into GPU registers, making the process bottlenecked by memory bandwidth rather than computational power. 2 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Figure 3: Raster Order vs. Flexible Parallelized Autoregressive Modeling. (a) In raster order, each token is encoded to provide context for the future tokens and to generate the next token. This fixed input-output structure limits the generation flexibility and efficiency. (b) Our flexible parallelized autoregressive modeling decouples these two roles by using separate tokens for context and generation. Previously generated tokens provide context, while position query tokens enable parallel generation of target tokens. This design enables flexible generation order and parallelization."
        },
        {
            "title": "2.1 RETHINKING AUTOREGRESSIVE MODELING",
            "content": "In next-patch autoregressive modeling, images are split into patches and usually discretized via tokenizer into image tokens. While the joint distribution of the tokens x1, , xN and condition is extremely high dimensional and therefore hard to model directly, the autoregressive framework makes this amenable by factorizing the total joint distribution as p(x1, x2, . . . , xN ; c) = (cid:89) n=1 p(xnx<n; c) (1) The training objective of the autoregressive model is therefore to optimize parametric approximations pθ(xnx<n; c) for those one-step conditionals. This factorization needs predefined order, typically raster order, as shown in Figure 3 (a). However, during sampling, this leads to sequential steps, creating major efficiency bottleneck. To reduce the number of sequential generation steps, we can partition tokens into disjoint groups {X1, , XG}, where each group Xg = {xg1, , xgm } is predicted jointly, resulting in the following: p(x1, x2, . . . , xN ; c) = (cid:89) g=1 p(Xg X<g; c) (2) The training objective becomes optimizing pθ(Xg X<g; c). Previous work has shown that directly grouping tokens in raster order causes significant performance degradation (Wang et al., 2024b; Pang et al., 2024). This is because spatially adjacent tokens exhibit strong mutual dependencies, and independent sampling usually leads to generation inconsistencies inside group. It is essential to break the raster order when grouping. In addition, the size of the prediction group Xg should gradually increase. As the context X<g grows, it offers stronger conditioning, allowing more tokens to be predicted in parallel. Previous work using masked transformers (Chang et al., 2022) also mirrors this intuition by increasing parallelism progressively, predicting fewer tokens early when context is sparse. Therefore, an effective parallelized autoregressive model should support: (1) Flexible generation order to alleviate the issue caused by mutual interdependency of concurrently predicted tokens and (2) Dynamic group sizes increasing the number of tokens predicted per step with available context. However, it is difficult to achieve these within the standard decoder-only autoregressive models, which are inherently designed with fixed input-output structure, e.g. next-token prediction. In this modeling, each token simultaneously serves two roles: it provides context via its hidden state and enables generation via its output logits. This coupling limits flexibility in the the generation order and output size. To address these challenges, we propose novel flexible parallelized autoregressive modeling which is able to support arbitrary generation order and degrees of parallelization. 3 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Figure 4: Illustration of the training attention mask. Context Attention allows subsequent tokens to attend to the context tokens causally. Query Attention ensures mutual visibility among the position query tokens within the same step, and prevents any subsequent tokens from attending to the query tokens. For example, image token 4 can be attended to by all subsequent tokens, including image tokens and position query tokens, to provide context information. The two position query tokens P3 and P5 in the same generation step attend to the condition, to the image token 4, and to each other, while ignoring the earlier query P4. Figure 5: Illustration of the inference attention mask. Encoding with image tokens and Decoding with position query tokens can be fused into single step. Taking step 2 in Figure 3 (b) as the example, it simultaneously encodes the previously generated image tokens 3, 5 to update the KV-cache and decodes the desired image tokens 1, 2 and 6 in parallel."
        },
        {
            "title": "2.2 FLEXIBLE PARALLELIZED AUTOREGRESSIVE MODELING",
            "content": "Our core idea is to decouple the context representation and token generation by leveraging separate tokens. We illustrate this in Figure 3 (b). In this formulation, previously generated tokens are encoded to provide context and the generation is driven by learnable position query tokens corresponding to the desired target positions. These position query tokens are constructed by adding the positional embedding of the target location to shared learnable embedding. By directly inputting these positionspecific queries, the model can generate tokens at arbitrary target positions in parallel. This design allows the model to leverage positional information in both the context and generation pathways, enabling arbitrary generation order. Training formulation. We train the model to transform each position query token into the corresponding ground-truth image token, conditioned on all ground-truth tokens that precede it. To preserve teacher-forcing while allowing parallel prediction, we interleave position query tokens with ground-truth tokens and apply specialized training attention mask as shown in Figure 4 that contains two attention patterns: 1. Context Attention allows subsequent tokens to attend to context tokens causally. 2. Query Attention ensures mutual visibility among the position query tokens within the same step, and prevents any subsequent tokens from attending to the query tokens. Inference formulation. At test time we alternate between encoding the generated image tokens and decoding with position query tokens. 1. Encoding. The sampled image tokens go through forward pass to store the KV-cache which provides context for the future decoding steps. 2. Decoding. We feed set of learnable position query tokens that can attend to all previously generated image tokens in the KV-cache. The forward pass outputs the logits for every target position, thereby decoding those tokens in parallel. In this step, we dont store the KV-cache for the position query tokens. However, sequentially execute these two operations double the generation steps. As shown in Figure 3 (b), these two operations can be fused into single step via specialized inference attention mask as shown in Figure 5. 4 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Figure 6: Comparison with other methods. (a) Represented by SAR and ARPG, which use an encoder-decoder architecture where parallel-generated tokens are independent, as query tokens provide no key-value pairs. (b) Represented by RANDAR, decoder-only architecture with positional instruction tokens. The causal mask used in training degenerated parallel generation into batched next-token prediction and requires instruction tokens stored in the KV cache. (c) Thanks to the specialized training mask, our method guarantees the visibility among all concurrently predicted target positions and only stores the generated tokens in the KV cache. Comparison with other methods. Recent efforts have also pursued parallel generation in autoregressive modeling, yet each carries inherent limitations. One line of work, exemplified by SAR (Liu et al., 2024b) and ARPG (Li et al., 2025a), adopts an encoder-decoder architecture where target-aware query tokens attend to the encoders key-value cache via cross-attention. However, as illustrated in Figure 6 (a), the target positions themselves do not contribute any key-value pairs, resulting in the tokens generated within the same parallel step being produced independently of one another. Another approach, represented by RANDAR (Pang et al., 2024), adheres to the prevailing decoderonly architecture. It achieves arbitrary order by inserting positional instruction tokens to designate target positions. However, it still leverages standard causal mask during training. This strategy, as depicted in Figure 6 (b), leads to two notable issues: (1) the parallel generation degenerates into batched next-token prediction instead of joint prediction and (2) the positional instruction tokens must be stored in the KV cache during inference, doubling the memory consumption. Compared with these two methods, our method as shown in Figure 6 (c) guarantees the visibility among all concurrently predicted target positions and only stores the generated tokens in the KV cache. PAR (Wang et al., 2024b) and NAR (He et al., 2025) leave the decoder-only design almost unchanged but uses fixed parallel order, limiting generation flexibility thus achieved limited parallelization and generation quality. ACDIT (Hu et al., 2024) shares similar attention scheme with us, yet it was used for evenly interpolating between autoregressive and diffusion modeling."
        },
        {
            "title": "2.3 LOCALITY-AWARE GENERATION ORDER SCHEDULE",
            "content": "To fully leverage our flexible parallelized autoregressive modeling architecture, we introduce locality-aware generation order schedule. This schedule is guided by two key principles (1) High proximity to previously generated tokens: target positions should be spatially close to existing context to ensure strong conditioning and (2) Low proximity among concurrently generated tokens: tokens predicted in the same parallel step should be spatially distant to reduce mutual dependency. These principles are derived from systematic analysis of the attention patterns in autoregressive image generation by the widely adopted LLAMAGEN (Sun et al., 2024) model. Using LLAMAGEN, we generate 50,000 images and collect attention scores at each decoding step. Qualitative attention patterns are shown in Figure 2, and quantitative results are presented in Figure 7. To quantify locality, we define the Per-Token Attention (PTA) to neighborhood of radius 2 as: As ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:80) Attention(Ti, Tj) I[d(Ti, Tj) = s] I[d(Ti, Tj) = s] (cid:80) (3) where Attention(Ti, Tj) denotes the attention weight from token Ti to token Tj, and d(Ti, Tj) is their Euclidean distance on the 2D image grid. 2The neighborhood is defined as the set of tokens whose centers are exactly euclidean distance of away. 5 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation As shown in Figure 7 (a), PTA decreases sharply with increasing distance, indicating strong spatial locality in the attention mechanism. This suggests that nearby tokens carry significantly more useful information during decoding, and that spatially adjacent tokens are highly dependent on one another for accurate prediction. This locality pattern is consistently observed across all attention heads. In Figure 7 (b), we visualize the Attention Sum, defined as the total attention score decoding token assigns to tokens within relative distance s. The plot uses = 3 and confirms that most attention is concentrated within local neighborhoods, reinforcing the importance of spatial locality. Figure 7: Attention Analysis of LLAMAGEN. (a) Attention diminishes rapidly over distance, indicating the strong spatial locality. (b) The spatial locality is consistently observed in all heads. This analysis supports our two principles: decoding tokens should remain close to previously generated tokens to maximize contextual support, and distant from concurrently generated tokens to minimize intra-group dependency. Based on these principles, we implement locality-aware generation order schedule described in Algorithm 1. Suppose we use decoding steps to generate 2 tokens, with group sizes = [o1, o2, . . . , oK], where ok is the number of tokens generated in step k, typically increasing via cosine schedule. At each step k, we compute the euclidean distance between unselected and already selected tokens to measure spatial proximity, where closer distance leads to higher proximity. We sort unselected tokens by proximity and split them into two sets: c1 are tokens with sufficient proximity larger than the threshold τ which are eligible for the following high-proximity selection, and c2 are the rest. We sequentially select tokens from c1, adding each to the selected set while filtering out nearby tokens that the relative distance is smaller than the repulsion threshold ρ, which are added to c2. If all the grids in c1 are considered and the number of selected grids is less than ok, we use farthest point sampling (Qi et al., 2017) to select the remaining grids from c2 to ensure spatial low dependency. It is worth noting that the generation order can be precomputed and stored for direct use during inference, incurring no additional latency. We provide the PyTorch implementation in the Appendix C. For intuitive understanding, we illustrate an example of our generation order schedule in Figure 8. We also plot the schedule for raster order, random order and Halton order (Besnier et al., 2025) for comparison. The raster order generates tokens in raster-scan manner and the random order generates tokens in random manner. The Halton order is low-discrepancy sequence to arrange the generation positions which spreads out the tokens to achieve uniform image coverage step by step. Algorithm 1: Locality-aware Generation Order Schedule Input: decoding steps K, group sizes = [o1, o2, . . . , oK], grids = {(i, j)}N i,j=1, proximity proximity measurement high-proximity selection = queue_push(s, farthest_point_sampling(c2, s, ok len(s))); low-dependency selection threshold τ , repulsion threshold ρ; schedule = [ ]; for = 1, . . . , do = [ ]; = 1/ euclidean(G S, S) ; = sorted(G S, key = p, reverse = rue); c1, c2 = cutoff(c, τ ); while len(s) < ok and len(c1) > 0 do = queue_push(s, queue_pop(c1, 1)) ; c1, = filter(c1, s, ρ); c2 = queue_push(c2, ); if len(s) < ok then = queue_push(S, s); return 6 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Figure 8: Illustration of different generation order schedules. All schedules leverage 20 decoding steps for 162 tokens. Dark green marks newly selected grids and light green marks those already selected. Compared to others, our schedule selects grids close to previous ones and far from concurrent ones, maximizing the contextual support and minimizing the mutual dependency."
        },
        {
            "title": "3.1 SETUP",
            "content": "Models. For fair comparisons with existing autoregressive image generation methods, we use the LLAMAGEN tokenizer (Sun et al., 2024) with codebook size 16384 and downsample factor 16. We train three models of different sizes: 337M, 752M, and 1.4B parameters. We use standard decoder-only transformer architecture, and refer to them as LPD-L, LPD-XL, and LPD-XXL, respectively. Please refer to the Appendix A.1 for more details. Training and Evaluation. We train and evaluate our models on the class-conditional ImageNet (Russakovsky et al., 2015) 256256 and ImageNet 512512 datasets. We first train all models on ImageNet 256256 for 450 epochs. The initial learning rate is 1e-4 per 256 batch size with 50 epochs linear warmup at the beginning and 50 epochs cosine decay to 1e-5 at last. For 512-resolution models, we load the pre-trained 256-resolution models and interpolate the positional embeddings. We then continue training on ImageNet 512512 for another 50 epochs. During training, we randomly shuffle the token sequence with the class token at the beginning. We train on range of predefined decoding steps where the number of tokens in each step is determined by cosine schedule. We use Fréchet Inception Distance (FID) (Heusel et al., 2017) as the primary metric computed on 50,000 generated samples. We also report Inception Score (IS) (Salimans et al., 2016), Precision and Recall (Kynkäänniemi et al., 2019). For all results, we sweep the optimal classifier-free guidance scale with an interval of 0.1. Please refer to the Appendix A.2 for more details. Efficiency Profiling. We profile all the efficiency results on single NVIDIA A100 GPU with BFloat16 precision. We measure the latency with batch size of 1 and throughput with batch size of 64. We report the average latency over 500 inference steps, with 100-step warm-up period."
        },
        {
            "title": "3.2 MAIN RESULTS",
            "content": "We compare our models against broad set of generative baselines on ImageNet 256256  (Table 1)  . For fair comparison, we also create raster order counterpart following the same setup. As shown in the table, we reduce the generation steps from 256 to 20, achieving 12.8 generation steps reduction, without sacrificing the generation quality. Compared with other parallelized autoregressive models, we achieve significantly better image generation quality and efficiency. Taking LPD-XL model as an example, it achieves FID of 2.10 with only 20 steps, reducing the number of generation steps by 7 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Table 1: System-level comparison on ImageNet 256256 class-conditional generation. We evaluate the generation quality by metrics including Fréchet inception distance (FID), inception score (IS), precision and recall. #Steps is the number of model runs needed to generate an image. We measure latency with batch size of 1 and throughput with batch size of 64 on single NVIDIA A100 GPU under BFloat16 precision, with classifier-free guidance (CFG) for both. Type Model #Para. FID IS Precision Recall #Steps Latency(s) Throughput(img/s) Diffusion Mask VAR AR Parallelized AR ADM-G [16] CDM [27] LDM-4 [54] DiT-XL/2 [48] SiT-XL/2 [42] MaskGIT [7] MAGVIT-v2 [80] MaskBit [69] MAR-B [35] MAR-L [35] MAR-H [35] VAR-d16 [61] VAR-d20 [61] VAR-d24 [61] VAR-d30 [61] VQGAN-re [17] RQTran.-re [32] LlamaGen-L [59] LlamaGen-XL [59] LlamaGen-XXL [59] LlamaGen-3B [59] RAR-B [81] RAR-L [81] RAR-XL [81] RAR-XXL [81] PAR-L-4 [68] PAR-XL-4 [68] PAR-XXL-4 [68] PAR-3B-4 [68] RandAR-L [46] RandAR-XL [46] RandAR-XXL [46] ARPG-L [34] ARPG-L [34] ARPG-XL [34] ARPG-XXL [34] NAR-L [25] NAR-XL [25] NAR-XXL [25] 554M 4.59 4.88 400M 3.60 675M 2.27 675M 2.06 227M 6.18 307M 1.78 305M 1.62 208M 2.31 479M 1.78 943M 1.55 310M 3.30 600M 2.57 2.09 1.0B 1.92 2.0B 5.20 1.4B 3.8B 3.80 343M 3.07 775M 2.62 1.4B 2.34 3.1B 2.18 261M 1.95 461M 1.70 955M 1.50 1.48 1.5B 343M 3.76 775M 2.61 1.4B 2.35 2.29 3.1B 343M 2.55 775M 2.25 1.4B 2.15 320M 2.44 320M 2.44 719M 2.10 1.94 1.3B 372M 3.06 816M 2.70 2.58 1.5B AR Raster Counterpart-L Raster Counterpart-XL Raster Counterpart-XXL 337M 2.48 752M 2.12 2.01 1.4B Parallelized AR LPD-L LPD-XL LPD-XXL LPD-L LPD-XL 337M 2.40 752M 2.10 2.00 1.4B 337M 2.29 752M 1.92 186.7 158.7 247.7 278.2 270.3 182.1 319.4 338.7 281.7 296.0 303.7 274.4 302.6 312.9 323. 280.3 323.7 256.1 244.1 253.9 263.3 290.5 299.5 306.9 326.0 218.9 259.2 263.2 255.5 288.8 317.8 322.0 291.7 287.1 331.0 339.7 263.9 277.5 293.5 278.0 307.4 316.0 284.5 326.7 337.6 282.7 319.4 0.82 0.83 0. 0.80 0.82 0.81 0.81 0.84 0.83 0.82 0.82 0.83 0.80 0.80 0.81 0.82 0.81 0.80 0.80 0.84 0.82 0.82 0.82 0.81 0.80 0.79 0.82 0.82 0.79 0.81 0.81 0.81 0.82 0.81 0.81 0.80 0.81 0.80 0. 0.81 0.79 0.52 0.57 0.59 0.51 0.57 0.60 0.62 0.51 0.56 0.59 0.59 0.52 0.57 0.59 0.58 0.58 0.60 0.62 0.63 0.50 0.56 0.57 0.58 0.58 0.60 0.62 0.55 0.55 0.61 0.59 0.53 0.58 0. 0.58 0.60 0.59 0.57 0.59 0.60 0.58 0.61 250 8100 250 250 250 8 64 64 64 64 64 10 10 10 256 256 576 576 576 576 256 256 256 256 147 147 147 147 88 88 88 32 64 64 64 31 31 31 256 256 256 20 20 20 32 32 4.34 1.03 18.14 20.80 25.96 0.12 0.15 0.17 0.26 12.22 18.51 24.40 12.37 4.18 4.04 5.47 6.59 3.16 4.79 6.26 3.29 1.97 2.78 3.58 0.58 1.15 1.71 2.24 1.01 1.42 1.88 3.73 5.29 7.10 0.28 0.41 0. 0.46 0.66 0.58 5.39 2.93 2.11 1.45 70.58 52.53 39.30 25.89 2.08 1.14 0.72 0.58 13.76 12.63 8.76 6.72 6.83 3.69 2.33 2.32 28.59 17.06 11.49 104.92 54.70 36.53 26.23 41.03 23.36 15. 17.53 12.31 8.99 139.11 75.20 45.07 110.34 61.24 3.2 compared to ARPG and achieving 4.2 lower latency. Increasing the steps slightly to 32 yields FID of 1.92, even matching ARPG-XXL, while reducing latency by 3.4. We further report our results on ImageNet 512512  (Table 2)  . As shown in the table, we reduce the generation steps from 1024 to 48, achieving 21.3 generation steps reduction, without sacrificing the generation quality. These results validate the effectiveness of our flexible parallelized autoregressive modeling and the locality-aware generation order schedule. We also provide visualization results in Figure 10."
        },
        {
            "title": "3.3 ZERO-SHOT GENERALIZATION",
            "content": "Our model can naturally perform zero-shot editing tasks since we support image generation in arbitrary order. As shown in Figure 10, we can conduct image inpainting, image outpainting, and class-conditional editing. For image inpainting and outpainting, we prefill the KV cache with all tokens from the non-repaint regions along with class token and generate the masked region in random order. For class-conditional editing, we substitute the class embedding with new class embedding and generate the edited region in random order. 8 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Table 2: System-level comparison on ImageNet 512512 class-conditional generation. Metrics and evaluation setup are the same as in Table 1. Type Model #Para. FID IS Precision Recall #Steps Latency(s) Throughput(img/s) Diffusion Mask VAR AR ADM-G [16] DiT-XL/2 [48] SiT-XL/2 [42] MaskGIT [7] MAGVIT-v2 [80] MAR-L [35] 554M 7.72 675M 3.04 675M 2.62 172.71 240.82 252.21 227M 7.32 307M 1.91 481M 1. 156.0 324.3 279.9 VAR-d36-s [61] 2.3B 2.63 303.2 VQGAN [17] 227M 26.52 66.8 Parallelized AR ARPG-XL [34] 719M 3.38 257.8 AR Raster Counterpart-L 337M 2.54 Raster Counterpart-XL 752M 2.09 Parallelized AR LPD-L LPD-XL 337M 2.54 752M 2.10 278.5 315.0 292.2 326. 0.87 0.84 0.84 0.78 - 0.73 0.80 0. 0.81 0.80 0.42 0.54 0.57 0.50 - 250 250 250 12 64 10 0.31 1024 0.58 0.57 0.55 0. 1024 1024 48 48 - 11.32 0. 14.25 20.93 0.69 1.01 - 0.10 OOM 3.79 2.36 35.16 18.18 Figure 9: Ablation Studies. All ablation experiments are conducted with XL size models on 256256 resolution. (a) Effectiveness of flexible parallelized autoregressive modeling. (b) Effectiveness of locality-aware generation order schedule. (c) Effectiveness of the locality principles."
        },
        {
            "title": "4 ABLATION",
            "content": "Effectiveness of Flexible Parallelized Autoregressive Modeling. One key design of our flexible parallelized autoregressive modeling is the guarantee of the mutual visibility among all concurrently predicted target positions. This is critical to maintain the consistency of the generated tokens in the same group when the degree of the parallelization is high. We show the effectiveness of this design in Figure 9 (a). We compare our model with RANDAR and ARPG which lack this design. To only ablate the effectiveness of our flexible parallelized autoregressive modeling, we use random generation order for all models without our locality-aware parallel generation order schedule. As shown in the figure, with the generation steps decrease and the parallelization increases, our model exhibits smaller FID increase compared with the other two models. For example, with 32 steps, our model almost maintain the performance with 256 steps but ARPG and RANDAR have significant FID increase. This design is crucial for us to achieve fewer generation steps while maintaining the generation performance. Effectiveness of Locality-aware Generation Order Schedule. We compare our schedule with another two generation order schedules as shown in Figure 9 (b). Random order just arrange the generation positions randomly. Halton order leverages the Halton low-discrepancy sequence to arrange the generation positions which spreads out the tokens to achieve uniform image coverage step by step. Intuitively it mainly focus on reducing the dependency inside parallel group which shares the same insight with our second principle that low proximity is needed among concurrently generated tokens. However, the low-discrepancy sequence omits the importance of the already generated context which is our first principle that we need to maintain high proximity to previously generated tokens. As shown in the figure, our locality-aware parallel decoding order consistently outperforms the other two orders, showing the effectiveness of our method. 9 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Effectiveness of the Locality Principles. Our locality-aware generation order schedule is guided by two principles: (1) high proximity to previously generated tokens and (2) low proximity among concurrently generated tokens. We ablate the effectiveness of these two principles in Figure 9 (c). As shown, the random order baseline yields an FID of 2.11. We first apply Principle 1 only, selecting points close to previously generated tokens without considering their mutual dependency. This improves the performance to 2.00. We then apply Principle 2 alone, using farthest point sampling at each step to ensure concurrently generated tokens are well separated, without considering context from previously generated tokens. This improves the FID to 2.06. Combining both in our locality-aware generation order achieves 1.92, highlighting the value and synergy of both principles."
        },
        {
            "title": "5.1 AUTOREGRESSIVE IMAGE GENERATION",
            "content": "Autoregressive models generate the current output conditioned only on previous outputs. Usually this dependency is captured by causal attention mechanisms, enabling efficient inference via KV caching. Autoregressive modeling with GPT-style \"next-token-prediction\" (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a;b; Chiang et al., 2023; Jiang et al., 2024) has dominated the field of language generation due to its simplicity and scalability. Inspired by this success, autoregressive visual generation has shifted from operating on sequences of pixels (Van Den Oord et al., 2016; Van den Oord et al., 2016; Parmar et al., 2018; Chen et al., 2018; Salimans et al., 2017; Yu et al., 2021; Li et al., 2025b) to sequences of latent discrete tokens (Esser et al., 2021; Lee et al., 2022; Ramesh et al., 2021; Razavi et al., 2019; Yu et al., 2021; 2022; Sun et al., 2024; Yu et al., 2024; Wang et al., 2024a; Teng et al., 2024; Ren et al., 2025; He et al., 2025; 2024). However, the token-bytoken decoding strategy is often bottlenecked by memory bandwidth. This limitation prevents full utilization of computation and results in high latency. Recently, \"next-scale-prediction\" (Tian et al., 2024; Han et al., 2024) has emerged to predict the next scale of the image instead of the next token thus accelerates the generation process. However, its multi-scale token representation fundamentally differs from the universal flat token representation, making it incompatible with widely used flat vision perception foundation models."
        },
        {
            "title": "5.2 PARALLEL GENERATION IN SEQUENCE MODELING",
            "content": "Parallel generation has been widely studied in the field of language modeling. Prior to the era of large language models, masked-prediction architectures (Gu et al., 2017; Ghazvininejad et al., 2019; Gu et al., 2019) are used to do parallel generation and iterative refinement. Recently, with the success of large language models, speculative decoding (Chen et al., 2023; Leviathan et al., 2023) and its derivatives (Cai et al., 2024; Ankner et al., 2024) employ draft model to generate the next few tokens and then the main model conducts the verification. In visual generation, masked-prediction models (Chang et al., 2022; Yu et al., 2023a;b; Chang et al., 2023) are widely used to generate masked tokens step by step leveraging masked prediction transformer similar to BERT (Devlin et al., 2019; Bao et al., 2021; He et al., 2022), which are able to generate multiple tokens in parallel. However, they are non-autoregressive models and need bidirectional attention which is computationally expensive and KV cache is not applicable to accelerate the inference. Recent works (Wang et al., 2024b; Pang et al., 2024; Li et al., 2025a; He et al., 2025) have explored parallel generation in autoregressive models, but with limited parallelization and generation quality. We systematically analyze its challenges and our proposed method enables greater parallelization without sacrificing performance."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Our contributions lie in two key aspects: (1) flexible parallelized autoregressive modeling and (2) locality-aware generation order schedule. With these two techniques, we can significantly reduce the generation steps required by the traditional autoregressive models without compromising the generation quality and achieve at least 3.4 lower latency than previous parallelized autoregressive models. We hope this work can inspire future research on the acceleration of autoregressive image generation. 10 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Figure 10: Generation Examples of Our Model. We show 512512 generation samples (top), 256256 generation samples (middle) and zero-shot image editing results including class-conditional editing, inpainitng and outpainting (bottom)."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank MIT-IBM Watson AI Lab, National Science Foundation, Hyundai, and Amazon for supporting this research. 11 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation"
        },
        {
            "title": "REFERENCES",
            "content": "Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan RaganKelley, and William Brandon. Hydra: Sequentially-dependent draft heads for medusa decoding. arXiv preprint arXiv:2402.05109, 2024. Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. Victor Besnier, Mickael Chen, David Hurych, Eduardo Valle, and Matthieu Cord. Halton scheduler for masked generative image transformer. arXiv preprint arXiv:2503.17076, 2025. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. In International conference on machine learning, pp. 864872. PMLR, 2018. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025a. Zisheng Chen, Chunwei Wang, Xiuwei Chen, Hang Xu, Jianhua Han, and Xiaodan Liang. Semhitok: unified image tokenizer via semantic-guided hierarchical codebook for multimodal understanding and generation. arXiv preprint arXiv:2503.06764, 2025b. Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 12 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. Mask-predict: Parallel decoding of conditional masked language models. arXiv preprint arXiv:1904.09324, 2019. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017. Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. Advances in neural information processing systems, 32, 2019. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Yefei He, Feng Chen, Yuanyu He, Shaoxuan He, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Zipar: Accelerating autoregressive image generation through spatial locality. arXiv preprint arXiv:2412.04062, 2024. Yefei He, Yuanyu He, Shaoxuan He, Feng Chen, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. Neighboring autoregressive modeling for efficient visual generation. arXiv preprint arXiv:2503.10696, 2025. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, and Maosong Sun. Acdit: Interpolating autoregressive conditional modeling and diffusion transformer. arXiv preprint arXiv:2412.07720, 2024. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Yang Jiao, Haibo Qiu, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. Unitoken: Harmonizing multimodal understanding and generation through unified visual encoding. arXiv preprint arXiv:2504.04423, 2025. Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Haopeng Li, Jinyue Yang, Guoqi Li, and Huan Wang. Autoregressive image generation with randomized parallel decoding. arXiv preprint arXiv:2503.10568, 2025a. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37: 5642456445, 2024. Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025b. Chao Liao, Liyang Liu, Xun Wang, Zhengxiong Luo, Xinyu Zhang, Wenliang Zhao, Jie Wu, Liang Li, Zhi Tian, and Weilin Huang. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472, 2025. Haokun Lin, Teng Wang, Yixiao Ge, Yuying Ge, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun, and Ying Shan. Toklip: Marry visual tokens to clip for multimodal comprehension and generation. arXiv preprint arXiv:2505.05422, 2025. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Wenze Liu, Le Zhuo, Yi Xin, Sheng Xia, Peng Gao, and Xiangyu Yue. Customize your visual autoregressive recipe with set autoregressive modeling. arXiv preprint arXiv:2410.10511, 2024b. Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pp. 2340. Springer, 2024. OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023. OpenAI. Introducing 4o image generation, Mar 2025. URL https://openai.com/index/ introducing-4o-image-generation/. Accessed 2025-06-20. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. arXiv preprint arXiv:2412.01827, 2024. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pp. 40554064. PMLR, 2018. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pp. 88218831. Pmlr, 2021. Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond nexttoken: Next-x prediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017. Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, and Kaicheng Yu. Dualtoken: Towards unifying visual understanding and generation with dual visual vocabularies. arXiv preprint arXiv:2503.14324, 2025. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Accelerating auto-regressive text-to-image generation with training-free speculative jacobi decoding. arXiv preprint arXiv:2410.01699, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. Aäron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pp. 17471756. PMLR, 2016. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024a. 15 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. arXiv preprint arXiv:2412.15119, 2024b. Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. arXiv preprint arXiv:2409.16211, 2024. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024a. Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024b. Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation. arXiv preprint arXiv:2503.21979, 2025. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024c. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern transformer. Recognition, pp. 1045910469, 2023a. Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023b. Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Krähenbühl, and De-An Huang. Qlip: Text-aligned visual tokenization unifies auto-regressive multimodal understanding and generation. arXiv preprint arXiv:2502.05178, 2025. 16 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation"
        },
        {
            "title": "A ADDITIONAL IMPLEMENTATION DETAILS",
            "content": "A.1 MODEL ARCHITECTURE We provide the model architecture configurations in Table 3. All models use standard decoder-only transformer architecture. We vary model scale by adjusting the number of layers, the hidden size, and the number of attention heads."
        },
        {
            "title": "Parameters Layers Hidden Size Heads",
            "content": "LPD-L LPD-XL LPD-XXL 111M 775M 1.4B 12 36 48 1024 1280 1536 12 20 48 Table 3: Model architecture configurations. A.2 TRAINING DETAILS We take the training of LPD-L model on 256 256 resolution as an example and list all the training hyper-parameters in Table 4. For LPD-XL and LPD-XXL, we use batch size 1024 and the same base learning rate. For 512 512 models, we load the 256256 pretrained checkpoints and interpolate the positional embeddings. The continued training is conducted for 50 epochs using cosine learning rate decay schedule, preceded by 1 epoch of warm-up. We use batch size 512 for LPD-L and 256 for LPD-XL. Hyper-parameters"
        },
        {
            "title": "Configuration",
            "content": "optimizer β1 β2 learning rate batch size training precision total epochs warm-up epochs constant LR epochs cosine decay epochs offsets AdamW 0.9 0.95 8 1043 2048 (64 32 GPUs) BFloat16 450 50 350 50 random per-sample Table 4: Training hyper-parameters for LPD-L on 256 256 resolution. We train on range of predefined decoding steps where the number of tokens in each step is determined by cosine schedule. For the 256 256 resolution, the decoding steps are randomly selected from the set {8, 12, 16, 20, 24, 32, 64, 128, 256}. For the 512 512 resolution, the decoding steps are randomly selected from the set {32, 40, 48, 56, 64, 80, 96, 128, 160, 192, 224, 256, 512, 1024}. Take 20 steps in the 256 256 resolution as an example, the number of tokens in each step is [1, 2, 4, 5, 7, 8, 10, 11, 12, 14, 15, 16, 17, 18, 18, 19, 19, 20, 20, 20]."
        },
        {
            "title": "B MORE VISUALIZATION OF ATTENTION MAPS",
            "content": "We provide partial visualization of the attention maps in Figure 2 and we provide more here. We select two layers each consists of 24 attention heads during the decoding and visualize them in Figure 11 and Figure 12. 3Effective LR computed as blr (global batch size/256) with blr = 1 104. 17 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation Figure 11: More visualization of attention maps in the LLAMAGEN-1.4B model. Figure 12: More visualization of attention maps in the LLAMAGEN-1.4B model. 18 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation PYTORCH IMPLEMENTATION OF LOCALITY-AWARE GENERATION ORDER 1 import numpy as np 2 import random 3 4 from scipy.spatial.distance import cdist 5 from scipy.spatial.distance import euclidean 6 7 8 def lpd_order_schedule(group_sizes=None, grid_size=16, proximity_threshold=1, repulsion_threshold=1): if group_sizes is None: 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 group_sizes = [1] * (grid_size * grid_size) grid_coords = [[i, j] for in range(grid_size) for in range(grid_size)] selected_coords = [] for step, group_size in enumerate(group_sizes): if step == 0: # For the first step, select random coord. We always assume the group size for the first step is 1. selected_coords.append(random.choice(grid_coords)) continue # Calculate the proximity score for all remaining grid coords candidates = [] for coord in grid_coords: if coord in selected_coords: continue # Calculate the proximity score based on euclidean distance to already selected grid coords proximity_score = 0 for selected_coord in selected_coords: if abs(coord[0] - selected_coord[0]) <= 1 and abs(coord[1] - selected_coord[1]) <= 1: distance = euclidean(coord, selected_coord) if distance > 0: candidates.append([proximity_score, coord]) proximity_score += 1.0 / distance # Shuffle candidates so that grid coords with the same proximity score are randomly ordered random.shuffle(candidates) candidates.sort(key=lambda x: x[0], reverse=True) candidates1 = [item[1] for item in candidates if item[0] >= proximity_threshold] candidates2 = [item[1] for item in candidates if item[0] < proximity_threshold] step_selected = [] step_filtered = [] # Proximity-based selection while len(step_selected) < group_size and candidates1: candidate = candidates1.pop(0) too_close = False for selected in step_selected: if abs(candidate[0] - selected[0]) <= repulsion_threshold and abs(candidate[1] - selected[1]) <= repulsion_threshold: too_close = True step_filtered.append(candidate) 19 Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation break if not too_close: step_selected.append(candidate) step_filtered.extend(candidates1) candidates2.extend(step_filtered) # Low-dependency selection remaining = group_size - len(step_selected) if remaining > 0: step_selected.extend(farthest_point_sampling(step_selected, candidates2, remaining)) selected_coords.extend(step_selected) return np.ravel_multi_index(np.array(selected_coords).T, (grid_size, grid_size)).tolist() 53 54 56 57 58 59 60 62 63 64 65 66 68 69 70 71 def farthest_point_sampling(existing_points, candidate_points, num_to_select): if len(candidate_points) <= num_to_select: return candidate_points # Convert to numpy arrays for efficient computation existing_np = np.array(existing_points) candidates_np = np.array(candidate_points) # Initialize with existing points selected_np = existing_np.copy() selected_indices = [] for _ in range(num_to_select): if len(selected_np) == 0: # If no existing points, select randomly idx = np.random.randint(len(candidates_np)) selected_np = candidates_np[idx][np.newaxis, :] else: # Calculate distances from all candidates to selected points distances = cdist(candidates_np, selected_np) min_distances = np.min(distances, axis=1) # Set already selected candidates to 0 distance min_distances[selected_indices] = 0 # Select the candidate with maximum minimum distance idx = np.argmax(min_distances) selected_np = np.vstack([selected_np, candidates_np[idx]]) selected_indices.append(idx) return [candidate_points[i] for in selected_indices] 72 74 75 76 77 78 80 81 82 83 84 86 87 88 89 90 92 93 94 95 96 98 99 100 101"
        }
    ],
    "affiliations": [
        "First Intelligence",
        "MIT",
        "NVIDIA"
    ]
}