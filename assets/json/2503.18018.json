{
    "paper_title": "Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts?",
    "authors": [
        "Aabid Karim",
        "Abdul Karim",
        "Bhoomika Lohana",
        "Matt Keon",
        "Jaswinder Singh",
        "Abdul Sattar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have significantly advanced various fields, particularly coding, mathematical reasoning, and logical problem solving. However, a critical question remains: Do these mathematical reasoning abilities persist when LLMs are presented with culturally adapted math problems? Specifically, how do LLMs perform when faced with math problems embedded in cultural contexts that have no significant representation in main stream web-scale AI training data? To explore this, we generated six synthetic cultural datasets from GSM8K, a widely used benchmark for assessing LLMs' mathematical reasoning skills. While preserving the mathematical logic and numerical values of the original GSM8K test set, we modify cultural elements such as personal names, food items, place names, etc. These culturally adapted datasets provide a more reliable framework for evaluating LLMs' mathematical reasoning under shifting cultural contexts. Our findings reveal that LLMs struggle with math problems when cultural references change, even though the underlying mathematical structure remains constant. Smaller models exhibit greater performance drops compared to larger models. Interestingly, our results also suggest that cultural familiarity can enhance mathematical reasoning. Even models with no explicit mathematical training but exposure to relevant cultural contexts sometimes outperform larger, mathematically proficient models on culturally embedded math problems. This study highlights the impact of cultural context on the mathematical reasoning abilities of LLMs, underscoring the need for more diverse and representative training data to improve robustness in real-world applications. The benchmark data sets and script for reproducing the results are available at https://github.com/akarim23131/Lost_in_Cultural_Translation"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 1 0 8 1 . 3 0 5 2 : r Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts? Aabid Karim*1, Abdul Karim*2, Bhoomika Lohana1, Matt Keon1, Jaswinder Singh3, and Abdul Sattar4 155mv Research Lab 2Microsoft 3Millcrest Technology 4Griffith University March 25,"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have significantly advanced various fields, particularly coding, mathematical reasoning, and logical problem solving. However, critical question remains: Do these mathematical reasoning abilities persist when LLMs are presented with culturally adapted math problems? Specifically, how do LLMs perform when faced with math problems embedded in cultural contexts that have no significant representation in main stream web-scale AI training data? To explore this, we generated six synthetic cultural datasets from GSM8K, widely used benchmark for assessing LLMs mathematical reasoning skills. While preserving the mathematical logic and numerical values of the original GSM8K test set, we modify cultural elements such as personal names, food items, place names, etc. These culturally adapted datasets provide more reliable framework for evaluating LLMs mathematical reasoning under shifting cultural contexts. Our findings reveal that LLMs struggle with math problems when cultural references change, even though the underlying mathematical structure remains constant. Smaller models exhibit greater performance drops compared to larger models. Interestingly, our results also suggest that cultural familiarity can enhance mathematical reasoning. Even models with no explicit mathematical training but exposure to relevant cultural contexts sometimes outperform larger, mathematically proficient models on culturally embedded math problems. This study highlights the impact of cultural context on the mathematical reasoning abilities of LLMs, underscoring the need for more diverse and representative training data to improve robustness in real-world applications. The benchmark data sets and script for reproducing the results are available at https://github.com/akarim23131/Lost_in_Cultural_Translation"
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have transformed AI, taking on tasks that once seemed to require human intuition. These models have shown remarkable progress in the field of natural language processing, problem solving, question answering, and computer vision tasks (Li et al. [1], Chu et al. [2], Gunter et al. [3], Team et al. [4]). Their ability to interpret and code complex mathematical reasoning has gained tremendous amount of attention from the research community (Mirzadeh et al. [5]). abed.karim@55mv.co abdulkarim@microsoft.com bhoomikalohana70@gmail.com mattk@55mv.co jaswinder@millcrest.com.au a.sattar@griffith.edu.au *These authors contributed equally to this work. 1 However, beneath their impressive capabilities lies crucial question: Do LLMs truly understand the worlds cultural diversity, or reflect simply the cultural limitation embedded in their training data? More specifically, do they retain their mathematical reasoning when presented with culturally adapted math word problems? How does cultural diversityor lack thereofin their training data impact their mathematical reasoning abilities? Studies have consistently highlighted cultural, gender, and sociopolitical biases in LLMs, particularly their Western-centric learning, which affects fairness and adaptability in different linguistic and cultural contexts (Ramesh et al. [6], Ramezani and Xu [7]). Previous research evaluating moral reasoning for LLM in cultures (Ramezani and Xu [7]) further reinforces this concern, showing that these models are predominantly shaped by Western perspectives and struggle in non-Western settings. Research suggests that LLMs do not engage in formal reasoning (Valmeekam et al. [8]) but instead rely on probabilistic pattern-matching to generate outputs based on training data rather than true conceptual understanding of symbols or ideas (Boix-Adsera et al. [9]). This same mechanism contributes to Western biases in multilingual models, where perspectives from economically dominant English-speaking countries are prioritized over local cultural contexts (Naous et al. [10]). For instance, Arabic text generation in models like ChatGPT and BLOOM reflects Western-influenced narratives more strongly than monolingual models such as AraGPT-2, highlighting how imbalanced training data shapes both reasoning pathways and cultural representations. As result, LLMs not only fail to engage in genuine reasoning but also exhibit skewed decision-making patterns that reinforce existing biases in global discourse. Although LLMs generate coherent responses, their logical reasoning remains inconsistent due to fundamental computational limitations in transformer architectures (Li et al. [11]). Structured memory, such as scratchpads, has been proposed to mitigate these limitations, allowing models to handle more complex reasoning tasks (Li et al. [11]). However, this approach requires generating large volumes of tokens to arrive at solution (Peng et al. [12]), making it computationally expensive. Studies have shown that LLMs struggle with formal logical reasoning, as their outputs are highly sensitive to individual token changes (Jiang et al. [13]). Even minor variations in input can lead to vastly different reasoning paths, resulting in inconsistent problem-solving performance. The underlying reason for this variability lies in the way transformers process information. Research suggests that single transformer layer functions similarly to one-nearest neighbor algorithm (Li et al. [14]), meaning the models reasoning process is heavily influenced by the closest matching examples in its training data. Consequently, tasks that require multiple correct token predictions see an exponential decrease in precision as the number of required tokens increases (Shi et al. [15]). The relationship between training frequency and test performance further supports this idea (Razeghi et al. [16]), demonstrating that LLMs rely more on statistical associations than genuine logical inference, limiting their ability to perform consistent reasoning. LLMs are highly sensitive to input tokens, and even slight changes in their tokenization process can alter their reasoning (Grattafiori et al. [17]). If they have not been exposed to diverse and underrepresented cultural norms and contexts during training, they may tokenize culturally specific prompts differently, potentially leading to shifts in reasoning and varied responses. Furthermore, as the number of tokens increases or tasks become more complex, the probability of accurately predicting tokens decreases (Shi et al. [18]). This suggests that LLMs can easily be distracted by increased complexity. But what happens when they are prompted by culturally adapted math problems, how does this impact their mathematical reasoning? These findings raise fundamental questions about their ability to generalize mathematical reasoning in diverse cultural backgrounds. To address these questions, we use the GSM8K dataset (Cobbe et al. [19]), widely recognized benchmark to evaluate the mathematical reasoning capabilities of LLM (Mirzadeh et al. [5]). However, GSM8K has two key limitations. First, given its extensive use, there is high probability that LLMs have encountered this dataset during training, making it less reliable for assessing their reasoning abilities. Second, it consists of single set of math problems, lacking any cultural diversity. To overcome these limitations, we modify the GSM8K test set by introducing culturally adapted versions of each question. Specifically, we synthesize six culturally diverse variants from the original GSM8K test set , one for each continent, while preserving the original mathematical logic and numerical values. The only changes involve replacing culturally specific entities such as names, food items, and contextual references with those relevant to each target culture. detailed explanation of this synthetic dataset generation is provided in Section 3. We then evaluate 14 LLMs, varying in size and release period, in these culturally adapted versions of 2 GSM8K. The remainder of the paper is structured as follows. Section 2 details the dataset creation process, Section 3 discusses our culture (country) selection process, Section 4 shows detailed performance analysis of LLMs, and Sections 5 present evaluation methods and results."
        },
        {
            "title": "2 Dataset Creation",
            "content": "Our dataset creation process and structure are strongly inspired by the GSM symbol dataset introduced by (Mirzadeh et al. [5]). They used the GSM8K test dataset to generate symbolic variants of questions by replacing names with placeholders. This approach enables variations in names while preserving the core mathematical logic, reasoning, and numerical structure of GSM8K. Additionally in their research, numerical values are also adjusted to introduce further complexity to the questions in the GSM8K test set. Similarly, we used the GSM8K test dataset, converting total of 1,319 questions into symbolic duplicates. However, our approach extends beyond just name substitutions. We systematically identify various cultural entities present in the questions, including person names, food items, clothing, city names, school subjects, common sports, etc. Each identified entity is carefully examined and replaced, ensuring that the questions reflect different cultural contexts without altering their mathematical logic. Importantly, we maintain all the numerical values and the original structure of the questions, making targeted adjustments only to reflect cultural variations. The general data set creation process is shown in Figure 1. Figure 1: Cultural Datasets Creation Flow"
        },
        {
            "title": "2.1 Cultural Entities Recognition",
            "content": "Initially, we select representative sample of 200 questions from the 1,319 questions in the GSM8K dataset and manually identify cultural entities through detailed human evaluation. Subsequently, we manually create symbolic versions of seven randomly chosen questions from this subset, replacing the identified cultural entities with accurate placeholders. These manually prepared symbolic questions form the basis of 7-shot prompt constructed for GPT-4o provided in Figure A1 (Appendix 1). Using the established 7-shot prompt, we employ GPT-4o to systematically recognize cultural entities throughout the entire test dataset, processing the questions in batches of 100. GPT-4o outputs each batch in structured format, clearly delineating the original questions, identified entities, and symbolic versions, as shown in Figure 2a. From the GPT-4o output, we extract unique cultural entities identified in all batches. Furthermore, we find that 121 questions did not contain any identifiable cultural entities, reducing the total number of culturally adaptable questions to 1,198. 3 We repeat this process multiple times to have the final correct output as explained in Appendix 2. Finally, we obtained clean and standardized symbolic version of each question, with only identified cultural entities and placeholders at their specific locations, as well as clean set of 55 cultural entities. An example question can be seen in Figure 2a, and the full list of cultural entities is included in Table A1 (Appendix 3)."
        },
        {
            "title": "2.2 Dictionary Creation",
            "content": "After obtaining clean symbolic version with 55 unique cultural entities, we create dictionary for each culture. The process of selecting different cultures (countries) is explained in Section 3. This dictionary is primarily constructed using manual web searches, Wikipedia, country-specific websites, and blogs to gather information about each cultural entity. For instance, if cultural entity is Person name, we search the Web, Wikipedia, and various country-specific sources to collect the most common names associated with that culture. Similarly, for the entity food items, we look for traditional foods specific to that culture and so on for the rest of the entities. In the dictionary, each cultural entity serves as key (e.g., Person name), and the corresponding values are the most common names or items related to that entity. The number of values in the dictionary is directly proportional to how frequently that entity appears in the dataset. For example: Person name occurs the most in our dataset, so we have the largest number of values (names) in the dictionary for the person name entity. Food item is the second most frequent entity, so it has the next highest number of values, and so on accordingly, for the rest of the entities. The scraping process to fill the placeholder with various cultural entities is performed manually, allowing us to inspect and validate each entry to ensure accuracy. This also gives us the opportunity to review our list of identified cultural entities again, so we can remove any discrepancies. Through this process, we build and populate our dictionary for each culture. The values in the dictionary are then used to replace the placeholders in the symbolic version of our dataset. snapshot of dictionary is given in Figure A3 (Appendix 4)."
        },
        {
            "title": "2.3 Mapping Rules",
            "content": "One of the final challenges in our dataset creation is ensuring that the mathematical logic of the cultural dataset questions remains consistent with the original GSM8K test dataset, even after replacing placeholders with cultural entities from our dictionary. The objective is to precisely substitute only cultural entities in their designated placeholders while maintaining consistency for repeated entities in their respective positions. This means: 1. If an entity appears multiple times in question (e.g., the same person name or food item), it should be replaced with the same entity at all occurrences. 2. All of these aspects must be strictly controlled to maintain the logical consistency of the questions. To achieve this, we create an indexing or mapping rule for each question because every question has unique structure and different indexing patterns. Consider the original GSM8K question in Figure 2a, the identified cultural entities are Lisa, Peter, and chocolate bars. The symbolic question replaces them with placeholders {Person name} and {food items} in the appropriate places. However, the problem arises when replacing placeholders with actual values from the cultural dictionary. We only have one key for person names: {Person name} (which contains multiple names (values) in the dictionary. If placeholders are replaced randomly, there is risk that the first two placeholders {Person name} get different names, and when Lisa or Peter appears again, they might be replaced with different name instead of the previously used one. Similarly, {food items} appears four times in the symbolic question, but in the original question, it always refers to chocolate bars. If the logic selects different food items from the dictionary at each occurrence, the mathematical logic of the question breaks down. To fix this issue, we create mapping or indexing rule for each question, as shown in Figure 2b. 4 (a) Symbolic version of an original sample question from GSM8K test dataset (b) Mapping rules for the sample question from GSM8K test dataset (c) Original GSM8K test set sample question, its symbolic version and its cultural variant after replacement Figure 2 1. Word Level Tokenization & Placeholder Indexing: First, we tokenize the symbolic version of the question at the word level. We treat each placeholder (e.g., {Person name}) as single word token rather than separate words. We then identify the exact indices where the placeholders appear. 2. Mapping Back to Original Question: We tokenize the original question at the word level as well and locate the positions where the actual entities appear. We compare these positions with the placeholder indices in the symbolic version. This gives us one-to-one mapping between placeholders and their original entities. 3. Maintaining Logical Consistency in Replacement: This type of mapping ensures that when {Person name} appears at index 0, it is replaced with Lisa. When {Person name} appears again at index 10, it must be replaced with the same entity (Lisa), not another random name. 5 The same rule applies to {food items}, ensuring that all four instances are replaced consistently with chocolate bars, maintaining the original questions meaning. This method removes randomness in entity replacement and ensures that the logical structure of the question remains unchanged. It should be noted that although the provided example illustrates the mapping rules using {Person name} and {food items}, these rules apply universally to all entity types present in our dictionary."
        },
        {
            "title": "2.4 Replacement",
            "content": "Now, we have clean dataset of symbolic questions, set of well-defined mapping rules for each question, and dictionary for each culture. Our dataset consists of 1,198 questions from the GSM8K test set, along with an exact symbolic version of those specific questions. To generate culturally adapted questions, we use simple Python script to replace placeholders with cultural entities while strictly following the mapping rule for each question. The process works as follows: The mapping file is read simultaneously to locate placeholders in the tokenized question using predefined indices. For each placeholder, its type (e.g., {Person name}) is identified, and corresponding entity is selected from the cultural dictionary. tracking dictionary ensures that repeated placeholders in the same question receive the same entity throughout, maintaining logical consistency. The updated tokens are then reconstructed into complete question, ensuring that the structure, logic, and mathematical reasoning remain identical to the original GSM8K test question. This systematic replacement process ensures that our culturally adapted dataset remains logically consistent while introducing culturally relevant variations for evaluation. An example question can be seen in the Figure 2c."
        },
        {
            "title": "3 Countries Selection",
            "content": "For the initial selection of country, we chose Pakistan, as the authors have first-hand cultural knowledge and lived experience in this region. This allowed us to ensure more accurate and contextually relevant adaptation of the dataset. Since deep understanding of cultural nuances is essential for meaningful modifications, selecting country with which the authors are familiar provides reliable foundation for this study. Beyond this, we established structured selection criterion for additional countries. We aimed for broad representation by choosing one country from each continent (excluding Antarctica due to the absence of permanent population) and prioritizing underrepresented and economically disadvantaged countries. These countries were chosen to reflect diverse cultural and socioeconomic contexts that may lack strong representation in AI and technology. By combining personal expertise with systematic selection process, we strive to create more comprehensive and meaningful evaluation of cultural biases in LLMs. We used four indicators given below to define underrepresented and economically disadvantaged countries. Human Development Index (HDI): Published by the United Nations Development Program (UNDP). It combines life expectancy, education, and per-capita income indicators. Gross National Income (GNI) per capita: Used by the World Bank to classify countries into low-income, lower-middle, upper-middle, and high-income categories. Least Developed Countries (LDC): UN-designated list based on income per capita, human assets (health, education) and economic vulnerability. Multidimensional Poverty Index (MPI): Focuses not only on income, but also on education, health, and standard of living. 6 We define country as underrepresented and economically disadvantaged if it is classified by the UN as Least Developed Country (LDC) and appears in the bottom quartile of the Human Development Index (HDI) rankings for its region. To ensure that our research covers broad range of underrepresented countries, one from each of the five populated continents (Africa, Europe, North America, South America and Oceania), we select based on the following criteria: Countries are on the list of LDC of the United Nations for their region and appear in the bottom quartile of the Human Development Index (HDI) rankings. In the absence of an LDC in that region, for example, Europe rarely has LDCs, we select the lowest-ranked country on the Human Development Index as well as falling below certain Gross National Income (GNI) index. Beyond economic metrics, we ensure that countries also reflect distinct languages, ethnicities, or cultural norms, as our study focuses on cultural biases. Table 1 contains the list of selected countries. No 1 2 3 4"
        },
        {
            "title": "Continent\nAsia\nEurope\nAfrica",
            "content": "Dataset PakGSM8K MolGSM8K SomGSM8K North America HaiGSM8K SurGSM8K South America SolIGSM8K Oceania Table 1: Countries and Datasets"
        },
        {
            "title": "4.1 Evaluation Methodology",
            "content": "To assess the mathematical reasoning capabilities of LLMs in culturally adapted math problems, we systematically evaluate their accuracy across six cultural variants of the GSM8K test set. Each model is prompted with identical math problems, including the original GSM8K test dataset as well as its culturally modified versions for Haiti (HaiGSM8K) , Moldova (MolGSM8K) , Pakistan (PakGSM8K) , Solomon Islands (SolIGSM8K), Somalia (SomGSM8K), and Suriname (SurGSM8K). We select diverse set of models from Anthropic, OpenAI, Google, Meta, DeepSeek, Mistral, and Microsoft based on both the size and release timeline to assess how mathematical reasoning evolves across different architectures. Our selection includes smaller models with relatively fewer parameters and larger models, allowing us to examine how scale impacts the performance of these models for our culturally adapted datasets. To ensure controlled evaluation, we keep all hyperparameters constant across models and use an identical prompting strategy. Each model attempts every question three times, generating three independent responses per dataset. The prompt used for the evaluation is provided in Figure A4 (Appendix 5). We evaluate accuracy by comparing model-generated answers with the ground truth from the GSM8K test set. Accuracy is defined as the number of correctly answered questions divided by the total number of questions. To ensure reliability, we adopt strict consistency accuracy metric hereafter called strict accuracy: question is considered correct only if all three generated responses exactly match the ground truth. If even one of the three responses is incorrect, the question is marked as incorrect. This approach mitigates random correct guesses and ensures that the accuracy reflects consistent performance rather than chance. To ensure robust accuracy estimates, we calculate 95% Confidence Intervals (CIs) using the Wilson score interval, which provides more reliable estimates than the normal approximation, particularly when accuracy values are near the extremes, such as 0% or 100%. In our case, model accuracy tends to be high (typically around 95%), with only small proportion of questions answered incorrectly. Since accuracy is computed as proportion of binary outcomes, each question is either fully correct or not to create Bernoulli distribution. The Wilson method accounts for this distributional shape and corrects for the skew introduced near the boundary values. 7 To determine whether models perform significantly different or not in cultural variants, we use McNemars test, paired significance test that compares two datasets. We define our hypotheses as follows. Null Hypothesis (H0): Cultural adaptation of the question does not affect the models accuracy. (The model is equally likely to be correct on GSM8K and its cultural variant.) Alternate Hypothesis (H1): The model is more likely to be correct on the original GSM8K question than on its culturally adapted version. (Cultural adaptation leads to lower accuracy.) McNemars test counts: Cases where the model is correct on GSM8K but incorrect on the cultural variant. Cases where the model is incorrect on GSM8K but correct on the cultural variant."
        },
        {
            "title": "4.2.1 Model Accuracy Across Cultural Variants",
            "content": "To analyze the impact of cultural modifications on the mathematical reasoning ability of LLMs, we compare their strict accuracy on the original GSM8K dataset and its six culturally adapted variants for each model. The results are presented in Figure 3, where each subplot compares the performance of the model in GSM8K (red) with its corresponding cultural dataset (blue). Each point represents strict accuracy, calculated under consistency criterion where model is only marked correct if all three outputs match the ground truth. These points in all the plots are accompanied by horizontal lines indicating the 95% confidence interval, computed using the Wilson score method. These intervals reflect the uncertainty around the strict accuracy estimates and help visualize whether observed differences between GSM8K and its cultural variants are likely statistically meaningful. Across all models and datasets, we observe consistent pattern: models perform better on GSM8K than on the culturally modified versions. This is visually evident by the leftward shift of blue dots compared to red ones, indicating drop in accuracy when problems are reframed in different cultural contexts. The drop is relatively smaller for Haiti and Moldova, while more pronounced for Pakistan, Somalia, Solomon Islands, and Suriname. These results suggest that even when the underlying mathematical logic remains unchanged, cultural framing can significantly influence model performance. This variation suggests that certain cultural shifts disrupt reasoning more than others, possibly due to differences in entity recognition, familiarity with cultural references, or exposure to training data. Claude 3.5 Sonnet, GPT-4o, Gemini 2.0, and Qwen 2.5-32B showed the smallest accuracy drop, implying better generalization across cultural contexts. These models retained strong reasoning abilities despite cultural variations. While, Meta LLaMA 3.1-8B, Microsoft Phi-3 Medium, Gemma-2-9B showed substantial accuracy reductions, indicating difficulty adapting to cultural modifications. Phi-3 Medium and LLaMA 3.1-8B exhibited significant drops on Pakistan, Solomon Islands, and Somalia datasets, suggesting that these specific variations posed greater challenges for smaller models. Solomon Islands dataset resulted in the largest performance gap across models. Meta LLaMA 3.1-8B and Microsoft Phi-3 Medium exhibited the most drastic accuracy reductions on Pakistan and Somalia, where reasoning failures were more prominent. Mistral models performed relatively well across all datasets, particularly Mistral Saba, which is trained on highly diverse dataset encompassing linguistic and cultural nuances from the Middle East and South Asia, but still exhibited noticeable drops, particularly on Suriname and Pakistan. For detailed numerical values corresponding to these accuracy scores, including confidence intervals, refer to Table A2 in (Appendix 6). 8 Figure 3: Accuracy Comparison of GSM8K vs culturally variant versions of GSM8K across various models"
        },
        {
            "title": "4.2.2 Performance Gap Analysis",
            "content": "We compute performance gap between the GSM8K test set and its culturally adapted versions. The performance gap is calculated by subtracting the accuracy of each model in cultural variant from its accuracy in the original GSM8K. higher value indicates greater drop in performance when faced with culturally adapted math problems and vice versa. This gap analysis allows us to identify which models are more sensitive to cultural variations and which ones generalize better across diverse linguistic or contextual settings. The bar charts in Figure 4 display this accuracy drop across multiple cultural variants for each model, with the models listed on the y-axis and the magnitude of the accuracy drop shown on the x-axis. Across all graphs, Meta-LLaMA 3.1-8B-Instruct consistently shows the highest accuracy drop for multiple datasets. This indicates that this model struggles the most with math reasoning in diverse cultural contexts. Anthropic Claude 3.5-Sonnet consistently shows the smallest accuracy drop, meaning it maintains the most stable performance across datasets. In Haiti for instance, Meta-LLaMA 3.1-8B-Instruct has the highest accuracy drop (4.0%), showing that it struggles with numerical reasoning in Haitian contexts. Most models have drop of 1-3%, with Claude 3.5-Sonnet dropping only 0.3%. Meta-LLaMA 3.1-8B and 70B have the highest accuracy drop on Moldova( 3.8%), followed by Microsoft Phi-3 Medium. Claude 3.5-Sonnet remains the most stable, with only 0.4% drop. This shows that models that are heavily optimized for English-centric training data tend to show accuracy drops on Moldova, probably due to insufficient exposure of cultural reasoning patterns in training data. More balanced multilingual training may help address this issue, as shown by models such as Claude 3.5-Sonnet, which demonstrates stronger generalization across various datasets. Similar trends are observed in the other subplots as well. Detailed values can be found in Table A3 and Table A4 in Appendix 7. Claude 3.5 consistently outperforms other models across various datasets and is notably robust in mathematical reasoning. It achieved score of 71.1% on the MATH benchmark and an impressive 91.6% on multilingual math tasks (MGSM) (Anthropic [20]). However, when cultural and linguistic variations are introduced, the performance dynamics changes slightly. Interestingly, Mistral Saba, despite not having dedicated mathematical reasoning benchmark and not being explicitly tuned for math, handles the Pakistan variant and some other cultural adaptations better. This may be attributed to its training on data from Middle Eastern and South Asian sources. This indicates that even if model isnt specifically designed for mathematical reasoning, having deeper understanding of regions cultural context and reasoning patterns can help it perform better on math tasks within that specific cultural framework. There could be other possible reasons for the accuracy drop of models when the context is shifted. Different languages and cultural terminologies lead to variations in tokenization. If the models tokenizer is not well-suited to the culturally adapted language, single concept might be represented by more tokens, leading to increased complexity and potential for error. The model may also have fewer training examples with the specific terms used in the adapted problems (Petrov et al. [21], Dang et al. [22]). This suggests that models are not entirely language-agnostic for mathematical reasoning performance, even if they are multilingual. Their mathematical reasoning performance could be tied to how efficiently their tokenizers can represent different languages and cultural vocabularies. As shown in Figure A5 (Appendix 8), adapting names from Amalia, Megan, and Dior to Aleskandra, Nicolae, and Albert changes the total number of tokens and characters. In the original text, we have 104 tokens and 452 characters, while the Moldovan-adapted version yields 109 tokens and 477 characters. This variation reflects how the tokenizer handles different linguistic structures and could play role in affecting model performance. 10 Figure 4: Performance Gap of Models across various culturally adapted GSM8K variants Similarly, mathematical reasoning isnt always culturally neutral. The way problem is framed, even with the same numbers, can subtly influence how someone may approach it. Different cultures might have different common-sense assumptions or preferred methods for organizing information (Meng and Liu [23], Tajika [24]). This implies that LLMs arent just learning to solve math problems; theyre also learning to solve them in particular way, based on the dominant problem-solving styles in their training data. Cultural versions might require slightly different reasoning pathways that the model is less familiar with. 11 Also, LLMs are trained on vast amounts of data. If the pre-training data is heavily biased towards certain cultures or problem-solving styles, the model will naturally perform better in those familiar contexts. The cultural versions of GSM8K introduce scenarios the model hasnt seen as often (or at all) during pre-training. This underscores the importance of diverse and representative pre-training datasets. model trained primarily on Western-centric data might struggle with problems framed in the context of other cultures. Last but not the least, LLMs are prone to hallucination (Banerjee et al. [25]). In culturally adapted versions, models might unintentionally introduce stereotypes or incorrect cultural assumptions into their reasoning process, leading to errors. For example, it might associate specific region with certain type of economic activity and introduce that (incorrect) assumption into the calculation."
        },
        {
            "title": "4.2.3 Statistical Significant Testing",
            "content": "To statistically assess whether LLMs perform differently on culturally adapted math questions compared to the original GSM8K test set, we conduct McNemar tests using one-to-one aligned question pairs. The results reveal clear pattern: models like LLaMA 3.1-70B, Gemini Flash 2.0, and Mistral Large 2411 consistently exhibit statistically significant performance drops (p < 0.01 across most datasets), with high b-values indicating that these models frequently answered questions correctly on GSM8K but failed on their culturally adapted versions. This supports the rejection of the null hypothesis, suggesting these models struggle more in mathematical reasoning when cultural contexts shift. In contrast, Claude 3.5 and Mistral Saba generally show no significant performance difference (p > 0.05 in most cases), with balanced b/c values, indicating that their performance is more stable across cultural contexts, and we fail to reject the null hypothesis for these models. detailed breakdown of McNemar test statistics, p-values, and b/c counts for each model is provided in Table A5 (Appendix 9)."
        },
        {
            "title": "4.2.4 Qualitative Error Analysis",
            "content": "In addition to quantitative evaluations, we conduct detailed qualitative error analysis to better understand how LLMs handle culturally adapted math word problems. Our analysis reveals three major patterns of reasoning failure across models: Models often struggle with numerical reasoning when using less familiar currency units (e.g., Haitian Gourde, HTG). For instance, some models treat 0.1 HTG as though it were 1 HTG, whereas they correctly handle 0.1 USDlikely because HTG is rarely used in decimals due to inflation and rounding practices, causing the models to miss consistent arithmetic across currency formats. Replacing wife with unfamiliar family terms like Jija (Pakistani) or Tambu man (Solomon Islands) often led models to miscalculate. While they correctly process husband-wife, they struggle with non-Western family structurespointing to errors in how they interpret different cultural entities. Sometimes the models fail to link culturally specific terms to their real meanings. For example, substituting local animal names in counting problem lead the models to default to incorrect assumptions, suggesting when confronted with unfamiliar cultural words, they may be relying on learned patterns from their original training data. These insights highlight how cultural context can introduce variability in reasoning even when the underlying math remains unchanged. Examples and detail of these errors are provided in Figures A6, A7, and A8 (Appendix 10.1 and 10.2)"
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we investigate whether LLMs can maintain stable performance in solving mathematical problems in diverse cultural contexts. To do so, we adapted the GSM8K dataset by incorporating cultural elements, thus creating six regional variants alongside the original dataset. Our evaluation of these variants 12 reveals that while the mathematical principles underlying GSM8K remain unchanged, the introduction of cultural variations does affect model performance. The results show that larger models, such as Anthropic Claude 3.5-Sonnet and GPT-4o, tend to generalize better across these diverse contexts; however, they still experience noticeable performance drops when faced with culturally adapted math problems. In contrast, models like Mistral Sabadespite not being explicitly tuned for mathdemonstrate improved performance in regions where they have been exposed to local data. This suggests that cultural familiarity can enhance mathematical problem solving, as models leverage context-specific linguistic and structural cues. Furthermore, our analysis indicates that changes in cultural context may also pose challenges to the tokenization process. Variations in vocabulary and linguistic structures across different regions can lead to differences in how input text is tokenized, which in turn affects model performance on math problem solving tasks. In general, our study underscores the importance of considering cultural context when evaluating the mathematical reasoning capabilities of LLMs. It highlights that while universal mathematical principles are at play, factors such as cultural familiarity play significant roles in shaping model performance."
        },
        {
            "title": "References",
            "content": "[1] Yifan Li, Zhixin Lai, Wentao Bao, Zhen Tan, Anh Dao, Kewei Sui, Jiayi Shen, Dong Liu, Huan Liu, and Yu Kong. Visual Large Language Models for Generalized and Specialized Applications. arXiv preprint arXiv:2501.02765, 2025. [2] Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: unified llama backbone for vision tasks. In European Conference on Computer Vision, pages 118. Springer, 2024. [3] Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, et al. Apple intelligence foundation language models. arXiv preprint arXiv:2407.21075, 2024. [4] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [5] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229, 2024. [6] Krithika Ramesh, Sunayana Sitaram, and Monojit Choudhury. Fairness in language models beyond English: Gaps and challenges. arXiv preprint arXiv:2302.12578, 2023. [7] Aida Ramezani and Yang Xu. Knowledge of cultural moral norms in large language models, 2023. URL https://arxiv.org/abs/2306.01857. [8] Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change, 2023. URL https://arxiv.org/abs/2206.10498. [9] Enric Boix-Adsera, Omid Saremi, Emmanuel Abbe, Samy Bengio, Etai Littwin, and Joshua Susskind. When can transformers reason with abstract symbols?, 2024. URL https://arxiv.org/abs/2310. 09753. [10] Tarek Naous, Michael J. Ryan, Alan Ritter, and Wei Xu. Having Beer after Prayer? Measuring Cultural Bias in Large Language Models, 2024. URL https://arxiv.org/abs/2305.14456. [11] Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of Thought Empowers Transformers to Solve Inherently Serial Problems, 2024. URL https://arxiv.org/abs/2402.12875. [12] Binghui Peng, Srini Narayanan, and Christos Papadimitriou. On Limitations of the Transformer Architecture, 2024. URL https://arxiv.org/abs/2402.08164. [13] Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su, Camillo J. Taylor, and Dan Roth. Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners, 2024. URL https://arxiv.org/abs/2406.11050. [14] Zihao Li, Yuan Cao, Cheng Gao, Yihan He, Han Liu, Jason M. Klusowski, Jianqing Fan, and Mengdi Wang. One-Layer Transformer Provably Learns One-Nearest Neighbor In Context, 2024. URL https: //arxiv.org/abs/2411.10830. [15] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Scharli, and Denny Zhou. Large Language Models Can Be Easily Distracted by Irrelevant Context, 2023. URL https://arxiv.org/abs/2302.00093. [16] Yasaman Razeghi, Robert L. Logan IV, Matt Gardner, and Sameer Singh. Impact of Pretraining Term Frequencies on Few-Shot Reasoning, 2022. URL https://arxiv.org/abs/2202.07206. [17] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [18] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Scharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pages 3121031227. PMLR, 2023. [19] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [20] Anthropic. Introducing claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. Accessed: 2025-03-22. [21] Aleksandar Petrov, Emanuele La Malfa, Philip Torr, and Adel Bibi. Language model tokenizers introduce unfairness between languages. Advances in neural information processing systems, 36: 3696336990, 2023. [22] Thao Anh Dang, Limor Raviv, and Lukas Galke. Tokenization and Morphology in Multilingual Language Models: Comparative Analysis of mT5 and ByT5. arXiv preprint arXiv:2410.11627, 2024. [23] Jingyi Meng and Simiao Liu. Effects of culture on the balance between mathematics achievement and subjective wellbeing. Frontiers in Psychology, 13:894774, 2022. [24] Hidetsugu Tajika. Differences in Mathematical Problem-Solving Skills Between Japanese and American Children. PhD thesis, Aichi University of Education, 2004. [25] Banerjee, Agarwal, and Singla. LLMs Will Always Hallucinate, and We Need to Live with This. arXiv preprint arXiv:2409.05746, 2004. Appendix A: Dataset Creation"
        },
        {
            "title": "1 Prompt for Cultural Entities Recognition",
            "content": "Figure A1: Prompt for Cultural Entities Recognition 15 Figure A2: Prompt for Recognized Cultural Entities Evaluation"
        },
        {
            "title": "2 Evaluation and Manual Correction of Recoganized cultural Entities",
            "content": "To verify that GPT-4o accurately identified only cultural entities, we prepare 5-shot prompt as shown in the Figure A3 and use it to verify its own output. The goal is to check whether it correctly identified cultural entities, replaced them with placeholders in the right spots, and ensure that the mathematical logic and structure of the questions remained intact. The prompt can be found in the appendix. After this step, we conduct human evaluation as well. We manually review symbolic questions and the identify cultural entities to verify that: 16 1. The mathematical logic of the questions remained unchanged. 2. There were no modifications to numerical values. During this process, we encounter small inconsistency. While GPT-4o correctly identified cultural entities and verified them using our 5-shot prompt, it used inconsistent names for the same entity. For example, it correctly identified Person name in each question and replaced it with placeholder. But sometimes, it used name, common name, or Person name in the placeholders. Similarly, there were variations in other entitieslike food items, which appeared as food item, common food items, types of food, and cooking items. These technically mean the same thing but have different representative names. This inconsistency could cause issues in our dictionary creation because we would end up with different keys for the same set of values, unnecessarily inflating the dictionary. To fix this, we modify our 7-shot prompt, run another iteration, and manually review the symbolic version again. Any remaining inconsistent placeholder names are corrected."
        },
        {
            "title": "3 Recognized Cultural Entities by GPT-4o",
            "content": "Person name Types of pastries/local deserts City name Types of houses Types of goods merchant purchase food items Common type of sport cooking item Types of beverages Types of books Types of places Recreation activity types of shows School subject Types of games family member Types of musical compositions Types of classes company names restaurant name Mythical character Types of entertainment places Government body Cultural songs common places appliances religious place school subject currency currency sign Types of commercial establishments Types of dance Types of common jobs clothing items Common brand name Types of events Common clothing items Types of vehicles animal Types of family events Village names cultural event Types of flowers recreation places profession holiday Types of teacher cultural landmark online shopping platforms cultural dance style Types of scents school name Types of tea newspaper names Language Table A1: Cultural Entities"
        },
        {
            "title": "4 Dictionary",
            "content": "Figure A3: Screenshot of Dictionary Appendix B: LLMs Evaluation"
        },
        {
            "title": "5 Prompt",
            "content": "Figure A4: Prompt for LLMs Evaluation on all Datasets Our prompt is straightforwardit does not instruct the LLM to solve math problems in any specific way, nor does it emphasize math, numerical reasoning, or cultural elements. Instead, it simply guides the LLM to 18 solve the problem step by step. The actual math problem is then introduced within this structured approach. We also use the same prompt on all of the models for all the datasets. By following this simple prompting method, we ensure that no external factors influence the LLMs response. The model focuses solely on solving the given mathematical problem, independent of any cultural or contextual elements. This allows us to obtain clean responses from the models when we make cultural modifications to the GSM8K questions."
        },
        {
            "title": "6 Models Accuracy Across Cultural Variants",
            "content": "Model G8K Hti Mld Pak Sol Som Sur C3.5 DSeek G2.0 G1. G27B G9B L70B L8B P3M M2411 MSaba G4o Q32B 0.95 0.94 (0.94-0.96) (0.93-0.96) (0.93-0.96) (0.92-0.95) (0.93-0.95) (0.93-0.95) (0.93-0.95) 0.94 0.94 0.95 0.94 0.94 0. 0.90 (0.91-0.94) (0.90-0.93) (0.89-0.92) (0.88-0.92) (0.88-0.91) (0.89-0.92) (0.88-0.92) 0.91 0.89 0.90 0.90 0. 0.94 0.91 (0.92-0.95) (0.90-0.93) (0.90-0.93) (0.89-0.92) (0.90-0.93) (0.89-0.93) (0.90-0.93) 0.92 0.92 0.91 0. 0.91 0.83 0.80 (0.80-0.85) (0.78-0.82) (0.79-0.83) (0.78-0.83) (0.79-0.83) (0.78-0.83) (0.79-0.83) 0.81 0.81 0. 0.80 0.81 0.86 0.84 (0.84-0.88) (0.82-0.86) (0.82-0.86) (0.81-0.86) (0.82-0.86) (0.82-0.86) (0.81-0.85) 0.85 0. 0.84 0.84 0.83 0.82 0.80 (0.79-0.84) (0.78-0.82) (0.78-0.82) (0.78-0.82) (0.78-0.82) (0.77-0.82) (0.77-0.82) 0. 0.80 0.80 0.80 0.80 0.91 0.87 (0.89-0.93) (0.87-0.90) (0.85-0.89) (0.86-0.89) (0.87-0.90) (0.86-0.90) (0.86-0.89) 0.89 0.87 0.89 0.88 0.88 0. 0.58 (0.61-0.67) (0.57-0.63) (0.57-0.63) (0.56-0.61) (0.58-0.63) (0.55-0.61) (0.57-0.62) 0.60 0.60 0.60 0.58 0. 0.77 0.75 (0.75-0.79) (0.72-0.77) (0.72-0.77) (0.73-0.78) (0.68-0.73) (0.73-0.78) (0.72-0.77) 0.71 0.75 0.75 0. 0.75 0.91 0.89 (0.89-0.92) (0.88-0.91) (0.87-0.91) (0.88-0.91) (0.87-0.90) (0.86-0.90) (0.87-0.91) 0.89 0.89 0. 0.88 0.89 0.92 0.88 (0.91-0.94) (0.88-0.91) (0.89-0.92) (0.86-0.90) (0.88-0.91) (0.86-0.89) (0.87-0.91) 0.90 0. 0.91 0.88 0.89 0.87 0.87 (0.86-0.89) (0.85-0.89) (0.84-0.88) (0.85-0.89) (0.85-0.88) (0.84-0.88) (0.84-0.88) 0. 0.87 0.86 0.86 0.86 0.93 0.91 (0.92-0.95) (0.91-0.94) (0.90-0.93) (0.90-0.93) (0.91-0.94) (0.91-0.94) (0.90-0.93) 0.93 0.93 0.91 0.92 0.92 0. 0.88 (0.89-0.92) (0.87-0.91) (0.88-0.91) (0.86-0.90) (0.86-0.90) (0.88-0.91) (0.87-0.90) 0.88 0.89 0.90 0.89 0. Accuracy Scores Across Models and Datasets. indicate Table A2: confidence intervals (CI). C3.5 = anthropic claude-3.5-sonnet, DSeek = deepseek deepseek-v3, G2.0 = google gemini-2.0-flash-001, G1.5 = google gemini-flash-1.5-8b, G27B = google gemma-2-27b-it, G9B = google gemma-2-9b-it, L70B = meta-llama llama-3.1-70b-instruct, L8B = meta-llama llama-3.1-8b-instruct, P3M = microsoft phi-3-medium-128k-instruct, P4 = microsoft phi-4, M2411 = mistralai mistral-large-2411, MSaba = Mistral Saba, G4o = chatgpt-4o-latest, Q32B = qwen2.5-32b-instruct. G8K = GSM8K, Hti = HaiGSM8K, Mld = MolGSM8K, Pak = PakGSM8K, Sol = SolIGSM8K, Som = SomGSM8K, Sur = SurGSM8K. in parentheses"
        },
        {
            "title": "Values",
            "content": ""
        },
        {
            "title": "7 Performance Gap",
            "content": "Model Hti Gap Mld Gap Pak Gap Sol Gap Som Gap Sur Gap Claude 3.5 DeepSeek Gemini 2.0 Gemini 1.5 Gemma 27B Gemma 9B LLaMA 70B LLaMA 8B Phi-3 Medium Phi-4 Mistral Large Mistral Saba ChatGPT-4o Qwen 32B 0.0025 0.0117 0.0184 0.0275 0.0242 0.0142 0.0250 0.0401 0.0234 0.0142 0.0267 0.0033 0.0067 0.0142 0.0042 0.0209 0.0192 0.0175 0.0184 0.0150 0.0376 0.0376 0.0217 0.0175 0.0167 0.0134 0.0184 0.0109 0.0109 0.0225 0.0292 0.0225 0.0275 0.0159 0.0359 0.0551 0.0167 0.0159 0.0417 0.0025 0.0200 0. 0.0083 0.0301 0.0200 0.0167 0.0250 0.0142 0.0250 0.0351 0.0626 0.0242 0.0259 0.0083 0.0075 0.0250 0.0083 0.0217 0.0275 0.0217 0.0259 0.0209 0.0317 0.0593 0.0175 0.0309 0.0459 0.0117 0.0109 0.0134 0.0067 0.0242 0.0242 0.0175 0.0317 0.0209 0.0342 0.0426 0.0234 0.0200 0.0326 0.0117 0.0142 0.0192 Table A3: Performance Gap Analysis Across Datasets The Table A3 4 explains the difference in performance (accuracy) of all the 14 models across all the datasets. These gaps are explained in detail in results section. The values in the table are presented in decimal form, whereas in the main paper (Graph 5), they have been converted to percentages for ease of interpretation."
        },
        {
            "title": "Sur",
            "content": "Count Mean Std Min 25% 50% (Median) 75% Max 14 0.0180 0.0105 0.0025 0.0123 0.0163 0.0248 0.0401 14 0.0192 0.0090 0.0042 0.0154 0.0179 0.0205 0.0376 14 0.0245 0.0133 0.0025 0.0161 0.0225 0.0288 0.0551 14 0.0234 0.0141 0.0075 0.0148 0.0246 0.0257 0.0626 14 0.0248 0.0141 0.0083 0.0144 0.0217 0.0301 0. 14 0.0231 0.0096 0.0067 0.0179 0.0221 0.0298 0.0426 Table A4: Descriptive Statistics of accuracy drops across models Table A4 presents descriptive statistics of accuracy drops across different models for six datasets: Haiti (Hti), Moldova (Mld), Pakistan (Pak), Solomon Islands (Sol), Somalia (Som), and Suriname (Sur). The values represent the magnitude of performance drops when comparing each models accuracy on the culturally adapted datasets against the original GSM8K dataset. Count shows that all datasets have results from 14 different models. Min and Max indicate the smallest and largest observed accuracy drops, respectively. Percentiles (25%, 50% (Median), and 75%) show the spread of accuracy drops."
        },
        {
            "title": "8 Difference in Tokenization",
            "content": "Figure A5: Difference in Tokenization We utilize the OpenAI tokenizer to analyze how question from the original GSM8K dataset is tokenized compared to its Moldovan-contextualized version. Notably, the only modification in this example is the change of names, yet the tokenization process treats them differently. This discrepancy highlights how even minor cultural adaptations can alter tokenization, leading to variations in how the model processes and understands the question."
        },
        {
            "title": "9 McNemar Test",
            "content": "The McNemar test results provide statistical measure of whether the accuracy of different models significantly dropped when tested on culturally adapted datasets. This helps us understand which models are more sensitive to cultural shifts and which ones are more robust. Each cell in the table contains p-value, along with two numbers (b, c) in parentheses. 21 Model Hti Mld Pak Sol Som Sur Mistral Saba 0.74933 (46,42) 0.12929 (57,41) 0.83585 (48,45) 0.36820 (55,45) 0.19335 (57,43) 0.17498 (53,39) Gem Flash 1.5-8B 0.00293 (75,42) 0.06171 (68,47) 0.01773 (74,47) 0.08241 (70,50) 0.01988 (71,45) 0.06399 (69,48) Gemma 2-27B 0.00346 0.01832 0.00119 0.00288 0.00152 0.00021 (61,32) (51,29) (66,33) (63,33) (61,30) (70,32) LLaMA 3.1-70B 0.00231 0.00001 0.00003 0.00423 0.00018 0.00007 (61,31) (75,30) (73,30) (67,37) (69,31) (72,31) Gemma 2-9B 0.16826 (76,59) 0.10461 (64,46) 0.10420 (71,52) 0.15212 (71,54) 0.04588 (85,60) 0.04140 (82,57) Phi-4 DeepSeek 0.06037 (45,28) 0.02203 (49,28) 0.05025 (52,33) 0.00169 0.00016 0.00631 (65,28) (48,24) (55,26) 0.14564 (47,33) 0.00804 0.00280 0.00016 0.00734 0.00169 (62,26) (55,26) (54,29) (57,31) (52,25) Gem Flash 2.0 0.00094 0.00061 0.00000 0.00027 0.00000 0.00002 (32,10) (33,10) (46,11) (33,9) (41,8) (38,9) Phi-3 Medium 0.08496 (137,109) 0.10346 (131,105) 0.20416 (122,102) 0.00000 (161,86) 0.18424 (124,103) 0.07479 (129,101) Mistral Large 0.00031 0.03079 0.00000 0.00117 0.00000 0.00002 (54,22) (49,29) (66,16) (59,28) (75,20) (61,22) ChatGPT-4o Qwen 2.5-32B Claude 3.5 LLaMA 3.1-8B 0.33175 (30,22) 0.06755 (47,30) 0.74283 (20,17) 0.00535 0.00427 (40,18) (45,21) 0.27168 (31,22) 0.19276 (49,36) 0.47313 (18,13) 0.00111 0.00161 (62,30) (58,28) 0.06599 (28,15) 0.09874 (20,10) 0.11116 (35,22) 0.10523 (51,35) 0.14331 (24,14) 0.02701 (35,18) 0.02202 (58,35) 0.24298 (22,14) 0.00674 0.00879 0.00017 0.01628 0.00005 0.00242 (162,111) (175,127) (167,125) (164,119) (184,118) (185,114) Table A5: McNemar Test Results for Model Performance Across Datasets. Values represent p-values (rounded to 5 decimal places). Significance: < 0.10, < 0.05, < 0.01. (b,c) values in parentheses. represents the number of times the model got the question right on the original GSM8K dataset but wrong on the culturally adapted dataset. represents the number of times the model got the question wrong on GSM8K but right on the adapted dataset. higher compared to indicates that the model struggles more with the adapted dataset. low p-value (p < 0.05) means the accuracy drop is statistically significant and not due to random chance. Certain models showed significant performance drops, suggesting that they struggle to generalize to culturally adapted datasets. LLaMA 3.1-70B consistently had high b-values across all datasets, meaning it frequently failed on culturally adapted versions of the questions while performing well on the original GSM8K. It had some of the lowest p-values (p < 0.01), particularly in Moldova, Pakistan, Somalia, and Suriname. This suggests that the models pretraining data might not be diverse enough to handle different cultural contexts in mathematical reasoning. Gemini Flash 2.0 also exhibited significant accuracy drops on all datasets, with particularly large b-values in Pakistan, Solomon Islands, and Somalia. The very low p-values indicate that these failures were systematic rather than random. This suggests that Gemini Flash 2.0 may have 22 strong Western-centric bias, causing difficulties in understanding culturally adapted variations of math problems. Mistral Large 2411 aced similar issues, with substantial accuracy drops across all datasets, particularly in Pakistan and Somalia. The fact that b-values are consistently high compared to c-values means the model performs well in the original setting but fails when cultural factors are introduced. Some models had moderate but still statistically significant performance drops, meaning they werent completely failing, but they still showed weaknesses. DeepSeek showed notable accuracy drop in Moldova, Pakistan, Solomon Islands, and Somalia. The p-values are under 0.01 in these cases, showing that model probably struggles with mathematical reasoning when cultural adaptations are introduced. However, its b-values are not as extreme as those of Gemini Flash or LLaMA 3.1-70B, suggesting some ability to adapt. Phi-4 also showed significant drops in Moldova, Solomon Islands, and Somalia, with p-values below 0.05. While the drop was not as severe as in LLaMA or Gemini Flash, it suggests that Phi-4 might not generalize well to unfamiliar cultural settings in solving math problems. Gemma 2-27B showed consistent accuracy reductions across all datasets, particularly in Pakistan, Somalia, and Suriname. The p-values and high b-values confirm that the model is sensitive to cultural variations. The McNemar test results show that certain models, like LLaMA 3.1-70B and Gemini Flash 2.0, struggle significantly with cultural variations, while others, like Mistral Saba and Claude 3.5, remain more stable."
        },
        {
            "title": "10 Qualitative Error Analysis",
            "content": "Qualitative error analysis examines mathematical reasoning errors in culturally adapted versions of GSM8K problems across 14 models, each evaluated over three runs. While the mathematical logic remains unchanged, cultural adaptations of GSM8K introduce linguistic and contextual variations. We assess correctness by comparing model-generated answers to the GSM8K ground truth, identifying discrepancies caused by reasoning errors or cultural misinterpretations. The goal is to uncover error patterns and model weaknesses, determining whether failures are universally difficult or model-specific. question is marked incorrect if at least one of the three runs produces an incorrect reasoning answer, rather than relying on majority voting. This approach captures model inconsistencies, highlighting cases where failures occur occasionally rather than systematically. Each answer is compared against the GSM8K ground truth to distinguish genuine numerical reasoning errors from those influenced by cultural adaptations. By flagging errors based on single incorrect run, we account for response variability, identifying unstable performance patterns that might be overlooked with stricter majority-based approach."
        },
        {
            "title": "10.1 Currency-Based Errors",
            "content": "One of the key errors we identify is the models inconsistency in handling mathematical reasoning across different currency units. For example, When solving problem with cost of 0.1 USD per cubic foot, almost all models correctly compute the total cost. However, when the same problem uses another cultures currency units such as, 0.1 HTG (Haitian Gourde) per cubic foot, the models incorrectly return an answer as if the cost is 1 HTG per cubic foot, leading to tenfold overestimation. 23 Figure A6: GPT-4o Reasoning This discrepancy suggests contextual bias in numerical interpretationwhile the model accurately processes decimals in USD, it defaults to whole-number assumptions for HTG. This aligns with real-world usage, as HTG is rarely used in decimal form due to inflation and economic practices, with transactions typically rounded to whole numbers. However, this exposes bias in numerical reasoning, where the model fails to apply consistent mathematical principles across currencies and instead relies on learned heuristics from their training data. Additionally, the models reasoning process differed based on currencywhen using USD, they explicitly recognize and multiply by 0.1, but with HTG, they misinterpret the decimal, either skipping or rounding the value incorrectly. This highlights deeper issue: models do not merely struggle with arithmetic but also with the contextual expectations of numerical formats in different cultures, leading to systematic reasoning errors."
        },
        {
            "title": "10.2 Errors in Cultural Entity Interpretation",
            "content": "Another key error pattern observed in our qualitative analysis is the models inconsistent handling of mathematical reasoning when question is culturally adapted. For example, in the original GSM8K dataset, the question states: Tom decides to give his wife an anniversary getaway. The models correctly identifies that two plane tickets are required and accurately compute the total trip cost. However, when the question is culturally adaptedsuch as Mary decides to give her tambu man (father-in-law) Christmas getaway (Solomon Islands context) or Khalid Sulehri decides to give his Jija (brother-in-law) an Eid-ul-Fitar getaway (Pakistani context)the models incorrectly assumes only single traveler, leading to errors in the total cost calculation. 24 Figure A7: GPT-4o Reasoning This pattern suggests mathematical reasoning error when interpreting different cultural entities. While the models correctly process common familial relationships (husband-wife), it struggles with non-Western family structures (tambu man, Jija). Even though the numerical reasoning should remain unchanged, the errors indicate implicit biases in how the models associates relationships with travel expectations, affecting their ability to generalize reasoning across cultures. We also identify contextual misinterpretation error that lead to systematic inaccuracies. The original question, in GSM8K, states: pet store currently has 5 dogs, 2 cats, and 10 birds. How many legs in total do the pets in the store have? All models tested produced the correct response when presented with this version. However, when the question is culturally adapted to, lets say Somalian culture, replacing dog, cat, and bird with maroodi, shabeel, and gorgor, the models consistently produce incorrect answers. Figure A8: Claud-3.5-sonnet Reasoning This finding highlights contextual misinterpretation error, where models fail to correctly associate culturally specific terms with their actual meanings. Rather than understanding the new entities within their cultural and linguistic context, the models defaulted to familiar patterns from their training data."
        }
    ],
    "affiliations": [
        "155mv Research Lab",
        "Griffith University",
        "Microsoft",
        "Millcrest Technology"
    ]
}