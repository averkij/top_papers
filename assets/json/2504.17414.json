{
    "paper_title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models",
    "authors": [
        "Min Wei",
        "Chaohui Yu",
        "Jingkai Zhou",
        "Fan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video try-on replaces clothing in videos with target garments. Existing methods struggle to generate high-quality and temporally consistent results when handling complex clothing patterns and diverse body poses. We present 3DV-TON, a novel diffusion-based framework for generating high-fidelity and temporally consistent video try-on results. Our approach employs generated animatable textured 3D meshes as explicit frame-level guidance, alleviating the issue of models over-focusing on appearance fidelity at the expanse of motion coherence. This is achieved by enabling direct reference to consistent garment texture movements throughout video sequences. The proposed method features an adaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe for initial 2D image try-on, followed by (2) reconstructing and animating a textured 3D mesh synchronized with original video poses. We further introduce a robust rectangular masking strategy that successfully mitigates artifact propagation caused by leaking clothing information during dynamic human and garment movements. To advance video try-on research, we introduce HR-VVT, a high-resolution benchmark dataset containing 130 videos with diverse clothing types and scenarios. Quantitative and qualitative results demonstrate our superior performance over existing methods. The project page is at this link https://2y7c3.github.io/3DV-TON/"
        },
        {
            "title": "Start",
            "content": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models Min Wei1,2 Chaohui Yu1,2 Jingkai Zhou1,2,3 Fan Wang1 1DAMO Academy, Alibaba Group 2Hupan Lab 3Zhejiang University, {weimin.wei, huakun.ych}@alibaba-inc.com 5 2 0 2 4 ] . [ 1 4 1 4 7 1 . 4 0 5 2 : r Figure 1. Try-on videos generated by 3DV-TON. Our method can handle various types of clothing and body poses, while accurately restoring clothing details and maintaining consistent texture motion."
        },
        {
            "title": "Abstract",
            "content": "Video try-on replaces clothing in videos with target garments. Existing methods struggle to generate high-quality and temporally consistent results when handling complex clothing patterns and diverse body poses. We present 3DVTON, novel diffusion-based framework for generating high-fidelity and temporally consistent video try-on results. Our approach employs generated animatable textured 3D meshes as explicit frame-level guidance, alleviating the issue of models over-focusing on appearance fidelity at the expanse of motion coherence. This is achieved by enabling direct reference to consistent garment texture movements throughout video sequences. The proposed method features an adaptive pipeline for generating dynamic 3D guidance: (1) selecting keyframe for initial 2D image try-on, followed by (2) reconstructing and animating textured 3D mesh synchronized with original video poses. We further inCorresponding author. troduce robust rectangular masking strategy that successfully mitigates artifact propagation caused by leaking clothing information during dynamic human and garment movements. To advance video try-on research, we introduce HRVVT, high-resolution benchmark dataset containing 130 videos with diverse clothing types and scenarios. Quantitative and qualitative results demonstrate our superior performance over existing methods. The project page is at this link https://2y7c3.github.io/3DV-TON/ 1. Introduction Video try-on aims to change the persons clothing in the given video to target garment, enabling customers to visualize themselves wearing clothing items without physical trials through enhanced immersion and interactivity. The process must preserve intricate garment details while maintaining consistent texture representation throughout the video sequence. Prior video try-on works [11, 30, 70] typically employ 1 ing methods [71, 72] employ 3D human priors, they exclusively utilize geometric structural cues without textured guidance. Our experiments demonstrate that geometriconly guidance (e.g., SMPL [41, 45]) often fails to sufficiently constrain models, resulting in appearance-biased optimization and motion artifacts. Crucially, our textured 3D guidance uniquely preserves garment identity throughout video sequences, addressing critical oversight in current video try-on works. As illustrated in Figure 2, our pipeline begins with selecting frame through pose estimation, processed using advanced diffusion-based image try-on methods [6, 7, 61]. This initial frame undergoes animatable textured 3D mesh reconstruction aligned with the source video motion to generate temporally consistent reference sequences. Unlike the previous warp module, our framework leverages singleimage 3D reconstruction [48, 59, 60, 67] to inherently establish spatiotemporal consistency, delivering robust appearance priors for the denoising UNet while reducing temporal attention dependencies. This strategy effectively bypasses complex warping operations through mesh animation, while benefiting from mature single-image reconstruction methods without task-specific retraining. We further propose dynamic rectangular masking strategy to prevent garment information leakage during human motion, which is primary failure source in video try-on. To counter excessive masking, we implements both clothing images and try-on images as references to provide garments and environment context, and design an effective guidance feature extraction and fusion diffusion-based architecture. Comprehensive experiments demonstrate that our 3D-aware framework achieves superior visual quality and consistency in complex dynamic scenarios compared to existing approaches. In summary, our main contributions are as follows: We propose 3DV-TON, novel diffusion-based video try-on method that employs textured 3D guidance to alleviate motion incoherence stemming from appearance bias. Our method effectively generates try-on videos maintaining consistent texture motion across varying body poses and camera viewpoints. We introduce 3D guidance pipeline capable of adaptively generating animatable textured 3D meshes, ensuring consistent texture guidance across both spatial and temporal domains. The framework seamlessly integrates with existing methodologies without necessitating additional training. We establish high-resolution video try-on benchmark enabling better evaluation of recent works, and demonstrate that our 3DV-TON outperforms existing video try-on methods in both quantitative and qualitative experiments. Figure 2. Textured 3D guidance. We construct the textured 3D guidance based on image try-on results, then animate the mesh after pasting the texture, providing consistent texture motion reference on the appearance level. flow-driven warping modules [5, 12, 16, 54, 58] for precise garment alignment on human figures, complemented by neural generators to synthesize the final appearance. However, these methods face inherent limitations from their reliance on warping operations: while effectively adapting garment geometry through shape deformation to match pose variations, they inherently compromise temporal coherence in generated sequences. This fundamental constraint hinders handling of substantial clothing deformations and complex occlusions, limiting practical application to simplified scenarios. Recent advancements [13, 62] harness pre-trained diffusion models [27, 49] to address limitations of conventional warping modules. These works [13, 62] implement dualUNet architecture: primary denoising UNet [50] alongside parallel reference UNet that directly extracts garment features, eliminating explicit warping. Hierarchical temporal attention layers [20] are integrated within the denoising net to model motion dynamics and mitigate inter-frame inconsistencies. Concurrently, Diffusion Transformer (DiT)- based frameworks [46] demonstrate enhanced performance in video try-on through superior generative scalability, as evidenced by works like [8, 69]. Nevertheless, empirical analysis in [4] reveals that pixel-reconstruction objectives in video diffusion models remain constrained in achieving robust temporal coherence. In this paper, we present 3DV-TON, diffusionbased framework for generating high-fidelity temporallyconsistent video try-ons. To tackle the limitation, models prioritize appearance fidelity over motion coherence, in prior literature where pixel-based reconstruction objectives inherently, we introduce explicit frame-level textured 3D guidance. Our method directly models 3D human meshes wearing target garments, ensuring spatiotemporal consistency across diverse poses and viewpoints through motionaligned mesh propagation, which providing consistent motion reference on the appearance level. While exist2 2. Related Works Image Virtual Try-on. Image virtual try-on aims to generate images of target person wearing given clothing. Many GAN-based methods [5, 12, 16, 24, 29, 54, 58, 65] typically first warp the clothing image onto the target persons body. Then, generator is used to blend the warped clothing with the human body to produce realistic results. These methods rely on the accuracy of the warping module. Due to undesired distortions and artifacts caused by the TPS [1]-based methods [21, 43], many subsequent methods [16, 22, 24, 29, 58] have focused on predicting dense flow to achieve better warping of clothing, and have made significant progress. However, explicit warping techniques still struggle with complex poses and occlusions. Recently, several works [6, 7, 18, 33, 44, 61] tend to employ powerful pre-trained diffusion models [27, 49] as an alternative to GANs [17] to generate more realistic try-on results. OOTDiffusion and IDM-VTON [6, 61] utilized dual-UNet [50] structure and integrated clothing features and person features through self-attention. CatVTON [7] proposed to merge dual-UNet architectures, simplifying the training parameters and the inference process. However, applying image-based try-on techniques frame by frame to videos can lead to temporal inconsistent results. Video Virtual Try-on. Compared to image try-on, video try-on needs to maintain temporal consistency between frames to generate realistic, high-quality results, which adds more challenges to the task. Previous works [11, 30, 70] typically employs flow-based warping module [5, 12, 16, 54, 58] for precise garment alignment on human bodys, and combine the warped clothing with the person in the video. In video try-on, warp-based methods also face challenges in handling complex textures and motion. Recent diffusion-based works [13, 25, 55, 62], build on the dual-UNet architecture, primary denoising UNet [50] alongside parallel reference UNet that directly extracts garment features to preserve the visual quality, and insert hierarchical temporal modules [20] to ensure temporal smoothness. ViViD [13] releaseed new dataset and improves the generation resolution from 256 to 512. Some works [55, 62] utilized private datasets with resolution of 512 and introduced techniques to emphasize the clothing. More recently, some works [8, 69] utilized the powerful diffusion transformer (DiT) framework and have made significant progress in the video try-on task. However, these methods struggle to maintain consistent temporal coherence between frames, tend to generate over-smoothed deformed clothing textures. Clothed 3D Human Reconstrction. Previous works [10, 15, 39, 68] typically requires modeling 3D human body model and clothing model, and then fit the clothing onto the human body model. Additionally, when animate the model, physical simulation is introduced to generate natural clothing movement. One line of research, e.g., DiffAvatar [39], proposed methods for body shape and garment assets recovery from 3D scan of clothed person, and utilized differentiable simulation for co-optimizing garment and human body. Such methods have very high requirements for the input data. On the other hand, several methods reconstructs clothed human from single image and simultaneously models the clothing along with the person for animation. ICON [59] used body-based normal estimation for implicit 3D reconstruction. ECON [60] significantly improved reconstruction robustness by integrating explicit shape-based approaches with normal priors. SIFU [67] proposed side-view conditioned implicit function to achieve more accurate reconstruction results. More recently, some works [48, 57] introduced large reconstruction models (LRMs) to enable feed-forward clothed human reconstructions. These works are capable of generating photorealistic, animatable human avatars in seconds, but they struggle to produce flexible clothing motions. 3. Method The overview pipeline of our 3DV-TON is illustrated in Figure 3. We first introduce the textured 3D guidance generation pipeline in Section 3.1. Then, the model architecture and training strategy are illustrated in Section 3.2. 3.1. Animatable Textured 3D Guidance SMPL&SMPLX. The Skinned Multi-Person Linear (SMPL) model [41] is 3D parametric human model It uses shape that defines the shape topology of body. parameters β R10 and pose parameters θ R243 to represent the 3D human body mesh (β, θ) as: Tp(β, θ) = + Bs(β) + Bp(θ), (β, θ) = (Tp(β, θ), J(β), θ, W), (1) where is the mean template shape, Bs(β), Bp(θ) are vectors of vertices representing offsets from the template. Tp(β, θ) is the non-rigid deformation from . () is the linear blend skinning (LBS) [53] function applied to rotate the vertices around the joint center J(β) with the smoothing defined by the blend weights W. The SMPL-X model [45] builds upon SMPL, adding features for hands and face, enhanceing facial expressions, finger movements, and detailed body poses. Clothed Human Reconstruction&Animation. Our human reconstruction method is based on ECON [60]. Given video, we choose frame according to estimated body pose adaptively, which performing image try-on [6, 7, 61] during inference, as the input of normal estimation network [32, 59, 60]. To guide the normal map prediction for clothed normal map (denoted as ˆN {F,B} where F, denote front/back view), and ensure robustness across poses, 3 Figure 3. The overview of 3DV-TON. Given video, we first use our 3D guidance pipeline to select frame adaptively, then reconstruct textured 3D guidance and animate it align with the original video, i.e. . We employ guidance feature extractor for the clothing image and the try-on images Ct, and perform feature fusion using the self-attentions in the denoising UNet. we use body normal maps {F,B} rendered from the estimated SMPL-X b(β, θ) as reconstruction conditions. Accurate alignment between body estimation and clothing silhouettes proves crucial for this process. However, existing human pose and shape (HPS) regressors [36, 37, 51, 52] fails to provide pixel-aligned SMPL-X fits. Unlike previous works [59, 60] that require precise body pose optimization, our method prioritizes clothing reconstruction accuracy over anatomical details. By eliminating SMPL-X pose θ optimization during parameter refinement, we reduce optimization steps and reconstruction time to 30s while maintaining performance. We additionally optimize camera scale to address systematic camera estimation errors in HPS methods. Our optimization process initializes with estimated SMPL-Xs shape β, translation parameters and camera scale s, focusing on minimizing silhouette and normal loss: LSMPL-X = LNd + LSd + λ min(d s, 0), (2) LNd = ˆN b(β, t, s), LSd = ˆS b(β, t, s), where LNd is normal map L1 loss, LSd is L1 loss between the silhouettes of the SMPL-X and the clothed human mask ˆS segmented from image I. We additionally introduce unidirectional regularization penalty to address frequent partial body observations in training data (see Supplementary Materials.), activated during loss computation when camera scale falls below the dataset-defined threshold d. Following SMPL-X refinement, we iteratively updating the normal map and SMPL-X parameters through refinement cycles. We reconstruct the front and back surface using depthaware silhouette-consistent bilateral normal integration (dBiNI) method introduced by [2, 60]. However, poses often result in self-occlusions, which cause large portions of the surfaces to be missing. In such cases, we use simple way to infill the missing surface using the estimated SMPL-X body that invisible to front or back cameras, and union the parts of surface by surface reconstruction methods [28, 31]. Since the reconstructed mesh is aligned with the image pixels, we can simply use interpolated pixel values as the mesh texture after calculating visibility. For the invisible body areas, we use normals as the texture. The reconstructed clothed human inherit the hierarchical skeleton and skinning weights from the underling SMPL4 body model, allowing to animate it using the estimated SMPL poses [51] from the original video. Specifically, for each vertices of the clothed human mesh, we use k-nearest neighbor (KNN) search to obtain set Kj composed of neighboring control points denoted as {pkk Kj} in canonical SMPL-X model. Then, the interpolation weights for control points pk can be computed as: wjk = (cid:80) ˆwjk kKj , ˆwjk ˆwjk = exp(d jk), (3) where djk is the distance between vertices and the neighboring vertices pk in SMPL-X. The overall 3D guidance generation pipeline is depicted in the upper part of Figure 3. 3.2. Network Architecture Controlled Diffusion Model. Stable Diffusion [27, 49] is the basis for our network that consists of varialtional autoencoder (VAE) [34] and denoising UNet [50]. Given an image x0 and control condition c, the VAE first encodes the image x0 into latent space: z0 = E(x0). The UNet learns to predict noise ϵθ or velocity vθ based on the control condition and the noisy latent zt : zt = αtz0 + σtϵ. The training loss of the UNet can be formulated as: LLDM = Ez,c,ϵ,t[vt vθ(zt, t, c)2 2], (4) where represent the diffusion timestep, ϵ U(0, I), vt = αtϵ σtz0 [40]. In inference, data samples can be generated from Gaussian noise zT (0, I) by the denoising process. Guidance Feature Extractor. Our method employs two reference conditions: clothing images Rb3HW and try-on images Ct Rb3HW encoded into latent space through VAE encoder as = E(C) and Ct = E(Ct). These latent representations (C, Ct Rb4hw are concatenated along the batch dimension to form composite reference features R2b4hw. We duplicate the denoising UNet as the Guidance Feature Extractor that capture the visual features of the clothing images and tryon images. Note that we remove text encoders and all cross attention layers cause our textured 3D guidance provided sufficiently explicit visual reference. Denoising Network. We employ UNet architecture from Stable Diffusion [49] without cross-attention layers, extended into pseudo-3D structure through temporal module [20] integration to enable realistic motion generation, serving as our base denoising network. Given batch of source videos Vs Rb3f HW , with corresponding clothing-agnostic videos Va Rb3f HW and mask videos Vm Rb1f HW . We extimate the SMPL sequences using HPS methods [51, 52]. Our adaptive 3D guidance pipeline then generates textured 3D guidance . The denoising input comprises concatenated features along 5 the noisy latent video zt, the lathe channel dimension: tent clothing-agnostic video E(Va), the resized mask video m, the SMPL geometic guidance E(M ) and the textured 3D guidance E(v). To accommodate this 17-channel input, we expand the UNets initial convolutional layer with zeroinitialized weights. and the try-on feature xi Our guidance feature extractor avoids feature fusion between clothing and try-on images. Instead, we implement texture-aware fusion through spatial attention mechanisms Rbcf hw entering the (Figure 3). For each latent xi i-th self-attention layer, we retrieve corresponding reference R2bchw features corresponding reference feature xi from the extractor. These features split into clothing feature xi ct, which we temporally align by replicating along the frame dimension to obtain Rbcf hw. As shown in Figure 3, the three ˆxi c, ˆxi ct types of features are concatenated along the spatial dimens Rbcf h3w, . Then feature fusions, denoted by ˆxi sion is performed by the attention layer of the denoising network to obtain the latent xi+1 , which incorporates both the fine clothing textures and the frame-consistent 3D features. Training Strategy. Inspired by [13, 47, 63], our model is trained on both image and video datasets by treating images as single-frame videos. During training, we randomly select type of dataset via random number U(0, 1), where U(, ) is the uniform distribution. If < τ , we use the sampled images for training, and set the gradients of the temporal attention as zero to freeze the temporal module. Otherwise, we sample data from the video dataset, and make the temporal attention trainable. Hence, our training objective can be formulated as: = Ez,ϵ,t[vt vθ(zt, t, C, Ct, V)], {zimg, z1:f vid}. (5) We incorporate control conditions through Classifier-Free Guidance (CFG) [26]. Specifically, we randomly omit the clothing image with probability of p1, the try-on image Ct with probability of p2, and the textured 3D guidance with probability of p3. Masking Strategy. Our pipeline begins with garment segmentation using either human parsing [38] or segmentation model [35] to generate clothing masks. We compute bounding boxes from these masks and employ human estimation model [19, 36, 37]to selectively critical anatomical regions (e.g. face and hands) while preserving body detail. This streamlined approach effectively prevents garment transfer failures caused by leaking clothing. Please refer to Section 4.5 for illustration. Method StableVITON [33] OOTDiffusion [61] IDM-VTON [6] StableVITON+AM [63] OOTDiffusion+AM [63] IDM-VTON+AM [63] ViViD [13] CatV2TON [8] 3DV-TON (Ours) 3DV-TON (Ours) SSIM LPIPS 0.1338 0.8019 0.1232 0.8087 0.1163 0.8227 0.1291 0.8207 0.1244 0.8154 0.1212 0.8252 0.1221 0.8029 0.0639 0.8727 0.0707 0.8681 0.0521 0.8992 Paired IDI3D 34.2446 29.5253 20.0812 19.9239 19.3173 18.2048 17.2924 13.5962 13.4062 10.9680 Unpaired IDRexN eXt 0.7735 3.9372 0.3674 0.7586 0.9382 0.4481 0.6209 0.2963 0.2741 0.2033 IDI3D 36.8985 35.3170 25.4972 22.0262 23.3938 22.5881 21.8032 19.5131 19.4714 18. IDRexN eXt 0.9064 5.7078 0.7167 0.8283 1.1485 0.5397 0.8212 0.5283 0.3664 0.3149 Table 1. Quantitative comparison on the ViViD dataset. indicates our method using the same mask with ViViD [13]. rately assess video try-on methods. Therefore, we have constructed high-resolution (720p) video try-on benchmark called HR-VVT that includes 130 videos with 50 upperbody clothing, 40 lower-body clothing, and 40 dresses, with variety of garments and motions in complex scenarios. Please refer to our Supplementary Materials for more dataset details. 4.2. Implementation Details Textured 3D guidance. During the 3D human reconstruction process, to achieve better body estimation, we use image-based HPS regressor [14, 37, 64] with SMPL-X and iteratively refined the SMPL-X parameters = 10 times for clothed human reconstruction based on ECON [60]. To ensure smooth animation of the 3D guidance, we employ video-based SMPL estimation approach [51, 52]. Due to the differences between image-based and video-based estimation methods, we use the body from the video-based estimation as the binding template to avoid texture distortion and animate it to render guidance video, which is aligned with the source video. Please refer to Supplementary Materials for more details. Training. We initialize our guidance feature extractor and denoising network using the weights from SD1.5 [49], and employ Animatediff [20] to initialize the temporal attention. Our model is trained in single-stage manner using 768 576 resolution, 32 frames (2 strides for videos) data. The model was trained using A800 GPUs for 40000 steps with learning rate of 1e-5. 4.3. Qualitative Results We conduct qualitative comparisons with the currently available method for video try-on that released the inference code, ViViD [13] and CatV2TON[8]. The clothing images and the person videos are from ViViD-S [8, 13] test set and our HR-VVT set. None of the images or videos have appeared in the training data. Comparisons on ViViD dataset. As shown in Figure 4 and Figure 5, other methods suffer from artifacts and generate garments limited by the patterns of the original clothing, while our method generates accurate garment shapes, offers better visual quality, and produces realistic clothing motion that adapts to the persons motions. Figure 6 shows that Figure 4. Qualitative comparison for dress try-on on the ViViD dataset. 4. Experiments 4.1. Datasets Training datasets. We use two image datasets, VITONHD[5] and DressCode [9], along with one video dataset, ViViD [13], to train our diffusion model. Due to the low resolution of the VVT [11] dataset, we opt not to use it for training. Specifically, the VITON-HD dataset contains 13,678 images of upper-body clothing with corresponding model images. The DressCode dataset includes 15,363 images of upper-body clothing, 8,951 images of lower-body clothing, and 2,947 images of dresses, along with images of models wearing these garments. The ViViD dataset consists of 9,700 videos featuring models along with corresponding clothing images. This dataset contains 4,823 video-image pairs for upper-body clothing, 2,133 for lower-body clothing, and 2,744 for dresses, with total of 1,213,694 frames. All image and videos are resized to 768 576 for training. For video data, we randomly select frame to construct the try-on image condition using the image try-on method [6, 7, 61]. For image data, we set all try-on conditions to be empty. HR-VVT benchmark. Owing to the limitations of the ViViD dataset, which contains limited scenarios, and VVT dataset, which only includes upper-body clothing and exhibits relatively uniform body poses, coupled with low resolution of only 256 192, it is challenging to accuFigure 7. Qualitative comparison for dress try-on on HR-VVT. Figure 5. Qualitative comparison for upper garment try-on on the ViViD dataset. Figure 8. Comparison on upper garment try-on on HR-VVT. contrast, Our 3DV-TON generates accurate clothing with good temporal consistency. Comparisons on HR-VVT benchmark. Our HR-VVT benchmark includes more diverse set of environments, clothing. As shown in Figure 7, ViViD [13] struggles with perspective changes during subject movement, and CatV2TON fails to preserve garment consistency. In contrast, our method leverages explicit textured 3D guidance to maintain visual coherence across viewpoints and motion sequences. Figure 8 shows that our approachs superiority in outdoor scenarios, where competing methods exhibit artifacts and unrealistic texturing. Figure 9 demonstrates how our robust 3D guidance pipeline ensures reliable perFigure 6. Comparison for lower garment try-on on ViViD. ViViD [13] fails at this case, while CatV2TON generates inconrrect gaments along with blurriness and artifacts. In 7 Figure 9. Comparison on lower garment try-on on HR-VVT. Method ViViD [13] CatV2TON [8] 3DV-TON (Ours) SSIM LPIPS 0.8889 0.0876 0.1144 0.8670 0.0857 0.8801 Paired IDI3D 10.2367 12.1280 10. Unpaired IDRexN eXt 0.1785 0.1798 0.1420 IDI3D 16.4684 16.8880 14.5499 IDRexN eXt 0.6807 0.3454 0.4217 Table 2. Quantitative comparison on HR-VVT benchmark. Best results are highlighted in bold, the second are underlined. Datasets Method Fidelity (%) Consistency (%) Overall Quality (%) ViViD HR-VVT ViViD [13] CatV2TON [8] 3DV-TON (Ours) ViViD [13] CatV2TON [8] 3DV-TON (Ours) 24.55 12.09 63.36 14.02 5.97 80.01 20.25 10.92 68.83 11.82 3.36 84. 20.39 10.53 69.08 11.77 2.70 85.53 Table 3. User preference rate on the HR-VVT benchmark and ViViD dataset. formance even with partial character visibility. Please refer to our Project Page for more qualitative comparisons and video results. 4.4. Quantitative Results Comparisons on ViViD dataset. We report quantitative results with SSIM [56], LPIPS [66] to evaluate the image visual quality in the paired setting, and use Video Frechet Inception Distance(VFID) [11, 42] to measure the generation quality and temporal consistency in the both paired and unpaired setting, following [8, 11, 13, 30]. VFID extracts features of video clips for computation using pretrained video backbone I3D [3] and 3D-ResNeXt101 [23]. Our method employs rectangular mask strategy that enlarges the area to be generated, which creates an unfair comparison. Nonetheless, as reported in Table 1, our method still achieves comparable results in SSIM and LPIPS metrics, while surpassing existing methods in the VFID metric. Figure 10. Ablations for the mask strategy. Figure 11. Ablations for the SMPL guidance. When we use the mask from ViViD [13], our method delivers better results across all metrics. Comparisons on HR-VVT benchmark. We compare the current state-of-the-art and code released video try-on method, ViViD [13] and CatV2TON on our benchmark. As shown in Table 2, although we use the larger mask, our method outperforms other works. This improvement can be attributed to the consistent texture features brought by our textured 3D guidance. We also demonstrated advantages in LPIPS, which proves that our method is capable of generating try-on results with better visual quality. User Study. Considering that the current quantitative metrics are difficult to accurately evaluate the quality of the model in terms of human preference in the unpaired setting without ground truth. We conduct user study that includes 130 video results and involved 20 annotators to provide comprehensive comparison in terms of visual quality and motion consistency. Table 3 shows that our 3DV-TON achieves better motion coherence and effectively restores clothing details (i.e. Fidelity), resulting in superior visual quality. 4.5. Ablation Study We conducted ablation studies to verify the effectiveness of our textured 3D guidance. Speed Analysis. After optimizing the SMPL fitting process, our method is capable of completing the reconstruction in 30s, and generating 32-frame video with dif8 persons arms and shoulders are accurately generated after using SMPL. Textured 3D Guidance. Recent studies [4] demonstrate that conventional pixel reconstruction objective biases diffusion models toward appearance fidelity while compromising geometric accuracy, leading to motion artifacts. As demonstrated in Figure 12, while SMPL-based geometric guidance improves body structure estimation in masked regions, it exhibits persistent limb ambiguity during legcrossing scenarios. Our textured 3D guidance resolves this limitation by supplementing explicit appearance constraints, effectively balancing visual quality and motion coherence. Our texture 3D guidance ensures accurate clothing texture preservation across arbitrary poses and viewpoints. As shown in Figure 13 (a), our method faithfully reconstructs the boss logo while maintaining anatomically consistent body proportions during lateral rotation. Figure 13 (b) demonstrates viewpoint-consistent rendering of the lee text across dynamic poses. In Table 4, we present quantitative ablation experiments, where geometric features and textured 3D guidance significantly improved the SSIM, LPIPS, and VFID metrics. 5. Conclusion In this paper, we propose 3DV-TON, novel diffusionbased framework guided by geometric and textured 3D guidance. By leveraging SMPL as parametric body geometry and employing single-image reconstructed 3D humans as animatable textured 3D guidance to provide framespecific appearance conditions, 3DV-TON alleviates the critical limitation of inconsistent results caused by existing methods over-focus on appearance fidelity. The framework learns geometrically plausible human body across diverse poses and viewpoints, while maintaining temporally consistent motion of clothing textures. Quantitative and qualitative evaluations on existing datasets and our newly introduced HR-VVT demonstrate state-of-the-art performance in the video try-on task."
        },
        {
            "title": "References",
            "content": "[1] Fred Bookstein and WDK Green. thin-plate spline and the decomposition of deformations. Mathematical Methods in Medical Imaging, 2(14-28):3, 1993. [2] Xu Cao, Hiroaki Santo, Boxin Shi, Fumio Okura, and Yasuyuki Matsushita. Bilateral normal integration. In European Conference on Computer Vision, pages 552567. Springer, 2022. [3] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 62996308, 2017. [4] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Figure 12. Ablations for the textured 3D guidance. Textured 3D guidance helps to improve the motion conherence. Figure 13. Ablations for our textured 3D guidance. Textured 3D guidance helps to improve the clothing consistency. SMPL Tex. 3D SSIM LPIPS 0.078 0.059 0.048 0.858 0.880 0.909 IDI3D 5.236 4.087 2.381 IDRexN eXt 1.0257 0.5854 0. Table 4. Quantitative ablations for the 3D guidance. fusion after removing cross attention takes 35s under 768 576 resolution. Due to our use of single-image reconstruction, the diffusion model accounts for the majority of the inference time for longer videos. Mask Strategy. Our robust rectangular mask strategy can effectively addresses the issue of try-on failures caused by the leakage of original clothing information in videos. Figure 10 demonstrates that our method can generate try-on results that align more closely with the target garment patterns. SMPL Guidance. As shown in Figure 11, the introduction of SMPL guidance helps in generating more accurate human bodies and properly fit clothing on the body. The 9 Videojam: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. [5] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. try-on via Viton-hd: High-resolution virtual In Proceedings of the misalignment-aware normalization. IEEE/CVF conference on computer vision and pattern recognition, pages 1413114140, 2021. [6] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for virtual try-on. arXiv preprint arXiv:2403.05139, 2024. [7] Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing Zhao, and Xiaodan Liang. Catvton: Concatenation is all you need for virtual tryon with diffusion models. arXiv preprint arXiv:2407.15886, 2024. [8] Zheng Chong, Wenqing Zhang, Shiyue Zhang, Jun Zheng, Xiao Dong, Haoxiang Li, Yiling Wu, Dongmei Jiang, and Xiaodan Liang. Catv2ton: Taming diffusion transformers for vision-based virtual try-on with temporal concatenation. arXiv preprint arXiv:2501.11325, 2025. [9] Morelli Davide, Fincato Matteo, Cornia Marcella, Landi Federico, Cesari Fabio, and Cucchiara Rita. Dress code: In ProceedHigh-resolution multi-category virtual try-on. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. [10] Luca De Luigi, Ren Li, Benoit Guillard, Mathieu Salzmann, and Pascal Fua. Drapenet: Garment generation and selfsupervised draping. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14511460, 2023. [11] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bowen Wu, Bing-Cheng Chen, and Jian Yin. Fw-gan: Flow-navigated In Proceedings of warping gan for video virtual try-on. the IEEE/CVF international conference on computer vision, pages 11611170, 2019. [12] Haoye Dong, Xiaodan Liang, Yixuan Zhang, Xujie Zhang, Xiaohui Shen, Zhenyu Xie, Bowen Wu, and Jian Yin. Fashion editing with adversarial parsing learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81208128, 2020. [13] Zixun Fang, Wei Zhai, Aimin Su, Hongliang Song, Kai Zhu, Mao Wang, Yu Chen, Zhiheng Liu, Yang Cao, and ZhengJun Zha. Vivid: Video virtual try-on using diffusion models. arXiv preprint arXiv:2405.11794, 2024. [14] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael Black. Collaborative regression of expressive bodies using moderation. In 2021 International Conference on 3D Vision (3DV), pages 792804. IEEE, 2021. [15] Daiheng Gao, Xu Chen, Xindi Zhang, Qi Wang, Ke Sun, Bang Zhang, Liefeng Bo, and Qixing Huang. Cloth2tex: customized cloth texture generation pipeline for 3d virtual try-on. arXiv preprint arXiv:2308.04288, 2023. [16] Yuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo. Parser-free virtual try-on via distilling In Proceedings of the IEEE/CVF conappearance flows. 10 ference on computer vision and pattern recognition, pages 84858493, 2021. [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [18] Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, and Liqing Zhang. Taming the power of diffusion models for high-quality virtual try-on with appearance flow. In Proceedings of the 31st ACM International Conference on Multimedia, 2023. [19] Rıza Alp Guler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 72977306, 2018. [20] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toInternaimage diffusion models without specific tuning. tional Conference on Learning Representations, 2024. [21] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry Davis. Viton: An image-based virtual try-on network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 75437552, 2018. [22] Xintong Han, Xiaojun Hu, Weilin Huang, and Matthew Scott. Clothflow: flow-based model for clothed person generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1047110480, 2019. [23] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 65466555, 2018. [24] Sen He, Yi-Zhe Song, and Tao Xiang. Style-based global In Proceedings of the appearance flow for virtual try-on. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 34703479, 2022. [25] Zijian He, Peixin Chen, Guangrun Wang, Guanbin Li, Philip HS Torr, and Liang Lin. Wildvidfit: Video virtual tryon in the wild via image-based controlled diffusion models. In European Conference on Computer Vision, pages 123 139. Springer, 2024. [26] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [28] Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja Fidler, and Francis Williams. Neural kernel surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4369 4379, 2023. [29] Zaiyu Huang, Hanhui Li, Zhenyu Xie, Michael Towards hardtry-on via 3d-aware global correspondence Advances in Neural Information Processing Kampffmeyer, Xiaodan Liang, et al. pose virtual learning. Systems, 35:3273632748, 2022. [30] Jianbin Jiang, Tan Wang, He Yan, and Junhui Liu. Clothformer: Taming video virtual try-on in all module. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1079910808, 2022. [31] Michael Kazhdan, Matthew Bolitho, and Hugues Hoppe. Poisson surface reconstruction. In Proceedings of the fourth Eurographics symposium on Geometry processing, 2006. [32] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision modIn European Conference on Computer Vision, pages els. 206228. Springer, 2025. [33] Jeongho Kim, Guojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81768185, 2024. [34] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [36] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, and Cewu Lu. Hybrik: hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 33833393, 2021. [37] Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin Yang, and Cewu Lu. Hybrik-x: Hybrid analytical-neural inverse kinematics for whole-body mesh recovery. arXiv preprint arXiv:2304.05690, 2023. [38] Peike Li, Yunqiu Xu, Yunchao Wei, and Yi Yang. Selfcorrection for human parsing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(6):32603271, 2020. [39] Yifei Li, Hsiao-yu Chen, Egor Larionov, Nikolaos Sarafianos, Wojciech Matusik, and Tuur Stuyck. Diffavatar: Simulation-ready garment optimization with differentiable In Proceedings of the IEEE/CVF Conference simulation. on Computer Vision and Pattern Recognition, pages 4368 4378, 2024. [40] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 54045411, 2024. [41] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. [42] Heusel Martin, Ramsauer Hubert, Unterthiner Thomas, Nessler Bernhard, and Hochreiter Sepp. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30:66266637, 2017. virtual try-on. computer vision, 2020. In Proceedings of the Asian conference on [44] Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, and Rita Cucchiara. Ladi-vton: Latent diffusion textual-inversion enhanced virtual try-on. In Proceedings of the 31st ACM International Conference on Multimedia, pages 85808589, 2023. [45] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. [46] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [47] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [48] Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, et al. Lhm: Large animatable human reconstruction model from single image in seconds. arXiv preprint arXiv:2503.10625, 2025. [49] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [51] Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia Conference Proceedings, 2024. [52] Soyong Shin, Juyong Kim, Eni Halilaj, and Michael Black. Wham: Reconstructing world-grounded humans with accurate 3d motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2070 2080, 2024. [53] Robert Sumner, Johannes Schmid, and Mark Pauly. EmIn ACM sigbedded deformation for shape manipulation. graph 2007 papers, pages 80es. 2007. [54] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang. Toward characteristicpreserving image-based virtual try-on network. In Proceedings of the European conference on computer vision (ECCV), pages 589604, 2018. [43] Matiur Rahman Minar and Heejune Ahn. Cloth-vton: Clothing three-dimensional reconstruction for hybrid image-based [55] Yuanbin Wang, Weilun Dai, Long Chan, Huanyu Zhou, Aixi Zhang, and Si Liu. Gpd-vvto: Preserving garment details 11 In Proceedings of the able clothed human reconstruction. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 99369947, 2024. [68] Fuwei Zhao, Zhenyu Xie, Michael Kampffmeyer, Haoye Dong, Songfang Han, Tianxiang Zheng, Tao Zhang, and Xiaodan Liang. M3d-vton: monocular-to-3d virtual tryon network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1323913249, 2021. [69] Jun Zheng, Fuwei Zhao, Youjiang Xu, Xin Dong, and Xiaodan Liang. Viton-dit: Learning in-the-wild video try-on from human dance videos via diffusion transformers. arXiv preprint arXiv:2405.18326, 2024. [70] Xiaojing Zhong, Zhonghua Wu, Taizhe Tan, Guosheng Lin, and Qingyao Wu. Mv-ton: Memory-based video virtual tryon network. In Proceedings of the 29th ACM International Conference on Multimedia, pages 908916, 2021. [71] Jingkai Zhou, Benzhi Wang, Weihua Chen, Jingqi Bai, Dongyang Li, Aixi Zhang, Hao Xu, Mingyang Yang, and Fan Wang. Realisdance: Equip controllable character animation with realistic hands. arXiv preprint arXiv:2409.06202, 2024. [72] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pages 145162. Springer, 2024. In Proceedings of the 32nd ACM in video virtual try-on. International Conference on Multimedia, pages 71337142, 2024. [56] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [57] Zhenzhen Weng, Jingyuan Liu, Hao Tan, Zhan Xu, Yang Zhou, Serena Yeung-Levy, and Jimei Yang. Template-free single-view 3d human digitalization with diffusion-guided lrm. arXiv preprint arXiv:2401.12175, 2024. [58] Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, and Xiaodan Liang. Gpvton: Towards general purpose virtual try-on via collaboraIn Proceedings of tive local-flow global-parsing learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2355023559, 2023. [59] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. ICON: Implicit Clothed humans Obtained from Normals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1329613306, 2022. [60] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. ECON: Explicit Clothed humans In Proceedings of the Optimized via Normal integration. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [61] Yuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen. Ootdiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on. arXiv preprint arXiv:2403.01779, 2024. [62] Zhengze Xu, Mengting Chen, Zhao Wang, Linyu Xing, Zhonghua Zhai, Nong Sang, Jinsong Lan, Shuai Xiao, and Changxin Gao. Tunnel try-on: Excavating spatial-temporal tunnels for high-quality virtual try-on in videos. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 31993208, 2024. [63] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human imIn Proceedings of age animation using diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. [64] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: Towards well-aligned full-body model regression from monocular images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):1228712303, 2023. [65] Jinsong Zhang, Kun Li, Yu-Kun Lai, and Jingyu Yang. Pise: Person image synthesis and editing with decoupled gan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79827990, 2021. [66] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [67] Zechuan Zhang, Zongxin Yang, and Yi Yang. Sifu: Side-view conditioned implicit function for real-world us12 3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "A. HR-VVT benchmark. Owing to the limitations of the exist datasets, which exhibits relatively simple scenarios, it is challenging to accurately assess video try-on methods. Therefore, we have constructed high-resolution (720p) video try-on benchmark called HR-VVT that includes 130 videos with 50 upperbody clothing, 40 lower-body clothing, and 40 dresses, with variety of garments and motions in complex scenarios. Figure 14 show some examples in our benchmark. Our HR-VVT benchmark was sourced from e-commerce platforms for research purposes and will remain strictly reserved for academic use. Our framework contains no personal identity information, with facial regions excluded from inpainting operations to ensure privacy preservation during the training process. B. Disccusion Limitation. Although we have significantly reduced the reconstruction time of clothed 3D humans by improving the optimization objectives of SMPL refinement and keeping it within an acceptable inference time, this is still insufficient in scenarios with higher speed requirements. Recently, works [48] on reconstructing animatable clothed 3D humans using single feed-forward approach has greatly accelerated inference times and achieved remarkable improvements in visual quality. We believe that updating our 3D guidance pipeline to single feed-forward paradigm can accelerate the reconstruction process, further advancing the application of textured 3D human guidance in more scenarios. Potential societal impact. This paper delves into the realm of video try-on generation. Because of the powerful generative capacity, these models pose risks such as the potential for misinformation and the creation of fake videos. We sincerely remind users to pay attention to generated content. Besides, it is crucial to prioritize privacy and consent, as generative models frequently rely on vast datasets that may include sensitive information. Users must remain vigilant about these considerations to uphold ethical standards in their applications. Note that our method only focus on technical aspect. Both videos and model weights used in this paper will be open-released. C. Animatable Textured 3D Guidance Refine SMPL-X. Since our clothed human reconstruction method is based on the SMPL-X [41, 45] model, it is important to accurately align the estimated body and clothing silhouette. In practice, human pose and shape (HPS) regressors [36, 37, 51] can not give pixel-aligned SMPL-X fits. We refine the SMPL-X parameters by minimizing LSMPL-X in Section 3.1 of our paper. Unlike the optimization of shape β, pose θ, and translation of SMPL-X in ICON [59], Since we primarily focus on the clothing area and do not have high accuracy requirements for the body pose details, we adjust the optimization target without optimizing the SMPL pose θ. This allows us to significantly reduce the number of optimization steps to reduce the reconstruction time (30s). And we optimize the shape β, camera scale s, and translation to mitigate the anomalies in loss caused by incomplete human body parts and inaccurate camera estimation in real data, which may lead to errors in the refined SMPL-X. We additionally introduce unidirectional regularization penalty to prevent the incorrect decrease in loss caused by abnormal reduction in camera scale caused by partial bodies present in the training data. As shown in Figure 15, if we optimize the pose of SMPL-X in Panel (a), the pose refinement may be abnormal due to the incomplete human body parts, leading to reconstruction failure. Thanks to the powerful generative capabilities of the diffusion model [27, 49], which do not require high precision for the pose accuracy in 3D guidance, we choose to freeze the pose parameters θ, as this approach is sufficient to yield usable results for the robust 3D guidance reconstruction. In Panel (b) and (c), current HPS regressors often yield inaccurate camera scale estimations. To address this issue, we simultaneously optimize the camera scale applied to SMPL-X. However, for incomplete human bodies, the camera scale tends to be abnormally reduced. We use unidirectional regularization penalty to constrain the optimization direction of the s. Figure 19 demonstrates that our 3D pipeline is applicable to most scenarios. Animation. To ensure smooth animation of the 3D guidance, we employ video-based SMPL estimation approach [51, 52]. However, during the reconstruction phase, our input is an image, and to achieve more accurate reconstruction, we employ an image-based SMPL-X estimation method. Since there are differences in shape and other parameters between the body estimations from video and image-based methods, directly using the body sequences estimated from video for animating may result in texture distortion and deformation, as shown in Figure 16 (left). To address this issue, we utilize the video-based estimated SMPL for rigging the reconstructed clothed human before animation. Figure 16 (right) demonstrates that our method effectively avoids texture distortion and deformation. Figure 14. Illustration of the HR-VVT benckmark. D. Network Architecture smoothing inconsistent content between frames. Temporal attention. Since our textured 3D guidance provides sufficiently explicit frame-level references, we find that our 3DV-TON can maintain texture consistency even when temporal attention is freezed (initialized with AnimateDiff [20]), albeit with some minor jitter and mask aritfacts. Texture errors occur only when the 3D guidance hard to provide texture references, as shown in the first two columns of Figure 17. This demonstrates that our 3DVTON, using textured 3D guidance, is capable of generating consistent texture motion rather than overly focusing on 14 E. More Results As shown in Figure 18, our method can handle various shape, materials, and complex textures of clothing, while generating consistent texture motions. For more qualitative comparisons and video try-on results, please refer to the project page. before (a) after before after (b) Figure 15. Effectiveness of our SMPL-X Refinement. before (c) after Figure 16. Effectiveness of our textured 3D animation method. Figure 17. Effectiveness of freezing temporal attention of our 3DV-TON. 15 Figure 18. More results generated by 3DV-TON. 16 Figure 19. Animated 3D guidance."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "Zhejiang University"
    ]
}