{
    "paper_title": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens",
    "authors": [
        "Shiqi Liu",
        "Zeyu He",
        "Guojian Zhan",
        "Letian Tao",
        "Zhilong Zheng",
        "Jiang Wu",
        "Yinuo Wang",
        "Yang Guan",
        "Kehua Sheng",
        "Bo Zhang",
        "Keqiang Li",
        "Jingliang Duan",
        "Shengbo Eben Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\\%, which we term \\emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\\% over GRPO, 20-Entropy and JustRL."
        },
        {
            "title": "Start",
            "content": "STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens Shiqi Liu1,2, , Zeyu He1,2, , Guojian Zhan1,2, Letian Tao1,2, Zhilong Zheng1,2, Jiang Wu1, Yinuo Wang1,2, Yang Guan1, Kehua Sheng2, Bo Zhang2, Keqiang Li1, Jingliang Duan1,2,(cid:66), Shengbo Eben Li1,(cid:66) 1School of Vehicle and Mobility & College of AI, Tsinghua University 2DiDi Voyager Labs, DiDi Autonomous Driving Equal contribution, (cid:66)Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by tiny fraction of tokens, approximately 0.01%, which we term spurious tokens. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13% over GRPO, 20-Entropy and JustRL. Date: February 17, 2026 Correspondence: lishbo@tsinghua.edu.cn; duanjl15@163.com. 6 2 0 2 7 1 ] . [ 1 0 2 6 5 1 . 2 0 6 2 : r Figure 1 (a) Conceptual analogy: We argue that spurious tokens, which are rare and uninformative tokens within otherwise correct responses that receive disproportionately large gradient updates, can harm training stability, analogous to dissonant vocalist disrupting the harmony of performance. (b) By masking this negligible fraction (near 0.01%) of spurious tokens during the RL process of Qwen3-8B-Base, STAPO approaches the Pareto frontier of performance (AIME24 Acc) and entropy stability, compared to GRPO [1], 20-Entropy [2], and JustRL [3]."
        },
        {
            "title": "Introduction",
            "content": "Recent large language models (LLMs), including OpenAI-o1 [4], DeepSeek-R1 [5], and Qwen3 [6], have demonstrated remarkable capabilities in complex reasoning domains such as mathematics and programming. Central to this success is Reinforcement Learning (RL), which optimizes directly for outcome-level correctness and has been empirically linked to the emergence of advanced, long-horizon reasoning behaviors [7]. However, training stability remains critical bottleneck in RL. Models often suffer from catastrophic degradation, abruptly collapsing from coherent reasoning chains into shallow, repetitive, or nonsensical patterns [8]. Prior work has approached this instability from two perspectives: entropy regulation and gradient modulation. On the entropy front, methods such as selective regularization [2], sample augmentation [9], and clipping modifications [1012] attempt to arrest entropy collapse. However, these interventions often precipitate the opposite mode, inducing oscillatory or excessive entropy that degrades reasoning coherence [13]. Turning to the gradient modulation perspective, the disproportionate impact of rare tokens on parameter updates has motivated efforts to suppress unstable gradients via advantage reweighting [14], probability shaping [15], or spark-preserving regularization [16]. Nevertheless, existing approaches lack fine-grained diagnosis of token instability. By overlooking the interplay between token probability and policy entropy, they fail to distinguish varying degrees of model confidence within low-probability regions, conflating valuable exploration with detrimental noise and leaving the structural sources of instability. In this work, we trace RL instability to token-level optimization mechanism. Through systematic analysis of token updates along the dimensions of entropy, probability, and advantage sign, we uncover pathological update regime driven by small subset of spurious tokens. As illustrated in Figure 1, these tokens are rare within otherwise correct responses, yet they provide little to no contribution to the reasoning process and may even introduce incorrect or misleading signals. By masking negligible fraction (about 0.01%) of such spurious tokens during training, we substantially stabilize policy entropy and consistently improve performance across benchmarks. Overall, our contributions are summarized as follows: We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. We further establish unified framework for analyzing training stability in the joint space of policy entropy, token probability, and advantage sign. We identify pathological class of low-probability, low-entropy tokens with positive advantage, termed spurious tokens, whose amplified gradients destabilize optimization. To address this, we propose Spurious-Token-Aware Policy Optimization (STAPO), which masks these tokens (approximately 0.01%) and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks and three model scales, STAPO substantially stabilizes policy entropy and consistently improves reasoning performance under diverse evaluation settings, demonstrating its robustness and effectiveness in stabilizing RL training."
        },
        {
            "title": "2.1 Problem Formulation\nWe focus on fine-tuning LLMs using RL for reasoning tasks. Let D denote a distribution over input prompts.\nGiven an input prompt x ∼ D, the LLM parameterized by θ acts as a stochastic policy πθ that autoregressively\ngenerates an output sequence y = (y1, . . . , yT ). Specifically, at each step t, a token yt ∈ V is sampled according\nto πθ(yt | x, y<t). Supervision is provided via a sparse, sequence-level verifiable reward R(x, y) ∈ {−1, 1},\nwhich evaluates the correctness of the completed sequence y using an external verifier (e.g., a code compiler\nor a math answer checker). The optimization objective is to maximize the expected reward:",
            "content": "J (θ) = ExD, yπθ(x) [R(x, y)] ."
        },
        {
            "title": "2.2 Group Relative Policy Optimization (GRPO)",
            "content": "We briefly review Group Relative Policy Optimization (GRPO) [17], which estimates advantages without an explicit value function. For each prompt x, GRPO samples group of output sequences {y1, . . . , yG} from 2 reference policy πθold group: . The optimization objective is defined as the average clipped surrogate loss over the JGRPO(θ) = xD,{yi}G i=1πθold (x)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 yi yi (cid:88) t=1 min(cid:0)ρi,t(θ) ˆAi, clip(ρi,t(θ), 1 ϵ, 1 + ϵ) ˆAi (cid:1) , where ρi,t(θ) is the importance sampling ratio. Since rewards are assigned at the sequence level, all tokens within the same sequence share common training signal ˆAi, which is computed by normalizing the sequence reward across samples in the same group."
        },
        {
            "title": "2.3 Clip-Higher and Token Normalization",
            "content": "Building upon the GRPO objective, DAPO [10] proposes four additional training tricks, among which token-level normalization and an asymmetric clip-higher mechanism are shown to be particularly effective for stabilizing RL training [11, 12, 18]. JustRL [3] adopts these two components and achieves strong empirical performance. The corresponding objective is written as JDAPO(θ) = xD,{yi}G i=1πθold (x) 1 i=1 yi (cid:80)G (cid:88) yi (cid:88) i=1 t= (cid:16) ρi,t(θ) ˆAi, clip(ρi,t(θ), 1 ϵlow, 1 + ϵhigh) ˆAi (cid:17) min , where ϵlow and ϵhigh are the asymmetric clipping parameters introduced by the clip-higher mechanism. Due to its simplicity and strong empirical performance, we adopt this configuration as our baseline objective."
        },
        {
            "title": "3.1 Token-Level Optimization Mechanisms in RL Training",
            "content": "Understanding how RL-style optimization updates reasoning models is central to analyzing training stability. Prior work has examined entropy dynamics [12, 19] or gradient behavior separately [14, 16]. In contrast, we provide unified analysis that jointly characterizes gradient scaling and entropy evolution under group-style updates. We first recall fundamental gradient property from [14] that characterizes how token probability modulates gradient magnitude. At decoding step of sequence yi, the LLM produces logit vector ai,t RN over the vocabulary V, which induces the policy distribution πθ( x, yi,<t) via softmax. We analyze the per-token gradient associated with yi,t as it propagates to intermediate layers. Theorem 3.1. (Policy Gradient Norm Bounds). Consider the optimization objective at step for sample with target token yi,t. The squared ℓ2-norm of the gradient aJ w.r.t. the logits is directly bounded by the entropy H(πθ) and the target probability πθ(yi,t x, yi,<t) as follows: wi,t2 (cid:16) aJ (yi,t)2 wi,t2 (cid:0)2 2πθ(yi,t x, yi,<t) CV H(πθ)2(cid:1) , 1 2πθ(yi,t x, yi,<t) + eH(πθ)(cid:17) where CV = V(ln V)2 and the weight wi,t is defined as: wi,t = (cid:40) 0, if ( ˆAi > 0 ri,t > 1 + ϵ) ( ˆAi < 0 ri,t < 1 ϵ), πθ(yi,tx,yi,<t) πθold (yi,tx,yi,<t) ˆAi, otherwise. Proof. See Appendix for the detailed derivation. Theorem 3.1 indicates that the gradient lower bound scales inversely with the token probability πθ(yi,t x, yi,<t) and the policy entropy H(πθ). Therefore, token characterized by both low probability and low entropy will lead to larger gradient norm, which is verified by Figure 2b. Beyond gradient magnitude, the behavior of LLMs is closely linked to the evolution of policy entropy, which characterizes uncertainty over action distributions. Recent analyses establish direct connection between policy updates and entropy dynamics. 3 Table 1 Token Update Phase Diagram. Left: token properties. Right: influence on optimization. Green indicates beneficial updates, and red indicates detrimental updates. Token Properties Influence on Optimization Token Prob. Advantage Entropy Gradient Norm Entropy Change [19] Learning Potential [2] πt τp πt τp πt < τp πt < τp πt τp πt τp πt < τp πt < τp ˆA < 0 ˆA > 0 ˆA < 0 ˆA > 0 ˆA < 0 ˆA > 0 ˆA < 0 ˆA > 0 τh τh τh τh < τh < τh < τh < τh Low Low High High Low Low High High High High High High Low Low Low Low Lemma 3.2 (Entropy Update Mechanism [19]). Consider the language policy πθ() updated via natural policy gradient step, the entropy H(πθ) between two consecutive policy iterates satisfies H(πθk+1 ( x, yi,<t)) H(πθk ( x, yi,<t)) η Covyi,tπθk (x,yi,<t) (cid:16) log πθk ( x, yi,<t), ˆAi (cid:17) . The change in policy entropy can be approximated by the interaction between token probability and advantage, as characterized by Lemma 3.2. Several prior works [12, 13, 20] adopt this perspective to explain entropy dynamics during RL training. However, these approaches mainly regulate entropy through sample selection or reweighting, without fully analyzing the underlying causes of token-level instability. Complementary to this dynamical view, prior studies have observed that learning efficacy is not uniform across the state space, but is concentrated in regions of high policy uncertainty. Lemma 3.3 (Entropy-Conditioned Learning Potential [2]). In RL, low-entropy tokens H(πθk+1 ( x, yi,<t)) < τh correspond to regions where the policy is already confident and well-optimized, so further updates yield limited learning benefit. In contrast, high-entropy tokens H(πθk+1 ( x, yi,<t)) τh mark decision-uncertain regions where multiple plausible continuations exist, and thus constitute the primary loci where policy learning remains effective. Proposition 3.1 and Lemmas 3.23.3 jointly characterize gradient norm, entropy change, and learning potential. Based on these results, we summarize the optimization impact of token-level updates using the following criteria. Criteria 1 (Optimization Impact Criteria). We evaluate the impact of token-level updates : 1. Gradient Norm: Amplified gradients can dominate updates and lead to optimization instability. 2. Entropy Change: Training in RL typically drives entropy upward; updates that further increase entropy can accelerate entropy explosion and distributional drift. 3. Learning Potential: Updates in low-entropy tokens yield limited benefit, as these are already well pretrained."
        },
        {
            "title": "3.2 Spurious Tokens as a Destructive Update Mode",
            "content": "To systematically analyze how different token types influence optimization, we categorize tokens along three binary factors: state entropy (high/low), token probability (high/low), and advantage sign (positive/negative). We then interpret the optimization behavior of each regime using Criteria 1, as summarized in Table 1. We observe that tokens characterized by low entropy, low probability, and positive advantage lead to negative optimization effects across all evaluation criteria. To empirically validate this hypothesis and examine the real-world behavior of such tokens, we fine-tune Qwen3-8B [6] under the JustRL setting [3] and explicitly record the token statistics during training. As 4 (a) Token Quadrants Visualization (b) Gradient Norm Comparison (c) Illustrative examples of spurious tokens. Three instances where the LLM selects spurious tokens (pink) over reasonable top-2 candidates (green). Spurious tokens often represent semantic errors despite the final response correctness. More comprehensive examples are provided in Appendix C. Figure 2 Comprehensive Analysis of Spurious Tokens. visualized in Figure 2a, the recorded tokens naturally form distinct clusters in the probability-entropy plane, confirming the existence of the four hypothesized regimes. Crucially, statistical analysis of these recorded dynamics (Figure 2b) reveals that spurious tokens trigger the highest mean gradient norms among all categories (e.g., +16.7% relative to the high-entropy & high-probability baseline). This quantitative evidence suggests that when these rare, low-entropy errors occur, they result in disproportionately large model updates. Qualitative examples in Figure 2c further illustrate that these tokens are often semantically inappropriate or erroneous breaks in coherence, whereas top candidates represent more reasonable choices. Since the final response may still receive high reward, the large gradient updates observed in Figure 2b significantly reinforce these erroneous patterns. Building on these empirical observations, we formally define this phenomenon under sequence-level reward assignment: Definition 3.4 (Spurious Tokens). Spurious tokens are tokens that contribute negligibly to the correctness of reasoning outcome but receive disproportionately large positive updates due to sequence-level reward assignment. This over-amplification of semantic errors disrupts chain-of-thought coherence and ultimately degrades training stability. As demonstrated by the alignment between the statistical anomalies (Figure 2b) and semantic errors (Figure 2c), the condition of low entropy, low probability, and positive advantage serves as robust practical criterion for identifying spurious tokens."
        },
        {
            "title": "3.3 Spurious-Token-Aware Policy Optimization (STAPO)",
            "content": "To mitigate the detrimental effects of spurious tokens while preserving the policys capacity for legitimate error correction, we propose Spurious-Token-Aware Policy Optimization (STAPO). The core of STAPO is the Silencing Spurious Tokens (S2T) mechanism, which selectively discards policy gradient contributions from spurious tokens. Specifically, S2T introduces binary mask IS2T by suppressing gradient contributions from tokens that fall into the destructive regime: i,t IS2T i,t = (cid:40) 0, 1, if ˆAi > 0 π(yi,t) < τp Ht < τh, otherwise. 5 Algorithm 1 Spurious-Token-Aware Policy Optimization (STAPO) Require: Dataset D, Initial Policy πθ, Group size G, Thresholds τp, τh, Batch size 1: Initialize policy parameters θ 2: for each training iteration do Synchronize: θold θ 3: Sample prompts xB and generate responses {y1, . . . , yG}B πθold ( xB) 4: Compute advantages ˆAB for each mini-batch Batch Data of size do using Eq. (2) for each response yi and token do Obtain pi,t = πθ(yi,t x, yi,<t) and hi,t = H(πθ( x, yi,<t)) Isilence i,t 1 if ˆAi > 0 pi,t < τp hi,t < τh then IS2T i,t end for Update θ through Eq. (1) with IS2T i,t 5: 6: 7: 8: 9: 10: 11: 12: Default: Keep Spurious Tokens: Mask end for 13: 14: end for 15: return Final policy πθ Here, τp and τh are hyperparameters representing the probability and entropy thresholds, respectively. Then the S2T mask is applied to the token-level policy gradient, yielding the STAPO objective: JSTAPO(θ) = {yi}G xD i=1πold 1 (cid:80)yi t=1 (cid:80)G i=1 (cid:88) yi (cid:88) IS2T i,t i=1 t=1 IS2T i,t min (cid:16) ρi,t ˆAi, clip(ρi,t, 1 ϵ, 1 + ϵ) ˆAi (cid:17) . (1) The terms ρi,t and ˆAi follow the standard definitions for probability ratio and group-normalized advantage, respectively: ρi,t(θ) = πθ(yi,t x, yi,<t) πθold (yi,t x, yi,<t) , ˆAi = R(x, yi) mean({R(x, yj)}G std({R(x, yj)}G j=1) j=1) , (2) Comparing Eq. (1) with the standard DAPO objective, there are two main differences highlighted in red. Firstly, STAPO selectively masks the loss calculation for spurious tokens. Furthermore, the normalization term in Eq. (1) is adjusted to account only for the remaining valid tokens. This design effectively prevents the misleading update due to the stochastic sampling process, thereby stabilizing the RL alignment process. The overall algorithm is summarized in Algorithm 1. Remark 3.5 (Thresholding Strategy). Following [2], the entropy threshold τh is implemented as dynamic quantile within each mini-batch to adaptively capture tokens with low epistemic uncertainty. In contrast, we purposefully employ fixed absolute value for the probability threshold τp rather than quantile. Our empirical findings indicate that applying quantile-based τp would systematically discard fixed proportion of tokens regardless of their absolute confidence, which inevitably prunes legitimate high-probability tokens and compromises training stability."
        },
        {
            "title": "4 Related Work",
            "content": "Reinforcement Learning for LLMs. In contrast to traditional reinforcement learning paradigms applied to control and decision-making problems [2123], reinforcement learning from human feedback (RLHF) has emerged as predominant approach for aligning LLMs with human preferences and diverse downstream objectives [4, 5]. While early approaches relied on on-policy optimization such as PPO [24], recent work has shifted toward more efficient preference-based DPO [25], which avoids explicit reward modeling and online rollouts. The focus has further expanded from general alignment to improving reasoning capabilities, motivating RL and training schemes such as GRPO [17]. In this context, range of policy optimization 6 variants, including DAPO [10], GSPO [18], SAPO [11], and other related methods [9, 26, 27], have been developed to enhance optimization stability, sample efficiency, and scalability for reasoning-oriented language models. Entropy Instability in RL. persistent challenge in training reasoning-oriented language models with RL is the rapid collapse of policy entropy during early optimization. Prior work mitigates this issue through interventions such as selectively regularizing high-entropy tokens [2], increasing the proportion of entropyenhancing samples [12], and modifying clipping strategies in policy optimization [10, 11, 28]. However, these methods often introduce the opposite failure mode, where entropy grows excessively or becomes unstable, degrading reasoning coherence and leading to repetitive or unstructured outputs. Although some studies analyze entropy dynamics during RL training [19, 20] and others explicitly enforce entropy stability as an optimization objective [13], existing approaches largely treat entropy as surface-level training signal rather than addressing the underlying sources of instability. Gradient Domination by Low-Probability Tokens. key microscopic source of instability stems from the disproportionate gradient influence of low-probability tokens. As proved by [14], rare tokens can generate excessively large updates, allowing small subset of unstable predictions to dominate optimization, which harms fine-tuning stability. Recent work addresses this through probability-aware modulation. Yang et al. [14] propose Advantage Reweighting and Low-Probability Token Isolation to attenuate or asynchronously update rare tokens, while Token-Regulated GRPO (TR-GRPO) [15], using probability shaping to suppress sharp directions induced by low-confidence predictions. To avoid suppressing informative exploratory signals, Low-Probability Regularization (Lp-Reg) [16], which filters noise while preserving meaningful rare tokens, and GTPO [29] mitigates conflicting gradients arising from such tokens across reward trajectories. However, these approaches largely rely on scalar probability thresholds, lacking joint, fine-grained treatment of token-level confidence and probability, and thus failing to distinguish useful exploration from aleatoric noise under local model calibration."
        },
        {
            "title": "5.1 Settings\nBaselines. We compare our approach against several state-of-the-art reinforcement learning algorithms for\nLLMs, including GRPO [17], 20-Entropy [2] and JustRL [3]. For a fair comparison, the dynamic sampling\ntechnique introduced in [10] is not applied to any of the baselines.",
            "content": "Training details. All experiments are implemented within the verl [30] codebase and executed on cluster of 64 NVIDIA H20 GPUs. We utilize the DAPO-Math-17K [10] as the training dataset, where each prompt is formatted with the instruction: Please reason step by step, and put your final answer within boxed{}. All models are trained with total batch size of 256 and mini-batch size of 64, resulting in 4 gradient updates per rollout step. We utilize the AdamW [31] optimizer with learning rate of 1 106, complemented by linear warmup over the initial 10 rollout steps. For each problem, we generate 8 rollouts with maximum response length of 15k tokens. To ensure consistency in the reward signal, the reward function employs the lightweight, rule-based verifier from DAPO without modification. To investigate the scaling laws, we conduct experiments across three model scales: Qwen3-1.7B-Base, Qwen3-8B-Base and Qwen3-14B-Base. Detailed training hyperparameters are provided in Appendix D. Benchmarks. We evaluate our models on six widely adopted and challenging mathematical reasoning benchmarks: AIME24 [32], AIME25 [33], AMC23 [32] , MATH500 [34], Minerva [35], and OlympiadBench [36]. We generate independent responses per problem (N = 4 for MATH500, Minerva, and OlympiadBench; = 32 for others) across two decoding configurations: temperature=1.0, top-p=1.0 and temperature=0.7, top-p=0.9, with maximum length of 20,480 tokens. Results are reported as the average accuracy. To ensure evaluation rigour, we employ CompassVerifier-3B [37], lightweight LLM verifier, to rectify misclassifications from the rule-based verification. 7 (a) AIME24 Acc, Entropy, and Training Reward for Qwen3-1.7B Base. (b) AIME24 Acc, Entropy, and Training Reward for Qwen3-8B Base. (c) AIME24 Acc, Entropy, and Training Reward for Qwen3-14B Base. Figure 3 Training Results across Different Model Scales. Each row presents the training dynamics for specific model size (1.7B, 8B, and 14B). Notably, our proposed STAPO method achieves superior performance while maintaining stable policy entropy across all model sizes."
        },
        {
            "title": "5.2 Main Results",
            "content": "In this section, we present comprehensive analysis of STAPO across three model scales (1.7B, 8B, and 14B). We structure our analysis into three dimensions: the training dynamics, the performance under training-aligned configurations, and the robustness across different evaluation protocols. 5.2.1 Training Behaviors Analysis We analyze the training behaviors in Figure 3, where policy entropy serves as key indicator of RL stability in reasoning tasks. As shown in the middle column, 20-Entropy and JustRL frequently suffer from entropy explosion, while GRPO exhibits entropy collapse, both of which are detrimental to stable training. In contrast, STAPO maintains stable and well-regulated entropy level across all model scales. Notably, this stability is achieved by masking only negligible fraction of spurious tokens (approximately 0.01% in our 8 Table 2 Main Results on Six Mathematical Reasoning Benchmarks across Three Model Scales. Each cell reports performance under the training-aligned configuration (temperature = 1.0, top-p = 1.0) and the JustRL [3] evaluation setting (temperature = 0.7, top-p = 0.9, shown in gray). For each configuration, the best result is highlighted in bold (training-aligned setting) and in dark gray (JustRL setting), respectively. Baseline AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Avg RL from the Qwen3-1.7B Base Model GRPO 6.88 (10.83) 4.06 (4.06) 40.47 (41.17) 68.60 (67.15) 32.44 (32.53) 30.64 (30.64) 30.52 (31.06) 20-Entropy 13.33 (13.85) 8.44 (7.60) 46.56 (44.53) 69.85 (69.10) 28.03 (29.04) 35.65 (34.35) 33.64 (33.08) JustRL 8.85 (14.58) 6.25 (10.93) 45.39 (51.09) 62.70 (69.75) 22.61 (28.89) 29.15 (34.16) 29.16 (34.90) STAPO 17.40 (16.04) 15.42 (14.27) 55.94 (52.42) 73.55 (70.30) 28.22 (29.42) 38.54 (37.87) 38.18 (36.72) RL from the Qwen3-8B Base Model GRPO 31.25 (32.40) 24.69 (24.17) 75.23 (74.14) 88.90 (88.85) 55.88 (53.58) 61.50 (58.34) 56.24 (55.25) 20-Entropy 31.25 (35.10) 27.50 (27.39) 79.92 (80.00) 89.85 (89.85) 54.78 (54.23) 62.50 (60.39) 57.63 (57.83) JustRL 25.21 (34.06) 21.98 (26.04) 73.52 (81.48) 84.90 (87.55) 47.33 (51.03) 51.26 (56.57) 50.70 (56.12) STAPO 33.44 (39.48) 28.65 (29.37) 79.92 (80.23) 90.40 (91.40) 57.17 (55.70) 62.98 (63.32) 58.76 (59.92) RL from the Qwen3-14B Base Model GRPO 42.19 (40.52) 30.31 (32.40) 81.95 (78.12) 90.05 (92.00) 55.15 (57.54) 64.76 (64.28) 60.74 (60.81) 20-Entropy 42.08 (51.25) 34.06 (39.48) 84.45 (87.73) 91.65 (92.30) 51.10 (54.78) 61.30 (63.95) 60.77 (64.92) JustRL 37.40 (52.08) 26.56 (38.96) 81.95 (89.61) 87.65 (94.25) 43.84 (58.92) 57.01 (68.81) 55.74 (67.11) STAPO 46.98 (54.27) 35.21 (41.67) 87.11 (90.62) 92.45 (93.85) 59.47 (59.93) 68.21 (71.62) 64.38 (68.66) experiments). Moreover, STAPO attains the highest training reward (right column) and steadily improves AIME24 validation accuracy (left column) across all model sizes, suggesting that filtering out spurious tokens mitigates optimization distortions and leads to more stable and reliable learning. 5.2.2 Performance under Training-Aligned Settings Table 2 summarizes the main results. We first focus on the Training-Aligned evaluation setting (temperature=1.0, top-p=1.0), which reflects the models intrinsic capability without the aid of greedy decoding strategies. Under this setting, STAPO demonstrates consistent scalability and superior performance across all model sizes. For the 1.7B model, STAPO achieves substantial lead, surpassing the strongest baseline (20-Entropy) by relative margin of 13.50% in average accuracy. At the 8B scale, STAPO continues to outperform all competing methods, maintaining its position as the state-of-the-art. The efficacy of our method is further validated at the 14B scale, where STAPO extends its lead, achieving relative improvement of approximately 5.94% over the best-performing baseline. These results collectively demonstrate that STAPO effectively unlocks the reasoning potential of base models across varying scales. 5.2.3 Performance Across Evaluation Settings Recent works adopt different evaluation configurations [2, 3, 10, 12]. To ensure fair comparison, we additionally evaluate STAPO under the JustRL setting (shown in parentheses/gray in Table 2). The results indicate that STAPO remains the top-performing method across different model scales under this setting. However, the performance gap between STAPO and the baselines becomes slightly narrower compared to the training-aligned configuration. We attribute this effect to the evaluation protocol: baselines with unstable, high-entropy distributions benefit more from constrained decoding, which suppresses low-probability tail generations. In contrast, STAPO learns more intrinsically stable distribution and therefore relies less on decoding heuristics. Overall, STAPO achieves state-of-the-art performance under both configurations, demonstrating its robustness and stability. 9 (a) Sensitivity to Probability Threshold (τp) (b) Sensitivity to Entropy Threshold (τh) Figure 4 Sensitivity Analysis of Probability Threshold τp and Entropy Threshold τh for Qwen3-1.7B Base. Performance is reported as Acc (avg@32) on AIME24 and AIME25 benchmarks."
        },
        {
            "title": "5.3 Hyperparameter Sensitivity\nSensitivity to Probability Threshold (τp). We evaluate the sensitivity of the Qwen3-1.7B Base model to the\nprobability threshold τp. As illustrated in Figure 4a, accuracy declines sharply as τp increases. On AIME24,\nperformance drops from 17.4% to 7.2% as τp shifts from 2 × 10−3 to 2 × 10−1. This trend indicates that\noverly aggressive thresholds remove low-probability tokens that are crucial for maintaining valid reasoning\nchains. Therefore, effective filtering must remain highly selective, targeting only genuinely spurious tokens\nwhile preserving rare yet semantically meaningful ones.",
            "content": "Sensitivity to Entropy Threshold (τh). The threshold τh defines the percentile boundary for entropy-based masking. In our optimal configuration (τh = 20%), we mask the bottom 80% of low-entropy tokens. As τh increases, performance consistently declines. Raising τh from 20% to 80% reduces AIME24 accuracy from 17.4% to 11.6% and AIME25 accuracy from 15.4% to 7.3%. This degradation suggests that less stringent masking permits more detrimental updates from uninformative tokens, which can bias optimization toward trivial patterns. Overall, these results indicate that lower τh is crucial for stabilizing training. (a) Average Acc (avg@32) across six benchmarks with temperature=1.0, top-p=1.0. (b) Average Acc (avg@32) across six benchmarks with temperature=0.7, top-p=0.9. Figure 5 Ablation Results of Different Masking Strategies across Three Model Scales."
        },
        {
            "title": "5.4 Masking Strategy Ablation",
            "content": "As illustrated in Figure 5, our ablation study across three model scales reveals distinct performance trajectories contingent upon the masking criteria employed. Specifically, masking tokens based exclusively on probability threshold consistently underperforms the baseline, suggesting that an indiscriminate pruning of low-probability 10 (a) Spurious Tokens Ratio. (b) Frequent Spurious Tokens. (c) Frequent Normal Tokens. Figure 6 Analysis of Spurious Tokens: (a) indicates the ratio of spurious tokens during training; (b) and (c) present the word clouds of frequently masked spurious tokens and retained normal tokens tokens inadvertently discards infrequent but logically indispensable reasoning steps. In contrast, the strategy of masking high-entropy, low-probability tokens exhibits significant scale-dependent divergence: while it remains competitive for the 8B and 14B models, it leads to catastrophic performance collapse on the 1.7B scale. This disparity implies that while larger models may possess the structural redundancy to withstand the removal of high-uncertainty noise, smaller models rely heavily on these tokens for exploratory knowledge representation. Most notably, STAPO is the only approach that achieves robust performance gain over the baseline across all parameter scales. By precisely isolating these spurious signals, STAPO effectively regularizes gradient updates without compromising the models fundamental capacity for legitimate mathematical derivation."
        },
        {
            "title": "5.5 Detailed Analysis",
            "content": "5.5.1 Spurious Tokens Ratio As shown in Figure 6a, the proportion of spurious tokens identified by STAPO remains extremely small throughout RL training. In the early stage, the ratio briefly peaks at approximately 0.03% for the 14B model before rapidly declining. For the majority of training, it consistently stays below 0.01% across all model scales. Despite their rarity, masking these tokens yields substantial performance improvements (see Section 5.2). These results suggest that RL instability is not primarily caused by frequent errors, but rather by sparse set of positive advantage yet semantically uninformative tokens that exert disproportionate influence on gradient updates. 5.5.2 Word Cloud Visualization The qualitative distinction between spurious and normal tokens is illustrated in Figures 6b and 6c. The most frequent spurious tokens mainly include specific digits (e.g., 4, 1, 2), mathematical symbols (e.g., $), and transitional words such as Wait, But, and Since. Although these tokens may appear in correct responses, they typically contribute little to the underlying logical derivation. When coupled with large gradient updates, they can induce disproportionately unstable optimization dynamics. In contrast, the retained normal tokens correspond to the structural vocabulary of mathematical reasoning, including words such as Let, find, we, and can. These tokens represent essential procedural and logical components that sustain coherent reasoning chains."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we investigate the instability of RL from token-level perspective and identify pathological update regime driven by the interaction between token probability, policy entropy, and advantage sign. We show that negligible fraction of spurious tokens, defined as rare and uninformative tokens that coincidentally co-occur with correct outcomes, can receive disproportionately amplified gradients and exert an outsized 11 influence on optimization dynamics. Based on this analysis, we propose STAPO, simple yet effective method that suppresses approxim ately 0.01% of destructive token updates and renormalizes the loss over valid tokens. Empirical results across six mathematical reasoning benchmarks demonstrate that this minimal intervention substantially stabilizes policy entropy and consistently improves reasoning performance. Due to resource and time constraints, we have not yet conducted ablation studies covering all classification scenarios, and our experiments currently focus primarily on mathematical reasoning tasks, lacking evaluation in other domains such as code generation. Moreover, the current analysis focuses primarily on tokens within correct responses. In future work, we plan to extend both the empirical evaluation to broader task domains and to include tokens from incorrect responses, aiming to develop more comprehensive understanding of token-level dynamics in RL. We hope this study provides new insights into improving the stability of RL for LLM reasoning."
        },
        {
            "title": "References",
            "content": "[1] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [2] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. [3] Bingxiang He, Zekai Qu, Zeyuan Liu, Yinghao Chen, Yuxin Zuo, Cheng Qian, Kaiyan Zhang, Weize Chen, Chaojun Xiao, Ganqu Cui, et al. Justrl: Scaling 1.5 llm with simple rl recipe. arXiv preprint arXiv:2512.16649, 2025. [4] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [5] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. [6] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [7] Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, and Yun Li. Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods. IEEE Transactions on Neural Networks and Learning Systems, 2024. [8] Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025. [9] Longtian Qiu, Shan Ning, Jiaxuan Sun, and Xuming He. NoisyGRPO: Incentivizing multimodal cot reasoning via noise injection and bayesian estimation. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [10] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [11] Chang Gao, Chujie Zheng, Xiong-Hui Chen, Kai Dang, Shixuan Liu, Bowen Yu, An Yang, Shuai Bai, Jingren Zhou, and Junyang Lin. Soft adaptive policy optimization. arXiv preprint arXiv:2511.20347, 2025. [12] Zhiheng Xi, Xin Guo, Yang Nan, Enyu Zhou, Junrui Shen, Wenxiang Chen, Jiaqi Liu, Jixuan Huang, Zhihao Zhang, Honglin Guo, et al. Bapo: Stabilizing off-policy reinforcement learning for llms via balanced policy optimization with adaptive clipping. arXiv preprint arXiv:2510.18927, 2025. [13] Kai Yang, Xin Xu, Yangkun Chen, Weijie Liu, Jiafei Lyu, Zichuan Lin, Deheng Ye, and Saiyong Yang. Entropic: Towards stable long-term training of llms via entropy stabilization with proportional-integral control. arXiv preprint arXiv:2511.15248, 2025. [14] Zhihe Yang, Xufang Luo, Zilong Wang, Dongqi Han, Zhiyuan He, Dongsheng Li, and Yunjian Xu. Do not let low-probability tokens over-dominate in rl for llms. arXiv preprint arXiv:2505.12929, 2025. [15] Tue Le, Nghi DQ Bui, Linh Ngo Van, and Trung Le. Token-regulated group relative policy optimization for stable reinforcement learning in large language models. arXiv preprint arXiv:2511.00066, 2025. [16] Guanhua Huang, Tingqiang Xu, Mingze Wang, Qi Yi, Xue Gong, Siheng Li, Ruibin Xiong, Kejiao Li, Yuhao Jiang, and Bo Zhou. Low-probability tokens sustain exploration in reinforcement learning with verifiable reward. arXiv preprint arXiv:2510.03222, 2025. [17] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [18] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization, 2025. 13 [19] Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. [20] Shumin Wang, Yuexiang Xie, Wenhao Zhang, Yuchang Sun, Yanxi Chen, Yaliang Li, and Yanyong Zhang. On the entropy dynamics in reinforcement fine-tuning of large language models. arXiv preprint arXiv:2602.03392, 2026. [21] Shengbo Eben Li. Reinforcement Learning for Sequential Decision and Optimal Control. Springer, Singapore, 2023. [22] Jingliang Duan, Wenxuan Wang, Liming Xiao, Jiaxin Gao, Shengbo Eben Li, Chang Liu, Ya-Qin Zhang, Bo Cheng, and Keqiang Li. Distributional soft actor-critic with three refinements. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47(5):39353946, 2025. [23] Likun Wang, Xiangteng Zhang, Yinuo Wang, Guojian Zhan, Wenxuan Wang, Haoyu Gao, Jingliang Duan, and Shengbo Eben Li. Off-policy reinforcement learning with model-based exploration augmentation. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. [24] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, volume 36, pages 5372853741, 2023. [26] Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. [27] Jiakang Wang, Runze Liu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, and Kun Gai. Aspo: Asymmetric importance sampling policy optimization. arXiv preprint arXiv:2510.06062, 2025. [28] Aili Chen, Aonian Li, Bangwei Gong, Binyang Jiang, Bo Fei, Bo Yang, Boji Shan, Changqing Yu, Chao Wang, Cheng Zhu, et al. Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025. [29] Marco Simoni, Aleksandar Fontana, Giulio Rossolini, Andrea Saracino, and Paolo Mori. Gtpo: Stabilizing group relative policy optimization via gradient and entropy control. arXiv preprint arXiv:2508.03772, 2025. [30] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. [31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [32] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. [33] OpenCompass. Aime2025 dataset. https://huggingface.co/datasets/opencompass/AIME2025, 2025. Accessed: 2025-01-23. [34] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. [35] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems, volume 35, pages 38433857, 2022. [36] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1586515895. Association for Computational Linguistics, 2024. 14 [37] Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek Wong, Songyang Zhang, et al. Compassverifier: unified and robust verifier for llms evaluation and outcome reward. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3345433482, 2025. [38] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024."
        },
        {
            "title": "A Gradient Norm Decomposition",
            "content": "We first establish an exact decomposition of the gradient norm with respect to the logits, which serves as the foundation for our bounds. Lemma A.1 (Gradient Norm Decomposition [14]). Let RV denote the logits and πθ(vn) = ean eam be the induced softmax distribution. Let yi,t be the target token with index (i.e., vk = yi,t). The squared ℓ2-norm of the gradient of the group-style objective with respect to satisfies: (cid:80) aJ (yi,t)2 = wi,t2 1 2πθ(yi,t) + πθ(vn)2 . (cid:88) n=1 Proof. Recall that the gradient of the log-likelihood ln πθ(yi,t) with respect to the logit an is given by δkn πθ(vn), where δkn is the Kronecker delta. The gradient of the weighted objective is therefore: (yi,t) an = wi,t (δkn πθ(vn)) = (cid:40) wi,t(1 πθ(vk)), wi,tπθ(vn), if = k, if = k. Computing the squared ℓ2-norm by summing over all {1, . . . , V}: aJ (yi,t)2 = (cid:0)wi,t(1 πθ(vk))(cid:1) + (wi,tπθ(vn))2 (cid:88) n=k (1 2πθ(vk) + πθ(vk)2) + = wi,t πθ(vn)2 (cid:88) n=k 1 2πθ(vk) + πθ(vk)2 + = wi,t2 πθ(vn)2 , (cid:88) n=k πθ(vn)2 . 1 2πθ(vk) + = wi,t2 (cid:88) n= Identifying vk as yi,t completes the proof. Proof of Proposition 3.1 We now complete the proof of Proposition 3.1. By Lemma A.1, the squared gradient norm can be written as aJ (yi,t)2 = wi,t2 1 2πθ(yi,t) + πθ(vn)2 . (cid:88) n=1 (3) Therefore, the term (cid:80)V n=1 πθ(vn)2, is known as the collision probability of the distribution. This quantity measures the concentration of the distribution: it increases as the distribution becomes more peaked and decreases as the distribution becomes more uniform. 16 Lower bound. To derive lower bound, we relate the collision probability to the Shannon entropy. Recall that the Rényi entropy of order 2 is defined as H2(πθ) = ln πθ(vn) . (cid:88) n=1 For discrete distributions, the Rényi entropy of order 2 is always upper bounded by the Shannon entropy: H2(πθ) H(πθ), where H(πθ) = (cid:80)V lower bound: n=1 πθ(vn) ln πθ(vn). Substituting this inequality into Eq. (3) gives the entropy-based 1 2πθ(yi,t) + eH(πθ)(cid:17) aJ (yi,t)2 wi,t2 (cid:16) . Upper bound. To derive an upper bound, we use an inequality that characterizes how far discrete distribution deviates from uniformity. For finite alphabet V, the collision probability satisfies [14] (cid:88) n=1 πθ(vn)2 1 CV H(πθ)2, where CV = V1 V(ln V)2 . Substituting this bound into Eq. (3) yields aJ (yi,t)2 wi,t2 (cid:0)1 2πθ(yi,t) + 1 CV H(πθ)2(cid:1) = wi,t2 (cid:0)2 2πθ(yi,t) CV H(πθ)2(cid:1) . Combining the lower and upper bounds completes the proof of Proposition 3.1."
        },
        {
            "title": "C Supplementary Spurious Cases",
            "content": "In this section, we provide detailed taxonomic description of the three categories of spurious tokens identified in our mechanistic analysis. Below we detail each category with representative example referenced from the accompanying tables. Category I: Uncommon Syntax  (Table 3)  . This category comprises tokens that are linguistically valid but highly improbable given the models pre-trained distribution. The model often substitutes standard technical terminology with colloquialisms or awkward phrasing. Example: As shown in Case 2, when describing operations on graph, the model selects the token broken (Prob: 0.05%) to describe the removal of edges. The canonical term removed dominates the Top-5 distribution (85.53%). Reinforcing such low-probability synonyms forces the policy to drift away from standard mathematical language, destabilizing the linguistic entropy. Category II: Hallucinations and Math Errors  (Table 4)  . This category involves factual inaccuracies, calculation errors, or hallucinated values embedded within trajectory that coincidentally results in correct final answer (trajectories with locally incorrect steps but correct final answers). Example: In Case 3, the model attempts to verify factorization with the equation 6901 = 67 103 - 1. Since 67 103 equals 6901 exactly, the inclusion of the subtraction operator - renders the statement mathematically false (6900 = 6901). Because the final answer matches the ground truth, the RL objective erroneously increases the likelihood of this arithmetic hallucination. Category III: Formatting Errors  (Table 5)  . The final category pertains to LATEX syntax errors, malformed delimiters, or hallucinated stop sequences that occur in the trajectory but are ignored by the answer extraction parser. Example: Case 4 illustrates latent formatting error. In the summation sequence, the model generates 17 the integer token 3 immediately following the plus sign, bypassing the canonical token (space) which carries significantly higher prior probability. Although LATEX renderers automatically correct the spacing (making the error invisible in the final output), the policy learns to overfit to non-standard sequence that omits canonical whitespace. Throughout this section, model outputs are presented as rendered LATEX to facilitate content verification. However, for errors categorized under formatting or syntax, we retain the raw token sequence to explicitly demonstrate the distributional anomalies. Table 3 Category I: Uncommon Syntax. Case 1 Context: The length of is 1981, and the length of is 1014. Since the length of is 2012, for the minimum length of B, should start immediately from the right of A. The length of is the sum of the lengths of and minus the length of , so its minimum value is 1981 + 1014 2012 = 983. Here is Python calculation to verify the above reasoning... Spurious Token calculation Prob (P ) Adv 0.0115% 1.62 Top-5 Distribution code(51.90%) program(31.48%) solution(11.58%) script(3.32%) function(0.58%) Case 2 Context: We are given islands with ferry lines between each pair of distinct islands, which is essentially complete graph Kn. Each edge of the graph is colored by one of colors, corresponding to the companies operating the ferry lines. For any closure of one company, all its lines must be broken (the corresponding edges ignored) such that the remaining graph can no longer support Hamiltonian Cycle, meaning its no longer possible to visit all the islands exactly once without returning to the starting point. According to the condition, every color, therefore, must serve to connect the graph in such way that the removal of edges of any single color makes the graph Hamiltonian non-existent. In complete graph Kn, we can color the edges in such way that removing any single color results in graph that lacks Hamiltonian Cycle. For this, we can use result from graph theory, informally known as \"edge coloring minimum spanning trees\": for nodes, we can divide the set of nodes into 2 2 color classes (as far as coloring spans, and where means \"the greatest integer less than x\"). So when we have = 2m (even) nodes, the maximal number of colors we can use is 1, since we can divide our nodes into colors and deleting any one will still result in complete graph K2m1 which remains Hamiltonian (since K2m1 is always Hamiltonian for 1). For = 2m + 1 (odd) nodes, we can divide our nodes into classes, so the discard of any one of these classes will disintegrate the graph such that Hamiltonian Cycle will become impossible because it contains vertex of degree 1 or subdivision thereof. Spurious Token broken Prob (P ) Adv 0.0471% 2.47 Top-5 Distribution removed(85.53%) closed(3.32%) deleted(2.58%) disconnected(1.57%) taken(0.95%) discard 0.0001% 2.47 maximal(85.89%) maximum(9.05%) number(1.57%) largest(0.45%) answer(0.45%) Case 3 Context: Therefore, the final boxed answer is:[ boxed {41} ]<endoftext> Spurious Token boxed Prob (P ) Adv 0.1486% 0. Top-5 Distribution answer(98.88%) result(0.52%) value(0.19%) boxed(0.15%) solution(0.15%) Case 4 Continued on next page... 18 Table 3 continued from previous page Context: To solve the given problem, we need to find the smallest number of candies that satisfies the given conditions. Specifically: 1. When candies are divided between 21 people (Albert and his 20 friends), the remainder is 5. Spurious Token between Prob (P ) Adv 0.0667% 0.72 Top-5 Distribution among(64.55%) by(23.75%) evenly(8.74%) amongst(1.95%) equally(0.92%) Case 5 Context: To trisegment, we must first count the number of points in the polygon. ###### Step 1:sume the number of points in the polygon Let yea the number of sides (or vertices) in polygon as given in the problem statement. We are given that P1, P2, . . . Pn/ are the vertices of the polygon. Spurious Token tr Prob (P ) Adv 0.0004% 0.72 0.0003% 0.72 Top-5 Distribution olve(63.83%) determine(30.15%) find(5.24%) triang(0.20%) count(0.10%) be(88.02%) represent(5.63%) denote(3.41%) =(0.76%) and(0.59%) Table 4 Category II: Hallucinations and Math Errors. Case 1 Context: = 1 x1(y 2 3) + x2(y 3 2) + x3(y 1 2) Here, $ x$s and $y$s are the coordinates of the points. Spurious Token 2 Prob (P ) Adv 0.0179% 0.54 Top-5 Distribution 1(99.95%) 2(0.01%) 3(0.01%) 0(0.00%) }_- (0.00%) $s 0.0117% 0.54 _(94.78%) 1(1.05%) =(0.82%) _{(0.82%) $(0.50%) Case 2 Context: (a 10, 10) = (193, 11) = 203, = 213 Spurious Token 3 Prob (P ) Adv 0.0017% 0.94 Top-5 Distribution (99.98%) n(0.01%) 1(0.01%) n(0.00%) 3(0.00%) Case 3 Context: 6901 = 67 103-1 Spurious Token - Prob (P ) Adv 0.0202% 1.21 Case 4 Context: 16a3 = 350, a3 = 21.5625. Top-5 Distribution $,(99.48%) $(0.36%) +(0.13%) =(0.00%) -(0.02%) Spurious Token 5 Case 5 Prob (P ) Adv 0.1896% 1.62 Top-5 Distribution 8(98.16%) 2(1.40%) 5(0.24%) 9(0.07%) 7(0.04%) Continued on next page... Context: Compute for Each Pair: Table 4 continued from previous page (5, 30) = 25 (4, 12) = 8 (3, 6) = 3 (2, 3) = (0, 0) = 0 (7, 42) = 49 (8, 24) = 32 (9, 18) = 9 (10, 15) = 5 (12, 12) = 24 Spurious Token 9 Case 6 Context: Thus, Prob (P ) Adv 0.0123% 0.35 Top-5 Distribution 2(99.98%) 9(0.01%) 1(0.01%) 3(0.00%) 8(0.00%) sin = 435 435 = 435 Spurious Token 4 Prob (P ) Adv 0.0867% 1.62 Top-5 Distribution 5(95.28%) 3(3.69%) (0.64%) 6(0.11%) 4(0.09%) Table 5 Category III: Formatting Errors. Case 1 Context: To solve this problem, lets start by using the given information about the function (x) = ax2 + bx + c: 1. Since (1) = 0, we know that: This means = b. Now, substitute = into the quadratic function: + + = 0 (x) = ax2 + bx (a + b) Spurious Token Now Prob (P ) Adv 0.0708% 0.35 Top-5 Distribution 2(99.74%) Next(0.12%) Now(0.06%) So(0.03%) Sub(0.02%) Case 2 Context: boxed{5text{ agony}} Spurious Token Prob (P ) Adv 0.0015% 0.35 Top-5 Distribution }n(99.57%) }(0.25%) }nn(0.12%) }(0.03%) }.n(0.01%) Case 3 Context: therefore 10^y)$ (sqrt{25}=5 Continued on next page... Table 5 continued from previous page Prob (P ) Adv 0.0063% 0.72 =(84.69%) =(6.95%) =(4.22%) n(1.55%) Top-5 Distribution n(0.35%) Spurious Token )$ Case 4 Context: Σ3 = 113 + 103 + 93 + 83 + 73 + 63 + 53 + 43 +33 + 23 + 13 + 03 Spurious Token 3 Prob (P ) Adv 0.0911% 1.62 Top-5 Distribution (99.92%) 3(0.07%) n(0.00%) (0.00%) +(0.00%) Case 5 Context: Repeating these steps three more times for each roll back to P4: - If was rolled, P5 = (56, 368) - If was rolled, P5 = (56, 184) - If was rolled, P5 = (768, 184): Spurious Token : Prob (P ) Adv 0.0035% 2.47 Top-5 Distribution nn(88.06%) ((4.38%) n(4.38%) nn(1.26%) ,(0.36%) Case 6 Context: Hence, the maximum value of $frac{n_i}{k}$ for 1 leq leq 70 is: [ boxed{553} ] struckuser<endoftext> Spurious Token struck Prob (P ) Adv 0.0000% 1.21 Top-5 Distribution (98.58%) )(0.13%) (0.10%) $(0.08%) $$(0.07%)"
        },
        {
            "title": "D Training Details",
            "content": "We implement our proposed STAPO algorithm based on the open-source alignment framework veRL [38]. All models are trained using the AdamW optimizer with constant learning rate of 1 106 and warm-up phase of 10 steps. To ensure training stability, we apply global gradient clipping norm of 1.0. For efficient data generation, we utilize vLLM as the inference backend. The training process employs global batch size of 256, with each prompt generating group of = 8 rollouts. Following the DAPO formulation, we do not employ separate value network or an additional KL divergence penalty term in the loss function; instead, we rely on group-relative advantages and the clipping mechanism to constrain policy updates. We conduct all experiments on 64 NVIDIA H20 GPUs. The complete set of hyperparameters used across all model scales is detailed in Table 6. 21 Table 6 Full Training Hyperparameters for All Model Training Hyperparameter Advantage Estimator Use KL Loss Use Entropy Regularization Train Batch Size Max Prompt Length Max Response Length PPO Mini Batch Size PPO Micro Batch Size/GPU Clip Ratio Range Grad Clip Learning Rate Warm-up Step Training Temperature Training Top-p Validation Temperature Validation Top-p Rollout Reward Function Value GRPO No No 256 1k 15k 64 1 [0.8, 1.28] 1.0 1e-6 10 1.0 1.0 0.7 0.9 8 DAPO"
        }
    ],
    "affiliations": [
        "DiDi Voyager Labs, DiDi Autonomous Driving",
        "School of Vehicle and Mobility & College of AI, Tsinghua University"
    ]
}