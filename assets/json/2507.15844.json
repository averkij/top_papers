{
    "paper_title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning",
    "authors": [
        "Shangke Lyu",
        "Linjuan Wu",
        "Yuchen Yan",
        "Xingyu Wu",
        "Hao Li",
        "Yongliang Shen",
        "Peisheng Jiang",
        "Weiming Lu",
        "Jun Xiao",
        "Yueting Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 2 4 4 8 5 1 . 7 0 5 2 : r a"
        },
        {
            "title": "HIERARCHICAL BUDGET POLICY OPTIMIZATION FOR\nADAPTIVE REASONING",
            "content": "Shangke Lyu1,, Linjuan Wu1,, Yuchen Yan1, Xingyu Wu1, Hao Li2 Yongliang Shen1, Peisheng Jiang2, Weiming Lu1, Jun Xiao1, Yueting Zhuang1 1Zhejiang University 2SF Technology {lyusk, wulinjuan525, syl, luwm}@zju.edu.cn GitHub: https://github.com/zju-real/hbpo (cid:128) Project: https://zju-real.github.io/hbpo"
        },
        {
            "title": "ABSTRACT",
            "content": "Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet exhibit significant computational inefficiency by applying uniform reasoning strategies regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. HBPO addresses the fundamental challenge of exploration space collapse in efficiency-oriented training, where penalties on long output length systematically bias models away from necessary long reasoning paths. Through hierarchical budget exploration, our approach partitions rollout samples into multiple subgroups with distinct token budgets, aiming to enable efficient resource allocation while preventing degradation of capability. We introduce differentiated reward mechanisms that create budget-aware incentives aligned with the complexity of the problem, allowing models to discover natural correspondences between task requirements and computational effort. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Unlike existing methods that impose external constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity."
        },
        {
            "title": "INTRODUCTION",
            "content": "Advances in large reasoning models have led to impressive performance on complex reasoning tasks through chain-of-thought methodologies (OpenAI, 2024; DeepSeek-AI, 2025b). However, these models exhibit fundamental inefficiency: they generate unnecessarily long reasoning chains even for simple problems, sometimes consuming thousands of tokens for basic arithmetic (Chen et al., 2025; 2024). This phenomenon reveals fundamental misalignment, as current reasoning models lack the ability to adapt their computational effort to the actual complexity of problems. Recent empirical findings challenge the conventional belief that longer reasoning always leads to better outcomes. Research shows that models can maintain competitive accuracy even without intermediate steps (Ma et al., 2025), and in some cases, shorter reasoning paths perform comparably or even better on simpler tasks (Li et al., 2025). This is further supported by stark variations in optimal reasoning lengths across tasks. For instance, L1 (Aggarwal & Welleck, 2025) achieves peak performance with 1,100 tokens on GSM8K, but requires over 3,000 tokens on OlympiadBench. Such heterogeneity highlights key insight: the computational requirements for effective reasoning * The first two authors have equal contributions."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: HBPO prevents exploration space collapse through hierarchical budget exploration. While efficient reasoning methods progressively abandon long reasoning paths during training, and length control methods like L1 achieve efficiency through mechanical constraints, HBPO partitions the exploration space into budget-constrained hierarchies (512, 1024, 2048, 2560 tokens). This structure maintains reasoning diversity throughout training, enabling emergent adaptive behavior where models match computational resources to problem complexity. The result is superior accuracy with efficient token usage varying from hundreds of tokens on GSM8K to thousands on AIME25. are inherently problem-dependent, yet current models apply uniform reasoning strategies regardless of task complexity. To address these inefficiencies, an increasing number of studies aim to improve the inference efficiency of reasoning models. Current approaches fall into two primary categories. Lengthcontrolled methods directly constrain generation through explicit mechanisms: prompts like think for tokens and corresponding length-control rewards in L1 (Aggarwal & Welleck, 2025); progressively limits on the models reasoning space during training in ThinkPrune (Hou et al., 2025); and Scalable Chain of Thoughts (Xu et al., 2025) enforces budget constraints through forced termination. Reward-based approaches incorporate length penalties into training objectives: HAPO (Huang et al., 2025) leverages history-aware optimization to track minimal sufficient reasoning lengths, while Think When You Need (Jiang et al., 2025) employs pairwise comparison rewards to trade off between quality and brevity. While effective at reducing token usage, these methods share key limitation: they prioritize efficiency at the cost of accuracy, lacking mechanisms for models to decide autonomously when longer or shorter reasoning is appropriate. First, We identify two key challenges that hinder existing methods from achieving genuine reasoning efficiency. length penalties introduce systematic training biases that impair reasoning capabilities. In standard reinforcement learning settings (DeepSeek-AI, 2025b), correct solutions receive equal rewards regardless of length, allowing for unbiased exploration. However, length penalties disrupt this balance by consistently favoring shorter outputs, leading policies to gradually abandon long-reasoning strategies (Hou et al., 2025; Huang et al., 2025; Lou et al., 2025). Second, static efficiency constraints fail to capture the continuous nature of reasoning complexity. Even adaptive methods rely on coarse mechanisms, such as binary think/no-think decisions (Zhang et al., 2025a; Fang et al., 2025) or fixed confidence thresholds (Qiao et al., 2025), which overlook the nuanced relationship between problem characteristics and computational requirements. These limitations raise fundamental question: rather than enforcing uniform constraints, can models learn differentiated reasoning strategies through structured exploration? This question motivates our study of hierarchical budget exploration, where efficiency emerges not from rigid control but from structured exploration within budget-constrained subspaces. We propose Hierarchical Budget Policy Optimization (HBPO), illustrated in the bottom-left part of Figure 1, reinforcement learning framework that enables models to learn problem-specific reasoning strategies while retaining their ability to perform complex reasoning. The core idea is to partition the exploration space into multiple budget-constrained subgroups, allowing models to preserve reasoning diversity and uncover natural alignments between problem characteristics and required computational effort. Specifically, HBPO employs hierarchical sampling strategy that partitions rollout samples into subgroups, each governed by distinct token budgets. We implement"
        },
        {
            "title": "Preprint",
            "content": "this by inserting length prompts (e.g., will answer the question within tokens) after the reasoning tag, thereby constructing multiple exploration spaces with budgets ranging from 512 to 2560 tokens. Unlike uniform sampling, this structure encourages the model to explore both concise and extended reasoning paths throughout training, effectively mitigating the systematic degradation of reasoning capabilities caused by global length penalties. To guide efficient reasoning within each budget hierarchy, we introduce piecewise reward function that combines the strengths of classical and cosine-shaped reward forms. Within the assigned budget, rewards are monotonically non-decreasing to preserve exploratory flexibility. Beyond the budget, cosine decay and length deviation penalties are applied to encourage the model to return to its designated exploration space. This design establishes differentiated incentives across subgroups: shorter budgets favor concise solutions with higher rewards, while longer budgets retain standard rewards for extended reasoning, enabling adaptive resource allocation in line with problem complexity. Our evaluation on four reasoning benchmarks shows that HBPO achieves superior trade-off between efficiency and accuracy compared to existing approaches, as the results shown in the right part of Figure 1. It consistently outperforms both length-controlled methods with hard constraints and adaptive methods based on discrete mode selection. More importantly, HBPO demonstrates genuinely adaptive behavior: it automatically adjusts token usage, ranging from hundreds for simple problems to thousands for complex reasoning tasks, indicating that the model has learned to allocate computational resources based on problem complexity. Our contributions are threefold: We introduce Hierarchical Budget Policy Optimization, reinforcement learning framework that partitions the exploration space into budget-constrained hierarchies with differentiated rewards, preserving reasoning diversity while enabling adaptive resource allocation. We demonstrate that uniform efficiency constraints systematically collapse the exploration space and degrade reasoning capabilities, validating the necessity of structured exploration for maintaining model performance. We provide evidence of emergent adaptive reasoning, where HBPO-trained models automatically adjust reasoning depth based on problem characteristics, achieving up to 60.6% reduction in token usage while improving accuracy by 3.14% across mathematical reasoning benchmarks."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 EFFICIENT REASONING Recent advances in reasoning models have spurred various efforts to reduce computational overhead while preserving performance. Existing approaches can be broadly categorized into three types: Length-constrained methods explicitly restrict generation through predefined mechanisms. For example, L1 (Aggarwal & Welleck, 2025) introduces token budget prompts with corresponding rewards; ThinkPrune (Hou et al., 2025) progressively tightens constraints via iterative training; and Scalable Chain of Thoughts (Xu et al., 2025) separates the thinking and solution phases, each with its budget. While effective in limiting token usage, these methods require manual budget specification and lack adaptability to varying problem complexity. Reward-based methods incorporate efficiency into training objectives more implicitly. HAPO (Huang et al., 2025) incentivizes concise reasoning by tracking minimal correct response lengths, while Think When You Need (Jiang et al., 2025) balances brevity and quality through pairwise comparisons and adaptive target lengths. These approaches offer finer control but still impose global objectives across diverse problem types, limiting flexibility. Training-free approaches (Muennighoff et al., 2025; Yang et al., 2025) intervene at inference time through symbolic control tokens or confidence-based early stopping. While costeffective, these methods are heuristic-driven and lack learning-based adaptation. Despite their differences, all these approaches share fundamental limitation: they treat efficiency as uniform constraint, overlooking the fact that optimal reasoning length varies significantly with problem complexity."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of Hierarchical Budget Policy Optimization. Given query, HBPO generates responses across multiple budget-constrained subgroups (512, 1024, 2048, 2560 tokens), each guided by piecewise reward function that preserves exploration within budgets while penalizing excess through deviation penalties. The advantage computation decomposes into intra-subgroup advantages (comparing responses against budget-specific baselines) and inter-subgroup advantages (enabling cross-budget learning through global comparison). This hierarchical structure enables models to learn efficient reasoning within constraints and adaptive budget selection based on problem complexity. 2.2 ADAPTIVE REASONING Recognizing heterogeneous reasoning requirements, recent work explores adaptive strategies that adjust computational effort based on problem characteristics. Binary mode selection represents the most common approach, with models choosing between thinking and non-thinking modes (Lou et al., 2025; Zhang et al., 2025a; Fang et al., 2025). These methods employ various techniques including selective loss masking, simplified mode definitions, and decoupled optimization to prevent mode collapse. Multi-stage training strategies (Jiang et al., 2025; Tu et al., 2025; Zhang et al., 2025b) use sophisticated reward designs and batch-level balancing to achieve better mode distributions. Beyond binary selection, multi-modal approaches define richer reasoning taxonomies: ARM (Wu et al., 2025) uses four modes with adaptive scaling, while PATS (Wang et al., 2025) enables steplevel switching between complexity levels. Some methods introduce auxiliary components like regression models for mode prediction (Liang et al., 2025) or self-budgeting mechanisms (Li et al., 2025). While these adaptive approaches demonstrate significant efficiency gains, they operate within discrete categories rather than enabling continuous adaptation. Complex multi-stage procedures and predefined mode taxonomies limit their flexibility and generalization. In contrast, our hierarchical budget exploration framework enables continuous adaptation through unified policy optimization process. Without relying on manually defined modes or external modules, our approach allows the model to learn problem-specific reasoning depths, leading to emergent adaptive behavior that naturally aligns computational effort with problem complexity."
        },
        {
            "title": "3 METHOD",
            "content": "We present Hierarchical Budget Policy Optimization, as shown in Figure 2, which extends the Group Relative Policy Optimization (GRPO) (DeepSeek-AI, 2025b) framework to enable adaptive reasoning through structured exploration. The core innovation lies in partitioning the exploration space into budget-constrained hierarchies and designing differentiated reward mechanisms that preserve reasoning diversity. We first introduce the hierarchical rollout strategy (Section 3.1), then"
        },
        {
            "title": "Preprint",
            "content": "detail the budget-aware reward design (Section 3.2)), and finally describe the training procedure (Section 3.3))."
        },
        {
            "title": "3.1 HIERARCHICAL BUDGET EXPLORATION",
            "content": "The fundamental challenge in efficient reasoning training is that uniform length penalties systematically bias models away from necessary long reasoning paths. To address this, we partition rollout samples into hierarchical subgroups, each operating within distinct token budget constraints. This structure ensures that models maintain exposure to diverse reasoning lengths throughout training. Given query q, we generate rollout samples and partition them into subgroups {G1, G2, ..., Gk}, where each subgroup Gi is associated with token budget bi. We implement this through budget-specific prompts embedded after the reasoning tag: will answer the question within bi tokens. The budget values form an ascending sequence (b1 < b2 < ... < bk), spanning from compact reasoning (e.g., 512 tokens) to extended deliberation (e.g., 2560 tokens). This hierarchical structure serves two key purposes. First, it prevents exploration space collapse, common issue in efficiency training where models abandon long reasoning. By preserving separate exploration spaces, HBPO ensures sampling across diverse reasoning lengths. Second, it enables structured comparative learning: the model discovers the suitable computation for each problem by contrasting performance across budget levels, rather than relying on global optimization. 3.2 BUDGET-AWARE REWARD DESIGN The effectiveness of hierarchical exploration hinges on careful reward design. Existing methods either use uniform rewardssupporting fair exploration but lacking efficiency incentivesor apply global length penalties, which improve efficiency at the cost of reasoning ability. HBPO addresses this trade-off with piecewise reward function that integrates the strengths of both approaches. 3.2. INTRA-BUDGET REWARD FUNCTION Within each budget-constrained subgroup, we design reward function that balances reason exploration and efficiency. For given budget b, the reward integrates length-based penalties f1 that promote token efficiency with classical rewards f2 that encourage diverse reasoning. The reward is formally defined as: R(ngen b) = f1(ngen, b), f2(b), 0, if correct, ngen > b, and ngen Lmax if correct, ngen b, and ngen Lmax otherwise where: f1(ngen, b) = β cos f2(b) = β cos (cid:18) πngen 2Lmax (cid:18) πb (cid:19) (cid:19) 2Lmax αngen (1) (2) (3) Here, ngen denotes the number of generated tokens, Lmax is the maximum context length, β is scaling factor, and α controls deviation sensitivity. The piecewise structure serves distinct purposes across different generation lengths. When ngen > b, the reward follows f1, incorporating both cosine decay and deviation penalty to guide the model back to its designated exploration space. When ngen b, the reward is bounded by f2, ensuring monotonic non-decreasing behavior that preserves exploration within the budget. 3.2.2 INTER-BUDGET REWARD DIFFERENTIATION The hierarchical structure naturally induces reward differentiation across budgets. For fixed generation length ngen, different budget assignments yield different rewards according to Equation 1, signaled as R(b ngen). This creates systematic preferences that align with problem complexity. When ngen < min(bi), all budgets yield rewards determined by f2, and smaller budgets receive higher rewards due to the monotonic decrease of the cosine function over the interval. This"
        },
        {
            "title": "Preprint",
            "content": "Sample batch of queries from training data for each query do Algorithm 1 Hierarchical Budget Policy Optimization (HBPO) Require: Initial policy πθ0, budget levels = {b1, ..., bk}, learning rate η 1: for iteration = 1, 2, ..., do 2: 3: 4: 5: 6: 7: 8: 9: end for for each subgroup Gi do for each budget bi do Generate n/k responses with prompt will answer within bi tokens Store responses in subgroup Gi Compute rewards {Ri,j} using Equation 1 Compute intra-subgroup mean reward: µi = 1 Gi Compute budget rewards Rbi using Equation 3 Compute intra-subgroup advantage: Aintra = µi Rbi (cid:80)Gi j=1 Ri,j end for Compute inter-subgroup advantage: Ainter Normalize final advantage: Ai,j = Aintra i,j = + Ainter i,j (cid:80) Ri,j 1 std(R) i,j Ri,j end for Update policy: θt+1 θt ηθL(θt) 10: 11: 12: 13: 14: 15: 16: 17: 18: end for preference for smaller budgets on short responses encourages efficiency for simple problems. Conversely, when ngen > max(bi), larger budgets provide higher rewards through smaller deviation penalties ngen bi in f1, preserving the models ability to engage in extended reasoning when necessary. As ngen increases from below min(bi) to above max(bi), the reward functions corresponding to different budgets transition in relative preference. The intersection points between reward curves represent complexity thresholds where the optimal budget choice transitions. Through comparative advantage across these differentiated rewards, the model learns to match computational resources to problem requirements without explicit complexity labels or external guidance. 3.3 TRAINING PROCEDURE HBPO extends the standard GRPO framework by incorporating hierarchical sampling and budgetaware advantage computation into the policy optimization process, the algorithm is shown in Algorithm 1. During each training iteration t, the model generates responses for given query, which are automatically partitioned into subgroups based on their associated budget constraints. Each response is generated with an embedded budget prompt will answer the question within bi tokens, where bi {b1, b2, ..., bk} represents the predetermined budget levels. The advantage computation leverages the hierarchical structure to enable both efficient reasoning within budgets and adaptive budget selection across problems. For the j-th response in the i-th subgroup, we compute the reward Ri,j using the budget-aware reward function described in Section 3.2. To capture the hierarchical nature of our exploration, we decompose the advantage into two complementary components that guide different aspects of learning. = µi Rbi, where µi = 1 Gi The intra-subgroup advantage measures how well responses perform relative to their budget expectation: Aintra j=1 Ri,j is the mean reward within subgroup i, and Rbi represents the budget-specific baseline computed using Equation 3. This term encourages optimization within each budget constraint, teaching the model to reason efficiently given specific token allocation. (cid:80)Gi The inter-subgroup advantage enables comparative learning across different budgets: Ainter i,j = Ri,j 1 (cid:80) i,j Ri,j std(R) 6 (4)"
        },
        {
            "title": "Preprint",
            "content": "This term compares each response against the global mean, creating natural preferences for budget selection. Responses from shorter budgets that achieve high rewards receive positive advantages, while unnecessarily long responses receive negative advantages, teaching the model to match computational effort to problem requirements. The final advantage combines both components with normalization for stable training: Ai,j = Aintra The policy optimization adopts GRPOs clipped objective to prevent destructive updates: + Ainter i,j (5) L(θ) = E(s,a)πθold [min (ρθ(s, a)A(s, a), clip(ρθ(s, a), 1 ϵlow, 1 + ϵhigh)A(s, a))] (6) where ρθ(s, a) = πθ(as)/πθold(as) represents the probability ratio. The hierarchical advantages Ai,j naturally flow through this objective, enabling the model to improve both within-budget efficiency and cross-budget selection without requiring separate optimization objectives or complex multi-stage training procedures."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Datasets and Models. We evaluate HBPO on mathematical reasoning tasks using the DeepScaleR dataset (Luo et al., 2025) for training, which comprises 40K high-quality mathematical problems from AIME, AMC, Omni-Math (Gao et al., 2025), and STILL (Min et al., 2024). We employ two base models: DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI, 2025a) and DeepScaleR-Preview1.5B (Luo et al., 2025). Implementation Details. We implement HBPO using the VeRL framework (Sheng et al., 2024) with context window of 4,096 tokens during training. Following DAPO (Yu et al., 2025), we set clipping thresholds ϵhigh = 0.28 and ϵlow = 0.2, with KL divergence disabled to encourage exploration. Training proceeds for one epoch (629 steps) with learning rate of 106 and batch size of 64. For hierarchical exploration, we generate 16 rollouts per query, partitioned equally into 4 subgroups with budget constraints = 512, 1024, 2048, 2560 tokens. Evaluation Protocol. We evaluate on four mathematical reasoning benchmarks of increasing difficulty: GSM8K (Cobbe et al., 2021), Math500 (Lightman et al., 2023), OlympiadBench (He et al., 2024), and AIME25. Following standard practice (DeepSeek-AI, 2025b), we use temperature = 0.6, top = 0.95, and maximum context length of 32,768 tokens. We report pass@1 accuracy and average token usage under two evaluation settings: (1) natural reasoning where models freely determine their computational effort, and (2) efficiency prompting using will answer the question with minimal tokens after <think> to guide models toward efficient responses. Baselines. We compare against several state-of-the-art efficient (1) TLMRE(Arora & Zanette, 2025), which adds length penalties to the RL objective with hyperparameter α controlling the penalty strength; (2) AdaptThink(Zhang et al., 2025a) and AutoThink(Tu (3) L1-Max(Aggarwal & et al., 2025), which enable binary think/no-think mode selection; Welleck, 2025), which uses two-stage RL with explicit length targets. These methods represent different approaches to reasoning efficiency: global penalties (TLMRE), discrete mode selection (AdaptThink, AutoThink), and explicit control (L1-Max). reasoning methods: 4.2 MAIN RESULTS Hierarchical training enables efficient reasoning without capability trade-offs. Tables 1 and 2 present our results under natural and efficiency-constrained settings, respectively. Under natural reasoning conditions, HBPO demonstrates consistent improvements across both base models. Applied to DeepSeek-R1-Distill-Qwen-1.5B, HBPO improves average accuracy from 56.3% to 59.4% while reducing token usage by 60.6% (from 7,921 to 3,120). On the stronger DeepScaleR model, HBPO maintains the baselines 63.7% accuracy while achieving 50.2% token reduction (from 4,744 to 2,364). Notably, HBPO achieves 31.1% accuracy on AIME25, outperforming"
        },
        {
            "title": "Preprint",
            "content": "the DeepScaleR baseline and all efficiency methods. This improvement on the most challenging benchmark while using fewer tokens demonstrates that hierarchical exploration not only prevents capability degradation but can enhance reasoning by eliminating computational redundancy. The efficiency prompting setting makes the performance gains from hierarchical training more evident. While baseline models suffer catastrophic degradation when forced to minimize tokens (over 10% accuracy drop), HBPO maintains robust performance. Applied to DeepScaleR, HBPO achieves 59.4% average accuracy with only 947 tokens, matching L1-Max (1024)s accuracy while using 32% fewer tokens. This indicates that our training enables effective exploration across the entire efficiency spectrum. Table 1: Performance under natural reasoning conditions. Models freely allocate computational resources based on learned strategies. Method GSM8K MATH Olympiad AIME25 Average Acc Tokens Acc Tokens Acc Tokens Acc Tokens Acc Tokens Base: DeepSeek-R1-Distill-Qwen-1.5B Baseline TLMRE (α=0.4) AdaptThink HBPO (Ours) Baseline AutoThink L1-Max HBPO (Ours) 82.3 74.6 85.0 84.5 86.1 85.9 86.1 87.6 1,111 221 816 670 1,684 1,420 670 790 81.6 69.8 79.6 80. 4,696 1,835 1,220 2,147 42.3 35.8 42.9 45.0 10,225 4,838 2,501 4,058 Base: DeepScaleR-Preview-1.5B 87.0 86.6 85.0 86.2 2,938 1,992 3,260 1, 51.6 52.7 48.2 50.0 5,330 4,463 3,094 2,861 18.9 17.8 18.9 27.8 30.0 27.8 22.2 31.1 15,651 9,753 6,813 5,606 9,023 8,620 3,163 3, 56.3 49.5 56.6 59.4 63.7 63.2 60.4 63.7 7,921 4,162 2,838 3,120 4,744 4,124 2,547 2,364 Table 2: Performance under efficiency prompting reasoning. Models are explicitly prompted to minimize token usage. Method GSM8K MATH500 Olympiad AIME25 Average Acc Tokens Acc Tokens Acc Tokens Acc Tokens Acc Tokens Base: DeepSeek-R1-Distill-Qwen-1.5B Baseline HBPO (Ours) 73.6 83.9 267 340 67.4 79.6 806 30.6 43.0 1,950 1,305 13.3 18.9 3,737 1,454 46.2 56.3 1,690 Base: DeepScaleR-Preview-1.5B Baseline L1-Max (512) L1-Max (1024) HBPO (Ours) 78.6 85.7 87.6 85.6 270 331 1,188 394 74.4 81.4 82.2 82.4 1,037 609 1,235 37.2 42.0 45.4 47.2 1,963 861 1,518 1,193 16.7 7.8 22.2 22.2 4,733 996 1,661 1,476 51.7 54.2 59.4 59.4 2,001 699 1,401 Adaptive behavior emerges from hierarchical training rather than explicit control. The distinction between HBPO and existing methods becomes evident in their token allocation patterns. L1-Max exhibits remarkably uniform behavior across problem difficulties, using 3,260 tokens on MATH500 and 3,163 tokens on AIME25 despite the significant complexity gap between these benchmarks. In contrast, HBPO demonstrates genuine problem sensitivity with token usage varying from 1,818 on MATH500 to 3,988 on AIME25. This 2.2 variation directly correlates with problem complexity and emerges naturally from the differentiated reward mechanism, which creates distinct optimization landscapes for different budget levels. Through comparative advantage across these landscapes, models learn to assess problem requirements without external guidance."
        },
        {
            "title": "5 ANALYSIS",
            "content": "5.1 ANALYSIS OF HIERARCHICAL STRUCTURE"
        },
        {
            "title": "Preprint",
            "content": "Optimal hierarchy emerges from balancing intragroup learning and inter-group exploration. To understand the impact of hierarchical structure on performance, we systematically analyze different budget configurations while maintaining constant average budget of 1,536 tokens. Table 3 reveals clear performance progression: single-budget training achieves only 59.8% average accuracy, demonstrating the limitations of uniform exploration. The performance improves to 61.7% with dual budgets and reaches an optimal of 63.7% with our 4-budget configuration. Figure 3: Training Dynamics: Mean Token Length (solid Line) Standard Deviation (dotted line). Table 3: Impact of hierarchical granularity on performance. The 4-budget configuration achieves optimal balance between and within-group learning and exploration diversity. Configuration Single (b=1536) Dual (b {512, 2560}) 4-budget 6-budget 8-budget GSM8K MATH500 Olympiad AIME25 Average Acc 85.6 86.4 87.6 87.0 87. Tokens Acc Tokens Acc Tokens Acc Tokens Acc Tokens 327 816 790 809 83.4 85.6 86.2 87.2 85.6 1,055 1,849 1,818 1,893 1,836 48.1 48.2 50.0 50.9 49.9 2,301 2,938 2,861 3,084 2,899 22.2 27.8 31.1 26.7 28.9 3,686 4,104 3,988 3,934 4, 59.8 61.7 63.7 62.9 62.9 1,842 2,427 2,364 2,430 2,405 Single-budget training reduces to traditional uniform sampling without inter-budget reward differentiation. Dual budgets introduce basic differentiation between short (512) and long (2,560) reasoning, improving accuracy by 1.9%. The 4-budget configuration achieves optimal performance by offering sufficient granularity for adaptive learning, while ensuring enough samples per subgroup to support effective intra-group optimization. Further increasing the number of budgets to 6 or 8 slightly degrades performance, with 0.8% drop, as fewer samples per subgroup weaken intragroup learning signals. This reveals fundamental trade-off: exploration diversity must be balanced with statistical reliability for effective policy learning. Training dynamics demonstrate how hierarchical structure maintains exploration space. Figure 3 shows the generation dynamics between HBPO and single-budget baseline. While singlebudget training converges to narrower range of responses, HBPO exhibits different dynamics. The average generation length stabilizes around 1,400 tokens, accompanied by broader yet controlled variance direct consequence of its hierarchical structure. This sustained variability is essential, as it captures the preservation of exploration diversity, which is crucial in preventing degradation of reasoning capability. HBPO achieves efficiency through adaptive resource allocation rather than uniform compression. As results shown in Table 4 and Figure 4, traditional GRPO with cosine reward achieves some efficiency (average 1,150 tokens) but suffers significant accuracy degradation, particularly on complex tasks where it achieves only 23.3% on AIME25. The model learns to generate universally short responses regardless of problem requirements, form of mode collapse that sacrifices capability for efficiency. Table 4: Comparison with traditional efficient reasoning methods under natural inference conditions. Method Classic Reward Cosine Reward HBPO(Budget-aware Reward) GSM8K MATH500 Olympiad AIME Acc 86.2 83.0 87.6 Tokens Acc Tokens Acc Tokens Acc Tokens 661 195 790 86.2 77.6 86.2 1,605 478 1,818 49.1 42.0 50.0 3,174 1,271 2,861 24.4 23.3 31. 4,309 2,657 3,"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Comparison of training dynamics and validation performance between cosine reward (Left) Training token length evolution shows that cosine and budget-aware reward methods. reward exhibits sharp decline in early training stage(0-100 steps), while budget-aware reward maintains relatively consistent token lengths throughout training. (Middle) Validation accuracy demonstrates that budget-aware reward achieves sustained performance improvements with higher final accuracy, whereas cosine reward shows greater volatility and inferior performance. (Right) Validation token count reveals that budget-aware reward enables the model to gradually discover the optimal reasoning length, while cosine reward suffers from excessive compression from the early training stages, leading to suboptimal token generation. HBPO encourages effective exploration during training by employing budget-aware rewards rather than uniform compression, enabling the model to identify optimal reasoning lengths for efficient inference more accurately. 5.2 REASONING PATTERN ANALYSIS HBPO develops different reasoning strategies based on problem complexity. To understand how models improve efficiency, we analyze reasoning patterns through two lenses: the proportion of exploratory thinking versus direct solution generation, and the frequency of reflection keywords that indicate deliberative processes. Figure 5 reveals striking differences between methods. Figure 5: Reasoning pattern analysis across methods and problem difficulties. Thinking proportions and reflection keyword frequencies show HBPOs adaptive adjustment, with keywords properly contained within thinking segments. HBPO exhibits clear adaptation to problem difficulty. The proportion of thinking content increases monotonically from 81% on GSM8K to 89% on AIME25, while reflection keywords (wait, alternatively, but, remember, check, and verify) rise from 6 to 30 occurrences per problem. This pattern supports our differentiated reward design, showing that the model learns to identify when longer reasoning adds value. L1-Max improves efficiency through uniform length control, maintaining nearly constant thinking proportions (90-92%) and keyword frequencies (29-32) across three datasets. This rigidity reveals mechanical optimization rather than intelligent adaptation. AutoThink attempts adaptive reasoning but exhibits problematic patterns: excessive thinking on simple problems (86% on GSM8K) and insufficient adjustment for complex ones. Moreover, AutoThink exhibits an average of 1.7 and 1."
        },
        {
            "title": "Preprint",
            "content": "reasoning-related keywords per problem in the solution segments on the MATH500 and Olympiad benchmarks, indicating that reasoning processes leak into what should be direct answers. The efficiency prompting setting provides further insight into adaptive capabilities. When instructed to minimize tokens, HBPO exhibits progressive keyword scaling (1.8 on GSM8K to 13.1 on AIME25), demonstrating that the model has internalized problem-complexity relationships. L1Max, when explicitly prompted to think for 1024 tokens, shows minimal variation (10.6 to 13.5), revealing its inability to differentiate between problem requirements even under explicit efficiency instructions. These patterns confirm that hierarchical training enables genuine adaptive reasoning rather than uniform optimization. Table 5: Performance on GPQA-Diamond Generalization to scientific reasoning validates domain-agnostic efficiency learning. To assess whether hierarchical exploration enables general efficiency principles rather than task-specific optimization, we evaluate on GPQA-Diamond, challenging scientific reasoning benchmark outside our training domain. Table 5 shows that HBPO maintains the highest accuracy (34.72%) while reducing token usage by 55% compared to baseline. This performance on out-of-distribution tasks demonstrates that hierarchical training teaches fundamental principles of computational resource allocation that transfer across reasoning domains. DeepScaleR L1-Max AutoThink HBPO 4,762 1,227 3,787 2, 33.84 33.33 34.41 34.72 Tokens Model Acc These analyses collectively demonstrate that HBPOs hierarchical exploration framework addresses the fundamental challenges in efficient reasoning. By maintaining exploration diversity through budget hierarchies and enabling adaptive learning through differentiated rewards, HBPO teaches models to recognize the computational requirements of different problems and allocate resources accordingly. The result is system that achieves efficiency not through constraint but through understanding."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced Hierarchical Budget Policy Optimization, framework that enables reasoning models to achieve efficient computation without sacrificing capability. By maintaining diverse exploration through budget-constrained hierarchies and budget-aware rewards, HBPO prevents the exploration collapse and an optimized allocation of the length budget. Our experiments demonstrate that models trained with HBPO significantly reduce inference costs while improving performance, exhibiting adaptive behavior that naturally matches computational effort to problem complexity."
        },
        {
            "title": "REFERENCES",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: controlling how long reasoning model thinks with reinforcement learning. CoRR, abs/2503.04697, 2025. doi: 10.48550/ARXIV.2503.04697. URL https://doi.org/10.48550/arXiv.2503.04697. Daman Arora and Andrea Zanette. Training language models to reason efficiently. CoRR, abs/2502.04463, 2025. doi: 10.48550/ARXIV.2502.04463. URL https://doi.org/10. 48550/arXiv.2502.04463. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chainof-thought for reasoning large language models. CoRR, abs/2503.09567, 2025. doi: 10.48550/ ARXIV.2503.09567. URL https://doi.org/10.48550/arXiv.2503.09567. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do NOT think that much for 2+3=? on the overthinking of o1-like llms. CoRR, abs/2412.21187, 2024. doi: 10.48550/ARXIV.2412.21187. URL https://doi.org/10.48550/arXiv.2412. 21187."
        },
        {
            "title": "Preprint",
            "content": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025a. doi: 10.48550/ARXIV.2501.12948. URL https://doi.org/ 10.48550/arXiv.2501.12948. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025b. doi: 10.48550/ARXIV.2501.12948. URL https://doi. org/10.48550/arXiv.2501.12948. Gongfan Fang, Xinyin Ma, and Xinchao Wang. Thinkless: LLM learns when to think. CoRR, abs/2505.13379, 2025. doi: 10.48550/ARXIV.2505.13379. URL https://doi.org/10. 48550/arXiv.2505.13379. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id= yaqPf0KAlN. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. CoRR, Thinkprune: Pruning long chain-of-thought of llms via reinforcement abs/2504.01296, 2025. doi: 10.48550/ARXIV.2504.01296. URL https://doi.org/10. 48550/arXiv.2504.01296. learning. Chengyu Huang, Zhengxin Zhang, and Claire Cardie. HAPO: training language models to reason concisely via history-aware policy optimization. CoRR, abs/2505.11225, 2025. doi: 10.48550/ ARXIV.2505.11225. URL https://doi.org/10.48550/arXiv.2505.11225. Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, and Furu Wei. Think only when you need with large hybrid-reasoning models. CoRR, abs/2505.14631, 2025. doi: 10.48550/ARXIV.2505.14631. URL https:// doi.org/10.48550/arXiv.2505.14631. Zheng Li, Qingxiu Dong, Jingyuan Ma, Di Zhang, and Zhifang Sui. Selfbudgeter: Adaptive token allocation for efficient LLM reasoning. CoRR, abs/2505.11274, 2025. doi: 10.48550/ARXIV. 2505.11274. URL https://doi.org/10.48550/arXiv.2505.11274. Guosheng Liang, Longguang Zhong, Ziyi Yang, and Xiaojun Quan. Thinkswitcher: When to think hard, when to think fast. CoRR, abs/2505.14183, 2025. doi: 10.48550/ARXIV.2505.14183. URL https://doi.org/10.48550/arXiv.2505.14183. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. Chenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu. Adacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning. CoRR, abs/2505.11896, 2025. doi: 10.48550/ARXIV.2505.11896. URL https://doi.org/10.48550/arXiv.2505.11896. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/"
        },
        {
            "title": "Preprint",
            "content": "DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL -19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning models can be effective without thinking. CoRR, abs/2504.09858, 2025. doi: 10.48550/ARXIV. 2504.09858. URL https://doi.org/10.48550/arXiv.2504.09858. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and JiRong Wen. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. CoRR, abs/2412.09413, 2024. doi: 10.48550/ARXIV.2412.09413. URL https: //doi.org/10.48550/arXiv.2412.09413. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel J. Cand`es, and Tatsunori Hashimoto. s1: Simple testtime scaling. CoRR, abs/2501.19393, 2025. doi: 10.48550/ARXIV.2501.19393. URL https: //doi.org/10.48550/arXiv.2501.19393. OpenAI. Learning to reason with llms. OpenAI Blog, 2024. URL https://openai.com/ index/learning-to-reason-with-llms/. Accessed: 2025-07-22. Ziqing Qiao, Yongheng Deng, Jiali Zeng, Dong Wang, Lai Wei, Fandong Meng, Jie Zhou, Ju Ren, and Yaoxue Zhang. Concise: Confidence-guided compression in step-by-step efficient reasoning. CoRR, abs/2505.04881, 2025. doi: 10.48550/ARXIV.2505.04881. URL https://doi.org/ 10.48550/arXiv.2505.04881. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao. Learning when to think: Shaping adaptive reasoning in r1-style models via multi-stage RL. CoRR, abs/2505.10832, 2025. doi: 10.48550/ARXIV.2505.10832. URL https://doi.org/ 10.48550/arXiv.2505.10832. Yi Wang, Junxiao Liu, Shimao Zhang, Jiajun Chen, and Shujian Huang. PATS: process-level adaptive thinking mode switching. CoRR, abs/2505.19250, 2025. doi: 10.48550/ARXIV.2505. 19250. URL https://doi.org/10.48550/arXiv.2505.19250. Siye Wu, Jian Xie, Yikai Zhang, Aili Chen, Kai Zhang, Yu Su, and Yanghua Xiao. ARM: adaptive reasoning model. CoRR, abs/2505.20258, 2025. doi: 10.48550/ARXIV.2505.20258. URL https://doi.org/10.48550/arXiv.2505.20258. Yuhui Xu, Hanze Dong, Lei Wang, Doyen Sahoo, Junnan Li, and Caiming Xiong. Scalable chain of thoughts via elastic reasoning. CoRR, abs/2505.05315, 2025. doi: 10.48550/ARXIV.2505.05315. URL https://doi.org/10.48550/arXiv.2505.05315. Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang. Dynamic early exit in reasoning models. CoRR, abs/2504.15895, 2025. doi: 10. 48550/ARXIV.2504.15895. URL https://doi.org/10.48550/arXiv.2504.15895. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an opensource LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. doi: 10. 48550/ARXIV.2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476. Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. Adaptthink: Reasoning models can learn when to think. CoRR, abs/2505.13417, 2025a. doi: 10.48550/ARXIV.2505.13417. URL https://doi.org/10.48550/arXiv.2505.13417."
        },
        {
            "title": "Preprint",
            "content": "Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Haodong Zhao, Hao Li, Jiansong Chen, Ke Zeng, and Xunliang Cai. When to continue thinking: Adaptive thinking mode switching for efficient reasoning. CoRR, abs/2505.15400, 2025b. doi: 10.48550/ARXIV.2505.15400. URL https://doi.org/10.48550/arXiv.2505.15400."
        }
    ],
    "affiliations": [
        "SF Technology",
        "Zhejiang University"
    ]
}