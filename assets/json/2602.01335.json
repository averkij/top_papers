{
    "paper_title": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning",
    "authors": [
        "Yu Xu",
        "Yuxin Zhang",
        "Juan Cao",
        "Lin Gao",
        "Chunyu Wang",
        "Oliver Deussen",
        "Tong-Yee Lee",
        "Fan Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the \"creative essence\" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar (\"G\"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available."
        },
        {
            "title": "Start",
            "content": "Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning Yu Xu1,2* Yuxin Zhang1* Chunyu Wang2 Oliver Deussen3 1University of Chinese Academy of Sciences Juan Cao1 Tong-Yee Lee4 Fan Tang1 2Tencent Hunyuan Lin Gao1 3University of Konstanz 4National Cheng-Kung University 6 2 0 2 1 ] . [ 1 5 3 3 1 0 . 2 0 6 2 : r Figure 1. Diverse image metaphor transfer results generated by our framework. For each pair, the left image serves as the Reference and the right is the Generated Result. Our model demonstrates robust capability across distinct cognitive levels."
        },
        {
            "title": "Abstract",
            "content": "A visual metaphor constitutes high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the creative essence from reference image and re-materialize that abWork done during internship at Tencent Hunyuan. * Equal contribution. Corresponding author. tfan.108@gmail.com stract logic onto user-specified target subject. We propose cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through novel Schema Grammar (G). This structured representation decouples relational invariants from specific visual entities, providing rigorous foundation for cross-domain logic reinstantiation. Our pipeline executes VMT through collaborative system of specialized agents: perception agent that distills the reference into schema, transfer agent that maintains generic space invariance to discover apt carriers, generation agent for high-fidelity synthesis and hierarchical diagnostic agent that mimics professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly out1 performs SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available. 1. Introduction Visual metaphor operates at the upper limits of human creative cognition, where meanings are constructed through the integration of disparate semantic domains, enabling abstract ideas to be conveyed as visually articulated statements with layered, non-literal significance. Despite the remarkable progress of generative AI, existing text-to-image (T2I) [3, 8, 24, 26, 35] and image-to-image [6, 22, 23, 37] models remain largely confined to pixel-level instruction alignment and the preservation of surface-level visual appearance, such as style, texture, or subjects, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. Transitioning from pixel-level reconstruction to metaphorical synthesis requires identifying deep-seated relational invariants across disparate domains and executing creative conceptual blending to induce emergent meaning. Lacking an innate perception of such creative logic, current models cannot independently distill the metaphorical essence from reference image and adapt it flexibly to novel contexts as humans do. Research on visual metaphors has traditionally evolved along two parallel trajectories: interpretation and synthesis. Multimodal Large Language Models (MLLMs) have demonstrated fundamental cognitive abilities for metaphor interpretation, yet they struggle to parse non-literal semantics and deep symbolic relationships embedded in complex visual rhetoric without additional information or prompts [1, 16]. Simultaneously, synthesis methods remain predominantly text-driven, relying on mapping linguistic metaphors onto concrete objects through extensive textual prompts [5, 31]. Despite their respective advancements, both paradigms converge on shared limitation: an over-reliance on explicit, user-provided textual descriptions. This dependency creates critical technical barrier to more sophisticated creative capabilitythe ability for autonomously decoupling the underlying metaphorical logic from visual reference and fluidly re-instantiating it within novel context. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT). Unlike conventional subject customization or style transfer that focuses on visual appearance, VMT necessitates deconstructing the creative essence from reference image and re-materializing that abstract logic onto user-specified target subject, as present in Fig. 1. This paradigm shift presents two formidable challenges that push the boundaries of current generative AI: (1) Explicit metaphor modeling, which requires distilling domain-independent relational invariants from raw pixels into structured representation; (2) Autonomous carrier adaptation, which demands the retrieval of novel visual vehicle that not only aligns with the target subjects attributes but also preserves the original cognitive tension to induce fresh emergence of meaning. Addressing these challenges requires transition from passive pixel synthesis to active, agentic visual reasoning. To address these challenges, we propose multi-agent framework for VMT, operationalizing Conceptual Blending Theory (CBT) [10, 11] from cognitive linguistics into an executable computational paradigm. Central to our approach is the Schema Grammar (G), novel structural representation that decouples abstract relational invariants from specific visual entities. By encoding the intricate interplay between subjects, carriers, and semantic violations, provides rigorous foundation for cross-domain logic reinstantiation. Specifically, our framework executes VMT through collaborative pipeline of specialized agents: (1) perception agent that distills the reference image into structured schema; (2) transfer agent that maintains generic space invariance to autonomously discover contextually apt carriers for new subjects; and (3) generation agent that translates these logic blueprints into structured prompts for high-fidelity synthesis. Crucially, we introduce hierarchical backtracing mechanism within diagnostic agent, which mimics professional critic by identifying the root causes of failure across abstract logic, component selection, or prompt encoding. This closed-loop refinement ensures that the final output transcends mere pixel-level consistency to achieve profound logical alignment. Our main contributions are summarized as follows: Cognitive-Logic Formalization: We operationalize conceptual blending theory (CBT) by proposing the Schema Grammar representation, providing rigorous cognitive science foundation for cross-domain carrier matching and metaphor synthesis. Closed-Loop Multi-Agent Framework: We develop collaborative system encompassing perception, transfer, generation, and diagnostics. Notably, the proposed hierarchical backtracing mechanism significantly enhances generation reliability for complex metaphorical tasks. Superior Experimental Performance: Our method outperforms existing baselines in terms of metaphor consistency, analogy appropriateness, and visual creativity. 2. Related Work 2.1. Visual Metaphor Understanding and Generation Visual metaphors serve as powerful rhetorical devices that convey abstract concepts through symbolic imagery. Un2 like subject customization [2, 12, 27, 34, 36, 40] or style transfer [13, 39, 41, 42] approaches that focus on preserving visual appearance, metaphor understanding and generation requires capturing abstract symbolic relationships that convey meaning beyond literal visual similarity. MetaCLUE [1] shows that vision-language models struggle with metaphor understanding compared to literal images. Building on this foundation, metaphor understanding works primarily focus on multimodal contexts, employing techniques including linking metaphor text to visual concepts with prompting [33], concept drift mechanisms [25], and prompt optimizer with reinforcement learning [9] to detect and interpret metaphorical content. Complementing understanding approaches, recent works explore generating visual metaphors through text-driven synthesis. Ispy-a-metaphor [5] propose humanLLMdiffusion collaboration framework for generating visual metaphors from linguistic metaphors. Creative Blends [31] proposes an AI-assisted system that uses commonsense knowledge and LLMs to map abstract concepts to concrete objects and blend them via T2I models. TIAC [19] proposes framework that maps abstract concepts to clear intents and semantic objects via LLMs to generate concept-aligned images. Minds eye [15] further explore self-evaluating visual metaphor generation framework with reinforcement these text-driven methods generate learning. However, metaphors from linguistic input rather than learning from visual examples, requiring explicit textual specification of metaphorical concepts, which limits their ability to extract and transfer reusable metaphorical representations across different visual contexts. In contrast, our method analyzes the core semantic meaning of metaphors and leverages the generic space from conceptual blending theory to transfer metaphorical representations to new subjects, achieving high-fidelity image-driven metaphor transfer. 2.2. Image Generation with Multimodal LLMs Recent advances in multimodal large language models [4, 6, 17, 23, 29] demonstrate remarkable capabilities in understanding and reasoning across vision and language modalities. Building on these foundations, many image generation tasks leverage multimodal LLMs to decompose complex generation objectives into specialized subtasks and employ multi-agent frameworks to extend their functionality [28, 30]. For instance, SketchAgent [32] utilizes LLMs with in-context learning to generate SVG strings that are subsequently rendered into sketches. MCCD [18] proposes multi-agent scene parsing and hierarchical compositional diffusion framework to achieve image generation for complex multi-object prompts. However, these methods primarily adopt sequential execution paradigm where agents operate in feed-forward manner without retrospective analysis. In contrast, our method introduces critic module that Figure 2. Conceptual Blending Theory. traces back to evaluate the output of each agent in previous steps and performs targeted refinement, enabling iterative improvement and achieving more faithful metaphor transfer that aligns with the intended symbolic meaning. 3. Computational Modeling of Visual"
        },
        {
            "title": "Metaphors",
            "content": "To bridge the gap between human cognitive creativity and metaphorical transfer, we formalize the task by operationalizing conceptual blending theory (CBT). This section provides the theoretical foundation and the structural representation required for agentic reasoning. 3.1. Conceptual Blending Spaces Conceptual blending theory as proposed by Fauconnier and Turner [10, 11] posits that human creativity arises from the integration of disparate mental spaces to generate novel meanings. metaphor is not simple linear mapping but dynamic integration of four mental spaces as illustrated in Fig. 2: (1) Input Spaces contain the specific entities that provide the raw content for the blend. Elements between these spaces are often linked by counterparts. (2) The Generic Space captures the abstract, domain-independent relational invariants shared by both input spaces. This way it captures the underlying logic (e.g., specific roles, frames, or schemata) that allow mapping between inputs. (3) The Blended Space is where elements from the inputs are selectively projected and integrated. Through composition, completion, and elaboration, this space gives rise to emergent structuresnew relations and meanings that exist in neither of the input spaces alone. 3.2."
        },
        {
            "title": "Structured Representation",
            "content": "of Visual"
        },
        {
            "title": "Metaphoric",
            "content": "We bridge the gap between psychological operations and computational reasoning by mapping the aforementioned spaces into structured schema grammar (G). We represent visual metaphor as 7-tuple = {S, C, AS, Aes, G, V, I}, where each element operational3 Figure 3. Architecture of our Self-Reflective Agentic Framework for Visual Rhetoric Transfer. The system consists of Perception, Transfer, Generation, and Diagnostic agents. It transforms reference visual metaphor (Iref ) into new target context (Igen) by extracting and mapping structured graph representations (Gref Gtgt). hierarchical feedback loop ensures the generated output faithfully preserves the rhetorical logic while adapting to the new subject matter. izes specific component of the blending process: 3.3. Task Formulation Entity instantiation ({S, C, AS}): The content of the input spaces is represented by the subject (the primary entity being depicted, S) and carrier (the visual context or metaphorical vehicle providing the interpretive framework, C). The goal of visual metaphor is to embed the subject into the carriers domain. We further propose inherent attributes (AS) as the canonical properties of in its original domain, serving as the baseline for identifying deviations. We also propose (Aes) to capture the visual expression attributes of the entire image. Relational bridging ({G}): The generic space ({G}) acts as the logical invariant that connects disparate domains. The domain-independent relational structure is shared by and C, which may be functional, structural, relational, or emotional in nature. Synthesis operationalization ({V, I}): The selective projection in the blended space is realized through violation point ({V })the specific semantic incongruities where transgresses the expected norms of which is derived by analyzing conflicts between AS and G. The resulting Emergent Meaning (I) captures the high-level creative logic induced by the cognitive tension of these violations. By decoupling the relational logic (G, V, I) from specific visual entities (S, C), our schema grammar (G) allows us to manipulate the creative essence of metaphor as structured representation. Based on G, we formulate the visual metaphor transfer task as learning mapping function that migrates reference logic to new subject. Given reference schema Gref and target subject Stgt, the framework must synthesize target schema Gtgt such that: M(Gref , Stgt) Gtgt, s.t., Gtgt Gref . (1) In this paradigm, transfer is successful if Gtgt preserves the abstract relational logic of the reference while autonomously discovering novel carrier Ctgt and violations Vtgt that are contextually appropriate for the new subject. This formalization transforms VMT from pixellevel reconstruction problem into structured search-andinstantiation task within the space of Schema Grammars. 4. Method In this section, we present our framework for metaphorical transfer Our approach decomposes the complex cognitive process of creativity into four sequential executable stages: (1) Perception Agent for universal schema extraction, (2) Transfer Agent for cross-domain schema synthesis, (3) Generation Agent for visual realization, and (4) Diagnostic Agent for iterative quality refinement. The overall architecture is illustrated in Fig. 3. 4 4.1. Perception Agent We employ Vision-Language Model (VLM) and guide it through chain-of-thought (CoT) reasoning following the sequence: S/C AS I. The model first identifies the concrete entities and their inherent properties, then performs abstract reasoning to isolate the Generic Space, this way uncovering the relational invariants that enable metaphorical mapping. By contrasting AS against G, the model derives Violation Points that create cognitive tension, from which the Emergent Meaning is finally inferred as the creative message. Formally, this extraction process can be expressed as: Gref = VLM(Iref , pextract), (2) where pextract denotes the extraction-specific system prompt that guides the structured reasoning chain. This structured decomposition transforms an implicit creative concept into an explicit, manipulable representation. By isolating as the domain-independent relational core, we establish the foundation for cross-domain the same abstract logic can be remetaphor transfer: instantiated with different subjects and carriers. 4.2. Transfer Agent Given the extracted reference schema grammar Gref from the first step and user-specified target subject Stgt, we aim to synthesize new schema grammar Gtgt that preserves the abstract relational logic while re-grounding it in different conceptual domain. This transfer is also achieved through VLM-guided reasoning process that ensures the Generic Space remains invariant across domains. a) Transfer Objective: The goal is to generate Gtgt = {Stgt, tgt, Atgt es , G, tgt, tgt} where is preserved from Gref , , Atgt while all other components are re-instantiated to maintain context coherence with the new subject. This ensures that the transferred metaphor conveys an analogous creative message through distinct visual configuration. b) Reasoning process: We prompt the VLM to perform relational reasoning through the following chain-of-thought sequence: Domain-Independent Isolation: Deeply analyze from Gref to identify its domain-independent nature. Target Profiling: Identify the inherent attributes Atgt and typical functional or symbolic roles of Stgt in its original domain. Bridge Mapping: Search for new visual carrier tgt from different domain than Stgt that shares the exact same Generic Space relationship. Violation Synthesis: Design specific conflict points tgt where Stgt transgresses the expected norms of tgt, mirroring the violation logic ref from the reference. Meaning Alignment: Ensure the emergent meaning Itgt remains consistent in its metaphor while being regrounded in the target domains context. This transfer process is formalized as: Gtgt = VLM(Gref , Stgt, ptransf er), (3) where ptransf er specifies the relational reasoning chain and the invariance constraint. This structured transfer mechanism generates complete schema grammar that serves as the conceptual blueprint for visual synthesis. By constraining to remain invariant, we ensure that the transferred metaphor maintains the same metaphorical logic as the reference, while the newly configured components (C tgt, tgt, tgt) provide domainspecific instantiation. The resulting (Gtgt) provides explicit guidance for subsequent image generation. 4.3. Generation Agent Given the synthesized target schema Gtgt, we generate the corresponding visual metaphor by LLMs. Specifically, the LLM translates the structured components of Gtgt into high-fidelity descriptive prompt , conditioned on taskspecific system prompt pgeneration: = LLM(Gtgt, pgeneration). (4) The prompt construction emphasizes three key principles: (1) Structural Anchoring: utilizing Ctgt to define the spatial composition and scene layout, (2) Semantic Juxtaposition: explicitly articulating the violation Vtgt to induce conceptual dissonance, and (3) Affective Encoding: manifesting the emergent meaning Itgt through stylistic directives such as lighting, color palette, and cinematic atmosphere. This structured translation ensures the image generation model captures the nuanced conceptual blend rather than merely rendering isolated objects. Finally, the target metaphoric image Igen is synthesized via pre-trained image generation model Gen: Igen = Gen(P ). (5) 4.4. Diagnostic Agent. The initially generated image Igen may exhibit quality deficiencies due to limitations in prompt expressiveness or conceptual misalignments in the schema transfer process. To address this, we introduce VLM-based diagnostic agent that performs qualitative analysis and guides iterative refinement. a) Diagnostic dimensions. The VLM examines Igen across four complementary dimensions: Subject Salience: assessing whether Stgt is recognizable and retains its core attributes Atgt; Violation Realization: verifying whether tgt is visually explicit and structurally coherent 5 Relational Coherence: the Generic Space is successfully instantiated such that viewers can immediately perceive the metaphorical relationship; and determining whether Meaning Alignment: checking whether the emergent meaning conveyed by Igen matches the intended tgt without introducing negative ambiguities. Rather than producing numerical scores, the VLM outputs qualitative descriptions of identified issues (e.g., the carriers iconic geometry is obscured by texture blending). b) Hierarchical backtracking and refinement. Based on diagnostic findings, we perform cascaded attribution through three levels. First, we examine whether the T2I prompt accurately translates Gtgt into generative instructions. Common prompt-level issues include insufficient specification of tgts iconic features, ambiguous spatial relationships for tgt, or misaligned atmospheric encoding. If prompt refinement (e.g., reinforcing geometric keywords, adding negative prompts) resolves the issue, we regenerate with Prevised. If problems persist, we trace back to Gtgt itself, assessing whether tgt genuinely shares G, whether tgt is visually realizable, or whether the domain gap between Stgt and tgt is bridgeable. Component-level revisions may include searching for alternative carriers or redesigning violation configurations. In rare cases where transfer consistently fails, we revisit Gref to verify whether was extracted at an appropriate abstraction level. This iterative refinement can be formulated as: If inal = Refine(Igen, Gtgt, pcritic; τ ), (6) where pcritic denotes the diagnostic prompt, and τ represents the iteration threshold. This hierarchical strategy ensures that corrections target the actual error source rather than over-adjusting downstream components. The refinement loop continues until diagnostic feedback indicates satisfactory quality or maximum iteration limit is reached, yielding the final output If inal. 5. Experiments In this section, we first introduce the experiment settings in Sec. 5.1, and the present qualitative, quantitative and human evaluation results in Sec. 5.2, Sec. 5.3 and Sec. 5.4. Finally, we conduct ablation study and generalizability analysis in Sec. 5.5 and Sec. 5.6. 5.1. Experimental settings Baselines. We compare our approach against state-of-theart multimodal image generation models with integrated visual understanding and reasoning capabilities, as metaphor transfer inherently requires analyzing the source images creative concept before generating the target. We evaluate models including BAGEL-thinking [7], Midjourneyimagine [21], GPT-Image-1.5 [23], and Gemini-bananapro [6]. Datasets. We curated diverse dataset of 126 visual metaphors from the internet, including product ads (32), memes (33), film posters (15), comics (10), and other creative works (36). This heterogeneous collection spans multiple domains to comprehensively test our frameworks generalization across various metaphorical styles and compositions. Metrics. Unlike conventional image evaluation methods such as CLIP [14] or DINO [20] that primarily assess lowlevel visual features or semantic similarity, metaphor transfer requires evaluating high-level conceptual reasoning and abstract creative alignment, which necessitates the use of VLMs capable of understanding complex analogical relationships. We employ three frontier VLMs, Gemini-3-pro, GPT-5.2, and Claude-Sonnet-4.5 to assess generated images across multiple dimensions using 10-point scales : (1) Metaphor Consistency (MC), which measures whether the target metaphor preserves the core metaphor logic of the source; (2) Analogy Appropriateness (AA), which evaluates the validity of functional and formal correspondences between the carrier and target subject; and (3) Conceptual Integration (CI), which assesses whether the fusion between the subject and carrier appears natural and harmonious. We also evaluate image aesthetic quality with SigLip-based predictor [38] to ensure visual appeal. Note that these evaluation VLMs are distinct from the VLM used in our iterative refinement process (Section 4.4), ensuring independent assessment of generation quality. The complete VLM evaluation prompts and the validation of VLM-as-judge reliability are provided in the supplementary material. Implementation. We employ Gemini-3-pro as both the VLM and LLM in our pipeline, and utilize Banana-pro for image generation. The iteration threshold τ is set to 5 to balance refinement quality and computational efficiency. pextract, ptransf er, pgeneration and pcritic are provided in the supplementary material. 5.2. Qualitative comparisons As shown in Fig. 4, our method excels at decoupling abstract creative logic from source domains and rematerializing it within novel targets. While SOTA models like GPT-Image and Banana-Pro are visually proficient, they rely on surface-level manipulation rather than decoding underlying metaphoric logic. For instance, in the American Fries task, these baselines merely substitute components without grasping the regional architectural landmark with similar shape metaphor, whereas in the Rose Hand Cream case, they erroneously preserve the sliced geometry from the reference, which is semantically incongruent with the new subject. This conceptual deficiency extends to structural integration: in the Crab example, baseFigure 4. Qualitative comparison with baseline methods. lines scatter trash as background clutter instead of merging it into the organisms anatomy, and in the Child scene, they fail to project powerful shadow to convey the intended dream big message. Furthermore, models like Bagel and Midjourney tend to generate results from scratch, leading to loss of metaphoric alignment. In contrast, our method achieves superior semantic reasoning by successfully synthesizing New York landmarks that mirror the geometric form of fries while signifying their origin, blending organic rose textures with traditional packaging, and seamlessly embedding plastic waste into the crabs biological structure. By accurately mapping abstract relationships, our approach demonstrates unique capacity to re-materialize abstract creative intents while ensuring high-level conceptual consistency. 5.3. Quantitative comparisons dictor. Notably, we achieve the most significant improvement in the AA metric (a 16.8% increase over the runnerup) demonstrating that our proposed Metaphor Transfer Agent effectively identifies metaphorically consistent visual carriers that best match new subjects. Beyond AA, our approach maintains superior scores in MC and CI, while also securing the highest aesthetic score (5.68). This consensus among evaluators (Gemini, GPT, and Claude) underscores our frameworks robustness in generating logically sound and visually harmonious metaphorical images without sacrificing artistic quality. 5.4. Human evaluation study To validate the perceptual quality and creative effectiveness between our method and the baselines, we conduct comprehensive human evaluation study with 65 participants (age 14-55, 32 male, 33 female), comprising two tasks. As Tab. 1 shows, our method consistently outperforms all baselines across three frontier VLMs and the aesthetic preIn Task 1, each participant evaluates 20 images generated by each method (ours and 4 baselines, totaling 100 7 Methods BAGEL Midjourney GPT-Image Banana-pro Ablation 1 Ablation 2 Ablation 3 Ours Gemini-3-pro MC AA CI MC 6.21 5.17 6.33 5.33 7.71 8.08 7.95 8.75 8.13 8.79 8.33 8.91 8.44 9.14 8.62 9.31 4.55 5.57 7.59 7.68 8.03 8.09 8.47 8.97 5.05 6.09 7.47 7.33 7.63 7.58 8.33 8.76 GPT-5.2 AA 5.83 6.46 7.65 7.77 7.96 8.01 8.33 8.51 CI MC 6.05 6.07 6.51 6.24 7.95 7.54 8.08 7.37 8.44 7.69 8.56 7.71 8.62 8.29 8.73 8.58 Claude-4.5 AA 5.58 5.94 7.39 7.42 7.85 7.89 8.38 8. CI 5.95 6.06 7.51 7.74 7.92 7.97 8.19 8.36 Aes. 4.77 5.22 5.63 5.57 5.59 5.61 5.63 5.68 Table 1. Quantitative evaluation Results. Ablation 13 denote variants without CBT and Phases 12, without CBT, and without Phase 4, respectively. Best results in bold. Figure 5. Human evaluation study. images per participant) independently along five dimensions using 5-point Likert scales: (1) Metaphor Recognizability (MR); (2) Metaphor Ingenuity (MI); (3) Violation Appropriateness (VA); (4) Visual Integration (VI); and (5) Overall Visual Quality (VQ). The detail of definitions are in the supplementary material. As shown in the left of Fig. 5, our method consistently outperforms all baselines across all five dimensions. Notably, it achieves significant lead in MI (4.57) and VA (4.45), indicating that our framework produces more creative and purposeful metaphorical designs than current SOTA models like Banana-pro and GPT-Image. Furthermore, our approach attains the highest scores in VI (4.64) and VQ (4.77), confirming that our focus on conceptual reasoning does not compromise aesthetic fidelity. These results collectively demonstrate our methods superior capability in synthesizing metaphorical images. In Task 2, we conduct GSB (Good/Same/Bad) evaluation to assess user preference by asking participants which image in each pair delivered more compelling metaphorical message. As shown in the right of Fig. 5, our method is consistently favored by participants, securing over 60% Ours Better ratings across all baseline comparisons. Notably, our approach outperforms strong commercial competitors such as GPT-Image and Banana-pro in 63.54% and 61.85% of cases, respectively, while being judged as inferior in fewer than 10% of pairs. This preference margin widens further against Midjourney (71.54%) and BAGEL (76.15%), demonstrating our frameworks metaphorical syntheses are significantly more resonant and conceptually Figure 6. Qualitative comparison of ablation variants. w/o CBT & Phase 1,2 (Nano-Banana-Pro [6]) performs naive object replacement. w/o CBT fails to perform complex carrier migration. w/o Phase 4 exhibits specific agent failures. The Full Model correctly reasons that coffee acts as battery, rope represents hair texture, and the ashtray demonstrates the consequences of smoking via dual-panel layout. effective than existing SOTA models. 5.5. Ablation study We conduct qualitative and quantitative comparison across different ablation variants, as illustrated in Fig. 6 and Tab. 1. Using VLM model for I2I understanding and generation (w/o CBT and PAHSE 1, 2). Using Nano-Banana-Pro [6] with the prompt: Understand this advertisement image, analyze its metaphors and creative ideas, and transfer this creative idea to the new product. Without core reasoning and preparatory phases, the model regresses to literal object Figure 7. Qualitative comparison of different backbone combinations. We validate the frameworks generalizability by pairing different LLMs (Gemini, GPT) with various T2I models (Nano-Banana, GPT-Image, FLUX). replacement, failing to grasp underlying metaphors: Row 1 (Coffee): It swaps pills and pillows for coffee and beans. Row 2 (Hair Conditioner): By replacing the starfish and leg with hair, the model loses the cross-domain analogy. Row 3 (Quit Smoking): The model mimics the tissue boxs shape but ignores its mechanism, i.e. the resource depletion logic of tissue extraction. Quantitative results also show significant decrease in MC, AA and CI scores, indicating that the model fails to correctly understand metaphors. Impact of the reasoning module (w/o CBT). Retaining Phases 1-4 without the CBT module yields plausible but generic outputs lacking structural creativity and complex carrier migration. Row 1 (Coffee): The model adopts generic office scene, discarding the references unique source-to-recipient composition. Row 2 (Hair Conditioner): Using dried cactus creates visually disjointed transition. Row 3 (Quit Smoking): It outputs cigarette pack with action-consequence causality. As quantitative results show that compared to the full model, the AA score decrease significantly, indicates that ablating the CBT module makes it hard to find suitable carrier context. Impact of diagnostic agent (w/o phase 4). Phase 4 agents are crucial for rectifying semantic and structural hallucinations. Perception (Row 1): The model is misled by the pill context to generate an IV drip (implying sickness); the Full Model correctly selects battery pack. Transfer (Row 2): Without precise carrier selection, it produces an ambiguous cracked artifact. Generation (Row 3): Ignoring structural constraints, it generates single-view ashtray instead of the Full Models side-by-side Before vs. After layout. Quantitative experiments also show that ablating this module resulted in certain degree of decrease in various scores. Full model. The full model correctly identifying the battery metaphor, selecting rope for textural analogy, and enforcing dual-panel structures. This demonstrates each components necessity for high-quality creative synthesis. 5.6. Generalizability analysis To verify the robustness and model-agnostic nature of our framework, we evaluated its performance across different combinations of LLMs and T2I generators. As shown in Fig. 7, we tested two distinct reasoning backbones (Gemini, GPT-4) paired with three diverse rendering engines (NanoBanana-Pro, GPT-Image, FLUX). Consistency in metaphorical mapping. The top row illustrates the LEGO scenario, where the core metaphor contrasts disorganized effort with total inaction. Regardless of the T2I model used, the framework successfully translates the references missed target concept into chaotic LEGO pile, and the total inaction concept into an empty baseplate. This consistency proves that our method effectively preserves the logical structure of the metaphor across different visual generators. Diversity in narrative interpretation. The bottom row (Protecting Forests) highlights how different LLMs influence the creative narrative while maintaining visual coherence. Gemini-driven variants (Left): The LLM interprets the role reversal as an act of active retaliation, prompting scenes where anthropomorphic trees use axes to demolish urban buildings. GPT-driven variants (Right): The LLM interprets the reversal as cultural satire, depicting trees in civilized setting displaying chainsaws as hunting tro9 Figure 8. Badcase. Cognitive Overload and Obscure Symbolism. phies. Despite these narrative divergences, all T2I models faithfully render the respective prompts. This demonstrates that our framework allows for creative flexibility in the reasoning stage while ensuring high-fidelity visual execution in the generation stage. 5.7. Badcase The badcase of our method lies in the excessive cognitive barrier required to decode certain migrated metaphors, which may hinder instantaneous communication. As shown at the bottom of Fig. 8, the Achilles Heel allusion for band-aid advertisement relies heavily on the viewers specific cultural background. Without recognizing the mythological context, the symbolic ultimate protection is reduced to literal historical injury, losing its persuasive power. Similarly, the starved Siren metaphor for noisecanceling headphones necessitates an exhaustive multi-step logical inference (Sirens song attraction predation noise-blocking starvation). These instances highlight the trade-off between semantic depth and cognitive immediacy. In such cases, the agentic reasoning may prioritize logical completeness over ease of interpretation, yielding increase the viewers decoding effort. 5.8. Applications Commercial product advertisements In the realm of commercial advertising, our framework facilitates the automated synthesis of high-impact visual metaphors by mapping product attributes onto novel creative carriers, as presented in Fig 9. The system provides significant versatility, supporting both text-based descriptions and image-based references as the driving subjects for promotional design. By precisely inducing Category Violations to stimulate Emergent Meaning, our approach ensures high-fidelity visual results while establishing an end-to-end pipeline for the efficient production of narrative-driven marketing content across various digital platforms. Meme generation The proposed framework demonstrates significant potential in the automated generation of internet memes, creative domain where communicative impact and humor are deeply rooted in profound visual metaphors and cognitive dissonance, as shown in Fig. 10. Figure 9. Versatility of the proposed framework in referenceguided and text-guided scenarios. Our method can flexibly handle both visual-to-visual and text-to-visual creative workflows. Figure 10. Application of meme image generation. By precisely extracting the Generic Space from canonical meme templates, our approach facilitates the seamless transfer of underlying logical mechanisms and satirical intent to emerging specific target entities while preserving the structural integrity of the original metaphorical framework. Consequently, the system achieves nuanced Category Violations that enhance both visual wit and compositional coherence, establishing robust technical pipeline for highfidelity, context-aware content synthesis and personalized expression in digital social media. 10 6. Conclusions We introduced visual metaphor transfer, task that goes beyond pixel-level editing by extracting the underlying metaphor logic from reference image and re-instantiating it on user-specified new subject. To achieve this, we formalize metaphor structure with Schema Grammar and construct closed-loop multi-agent pipeline comprising: Perception Agent that extracts the schema, Transfer Agent that preserves the Generic Space while finding new carrier, Generation Agent that turns the schema into prompts, and Diagnostic Agent that backtraces failures across prompt, component, and abstraction levels. Experiments show that this design improves metaphor consistency, analogy appropriateness, and conceptual integration compared with baselines."
        },
        {
            "title": "References",
            "content": "[1] Arjun Akula, Brendan Driscoll, Pradyumna Narayana, Soravit Changpinyo, Zhiwei Jia, Suyash Damle, Garima Pruthi, Sugato Basu, Leonidas Guibas, William Freeman, et al. 2023. Metaclue: Towards comprehensive visual metaphors research. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2320123211. 2, 3 [2] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. 2023. Break-a-scene: Extracting multiple concepts from single image. In SIGGRAPH Asia 2023 Conference Papers. 112. 3 [3] blackforestlabs.ai. 2024. FLUX, offering state-ofthe-art performance image generation. https:// blackforestlabs.ai/. Accessed: 2024-10-07. 2 [4] Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, et al. 2025. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951 (2025). 3 [5] Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna ApidI spy ianaki, and Smaranda Muresan. 2023. metaphor: Large language models and diffusion models co-create visual metaphors. arXiv preprint arXiv:2305.14724 (2023). 2, [6] Google DeepMind. 2025. Gemini 3 Pro Image (Nano advanced AI image generation and Banana Pro): editing model. urlhttps://deepmind.google/models/geminiimage/pro/. 6, 8 [Online; accessed 2026-01]. 2, 3, [7] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. 2025. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683 (2025). 6 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for highresolution image synthesis. In Forty-first International Conference on Machine Learning. [9] Zezhong Fan, Xiaohan Li, Kaushiki Nag, Chenhao Fang, Topojoy Biswas, Jianpeng Xu, and Kannan Achan. 2024. Prompt optimizer of text-to-image diffusion models for abstract concept understanding. In Companion Proceedings of the ACM Web Conference 2024. 15301537. 3 [10] Gilles Fauconnier and Mark Turner. 1998. Conceptual 12 integration networks. Cognitive science 22, 2 (1998), 133187. 2, 3 [11] Gilles Fauconnier and Mark Turner. 2003. Conceptual blending, form and meaning. Recherches en communication 19 (2003), 5786. 2, 3 [12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. 2023. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. In The Eleventh International Conference on Learning Representations. [13] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. 2024. Style aligned image generation via shared attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 47754785. 3 [14] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Luddoi:10.5281/ wig Schmidt. 2021. OpenCLIP. zenodo.5143773 6 [15] Girish Koushik, Fatemeh Nazarieh, Katherine Birch, Shenbin Qian, and Diptesh Kanojia. 2025. The Minds Eye: Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation. arXiv preprint arXiv:2508.18569 (2025). 3 [16] Manishit Kundu, Sumit Shekhar, and Pushpak Bhattacharyya. 2025. Looking Beyond the Pixels: Evaluating Visual Metaphor Understanding in VLMs. In Findings of the Association for Computational Linguistics: EMNLP 2025. 2313723158. 2 [17] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning. PMLR, 1288812900. 3 [18] Mingcheng Li, Xiaolu Hou, Ziyang Liu, Dingkang Yang, Ziyun Qian, Jiawei Chen, Jinjie Wei, Yue Jiang, Qingyao Xu, and Lihua Zhang. 2025. MCCD: MultiAgent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1326313272. [19] Jiayi Liao, Xu Chen, Qiang Fu, Lun Du, Xiangnan He, Xiang Wang, Shi Han, and Dongmei Zhang. 2024. Text-to-image generation for abstract concepts. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 33603368. 3 [20] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. 2025. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision. Springer, 3855. 6 [21] Midjourney. 2026. Midjourney. https://www. midjourney.com. 6 [22] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. 2025. Dreamo: unified framework for image customization. In Proceedings of the SIGGRAPH Asia 2025 Conference Papers. 112. [23] OpenAI. 2025. GPT Image 1.5: The new ChatGPT Images is here. urlhttps://openai.com/zh-Hans-CN/index/newchatgpt-images-is-here. 2, 3, 6 [Online; accessed 2026-01]. [24] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. 2024. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. In The Twelfth International Conference on Learning Representations. 2 [25] Wenhao Qian, Zhenzhen Hu, Zijie Song, and Jia Li. 2025. Concept Drift Guided LayerNorm Tuning for Efficient Multimodal Metaphor Identification. In Proceedings of the 2025 International Conference on Multimedia Retrieval. 11001108. 3 [26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. Highresolution image synthesis with latent diffusion modthe IEEE/CVF conference els. In Proceedings of on computer vision and pattern recognition. 10684 10695. [27] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2250022510. 3 [28] Marcelo Sandoval-Castaneda, Bryan Russell, Josef Sivic, Gregory Shakhnarovich, and Fabian Caba Heilbron. 2025. EditDuet: Multi-Agent System for Video Non-Linear Editing. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers. 1 11. 3 [29] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. 2025. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427 (2025). 3 [30] Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob Andreas, and Antonio Torralba. 2024. multimodal automated interpretability agent. In Forty-first International Conference on Machine Learning. 3 [31] Zhida Sun, Zhenyao Zhang, Yue Zhang, Min Lu, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. 2025. Creative Blends of Visual Concepts. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. 117. 2, 3 [32] Yael Vinker, Tamar Rott Shaham, Kristine Zheng, Alex Zhao, Judith Fan, and Antonio Torralba. 2025. Sketchagent: Language-driven sequential sketch generation. In Proceedings of the Computer Vision and Pattern Recognition Conference. 2335523368. 3 [33] Bo Xu, Junzhe Zheng, Jiayuan He, Yuxuan Sun, Hongfei Lin, Liang Zhao, and Feng Xia. 2024. Generating multimodal metaphorical features for meme understanding. In Proceedings of the 32nd ACM International Conference on Multimedia. 447455. 3 [34] Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Oliver Deussen, Weiming Dong, Jintao Li, and Tong-Yee Lee. 2025. B4M: Breaking Low-Rank Adapter for Making Content-Style Customization. ACM Transactions on Graphics 44, 2 (2025), 117. [35] Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Xiaoyu Kong, Jintao Li, Oliver Deussen, and Tong-Yee Lee. 2024. HeadRouter: Training-free Image Editing Framework for MM-DiTs by Adaptively Routing Attention Heads. arXiv preprint arXiv:2411.15034 (2024). 2 [36] Yu Xu, Fan Tang, You Wu, Lin Gao, Oliver Deussen, Hongbin Yan, Jintao Li, Juan Cao, and Tong-Yee Lee. 2025. In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation. In Proceedings of the SIGGRAPH Asia 2025 Conference Papers. 112. 3 [37] Yu Xu, Hongbin Yan, Juan Cao, Yiji Cheng, Tiankai Hang, Runze He, Zijin Yin, Shiyi Zhang, Yuxin Zhang, Jintao Li, et al. 2026. TAG-MoE: Task-Aware Gating for Unified Generative Mixture-of-Experts. arXiv preprint arXiv:2601.08881 (2026). 2 [38] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision. 11975 11986. 6 [39] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. 2023. Inversion-based style transfer with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10146 10156. 3 [40] Yuxin Zhang, Minyan Luo, Weiming Dong, Xiao Yang, Haibin Huang, Chongyang Ma, Oliver Deussen, 13 IPTong-Yee Lee, and Changsheng Xu. 2025. Prompter: Training-Free Theme-Specific Image Generation via Dynamic Visual Prompting. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers. 112. 3 [41] Yuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Tong-Yee Lee, and Changsheng Xu. 2022. Domain enhanced arbitrary image style transfer via contrastive learning. In ACM SIGGRAPH 2022 conference proceedings. 18. 3 [42] Yuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Tong-Yee Lee, and Changsheng Xu. 2023. unified arbitrary style transfer framework via adaptive contrastive learning. ACM Transactions on Graphics 42, 5 (2023), 116."
        }
    ],
    "affiliations": [
        "National Cheng-Kung University",
        "Tencent Hunyuan",
        "University of Chinese Academy of Sciences",
        "University of Konstanz"
    ]
}