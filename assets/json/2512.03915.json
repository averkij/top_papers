{
    "paper_title": "A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models",
    "authors": [
        "X. Y. Han",
        "Yuan Zhong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . m [ 2 5 1 9 3 0 . 2 1 5 2 : r A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models X.Y. Han Chicago Booth XY.Han@chicagobooth.edu Yuan Zhong Chicago Booth Yuan.Zhong@chicagobooth.edu December 5, Abstract In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide theoretical framework for analyzing the AuxiliaryLoss-Free Load Balancing (ALF-LB) procedure proposed by DeepSeeks Wang et al. (2024) by casting it as one-step-per-iteration primal-dual method for an assignment problem. in stylized deterministic setting, our framework yields several insightful structural First, properties: (i) monotonic improvement of Lagrangian objective, (ii) preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using generalized online optimization formulation. In the online setting, we derive strong convexity property of the objective that leads to logarithmic expected regret bound under certain stepsize choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models. 1 Introduction: s-MoEs and Load Balancing in AI Training Scaling architecture size has been the dominant driver of modern AI performance, with larger models consistently achieving better results (Kaplan et al., 2020; Hoffmann et al., 2022; Epoch AI, 2023). However, AI development has reached the point where the scaling of computation becomes prohibitively expensive due to hardware and energy constraints (Strubell et al., 2019; Thompson et al., 2020; Sevilla et al., 2022). Adapting to the cost and hardware limitations of scaling, researchers Authors listed alphabetically. 1 have turned to sparse Mixture-of-Experts (s-MoE) architectures (Shazeer et al., 2017), which are sparse realizations within the mixture-of-experts (MoE) paradigm codified by Jacobs et al. (1991). In modern large-scale AI architectures, s-MoE layers consisting of several parallel subnetworks (experts) controlled by sparse gate or router that selects data to route to them have largely replaced single submodules through which all data must pass. In these s-MoEs, for each input, the sparse-gating component selects strict subset of experts (hence sparse) to apply to that input. Thus, only small subcomponent of an AI architecture is activated to process each piece of input data allowing models to have significantly more parameters while keeping inference and training costs manageable. As testament to s-MoEs utility, recent releases of OpenAIs GPT (Achiam et al., 2023), Googles Gemini (Gemini Team et al., 2024), and DeepSeek (DeepSeek-AI, 2025) have all leveraged s-MoE designs to improve efficiency and maintain performance scaling. However, crucial aspect of s-MoE design load balancing (controlling how many inputs per expert) is mostly developed using trial-and-error motivated by heuristic insights (see Section 1.2). Learning to precisely and mathematically balance the load across experts, which reduces monetary losses from idle GPUs, could lead to enormous monetary savings for AI training. Figure 1: Schematic of naïve s-MoE layer without load balancing."
        },
        {
            "title": "1.1 Naïve s-MoE Layers Without Load Balancing",
            "content": "Figure 1 describes the naïve setup for s-MoE layers within transformer-based AI models. In particular, the input is series of token embeddings x1, x2, ..., xT where each xi is high-dimensional vector corresponding (in language models) to language unit such as Hel, lo, world, etc. or (in vision models) patch within an image. Each piece of input data (a sentence, an image patch, etc.) is decomposed into constituent tokens; each token is mapped to its vector embedding xi; and those is called the context embeddings are input into the AI model. The entire tuple of vectors {xi}T and is the context length. i=1 Within the AI model, each of the original token embeddings xi is transformed into feature embeddings by each of the AI models layers. In Figure 1, to describe the action of some particular s-MoE layer, we use {zi}T to denote the feature embeddings before that s-MoE layer. i= 2 When feature embedding enters an s-MoE layer with experts, we calculate an unnormalized zi. affinity score ζi,k between zi and the k-th expert usually using an inner product: ζi,k := These scores are then normalized, typically using the softmax function, into the affinity scores: γi,k := SoftMax (cid:0)ζi,k; {ζi,k}E k=1 (cid:1) = exp(ζi,k) k=1 exp(ζi,k) (cid:80)E The router then selects the Top-K experts based on the largest γi,k. The final step in an s-MoE layer is to aggregate the outputs of the selected experts. This is done by computing weighted sum of the selected experts outputs: (cid:88) γi,kfk(zi), kChosenExpertsi (1) where fk represents the k-th expert. Note that the softmax is taken before the Top-K selection, which is typically the preferred order in recent s-MoEs (Dai et al., 2024; Riquelme et al., 2021). Moreover, the softmax is monotonic, so it is equivalent to choose the Top-K experts based on the largest {γi,k}E for each i. This completes the description of the schematic in Figure 1. k="
        },
        {
            "title": "1.2 Load Balancing of Experts: Background and Related Work",
            "content": "While the naïve routing method of choosing the top-K among {γi,k}E could easily cause some experts to idle while others are overloaded. Since GPU time is expensive, is conceptually simple, it k=1 such an imbalance could lead to significant monetary losses and inefficiencies in the training process. Several fixes have been proposed. The most commonly adopted approach is adding an auxiliary balancing loss directly to the training loss penalizing the network parameters during training for inducing imbalanced token allocations (Fedus et al., 2022; Lepikhin et al., 2021; Shazeer et al., 2017). However, this method interferes with the gradient updates of the performance-focused component of the objective (see Section 2.2 of Wang et al. (2024) for more detailed discussion). Another approach by Lewis et al. (2021) approximately solves via truncated auction algorithm based on Bertsekas (1992) an integer program that balances the load across experts in every training iteration (corresponding to one mini-batch of data). However, generating an AI models outputs for even one single batch of data (a forward pass) requires significant computation time and memory since it requires calculating matrix multiplications and non-linear transformations defined by millions to billions of parameters. If this is done during training, there is an additional computational and memory overhead for computing and storing the backpropagated gradients (the backward pass). Thus, it is inadvisable to spend additional time solving multi-iterative subroutine (whether an auction algorithm or an integer program) for every s-MoE layer and every mini-batch. To address this problem, DeepSeeks auxiliary-loss-free (ALF-LB) (Wang et al., 2024) procedure augments each expert with bias pk using single-shot update (as opposed to multi-step subroutine), nudging tokens toward underloaded experts without interfering with training gradients as is done in works leveraging auxiliary balancing losses. Notably, ALF-LB was used 3 to successfully train the recent DeepSeekV3 (DeepSeek-AI, 2024) models."
        },
        {
            "title": "1.3 DeepSeek’s ALF-LB Algorithm",
            "content": "The ALF-LB procedure (Wang et al., 2024) is as follows: 1. For each expert = 1, . . . , E, initialize scalar shift parameter pk to be 0. 2. Perform forward pass on mini-batch. During the forward pass, route token based on the experts with the highest shifted weights γik + pk. 3. Calculate the downstream network loss and update the main network parameters, treating the shifts {pk} as constants. 4. For each expert k, update its shift parameter as follows, where is small constant (e.g., 0.001): pk pk if expert had load > L; pk + if expert had load < L; pk otherwise. (2) 5. Repeat steps 2-4 for each mini-batch of input data. In the original publication, Wang et al. (2024) chose = 0.001 and exhibited empirical benefits of this procedure on 1B to 3B parameter DeepSeekMoE models (Dai et al., 2024)."
        },
        {
            "title": "Our main contribution is a rigorous theoretical framework for understanding and analyzing the",
            "content": "ALF-LB procedure, with specific contributions detailed across different sections. First, in Section 2, we cast the ALF-LB procedure as single-step primal-dual method for an assignment problem, connecting state-of-the-art heuristic from large-scale AI to the operations research and primaldual optimization literature for resource allocation such as those in Bertsekas (1992, 1998, 2008). However, the procedure we analyze differs from the aforementioned operations research problems since, as discussed in Section 1.2, the computational and memory requirements of performing forward pass through an AI model do not allow for one to run multi-iterative procedures as subroutines with those forward passes. Instead, s-MoE balancing routines (such as ALF-LB) must be updated in one-shot manner with computationally-minimal, constant-time updates per forward pass. Then, in Section 4, we analyze this procedure in stylized deterministic setting and establish several insightful structural properties: (i) monotonic improvement of Lagrangian objective (Theorem 1), (ii) preference rule that moves tokens from overloaded to underloaded experts (Theorem 2), and (iii) guarantee that expert loads converge to bounded deviation from perfect balance. Finally, in Section 5, we extend our analysis to more realistic online, stochastic setting by establishing strong convexity property of the expected dual objective (Section 5.6) and using it to derive logarithmic regret bound for the ALF-LB procedure (Theorem 13). 1.4.1 Related Work in Online Resource Allocation It is insightful to compare this paper to another recent line of work at the intersection of AI implementation and operations research: the online resource allocation in AI infrastructures (see Zhang et al. (2024) and citations therein) where computational jobs arrive in an online, stochastic manner at, say, large data center and must be optimally scheduled to the best server for the job. In comparison, in the s-MoE balancing problem, for each forward pass, every s-MoE layer must process batches of tokens that all arrive at once. These tokens must all pass through that s-MoE layer before they can collectively move to the next s-MoE layer an example for reference would be DeepSeekV3 (DeepSeek-AI, 2024), which contains 61 s-MoE layers. Thus, unlike the case studied in Zhang et al. (2024), the allocation in MoE load balancing must be done immediately and in computationally-minimal manner. (See Section 1.2 for additional details.) Another related line of works is Balseiro et al. (2020, 2021); Agrawal and Devanur (2014); Jenatton et al. (2016) (see Balseiro et al. (2021, Section 1.2) and citations therein) that design and analyze primal-dual methods for solving online resource allocation problems by formulating them as online stochastic convex programs or regularized allocation problems. These prior works utilize dual descent and mirror descent techniques to manage global resource constraints which can be formulated as load balancing. However, the algorithms proposed in those works often require solving auxiliary optimization sub-routines (such as linear programs or non-trivial projections) during their updates. As mentioned in Section 1.2, in the context of s-MoE training, multi-iterative subroutines are computationally impracticable because routing must occur in every s-MoE layer of the AI architecture during already-computationally-expensive forward passes, which does not allow for the extra overhead of solving auxiliary sub-routines at every s-MoE layer. Thus, in comparison, our paper instead analyzes single-shot update framework, built specially to encompass DeepSeeks ALF-LB procedure (Wang et al., 2024), that updates the load balancing parameters with negligible effect on the speed of the forward pass."
        },
        {
            "title": "2 A Primal-Dual Framework for Optimal Load Balancing",
            "content": "We establish rigorous mathematical framework to understand existing load balancing heuristics, particularly the DeepSeek ALF-LB method. In the remainder of the paper, for simplicity, we will refer to the normalized affinity scores γik as the affinity scores and adopt the convention of using them both for routing and aggregation."
        },
        {
            "title": "2.1 Allocation Problem: Integer Program and Relaxation",
            "content": "Consider the exact-balancing primal problem for assigning tokens to experts. As starting point, we make the following assumptions and stylizations: The number of tokens multiplied by the sparsity, KT , is exactly divisible by the number of experts E, so the perfectly balanced load is = KT /E. 5 The affinity scores γik are constant from iteration to iteration1. Hence, the target load is := KT /E and perfect balance is characterized by the solution of the following integer program (IP): (cid:88) γikxik max {xik} i,k s.t. (cid:88) (cid:88) xik = = 1, . . . , xik = = 1, . . . , (3) xik {0, 1} i, k. In practice, it is typically inadvisable (in terms of both time and memory requirements) to solve an IP for every MoE layer and on each individual mini-batch of data2. Instead, we first relax the IP to linear program (LP) by replacing the integer constraint xik {0, 1} with xik 0. It is routine to show that the IP and the LP relaxation have the same optimal value. The Lagrangian of the LP relaxation is γikxik + (cid:88) L(x, y, p) = = (cid:88) i,k (cid:88) i,k (γik + pk yi) xik + (cid:32) yi (cid:88) (cid:33) xik + (cid:88) pk (cid:32) (cid:88) xik (cid:33) (cid:88) yi (cid:88) pk. (4)"
        },
        {
            "title": "The corresponding dual problem is",
            "content": "min {yi},{pk} (cid:88) yi (cid:88) pk k s.t. yi pk γik i, k. However, even solving this LP relaxation to completion every iteration would still be too slow and often memory-infeasible."
        },
        {
            "title": "2.2 Deriving ALF-LB from Primal-Dual Principles",
            "content": "We show that DeepSeek ALF-LB (Section 1.3) can be formulated as primal-dual procedure that performs single-shot update per iteration for finding critical point of the Lagrangian (4). For conciseness, we introduce the following notation for the load of the k-th expert at iteration n: 1This is stylized assumption for the initial analysis in this section only. Later, in Section 5, we will consider the case where the affinity scores are new stochastic realizations from some distribution every iteration. 2One notable exception is the BASE layer heuristic invented by Lewis et al. (2021) which aims to approximately solve the IP using truncated auction algorithm modeled after Bertsekas (1992). 6 A(n) := (cid:80) x(n) ik . Indexing each training iteration with n, consider the following primal-dual scheme: Dual Update: p(n+1) Primal Update: x(n+1) ik (cid:16) A(n) (cid:17) + ϵ(n) p(n) 1 if TopKIndk 0 otherwise (cid:0)γ(n+1) ik + p(n+1) (cid:1) k. i, k, (5) (6) (cid:111) (cid:110) ϵ(n) where are step-sizes and TopKIndk() gives the indices that would induce the K-largest arguments. The Primal Update enforces (cid:80) xik = K. Maximizing (cid:80) i,k(γik + pk yi)xik subject to this constraint is equivalent to choosing the top values of γik + pk for each i, regardless of yi. Thus we can simplify the Lagrangian by dropping the yi terms, which gives: L(x, p) = (cid:88) i,k (γik + pk) xik (cid:88) pk. (7) Within this setup, the original DeepSeek ALF-LB update from Wang et al. (2024) described in (2) corresponds to the step-size ϵ(n) = (cid:12) (cid:12)L A(n) (cid:12) (cid:12) (cid:12) (cid:12) . (DeepSeek ALF-LB Step-Size) (8) Figure 2 illustrates the convergence behavior of this primal-dual scheme during the training of real 1B-parameter DeepSeekMoE models (Dai et al., 2024) with varying ϵ(n) experimental details are provided in Section 3. step-size choices. More k"
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "In all experiments in this paper (Figures 2-4), we train 1B-parameter DeepSeekMoE models (Dai et al., 2024) for 100K steps on the next-token prediction task on the Salesforce WikiText-103 dataset (Merity et al., 2016) with the cross-entropy loss. The text data is tokenized using the GPT-2 tokenizer (Radford et al., 2019). Here, we will provide only brief description of the DeepSeekMoE architecture for completeness and refer to Dai et al. (2024) for more in-depth details: The DeepSeekMoE architecture follows the paradigmatic practice of stacking decoder-only transformer layers (Vaswani et al., 2017) into full large language model. In its simplest form, the transformer layer contains several sub-layers among them multi-headed attention sub-layer and multi-layer perceptron (MLP) sub-layer. For our setting of interest, modern s-MoE architectures (Shazeer et al., 2017; Jiang et al., 2024; Dai et al., 2024) replace the MLP sub-layer of each transformer layer with an s-MoE sub-layer described in Sections 1.1-1.2, where each parallel expert is typically separate MLP. Additionally, the DeepSeekMoE architecture (Dai et al., 2024) is specifically characterized by its use of granular 7 Figure 2: Validation set load imbalance and loss during the training of 1B-parameter DeepSeekMoE model. Section 3 gives experiment details. Left: We measure the imbalance as the average load deviation from the target load = KT /E across all experts in the DeepSeekMoE1B architecture. Right: We measure the loss on the validation set. segmentation (using narrower experts but increasing the total number of experts) and the inclusion of two shared experts that are always chosen by the gate3. The architectural parameters of the 1B-parameter DeepSeekMoE models in our experiments are the same as those described in Wang et al. (2024, Table 5). For consistency with Wang et al. (2024), we also use = 64 experts with sparsity level = 6. During training, we optimize all 1B parameters within the transformer backbone and prediction head of the DeepSeekMoE architectures starting from random initializations. Each model was trained on 8xH100/H200 GPUs with batch size of 64 sequences/batch and 4096 tokens/sequence (so, 262K). To optimize the models, we use the AdamW (Loshchilov and Hutter, 2019) optimizer. Balancing Schemes. at iteration (denoted ϵ(n) particular, given some balancing hyperparameter u, we compare the following schemes: ϵ(n) (Original DeepSeek ALF-LB from Wang et al. (2024)) In our experiments, we compare three choices of the k-th expert step-sizes In , see Section 2.2) in the ALF-LB balancing scheme framework. ϵ(n) ϵ(n) Additionally, we include comparison with fourth scheme that trains with an auxiliary loss LA(n) = = = 3We will not include the shared experts within the theoretical framework presented in this paper because the shared experts represent fixed computational load that does not require dynamic balancing. Additionally, omitting the shared experts from our theoretical formulation leads to cleaner and more concise analyses. 8 (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2022; Wang et al., 2024). We calculate the auxiliary loss with the method described in Wang et al. (2024, Section 2.2). The auxiliary loss is multiplied by trade-off parameter that we will, for consistency, also denote by and then added to the main cross-entropy loss. Hyperparameter Search. For hyperparameter search over the following hyperparameters: each of four the scheduling schemes, we conducted balancing constants {1e4, 1e3, 1e2, 1e1, 1, 10}, learning rates lr {1e5, 1e4, 1e3}, and weight decay wd {0.01, 0.1, 0.001}. Thus, we trained 4 6 3 3 = 216 separate 1B-parameter DeepSeekMoE models to conduct this search. Then, for each of the four scheduling schemes, we select the hyperparameter setting that achieves the best cross-entropy loss on held-out validation set to be shown in the experimental plots and tables in this paper. We found that lr = 1e4 and wd = 1e1 consistently led to the best validation loss across all settings; the u/n and auxiliary loss scheduling schemes performed the best with parameter = 1; and the u/L A(n) scheduling schemes performed the best with = 1e3. and u/"
        },
        {
            "title": "We make some interesting empirical observations from our experiments that are of separate interest",
            "content": "from the theoretical framework proposed in this paper. Firstly, we found that, for the original constant update scheme considered by Wang et al. (2024) (which corresponds to u/L A(n) in our formalization), our hyperparameter search also yielded = 1e3 to be the optimal balancing constant, which corroborates the same observation from Wang et al. (2024). Secondly, Table 1 reports the final validation loss and overall imbalance of the different balancing schemes at the end of training. Observe that the u/ scheme achieves the lowest validation loss (best predictive performance) but the highest imbalance (worst computational efficiency); in contrast, the auxiliary loss approach (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2022) achieves the lowest imbalance (best computational efficiency) but the highest validation loss (worst predictive performance). The u/n scheme (which we will analyze in Section 5 through the lens of online optimization) and Wang et al. (2024)s original u/L A(n) scheme achieve balance between validation loss and imbalance with u/n achieving slightly better balance and u/LA(n) achieving slightly better predictive performance."
        },
        {
            "title": "4 Convergence Analysis for the Deterministic Case",
            "content": "To start, we present theoretical guarantees for the convergence of the procedure described by (5) and (6), assuming fixed scores γik. For simplicity, we consider the case where = 1 for Section 4 9 Table 1: Comparison of cross-entropy loss on validation data and overall imbalance at the end of training for different scheduling schemes. Experiment details in Section 3. Balancing Scheme Validation Loss Overall Imbalance Auxiliary Loss 0.07443 u/L A(n) 0.08928 0.08893 u/n 0.08961 u/ 3.68999 3.65369 3.68228 3.64642 only. We will later consider the case where the scores are stochastic and for general in Section 5."
        },
        {
            "title": "4.1 Monotonicity of the Lagrangian",
            "content": "Towards showing the convergence of this procedure, we will show that the Lagrangian decreases with (cid:17) γik + p(n) every iteration. Additionally, we define the assignment function αn (i) := arg maxk that gives the assigned expert to token at iteration n. The switching benefit is defined as (cid:16) b(n+1) (i) = (cid:16) γiαn+1(i) + p(n+1) αn+1(i) (cid:17) (cid:16) γiαn(i) + p(n+1) αn(i) (cid:17) , (9) which captures the benefit of switching to expert αn+1 (i) rather than staying with αn (i). Note that b(n+1)(i) 0 by definition of the primal update. Theorem 1. (Change in Lagrangian) Using the procedure described in Steps (5)-(6) (with = 1), the following holds for the Lagrangian (7): (cid:16) x(n+1), p(n+1)(cid:17) (cid:16) x(n), p(n)(cid:17) = (cid:88) b(n+1) (i) (cid:88) (cid:16) ϵ(n) A(n) (cid:17)2 . Proof. By analyzing the change in the two terms of the Lagrangian (7) over one iteration, we have: (cid:16) x(n+1), p(n+1)(cid:17) (cid:16) (cid:88) = (cid:16) x(n), p(n)(cid:17) γik + p(n+1) (cid:17) x(n+1) ik (cid:88) (cid:16) (cid:17) γik + p(n) x(n) ik (cid:88) (p(n+1) p(n) ) i,k (cid:88) (cid:104)(cid:16) = γiαn+1(i) + p(n+1) αn+1(i) (cid:17) i,k (cid:16) γiαn(i) + p(n) αn(i) (cid:17)(cid:105) (cid:88) (cid:104) = b(n+1)(i) + (cid:16) γiαn(i) + p(n+1) αn(i) (cid:17) (cid:16) γiαn(i) + p(n) αn(i) (cid:17)(cid:105) (cid:88) (L A(n) ϵ(n) ) (cid:88) (L A(n) ϵ(n) ) (cid:88) = b(n+1)(i) + (cid:88) (cid:16) αn(i) p(n) p(n+1) αn(i) (cid:17) i (cid:88) b(n+1)(i) + (cid:88) αn(i)(L A(n) ϵ(n) αn(i)) (cid:88) (cid:88) (L A(n) ϵ(n) ) (L A(n) ϵ(n) ) (cid:88) b(n+1)(i) + (cid:88) ϵ(n) A(n) (L A(n) ) k (cid:88) (L A(n) ϵ(n) ) 10 = = (cid:88) = b(n+1)(i) + (cid:88) (A(n) ϵ(n) L)(L A(n) ) (cid:88) = b(n+1)(i) (cid:88) (A(n) ϵ(n) L)2. This theorem shows that the improvement in the Lagrangian is the difference between the total switching benefit and the squared sum of load imbalances (weighted by step-sizes). We now specialize the analysis to the DeepSeek ALF-LB step-size choice."
        },
        {
            "title": "4.2 Analysis of the DeepSeek Step-Size",
            "content": "k = u/L , (B) token switched experts between iterations and + 1, and (C) there are no ties in Theorem 2. Assume (A) we use the procedure (5)-(6) with the DeepSeek step-size ϵ(n) A(n) bias-shifted scores. Then, the following must hold: 1. Token must have switched to strictly lower designation in the ordering: Overloaded Balanced Underloaded. 2. The switching benefit of token is bounded: 0 < b(n+1) 3. The iteration-n score difference is bounded: 2u < γiαn+1(i) + p(n) Proof. Since scores are not tied, the switching benefit b(n+1) p(n) αn+1(i) < γiαn(i) + p(n) . From the definition of b(n+1) < 2u. αn(i) αn+1(i) (cid:16) γiαn(i) + p(n) αn(i) (cid:17) < 0. must be strictly positive, and γiαn+1(i) + and the dual update, we have (cid:16) b(n+1) = γiαn+1(i) + p(n) αn+1(i) (cid:16) (cid:17) γiαn(i) + p(n) αn(i) (cid:17) + Sign (cid:16) A(n) αn+1(i) (cid:17) Sign (cid:16) A(n) αn(i) (cid:17) . (cid:16) (cid:17) (cid:16) (cid:17) > 0, it must be that uSign For b(n+1) > 0. This only happens αn+1(i) if token moves from an expert that is more loaded (relative to L) to one that is less loaded, which (cid:17) implies Sign , proving (1). The sign difference term is either αn(i) or 2u. The bounds in (2) and (3) follow from this observation and the strict inequalities. A(n) A(n) A(n) A(n) uSign > Sign αn+1(i) αn(i) (cid:16) (cid:17) (cid:16) Corollary 3. (DeepSeek Lagrangian) With the DeepSeek step-size, the Lagrangian change in Theorem 1 simplifies to (cid:16) x(n+1), p(n+1)(cid:17) (cid:16) x(n), p(n)(cid:17) = (cid:88) b(n+1) (i) (cid:12) (cid:12)A(n) (cid:12) (cid:12) (cid:12) (cid:12) . (cid:88) Furthermore, letting (n+1) be the set of tokens that switched experts, (cid:16) x(n+1), p(n+1)(cid:17) (cid:16) x(n), p(n)(cid:17) (cid:34) < 2 (cid:12) (cid:12) (cid:12)S (n+1)(cid:12) (cid:12) (cid:12) (cid:88) (cid:12) (cid:12)A(n) (cid:12) (cid:35) (cid:12) (cid:12) (cid:12) . Proof. Follows directly from combining Theorems 1 and 2, and using the strict inequality from the no-ties assumption. 11 From this, we can show that if the imbalance pattern (which experts are overloaded vs. underloaded) does not flip between iterations, the Lagrangian strictly decreases. Proposition 4. Suppose that at iteration n, set of experts K1 has load and set K2 has load L. If at iteration + 1, the same sets of experts remain in their respective loading states (i.e., experts in K1 still have load L, and experts in K2 still have load L), then (cid:16) x(n+1), p(n+1)(cid:17) (cid:16) x(n), p(n)(cid:17) < 0. Proof. By Theorem 2, tokens can only switch from an expert in K1 to an expert in K2. This implies that (n+1) = (cid:80) for K1, we have (n+1) (cid:80) L. Thus, 2S (n+1) A(n+1) L). Also, we know that 2 (cid:80) ). Since A(n+1) (A(n) L) = (cid:80) A(n) (A(n) (A(n) L. The result then follows from Corollary 3. kK1 A(n) kK1 kK (cid:80) k"
        },
        {
            "title": "4.3 Preference Analysis and Guarantee of Balance",
            "content": "We can reframe the routing decision in terms of preferences. Token prefers expert over at iteration + 1 if γik + p(n+1) kk := γik γik and the bias kk > Gp(n+1) gap be Gp(n+1) . . Let the score gap be Gγi . The preference is equivalent to Gγi p(n+1) := p(n+1) > γik + p(n+1) kk kk Proposition 5. Assume two tokens and concurrently switched from the same origin expert αn to the same destination expert αn+1. Then, their score gaps relative to those experts satisfy (cid:12) (cid:12)Gγi (cid:12) αn+1αn Gγj αn+1αn (cid:12) (cid:12) (cid:12) < 2u. Proof. For token to switch, its score gap must lie in specific interval determined by the bias gaps: Gp(n) , where sign_diff is between -2 and -1. The same holds for token j. Since the bounds are independent of the token and the interval has length at most 2u, the result follows. αnαn+1 + sign_diff < Gγi αn+1αn < Gp(n) αnαn+1 This proposition implies that if we choose the step parameter to be smaller than half the (cid:12) , (cid:12) (cid:12) minimum difference between any two score gaps, i.e., < where = 1 then token movements become unique. kk Gγj kk 2 mink,k,i,j (cid:12) (cid:12)Gγi (cid:12) Corollary 6. In the DeepSeek implementation, if we choose constant step-size satisfying < u, then no two tokens will move between the same pair of origin and destination experts in two consecutive iterations. As consequence, an experts load cannot change by more than (E 1) tokens between two consecutive iterations. Proof. direct consequence of Proposition 5. Finally, we guarantee that DeepSeek ALF-LB must achieve an approximately balanced state. 12 Theorem 7. (Guarantee of Approximate Balancing) When applying the DeepSeek ALF-LB procedure using constant step parameter < u, the load of all experts must converge to the range [L (E 1) , + (E 1)]. Moreover, once an experts load enters that range, it remains in that range. Proof. If perfect balance is achieved, the shifts pk stop changing and the algorithm terminates. Otherwise, there will always be an overloaded and an underloaded expert. If an expert ks load is above + (E 1), its shift pk will decrease by each iteration, causing it to shed tokens. By Corollary 6, its load can change by at most (E 1) per iteration. So, its load must eventually enter the range [L, + (E 1)] without overshooting below L. symmetric argument holds if an experts load is below (E 1). Once an experts load enters the range [L (E 1), + (E 1)], it cannot escape. For example, if an experts load is + for 1 1, it is overloaded and can only lose tokens. It can lose at most 1 tokens. So its load will remain above + (E 1), which is greater than E. similar argument holds for underloaded experts. Therefore, all expert loads will enter and remain within the stated range. When the number of tokens is much larger than the number of experts (T E), the deviation of (E 1) from the perfectly balanced load = KT /E is negligible. This completes the theoretical demonstration of why the DeepSeek ALF-LB procedure leads to the desirable balancing behavior seen experimentally."
        },
        {
            "title": "5 Stochastic Analysis via Online Optimization",
            "content": "k , where p(n) is the k-th coordinate of p(n). evolve during training. Thus, in this section, we generalize In practice, the affinity scores γ(n) ik the previously considered Lagrangian (7) by assuming that the scores γ(n) are stochastic and drawn ik from expert-dependent distributions. In particular, at iteration n, we assume random affinity score γ(n) ik (0, 1) is observed for each token {1, . . . , } and expert {1, . . . , E}. The algorithm updates shift vector p(n) RE and, for each token i, selects the experts with the largest values of γ(n) ik + p(n) Using the notation of Section 2.2, we note that this section will consider step-size choice of ϵ(n) = u/n instead of the u/L A(n) step-size chosen by Wang et al. (2024). This is because, when affinity scores become stochastic and time-varying, analyzing the coordinate-dependent u/LA(n) step-size sequence becomes technically intricate. In contrast, the diminishing and coordinateindependent u/n step-size connects directly with ideas from online convex analysis (Hazan, 2016, Section 3.3.1) leading to cleaner theoretical insights. This adjustment maintains experimental relevance and practicality: Figure 2 and Table 1 demonstrate that using the u/n step-size is comparable in effectiveness as using the original u/L A(n) In fact, Table 1 shows that the u/n step-size leads to slightly better load balancing performance than the u/L A(n) step-size at the cost of slightly higher validation loss. step-size."
        },
        {
            "title": "5.1 Notation",
            "content": "For any vector RE, define TopKInd(z) {1, . . . , E} to be the set of indices of the largest coordinates of with ties broken arbitrarily. For round and token i, denote Γ(n),i := (cid:16) RE. Routing at round sends token to the experts in TopKInd(cid:0)Γ(n),i + p(n)(cid:1). i1 , . . . , γ(n) γ(n) Fix an integer {1, . . . , E}. We frame this as minimizing the per-round online loss, which iE (cid:17) corresponds to the dual online objective: (n)(p) = (cid:88) (cid:88) (cid:16) γ(n) ik + pk (cid:17) i= kTopKInd(Γ(n),i+p) (cid:88) k=1 pk, (10) where := KT /E is the desired per-expert load for Top-K routing. The k-th component of the loss gradient g(n)(p) RE is given by (p) := kf (n)(p) = A(n) g(n) (p) L, (11) (cid:8)i : TopKInd(cid:0)Γ(n),i + p(cid:1)(cid:9)(cid:12) (p) := (cid:12) (cid:12) where A(n) (cid:12) counts the number of tokens for which expert lies in the per-token Top-K set under shifts p. The online dual update corresponding to (5) can then be rewritten as p(n+1) p(n) ϵ(n) g(n) (p(n))."
        },
        {
            "title": "5.2 Distributional Assumptions",
            "content": ", for fixed expert k, is drawn independently4 from distribution that Assume each affinity γ(n) ik depends only on the expert k, has bounded support on (0, 1), and has probability density function (pdf) φk that is upper bounded by some expert-independent constant. Thus, for fixed k, the vectors of random affinity scores Γ(n),i = for the i-th token in the n-th iteration are i.i.d across and n. While this assumption may seem strong, our experiments in Figure 3 from training 1B-parameter DeepSeekMoE models suggest that it is close to reality. k=1 (cid:17)E (cid:16) γ(n) ik"
        },
        {
            "title": "5.3.1 Unbiasedness\nIt is easy to check that the loss f (n) is convex. Thus, the expected loss f (p) = E (cid:2) f (n)(p)(cid:12)\n(cid:12) p(cid:3) is\nalso convex. We first show that the loss gradient g(n)(p) is an unbiased estimator of ∇f (p) with an\nexplicit form expression.",
            "content": "4This independence assumption is stylization: various mechanisms (attention, layer norm, etc.) in earlier layers could create dependencies between token embeddings. However, it gives us starting point for building tractable, baseline theory. Furthermore, Figure 3 demonstrates that the distributions of γ(n) remain stable and well-behaved ik throughout training on DeepSeekMoE-1B models, which indicates that independence is empirically well-founded as an approximating simplification at least at the marginal distribution level. 14 (a) u/L A(n) Step-Size (b) u/n Step-Size (c) u/ Step-Size (d) Auxiliary Loss Figure 3: Time-lapse histograms of the marginal distributions of γ(n) during the training of ik 1B-parameter DeepSeekMoE models using different choices of step-size (Section 2.2). Experimental details in Section 3. Proposition 8 (Unbiased Stochastic Gradient). For any fixed RE, (cid:104) g(n)(p) (cid:105) (cid:12) (cid:12) (cid:12) = (p) = π(p) 1, where 1 is the ones-vector, and π(p) RE is the selection probabilities vector with k-th coordinate πk(p) := Pr(k TopKInd(Γ + p)) , for some generic affinities Γ d= Γ(n),i. Necessarily, (cid:80) πk(p) = almost surely. Proof. For given token i, let Xik(p) := 1 (cid:110) TopKInd (cid:16) Γ(n),i + (cid:17)(cid:111) , (12) where 1{} denotes the indicator function. Then, [Xik(p) p] = πk(p) by definition and the k-th expert loads are A(n) = πk(p). Therefore, i=1 Xik(p). Hence, (p) = (cid:80)T (cid:105) (p) (cid:104) A(n) (cid:104) (cid:105) g(n)(p) = (cid:16) = (cid:105) (cid:104) (n)(p) (cid:105) (cid:104) 1 (p) A(n) L, . . . , (cid:104) A(n) (p) (cid:105) (cid:17) by Eq. 11 15 = π(p) 1. Since the distribution of Γ(n),i is independent of i, observe that (p) = (cid:88) (Γk + pk) kTopKInd(Γ+p) (cid:88) pk. To calculate , we can move the differentiation into the expectation via the dominated convergence theorem: the Γk have continuous densities so ties occur with probability zero; thus, for almost mTopKInd(Γ+p)(Γm + pm) exists and equals 1{k every realization, the partial derivative pk TopKInd(Γ + p)} 1, yielding pk = Pr{k TopKInd(Γ + p)} = πk(p). The desired result follows. (cid:105) mTopKInd(Γ+p)(Γm + pm) (cid:104)(cid:80) (cid:80) Using Proposition 8, we can compute the variance and second moment of g(n)(p). Proposition 9. (Variance and 2nd Moment) The variance and second moment of g(n)(p) are given by"
        },
        {
            "title": "Var",
            "content": "(cid:104) (cid:12) (cid:105) g(n)(p) (cid:12) (cid:12) (cid:32) = (cid:88) k=1 (cid:33) (cid:33) πk(p)2 , (cid:32) (cid:104) g(n)(p)2(cid:12) (cid:12) (cid:12) (cid:105) = 2 (cid:32) (cid:88) k= πk(p)2 K2 + (cid:33) πk(p)2 . (cid:88) k=1 Proof. Using Equation (11) and Proposition 8,"
        },
        {
            "title": "Var",
            "content": "(cid:16) (cid:12) (cid:17) g(n)(p) (cid:12) (cid:12) (cid:105) (cid:104) g(n)(p) (p)2 = (cid:105) (cid:104) π(p) A(n)(p)2 = (cid:16) 1 (p), . . . , A(n) A(n) where A(n)(p) = the assignment vector for token with components as in (12). Then, A(n)(p) = (cid:80)T E[Xi(p)] = π(p). So, is the vector of expert loads. Let Xi(p) {0, 1}E denote i=1 Xi(p) and (cid:17) (p)"
        },
        {
            "title": "Var",
            "content": "(cid:16) (cid:12) (cid:17) g(n)(p) (cid:12) (cid:12) = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) i=1 π(p) Xi(p) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:88) i=1 E(cid:2)π(p) Xi(p)2 p(cid:3) by independence across Var(X1k p) by identical distribution across = (cid:88) k=1 (cid:32) = K (cid:33) πk(p)2 , (cid:88) k= 16 since (cid:80) moment and applying Proposition 8 gives Xik = a.s. and Var(X1k p) = πk(p) (1 πk(p)). Finally, decomposing the second (cid:104) (cid:105) g(n)(p)2 (cid:105) (cid:104) g(n)(p) (p)2 = (p)2 + (cid:32) (cid:88) = 2 πk(p)2 K2 (cid:33) (cid:32) + K (cid:33) πk(p)2 . (cid:88) k= This completes the proof. k=1 Proposition 9 will be useful later to prove Theorem 13."
        },
        {
            "title": "5.4 Second-Order Analysis of Expected Loss",
            "content": "In the following Sections 5.4-5.6, we show that the expectation of the Top-K objective is strongly convex with respect to updates under certain (realistic) assumptions. The strong convexity then allows us to show logarithmic regret bound in Theorem 13. Without strong convexity, it is routine ) without additional assumptions. The next lemma to verify that the regret bound is at best O( characterizes the second directional derivative of the expected objective. Proposition 10 (Second Directional Derivative). Let Γ = (Γ1, . . . , ΓE) be random affinity vector in RE with the properties in Section 5.2. For biases RE define FK(p) = (cid:88) (Γk + pk) , kTopKInd(Γ+p) and let φk and Φk denote the density and CDF of Γk, respectively. Then, for any direction δ RE, its second directional derivative at is given by the formula D2FK(p)[δ, δ] = w(K) kℓ (p) (δk δℓ)2, (cid:88) k<ℓ (13) where the symmetric edge weights are w(K) kℓ (p) = (cid:90) φk(v pk) φℓ(v pℓ) B(K1) k,ℓ (v; p) dv, w(K) kℓ (p) 0, with B(K1) k,ℓ (v; p) = (cid:88) S[E]{k,ℓ} S=K1 Φc j(v pj) (cid:89) jS (cid:89) Φm(v pm). m[E]({k,ℓ}S) Proof. For some fixed argument γ [0, 1]E, define the function fp,K(γ) = (cid:88) (γk + pk), kTopKInd(γ+p) 17 so that FK(p) = (cid:82) fp,K(γ) φ(γ) dγ, where φ(γ) = (cid:81)E k=1 φk(γk) is the joint density of Γ. For R, define p(t) := + δ and (cid:101)FK(t) := FK (p(t)). Since ties occur with probability zero, FK is a.s. differentiable with gradient FK(p) = π(p). Hence, the chain rule gives (cid:101)F K(0) = (cid:88) k=1 δk πk(p). We will next compute (cid:101)F K(0). For each k, using independence and conditioning on Γk = v, πk(p) = Pr(k TopKInd(Γ + p)) = = = = (cid:90) 1 0 (cid:90) 0 (cid:90) 1 0 (cid:90) 1 0 φk(v) φk(v) = (cid:90) 1 0 φk(v) φk(v) Pr(k TopKInd(Γ + p) Γk = v) dv φk(v) Pr({j = : Γj + pj > + pk} K1) dv K1 (cid:88) r=0 K1 (cid:88) r=0 K1 (cid:88) r=0 Pr({j = : Γj + pj > + pk} = r) dv Pr(jS : Γj+pj > v+pk and / S{k} : Γm+pm v+pk) dv (cid:88) S[E]{k} S=r (cid:88) (cid:89) Φc j(v pj + pk) S[E]{k} S=r jS (cid:89) /S{k} Φm(v pm + pk) dv. j() := 1 Φj() and represents possible index sets within the top-K components of Γ + where Φc that are also larger than + pk. For notational convenience, set θjk(v, t) := (pj pk) (δj δk). Then, πk (p(t)) = (cid:90) 0 φk(v) K1 (cid:88) r=0 (cid:89) jS (cid:88) S[E]{k} S=r Φc (θjk(v, t)) (cid:89) Φm (θmk(v, t)) dv. /S{k}"
        },
        {
            "title": "Consider the integrand",
            "content": "φk(v) K1 (cid:88) r=0 (cid:88) (cid:89) Φc (θjk(v, t)) (cid:89) Φm (θmk(v, t)) . (14) S[E]{k} S=r jS /S{k} For each j, because Φj is differentiable everywhere on except (possibly) at 0 or 1, the derivative of the integrand (14) with respect to at = 0 exists for all but finitely many (0, 1). Thus, the 18 integrand (14) is differentiable at = 0 for almost all (0, 1). Next, observe that for each fixed [E] {k}, the product in (14) has the a.e. derivative (cid:89) jS dt Φc (θjk(v, t)) (cid:89) Φm (θmk(v, t)) (cid:88) = ℓ /S{k} /S{k} (δk δℓ) φℓ (θℓk(v, t)) (cid:89) jS Φc (θjk(v, t)) (cid:89) Φm (θmk(v, t)) /S{k,ℓ} (cid:124) (cid:88) ℓS (cid:123)(cid:122) := Ξ+ S,ℓ (cid:89) (δk δℓ) φℓ (θℓk(v, t)) (cid:125) Φc (θjk(v, t)) (cid:89) Φm (θmk(v, t)) . (cid:124) jS{ℓ} /S{k} (cid:123)(cid:122) := Ξ S,ℓ (cid:125) This leads to telescoping cancellation across in the integrand (14). Specifically, for each fixed < K1 and ℓ, every index set Sr such that Sr = corresponds to another index set Sℓ such r+1 = Sr {ℓ}. It is easy to check that Ξ+ that Sℓ r+1 = + 1 and Sℓ r+ . Sr,ℓ = Ξ Sℓ So, the sum in (14) telescopes over except at the = K1 boundary where Ξ+ has no r+1,ℓ SK1,ℓ corresponding Ξ Sℓ ,ℓ term to cancel with. Hence, for almost all fixed (0, 1), dt φk(v) K1 (cid:88) r=0 (cid:88) (cid:89) Φc (θjk(v, t)) (cid:89) S[E]{k} S=r jS /S{k} Φm (θmk(v, t)) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) t=0 (cid:89) = φk(v) (cid:88) ℓ=k (δk δℓ) φℓ (θℓk(v, 0)) (cid:88) S[E]{k,ℓ} S=K (cid:89) jS Φc (θjk(v, 0)) /S{k,ℓ} Φm (θmk(v, 0)) . Next, for any non-zero R, it is routine to check that the assumptions in Section 5.2 imply that the integrand of the following is uniformly bounded: 1 [πk (p(h)) πk (p(0))] = (cid:90) 0 1 φk(v) K1 (cid:88) r= (cid:88) (cid:89) Φc (θjk(v, h)) (cid:89) Φm (θmk(v, h)) S[E]{k} S=r jS /S{k} φk(v) K1 (cid:88) r= (cid:88) (cid:89) Φc (θjk(v, 0)) (cid:89) S[E]{k} S=r jS /S{k} Φm (θmk(v, 0)) dv Therefore, by the dominated convergence theorem, we have that πk (p(t)) is differentiable at = 0, 19 with the derivative being given (after change of variables) by Dπk(p)[δ] (cid:88) = (δk δℓ) ℓ=k (cid:90) φk(v pk) φℓ (v pℓ) (cid:88) S[E]{k,ℓ} S=K (cid:89) jS Φc j(v pj) (cid:89) Φm(v pm) dv /S{k,ℓ} Finally, by symmetry, D2FK(p)[δ, δ] = dt (cid:88) k= δk πk(p + tδ) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)t=0 = (cid:88) k=1 δk Dπk(p)[δ] (cid:88) (cid:88) = k=1 ℓ=k δk(δk δℓ) w(K) kℓ (p) = (cid:88) w(K) kℓ (p) (δk δℓ)2, 1k<ℓE which is exactly (13)."
        },
        {
            "title": "5.5 Experimentally-Realistic Assumptions on p",
            "content": "Observe that the TopKInd decision of the MoE router is invariant to adding the same constant to all coordinates of p. Motivated by this, we define the zero-sum subspace (cid:40) := RE : (cid:41) zk = , (cid:88) k=1 where is the linear subspace orthogonal to the all-ones vector. Thus, we assume the following about ALF-LB for some update direction δ: p(n+1) ProjZ (cid:16) p(n) δ(cid:17) . (15) Remark 11 (Practicality of Assumptions). The assumption (15) is not artificial; it arises naturally from the problem definition: Zero-sum gradients. Since (cid:80) (p)=T K, the components of the gradient (11) sum to zero: A(n) kf (n)(p) = (cid:88) (cid:88) (A(n) (p) L) = EL = 0. Thus, any update of the form p(n+1) p(n) ϵ(n)f (n)(p(n)) automatically preserves p(n+1) as long as p(n) Z. In practice, we initialize with p(0) = 0, so the projection in (15) is just the identity mapping. Explicit Z-projection with per-coordinate step-sizes."
        },
        {
            "title": "In the more general case where",
            "content": "20 heterogeneous step-sizes ϵ(n) are used across coordinates, p(n+1) p(n) (cid:16) 1 g(n) ϵ(n) 1 , . . . , ϵ(n) g(n) (cid:17) , the updated p(n+1) may not reside in Z. (In fact, the difference between per-coordinate step-sizes and homogeneous step-sizes can be seen in Figure 4 where the per-coordinate ϵ(n) = u/L A(n) step-size results in bias distribution that shifts rightward over time while the homogeneous ϵ(n) = u/n and ϵ(n) = u/ step-sizes result in bias distributions that stay centered around zero.) However, in this per-coordinate step-size case, it is well-known that the projection onto is equivalent to subtracting the componentwise mean: ProjZ (p) = (cid:32) 1 (cid:88) k=1 (cid:33) pk 1, which is computationally negligible. (a) u/L A(n) Step-Size (b) u/n Step-Size (c) u/ Step-Size Figure 4: Time-lapse histograms of the marginal distributions of the ALF-LB biases during the training of 1B-parameter DeepSeekMoE models using different choices of step-size (Section 2.2). No explicit constraints were enforced on p. Section 3 provides experimental details. Additionally, we will make the technical assumption that diam(p) := maxj pj minj pj 1 κ for some constant κ > 0, which we found holds without explicit enforcement in our experiments on 21 1B-parameter DeepSeekMoE models (Figure 4). Thus, we can realistically assume domκ := {p : diam(p) 1 κ}."
        },
        {
            "title": "5.6 Strong Convexity",
            "content": "Next, observe that the component densities of the affinity scores Γ are continuous and strictly positive on (0, 1). Thus, by the continuity of w(K) kℓ (p) in and compactness of domκ , cK(d) := inf pdomκ w(K) kℓ (p) > 0. min k<ℓ Hence, by Proposition 10, δ2FK(p) δ cK(d) (δk δℓ)2. (cid:88) k<ℓ (16) The assumption (15) ensures that p(n) for all n. Thus, since is linear subspace, p(n+1) = ProjZ (cid:16) p(n) δ(cid:17) p(n)(cid:17) (cid:16) = ProjZ = p(n) ProjZ (cid:123)(cid:122) δ (cid:124) ProjZ (cid:0)δ(cid:1) (cid:125) . (cid:0)δ(cid:1) Since the update direction δ lies in Z, (δk δℓ)2 = δ2 (cid:88) k<ℓ (cid:33) δk = δ2. (cid:32) (cid:88) k=1 Combining with property (16), this yields δ2FK(p) δ cK(d)E δ2. Recall the expected loss is (p) = FK(p) (cid:80) curvature; thus, for all p, δ with having diameter at most d, is µK-strongly convex with pk and observe the linear term does not affect µK := cK(d)E. (17)"
        },
        {
            "title": "Consider the minimizer of the expected loss",
            "content": "Since is µK-strongly convex in Z, is necessarily unique. = arg min pZ (p). 22 Define the regret RN := (cid:80)N n= (cid:0)f (n)(p(n)) (n)(p)(cid:1). We now give logarithmic bound on the expected regret E[RN ] with the ALF-LB update p(n+1) ProjZ (cid:16) p(n) ϵ(n) (n) (cid:16) p(n)(cid:17)(cid:17) . (18) While the details are adapted to the specific problem at hand, the proof technique is standard in online convex optimization (see, for example, Hazan (2016, Section 3.3.1)). For clarity, define the following short-hand notations: (cid:104) := p(n)p2(cid:105) , sn := (cid:88) k=1 (cid:16) p(n)(cid:17) πk , an := (cid:104) (cid:16) p(n)(cid:17) (p) (cid:105) , σ2 T,E,K := 2 (cid:16) K2 (cid:17) . Lemma 12 (One-step accounting). Under the assumptions and notations of Section 5.1-5.6, for any ϵ(n)>0, the iteration (18) satisfies 2 an n+1 ϵ(n) µK + ϵ(n) σ2 T,E,K. (19) Proof. Since is linear subspace, the projection operator is nonexpansive. Thus, p(n+1)p2 = p(n) ϵ(n)f (n) (cid:16) p(n)(cid:17)(cid:17) p(cid:13) 2 (cid:13) (cid:13) (cid:16) (cid:13) (cid:13) (cid:13)ProjZ (cid:13) (cid:13)p(n) ϵ(n)f (n) (cid:16) (cid:13) p(n)p2 2ϵ(n)(cid:68) p(n)(cid:17) (n) (cid:16) p(cid:13) 2 (cid:13) (cid:13) p(n)(cid:17) , p(n)p(cid:69) + (cid:16) ϵ(n)(cid:17)2 (cid:13) (cid:13) (cid:13)f (n) (cid:16) p(n)(cid:17)(cid:13) 2 (cid:13) (cid:13) . Taking conditional expectation and using Proposition 8 gives p(n+1)p2 p(n)(cid:105) (cid:104) p(n)p2 2ϵ(n)(cid:68) (p(n)), p(n)p(cid:69) + (ϵ(n))2 (cid:104) (n)(p(n))2 p(n)(cid:105) . Since the TopKInd decision is invariant to adding the same constant to all coordinates of p, we can assume without loss of generality that Z. Thus, since is linear subspace, p(n) Z. Then, the µK-strong convexity of in (Section 5.6) gives (cid:16) (cid:16) p(n)(cid:17) 2 (p) (cid:17) + µK p(n)p2 2 (cid:68) (p(n)), p(n)p(cid:69) . Combining the last two expressions, taking total expectation, and rearranging gives 2 an n+1 ϵ(n) (cid:104) µK + ϵ(n) (n)(p(n))2(cid:105) . (20) Recall from Section 5.1 that the gradient is (n)(p) = A(n)(p) 1 where (cid:80) A(n) (p) = and each A(n) (p) [0, ]. It is then easy to check that, for any p, (n)(p)2 σ2 T,E,K. Substituting this bound into (20) yields the desired result. Theorem 13. (Logarithmic Regret) Consider the update (18) run for iterations with ϵ(n) = 1/(µKn). Then, E[RN ] (1 + ln ). σ2 T,E,K 2µK Proof. Observe that E[RN ] = (cid:80)N n=1 an. Summing (19) over = 1, . . . , gives (cid:88) n=1 an (cid:88) n=1 (cid:18) n+1 ϵ(n) (cid:19) µKn + (cid:88) n=1 ϵ(n)σ T,E,K. The first term on the right-hand side is telescoping sum which evaluates to (cid:88) n=1 (cid:18) n+1 ϵ(n) (cid:19) µKn = (cid:88) n=1 (µKn(n n+1) µKn) = µK (cid:88) n=1 ((n1)n nn+1) = µK +1. Dropping this non-positive term, we are left with (cid:88) n=1 an (cid:88) n=1 ϵ(n) σ T,E,K = σ2 T,E,K µK (cid:88) n=1 1 . Invoking the classic (cid:80)N n=1 1 1+ ln inequality and dividing by 2 yields the desired result. Acknowledgements. The authors thank Alexandre Belloni, John Birge, Rene Caldentey, and Chamsi Hssaine for helpful discussions and feedback during the preparation of this paper."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, and Shyamal Anadkat. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Shipra Agrawal and Nikhil Devanur. Fast algorithms for online stochastic convex programming. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms, pages 14051424. SIAM, 2014. Santiago Balseiro, Haihao Lu, and Vahab Mirrokni. Dual mirror descent for online allocation problems. In International Conference on Machine Learning, pages 613628. PMLR, 2020. Santiago Balseiro, Haihao Lu, and Vahab Mirrokni. Regularized online allocation problems: Fairness and beyond. In International Conference on Machine Learning, pages 630639. PMLR, 2021. Dimitri Bertsekas. Network optimization: continuous and discrete models, volume 8. Athena Scientific, 1998. Dimitri Bertsekas. Auction algorithms for network flow problems: tutorial introduction. Computational optimization and applications, 1(1):766, 1992. Dimitri Bertsekas. Auction algorithms. In Encyclopedia of optimization, pages 128132. Springer, 2008. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, and Yu Wu. Deepseekmoe: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. DeepSeek-AI. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Epoch AI. Key trends and figures in machine learning, 2023. URL https://epoch.ai/trends. Accessed: 2025-09-27. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, and Shibo Wang. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 25 Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157325, 2016. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Hendricks, Johannes Welbl, and Aidan Clark. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Rodolphe Jenatton, Jim Huang, Dominik Csiba, and Cedric Archambeau. Online optimization and regret guarantees for non-additive long-term constraints. arXiv preprint arXiv:1602.05394, 2016. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen, and Yonghui Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2021. Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pages 62656274. PMLR, 2021. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595, 2021. Jaime Sevilla, Lasse Heim, Amanda Askell Ho, Noah Buchan, Alex Snell, Maruan Alhussein, Natasha Jaques McAleese, William Biles, Kevin McKee, and Joey Leung. Compute trends across three eras of machine learning. arXiv preprint arXiv:2202.05924, 2022. 26 Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. URL https://openreview.net/ forum?id=B1ckMDqlg. Strubell, Ganesh, and McCallum. Energy and policy considerations for deep learning in nlp. proceedings of the 57th annual meeting of the association for computational linguistics (acl). Stroudsburg, PA, USA. Association for Computational Linguistics, 2019. Neil Thompson, Kristjan Greenewald, Keeheon Lee, and Gustavo F. Manso. The computational limits of deep learning. arXiv preprint arXiv:2007.05558, 2020. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024. Wenxin Zhang, Santiago Balseiro, Robert Kleinberg, Vahab Mirrokni, Balasubramanian Sivan, and Bartek Wydrowski. Optimal and stable distributed bipartite load balancing. arXiv preprint arXiv:2411.17103, 2024."
        }
    ],
    "affiliations": [
        "Chicago Booth"
    ]
}