{
    "paper_title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation",
    "authors": [
        "Bowen Xue",
        "Qixin Yan",
        "Wenjing Wang",
        "Hao Liu",
        "Chen Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating high-fidelity human videos that match user-specified identities is important yet challenging in the field of generative AI. Existing methods often rely on an excessive number of training parameters and lack compatibility with other AIGC tools. In this paper, we propose Stand-In, a lightweight and plug-and-play framework for identity preservation in video generation. Specifically, we introduce a conditional image branch into the pre-trained video generation model. Identity control is achieved through restricted self-attentions with conditional position mapping, and can be learned quickly with only 2000 pairs. Despite incorporating and training just $\\sim$1% additional parameters, our framework achieves excellent results in video quality and identity preservation, outperforming other full-parameter training methods. Moreover, our framework can be seamlessly integrated for other tasks, such as subject-driven video generation, pose-referenced video generation, stylization, and face swapping."
        },
        {
            "title": "Start",
            "content": "Stand-In: Lightweight and Plug-and-Play Identity Control for Video Generation Bowen Xue*, Qixin Yan*, Wenjing Wang, Hao Liu, Chen Li WeChat Vision, Tencent Inc. bowenxue2005@gmail.com, {qixinyan,augustawang,leweshaoliu,chaselli}@tencent.com 5 2 0 2 2 1 ] . [ 2 1 0 9 7 0 . 8 0 5 2 : r Figure 1: Given reference image, our method generates videos with strong identity preservation. Furthermore, the frameworks plug-and-play design enables seamless integration into diverse applications for enhanced identity consistency. Abstract Generating high-fidelity human videos that match userspecified identities is important yet challenging in the field of generative AI. Existing methods often rely on an excessive number of training parameters and lack compatibility with other AIGC tools. In this paper, we propose StandIn, lightweight and plug-and-play framework for identity preservation in video generation. Specifically, we introduce conditional image branch into the pre-trained video generation model. Identity control is achieved through restricted self-attentions with conditional position mapping, and can be learned quickly with only 2000 pairs. Despite incorporating and training just 1% additional parameters, our framework achieves excellent results in video quality and identity preservation, outperforming other full-parameter training methods. Moreover, our framework can be seamlessly integrated for * Equal contribution. other tasks, such as subject-driven video generation, posereferenced video generation, stylization, and face swapping. Introduction With the rapid advancement of diffusion models (Ho, Jain, and Abbeel 2020; Podell et al. 2024; Peebles and Xie 2023), video generation (Zheng et al. 2024; Peng et al. 2025; Kong et al. 2024; Hong et al. 2023) has become pivotal aspect of generative AI. Among its diverse applications, identitypreserving video generation holds profound significance. The goal of this task is to generate high-quality videos that consistently maintain the identity of given reference image containing face. It has widespread utility across film, advertising and gaming industries, etc. Existing methods can be classified into two categories. Traditional methods (He et al. 2024; Yuan et al. 2025b) use 1 extractors, we introduce conditional image branch to the video generation model. The image and video branches share information through restricted self-attention with conditional position mapping. With this lightweight design, identity preservation can be learned well only with small dataset. The proposed framework exhibits high compatibility and generalizability. Although trained only on real-people data, our method generalizes to other subjects, such as cartoons and objects. Moreover, our method can be plugand-play applied to other tasks, such as pose-guided video generation, video stylization and face swapping. Our framework, dataset, and more can be found in our website: https://stand-in-video.github.io/. Related Work Video Generation Models Current video generation models are predominantly built on diffusion frameworks (Ho, Jain, and Abbeel 2020), with notable evolution in architecture from U-Net-based designs (Blattmann et al. 2023) to DiT-based approaches (Kong et al. 2024; Team 2025; Ma et al. 2025). In the era of U-Net-based diffusion models, text-to-image (T2I) frameworks (Rombach et al. 2022; Podell et al. 2024) were extended to video generation by introducing 3D convolutions and temporal attention (Blattmann et al. 2023). AnimateDiff (Guo et al. 2024) further advanced this direction by reusing pre-trained text-toimage model weights to leverage their strong spatial generation capabilities by adding temporal layers. Latte (Ma et al. 2025) introduced spatial-temporal separation mechanism, assigning distinct DiT blocks to process spatial and temporal information independently. This approach was later replaced by 3D full attention mechanisms, which offered more integrated processing. CogVideoX (Yang et al. 2025) and HunyuanVideo (Kong et al. 2024) combined 3D-VAE (Yu et al. 2024) with MM-DiT (Esser et al. 2024) to enhance video generation capabilities. WAN2.1 (Team 2025) employs 3D-VAE and adopts DiT backbone for denoising, injecting semantic prompt information into the diffusion process through cross-attention. Identity-Preserving Generation Conventional methods commonly rely on explicit face encoders for facial feature extraction to generate videos with identity-preserving. IDanimator (He et al. 2024) leverages pre-trained text-tovideo diffusion model in conjunction with lightweight face adapter to encode ID-relevant embeddings from adaptable facial latent queries. ConsistID (Yuan et al. 2025b) aims to maintain identity consistency through frequency decomposition in diffusion transformer. Phantom (Liu et al. 2025) can also preserve identity consistency in the human domain as unified subject-consistent video generation framework. HunyuanCustom (Hu et al. 2025) is multi-modal customized video generation framework that emphasizes identity consistency while supporting diverse input modalities. By introducing advanced condition injection mechanisms and identity-preserving strategies, it achieves excellent performance in high-quality video generation. They employed Figure 2: Comparison with SOTA identity-preserving video generation methods. The size of bubbles represents the number of need-to-train parameters for identity preservation. Our approach achieves the highest performance in both face similarity and naturalness, while utilizing the fewest parameters. an explicit face encoder for identity feature extraction. Recent methods (Hu et al. 2025; Liu et al. 2025) fully train the diffusion transformer. However, face-encoder-based methods lack flexibility and struggle to capture fine facial details essential for high-quality video generation. The fullparameter training methods require large number of parameters and lack compatibility with other applications. Achieving robust identity preservation in lightweight and flexible way remains critical yet challenging. To overcome these limitations, we leverage the pretrained VAE from the video generation model itself, enabling the conditional image to be mapped directly into the same latent space as the video. This approach naturally utilizes the models inherent capabilities to extract rich and detailed facial features, offering more integrated and effective solution. Furthermore, to achieve lightweight design and avoid introducing an excessive number of parameters, we employ restricted self-attention with conditional position mapping. On the one hand, it can effectively merge the features of the reference image into the video; on the other hand, it does not alter the architecture of the main video generation model and thus can be used in plug-and-play manner for other applications, as shown in Figure 1. Experiments demonstrate that our method achieves the highest facial similarity and naturalness in identity-preserving video generation, with the least number of need-to-train parameters, as shown in Figure 2. Thanks to its plug-and-play ability, our framework can be extended to various tasks, including subject-driven generation, video stylization, and face swapping, all while preserving identity consistency. Additionally, by integrating compatibility with VACE (Jiang et al. 2025), our approach significantly enhances facial similarity in pose-guided video generation. Our main contributions can be summarized as follows: We present Stand-In, lightweight and plug-and-play framework designed for identity-preserving video generation. By incorporating and training just 1% additional parameters, our approach achieves SOTA results in identity preservation, video quality, and prompt following. To inject identity information without explicit face feature 2 full fine-tuning for the diffusion transformer, resulting in large number of trainable parameters. Method In this section, we first introduce the overall framework of the proposed method. Next, we detail the restricted selfattention mechanism with conditional position mapping. Finally, we present the data collection process. Conditional Image Branch To extract facial features, traditional methods rely on explicit face encoders, which lack flexibility and often fail to preserve fine facial details critical for high-quality reconstruction. In contrast, we propose using the pre-trained VAE from the video generation model. This strategy maps the conditional image directly into the same latent space as the video and allows us to naturally take advantage of the built-in ability of the pre-trained video generation model to extract rich facial features. The overall framework is illustrated in Figure 3. We use Wan2.1 14B T2V (Team 2025) as the video generation base model, which adopts Diffusion Transformer (DiT) architecture. Given reference image containing face, we first encode it into the latent space using the pre-trained VAE encoder. The image latents undergo the same patchification and encoding procedures as the video latents. Then, the image tokens are concatenated with video tokens along the sequence dimension and processed jointly through successive blocks. Finally, image tokens are discarded at the final layer. Let denote the denoising timestep in diffusion. To preserve the static nature of the reference image, which serves as conditioning input rather than undergoing denoising, we maintain its temporal invariance. This is done by fixing its timestep to zero, i.e., sref = 0. Now that we have encoded the conditional image into the same feature space as the video, the next challenge is: How can the video features effectively refer to the image information in way that is lightweight and easy to learn? Restricted Self-Attention In the aforementioned DiT blocks, reference image and video tokens are processed independently through most modules (including layer normalization, cross-attention, and feed-forward networks), except for the self-attention layer. The self-attention layer enables information exchange among all tokens, naturally allowing video tokens to refer to identity information. However, since the reference image serves as static condition, it should remain unaffected by the dynamic contents of the video. Therefore, to incorporate identity information while preserving its independence, we propose replacing the self-attention layers in DiT with restricted version that explicitly prevents image queries from attending to video keys. As shown in Figure 4, for self-attention layer, we first independently compute the Query, Key, and Value for image and video tokens, denoted as QI , KI , VI and QV , KV , VV respectively. Then, we concatenate KV with KI and Figure 3: The overview of our identity-preserving text-tovideo generation framework. We introduce conditional image branch alongside the original video branch. Given the conditional image, the VAE encoder maps it into tokens, which are concatenated with the video latent tokens and then sent to the DiT. Within the DiT blocks, identity information is incorporated into the video features through restricted self-attention. VV with VI for QV . To enhance the models ability to utilize identity-related information while preserving its inherent generative robustness, we incorporate Low-Rank Adaptation (LoRA) into the QKV projection of image tokens. Conditional Position Mapping To effectively differentiate image and video tokens in the restricted self-attention, we use specialized conditional position mapping strategy. Specifically, we employ 3D Rotary Positional Embedding (RoPE) (Su et al. 2024), where all tokens associated with the reference image are assigned distinct and dedicated coordinate space. This ensures clear separation and facilitates precise modeling of interactions between the reference image and video tokens. For the temporal dimension, we assign fixed temporal index of -1 to the reference image tokens, while mapping video tokens to nonnegative temporal positions. This assignment establishes image tokens as temporally invariant conditional inputs. In this way, the model is encouraged to treat the identity information from the reference image as constant guide throughout the entire denoising process, rather than conflating it with transient, frame-specific features in the temporal sequence. For spatial dimensions, we employ disjoint coordinate strategy to enforce spatial decoupling between reference image and video content. While video frames occupy coordinates within the domain (h, w) [0, HV ) [0, WV ), we map reference image tokens to dedicated coordinate subspace [HV , HV + HI ) [WV , WV + WI ), where HI and WI represent the reference image dimensions. This non-overlapping spatial allocation achieves two main goals through geometric separation. By separating the reference tokens from the video coordinate grid, the design naturally reduces false spatial correlations that could 3 Figure 4: Design of our restricted self-attention: For the input video and image tokens, we compute their Query, Key, and Value matrices independently. Next, we apply 3D RoPE to the Query and Key matrices. Finally, the image matrices operate independently, while the video Query performs attention using the concatenation of the image and video Key and Value matrices. sources. The dataset guarantees diverse and comprehensive representation, comprising various ethnic groups, age ranges, gender identities, and wide array of actions. Using the VILA (Lin et al. 2024) multimodal captioning framework, we automatically generate dense textual annotations for each video, establishing strong text-video alignment. To align the dataset with the pre-training distribution of our video generation base model (Team 2025) and to mitigate potential degradation in generation quality, we preprocess the videos as follows: each video is resampled to 25 FPS, then cropped and resized to resolution of 832480 pixels. From these processed videos, we randomly sample clips of 81 consecutive frames for training. For each video clip, the corresponding reference facial image is extracted from the original, pre-resampled video. The extraction pipeline is as follows: 1. 5 frames are randomly selected from the original video. 2. Face regions are detected and cropped using RetinaFace(Deng et al. 2020). 3. The cropped face images are resized to 512512 pixels. 4. BiSeNet (Yu et al. 2018) is used for face parsing, and the background is replaced with solid white color to prevent any leakage of background information. Examples of the final image-text-video pairs for our training can be found in Figure 5."
        },
        {
            "title": "Experiments",
            "content": "Implementation Details We adopt LoRA with rank 128, applied only to the QKV projections for image tokens in each DiT block. For the 14Bparameter Wan2.1 model, this adds just 153M trainable parameters (1% of the base model), increasing feed-forward time by 3.6% and FLOPs by 2.6%. During inference with KV caching, overhead is minimal: runtime rises by only 2.3% and FLOPs by 0.07% compared to the video generation base model. This negligible cost shows our identitypreserving method is lightweight. The model is trained over 3000 steps on Nvidia H20 GPUs with batch size of 48. For inference, BiSeNet is adopted as an automatic preprocessing step. Figure 5: Examples from our human-centric video dataset. lead the model to rely too much on superficial pixel-level matches. At the same time, this separate coordinate system maintains the reference images semantic meaning by keeping it as global identity prior. Consequently, the model is guided to focus on extracting overall semantic features from the reference tokens, rather than treating them as spatially localized patterns that need to align positionally with the video content. Denoting pI as the coordinate for image tokens and pV for video tokens, we apply 3D RoPE as follows: = KI pI , = KV pV . = QI pI , = QV pV , (2) where denotes the Hadamard product. The restricted selfattention outputs are computed as: , OutI = Attention(Q , [K OutV = Attention(Q where [, ] denotes concatenation. , VI ), , ], [VV , VI ]), (1) (3) (4) KV Caching Given that the timestep for the conditional image is fixed at sref = 0, its Key and Value matrices remain constant throughout the diffusion denoising process. Therefore, during inference, we can cache KI and VI to accelerate computation. These matrices are computed and stored during the first denoising step, eliminating the need for redundant recalculations in subsequent steps. Dataset Collection and Processing We construct human-centric video dataset containing 2,000 high-resolution sequences from publicly available"
        },
        {
            "title": "Method",
            "content": "Kling Hailuo Pika-2.1 Vidu-2.0 ID-Animator SkyReels-A2-P14B EchoVideo ConcatID-CogVideoX ConcatID-WAN-AdaLN Hunyuan-Custom ConsistID VACE-P1.3B VACE-1.3B VACE-14B Phantom-1.3B Phantom-14B Stand-In (Ours) Open-Source Face Similarity Naturalness Prompt Following 0.410 0.577 0.323 0.361 0.316 0.546 0.487 0.439 0.501 0.622 0.432 0.180 0.223 0.647 0.440 0.519 0. 3.900 3.750 3.644 3.600 3.211 3.411 3.456 3.372 3.650 3.367 3.233 3.567 3.611 3.728 3.567 3.828 3.922 19.921 20.649 20.649 18.998 16.677 19.110 19.263 19.359 19.671 19.853 20.552 20.591 20.527 19.520 20.364 20.476 20.594 Table 1: Quantitative comparison with state-of-the-art identity-preserving video generation methods. We evaluate across three key metrics: Face Similarity, Naturalness, and Prompt Following. For all metrics, higher values indicate better performance. The best and second-best results in each column are highlighted in bold and underlined, respectively. Method Face Similarity Video Quality Hunyuan-Custom VACE-14B Phantom-14B ConsistID Kling Stand-In (Ours) 3.34 3.00 2.37 2.25 2.21 4.10 2.92 3.07 2.92 2.46 3.09 4.08 Table 2: User study results for subjective evaluation. The best and second-best results in each column are highlighted in bold and underlined, respectively. Quantitative Analysis To evaluate identity preservation and visual quality, we use the two most important and heavily weighted evaluation metrics from the OpenS2V benchmark (Yuan et al. 2025a): facial similarity and naturalness. To evaluate the relevance between generated video and textual description, we use XCLIP (Ma et al. 2022), pre-trained video-text multimodal model. Results are shown in Table 1 and Figure 6. Face Similarity This metric evaluates the models ability to maintain identity consistency. It is calculated as the average cosine similarity between the CurricularFace embeddings of the reference image and the faces detected in the generated video frames. As shown in Table 1, our proposed method, Stand-In, achieves score of 0.724, outperforming all other compared methods. This result demonstrates the effectiveness of our approach in generating facial features that remain highly consistent with the source identity. Naturalness This metric is primarily designed to evaluate the naturalness of the generated videos. It leverages GPT4o to approximate human judgment of video realism, taking into account factors such as physical plausibility and the absence of noticeable AI artifacts. Following the OpenS2V protocol, holistic score ranging from 1 to 5 is assigned. In this evaluation, our method achieves score of 3.922, demonstrating that the improvements in identity fidelity are achieved without compromising the overall visual realism of the generated videos. Prompt Following As shown in Table 1, our method ranks second among all compared methods and first among open-source methods. This result demonstrates that our identity-preserving ability can be achieved without hurting the prompt-following performance."
        },
        {
            "title": "User Study",
            "content": "The user study involves 20 participants. We randomly select 10 test videos from the benchmark and ask the participants to rate each video across two dimensions: facial similarity and video quality. The latter dimension encompasses aspects such as naturalness, visual aesthetics, and alignment with the provided text descriptions. Ratings are given on 5-point scale (1 to 5), and the final scores for each dimension are obtained by averaging the ratings across all participants and test videos. As shown in Table 2, our method outperforms the comparative methods. Plug-and-Play to Other Applications Subject-Driven Video Generation Although trained only with human data, our framework can be zero-shot applied to non-human subjects without any additional fine-tuning. This is because we use the pretrained VAE and video generation model to extract rich features, and learn alignment through 5 Figure 6: Comparison on identity-preserving video generation. Please refer to the supplementary material for full prompts. Figure 7: Our results on subjects other than real-person. Please refer to the supplementary material for full prompts. paired data and an effective attention mechanism. This zeroshot ability can hardly be achieved by traditional identitypreserving methods relying on face encoders. As shown in Figure 7, our method exhibits strong subject consistency on the teddy bear object and the cartoon character. Pose-Guided Video Generation The proposed conditional image branch is designed based on the LoRA module, which ensures inherent compatibility with other DiTbased models. To validate this, we conducted experiments on the pose-guided video generation task using the VACE framework (Jiang et al. 2025). As illustrated in Figure 8, integrating our method significantly improves the facial identity similarity in the generated videos. This not only demonstrates the plug-and-play nature of our approach but also highlights its robustness in preserving identity consistency. Video Stylization By applying our framework in conjunction with video stylization LoRA, we demonstrate its ability to achieve effective style transfer while maintaining strong identity consistency. As shown in Figure 10, our method successfully renders the artistic style as well as preserving the facial features of the reference image, further showing its versatility and robustness. Video Face Swapping Our framework is also capable of video face swapping, which can be achieved via zero-shot inpainting (Lugmayr et al. 2022). Figure 9 shows that our method not only achieves high-quality facial identity transfer but also maintains strong temporal consistency across frames, resulting in coherent and high-quality videos. Figure 8: Comparison on pose-guided video generation against VACE. Figure 9: Application of our model in video face swapping. Method Face Similarity Naturalness Prompt Following w/o RSA w/o CPM Full Model 0.022 0.021 0.724 3.528 3.539 3. 19.956 19.901 20.594 Table 3: Ablation study on the core components of our method. The results show that both the Restricted SelfAttention (RSA) and the Conditional Position Mapping (CPM) are crucial for achieving high performance. Figure 10: Our model applied with stylization LoRA. ing that conditional position mapping is essential for achieving accurate and robust alignment in our framework."
        },
        {
            "title": "Ablation Study",
            "content": "Effectiveness of Restricted Self-Attention We perform an experiment by removing the LoRA module from the conditional image branch. In this setting, the video latent tokens gather information from the reference image tokens exclusively through training-free self-attention mechanism. Experimental results in Table 3 demonstrate that this approach fails to preserve identity consistency, confirming the necessity of our restricted self-attention design in the framework. Effectiveness of Conditional Position Mapping To validate the effectiveness of the proposed conditional position mapping, we perform an ablation study by removing the reference images positional mapping in the 3D RoPE mechanism and retraining the conditional image branch. As shown in Table 3, experimental results demonstrate that eliminating this mapping significantly degrades performance, confirm-"
        },
        {
            "title": "Conclusion",
            "content": "We propose Stand-In, lightweight, plug-and-play framework for high-fidelity, identity-preserving video generation. We introduce conditional image branch into pre-trained video generation model, and propose restricted attention mechanism with conditional positional encoding to enable cross-branch information exchange. Despite training only 1% of the models additional parameters on limited dataset of 2,000 pairs, our approach achieves high-quality video generation while maintaining strong identity fidelity. Experimental results demonstrate that Stand-In achieves state-ofthe-art performance in identity-preserving text-to-video generation. Furthermore, it exhibits excellent performance on other tasks, including pose-guided video generation, stylization, and face swapping, proving its strong compatibility and broad application potential. 7 References Blattmann, A.; Dockhorn, T.; Kulal, S.; Mendelevitch, D.; Kilian, M.; Lorenz, D.; Levi, Y.; English, Z.; Voleti, V.; Letts, A.; Jampani, V.; and Rombach, R. 2023. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets. ArXiv preprint. Deng, J.; Guo, J.; Ververas, E.; Kotsia, I.; and Zafeiriou, S. 2020. RetinaFace: Single-Shot Multi-Level Face LocalisaIn Conference on Computer Vision and tion in the Wild. Pattern Recognition. Esser, P.; Kulal, S.; Blattmann, A.; Entezari, R.; Muller, J.; Saini, H.; Levi, Y.; Lorenz, D.; Sauer, A.; Boesel, F.; Podell, D.; Dockhorn, T.; English, Z.; and Rombach, R. 2024. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. In International Conference on Machine Learning. Guo, Y.; Yang, C.; Rao, A.; Liang, Z.; Wang, Y.; Qiao, Y.; Agrawala, M.; Lin, D.; and Dai, B. 2024. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. In International Conference on Learning Representations. He, X.; Liu, Q.; Qian, S.; Wang, X.; Hu, T.; Cao, K.; Yan, K.; and Zhang, J. 2024. ID-Animator: Zero-Shot IdentityPreserving Human Video Generation. ArXiv preprint. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising Diffusion Probabilistic Models. In Conference and Workshop on Neural Information Processing Systems. Hong, W.; Ding, M.; Zheng, W.; Liu, X.; and Tang, J. 2023. CogVideo: Large-scale Pretraining for Text-to-Video GenIn International Conference on eration via Transformers. Learning Representations. Hu, T.; Yu, Z.; Zhou, Z.; Liang, S.; Zhou, Y.; Lin, Q.; and Lu, Q. 2025. HunyuanCustom: Multimodal-Driven Architecture for Customized Video Generation. ArXiv preprint. Jiang, Z.; Han, Z.; Mao, C.; Zhang, J.; Pan, Y.; and Liu, Y. 2025. VACE: All-in-One Video Creation and Editing. ArXiv preprint. Kong, W.; Tian, Q.; Zhang, Z.; Min, R.; Dai, Z.; Zhou, J.; Xiong, J.; Li, X.; Wu, B.; Zhang, J.; et al. 2024. Hunyuanvideo: systematic framework for large video generative models. ArXiv preprint. Lin, J.; Yin, H.; Ping, W.; Molchanov, P.; Shoeybi, M.; and Han, S. 2024. VILA: On Pre-training for Visual Language In Conference on Computer Vision and Pattern Models. Recognition. Liu, L.; Ma, T.; Li, B.; Chen, Z.; Liu, J.; Li, G.; Zhou, S.; He, Q.; and Wu, X. 2025. Phantom: Subject-consistent video generation via cross-modal alignment. ArXiv preprint. Lugmayr, A.; Danelljan, M.; Romero, A.; Yu, F.; Timofte, R.; and Gool, L. V. 2022. RePaint: Inpainting using Denoising Diffusion Probabilistic Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, 11451 11461. IEEE. Ma, X.; Wang, Y.; Chen, X.; Jia, G.; Liu, Z.; Li, Y.-F.; Chen, C.; and Qiao, Y. 2025. Latte: Latent Diffusion Transformer for Video Generation. Transactions on Machine Learning Research. Ma, Y.; Xu, G.; Sun, X.; Yan, M.; Zhang, J.; and Ji, R. 2022. X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval. In ACM International Conference on Multimedia. Peebles, W.; and Xie, S. 2023. Scalable Diffusion Models In International Conference on Comwith Transformers. puter Vision. Peng, X.; Zheng, Z.; Shen, C.; Young, T.; Guo, X.; Wang, B.; Xu, H.; Liu, H.; Jiang, M.; Li, W.; Wang, Y.; Ye, A.; Ren, G.; Ma, Q.; Liang, W.; Lian, X.; Wu, X.; Zhong, Y.; Li, Z.; Gong, C.; Lei, G.; Cheng, L.; Zhang, L.; Li, M.; Zhang, R.; Hu, S.; Huang, S.; Wang, X.; Zhao, Y.; Wang, Y.; Wei, Z.; and You, Y. 2025. Open-Sora 2.0: Training CommercialLevel Video Generation Model in 200k. ArXiv preprint. Podell, D.; English, Z.; Lacey, K.; Blattmann, A.; Dockhorn, T.; Muller, J.; Penna, J.; and Rombach, R. 2024. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. In International Conference on Learning Representations. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In Conference on Computer Vision and Pattern Recognition. Su, J.; Ahmed, M. H. M.; Lu, Y.; Pan, S.; Bo, W.; and Liu, Y. 2024. RoFormer: Enhanced transformer with Rotary Position Embedding. Neurocomputing. Team, W. 2025. Wan: Open and Advanced Large-Scale Video Generative Models. ArXiv preprint. Yang, Z.; Teng, J.; Zheng, W.; Ding, M.; Huang, S.; Xu, J.; Yang, Y.; Hong, W.; Zhang, X.; Feng, G.; Yin, D.; Zhang, Y.; Wang, W.; Cheng, Y.; Xu, B.; Gu, X.; Dong, Y.; and Tang, J. 2025. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. In International Conference on Learning Representations. Yu, C.; Wang, J.; Peng, C.; Gao, C.; Yu, G.; and Sang, N. 2018. BiSeNet: Bilateral Segmentation Network for RealTime Semantic Segmentation. In European Conference on Computer Vision. Yu, L.; Lezama, J.; Gundavarapu, N. B.; Versari, L.; Sohn, K.; Minnen, D.; Cheng, Y.; Gupta, A.; Gu, X.; Hauptmann, A. G.; Gong, B.; Yang, M.; Essa, I.; Ross, D. A.; and Jiang, L. 2024. Language Model Beats Diffusion - Tokenizer is In International Conference on key to visual generation. Learning Representations. Yuan, S.; He, X.; Deng, Y.; Ye, Y.; Huang, J.; Lin, B.; Luo, J.; and Yuan, L. 2025a. OpenS2V-Nexus: Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation. ArXiv preprint. Yuan, S.; Huang, J.; He, X.; Ge, Y.; Shi, Y.; Chen, L.; Luo, J.; and Yuan, L. 2025b. Identity-Preserving Text-to-Video Generation by Frequency Decomposition. In Conference on Computer Vision and Pattern Recognition. Zheng, Z.; Peng, X.; Yang, T.; Shen, C.; Li, S.; Liu, H.; Zhou, Y.; Li, T.; and You, Y. 2024. Open-sora: Democratizing efficient video production for all. ArXiv preprint."
        },
        {
            "title": "Supplementary Materials for",
            "content": "Stand-In: Lightweight and Plug-and-Play Identity Control for Video Generation"
        },
        {
            "title": "E More Results",
            "content": "10 15 16 17 18 We provide more comparisons of our model with other identity-preserving video generation methods in Figures 1115."
        },
        {
            "title": "A More Comparison Results",
            "content": "Figure 11: Visual comparison with other cutting-edge methods. Our method, Stand-In, demonstrates superior identity fidelity and detail preservation (1/5). 10 Figure 12: Visual comparison with other cutting-edge methods. Our method, Stand-In, demonstrates superior identity fidelity and detail preservation (2/5). 11 Figure 13: Visual comparison with other cutting-edge methods. Our method, Stand-In, demonstrates superior identity fidelity and detail preservation (3/5). Figure 14: Visual comparison with other cutting-edge methods. Our method, Stand-In, demonstrates superior identity fidelity and detail preservation (4/5). 13 Figure 15: Visual comparison with other cutting-edge methods. Our method, Stand-In, demonstrates superior identity fidelity and detail preservation (5/5)."
        },
        {
            "title": "B More Ablation Study Results",
            "content": "The visual results of our ablation study, presented in Figure 16, together with the quantitative metrics from the main text, demonstrate that both Restricted Self-Attention and Conditional Position Mapping are crucial to our method. standard, training-free self-attention mechanism is insufficient to establish effective information exchange between the video latents and the conditional image. Furthermore, omitting the positional mapping for the conditional image makes this reference relationship exceedingly difficult for the model to learn. Figure 16: Visual results of the ablation study."
        },
        {
            "title": "C Pseudocode",
            "content": "To more clearly and systematically present the core inference mechanism of the Stand-In method, we formalize its complete inference procedure in Algorithm 1. This pseudocode illustrates how the model jointly leverages the reference image during the diffusion process to preserve identity consistency, thereby enabling high-quality, identity-preserving video generation. Algorithm 1: Inference Process of Stand-In Input: Reference image tokens TI , initial video latents TVN , list of timesteps = {sN , sN 1, ..., s1}, DiT model M. Parameter: Zero timestep for the reference image, sref = 0; Positional Encodings pI , pV . Output: Denoised video latents TV0. 1: Initialize an empty cache for each DiT block: Ck, Cv {}, {} 2: TV TVN 3: Let is irst step true 4: for in do 5: for each block in model do if is irst step then Compute (QI , KI , VI ) Bbefore self-attn(TI , sref ) Store KI , VI in cache: Ck[B] KI , Cv[B] VI else Retrieve from cache: KI , VI Ck[B], Cv[B] ) APPLYROPE((QV , KV ), pV ) ) APPLYROPE((QI , KI ), pI ) end if Compute (QV , KV , VV ) Bbefore self-attn(TV , s) // Self-attention start (Q , (Q , cat Concat(K OutI Attention(Q OutV Attention(Q // Self-attention end (TV , TI ) Bafter self-attn(OutV , OutI ) , VI ) cat, Vcat) , , ), Vcat Concat(VV , VI ) , 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end for is irst step false Predict denoised TV using scheduler and update for next step. 23: 24: end for 25: return TV"
        },
        {
            "title": "D Complete Video Generation Prompts",
            "content": "In this section, we present the complete prompts for Figures in the main paper. Figure 6: Comparison on identity-preserving video generation Left Panel: The video features man standing at an easel, focused intently as his brush dances across the canvas. His expression is one of deep concentration, with hint of satisfaction as each brushstroke adds color and form. He wears paint-splattered apron, and his hands move with confident precision. The setting, filled with scattered art supplies, open paint tubes, and unfinished sketches pinned to the wall, suggests an artists studio. large window on one side allows sunlight to stream in, casting soft glow across the room and illuminating the colors on his canvas. The atmosphere is creative and inspired, with the mans intense focus and the lively colors on the canvas indicating moment of artistic passion and expression. Right Panel: The video features man with dark-haired hair, wearing blue tank top and holding pink tank top on hanger. he appears to be in clothing store or similar retail environment, as there are racks of clothes visible in the background. The man is speaking to the camera, possibly providing review or discussing the tank top he is holding. He has colorful bracelets on his wrist and is wearing necklace with multiple beads. his expression suggests he is engaged in conversation or presentation. The setting seems to be indoors, with artificial lighting illuminating the scene. Figure 7: Our results on subjects other than real-person Left Panel: Shot in medium shot of brightly lit room, girl, approximately seven or eight years old, stands in the center. She has long black hair and wears light blue dress, her expression focused and gentle. Holding doll in both hands, she presents her beloved toy to the camera. As the camera slowly zooms in, the details of her face are clearly visible: the soft fabric, the delicate stitching, and the slightly upturned corners of her mouth are all captured. The entire scene is filled with childlike innocence and warmth. Right Panel: The video features an anime girl standing on busy street, surrounded by hurried crowd. The buildings and shops in the background create classic cityscape. The girl smiles as she puts her headphones on, her movements smooth and natural. Her expression is playful and relaxed, as if shes about to immerse herself in her favorite music. The camera focuses on her face, capturing her joyful expression and vibrant energy. The background is slightly blurred, emphasizing the contrast between her and her surroundings, creating sense of relaxed urban living. Figure 9: Application of our model in video face swapping Top Panel: woman is seated comfortably at desk, facing the camera directly as if engaged in video call with friend or family member. Her gaze is intent and warm, complemented by natural smile. The background consists of personalized space, with photographs and world map adorning the wall, thereby communicating feeling of intimate and contemporary connection. Bottom Panel: man is seated in comfortable armchair, holding thick, aged book on his lap. Drawn by the cameras presence, he looks up, his face bearing gentle and curious smile as if mildly interrupted. Soft, lateral lighting illuminates the scene. He is surrounded by floor-to-ceiling bookshelves, creating tranquil atmosphere enveloped by knowledge and narrative."
        },
        {
            "title": "E More Results",
            "content": "Figure 17: We present more visual results of Stand-In (1/2). 18 Figure 18: We present more visual results of Stand-In (2/2)."
        }
    ],
    "affiliations": [
        "WeChat Vision, Tencent Inc."
    ]
}