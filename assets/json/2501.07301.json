{
    "paper_title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning",
    "authors": [
        "Zhenru Zhang",
        "Chujie Zheng",
        "Yangzhen Wu",
        "Beichen Zhang",
        "Runji Lin",
        "Bowen Yu",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Process Reward Models (PRMs) emerge as a promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate current-step correctness, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to a misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop a consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates a more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release a new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models."
        },
        {
            "title": "Start",
            "content": "2025-01-14 Zhenru Zhang Chujie Zheng Yangzhen Wu Beichen Zhang Runji Lin Bowen Yu Dayiheng Liu Jingren Zhou Junyang Lin Qwen Team, Alibaba Group https://hf.co/Qwen/Qwen2.5-Math-PRM-7B https://hf.co/Qwen/Qwen2.5-Math-PRM-72B"
        },
        {
            "title": "Abstract",
            "content": "Process Reward Models (PRMs) emerge as promising approach for process supervision in mathematical reasoning of Large Language Models (LLMs), which aim to identify and mitigate intermediate errors in the reasoning processes. However, the development of effective PRMs faces significant challenges, particularly in data annotation and evaluation methodologies. In this paper, through extensive experiments, we demonstrate that commonly used Monte Carlo (MC) estimation-based data synthesis for PRMs typically yields inferior performance and generalization compared to LLM-as-a-judge and human annotation methods. MC estimation relies on completion models to evaluate currentstep correctness, which can generate correct answers from incorrect steps or incorrect answers from correct steps, leading to inaccurate step verification. Furthermore, we identify potential biases in conventional Best-of-N (BoN) evaluation strategies for PRMs: (1) The unreliable policy models generate responses with correct answers but flawed processes, leading to misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) The tolerance of PRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have significant proportion of minimum scores concentrated on the final answer steps, revealing the shift from process to outcome-based assessment in BoN Optimized PRMs. To address these challenges, we develop consensus filtering mechanism that effectively integrates MC estimation with LLM-as-a-judge and advocates more comprehensive evaluation framework that combines response-level and step-level metrics. Based on the mechanisms, we significantly improve both model performance and data efficiency in the BoN evaluation and the step-wise error identification task. Finally, we release new state-of-the-art PRM that outperforms existing open-source alternatives and provides practical guidelines for future research in building process supervision models. 5 2 0 2 3 1 ] . [ 1 1 0 3 7 0 . 1 0 5 2 : r Figure 1: Overview of evaluation results on the Best-of-8 strategy of the policy model Qwen2.5-Math-7BInstruct and the benchmark PROCESSBENCH (Zheng et al., 2024) across multiple PRMs (see Table 6 and Table 7 for details). Corresponding authors."
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) have made remarkable advances in mathematical reasoning (OpenAI, 2023; Dubey et al., 2024; Shao et al., 2024; Zhu et al., 2024; Yang et al., 2024a;c;b), yet they can make mistakes, such as miscalculations or logical errors, leading to wrong conclusions. Moreover, even when achieving correct final answers, these powerful models can still regularly make up plausible reasoning steps, where the final answers build upon flawed calculations or derivations, which undermine the reliability and trustworthiness of LLMs reasoning processes. To address these challenges, Process Reward Models (PRMs; Lightman et al. 2023; Wang et al. 2024b), as representative and recently focal approach, are proposed to identify and mitigate process errors, thereby enabling finer-grained supervision on the reasoning process. One critical challenge of developing PRMs lies in the data annotation for the correctness of reasoning processes, which is typically expensive and time-consuming. While Lightman et al. (2023) recruited human annotators with detailed instructions and elaborate procedures to achieve satisfactory annotation quality, the prohibitive cost pushes researchers to explore automated annotation methods. Among them, one commonly used approach is to assess process correctness by estimating the empirical probability of leading to the correct final answers through Monte Carlo (MC) methods, which has attracted great research interests and has also been commonly employed in practice (Xiong et al., 2024; Wang et al., 2024b; Luo et al., 2024). Another challenge lies in evaluating PRM performance, as previous studies (Lightman et al., 2023; Wang et al., 2024b; Luo et al., 2024) have predominantly relied on the Best-of-N (BoN) evaluation, which selects the highest-scored response from candidates according to PRM. Recently, PROCESSBENCH (Zheng et al., 2024) have emerged to evaluate the capability of PRMs in identifying step-wise correctness. Nevertheless, during the training of our own PRM following conventional principles to construct data using MC estimation and evaluate on BoN, we gain several crucial lessons. In terms of MC estimation, (1) we observe that the PRM trained via MC estimation demonstrated significantly inferior performance and generalization capabilities compared to LLM-as-a-judge (Zheng et al., 2023) and human annotation. (2) We attribute the suboptimal performance of MC estimation to its fundamental limitation, which attempts to evaluate deterministic current-step correctness based on potential future outcomes. It significantly relies on the performance of the completion model, which may generate correct answers based on incorrect steps, or incorrect answers based on correct steps, introducing substantial noise and inaccuracy verification into step-wise correctness estimation. Regarding the BoN evaluation, (1) the unreliable policy models generate responses with correct answers but flawed processes, leading to misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. (2) the limited process verification capability makes PRMs demonstrate tolerance for these cases, resulting in inflated BoN performance. (3) We find that in the step scores distribution of existing PRMs, significant proportion of minimum scores are concentrated on the final answer steps, indicating PRMs have shifted from process to outcome-based assessment in BoN. To address these challenges, we develop consensus filtering mechanism that combines MC estimation with LLM-as-a-judge. The instances are only retained when both LLM-as-a-judge and MC estimation show consensus on the error reasoning step locations in the solution. Our approach demonstrates more efficient data utilization and surpass existing open-source PRMs in the conventional BoN evaluation. Furthermore, we advocate for complementing response-level BoN with step-wise evaluation methods. We employ the step-wise benchmark PROCESSBENCH (Zheng et al., 2024) to measure the ability to identify process errors in mathematical reasoning. Our trained PRMs exhibit impressively stronger error identification performance than other open-source models, from PRMs to general language models, confirming that our training approach genuinely teaches PRMs to assess the correctness of intermediate reasoning steps. Our key contributions can be summarized as follows: We identify critical limitations in current data construction approaches for PRMs, demonstrating that MC estimation-based data construction yields inferior performance compared to LLM-as-ajudge and human annotation. We reveal the potential bias in using response-level BoN evaluation alone for PRMs and advocate for comprehensive evaluation strategies combining both response-level and step-level metrics. We propose simple yet efficient consensus filtering mechanism that integrates MC estimation with LLM-as-a-judge, significantly improving both model performance and data efficiency in PRM training. We substantiate our findings through extensive empirical studies and also open source our trained PRMs, which can establish practical guidelines and best practices for future research and development for reasoning process supervision."
        },
        {
            "title": "2 Preliminary Trials",
            "content": "In this section, we describe our preliminary attempts to train PRMs via MC estimation-based reasoning step annotation. Despite our efforts in scaling up training data and careful tuning of training objectives, we found that the MC estimation-based PRMs do not possess noticeable advantages over the one trained on human-annotated data (Lightman et al., 2023), and even lag significantly behind the latter in identifying specific erroneous reasoning steps. 2.1 Training Setup Training Data Synthesis We followed the commonly used MC estimation approach, Math-Shepherd (Wang et al., 2024b), to construct the PRM training data. Specifically, we collected large-scale dataset of approximately 500,000 queries with golden answers. For each query, we generate 6-8 diverse responses by mixing outputs from the Qwen2-Math-Instruct and Qwen2.5-Math-Instruct series models (Yang et al., 2024c), spanning the model sizes of 7B and 72B parameters. These responses are systematically split into individual steps using the delimiter nn. To assess the correctness of each step, we conduct eight independent completions starting from this step using Qwen2.5-Math-Instruct series with the corresponding model size, estimating the step labels based on the empirical probabilities of each step yielding the correct final answer. Training Details Our trained PRMs were initialized from the supervised fine-tuned Qwen2.5-Math7B/72B-Instruct models (Yang et al., 2024c), where we replace the original language modeling head (used for next token prediction) with scalar-value head, consisting of two linear layers. We trained PRMs with either hard labels or soft labels. For hard labels, we treat step as correct if any one of the eight completions yields the correct final answer, and negative otherwise. For soft labels, we determined the value (between 0 and 1) as the proportion of completions leading to the correct final answers. We calculated the cross-entropy (CE) loss and mean squared error (MSE) loss on the last tokens of each step for the binary classification task using hard labels and for the regression task using soft labels, respectively. Note that we eliminated all steps subsequent to those labeled as incorrect (label 0), as their validity becomes irrelevant after an error occurs. This removal was implemented to prevent potential model confusion during training. 2.2 Evaluation Setup We evaluate our trained PRMs from two aspects: their utilities in straightforwardly improving downstream task performance and their abilities to identify specific erroneous steps in reasoning processes. Best-of-N Consistent with previous work (Lightman et al., 2023; Wang et al., 2024b; Luo et al., 2024; Cobbe et al., 2021; Yang et al., 2024c), we employed the Best-of-N (BoN) sampling strategy for evaluation, which selects the highest-scored response from candidates according to PRM. We denote the evaluation metric as prm@N. Following Yang et al. (2024c), we sampled eight responses (i.e., = 8) from Qwen2.5-Math-7B-Instruct across multiple mathematical benchmarks, including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), Minerva Math (Lewkowycz et al., 2022), GaoKao 2023 En (Liao et al., 2024), OlympiadBench (He et al., 2024), College Math (Tang et al., 2024), and MMLU STEM (Hendrycks et al., 2021a). Each candidate response is scored using the product of all the individual scores of each step within the response, as computed in Lightman et al. (2023). We also report the result of majority voting among eight samplings (maj@8) as the baseline, and pass@8 (i.e., the proportion of test samples where any of the eight samplings lead to the correct final answers) as the upper bound. PROCESSBENCH We also evaluated on PROCESSBENCH as complement. PROCESSBENCH (Zheng et al., 2024) measures the capability of models to identify erroneous steps in mathematical reasoning. Models are required to identify the first step that contains an error or conclude that all steps are correct. Following the evaluation methods for PRMs in PROCESSBENCH, we locate the first erroneous step from predict scores yielded by PRMs. 2.3 Evaluation Results As shown in Table 1 and Table 2, we denote the models trained on our MC estimated dataset as Qwen2.5Math-7B-PRM-MC-hard (trained with hard labels) and Qwen2.5-Math-7B-PRM-MC-hard (trained with soft labels), respectively, and compare them with baseline model trained exclusively on the PRM800K (Lightman et al., 2023) dataset named Qwen2.5-Math-7B-PRM-PRM800K. The experimental results demonstrate that on the Best-of-8 evaluation, none of the PRMs achieved prm@8 scores superior to 3 Setting GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM pass@8 (Upper Bound) maj@8 Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-7B-PRM-MC-hard Qwen2.5-Math-7B-PRM-MC-soft 98.1 96.7 96.9 96.8 96.8 92.0 87.1 86.9 87.3 86. 49.3 41.2 37.1 40.1 37.9 80.5 72.5 71.2 70.6 70.6 59.6 44.4 44.0 43.7 41. 52.6 47.8 47.6 48.1 47.7 90.5 73.8 70.9 71.6 70.4 Avg. 74. 66.2 64.9 65.5 64.4 Table 1: Performance comparison on Best-of-8 using PRMs trained with MC estimated hard labels and soft labels, human-annotated PRM800K, denoted as Qwen2.5-Math-7B-PRM-MC-hard, Qwen2.5-Math7B-PRM-MC-soft, and Qwen2.5-Math-7B-PRM800K, respectively. Model GSM8K MATH OlympiadBench Omni-MATH Avg. F1 error correct F1 error correct F1 error correct F1 error correct F1 Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-7B-PRM-MC-hard Qwen2.5-Math-7B-PRM-MC-soft 53.1 67.1 65.7 95.3 90.2 93.3 68.2 77.0 77. 48.0 35.2 35.7 90.1 65.8 64.5 62.6 45.8 46.0 35.7 13.2 13.2 87.3 28.0 29.2 50.7 17.9 18. 29.8 13.3 12.9 86.1 41.9 40.2 44.3 20.2 19.6 56.5 40.2 40.2 Table 2: Performance comparison on PROCESSBENCH using PRMs trained with MC estimated hard labels and soft labels, human-annotated PRM800K, denoted as Qwen2.5-Math-7B-PRM-MC-hard, Qwen2.5Math-7B-PRM-MC-soft, and Qwen2.5-Math-7B-PRM800K, respectively. maj@8. Furthermore, on the PROCESSBENCH, Both Qwen2.5-Math-7B-PRM-MC-hard and Qwen2.5Math-7B-PRM-MC-soft exhibit significantly inferior erroneous step localization capabilities compared to Qwen2.5-Math-7B-PRM-PRM800K. These undesirable evaluation performances push us to reflect on the currently prevalent data synthesis approach and evaluation strategy. Through the subsequent optimization process, we have indeed gained several observations and lessons learned."
        },
        {
            "title": "3 Discussion and Analysis",
            "content": "In this section, we present the critical lessons gained during the PRM training. Our discussion comprises three main aspects: (1) the limitations of commonly adopted MC estimation approaches in PRMs training, and (2) the bias in using BoN as the sole evaluation metric for optimizing PRMs. 3.1 Limitations of MC Estimation for PRMs Training 3.1.1 Distinguishing PRMs from Value Models Reward models in mathematical reasoning serve as correctness verifiers and PRMs provide fine-grained supervision by evaluating the correctness of intermediate reasoning steps. In contrast, value models estimate the potential of reaching the correct final answer from the current step in the future. The key difference between PRM and value model lies in that PRMs function as deterministic evaluators of current step correctness, while value models operate as predictive estimators of future solution potential. MC estimation attempts to estimate the potential of reaching the correct final answer in the future from the current step. When we follow this approach to construct data and train the PRMs, the value model principles are incorporated into PRMs training essentially. This methodology potentially introduces performance and generalization limitations which we will discuss in subsequent sections. 3.1.2 MC Estimation vs. LLM-as-a-judge vs. Human Annotation We found that MC estimation methods limit PRMs capability to identify erroneous steps as demonstrated in the experiments of Section 2.3. For further investigation, we compare the performance using 3 distinct data construct approaches: MC estimation, LLM-as-a-judge, and human annotation. For the MC estimation approach, we respectively train the PRM on 445k open-source datasets Math-shepherd (Wang et al., 2024b) and our 860k similarly constructed dataset. For our constructed dataset, the MC estimation employs responses from Qwen2-Math-Instruct and completes subsequent reasoning processes by Qwen2.5-Math-Instruct. For the LLM-as-a-judge approach, we use the same 860k query and response and employ Qwen2.5-72B-Instruct to verify the correctness of each step in the responses. We show the prompt template we implement for verification in Appendix C. For the human annotation approach, we use the open-source dataset PRM800K (Lightman et al., 2023) which consists of approximately 265k samples after deduplication against the test set. 4 Setting # samples GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM MC Estimation (Math-Shepherd) MC Estimation (our data) LLM-as-a-judge (our data) Human Annotation (PRM800K) 440k 860k 860k 264k 96.9 97.0 96.9 96. 86.5 87.6 86.8 86.9 36.8 41.9 39.0 37.1 71.4 71.4 71.2 71.2 41.6 43.6 43.7 44.0 47.7 48.2 47.7 47.6 69.3 71.9 71.9 70. Avg. 64.3 65.9 65.3 64.9 Table 3: PRMs performance comparison on the Best-of-8 strategy of the policy model Qwen2.5-Math-7BInstruct. The models are trained on the different data construction methods including MC estimation, LLM-as-a-judge, and human annotation. Method # samples GSM8K MATH OlympiadBench Omni-MATH Avg.F1 error correct F1 error correct F1 error correct F1 error correct F1 MC Estimation (Math-Shepherd) MC Estimation (our data) LLM-as-a-judge (our data) Human Annotation (PRM800K) 440k 860k 860k 264k 46.4 62.3 44.0 53.1 95.9 91.2 99.0 95.3 62.5 74.0 60.9 68.2 18.9 35.2 33.5 48.0 96.6 71.9 94.8 90.1 31.6 47.3 49.5 62. 7.4 12.7 24.7 35.7 93.8 41.3 97.1 87.3 13.7 19.4 39.4 50.7 4.0 12.1 22.3 29.8 95.0 54.4 95.4 86.3 7.7 19.8 36.1 44. 28.9 40.1 46.5 56.5 Table 4: PRMs performance comparison on PROCESSBENCH. The models are trained on the different data construction methods including MC estimation, LLM-as-a-judge, and human annotation. The experimental results of Best-of-8 and PROCESSBENCH are shown in Table 3 and Table 4, respectively. For Best-of-8, Table 3 shows that the PRM trained on our MC estimated data achieves the best average accuracy and human annotation performs worst. However, the two models exhibit inverse performance relationships in PROCESSBENCH, which catches our attention and is thoroughly investigated in Section 3.2. Table 4 demonstrates that human annotation achieves the best performance with the least amount of data, followed by LLM-as-a-judge, while MC estimation performed the worst despite having the largest dataset overall. Specifically, (1) human annotation, despite being only performed on the MATH dataset, exhibited superior generalization capabilities on more complex tasks OlympiadBench and Omni-MATH. (2) Given identical data with different annotation approaches, LLM-as-a-judge demonstrates better generalization performance on challenging problems than MC estimation, although the latter showed favorable results on GSM8K. (3) For MC estimation, comparison between our 860k dataset and Math-Shepherd 440k data indicates that performance improvements can still be achieved through data scaling. 3.1.3 Stringent Data Filtering Mechanisms Required in MC Estimation We attribute the inferior performance of MC estimation compared to LLM-as-a-judge and human annotation to its high noise in reasoning step correctness estimation and inaccurate error position identification due to its heavy dependence on the policy model. For instance, the policy model may generate correct final answers but incorrect reasoning steps, which will be investigated thoroughly in Section 3.2.1. Motivated by LLM-as-a-judges encouraging results in Section 3.1.2, we naturally propose simple yet efficient consensus Filtering mechanism that integrates LLM-as-a-judge with MC estimation. Based on the aforementioned 860K samples, the instances are only retained when both LLM-as-a-judge and MC estimation show consensus on the error reasoning step locations in the solution. As demonstrated in Figure 2, it can be found that only approximately 40% of the data are preserved after consensus filtering. For evaluation on PROCESSBENCH, the results reveal that the reduced dataset after consensus filtering significantly outperforms MC estimation, and notably, achieves comparable performance to LLM-as-a-judge while using only 40% of the data. Regarding the BoN evaluation, the performance variations among these three models are marginal. The limitations of BoN evaluation in PRMs will be elaborated on in Section 3.2 later. 3.1.4 Hard Label vs. Soft Label in MC Estimation Although we have previously demonstrated that MC estimation is not as effective as LLM-as-a-judge and human annotation, there remains noteworthy point of MC estimation to be discussed, i.e., whether to train with soft label or hard label. We construct 3 million training data using MC estimation, where for each reasoning step we perform 8 completions. Subsequently, we apply the consensus filtering strategy discussed in Section 3.1.3 to filter the 3 million samples, which reduces the dataset to 1.5 million samples. We respectively train PRMs using both soft labels and hard labels on 3 million and 1.5 million data. The performance of trained PRMs on Best-of-8 and PROCESSBENCH are illustrated in Figure 3 and 4 separately. Before data filtering, the performance difference between soft and hard labels is not significant, which we attribute to the high noise level masking their distinctions. However, this difference becomes much more pronounced after data filtering, with hard labels substantially outperforming soft labels 5 Figure 2: Performance comparison on Best-of-8 and PROCESSBENCH using PRMs trained with different data synthesis methods. Figure 3: Performance comparison on Best-of-8 for the PRMs trained on soft and hard labels before and after filtering. Figure 4: Performance comparison on PROCESSBENCH for the PRMs trained on soft and hard labels before and after consensus filtering. Figure 5: PRM Performance changes on Bestof-8 and PROCESSBENCH across different hard label thresholds. on both Best-of-8 and PROCESSBENCH. We consider the limitations of soft labels are: (1) as discussed in Section 3.1.1, the correctness of steps (i.e., rewards) should be deterministic. Training PRMs with soft labels that represent future possibilities introduces additional noise. For instance, when numerous completely correct steps are assigned with soft labels lower than 1, it actually reduces the models ability to discriminate between positive and negative labels; (2) only 8 completions for step correctness estimation exhibit high variance and are relatively crude. Although we can achieve better estimation accuracy by increasing the number of completions, the associated costs may outweigh the incremental benefits. Moreover, the experimental results indicate that the consensus filtering strategy yields performance benefits across both soft and hard label schemes. Last but not least, we investigate the threshold selection for distinguishing between positive and negative labels based on the MC estimation result of 8 completions. Following our previous experimental setup, we conduct series of experiments on the 3 million with threshold values from 1/8 to 7/8 at 1/8 intervals, with results shown in Figure 5. It can be easily observed that as the threshold increases, the performance deteriorates on both Best-of-8 and PROCESSBENCH, indicating that using an MC estimated value of 0 as the negative label and all others as positive labels yields the best results. Therefore, if we have to rely on MC estimation for step-wise correctness verification, we suggest setting the threshold to 0, meaning that step is considered correct if any completion start from this step reaches the correct final answer. This threshold has also been employed throughout our all experimental studies. 3.1.5 Summary Through extensive experimentation, we have demonstrated that MC estimation yields inferior performance and generalization compared to both LLM-as-a-judge and human annotation. However, incorporating MC estimation with LLM-as-a-judge via consensus filtering strategy leads to enhanced performance and improved data efficiency. Furthermore, optimal results are achieved when treating MC estimation values of 0 as negative labels and training with hard labels. 3.2 Bias in BoN Sampling for PRM Performance Evaluation Although BoN evaluations are commonly used in PRM optimization, their effectiveness as sole optimization criterion is worth careful consideration due to potential limitations in performance assessment. 3.2.1 Unreliable Policy Models Cause BoN-PRMs Misalignment In an ideal scenario, the responses generated by the policy model would exhibit both correct answers and accurate solution steps or conversely, flawed processes would correspond to incorrect answers. However, existing policy models are prone to generating responses with correct answers but flawed processes, while BoN inherently only focuses on answers, leading to misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. To provide empirical evidence for this phenomenon, we sample 8 responses from GSM8K, MATH, OlympiadBench, and Omni-MATH using the policy model Qwen2.5-Math-7B-Instruct. Then we randomly choose correct-answer responses from them and conduct thorough manual annotations. As detailed in Figure 6, substantial percentage of responses contain process errors while maintaining correct answers. Notably, compared with easy task GSM8K and hard task Omni-MATH, this phenomenon becomes more pronounced as the problems complexity increases. This implies that an effective PRM might assign low scores to responses with correct answers but flawed processes, resulting in overall lower performance on the BoN evaluation. 6 Figure 6: Proportion of incorrect reasoning steps in responses with correct final answers generated by policy model Qwen2.5-Math-7B-Instruct. Figure 7: Percentage of responses where the minimum step score predict by PRMs appears in the final step (among all Best of 8 responses from Qwen2.5-Math-7B-Instruct). Figure 8: Performance trends on BoN and PROCESSBENCH for models trained with different data sources. Figure 9: Performance on BoN across multiple PRMs with different scoring methods: minimum, product and last. GSM8K MATH OlympiadBench Omni-MATH Avg. # samples 1.5B Skywork-PRM-1.5B 7B+ Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-7B EurusPRM-Stage1 EurusPRM-Stage2 Qwen2.5-Math-7B-Math-Shepherd Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-PRM-7B 72B Qwen2.5-Math-PRM-72B 7 94 42.9 36.2 14.3 14.3 0.0 57.1 28.6 42.9 0.0 42.9 42. 12.8 13.8 18.1 26.6 25.5 27.7 9.6 50.0 68.1 28.6 76.6 161 12.4 13.7 7.5 9.9 14.3 19.9 18.0 4.3 31.7 48. 62.7 259 13.9 14.7 10.0 10.8 13.1 20.1 20.8 1.2 28.2 56.0 64.5 26. 13.9 11.4 9.7 27.8 23.5 27.4 3.8 38.2 53.9 58.1 Table 5: The accuracy in identifying erroneous steps on the test cases of PROCESSBENCH containing correct answers but erroneous reasoning steps. # samples represents the number of test cases. 3.2.2 Limited Process Verification Capability in PRMs Lead to BoN Scores Inflation When the PRM cannot distinguish responses that have correct answers but flawed processes and assign them high scores, this leads to overestimated performance in the BoN evaluation, thereby creating an overly optimistic and potentially misleading assessment of PRM capabilities. typical example is the comparative experiment in Section 3.1.2, as shown in Figure 8, where the PRMs trained on our MC estimated data, LLM-as-a-judge and PRM800K demonstrate opposite performance trends in BoN and PROCESSBENCH evaluation. It can be observed that the model trained on our MC estimated data shows limited process verification capability but inflated results on the BoN. To investigate the discriminative capability of PRMs for such cases, we extracted instances from PROCESSBENCH where answers were correct but processes were erroneous and analyzed the detection accuracy rates of PRMs for these cases. As shown in Table 5, except our released PRMs Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B, all other open-sourced PRMs demonstrate detection accuracy rates below 50%. This limited discriminative capability indicates that PRMs struggle to differentiate between genuinely correct responses and those with merely superficial answer correctness in BoN evaluations. Consequently, this implies that beyond BoN evaluation, supplementary benchmarks are necessary to assess the actual capability of PRMs, especially in detecting process errors. 3.2.3 Process-to-Outcome Shift in BoN Optimized PRMs The majority of current PRMs are optimized towards BoN. However, the limitations of BoN result in PRMs process-to-outcome shift. During the BoN selection process based on PRM-predicted scores and follow the scoring method for responses in (Lightman et al., 2023), it can be found that regardless of whether we employ the minimum score or the product of scores to evaluate the full solution, the lowest step score acts as the key limiting factor that affects the selection criteria of PRMs. As shown in Figure 7, we analyze the distribution of minimum step scores assigned by multiple opensourced PRMs, specifically focusing on cases where the lowest score occurred at the final step, which typically contains the final answer. The results show that models EurusPRM-Stage1, EurusPRM-Stage2, Math-Shepherd-PRM-7B and Skywork-PRM-7B exhibit notably high proportions in this category, which exceed 40%. In contrast, our released PRMs Qwen2.5-Math-PRM-72B and Qwen2.5-Math-PRM-7B exhibit significantly lower proportion of minimum scores at the final step. This analysis reveals that some PRMs performance in BoN evaluation is predominantly determined by final answer scores rather than intermediate reasoning steps, indicating model degradation from process-based to outcome-oriented assessment. In other words, optimizing solely for the BoN evaluation has made current PRMs perform more like ORMs in practice. Hence, it is essential to supplement response-level evaluation BoN with step-level assessment methods to avoid the process-to-outcome shift. Specifically, we can employ process error localization tasks such as PROCESSBENCH. Other commonly used step-wise BoN methodologies leverage the integration of PRMs or value models with search mechanisms, which provide more granular assessment of process reliability. It worth noting that the latter requires more computational costs. 3.2.4 Different PRMs, Different Optimal Scoring Strategies In the BoN evaluation, the overall solution score is derived by combining individual step scores. When each steps score represents the probability of that specific step being correct, its generally acceptable to combine these step-level scores (through methods like product or minimum) to calculate the overall solution score. However, the situation becomes different when using MC estimation. In this case, each steps score actually estimates the probability of reaching the correct final answer in the future from the current position. Given this forward-looking nature of MC estimation, we should neither multiply the estimated probabilities across steps (as these estimates are dependent on each other), nor simply take the minimum estimated value from particular step as the overall score. Instead, the estimated value from the final step naturally integrates information from the entire solution process, making it more suitable as the final score for the complete solution. To validate that, we evaluate BoN in different scoring strategies for the PRMs trained on MC estimation, LLM-as-a-judge, and human annotation data, as shown in Figure 9. We found that in MC estimation, using the last score shows significantly better performance than product and minimum approaches across multiple PRMs. And the trend is the opposite for human annotation and LLM-as-a-judge. This suggests that if the PRM has to be trained via MC estimation and evaluated in BoN, the last score strategy may be more reasonable and effective. However, its worth noting that this use of PRM in BoN has deviated from PRMs original intended purpose. 3.2.5 Summary The above observations underscore critical limitations in BoN evaluation. Firstly, the unreliable policy models generate responses with correct answers but flawed processes, leading to misalignment between the evaluation criteria of BoN and the PRM objectives of process verification. Secondly, the limited process verification capability makes PRMs demonstrate tolerance for the responses with correct answers but flawed reasoning processes, resulting in inflated BoN performance. Thirdly, model optimization solely focused on BoN evaluation leads PRMs to drift to prioritize final answers over reasoning processes. 8 Therefore, we argue that supplementary step-level evaluation plays crucial role in PRM evaluation. Finally, In BoN, different PRMs have different optimal scoring strategies. The last score strategy may be more reasonable and effective for the PRM trained via MC estimation. In contrast, product and minimum scoring are more appropriate for LLM-as-judge and human annotation."
        },
        {
            "title": "4 Our Approach",
            "content": "This section presents our methodology for overcoming the previously discussed limitations and the details of our trained PRM achieving state-of-the-art performance. Additionally, we outline our experimental settings, and baseline models for comparison and evaluation results. 4.1 Training Details The data construction procedure comprises two primary phases: data expansion and data filtering. In the expansion phase, we follow the MC estimation to construct data described in Section 2.1. We employ hard labels, where response is classified as negative only if none of the 8 completions achieves the correct final answer. In the subsequent filtering phase, we employ the LLM instantiated by Qwen2.5-Instruct-72B (Yang et al., 2024b) to serve as critic to verify the reasoning process for all responses step by step, i.e., LLM-as-a-judge. We implement simple yet efficient consensus filtering mechanism by filtering out instances where there is discrepancy between the LLM-annotated and MC-estimated process labels. This ensures the retained data maintains high quality and consistency in the reasoning process annotation. For the training task, we employ cross-entropy loss on the tokens at the end of each step to train the binary classification task based on hard labels. We trained both 7B and 72B-parameter PRMs, initialized with Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct respectively. 4.2 Experimental Setup To validate the effectiveness of our trained PRM Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B, we respectively conduct the response-level BoN evaluation and the step-level process errors identification task PROCESSBENCH (Zheng et al., 2024). Best-of-N We follow the experimental setting in Section 2.2. In rm@8, we evaluate Outcome Reward Models (ORMs) and Process Reward Models (PRMs). For ORMs, we introduce Qwen2.5-Math-RM-72B (Yang et al., 2024c), which assigns single score to each complete response. For PRMs, we compute the product of each step score as the final response score. We compare with the following PRMs: Math-Shepherd-PRM-7B (Wang et al., 2024b): determining process labels for each step by estimating the empirical probability of reaching the correct final answer. RLHFlow-PRM-Mistral-8B & RLHFlow-PRM-Deepseek-8B (Xiong et al., 2024): two LLaMA3.1-based PRMs that adopt Math-Shepherds training methodology while implementing different solution generation models and optimization objectives. Skywork-PRM-1.5B & Skywork-PRM-7B (Skywork, 2024): two recently released Qwen2.5Math-based PRMs by Skywork. EurusPRM-Stage1 & EurusPRM-Stage2 (Cui et al., 2025): two PRMs trained using Implicit PRM approach (Yuan et al., 2024) with 7B parameters, which obtains process rewards replying on the ORM trained on the response-level labels. Qwen2.5-Math-7B-Math-Shepherd & Qwen2.5-Math-7B-PRM800K: two additional PRMs our developed by fine-tuning Qwen2.5-Math-7B-Instruct separately on the PRM800K (Lightman et al., 2023) and Math-Shepherd (Wang et al., 2024b) opensource datasets. PROCESSBENCH The compared PRMs are consistent with the previously mentioned PRMs. For the LLM prompted as Critic Models, i.e., LLM-as-a-judge, we compare with proprietary language models GPT-4o-0806 (Hurst et al., 2024) and o1-mini (OpenAI, 2024), open-source language models Llama-3.370B-Instruct (Dubey et al., 2024), Qwen2.5-Math-72B-Instruct (Yang et al., 2024c), Qwen2.5-72B-Instruct (Yang et al., 2024b) and QwQ-32B-Preview (Qwen, 2024). We also decompose the N-step response trajectory into separate instances to enable individual scoring by the ORM Qwen2.5-Math-RM-72B. 9 GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM Setting pass@8 (Upper Bound) maj@ 1.5B Skywork-PRM-1.5B 98.1 96.7 92 87.1 96.9 86.7 7B+ Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-7B EurusPRM-Stage1 EurusPRM-Stage2 Qwen2.5-Math-7B-Math-Shepherd Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-PRM-7B 72B Qwen2.5-Math-RM-72B Qwen2.5-Math-PRM-72B 97.3 97.0 97.3 97.3 95.6 95.4 96.9 96.9 97.1 97.9 97.6 85.4 86.1 86.3 87.3 83.0 83.4 86.5 86.9 88.0 88.5 88.7 49.3 41. 37.9 37.9 37.1 40.8 38.2 35.7 34.9 36.8 37.1 42.6 42.6 46.0 80.5 72.5 70.1 70.6 70.6 70.9 71.9 66.2 67.3 71.4 71.2 74. 75.1 74.3 59.6 44.4 42.1 40.4 41.2 42.2 43.7 38.2 39.1 41.6 44.0 47.6 49.9 48.1 52.6 47. 90.5 73.8 Avg. 74.7 66.2 47.9 67.9 64. 47.2 47.6 47.2 47.8 46.2 46.3 47.7 47.6 48.7 49.6 49.3 70.5 69.5 69.3 67.7 66.6 67.3 69.3 70.9 74.5 78.7 81.1 64.2 64.2 64.9 64.8 61.6 62.0 64.3 64.9 67.6 68.9 69. Table 6: Performance comparison on the Best-of-8 strategy of the policy model Qwen2.5-Math7B-Instruct. represents the models we trained. Model GSM8K MATH OlympiadBench Omni-MATH Avg. F1 error correct F1 error correct F1 error correct F1 error correct F1 LLM-as-judge, Proprietary language models GPT-4-0806 o1-mini 70.0 88.9 LLM-as-judge, Open-source language models Llama-3.3-70B-Instruct Qwen2.5-Math-72B-Instruct Qwen2.5-72B-Instruct QwQ-32B-Preview 72.5 49.8 62.8 81.6 91.2 97. 96.9 96.9 96.9 95.3 79.2 93.2 82.9 65.8 76.2 88.0 54.4 83.5 43.3 36.0 46.3 78.1 76.6 95. 83.2 94.3 93.1 79.3 63.6 88.9 59.4 52.1 61.8 78.7 45.8 80.2 31.0 19.5 38.7 61.4 58.4 95. 94.1 97.3 92.6 54.6 51.4 87.2 46.7 32.5 54.6 57.8 45.2 74.8 28.2 19.0 36.6 55.7 65.6 91. 90.5 96.3 90.9 68.0 53.5 82.4 43.0 31.7 52.2 61.3 61.9 87.9 58.0 45.5 61.2 71.5 PRMs 1.5B Skywork-PRM-1.5B 50.2 71.5 59.0 37.9 65. 48.0 15.4 26.0 19.3 13.6 32. 19.2 36.4 7B+ Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-7B EurusPRM-Stage1 EurusPRM-Stage2 Qwen2.5-Math-7B-Math-Shepherd Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-PRM-7B 72B Qwen2.5-Math-RM-72B Qwen2.5-Math-PRM-72B 32.4 33.8 24.2 61.8 46.9 51.2 46.4 53.1 72.0 41.1 78. 91.7 99.0 98.4 82.9 42.0 44.0 95.9 95.3 96.4 46.1 97.9 47.9 50.4 38.8 70.8 44.3 47.3 62.5 68.2 82.4 18.0 21.7 21.4 43.8 33.3 36.4 18.9 48.0 68.0 43.5 87.3 39.7 74. 82.0 72.2 80.0 62.2 38.2 35.0 96.6 90.1 90.4 58.1 88.2 29.5 33.4 33.8 53.6 35.6 35.7 31.6 62.6 77.6 15.0 8.2 10.1 17.9 23.9 25.7 7.4 35.7 55.7 47.2 80.6 28.1 67. 71.1 43.1 51.0 31.9 19.8 18.0 93.8 87.3 85.5 56.6 82.0 24.8 13.8 16.9 22.9 21.7 21.2 13.7 50.7 67.5 14.2 9.6 10.9 14.0 21.9 23.1 4.0 29.8 55.2 37.6 74.3 18.8 64. 73.0 45.2 51.9 41.9 24.5 19.1 95.0 86.1 83.0 50.2 78.8 23.8 15.8 16.9 21.0 23.1 20.9 7.7 44.3 66.3 27.4 71.1 31.5 28.4 26.6 42.1 31.2 31.3 28.9 56.5 73.5 38.9 78. Table 7: Performance comparison on PROCESSBENCH. represents the models we trained. We report the results in the same calculation method with PROCESSBENCH. 4.3 Experimental Results Best-of-N The evaluation on policy model Qwen2.5-Math-7b-Instruct is shown in Table 6. Qwen2.5Math-PRM-7B demonstrates superior performance compared to other PRMs of equivalent model scale. Notably, it outperforms maj@8 across all 7 tasks, achieving an average improvement of 1.4%. Furthermore, the Qwen2.5-Math-PRM-72B exhibits slightly better overall performance than Qwen2.5-Math-RM-72B, with particularly significant improvements observed in the Minerva Math and MMLU STEM tasks. Futhermore, detailed experimental results, including BoN performance on Policy model Qwen2.5-Math72b-Instruct, alternative scoring strategies, and evaluations on Chinese benchmarks, are comprehensively documented in the Appendix A. PROCESSBENCH The evaluation results on PROCESSBENCH are presented in Table 7. When compared with LLM-as-judge, Qwen2.5-Math-PRM-7B in smaller model size demonstrates superior performance over all open-source models. For proprietary language models, Qwen2.5-Math-PRM-7B outperforms GPT10 4o-0806, while there remains performance gap compared to o1-mini. Furthermore, in comparison with existing PRMs, both Qwen2.5-Math-PRM-7B and Qwen2.5-Math-PRM-72B exhibit substantial advantages over their counterparts. An interesting observation worth noting is that the ORM Qwen2.5-Math-RM-72B exhibits considerable capability in identifying step errors, even surpassing some open-source PRMs, which validates its potential as complementary reward beyond solely rule-based mechanism."
        },
        {
            "title": "5 Related Work",
            "content": "Reward Model in Mathematical Reasoning To further improve mathematical reasoning accuracy, the reward model plays crucial role in selecting the best answers. Two main types of reward models have emerged: (1) Outcome Reward Model (ORM) which provides an evaluation score for the entire solution, especially for the final answer. (2) Process Reward Model (PRM) (Uesato et al., 2022; Lightman et al., 2023) which evaluates each step in the reasoning process. Previous work (Lightman et al., 2023; Wang et al., 2024b) has demonstrated that PRM outperforms ORM which exhibits greater potential, though it requires more high-quality training data. Mathematical Reasoning Step Verification There are two primary approaches to evaluating the correctness of reasoning steps. The first approach relies on human annotation (Lightman et al., 2023), which produces high-quality data but suffers from substantial costs. The second approach, which has attracted considerable research attention, focuses on automated evaluation of reasoning step correctness. Current automated methods can be categorized into two main types: (1) backward-propagation based methods that infer step correctness from solution outcomes, including MC estimation (Wang et al., 2024b; Luo et al., 2024; Chen et al., 2024), progressive ORM labeling (Xi et al., 2024), and credit assignment (Wang et al., 2024a; Cui et al., 2025; Yuan et al., 2024) techniques; (2) prompting-based methods that leverage LLMs serve as critic, i.e., LLM-as-a-judge (Zhang et al., 2024; Gao et al., 2024; Xia et al., 2024) to assess step correctness directly. In this work, we integrate the two approaches MC estimation and LLM-as-a-judge."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we investigate the Process Reward Model (PRM) and release an effective PRM that demonstrates superior performance. Firstly, we discuss the undesirable trials on MC estimation. Then we demonstrate that data construction via MC estimation yields inferior performance and generalization compared to both LLM-as-a-judge and human annotation through extensive experiments. Besides, we investigate the limitations of vanilla BoN evaluation for PRMs which leads to inaccurate assessment of the PRMs ability and causes an optimization bias that shifts focus from process-oriented to outcome-oriented verification. Finally, we propose simple yet effective consensus filtering strategy combining MC estimation and LLM-as-a-judge to overcome the limitation of MC estimation. In terms of evaluation, we conduct the response-level BoN evaluation and the step-level process errors identification task PROCESSBENCH to avoid the bias of relying solely on BoN. The experiments demonstrate our strategy significantly improves both data efficiency and model performance. In the future, there remains substantial potential in data construction and evaluation for PRMs, driving the development of more robust and reliable PRMs. Limitation Several limitations remain in our current work. Firstly, there exists considerable performance gap between our PRM and the BoN upper bound (pass@8), suggesting substantial optimization potential. Finally, although our approach combines LLM-as-a-judge with MC estimation for consensus filtering, the efficient utilization of existing high-quality human annotation data is still largely underexplored. For instance, gradually expanding high-quality datasets through weakly supervised methods can be investigated as promising direction for future exploration."
        },
        {
            "title": "References",
            "content": "Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: Process supervision without process, 2024. URL https://arxiv.org/abs/2405.03553. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards, 2025. 11 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, Junjie Hu, Tianyu Liu, and Baobao Chang. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback, 2024. URL https://arxiv. org/abs/2406.14024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Minpeng Liao, Chengxi Li, Wei Luo, Jing Wu, and Kai Fan. MARIO: math reasoning with code interpreter output - reproducible pipeline. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 905924. Association for Computational Linguistics, 2024. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision, 2024. URL https://arxiv.org/abs/2406.06592. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. OpenAI. Openai o1-mini: Advancing cost-efficient reasoning, 2024. URL https://openai.com/index/ openai-o1-mini-advancing-cost-efficient-reasoning/. Team Qwen. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL https: //qwenlm.github.io/blog/qwq-32b-preview/. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. o1 Team Skywork. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024. URL https://huggingface.co/Skywork. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id= Kjww7ZN47M. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcomebased feedback, 2022. URL https://arxiv.org/abs/2211.14275. 12 Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. Q*: Improving multi-step reasoning for llms with deliberative planning, 2024a. URL https://arxiv.org/abs/2406. 14283. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, August 2024b. doi: 10.18653/v1/2024.acl-long.510. URL https://aclanthology.org/ 2024.acl-long.510. Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. CMATH: can your language model pass chinese elementary school math test? CoRR, abs/2306.16636, 2023. Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, and Xuanjing Huang. Training large language models for reasoning through reverse curriculum reinforcement learning, 2024. Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical reasoning beyond accuracy, 2024. URL https://arxiv.org/abs/2404.05692. Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm. https://github.com/RLHFlow/RLHF-Reward-Modeling, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024c. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction, 2024. URL https://arxiv.org/abs/ 2408.15240. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems, volume 36, pages 4659546623, 2023. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. In NAACL-HLT (Findings), pages 22992314. Association for Computational Linguistics, 2024. Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024."
        },
        {
            "title": "A Supplementary Experimental Results",
            "content": "A.1 The BoN Evaluation on Qwen2.5-Math-72b-Instruct The BoN evaluation on policy model Qwen2.5-Math-72b-Instruct is shown in Table 8. Qwen2.5Math7B-PRM outperforms other PRMs of equivalent model scale. However, its performance is inferior to maj@8, suggesting challenges in employing 7B PRM for the supervision of 72B policy modelgenerated responses. Besides, Qwen2.5-Math-PRM-72B surpasses maj@8 in prm@8 and is comparable with Qwen2.5-Math-RM-72B in orm@8. GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM Setting pass@8 maj@8 1.5B Skywork-PRM-1.5B 7B+ Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-7B EurusPRM-Stage1 EurusPRM-Stage2 Qwen2.5-Math-7B-Math-Shepherd Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-PRM-7B 72B Qwen2.5-Math-RM-72B Qwen2.5-Math-PRM-72B 97.3 96.0 93.2 88.6 96.5 88.1 96.5 96.6 96.5 97.0 95.4 95.3 96.9 96.5 96. 96.4 96.4 86.8 87.5 87.7 89.0 85.6 85.1 88.5 88.9 89.6 89.8 89.9 56.6 47.8 45.2 45.6 46.3 44.5 47.1 44.1 44.9 46.0 47.4 46. 47.4 46.0 83.6 73.8 74.3 71.9 73.5 73.5 75.3 72.5 72.5 75.8 75.3 77.7 76.9 77.4 62.4 50. 48.4 49.2 48.9 48.7 49.8 46.5 47.1 49.9 50.7 51.4 54.5 52.9 54.1 50.2 95.3 84.9 Avg. 77.5 70.2 49.7 79.7 68.8 49.5 49.4 49.4 49.9 49.2 49.0 49.5 50.1 50.4 50.6 50. 77.5 83.4 84.6 76.3 80.3 80.2 79.7 76.6 76.4 80.1 82.3 68.1 69.4 69.3 69.2 67.7 67.7 69.5 69.4 69.9 70.8 70.7 Table 8: Performance comparison on the Best-of-8 strategy of the policy model Qwen2.5-Math-72BInstruct. represents the models we trained. A.2 The BoN Evaluation with Various Scoring Strategies We demonstrate experimental results using the last step score, the minimum step score or the production of step scores as the solution-level score. The BoN results with policy model Qwen2.5-Math-7B-Instruct and Qwen2.5-Math-72B-Instruct are shown in Table 10 and Table 11 respectively. A.3 The BoN Evaluation on Chinese Benchmarks We evaluate across three Chinese benchmarks including Chinese math benchmarks CMATH (Wei et al., 2023), GaoKao Math Cloze (Zhong et al., 2024), and GaoKao Math QA (Zhong et al., 2024) following Yang et al. (2024c), as shown in Table 12 and Table 13."
        },
        {
            "title": "B PRM Guided Search",
            "content": "We further integrate PRM with greedy search by generating candidate steps at each step, evaluating these candidates using PRM scoring, and selecting the highest-scoring step for subsequent expansion. For the policy model, we employed Qwen2.5-7B-Instruct which has greater diversity in generation to sample 8 candidates at each step, with sampling parameters set to temperature = 1.0 and top = 1.0. We conduct comparative experiments with ORM in BoN approach. As shown in Table 9, Qwen2.5-Math-PRM-72B with greedy search@8 is slightly superior performance compared to Qwen2.5-Math-RM-72B with orm@8. We argue the potentially smaller performance differential between PRM and ORM lies in the consistency of generated token counts between greedy search and BoN outputs. Furthermore, although greedy search always selects the highest-scoring candidate at each step, the highest-scoring step may not be the correct one. Therefore, implementing either Depth-First Search (DFS) with backtracking capabilities or search approaches incorporating score constraints could prove more suitable for this cases. We choose the highest-scoring candidate at each step which the score predicted by PRM represents the correctness of this step. But such locally optimal choices may not lead to the correct final answer. In contrast, value models can predict the future probability of reaching the correct answer, rather than reflecting the correctness of the current step like rewards do, making them particularly well-suited for 14 integration with search strategies. Based on these considerations, we believe there is still significant potential for exploration in the future regarding more appropriate search strategies or combining rewards and values to simultaneously consider both the correctness of the current step and the possibility of reaching the correct future outcomes. Setting GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM pass@8 (Upper Bound) pass@1 maj@ orm@8 Qwen2.5-Math-RM-72B Greedy Search@8 Skywork-PRM-7B Qwen2.5-Math-PRM-7B Qwen2.5-Math-PRM-72B 96.9 91.2 93.7 89.6 74.0 80.3 95.4 84. 95.3 95.5 95.9 83.2 82.6 84.7 48.2 32.0 37.1 38.6 33.8 32.0 37.9 79.7 64.7 69. 73.0 70.4 71.4 73.2 58.4 36.9 45.8 48.6 44.1 44.9 48.9 55.0 46.2 48. 50.1 48.2 48.8 50.0 Avg. 72.8 57.4 62.5 81.6 57.1 61.9 75. 66.5 60.1 69.6 75.3 62.2 63.5 66.6 Table 9: The performance of PRM guided greedy search and ORM of Best-of-8 with policy model Qwen2.5-7B-Instruct. For greedy search, 8 candidates is proposed at each step. Prompt Template for LLM-as-a-judge will provide math problem along with solution. They will be formatted as follows: [Math Problem] <math_problem> ...(math problem)... </math_problem> [Solution] <paragraph_1> ...(paragraph 1 of solution)... </paragraph_1> ... <paragraph_n> ...(paragraph of solution)... </paragraph_n> Your task is to review each paragraph of the solution in sequence, analyzing, verifying, and critiquing the reasoning in detail. You need to provide the analyses and the conclusion in the following format: <analysis_1> ...(analysis of paragraph 1)... </analysis_1> ... <analysis_n> ...(analysis of paragraph n)... </analysis_n> <conclusion> Correct/Incorrect </conclusion> * When you analyze each paragraph, you should use proper verification, recalculation, or reflection to indicate whether it is logically and mathematically valid. Please elaborate on the analysis process carefully. * If an error is detected in any paragraph, you should describe the nature and cause of the error in detail, and suggest how to correct the error or the correct approach. Once paragraph is found to contain any error, stop further analysis of subsequent paragraphs (as they may depend on the identified error) and directly provide the conclusion of \"Incorrect.\" For instance, given solution of five paragraphs, if an error is found in the third paragraph, you should reply in the following format: <analysis_1> ...(analysis of paragraph 1)... </analysis_1> <analysis_2> ...(analysis of paragraph 2)... </analysis_3> <analysis_3> ...(analysis of paragraph 3; since an error is found here, also provide detailed critique and correction guideline)... </analysis_3> <conclusion> Incorrect </conclusion> Note that the analyses of paragraphs 4 and 5 should be skipped as the paragraph 3 has been found to contain an error. * Respond with your analyses and conclusion directly. -------------------------------------------------- The following is the math problem and the solution for you task: [Math Problem] {tagged_problem} [Solution] {tagged_response} 16 Setting Scoring GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM Avg. pass@8 (Upper Bound) maj@8 Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-1.5B Skywork-PRM-7B EurusPRM-Stage1 EurusPRM-Stage2 Qwen2.5-Math-7B-Math-Shepherd Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-PRM-7B Qwen2.5-Math-PRM-72B - - last product min last product min last product min last product min last product min last product min last product min last product min last product min last product min last product min 98.1 96.7 96.8 97.3 96.9 97.0 97.0 97.0 97.0 97.3 97.3 96.8 96.9 96.6 97.2 97.3 96. 94.7 95.6 95.8 94.7 95.4 96.1 97.1 96.9 97.0 96.7 96.9 96.9 96.9 97.1 97.0 97.6 97.6 97. 92 87.1 85.2 85.4 85.3 85.3 86.1 84.3 84.7 86.3 84.5 86.4 86.7 86.6 87.3 87.3 87. 79.7 83.0 83.3 79.7 83.4 83.6 87.7 86.5 86.7 86.3 86.9 86.6 87.2 88.0 87.8 88.9 88.7 88. 49.3 41.2 39.0 37.9 39.0 39.0 37.1 37.1 35.7 40.8 38.2 39.0 37.9 37.9 41.2 38.2 39. 32.7 35.7 39.0 33.1 34.9 39.3 38.6 36.8 36.8 37.9 37.1 39.7 39.0 42.6 42.3 43.4 46.0 45. 80.5 72.5 70.1 70.6 69.9 71.2 70.6 69.4 70.4 70.9 69.6 71.7 70.1 71.9 73.8 71.9 71. 61.6 66.2 67.8 61.3 67.3 68.8 73.8 71.4 72.5 71.9 71.2 71.7 73.5 74.5 74.3 73.8 74.3 74. 59.6 44.4 42.8 40.4 42.2 44.0 41.2 40.4 43.0 42.2 40.7 45.0 42.1 43.1 45.8 43.7 42. 33.8 38.2 37.9 34.2 39.1 38.8 44.6 41.6 43.1 44.3 44.0 45.6 45.5 47.6 46.2 49.2 48.1 48. 52.6 47.8 47.2 47.2 47.4 47.1 47.6 46.9 46.8 47.2 46.5 47.9 47.9 48.2 48.3 47.8 48. 45.7 46.2 46.6 45.7 46.3 46.7 48.1 47.7 47.6 47.6 47.6 47.8 48.5 48.7 48.3 49.6 49.3 49. 90.5 73.8 67.7 70.5 70.6 64.0 69.5 68.7 63.8 69.3 67.6 68.2 67.9 66.9 65.3 67.7 66. 63.4 66.6 67.4 63.5 67.3 67.5 68.0 69.3 70.7 68.1 70.9 71.1 72.0 74.5 74.1 76.8 81.1 80. 74.7 66.2 64.1 64.2 64.5 63.9 64.2 63.4 63.1 64.9 63.5 65.0 64.2 64.5 65.6 64.8 64. 58.8 61.6 62.5 58.9 62.0 63.0 65.4 64.3 64.9 64.7 64.9 65.6 66.1 67.6 67.1 68.5 69.3 69. Table 10: Performance comparison on the Best-of-8 strategy of the policy model Qwen2.5-Math-7BInstruct with 3 scoring strategies: last, product and minimum. represents the models we trained. 17 Setting Scoring GSM8K MATH Minerva Math GaoKao 2023 En Olympiad Bench College Math MMLU STEM Avg. pass@8 (Upper Bound) maj@8 Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-1.5B Skywork-PRM-7B EurusPRM-Stage1 EurusPRM-Stage Qwen2.5-Math-7B-Math-Shepherd Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-PRM-7B Qwen2.5-Math-PRM-72B - - last product min last product min last product min last product min last product min last product min last product min last product min last product min last product min last product min 97.3 96.0 96.2 96.5 96. 96.3 96.6 96.4 96.1 96.5 96.6 96.1 96.5 96.0 97.0 97.0 96.9 95.9 95.4 96.4 96.0 95.3 96. 97.0 96.9 97.0 96.7 96.5 96.5 96.8 96.8 96.7 96.3 96.4 96.4 93.2 88.6 87.0 86.8 86. 86.6 87.5 86.3 86.6 87.7 87.4 88.6 88.1 88.3 89.0 89.0 89.2 87.3 85.6 88.2 87.7 85.1 88. 89.6 88.5 88.6 88.8 88.9 89.1 89.0 89.6 89.6 89.8 89.9 89.7 56.6 47.8 46.7 45.6 45. 44.9 46.3 44.5 46.3 44.5 44.1 44.9 45.2 45.6 46.0 47.1 46.7 44.9 44.1 44.9 44.5 44.9 45. 44.9 46.0 46.0 47.1 47.4 47.1 46.7 46.7 46.3 47.8 46.0 46.3 83.6 73.8 73.0 71.9 73. 74.3 73.5 71.9 73.2 73.5 74.0 72.2 74.3 73.8 74.8 75.3 73.5 72.7 72.5 75.1 73.5 72.5 75. 77.4 75.8 74.8 76.1 75.3 76.1 75.3 77.7 77.9 76.6 77.4 77.7 62.4 50.1 47.3 49.2 48. 47.6 48.9 47.9 49.2 48.7 48.6 47.9 48.4 48.6 51.0 49.8 49.8 47.0 46.5 49.0 47.0 47.1 48. 50.8 49.9 50.2 50.1 50.7 50.7 49.8 51.4 50.8 53.3 52.9 52.4 54.1 50.2 49.8 49.5 49. 49.3 49.4 49.3 49.2 49.4 49.3 50.1 49.7 50.1 49.7 49.9 49.8 49.4 49.2 49.5 49.4 49.0 49. 50.5 49.5 49.6 49.5 50.1 49.9 50.3 50.4 50.3 50.9 50.1 50.4 95.3 84.9 76.3 77.5 76. 67.1 83.4 76.0 71.7 84.6 74.8 74.2 79.7 75.9 66.7 76.3 73.2 78.4 80.3 83.7 78.1 80.2 83. 74.9 79.7 79.6 71.8 76.6 75.3 78.4 76.4 76.0 80.5 82.3 81.2 77.5 70.2 68.0 68.1 68. 66.6 69.4 67.5 67.5 69.3 67.8 67.7 68.8 68.3 67.7 69.2 68.4 67.9 67.7 69.5 68.0 67.7 69. 69.3 69.5 69.4 68.6 69.4 69.2 69.5 69.9 69.7 70.7 70.7 70.6 Table 11: Performance comparison on the Best-of-8 strategy of the policy model Qwen2.5-Math-72BInstruct with 3 scoring strategies: last, product and minimum. represents the models we trained. Setting pass@8 (Upper Bound) maj@8 Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-1.5B Skywork-PRM-7B EurusPRM-Stage1 EurusPRM-Stage2 Qwen2.5-Math-7B-Math-Shepherd Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-PRM-7B Qwen2.5-Math-PRM-72B Scoring CMATH CN Middle School 24 GaoKao Avg. - - last product min last product min last product min last product min last product min last product min last product min last product min last product min last product min last product min 95.3 92.7 91.8 92.0 91. 92.8 92.7 92.8 93.2 92.7 93.0 93.8 92.8 93.3 94.0 93.3 93.8 91.8 91.7 91.7 91.8 92.0 92. 93.0 93.0 92.5 92.8 92.7 93.0 93.3 93.7 93.5 94.3 94.2 94.2 82.2 78.2 80.2 80.2 80. 79.2 77.2 76.2 75.2 76.2 74.3 80.2 79.2 80.2 81.2 79.2 80.2 77.2 77.2 78.2 77.2 77.2 78. 81.2 79.2 80.2 78.2 77.2 77.2 80.2 80.2 80.2 80.2 80.2 80.2 84.3 68.1 63.0 69.1 69. 57.2 65.8 62.1 56.9 63.6 67.3 66.6 66.3 66.6 66.7 68.1 66.3 55.4 52.6 64.4 55.7 52.4 64. 65.4 67.7 69.8 67.1 68.9 69.4 68.2 70.1 71.7 72.1 73.5 73.1 87.3 79.7 78.3 80.4 80. 76.4 78.6 77.0 75.1 77.5 78.2 80.2 79.4 80.0 80.6 80.2 80.1 74.8 73.8 78.1 74.9 73.9 78. 79.9 80.0 80.8 79.4 79.6 79.9 80.6 81.3 81.8 82.2 82.6 82.5 Table 12: Best-of-8 performance comparison on the Chinese benchmarks with the policy model Qwen2.5Math-7B-Instruct in 3 scoring strategies: last, product and minimum. represents the PRMs we trained. Setting pass@8 (Upper Bound) maj@8 Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-1.5B Skywork-PRM-7B EurusPRM-Stage1 EurusPRM-Stage2 Qwen2.5-Math-7B-Math-Shepherd Qwen2.5-Math-7B-PRM800K Qwen2.5-Math-PRM-7B Qwen2.5-Math-PRM-72B Scoring CMATH CN Middle School 24 GaoKao Avg. - - last product min last product min last product min last product min last product min last product min last product min last product min last product min last product min last product min 96.8 95.3 93.7 94.0 93. 94.3 93.8 93.3 94.3 94.3 94.5 94.8 93.8 94.5 95.3 94.7 94.8 94.0 93.8 94.7 94.2 93.7 94. 95.0 94.5 94.3 94.2 94.2 93.8 94.7 94.3 94.5 96.0 96.0 95.8 83.2 79.2 78.2 80.2 80. 79.2 79.2 79.2 79.2 79.2 79.2 80.2 79.2 80.2 80.2 80.2 80.2 79.2 80.2 79.2 79.2 80.2 79. 81.2 80.2 80.2 79.2 82.2 80.2 79.2 81.2 81.2 79.2 80.2 80.2 86.2 75.0 73.2 72.1 73. 65.5 72.0 71.2 63.0 72.5 73.5 74.3 69.7 74.6 72.6 71.5 76.0 64.5 64.5 70.8 63.4 65.4 69. 74.6 73.0 71.5 76.5 70.8 72.9 74.5 77.6 77.6 76.1 77.2 77.5 88.7 83.2 81.7 82.1 82. 79.7 81.7 81.2 78.8 82.0 82.4 83.1 80.9 83.1 82.7 82.1 83.7 79.2 79.5 81.6 78.9 79.8 81. 83.6 82.6 82.0 83.3 82.4 82.3 82.8 84.4 84.4 83.8 84.5 84.5 Table 13: Best-of-8 performance comparison on the Chinese benchmarks with the policy model Qwen2.5Math-72B-Instruct in 3 scoring strategies: last, product and minimum. represents the PRMs we trained."
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}