{
    "paper_title": "Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large Language Models with CheckboxQA",
    "authors": [
        "Michał Turski",
        "Mateusz Chiliński",
        "Łukasz Borchmann"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Checkboxes are critical in real-world document processing where the presence or absence of ticks directly informs data extraction and decision-making processes. Yet, despite the strong performance of Large Vision and Language Models across a wide range of tasks, they struggle with interpreting checkable content. This challenge becomes particularly pressing in industries where a single overlooked checkbox may lead to costly regulatory or contractual oversights. To address this gap, we introduce the CheckboxQA dataset, a targeted resource designed to evaluate and improve model performance on checkbox-related tasks. It reveals the limitations of current models and serves as a valuable tool for advancing document comprehension systems, with significant implications for applications in sectors such as legal tech and finance. The dataset is publicly available at: https://github.com/Snowflake-Labs/CheckboxQA"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 2 9 1 4 0 1 . 4 0 5 2 : r Unchecked and Overlooked: Addressing the Checkbox Blind Spot in Large Language Models with CheckboxQA Michał Turski, Mateusz Chiliński, and Łukasz Borchmann Snowflake AI Research michal.turski@snowflake.com Abstract. Checkboxes are critical in real-world document processing where the presence or absence of ticks directly informs data extraction and decision-making processes. Yet, despite the strong performance of Large Vision and Language Models across wide range of tasks, they struggle with interpreting checkable content. This challenge becomes particularly pressing in industries where single overlooked checkbox may lead to costly regulatory or contractual oversights. To address this gap, we introduce the CheckboxQA dataset, targeted resource designed to evaluate and improve model performance on checkbox-related tasks. It reveals the limitations of current models and serves as valuable tool for advancing document comprehension systems, with significant implications for applications in sectors such as legal tech and finance. The dataset is publicly available at: https://github.com/Snowflake-Labs/CheckboxQA Keywords: Dataset Visual Question Answering Visually Rich Documents Document Understanding Information Extraction"
        },
        {
            "title": "Introduction",
            "content": "Accurate checkbox interpretation is vital to organizational workflows, as any oversight can result in incorrect data entry, unaddressed legal obligations, or compliance breaches. Legal contracts, for instance, often include checkboxes to confirm acceptance of clauses like Non-Disclosure Agreement Accepted, while financial documents rely on them for optional selections such as Include Life Insurance. Processing errors in these small, yet significant elements can trigger substantial repercussions, ranging from data inaccuracies and legal misunderstandings to operational inefficiencies and regulatory violations. Automation of processing documents with such elements promises substantial gains in efficiency and accuracy but requires robust visual detection capabilities and nuanced contextual understanding. Although recent advances in large vision and language models (LVLMs) have shown remarkable effectiveness in 2 M. Turski et al. Question: Does the company disclose grants exceeding $5K? Answer: No Question: What vehicle type categories are recorded? Answer: CMV, HAZMAT Fig. 1. CheckboxQA consists of varied questions requiring interpretation of checkable content in the context of visually rich documents. Required answers range from simple yes/no to lists of values. tasks spanning image classification, object detection, and text recognition, these models frequently stumble when encountering checkable content in documents. Several factors may contribute to this shortfall: checkboxes are typically small and visually subtle, demanding fine-grained detection; their significance often hinges on the surrounding text and overall document structure; and available training data fail to include examples that capture the intricacies of checked versus unchecked states. In response to these shortcomings, we present CheckboxQAa specialized dataset designed to advance Document AI capabilities. It comprises diverse documents annotated with question-answer pairs that hinge on accurate checkbox interpretation (Figure 1). By focusing on this often-overlooked facet of document processing, CheckboxQA bridges critical gap in existing benchmarks and paves the way for more precise, robust, and context-sensitive models."
        },
        {
            "title": "2 Related Works",
            "content": "Checkbox comprehension is longstanding challenge in document processing. Before the deep learning era, traditional approaches relied on rule-based image analysis to locate checkbox squares through geometric heuristics and morphological operations. Once regions were detected, determining whether they were checked typically involved measuring pixel density or connectivity within those boxes. While these heuristic-driven methods were computationally efficient and required minimal labeled data, they often struggled with varied layouts and forms, necessitating extensive parameter tuning for each new layout [7,1,21,25]. Addressing the Checkbox Blind Spot in LLMs with CheckboxQA 3 With the advent of learning-based vision models, researchers began exploring neural networks for more robust and generalizable checkbox recognition [14,15,26,5]. By learning features directly from data, these methods outperformed template-based systems and could handle heterogeneous checkbox styles and noisy inputs. More recently, the rapid development of LLMs has triggered paradigm shift, unifying diverse taskspreviously addressed by specialized architecturesunder broader question-answering framework [13,20,19]. Document intelligence has benefited from this shift, moving towards natural language interfaces for visually rich documents [18,23,8]. In line with this trend, recent Document VQA benchmarks have emerged to promote research into visually grounded QA, including DocVQA [12], InfographicsVQA [11], SlideVQA [22], and DUDE [10]. These address variety of question types and visual complexities; however, they do not explicitly isolate the unique challenge of interpreting checkbox fields. The proposed CheckboxQA dataset fills this gap: It maintains the Document VQA paradigm while targeting critical but underrepresented element checkboxesand thus complements existing resources by focusing on form component they often treat implicitly or overlook entirely."
        },
        {
            "title": "3 CheckboxQA Benchmark",
            "content": "This section introduces CheckboxQA, curated dataset dedicated to the interpretation of checkboxes in visually rich documents. We describe how the dataset was compiled, annotated, and validated, along with key statistical insights that underscore its diversity and real-world applicability. Fig. 2. Excerpts from CheckboxQA documents (not an exhaustive list). 4 M. Turski et al."
        },
        {
            "title": "3.1 Documents Collection",
            "content": "We collected document samples from public subset of DocumentCloud,1 ensuring balance of form types and visual styles. Our primary selection criteria emphasized the following. Presence of Checkboxes. Each document contains one or more fields of varying shapes and sizes. Additionally, we required that at least one of the selections in the document was positive. Visual Diversity. To further ensure diversity, we cross-checked layout complexities (multi-column forms, tabular structures, single-page vs. multi-page) and document qualities (transparent vs. slightly degraded scans) to mimic real-life digitization scenarios (see Figure 2). Language and License. Only English documents published under permissive licenses were included. Fig. 3. Histogram of collected documents lengths in terms of PDF pages and words. The plot on the right indicates long tail of lengthy documents. Overall, around 90 multi-page documents fulfilling these rigorous selection criteria were collected and used in the QA annotation process. Figure 3 analyzes their length in terms of the total number of pages and words."
        },
        {
            "title": "3.2 Annotation of Question-Answer Pairs",
            "content": "CheckboxQA was constructed using guidelines adapted from broader questionanswer annotation framework by annotators experienced in creating document VQA datasets. 1 https://www.documentcloud.org/ Addressing the Checkbox Blind Spot in LLMs with CheckboxQA 5 Fig. 4. Most popular question prefixes. Fig. 5. Histogram of annotated questions and answers lengths. 6 M. Turski et al. Over three weeks (at an estimated cost of 3K USD), contractors systematically reviewed each document and generated up to ten questions targeting checked versus unchecked items. Answers were kept conciseYes, No, single label, or short list of labelsto pinpoint which boxes were marked without unnecessary phrasing. The process yielded approximately 600 question-answer pairs, half requiring Yes/No answers and half being open-ended. Figure 4 studies popular patterns of question prefixes, whereas Figure 6 presents statistics of QA lengths and their distribution. Fig. 6. Distribution of QA pairs across CheckboxQA documents."
        },
        {
            "title": "3.3 Problem Formulation and Experimental Setup",
            "content": "We define checkbox interpretation in Document VQA paradigm [12]. Given the document images (typically resulting from rendering PDF file) and question focusing on checkbox-related information, the model must produce the correct textual answer. The answer can take various forms, depending on the question: Binary (Yes/No): Is this checkbox checked? Singleton Identifier: Which option is checked here? (if exactly one option is selectable) List of Checked Items: Which vehicle categories are indicated as applicable? (if multiple options can be selected) To succeed, system must jointly parse textual content, identify relevant checkbox regions, and link them to the question context. This involves detecting checkbox-like elements and determining if they are checked or unchecked, reading context (surrounding text, labels, or instructions) to ground each checkbox in meaningful semantics, and answering the question accurately by fusing the checkbox state with the textual context."
        },
        {
            "title": "3.4 Evaluation Metrics",
            "content": "Following prior work on Document VQA and Scene Text VQA, we adopt single metric termed Average Normalized Levenshtein Similarity (ANLS) to evaluate model predictions for CheckboxQA [3,12]. Effectively, it is fuzzy variant of accuracy, where string similarity above the threshold yields partial score. perfect string match results in ANLS = 1 Addressing the Checkbox Blind Spot in LLMs with CheckboxQA 7 (that is zero edit distance), while answers with significant mismatch reduce the score accordingly. Consequently, ANLS captures minor variations in wording (small edit distance) and significant discrepancies between the predicted and ground-truth answers. Specifically, we rely on the ANLS variant of ANLS (that, among others, supports list answers in addition to plain values) with minimal similarity threshold of 0.5 [17]."
        },
        {
            "title": "3.5 Baseline Approaches and Human Performance",
            "content": "We evaluate suite of baseline models to assess the difficulty of CheckboxQA and provide reference performance levels. Evaluation of commercial LVLMs follows the previously established protocol and prompts [4]. Specifically, we convert PDFs into series of PNG images having 2048px along longer dimension,2 and feed them to the model with question preceded by short instruction: Answer the question. Do not write full sentence. Provide value as Python list. If there is single answer, the output should be one-element list like [\"ANSWER\"]. If there are multiple valid answers, the list will have several elements, e.g., [\"ANSWER 1\", \"ANSWER 2\"]. Question: Open-source LVLMs are evaluated using vLLM [9] with default inference options. Finally, we employ human annotators to gauge the upper bound of CheckboxQA performance. It was obtained by passing all of the documents and assigning questions to different annotator in precisely the same way models see them during the inference."
        },
        {
            "title": "4 Results and Analysis",
            "content": "We conducted experiments with state-of-the-art commercial and open-source solutions, including models from GPT-4o [16], Gemini 2.0 [6], Qwen 2.5 VL [24], and Pixtral [2] families to evaluate how well large vision-language models handle the fine-grained task of interpreting checkboxes."
        },
        {
            "title": "4.1 Quantitative Analysis",
            "content": "Table 1 reports the performance of various LVLMs and human baseline. Among the tested systems, Qwen 2.5 VL 72B attains the highest score at 83.2%, significantly outperforming GPT-4o. Smaller Qwen variants, Pixtral 12B, and the Gemini series exhibit more modest results in the range of 43.6% to 71.9%. GPT4o mini remains at the lower end with score of 40.4%. 2 In rare cases where given this size, the model couldnt fit the entire context, the longer dimension was reduced to 1024px or 768px. 8 M. Turski et al. Table 1. CheckboxQA evaluation results, compared to human performance. Model Score (ANLS) Qwen 2.5 VL 72B Qwen 2.5 VL 7B Snowflake Arctic-TILT 0.8B 2025-03 GPT-4o 2024-11-20 Gemini 2.0 Pro exp-02-05 Gemini 2.0 Flash Lite preview-02-05 Pixtral 12B Gemini 2.0 Flash 001 Qwen 2.5 VL 3B GPT-4o mini 2024-07-18 human performance 83.2 71.9 66.8 66.7 59.7 55.2 56.9 54.4 43.6 40.4 97. Notably, the top Qwen models perform relatively well, which may suggest that their pretraining data includes substantial number of form-like images with checkbox annotations. If so, it substantiates our core claim: checkbox content has generally been overlooked in conventional large-scale training, and models that happen (by design or otherwise) to include such specialized examples gain distinct advantage in tasks like those included in CheckboxQA. Despite these advances, every model still falls short of the near-ceiling human baseline of 97.5%, underscoring the difficulty of accurately identifying and interpreting checkmarks. These elements are often visually subtle or positioned unpredictably, demanding fine-grained spatial and textual reasoning that remains challenging for LVLMs."
        },
        {
            "title": "4.2 Qualitative Observations",
            "content": "Qualitative assessment of CheckboxQA reveals specific scenarios where these models systematically fail. Below, we highlight examples from leading models to illustrate recurring mistakes. Misaligned Checkbox and Text Context. In certain documents, the checkbox for given label can appear on either the left or right side of the text. Some LVLMs fail to associate the correct checkbox with its label (Figure 7). Defaulting to Textual Clues Instead of Checkbox States. When asked binary question about additional tasks, models sometimes rely on textual context rather than checking the actual box state. Selecting All Possible Options in List. In scenarios where only one or few checkboxes should be selected, models occasionally list every available option (Figure 8). Addressing the Checkbox Blind Spot in LLMs with CheckboxQA Question: Is additional tasking required? GPT-4o, Gemini: Yes Question: What option is selected in Education: minimum level required ? GPT-4o: Bachelors Fig. 7. While visually, the label for the checkbox is clearly anchored to different answer, strong LVLMs incorrectly associate the text with the checkbox. Ignoring Table Structure for Checkbox Fields. When checkboxes are placed in table, the surrounding text may become distracting for the model. Returning Special Symbols Instead of Textual Answers. Certain models respond with literal symbols (e.g., or ) in place of text. Confusing the Question Text as the Answer. Another frequent error occurs when the model interprets the question text as part of the response. Overall, these failures underscore the need for robust representations that jointly model the spatial arrangement of checkbox fields and the visual distinction between checked and unchecked states. They also demonstrate that large language modelseven ones with robust text understandingstill struggle when the question hinges on subtle visual or structural cue rather than text alone."
        },
        {
            "title": "5 Limitations",
            "content": "Although CheckboxQA advances the study of checkbox-related tasks in Document AI, it remains subject to several limitations. First, all documents and annotations are in English, potentially restricting the datasets applicability to other languages and character sets. Second, despite efforts to diversify the document collection, certain domains (e.g. medical or highly technical forms) may be underrepresented, limiting the datasets coverage of specialized use cases. Finally, while the results provide insights into model performance, they focus on specific subset of commercial and open-source models. 10 M. Turski et al. Question: What are the types of products to benefit from use of reported information? GPT-4o, Gemini: BASIC INTELLIGENCE, CURRENT INTELLIGENCE, ESTIMATIVE INTELLIGENCE, S&T INTELLIGENCE Fig. 8. Models commonly fail to discriminate among checked and unchecked boxes and instead enumerate all available labels."
        },
        {
            "title": "6 Summary",
            "content": "We presented CheckboxQA, targeted dataset designed to evaluate how large vision-language models handle checkboxes in visually rich documents. This task is of considerable practical importance, given that checkbox errors can lead to significant operational missteps. For instance, missed opt-out box in legal contract could expose firm to privacy breaches, underscoring how even minor checkable fields can carry significant practical consequences. While large vision-language models have made substantial strides in document understanding, accurately interpreting checkboxes remains significant challenge. Even the top-performing models in our experiments fell substantially short of human-level performance, indicating persistent gaps in fine-grained visual reasoning and layout comprehension. These gaps are particularly evident in misaligning checkboxes with the appropriate text, defaulting to textual cues when visual inspection is required, and failing to filter out unchecked items in multi-selection scenarios. Ultimately, the performance trends observed in CheckboxQA suggest that progress in broad document understanding does not uniformly translate to proficiency in micro-level visual tasks. By isolating the challenges posed by checkboxes, our benchmark aims to catalyze research on more specialized form understanding methods, paving the way for systems that can handle all the intricate details of real-world forms with high accuracy. Addressing the Checkbox Blind Spot in LLMs with CheckboxQA"
        },
        {
            "title": "A Dataset Card for CheckboxQA",
            "content": "Dataset Overview. CheckboxQA is specialized benchmark focused on interpreting checkboxes in visually rich documents. It comprises multi-page English documents containing one or more checkboxes and about 600 question-answer (QA) pairs. The dataset provides an in-depth look at model capabilities for checkbox detection and state interpretation within real-world forms. Motivation. Although Document AI benchmarks are abundant, most do not isolate the challenge of distinguishing checked vs. unchecked states. CheckboxQA addresses this gap, ensuring models accurately associate checkbox states with nearby textual descriptions. Data Collection. Underlying PDF files were gathered from http://documentcloud. org, emphasizing diverse form layouts (multi-column, tabular, single/multi-page) and variations in scan quality (clear vs. mildly degraded). Language. All documents are in English, reflecting the prevalent use of Englishlanguage forms and permits in public-domain sources. Annotation Process. Trained annotators generated up to ten question-answer pairs per document, each focusing on checkbox state or label interpretation. Answers typically take one of the following forms: Yes/No (binary), Single selection, List of selections. Dataset Composition. Document count: 88. Total QA pairs: 579. Intended Use. CheckboxQA is designed for: Benchmarking vision-language models on fine-grained checkbox detection, Research on layout-aware document understanding, Testing end-to-end systems combining OCR, layout parsing, and QA. Licensing and Distribution. All documents were sourced under permissive or publicdomain licenses. We do not rehost files but provide script to download them from the original providers instead. We release annotations on Apache 2.0. Limitations. Focused on English-language documents only. Primarily covers forms, surveys, and agreements, which may not generalize to all document domains. Annotator biases or small sample sizes could limit coverage of rare checkbox designs. 12 M. Turski et al."
        },
        {
            "title": "References",
            "content": "1. Adams, J., Yfantis, E., Curtis, D., Pack, T.: Feature extraction methods for form recognition applications. WSEAS Transactions on Information Science and Applications 3(3), 666671 (2006) 2. Agrawal, P., Antoniak, S., Hanna, E.B., Bout, B., Chaplot, D., Chudnovsky, J., Costa, D., Monicault, B.D., Garg, S., Gervet, T., Ghosh, S., Héliou, A., Jacob, P., Jiang, A.Q., Khandelwal, K., Lacroix, T., Lample, G., Casas, D.L., Lavril, T., Scao, T.L., Lo, A., Marshall, W., Martin, L., Mensch, A., Muddireddy, P., Nemychnikova, V., Pellat, M., Platen, P.V., Raghuraman, N., Rozière, B., Sablayrolles, A., Saulnier, L., Sauvestre, R., Shang, W., Soletskyi, R., Stewart, L., Stock, P., Studnia, J., Subramanian, S., Vaze, S., Wang, T., Yang, S.: Pixtral 12b (2024), https://arxiv.org/abs/2410.07073 3. Biten, A.F., Tito, R., Mafla, A., Gomez, L., Rusiñol, M., Mathew, M., Jawahar, C.V., Valveny, E., Karatzas, D.: Icdar 2019 competition on scene text visual question answering (2019), https://arxiv.org/abs/1907.00490 4. Łukasz Borchmann: Notes on applicability of gpt-4 to document understanding (2024), https://arxiv.org/abs/2405.18433 5. Folks, R.D., Naik, B.I., Brown, D.E., Durieux, M.E.: Computer vision digitization of smartphone images of anesthesia paper health records from low-middle income countries. BMC bioinformatics 25(1), 178 (2024) 6. Gemini Team, et al.: Gemini: family of highly capable multimodal models (2024), https://arxiv.org/abs/2312.11805 7. Istle, J.M.: Optical character recognition for checkbox detection. University of Nevada, Las Vegas (2004) 8. Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W., Yun, S., Han, D., Park, S.: Ocr-free document understanding transformer (2022), https: //arxiv.org/abs/2111.15664 9. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.H., Gonzalez, J.E., Zhang, H., Stoica, I.: Efficient memory management for large language model serving with pagedattention (2023), https://arxiv.org/abs/2309. 10. Landeghem, J.V., Tito, R., Łukasz Borchmann, Pietruszka, M., Józiak, P., Powalski, R., Jurkiewicz, D., Coustaty, M., Ackaert, B., Valveny, E., Blaschko, M., Moens, S., Stanisławek, T.: Document Understanding Dataset and Evaluation (DUDE) (2023), https://arxiv.org/abs/2305.08455 11. Mathew, M., Bagal, V., Tito, R.P., Karatzas, D., Valveny, E., Jawahar, C.V.: InfographicVQA (2021), https://arxiv.org/abs/2104.12756 12. Mathew, M., Karatzas, D., Jawahar, C.V.: DocVQA: Dataset for VQA on Document Images (2021), https://arxiv.org/abs/2007.00398 13. McCann, B., Keskar, N.S., Xiong, C., Socher, R.: The natural language decathlon: Multitask learning as question answering (2018), https://arxiv.org/abs/1806. 14. Murphy, E., Samuel, S., Cho, J., Adorno, W., Durieux, M., Brown, D., Ndaribitse, C.: Checkbox detection on rwandan perioperative flowsheets using convolutional neural network. In: 2021 systems and information engineering design symposium (SIEDS). pp. 16. IEEE (2021) 15. Nagarikar, A., Dangi, R.S., Maity, S.K., Kuvelkar, A., Wandhekar, S.: Input fields recognition in documents using deep learning techniques. REVISTA GEINTECGESTAO INOVACAO TECNOLOGIAS 11(4), 44054415 (2021) 16. Open AI, et al.: Gpt-4o system card (2024), https://arxiv.org/abs/2410.21276 Addressing the Checkbox Blind Spot in LLMs with CheckboxQA 13 17. Peer, D., Schöpf, P., Nebendahl, V., Rietzler, A., Stabinger, S.: Anls* universal document processing metric for generative large language models (2024), https: //arxiv.org/abs/2402. 18. Powalski, R., Borchmann, Ł., Jurkiewicz, D., Dwojak, T., Pietruszka, M., Pałka, G.: Going full-tilt boogie on document understanding with text-image-layout transformer. In: Lladós, J., Lopresti, D., Uchida, S. (eds.) Document Analysis and Recognition ICDAR 2021. pp. 732747. Springer International Publishing, Cham (2021) 19. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language models are unsupervised multitask learners (2019), https://api.semanticscholar. org/CorpusID:160025533 20. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with unified text-to-text transformer. CoRR abs/1910.10683 (2019), http://arxiv.org/abs/1910.10683 21. Shengnan, Z., Shanlei, Y., Lianqiang, N.: Automatic recognition method for checkbox in data form image. In: 2014 Sixth International Conference on Measuring Technology and Mechatronics Automation. pp. 159162 (2014). https: //doi.org/10.1109/ICMTMA.2014.42 22. Tanaka, R., Nishida, K., Nishida, K., Hasegawa, T., Saito, I., Saito, K.: SlideVQA: Dataset for Document Visual Question Answering on Multiple Images (2023), https://arxiv.org/abs/2301.04883 23. Tang, Z., Yang, Z., Wang, G., Fang, Y., Liu, Y., Zhu, C., Zeng, M., Zhang, C., Bansal, M.: Unifying vision, text, and layout for universal document processing (2023), https://arxiv.org/abs/2212.02623 24. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., Lin, J.: Qwen2-vl: Enhancing vision-language models perception of the world at any resolution (2024), https://arxiv.org/abs/2409. 25. Zahray, L.: Automating data extraction from prescription document images to reduce human error. Ph.D. thesis, Massachusetts Institute of Technology (2019) 26. Zhao, F., Zhang, C., Saxena, N., Wallach, D., Rabby, A.S.A.: Ballot tabulation using deep learning. In: 2023 IEEE 24th International Conference on Information Reuse and Integration for Data Science (IRI). pp. 107114 (2023). https://doi. org/10.1109/IRI58017.2023."
        }
    ],
    "affiliations": [
        "Snowflake AI Research"
    ]
}