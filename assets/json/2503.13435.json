{
    "paper_title": "WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes",
    "authors": [
        "Ling Yang",
        "Kaixin Zhu",
        "Juanxi Tian",
        "Bohan Zeng",
        "Mingbao Lin",
        "Hongjuan Pei",
        "Wentao Zhang",
        "Shuicheng Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid development of 3D reconstruction technology, research in 4D reconstruction is also advancing, existing 4D reconstruction methods can generate high-quality 4D scenes. However, due to the challenges in acquiring multi-view video data, the current 4D reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. In practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4D reconstruction datasets. Additionally, existing 4D reconstruction methods rely on deformation fields to estimate the dynamics of 3D objects, but deformation fields struggle with wide-range spatial movements, which limits the ability to achieve high-quality 4D scene reconstruction with wide-range spatial movements. In this paper, we focus on 4D scene reconstruction with significant object spatial movements and propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark includes rich 4D scene data with large spatial variations, allowing for a more comprehensive evaluation of the generation capabilities of 4D generation methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D, which generates stable and high-quality 4D results across various complex 4D scene reconstruction tasks. We conduct both quantitative and qualitative comparison experiments on WideRange4D, showing that our Progress4D outperforms existing state-of-the-art 4D reconstruction methods. Project: https://github.com/Gen-Verse/WideRange4D"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 5 3 4 3 1 . 3 0 5 2 : r WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes Ling Yang1*, Kaixin Zhu1*, Juanxi Tian1*, Bohan Zeng1* Mingbao Lin3, Hongjuan Pei2, Wentao Zhang1, Shuicheng Yan3 1Peking University 2University of Chinese Academy of Sciences 3National University of Singapore https://github.com/Gen-Verse/WideRange4D Figure 1. Overview of our WideRange4D, which features wide-range spatial movements and wide variety of scenes."
        },
        {
            "title": "Abstract",
            "content": "With the rapid development of 3D reconstruction technology, research in 4D reconstruction is also advancing, existing 4D reconstruction methods can generate high-quality 4D scenes. However, due to the challenges in acquiring multi-view video data, the current 4D reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. In practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4D reconstruction datasets. Additionally, existing 4D reconstruction methods rely on deformation fields to estimate the dynamics of 3D objects, but deformation fields struggle with wide- *Contributed equally. Contact: yansc@nus.edu.sg yangling0818@163.com, wentao.zhang@pku.edu.cn, 1 range spatial movements, which limits the ability to achieve high-quality 4D scene reconstruction with wide-range spatial movements. In this paper, we focus on 4D scene reconstruction with significant object spatial movements and propose novel 4D reconstruction benchmark, WideRange4D. This benchmark includes rich 4D scene data with large spatial variations, allowing for more comprehensive evaluation of the generation capabilities of 4D generation methods. Furthermore, we introduce new 4D reconstruction method, Progress4D, which generates stable and highquality 4D results across various complex 4D scene reconstruction tasks. We conduct both quantitative and qualitative comparison experiments on WideRange4D, showing that our Progress4D outperforms existing state-of-the-art 4D reconstruction methods. tion computed by neural networks is suitable for actions performed in place. Existing 4D reconstruction methods generate 4D results with poor quality and blur when widerange spatial movements of entire objects occur, as shown in Fig. 2. Additionally, some video-to-4D generation methods [5, 33] use segmentation and tracking models to generate 4D scenes with wide-range dynamics, however, the results generated by these methods generally have low quality. In this paper, we focus on high-quality 4D scene reconstruction in complex scenarios. We curate new test benchmark, WideRange4D, which contains various real and virtual scenes, each with an unspecified number of foreground subjects. The movement distance and action complexity of each foreground subject are diverse. Compared to previous 4D benchmark, our WideRange4D greatly enhances the richness, diversity, and difficulty, providing more comprehensive and effective evaluation of 4D generation methods, as shown in Fig. 1. To achieve stable and highquality 4D reconstruction, we propose Progress4D, which divides the 4D generation process into two stages: highquality 3D scene reconstruction and progressive fitting of 4D dynamics. This method ensures high-quality 4D scene reconstruction while also maintaining stability, especially in complex 4D scenes involving wide-range spatial movements. We validate the reconstruction ability of Progress4D on our proposed WideRange4D, and experimental results show that Progress4D outperforms existing state-of-the-art methods. Our contributions are as follows: We introduce WideRange4D, 4D benchmark that includes 4D scene data with wide-range spatial movements. Compared to existing 4D benchmarks, our datasets richness and reconstruction difficulty are greatly enhanced. We propose new 4D reconstruction method, Progress4D. This method divides the 4D scene reconstruction process into two stages: high-quality 3D reconstruction and progressive fitting of 4D dynamics. Compared to existing 4D reconstruction methods, Progress4D can more stably generate high-quality and rational 4D scenes. We conduct qualitative and quantitative comparisons on our proposed WideRange4D. The comparison results indicate that our Progress4D achieves SOTA performance in reconstructing 4D scenes with wide-range spatial movements. 2. Related Work 2.1. 4D Dataset 4D dataset refers to three-dimensional dataset that includes the time dimension, typically represented by sequence of videos captured from different viewpoints simultaneously, used to capture the continuous spatial and temporal changes of dynamic scenes or objects. Some early 2 Figure 2. Visualization of the 4D scenes generated by our Progress4D and 4DGS [32] on the WideRange4D. Existing 4D reconstruction methods struggle to generate 4D scenes with widerange movements, highlighting the need for our proposed new benchmark WideRange4D, and the high-quality 4D scenes produced by Progress4D validate the effectiveness of our method. 1. Introduction Constructing high-quality 4D scenes holds immense value for both academic research and industrial applications, and this task is highly challenging. With the continuous development of 3D generation technologies, some 4D reconstruction methods can achieve high-quality 4D generation in certain scenarios, positively contributing to the development of multimedia fields such as gaming and video. Compared to the challenging task of acquiring 3D data, obtaining 4D data is more difficult. It requires capturing dynamic videos of scenes from different viewpoints within the same time period. Due to the challenges in data acquisition, particularly the difficulty of recording motion videos of the same scene from multiple cameras, each capturing different viewpoints, in real-world environments, existing 4D reconstruction methods often provide 4D data [16, 18, 35] with only limited, localized motions, such as dancing in place, cooking, or assembling bicycle, lacking 4D data involving wide-range spatial movements of objects in the scene. Due to the limitations of existing 4D reconstruction benchmark, although 4D reconstruction methods [32, 35] can construct high-quality 4D results, they cannot verify the performance of these methods in 4D scenes involving wide-range spatial movements due to the limitations of test data. Furthermore, these methods utilize deformation fields to achieve dynamic 4D scenes, but the deformaFigure 3. Exhibition of Testing Examples in WideRange4D. works [16, 18, 35] introduced 4D reconstruction datasets for real-world scenarios. However, due to constraints in shooting conditions and limited environments, these datasets lack scene diversity, and the subjects do not exhibit wide-range movement. The 3D dataset, Objaverse dataset [7], also contains 4D data. While the dataset is large, it primarily includes cartoon characters or animals performing localized actions. More recent works, such as DimensionX [27] and 4Real [41], have proposed methods for generating higher-quality 4D data, yet generating from 360 viewpoints still fails to ensure high fidelity in every view. To address this issue, we construct richly contextual 4D dataset, WideRange4D, using the Unreal Engine, which includes objects with significant foreground movement across 360 viewpoints. 2.2. 4D Generation Methods With the maturity of 3D generation technologies such as NeRF [2, 15, 1922, 31, 42] and 3D Gaussian Splatting (3DGS) [6, 10, 12, 17, 28, 38], 4D generation models based 3 Figure 5. Exhibition of foreground objects and tracking networks to synthesize 4D scenes with significant foreground object movement. However, the quality of the generated 4D scenes is often low and lacks realism. In this paper, we propose the 4D reconstruction method Progress4D, which divides the 4D reconstruction process into two stages: high-quality 3D reconstruction and 4D dynamic progressive fitting. This method provides more stable and rational construction of high-quality 4D scenes for 4D scene reconstruction tasks involving wide-range spatial movements. 3. New 4D Benchmark - WideRange4D To address the limitation of existing 4D datasets, which lack examples of significant foreground object movement, in this section, we introduce our proposed WideRange4D, which serves as new comprehensive benchmark for 4D reconstruction tasks. Next, we will categorize the dataset from various perspectives, count the elements used in the dataset (including foreground objects, weather, actions, etc.), and explain how we curate our WideRange4D to showcase its comprehensiveness and richness. 3.1. Data Categories the To ensure and richness of comprehensiveness WideRange4D, we construct the dataset by considering multiple dimensions to capture wide range of scenarios. WideRange4D can be categorized into several types in each perspective. First, we consider Scene Types, which include Real-world Scenes, Virtual Scenes, and various weather conditions. These diverse scenes are used to validate the generalization capability of 4D reconstruction methods. In Fig. 3(a), we present data samples that illustrate environments across different categories. Next, we examine movement distance and speed, which can be broadly classified into short-range, medium-range, and long-range, with different movement trajectories. Longer movement distances and more complex movement paths generally lead to higher reconstruction difficulty. Compared to existing 4D reconstruction datasets, WideRange4D significantly increases the reconstruction challenge, providing better Figure 4. Statistical Distribution of WideRange4D. on these 3D generation models have been proposed. 4D generation model can be defined as 3D model with temporal information. The initial 4D reconstruction models based on NeRF [16, 18, 23, 35] collect 4D reconstruction benchmarks and proposed feasible 4D reconstruction methods, with 4K4D [35] ensuring very high-quality 4D reconstruction. Subsequently, due to the higher efficiency and greater flexibility of the 3DGS model compared to NeRF, 4D reconstruction model based on 3DGS [14, 32, 36, 40] is introduced. These methods improve generation efficiency while achieving high-quality 4D scene reconstruction on existing 4D reconstruction benchmarks. However, due to limitations in the test data and the deformation field capabilities used in 4D reconstruction methods, existing 4D reconstruction methods struggle to achieve high-quality reconstruction results when dealing with 4D scene reconstruction involving wide-range movement of foreground objects. There are also methods for 4D scene generation conditioned on text or monocular video [5, 33, 34, 43]. These methods leverage segmentation models, large language models, 4 Figure 6. WideRange4D Construction Pipeline. benchmark for evaluating the reconstruction ability and stability of 4D reconstruction methods. Examples of movement paths with varying degrees of complexity are shown in Fig. 3(b). Finally, we consider motion complexity, which encompasses rapid, complex motion changes, slow motion changes, and repetitive motion patterns. This range of action complexities allows for more effective testing of the realism and quality of 4D scene reconstruction methods. We illustrate different levels of action complexity in the 4D data presented in Fig. 3(c). of the data, as shown in Fig. 5, we selected variety of foreground objects with different sizes and styles to construct the dataset. The statistics of the Foreground Objects category in the dataset are shown in Fig. 4(b). Finally, we perform statistical analysis on motion types and movement ranges, as shown in Fig. 4(c). In our WideRange4D, more challenging motion types and wider movement ranges account for higher proportion, making WideRange4D not only comprehensive but also more challenging than previous 4D reconstruction benchmarks. 3.2. Statistical Analysis 3.3. Data Acquisition and Annotation In order to present our WideRange4D more clearly, we conduct statistical analysis on the elements included in the data samples. First, regarding the environment, the scenes we use include cities, cartoon prairies, and country roads. These environments encompass real-world and virtual scenarios, and feature varying amounts of background objects, from densely packed structures, to open and unobstructed landscapes. We also use three types of weather: sunny, rainy, and sandstorm. The sunny weather has minimal background changes, the rainy weather introduces fine raindrops, increasing the difficulty of detail reconstruction, and the sandstorm significantly affects background changes, further increasing the difficulty of 4D reconstruction. The statistics of scenes and weather are shown in Fig. 4(a). Next, for Foreground Objects, to ensure the richness and diversity We employ several workers with experience in Unreal Engine to build the dataset using open-source 3D assets. For the 4D reconstruction data, we utilize SfM [25] to initialize the scenes. When the quality of 3D scene initialization is poor, we perform further refinement. Additionally, we conduct user study to evaluate the realism of the motions and the plausibility of the movement paths in the data samples of WideRange4D. This process helps to filter out unreasonable data, ensuring that our WideRange4D can serve as more reliable and validated benchmark for the 4D generation field. As shown in Fig. 6. The dataset WideRange4D is constructed through meticulously designed pipeline emphasizing multi-source asset integration, biomechanically diverse motion synthesis, and environmentally dynamic scene 5 Figure 7. 4D scene initial comparison. The reason why existing methods fail to generate high-quality 4D scenes is that when 4D scene involves wide-range variations, the initialized foreground objects become so blurred that they seem to vanish completely, making it difficult to achieve high-quality 4D synthesis in subsequent generation processes. 4.1. Preliminaries Before analyzing 4D generation, it is essential to first understand 3D generation models. Among existing 3D generation models, the 3DGS model is the most flexible and effective. This model consists of multiple 3D Gaussian Points G, and each point is represented by center position µ, covariance Σ, opacity α, and color c. The covariance Σ is computed using the scaling matrix and rotation matrix as Σ = RSSR. During the rendering process, each pixel of the rendering result is computed by combining points. This process can be expressed as: (cid:88) G(x) = (cid:18) αi ci exp (x µi)Σ1 (cid:19) (x µi) , 1 i=1 (1) where represents the position of random point in the 3D scene during the rendering process. The 4D model extends the 3D model by incorporating temporal information. The method involves calculating the deformation of each Gaussian Point at time t, specifically by introducing deformation field represented by Multi-Layer Perceptrons (MLPs). This deformation includes changes to the position µ, scaling S, and rotation R. The deformed attributes of the Gaussian Point (µ, R, S) can be represented as: (µ, R, S) = (µ + µ, + R, + S). (2) Since it is challenging for MLPs to estimate rational widerange spatial movement directly, existing 4D reconstruction methods struggle to handle 4D scenes with wide-range variations. Next, we explain how our proposed Progress4D achieves stable and high-quality 4D reconstruction in complex scenarios. 4.2. The Progress4D Framework To achieve high-quality 4D scene reconstruction, we divide the process into two steps: generating high-quality 3D scene and gradually fitting the dynamic 4D scene. High-quality scene initialization. The first step is to create the high-quality 3D scene initialization. Since the test cases in existing 4D reconstruction benchmarks typically involve relatively simple in-place movements, it is easier to initialize higher-quality 3D scene. However, for 4D scenes with wide-range movement, as shown in Fig. 7, existing 4D reconstruction methods struggle to initialize high-quality 3D scene. This leads to difficulties in generating high-quality 4D results in complex 4D reconstruction tasks. Therefore, in our method, when initializing the 3D scene, we perform high-quality 3D reconstruction of all objects in the 4D scene in their stationary state, ensuring highquality 4D scene reconstruction. Figure 8. Framework of our Progress4D. composition. This process ensures the generation of largescale benchmark tailored for evaluating 4D reconstruction robustness under complex scenarios. 4. New Baseline Method In this section, we detail the process of reconstructing rational 4D scenes with wide-range spatial movement. Firstly, we introduce the preliminaries related to 4D generation. Then, we present the specific algorithm of our Progress4D and analyze how it achieves more stable and higher-quality 4D scene synthesis compared to existing methods. 6 Algorithm 1 Optimized Training Pipeline of Progress4D Require: Multi-view video frames I, initial 3D Gaussian representation G0, timestep partition = T0, T1, T2 Ensure: Optimized 4D scene representation 1: Stage 1: High-Fidelity 3D Scene Initialization 2: Optimize G0 with multi-view consistency to minimize"
        },
        {
            "title": "Linit",
            "content": "3: Freeze the stabilized 3D scene representation 4: Stage 2: Progressive Fitting of 4D Dynamics 5: while Training has not converged do 6: for each timestep ti 1 T1 do 7: 8: 9: 10: 11: 12: 13: (µti 1 deformation Compute ) using MLP , Sti Identify closest aligned timestep tk , Rti 1 1 parameters 0 T0 based on temporal similarity Compute temporal distance dt = ti Compute adaptive alignment weight: 1 tk 0 wti 1 = w0 dt + 1. 1 1 + exp(µti 1 µtk ) Compute timestep alignment loss: Lalign(ti 1) = wti 1 I(µti 1 µtk 0 > τ )µti 1 µtk 0 end for Compute overall loss: = L1 + Ltv + Lalign(ti 1) (cid:88) ti 1T1 Update timestep sets: 14: 15: 16: 17: end while 18: return optimized 4D scene representation Move aligned timesteps T1 T0 Select nearest timesteps from T2 T1 4D dynamics progressive fitting. With the high-quality initialized 3D scene, the next step is implementing the dynamics in the complex 4D scene. Implementing rational wide-range deformation is difficult for randomly initialized deformation field. We adopt step-by-step data loading strategy to align the 4D scene with the initialized 3D scene and the inference multi-view video. This alignment is done progressively based on the similarity between the 4D scene and each frame of the multi-view video, from high similarity to low similarity. Specifically, we divide the timesteps of the 4D scene into three parts: the aligned 0, t1 timesteps T0 = {t0 0, ...}, the timesteps currently being aligned T1 = {t0 1, t1 1, ...}, and the timesteps yet to be 2, t1 aligned T2 = {t0 2, ...}. During training, the 4D scene is mainly aligned with the video frames ˆI corresponding to the timesteps in T1. The data loading update strategy is to add all the aligned timesteps in T1 to T0, and add the timesteps in T2 that are closest to the previous T1 timesteps into T1. This gradual process allows the deformation field to adapt to the dynamics of the 4D scene, enabling it to achieve complex wide-range movement. 1, tk To ensure the training of the 4D scene during the timesteps in T1 is as stable as possible, we introduce an advanced timestep alignment loss Lalign, intricately guided by motion mask (ti 0) derived from per-frame kinematic saliency. This mask prioritizes regions of significant dynamic variation, enhancing the deformation fields precision in capturing complex spatial-temporal transitions. Let µti represent the position deformation estimated by the 1 in T1, and let tk deformation field at particular timestep ti 0 denote the timestep in T0 most similar to ti 1, then Lalign is expressed with richly parameterized weight as: 1 = w0 ti 1 tk 0 + 1.0 (cid:16) µti 1 Lalign = (cid:124) µtk (cid:123)(cid:122) 1,tk (ti 0 ) 0 (cid:16) 1 + exp (cid:17) , µtk 0 > τ µti 1 µtk 0 , µti (cid:17) 1 (cid:125) (3) where w0 is baseline weight, I() is the indicator function, τ is predefined threshold, (ti 0) is the motion mask that emphasizes regions with significant dynamic variation. Similarly to other reconstruction methods [1, 8, 9, 12, 23, 26, 32, 35], we also use L1 color loss and grid-based totalvariation loss Ltv. The full loss function can be defined as: 1, tk = L1 + Ltv + Lalign. (4) To comprehensively illustrate the optimization process of Progress4D, we provide specific process in Fig. 8 and detailed explanation of its algorithmic workflow 1 for reconstructing 4D scenes with large-scale spatial movement. The algorithm of Progress4D consists of two key stages: (1) high-fidelity 3D scene initialization and (2) progressive adaptation to 4D dynamics. 5. Experiment 5.1. Implementation Details Data Setting. Consistent with previous 4D benchmarks, WideRange4D also provides sequential frames from multiple viewpoints. The input data is processed into specific dataset formats as each method requires. Each testing case includes 40 viewpoints, with each viewpoint containing 60 to 150 frames. Most testing cases involve wide-range spatial movement. Optimization. All experiments are conducted on 8 RTX 4090 GPUs, and each 4D reconstruction task is completed Figure 9. Qualitative comparison. For images that seem similar, we zoom in on the same regions in both our outputs and the comparison images to highlight the finer details of our Progress4D. on single RTX 4090 GPU. We utilize the Adam optimizer [13] with learning rate of 1.6e4 to optimize our 4D model. For our Progress4D, the timestep lists T0, T1, and T2 are updated every 1000 iterations. Metrics. We employ L1, PSNR, SSIM [30], and LPIPS [44] to evaluate the performance of 4D generation methods. L1 and PSNR measure the difference between the generated image and its ground truth, where lower L1 value and higher PSNR value indicate better reconstruction quality. SSIM measures the structural similarity between the generated image and its ground truth, with higher values indicating better reconstruction quality. LPIPS assesses the perceptual quality of the generated images, with lower values indicating better generation results. 8 Table 1. Quantitative Comparison of 4D Reconstruction Methods (Best in Bold) Method L1 PSNR SSIM LPIPS Dreamscene4D 0.0168 0.0165 SC4D 0.0155 4DGS 0.0153 ST-4DGS 0.0145 Ours 21.33 21.72 24.65 26.35 28. 0.75 0.77 0.82 0.84 0.87 0.30 0.29 0.25 0.24 0.22 Figure 10. Ablation study of timestep alignment loss. Baseline. For baseline methods, we compare our Progress4D with other 4D reconstruction methods, including 4DGS [32] and ST-4DGS [14], as well as monocular video-to-4D generation methods such as DreamScene4D [5] and SC4D [33], which use segment models and track models [3, 4, 11, 24] to achieve rational 4D scene generation. to ensure fairness in comparison, we only compare the 4D reconstruction results of these methods with the ground truth under the corresponding viewpoint of the input video. To reduce the impact of the background, we additionally introduce mask for monocular video-to-4D generation methods. All comparisons are based on the official code of the baseline methods available on GitHub. For monocular video-to-4D generation, 5.2. Main Results We conduct both qualitative and quantitative comparisons between our Progress4D and other baseline methods on WideRange4D to validate the effectiveness of Progress4D, while also highlighting the significant challenges that WideRange4D presents to existing 4D generation methods. In Table 1, we present the results of the quantitative comparison. It is evident that, for the metrics measuring reconstruction consistency (L1 and PSNR), the metric assessing structural similarity (SSIM), and LPIPS, which gauges the authenticity of generated images, our Progress4D achieves the best performance, thereby demonstrating the effectiveness of proposed 4D reconstruction method Progress4D. At the same time, the testing results of the baseline methods on WideRange4D are significantly worse than those on other 4D reconstruction benchmarks, proving both the difficulty of our WideRange4D and the richness of its data samples, 9 while also indicating the stability of our Progress4D. Additionally, we provide qualitative comparison in Fig. 9, which visualizes the generated results of our Progress4D and the baseline methods, offering more intuitive demonstration of the generation quality of our Progress4D and the existing methods. In Sample 1, we showcase the results generated by various methods from different viewpoints and times. For the 4D reconstruction methods 4DGS and ST-4DGS, especially 4DGS, although the background quality is high, the moving objects in the foreground are highly blurred. Additionally, using the default parameter settings in the official code of ST-4DGS fails to produce high-quality 4D scenes, demonstrating that reconstructing 4D scenes with wide-range spatial movement is challenging for existing 4D reconstruction methods. Furthermore, for SC4D and DreamScene4D, which utilize segmentation and tracking models to achieve rational 4D scene generation, it is difficult to achieve clear and accurate 4D generation using the data samples in our WideRange4D, further proving the challenge of testing 4D scene examples in WideRange4D. Our method successfully and stably reconstructs rational, high-quality 4D scenes. To further demonstrate the quality of 4D scene generation by our Progress4D, in Sample 2, we show 4D scene of actions performed in place. Compared to previous approaches, our methods reconstruction results are the clearest, with the best detail preservation. 5.3. Ablation Study In Section 4.2, we have already analyzed the contribution of high-quality scene initialization in 4D scene generation. In this section, we analyze the effectiveness of the timestep alignment loss Lalign in 4D dynamics progressive fitting. As shown in Fig. 10, without Lalign, the scene generation quality is high when the deviation from the initial scene is small. However, as time progresses and the movement distance increases, the foreground significantly deteriorates. With Lalign, even with substantial deviations, the generation effect remains impressive, proving that the timestep alignment loss plays crucial role in the dynamic synthesis of 4D scenes. 6. Conclusion In this work, we propose new 4D reconstruction benchmark, WideRange4D, which addresses critical limitation in existing 4D benchmarks, the lack of data with wide-range movements, and fills this gap by introducing data samples that encompass these dynamics. Our WideRange4D contains diverse and rich content, enabling more comprehensive evaluation of 4D generation methods. Additionally, to overcome the limitations of previous 4D reconstruction methods in handling wide-range variations, we introduce Progress4D. Our method decomposes 4D scene generation into high-quality 3D scene initialization and progressive 4D dynamic fitting. Both qualitative and quantitative experiments demonstrate that our method generates higher-quality 4D scenes more stably compared to baseline 4D generation methods. For future work, we consider (i) to utilize flow matching methods [29, 39] to improve reconstruction efficiency (ii) to incorporate more (multimodal) LLMs and reward models [37, 45] to enhance data diversity and quality."
        },
        {
            "title": "References",
            "content": "[1] Ang Cao and Justin Johnson. Hexplane: fast representaIn CVPR, pages 130141, 2023. tion for dynamic scenes. 7 [2] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57995809, 2021. 3 [3] Ho Kei Cheng and Alexander Schwing. Xmem: Longterm video object segmentation with an atkinson shiffrin memory model. In European Conference on Computer Vision (ECCV), 2022. 9 [4] Wen-Hsuan Chu, Adam Harley, Pavel Tokmakov, Achal Dave, Leonidas Guibas, and Katerina Fragkiadaki. Zeroshot open-vocabulary tracking with large pre-trained models. In International Conference on Robotics and Automation (ICRA), 2024. 9 [5] Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene generation from monocular videos. arXiv preprint arXiv:2405.02280, 2024. 2, 4, [6] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free genarXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. 3 [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, pages 13142 13153, 2023. 3 [8] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nießner, and Qi Tian. Fast dynamic radiance fields with time-aware neural voxels. In SIGGRAPH Asia, pages 19, 2022. 7 [9] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, pages 1247912488, 2023. 7 [10] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. 3 [11] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [12] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 3, 7, 12 [13] Diederik P. Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 8 [14] Deqi Li, Shi-Sheng Huang, Zhiyuan Lu, Xinran Duan, and Hua Huang. St-4dgs: Spatial-temporally consistent 4d gaussian splatting for efficient dynamic scene rendering. In SIGGRAPH, pages 111, 2024. 4, 9 [15] Hong Li, Yutang Feng, Song Xue, Xuhui Liu, Bohan Zeng, Shanglin Li, Boyu Liu, Jianzhuang Liu, Shumin Han, and Baochang Zhang. Uv-idm: identity-conditioned latent diffusion model for face uv-texture generation. In CVPR, pages 1058510595, 2024. 3 [16] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In CVPR, pages 55215531, 2022. 2, 3, [17] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards highfidelity text-to-3d generation via interval score matching. In CVPR, pages 65176526, 2024. 3 [18] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance In SIGGRAPH fields for interactive free-viewpoint video. Asia, pages 19, 2022. 2, 3, 4 [19] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 3 [20] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. TOG, 41(4):115, 2022. [21] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In CVPR, pages 1145311464, 2021. [22] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2022. [23] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In CVPR, pages 1031810327, 2021. 4, 7 [24] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 9 [25] Johannes Schonberger and Jan-Michael Frahm. StructureIn CVPR, pages 41044113, 2016. from-motion revisited. 5 [26] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In CVPR, pages 54595469, 2022. 7 10 Tulyakov, and Hsin-Ying Lee. 4real: Towards photorealistic 4d scene generation via video diffusion models. arXiv preprint arXiv:2406.07472, 2024. [42] Bohan Zeng, Shanglin Li, Yutang Feng, Ling Yang, Hong Li, Sicheng Gao, Jiaming Liu, Conghui He, Wentao Zhang, Jianzhuang Liu, et al. Ipdreamer: Appearance-controllable 3d object generation with complex image prompts. arXiv preprint arXiv:2310.05375, 2023. 3 [43] Bohan Zeng, Ling Yang, Siyu Li, Jiaming Liu, Zixiang Zhang, Juanxi Tian, Kaixin Zhu, Yongzhen Guo, Fu-Yun Wang, Minkai Xu, et al. Trans4d: Realistic geometry-aware arXiv transition for compositional text-to-4d synthesis. preprint arXiv:2410.07155, 2024. 4 [44] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, pages 586 595, 2018. 8 [45] Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, and Bin Cui. Itercomp: Iterative composition-aware feedback learning from model gallery for text-to-image generation. arXiv preprint arXiv:2410.07171, 2024. 10 [27] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. 3 [28] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In ICLR, 2024. 3 [29] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. [30] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: From error visibility to structural similarity. TIP, 13(4):600612, 2004. 8 [31] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. NeurIPS, 36:84068441, 2023. 3 [32] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In CVPR, pages 2031020320, 2024. 2, 4, 7, 9, 13 [33] Zijie Wu, Chaohui Yu, Yanqin Jiang, Chenjie Cao, Fan Wang, and Xiang Bai. Sc4d: Sparse-controlled video-to-4d generation and motion transfer. In ECCV, pages 361379, 2024. 2, 4, 9 [34] Dejia Xu, Hanwen Liang, Neel Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos Plataniotis, and Zhangyang Wang. Comp4d: Llm-guided compositional 4d scene generation. arXiv preprint arXiv:2403.16993, 2024. 4 [35] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming Sun, Yujun Shen, Hujun Bao, and Xiaowei Zhou. 4k4d: Real-time 4d view synthesis at 4k resolution. In CVPR, pages 2002920040, 2024. 2, 3, 4, 7 [36] Zhen Xu, Yinghao Xu, Zhiyuan Yu, Sida Peng, Jiaming Sun, Hujun Bao, and Xiaowei Zhou. Representing long volumetric video with temporal gaussian hierarchy. TOG, 43(6):1 18, 2024. [37] Ling Yang, Bohan Zeng, Jiaming Liu, Hong Li, Minghao Xu, Wentao Zhang, and Shuicheng Yan. Editworld: Simulating world dynamics for instruction-following image editing. arXiv preprint arXiv:2405.14785, 2024. 10 [38] Ling Yang, Zixiang Zhang, Junlin Han, Bohan Zeng, Runjia Li, Philip Torr, and Wentao Zhang. Semantic score distillation sampling for compositional text-to-3d generation. arXiv preprint arXiv:2410.09009, 2024. 3 [39] Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024. 10 [40] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Realtime photorealistic dynamic scene representation and rendering with 4d gaussian splatting. In ICLR, 2024. 4 [41] Heng Yu, Chaoyang Wang, Peiye Zhuang, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Laszlo Jeni, Sergey WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes"
        },
        {
            "title": "Supplementary Material",
            "content": "7. WideRange4D Construction Pipeline In this section, we provide specific construction pipeline of our WideRange4D. 7.1. Asset Integration and Motion Synthesis The construction of WideRange4D begins with the integration of heterogeneous 3D assets and biomechanically valid motion adaptation. Foreground objects, including humans, animals, and stylized characters, are sourced from three platforms: rigged humanoid models from Mixamo and Unreal Engines MetaHuman, and some other assets (e.g., animal, ancient) from the Unreal Marketplace (FAB). These models are categorized by geometric scale (Small, Medium, Large) and artistic style (realistic, cartoonish) to ensure diversity in object geometry, texture, and semantic representation. Animation assets, which span locomotion and gestures are extracted from Mixamos animation library and retargeted to heterogeneous skeletons using Unreal Engines Control Rig system. Key technical steps include resolving joint discrepancies through inverse kinematics (IK) solvers to ensure anatomical plausibility and dynamically scaling root motion parameters to adapt animations across varying model proportions, such as adjusting stride lengths for quadrupedal animals or gait cycles for bipedal humans. To amplify reconstruction challenges, motion trajectories are procedurally synthesized into three complexity tiers: short-range linear paths, medium-range curved trajectories, and long-range multi-segment paths. These trajectories are generated via spline-based procedural animation, incorporating collision constraints to enforce kinematic validity while maintaining natural movement patterns. The integration of diverse objects, biomechanical motion adaptation, and complex trajectory design collectively ensure that WideRange4D captures broad spectrum of motion scenarios critical for evaluating 4D reconstruction robustness. 7.2. Environmentally Dynamic Scene Composition Dynamic scene composition in WideRange4D emphasizes environmental variability and multi-view geometric consistency. Scenes are constructed using modular assets from Unreal Engines FAB library, categorized into real-world environments (urban streets, country roads), virtual settings (cartoon prairies), and hybrid configurations blending synthetic and real-world elements. Environmental dynamics are parametrically controlled via the Ultra Dynamic Sky plugin, introducing three weather states: sunny conditions with stable illumination and minimal atmospheric interference, rainy weather with particle-based raindrops, surface wetness effects, and refractive distortions, and sandstorms characterized by volumetric dust clouds, wind-driven debris, and visibility degradation. The multi-view data acquisition is implemented through configurable array of CineCamera Actors, strategically positioned with parametric control over azimuth (0360), elevation (-1515), and radial distance (210 meters) relative to scene centroids. Each camera is configured with 15.0mm focal length, f/2.8 aperture, and Universal Zoom lens system paired with 16:9 Digital Film sensor backplate to ensure cinematic depth-of-field and optical consistency across viewpoints. Scene illumination and environmental conditions are dynamically modulated under three configurations (sunny, rainy, and sandstorm), with synchronized rendering performed across all cameras to generate 60 temporally aligned RGB sequences per scene. Rendered at 2K resolution (25601440 pixels) and 30 FPS. This setup ensures high-fidelity multi-perspective capture while maintaining photometric and geometric coherence, enabling robust evaluation of 4D reconstruction algorithms under diverse environmental and viewpoint constraints. 8. Algorithm Procedure of Progress4D Below, we present structured breakdown of the methodology, elaborating on the critical aspects of optimization and alignment mechanisms involved. 8.1. High-Quality 3D Scene Initialization The foundation of Progress4D relies on constructing geometrically precise and visually consistent 3D scene. To this end, the Gaussian representation G0 is optimized by refining its opacity α, color attributes c, spatial coordinates µ, and covariance matrix Σ through multi-view image supervision. The initialization objective function is formulated as: (5) Linit = L1 + Ltv, where L1 represents pixel-wise color reconstruction loss, ensuring photometric consistency, while Ltv enforces smoothness regularization to suppress noise and artifacts. Once convergence is achieved, the optimized 3D Gaussian [12] representation is retained as stable foundation for subsequent temporal adaptation. 8.2. Progressive Fitting of 4D Dynamics Following 3D initialization, the next stage involves learning temporally coherent deformations to accurately model 12 Figure 11. Other Qualitative Comparison Results. large-scale object and scene transformations. Given the inherent complexity of estimating non-rigid motion, we introduce progressive timestep alignment mechanism that progressively refines the reconstructed 4D scene. The temporal optimization process partitions timesteps into three categories: T0: Timesteps that have been successfully aligned and stabilized. T1: Timesteps undergoing active alignment and refinement. T2: Future timesteps yet to be processed. At each iteration, deformation parameters µ, S, and are estimated using the deformation field. To ensure robust alignment, similarity metric is computed between timesteps in T1 and their closest reference frames in T0, defining an adaptive weighting function that modulates the influence of each timestep in the optimization process. By iteratively refining timestep alignments and leveraging adaptive deformation estimation, Progress4D attains stable and high-fidelity 4D representation. The integration of spatial smoothness priors, motion-aware losses, and progressive optimization ensures seamless temporal transitions while preserving geometric precision. This approach surpasses conventional methods by significantly enhancing reconstruction stability, robustness, and overall visual plausibility. 9. Additional Generated Results To further demonstrate the efficacy of Progress4D in reconstructing complex 4D scenes, we present additional qualitative comparisons with 4DGS [32] across various dynamic environments. These comparisons highlight the methods ability to handle challenging scenarios involving extensive motion, occlusions, and intricate structural deformations. As shown in Fig. 11, each example presents sequence of rendered frames, demonstrating our reconstructions temporal coherence and spatial accuracy. The results highlight the smoothness of temporal transitions and the preservation of fine-grained details, even in the presence of wide-range spatial movements. By integrating our progressive alignment strategy, artifacts and distortions are minimized, resulting in more visually plausible reconstructions than those produced by existing methods. The robustness of Progress4D is further validated by its 13 performance across diverse scene types, including both virtual and real-world scenarios. Our method effectively generalizes to varying motion patterns and wide movement ranges, establishing it as reliable solution for high-fidelity 4D scene reconstruction. Moreover, Progress4D consistently outperforms baseline approaches in terms of rendering clarity, geometric consistency, and overall realism."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Peking University",
        "University of Chinese Academy of Sciences"
    ]
}