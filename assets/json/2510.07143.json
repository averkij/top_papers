{
    "paper_title": "Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods",
    "authors": [
        "Chenfei Liao",
        "Wensong Wang",
        "Zichen Wen",
        "Xu Zheng",
        "Yiyu Wang",
        "Haocong He",
        "Yuanhuiyi Lyu",
        "Lutao Jiang",
        "Xin Zou",
        "Yuqian Fu",
        "Bin Ren",
        "Linfeng Zhang",
        "Xuming Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily focused on visual token compression. The effectiveness of these methods is typically assessed by measuring the accuracy drop on established benchmarks, comparing model performance before and after compression. However, these benchmarks are originally designed to assess the perception and reasoning capabilities of MLLMs, rather than to evaluate compression techniques. As a result, directly applying them to visual token compression introduces a task mismatch. Strikingly, our investigation reveals that simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks. Through extensive experiments, we make the following observations: (i) Current benchmarks are noisy for the visual token compression task. (ii) Down-sampling is able to serve as a data filter to evaluate the difficulty of samples in the visual token compression task. Motivated by these findings, we introduce VTC-Bench, an evaluation framework that incorporates a data filtering mechanism to denoise existing benchmarks, thereby enabling fairer and more accurate assessment of visual token compression methods. All data and code are available at https://github.com/Chenfei-Liao/VTC-Bench."
        },
        {
            "title": "Start",
            "content": "Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods Chenfei Liao1,2,6 Wensong Wang3,2 Zichen Wen2,5 Xu Zheng1,4,6 Yiyu Wang2 Haocong He2 Yuanhuiyi Lyu1,6 Lutao Jiang1,6 Xin Zou1,6 Yuqian Fu4 Bin Ren7,8,4 Linfeng Zhang2,* Xuming Hu1,6,* 1Hong Kong University of Science and Technology (Guangzhou) 2Shanghai Jiao Tong University 3Northeastern University 5Shanghai AI Laboratory 4INSAIT, Sofia University St. Kliment Ohridski 6Hong Kong University of Science and Technology 7University of Pisa 8University of Trento 5 2 0 2 8 ] . [ 1 3 4 1 7 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily focused on visual token compression. The effectiveness of these methods is typically assessed by measuring the accuracy drop on established benchmarks, comparing model performance before and after compression. However, these benchmarks are originally designed to assess the perception and reasoning capabilities of MLLMs, rather than to evaluate compression techniques. As result, directly applying them to visual token compression introduces task mismatch. Strikingly, our investigation reveals that simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks. Through extensive experiments, we make the following observations: (i) Current benchmarks are noisy for the visual token compression task. (ii) Down-sampling is able to serve as data filter to evaluate the difficulty of samples in the visual token compression task. Motivated by these findings, we introduce VTC-Bench, an evaluation framework that incorporates data filtering mechanism to denoise existing benchmarks, thereby enabling fairer and more accurate assessment of visual token compression methods. All data and code are available at https://github.com/Chenfei-Liao/VTC-Bench."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) have shown impressive abilities in understanding, reasoning, and generating content across vision and language (Chen et al., 2024c; Kang et al., 2025), enabling applications such as embodied AI (Yin et al., 2024; Fu et al., 2025; Cheng et al., 2025; Yang et al., 2025c). However, their efficiency is often constrained by the high computational cost *Corresponding authors. 1 Figure 1: (a) Average Decline Ratio (ADR) of five visual token compression methods on eight benchmarks (Model: Qwen2-VL-7B; Benchmark: as shown in Table 1; Device: 1 A800)). (b) Inference time per image comparison of DART and Downsample (Model: Qwen2VL-7B; Benchmark: MMstar; Compression Ratio: 0.75; Device: 1 A800). of processing images, particularly at high resolutions (Liu et al., 2025). This bottleneck arises because visual tokens, derived from image patches, typically far outnumber textual tokens, leading to substantial memory consumption and inference latency (Wang et al., 2025; Chen et al., 2025; Wen et al., 2025c). To mitigate this issue, numerous visual token compression methods have been proposed to reduce redundancy while retaining essential visual information (Yang et al., 2025a; Xing et al., 2024; Wen et al., 2025a; Xiong et al., 2025; Zou et al., 2025). Yet, these methods are typically evaluated on general MLLM benchmarks (Li et al., 2024c), which are not designed for compression, therefore failing to provide appropriate evaluation criteria. Thus, in this paper, we uncover surprising finding: as in Figure 1, simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks. This suggests that current evaluation frameworks dont adequately capture the challenges inherent in visual token compression. To investigate this, we conduct comprehensive empirical study comparing multiple state-of-theart visual token compression methods against simple downsampling baseline across eight widely used benchmarks. Based on the study results in Table 1 and 2, two crucial findings are concluded: ① The counterintuitive phenomenon mentioned above generally exists in popular benchmarks, proving that current benchmarks are noisy for the visual token compression task. ② The correct sample group under downsampling methods has achieved significantly better accuracy than the incorrect sample group under downsampling methods across various compression methods and benchmarks, proving that downsampling can serve as data filter to evaluate the difficulty of samples upon the visual token compression task. Based on these findings, we propose VTCBench, new evaluation framework specifically designed to optimize and denoise current existing benchmarks, aiming to evaluate visual token compression methods fairly. By explicitly distinguishing between simple and difficult samples through downsampling, VTC-Bench adaptively and fairly selects \"difficult\" samples that satisfy the requirements of evaluating visual token compression methods. Overall, our contributions are threefold: ① We identify and validate the data noise in existing MLLM benchmarks on the visual token compression task. ② We introduce data filtering mechanism using downsampling as discriminator to categorize benchmark samples by difficulty. ③ We propose VTC-Bench, the first evaluation framework tailored for fairly evaluating visual token compression methods, aiming to foster more meaningful progress in this emerging field."
        },
        {
            "title": "2.1 Visual Token Compression for MLLMs",
            "content": "Since visual tokens typically outnumber text tokens in MLLMs, compressing visual tokens has emerged as promising strategy to accelerate inference (Liu et al., 2025). Leveraging the inherent redundancy in visual tokens, variety of training-free methods have been proposed. FastV (Chen et al., 2024a), the first to explore visual token compression in MLLMs, prunes redundant tokens based on their average attention scores. Building on this idea, SparseVLM (Zhang et al., 2025) introduces recycling strategy to achieve more compact and flexible compression. Other methods, such as PyramidDrop (Xing et al., 2024), FiCoCo-V (Han et al., 2024), and MustDrop (Liu et al., 2024a), divide the compression process into multiple stages, enabling more precise identification of redundant tokens. In contrast, DART (Wen et al., 2025b) departs from importance-based selection entirely and instead prioritizes token duplication as key criterion, achieving surprisingly strong compression performance. Similarly, G-Prune (Jiang et al., 2025) identifies critical tokens through graph-based perspective. Beyond these, GreedyPrune (Pei et al., 2025) and ToDRE (Li et al., 2025) cast token compression as an optimization problem and employ greedy algorithms to search for efficient pruning strategies. However, as in Sec. 3, we have surprising observation: across most MLLM benchmarks, these sophisticated visual token compression methods under-perform compared to simply reducing the original image resolution, which motivates deeper investigation into the underlying causes."
        },
        {
            "title": "2.2 MLLM Benchmarks",
            "content": "Existing MLLM benchmarks primarily focus on areas such as perception and reasoning (Li et al., 2024c). For example, MME (Yin et al., 2024), MMBench (Liu et al., 2024b), SEED-Bench (Li et al., 2024b), and MM-Vet (Yu et al., 2023, 2024) provide broad perception-focused evaluations of MLLMs visual understanding. In parallel, domain benchmarks target specific applications such as autonomous driving (Sima et al., 2024; Qian et al., 2024) and remote sensing (Muhtar et al., 2024). For visual token compression in MLLMs, only one benchmark currently exists: EffiVLM (Wang et al., 2025). It offers unified framework for benchmarking training-free acceleration methods but relies on existing datasets (e.g., DocVQA (Mathew et al., 2021), ChartQA (Masry et al., 2022)) rather than data tailored to token compression. Building on data-driven insights into compression behavior, we introduce VTC-Bench, the first dedicated, challenging evaluation framework for visual token compression in MLLMs. We aim for VTC-Bench to catalyze new research and insights, enabling fair comparisons and sharper evaluations of tokencompression methods."
        },
        {
            "title": "3.1 Motivation",
            "content": "Some of the recent MLLMs, such as Qwen2VL (Wang et al., 2024) and Qwen2.5-VL (Bai et al., 2 Table 1: Comparison of Advanced Token Compression Methods and Downsampling on Qwen2-VL-7B. ADR refers to the average decline ratio, which is the average value of the decline ratio of each benchmark."
        },
        {
            "title": "ADR",
            "content": "Qwen2-VL-7B Upper Bound. All Tokens (100%)"
        },
        {
            "title": "Vanilla",
            "content": "62.3 78.9 78.0 2306 88.4 57. Qwen2-VL-7B + FastV + VisionZip + PruMerge+ + DART + Downsample Qwen2-VL-7B + FastV + VisionZip + PruMerge+ + DART + Downsample Qwen2-VL-7B + FastV + VisionZip + PruMerge+ + DART + Downsample Qwen2-VL-7B + FastV + VisionZip + PruMerge+ + DART + Downsample Qwen2-VL-7B + FastV + VisionZip + PruMerge+ + DART + Downsample 57.0 58.6 59.4 56.9 59.2 52.3 53.3 54.8 51.9 55. 49.0 49.0 48.7 49.2 52.6 46.1 46.4 45.0 45.6 50.1 38.2 41.9 39.0 40.5 43.5 73.7 71.1 72.1 72.5 75.0 65.0 62.9 62.2 61.3 69.0 57.1 54.8 48.4 53.4 66. 43.9 49.5 39.1 47.9 62.0 23.9 40.5 23.7 30.8 51.6 Token Reduction ( 75.00%) 2083 2062 2044 2066 2259 84.5 87.1 87.2 84.7 86.2 44.6 47.2 48.0 47.2 50. Token Reduction ( 88.89%) 1854 1820 1806 1915 2127 77.4 83.6 84.3 80.5 82.9 40.3 40.2 38.4 39.8 44.0 Token Reduction ( 93.75%) 1684 1704 1679 1786 74.9 80.2 79.2 78.1 79.5 37.5 35.2 33.2 33.6 40.9 Token Reduction ( 96.00%) 1589 1628 1544 1701 1938 72.4 77.8 74.0 74.7 78.8 33.6 33.4 30.5 31.7 37. Token Reduction ( 99.00%) 1189 1335 1165 1346 1589 55.0 65.5 51.6 60.0 72.8 26.1 30.8 25.7 28.8 33.8 73.1 70.5 72.0 70.2 73.8 65.5 63.0 61.3 61.8 70. 57.9 54.0 48.1 54.0 66.8 46.6 50.0 40.9 48.2 61.4 24.5 40.5 24.4 30.7 51.9 80.7 42.0 42.1 33.9 52.5 64.9 25.9 25.1 22.2 41.0 48. 18.7 15.9 14.4 33.7 40.3 14.4 12.0 10.5 29.3 32.3 5.8 4.9 3.5 23.2 13.2 81.6 100.0 58.1 66.9 56.2 52.7 65. 32.9 48.4 44.2 30.8 24.8 20.6 28.0 30.0 19.2 12.7 15.8 19.4 20.9 16.6 11.7 11.9 12.8 13.9 11.8 12.1 83.2 84.9 82.7 83.9 91.0 70.2 72.5 71.0 71.6 77. 62.1 62.2 59.5 63.2 71.0 54.5 57.1 52.1 58.3 66.4 38.0 47.3 37.4 45.4 55.4 2025), natively support inputs of varying resolutions. trivial yet efficient method to handle highresolution images is to simply downsample them to lower resolution, effectively using naive pixel sampling as form of compression. However, as shown in Sec. 2.1, most token compression methods for MLLMs choose to adaptively drop useless tokens or merge similar tokens instead of directly downsampling the original image, which should be more intelligent and reasonable methods. While in recent works (Yang et al., 2025b), it is surprising that image downsampling exceeds other sophisticated methods under some settings. In order to further investigate the causes of this anomalous phenomenon, we decide to comprehensively compare the results of the downsampling methods with other methods under various settings."
        },
        {
            "title": "3.2 Experiments Setup",
            "content": "Before conducting experiments, it is crucial to choose suitable MLLM for achieving the downsampling method. Most MLLMs only support 3 fixed-resolution inputs, which makes it impossible to achieve the downsampling method. In other words, for such MLLMs, no matter which resolution the original image is downsampled to, the image will finally be resized to fixed resolution, making the downsampling meaningless. Considering that Qwen2-VL (Wang et al., 2024) and Qwen2.5VL (Bai et al., 2025), based on the naive dynamic resolution mechanism and M-RoPE techniques, are the open-source MLLMs closest to realizing the concept of allowing arbitrary resolution inputs, we choose Qwen2-VL in our comparison experiments, which supports the downsampling method the best. In order to ensure that downsampling occurs at the original resolution as much as possible without adding extra resizing operations, we set Qwen2VLs max pixels and min pixels to 2408448 and 3136. In this case, only few extremely highresolution images will be resized before downsampling to ensure sufficient GPU memory. To guarantee comprehensive experiments, we token compression methtypical four select ods(FastV (Chen et al., 2024a), VisionZip (Yang et al., 2025a), PruMerge+ (Shang et al., 2024), and DART (Wen et al., 2025b)) with the token compression ratio set to 75.00%, 88.89%, 93.75%, 96.00%, and 99.00%. For the token compression ratio C, the downsampling method applies an equivalent downsampling ratio for fairness. The rule is shown in Eq. 1. Moreover, we choose eight popular benchmarks, including six general benchmarks (GQA (Hudson and Manning, 2019), MMBench_EN (Liu et al., 2024b), MMBench_CN (Liu et al., 2024b), MME (Yin et al., 2024), POPE (Li et al., 2023), and MMStar (Chen et al., 2024b)) and two resolution-sensitive OCR benchmarks (OCRBench (Liu et al., 2024c), and ChartQA (Masry et al., 2022))."
        },
        {
            "title": "1\nD2 × 100% = 1 − C",
            "content": "(1)"
        },
        {
            "title": "3.3 Results Analysis",
            "content": "Comparison between Different Methods: Across wide range of compression ratios and generalpurpose benchmarks, naive image downsampling achieves superior performance compared to sophisticated token compression methods in most conditions. For instance, at 93.75% compression, downsampling achieves 66.4% on MMBench, outperforming the best advanced method, DART, by 24.3% relative improvement. Similarly, on GQA, downsampling maintains consistent lead across all compression ratios. The results verify basic phenomenon in the field of visual token compression: substantial portion of samples in generalpurpose benchmarks can be correctly answered using only low-resolution global information, without requiring the fine-grained visual details that advanced methods strive to preserve. Comparison between different compression ratios: As compression becomes increasingly aggressive (96.00% and 99.00%), all sophisticated token compression methods experience performance degradation, while image downsampling demonstrates remarkably graceful degradation. At 99.00% compression, downsampling maintains score of 51.6% on MMBench, while FastV and PruMerge+ decrease to approximately 24%. The results further verify the phenomenon above: in the existing general-purpose benchmarks, image downsampling can fully meet the acceleration requirements for most samples. Comparison between Different Tasks: On Figure 2: Comparison of advanced token compression methods and downsampling on Qwen2-VL-7B by groups at 75% compression. tasks requiring fine-grained visual understandingparticularly chart comprehensionwe observe reversal of the phenomenon mentioned above. At moderate compression ratios (93.75% and 88.89%), VisionZip and FastV outperform image downsampling on ChartQA by significant margins. This divergence is highly informative: while image downsampling uniformly preserves global information at the expense of local details, the sophisticated compression methods can selectively retain text regions and numeric values that are critical for chart understanding, which can be considered difficult to compress. Thus, deeper observation of the above phenomenon can be concluded:the sophisticated token compression methods demonstrate the expected effectiveness in tasks that require fine-grained visual understanding. The comparisons across methods, compression ratios, and tasks provide compelling evidence that current benchmarks contain substantial simplicity bias. The performance advantage of image downsampling emerges not from its sophistication but from its ability to adequately address samples that dont require fine-grained visual understandingprecisely the samples that dominate current benchmarks. Thus, based on the experimental results and the comparisons, we propose wellfounded hypothesis in Section 3.4."
        },
        {
            "title": "3.4 Hypothesis",
            "content": "In real life, if the difficulty of an exam is much lower than that of students, then students grades will be chaotic, mainly manifested in the confusion of good students and bad students grades. In the field of visual token compression, there is general reliance on existing benchmarks, without 4 Table 2: Comparison of advanced token compression methods and downsampling on Qwen2-VL-7B by groups. Method Group + FastV + VisionZip + PruMerge+ + DART + Downsample Group + FastV + VisionZip + PruMerge+ + DART + Downsample Group + FastV + VisionZip + PruMerge+ + DART + Downsample Group + FastV + VisionZip + PruMerge+ + DART + Downsample Group + FastV + VisionZip + PruMerge+ + DART + Downsample Group + FastV + VisionZip + PruMerge+ + DART + Downsample GQA MMB MMBCN MME POPE MMStar OCRBench ChartQA Average 87.6 91.2 91.9 88.1 100.0 57.8 59.3 57.7 58.9 0. 82.5 83.4 85.8 81.2 100.0 44.5 49.4 50.4 47.5 0.0 81.4 79.0 76.7 78.8 100.0 35.7 41.0 43.0 41.9 0.0 95.9 93.8 95.1 94.9 100.0 45.2 42.4 51.2 54.8 0. 90.3 89.0 87.2 87.7 100.0 39.2 33.2 36.9 40.5 0.0 85.7 81.9 76.9 81.8 100.0 31.9 34.5 29.6 33.8 0.0 Token Reduction ( 75.00%) 96.7 95.3 95.9 94.9 100. 94.8 96.8 97.5 94.5 100.0 76.0 81.4 82.3 77.7 100.0 Token Reduction ( 75.00%) 78.9 54.9 62.0 67.6 0.0 65.4 72.5 72.1 69.4 0.0 41.0 45.9 48.1 47.0 0. Token Reduction ( 88.89%) 94.0 92.2 91.9 91.7 100.0 88.7 92.3 94.2 90.9 100.0 73.0 73.0 71.6 70.0 100.0 Token Reduction ( 88.89%) 59.4 48.1 42.9 49.6 0. 46.8 70.0 71.5 57.7 0.0 31.0 30.3 28.8 35.4 0.0 Token Reduction ( 93.75%) 91.5 88.4 87.8 88.9 100.0 88.1 89.4 87.6 88.5 100.0 74.5 69.8 65.5 61.8 100. Token Reduction ( 93.75%) 48.8 43.5 43.0 46.9 0.0 37.4 66.3 67.7 57.0 0.0 22.8 24.3 25.5 26.2 0.0 95.8 93.6 94.6 94.6 100.0 56.5 42.2 52.6 52.2 0. 90.8 88.1 86.4 86.9 100.0 44.1 44.4 38.4 40.9 0.0 86.6 82.2 76.1 80.4 100.0 35.3 33.3 34.1 38.4 0.0 57.2 58.1 46.2 70.2 100.0 29.1 29.6 21.2 40.2 0. 41.3 36.4 33.0 63.2 100.0 17.8 22.0 18.1 31.5 0.0 33.0 25.2 21.8 57.4 100.0 13.3 14.0 12.6 25.6 0.0 78.1 87.3 73.6 69.0 100.0 35.0 51.2 40.5 39.0 0. 61.7 74.4 73.8 57.6 100.0 28.4 49.7 43.5 27.3 0.0 74.8 71.3 68.9 67.1 100.0 14.8 26.1 29.4 14.5 0.0 85.3 87.2 84.6 85.5 100.0 51.1 49.8 50.7 53.6 0. 77.8 78.6 78.0 78.6 100.0 38.9 43.4 41.3 41.3 0.0 77.0 73.4 70.2 75.6 100.0 30.0 35.4 35.6 35.5 0.0 ever considering whether these data are suitable for the visual token compression task. Thus, we propose bold hypothesis: Some data in the existing benchmarks is overly simplistic, leading to the unreasonable phenomenon that even the simplest downsampling method is sufficient to deal with the visual token compression task. To validate this hypothesis, we design datacentric analysis using downsampling as discriminator. We first drop out the samples answered incorrectly at the original resolution, which we consider are too hard for the original models to understand, not to mention the compressed models. Then, for given compression ratio, we classify each sample in benchmark into one of two groups based on the performance of the downsampling method: ① Difficult Samples (Group A): Samples that are answered incorrectly by the downsampling method. ② Simple Samples (Group B): Samples that are answered correctly by the downsampling method. We then evaluate all compression methods on these two groups separately to assess whether the sophisticated methods demonstrate their expected superiority on the difficult samples where image downsampling fails. Results from Table 2 and Figure 2 strongly confirm our hypothesis, followed by two key conclusions as follows. ① Significant performance gap between groups: Across all benchmarks and compression methods, the accuracy on simple samples (Group B) is dramatically higher than on difficult samples (Group A). For instance, on GQA at 75% compression, the accuracy of all methods on simple samples is above 87.6%, while on difficult samples, it drops to maximum of 59.3% (VisionZip). This stark contrast is common in Table 2, demonstrating that the two groups represent essentially different levels of visual comprehension difficulty. The existence of this gap validates our core hypothesis that the benchmark comprises mixture of simple and difficult samples. In other words, the current benchmarks are noisy for evaluating the 5 visual token compression methods. Moreover, the significant gap also proves that downsampling can serve as clever filter to distinguish between \"simple\" and \"difficult\" samples, which can be the key to denoise the current benchmarks. ② Ideal reference points brought by downsampling: The 0%/100% dichotomy created by image downsampling provides ideal reference points for evaluation. In Group B, where downsampling achieves 100% accuracy, advanced methods show comparable but not superior performance (e.g., 87.6-91.9% on GQA at 75% compression), confirming that their sophisticated approaches offer no advantage for simple samples. In Group A, where downsampling fails completely (0% accuracy), advanced methods demonstrate their true value by significantly exceeding this baseline. For instance, DART achieves 40.2% on OCRBench and VisionZip reaches 51.2% on ChartQA at 75% compression, proving their ability to preserve crucial details that downsampling loses."
        },
        {
            "title": "3.5 Summary",
            "content": "In this section, we conduct two comprehensive experiments to further understand the anomalous phenomenon: image downsampling exceeds other sophisticated methods under some settings. The first experiment validates the universality of this anomalous phenomenon and introduces our basic hypothesis: Some data in the existing benchmarks is overly simplistic, leading to the unreasonable phenomenon that even the simplest downsampling method is sufficient to deal with the visual token compression task. Furthermore, the second experiment further validates this hypothesis and proves that the current benchmarks are noisy for evaluating the visual token compression methods. Moreover, the second experiment simultaneously demonstrates that downsampling can serve as clever filter to distinguish between simple and difficult samples, which can be the key to denoise the current benchmarks."
        },
        {
            "title": "4.1 Framework Construction",
            "content": "To address the simplicity bias and denoise existing benchmarks for the visual token compression task, we propose the VTC-Bench (Visual Token Compression Benchmark) framework, novel framework specifically designed for the fair and effective evaluation of visual token compression methods. The construction is based on the key insightvalidated in Section 3.4that \"downsampling can serve as clever filter to distinguish between simple and difficult samples\". We leverage this idea to construct challenging benchmark comprising predominantly difficult samples that require fine-grained visual understanding. This process, summarized in Figure 3, does not create new data but rather applies rigorous filtering mechanism to existing benchmarks to identify challenging evaluation and noise-free set. The pipeline consists of three critical steps executed for each candidate sample and dynamically adapts to different compression ratios: Step 1: Inference & Compression. Given sample and target token compression ratio, we run two inference pipelines: ① downsampling baseline (the filter) including one model that applies the equivalent ratio from Eq. 1 for fair comparison and another original model without downsampling, implemented with Qwen2-VL which has similar number of parameter with the target MLLM; and ② advanced visual token compression methods (e.g., FastV, VisionZip, DART) evaluated directly on the target MLLM. This step both establishes fair basis for assessing compression approaches and provides signals for subsequent sample filtering. Step 2: Grouping: We first drop out the samples that are incorrectly answered by the original Qwen2-VL. Then, we use the performance of the downsampling method as binary discriminator to categorize the sample into two groups: ① Group A: Samples considered as \"difficult\", which are incorrectly answered by the downsampling method. ② Group B: Samples considered as \"simple\", which are correctly answered by the downsampling method. This step effectively tags each sample with the labels of \"simple\" or \"difficult\", filtering the existing benchmarks and removing noisy data that is not applicable for evaluating the visual token compression methods. Step 3: Result Aggregation: Based on the classification in Step 2 and the inference results of visual token compression methods in Step 1, we perform statistical analysis on the accuracy of the \"difficult\" samples in the methods to be evaluated. Thus, an indicator that can truly reflect the visual compression methods fairly can be obtained. In summary, we develop VTC-Bench, simple but effective framework for evaluating visual token 6 Figure 3: The VTC-Bench is simple but effective framework that can transform any existing benchmarks to subset that can fairly evaluate VTC (Visual Token Compression) methods. The samples that are answered correctly by the original Qwen2-VL model without downsampling form the input samples. More details in Sec. 4.1. Table 3: VTC-Bench results on Qwen2-VL-7B. Method GQA MMB MMBCN MME POPE MMStar OCRBench ChartQA Average Qwen-VL-7B + FastV + VisionZip + PruMerge+ + DART + Downsample Qwen-VL-7B + FastV + VisionZip + PruMerge+ + DART + Downsample Qwen-VL-7B + FastV + VisionZip + PruMerge+ + DART + Downsample Qwen-VL-7B + FastV + VisionZip + PruMerge+ + DART + Downsample Qwen-VL-7B + FastV + VisionZip + PruMerge+ + DART + Downsample 57.8 59.3 57.7 58.9 0.0 44.5 49.4 50.4 47.5 0.0 35.7 41.0 43.0 41.9 0.0 29.6 38.6 38.8 36.4 0. 18.3 23.4 20.7 24.5 0.0 45.2 42.4 51.2 54.8 0.0 39.2 33.2 36.9 40.5 0.0 31.9 34.5 29.6 33.8 0.0 24.7 31.2 26.0 33.7 0.0 18.3 28.8 17.8 26.5 0. Token Reduction ( 75.00%) 78.9 54.9 62.0 67.6 0.0 65.4 72.5 72.1 69.4 0.0 41.0 45.9 48.1 47.0 0.0 Token Reduction ( 88.89%) 59.4 48.1 42.9 49.6 0. 46.8 70.0 71.5 57.7 0.0 31.0 30.3 28.8 35.4 0.0 Token Reduction ( 93.75%) 48.8 43.5 43.0 46.9 0.0 37.4 66.3 67.7 57.0 0.0 22.8 24.3 25.5 26.2 0. Token Reduction ( 96.00%) 35.6 37.9 37.0 37.9 0.0 35.6 60.0 56.4 53.2 0.0 21.5 24.5 22.6 22.3 0.0 Token Reduction ( 99.00%) 21.5 28.5 22.9 30.6 0. 44.3 53.6 52.6 41.5 0.0 15.0 19.4 17.1 19.2 0.0 56.5 42.2 52.6 52.2 0.0 44.1 44.4 38.4 40.9 0.0 35.3 33.3 34.1 38.4 0.0 33.1 32.6 29.3 36.4 0. 21.5 32.2 21.1 28.1 0.0 29.1 29.6 21.2 40.2 0.0 17.8 22.0 18.1 31.5 0.0 13.3 14.0 12.6 25.6 0.0 11.0 11.0 9.2 24.7 0.0 4.2 3.7 2.5 25.6 0. 35.0 51.2 40.5 39.0 0.0 28.4 49.7 43.5 27.3 0.0 14.8 26.1 29.4 14.5 0.0 9.3 15.1 17.0 10.5 0.0 3.8 5.5 7.1 4.2 0.0 51.1 49.8 50.7 53.6 0. 38.9 43.4 41.3 41.3 0.0 30.0 35.4 35.6 35.5 0.0 25.0 31.4 29.5 31.9 0.0 18.4 24.4 20.2 25.0 0.0 compression methods. The building pipeline of VTC-Bench is shown in Figure 4. Importantly, the VTC-Bench framework can be applied easily to any existing benchmark, transforming it into more effective benchmark for evaluating visual token compression methods. Meanwhile, the VTC-Bench framework dynamically and reasonably provides corresponding benchmark subset for each compression ratio, while offering explainable theoretical upper and lower bounds for the final metrics. 7 Table 4: VTC-Bench results on LLaVA-OV-7B. Method GQA MMB MMBCN POPE MMStar LLaVA-OV-7B + FastV + VisionZip + PruMerge+ + Downsample LLaVA-OV-7B + FastV + VisionZip + PruMerge+ + Downsample LLaVA-OV-7B + FastV + VisionZip + PruMerge+ + Downsample LLaVA-OV-7B + FastV + VisionZip + PruMerge+ + Downsample LLaVA-OV-7B + FastV + VisionZip + PruMerge+ + Downsample 54.3 59.0 60.4 0.0 45.3 56.6 57.4 0.0 36.7 49.1 50.2 0.0 31.4 42.6 42.7 0. 25.7 28.3 25.3 0.0 Token Reduction ( 75.00%) 70.5 67.7 74.2 0.0 69.1 71.3 73.5 0.0 63.8 80.8 75.6 0.0 Token Reduction ( 88.89%) 64.6 71.9 68.8 0.0 66.4 71.2 71.5 0.0 39.1 69.6 76.0 0.0 Token Reduction ( 93.75%) 51.2 64.3 66.6 0.0 53.3 62.4 65.3 0. 29.7 53.6 59.9 0.0 Token Reduction ( 96.00%) 37.4 55.4 57.8 0.0 43.1 56.9 59.6 0.0 24.5 45.4 49.9 0.0 Token Reduction ( 99.00%) 25.8 28.1 25.5 0.0 29.6 32.8 28.5 0.0 39.3 42.1 40.4 0.0 48.6 44.8 48.6 0.0 42.4 43.5 45.8 0.0 32.6 36.6 34.8 0. 28.6 30.8 31.3 0.0 21.9 24.7 25.2 0."
        },
        {
            "title": "4.2 Evaluation Results & Discussions",
            "content": "We conduct extensive experiments across multiple mainstream MLLMs and benchmarks based on VTC-Bench. We select Qwen2-VL-7B (Wang et al., 2024) and LLaVA-OV-7B (Li et al., 2024a) as the base MLLMs and evaluate various visual token compression methods (including FastV, VisionZip, PruMerge+, DART) on subset of \"difficult samples\" filtered by VTC-Bench. The experimental results are shown in Table 3 and 4 and Figure 4, followed by several analysis: Is downsampling all you need? Across many benchmarks, simple image downsampling often beats more advanced compression methods, suggesting that sophisticated approaches are unnecessary. VTC-Bench overturns this impression: when we restrict evaluation to the compression-relevant difficult samples (Group A), the trend reverses. The apparent superiority of downsampling largely stems from original benchmarks being saturated with easy cases that do not require fine-grained cues. By filtering out such samples, VTC-Bench reveals that for truly challenging instancesthose that test visual understandingadvanced compression methods are not only effective but necessary. What makes an effective benchmark? Simple cross-benchmark comparisons (e.g., \"Benchmark outperforms Benchmark B\") only imply that one is harder, without revealing which skills drive the Figure 4: VTC-Bench results on Qwen2-VL-7B. difficulty or whether it is relevant to visual token compression. VTC-Bench addresses this by filtering out samples that do not inform compression performance, yielding an analysis set that is explicitly sensitive to token compression. This suggests design principle for future work: effective benchmarks for visual token compression should deliberately increase the share of compression-relevant hard cases. Further Expand the Accuracy Gap: VTC-Bench amplifies and clarifies method differences. At 75% compression on ChartQA, the VisionZipFastV gap widens from 8.8% to 16.2%; at 96% compression on GQA, it grows from 0.3% to 9.0%. These phenomenon effectively indicates that VTC-Bench indeed eliminates data noise unrelated to the visual token compression task, thereby promoting the fairness and effectiveness of the benchmark in the visual token compression task."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper systematically analyzes the task mismatch problem presented in current MLLM benchmarks when evaluating visual token compression methods. Based on surprising and counterintuitive finding: simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks, we conduct comprehensive empirical study across several advanced visual token compression methods. Thus, two crucial findings are concluded based on the empirical study: ① Current benchmarks are noisy for the visual token compression task. ② Downsampling can serve as data filter to evaluate the difficulty of samples upon the visual token compression task. Furthermore, we propose VTCBench, new evaluation framework specifically designed to optimize and denoise current existing benchmarks by explicitly distinguishing between 8 simple and difficult samples through downsampling. Through this work, we hope to not only advance the field of visual token compression but also inspire more discussions within the community on \"how to properly evaluate efficient MLLMs.\""
        },
        {
            "title": "6 Limitations",
            "content": "① Relying on downsampling as filter: If downsampling itself performs poorly on certain tasks, it may result in an insufficient number of \"difficult samples\" being selected. ② Not considering model differences: Different MLLMs have varying sensitivities to image resolution and visual details, which may affect the generalizability of sample grouping."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Junjie Chen, Xuyang Liu, Zichen Wen, Yiyu Wang, Siteng Huang, and Honggang Chen. 2025. Variationaware vision token dropping for faster large visionlanguage models. arXiv preprint arXiv:2509.01552. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. 2024a. An image is worth 1/2 tokens after layer 2: Plug-andplay inference acceleration for large vision-language models. In European Conference on Computer Vision, pages 1935. Springer. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and 1 others. 2024b. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330. Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, and 1 others. 2024c. Mj-bench: Is your multimodal reward model really good judge for text-to-image generation? arXiv preprint arXiv:2407.04842. Yuhang Han, Xuyang Liu, Zihan Zhang, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang. 2024. Filter, correlate, compress: Training-free token reduction for mllm acceleration. arXiv preprint arXiv:2411.17686. Drew Hudson and Christopher Manning. 2019. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709. Yutao Jiang, Qiong Wu, Wenhao Lin, Wei Yu, and Yiyi Zhou. 2025. What kind of visual tokens do we need? training-free visual token pruning for multi-modal large language models from the perspective of graph. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 40754083. Hengrui Kang, Siwei Wen, Zichen Wen, Junyan Ye, Weijia Li, Peilin Feng, Baichuan Zhou, Bin Wang, Dahua Lin, Linfeng Zhang, and 1 others. 2025. Legion: Learning to ground and explain for synthetic image detection. arXiv preprint arXiv:2503.15264. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and 1 others. 2024a. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. 2024b. Seedbench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308. Duo Li, Zuhao Yang, and Shijian Lu. 2025. Todre: Visual token pruning via diversity and task awareness for efficient large vision-language models. arXiv preprint arXiv:2505.18757. Lin Li, Guikun Chen, Hanrong Shi, Jun Xiao, and Long Chen. 2024c. survey on multimodal benchmarks: In the era of large ai models. arXiv preprint arXiv:2409.18142. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355. Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, and 1 others. 2025. Embodiedeval: Evaluate multimodal llms as embodied agents. arXiv preprint arXiv:2501.11858. Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. 2024a. Multistage vision token dropping: Towards efficient mularXiv preprint timodal large language model. arXiv:2411.10803. Yuqian Fu, Runze Wang, Yanwei Fu, Danda Pani Paudel, Xuanjing Huang, and Luc Van Gool. 2025. Objectrelator: Enabling cross-view object relation understanding in ego-centric and exo-centric videos. ICCV. Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, and 1 others. 2025. Shifting ai efficiency from model-centric to data-centric compression. arXiv preprint arXiv:2505.19147. 9 Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, and 1 others. 2024b. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, ChengLin Liu, Lianwen Jin, and Xiang Bai. 2024c. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209. Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, and Pengfeng Xiao. 2024. Lhrs-bot: Empowering remote sensing with vgi-enhanced large multimodal language model. In European Conference on Computer Vision, pages 440457. Springer. Ruiguang Pei, Weiqing Sun, Zhihui Fu, and Jun Wang. 2025. Greedyprune: Retenting critical visual token set for large vision language models. arXiv preprint arXiv:2506.13166. Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. 2024. Nuscenes-qa: multimodal visual question answering benchmark for In Proceedings of autonomous driving scenario. the AAAI Conference on Artificial Intelligence, volume 38, pages 45424550. Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. 2024. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388. Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li. 2024. Drivelm: Driving with graph visual question answering. In European conference on computer vision, pages 256274. Springer. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, and 1 others. 2024. Qwen2vl: Enhancing vision-language models perception arXiv preprint of the world at any resolution. arXiv:2409.12191. Zekun Wang, Minghua Ma, Zexin Wang, Rongchuan Mu, Liping Shan, Ming Liu, and Bing Qin. 2025. Effivlm-bench: comprehensive benchmark for evaluating training-free acceleration in large visionlanguage models. arXiv preprint arXiv:2506.00479. Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, and Linfeng Zhang. 2025a. Token pruning in multimodal large language models: Are we solving the right problem? arXiv preprint arXiv:2502.11501. Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, and Linfeng Zhang. 2025b. Stop looking for important tokens in multimodal language models: Duplication matters more. arXiv preprint arXiv:2502.11494. Zichen Wen, Shaobo Wang, Yufa Zhou, Junyuan Zhang, Qintong Zhang, Yifeng Gao, Zhaorun Chen, Bin Wang, Weijia Li, Conghui He, and 1 others. 2025c. Efficient multi-modal large language models via progressive consistency distillation. arXiv preprint arXiv:2510.00515. Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, and 1 others. 2024. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247. Minhao Xiong, Zichen Wen, Zhuangcheng Gu, Xuyang Liu, Rui Zhang, Hengrui Kang, Jiabing Yang, Junyuan Zhang, Weijia Li, Conghui He, and 1 others. 2025. Prune2drive: plug-and-play framework for accelerating vision-language models in autonomous driving. arXiv preprint arXiv:2508.13305. Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. 2025a. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1979219802. Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, and Jiaya Jia. 2025b. Visionthink: Smart and efficient vision language model via reinforcement learning. arXiv preprint arXiv:2507.13348. Yantai Yang, Yuhao Wang, Zichen Wen, Luo Zhongwei, Chang Zou, Zhipeng Zhang, Chuan Wen, and Linfeng Zhang. 2025c. Efficientvla: Training-free acceleration and compression for vision-languageaction models. arXiv preprint arXiv:2506.10100. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2024. survey on multimodal large language models. National Science Review, 11(12):nwae403. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490. Weihao Yu, Zhengyuan Yang, Lingfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. 2024. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. arXiv preprint arXiv:2408.00765. nitive abilities of MLLMs. It encompasses 14 subtasks across the domains of visual perception, text understanding, reasoning, and cross-modal alignment. B.4 POPE POPE (Li et al., 2023) is benchmark designed to evaluate object hallucination in MLLMs. The pipeline of POPE measures hallucination under random, popular, and adversarial sampling strategies. B.5 MMStar MMStar (Chen et al., 2024b) is vision-dependent benchmark for evaluating the reasoning and perception abilities. It has 1500 samples, covering six core abilities with 18 sub-dimensions. B.6 OCRBench OCRBench (Liu et al., 2024c) is comprehensive benchmark for evaluating the OCR capabilities of multimodal large models. The benchmark includes 1,000 manually verified samples from 29 datasets. B.7 ChartQA ChartQA (Masry et al., 2022) evaluates visual and logical reasoning over real-world charts. It includes 9.6k human-written and 23.1k automatically generated questions across different kinds of charts. Complete VTC-Bench Results Due to the page limitation, we are unable to offer complete results in the experiment sections. Thus, we provide the evaluation results by group of Qwen2-VL-7B and LLaVA-OV-7B on eight benchmarks here, as shown in Table 5 and 6. Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, and 1 others. 2025. Sparsevlm: Visual token sparsification for efficient vision-language model inference. In Forty-second International Conference on Machine Learning. Xin Zou, Di Lu, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Xu Zheng, Linfeng Zhang, and Xuming Hu. 2025. Dont just chase \"highlighted tokens\" in mllms: Revisiting visual holistic context retention. arXiv preprint arXiv:2510.02912."
        },
        {
            "title": "A Experiment Details",
            "content": "In this paper, all the experiments are conducted based on one A800 GPU. For the downsampling method and DART, we apply the official code of DART (Wen et al., 2025b). As to the downsampling method, we resize the image before it enters the MLLM. As to DART, we control the compression ratio through the parameter \"Reduction_Ratio\". For VisionZip, PruMerge+, and FastV, we apply the EffiVLM-Bench (Wang et al., 2025), which offers unified toolkit to evaluate efficient MLLM. As to these three methods, we control the compression ratio through the parameter \"Budget\". Considering this paper focuses on the evaluation, it is not related to hyperparameter search. All results come from single run. The code environment includes Python=3.10, torch=2.6.0, torchvision=0.21.0, and torchaudio=2.6.0. We will release all the results, including the output results of each sample and the accuracy results of each benchmark."
        },
        {
            "title": "B Benchmark Details",
            "content": "B.1 GQA GQA (Hudson and Manning, 2019) is large-scale benchmark for visual reasoning and compositional question answering. Based on strict distribution control, GQA offers 22M valuable reasoning questions. B.2 MMBench MMBench (Liu et al., 2024b) is comprehensive benchmark designed to evaluate the capabilities of MLLMs. It includes 3,217 multiple-choice questions spanning 20 fine-grained dimensions, supporting several languages such as Chinese and English. B.3 MME MME (Yin et al., 2024) provides systematic framework for evaluating the perceptual and cog11 Table 5: Comparison of Advanced Token Compression Methods and Downsampling on Qwen2-VL-7B."
        },
        {
            "title": "Group B",
            "content": "+ FastV + VisionZip + PruneMerge+ + DART + Downsample"
        },
        {
            "title": "Group A",
            "content": "+ FastV + VisionZip + PruneMerge+ + DART + Downsample"
        },
        {
            "title": "Group B",
            "content": "+ FastV + VisionZip + PruneMerge+ + DART + Downsample"
        },
        {
            "title": "Group A",
            "content": "+ FastV + VisionZip + PruneMerge+ + DART + Downsample"
        },
        {
            "title": "Group B",
            "content": "+ FastV + VisionZip + PruneMerge+ + DART + Downsample"
        },
        {
            "title": "Group A",
            "content": "+ FastV + VisionZip + PruneMerge+ + DART + Downsample"
        },
        {
            "title": "Group B",
            "content": "+ FastV + VisionZip + PruneMerge+ + DART + Downsample"
        },
        {
            "title": "Group A",
            "content": "+ FastV + VisionZip + PruneMerge+ + DART + Downsample"
        },
        {
            "title": "Group B",
            "content": "+ FastV + VisionZip + PruneMerge+ + DART + Downsample"
        },
        {
            "title": "Group A",
            "content": "+ FastV + VisionZip + PruneMerge+ + DART + Downsample"
        },
        {
            "title": "GQA MMB MMBCN MME POPE MMStar OCRBench ChartQA Average",
            "content": "87.6 91.2 91.9 88.1 100.0 95.9 93.8 95.1 94.9 100.0 57.8 59.3 57.7 58.9 0.0 45.2 42.4 51.2 54.8 0.0 82.5 83.4 85.8 81.2 100.0 90.3 89.0 87.2 87.7 100. 44.5 49.4 50.4 47.5 0.0 39.2 33.2 36.9 40.5 0.0 81.4 79.0 76.7 78.8 100.0 85.7 81.9 76.9 81.8 100.0 35.7 41.0 43.0 41.9 0.0 31.9 34.5 29.6 33.8 0. 79.2 75.6 72.5 74.5 100.0 74.5 80.3 69.3 77.3 100.0 29.6 38.6 38.8 36.4 0.0 24.7 31.2 26.0 33.7 0.0 75.5 79.4 73.9 76.1 100.0 55.6 78.3 55.9 63.2 100. 18.3 23.4 20.7 24.5 0.0 18.3 28.8 17.8 26.5 0.0 95.8 93.6 94.6 94.6 100.0 56.5 42.2 52.6 52.2 0.0 90.8 88.1 86.4 86.9 100.0 44.1 44.4 38.4 40.9 0. 86.6 82.2 76.1 80.4 100.0 35.3 33.3 34.1 38.4 0.0 74.4 80.2 69.1 75.1 100.0 33.1 32.6 29.3 36.4 0.0 53.2 76.9 52.6 59.0 100.0 21.5 32.2 21.1 28.1 0. Token Reduction ( 75.00%) 96.7 95.3 95.9 94.9 100.0 94.8 96.8 97.5 94.5 100.0 76.0 81.4 82.3 77.7 100.0 Token Reduction ( 75.00%) 78.9 54.9 62.0 67.6 0. 65.4 72.5 72.1 69.4 0.0 41.0 45.9 48.1 47.0 0.0 Token Reduction ( 88.89%) 94.0 92.2 91.9 91.7 100.0 88.7 92.3 94.2 90.9 100.0 73.0 73.0 71.6 70.0 100. Token Reduction ( 88.89%) 59.4 48.1 42.9 49.6 0.0 46.8 70.0 71.5 57.7 0.0 31.0 30.3 28.8 35.4 0.0 Token Reduction ( 93.75%) 91.5 88.4 87.8 88.9 100. 88.1 89.4 87.6 88.5 100.0 74.5 69.8 65.5 61.8 100.0 Token Reduction ( 93.75%) 48.8 43.5 43.0 46.9 0.0 37.4 66.3 67.7 57.0 0.0 22.8 24.3 25.5 26.2 0. Token Reduction ( 96.00%) 90.1 87.4 83.9 84.9 100.0 85.8 86.5 82.2 84.3 100.0 68.6 69.6 61.4 63.5 100.0 Token Reduction ( 96.00%) 35.6 37.9 37.0 37.9 0. 35.6 60.0 56.4 53.2 0.0 21.5 24.5 22.6 22.3 0.0 Token Reduction ( 99.00%) 75.3 80.5 73.9 73.2 100.0 59.3 70.2 49.7 67.4 100.0 61.0 69.4 57.0 63.7 100. Token Reduction ( 99.00%) 21.5 28.5 22.9 30.6 0.0 44.3 53.6 52.6 41.5 0.0 15.0 19.4 17.1 19.2 0.0 12 57.2 58.1 46.2 70.2 100. 29.1 29.6 21.2 40.2 0.0 41.3 36.4 33.0 63.2 100.0 17.8 22.0 18.1 31.5 0.0 33.0 25.2 21.8 57.4 100.0 13.3 14.0 12.6 25.6 0.0 27.5 20.4 18.4 53.7 100. 11.0 11.0 9.2 24.7 0.0 19.5 17.1 13.0 43.1 100.0 4.2 3.7 2.5 25.6 0.0 78.1 87.3 73.6 69.0 100.0 35.0 51.2 40.5 39.0 0.0 61.7 74.4 73.8 57.6 100. 28.4 49.7 43.5 27.3 0.0 74.8 71.3 68.9 67.1 100.0 14.8 26.1 29.4 14.5 0.0 75.3 69.2 70.3 71.1 100.0 9.3 15.1 17.0 10.5 0.0 73.3 70.3 69.5 70.7 100. 3.8 5.5 7.1 4.2 0.0 85.3 87.2 84.6 85.5 100.0 51.1 49.8 50.7 53.6 0.0 77.8 78.6 78.0 78.6 100.0 38.9 43.4 41.3 41.3 0.0 77.0 73.4 70.2 75.6 100. 30.0 35.4 35.6 35.5 0.0 71.9 71.2 65.9 73.1 100.0 25.0 31.4 29.5 31.9 0.0 59.1 67.8 55.7 64.6 100.0 18.4 24.4 20.2 25.0 0.0 Table 6: Comparison of Advanced Token Compression Methods and Downsampling on LLaVA-ov-7B Method Group GQA MMB MMBCN POPE MMStar Average Token Reduction ( 75.00%) + FastV + VisionZip + PruneMerge+ + Downsample 84.0 86.2 87.1 100.0 93.5 93.4 93.8 100.0 94.7 94.2 94.0 100.0 92.2 95.6 96.3 100.0 73.1 62.9 62.1 100. Group + FastV + VisionZip + PruneMerge+ + Downsample Group Token Reduction ( 75.00%) 54.3 59.0 60.4 0.0 70.5 67.7 74.2 0. 69.1 71.3 73.5 0.0 63.8 80.8 75.6 0.0 48.6 44.8 48.6 0.0 Token Reduction ( 88.89%) + FastV + VisionZip + PruneMerge+ + Downsample 76.1 82.8 82.9 100. 91.7 92.5 92.8 100.0 92.3 92.1 93.2 100.0 85.2 93.6 93.8 100.0 66.5 59.1 54.9 100.0 Group + FastV + VisionZip + PruneMerge+ + Downsample Group Token Reduction ( 88.89%) 45.3 56.6 57.4 0.0 64.6 71.9 68.8 0.0 66.4 71.2 71.5 0.0 39.1 69.6 76.0 0. 42.4 43.5 45.8 0.0 Token Reduction ( 93.75%) + FastV + VisionZip + PruneMerge+ + Downsample 73.0 79.4 78.7 100.0 85.8 91.2 91.1 100.0 86.5 90.9 91.0 100. 81.5 90.9 91.0 100.0 64.3 54.6 53.6 100.0 Group + FastV + VisionZip + PruneMerge+ + Downsample Group Token Reduction ( 93.75%) 36.7 49.1 50.2 0.0 51.2 64.3 66.6 0.0 53.3 62.4 65.3 0.0 29.7 53.6 59.9 0.0 32.6 36.6 34.8 0.0 Token Reduction ( 96.00%) + FastV + VisionZip + PruneMerge+ + Downsample 71.9 76.3 74.5 100.0 77.3 86.9 84.1 100.0 77.9 86.8 84.2 100.0 79.0 86.6 85.9 100.0 57.1 49.9 49.7 100. Group + FastV + VisionZip + PruneMerge+ + Downsample Group Token Reduction ( 96.00%) 31.4 42.6 42.7 0.0 37.4 55.4 57.8 0. 43.1 56.9 59.6 0.0 24.5 45.4 49.9 0.0 28.6 30.8 31.3 0.0 Token Reduction ( 99.00%) + FastV + VisionZip + PruneMerge+ + Downsample 63.0 64.4 60.4 100. 50.6 54.5 46.9 100.0 46.5 53.8 45.5 100.0 59.0 61.8 56.7 100.0 47.4 36.0 32.6 100.0 Group + FastV + VisionZip + PruneMerge+ + Downsample Token Reduction ( 99.00%) 25.7 28.3 25.3 0.0 25.8 28.1 25.5 0.0 29.6 32.8 28.5 0.0 39.3 42.1 40.4 0.0 21.9 24.7 25.2 0. 87.5 86.5 86.7 100.0 61.3 64.7 66.5 0.0 82.4 84.0 83.5 100.0 51.6 62.6 63.9 0.0 78.2 81.4 81.1 100.0 40.7 53.2 55.4 0. 72.6 77.3 75.7 100.0 33.0 46.2 48.3 0.0 53.3 54.1 48.4 100.0 28.5 31.2 29.0 0."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "INSAIT, Sofia University St. Kliment Ohridski",
        "Northeastern University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "University of Pisa",
        "University of Trento"
    ]
}