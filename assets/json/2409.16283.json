{
    "paper_title": "Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation",
    "authors": [
        "Homanga Bharadhwaj",
        "Debidatta Dwibedi",
        "Abhinav Gupta",
        "Shubham Tulsiani",
        "Carl Doersch",
        "Ted Xiao",
        "Dhruv Shah",
        "Fei Xia",
        "Dorsa Sadigh",
        "Sean Kirmani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "How can robot manipulation policies generalize to novel tasks involving unseen object types and new motions? In this paper, we provide a solution in terms of predicting motion information from web data through human video generation and conditioning a robot policy on the generated video. Instead of attempting to scale robot data collection which is expensive, we show how we can leverage video generation models trained on easily available web data, for enabling generalization. Our approach Gen2Act casts language-conditioned manipulation as zero-shot human video generation followed by execution with a single policy conditioned on the generated video. To train the policy, we use an order of magnitude less robot interaction data compared to what the video prediction model was trained on. Gen2Act doesn't require fine-tuning the video model at all and we directly use a pre-trained model for generating human videos. Our results on diverse real-world scenarios show how Gen2Act enables manipulating unseen object types and performing novel motions for tasks not present in the robot data. Videos are at https://homangab.github.io/gen2act/"
        },
        {
            "title": "Start",
            "content": "Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation Homanga Bharadhwaj1,2, Debidatta Dwibedi1, Abhinav Gupta2, Shubham Tulsiani2, Carl Doersch1, Ted Xiao1, Dhruv Shah1, Fei Xia1, Dorsa Sadigh1,3, Sean Kirmani1 4 2 0 2 4 2 ] . [ 1 3 8 2 6 1 . 9 0 4 2 : r Abstract How can robot manipulation policies generalize to novel tasks involving unseen object types and new motions? In this paper, we provide solution in terms of predicting motion information from web data through human video generation and conditioning robot policy on the generated video. Instead of attempting to scale robot data collection which is expensive, we show how we can leverage video generation models trained on easily available web data, for enabling generalization. Our approach Gen2Act casts language-conditioned manipulation as zero-shot human video generation followed by execution with single policy conditioned on the generated video. To train the policy, we use an order of magnitude less robot interaction data compared to what the video prediction model was trained on. Gen2Act doesnt require fine-tuning the video model at all and we directly use pre-trained model for generating human videos. Our results on diverse real-world scenarios show how Gen2Act enables manipulating unseen object types and performing novel motions for tasks not present in the robot data. https://homangab.github.io/gen2act/ I. INTRODUCTION it To realize the vision of robot manipulators helping us in the humdrum everyday activities of messy living rooms, offices, and kitchens, it is crucial to develop robot policies capable of generalizing to novel tasks in unseen scenarios. In order to be practically useful, is desirable to not require adapting the policy to new tasks through test-time optimizations and instead being able to directly execute task specification such as language it given colloquial instructions. Further, such policy should be able to tackle broad array of everyday tasks like manipulating articulated objects, pouring, re-orienting objects, wiping tables without the need to collect robot interaction data for every task unlike recent efforts on behavior cloning with robot datasets [14]. This is because collecting large robot datasets that cover the diversity of everyday scenarios is extremely challenging and might be deemed impractical. In order to mitigate issues with purely scaling robotic datasets, line of recent works have sought to incorporate additional behavioral priors in representation learning by pre-training visual encoders with non-robotic datasets [59] and co-training policies with vision-language models [10 12]. Going beyond abstract representations, other works have learned attributes from web videos more directly informative of motion in the form of predicting goal images [13 15], hand-object mask plans [16], and embodiment-agnostic 1 Google DeepMind 2 The Robotics Institute, Carnegie Mellon University 3 Computer Science Department, Stanford University correspondence to Homanga Bharadhwaj: hbharadh@cs.cmu.edu Fig. 1: Gen2Act learns to generate human video followed by robot policy execution conditioned on the generated video. This enables diverse realworld manipulation in unseen scenarios. point tracks [17]. These approaches show promising signs of generalization to tasks unseen in the robot interaction datasets, but training such specific predictive models from web video data requires utilizing other intermediate models for providing ground-truths and thus are hard to scale up. Our key insight for enabling generalization in manipulation is to cast motion prediction from web data in the very generic form of zero-shot video prediction. This lets us directly leverage advances in video generation models, by conditioning robot policy on the generated video for new tasks that are unseen in the robot datasets. We posit that as video generation models get better due to large interest in generative AI [1820] beyond robotics, an approach that relies on learning policy conditioned on zero-shot video prediction can effectively scale and generalize to increasingly diverse real-world scenarios. For performing manipulation task in novel scene, generated video conditioned on the language description of the task is particularly useful for Fig. 2: Architecture of the translation model of Gen2Act (closed-loop policy πθ). Given an image of scene I0 and language-goal description of the task G, we generate human video Vg with pre-trained video generation model V(I0, G). During training of the policy, we incorporate track prediction from the policy latents as an auxiliary loss in addition to behavior cloning loss. Dotted pathways show training-specific computations. During inference, we do not require track prediction and only use the video model in conjunction with the policy πθ(Itk:t, Vg). by We develop Gen2Act conveying what needs to be done and in capturing motioncentric information of how to perform the task that can then be converted to robot actions through learned policy. Compared to generated video, language description or goal image alone only conveys what the task is. instantiating languageconditioned manipulation as human video generation followed by generated human video to robot translation with closed-loop policy  (Fig. 1)  . We opt for generating human videos as opposed to directly generating robot videos since video generation models are often trained with human data on the web, and they are able to generate human videos zero-shot given new scene. We then train translation model that needs some offline robot demonstrations and corresponding generated human videos. We generate these corresponding human videos offline with an off-the-shelf model [20] by conditioning on the first frame of each trajectory (the first frame doesnt have the robot in the scene) and the language description of the task. We instantiate this translation model as closed loop policy that is conditioned on the history of robot observations in addition to the generated human video so that it can take advantage of the visual cues in the scene and adjust its behavior reactively. In order to capture motion information beyond that implicitly provided by visual features from the generated video, we extract point tracks from the generated human video and the video of robot observations (through an off-theshelf tracker [21]) and optimize track prediction auxiliary loss during training. The aim of this loss function is to ensure that the latent tokens of the closed-loop policy are informative of the motion of points in the scene. We train the policy to optimize the typical behavior cloning loss for action prediction combined with this track prediction loss. For deployment, give language description of task to be performed, we generate human video and run the policy conditioned on this video. diverse results of Gen2Act (featured in Fig. 1) demonstrate the broad generalization capabilities enabled by learning to infer manipulation real-world The track prediction for motion cues from web video data through zero-shot video generation combined with motion extraction through solving novel manipulation point tasks in unseen scenarios. For generalization to novel object types and novel motion types unseen in the robot interaction training data, we show that Gen2Act achieves on average 30% higher absolute success rate over the most competitive baseline. Further, we demonstrate how Gen2Act can be chained in sequence for performing long-horizon activities like making coffee consisting of several intermediate tasks. II. RELATED WORKS We discuss prior works in imitation learning with visual observations, learning representations from non-robotic datasets, and approaches for conditional behavior cloning. Visual Imitation. Visual imitation is scalable approach for robotic manipulation [2224] and end-to-end policy learning more broadly [25, 26]. While early works in multi-task imitation learning collected limited real-world data [27, 28], more recent approaches [1, 29, 30] collect much larger datasets. In fact, recent works that have attempted to directly scale this for training large models have required years of expensive data collection [1, 2, 10] and have still been restricted to limited generalization especially with respect to novel object types and novel motions in unseen scenarios. Visual Representations for Manipulation. To enable generalization, many recent works propose using pre-trained visual representations trained primarily on non-robot datasets [31, 32], for learning manipulation policies [5, 6, 69, 3336]. However, they are primarily limited to learning task-specific policies [5, 8, 37, 38] as they rely on access to lot of in-domain robot interaction data. Apart from training visual encoders, line of works augment existing robot datasets with semantic variations using generative models [2, 3942]. While this enables policies to generalize to unseen scenes and become robust to distractors, generalization to unseen object types and motion types still remains challenge. Conditional Behavior Cloning. Some prior works train robotic policies conditioned on human videos but require paired in-domain human-robot data [4348] and are not capable of leveraging web data. Others use curated data of human videos to leverage human hand motion information [49, 50] for learning task-specific policies (instead of single model across generic tasks). Towards learning structure more directly related to manipulation from web videos, some works try to predict visual affordances in the form of where to interact in an image, and local information of how to interact [5155]. While these could serve as good initializations for robotic policy, they are not sufficient on their own for accomplishing tasks, and so are typically used in conjunction with online learning, requiring several hours of deployment-time training and robot data [13, 53, 56]. Others learn to predict motion from web data more directly in the form of masks of hand and objects in the scene [16] and tracks of how arbitrary points in the scene should move [17], for conditional behavior cloning. However, training such predictive models from web videos requires reliance on intermediate models for providing ground-truth information and are thus hard to scale up broadly. III. APPROACH We develop language-conditioned robot manipulation system, Gen2Act that generalizes to novel tasks in unseen scenarios. To achieve this, we adopt factorized approach: 1) Given scene and task description, using an existing video prediction model generate video of human solving the task, 2) Conditioned on the generated human video infer robot actions through learned human-to-robot translation model that can take advantage of the motion cues in the generated video. We show that this factorized strategy is scalable in leveraging web-scale motion understanding inherent in large video models, for synthesizing how the manipulation should happen for novel task, and utilizing orders of magnitude less robot interaction data for the much simpler task of translation from generated human video to what actions the robot should execute. A. Overview and Setup Given scene specified by an image I0 and goal describing in text the task to be performed, we want robot manipulation system to execute actions a1:H for solving the task. To achieve this in unseen scenarios, we learn motion predictive information from web video data in the form of video prediction model V(I0, G) that zero-shot generates human video of the task, Vg. In order to translate this generated video to robot actions, we train closed-loop policy πθ(Itk:t, Vg) conditioned on the video and the last robot observations, through behavior cloning on small robot interaction dataset Dr. In order to implicitly encode motion information from Vg in the policy πθ, we extract point tracks from both Vg and Itk:t, respectively τg and τr, and incorporate track prediction as an auxiliary loss Lτ during training. Fig. 2 shows an overview of this setup. B. Human Video Generation We use an existing video generation model for the task of text+image conditioned video generation. We find that Fig. 3: Visualization of zero-shot video generation for different tasks. The blue frame and the language description are input to the video generation model of Gen2Act and the black frames show sub-sampled frames of the generated video. These results demonstrate the applicability of off-the-shelf video generation models for image+text conditioned video generation that preserves the scene and performs the desired manipulation task. current video generation models are good at generating human videos zero-shot without requiring any fine-tuning or adaptation (some examples in Fig. 3). Instead of trying to generate robot videos as done by some prior works [57, 58], we focus on just human video generation because current video generation models cannot generate robot videos zeroshot and require robot-specific fine-tuning data for achieving this. Such fine-tuning often subtracts the benefits of generalization to novel scenes that is inherent in video generation models trained on web-scale data. For training, given an offline dataset of robot trajectories Dr along with language task instructions G, we create corresponding generated human video dataset Dg by generating videos conditioned on the first frame of the robot trajectories and the language instruction. This procedure of generating paired datasets {Dr, Dg} is fully automatic and does not require manually collecting human videos as done by prior works [46, 59]. We do not require the generated human videos to have any particular structure apart from looking visually realistic, manipulating the relevant objects plausibly, and having minimal camera motion. As seen in the qualitative results in Fig. 3, all of this is achieved zero-shot with pre-trained video model. During evaluation, we move the robot to new scene I0, specify task to be performed in language G, and then generate human video Vg = V(I0, G) that is fed into the human-to-robot translation policy, described in Section IIIC. Our approach is not tied to specific video generative model and as video models become better, this stage of our approach will likely scale upwards. We expect the overall approach to generalize as well since the translation model is tasked with simpler job of inferring motion cues from the generated human video in novel scenarios, and implicitly converting that to robot actions. As we show through results in Section III-C only small amount of diverse robot trajectories ( 400) combined with existing offline datasets is enough to train robust translation model. Fig. 4: Visualization of the closed-loop policy rollouts (bottom row) conditioned on the generated human videos (top row) for four tasks. The red frame and the language description are input to the video generation model of Gen2Act . The black frames show sub-sampled frames of the generated video, and the blue frames show robot executions conditioned on the generated video. C. Generated Human Video to Robot Action Translation We instantiate generated human video to robot action translation as closed loop policy πθ. Given new scene and task description, the generated human video provides motion cues for how the manipulation should happen in the scene, and the role of the policy is to leverage relevant information from the generated video, combined with observations in the robots frame, for interacting in the scene. Instead of attempting to explicitly extract waypoints from the generated video based on heuristics, we adopt more end-to-end approach that relies on general visual features of the video, and general point tracks extracted from the video. This implicit conditioning on the generated video is helpful in mitigating potential artifacts in the generation and in making the approach more robust to mismatch in the video and the robots embodiment. Note that we perform human video generation and ground-truth track extraction completely offline for training. Visual Feature Extraction. For each frame in the generated human video Vg and the robot video Itk:k, we first extract features, ig and ir through ViT encoder χ. The number of video tokens extracted this way is very large and they are temporally uncorrelated, so we have Transformer encoders Φg and Φr that process the respective video tokens through gated Cross-Attention Layers based on PerceiverResampler architecture [60] and output fixed number = 64 of tokens. These tokens respectively are zg = Φg(ig) and zr = Φr(ir). In addition to visual features from the generated video, we encode explicit motion information in the human-to-robot translation policy through point track prediction. Point Track Prediction. We run an off-the-shelf tracking model [21, 61] on the generated video Vg to obtain tracks τg of random set of points in the first frame 0. In order to ensure that the latent embeddings from the generated video zg can distill motion information in the video, we set up track prediction task conditioned on the video tokens. For this, we define track prediction transformer ψg(P 0, i0 g, zg) to predict tracks ˆτg and define an auxiliary loss τg ˆτg2 to update tokens ge. Similarly, for the current robot video Itk:k, we set up similar track prediction auxiliary loss. We run the groundtruth track prediction once over the entire robot observation sequence (again with random points in the first frame P0), but during training, the policy is input chunk of length in one pass. So here, the track prediction transformer ψr(P tk, itk, rtk:t ) is conditioned on the points in the beginning of the chunk Ptk, the image features at that timestep itk and the observation tokens for the chunk zr. BC Loss. For ease of prediction, we discretize the action space such that each dimension has 256 bins. We optimize Behavior Cloning (BC) objective by minimizing error between the predicted actions ˆat:t+h and the ground-truth at:t+h through cross-entropy loss. In Gen2Act, we incorporate track prediction as an auxiliary loss during training combined with the BC loss and the track prediction transformer is not used at test-time. This is helpful in reducing test-time computations for efficient deployment. D. Deployment For deploying Gen2Act to solve manipulation task, we first generate human video conditioned on the language description of the task and the image of the scene. We then roll out the generated video conditioned closed-loop policy. For chaining Gen2Act to perform long-horizon activities consisting of several tasks, we first use an off-the-shelf LLM (e.g. Gemini) to obtain language descriptions of the different tasks. We chain Gen2Act for the task sequence by using the last image of the previous policy rollout as the first frame for generating human video of the subsequent task. We do this chaining in sequence as opposed to generating all the videos from the first image because the final state of the objects in the scene might be different after the robot execution of an intermediate task. IV. EXPERIMENTS We perform experiments in diverse kitchen, office, and lab scenes, across wide array of manipulation tasks. Through these experiments we aim to answer the following questions: Is Gen2Act able to generate plausible human videos of manipulation in diverse everyday scenes? How does Gen2Act perform in terms of varying levels of generalization with new scenes, objects, and motions? Can Gen2Act enable long-horizon manipulation through chaining of the video generation and video-conditioned policy execution? Can the performance of Gen2Act for new tasks be improved by co-training with small amount of additional diverse human tele-operated demonstrations? A. Details of the Evaluation Setup Following prior works in language/goal-conditioned policy learning, we quantify success in terms of whether the executed robot trajectory solves the task specified in the instruction, and define success rate over different rollouts for the same task description. We categorize evaluations with respect to different levels of generalization by following the terminology of prior works [1, 17]: Mild Generalization (MG): unseen configurations of instances in seen scenes; organic scene seen object variations like lighting and background changes Standard Generalization (G): unseen object instances in seen/unseen scenes Object-Type Generalization (OTG): completely unseen object types, in unseen scenes Motion-Type Generalization (MTG): completely unseen motion types, in unseen scenes Here, seen vs. unseen is defined with respect to the robot interaction data, and the assumption is that the video generation model has seen diverse web data including things that are unseen in the robot data. B. Dataset and hardware details For video generation, we use an existing video model, VideoPoet [20] by adapting it to condition on square images in addition to language description of tasks. We do not do any fine-tuning of this model for our experiments, and find that it directly generalizes to human video generation in all the robot experiment scenes. For robot experiments, we use mobile manipulator with compliant two finger-grippers, and operate this robot for policy deployment through end-effector control. The arm is attached to the body of the robot on the right. We manually move the robot around across offices, kitchens, and labs and ask it to manipulate different objects in these scenes. We operate the robot for manipulation at frequency of 3Hz. Before each task, we reset the robot arm to fixed predefined reset position such that the scene is not occluded through the robots camera. For training the video-conditioned policy, we use an existing offline dataset of robot demonstrations collected TABLE I: Comparison of success rates for Gen2Act with different baselines and an ablated variant for the different levels of generalization as defined in Section IV-A Mild (MG) Standard (G) Obj. Type (OTG) Motion. Type (MTG) Avg. RT1 RT1-GC Vid2Robot Gen2Act (w/o track) Gen2Act 68 75 83 83 83 18 24 38 58 67 0 5 25 50 58 0 0 0 5 30 22 26 37 49 60 by prior work [1] and augment this with some paired demonstrations of human videos collected by another prior work [46]. the form In addition, we create pairs of (generated human video, robot demo) by using the video generation model conditioned on the first frame of the respective robot demo, to generate corresponding human video. For obtaining tracks on the generated human video and the robot demo, we use an off-the-shelf tracking approach [21, 61]. Generating human videos, and generating point tracks are done completely offline once and do not induce any additional cost during policy training. C. Baselines and Comparisons We perform comparisons with baselines and ablations with variants of Gen2Act. In particular, we compare with language-conditioned policy baseline (RT1) [1] trained on the same robot data as Gen2Act. We also compare with video-conditioned policy baseline trained on paired real human and robot videos (Vid2Robot) [46], goal-image conditioned policy baseline trained with the same real and generated videos of Gen2Act but by conditioning on just the last video frames (i.e. goal image) of the generated human videos (RT1-GC). Finally, we consider an ablated variant of Gen2Act without the track prediction loss. D. Analysis of Human Video Generations Fig. 3 shows qualitative results for human video generation in diverse scenarios. We can see that the generated videos correspond to plausibly manipulating the scene in the initial instruction. We can see image as described by the text that the respective object in the scene is manipulated while preserving the background and without introducing camera movements and artifacts in the generations. This is exciting because these generations are zero-shot in novel scenarios and can be directly used in robots context to imagine how an unseen object in an unseen scene should be manipulated by human. E. Generalization of Gen2Act to scenes, objects, motions In this section we compare performance of Gen2Act with baselines and ablated variants for different levels of generalization. Table shows success rates for tasks averaged across different levels of generalization. We observe that for higher levels of generalization, Gen2Act achieves much higher success rates indicating that human video generation combined with explicitly extracting motion information from track prediction is helpful in unseen tasks. n S p i e o i l e T t p Fig. 5: Robot executions for sequence of tasks. The last frame of the previous execution serves as the conditioning frame for next stage video generation. TABLE II: Comparison of success rates for long-horizon activities via chaining of different tasks. We first obtain sub-tasks for activities with an off-the-shelf LLM and then rollout Gen2Act in sequence for the different intermediate tasks. Activity Stages (from Gemini) Success % Stage 1, Stage 2, Stage 3 TABLE III: Analysis of co-training with an additional dataset of diverse tele-operated robot demonstrations ( 400 trajectories). Co-Training Mild (MG) Standard (G) Obj. Type (OTG) Motion. Type (MTG) Avg. Gen2Act (w/o co-train) Gen2Act (w/ co-train) 83 85 67 58 62 30 35 60 64 1. Open the Drawer 2. Place Apple in Drawer 3. Close the Drawer 80, 60, 60 small amount of diverse demonstrations, the translation model of Gen2Act can be improved to better condition on the generated videos for higher levels of generalization where robot data support is limited. 1. Open the Lid 2. Place K-Cup Pod inside 3. Close the Lid 40, 20, 20 H. Analysis of Failures 1. Pick Tissues from Box 2. Press the Sanitizer Dispenser 3. Wipe the Table with Tissues 1. Open the Microwave 2. Put Bowl inside Microwave 3. Close the Microwave 60, 40, 40, 20, 20 F. Chaining Gen2Act for long-horizon manipulation We now analyze the feasibility of Gen2Act for solving sequence of manipulation tasks through chaining. Table II shows results for long-horizon activities like Making Coffee that consist of multiple tasks to be performed in sequence. We obtain this sequence of tasks through Gemini [62], and for each task, condition the video generation on the last image of the scene from the previous execution and execute the policy for the current task conditioned on the generated human video. We repeat this in sequence for all the stages, and report success rates for successful completion upto each stage over 5 trials. Fig. 5 visually illustrates singletake rollouts from four such long-horizon activities. G. Co-Training with additional teleop demonstrations The offline dataset we used for experiments in the previous section had limited coverage over scenes and types of tasks thereby allowing less than 60% success rate of Gen2Act for higher levels of generalization (OTG and MTG in Table I). In this section, we perform experiments to understand if adding small amount of additional diverse tele-operated trajectories, for co-training with the existing offline dataset, can help improve generalization. We keep the video generation model fixed as usual. From the results in Table III we see improved performance of Gen2Act with such cotraining. This is exciting because it suggests that with only Here we discuss the type of failures exhibited by Gen2Act. We observe that for MG and to some extent in G, inaccuracies in video generation are less correlated with failures of the policy. While, for the higher levels of generalization, object type (OTG) and motion type (MTG), if video generation yields implausible videos, then the policy doesnt succeed in performing the tasks. This is also evidence that the policy of Gen2Act is using the generated human video for inferring motion cues while completing task, and as such when video generation is incorrect in scenarios where robot data support is limited (e.g. in OTG and MTG), the policy fails. V. DISCUSSION AND CONCLUSION Summary. In this work, we developed framework for learning generalizable robot manipulation by combining zero-shot human video generation from web data with limited robot demonstrations. Broadly, our work is indicative of how motion predictive models trained on non-robotic datasets like web videos can be used to used to enable generalization of manipulation policies to unseen scenarios, without requiring collection of robot data for every task. Limitations. Our work focused on zero-shot human video generation combined with point track prediction on the videos as way for providing motion cues to robot manipulation system for interacting with unseen objects and performing novel tasks. As such, the capabilities of our system are limited by the current capabilities of video generation models, like inability to generate realistic hands and thereby limited ability to perform very dexterous tasks. Future Work. It would be an interesting direction of future work to explore recovering more dense motion information from the generated videos beyond point tracks, like object meshes for addressing some of the limitations. Another important direction would be to enable reliable long-horizon manipulation by augmenting chaining with learning recovery policies for intermediate failures."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Jie Tan for feedback and guidance throughout the project. We are grateful to Peng Xu, Alex Kim, Alexander Herzog, Paul Wohlhart, Alex Irpan, Justice Carbajal, Clayton Tan for help with robot and compute infrastructures. We thank David Ross, Bryan Seybold, Xiuye Gu, and Ozgun Bursalioglu for helpful pointers regarding video generation. We enjoyed discussions with Chen Wang, Jason Ma, Laura Smith, Danny Driess, Soroush Nasiriany, Coline Devin, Keerthana Gopalakrishnan, and Joey Hejna that were helpful for the project. Finally, we thank Jacky Liang and Carolina Parada for feedback on the paper."
        },
        {
            "title": "REFERENCES",
            "content": "[1] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al., Rt-1: Robotics transformer for real-world control at scale, arXiv preprint arXiv:2212.06817, 2022. [2] H. Bharadhwaj, J. Vakil, M. Sharma, A. Gupta, S. Tulsiani, and V. Kumar, Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking, in 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024. [3] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, Bc-z: Zero-shot task generalization with robotic imitation learning, in Conference on Robot Learning. PMLR, 2022, pp. 991 1002. [4] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, et al., Droid: large-scale inthe-wild robot manipulation dataset, arXiv preprint arXiv:2403.12945, 2024. [5] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, R3m: universal visual representation for robot manipulation, arXiv preprint arXiv:2203.12601, 2022. [6] A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, P. Abbeel, J. Malik, et al., Where are we in the search for an artificial visual cortex for embodied intelligence? arXiv preprint arXiv:2303.18240, 2023. [7] T. Xiao, I. Radosavovic, T. Darrell, and J. Malik, Masked visual pre-training for motor control, arXiv preprint arXiv:2203.06173, 2022. [8] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta, The unsurprising effectiveness of pretrained vision models for control, arXiv preprint arXiv:2203.03580, 2022. [9] S. Karamcheti, S. Nair, A. S. Chen, T. Kollar, C. Finn, D. Sadigh, and P. Liang, Language-driven representation learning for robotics, arXiv preprint arXiv:2302.12766, 2023. [10] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid, et al., Rt-2: Vision-language-action models transfer web knowledge to robotic control, in Conference on Robot Learning. PMLR, 2023, pp. 21652183. [11] N. D. Palo and E. Johns, Dinobot: Robot manipulation via retrieval and alignment with vision foundation models, in IEEE International Conference on Robotics and Automation (ICRA), 2024. [12] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, et al., Openvla: An open-source vision-languageaction model, arXiv preprint arXiv:2406.09246, 2024. [13] K. Black, M. Nakamoto, P. Atreya, H. Walke, C. Finn, A. Kumar, and S. Levine, Zero-shot robotic manipulation with pretrained image-editing diffusion models, arXiv preprint arXiv:2310.10639, 2023. [14] H. Bharadhwaj, A. Gupta, and S. Tulsiani, Visual affordance prediction for guiding robot exploration, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 30293036. [15] I. Kapelyukh, V. Vosylius, and E. Johns, Dall-e-bot: Introducing web-scale diffusion models to robotics, IEEE Robotics and Automation Letters, vol. 8, no. 7, pp. 39563963, 2023. [16] H. Bharadhwaj, A. Gupta, V. Kumar, and S. Tulsiani, Towards generalizable zero-shot manipulation via translating human interaction plans, in 2024 IEEE International Conference on Robotics and Automation (ICRA), 2024. [17] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani, Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation, arXiv preprint arXiv:2405.01527, 2024. [18] R. Girdhar, M. Singh, A. Brown, Q. Duval, S. Azadi, S. S. Rambhatla, A. Shah, X. Yin, D. Parikh, and I. Misra, Emu video: Factorizing text-to-video generation by explicit image conditioning, arXiv preprint arXiv:2311.10709, 2023. [19] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi, R. G. Lopes, et al., Photorealistic text-toimage diffusion models with deep language understanding, arXiv preprint arXiv:2205.11487, 2022. [20] D. Kondratyuk, L. Yu, X. Gu, J. Lezama, J. Huang, R. Hornung, H. Adam, H. Akbari, Y. Alon, V. Birodkar, et al., Videopoet: large language model for zero-shot video generation, arXiv preprint arXiv:2312.14125, 2023. [21] C. Doersch, Y. Yang, D. Gokay, P. Luc, S. Koppula, A. Gupta, J. Heyward, R. Goroshin, J. Carreira, and A. Zisserman, Bootstap: Bootstrapped training for tracking-any-point, arXiv preprint arXiv:2402.00847, 2024. [22] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine, One-shot visual imitation learning via meta-learning, in Conference on robot learning. PMLR, 2017, pp. 357368. [23] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto, Visual imitation made easy, in Conference on Robot Learning (CoRL), 2020. [24] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta, E. Orbay, et al., Roboturk: crowdsourcing platform for robotic skill learning through imitation, in Conference on Robot Learning. PMLR, 2018, pp. 879893. [25] S. Ross, N. Melik-Barkhudarov, K. S. Shankar, A. Wendel, D. Dey, J. A. Bagnell, and M. Hebert, Learning monocular reactive uav control in cluttered natural environments, in 2013 IEEE international conference on robotics and automation. IEEE, 2013, pp. 1765 1772. [26] Z. Chen and X. Huang, End-to-end learning for lane keeping of self-driving cars, in 2017 IEEE intelligent vehicles symposium (IV). IEEE, 2017, pp. 18561860. [27] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta, E. Orbay, et al., Roboturk: crowdsourcing platform for robotic skill learning through imitation, in Conference on Robot Learning. PMLR, 2018, pp. 879893. [28] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, Bc-z: Zero-shot task generalization with robotic imitation learning, in Conference on Robot Learning. PMLR, 2022, pp. 991 1002. [29] H. R. Walke, K. Black, T. Z. Zhao, Q. Vuong, C. Zheng, P. Hansen-Estruch, A. W. He, V. Myers, M. J. Kim, M. Du, et al., Bridgedata v2: dataset for robot learning at scale, in Conference on Robot Learning. PMLR, 2023, pp. 17231736. [30] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, et al., Scalable deep reinforcement learning for vision-based robotic manipulation, in Conference on robot learning. PMLR, 2018, pp. 651673. [31] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, et al., Ego4d: Around the world in 3,000 hours of egocentric video, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 18 99519 012. [32] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248255. [33] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang, Vip: Towards universal visual reward and representation via value-implicit pretraining, arXiv preprint arXiv:2210.00030, 2022. [34] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta, The unsurprising effectiveness of pretrained vision models for control, arXiv preprint arXiv:2203.03580, 2022. [35] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu, X. Li, M. Liu, H. Li, and T. Kong, Unleashing large-scale video generative pre-training for visual robot manipulation, arXiv preprint arXiv:2312.13139, 2023. [36] S. Yang, J. Walker, J. Parker-Holder, Y. Du, J. Bruce, A. Barreto, P. Abbeel, and D. Schuurmans, Video as the new language for real-world decision making, arXiv preprint arXiv:2402.17139, 2024. [37] M. Sharma, C. Fantacci, Y. Zhou, S. Koppula, N. Heess, J. Scholz, and Y. Aytar, Lossless adaptation of pretrained vision models for robotic manipulation, arXiv preprint arXiv:2304.06600, 2023. [38] N. Hansen, Z. Yuan, Y. Ze, T. Mu, A. Rajeswaran, H. Su, H. Xu, and X. Wang, On pre-training for visuo-motor control: Revisiting learning-from-scratch baseline, arXiv preprint arXiv:2212.05749, 2022. [39] Z. Mandi, H. Bharadhwaj, V. Moens, S. Song, A. Rajeswaran, and V. Kumar, Cacti: framework for scalable multi-task multi-scene visual imitation learning, arXiv preprint arXiv:2212.05711, 2022. [40] Z. Chen, S. Kiami, A. Gupta, and V. Kumar, Genaug: Retargeting behaviors to unseen situations via generative augmentation, arXiv preprint arXiv:2302.06671, 2023. [41] T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan, S. Wang, J. Singh, C. Tan, J. Peralta, B. Ichter, et al., learning with semantically imagined Scaling robot experience, arXiv preprint arXiv:2302.11550, 2023. [42] Z. Chen, Z. Mandi, H. Bharadhwaj, M. Sharma, S. Song, A. Gupta, and V. Kumar, Semantically controllable augmentations for generalizable robot learning, arXiv preprint arXiv:2409.00951, 2024. [43] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu, Y. Zhu, and A. Anandkumar, Mimicplay: Longhorizon imitation learning by watching human play, arXiv preprint arXiv:2302.12422, 2023. [44] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and S. Levine, Avid: Learning multi-stage tasks via pixellevel translation of human videos, arXiv, 2019. [45] H. Xiong, Q. Li, Y.-C. Chen, H. Bharadhwaj, S. Sinha, and A. Garg, Learning by watching: Physical imitation of manipulation skills from human videos, arXiv, 2021. [46] V. Jain, M. Attarian, N. J. Joshi, A. Wahid, D. Driess, Q. Vuong, P. R. Sanketi, P. Sermanet, S. Welker, C. Chan, et al., Vid2robot: End-to-end videoconditioned policy learning with cross-attention transformers, arXiv preprint arXiv:2403.12943, 2024. [47] C. Wen, X. Lin, J. So, K. Chen, Q. Dou, Y. Gao, and P. Abbeel, Any-point trajectory modeling for policy learning, arXiv preprint arXiv:2401.00025, 2023. [48] J. Gu, S. Kirmani, P. Wohlhart, Y. Lu, M. G. Arenas, K. Rao, W. Yu, C. Fu, K. Gopalakrishnan, Z. Xu, et al., Rt-trajectory: Robotic task generalization via hindsight trajectory sketches, arXiv preprint arXiv:2311.01977, 2023. [49] Y. Qin, Y.-H. Wu, S. Liu, H. Jiang, R. Yang, Y. Fu, and X. Wang, Dexmv: Imitation learning for dexterous manipulation from human videos, arXiv preprint arXiv:2108.05877, 2021. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 19 18719 197. [50] K. Shaw, S. Bahl, and D. Pathak, Videodex: Learning dexterity from internet videos, in 6th Annual Conference on Robot Learning. [51] K. Mo, L. J. Guibas, M. Mukadam, A. Gupta, and S. Tulsiani, Where2act: From pixels to actions for articulated 3d objects, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 68136823. [52] M. Goyal, S. Modi, R. Goyal, and S. Gupta, Human hands as probes for interactive object understanding, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 32933303. [53] S. Bahl, R. Mendonca, L. Chen, U. Jain, and D. Pathak, Affordances from human videos as versatile representation for robotics, in CVPR, 2023. [54] S. Liu, S. Tripathi, S. Majumdar, and X. Wang, Joint hand motion and interaction hotspots prediction from egocentric videos, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 32823292. [55] C. Yuan, C. Wen, T. Zhang, and Y. Gao, General flow as foundation affordance for scalable robot learning, arXiv preprint arXiv:2401.11439, 2024. [56] S. Bahl, A. Gupta, and D. Pathak, Human-to-robot imitation in the wild, RSS, 2022. [57] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel, Learning universal policies via text-guided video generation, Advances in Neural Information Processing Systems, vol. 36, 2024. [58] J. Liang, R. Liu, E. Ozguroglu, S. Sudhakar, A. Dave, P. Tokmakov, S. Song, and C. Vondrick, Dreamitate: Real-world visuomotor policy learning via video generation, arXiv preprint arXiv:2406.16862, 2024. [59] G. Papagiannis, N. Di Palo, P. Vitiello, and E. Johns, R+ x: Retrieval and execution from everyday human videos, arXiv preprint arXiv:2407.12957, 2024. [60] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: visual language model for few-shot learning, Advances in neural information processing systems, vol. 35, pp. 23 71623 736, 2022. [61] C. Doersch, A. Gupta, L. Markeeva, A. Recasens, L. Smaira, Y. Aytar, J. Carreira, A. Zisserman, and Y. Yang, Tap-vid: benchmark for tracking any point in video, Advances in Neural Information Processing Systems, vol. 35, pp. 13 61013 626, 2022. [62] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [63] N. Karaev, I. Rocco, B. Graham, N. Neverova, A. Vedaldi, and C. Rupprecht, Cotracker: It is better to track together, arXiv preprint arXiv:2307.07635, 2023. [64] G. Le Moing, J. Ponce, and C. Schmid, Dense optical tracking: connecting the dots, in Proceedings of the"
        },
        {
            "title": "Here we provide additional details on the method and",
            "content": "experiments of Gen2Act. A. Human Video Generation We use pre-trained VideoPoet model [20] directly without any adaptation or fine-tuning. The input to the model for video generation is language description of task (the prompt) and square-shaped image. By virtue of being trained on diverse large-scale video datasets (> 270M videos) we find that this model generalizes well to everyday tasks we develop Gen2Act for. It can generate realistic and plausible videos of humans manipulating objects, without introducing significant camera motions/artifacts in the generated videos. We ensure that the image of the scene input to the model doesnt have the robot in the frame (the initial reset position of the robot is such that the arm is mostly out of camera view). The language prompt to the model is of the form person task-name, static camera e.g. for the task opening the microwave the input prompt is person opening the microwave, static camera. B. Closed-Loop Policy For each frame in the generated human video Vg and the robot video Itk:k, we first extract features, ig and ir through ViT encoder χ. The number of video tokens extracted this way is very large and they are temporally uncorrelated, so we have Transformer encoders Φg and Φr that process the respective video tokens through gated Cross-Attention Layers based on Perceiver-Resampler architecture [60] and output fixed number = 64 of tokens. We use 2 Perceiver-Resampler layers for both the generated video token processing and the robot observation history video processing. These tokens respectively are zg = Φg(ig) and zr = Φr(ir). During training we sample fixed sequence of 16 frames from the generated video ensuring that we always sample the first and last frames. For the robot history, we choose the last 8 frames of robot observations. We resize all images to 224x224 dimensions. We run an off-the-shelf tracking model [21, 61] on the generated video Vg to obtain tracks τg of random set of points in the first frame 0. In order to ensure that the latent embeddings from the generated video zg can distill motion information in the video, we set up track prediction task conditioned on the video tokens. For this, we define track prediction transformer ψg(P 0, i0 g, zg) to predict tracks ˆτg and define an auxiliary loss τg ˆτg2 to update tokens ge. Similarly, for the current robot video Itk:k, we set up similar track prediction auxiliary loss. We run the groundtruth track prediction once over the entire robot observation sequence (again with random points in the first frame P0), but during training, the policy is input chunk of length in one pass. So here, the track prediction transformer ψr(P tk, itk, rtk:t ) is conditioned on the points in the beginning of the chunk Ptk, the image features at that timestep itk and the observation tokens for the chunk zr. The track prediction transformer has 6 self-attention layers with 8 heads and its role is solely to make the input tokens from generated video / robot observations informative of motion cues. Note that any ground-truth track prediction model can be used for this, and recent advances in point tracking can help improve this step [63, 64] For ease of prediction, we discretize the action space such that each dimension has 256 bins. So each action dimension can take values in the range [0, 255]. The bins are uniformaly distributed within the bounds of each dimension. We predict actions in the end-effector space, and also predict whether to terminate the episode, and whether the gripepr should be open/close. We optimize Behavior Cloning (BC) objective by minimizing error between the predicted actions ˆat:t+h and the ground-truth at:t+h through cross-entropy loss. This discrete action-space for prediction is based on prior works in multi-task imitation learning [1]. C. Chaining Gen2Act for Long-Horizon Manipulation For deploying Gen2Act to solve manipulation task, we first generate human video conditioned on the language description of the task and the image of the scene. We then roll out the generated video conditioned closed-loop policy. For chaining Gen2Act to perform long-horizon activities consisting of several tasks, we first use an off-the-shelf LLM (e.g. Gemini) to obtain language descriptions of the different tasks. We chain Gen2Act for the task sequence by using the last image of the previous policy rollout as the first frame for generating human video of the subsequent task. We do this chaining in sequence as opposed to generating all the videos from the first image because the final state of the objects in the scene might be different after the robot execution of an intermediate task. This iterative sequencing of video generation after the previous robot execution is made possible by the fast generation capabilities of the Video model we use, VideoPoet (it takes less than 10 seconds to generate new video after generating the very first video). We provide examples of the prompt to Gemini and the subsequent outputs for task decomposition below: Cleaning the Table. Q. What are the steps involved in cleaning table with sanitizer dispenser and tissues? (write three steps) A: Steps for Cleaning Table with Sanitizer Dispenser and Tissues: Gather materials: Pick tissues from the box. Apply sanitizer: Press the sanitizer dispenser onto the tissue. Wipe: Gently wipe the table surface with the sanitizerdampened tissues. Making Coffee. Q. What are the steps involved in making coffee with Keurig? (write first three steps) A: Steps for Making Coffee with Keurig: Open the lid: Open the lid of the Keurig machine. Insert K-Cup: Place K-Cup pod into the machine. Close the lid: Close the lid of the Keurig machine. Videos for these tasks are best viewed in the project website. Fig. 6: Analysis of failures of Gen2Act. The tasks here correspond to object type generalization. We can see that most of the failures of robot execution (top 3 rows) are correlated with incorrect video generations. In the last row the video generation is plausible but the execution is incorrect in following the trajectory of the generated video afetr grasping the object. D. Analysis of Failures Here we discuss the type of failures exhibited by Gen2Act. We observe that for MG and to some extent in G, inaccuracies in video generation are less correlated with failures of the policy. While, for the higher levels of generalization, object type (OTG) and motion type (MTG), if video generation yields implausible videos, then the policy doesnt succeed in performing the tasks. This is also evidence that the policy of Gen2Act is using the generated human video for inferring motion cues while completing task, and as such when video generation is incorrect in scenarios where robot data support is limited (e.g. in OTG and MTG), the policy fails. Fig. 6 shows some examples of failures of Gen2Act in different tasks. Most of the failures are correlated with video generation (first three rows) but generating video plausibly (fourth row) is not guarantee of the policy succeeding because there might be issues with grasping the object correctly and following the trajectory of the object post grasp. This indicates potential for future work to explore recovering more dense motion information from the generated videos beyond point tracks, like object meshes for mitigating some of the failures."
        }
    ],
    "affiliations": [
        "Computer Science Department, Stanford University",
        "Google DeepMind",
        "The Robotics Institute, Carnegie Mellon University"
    ]
}