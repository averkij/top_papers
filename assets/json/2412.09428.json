{
    "paper_title": "Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation",
    "authors": [
        "Baisen Wang",
        "Le Zhuo",
        "Zhaokai Wang",
        "Chenxi Bao",
        "Wu Chengjing",
        "Xuecheng Nie",
        "Jiao Dai",
        "Jizhong Han",
        "Yue Liao",
        "Si Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal music generation aims to produce music from diverse input modalities, including text, videos, and images. Existing methods use a common embedding space for multimodal fusion. Despite their effectiveness in other modalities, their application in multimodal music generation faces challenges of data scarcity, weak cross-modal alignment, and limited controllability. This paper addresses these issues by using explicit bridges of text and music for multimodal alignment. We introduce a novel method named Visuals Music Bridge (VMB). Specifically, a Multimodal Music Description Model converts visual inputs into detailed textual descriptions to provide the text bridge; a Dual-track Music Retrieval module that combines broad and targeted retrieval strategies to provide the music bridge and enable user control. Finally, we design an Explicitly Conditioned Music Generation framework to generate music based on the two bridges. We conduct experiments on video-to-music, image-to-music, text-to-music, and controllable music generation tasks, along with experiments on controllability. The results demonstrate that VMB significantly enhances music quality, modality, and customization alignment compared to previous methods. VMB sets a new standard for interpretable and expressive multimodal music generation with applications in various multimedia fields. Demos and code are available at https://github.com/wbs2788/VMB."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 8 2 4 9 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Retrieval Augmentation",
            "content": "Baisen Wang1,2 Le Zhuo3 Zhaokai Wang4,3 Chenxi Bao5 Xuecheng Nie6 Jiao Dai1, Jizhong Han1,2 Yue Liao7 Chengjing Wu6 Si Liu8 1Institute of Information Engineering, Chinese Academy of Sciences 2School of Cyberspace Security, University of Chinese Academy of Sciences 4Shanghai Jiao Tong University 7The Chinese University of Hong Kong {wbs2788, zhuole1025, zhaokaiwang99, liaoyue.ai}@gmail.com 3Shanghai AI Laboratory 6MT Lab, Meitu Inc. 5University of Edinburgh 8Beihang University"
        },
        {
            "title": "Abstract",
            "content": "Multimodal music generation aims to produce music from diverse input modalities, including text, videos, and images. Existing methods use common embedding space for multimodal fusion. Despite their effectiveness in other modalities, their application in multimodal music generation faces challenges of data scarcity, weak cross-modal alignment, and limited controllability. This paper addresses these issues by using explicit bridges of text and music for multimodal alignment. We introduce novel method named Visuals Music Bridge (VMB). Specifically, Multimodal Music Description Model converts visual inputs into detailed textual descriptions to provide the text bridge; Dual-track Music Retrieval module that combines broad and targeted retrieval strategies to provide the music bridge and enable user control. Finally, we design an Explicitly Conditioned Music Generation framework to generate music based on the two bridges. We conduct experiments on video-to-music, image-to-music, text-to-music, and controllable music generation tasks, along with experiments on controllability. The results demonstrate that VMB significantly enhances music quality, modality, and customization alignment compared to previous methods. VMB sets new standard for interpretable and expressive multimodal music generation with applications in various multimedia fields. Demos and code are available at https://github. com/wbs2788/VMB. 1. Introduction Music transcends linguistic and cultural barriers, uniquely resonating with human emotions and fostering connections Corresponding authors. beyond words. Typically, when people experience visual, textual, or auditory inputs in their daily lives, they instinctively associate them with certain music motifs. This inherent ability to link sensory experiences with music underscores the potential for system designed to generate music from various modalities. Recent advancements in generative models have led to significant progress in producing music mainly from textual descriptions, i.e., text-to-music generation [1, 8, 17] that translate the semantics conveyed in text into corresponding music. However, extending music generation to other modalities, such as images [7, 52] or videos [13, 26, 61], is still challenging and remain in its preliminary stage. For the more challenging task multimodal music generation that takes multiple input modalities (e.g., text, image, video) to generate music, existing methods mostly use common embedding space as the bridge between multiple modalities [34, 50, 55]. Such approaches often struggle with maintaining high quality and accurate modality correlations in their musical outputs. We argue that several key limitations hinder the development of effective multimodal music generation systems. Firstly, for modalities other than text, the scarcity of largescale, high-quality music-paired datasets hinders the learning of meaningful cross-modal relationships. Secondly, different modalities contribute to music generation in distinct and complementary ways, e.g., text offers explicit semantic cues such as themes and emotions, while images and videos encapsulate visual emotions, atmosphere, style, and finegrained temporal dynamics like rhythm. This highlights the importance of stronger multimodal alignment. Thirdly, existing methods using common embedding space as the intermedia often lack interpretability and fine-grained controllability, limiting the ability to manipulate specific musical attributes such as instruments, style, or rhythm. 1 Figure 1. Overview of the VMB framework. We employ text and music as two explicit bridges for multimodal music generation. Textform music description is obtained with the Multimodal Music Description model. Reference music is retrieved with the Dual-track Music Retrieval module. The two bridges are fed into the Explicitly Conditioned Music Generation module to generate output music. To address these challenges, we propose to utilize text and music as explicit bridges for multimodal alignment. In contrast to previous methods that use joint embedding space for implicit alignment, the explicit bridge helps to mitigate the data scarcity issue and improve multimodal alignment by utilizing abundant text-music paired data. For text-bridge, we convert images and videos into detailed music descriptions with multimodal music description model that guides the music generation process. For music-bridge, we introduce retrieval-augmented generation (RAG) into multimodal music synthesis to condition the model on relevant music pieces with both broad thematic alignment and targeted attribute control. Text and music complement each other with their distinct roles in multimodal alignment. This method also enhances controllability as users can specify the output music by modifying the text description, or even providing their own music for reference. Based on the above design principles, we design novel multimodal music generation framework, namely Visuals Music Bridge (VMB), that supports creating high-quality music from various modalities including text, audio, video, and music. As illustrated in Fig. 1, our VMB framework consists of the following core components: 1. Multimodal Music Description Model: We curate multimodal music dataset consisting of video-musicdescription triplets and various musical attribute annotations, e.g., genre, mood, and instruments. Given this dataset, we introduce the Multimodal Music Description Model, which is built upon InternVL2 [6] to accurately interpret visual inputs and translate them into detailed musical descriptions in natural language, serving as the text bridge for music generation. 2. Dual-track Music Retrieval: We design Dual-track Music Retrieval framework tailored for multimodal music generation, which retrieves relevant music pieces leveraging both broad and targeted retrieval strategies. Broad retrieval identifies overall alignments of emotional and thematic content, establishing global coherence, while targeted retrieval focuses on specific musical attributes like tempo, instrumentation, and genre, allowing users to customize individual elements of the music. 3. Explicitly Conditioned Music Generation: Given the text bridge and the music bridge, we propose the architecture of Explicitly Conditioned Music Generation, which integrates the two explicit bridges into text-tomusic diffusion transformer. Our model employs Music ControlFormer to integrate fine-grained control from the broad retrieval and leverages Stylization Module for overall conditions from the targeted retrieval. We conduct extensive experiments on video-to-music, text-to-music, image-to-music, and controllable music generation tasks. The results demonstrate that VMB is robust, flexible framework that improves music quality, enhances alignment between input modalities and generated music, and offers high controllability. Our approach represents step toward sophisticated and accessible multimodal music generation, with broad applications in multimedia. 2. Related Work Multi-modal Music Generation. In recent years, automatic music generation has witnessed significant advancements in symbolic form [14, 22, 24, 46, 57] and audio form [17, 23, 33]. Meanwhile, many efforts has been made towards conditional music generation from other input modalities. Text-to-music methods aim to generate music from text descriptions and have made notable progress, such as AudioLDM [33], MusicLM [1], MusicGen [8], Stable Audio Open [17]. Building on this, MeLFusion [7] enhances the generation process by using an image as condition to assist the textual description in producing music. For video-to-music generation, CMT [13], an early at2 Figure 2. Pipeline of the Multimodal Music Description Model (MMDM). This process starts with the collection of music videos, followed by automated tagging to refine audio annotations using CLAP embedding similarities. Metadata and thematic descriptions are synthesized by the Llama 3.1 model to create training targets. The training utilizes LoRA fine-tuning in the MMDM to transform multimodal inputs into targeted music descriptions that align with the visual contents themes. tempt to generate background music from videos, uses rulebased rhythmic features to translate video dynamics into musical rhythms to address the challenge of sparse data. MIDI-based video soundtrack generation methods [13, 26, 31, 61] leverage the characteristics of MIDI to extract rhythmic elements from videos, enabling the generation of music that aligns with the videos rhythm. Additionally, these methods incorporate music theory knowledge to assist in generating background music for videos. Audio-based methods [49, 51], unlike MIDI-based approaches that are constrained by the limited availability of MIDI data, take advantage of large-scale audio datasets. By leveraging variety of pre-trained models, audio-based methods aim to uncover the complex connections between video content and music, addressing the challenges posed by the scarcity of large, labeled datasets. However, video-music alignment in current methods is still unsatisfying due to the inherent intricate alignment between visual inputs and music outputs. In contrast to these methods that generate music from single input, multimodal music generation consider multiple input modalities. Some multimodal generation works like NExT-GPT [55] and CoDi [50] does not treat music as separate modality but as part of audio generation. Consequently, their composed music has limited quality. M2UGen [34] proposes multimodal music generation method for video-to-music, image-music and textmusic tasks. However, the feature-based modality fusion approach, adopted by most multimodal music generation methods, fails to fully capture the complex visual-music relationships. Moreover, their controllability is limited as extracted features cannot be manipulated directly. In this work, we employ text and music as the explicit bridges for multimodal fusion, which proves to be effective approach in multimodal music generation. Retrieval-Augmented Generation. Retrieval-augmented generation (RAG) has become common technique for enhancing the fidelity and diversity of language models with an additional knowledge base in the field of natural language processing [3, 19, 30, 60]. Due to its effectiveness, RAG is further generalized to diverse fields and tasks, such as visual recognition [21, 35], image generation [4, 48], 3D generation [53, 54], and drug discovery [2]. However, RAG in music generation has not yet been explored. To the best of our knowledge, we propose the first retrieval-augmented pipeline for multimodal music generation, which dynamically incorporates dual-form of musical knowledge into pre-trained music generation model to bridge the modality gap, boost cross-modal generation performance, and increase controllability. 3. Methodology In this section, we present our multimodal music generation framework, consisting of Multimodal Music Description Model that generates textual descriptions for emotional and thematic framing, providing the text bridge; Dual-track Music Retrieval module to retrieve relevant music, providing the music bridge; and an Explicitly Conditioned Music Generation module that leverages the two bridges to generate the final music. 3.1. Multimodal Music Description Model To bridge the gap between multimodal information and music modality, our approach employs textual descriptions as the first explicit bridge, effectively linking complex multimodal inputs with music generation. Leveraging the crossmodal understanding and generation ability of multimodal large language models [32, 38, 58], we introduce the Multimodal Music Description Model (MMDM), which is built upon InternVL2 [6] to map multimodal signals into textual music description to guide the music generation process. The primary challenge is constructing effective video and music description paired data for efficient model training. To address this, we introduce large-scale multimodal music dataset and detail the comprehensive dataset construction process, including data collection and the gener3 Figure 3. Framework of Dual-track Music Retrieval and Explicitly Conditioned Music Generation. The left part illustrates the Dualtrack Music Retrieval process, which leverages our multimodal dataset to perform both broad and targeted retrieval. The right part shows the Explicitly Conditioned Music Generation pathway, where music is generated through ControlFormer block integrating embeddings from selected music bridge, text bridge, and noisy inputs. ation of music-description labels, which is shown in Fig. 2. Music Video Data Collection. In the data collection phase, we selected music videos (MVs) from online sources as primary dataset, focusing on those with high degree of alignment between visual content and musical expression. Each entry in the dataset comprises three components: Visual Content: Key frames were extracted from each MV to capture important scenes and transitions, establishing coherent visual narrative. Audio Content: The original music tracks accompanying each MV serve as the primary auditory context. Using Demucs [11], we isolate instrumental components by filtering out vocal parts to focus on the instrumental aspects. Metadata: We collect metadata such as titles, artist information, and video descriptions from platforms like YouTube and Shazam, providing supplementary context for each music-video pair. Automatic Music Description Generation. After constructing the initial MV dataset, we proceed to generate music descriptions from the raw data. This process begins with an auto-tagging method that annotates each audio file across dimensions such as emotional tone and music theory elements. We then employ large language models to formulate unstructured metadata and tags into detailed music descriptions in natural language. The auto-tagging process is grounded in comprehensive label set developed by music experts, covering essential categories such as instruments, genres, and emotions for standardized and interpretable tagging. We extract the CLAP embeddings [16] of candidate tags and align them with audio CLAP embeddings through cosine similarity: cos(etag, eaudio) = etag eaudio etageaudio . (1) 4 To ensure high precision in labeling, we use carefully calibrated similarity thresholds to filter out low-alignment tags. To generate the detailed music descriptions, we combine auto-generated tags with crawled metadata to create comprehensive, context-rich descriptions. Leveraging the Llama 3.1 model [15], we craft descriptions that encapsulate the emotions and themes of each piece of music. By creating structured prompts, we guide the model to synthesize information from both tags and metadata, linking the music to relevant themes, emotions, and imagery. Through series of carefully designed instructions, our approach produces nuanced descriptions that reflect the evocative and interpretative qualities of the music, providing multifaceted understanding of each track. Multimodal Music Description Generation. To use text as the bridge for multimodal music generation, we design multimodal music description model that can extract visual and emotional cues from videos and images into textual music descriptions. To incorporate visual and emotional cues for music description generation, we design an instruction system that guides the language model to first generate descriptions of the visual elements present in the input. The MMDM is built by finetuning InternVL2 [6] on video-to-music description data. For efficient adaptation, we employed Low-Rank Adaptation (LoRA) [20], enabling targeted fine-tuning while preserving the core multimodal strengths of InternVL2. 3.2. Dual-track Music Retrieval When musicians begin composing for multimedia, they often start by searching for existing music that aligns with the medias elements, learning from these examples to enhance their own compositions. Similarly, AI models can leverage retrieval-based techniques to assist in generation when datasets are insufficient, process known as retrievalaugmented generation. In the context of music generation, particularly with music descriptions obtained via MMDM, we aim to tackle the challenge of producing music that faithfully aligns with multimodal content. Direct text-to-music approaches often struggle to capture intricate musical elements such as chord progressions, tempo, and rhythm, revealing gap between language and music modalities. To bridge this gap, we propose using music itself as secondary explicit bridge. Inspired by the way composers select music to match the media and improve their compositions, we enhance the text-tomusic generation model with Dual-track Music Retrieval module. This module utilizes multimodal inputs to retrieve the most relevant music, which is then used as conditional input. This music-to-music conditioning enables direct transfer of information, resulting in high-quality music outputs that closely reflect the intended multimodal content. As illustrated in Fig. 3, our dual-track music retrieval pipeline combines broad and targeted retrieval strategies for fine-grained and coarse-grained control, respectively. We leverage our auto-collected dataset in Sec. 3.1 as the retrieval database, containing 25,046 high-quality videomusic-text triplets to provide external knowledge. Broad retrieval for Fine-Grained Control. For each conditional modality, we first retrieve music that closely aligns with the input by computing similarities using CLIP [43] or CLAP [16] embeddings. Given target music, we directly use CLAP to compute audio embeddings ainput and compare them with embeddings {eaudio } in our music database: = arg max cos (cid:0)eaudio input , eaudio (cid:1) . (2) The music pieces retrieved from the broad retrieval mi serve as fine-grained conditions, as illustrated in Sec. 3.3. Notably, although we only apply music-centric retrieval during training, it is feasible to use CLIP embeddings of the input text tinput or visual signals vinput and compare them with text-music or visual-music pairs in the database as inference-time retrieval. The music associated with the closest matches is retrieved as: = arg max cos (cid:0)etext/visual input , etext/visual (cid:1) , (3) input and etext/visual where etext/visual are the CLIP embeddings of the input text or visual content and each dataset entry. This ensures that the retrieved music complements the semantic and emotional context of the visual or textual input, allowing the model to effectively capture complex musical features, such as melody and rhythm. Targeted retrieval for Coarse-Grained Control. In parallel to the broad retrieval, we conduct targeted retrieval within partitions of our dataset, specifically organized based on musical attributes labeled in Sec. 3.1. For instance, the 5 tempo partition is categorized into fast, medium, and slow subsets. This allows users to flexibly select songs that match specific attributes among genre, tempo, or mood partition. To retrieve fast-paced song, we query directly within the fast subset in the tempo partition. In each subset, the most suitable music piece is determined by computing the cosine similarity between the CLAP embeddings of the desired textual attribute, encoded as eattr }: desired, eaudio desired and {eaudio = arg max cos (cid:0)eattr (cid:1) . (4) k The embedding of the retrieved music eaudio is used as input in subsequent module, where it is integrated into the generation process. 3.3. Explicitly Conditioned Music Generation We first introduce our base music generation model, which leverages latent diffusion transformer (DiT) [42] to convert textual descriptions into music. Our model is built on the stable audio open framework [17], where music is encoded into compact latent space using pre-trained VAE, with Gaussian noise added to the latent representation. sequence of transformer blocks then processes the noisy input, with text features encoded by T5 and the current timestep injected into each block via cross-attention. The model is optimized using the standard diffusion objective with v-prediction [47]. Music ControlFormer. For retrieved music from broad retrieval, we draw inspiration from ControlNet [56, 59] since it is designed for structural guidance according to the conditions. However, considering original ControlNet is tailored for U-Net diffusion, we design Music ControlFormer for our diffusion transformer, which is shown in Fig. 3. Rather than duplicating the entire model, we replicate only the early transformer layers of the model to create the ControlFormer branch. This selective duplication maintains computational efficiency while allowing control signals to influence the foundational stages of generation, where structural and semantic alignment can be established most effectively. At each layer l, the main branch produces hidden states . We comhmain , and the ControlFormer produces hcontrol bine these hidden states by element-wise addition: hl = hmain + hcontrol (5) . To ensure stable training and prevent disrupting the pretrained models representations, we initialize the input and output convolution layers of the ControlFormer to zero, denoted as ConvZ (). The hidden states are computed as: hcontrol = ConvZ (Blockcontrol where mi represents the retrieved music embedding from broad retrieval and Encodercontrol indicates the sequential 1:l application of the first layers of the ControlFormer block. 0 (mi ))), (ConvZ (6) 1:l Method Inference Time Objective Metrics Subjective Metrics KLpasst FDopenl3 IB MP EC TC RC CMT [13] Video2music [26] VidMuse [51] M2UGen [34] VMB (ours) 3 min 1 min 13 min 40 20 52.76 103.56 56.48 60.41 48.84 269.63 533.46 187.13 180.72 105.84 8.54 5.26 22.09 15.58 21. 3.19 3.05 3.01 2.84 3.85 2.81 2.58 2.91 2.32 3.36 2.79 2.64 3.05 2.37 3.38 3.10 2.67 3.02 2.71 3.62 Table 1. Video-to-music generation performance on SymMV dataset. Up/down arrows indicate the desired direction for improvement. Stylization Module. Considering music from the targeted retrieval reflects specific user-specified attributes instead of fine-grained alignment, we introduce Stylization Module that effectively fuses the overall characteristic of the retrieved music with the generation process. We first add the textual description of the musical attribute to the end of the input prompt, which unlocks the ability to leverage language as an interface for guiding the overall style of the generated music. Then, the Stylization Module concatenates the CLAP embeddings of retrieved music with text and timestep embeddings to create unified conditional representation. This combination is then adjusted to match the text embedding spaces dimensions. We employ cross-attention to integrate the conditional representation into noisy music, to focus on stylistic cues from both the music and textual data. This method refines the alignment between the generated music and the specified attributes, enabling precise adjustments and yielding music that better fits the users requirements. 4. Experiments In this section, we conduct comprehensive evaluations of the proposed VMB framework on video-to-music (V2M), text-to-music (T2M), image-to-music (I2M) generation tasks, controllable generation, visuals-to-description generation, and ablation studies. We compare VMB with existing methods in zero-shot scenarios to ensure fair assessment. Furthermore, we validate the effectiveness of the models components through ablation studies. 4.1. Experimental Setup Model Training. For MMDM in Sec. 3.1, We use the InternVL2 model and AdamW optimizer with learning rate of 1e-6. Training uses local batch size of 2 with 8-step gradient accumulation. We adopt LoRA finetuning with the rank of 16 on 8 NVIDIA A800 GPUs trained for 10 epochs. During inference, we use top-k (k=50), nucleus sampling (p=1.0), and temperature 1.0. For ECMG in Sec. 3.3, We use DiT model with T5 conditioning and an audio autoencoder for audio-text alignment. The model has 24 layers, 1536 dimensions, and 24 heads. Audio is processed with Oobleck encoders/decoders with latent dim of 64. Training uses AdamW optimizer with learning rate of 1e-6 and weight decay of 0.001. We conduct training on 4 NVIDIA A100 GPUs with local batch size of 1 and 8-step gradient accumulation. More details are provided in Sec. B.1 Training Dataset. We use dataset of 512K music tracks with text labels, collected using the labeling method described in Sec. 3.1. We filtered these tracks by applying PAM score [12] threshold, resulting in subset of 54,112 high-quality tracks. Additionally, to support retrievalaugmented generation tasks, we incorporated curated subset of dynamic music videos from the DISCO-200K-highquality [29], along with videos collected by ourselves. Objective Metrics. We used objective metrics including KLpasst [28], FDopenl3 [9], CLAPScore [16], and ImageBind score (IB) [18]. KLpasst and FDopenl3 evaluate the music quality from its statistical similarity to real music data and perceptual audio quality. CLAPScore measures the alignment between generated music descriptions and video content. IB measures the cross-modal semantic alignment between videos or images and the corresponding generated music. All objective metrics are scaled by 100. 4.2. Video-to-music Generation We first evaluate the performance of our proposed VMB model on the task of video-to-music generation. Given an input video, models are expected to generate background music for the video with both music quality and videomusic alignment. We use SymMV [61] as our evaluation set, which is high-quality dataset specifically for video-tomusic generation, featuring diverse video segments and music genres. We utilize all available videos from the SymMV dataset as of November 1, 2024. For consistency in evaluation across different methods, all video segments were truncated to 30 seconds and downsampled to 360p resolution. This configuration was selected to accommodate the requirements of certain models employed in our experiments. Subjective Evaluation. As complementary to the objective evaluation, we follow previous methods [8, 13, 61] to conduct subjective evaluation. We use metrics of Musical Pleasantness (MP), Emotional Correspondence (EC), Thematic Correspondence (TC), Rhythmic Correspondence 6 Method Objective Metrics Subjective Metrics KLpasst FDopenl3 CLAPScore IB MP TMA Stable Audio Open [17] MusicGen [8] AudioLDM [33] M2UGen [34] VMB (ours) 42.89 46.89 99.85 49.03 37. 183.09 181.59 293.86 188.84 132.16 40.92 33.95 17.61 28.76 39.66 24.67 22.46 20.01 16.70 29.36 3.41 3.11 2.34 3.19 3.78 3.52 3.35 2.71 3.27 3.48 Table 2. Text-to-music generation performance on SongDescriber dataset. (RC), and Text-Music Alignment (TMA). Here EC, TC, and RC is only for the V2M task, while TMA is only for T2M. We generate background music for the same input video/text with different models, and send out questionnaires to ask participants to rate them in Likert Scale (from 1 to 5). We gathered total of 64 valid responses. The questionnaire takes about 50 minutes to complete. Further details are provided in the supplementary material. Baselines. We benchmark the performance of VMB against several state-of-the-art methods. For symbolic music generation, we include CMT [13] and Video2music [26], which represent the current best in generating MIDI files from visual inputs. In the category of audio music generation, we compare against VidMuse [51] and M2UGen [34]. VidMuse generates music tracks by progressively integrating visual cues, ensuring piecewise alignment with audiovisual elements, while M2UGen utilizes large language models alongside text-to-music model to transform visual inputs into musical outputs. Results. As shown in Tab. 1, VMB outperforms the baseline models on all the objective metrics. It achieves the lowest KLpasst and FDopenl3 scores of 48.84 and 105.84, respectively, indicating that the music generated by VMB is statistically closer to real-world music and exhibits higher perceptual quality. Although its IB of 21.62 is slightly below VidMuses 22.09, our model demonstrates effective semantic alignment between video and music. notable feature is VMBs reduced inference time of about 20 seconds, significantly faster than VidMuses 13 minutes. This improvement results from avoiding the extraction of redundant temporal features, while maintaining functionality by the retrieval module, making it ideal for applications requiring rapid music generation without compromising quality. Subjective evaluations highlight its ability to create music that emotionally and thematically aligns with visuals. The DMR module ensures effective synchronization with video dynamics, enhancing the audiovisual narrative and user experience. While our method does not explicitly control local rhythm, subjective evaluations show significant improvements in rhythm metrics compared to baselines. We attribute this to the potential drawbacks of excessive focus on local rhythm at the cost of overall rhythm, which can disrupt coherence and emotional alignment with the visuals, limiting the musics impact on the narrative. We conduct an additional manual review to ensure thorough filtering. After this process, we retain total of 279 text-music pairs. 4.3. Text-to-music Generation We further evaluate the proposed VMB model on the textto-music generation task. Given an input music description, each model is required to generate music that aligns with the description. We report our results on the SongDescriber dataset [39], high-quality collection specifically curated for text-to-music tasks. We note that samples containing vocal parts are filtered out to ensure instrumental coherence. Baselines. We compare VMB with several state-of-the-art text-to-music models. AudioLDM [33] utilizes latent diffusion models for general audio tasks. MusicGen [8] is state-of-the-art transformer-based model trained on musictext pairs. Stable Audio Open [17] generates high-fidelity audio using latent diffusion transformer conditioned on text. M2UGen [34] also supports text-to-music generation through its utilization of large language models. Results. As presented in Tab. 2, VMB outperforms baseline models across the objective metrics. It achieves the lowest KLpasst of 37.43 and the lowest FDopenl3 of 132.16, indicating that the music generated by VMB is statistically closer to real-world music distributions and exhibits higher perceptual quality. VMB also attains CLAPScore of 39.66, closely matching the ground truth of 40.34, highlighting its strong ability to capture the semantic content of text prompts. Additionally, VMB achieves the highest ImageBindScore of 29.36, further validating its robust performance in aligning text with music. Subjectively, VMB excels with the highest MP of 3.78, and TMA score of 3.48, which is slightly lower than the baselines score of 3.52, suggesting it generates musically coherent and contextually accurate compositions. The results demonstrate that VMB not only produces high-quality music but also aligns well with the given textual descriptions, outperforming existing state-of-the-art models in the text-to-music generation task. 7 4.4. Image-to-music Generation To demonstrate the versatility of our proposed framework, we evaluated the VMB model on the image-to-music (I2M) generation task. In this setting, models are required to generate music that semantically and emotionally aligns with given image. Evaluations were conducted using the MUImage [34] dataset, high-quality collection of image-music pairs curated for nuanced cross-modal generation. We sort the entire dataset alphabetically by file name and used the first 1,500 image-music pairs as the test set. Baselines. We benchmarked VMB against state-of-the-art models, including CoDi [50] and M2UGen [34]. CoDi, an any-to-any generation model, is optimized for flexible cross-modal tasks, including I2M, making it robust baseline for comparison. Results. Tab. 3 summarizes the performance of VMB, CoDi, and M2UGen on the I2M task. VMB demonstrates substantially lower KLpasst and FDopenl3 compared to the baselines, alongside higher Inception-Based Score (IB), indicating its ability to generate music that aligns closely with real-world distributions. Notably, although VMB is not explicitly trained on images, it effectively captures both the semantic and emotional content of visual inputs, underscoring its strong generalizability and its capacity to produce perceptually high-quality music from images. 4.5. Controllable Generation To assess the controllability of our model, we evaluate its capability to generate music with distinct attributes, specifically focusing on genre, instrument, mood, and tempo. We select contrasting attribute pairs, such as happy vs. nonhappy moods, and random sample 20 songs for each attribute pair. For each song, we generated 10 variations conditioned on the sampled song itself to assess the models capability to adjust each attribute independently. For each attribute, we report the average change in CLAPScore (), quantifying how well the generated music aligns with the specified textual description. Tab. 5 summarizes the average in CLAPScore across instrument, genre, and mood controls. Instrument control achieves the largest average change, indicating that the model effectively differentiates between instruments, likely due to CLAPs sensitivity to instrument features. Genre and mood control yield smaller values, suggesting that the model can modulate genre and emotional tone. We also conduct additional experiment in Sec. B.5 The results highlight the VMBs ability for attribute-specific control, demonstrating its adaptability to diverse inputs and effective modulation of musical attributes. 4.6. Visual-to-Description Generation We evaluated our MMDM model on subset of 8,042 dynamic videos from the DISCO-200K-high-quality Method KLpasst FDopenl3 IB CoDi [50] M2UGen [34] VMB (ours) 216.48 128.33 105.60 251.52 247.42 119.76 9.60 2.28 11. Table 3. dataset. Image-to-music generation performance on MUImage Model CLAPScore Attribute Change () GPT-4V [41] InternVL [6] MMDM 44.41 44.21 50.88 Instrument Genre Mood +11.46 +3.03 +4.14 Table 4. Video-to-Description Generation Performance. Table 5. Attribute control effectiveness measured by average change () in CLAPScore. dataset [29], selected by filtering out static or lyrics-based videos through pixel difference analysis and manual review. To assess the quality of the generated descriptions, we used the CLAPScore to measure the similarity between the original and generated textual descriptions in terms of their alignment with the corresponding music. As shown in Tab. 4, our MMDM model achieves CLAPScore of 50.88, outperforming the baseline models GPT-4V and InternVL, which scored 44.41 and 44.21, respectively. GPT-4V [41] is SOTA multimodal large language model (MLLM) that extends the capabilities of GPT4 [40] to handle both image and text inputs. While MLLMs excel at generating descriptive text from visual cues, GPT4V and InternVL are not specifically designed for musictext generation tasks, which may account for their lower performance in this context. These results confirm the superior description generation capability of the MMDM model, demonstrating its ability to produce more accurate and contextually relevant descriptions of music compared to the baselines. 4.7. Ablation Studies We conduct ablation studies to assess the individual contributions of two key components in our framework: broad retrieval (BR) and targeted retrieval (TR). The ablations are performed on the video-to-music generation task using the SymMV dataset. The results of these ablations are presented in Tab. 6, demonstrate that combining broad retrieval (BR) and targeted retrieval (TR) yields the best performance across all metrics. Specifically, the combination achieves the lowest KLpasst (75.29) and FDopenl3 (177.27), indicating improved thematic alignment and perceptual quality. BR improves alignment with the input content, while TR enhances creativity, reflected in the higher IB (24.70). These findings highlight the complementary roles of BR and TR in improving music generation. 8 BR TR KLpasst FDopenl3 IB 75.29 91.89 91.07 96.42 177.27 199.74 387.14 360.29 24.70 20.73 20.51 14. Table 6. Ablation of model components on video-to-music generation with SymMV dataset. BR, TR represent broad retrieval and target retrieval respectively. When either broad or targeted retrieval is omitted, the performance decreases, particularly in terms of KLpasst and FDopenl3, further emphasizing the complementary nature of these components. The ablation results demonstrate that both broad and targeted retrieval are crucial for achieving high-quality, relevant, and creative music generation in the video-to-music task. 5. Broader Impacts, Limitations, and Future"
        },
        {
            "title": "Works",
            "content": "Broader Impacts. The VMB framework introduces several significant impacts across various domains. Primarily, VMB enhances accessibility in entertainment technologies such as gaming and virtual reality by enabling the automatic generation of emotionally resonant background music, which could improve user engagement and accessibility for people with diverse abilities. However, the technology also presents potential negative impacts. The automation of music generation might reduce the need for human composers in certain contexts, potentially affecting their livelihoods. Moreover, biases in the training data could lead to outputs that do not adequately represent the diversity of global musical expressions, potentially introducing unfairness in musical representation. Limitations. The VMB framework represents significant advancement in multimodal music generation; however, it is not devoid of limitations. The models effectiveness heavily relies on the diversity and quality of the dataset it is trained on. Presently, available datasets may not adequately represent the vast array of musical styles and cultural expressions, thus limiting the systems ability to produce broad spectrum of musical outputs. Moreover, the challenge of accurately translating complex emotional and thematic nuances across different modalities persists, as the system occasionally fails to capture the depth and subtlety inherent in human compositions. Future Works. Future enhancements to the VMB framework should focus on several key areas. Broadening the diversity of the dataset to encompass greater variety of musical styles and cultural expressions would considerably enhance the models generative capabilities. Improving the systems understanding of and ability to accurately translate intricate emotional and thematic nuances between modalities would make the technology more effective and emotionally impactful. Integrating music theory into the music generation process could provide more robust framework for generating musically coherent outputs. 6. Conclusion In this paper, we propose novel multi-modal music generation system named VMB to address the challenges of existing methods. VMB accepts diverse inputs, including text, image, and video, and integrates them effectively by using text and music as bridge. Our system enables finegrained control over key musical elements, enabling users to direct the generation process according to their preferences. We conduct extensive experiments to demonstrate that VMB generates music that aligns well with multimodal inputs and exhibits high controllability, outperforming current state-of-the-art methods. VMB holds significant potential for multimedia applications, facilitating personalized and contextually rich music generation across domains such as entertainment and interactive media."
        },
        {
            "title": "References",
            "content": "[1] Andrea Agostinelli, Timo Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. arXiv preprint Musiclm: Generating music from text. arXiv:2301.11325, 2023. 1, 2 [2] Mauricio Aguilar Rangel, Alice Bedwell, Elisa Costanzi, Ross Taylor, Rosaria Russo, Goncalo JL Bernardes, Stefano Ricagno, Judith Frydman, Michele Vendruscolo, and Pietro Sormanni. Fragment-based computational design of antibodies targeting structured epitopes. Science Advances, 8(45):eabp9540, 2022. 3 [3] Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18181826, 2024. 3 [4] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William Cohen. Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022. 3 [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Internvl: Scaling up vision foundation modJifeng Dai. els and aligning for generic visual-linguistic tasks. arXiv: 2312.14238, 2023. 14 [6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv:2404.16821, 2024. 2, 3, 4, 8, 9 [7] Sanjoy Chowdhury, Sayan Nag, KJ Joseph, Balaji Vasan Srinivasan, and Dinesh Manocha. Melfusion: Synthesizing music from image and language cues using diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2682626835, 2024. 1, 2 [8] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36, 2024. 1, 2, 6, 7 [9] Aurora Linh Cramer, Ho-Hsiang Wu, Justin Salamon, and Juan Pablo Bello. Look, listen, and learn more: Design choices for deep audio embeddings. In ICASSP, pages 3852 3856. IEEE, 2019. 6 [10] Tri Dao. Flashattention-2: Faster attention with betarXiv preprint ter parallelism and work partitioning. arXiv:2307.08691, 2023. 14 [11] Alexandre Defossez, Nicolas Usunier, Leon Bottou, and for music Francis Bach. sources with extra unlabeled data remixed. arXiv preprint arXiv:1909.01174, 2019. 4 Demucs: Deep extractor [12] Soham Deshmukh, Dareen Alharthi, Benjamin Elizalde, Hannes Gamper, Mahmoud Al Ismail, Rita Singh, BhikPam: Prompting audiosha Raj, and Huaming Wang. arXiv language models for audio quality assessment. preprint arXiv:2402.00282, 2024. 6, 12 [13] Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan Zhu, Zexin He, Hongming Liu, and Shuicheng Yan. Video background music generation with controllable music transformer. In MM, 2021. 1, 2, 3, 6, 7, 12, 15 [14] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang. Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. In AAAI, 2018. [15] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 4, 12 [16] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap learning audio concepts from natural language supervision. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 4, 5, 6, 12 [17] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. arXiv preprint arXiv:2407.14358, 2024. 1, 2, 5, 7, 14, 15 [18] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind one embedding space to bind them all. In CVPR, pages 1518015190, 2023. 6 [19] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pretraining. In International conference on machine learning, pages 39293938. PMLR, 2020. 3 [20] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ICLR, 2022. 4, 14 In [21] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memIn Proceedings of the IEEE/CVF conference on comory. puter vision and pattern recognition, pages 2336923379, 2023. 3 [22] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew Dai, Matthew Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer: Generating music with long-term structure. In ICLR, 2019. 2 [23] Qingqing Huang, Daniel Park, Tao Wang, Timo Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2music: Textconditioned music generation with diffusion models. arXiv preprint arXiv:2302.03917, 2023. 2 [24] Yu-Siang Huang and Yi-Hsuan Yang. Pop music transformer: Beat-based modeling and generation of expressive pop piano compositions. In MM, 2020. [25] Insu Jang, Zhenning Yang, Zhen Zhang, Xin Jin, and Mosharaf Chowdhury. Oobleck: Resilient distributed trainIn Proceeding of large models using pipeline templates. ings of the 29th Symposium on Operating Systems Principles, pages 382395, 2023. 14 [26] Jaeyong Kang, Soujanya Poria, and Dorien Herremans. Video2music: Suitable music generation from videos using an affective multimodal transformer model. Expert Systems with Applications, page 123640, 2024. 1, 3, 6, 7 [27] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:28802894, 2020. 12 [28] Khaled Koutini, Jan Schluter, Hamid Eghbal-Zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021. 6 [29] Luca Lanzendorfer, Florian Grotschla, Emil Funke, and Roger Wattenhofer. Disco-10m: large-scale music dataset. Advances in Neural Information Processing Systems, 36, 2024. 6, 8, 14 [30] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. [31] Sizhe Li, Yiming Qin, Minghang Zheng, Xin Jin, and Yang Liu. Diff-bgm: diffusion model for video background music generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27348 27357, 2024. 3 [32] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv: 2310.03744, 2023. 3 [33] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. 2, 7 [34] Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, and Ying Shan. M2ugen: Multi-modal music understanding and generation with the power of large language models. arXiv preprint arXiv:2311.11255, 2023. 1, 3, 6, 7, 8, 14, 15 [35] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented clasIn CVPR, pages sification for long-tail visual recognition. 69596969, 2022. 3 [36] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [37] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 15 [38] Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. arXiv preprint arXiv:2410.08202, 2024. 3 [39] Ilaria Manco, Benno Weck, Seungheon Doh, Minz Won, Yixiao Zhang, Dmitry Bogdanov, Yusong Wu, Ke Chen, Philip Tovstogan, Emmanouil Benetos, et al. The song describer dataset: corpus of audio captions for musicand-language evaluation. arXiv preprint arXiv:2311.10057, 2023. 7, 12 [40] OpenAI. GPT-4 technical report. arXiv: 2303.08774, 2023. 8 [41] OpenAI. Gpt-4v(ision) system card. 2023. 8 [42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, pages 41954205, 2023. 5, 14 [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21:140:1140:67, 2020. 14 [45] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. 14 [46] Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck. hierarchical latent vector model for learning long-term structure in music. In ICML, 2018. 2 [47] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 5 [48] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman. Knndiffusion: Image generation via large-scale retrieval. arXiv preprint arXiv:2204.02849, 2022. 3 [49] Kun Su, Judith Yue Li, Qingqing Huang, Dima Kuzmin, Joonseok Lee, Chris Donahue, Fei Sha, Aren Jansen, Yu Wang, Mauro Verzetti, et al. V2meow: Meowing arXiv preprint to the visual beat via music generation. arXiv:2305.06594, 2023. [50] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via composable diffusion. NeurIPS, 36, 2024. 1, 3, 8 [51] Zeyue Tian, Zhaoyang Liu, Ruibin Yuan, Jiahao Pan, Xiaoqiang Huang, Qifeng Liu, Xu Tan, Qifeng Chen, Wei Xue, and Yike Guo. Vidmuse: simple video-to-music generation framework with long-short-term modeling. arXiv preprint arXiv:2406.04321, 2024. 3, 6, 7, 12 [52] Yajie Wang, Mulin Chen, and Xuelong Li. Continuous IEEE Transacemotion-based image-to-music generation. tions on Multimedia, 2023. 1 [53] Zhenwei Wang, Tengfei Wang, Gerhard Hancke, Ziwei Liu, and Rynson WH Lau. Themestation: Generating themeaware 3d assets from few exemplars. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 3 [54] Rundi Wu and Changxi Zheng. ate 3d shapes from single example. arXiv:2208.02946, 2022. 3 Learning to generarXiv preprint [55] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and TatSeng Chua. Next-gpt: Any-to-any multimodal llm. arXiv: 2309.05519, 2023. 1, 3 [56] Shih-Lun Wu, Chris Donahue, Shinji Watanabe, and Nicholas Bryan. Music controlnet: Multiple time-varying controls for music generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:26922703, 2024. 5 [57] Li-Chia Yang, Szu-Yu Chou, and Yi-Hsuan Yang. Midinet: for network In ISMIR, 2017. convolutional symbolic-domain music generation. adversarial generative [58] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv: 2309.17421, 9, 2023. 3 [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 5 [60] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to augment large language models. arXiv preprint arXiv:2310.07554, 2023. 3 [61] Le Zhuo, Zhaokai Wang, Baisen Wang, Yue Liao, Chenxi Bao, Stanley Peng, Songhao Han, Aixi Zhang, Fei Fang, and Si Liu. Video background music generation: Dataset, method and evaluation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15637 15647, 2023. 1, 3, 6, 11 Figure 5. Histogram of music duration in the training dataset. Figure 4. Distribution of PAM Scores across the raw training dataset. A. Dataset Analysis We utilize four self-curated datasets for the following purposes: 1) training the ECMG module; 2) constructing the DMR retrieval dataset; 3) conducting subjective and qualitative evaluations; and 4) assessing the performance of the MMDM module. Additionally, we document modifications made to existing datasets, namely SymMV [61] and SongDescriber [39]. A.1. ECMG Training Dataset As outlined in Sec. 4.1, we collect total of 512K music tracks. Tracks containing vocal components are identified using PANN [27] and CLAP [16], based on pre-defined tags such as vocal, choir, and human voice. Scores are computed between each track and the target tags, and threshold of 0.1 is selected to identify tracks with vocal components. This threshold is determined through manual review process, where we listen to subset of music tracks to ensure the balance between accurately identifying vocal parts and minimizing false positives. The decision to remove tracks with human voices follows previous works [51], and is guided by three key considerations: 1) to lower the complexity of modeling voice components; 2) to enable the model to focus more effectively on instrumental music; and 3) to align with the primary goal of video background music generation [13, 61], which typically excludes vocal elements. Additionally, we filter out low-quality tracks by applying the PAM [12] score with threshold of 0.95. After filtering, we obtain 54,112 highquality tracks. The distribution of PAM score in the whole dataset is illustrated in Fig. 4. The selected tracks are annotated using the methodology described in Sec. 3.1. Candidate tags are pre-defined by musicians who serve as domain experts to align with commonly used labels in music generation while minimizing the effect of long-tail distribution. The detailed annotations are provided in the file label.json. Figure 6. Histogram of text word counts in the training dataset. the experts taxonomy. Instruments are first categorized into five primary groups: strings, keyboards, wind instruments, percussion instruments, and others. To address the issue of long-tail distributions, rare tags are combined into broader categories. For instance, instruments like the double bass, which are seldom played independently, are treated as indivisible units. Therefore, we use strings to replace double bass. This approach ensures robust categorization while accurately reflecting the practical usage of instruments in compositions. Other tagging principles, which align with the general design philosophy of instrument categorization, are omitted here for brevity. To generate natural descriptions, we employ the Llama3.1-8B-Instruct [15] model to generate one-sentence descriptions for each music track. The system prompt is: You are professional music expert. Here are tags about this music. You need to generate one-sentence description for this music. Only generate the description without any other information. Metadata tags were supplied via the user prompt. Fig. 5 and Fig. 6 illustrate the distribution of music durations and the corresponding descriptions, providing insight into the characteristics of the dataset. A.2. MMDM Training and Retrieval Dataset We have introduced the methodology of collecting the 3.1 Here, MMDM training and retrieval dataset in Sec. we provide other details."
        },
        {
            "title": "We use instrument tagging as an example to illustrate",
            "content": "First, we scrape the hottest singers and their songs 12 Figure 7. Distribution of mood tags across the retrieval dataset. This histogram shows the frequency of various mood categories, illustrating the emotional diversity captured in our data. Figure 9. Histogram of instrument tags in our retrieval dataset. This figure shows the range of musical instruments represented, underscoring the datasets comprehensive coverage of instrumental music. Figure 8. Genre distribution within the retrieval dataset. This bar graph reflects the variety of music genres represented, indicating the datasets broad applicability for genre-specific retrieval tasks. from Spotify. We use YoutubeAPI1 to search for their original official music video with keywords in the format of {singer} + {song name} + Official Music Video. We scrape all the music videos with their Youtube description boxes and top-10 comments. We also use Shazam2 to get other metadata. Finally we obtain the primary dataset introduced in Sec. 3.1 We use methodology stated in Sec. 3.1 and Sec. A.1 to label each music track. Eventually our dataset contains 24,719 video-text-music pairs. In targeted retrieval detailed introduced in Sec. 3.2, we partition our whole datasets. We follow three standards and get three different partitions, i.e., genre, tempo, and mood partition. We use the labeled tags to partition the whole dataset. Distribution of each attribute is shown in Fig. 7, 8, 9, and 10. Fig. 11 shows the distribution of audio durations in the dataset. The majority of the audio files have durations concentrated within specific range, suggesting consistent lengths across the dataset. The associated text data is char1https://developers.google.com/youtube/v3 2https://www.shazam.com/ Figure 10. Density curve of BPM across the retrieval dataset. This plot illustrates the distribution of Beats Per Minute, showcasing the tempo range covered in our collection. Figure 11. Histogram of audio durations in retrieval dataset. This shows the distribution of song lengths in the dataset. acterized by word count distribution in Fig. 12, showing variety of text lengths, with most falling in the mid-range. Finally, the lexical diversity, representing vocabulary usage variation, is displayed in Fig. 13. Most texts demonstrate high lexical diversity, indicating rich vocabulary usage across the dataset. 13 curated subset of 8,042 videos that demonstrated sufficient motion suitable for our analysis. A.5. SongDescriber and MUImage While Stable Audio Open [17] claimed to have filtered the SongDescriber dataset to exclude tracks with vocal components, For the MUImage dataset [34], as the authors do not specify valid test set but only provided the training set, we sort the entire dataset alphabetically by file name and used the first 1,500 image-music pairs as the test set. Consequently, the evaluation of M2UGen on MUImage is actually conducted on its training set. The specific video-music / text-music pairs used in our study are documented in the files SymMV.csv and SongDescriber.csv. B. Experiment Details In this section, we provide details of the experiments in the main paper. B.1. Implementation Details 3.1, We utilize the For MMDM module stated in Sec. InternVL2 [6] architecture with mixed precision training (bfloat16) and the AdamW optimizer (learning rate: 1e-6, cosine decay). Top-k sampling (k=50) and nucleus sampling (p=1.0) are used for generation, with temperature of 1.0. The model features FlashAttention v2 [10], 32 layers, and 16 attention heads, paired with vision backbone (Intern-ViT-6B) [5] at 448x448 resolution. Training is performed using batch size of 2 with gradient accumulation over 8 steps. Fine-tuning is conducted with LoRA [20] at rank 16, utilizing 8 NVIDIA A800-SXM4-80GB GPUs over 3 days for total of 10 epochs. In ECMG module introduced in Sec. 3.3, We leverage DiT [42] model with T5 [44] conditioning and an audio autoencoder for fine-tuning on audio-text alignment tasks. The architecture includes 24-layer continuous transformer with 1536 embedding dimensions and 24 attention heads. Audio inputs are preprocessed via an autoencoder featuring Oobleck [25] encoders/decoders with multi-scale channel configurations and latent dimension of 64. Training uses AdamW [36] (learning rate: 1e-6, weight decay: 0.001) with an InverseLR scheduler. The model operates at 44.1kHz sample rate with batch size of 1 and gradient accumulation over 8 steps. Mixed precision (16bit) training is employed, using DeepSpeed [45] for optimization with 4 NVIDIA A100-PCIE-40GB GPUs. Conditioning involves audio features, time attributes, and textual prompts (via T5-base, max length: 128). Figure 12. Histogram of text word counts in retrieval dataset. This represents the distribution of word counts in the associated text data. Figure 13. Histogram of lexical diversity scores in retrieval dataset. This shows the variation in vocabulary usage across text samples. A.3. Subjective Evaluation Dataset To evaluate our model across diverse video contexts, we construct dataset encompassing seven distinct video categories: Scene/Vlog, Documentary, Advertisement, Movie, Game, Anime, and Sports. These categories are selected to ensure comprehensive benchmark that reflects wide range of video types and their corresponding music needs. Each video is manually reviewed to ensure quality, resulting in final dataset of 35 high-quality videos. We apply the following criteria for video selection: Duration: Videos are limited to maximum length of three minutes to ensure efficient evaluation. Content Quality: Only classic or high-quality videos are included, reflecting diverse and impactful visual themes. A.4. Visuals-to-Description Evaluation Dataset We downloaded all available videos from the Disco-200Khigh-quality dataset [29] as of July 30, 2023. Upon review, we noted that many videos consisted of static images or were lyrics-based music videos. To filter for dynamic content, we calculated the average pixel difference between consecutive frames to identify static videos. Videos close to the threshold arere manually reviewed to confirm their dynamic nature. Following this rigorous selection process, we 14 Figure 14. Screenshot of the user study questionnaire in subjective evaluation. B.2. Video-to-Music Generation We evaluate video-to-music generation using several stateof-the-art models, adhering to their default settings for fair comparison. Due to the sequence length limitation of M2UGen [34], all samples are truncated to 30 seconds. Additionally, to accommodate the resolution constraint of CMT [13], all videos are resized to 360p. We follow the default setting in each generation model. For VMB, we set standard sample rate of 44,100 Hz and adjust the CFG scale to 7 for balancing conditioning influence, with 100 steps for detailed noise reduction. Our sampler, DPM++ 3M SDE [37], manages noise sampling across sigma range from 0.03 to 1000. In our experiments, we use server with dual Intel Xeon Processors (Icelake) with 64 cores each and four NVIDIA A100-PCIE-40GB GPUs, under KVM virtualized Linux environment. These configurations ensure the reproducibility of our inference time measurements. B.3. Text-to-Music Generation For text-to-music models, we also adhere to their default settings for fair comparison. Due to the maximum sequence length limitation of M2UGen [34], all generated music samples are capped at 30 seconds. VMB follows the parameters outlined in [17]. It uses 44,100 Hz sample rate, CFG scale of 7, 100 steps for diffusion, and DPM++ 3M SDE sampler with sigma range from 0.03 to 1000. B.4. Subjective Evaluation For the subjective evaluation, we provide each participant with two sets of evaluations: one for video-to-music generation and another for text-to-music generation. Each set consists of seven groups, corresponding to the seven categories in our subjective evaluation dataset. For each category, video is randomly sampled. Each group include five Table 7. Average BPM of music generated under varying tempo conditions."
        },
        {
            "title": "Fast\nMedium\nSlow",
            "content": "143.55 122.64 93.88 music pieces, each generated by one of the models. For video-to-music (V2M) generation, participants are briefed on the purpose of the study and instructed to evaluate how well the generated music matched the video in terms of mood, thematic alignment, and overall enhancement of the viewing experience. Each video is presented seven times, each time paired with background music track generated from different model. The order of music presentations is randomized to eliminate order effects. After watching all versions of video, participants are asked to rate each version using Likert scale, i.e. rate from 1 to 5, considering factors such as emotional impact, thematic coherence, and suitability for the videos content. Additionally, participants are encouraged to provide qualitative feedback explaining their preferences, highlighting specific emotional or thematic factors that influenced their choices. For the V2M task, we utilize the following metrics to guide participant evaluations: Musical Pleasantness (MP): The aesthetic quality of the music, independent of context. Emotional Correspondence (EC): How well the music conveys the intended emotions of the video. Thematic Correspondence (TC): The alignment between the videos theme and the generated music. Rhythmic Correspondence (RC): The synchronization between the videos motion and the musics rhythm. The evaluation process for text-to-music (T2M) generaC. Demos Due to PDF limitations, we only showcase the video-todescription generation demo here. Demos for other generation will be uploaded on https://github.com/ wbs2788/VMB. To evaluate our models performance, we sampled 10 frames from each video and provided the following prompt to the models: Based on the emotional tone, pacing, and visuals of this video, how would you compose the music? Provide one-sentence summary of the key musical elements that you use. This method ensures that the generated music aligns well with the emotional and visual characteristics of the video. Samples of these music descriptions are displayed in Table 8. Additionally, the content shown in Figure 1 of the main paper, which illustrates how poem by P.B. Shelley, The Flower that Smiles Today, can also be transformed into music description, is also generated using our model. tion follows similar structure. Participants are provided with textual prompts and asked to evaluate the generated music based on how well it reflects the mood, style, and thematic elements described in the text. Each textual prompt is paired with five music tracks generated from five models, and participants rate the tracks using the same Likert scoring system. Feedback is collected to identify specific strengths and weaknesses of the generated music in conveying textual meaning. For the T2M task, we select the following metrics: Musical Pleasantness (MP): The aesthetic quality of the music, independent of context. Text-Music Alignment (TMA): How effectively the music captures the mood, style, and themes described in the text. Each participant rated total of 35 music tracks per task (V2M and T2M), completing the questionnaire in approximately 50 minutes. Participants also provide qualitative feedback, explaining their preferences and highlighting emotional or thematic factors influencing their ratings. The reward for each participant is $10. We finally gather 64 valid responses from social media. notable result is observed in the RC metric. Despite not explicitly introducing rhythm-focused features, our model achieves superior performance compared to baseline methods. This suggests that an alternative approach, which emphasizes broader perspective rather than heavily focusing on local rhythmic features, can also be effective. Overemphasis on local rhythms may sometimes challenge the overall coherence of the music and its alignment with the broader narrative. By adopting balanced approach, our model maintains rhythmic flow while aligning closely with the intended emotional context, leading to an enhanced audiovisual experience. We also provide screenshot of the questionnaire with full text of instruction in Fig. 14. B.5. Controllability Experiment To further evaluate the controllability of our model, we test its ability to adjust the tempo of the generated music. Similar to other music attributes such as genre, instrument, and mood, we report the average change in tempo on the generated variations to quantify the models performance. For this experiment, we categorize the dataset into distinct beats per minute (BPM) groups: Fast, Medium, and Slow. We randomly sample 20 songs for each BPM group and generate 10 variations for each song, conditioned on the sampled song itself. This setup allows us to assess the models capability to independently control tempo while maintaining the overall coherence of the generated music. We calculate the average BPM across the 200 generated songs per group (20 songs 10 variations) to measure how well the generated music aligns with the expected tempo adjustments. The results are summarized in Table 7. 16 Table 8. Samples of visual-to-description generation. Image Description GPT-4 Evaluation vibrant, energetic, and epic soundtrack featuring dynamic blend of strings, brass, and orchestral elements, perfectly capturing sense of adventure and excitement. dynamic and emotive soundtrack featuring mix of guitar, bass, strings, and orchestra, perfectly capturing range of moods from action-packed and adventurous to melancholic and romantic. fast-paced, dynamic electronic soundtrack featuring pulsating rhythms, vibrant synthwave beats. gentle piano melody, accompanied by soft strings, to evoke sense of tenderness. gentle, melancholic melody, featuring soft piano and strings, to evoke the serene yet poignant atmosphere. sense of awe and wonder, using combination of orchestral strings, and electronic soundscapes. slow, eerie, and melancholic melody, using combination of dissonant chords and haunting vocal line to evoke the sense of despair and isolation. Score: 5 / 5 Reason: Emotion Match: The energy and vibrancy of the description match perfectly with the lively festival scene. Scene Association: The use of strings, brass, and orchestral elements effectively aligns with the celebratory and grand setting. Conclusion: The description is highly appropriate for this image, requiring no further improvement. Score: 4.5 / 5 Reason: Emotion Match: The descriptions adventurous tone works well for the desert landscape. However, the melancholic and romantic portion slightly detracts from the overall alignment. Scene Association: The instrumentation (guitar, bass, and orchestra) complements the sense of vastness and exploration in the desert. Conclusion: more focused description on adventure and solitude could enhance the fit. Score: 5 / 5 Reason: Emotion Match: The fast-paced, electronic, and vibrant tone perfectly reflects the energy, dynamism, and modernity of the urban night scene. Scene Association: The use of synthwave beats aligns exceptionally well with the futuristic, neon-lit visuals. Conclusion: The updated description captures the essence of the image flawlessly, making it an excellent match. Score: 5 / 5 Reason: Emotion Match: The gentle piano melody perfectly evoke the tenderness and nostalgia expressed in the characters emotional moment. Scene Association: The use of soft strings aligns with the intimate and heartfelt nature of the scene, enhancing the emotional depth. Conclusion: The description is highly appropriate for this image, requiring no further improvement. Score: 4.5 / 5 Reason: Emotion Match: The melancholic melody matches the serene and poignant atmosphere of the comet-lit sky. Scene Association: The inclusion of soft piano and strings reflects the calmness and wonder of the scene but does not fully emphasize the awe-inspiring grandeur of the comet. Conclusion: While the description aligns well with the scene, adding sense of scale and majesty could enhance the match. Score: 5 / 5 Reason: Emotion Match: The energy and vibrancy of the description match perfectly with the lively festival scene. Scene Association: The use of strings, brass, and orchestral elements effectively aligns with the celebratory and grand setting. Conclusion: The description is highly appropriate for this image, requiring no further improvement. Score: 5 / 5 Reason: Emotion Match: The slow, eerie melody and dissonant chords align seamlessly with the despair and isolation depicted in The Scream. Scene Association: The dissonant chords effectively complements the paintings unsettling and surreal nature. Conclusion: The description accurately reflects the psychological intensity of the image, requiring no further improvement."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Institute of Information Engineering, Chinese Academy of Sciences",
        "MT Lab, Meitu Inc.",
        "School of Cyberspace Security, University of Chinese Academy of Sciences",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "University of Edinburgh"
    ]
}