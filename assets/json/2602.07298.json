{
    "paper_title": "Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation",
    "authors": [
        "Benyu Zhang",
        "Qiang Zhang",
        "Jianpeng Cheng",
        "Hong-You Chen",
        "Qifei Wang",
        "Wei Sun",
        "Shen Li",
        "Jia Li",
        "Jiahao Wu",
        "Xiangjun Fan",
        "Hong Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) represent a promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces a novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating a curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform ($+130\\%$ on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish a foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 2 8 9 2 7 0 . 2 0 6 2 : r Principled Synthetic Data Enables the First Scaling Laws for LLMs in Recommendation Benyu Zhang1, Qiang Zhang1, Jianpeng Cheng, Hong-You Chen, Qifei Wang, Wei Sun, Shen Li, Jia Li, Jiahao Wu, Xiangjun Fan, Hong Yan 1Equal contributions Large Language Models (LLMs) represent promising frontier for recommender systems, yet their development has been impeded by the absence of predictable scaling laws, which are crucial for guiding research and optimizing resource allocation. We hypothesize that this may be attributed to the inherent noise, bias, and incompleteness of raw user interaction data in prior continual pre-training (CPT) efforts. This paper introduces novel, layered framework for generating high-quality synthetic data that circumvents such issues by creating curated, pedagogical curriculum for the LLM. We provide powerful, direct evidence for the utility of our curriculum by showing that standard sequential models trained on our principled synthetic data significantly outperform (+130% on recall@100 for SasRec) models trained on real data in downstream ranking tasks, demonstrating its superiority for learning generalizable user preference patterns. Building on this, we empirically demonstrate, for the first time, robust power-law scaling for an LLM that is continually pre-trained on our high-quality, recommendation-specific data. Our experiments reveal consistent and predictable perplexity reduction across multiple synthetic data modalities. These findings establish foundational methodology for reliable scaling LLM capabilities in the recommendation domain, thereby shifting the research focus from mitigating data deficiencies to leveraging high-quality, structured information. Date: February 16, 2026 Correspondence: Benyu Zhang at byzhang@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) represent promising frontier for recommender systems, offering advanced sequence modeling capabilities and extensive world knowledge that can transcend the limitations of traditional embedding-table architectures Kang and McAuley (2018); Zhai et al. (2024). By representing users, items, and interactions within unified linguistic space, LLMs can leverage textual descriptions to generate more sophisticated, content-aware, and explainable recommendations Rajput et al. (2023); Chen et al. (2024); He et al. (2025). The development of such systems at scale, however, requires the establishment of predictable scaling lawsindispensable instruments for navigating the substantial investments in data, computation, and engineering that modern LLM development demands Hoffmann et al. (2022). Yet, despite their established significance in NLP, no robust scaling laws have been established for the continual pre-training (CPT) of LLMs within the recommendation domain Zhou et al. (2025a); He et al. (2025), compelling researchers to depend on costly trial-and-error methodologies. Prior work on scaling LLMs for recommendation has taken model-centric Yan et al. (2025), distillationcentric Lai et al. (2025), or system-centric approaches, while still relying on raw user interaction logs as the primary data source. However, such logs are characterized by systemic flaws: noise, sparsity, andmost criticallypervasive biases including position bias, popularity bias, and exposure bias Chen et al. (2021); Dooley et al. (2023). Recent theoretical and empirical work has shown that scaling laws only manifest when data has sufficient quality and diversity Yang et al. (2024); Allen-Zhu and Li (2024a). Indeed, the PLUM framework He et al. (2025) encountered suboptimal scaling behavior where 3B parameter model failed to consistently outperform its 900M counterparta clear symptom of training on information-poor, structurally biased data."
        },
        {
            "title": "The central insight of this work is that prior scaling failures were not primarily algorithmic but stemmed",
            "content": "1 from deficient data paradigm. Rather than designing models robust to data imperfections, we shift focus to constructing high-quality, pedagogical data engineered to teach the model the principles of recommendation in structured, unbiased manner. We introduce novel, layered synthetic data framework that systematically decouples true user preference signals from system-induced artifacts. Layer 1 establishes foundational knowledge through item-text alignment and collaborative filtering data, while Layer 2 generates unbiased user interaction histories via graph-based random walks that have no intrinsic concept of position or presentation order. We provide powerful, direct evidence for the utility of this framework. First, standard sequential recommendation models (GRU4Rec Hidasi et al. (2016), NARM Li et al. (2017), STAMP Gao et al. (2025), and SASRec Kang and McAuley (2018)) trained on our synthetic data achieve significantly higher Recall@K than models trained on real data across all cutoff points, demonstrating that our data captures more generalizable user preference patterns. Second, we empirically demonstrate, for the first time, robust power-law scaling for LLMs continually pre-trained on this data. Across model scales from 0.6B to 8B parameters trained on 163B tokens, we observe consistent scaling behavior characterized by the law L(D) = + Dα. Third, ablation studies reveal an asymmetric synergy between data layers: including collaborative filtering data alongside user interaction histories reduces asymptotic UIH loss by 31% (L = 0.66 vs. 0.95), while the reverse transfer does not holddemonstrating that our layered curriculum provides complementary, non-redundant learning signals. Our quantitative analysis reveals clear hierarchy of learning efficiency across data modalities. User Interaction History (UIH) data exhibits exceptional scaling exponents (α 0.450.59), indicating the model continues to benefit substantially from additional training tokens. Collaborative Filtering (CF) data shows strong scaling (α 0.35), followed by Item-Text alignment (α 0.15). In addition, our experiment has shown introducing CF data helped the learning of UIH data. These findings establish foundational methodology for reliable, predictable LLM development in the recommendation domain, providing practitioners with the first quantitative roadmap for resource allocation and performance forecasting. This paper makes the following principal contributions: layered synthetic data framework that transforms noisy, biased user interaction logs into high-quality curriculum. Layer 1 grounds semantic and collaborative knowledge through item-text alignment and association-rule data; Layer 2 generates unbiased user interaction histories via graph-based random walks that eliminate position and popularity biases. Direct empirical validation of data utility: We demonstrate that standard sequential models trained exclusively on our synthetic data outperform models trained on real data in downstream ranking tasks (Recall@10, @100, @1000), providing strong evidence that our curriculum captures more generalizable user preference patterns than raw interaction logs. The first scaling laws for LLMs in recommendation: Across model scales from 0.6B to 8B parameters trained on 163B tokens, we establish robust power-law scaling across seven evaluation domains. User interaction history exhibits the strongest scaling (α 0.450.59), followed by collaborative filtering (α 0.35) and item-text alignment (α 0.15), revealing hierarchy of learning efficiency that provides quantitative roadmap for data curation and resource allocation. Discovery of asymmetric cross-domain transfer: Ablation studies reveal that collaborative filtering data provides complementary signals that significantly improve user behavior modeling (L reduced from 0.95 to 0.66, 31% improvement), while the reverse transfer does not hold. This validates the pedagogical design of our layered curriculum."
        },
        {
            "title": "2.1 Scaling Laws in Recommender Systems",
            "content": "Scaling raw Kaplan et al. (2020) characterize how models performance predictably (e.g., loss on evaluation set) improves as three key factors are increased: model size in the number of parameters (N ); dataset size 2 (D), or the number of training tokens; and the total compute (C in FLOPS, which could be estimated as = 6N for transformer) used for training. The scaling law could be written as: L(N, D) = AN α + BDβ + (1) is the loss and is some constants. Chinchilla scaling raw Hoffmann et al. (2022) shows for fixed compute budget, model size and dataset size must be scaled in tandem to achieve optimal performance. However no such scaling law is yet established in recommendation models and the challenge of achieving predictable scaling for recommendation models has recently become central focus of industrial and academic research. Several high-profile approaches have been proposed, each with distinct philosophy. The Large User Model (LUM) paradigm from Yan et al. (2025) introduces novel training task, \"next-conditionitem prediction,\" designed to better capture contextual user preferences. This approach is fundamentally model-centric, seeking to unlock scaling through more sophisticated model architecture and training objective while still relying on raw user interaction logs as its primary data source. Another approach, the Stacked Unified Attention Network (SUAN) Lai et al. (2025), focuses on the task of Click-Through Rate (CTR) prediction. The authors first construct highly accurate, scalable \"teacher\" model and then use knowledge distillation to transfer its capabilities to smaller, lightweight \"student\" model that can be deployed online efficiently. PLUM He et al. (2025) adapts pre-trained LLMs for industrial-scale generative recommendations through continual pre-training on synthetic user-item interaction data. While PLUM demonstrates the effectiveness of CPT for recommendation tasks, it does not systematically study scaling laws or investigate the impact of data mixture ratios on model performance. OneRec series Zhou et al. (2025a,b); Liu et al. (2025) proposed generative recommendation algorithm and studies their scaling behavior in the axis of model scales and test-time. These prominent works are either model-centric, system-centric, or distillation-centric. Our work is orthogonal and complementary, proposing fundamentally data-centric solution. It posits that even the most advanced architectures, such as LUM, would struggle to exhibit predictable power-law scaling if trained on the pathologically flawed data we identify as the root inhibitor. By first solving the data quality problem, our framework provides the necessary foundation upon which these and other architectural innovations can be built and scaled predictably."
        },
        {
            "title": "2.2 Principled Synthetic Data Generation for Recommendation",
            "content": "Training LLM with synthetic data was used in understanding the training dynamics of LLM Yang et al. (2024); Allen-Zhu and Li (2024a); Hron et al. (2024); Wang et al. (2025); Kang et al. (2025) and recently in industry LLM training Yang et al. (2025a). The use of synthetic data in recommender systems is an established field of research, though its application has traditionally focused on data augmentation Qi et al. (2020) and privacy preservation Adouani and Dagdia (2025) rather than enabling fundamental scaling properties. Early methods relied on statistical techniques to generate new data points that mirrored the distributions of the original dataset. Deep generative models such as Generative Adversarial Networks (GANs) Bharadhwaj et al. (2018) and Variational Autoencoders (VAEs) Adouani and Dagdia (2025) have been employed to create more sophisticated synthetic data, often with the goal of augmenting sparse datasets or creating privacy-preserving alternatives to real user data. Our framework represents conceptual departure from these traditional applications. While GANs and VAEs are typically designed to faithfully replicate the statistical properties of the flawed source data, our methodology is explicitly designed to purify it. The objective is not mimicry but pedagogy. Our layered framework does not attempt to reproduce the noisy, biased raw logs. Instead, it deconstructs the recommendation problem into fundamental conceptssemantics, collaborative patterns, and unbiased sequential behaviorand generates clean, structured curriculum to teach these concepts to the LLM in principled manner Chen et al. (2025a); Soviany et al. (2022). This positions our work as novel application of synthetic data generation, shifting its purpose from simple augmentation to the creation of high-fidelity, pedagogical training corpora."
        },
        {
            "title": "2.3 Mitigating Bias in Recommendation Data",
            "content": "The literature on debiasing recommender systems is vast, encompassing techniques such as inverse propensity scoring Cardoso et al. (2022); Agarwal et al. (2019); Wang et al. (2018); Hu et al. (2025), causal inference Pan et al. (2025); Li et al. (2023a), and adversarial training Yang et al. (2025b); Feng et al. (2019); Hu et al. (2023) to counteract the effects of systemic biases in training data. These approaches typically involve designing more complex model architectures or loss functions that can learn true user preferences despite being trained on confounded data. Our work proposes more direct and arguably more fundamental solution. In alignment with the broader \"data-centric AI\" movement Zha et al. (2023); Angelakis and Rass (2024), we shift the engineering focus from the model to the data itself. Instead of building complex models to be robust to bias, we engineer the data to be free of bias. This allows for the use of powerful but standard LLM architectures without requiring bespoke modifications for debiasing, simplifying the modeling process and focusing resources on the creation of high-quality, reliable source of truth for training."
        },
        {
            "title": "3.1 Fundamental Challenges in Adapting LLMs for Recommendation",
            "content": "The adaptation of pre-trained LLMs for the recommendation domain presents several non-trivial challenges. primary concern is real-time efficiency; the computational demands of large models can introduce significant latency, which is often unacceptable in production environments that require instantaneous recommendation generation. Another well-documented challenge is the cold-start problem, wherein the system struggles to provide accurate recommendations for new users or items with limited interaction histories. However, arguably the most significant challenge is bridging the domain gap between the general-purpose knowledge encoded in base LLM and the specific, often idiosyncratic, patterns of user behavior within given recommendation domain. The model must learn to associate its understanding of language and concepts with the implicit signals embedded in user interaction sequences. The quality and nature of the data utilized for this domain adaptation are therefore of paramount importance."
        },
        {
            "title": "3.2 A Taxonomy of Systemic Biases in User Interaction Logs",
            "content": "Raw user interaction logs, despite their volume, constitute deeply flawed source of training data for sophisticated pattern-matching system like an LLM. These logs are not pure reflection of user preference but rather distorted record heavily influenced by the very system that generated them Chen et al. (2021); Dooley et al. (2023); Li et al. (2023b); Das and Sakib (2024). These data pathologies can be categorized as follows: Table 1 categorizes these pathologies with concrete examples. Continually pretraining an LLM on such data compels it to internalize these pathological patterns. The models objective is to minimize its prediction error on the provided data, and the most efficient means to do so is to learn the dominant statistical regularitieswhich are the biases themselves."
        },
        {
            "title": "3.3 Analysis of Prior Art: The Sub-Scaling of PLUM",
            "content": "The PLUM He et al. (2025) framework serves as an illustrative case study of the consequences of training on raw user data. PLUMs methodology involves CPT phase that utilizes domain-specific data, including user activity sequences, to adapt pre-trained LLM for recommendation tasks. However, empirical results indicated that this approach faced significant scaling challenges. Specifically, the performance of the MoE-3B model was constrained by insufficient training data, ultimately preventing it from demonstrating clear superiority over the smaller MoE-900M model. This outcome is clear example of the \"sub-scaling\" phenomenon observed in the broader NLP literature. Recent analysis of scaling laws, such as the \"Sub-Optimal Scaling Law\" framework proposed in Chen et al. (2025b), explicitly identifies high data density and redundancy as critical factors that cause performance gains to decelerate. When training data lacks diversity or contains significant redundancy Tirumala et al. (2023); 4 Bias Type Description Example Incom- & Data pleteness Sparsity typical user interacts with more than one platform. Furthermore, user profiles and item attributes are frequently incomplete, depriving the model of the rich contextual information required for nuanced reasoning. User buys camera on Amazon, accessories on B&H neither platform sees full intent Data Noise Position Bias Popularity Bias Exposure Bias ser interactions are inherently noisy. click may be accidental, \"like\" may be reflexive, and high numerical rating may be contradicted by negative textual review. Such inconsistencies introduce conflicting signals that can degrade the quality of the models learned representations. In ranked lists, users exhibit strong tendency to interact with items presented at the top, irrespective of their absolute relevance Bito et al. (2025); Ovaisi et al. (2024). model trained on such data will learn simple, fallacious heuristic: \"items in the first position are preferable.\" It learns to replicate the presentation bias of the preceding system, not the underlying preferences of the user. Popular items are recommended more frequently, which leads to more interactions, which in turn reinforces their popularity Braun et al. (2023); Alshabanah et al. (2025); Pipergias Analytis and Hager (2023); Klimashevskaia et al. (2024). This feedback loop systematically under-represents and suppresses niche or long-tail content, leading to increasingly homogenized recommendations. Users can only interact with items they are shown . The interaction log is therefore censored view of user preference, reflecting the past decisions and biases of the recommendation algorithm rather than the users complete preference landscape Mansoury et al. (2022); Xu et al. (2025). model trained on this data cannot learn about items to which it was never exposed. 5-star rating with review: Arrived broken, returning it CTR drops 50%+ from position 1 to 5, even for equally relevant items items receive Top 1% of 80% of interactions; long-tail items never surface User loves jazz but system only recommends poplogs show zero jazz preference Table 1 Taxonomy of systemic biases in user interaction logs. These artifacts are strong, consistent patterns that LLMs readily learn, mistaking them for genuine user intent. He et al. (2024)characteristics typical of raw user interaction logs with their popularity-driven feedback loopsthe marginal benefit of additional data diminishes rapidly. This aligns with the finding that models trained in high-density regimes deviate significantly from the anticipated power-law trajectory . This leads to critical insight regarding LLM training in this domain. When an LLM is continually pre-trained on biased user logs, it does not merely learn the bias; its powerful pattern-matching capabilities codify and amplify it. The model internalizes the systems flaws more effectively than simpler, traditional models. If this newly biased LLM is subsequently deployed, it will generate recommendations that are even more skewed, producing interaction data that is more contaminated than before. Using these new logs for subsequent round of CPT would create vicious cycle of bias amplification, where the systems quality degrades over time by continually learning and reinforcing its own errors. The only way to interrupt this degenerative loop is to fundamentally alter the nature of the training data itself."
        },
        {
            "title": "4.1 Methodology: From Raw Logs to a Curated Curriculum",
            "content": "To overcome the fundamental limitations of real-world interaction data, this work proposes paradigm shift in data strategy. Rather than viewing data as raw resource to be processed, our framework treats it as the foundation for structured, pedagogical curriculum. The objective is not to mitigate biases in flawed dataset but to construct an entirely new, high-fidelity dataset designed to teach the LLM the core concepts of 5 the recommendation domain in principled and progressive manner. This approach aligns with modern LLM development best practices, where models are trained on carefully curated and processed datasets rather than unfiltered web scrapes. For example Yang et al. (2024) found that continued pretraining with next-token prediction fails to teach the model to learn the knowledge its condensed representation, even with heavy repetition. Instead, generating larger synthetic dataset grounded by the source documents can enable continually pretrained model to learn the knowledge. However, for synthetic CPT to scale, the synthetic data must be sufficiently diverse. Allen-Zhu and Li (2024a) also showed diversity of the data (e.g., rewriting in different format) is the critical to the success of (continual) pre-training and merely repeating the data wont work. This framework implements formal curriculum, organized into two primary layers that progress from foundational concepts (semantics and collaborative logic) to more complex, integrated patterns (full sequential behavior), with each layer designed to impart specific type of knowledge."
        },
        {
            "title": "4.2 Layer 1: Grounding Semantic and Collaborative Knowledge",
            "content": "The first layer of the curriculum furnishes the LLM with the foundational \"vocabulary\" and \"grammar\" of the recommendation domain. It teaches the model what items are and how they relate to one another, both through their intrinsic properties and through the collective behavior of users. 4.2.1 Item-Text Alignment Data Purpose: The primary objective of this data type is to establish robust semantic link between the abstract identifiers used to represent items (e.g., semantic tokens) and their real-world meaning as conveyed through textual descriptions. This step is critical for transcending the pure memorization of ID-based co-occurrence patterns that characterizes traditional collaborative filtering models. By grounding the item representations in language, the model can reason about content-based relationships, enabling superior generalization to new, long-tail, or cold-start items. Methodology: The generation process involves creating direct mappings between an items tokenized representation and its descriptive text. These pairs are formatted as natural language sequences. For example: This item <RECTOKEN> REC6594 REC5411 REC1547 REC941 REC7587 REC7639 REC3383 REC6576 </RECTOKEN> is described as [redacted] lotus yoga seed bead bracelet, by Handmade in Women Jewelry Necklaces. Here <RECTOKEN> and </RECTOKEN> are added special tokens to deliminate the semantic tokens; REC6594 are added tokens for semantic tokens vocab. The textual descriptions can be sourced from product catalogs, content creator metadata, or synthesized by other LLMs to ensure richness and consistency. This data provides the model with the fundamental building blocks of content understanding. 4.2.2 Collaborative Filtering (CF) Data Purpose: This data type aims to explicitly teach the model the statistical logic of collaborative filtering by translating user behavior patterns into linguistic format that the LLM can process. Instead of requiring the model to implicitly infer these patterns from noisy sequences, this data makes the relationships explicit, providing clear and direct learning signal. Methodology: The process begins by mining association rules from the raw user interaction histories and existing item-to-item (I2I) data, converting these statistical relationships into templated natural language statements. For example, basic association might be formatted as: When user interacts with item <RECTOKEN> REC3078 REC3311 REC7479 REC2862 REC5552 REC4015 REC6914 REC4637 </RECTOKEN>, there is 4.9% probability they will also interact with item <RECTOKEN> REC3078 REC3311 REC7479 REC4015 REC5211 REC2862 REC1723 REC6914 </RECTOKEN> (confidence: 0.049, lift: 652.45)"
        },
        {
            "title": "4.3 Layer 2: Simulating Unbiased User Behavior",
            "content": "The second layer of the curriculum builds upon the foundational knowledge established in Layer 1. It utilizes the learned relationships to generate complete interaction sequences that are realistic yet free from the contaminating artifacts present in raw data. 4.3.1 Synthetic User Interaction Histories (UIH) Purpose: The objective of Synthetic UIH is to generate clean, diverse, and privacy-preserving user interaction sequences. This data directly addresses the core problems of missing positives and position bias found in real logs. As the sequences are generated algorithmically, they are not tied to any individual users real history, thus preserving privacy. Methodology: The generation process employs novel CF-to-LLM training pipeline. First, graph is constructed where items are represented as nodes, and the weighted edges between them are determined by the strength of the collaborative filtering relationships mined in Layer 1. Realistic user journeys are then simulated using 2nd-order biased random walks, following the Node2Vec Grover and Leskovec (2016) algorithm. Unlike 1st-order walk where the next step depends only on the current node, 2nd-order walks transition probability is conditioned on both the current node and the previous node t. The unnormalized transition probability πv,x to next node is given by πv,x = αp,q(t, x) wv,x, where wv,x is the static edge weight and αp,q(t, x) is the Node2Vec search bias. Return parameter controls the likelihood of returns to previous node and in-out parameter controls exploration (q > 1) vs exploitation (q < 1). This layered approach functions as powerful data purification process. It systematically decouples the true user preference signal from the system-induced artifacts. Layer 1, particularly the CF Data component, acts as an initial filter by extracting the core statistical signal of item co-occurrence from the noisy and biased raw logs. This process inherently averages over numerous user sessions, rendering it less susceptible to individual instances of position or presentation bias. Layer 2 then utilizes this purified signal as the sole basis for generating entirely new behavioral data. The random walk generation process has no intrinsic concept of \"rank,\" \"position,\" or \"presentation order,\" ensuring that the resulting Synthetic UIH data is free from these artifacts. This two-stage filtering mechanism is the core mechanic that enables the creation of clean, high-quality training curriculum, preserving valuable collaborative information while discarding contaminating biases. For example: user interacted with the following sequence of items: <RECTOKEN> REC870 REC5932 REC6271 REC1852 REC1624 REC409 REC6034 REC3608 </RECTOKEN>, <RECTOKEN> REC870 REC5524 REC180 REC4637 REC5552 REC6862 REC6033 REC4948 </RECTOKEN>, <RECTOKEN> REC7402 REC1581 REC5202 REC5289 REC2325 REC5067 REC6960 REC73 </RECTOKEN>, <RECTOKEN> REC1815 REC4896 REC2334 REC7479 REC4502 REC4861 REC1295 REC6855 </RECTOKEN>, <RECTOKEN> REC7479 REC1815 REC2334 REC3927 REC7667 REC2958 REC6513 REC4896 </RECTOKEN>, <RECTOKEN> REC1815 REC4224 REC5068 REC2334 REC4861 REC6855 REC7766 REC2410 </RECTOKEN>, <RECTOKEN> REC1815 REC2633 REC2433 REC2372 REC650 REC4064 REC2334 REC1295 </RECTOKEN>, <RECTOKEN> REC1815 REC5412 REC6298 REC3831 REC6513 REC138 REC5680 REC4210 </RECTOKEN>"
        },
        {
            "title": "5 Empirical Validation of Synthetic Data Quality",
            "content": "The central thesis of this work is that high-quality synthetic data is the key to unlocking scaling laws. To empirically substantiate this claim, we evaluate the generated data across three dimensions: fidelity, utility, and privacy."
        },
        {
            "title": "5.1 Statistical Fidelity Analysis",
            "content": "Fidelity measures how closely the synthetic data mirrors the statistical properties of the original data. We compared the distributions of key characteristics, such as item popularity and sequence length, between the generated Synthetic UIH and the original Merrec user logs."
        },
        {
            "title": "5.2 Empirical Validation of Data Utility for Downstream Ranking",
            "content": "To assess the practical utility of our synthetic data for the core recommendation task of ranking, we conduct \"Train on Synthetic, Test on Real\" (TSTR) evaluation and compare it against \"Train on Real, Test on Real\" (TRTR) baseline. To ensure fair comparison, this evaluation uses constrained methodology: the test set, drawn from real user interactions, is filtered to include only items present in the vocabulary of both the synthetic and real training sets. We trained four standard sequential recommendation models (GRU4Rec, NARM, STAMP, and SASRec) in both the TSTR (SynReal) and TRTR (RealReal) settings. The results, visualized in Figure 1 below, reveal striking and powerful finding. In all cases, models trained exclusively on our synthetic data (Blue lines) achieve significantly better ranking performance (Recall@K) than models trained on real data (Red lines) across all cutoff points (@10, @100, @1K). Figure 1 TSTR vs TRTR: Recall@K Comparison Across Models. TSTR (Train on Synthetic, Test on Real) consistently outperforms TRTR (Train on Real, Test on Real) across all models (GRU4Rec, NARM, SASRec, STAMP) and K-values. Note: Real UIHs are filtered to the same set of items as Synthetic UIHs. These results provide strong, direct evidence that our synthetic data is not only high-fidelity substitute but is in fact superior training resource for teaching the general principles of user preference required for high-quality recommendation. By learning from the purified collaborative filtering signal rather than the noisy 8 specifics of raw interaction logs, the models trained on synthetic data acquire more generalizable co-occurrence patterns. This leads to superior performance on ranking tasks, which require an understanding of the broad set of plausible next items rather than just the single most likely one."
        },
        {
            "title": "5.3 Privacy",
            "content": "By design, our data generation process offers inherent privacy benefits. The Synthetic UIH sequences are generated from an aggregated graph model of item relationships, not from individual user histories. This ensures that the training data is decoupled from any single users activity, mitigating privacy risks associated with using raw interaction logs."
        },
        {
            "title": "6.1 Experimental Configuration",
            "content": "Having established the superior utility of our synthetic data for downstream tasks, we now turn to our central hypothesis: that this high-quality data enables predictable scaling laws for LLMs. To empirically validate this, series of continual pre-training experiments were conducted with the following settings: Base Model: We use Qwen3 models scaling from 0.6B to 8B parameters (0.6B, 1.7B, 4B and 8B), which is one of the best open-weight LLM model available now; Dataset: Table2 describes the statistics of our training dataset. General: We use three subsets (cosmopedia-v2, fineweb-edu-dedup and python-edu) from SmolLMCorpus Ben Allal et al. (2024) as our general domain data to avoid catastrophic forgetting in continual pretraining. Recommendation: The source for recommendation data was the merrec dataset Li et al. (2025), publicly available C2C (consumer-to-consumer) e-commerce dataset from HuggingFace with 1.2B interactions from 65.7M items and 2.6M users. We use sparse auto-encoder generated semantic ID to represent the item and experiments around semantic ID is included in Appendix CPT Hyperparameters: Continual pre-training was performed on B200 clusters using global batch size = 512 sequences, context windows = 512 tokens, learning rate (peak at 1e-4) with linear warmup (100 steps) and cosine decay schedule. All models would be trained with 163B tokens, which is over 20x (chinchilla scaling multiplier Hoffmann et al. (2022)) for 8B models. Evaluation Metric: Model performance was measured using perplexity on held-out test set across all domains (general domain, item-text, CF both seen, CF one unseen, CF both unseen, UIH OOD, and UIH full graph). The statistics are presented in Table 3. General domain: We randomly sample 200K entries (102.4M tokens) for evaluation purpose. Item-text: item-text pairs for items not in training set. We then sample 200K entries (102.4M tokens) for evaluation purpose. CF both seen: collaborative filtering edge between two items with items presented in training data but edges are not included in training data. We then sample 200K entries (102.4M tokens) for evaluation purpose. CF one unseen: CF edge between two items with one item presented in training data and the other items not. Obvious those edges wont present in training data. We then sample 200K entries (102.4M tokens) for evaluation purpose. CF both unseen: CF edge between two items with neither items nor edges presented in training data. There are about 75k entries (38M tokens) in total. UIH OOD: UIH sampled only based on edges from CF test set. There are about 48.3K entries (24.7M tokens). For the parameters used to generate this UIH, please refer to Appendix 9 Table 2 Statistics of the dataset used in this experiment. Our dataset consists of both general domain data Ben Allal et al. (2024) and recommendation data from Merrec Li et al. (2025) We mixes those two domain with 50% and 50% ratio to avoid catastrophic forgetting Ibrahim et al. (2024) Within recommendation data, it is consisted of three types synthetic data from two layers as described in Section 4. All models would be trained with 163B tokens and last row estimated the number of repeats each domain would be exposed after the training."
        },
        {
            "title": "UIH",
            "content": "128B Token Ratio 50% Repeats < 1 2.0B 9% 6.2 7.6B 2.8B 30% 11% 7.3 6.1 Table 3 Statistics of the holdout test dataset used in this experiment, which is consisted of four domains: general, item-text, CF and UIH OOD."
        },
        {
            "title": "Token",
            "content": "102.4M 102.4M 243M 51M UIH full graph: UIH sampled from both CF train and CF test set. There are about 51.7K entries (26.5M tokens)."
        },
        {
            "title": "6.2 Power-Law Scaling Across Data Modalities",
            "content": "The primary results are presented in Figure 2, which depicts perplexity as function of training tokens for seven data modalities. Both axes use logarithmic scales. Across all conditionsGeneral Domain, Item-Text, CF Both Seen, CF One Unseen, CF Both Unseen, UIH OOD, and UIH Full Graphthe empirical data points follow remarkably consistent power-law trends. 6.2.1 Per-Model Data Scaling Analysis We first fit per-model scaling laws of the form ℓ(D) = + ADα, where is the ideal loss and α is the scaling exponent measuring the rate of perplexity reduction per unit of training data. The results are summarized in Table 5. The scaling exponent α reveals clear hierarchy of data efficiency across modalities. UIH exhibits the strongest scaling (α 0.450.59), followed by CF (α 0.280.36), Item-Text (α 0.130.21), and General Domain (α 0.020.03). The near-saturation of General Domain is expected: the pretrained Qwen3 checkpoint already encodes substantial general-domain knowledge, leaving little room for improvement through continued pretraining on similar data. Two model-size-dependent patterns emerge. First, larger models exhibit higher α for CF and UIH, indicating that increased model capacity improves the ability to absorb new recommendation knowledge from data. Second, despite this trend, remains nearly constant across model sizes for CF and UIH, suggesting that even 0.6B parameters are sufficient to represent the underlying patterns in these domains at convergencethe bottleneck is data coverage, not model capacity. 6.2.2 Joint Scaling Law over Model Size and Data To disentangle the contributions of model size (N ) and training data (D), we fit joint scaling laws of the form ℓ = + α + Dβ, where is the irreducible loss, α governs model-size scaling, and β governs data 10 Table 4 Compute-optimal trade-off coefficients. Minimizing loss ℓ = + AN α + BDβ subject to compute budget = 6N yields the optimality condition αA α = βB Dβ. The ratio of αA to βB determines whether compute is best allocated to model size or data."
        },
        {
            "title": "Domain",
            "content": "General Item-Text CF UIH αA 8,432 2,923 5.82 0.261 βB"
        },
        {
            "title": "0.185 Overwhelmingly model-size dominant\n0.283\nStrongly model-size dominant\n1.005 Moderately model-size dominant\nStrongly data dominant\n17.38",
            "content": "scaling: ℓgeneral = 0.79 + 16500 0.511 + 3.85 D0.048 ℓitem-text = 0.473 + 5720 0.511 + 4.04 D0.070 ℓcf = 0.193 + 21 0.277 + 6.79 D0.148 ℓUIH = 0.514 + 1.89 0.138 + 63.9 D0.272 (2) (3) (4) (5) The joint fit yields several findings that complement and deepen the per-model analysis. Text and recommendation domains exhibit opposite scaling bottlenecks. There is striking inverse relationship between α (model-size exponent) and β (data exponent) across domains: General/Item-Text: high α (0.511), low β (0.0480.070). These domains are model-size dominantperformance improves primarily by scaling up model capacity. UIH: low α (0.138), high β (0.272). This domain is data dominantperformance improves primarily by increasing data coverage. CF: intermediate on both axes (α = 0.277, β = 0.148), representing balanced regime. This pattern is consistent with the per-model observation that for CF and UIH varies little across model sizes: the joint fit explains this through the small coefficients for these domains (CF: 21, UIH: 1.89), compared to text domains (General: 16,500, Item-Text: 5,720). The model-size-dependent loss term AN α contributes minimally for recommendation domains, confirming that their bottleneck lies in data rather than model capacity. For text domains, the high α but low β reflects two complementary factors: (1) the pretrained checkpoint already contains extensive general-domain knowledge, making additional text data marginally redundant; and (2) language modeling involves complex compositional patterns that inherently benefit from increased model capacity. Compute-optimal allocation differs drastically by domain. Given fixed compute budget = 6N D, minimizing ℓ subject to this constraint yields the optimality condition αA α = βB Dβ, requiring that the marginal loss reduction per FLOP be equal for model size and data. The coefficients αA and βB  (Table 4)  reveal sharply different optimal strategies: General and Item-Text domains should allocate compute overwhelmingly toward model size, while UIH should prioritize data collection. Concretely, halving the data-dependent loss term for General requires 21/0.048 1.8 106 times more data, whereas for UIH it requires only 21/0.272 13 times more data. Irreducible loss reflects intrinsic predictability. The irreducible loss captures the entropy floor of each domain: CF (E = 0.193): Most predictable, consistent with the low-rank structure of user-item co-occurrence matrices. 11 Item-Text (E = 0.473): Moderately predictable, reflecting the constrained vocabulary and repetitive structure of item descriptions. UIH (E = 0.514): User behavior sequences exhibit moderate stochasticityusers are somewhat predictable but not deterministic. General (E = 0.79): Natural language has the highest inherent entropy, as expected. CF and UIH capture fundamentally different recommendation signals. Despite both encoding user behavior, CF and UIH exhibit distinct scaling profiles. CF scales better with model size (α = 0.277 vs. 0.138) but less with data (β = 0.148 vs. 0.272). This distinction reflects their structural differences: CF encodes global user-item co-occurrence patterns, which benefit from model capacity to learn richer latent factors, while UIH encodes sequential, individual-level interaction trajectories, where the primary bottleneck is observational coverageseeing enough diverse user historiesrather than model expressiveness. Consistency between the two analyses. The per-model and joint scaling analyses yield consistent conclusions through complementary lenses. The per-model fits establish the data-efficiency hierarchy (UIH > CF > Item-Text > General), while the joint fit reveals the complementary model-size dimension and explains why the hierarchy exists: domains where data scaling is efficient (high β) tend to have weak model-size scaling (low α), and vice versa. The per-model observation that is stable across model sizes for CF and UIH is explained by the small coefficients in the joint fit. However, the per-model analysis also reveals that αdata increases with model size for CF and UIH, suggesting model-data interaction that the additive joint formulation does not fully capturean avenue for future investigation. Table 5 Scaling laws for different domains across different model scales. UIH exhibits strong scaling (αUIH = 0.453 0.689), indicating continued improvement with additional training tokens. General domain shows nearsaturation (α 0.025) as expected for pretrained models. should be the lower the better and α should be the higher the better. General Item CF UIH Model 0.6B 1.7B 4B 8B 0.96 0.87 0.82 0. α α α α 0.027 0.023 0.026 0.020 1.07 0.92 0.86 0.93 0.169 0.141 0.126 0.214 0.33 0.34 0.35 0. 0.282 0.315 0.357 0.358 0.64 0.65 0.66 0.66 0.453 0.514 0.564 0."
        },
        {
            "title": "7.1 Ablation Study: The Synergistic Effect of Layered Data",
            "content": "A core tenet of our methodology is that the different layers of synthetic data are complementary, each teaching distinct and necessary facet of the recommendation domain. To validate this hypothesis and quantify the contribution of each layer, we conduct an ablation study. The study involves running the scaling law analysis on models trained with different combinations of our synthetic data curriculum: (1) UIH data only, (2) CF and UIH data, and (3) the complete mixture of Item-Text, CF, and UIH data. Our hypothesis is that training on the complete, mixed dataset will yield the most efficient scaling, evidenced by larger scaling exponent (α) compared to the ablated conditions. Such result would demonstrate super-additive, or synergistic, effect, proving that the integrated knowledge from all layers enables the model to learn more efficiently than the sum of its parts. To understand the contribution of each data domain, we conduct ablation studies by training models with different combinations of item-text, collaborative filtering (CF), and user interaction history (UIH) data. We evaluate perplexity across multiple test sets and fit scaling laws to characterize asymptotic performance. Table 6 provides mixture ratio of each domain, the other settings are exactly same. Figure 3 shows perplexity on hold-out evaluation dataset for each domain. 12 Figure 2 Scaling laws for different domains across different model scales. UIH exhibits strong scaling (αUIH = 0.630.99), indicating continued improvement with additional training tokens. General domain shows near-saturation (α < 0.1) as expected for pretrained models. The dashed lines are the fitted scaling law curves and parameters are provided as legend of the curve and Table 5 Asymmetric Transfer Between CF and UIH. We observe notable asymmetry in cross-domain transfer. Including CF data alongside UIH significantly improves UIH modeling performance (Figure 4, with CF+UIH achieving the lowest asymptotic perplexity (L = 0.66) on UIH evaluation setseven outperforming the UIH-only baseline (L = 0.95). This suggests that collaborative filtering signals, which capture user-item affinity patterns, provide complementary information that benefits sequential user behavior modeling. In contrast, the reverse transfer does not hold: training on UIH data alone yields virtually no improvement on CF tasks (α 0), indicating that user interaction sequences do not implicitly encode the pairwise preference signals required for collaborative filtering. Domain-Specific Data Remains Essential. Despite the positive transfer from CF to UIH, we find that domainspecific training data remains critical for each task. Models trained without CF data plateau at substantially higher CF loss (L = 1.30) compared to those with CF data (L = 0.35). Similarly, excluding item-text 13 Table 6 Data mixture ratios of each domain for the experiments. For all experiments we keep 50% for general text and all experiments are trained for 163B tokens. Choice&Ratio"
        },
        {
            "title": "UIH",
            "content": "Item-text + CF + UIH 9% 0% CF + UIH 22.5% Item-text UIH 0% UIH 30% 11% 37% 13% 0% 0% 27.5% 50% data leads to severe degradation on item-text evaluation (L 3 vs. 1.2), while models retaining item-text data maintain stable performance throughout training. Trade-offs in Multi-Domain Training. Including item-text data introduces modest trade-off: while it prevents catastrophic forgetting of item semantics, it slightly increases UIH loss (L = 0.76 for Item+CF+UIH vs. = 0.66 for CF+UIH). This suggests that practitioners should consider their downstream priorities when designing data mixtures. More studies are provided in next section. Figure 3 Ablation studies on selection of domains for recommendation data. Obviously, excluding domain will causes degradation on the corresponding domain. 14 Figure 4 We plot the evaluation perplexity for UIH using number of UIH training tokens as X-axis. This figure indicates including CF data (Item-text + CF + UIH and CF + UIH) enables the model to learn UIH better."
        },
        {
            "title": "7.2 Ablation Study: Data Mixtures",
            "content": "A balanced mixture of different layers of data is also important to scaling of LLM training. To this end, we sample 42M tokens from 2.7B (or 39K samples from 5.748M samples) of our UIH data as reduced UIH data. we then tested the UIH mixture ratio in 0.5%, 1%, 2%, 5% and 15% for this reduced UIH data, the mixture ratio of other recommendation domain is normalized accordingly to maintain sum of 50%. As the UIH mixture ration on this reduced set increases, the more repeats of the reduced UIH data the model would go through (Column #repeats of Table 7). In this study, we focus on 4B model and results are shown in Figure 5 and Table 7. 15 Figure 5 Scaling laws on 4B models with different mixture ratio on UIH. Obviously the perplexity starts to increase when the UIH mixture ratio is too high (reduced UIH data is repeated too many times), which is sign of overfitting. The higher mixture ratio, this increase happens earlier in training stage. We omitted the figures for CF Both Seen and CF On Unseen here, as they are very similar to CF Both Unseen. Figure 6 (a) The training loss curve for experiment in Figure 2. All the models are trained smoothly and show monotonically decreasing training loss. (b) the training loss curve for experiment in Figure 5. The final training loss decreases in the order of 0.5% > 1% 30M 15% > 2% > 5%. 16 (a) UIH ratio=2% Figure 7 Scaling laws with mixture ratio on the reduced UIH data = (a) 2% and (b) 15% across different model scales. When the perplexity starts to increase depends on UIH mixture ratio, independent of model scales. We omitted the results on other domains as they shown the same patterns as Figure 5. (b) UIH ratio=15% Table 7 Scaling laws on 4B models with different mixture ratio on UIH. We also provide the number of training tokens and repeats on the UIH data for better understanding here. We provide mixture ratio for each domain in the left five columns and scaling law in the remaining 8 columns. The first row shows the result using full UIH dataset, which is set up in Figure 2; the other rows are for reduced UIH dataset. should be the lower the better and α should be the higher the better. UIH Item Text % #tokens #repeats % CF % General Item-Text CF UIH α α α α 11.0% 17.93B 0.5% 1.0% 2.0% 5.0% 15.0% 24.45B 0.82B 1.63B 3.26B 8.15B 6.1 4.1 8.2 16.3 40.8 122.3 9.0% 13.5% 13% 12.0% 12.0% 20.0% 30.0% 0.83 36.0% 0.84 36.0% 0.83 36.0% 0.83 33.0% 0.83 15.0% 0.85 0.024 0.034 0.024 0.024 0.023 0.018 1.25 1.38 1.45 1.43 1.42 1.51 0.111 0.175 0.213 0.193 0.183 0.294 0.35 0.35 0.37 0.37 0.37 0.35 0.332 0.317 0.391 0.394 0.383 0. 0.76 0.75 0.78 0.81 0.84 0.90 0.421 0.423 0.485 0.576 0.685 0.754 Analysis: Figure 5 indicates starting from UIH mixture ratio=2% (about 16 repeats at the end of training according to Table7), the models perplexity on the hold-out evaluation set starts to increase, which suggests an overfit is happening (according to Figure 6(b), the training loss decreases monotonically). Increasing ratio would cause the overfit happens even earlier: at 15%, it happens at 20B training tokens (about 16 repeats); and at 5%, it happens at 80B training tokens (about 20 repeats). This result generally aligns with Yang et al. (2024) that given well constructed synthetic dataset, CPT with 4 repeats could enable the model to learn the knowledge, and Muennighoff et al. (2025) training beyond four repeats gives diminished returns, continuing for 20+ repeats could Hron et al. (2024) decrease ability to 17 generalize unseen data. Allen-Zhu and Li (2024b); Yang et al. (2024), Performance degrades sharply when repetitions exceed approximately 100 the original dataset size. We further analyze model scales affect this behavior with two UIH mixture ratio=(a) 2% and (b) 15% on the reduced UIH data. Figure 7 indicates when the perplexity starts to increase depends on UIH mixture ratio and independent of model scales: the perplexity on evaluation set starts to increase at 20B total training tokens (about 16 repeats on reduced UIH data) when ratio=15%; or at 160B tokens (also about 16 repeats) when ratio=2%, regardless of model scales. However it becomes more significant as the model gets larger."
        },
        {
            "title": "7.3 Scaling Laws Across Model and Data Scale",
            "content": "The initial results establish that predictable scaling is achievable. The next critical step is to characterize these scaling laws across the two primary dimensions of LLM developmentdata and model sizeto identify compute-optimal training regimes. This endeavor seeks to replicate the rigorous, multi-faceted scaling analysis of the Chinchilla paper within the recommendation domain, an objective made possible by our scalable source of high-quality data. We conducted compute-optimal scaling analysis across four model sizes (0.6B8B parameters), all trained on 163.84B tokens with mixed-domain curriculum (shown in Figure 8). This configuration places models at varying positions relative to Chinchilla-optimal allocation: the 8B model operates near the optimal 20 tokens-per-parameter ratio, while smaller models are progressively over-trained (0.6B at 454 tokens/param). For general-domain evaluation, where training data is not repeated, we observe classic Chinchilla scaling with clear Pareto frontierthe 8B model achieves the lowest perplexity at highest compute. However, recommendation-specific domains exhibit markedly different behavior. Collaborative filtering tasks saturate at model with 4B parameters ( 1.47 PPL), indicating that the pattern complexity is bounded and fully learnable by 4B-parameter models given sufficient repetition. Most notably, user interaction history evaluation on out-of-distribution items reveals inverse scaling: the heavily over-trained 0.6B model ( 2.01 PPL) outperforms the near-optimal 8B model (2.10 PPL). We attribute this to the interaction between data repetition and model capacitylarger models more readily memorize repeated behavioral sequences, leading to poorer OOD generalization, while the extreme over-training regime of smaller models appears to act as implicit regularization against such memorization. 18 Figure 8 Compute-optimal analysis for models from 0.6B to 8B. Our scaling experiments train models from 0.6B to 8B parameters on 163.84B tokens, spanning from heavily over-trained (454 tokens/param for 0.6B) to near Chinchilla-optimal (20 tokens/param for 8B). While general-domain perplexity follows expected compute-optimal scaling, recommendation taskswhere domain-specific data is repeated 67 due to limited unique tokensexhibit divergent behavior."
        },
        {
            "title": "8 Case Studies",
            "content": "In this section, we provide some examples of inferring the trained models. Figure 9 shows how the model recommend an item given the UIH. 19 Case Study: Recommendation given UIH Semantic ID Format Decoded Natural Language Prompt: user interacted with the following sequence of items: <RECTOKEN> REC2981 REC2337 REC5067 REC1796 REC2985 REC6054 REC988 REC5454 </RECTOKEN>, <RECTOKEN> REC2284 REC2581 REC2334 REC8032 REC6083 REC4502 REC5357 REC7604 </RECTOKEN>, <RECTOKEN> REC5411 REC2334 REC6499 REC5622 REC2394 REC6054 REC7338 REC7704 </RECTOKEN>, <RECTOKEN> REC6594 REC2334 REC5261 REC4570 REC6576 REC906 REC6676 REC6499 </RECTOKEN>, <RECTOKEN> REC2284 REC641 REC8166 REC7604 REC2581 REC2312 REC7393 REC1231 </RECTOKEN>, <RECTOKEN> REC5824 REC2334 REC4570 REC919 REC6513 REC2773 REC3722 REC7587 </RECTOKEN>, <RECTOKEN> REC5824 REC7479 REC2334 REC5622 REC6499 REC344 REC3722 REC1553 </RECTOKEN> Response: REC5824 REC7479 REC2334 REC5622 REC2006 REC6499 REC1553 REC344 Ground Truth: REC5824 REC7479 REC2334 REC344 REC5622 REC2006 REC6499 REC3722 Prompt: user interacted with the following sequence of items: [redacted] top, by [redacted] in Women Athletic apparel Athletic Tank Tops. Size: XS (0-2)., [redacted] Dress, by [redacted] in Women Dresses Above knee, mini. Size: XS (0-2)., Bracelet, in Women Jewelry Bracelets., Necklace, in Women Jewelry Necklaces., Dress, in Women Dresses Knee-length. Size: (4-6)., crossbody bag, in Women Womens handbags Crossbody Bags, [redacted] purse, by [redacted] in Women Womens handbags Shoulder Bags. Color: black. Response: [redacted] handbag, by [redacted] in Women Womens handbags Shoulder Bags. Ground Truth: [redacted] purse, by [redacted] in Women Womens handbags Shoulder Bags. Color: black/gold Figure 9 An example of using the model to recommend item given the UIH. On the left, we provide the input and output with semantic ID format; and decoded natural language on the right. In this example, the models response match perfectly with ground truth, except missing on the color. Applying beam search in inference, the model could generate relevant and diverse responses, which is shown in Figure 10. 20 Case Study: Beam Search Semantic ID Format Decoded Natural Language Prompt: user interacted with the following sequence of items: <RECTOKEN> REC3972 REC7479 REC906 REC641 Prompt: user interacted with the following sequence of items: in Women Womens [redacted] Tote, by [redacted] handbags Tote Bags, Ring, in Women Jewelry Rings, [redacted], by [redacted] in Beauty Skin care Body, [redacted] Brazilian Crush, by [redacted] in Beauty Fragrance Women, [redacted] daisy, by [redacted] in Beauty Fragrance Women, crossbody bag, in Women Womens handbags Crossbody Bags, [redacted] Purse, by [redacted] in Women Womens handbags Crossbody Bags. Color: other, [redacted] crossbody Medium purse, by [redacted] in Women Womens handbags Crossbody Bags, [redacted] cross body bag, by [redacted] in Women Womens handbags Crossbody Bags. Color: other, [redacted] purse, by [redacted] in Women Womens handbags Satchel. [redacted] Field tote, by [redacted] in Response 1: Women Womens handbags Shoulder Bags. Response 2: [redacted] Purse, by [redacted] in Women Womens handbags Shoulder Bags. Color: brown. Response 3: [redacted] purse, by [redacted] in Women Womens handbags Shoulder Bags. Color: black. REC6621 REC4502 REC5992 REC2888 </RECTOKEN>, <RECTOKEN> REC805 REC7607 REC2334 REC6499 REC1209 REC2633 REC6341 REC3072 </RECTOKEN>, <RECTOKEN> REC3972 REC2590 REC2334 REC641 REC5454 REC5897 REC7479 REC4570 </RECTOKEN>, <RECTOKEN> REC2284 REC7604 REC641 REC2898 REC6581 REC2312 REC2581 REC748 </RECTOKEN>, <RECTOKEN> REC2284 REC641 REC7604 REC6581 REC2898 REC2581 REC8166 REC748 </RECTOKEN>, <RECTOKEN> REC2284 REC641 REC2581 REC8166 REC748 REC2312 REC1231 REC7604 </RECTOKEN>, <RECTOKEN> REC3972 REC6581 REC7479 REC2410 REC641 REC8174 REC3722 REC1231 </RECTOKEN>, <RECTOKEN> REC3375 REC2913 REC5824 REC7366 REC2752 REC984 REC1796 REC7604 </RECTOKEN>, <RECTOKEN> REC5824 REC82 REC5024 REC7427 REC1030 REC906 REC3968 REC2269 </RECTOKEN>, <RECTOKEN> REC5824 REC2334 REC7479 REC1776 REC1553 REC2269 REC5622 REC6499 </RECTOKEN> Response 1: REC5824 REC7479 REC2334 REC5024 REC82 REC6513 REC7736 REC1030 Response 2: REC5824 REC7479 REC2334 REC5011 REC5622 REC2006 REC1553 REC3722 Response 3: REC5824 REC7479 REC2334 REC5622 REC6499 REC344 REC3722 REC1553 Figure 10 An example of applying beam search on the models inference. On the left, we provide the input and output with semantic ID format; and decoded natural language on the right. In this example, the models responses are relevant and diverse. It would be interesting to find out how the model would react to random UIH. We provide such example in Figure 11, which shows when given random UIH, the model still tries to recommendation items relevant to items to UIH; but given the items in the UIH are random, the models recommendations are more diverse. 21 Case Study: Recommendation given Random UIH Semantic ID Format Decoded Natural Language Prompt: user interacted with the following sequence of items: <RECTOKEN> REC3311 REC953 REC418 REC7643 REC5624 REC419 REC7078 REC6875 </RECTOKEN>, <RECTOKEN> REC6812 REC6264 REC3311 REC7338 REC844 REC1746 REC237 REC5624 </RECTOKEN>, <RECTOKEN> REC6875 REC3311 REC2145 REC6576 REC5552 REC1796 REC2136 REC4685 </RECTOKEN>, <RECTOKEN> REC729 REC6261 REC4334 REC2044 REC1684 REC3927 REC4029 REC507 </RECTOKEN>, <RECTOKEN> REC5411 REC985 REC2394 REC7373 REC7338 REC4863 REC7366 REC6264 </RECTOKEN>, <RECTOKEN> REC7795 REC3245 REC856 REC6326 REC3812 REC6566 REC6855 REC7479 </RECTOKEN>, <RECTOKEN> REC3972 REC2145 REC7479 REC5454 REC5885 REC4540 REC6914 REC3812 </RECTOKEN>, <RECTOKEN> REC1166 REC237 REC984 REC1796 REC1295 REC5947 REC6341 REC6676 </RECTOKEN>, <RECTOKEN> REC6581 REC641 REC95 REC4647 REC5959 REC4208 REC4531 REC3722 </RECTOKEN>, <RECTOKEN> REC5695 REC4190 REC6261 REC2269 REC5624 REC1127 REC1796 REC729 </RECTOKEN> Response 1 REC4905 REC2334 REC1098 REC3339 REC6326 REC4190 REC2269 REC4863 Response 2 REC641 REC2981 REC7479 REC524 REC6631 REC2334 REC2581 REC8032 Response 3 REC641 REC2981 REC5980 REC4071 REC95 REC4531 REC2479 REC1685 Response 4 REC641 REC2981 REC7479 REC524 REC6631 REC2334 REC573 REC3812 Response 5 REC641 REC2981 REC7479 REC524 REC6631 REC2334 REC573 REC409 Prompt: user interacted with the following sequence of items: in Toys & Play Air Maintenance Station Toy Plane, Collectibles Sports & Outdoor Play Sand & Water Toys, [redacted] Star Wars Snowspeeder Pilot Light Gray Helmet, by [redacted] in Toys & Collectibles Building Toys [redacted] Toys, [redacted] Wooden Railway Bill and Ben tank engine wood train 23102701, by [redacted] & Friends in Toys & Collectibles Remote Control Toys & Vehicles Trains & Train Sets, [redacted] Flight Suit Mens Medium Large Costume Bradley Bradshaw Costume Halloween, by [redacted] in Men Coats & jackets Military. Size: (38-40), Rainbow Eraser Multicolor Charm Handmade Friendship Bracelet, by Handmade in Women Jewelry Bracelets, <Fantastic four [redacted] visionaries John Byrne, by [redacted] in Books Fiction Books Comics, Flia Shoes, in Kids Girls shoes, samsung galaxy s8 [redacted] case. used, by [redacted] in Electronics Cell phones & accessories Cases, covers & skins, Floral Cardigan by [redacted], by [redacted] in Women Sweaters Cardigan. Size: (8-10), VTG Sno Rider Womens Snow Pants Black 12 Insulated Bibs Snowmobile Zip up Legs, in Sports & outdoors Apparel Women. Size: (12-14). Response 1 Vintage 90s Ford Mustang Overalls, by Ford in Men Pants Other. Size Response 2 Princess Polly, by Princess Polly in Women Dresses Above knee, mini. Size: ( Response 3 Womens size large tops bundle, by Boutique in Women Tops & blouses Blouse. Response 4 Dollskill Alice in Wonderland, by Dolls Kill in Women Tops & blouses Other Response 5 Dolls Kill The Powerpuff Girls - Bubbles & Butterflies Corset, by Doll Figure 11 An example of model makes recommendation given random UIH. On the left, we provide the input and output with semantic ID format; and decoded natural language on the right. In this example, the models responses are still relevant to the items presented in the UIH. Finally we provide an example of retrieving top-10 similar items given an generated item with semantic ID in Table 8. This example shows the effectiveness of the semantic ID in representing the items and capture their semantics."
        },
        {
            "title": "9 Discussion and Conclusion",
            "content": "This paper has addressed foundational gap in the application of Large Language Models to recommender systems. The absence of predictable scaling laws has, until now, hindered the scientific and systematic development of these powerful new models. The central contribution of this work is the demonstration that 22 Table 8 Example of running similarity search based on semantic IDs. Given the query (Row 1), we retrieval top-10 most similar items."
        },
        {
            "title": "Query",
            "content": "REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC2388 REC6054 Handbag, in Women Womens handbags Shoulder Bags REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC2388 REC6054 Handbag, in Women Womens handbags Shoulder Bags REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC2388 REC2240 Beautiful Bag Designer Style!!, in Women Womens handbags Shoulder Bags REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC2388 REC7587 Light Gold Tote, in Women Womens handbags Shoulder Bags REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC2388 REC984 luxury bags, in Women Womens handbags Shoulder Bags REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC1030 REC2633 inspired bag, in Women Womens handbags Shoulder Bags REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC1228 REC5454 shoes women, in Women Womens handbags Crossbody Bags REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC1553 REC6842 Stay Real Bag, in Women Womens handbags Shoulder Bags REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC2240 REC1228 Fancy Handbag, in Women Womens handbags Shoulder Bags REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC2240 REC2388 beautiful bag, by Op in Women Womens handbags Shoulder Bags REC5824 REC2334 REC4570 REC3722 REC7479 REC3968 REC2240 REC4075 Stylish Purse, Statement Piece, in Women Womens handbags Shoulder Bags"
        },
        {
            "title": "Query",
            "content": "100.0% 87.5% 87.5% 87.5% 75.0% 75.0% 75.0% 75.0% 75.0% 75.0% this barrier is not inherent to the domain but is consequence of relying on pathologically flawed raw user data. By introducing novel, layered framework for generating high-quality synthetic data, this research has shown that it is possible to create curated curriculum that enables robust, predictable, power-law scaling. The empirical results provide the first concrete evidence that LLM performance in the recommendation domain can be systematically improved with increasing data, placing the field on much firmer and more predictable scientific footing. Implications for Practitioners and Researchers The findings presented in this paper carry significant practical and theoretical implications. For practitioners, the establishment of the first scaling laws for LLMs in recommendation provides an essential tool for project planning and resource management. It enables teams to transition from speculative, heuristic-based approaches to more scientific method of forecasting the computational budget and data volume required to achieve target level of performance, thereby making the development of large-scale systems more predictable and justifiable."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning, specifically the development of LLMs for recommender systems. We highlight several aspects of broader impact. Positive Societal Impacts. Our synthetic data framework offers inherent privacy benefits: the generated user interaction histories are derived from aggregated item-to-item graphs rather than individual user traces, decoupling training data from real user behavior. Additionally, our methodology explicitly addresses systemic 23 biases (position, popularity, and exposure bias) that plague real-world recommendation systems. By training on debiased synthetic curricula, LLMs may produce more equitable recommendations that surface long-tail and niche content rather than perpetuating popularity-driven feedback loops. Potential Risks and Mitigations. More effective recommendation systems could, in principle, increase user engagement in ways that raise concerns about digital well-being. However, our work focuses on the predictability of model development rather than optimizing for engagement metrics. Furthermore, while our synthetic data generation aims to remove systemic biases, practitioners must validate that downstream applications do not inadvertently introduce new biases through their choice of source data or graph construction. We encourage future work to develop evaluation frameworks that audit both the fidelity and fairness of synthetic recommendation data."
        },
        {
            "title": "References",
            "content": "Malek Adouani and Zaineb Chelly Dagdia. Fair and Privacy-preserving Synthetic Data Generation via Clustering-based Variational Autoencoder and Adversarially Debiased Wasserstein Generative Adversarial Networks with Gradient Penalty. In ECMLPKDD 2025, Porto (Portugal), Portugal, September 2025. https://hal.science/hal-05113907. Aman Agarwal, Ivan Zaitsev, Xuanhui Wang, Cheng Li, Marc Najork, and Thorsten Joachims. Estimating Position Bias without Intrusive Interventions. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pages 474482, January 2019. doi: 10.1145/3289600.3291017. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction, 2024a. https://arxiv.org/abs/2309.14316. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity scaling laws, 2024b. https://arxiv.org/abs/2404.05405. Abdulla Alshabanah, Keshav Balasubramanian, Elan Markowitz, Greg Ver Steeg, and Murali Annavaram. The upside of bias: Personalizing long-tail item recommendations with biased sampling. ACM Trans. Recomm. Syst., October 2025. doi: 10.1145/3771279. https://doi.org/10.1145/3771279. Just Accepted. Athanasios Angelakis and Andrey Rass. data-centric approach to class-specific bias in image data augmentation, 2024. https://arxiv.org/abs/2403.04120. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm-corpus: high-quality dataset for training small language models. Hugging Face, 2024. https://huggingface.co/datasets/ HuggingFaceTB/smollm-corpus. Homanga Bharadhwaj, Homin Park, and Brian Y. Lim. RecGAN: Recurrent generative adversarial networks for recommendation systems. In Proceedings of the 12th ACM Conference on Recommender Systems, pages 372376, Vancouver British Columbia Canada, September 2018. ACM. ISBN 978-1-4503-5901-6. doi: 10.1145/3240323.3240383. Ethan Bito, Yongli Ren, and Estrid He. Evaluating position bias in large language model recommendations, 2025. https://arxiv.org/abs/2508.02020. Valentijn Braun, Debarati Bhaumik, and Diptish Dey. Metrics for popularity bias in dynamic recommender systems, 2023. https://arxiv.org/abs/2310.08455. Gabriel Cardoso, Sergey Samsonov, Achille Thin, Eric Moulines, and Jimmy Olsson. Br-snis: Bias reduced selfnormalized importance sampling, 2022. https://arxiv.org/abs/2207.06364. Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. Bias and debias in recommender system: survey and future directions, 2021. https://arxiv.org/abs/2010.03240. Junyi Chen, Lu Chi, Bingyue Peng, and Zehuan Yuan. HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling, September 2024. Xiaoyin Chen, Jiarui Lu, Minsu Kim, Dinghuai Zhang, Jian Tang, Alexandre Piché, Nicolas Gontier, Yoshua Bengio, and Ehsan Kamalloo. Self-evolving curriculum for llm reasoning, 2025a. https://arxiv.org/abs/2505.14970. Zhengyu Chen, Siqi Wang, Teng Xiao, Yudong Wang, Shiqi Chen, Xunliang Cai, Junxian He, and Jingang Wang. Subscaling laws: On the role of data density and training strategies in llms, 2025b. https://arxiv.org/abs/2507.10613. Anindya Bijoy Das and Shahnewaz Karim Sakib. Unveiling and mitigating bias in large language model recommendations: path to fairness, 2024. https://arxiv.org/abs/2409.10825. Samuel Dooley, Rhea Sukthanker, John Dickerson, Colin White, Frank Hutter, and Micah Goldblum. Rethinking bias mitigation: Fairer architectures make for fairer face recognition. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 7436674393. Curran Associates, Inc., 2023. https://proceedings.neurips.cc/paper_files/paper/2023/file/ eb3c42ddfa16d8421fdba13528107cc1-Paper-Conference.pdf. Rui Feng, Yang Yang, Yuehan Lyu, Chenhao Tan, Yizhou Sun, and Chunping Wang. Learning fair representations via an adversarial framework, 2019. https://arxiv.org/abs/1904.13341. Xiangbo Gao, Runsheng Xu, Jiachen Li, Ziran Wang, Zhiwen Fan, and Zhengzhong Tu. Stamp: Scalable task and model-agnostic collaborative perception, 2025. https://arxiv.org/abs/2501.18616. 25 Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks, 2016. https://arxiv.org/abs/1607. 00653. Nan He, Weichen Xiong, Hanwen Liu, Yi Liao, Lei Ding, Kai Zhang, Guohua Tang, Xiao Han, and Wei Yang. Softdedup: an efficient data reweighting method for speeding up language model pre-training, 2024. https: //arxiv.org/abs/2407.06654. Ruining He, Lukasz Heldt, Lichan Hong, Raghunandan Keshavan, Shifan Mao, Nikhil Mehta, Zhengyang Su, Alicia Tsai, Yueqi Wang, Shao-Chuan Wang, Xinyang Yi, Lexi Baugher, Baykal Cakici, Ed Chi, Cristos Goodrow, Ningren Han, He Ma, Romer Rosales, Abby Van Soest, Devansh Tandon, Su-Lin Wu, Weilong Yang, and Yilin Zheng. PLUM: Adapting Pre-trained Language Models for Industrial-scale Generative Recommendations, October 2025. Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. Session-based recommendations with recurrent neural networks, 2016. https://arxiv.org/abs/1511.06939. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. Advances in Neural Information Processing Systems, 35:3001630030, 2022. Jiri Hron, Laura Culp, Gamaleldin Elsayed, Rosanne Liu, Ben Adlam, Maxwell Bileschi, Bernd Bohnet, J. D. Co-Reyes, Noah Fiedel, C. Daniel Freeman, Izzeddin Gur, Kathleen Kenealy, Jaehoon Lee, Peter J. Liu, Gaurav Mishra, Igor Mordatch, Azade Nova, Roman Novak, Aaron Parisi, Jeffrey Pennington, Alex Rizkowsky, Isabelle Simpson, Hanie Sedghi, Jascha Sohl-dickstein, Kevin Swersky, Sharad Vikram, Tris Warkentin, Lechao Xiao, Kelvin Xu, Jasper Snoek, and Simon Kornblith. Training Language Models on the Knowledge Graph: Insights on Hallucinations and Their Detectability, August 2024. Hengchang Hu, Yiming Cao, Zhankui He, Samson Tan, and Min-Yen Kan. Automatic feature fairness in recommendation via adversaries. In Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region, SIGIR-AP 23, page 245252. ACM, November 2023. doi: 10.1145/3624918.3625318. http://dx.doi.org/10.1145/3624918.3625318. Wenbo Hu, Xin Sun, Qiang Liu, Le Wu, and Liang Wang. Uncertainty calibration for counterfactual propensity estimation in recommendation. IEEE Transactions on Knowledge and Data Engineering, 37(6):37813793, June 2025. ISSN 2326-3865. doi: 10.1109/tkde.2025.3552658. http://dx.doi.org/10.1109/TKDE.2025.3552658. Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continually pre-train large language models, 2024. https://arxiv.org/abs/2403.08763. Feiyang Kang, Newsha Ardalani, Michael Kuchnik, Youssef Emad, Mostafa Elhoushi, Shubhabrata Sengupta, ShangWen Li, Ramya Raghavendra, Ruoxi Jia, and Carole-Jean Wu. Demystifying synthetic data in llm pre-training: systematic study of scaling laws, benefits, and pitfalls, 2025. https://arxiv.org/abs/2510.01631. Wang-Cheng Kang and Julian McAuley. Self-Attentive Sequential Recommendation, August 2018. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. https://arxiv.org/abs/ 2001.08361. Anastasiia Klimashevskaia, Dietmar Jannach, Mehdi Elahi, and Christoph Trattner. survey on popularity bias in recommender systems. User Modeling and User-Adapted Interaction, 34(5):17771834, July 2024. ISSN 1573-1391. doi: 10.1007/s11257-024-09406-0. http://dx.doi.org/10.1007/s11257-024-09406-0. Weijiang Lai, Beihong Jin, Jiongyan Zhang, Yiyuan Zheng, Jian Dong, Jia Cheng, Jun Lei, and Xingxing Wang. Exploring scaling laws of ctr model for online performance improvement. In Proceedings of the Nineteenth ACM Conference on Recommender Systems, RecSys 25, page 114123. ACM, September 2025. doi: 10.1145/3705328. 3748046. http://dx.doi.org/10.1145/3705328.3748046. Haoxuan Li, Chunyuan Zheng, and Peng Wu. StableDR: Stabilized doubly robust learning for recommendation on data missing not at random. In The Eleventh International Conference on Learning Representations, 2023a. https://openreview.net/forum?id=3VO1y5N7K1H. Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, and Jun Ma. Neural attentive session-based recommendation, 2017. https://arxiv.org/abs/1711.04725. Lichi Li, Zainul Abi Din, Zhen Tan, Sam London, Tianlong Chen, and Ajay Daptardar. Merrec: large-scale In Proceedings of the 31st multipurpose mercari dataset for consumer-to-consumer recommendation systems. ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1, KDD 25, page 23712382, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400712456. doi: 10.1145/3690624.3709394. https://doi.org/10.1145/3690624.3709394. Tianlin Li, Qing Guo, Aishan Liu, Mengnan Du, Zhiming Li, and Yang Liu. Fairer: Fairness as decision rationale alignment, 2023b. https://arxiv.org/abs/2306.15299. Zhanyu Liu, Shiyao Wang, Xingmei Wang, Rongzhou Zhang, Jiaxin Deng, Honghui Bao, Jinghao Zhang, Wuchao Li, Pengfei Zheng, Xiangyu Wu, Yifei Hu, Qigen Hu, Xinchen Luo, Lejian Ren, Zixing Zhang, Qianqian Wang, Kuo Cai, Yunfan Wu, Hongtao Cheng, Zexuan Cheng, Lu Ren, Huanjie Wang, Yi Su, Ruiming Tang, Kun Gai, and Guorui Zhou. OneRec-Think: In-Text Reasoning for Generative Recommendation, October 2025. Masoud Mansoury, Bamshad Mobasher, and Herke van Hoof. Exposure-aware recommendation using contextual bandits, 2022. https://arxiv.org/abs/2209.01665. Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling Data-Constrained Language Models, June 2025. Zohreh Ovaisi, Parsa Saadatpanah, Shahin Sefati, Mesrob Ohannessian, and Elena Zheleva. Fairness of interaction in ranking under position, selection, and trust bias. ACM Trans. Recomm. Syst., 3(2), November 2024. doi: 10.1145/3652864. https://doi.org/10.1145/3652864. Le Pan, Yuanjiang Cao, Chengkai Huang, Wenjie Zhang, and Lina Yao. Counterfactual inference for eliminating sentiment bias in recommender systems, 2025. https://arxiv.org/abs/2505.03655. Pantelis Pipergias Analytis and Philipp Hager. Collaborative filtering algorithms are prone to mainstream-taste bias. In Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 23, page 750756, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702419. doi: 10.1145/3604915.3608825. https://doi.org/10.1145/3604915.3608825. Pi Qi, Xiaoqiang Zhu, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, and Kun Gai. Searchbased user interest modeling with lifelong sequential behavior data for click-through rate prediction, 2020. https: //arxiv.org/abs/2006.05639. Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H. Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q. Tran, Jonah Samost, Maciej Kula, Ed H. Chi, and Maheswaran Sathiamoorthy. Recommender Systems with Generative Retrieval, November 2023. Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: survey, 2022. https: //arxiv.org/abs/2101.10382. Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S. Morcos. D4: Improving llm pretraining via document de-duplication and diversification, 2023. https://arxiv.org/abs/2308.12284. Haohui Wang, Jingyuan Qi, Jianpeng Chen, Jun Wu, Lifu Huang, Lecheng Zheng, Kevin Choi, Balaji Veeramani, Edward Bowen, Alison Hu, Tyler Cody, and Dawei Zhou. Data value in the age of scaling: Understanding llm scaling dynamics under real-synthetic data mixtures, 2025. https://arxiv.org/abs/2511.13640. Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork. Position Bias Estimation for Unbiased Learning to Rank in Personal Search. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pages 610618, Marina Del Rey CA USA, February 2018. ACM. ISBN 978-1-4503-5581-0. doi: 10.1145/3159652.3159732. Xiaoxiao Xu, Hao Wu, Wenhui Yu, Lantao Hu, Peng Jiang, and Kun Gai. Enhancing interpretability and effectiveness in recommendation with numerical features via learning to contrast the counterfactual samples, 2025. https: //arxiv.org/abs/2509.03187. Bencheng Yan, Shilei Liu, Zhiyuan Zeng, Zihao Wang, Yizhen Zhang, Yujin Yuan, Langming Liu, Jiaqi Liu, Di Wang, Wenbo Su, Wang Pengjie, Jian Xu, and Bo Zheng. Unlocking scaling law in industrial recommendation systems with three-step paradigm based large user model, 2025. https://arxiv.org/abs/2502.08309. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 Technical Report, May 2025a. Ran Yang, Yihao Zhang, Kaibei Li, Qinyang He, Xiaokang Li, and Wei Zhou. Adversarial regularized diffusion model for fair recommendations. Neural Networks, 190:107695, October 2025b. ISSN 08936080. doi: 10.1016/j.neunet.2025. 107695. Zitong Yang, Neil Band, Shuangping Li, Emmanuel Candès, and Tatsunori Hashimoto. Synthetic continued pretraining, 2024. https://arxiv.org/abs/2409.07431. Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. Data-centric artificial intelligence: survey, 2023. https://arxiv.org/abs/2303.10158. Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, and Yu Shi. Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations, May 2024. Guorui Zhou, Jiaxin Deng, Jinghao Zhang, Kuo Cai, Lejian Ren, Qiang Luo, Qianqian Wang, Qigen Hu, Rui Huang, Shiyao Wang, Weifeng Ding, Wuchao Li, Xinchen Luo, Xingmei Wang, Zexuan Cheng, Zixing Zhang, Bin Zhang, Boxuan Wang, Chaoyi Ma, Chengru Song, Chenhui Wang, Di Wang, Dongxue Meng, Fan Yang, Fangyu Zhang, Feng Jiang, Fuxing Zhang, Gang Wang, Guowang Zhang, Han Li, Hengrui Hu, Hezheng Lin, Hongtao Cheng, Hongyang Cao, Huanjie Wang, Jiaming Huang, Jiapeng Chen, Jiaqiang Liu, Jinghui Jia, Kun Gai, Lantao Hu, Liang Zeng, Liao Yu, Qiang Wang, Qidong Zhou, Shengzhe Wang, Shihui He, Shuang Yang, Shujie Yang, Sui Huang, Tao Wu, Tiantian He, Tingting Gao, Wei Yuan, Xiao Liang, Xiaoxiao Xu, Xugang Liu, Yan Wang, Yi Wang, Yiwu Liu, Yue Song, Yufei Zhang, Yunfan Wu, Yunfeng Zhao, and Zhanyu Liu. OneRec Technical Report, June 2025a. Guorui Zhou, Hengrui Hu, Hongtao Cheng, Huanjie Wang, Jiaxin Deng, Jinghao Zhang, Kuo Cai, Lejian Ren, Lu Ren, Liao Yu, Pengfei Zheng, Qiang Luo, Qianqian Wang, Qigen Hu, Rui Huang, Ruiming Tang, Shiyao Wang, Shujie Yang, Tao Wu, Wuchao Li, Xinchen Luo, Xingmei Wang, Yi Su, Yunfan Wu, Zexuan Cheng, Zhanyu Liu, Zixing Zhang, Bin Zhang, Boxuan Wang, Chaoyi Ma, Chengru Song, Chenhui Wang, Chenglong Chu, Di Wang, Dongxue Meng, Dunju Zang, Fan Yang, Fangyu Zhang, Feng Jiang, Fuxing Zhang, Gang Wang, Guowang Zhang, Han Li, Honghui Bao, Hongyang Cao, Jiaming Huang, Jiapeng Chen, Jiaqiang Liu, Jinghui Jia, Kun Gai, Lantao Hu, Liang Zeng, Qiang Wang, Qidong Zhou, Rongzhou Zhang, Shengzhe Wang, Shihui He, Shuang Yang, Siyang Mao, Sui Huang, Tiantian He, Tingting Gao, Wei Yuan, Xiao Liang, Xiaoxiao Xu, Xugang Liu, Yan Wang, Yang Zhou, Yi Wang, Yiwu Liu, Yue Song, Yufei Zhang, Yunfeng Zhao, Zhixin Ling, and Ziming Li. OneRec-V2 Technical Report, August 2025b. 28 Table 9 Experiments on different hyperparameters for Node2Vec-based random walk algorithm to construct synthetic UIH. Based on the metrics, we selected 1st_order_a015_thresh1e-09, which is highlighted in gray. Config bfs_p05q2_a10_threshNone bfs_p05q2_a15_threshNone bfs_p05q2_a20_threshNone bfs_p05q2_a10_thresh1e-09 bfs_p05q2_a15_thresh1e-09 bfs_p05q2_a20_thresh1e-09 bfs_p05q2_a10_thresh2e-09 bfs_p05q2_a15_thresh2e-09 bfs_p05q2_a20_thresh2e-09 bfs_p05q2_a10_thresh3e-09 bfs_p05q2_a15_thresh3e-09 bfs_p05q2_a20_thresh3e-09 1st_order_a015_threshNone 1st_order_a015_thresh1eAlpha 0.10 0.15 0.20 0.10 0.15 0.20 0.10 0.15 0.20 0.10 0.15 0.20 0.15 0.15 Thresh None None None 1e-09 1e-09 1e-09 2e-09 2e-09 2e-09 3e-09 3e-09 3e-09 None 1e-09 Length 14.0 10.6 9.0 8.6 8.1 7.6 8.4 8.0 7.5 8.3 7.9 7.5 10.6 8.0 Items Gini 0.732 189K 0.697 183K 0.672 179K 0.660 178K 0.650 176K 0.643 174K 0.656 177K 0.648 175K 0.641 174K 0.655 177K 0.647 175K 0.639 174K 0.680 188K 0.635 180K Tokens 1,580 1,577 1,584 1,574 1,573 1,568 1,571 1,569 1,566 1,569 1,571 1,570 1,579 1,581 Token Gini Geom Lift 0.856 0.851 0.849 0.845 0.844 0.844 0.844 0.844 0.843 0.844 0.844 0.843 0.850 0. 353.00 372.28 385.22 368.91 384.71 388.84 375.29 384.51 390.54 376.11 384.48 392.59 376.80 385."
        },
        {
            "title": "A Hyperparameter Tuning for Synthetic UIH Generation",
            "content": "To generate the highest quality Synthetic User Interaction Histories (UIH), we conducted comprehensive hyperparameter sweep of our Node2Vec-based random walk algorithm. We evaluated 14 distinct configurations, including 1st-order (DeepWalk-style) baseline and 12 variants of 2nd-order, BFS-like strategy (p = 0.5, = 2.0). The sweep varied two key parameters: 1. αstop: The probability of terminating walk at any given step. 2. path_conf_threshold: minimum cumulative path confidence threshold used to filter out low-quality sequences."
        },
        {
            "title": "B Methodology for Semantic Tokenization",
            "content": "This appendix details the decision-making framework and methodology for determining the semantic item representation (<RECTOKEN>). As this representation serves as the fundamental vocabulary for our LLM, its quality is paramount. We compare two distinct strategiesa pre-trained Sparse Autoencoder (SAE) and an in-domain Residual-Quantized VAE (RQ-VAE)and justify the selection of the latter based on risk analysis of semantic mismatch versus data leakage. B.1 Candidate Methodologies We evaluated two primary approaches for generating discrete item tokens: Approach 1: Pre-trained Sparse Autoencoder (SAE). This method utilizes large-scale SAE originally trained on an internal multi-modal dataset comprising video, photo, and text data. The procedure involves passing the items textual description through the pre-trained embedding model and then into the SAE. The top-k activated features (concepts) from the SAE are selected as the tokens. Potential Strength: Leverage of \"foundation\" encoder with exposure to massive, diverse multimodal data. Critical Risk (Semantic Mismatch): The SAE features were learned in video-centric domain. Applying this vocabulary to e-commerce product text introduces severe risk of negative transfer, 29 where the \"concepts\" extracted are mathematically proximal but semantically incongruous for the recommendation task. Approach 2: In-Domain Residual-Quantized VAE (RQ-VAE). This method involves training hierarchical RQ-VAE from scratch specifically on the target dataset (merrec). We utilize the Qwen2.5-7B model to generate high-quality text embeddings for all items. The RQ-VAE is then trained to quantize these embeddings into sequence of discrete codes using residual codebook structure. Potential Strength: The learned vocabulary is perfectly aligned with the target domains semantic distribution, as it is trained directly on the item descriptions. Critical Risk (Data Leakage): naive implementation that trains the tokenizer on the full dataset would leak test-set information into the training process, invalidating downstream scaling law experiments. Approach 3: In-Domain RQ-kmeans. This method is similar to RQ-VAE but more stable and scalable. In our experiment we found using 6 layers of codebook and 256 codes per layer, the RQ-kmeans could reach collision rate 5.33% but RQ-VAE is stuck at 50.54%. B.2 Selected Methodology and Mitigation Protocol We selected Approach 1 (SAE) as the our tokenization method after ablation against RQ-kmeans (6 layers of codebook and 256 codes per layer, after sweeping hyper-parameters Table 10). Figure 12 shows the comparison of scaling laws based on SAE vs RQ-Kmeans on 4B models. This figure shows SAE consistently outperformed RQ-Kmeans across all domains and steps. 30 Figure 12 Scaling laws on 4B models with two tokenizaton methods SAE vs RQ-Kmeans. The other settings are exactly the same. SAE clearly outperformed SAE across all domains. Table 10 RQ configuration sweep on Qwen-0.6B embeddings (64.1M items). All experiments use beam size 5 with progressive search and uniform sampling. Collision rate measured after uniform sampling. Layers Codes/Layer 3 3 5 6 8 256 512 256 256 128 Total Codes 2563 (16M) 5123 (134M) 2565 (1.1T) 2566 (281T) 1288 (72057T) Collision % 82.7 37.4 9.7 3.5 1."
        }
    ],
    "affiliations": [
        "Meta"
    ]
}