{
    "paper_title": "How to Synthesize Text Data without Model Collapse?",
    "authors": [
        "Xuekai Zhu",
        "Daixuan Cheng",
        "Hengli Li",
        "Kaiyan Zhang",
        "Ermo Hua",
        "Xingtai Lv",
        "Ning Ding",
        "Zhouhan Lin",
        "Zilong Zheng",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 9 8 6 4 1 . 2 1 4 2 : r HOW TO SYNTHESIZE TEXT DATA WITHOUT MODEL COLLAPSE? Xuekai Zhuα,β, Daixuan Chengβ, Hengli Liβ,δ, Kaiyan Zhangγ, Ermo Huaγ, Xingtai Lvγ, Ning Dingγ, Zhouhan Linα,ε, Zilong Zhengβ, Bowen Zhouγ,ε αLUMIA Lab, Shanghai Jiao Tong University βState Key Laboratory of General Artificial Intelligence, BIGAI γDepartment of Electronic Engineering, Tsinghua University δInstitute for Artificial Intelligence, Peking University εShanghai Artificial Intelligence Laboratory {xuekaizhu0,lin.zhouhan}@gmail.com, zlzheng@bigai.ai"
        },
        {
            "title": "ABSTRACT",
            "content": "Model collapse in synthetic data indicates that iterative training on self-generated data leads to gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-{n} models will inevitably be trained on blend of synthetic and humanproduced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised finetuning. The results validate our theoretical proof that token-level editing improves data quality and enhances model performance."
        },
        {
            "title": "INTRODUCTION",
            "content": "As generative artificial intelligence (AI) (Rombach et al., 2021; Achiam et al., 2023) becomes increasingly prevalent in research and industry, synthetic data will proliferate throughout the web data ecosystem. Consequently, future training of GPT-{n} on mixture of synthetic and humanproduced data will be inevitable. Thus, model collapse is critical concern that must be considered when training models on synthetic data. Model collapse refers to degenerative process in which the output data of learned generative models contaminates the training sets of subsequent generations. As shown in Figure 1, iterative training coupled with data synthesis induces progressive accumulation of test errors (Shumailov et al., 2024; Dohmatob et al., 2024a). Consequently, generative models increasingly overfit to synthetic data distributions, failing to capture the complexity in human-produced data. Through successive iterations in Figure 1, these distortions accumulate, finally undermining the models capacity for generalization. Recent studies focus on two aspects. First, theoretical foundations of model collapse. Shumailov et al. (2024) and Dohmatob et al. (2024a) identify the model collapse phenomenon and formalize theoretical framework based on linear regression. Gerstgrasser et al. (2024) demonstrate that if synthetic data is accumulated while retaining the initial real data, the test error will be bounded, thus breaking model collapse. Dohmatob et al. (2024c;b) indicate that missing long tails of synthetic Corresponding Author. 1 Figure 1: Model collapse of synthetic data. ① The model continuously trains on its previously generated data, leading to gradual decline in model performance, i.e., model collapse. Starting from real data (xo, yo), the test error Etest increases as f0 undergoes iterative training on synthetic data (y1, y2, . . . , yn). ② ToEdit (ours), we use trained model for token-level editing rather than purely synthesizing data. Leveraging f0 and an operation matrix mi to edit the data, the test error is constrained within fixed upper bound. Therefore, we can preserve the distribution coverage to avoid model collapse. data lead to scaling law cutoff. Second, practical implementations on synthetic datasets by diverse prompting. Synthetic datasets (Trinh et al., 2024; Cui et al., 2023; Zhang et al., 2024) have been proven to boost the capabilities of language models. Cheng et al. (2024a;b); Maini et al. (2024) rephrase text into more formal styles, thereby improving the data quality. There are still two key questions that require further investigation: (Q1) What is the impact of synthetic data on language model training? (Q2) How can data be synthesized without causing model collapse? In this paper, we address the first question by training language models on varying mixtures of synthetic and human-produced data, demonstrating non-iterative model collapse. Unlike the original model collapse setting which iteratively trains on self-generated data, we directly mix synthetic and human-produced data to create training datasets with different mixing ratios. The results show negative correlation between performance and the proportion of synthetic data. Subsequent statistical analysis on distributions and features indicates coverage collapsesynthetic data covers only small portion of the human-produced data distributionand over-concentration of synthetic n-gram features. Based on the above findings, we address the second question by proposing token-level editing (ToEdit), resamples and replaces data points with relatively high model confidence. As illustrated in Figure 1, ToEdit preserves distribution coverage and theoretically constrains test error within fixed upper bound. Furthermore, extensive experiments across pre-training from scratch, continual pre-training, and supervised fine-tuning confirm its positive impact on model performance. Contributions. We summarize the key contributions of this work as follows: We demonstrate non-iterative model collapse by pre-training language models on mixture of synthetic and human-produced data ( 2.1): directly mixing pure synthetic data1, without iterative training, leads to performance degradation. We perform distributional statistical analysis, revealing that synthetic data leads to coverage collapse and over-concentration of n-gram features. Even subsequent data selection struggles to correct the distribution ( 2.2). 1In short, pure synthetic data is generated entirely by language models, while semi-synthetic data comes from limited modifications to human-produced data. Detailed definitions are provided in 5.2. 2 Table 1: PPL evaluation results for GPT-2 Small (124M) pre-trained on data mixture. The PPL increases as the proportion of synthetic data grows, providing further confirmation of Figure 2. ArXiv Books2 Books3 Math Enron EuroParl FreeLaw GitHub PG-19 HackerNews Human data 25% Synthetic Data 50% Synthetic Data 75% Synthetic Data Synthetic Data 22.26 21.86 22.50 24.35 35. 25.39 26.32 28.01 31.19 43.72 22.87 23.87 25.75 28.98 47.72 10.84 11.05 10.84 11.81 17.25 23.50 24.85 26.56 30.30 66.97 30.73 35.02 41.99 56.32 129.75 12.04 12.84 14.02 16.03 29. 4.15 4.35 4.67 5.30 12.00 16.88 17.99 19.70 22.75 50.14 32.54 33.80 36.12 40.44 87.95 NIH 23.53 23.76 24.61 26.19 39.48 OpenSubts OWT Phil Pile-CC PubMed-A PubMed-C StackEx Ubuntu USPTO Wikipedia Youtube Avg Human data 25% Synthetic Data 50% Synthetic Data 75% Synthetic Data Synthetic Data 28.08 29.25 31.00 34.18 57. 25.77 26.94 28.76 32.04 53.94 33.56 34.63 37.48 42.39 78.18 26.78 27.83 29.36 32.17 54.69 18.97 19.55 20.51 22.33 34.82 15.49 15.38 15.89 16.92 23.87 10.81 11.03 11.54 12.55 20. 20.86 22.32 23.53 26.54 51.78 19.32 19.58 20.51 22.21 37.24 24.31 25.88 27.57 30.68 46.12 21.54 22.63 24.91 28.98 65.49 21.37 22.31 23.90 27.03 49.30 Figure 2: Non-iterative model collapse. Training language models from scratch on AI-synthesized data or mixture of human and synthetic data leads to performance degradation. This degradation is negatively correlated with the proportion of synthetic data used in training. A. We pre-train GPT2 Small (124M) on human (Dolma (Soldaini et al., 2024)) and synthetic (Cosmopedia (Ben Allal et al., 2024)) data. As the proportion of synthetic data increases, the models loss decreases. B. As the proportion of synthetic data increases, the PPL also rises. This trend remains consistent across different validation sets. More results on downstream tasks are presented in 10 and 11. We propose token-level editing with theoretical proof to prevent model collapse ( 3) and validate its effectiveness through experiments spanning pre-training from scratch, continual pretraining, and supervised fine-tuning of language models ( 4)."
        },
        {
            "title": "2 NON-ITERATIVE MODEL COLLAPSE",
            "content": "In this section, we conduct pre-training on synthetic data mixtures and explore the reasons behind non-iterative model collapse. Prior studies (Shumailov et al., 2024; Dohmatob et al., 2024a) investigate the curse of recursion, where iterative training on self-generated data leads to degenerative process known as iterative model collapse. In contrast, we investigate non-iterative model collapse, training model directly on data synthesized by other models. More discussion is provided in 5.1. 2.1 PRE-TRAINING ON DATA MIXTURE Extensive prior work have proved synthetic data can boost language models capabilities, including instruction following (Wang et al., 2022), reasoning (Zhu et al., 2023; Trinh et al., 2024), alignment (Cui et al., 2023), biomedicine (Zhang et al., 2024) and so on. In this section, we investigate the impact of synthetic data on pre-training. Compared with prior studies focused on SFT and RLHF, we examine synthetic data integration in more fundamental stage of language model. Setup We define the mixing ratio between human-produced and synthetic data as α, where 0 α 1. The total amount of training data Dtotal is expressed as combination of human-produced data Dhuman and synthetic data Dsynthetic, represented by the formula: Dtotal = αDhuman + (1 α)Dsynthetic. 3 Figure 3: PPL distribution of human and synthetic data estimated by Llama-3-8B. The synthetic data lacks the long tail of the human-produced data and is also concentrated within the first 25% of the human-produced data distribution. A. Distribution of human-produced data is sharp with long tail, spanning wide range from 0 to over 100. B. The values are concentrated within much narrower range, mostly between 0 and 12. The experiment uses Dolma v6 and Cosmopedia as human and synthetic data, each with sampled 6B tokens. More results in Figure 9. We use Dolma (Soldaini et al., 2024) as source human-produced data. We use Cosmopedia (Ben Allal et al., 2024) as the source synthetic data, which is distilled from Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024). We train GPT-2 (Radford et al., 2019) and OLMo (Groeneveld et al., 2024) from scratch, using data mixtures containing 50B tokens each. The PPL evaluation sets are drawn from Paloma benchmark (Magnusson et al., 2023)2 and the 22 sub-domain validation sets of the Pile (Gao et al., 2020b), as used in (Maini et al., 2024). Finding I: Incorporating pure synthetic data harms the language models pre-training. Main results are presented in Figure 2, the PPL on real-world validation sets increases as the proportion of synthetic data grows, indicating degraded model performance. When training from scratch, synthetic data does not benefit the model and may even hinder its learning process. However, incorporating human-produced data into the training mixture mitigates model collapse to some extent. Compared to previous research on iterative model collapse (Shumailov et al., 2024; Dohmatob et al., 2024a;c), the non-iterative damage caused by synthetic data is more concerning and directly relevant to the training of next-generation language models. 2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING? We conduct three statistical analyses: (1) sample-level distribution, (2) feature-based overlap, and (3) distribution-reference data selection. The experimental results reveal that, compared to humanproduced data, synthetic data lacks long-tail coverage and suffers from coverage collapse. The limited diversity and concentrated features in synthetic data make using human-produced data as reference to select synthetic data particularly challenging. Setup We conduct statistical and feature-based analyses to explore why synthetic data fails in pretraining. (1) We leverage prior distribution to estimate the human-produced and synthetic data. We use Llama-3-8B (AI@Meta, 2024) and StableLM-Zephyr-3B (Bellagente et al., 2024). Different priors consistently yield the same results. (2) We analyze the n-gram features of human-produced and synthetic data from feature-based perspective, such as n-gram response values. (3) Based on the distribution of human-produced data, we sample data from the synthetic dataset that closely matches the human-produced data distribution in an attempt to filter the synthetic data. Finding II: Synthetic data distribution not only misses long tails, but also causes coverage collapse. Figure 3 and 9 illustrate that the PPL of synthetic data is confined to the lower 25% of the human-produced data, failing to capture the full range and complexity of human-produced data 2Paloma-bench is specifically designed for PPL evaluation, particularly for Dolma. And the Dolma team has already performed data de-contamination to prevent data leakage. 4 Figure 4: A. Embedding visualization using t-SNE and sentence-transformers. B. pre-training results for selected synthetic data and other data mixtures. distributions. Specifically, as illustrated in Figure 3A, human-produced data exhibit wide distribution in the range [1, 100+], characterized by sharp peak and pronounced long tail. In contrast, as shown in Figure 3B, the synthetic data is confined to narrower range of [0, 14], displaying smoother distribution. Additional results of StabLM are shown in Figure 9. While the absolute PPL ranges estimated by different models may vary, the relative shapes and proportional ranges of these two distributions remain consistent. This phenomenon provides evidence that when scaling up to larger synthetic datasets, there will be notable absence of the long tail. In summary, we observe more severe coverage collapse. This limited coverage reduces the datas ability to generalize effectively and may contribute to model collapse, as shown in Figure 2. Finding III: Synthetic data over-concentrates Ngram features. Based on the above distribution estimate, we further analyze why synthetic data fails at the feature level. Figure 10 and 11 demonstrate that synthetic data exhibits higher frequencies of certain bi-grams compared to human-produced data. To further examine feature-level differences, we hash unigram and bi-gram features into 10,000 hash buckets. As illustrated in Figure 5, human-produced data displays noticeably broader response range, while synthetic data features are concentrated in few specific buckets. This indirectly supports our earlier observation of feature over-concentration. We then expanded the hash bucket range to 1,000 20,000 buckets and used locality-sensitive hashing method to differentiate the features more precisely. Nevertheless, the results remained consistent. As shown in Figure 12, the majority of the response values are close to zero. Distinguishing features in synthetic data remains challenging. Finding IV: Distribution shifting cannot be mitigated through data selection. Inspired by recent data selection works (Xie et al., 2023; Albalak et al., 2024), we try to leverage the human-produced data features as reference distribution for selecting synthetic samples. We apply importance sampling from DSIR (Xie et al., 2023) to filter synthetic data. As shown in Figure 4A, the sampled data still fails to align with human-produced data in the embedding space, even at the boundary regions of the synthetic data. As illustrated in Figure 4B, the Figure 5: Uni/Bi-gram feature distribution across 10,000 hash buckets. 5 training results of selected synthetic samples still fluctuates around the original performance of the synthetic data, indicating that even biased sampling cannot correct the distributional shift."
        },
        {
            "title": "2.3 PROPOSED STRATEGY",
            "content": "Following these lessons so far, due to the coverage collapse and feature over-concentration properties of synthetic data, the best approach is to rely entirely on human-produced data and avoid including synthetic data. However, we are still wondering how synthetic data can be used to enhance human-produced data. This leads to general guideline for synthetic data: relying solely on synthetic data leads to model collapse, so preserving the primary humanproduced data distribution is essential. In that case, we propose token-level editing, which leverages prior distribution to adjust the data. Our method can maintain the source distribution while improving the source data, called semi-synthetic data."
        },
        {
            "title": "3 TOKEN-LEVEL EDITING",
            "content": "Figure 6: U-shape token probability distribution of Dolma-sampled V6 estimated by Qwen-0.5BInstruct (qwe, 2024). In this section, we introduce token-level editing as method for generating semi-synthetic data. Furthermore, we present theoretical analysis and proof demonstrating that the test squared error of our method has finite upper bound, regardless of the number of iterations. Consequently, our method not only prevents model collapse but also achieves improved performance. 3.1 METHOD We formulate data synthesis as follows: assuming is prior distribution, given sequence of tokens = (x1, . . . , xt), the full synthetic data is = (y1, . . . , yn). The synthesis process is derived as: (y1, . . . , yn x1, . . . , xt) = (cid:89) i=1 (yi y1, . . . , yi1, x1, . . . , xt). (1) This conditional probability formulation outlines the generation of synthetic data conditioned on the given token sequence. Then the synthetic data is used to train models. Inspired by previous studies of data selection (Mindermann et al., 2022; Ankner et al., 2024; Lin et al., 2024), prior distributions can serve as pointers indicating useless or learnable samples. In this case, we use pre-trained language model to infer the pretraining corpus. As illustrated in Figure 6, even model pre-trained on trillions of tokens can not fit the pretraining corpus perfectly. Specifically, 75% is under 0.6 probability. The tokens with both high and low probabilities are the most concentrated, suggesting the potential for data filtering. We leverage this U-shape distribution as pointer to resample tokens. Specifically, we use language model as prior distribution to compute each tokens conditional probability (x) if the probability exceeds certain threshold (x) p, it indicates that the token is easy to learn, and we proceed with resampling at that point. Token-level Editing doesnt generate the entire sequence but leverages conditional probability (xi x1, . . . , xi1) to revise the input sequence. In this way, we can avoid using purely synthetic data while modifying the dataset to preserve long-tail features of human-produced data, aiming to obtain higher-quality semi-synthetic data. Token-level Editing can be formulated as follows: = (cid:26)xi, xi, if (xi x1, . . . , xi1) < if (xi x1, . . . , xi 1) (2) 6 Where is the final token in the edited sequence. xi is token resampled from prior distribution. We can adjust the threshold to balance retaining the structure of human-produced data while avoiding overfitting to synthetic data."
        },
        {
            "title": "3.2 THEORETICAL ANALYSIS",
            "content": "To gain deeper mathematical insights, we utilize an analytical framework of the linear model and adopt notations in prior research (Mobahi et al., 2020; Dohmatob et al., 2024a; Gerstgrasser et al., 2024). This theoretical framework primarily considers linear model that iteratively trains on its own generated data, similar to pipelines like self-play and self-distillation, but without complex constraints. The process involves training continuously on the data generated by the previous generation of the model. Dohmatob et al. (2024a) point out that with iterative training, test errors accumulate progressively, eventually leading to model collapse. Building on this theoretical framework, we integrate our proposed token editing method and analyze whether our method can prevent model collapse. Notation and Preliminaries For given distribution PΣ,w,σ2 , the data (x, y) PΣ,w,σ2 on Rd R, where is drawn from multivariate normal distribution (0, Σ), ϵ is an independent noise term sampled from (0, σ2), and the label is given by the linear model = + ϵ. Iterative Data Editing Process We utilize the model obtained from the previous round of training to make limited number of modifications. Specifically, we resample and replace data points with relatively high confidence. The editing operations are defined by the matrices {M1, M2, . . . , Mn}. The iterative data synthesis and model-fitting process is formalized as follows: PΣ,w,σ2 PΣ, ˆw1,σ2 . . . PΣ, ˆwn,σ2, where is the number of iterations. The detailed process of data editing and iterations is described as follows: For = 1, we begin by initializing the covariates/features as X1 = X. The target values are defined as Y1 = ˆY1 = Xw + E1, where E1 (0, σ2IT ). The linear model is then fitted by solving for Y1. To proceed to the next iteration, we resample the data, obtaining ˆY2 = ˆw1 + E2, ˆw1 = 1 with E2 (0, σ2IT ). For 2, the input covariates/features remain as using the edited targets, following the equation model is then fitted by computing ˆwn = yielding ˆYn+1 = ˆwn + En+1, where En+1 (0, σ2IT ). The matrix Mn is diagonal matrix, where some diagonal elements are 1, while others are 0. The multiplication by can be interpreted as an operation that selectively modifies certain data points (those corresponding to 1s) while retaining others (those corresponding to 0s). Then, the data editing process can be formulated as follows: = X, while the target values are updated = Mn1 ˆYn + (1 Mn1) Yn1. The linear Yn. Finally, the data is resampled for the next iteration, = Mn1 ˆYn + (1 Mn1) Yn1 (3) where Yn1 is the data after editing in the n1 generation, and ˆYn is the synthetic data from the n-th generation. This process can be described as: firstly, synthesizing labels for all inputs; secondly, the matrix determining which data is edited and which is retained. For matrix with full column rank, its Moore-Penrose pseudo-inverse is A+ = (AA)1A. The noise terms E1, E2, . . . , En are independent of each other and the covariates/features. Since has full column rank, Xn retains this property for all 1. Test Error Model collapse is ultimately reflected through test error. Following previous work, we adopt the standard test error definition as presented in (Gerstgrasser et al., 2024). For any linear estimator ˆw derived from the training data, we evaluate the test error using the standard method as follows: Etest(w) def= (cid:2)(xT testw ytest)2(cid:3) σ2 = E[w w2 Σ] (4) 7 where the expectation is computed with respect to the training data, while the test pair (xtest, ytest) is sampled independently from PΣ,w,σ2 of the training set."
        },
        {
            "title": "3.3 TEST ERROR UNDER DATA EDITING",
            "content": "Our goal is to derive an analytical expression for the test error of the n-th model in the data editing setting. As indicated by the test error in Eq. 4, this process involves two main steps: (1) establishing the relationship between the fitted linear parameters wn and the true parameters w, and (2) simplifying the test error expression. We begin by formulating the relationship between wn and w. Proofs are detailed in Appendix A. Theorem 1 In the data editing setting, 1, the fitted linear parameters ˆwn+1 can be derived as: ˆwn+1 = + (X X)1X E1 + (cid:32) (cid:33) MiEi+1 (cid:88) i=1 (5) where, is the true parameter, is the original design matrix, Ei is the extra noise added at the ith iteration, and Mi is an idempotent diagonal matrix, defining the editing operation. Theorem 2 Consider an + 1 fold data editing process with + 2 samples per iteration and def = Id), the test error for the ridgeless linear model ˆwn learned on the edited isotropic features (Σ data up to iteration + 1, is bounded by: Etest( ˆwn+1) 2σ2d 1 (6) Furthermore, assuming the editing operation satisfies Mi = Mi1η with η (0, 1), the test error can be further bounded by: Etest( ˆwn+1) σ2d + σ2(cid:113) [tr ((X X)2)] (cid:112)E [tr(M1)] 1 η (7) Recalling that the cause of model collapse (Dohmatob et al., 2024a): training iteratively on synthetic data leads to an accumulation of error over iterations, as shown in the following equation: Ecollapse test ( ˆwn) = σ2d 1 (8) Comparing Eq. 6 with Eq. 8, the test error under data editing is bounded by fixed value, preventing continuous error accumulation and thus avoiding model collapse. Based on the theoretical derivations and statistical analysis of synthetic data ( 2.1), the underlying reason is that our approach retains the coverage of the initial distribution. We move away from pure data synthesis toward token-level editing, which allows us to obtain better data while avoiding model collapse. Moreover, remarkable previous studies (Dohmatob et al., 2024c; Gerstgrasser et al., 2024) pointed out similar conclusions. They indicated mixing real data with synthetic data will break model collapse and provide an upper bound under data accumulation. Different from their work, our data editing aims to yield better data, enabling synthetic data to perform well both in theory and practice, not only avoiding model collapse."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "To validate our proposed method, we conduct experiments across three stages of language model training including: pre-training, continual pre-training (CPT) and supervised fine-tuning (SFT). 8 4."
        },
        {
            "title": "IMPLEMENTATION",
            "content": "We use the Llama-3-8B (AI@Meta, 2024) as prior distribution to estimate the token distribution in each text sample. The modification probability is set to = 0.99. This means that we resample tokens in positions where the probability exceeds p, and the resampling is based on the conditional probability given the preceding context. The entire process requires only single forward pass, without auto-regressive generation. We integrate the fast inference engine vLLM (Kwon et al., 2023), allowing the entire data editing process to be completed on single 4090 GPU. After completing the data editing, we compare the original data and the edited data on language model training performance across pre-training, CPT, and SFT. Here, we used top-k as the sampling strategy with = 8. We also experimented with top-p and rejection sampling, which produced similar results."
        },
        {
            "title": "4.2 DATASETS AND MODELS",
            "content": "We provide an overview of our experimental setup. More training details are presented in Appendix C. As for pre-training, we pre-train the 1B OLMo model (Groeneveld et al., 2024) from scratch, using Dolma-sampled V6 (6B tokens) as the pre-training corpus. We use 8 general tasks in lm-evaluation-harness (Gao et al., 2024) to evaluate for pre-training models. As for continual pre-training, we follow Cheng et al. (2024a;b;c) to continual pre-train the OLMo-1B (Groeneveld et al., 2024) and Llama-3-8B (AI@Meta, 2024) on Biomedicine, Finance and Math. Each domain corpus contains 1B tokens. Correspondingly, we evaluate the continual pre-training models using 15 downstream tasks, with 5 tasks from each domain. As for supervised fine-tuning, we finetune Llama-3-8B on instruction tuning and code reasoning tasks, including 6 datasets in total. We evaluate the SFT models using 9 downstream tasks designed to measure instruction-following capabilities. All Llama-3-8B experiments use LoRA (Hu et al., 2021), while the OLMo-1B model is trained with full parameters. Table 2: Performance on domain-specific tasks for continual pre-training models. CPT indicates continual pre-training. denotes training with our edited data. Our method demonstrates consistent improvements across three domains on both OLMo-1B and Llama-3-8B. Biomedicine ChemProt PubMedQA Models OLMo-1B CPT ToEdit LLama-3-8B CPT ToEdit MQP 52.59 52.29 54.59 66.80 72.29 76.39 Models HeadLine OLMo-1B CPT ToEdit LLama-3-8B CPT ToEdit 69.00 70.31 71.77 81.28 85.68 83.83 17.2 21.00 22. 28.59 29.4 30.2 FPB 47.03 49.78 51.39 63.58 54.22 61.61 RCT 32.70 34.90 34. 73.85 72.65 73.30 51.40 58.50 65.00 60.8 69.1 65.3 Finance FiQA-SA ConvFinQA 48.05 40.36 46.06 81.60 81.88 80.82 Math 4.83 18.72 18.85 52.88 67.78 67.31 USMLE Average 28.90 27.49 27.96 40.61 36.76 37.23 NER 62.19 60.44 62.97 72.53 67.43 67.62 36.63 38.83 40. 54.13 56.04 56.48 Average 46.22 47.92 50.21 70.37 71.40 72.24 Models OLMo-1B CPT ToEdit ARC-c GPQA GSM8K MATH MMLU Average 0.00 0.10 0.10 26.56 27.23 23.63 16.23 16.26 16.59 28.67 28.41 28.92 24.23 24.03 28.12 1.67 1.52 2. 9 Table 3: General performance of the pre-trained base models. PT indicates we pre-train OLMo-1B from scratch. Experimental results demonstrate that our method can also enhance the effectiveness of pre-training. PIQA BoolQ OBQA ARC-c ARC-e HellaSwag SIQA Winogrande Average OLMo-1B (PT) ToEdit 53.97 54. 38.26 38.65 12.20 12.80 17.23 18.43 28.36 27.48 26.02 25.94 34.80 34. 51.14 52.49 32.75 33.11 Table 4: Performance of the SFT models. We fine-tune LLaMA-3-8B using instruction tuning and code reasoning tasks, comparing performance with the edited version produced by our method. The experimental results indicate that our approach can enhance the data for instruction-tuning and code reasoning tasks. Models PIQA BoolQ HellaSwag SIQA Winogrande Average Instruction Tuning Natural Instructions CoT FLAN v2 Open Assistant 1 Llama-3-8B 79.82 ToEdit 80. Llama-3-8B 79.87 ToEdit 80.25 Llama-3-8B 80.79 ToEdit 80.69 Llama-3-8B 79.65 ToEdit 79.98 87.06 87.80 81.28 81.16 84.04 85. 83.18 83.91 58.32 58.27 59.72 59.74 59.98 59.99 60.51 60.34 46.83 46. 49.69 50.56 51.43 52.00 48.52 48.31 74.66 74.90 74.51 74.59 74.66 75. 74.11 74.66 69.34 69.70 69.01 69.26 70.18 70.65 69.19 69.44 Models ARC-c GPQA GSM8K MMLU Average Code Reasoning OSS-Instruct-75K Evol-Instruct-110K Llama-3-8B ToEdit Llama-3-8B ToEdit 51.28 51.79 52.90 52.22 27.46 28.79 27.90 29.69 49.58 49.36 50.87 50. 62.14 62.04 62.40 62.60 45.76 46.13 46.62 46.92 4.3 RESULTS Table 2, 3, and 4 respectively demonstrate the effectiveness of our method in continual pre-training, pre-training, and fine-tuning tasks. Across these three stages of language model training, our method enhances the models performance on downstream tasks without increasing the data size. Our method unlocks the potential of potential of existing data, demonstrating that semi-synthetic data is viable path to improving model performance. Specifically, as shown in Table 2, our method shows consistent improvements over the source data across OLMo-1B and LLaMA-3-8B. For instance, in the Biomedicine domain, the average score for OLMo-1B increased from 36.63 to 40.89 with ToEdit, while LLaMA-3-8B saw an increase from 54.13 to 56.48. Table 3 further supports the effectiveness of our approach in pre-training. The average performance of OLMo-1B increases from 32.75 to 33.11, reflecting improved generalization capabilities. While the improvement is modest, the consistent trend across tasks like PIQA, BoolQ, and ARC-c highlights the broader applicability of our method. As for SFT results in Table 4, using both the original and edited data, the results indicate small but consistent improvement. Specifically, ToEdit improves original FLAN v2, with average performance increasing from 70.18 to 70.65. For Natural Instructions, the average performance of LLaMA-3-8B improves from 69.34 to 69.70, with gains in tasks like Winogrande and SIQA. These improvements demonstrate the adaptability of our method to instruction-tuning tasks. For coderelated tasks, the improvements are particularly evident in ARC-c and GPQA, indicating better reasoning and code comprehension. 4.4 ABLATION STUDIES We conduct 4 experiments on hyper-parameter p, including: (1) ablation studies on different values, (2) token percentage statistics, (3) comparisons of sampling strategies, and (4) an ablation study on sampling size. 10 As Table 7 shows different values of influences on BioMed, with fluctuations observed across various settings. The Table 8 presents the distribution percentages across different probability value ranges. As mentioned above, we need to refine the data while preserving mainly source distribution. As shown in Figure 6, larger indicates fewer Table 5: Results of different sampling strategies. tokens will be resampled, while smaller rePubMedQA MedMCQA MedQA (4 options) sults in more tokens being resampled. Balancing performance and the preservation of data distribution, we set = 0.99 as threshold for our experiments. Top-k Top-p Reject Sampling Sampling Strategy 26.13 27.11 28.90 24.82 25.61 28.20 64.5 63.8 64.5 Sampling Size (k) Table 6: Ablation study on sampling size for top-k. Table 5 shows the results of different sampling strategies. Specifically, to control variables, we set = 8 for top-k sampling and = 0.99 for top-p sampling. We use reject sampling implementation in Liu et al. (2023). The results of reject sampling, top-p, and top-k are comparable. However, top-p involves dynamic sampling range, and reject sampling requires multiple rounds of computation, leading to increased overhead. Considering computational efficiency, we chose top-k for sampling. This aligns with our original objective of maintaining minimal computational overhead. This aligns with our initial objective of minimizing computational overhead as much as possible. The Table 6 shows the ablation study on sampling size of top-k. The improvement achieved with larger values is relatively small. Therefore, we chose = 8 in our experiments. PubMedQA MedMCQA MedQA (4 options) = 8 = 64 24.82 27.34 26.13 28. 64.5 63.8 Table 7: Performance impact of different resampled token condition (p) in Biomedicine domain. PubMedQA MQP RCT USMLE ChemProt Avg 0.99 0.999 0.1 0.01 0. 64.5 63.6 62.4 65.4 64.2 55.73 55.4 51.47 54.91 56.39 30.95 29.09 25.6 28.19 35.0 27.65 28.12 29.14 27.80 27.80 14.6 16.2 10.0 11.0 12.4 38.69 38.48 35.72 37.46 39."
        },
        {
            "title": "5 DISCUSSION",
            "content": "5.1 WHAT IS THE DIFFERENCE BETWEEN NON-ITERATIVE AND ITERATIVE MODEL COLLAPSE? We define non-iterative model collapse as the performance degradation caused by directly mixing general synthetic data with humanproduced data, without iterative training. Theoretically, without additional regularization constraints to guide data generation, the variance of the model-generated data gradually decreases during this process. The diversity of the generated data diminishes over time, ultimately leading to the collapse of the model itself. Table 8: Token distribution across different probability ranges in BioMed dataset. Probability Range Percentage Token Count 0.0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 34.7% 8.1% 5.4% 4.4% 3.8% 3.6% 3.7% 4.0% 5.2% 27.1% 388,626,330 90,716,809 60,477,872 49,278,266 42,558,503 40,318,546 41,438,924 44,798,424 58,238,944 303,543,988 From setting perspective: The difference between the two lies in their scope. Noniterative model collapse is not confined to training on self-generated data, allowing it to uncover broader properties of synthetic data. For instance, in our experiments, we train GPT-2 on the Cosmopedia dataset in single generation, which was generated by Mixtral-8x7B-Instruct-v0.1. In contrast, iterative model collapse focuses on training the model over multiple generations using self-generated data. From property perspective: The non-iterative model collapse emphasizes the gap between human data and general purely synthetic data, particularly regarding distributional properties and ngram features. In contrast, the iterative model collapse illustrates the iterative evolution of the model, resembling self-play process. This process illustrates the gradual evolution of self-generated data. It does not involve an analysis of the differences in nature between self-generated and human-produced data. They both ultimately lead to model collapse, driven by the same underlying causesynthetic data, though they investigate different aspects of synthetic data. The most common setting is training model on mixture of human and synthetic data, where the synthetic data is not generated by the model itself, and its exact origin may be unknown. Moreover, there are already numerous popular datasets, such as UltraChat and OpenOrca, that combine synthetic and human-produced data to improve training diversity and robustness."
        },
        {
            "title": "5.2 DEFINITION AND CHARACTERISTICS OF SYNTHETIC DATA",
            "content": "Synthetic data (Ds) can be categorized based on its relationship with the distributions of language model (PLM) and human-produced data (Pdata) during the generation process, quantified as = KL(Pdata): Ds = (cid:26)Dpure Dsemi PLM, Psemi, if KL(PLMPdata) > ϵ, if KL(PsemiPdata) ϵ. (9) : Generated entirely from the language model (Dpure where Pure Synthetic Data Dpure PLM), with KL divergence KL(PLMPdata) exceeding threshold ϵ. This implies significant deviation of the language models distribution from the human-produced data distribution. Semi-Synthetic Data Dsemi : Derived from limited modifications to human-produced data (Pdata), ensuring that the resulting distribution (Psemi) has KL divergence KL(PsemiPdata) bounded by ϵ. This reflects closer alignment of semi-synthetic data with human-produced data. From the generation process, pure synthetic data Dpure : This data is induced by language model through prompts and does not modify human-produced data, resulting in low overlap content with human-produced data. For example, Cosmopedia (Ben Allal et al., 2024) expands human-produced data and generates data without human-produced data. Semi-Synthetic Data Dsemi : This data is generated by directly modifying human-produced data, such as paraphrasing or token-level editing. It derives from transformations of human-produced data. For example, WRAP (Maini et al., 2024) generates paraphrases of human-produced data. ToEdit (ours) performs token editing on humanproduced data. 5.3 WHY DOES THE OBSERVED PROBABILITY DISTRIBUTION EXHIBIT FILTERING POTENTIAL? From the perspective of information theory, we can analyze the filtering potential of the U-shape distribution as follows: We utilize the U-shape distribution in Figure 6 to re-sample tokens in the high-probability region, to adjust the U-shaped distribution toward uniform distribution. By doing so, we can maximize the information entropy. According to information theory, maximizing information entropy is achieved when the distribution is uniform. Lemma 1: Let be discrete random variable with possible outcomes. If the probability of each outcome is uniform, i.e., (xi) = 1 for all {1, 2, . . . , n}, the Shannon entropy is maximized, given by: H(X) = (cid:88) i= 1 log 1 = log n. (10) This represents the maximum uncertainty achievable, implying that the dataset carries the maximum possible information content. Thus, the uniform distribution, which assigns equal probability to all outcomes, possesses the maximum information entropy. To leverage this property, we utilize the U-shape distribution to re-sample tokens in the high-probability region, adjusting the U-shaped distribution toward uniform distribution. By doing so, we can maximize the information entropy. From the perspective of language model learning, our method emphasizes the importance of poorly learned data. Specifically, we resample easy tokens and encourage the model to focus on learning more challenging ones. Our method can enhance the learning of underrepresented data by resampling high-probability tokens. 5.4 GRADUAL DECLINE IN EDITING 12 Table 9: Percentage of tokens requiring edits in the Natural-Instructions dataset. The total number of tokens is 4,671,834. and Gen is short for Generation. We present the percentage statistics of edited tokens in Table 9, demonstrating that the edited tokens indeed exhibit progressive decrease. Specifically, We observe that the percentage of edited tokens (above the threshold > 0.99) decreases as the generation number increases. Theoretically, this is process of distribution shifting. When tokens (p > 0.99) are resampled, randomness is introduced. The sampling process can select tokens with lower probabilities. Then, tokens (p > 0.99) is replaced, leading to reduction of edited tokens in subsequent generations. The Table 9 provides empirical evidence for this pattern of decay. 549,519 517,433 11.76% 11.08% Tokens (p > 0.99) Percentage 584,103 12.5% Gen 1 (source) Gen 2 Gen"
        },
        {
            "title": "6 RELATED WORK",
            "content": "Model Collapse Shumailov et al. (2024); Dohmatob et al. (2024a;c) demonstrate AI models trained recursively on data generated by earlier versions of themselves over time can result in performance degradation, ultimately rendering the AI model completely useless. This process can be formulated as follows: Etest( ˆwn+1) = σ2d 1 This indicates that the error will continuously increase with the number of iterations n. Dohmatob et al. (2024c) further point out that synthetic data also contribute to truncation of the scaling law. This phenomenon stems from the sampling strategy (e.g., Top-p) used during the language models generation process. Gerstgrasser et al. (2024) further adjust the data iteration setting by replacing data replacement with data accumulation during the iterative process. They demonstrate that data accumulation can prevent model collapse. Inspired by the above work, we believe that training language models on synthetic datasets will be inevitable in the future. Therefore, it is crucial to theoretically discuss how to prevent model collapse. Building on the above theoretical framework, we prove that token-level editing establishes an upper bound during the iterative process, thereby preventing the continuous accumulation of errors. Synthetic Data Phi-1/2 (Gunasekar et al., 2023) demonstrate that the synthetic data can boost training efficiency and performance compared to raw data in language model pre-training. Liu et al. (2024) highlight that synthetic data will play crucial role in the development of AI. For example, synthetic data can be used to construct highly specialized datasets, enhancing the performance of downstream tasks. Trinh et al. (2024) utilize synthetic math data to train 125M language model, which successfully solved 25 out of 30 selected problems from the International Mathematical Olympiad (IMO) problem set. Zhang et al. (2024) develop biomedical instruction dataset that was used to train specialized bio-models, enabling them to excel in answering questions related to medical exams and clinical scenarios. Eldan & Li (2023) introduce novel synthetic dataset and evaluation paradigm that enables small language models to generate coherent, diverse, and grammatically sound stories. As outlined above, in the post-training stages of LLMs, synthetic data enhances downstream task performance and aligns foundation models with humans. And Maini et al. (2024) propose rephrasing the pre-training data into Wikipedia or Q/A style to achieve better alignment with downstream tasks. Synthetic data is powerful tool for training. Our approach is also based on synthetic data methods. Instead of sampling data solely based on this prior, we modify the data using the prior as guide."
        },
        {
            "title": "7 CONCLUSION",
            "content": "With the growing prevalence of generative AI models like ChatGPT (Achiam et al., 2023) and Stable Diffusion (Rombach et al., 2021), when training next-generation AI models, it will be inevitable to use mixture of synthetic data and human-produced data. Therefore, we focus on two key questions: (1) What is the impact of synthetic data on language model pre-training, and what are the underlying causes? (2) How can we prevent model collapse and synthesize high-quality data? 13 We found that synthetic data can impair the effectiveness of pre-training when mixed with humanproduced data, leading to non-iterative model collapse. Statistical analysis reveals that synthetic data suffers from significant distribution gaps and overly concentrated n-gram features. To address this, we propose token-level editing instead of relying purely on synthetic data. Specifically, we perform token resampling guided by trained prior. Theoretically, our method can prevent model collapse. Experimentally, our approach demonstrates improvements over the source data across pre-training, continual pre-training, and supervised fine-tuning."
        },
        {
            "title": "REFERENCES",
            "content": "Qwen2 technical report. 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/ llama3/blob/main/MODEL_CARD.md. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024. Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew Leavitt, and Mansheej Paul. Perplexed by perplexity: Perplexity-based data pruning with small reference models. arXiv preprint arXiv:2405.20541, 2024. Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 technical report. arXiv preprint arXiv:2402.17834, 2024. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Cosmopedia, 2024. URL https://huggingface.co/datasets/HuggingFaceTB/ cosmopedia. Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu Wei. Instruction pre-training: Language models are supervised multitask learners. In Conference on Empirical Methods in Natural Language Processing, 2024a. URL https://api.semanticscholar. org/CorpusID:270620509. Daixuan Cheng, Shaohan Huang, and Furu Wei. Adapting large language models via reading comprehension. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=y886UXPEZ0. Daixuan Cheng, Shaohan Huang, Ziyu Zhu, Xintong Zhang, Wayne Xin Zhao, Zhongzhi Luan, Bo Dai, and Zhenliang Zhang. On domain-specific post-training for multimodal large language models. arXiv preprint arXiv:2411.19930, 2024c. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023. Elvis Dohmatob, Yunzhen Feng, and Julia Kempe. Model collapse demystified: The case of regression. arXiv preprint arXiv:2402.07712, 2024a. Elvis Dohmatob, Yunzhen Feng, Arjun Subramonian, and Julia Kempe. Strong model collapse. arXiv preprint arXiv:2410.04840, 2024b. Elvis Dohmatob, Yunzhen Feng, Pu Yang, Francois Charton, and Julia Kempe. tale of tails: Model collapse as change of scaling laws. arXiv preprint arXiv:2402.07043, 2024c. Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023. 14 Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. ArXiv, abs/2101.00027, 2020a. URL https://api.semanticscholar.org/CorpusID:230435736. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020b. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Henry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai, Andrey Gromov, et al. Is model collapse inevitable? breaking the curse of recursion by accumulating real and synthetic data. arXiv preprint arXiv:2404.01413, 2024. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hanna Hajishirzi. Olmo: Accelerating the science of language models. arXiv preprint, 2024. URL https://api.semanticscholar.org/CorpusID: 267365485. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richard Nagyfi, ES Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. Openassistant conversations - democratizing large language model alignment. ArXiv, abs/2304.07327, 2023. URL https://api.semanticscholar.org/ CorpusID:258179434. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data for language models. arXiv preprint arXiv:2404.07503, 2024. Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. ArXiv, abs/2309.06657, 2023. URL https://api.semanticscholar.org/CorpusID:261705578. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023. Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, A. Jha, Oyvind Tafjord, Dustin Schwenk, Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy, Hanna Hajishirzi, Noah A. Smith, Kyle Richardson, and Jesse Dodge. Paloma: benchmark for evaluating language model fit. ArXiv, abs/2312.10523, 2023. URL https://api.semanticscholar. org/CorpusID:266348815. Pratyush Maini, Skyler Seto, Richard He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephrasing the web: recipe for compute and data-efficient language modeling. In Annual Meeting of the Association for Computational Linguistics, 2024. URL https://api. semanticscholar.org/CorpusID:267312030. Soren Mindermann, Jan Brauner, Muhammed Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Holtgen, Aidan Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In International Conference on Machine Learning, pp. 1563015649. PMLR, 2022. Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation amplifies regularization in hilbert space. Advances in Neural Information Processing Systems, 33:33513361, 2020. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2021. Walter Rudin. Principles of Mathematical Analysis. McGraw-Hill, New York, 3rd edition, 1976. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755759, 2024. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024. URL https://arxiv.org/abs/2402.00159. Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 2024. doi: 10.1038/s41586-023-06747-5. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022. URL https://api.semanticscholar.org/ CorpusID:246411621. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. ArXiv, abs/2402.04333, 2024. URL https://api.semanticscholar.org/CorpusID:267522839. 16 Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36: 3420134227, 2023. Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, et al. Ultramedical: Building specialized generalists in biomedicine. arXiv preprint arXiv:2406.03949, 2024. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and In Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xinwei Long, Zhouhan Lin, and Bowen Zhou. Pad: Program-aided distillation can teach small models reasoning better than chain-of-thought finetuning. arXiv preprint arXiv:2305.13888, 2023."
        },
        {
            "title": "A PROOF",
            "content": "A.1 PROOF OF THEOREM 1 For = 1, we have: ˆw1 = 1 Y1 = (X X)1X (Xw + E1) = + (X X)1X E1 For 1, we have: Yn+1 ˆwn+1 = n+1 Xn+1)1 = ( = (X X)1X Yn+ n+1 n+1 Yn+1 Recalling that: Yi = (cid:26)Xw + E1, Mi1(X ˆwi1 + Ei) + (1 Mi1) Yi1, if = 1 if 2 + 1 Substituting this Yi into the expression for ˆwn+1: We begin the data editing data process: Y2 = M1(X ˆw1 + E2) + (1 M1) Y1 Y3 = M2(X ˆw2 + E3) + (1 M2) Y2 Then: We have: Y3 = M2(X ˆw2 + E3) + (1 M2) (cid:16) M1(X ˆw1 + E2) + (1 M1) Y1 (cid:17) = M2(X ˆw2 + E3) + (1 M2)M1(X ˆw1 + E2) + (1 M2)(1 M1) We can expand Yn+1 by recursively substituting the previous expressions: Yn+1 = Mn(X ˆwn + En+1) + (1 Mn) Yn (cid:104) (cid:105) = Mn(X ˆwn + En+1) + (1 Mn) = Mn(X ˆwn + En+1) + (1 Mn)Mn1(X ˆwn1 + En) + (1 Mn)(1 Mn1) Yn1 Mn1(X ˆwn1 + En) + (1 Mn1) Yn1 (14) ... (cid:88) = (cid:89) (1 Mj) Mi(X ˆwi + Ei+1) + i=1 j=i+1 Recalling properties of Mi: Y1 (1 Mj) (cid:89) j=1 Mi(1 Mi) = 0 and MiMj = 0 for (1 Mi)Mi = 0 = (1 Mi)(1 Mj) = 1 Mi Mj = for 18 (15) (16) (17) (18) (19) (20) (21) (11) (12) (13) Then we have: Yn+1 = = (cid:88) i=1 (cid:88) i=1 (cid:32) Mi(X ˆwi + Ei+1) + 1 Mi(X ˆwi + Ei+1) + 1 (cid:32) (cid:88) i=1 (cid:88) i=1 (cid:33) Mi Y1 (cid:33) Mi (Xw + E1) = Xw + E1 + (cid:88) i=1 Mi (X( ˆwi w) + (Ei+1 E1)) (22) (23) (24) Substituting this back into the expression for ˆwn+1: ˆwn+1 = (X X)1X (cid:34) Xw + E1 + (cid:88) i=1 Mi (X( ˆwi w) + (Ei+1 E1)) (25) (cid:35) = + (X X)1X E1 + (cid:34) (cid:88) i=1 MiX( ˆwi w) + (cid:35) Mi(Ei+1 E1) (26) (cid:88) i=1 We can observe: ˆw1 = (X X)1X (Xw + E1) = + (X X)1X E1 ˆw2 = + (X X)1X (cid:0)M1X(X X)1X E1 + M1E2 + (1 M1)E1 (cid:1) = + (X X)1X (E1 + M1E2) We prove this Theorem 1 by induction. Inductive Step: Assume the formula holds for n, we have: ˆwn+1 = + (X X)1X (E1 + M1E2 + M2E3 + + MnEn+1) = + (X X)1X E1 + (cid:32) (cid:33) MiEi+1 (cid:88) i=1 (27) (28) (29) (30) (31) Substitute ˆwi into ˆwn+1: Then we can get: ˆwn+1 = + (X X)1X E1 + = + (X X)1X E1 + = + (X X)1X E1 + (cid:32) where = X(X X)1X , i1 (cid:88) j= i1 (cid:88) j=1 MiP E1 + Mi Ei+1 + (cid:33) MiEi+1 (cid:88) i= (cid:88) i=1 (cid:88) i=1 MjEj+ + (cid:88) i=1 Mi(Ei+1 E1) MjEj+1 (32) (33) (34) (35) The above derivation aligns with Theorem 1, and the proof is complete. 19 A.2 PROOF OF THEOREM We substitute the Eq. 31 into Test Error Eq. 4: Etest( ˆwn+1) = (cid:32) (cid:13) (cid:13) (X X)1X (cid:13) (cid:13) (cid:13) (cid:88) i=1 MiEi+1 (cid:33)(cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) Σ (cid:32) E1 + (cid:33) MiEi+1 X(X X)2X E1 + (cid:32) = E1 + (cid:88) i=1 (cid:33) MiEi+1 (cid:88) i=1 = σ2E (cid:2)tr (cid:0)(X X)1(cid:1)(cid:3) + σ = σ2E (cid:2)tr (cid:0)(X X)1(cid:1)(cid:3) + σ2 (cid:88) i=1 (cid:88) i=1 (cid:2)tr (cid:0)Mi(X X)1Mi (cid:1)(cid:3) (cid:2)tr (cid:0)(X X)1Mi (cid:1)(cid:3) (36) (37) (38) (39) Further, by applying the Cauchy-Schwarz inequality (Rudin, 1976), we obtain: Etest( ˆwn+1) σ2E (cid:2)tr (cid:0)(X X)1(cid:1)(cid:3) + σ2(cid:113) [tr ((X X)2)] (cid:88) i=1 (cid:112)E [tr(Mi)] (40) We refer to the following lemma (Dohmatob et al., 2024a), which is essential for proving Theorem 2: Lemma 3 Let and be positive integers with + 2, and let RT be random matrix with i.i.d. rows from (0, Σ) with Σ positive definite. Then, has full rank a.s. Moreover, it holds that: EX (cid:2)(X X)1(cid:3) = 1 Σ1. Etest (cid:2)tr (cid:0)(X X)1(cid:1))(cid:3) = 1 Using Lemma 3, we have: Then, we have: Etest( ˆwn+1) = σ2E (cid:2)tr (cid:0)(X X)1(cid:1)(cid:3) + σ2 (cid:88) i=1 (cid:2)tr (cid:0)(X X)1Mi (cid:1)(cid:3) σ2d 1 + σ2(cid:113) [tr ((X X)2)] (cid:88) i=1 (cid:112)E [tr(Mi)] (41) (42) (43) (44) In our setting, the data is incrementally modified over iterations and modifications decreases progressively. This behavior can be modeled by the sum of geometric series, where the amount of modified data decreases by fixed ratio η with each iteration. Then, we assume the editing operation as Mi = Mi1η, for = 1, 2, . . . , n. Therefore, the test error for data editing can be bounded: Etest( ˆwn+1) σ2d 1 + σ2(cid:113) [tr ((X X)2)] (cid:112)E [tr(M1)] 1 η (45) Additionally, since Mi is not full-rank, as seen from Eq. 39, we can apply more relaxed and simplified bound, as follows: Etest( ˆwn+1) 2σ2d 1 (46) Thus, the above derivation satisfies the Theorem 2. 20 Compute conditional probability (xi x1, . . . , xi1) if (xi x1, . . . , xi1) then Algorithm 1 Token-level Editing 1: Input: Sequence of tokens = (x1, . . . , xt), prior distribution , probability threshold 2: Output: Edited sequence = (x 1, . . . , t) 3: for each token xi in sequence do 4: 5: 6: 7: 8: 9: end if 10: 11: end for 12: Return: Edited sequence = (x Resample token xi from prior distribution Set xi xi Set else 1, . . . , t) Table 10: Comparison of human and synthetic data performance across downstream tasks in (Maini et al., 2024), based on training with GPT-2. TruthfulQA LogiQA Wino. PIQA ARC-E BoolQ OBQA Avg Human Data 25% Synthetic Data 50% Synthetic Data 75% Synthetic Data Synthetic Data 32.68 27.91 30.84 29.5 28.89 23.03 21.37 22.58 22.65 22.58 51.3 50.12 52.41 49.8 49.72 64.42 63.93 63.33 63.44 63 44.4 43.94 44.02 44.53 46.3 60.98 62.29 62.14 61.56 54. 15 15.4 16 17.2 16.8 41.69 40.71 41.62 41.24 40."
        },
        {
            "title": "B MORE RESULTS OF HUMAN AND SYNTHETIC DATA MIXTURE TRAINING",
            "content": "We provide more training results for the human and synthetic data mixture. The main results and analysis can be found in Sec 2.1. Except for GPT-2 pretraining, we also use the OLMo models (Groeneveld et al., 2024) for further experiments. As shown in Figure 7, the training loss continues to decrease as the amount of synthetic data increases, which is consistent with GPT-2 pretriaing in Figure 2. More synthetic data can lead to better fitting. However, lower loss does not necessarily mean better model. As illustrated in Figure 2B and 8, models that fits better perform worse in real world tasks. Furthermore we follow Maini et al. (2024) to conduct more experiments including PPL results on 22 validation sets of Pile (Gao et al., 2020a) and general understanding tasks. The additional results in Table 10, 11 and 1 are consistent with our findings. Specifically, the PPL increases as the proportion of purely synthetic data grows, while the performance on downstream tasks similarly exhibits gradual decline with the increase in synthetic data. Table 11: Comparison of human and synthetic data performance across downstream tasks in (Maini et al., 2024), based on training with OLMo-237M. indicates the standard error. TruthfulQA LogiQA Wino. PIQA ARC-E OBQA Human Data 25% Synthetic Data 50% Synthetic Data 75% Synthetic Data Synthetic Data 26.81 1.550 26.44 1.543 25.95 1.534 25.34 1.522 23.01 1. 21.06 1.028 21.25 1.032 20.04 1.099 20.87 1.025 20.29 1.014 52.01 1.404 52.64 1.403 52.25 1.408 50.43 1.405 49.33 1.405 56.69 1.156 57.02 1.155 56.64 1.126 55.60 1.159 55.93 1.158 31.73 0.9550 31.78 0.9552 31.82 0.9557 32.74 0.9629 33.33 0.9673 13.80 1.543 12.40 1.475 12.80 1.495 12.00 1.454 14.20 1.562 Avg 33.68 33.59 33.25 32.83 32."
        },
        {
            "title": "C EXPERIMENT SETTINGS",
            "content": "In this section, we describe our experiments settings in detail. C.1 TRAINING Pre-training We utilized both GPT-2 and OLMo models. The pre-training datasets included Dolma, representing real data, and Cosmopedia, representing synthetic data. For GPT-2, we em21 Figure 7: OLMo-237M pretraining with mixed human and synthetic data proportions. We pretrain the OLMo-237M model using mixture of human data (Dolma (Soldaini et al., 2024)) and synthetic data (Cosmopedia (Ben Allal et al., 2024)). Figure 8: GPT-2 perplexity (PPL) on validation sets, trained from scratch. ployed the official FSDP (Fully Sharded Data Parallel) framework provided by Torch for training. For OLMo3, we used the official open-source computational code, which also incorporates the FSDP framework alongside Flash Attention for acceleration. Continual Pre-training We follow Cheng et al. (2024b) to conduct continual pre-training on biomedicine, finance, and math domains. Specifically, PubMed Abstracts from the Pile are utilized as the pre-training corpora for the biomedicine domain. For the finance domain, financial news data covering over 7,000 stocks from May 2022 to May 2023 is collected using the FinGPT framework. We continue pre-training OLMo-1B and LLaMA-3-8B on each domain. For implementation, we utilized the official training framework for OLMo-1B, leveraging Fully Sharded Data Parallel (FSDP) for continual pretraining. For LLaMA, we adopted the LLaMA-Factory framework to carry out the continual pretraining process. Our experiments was primarily conducted on OLMo-1B and Llama-3-8B models, with Llama-3-8B utilizing LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning. The data and evaluation are given in this repo4. We conducted the continual pretraining on total of 1B tokens. Supervised Fine-tuning We used the Llama-Factory (Zheng et al., 2024) framework to fine-tune Llama-3-8B. As for general instruction tuning tasks, we adopt instruction tuning datasets from (Xia et al., 2024) 5, including CoT (Wei et al., 2022) , FLAN v2 (Longpre et al., 2023), and Open Assistant 1 (Kopf et al., 2023). As for code-related reasoning tasks, we utilize OSS-Instruct-75K 6 and EvolInstruct-110K 7. These datasets provide sufficient diversity for verification on fine-tuning. C.2 EVALUATION Pre-training We use PPL and downstream tasks to conduct analysis and performance test. As for PPL, it stands for perplexity, commonly used metric in NLP to evaluate the quality of language models. It measures how well probabilistic model predicts given dataset, with lower values indicating better performance. Formally, the perplexity of language model is calculated as: PPL = 2 (cid:80)N i=1 log2 (xi) 3https://github.com/allenai/OLMo 4https://github.com/microsoft/LMOps/tree/main/adaptllm 5https://huggingface.co/datasets/princeton-nlp/less_data 6https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K 7https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K 22 Alternatively, it can also be expressed as: (cid:32) PPL = exp (cid:33) log (xi)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Where is the number of tokens in the dataset, and (xi) is the predicted probability of the i-th token. Perplexity essentially represents the exponential of the average negative log-likelihood of the predicted tokens, indicating how perplexed the model is when making predictions. As for downstream tasks, we use general understanding tasks in (Maini et al., 2024) to analyze model collapse in Table 10 and general test tasks in (Cheng et al., 2024a) to test our methods in Table 3. All downstream tasks we used can be found in (Gao et al., 2024)8. Continual Pre-training We use the test data and code in (Cheng et al., 2024b)9 to test domain specific task performance after CPT. Supervised Fine-tuning We utilize the general downstream tasks from (Cheng et al., 2024a) to evaluate instruction-tuning performance and reasoning tasks to assess reasoning capabilities. All downstream tasks we used can be found in (Gao et al., 2024)10. Table 12: PPL results of GPT-2 124M pretraining on mixture of human and synthetic data. Synthetic Data Ratio 25% 50% 75% Tokens Size 8.4B 16.8B 25.2B 33.6B 42B 8.4B 16.8B 25.2B 33.6B 42B 8.4B 16.8B 25.2B 33.6B 42B Epochs Wikitext-103 RedPajama Falcon-RefinedWeb c4-en mc4-en 1 2 45.97 42.28 56.40 48.15 62.46 39.87 37.62 50.62 43.14 56. 3 37.65 35.72 48.26 40.98 54.35 4 36.91 34.66 47.13 39.91 53.06 5 2 36.32 34.24 46.66 39.41 52.71 50.29 46.89 61.06 51.79 70.43 43.15 41.42 54.34 46.06 62.48 3 40.46 39.37 51.72 43.90 59. 4 39.43 38.21 50.39 42.73 57.66 5 1 2 38.65 37.72 49.87 42.23 57. 58.66 55.72 69.32 58.60 80.37 48.75 49.26 61.50 52.22 71.77 3 45.20 46.27 58.28 49.26 67.90 4 43.42 44.81 56.77 47.87 65. 5 42.95 44.30 56.19 47.27 64.82 Table 13: PPL results of OLMo-237M pretraining on mixture of human and synthetic data. Synthetic Data Ratio 0% Unique Tokens Training Tokens Epochs Wikitext-103 RedPajama Falcon-RefinedWeb c4-en mc4-en M2D2-Wiki M2D2-S2ORC 8.4B 8.4B 1 187.36 175.38 165.17 123.88 208.91 88.24 86.15 25% 8.4B 8.4B 1 185.5 183.93 166.69 127.68 208.94 87.34 81. 50% 8.4B 8.4B 1 260.08 236.33 199.68 147.69 263.35 107.77 97.61 75% 8.4B 8.4B 1 367.46 301.09 245.15 174.48 324.91 114.19 100. 100% DSIR (1M) DSIR (10M) Edu Classifier (1M) Edu Classifier (10M) PPL Filter (1M) PPL Filter (10M) Density Sampling (1M) Density Sampling (10M) 8.4B 8.4B 1 1605.73 907.91 523.93 410.19 800.40 189.06 204. 0.6B 8.4B 14 1309.53 649.36 573.61 457.96 861.01 234.45 170.78 8.4B 8.4B 1 1757.03 916.51 510.96 404.63 823.12 183.17 496.40 0.75B 10.5B 14 1111.29 811.14 522.97 415.88 769.86 161.58 145. 7.4B 7.4B 1 1612.95 1104.75 612.72 487.97 955.70 206.45 201.52 0.97B 13.68B 14 738.36 376.36 344.82 286.95 476.81 130.43 117.44 9B 9B 1 1193.25 645.82 449.86 367.44 662.00 162.08 163. 0.6B 8.9B 14 1188.40 789.67 501.99 414.55 740.75 167.20 131.22 7.1B 7.1B 1 1753.89 896.18 560.92 457.71 844.53 205.50 192."
        },
        {
            "title": "D DISCUSSION",
            "content": "D.1 WHAT IS COVERAGE COLLAPSE? Coverage collapse refers to phenomenon in which the distribution of synthetic data covers significantly narrower range of values compared to human data, even when the data sizes are identical. For instance, as shown in Figure 3, the PPL range of synthetic data is limited to [0, 14], whereas the PPL range of human data extends from [0, 100]. Despite this disparity, the overall coverage, represented by the area under the distribution curves, remains the same. This significant distribution gap is what we define as coverage collapse. D.2 HOW DOES THE DSIR WORK? DSIR (Xie et al., 2023) works by estimating importance weights for each data sample to measure its relevance to the target distribution. This involves three main steps: first, we leverage n-gram models to estimate two distributions of human and synthetic data, qf eat and pf eat, which represent the target 8https://github.com/EleutherAI/lm-evaluation-harness 9https://github.com/microsoft/LMOps/tree/main/adaptllm 10https://github.com/EleutherAI/lm-evaluation-harness Figure 9: PPL distribution of human and synthetic data estimated by StabLM-Zephyr-3B. This indicates that different prior distributions yielded the same result, which is consistent with Figure 3. The synthetic data lacks long tail and is concentrated within narrow portion of the distribution. and raw distributions, respectively. We use them to compute the likelihood ratio for each sample. Next, we calculate the importance weight for each sample zi as wi = ˆpfeat(zi) ˆqfeat(zi) . The weight wi quantifies how well the sample aligns with the target distribution. Finally, we perform importanceweighted sampling without replacement to select examples, ensuring that the selected data is more representative of the target distribution. We use DSIR in our data analysis as it allows for principled and computationally efficient selection of synthetic data points that align with the target distribution. Moreover, the importance weight also reflects the alignment between the n-gram features of synthetic data and human data. Using DSIR, we can analyze the differences between synthetic and human data across n-gram feature distributions and data matching. As shown in Figure 5, it is challenging to select synthetic data that matches human data characteristics under the significant distribution difference. To obtain highquality synthetic data, it is essential to focus on improving the data synthesis methods. Method Table 14: Comparison of different synthetic data methods. Result Data Type Approach Cosmopedia (Ben Allal et al., 2024) Rephrasing the Web (Maini et al., 2024) Pure synthetic Semi-synthetic Using prompt and source content to guide LLMs Using prompt to induce data from LLMs. Reveal non-iterative model collapse. Improve training performance. ToEdit (Ours) Semi-synthetic Using the distribution of source content estimated by LLMs (single forward pass) to replace tokens. Improve training performance. to reformat source content. D.3 NON-AUTOREGRESSIVE TOKEN REPLACEMENT MAY COMPROMISE TEXT COHERENCE. When designing data synthesis algorithms, we must balance synthesis efficiency and effectiveness, considering both autoregressive and non-autoregressive approaches. Autoregressive methods leverIn conage the inherent capabilities of language models to generate coherent text sequentially. trast, non-autoregressive methods resample individual tokens based on their probability distributions. Since data synthesis is prerequisite for model training, we aim to ensure that the cost of data synthesis does not exceed the cost of training itself. Specifically, our ToEdit modifies data using the probability distribution in single forward pass. For instance, if the generated sequence length is 1024, the computational cost of autoregressive methods would be 1024 times higher than ours. This efficiency advantage is why our method can run effectively on GPUs like the 3090 or 4090 series. However, this efficiency may come at the cost of coherence, as resampled tokens may not fit seamlessly into given sentence. To address this issue, we introduce hyperparameter, resampling probability p, to control the resampling threshold. We perform sampling in high-probability regions, focusing on tokens that are relatively easier to predict. We manually verify and tune on small validation set before applying it across all experiments. In our experiments, we set = 0.99. 24 Table 15: PPL results of GPT-2 124M pretraining on pure Human or Synthetic data. Data Type Tokens Size Epochs Wikitext-103 RedPajama Falcon-RefinedWeb c4-en mc4-en Human Data (Dolma) Synthetic Data (Cosmopedia) 8.4B 16.8B 25.2B 33.6B 42B 8.4B 16.8B 25.2B 33.6B 42B 1 43.62 40.18 54.85 45.87 61.00 38.57 35.84 49.10 41.00 54.44 3 36.11 33.97 46.93 39.10 52.11 4 34.89 32.74 45.43 37.95 50. 5 1 2 3 4 34.55 32.34 44.90 37.56 49.74 169.38 116.37 146.97 128.25 171.44 147.73 103.25 132.60 114.41 153.70 135.23 99.27 127.68 109.73 150.28 131.78 96.81 124.32 107.53 145.44 128.05 96.03 122.69 106.55 144. Additionally, we supplement more experiments and discussion about hyper-parameter p. As Table 7 shows, different values of influence BioMed performance, leading to fluctuations in data quality. Table 8 presents the distribution percentages of the token probabilities across different value ranges. We need to refine the data while primarily preserving the source distribution. larger indicates fewer tokens will be resampled, while smaller results in more tokens being resampled. Balancing performance and the preservation of data distribution, we set = 0.99 as the threshold for our experiments. D.4 COMPARISON WITH PURE SYNTHETIC DATA AND REFORMAT METHODS Specifically, both Rephrasing the Web (Maini et al., 2024) and our token-level editing aim to refine data while preserving the original distribution, producing semi-synthetic data. In contrast, purely synthetic data in Cosmopedia lacks the long-tail distribution and overly concentrates on n-gram features. Ultimately, semi-synthetic data enhances training performance, whereas purely synthetic data results in model collapse. Moreover, replacing whole real sample with synthetic data can damage the performance. The primary distinction between Cosmopedia, Rephrasing the Web (Maini et al., 2024), and our approach lies in how much of the original human data distribution is preserved. We provide detailed comparison of these synthetic methods in Table 14. D.5 MUST WE ASSUME THE DATA IS 100% HUMAN-AUTHORED? We do not need to assume that the data is 100% human authored; In experimental settings, some datasets used in our experiments include partially synthetic data: Datasets used in continual pretraining (e.g., Biomed, Finance) include partially synthetic data, which has been reformatted into reading comprehension structure (Cheng et al., 2024b). OSS-Instruct-75K and Evol-Instruct-110K also contain samples synthesized by ChatGPT. In the theoretical framework, synthetic data is generated iteratively through an n-generation process. (1) If the starting point is real distribution, our method preserves most of the initial distribution to generate higher-quality data. (2) If the starting point is mixture of synthetic and real data, the modifications are minimal, ensuring the original distribution remains largely unaffected. Therefore, applying our method in any generation i, we can further avoid issues, such as reduced variance and diminished diversity, which are key factors contributing to model collapse. In other words, whether the current data is fully real or mix of real and synthetic, using it as anchor data to synthesize data, our method builds upon the current data distribution to achieve improvements, rather than causing model collapse. In summary, we aim to improve the data synthesis method, specifically focusing on how to obtain higher-quality data from the existing datasets. We do not need to assume that the data at hand is 100% human-generated. Our algorithm is designed to minimize excessive distribution truncation of the original data."
        },
        {
            "title": "E POTENTIAL APPLICATIONS AND FUTURE WORK",
            "content": "Based on the above discussion, our approach can be applied to optimize the current data, even if it is mixture of real and synthetic data. From the findings and proposed method in our paper, we can influence future research in the following aspects: Potential applications of our work: (1) Data optimizations. We can quickly modify and optimize the current data, using trained language model with single forward pass. (2) Regularization in the data synthesizing process. When synthetic data becomes excessive, we can introduce real data as an anchor to balance the issues of excessive homogeneity and tail distribution cut-off in synthetic data, thereby preventing mode collapse. Lessons from our work: The key to improving the quality of synthetic data lies in balancing longtail distribution preservation and optimizing synthetic data approaches. In other words, we should focus on two questions: how to generate more informative synthetic data and how to integrate it with real data effectively. Building on this foundation, future improvements can focus on two aspects: first, obtaining more information gain by designing more efficient generation mechanisms to inject valuable information into the synthetic data; and second, optimizing methods to reduce noise during the synthesis process. This approach ensures that synthetic data retains its authenticity while enhancing its utility in practical tasks. Figure 10: The top 40 bi-grams from separately sampled 1M subsets of Dolma, Cosmopedia, and DSIR-selected datasets. Table 16: Dolma dataset statistics (v1.6), quoted from source (Soldaini et al., 2024). Source Doc Type UTF-8 bytes (GB) Documents (millions) Unicode words (billions) Llama tokens (billions) Common Crawl The Stack C4 Reddit PeS2o Project Gutenberg Wikipedia, Wikibooks web pages code web pages social media STEM papers books encyclopedic Total 9,022 1,043 790 339 268 20.4 16. 11,519 1,775 260 153 72 50 4.0 3.7 2,318 2,281 411 198 89 70 6.0 4.3 3,059 3,370 210 364 377 38.8 0.056 6. 4,367 26 Figure 11: The top 64 bi-grams from separately sampled 1M subsets of Dolma, Cosmopedia, and DSIR-selected datasets. Figure 12: Density sampling response values. This result further confirms the issue of feature collapse in synthetic data."
        }
    ],
    "affiliations": [
        "Department of Electronic Engineering, Tsinghua University",
        "Institute for Artificial Intelligence, Peking University",
        "LUMIA Lab, Shanghai Jiao Tong University",
        "Shanghai Artificial Intelligence Laboratory",
        "State Key Laboratory of General Artificial Intelligence, BIGAI"
    ]
}