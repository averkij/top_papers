{
    "paper_title": "Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs",
    "authors": [
        "Song Bian",
        "Tao Yu",
        "Shivaram Venkataraman",
        "Youngsuk Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become a pressing concern. Despite its importance, the trade-off between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and grouped-query attention (GQA), influence both inference cost and accuracy. We introduce a conditional scaling law that augments the Chinchilla framework with architectural information, along with a search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 5 4 2 8 1 . 0 1 5 2 : r Preprint SCALING LAWS MEET MODEL ARCHITECTURE: TOWARD INFERENCE-EFFICIENT LLMS Song Bian UW-Madison Tao Yu Amazon Web Services Shivaram Venkataraman UW-Madison Youngsuk Park Amazon Web Services"
        },
        {
            "title": "ABSTRACT",
            "content": "Scaling the number of parameters and the size of training data has proven to be an effective strategy for improving large language model (LLM) performance. Yet, as these models grow increasingly powerful and widely deployed, the cost of inference has become pressing concern. Despite its importance, the tradeoff between model accuracy and inference efficiency remains underexplored. In this work, we examine how key architectural factors, hidden size, the allocation of parameters between MLP and attention (mlp-to-attention ratio), and groupedquery attention (GQA), influence both inference cost and accuracy. We introduce conditional scaling law that augments the Chinchilla framework with architectural information, along with search framework for identifying architectures that are simultaneously inference-efficient and accurate. To validate our approach, we train more than 200 models spanning 80M to 3B parameters and 8B to 100B training tokens, and fit the proposed conditional scaling law. Our results show that the conditional scaling law reliably predicts optimal architectural choices and that the resulting models outperform existing open-source baselines. Under the same training budget, optimized architectures achieve up to 2.1% higher accuracy and 42% greater inference throughput compared to LLaMA-3.2."
        },
        {
            "title": "INTRODUCTION",
            "content": "Scaling law studies Kaplan et al. (2020); Hoffmann et al. (2022); Muennighoff et al. (2023); Krajewski et al. (2024); Abnar et al. (2025) have shown that increasing model parameters, training tokens, dataset quality, and compute budget consistently reduces pre-training loss, improves downstream task performance Hendrycks et al. (2021); Austin et al. (2021), and enables the emergence of novel capabilities Wei et al. (2022). These insights have driven the development of many state-of-the-art large language models Touvron et al. (2023); Yang et al. (2025); Guo et al. (2025). However, as the field advances, it has become increasingly clear that focusing exclusively on training overlooks the practical challenges of deploying these models at scale Chien et al. (2023); Wu et al. (2024); Muhamed et al. (2023). major limitation of existing scaling laws is their omission of inference costs, which constitute the dominant expense in deploying large models in real-world applications Sardana et al. (2023); Park et al. (2024). Moreover, the growing use of LLMs in reasoning systems highlights the need for scaling laws that account for inference costs Snell et al. (2024); Brown et al. (2024); Luo et al. (2024); Qi et al. (2024); Guan et al. (2025). Therefore, we ask the following question: Can we explicitly capture the trade-off between inference efficiency and accuracy of large language models? To address this question, recent study Sardana et al. (2023) proposed scaling laws that incorporate the total FLOPs from both training and inference. However, their formulation requires estimating the total number of tokens generated over models entire lifespan. Because inference is performed repeatedly during deployment, this assumption renders the proposed scaling law impractical for real-world use. Another study Bian et al. (2025) extends Chinchilla scaling laws by incorporating Work done during internship at Amazon Web Services. Correspondence to: Tao Yu (taou@amazon.com) Preprint model architecture. However, this work has notable limitations. First, the study considers only the aspect ratio, defined as hidden size over number of layers, as the architectural factor. Yet, as shown in Figure 1, aspect ratio alone fails to capture the full range of factors that influence inference efficiency in large language models. Second, the depth of the model strongly influences accuracy: cutting layers tends to impair the models generalization after fine-tuning Petty et al. (2023). Finally, the study lacks general framework for incorporating broader architectural factors, including hidden size and GQA, into scaling laws. In this work, we fix the number of layers and study the effect of other architectural factors, including GQA, hidden size, and the mlp-to-attention ratio. This design choice is motivated by recent open-weight models such as LLaMA Touvron et al. (2023), Qwen Yang et al. (2025), Gemma Team et al. (2024a), and Phi Abdin et al. (2024), which, despite having comparable number of parameters, adopt markedly different architectural designs. Our primary goal is to investigate how model architecture influences both inference efficiency and model accuracy. We begin by comparing the inference efficiency of models with identical parameter counts but varying architectures. Next, we train over 200 models, ranging from 80M to 297M parameters on up to 30B tokens, to systematically characterize the relationship between architectural design and accuracy. Guided by these empirical findings, we introduce conditional extension of the Chinchilla scaling laws that incorporates architectural parameters, establishing general framework for identifying model architectures that balance inference efficiency and performance. Figure 1: Although larger models generally achieve lower inference throughput than smaller ones, Qwen2.5-1.5B outperforms Qwen3-0.6B. Despite having the same number of layers, Qwen2.5-1.5B benefits from higher hidden size, GQA, and mlp-to-attention ratio. Finally, we validate this framework by fitting the proposed scaling law on models between 80M and 297M parameters, and evaluating its predictions when scaling up to 3B-parameter models. Our results demonstrate that, under identical training setups, the derived optimal 3B-parameter architecture achieves 42% higher inference throughput than the LLaMA-3.2-3B architecture, while maintaining better accuracy."
        },
        {
            "title": "2 BACKGROUND",
            "content": "Accurately predicting the performance of large language models during scaling is essential. This enables us to answer key questions: (i) what is the optimal allocation of available resources between model size and training tokens, and (ii) what performance gains can be expected from additional resources? Fortunately, the model loss has been observed to follow power-law relationship with respect to the number of parameters and training tokens Hoffmann et al. (2022); Muennighoff et al. (2023) with: L(N, D) = + α + Dβ (1) where is the model loss, is the number of total parameters and is the number of tokens used for training and A, B, E, α, β are parameters to be learned. To fit the learnable parameters in Eq. (1), Chinchilla Hoffmann et al. (2022) employs two strategies: (i) training models with fixed number of parameters while varying the number of training tokens, and (ii) training models under fixed compute budget1, varying both parameters and tokens. The resulting data are combined to fit the learned parameters in Eq. (1). With the fitted scaling laws, 1The compute cost is approximated as FLOPs(N, D) 6N in Hoffmann et al. (2022); Muennighoff et al. (2023), where denotes the number of parameters and the number of training tokens. In this work, we adopt the same settings as prior studies. 2 Preprint Figure 2: Inference throughput vs (left) hidden size = dmodel and (right) mlp-to-attention ratio = rmlp/attn on the 8B model. Under fixed parameter budget Nnon-embed, larger hidden sizes and higher mlp-to-attention ratios improve inference throughput for varying batch sizes. Chinchilla addresses the following question to determine optimal allocation: arg min N,D L(N, D) s.t. FLOPs(N, D) = (2) where denotes the resource constraint, the total number of parameters, and the number of training tokens. In this paper, we do not address how to optimally allocate compute between model size and training data under fixed compute budget. Instead, our focus is on identifying model architectures that optimize inference efficiency and accuracy under fixed parameter and token budgets. For example, given model with 7B parameters trained on 14T tokens, we study how to design an architecture that satisfies both efficiency and accuracy requirements."
        },
        {
            "title": "3 MODEL ARCHITECTURE-AWARE SCALING LAWS",
            "content": "3.1 MODEL ARCHITECTURE VARIATIONS The architecture of decoder-only transformer is composed of sequence of stacked decoder blocks, each sharing the same structure to facilitate model-parallel deployment across devices. Under this design, the overall architecture of dense LLMs is primarily determined by the hidden size and the MLP intermediate size, which together specify the attention and MLP layers structure. This work studies the optimal model architecture given fixed total number of non-embedding parameters Nnon-embed (at different levels). Although the number of layers nlayer also plays critical role (closely related to aspect ratio (Petty et al., 2023)), varying nlayer under fixed Nnon-embed substantially impacts both inference cost and accuracy (Tay et al., 2021; Alabdulmohsin et al., 2023). Therefore, we fix nlayer and focus on the effects of hidden size dmodel and the mlp-to-attention ratio rmlp/attn on inference efficiency (3.2) and accuracy (3.3), noting that nlayer still varies across different Nnon-embed levels. In 3.3, we introduce conditional scaling law to predict the performance of architectural variants, and in 3.4, we present lightweight framework for identifying architectures that optimally balance inference efficiency and accuracy. Note that the number of attention parameters is primarily determined by the hidden size dmodel and the attention projection dimension, since most open-weight models adopt non-square q, k, projection matrices, as seen in Gemma (Team et al., 2024a) and Qwen3 (Yang et al., 2025). For consistency, we fix the per-head dimension dhead to 64 for models with Nnon-embed 1B and to 128 for models with Nnon-embed 3B. Consequently, to maintain constant rmlp/attn, we adjust the number of attention heads nhead rather than altering the projection dimension directly. This design choice also provides flexibility to incorporate architectural variants such as grouped-query attention. 3.2 INFERENCE EFFICIENCY Inspired by the success and widespread adoption of open-weight dense models such as Qwen3 (Yang et al., 2025), LLaMA-3.2 (Dubey et al., 2024), and the Gemma-2 (Team et al., 2024b) family, we 3 Preprint construct architectural variants by modifying the configurations of the LLaMA-3.2 and Qwen3 dense models (Figure 11-13 in Appendix E). In addition to hidden size and the mlp-to-attention ratio, we find that group-query attention has critical impact on inference efficiency, even though it only modestly reduces the number of attention parameters (by shrinking the key and value matrices). To disentangle these effects, we conduct controlled ablations of hidden size, MLP-to-attention ratio, and GQA under the following setups: hidden size dmodel: fix Nnon-embed, rmlp/attn and GQA= 4, vary dmodel and number of attention mlp-to-attention ratio rmlp/attn: fix Nnon-embed, dmodel and GQA= 4, vary nhead and intermeheads nhead (Figure 2 left). diate size (Figure 2 right). GQA: fix Nnon-embed, dmodel and rmlp/attn, vary nhead and number of key-value heads (Appendix E). Figure 2 shows the ablation of varying hidden sizes dmodel and mlp-to-attention rmlp/attn on the LLaMA-3.1-8B model variants. We observe that larger hidden size (or fewer attention heads) and higher mlp-to-attention ratios improve inference throughput. Similar trends are observed in the LLaMA-3.2-1B and 3B model variants (Appendix E). These gains arise in part because larger dmodel and higher rmlp/attn reduce the total FLOPs, as detailed in the inference FLOPs analysis (Appendix H). In addition, these architectural choices shrink the KV cache, lowering I/O cost during inference and further improving throughput Adnan et al. (2024). Figure 10 in Appendix presents the GQA ablation, confirming prior observations Ainslie et al. (2023) that increasing GQA consistently improves inference throughput. comparable set of ablation experiments on Qwen3 models, also reported in Appendix E, further corroborates these findings. 3.3 CONDITIONAL SCALING LAW Improving inference efficiency should not come at the expense of significantly reducing model accuracy, making it crucial to understand how architectural choices affect accuracy and training loss. Because training large-scale language models is prohibitively expensive, common strategy is to study smaller models and use scaling laws to extrapolate insights to larger scales, for example, the Chinchilla scaling laws (Hoffmann et al., 2022). However, incorporating multiple architectural factors into such laws remains challenging. To address this, we examine the effect of architectural choices on training loss in conditional manner, varying one factor at time while keeping the others fixed. hidden size dmodel. We note that dmodel generally scales linearly with squared attention weight matrices, the number of attention parameters Nattn can be expressed as Nnon-embed. Assuming 4d2 model Nattn = Nnon-embed + , Nnon-embed and examine its relation to loss in Figure 3. The resulting U-shaped curves L(d/ where = rmlp/attn is fixed, and the constant factor 4 arises from the query, key, value, and output projection layers in each attention block. To capture this scaling behavior, we normalize dmodel by r, N, D) exhibit nearly identical optima across different model sizes. Moreover, Figure 3 confirms that excessively large hidden sizes, which reduce the number of attention heads nhead, can degrade accuracya phenomenon consistently observed in prior analyses of transformer capacity and head allocation (Kaplan et al., 2020; Hoffmann et al., 2022). mlp-to-attention ratio rmlp/attn. Figure 4 illustrates how the loss varies with rmlp/attn, conditioned on dmodel fixed at different levels, where we consistently observe U-shaped curve L(r d/ , N, D). While the attention mechanism is central to the success of transformers (Vaswani, 2017), recent open-weight models have allocated progressively smaller fraction of parameters to attention as overall model size increases (e.g., LLaMA and Qwen families). Our analysis indicates that this trend is not universally optimal: there exists an interior optimum in the allocation of attention parameters, and deviating from it in either direction degrades model performance. This suggests that careful tuning of the mlp-to-attention ratio is critical for scaling transformers effectively. As shown in Figures 3 and 4, both hidden size and the MLP-to-attention ratio exhibit U-shaped relationships with training loss. To capture these trends, we fit the function c0 + c1 log + c2/x 4 Preprint Figure 3: Loss vs. hidden size: (Left) 80M model variants; (Center) 145M model variants; (Right) 297M model variants. Across model sizes, the relationship between training loss and dmodel/ exhibits consistent U-shaped curve when architectural factors such as GQA and the MLP-to-attention ratio are held fixed. The legend denotes the MLP-to-attention ratio = rmlp/attn for each model. Figure 4: Loss vs. MLP-to-attention ratio: (Left) 80M model variants; (Center) 145M model variants; (Right) 297M model variants. Across model sizes, the relationship between training loss and rmlp/attn exhibits consistent U-shaped curve when architectural factors such as GQA and hidden size are held fixed. The legend denotes the hidden size = dmodel for each model. separately for = rmlp/attn and dmodel/ Nnon-embed. This formulation effectively models the Ushaped behavior while ensuring sublinear growth as increases. However, incorporating rmlp/attn, dmodel, , and into unified, architecture-aware scaling law remains challenging. Since fitting single all-purpose scaling law L(d/ , r, N, D) is unrealistic across all possible configurations, we instead propose two-step conditional approach: 1. For given and D, obtain the optimal loss Lopt(N, D) = min L(N, D) = min (cid:0)E + α + Dβ (cid:1) from the Chinchilla scaling law (Eq. 1) as reference point. 2. Calibrate the loss of architectural variants L(d/ , N, D) relative to this reference. We focus on two simple calibration schemes: (multiplicative) L(d/ , N, D) = (a0 + a1 log( ) + a2 ) (b0 + b1 log + b2 ) Lopt (3) (additive) L(d/ , N, D) = (a0 + a1 log( ) + a2 ) + (b1 log + r ) + Lopt Here, ai and bi are learnable parameters that are shared across all N, D. Unlike the unified formulation, the conditional scaling law assumes that the effects of rmlp/attn and dmodel on loss are separable. We further ablate joint, non-separable formulations in Appendix G, where we find that they yield inferior predictive performance. 3.4 SEARCHING FOR INFERENCE-EFFICIENT ACCURATE MODELS With the conditional scaling law, we can identify architectures that are both inference-efficient and accurate by solving the following optimization problem: given , D, and set of architectural choices , argmaxP IN (P ), s.t. L(P N, D) Lt, (4) 5 Preprint where IN (P ) denotes the inference efficiency of an architecture with total Nnon-embed parameters, and Lt, ( Lopt) is the maximum allowable training loss. As shown in Figure 10 (Appendix E), GQA has substantial impact on inference efficiency; However, unlike hidden size and the mlp-to-attention ratio, GQA does not exhibit consistent relationship with loss (Figure 14) and is highly variable, making it challenging to identify settings that achieve both accuracy and efficiency. Fortunately, the search space for GQA is relatively small once Nnon-embed, dmodel, and rmlp/attn are fixed, since GQA must be prime factor of the number of attention heads nhead. In practice, we perform local GQA search by enumerating feasible values and applying early stopping once performance falls below that of the GQA= 4 baseline. Algorithm 1 summarizes our overall framework for identifying inference-efficient and accurate architectures. Algorithm 1: Searching for Inference-Efficient Accurate Model Input: Model parameters , training tokens D, target loss Lt; inference efficiency IN (); optional: the optimal loss Lopt(N, D) Train smaller models to fit the Chinchilla scaling laws (Eq. 1) if Lopt(N, D) is unavailable Solve the constrained optimization (Eq. 4) for dmodel, rmlp/attn and corresponding architecture Perform local search over GQA values with early stopping to maximize inference efficiency return Final model architecture {P, GQA}"
        },
        {
            "title": "4 EXPERIMENT SETUP",
            "content": "We first detail the experimental setup of training, inference, and downstream task evaluation, and then describe how we derive the conditional scaling law and scale up to larger sizes. Training Setup. We sample the training data from Dolma-v1.7 Soldaini et al. (2024), which contains data from 15 different sources. Tokens are sampled with probability proportional to each sources contribution, ensuring the sampled dataset preserves similar distribution to Dolmav1.7. We train decoder-only LLaMA-3.2 (Dubey et al., 2024) style transformers with Nnon-embed in {80M, 145M, 297M, 1B, 3B}, for each Nnon-embed, we obtain model architecture candidates by varyNnon-embed and mlp-to-attention ratio rmlp/attn. (changing intermediate size ing hidden size dmodel/ and number of attention heads nhead) while holding other architectural factors fixed e.g. GQA= 4. full list of over 200 model architectures used can be found in Appendix C. All models are trained on 100Nnon-emb tokens (5 Chinchilla optimal) to ensure convergence. We tuned training hyperparameters (mainly following prior work Chen et al. (2025)), with full list in Appendix D. Inference Setup. We evaluate the inference efficiency using the vLLM framework Kwon et al. (2023). By default, inputs consist of 4096 tokens and outputs of 1024 tokens. We report the averaged inference throughput (tokens/second) from 5 repeated runs. Unless otherwise specified, all experiments are conducted on NVIDIA Ampere A100 GPUs (40GB). LLM Evaluation Setup. Following prior works Biderman et al. (2023); Zhang et al. (2024), we evaluate pretrained models in the zero-shot setting using lm-evaluation-harness2 on nine benchmarks: ARC-Easy Clark et al. (2018), ARC-Challenge Clark et al. (2018), LAMBADA Paperno et al. (2016), HellaSwag Zellers et al. (2019), OpenBookQA Mihaylov et al. (2018), PIQA Bisk et al. (2020), SciQ Welbl et al. (2017), WinoGrande Sakaguchi et al. (2021), and CoQA Reddy et al. (2019). Fitting Scaling Laws. Following Gadre et al. (2024); Bian et al. (2025), we use the LevenbergMarquardt algorithm to fit the conditional scaling laws (Eq. 3). The LevenbergMarquardt algorithm does least-squares curve fitting by estimating ˆβ as the solution to arg minβ i=1 [yi (xi, β)]2, where (xi, yi) are the observed data pairs. Note that instead of fitting the Chinchilla scaling law, we empirically searched over architecture variants to find the optimal loss Lopt(N, D) for Nnon-embed <1B scale. (cid:80)m 2https://github.com/EleutherAI/lm-evaluation-harness 6 Preprint Figure 5: Predictive performances of the fitted conditional scaling law on: (left) Task 1: Fit on 80M, evaluate on 145M; (center) Task 2: Fit on 80, 145M, evaluate on 297M; (right) Task 3: Fit on 80, 145, 297M, evaluate on 1B. Orange dots denote fitting data points, and purple crosses indicate the test data points. We compare scaling-law predicted loss with actual pretraining loss of architectures and observed consistently low MSE and high Spearman correlation across model scales. We scale up the scale law fitting in the following progressive manner: (Task 1) fit on the 80M results and evaluate on 145M results; (Task 2) fit on 80, 145M results and evaluate on 297M results; (Task 3) fit on 80, 145, 297M results and evaluate on 1B results; This ensures robust and consistent way of scaling up the model sizes and evaluating our conditional scaling law. Following prior work Kumar et al. (2024), we evaluate the fitted scaling law with mean i=1(li ˆli)2 where li denotes the actual loss and ˆli the squared error (MSE) metric, defined as 1 predicted loss. We additionally report the Spearmans rank correlation coefficient Spearman (1961) to compare predicted and actual rankings. Both metrics are calculated on the val data points. (cid:80)n"
        },
        {
            "title": "5 EXPERIMENT RESULTS",
            "content": "We begin by evaluating the predictive performances of the conditional scaling laws with multiplicative calibration. We then conduct ablation studies to assess the impact of data selection and to evaluate the performance of the scaling laws under additive calibration. Finally, we apply the fitted scaling laws to guide the training of large-scale models following the search framework (5.1). Predictive Accuracy. As Task 1-3 described in 4, we fit the conditional scaling laws on 80M, (80M, 145M), and (80M, 145M, 297M) loss-architecture data points, and subsequently evaluate on 145M, 297M, and 1B data, respectively. In Figure 5, the low MSE and high Spearman correlation in tasks across different model scales validate the effectiveness and strong predictive performance of the proposed conditional scaling laws. Ablation of Outliers. The mlp-to-attention ratio rmlp/attn of open-weights models typically fall between 0.5 and 5, for example, the mlp-to-attention ratio for LLaMA-3.2-1B, LLaMA-3.2-3B, and Qwen3-8B are 4.81, 1.5, and 4.67, respectively. In Figure 5, we fit the conditional scaling law using only model architectures with rmlp/attn [0.5, 5]. We ablate this choice by training model architectures with outlier rmlp/attn below 0.5 and above 5 (such as 0.1, 12.6) in Appendix C. In Figure 15 (left) and Figure 15 (center) in Appendix G, we show on Task 3 comparison of fitting the conditional scaling law without and with these outliers (with clear Spearman correlation score degradation), which suggests to exclude extreme outliers for better predicted performances. Ablation of Calibration. In Figure 15 (right), We ablate an alternative formulation of the scaling laws with additive calibration, as discussed in 3.3. The results on Task 3 show that multiplicative and additive calibrations achieve similar MSE and Spearman correlations, underscoring the robustness of our two-step reference plus calibration framework. 7 Preprint Table 1: Large-Scale Model Results: We evaluate the scaling laws and framework at the 1B and 3B scales by training Panda-1B, Surefire-1B, and Panda-3B, and compare them with LLaMA-3.21B and LLaMA-3.2-3B, respectively. The Avg. column reports the mean accuracy across the nine downstream tasks. Panda-1B and Panda-3B are trained using the optimal architectural configurations predicted by our scaling laws, whereas Surefire-1B and Surefire-3B satisfy the loss constraint in Eq. (4) and achieve Pareto optimality. Models LLaMA-3.2-1B Panda-1B Surefire-1B LLaMA-3.2-3B Panda-3B Surefire-3B dmodel 2048 2560 2560 3072 4096 fsize 8192 4096 6144 8192 4096 4096 nlayers GQA dmodel/ 4 4 9 0.066 0.082 0.082 16 16 16 28 28 3 3 7 0.058 0.077 0.077 Loss () Avg. () 4.80 1.07 3.6 4.80 1 1 2.803 2.782 2.804 2.625 2.619 2.620 54.9 57.0 55.4 61.9 62.5 62. Figure 6: Results for 1B and 3B models: (left) Panda-1B closely follows the scaling law predictions for minimizing training loss. (center) Inference throughput comparison between LLaMA-3.2-1B and Surefire-1B, showing that Surefire-1B consistently achieves higher efficiency across batch sizes. (right) Inference throughput comparison between LLaMA-3.2-3B and Surefire-3B, demonstrating that Surefire-3B consistently delivers higher efficiency across all batch sizes. 5.1 OPTIMAL MODEL ARCHITECTURE Validating the conditional scaling law. We validate the conditional scaling law at the 1B scale by applying multiplicative calibration on Task 3 using data from the (80M, 145M, and 297M) model variants. The learned parameters are a0 = 2.697, a1 = 0.0974, a2 = 0.0078, b0 = 0.3870, b1 = 0.0063, and b2 = 0.0065. = 0.08, = 1.032 for From this, we obtain the optimal architectural configuration of dmodel/ 1B model by solving r = 0. Using this configuration, we train LLaMA-3.2-style dmodel 1B dense model on 100B tokens, denoted as Panda-1B. Panda-1B outperforms the open-weight LLaMA-3.2-1B baseline configs by 2.1% on average across downstream tasks  (Table 1)  . Figure 6 (left) further confirms the effectiveness of the conditional scaling law by showing that Panda-1B achieves the lowest training loss among the exhaustively trained 1B variants under the same setup. = 0 and We also scale up our methodology to 3B models. Using the same approach but with data from the 80M, 145M, 297M, and 1B variants, we fit the scaling law and obtain dmodel/ = 0.08 and = 1.055 for the Panda 3B model. Trained on 100B tokens, Panda-3B outperforms the open weight LLaMA-3.2-3B configuration by 0.6% on average across downstream tasks  (Table 1)  . With all components in place, we apply the search framework for inference-efficient and accurate models (Alg. 1). For the Nnon-embed = 1B and 3B setting trained on 100B tokens, we set the target loss Lt to match the training loss achieved by the LLaMA-3.2-1B and LLaMA-3.2-3B architectures, respectively. Although inference efficiency IN (P ) could, in principle, be expressed analytically, it depends heavily on hardware and inference configurations. Therefore, rather than solving for IN (P ) directly, we search over feasible configurations Pi that satisfy the loss constraint and select Pareto-optimal points, which we denote as Surefire-1B and Surefire-3B. Surefire-1B and Surefire8 Preprint Table 2: 3B Model Ablation Study: We evaluate the robustness of scaling laws at 3B scale by training Panda-3B and Panda-3B, and compare them with LLaMA-3.2-3B. The Avg. column reports the mean accuracy across the nine downstream tasks. Panda-3B represents the optimal architectural configuration predicted by the conditional scaling laws fitted using the 80M, 145M, and 297M model data, whereas Panda-3B corresponds to the optimal configuration predicted from scaling laws fitted using the 1B model data. Models LLaMA-3.2-3B Panda-3B Panda-3B dmodel 3072 4096 4096 fsize 8192 4096 4608 nlayers GQA dmodel/ 3 3 3 0.058 0.077 0.076 28 28 28 Loss () Avg. () 4.80 1 1.23 2.625 2.619 2.606 61.9 62.5 62.5 Figure 7: Effect of the Fitting Dataset on Predictive Performance vs (left) Fit on 80, 145, 297M, 1B, evaluate on 3B; (right) Fit on 1B, evaluate on 3B. Orange dots denote fitting data points, and purple crosses indicate the test data points. We compare scaling-law predicted loss with actual pretraining loss of architectures and we observe that fitting the scaling laws with only 1B model data yields lower MSE and higher Spearman correlation for the 3B model loss prediction. 3B outperform LLaMA-3.2-1B and LLaMA-3.2-3B on downstream tasks  (Table 1)  and deliver up to 42% higher inference throughput (Figure 6, center and right). Detailed downstream task accuracies are provided in Appendix I. Ablation of fitting data. While we adopt progressive strategy for selecting fitting data across tasks (4), results from small models (e.g., 80M) may not reliably predict behavior at larger scales such as 3B. To examine this, we perform an ablation study by fitting the conditional scaling law for the 3B model using only results from the 1B variant. As shown in Figure 7, fitting with only the 1B data achieves lower MSE and higher Spearman correlation when predicting the 3B loss. This indicates that the coefficients of the conditional scaling law may shift as model size increases. We therefore refit the law with multiplicative calibration using only the 1B variants, yielding the coefficients a0 = 2.319, a1 = 0.238, a2 = 0.0176, b0 = 0.5104, b1 = 0.0051, and b2 = 0.0062. = 0.074 and This produces an alternative optimal configuration for the 3B model, with dmodel/ = 1.229. We train 3B model (Panda-3B) under this configuration on 100B tokens and compare it with both LLaMA-3.2-3B and Panda-3B (fitted from 80M, 145M, 297M, and 1B data). As shown in Table 2, Panda-3B achieves lower training loss and comparable downstream accuracy to Panda3B, with detailed results given in Appendix I. These findings suggest that when scaling up, it is often sufficient, and sometimes preferable, to fit the law using models within closer size range to the target, such as about one third of its scale. 9 Preprint"
        },
        {
            "title": "6 RELATED WORK",
            "content": "Large Language Models. Transformers Vaswani (2017) have shown strong performance across diverse downstream tasks, such as text classification Wang (2018); Sarlin et al. (2020), mathematical reasoning Cobbe et al. (2021); Hendrycks et al. (2021), and code generation Chen et al. (2021); Austin et al. (2021); Jain et al. (2024). The Transformer architecture serves as the foundation for many leading large language models, including GPT Brown et al. (2020); Achiam et al. (2023), LLaMA Touvron et al. (2023), Gemma Team et al. (2024a), Qwen Yang et al. (2025), Kimi Team et al. (2025), and DeepSeek Liu et al. (2024a); Guo et al. (2025). Scaling Laws for Language Models. Scaling laws are powerful tools to predict the performance of large language models. Existing scaling laws Hoffmann et al. (2022); Muennighoff et al. (2023); Sardana et al. (2023); Kumar et al. (2024); Gadre et al. (2024); Ruan et al. (2024) characterize how model performance varies with model size, dataset size, data quality, and compute budget. With the rise of Mixture-of-Experts (MoE) Shazeer et al. (2017); Guo et al. (2025), powerful architecture for large language models, recent studies Krajewski et al. (2024); Abnar et al. (2025) extend scaling laws to account for the number of experts, expert granularity, active parameters, and sparsity. Serving Systems. Due to the increased inference cost, many inference systems have been developed to speed up model serving Yu et al. (2022); Kwon et al. (2023); Zheng et al. (2023); Ye et al. (2025). Specifically, vLLM Kwon et al. (2023) proposes PagedAttention to manage KV cache memory more effectively, thereby improving throughput. Similarly, SGLang Zheng et al. (2023) introduces RadixAttention to achieve higher throughput and lower latency. Inference-Efficient Model Design. Efforts to improve the inference efficiency of large language models generally fall into two categories: one line of work investigates the trade-offs across different model configurations Alabdulmohsin et al. (2023); Bian et al. (2025), while the other focuses on designing more efficient model architectures Xiao et al. (2023); Gu & Dao (2023); Gao et al. (2024b); Jiang et al. (2024); Liu et al. (2024b); Dao & Gu (2024); Xiao et al. (2024); Yuan et al. (2025); Chandrasegaran et al. (2025)."
        },
        {
            "title": "7 LIMITATIONS AND FUTURE WORK",
            "content": "While our team has made notable progress, several open challenges remain that offer promising directions for future research. First, due to limitations in resources and time, our evaluation does not extend to 7B models. Second, our analysis is restricted to dense models, and it remains unclear whether the results extend to Mixture of Experts (MoE) architectures Shazeer et al. (2017). While we report inference efficiency measurements for MoE models under varying architectural choices in Appendix J, we have not yet established scaling laws for MoE architectures. Third, we adopt the experimental setup from Chen et al. (2025), and it is uncertain whether different model architectures warrant different hyperparameter configurations. Finally, our analysis is limited to pre-training, and it remains unclear how the results would change under post-training."
        },
        {
            "title": "8 CONCLUSION",
            "content": "This work explores the trade-off between model accuracy and inference cost under fixed training budget. We begin by demonstrating how architectural choices influence both inference throughput and model accuracy. Building on this, we extend Chinchilla scaling laws to incorporate architectural factors and propose framework for optimal model architecture search. Using the fitted scaling laws and our framework, we trained models up to 3B parameters, achieving up to 42% higher inference throughput and 2.1% accuracy gains across nine downstream tasks."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "All experiments in this work were conducted using publicly available frameworks. Section 4 In particular, we used provides details of our training, inference, and evaluation setups. 10 Preprint Megatron-LM (Shoeybi et al., 2019) for model training, vLLM (Kwon et al., 2023) for efficient inference, and lm-eval-harness (Gao et al., 2024a) for standardized evaluations. To facilitate reproducibility, we will release configuration files and scripts."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. Samira Abnar, Harshay Shah, Dan Busbridge, Alaaeldin Mohamed Elnouby Ali, Josh Susskind, and Vimal Thilak. Parameters vs flops: Scaling laws for optimal sparsity for mixture-of-experts language models. arXiv preprint arXiv:2501.12370, 2025. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference. Proceedings of Machine Learning and Systems, 6:114127, 2024. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. Getting vit in shape: Scaling laws for compute-optimal model design. Advances in Neural Information Processing Systems, 36:1640616425, 2023. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Song Bian, Minghao Yan, and Shivaram Venkataraman. Scaling inference-efficient language models. arXiv preprint arXiv:2501.18107, 2025. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pp. 23972430. PMLR, 2023. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Keshigeyan Chandrasegaran, Michael Poli, Daniel Fu, Dongjun Kim, Lea Hadzic, Manling Li, Agrim Gupta, Stefano Massaroli, Azalia Mirhoseini, Juan Carlos Niebles, et al. Exploring diffusion transformer designs via grafting. arXiv preprint arXiv:2506.05340, 2025. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 11 Preprint Mengzhao Chen, Chaoyi Zhang, Jing Liu, Yutao Zeng, Zeyue Xue, Zhiheng Liu, Yunshui Li, Jin Ma, Jie Huang, Xun Zhou, et al. Scaling law for quantization-aware training. arXiv preprint arXiv:2505.14302, 2025. Andrew Chien, Liuzixuan Lin, Hai Nguyen, Varsha Rao, Tristan Sharma, and Rajini Wijayawardana. Reducing the carbon impact of generative ai inference (today and in 2035). In Proceedings of the 2nd workshop on sustainable computer systems, pp. 17, 2023. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et al. Language models scale reliably with over-training and on downstream tasks. arXiv preprint arXiv:2403.08540, 2024. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024a. URL https://zenodo.org/records/12608602. Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai, Hayden Kwok-Hay So, Ting Cao, Fan Yang, et al. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024b. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024. Preprint Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pioro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Krol, Tomasz Odrzygozdz, Piotr Sankowski, et al. Scaling laws for fine-grained mixture of experts. arXiv preprint arXiv:2402.07871, 2024. Tanishq Kumar, Zachary Ankner, Benjamin Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Re, and Aditi Raghunathan. Scaling laws for precision. arXiv preprint arXiv:2411.04330, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pp. 611626, 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2024. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36:5035850376, 2023. Aashiq Muhamed, Christian Bock, Rahul Solanki, Youngsuk Park, Yida Wang, and Jun Huan. Training large-scale foundation models on emerging ai chips. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 58215822, 2023. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. Youngsuk Park, Kailash Budhathoki, Liangfu Chen, Jonas Kubler, Jiaji Huang, Matthaus Kleindessner, Jun Huan, Volkan Cevher, Yida Wang, and George Karypis. Inference optimization of foundation models on ai accelerators. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 66056615, 2024. Jackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, and Tal Linzen. The impact of depth on compositional generalization in transformer language models. arXiv preprint arXiv:2310.19956, 2023. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195, 2024. Siva Reddy, Danqi Chen, and Christopher Manning. Coqa: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. Yangjun Ruan, Chris Maddison, and Tatsunori Hashimoto. Observational scaling laws and the predictability of language model performance. arXiv preprint arXiv:2405.10938, 2024. 13 Preprint Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. arXiv preprint arXiv:2401.00448, 2023. Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 49384947, 2020. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. Charles Spearman. The proof and measurement of association between two things. 1961. Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insights from pre-training and fine-tuning transformers. arXiv preprint arXiv:2109.10686, 2021. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024a. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024b. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Alex Wang. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. Carole-Jean Wu, Bilge Acun, Ramya Raghavendra, and Kim Hazelwood. Beyond efficiency: Scaling ai sustainably. IEEE Micro, 44(5):3746, 2024. 14 Preprint Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, et al. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: distributed serving system for {Transformer-Based} generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pp. 521538, 2022. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Efficiently programming large language models using sglang. 2023. 15 Preprint"
        },
        {
            "title": "A LLM USAGE",
            "content": "We used an LLM to improve the writing by correcting grammar in our draft. It was not used to generate research ideas. OPEN-WEIGHTED MODEL ARCHITECTURES Table 3 presents an overview of the open-weight model architectures utilized in this paper. Table 3: Open-Weighted Model Architectures: We list the architectural configurations of all models used in this paper. nlayers is the number of layers, dmodel is the hidden size, nheads is the number of attention heads, and fsize is the intermediate size. Model Name Qwen2.5-1.5B Qwen3-0.6B nlayers 28 28 dmodel 1536 1024 nheads 12 16 fsize GQA 8960 3072"
        },
        {
            "title": "C MODEL ARCHITECTURES",
            "content": "Table 4 provides an overview of the model architectures, all configured with GQA = 4 and employing LLaMA-3.2 as the tokenizer. Table 4: Model Architectures: We list the architectural configurations of all models trained in this paper. Nnon-embed is the total number of non-embedding parameters, nlayers is the number of layers, dmodel is the hidden size, nheads is the number of attention heads, fsize is the intermediate size, and rmlp/attn is the MLP-to-attention ratio. Nnon-embed Variant nlayers 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v12 v13 v14 v15 v16 v17 v18 v19 v20 v21 v22 v23 v24 v25 v26 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 dmodel/ rmlp/attn 0.086 0.086 0.085 0.087 0.086 0.086 0.087 0.043 0.043 0.042 0.044 0.043 0.043 0.044 0.171 0.169 0.174 0.169 0.171 0.174 0.057 0.056 0.057 0.058 0.057 0.057 2.40 12.6 6.00 1.20 0.68 0.36 0.10 2.40 12.6 6.00 1.20 0.68 0.36 0.10 2.40 6.00 1.20 0.75 0.36 0.10 2.40 6.00 4.20 1.20 0.70 0.36 dmodel 768 768 768 768 768 768 768 384 384 384 384 384 384 384 1536 1536 1536 1536 1536 1536 512 512 512 512 512 nheads 16 4 8 24 32 40 48 32 8 16 48 64 80 96 8 4 12 16 20 24 24 12 16 36 48 60 fsize 2048 2688 2560 1536 1152 768 256 4096 5376 5120 3072 2304 1536 512 1024 1280 768 640 384 128 3072 3840 3584 2304 1792 1152 16 Preprint Nnon-embed Variant nlayers 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 80M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M 145M v27 v28 v29 v30 v31 v32 v33 v34 v35 v48 v49 v50 v51 v52 v53 v54 v55 v56 v57 v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v12 v13 v14 v15 v16 v17 v18 v19 v20 v21 v22 v23 v24 v25 v26 v27 v28 v29 v30 v31 v32 v33 v34 v35 v48 v49 v50 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 dmodel/ rmlp/attn 0.058 0.114 0.113 0.115 0.114 0.114 0.226 0.231 0.226 0.086 0.086 0.043 0.043 0.043 0.043 0.058 0.057 0.058 0.113 0.085 0.084 0.086 0.084 0.085 0.086 0.085 0.043 0.042 0.043 0.042 0.043 0.043 0.043 0.170 0.168 0.172 0.168 0.170 0.172 0.170 0.065 0.063 0.064 0.065 0.063 0.064 0.129 0.127 0.128 0.129 0.127 0.128 0.340 0.340 0.086 0.085 0. 0.10 2.40 4.20 1.50 0.70 0.13 4.20 1.50 0.30 1.68 0.94 1.68 1.11 0.94 0.80 1.50 1.02 0.82 1.08 3.60 8.40 2.00 1.35 0.84 0.50 0.15 3.60 8.40 2.00 1.35 0.84 0.50 0.15 3.60 8.40 2.00 1.35 0.84 0.50 0.15 3.00 2.10 1.44 1.00 0.77 0.53 3.00 2.10 1.44 1.00 0.77 0.53 3.60 0.15 1.59 1.07 1.85 fsize 384 1536 1792 1280 896 256 896 640 256 1792 1408 3584 3072 2816 2560 2560 2176 1920 1152 3072 3584 2560 2304 1792 1280 512 6144 7168 5120 4608 3584 2560 1024 1536 1792 1280 1152 896 640 256 3840 3584 3072 2560 2304 1792 1920 1792 1536 1280 1152 896 768 128 2368 2048 5120 dmodel 512 1024 1024 1024 1024 1024 2048 2048 2048 768 768 384 384 384 384 512 512 512 1024 1024 1024 1024 1024 1024 1024 1024 512 512 512 512 512 512 512 2048 2048 2048 2048 2048 2048 2048 768 768 768 768 768 768 1536 1536 1536 1536 1536 1536 4096 4096 1024 1024 512 nheads 72 12 8 16 24 36 4 8 16 20 28 40 52 56 60 32 40 44 20 16 8 24 32 40 48 64 32 16 48 64 80 96 128 8 4 12 16 20 24 32 24 32 40 48 56 64 12 16 20 24 28 32 4 16 28 36 52 17 Preprint Nnon-embed Variant nlayers 145M 145M 145M 145M 145M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 297M 1B 1B v51 v52 v53 v54 v55 v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 v11 v12 v13 v14 v15 v16 v17 v18 v19 v20 v21 v22 v23 v24 v25 v26 v27 v28 v29 v30 v31 v32 v45 v46 v47 v48 v49 v50 v51 v52 v53 v54 v55 v56 v57 v58 v59 v60 v61 v62 v1 v2 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 16 16 dmodel/ rmlp/attn 0.042 0.043 0.043 0.063 0.064 0.089 0.090 0.088 0.090 0.089 0.088 0.090 0.045 0.045 0.044 0.045 0.045 0.044 0.045 0.178 0.180 0.177 0.180 0.178 0.177 0.180 0.059 0.060 0.059 0.060 0.059 0.059 0.060 0.118 0.120 0.118 0.120 0.089 0.089 0.088 0.089 0.045 0.045 0.044 0.045 0.177 0.180 0.060 0.060 0.059 0.060 0.117 0.120 0.118 0.117 0.066 0.067 1.50 1.16 1.03 1.25 0.88 3.20 11.4 5.40 2.10 1.10 0.60 0.24 3.20 11.4 5.40 2.10 1.10 0.60 0.24 3.20 11.4 5.40 2.10 1.10 0.60 0.24 3.20 11.4 5.40 2.10 1.10 0.60 0.24 5.40 2.10 0.60 0.24 1.50 1.31 0.97 0.81 1.50 1.28 0.97 0.79 1.56 0.77 1.63 1.35 0.90 0.71 1.43 1.07 0.90 0.76 4.80 1.50 fsize 4800 4224 3968 2944 2432 4096 4864 4608 3584 2816 2048 1024 8192 9728 9216 7168 5632 4096 2048 2048 2432 2304 1792 1408 1024 512 6144 7296 6912 5376 4224 3072 1536 3456 2688 1536 768 3200 3072 2688 2432 6400 6016 5376 4736 1664 1152 4864 4608 3840 3328 2432 2048 1920 1792 8192 5760 dmodel 512 512 512 768 768 1536 1536 1536 1536 1536 1536 1536 768 768 768 768 768 768 768 3072 3072 3072 3072 3072 3072 3072 1024 1024 1024 1024 1024 1024 1024 2048 2048 2048 2048 1536 1536 1536 1536 768 768 768 768 3072 3072 1024 1024 1024 1024 2048 2048 2048 2048 2048 2048 nheads 60 68 72 44 52 24 8 16 32 48 64 80 48 16 32 64 96 128 160 12 4 8 16 24 32 40 36 12 24 48 72 96 120 12 24 48 60 40 44 52 56 80 88 104 112 20 28 56 64 80 88 32 36 40 44 32 18 Preprint Nnon-embed Variant nlayers 1B 1B 1B 1B 1B 1B 1B 1B 1B 1B 1B 1B 1B 1B 1B v3 v4 v5 v6 v7 v8 v9 v10 v11 v12 v13 v14 v15 v16 v17 16 16 16 16 16 16 16 16 16 16 16 16 16 16 dmodel 2816 2816 2816 2816 2816 2816 2816 2816 2816 2560 2560 2560 2560 2560 2560 nheads 92 76 68 60 56 24 48 40 36 64 72 80 56 88 48 fsize 2432 3072 3584 4096 4480 6144 4736 5120 5376 4480 4096 3648 4864 3200 5376 dmodel/ rmlp/attn 0.089 0.091 0.090 0.090 0.089 0.089 0.090 0.090 0.090 0.082 0.082 0.082 0.082 0.082 0.082 0.50 0.76 0.99 1.28 1.50 4.80 1.85 2.40 2.80 1.31 1.07 0.86 1.63 0.68 2.10 19 Preprint HYPER-PARAMETERS Table 5 lists the detailed hyper-parameters used for training in this paper. Table 5: Hyper-parameters: We show the hyper-parameters used for training in this paper. 3B 512 6.0e-4 Model Size Batch Size Max LR Min LR Optimizer Weight Decay Clip Grad Norm LR Schedule Warmup Steps Sequence Length 1B 512 6.0e-4 80M 145M 297M 256 512 256 8.0e-4 1.0e-3 1.5e-3 0.1 Max LR AdamW (β1 = 0.9, β2 = 0.95) 0.1 1.0 Cosine 500 2048 Preprint"
        },
        {
            "title": "E ADDITIONAL INFERENCE EVALUATION RESULTS",
            "content": "In this section, we present additional inference efficiency results on NVIDIA A100 GPUs. Figure 10 presents that, when parameter count, MLP-to-Attention ratio, and hidden size are fixed, increasing GQA leads to higher inference throughput, consistent with the findings of Ainslie et al. (2023). We alter model configurations of LLaMA-3.2-1B, 3B, and LLaMA-3.1-8B in Figure 10. Figure 8: Hidden size on Inference Throughput: (left) 1B model variants; (center) 3B model variants; (right) 8B model variants. Across varying batch sizes and model scales, larger hidden sizes yield higher inference throughput under fixed parameter budget. The legend indicates the hidden size of the models, where = dmodel. Figure 9: MLP-to-Attention ratio on Inference Throughput: (left) 1B model variants; (center) 3B model variants; (right) 8B model variants. Across varying batch sizes and model scales, larger MLP-to-Attention ratio increases inference throughput under fixed parameter budget. The legend indicates the MLP-to-Attention ratio of the models, where = rmlp/attn. Figure 10: GQA on Inference Throughput: (left) 1B model variants; (center) 3B model variants; (right) 8B model variants. This figure shows the impact of GQA on inference throughput. With the total parameter count fixed, hidden size is set to 2048 (1B), 3072 (3B), and 4096 (8B), and the MLPto-Attention ratio is 4.0, 2.67, and 4.2, respectively. Across varying batch sizes, models with larger GQA achieve higher throughput. All evaluations are performed using the vLLM framework Kwon et al. (2023) on single NVIDIA Ampere 40GB A100 GPU with 4096 input and 1024 output tokens. Furthermore, we derive architectural variants by altering the configurations of Qwen3-0.6B, 1.7B, and 4B to investigate the impact of model architectural factors on inference efficiency. The results are shown in Figure 11-13. Preprint Figure 11: Hidden size on Inference Throughput (Qwen3): (left) Qwen3-0.6B model variants; (center) Qwen3-1.7B model variants; (right) Qwen3-4B model variants. Across varying batch sizes and model scales, larger hidden sizes yield higher inference throughput under fixed parameter budget. The legend indicates the hidden size of the models, where = dmodel. All evaluations are performed using the vLLM framework Kwon et al. (2023) on single NVIDIA Ampere 40GB A100 GPU with 4096 input and 1024 output tokens. Figure 12: MLP-to-Attention ratio on Inference Throughput (Qwen3): (left) Qwen3-0.6B model variants; (center) Qwen3-1.7B model variants; (right) Qwen3-4B model variants. Across varying batch sizes and model scales, larger MLP-to-Attention ratio increases inference throughput under fixed parameter budget. The legend indicates the MLP-to-Attention ratio of the models, where = rmlp/attn. All evaluations are performed using the vLLM framework Kwon et al. (2023) on single NVIDIA Ampere 40GB A100 GPU with 4096 input and 1024 output tokens. Figure 13: GQA on Inference Throughput (Qwen3): (left) Qwen3-0.6B model variants; (center) Qwen3-1.7B model variants; (right) Qwen3-4B model variants. This figure shows the impact of GQA on inference throughput. With the total parameter count fixed, hidden size is set to 1024 (0.6B), 2048 (1.7B), and 2560 (4B), and the MLP-to-Attention ratio is 1.5, 3.0, and 2.85, respectively. Across varying batch sizes, models with larger GQA achieve higher throughput. All evaluations are performed using the vLLM framework Kwon et al. (2023) on single NVIDIA Ampere 40GB A100 GPU with 4096 input and 1024 output tokens. 22 Preprint ADDITIONAL RESULTS: LOSS VS. MODEL ARCHITECTURE In this section, we analyze the relationship between training loss and GQA while fixing the number of parameters, hidden size, and MLP-to-Attention ratio. As shown in Figure 14, unlike hidden size and MLP-to-Attention ratio, the relationship between loss and GQA is highly fluctuating. Figure 14: Loss vs. GQA: (left) 80M model variants; (center) 145M model variants; (right) 297M model variants. Across different model sizes, the relationship between training loss and GQA varies substantially when hidden size and the mlp-to-attention ratio are fixed. The legend denotes the hidden size of each trained model. 23 Preprint"
        },
        {
            "title": "G MORE ABLATION STUDY",
            "content": "In this section, We first evaluate the impact of outlier data on the fitting of the scaling laws in Figure 15 (left) and Figure 15 (center). Then, we evaluate the fitting performance of multiplicative calibrations and additive calibrations in Figure 15 (left) and Figure 15 (right). Finally, we evaluate the performance of Joint and non-separable calibrations shown below in Figure 16: (a0 + a1 log( dr ) + a2/( dr )) Lopt where = dmodel, = rmlp/attn, and = Nnon-embed. In Figure 16, we observe that the performance of joint and non-separable calibrations is significantly worse than that of multiplicative calibration, consistent with our discussion in 3.3. Figure 15: Ablation Study: (left) use multiplicative calibrations without outliers; (center) use multiplicative calibrations with outliers; (right) use additive calibrations without outliers. The outlier refers to models trained with an mlp-to-attention ratio below 0.5 or above 5. We observe that outlier data points harm the scaling law fit. Moreover, while multiplicative and additive calibrations differ in formulation, their MSE and Spearman values remain nearly identical. Dots denote the data points used for fitting, while crosses indicate the test data points. Figure 16: Joint and non-separable calibrations: (left) use multiplicative calibrations; (right) use joint and non-separable calibrations. We observe that joint and non-separable calibrations yield higher MSE and lower Spearman scores than multiplicative calibrations, indicating inferior performance. Dots denote the data points used for fitting, while crosses indicate the test data points. 24 Preprint"
        },
        {
            "title": "H INFERENCE FLOPS ANALYSIS",
            "content": "Building on the inference FLOPs analysis from prior work Kaplan et al. (2020), we begin with the following definition: dmodel: hidden size fsize: intermediate (feed-forward) size nlayers: number of layers A: number of query heads K: number of key/value heads dh: per-head hidden dimension (query and value) : per-head hidden dim the KV length prior to token generation Based on the above definition, we have dq = Adh and dkv = Kdh. We focus exclusively on non-embedding FLOPs, resulting in: Attention: QKV and Project nlayers(2dmodeldq (cid:124) (cid:125) (cid:123)(cid:122) + 2dmodeldkv (cid:123)(cid:122) (cid:125) (cid:124) + 2dmodeldkv (cid:123)(cid:122) (cid:125) (cid:124) ) + 2dmodeldq (cid:125) (cid:123)(cid:122) (cid:124) Attention: Mask Feedforward: nlayers(2T dq) nlayers(3 2dmodelfsize) Total Inference non-embedding FLOPs: Total-FLOPs = nlayers(2dmodeldq (cid:125) (cid:124) (cid:123)(cid:122) + 2dmodeldkv (cid:123)(cid:122) (cid:125) (cid:124) + 2dmodeldkv (cid:123)(cid:122) (cid:125) (cid:124) + 2dmodeldq (cid:123)(cid:122) (cid:125) (cid:124) + 2T dq (cid:124) (cid:123)(cid:122) (cid:125) qK ) + 3 2dmodelfsize (cid:125) (cid:123)(cid:122) up, gate, down (cid:124) Since Pnon-emb nlayers(2dmodeldq + 2dmodeldkv + 3dmodelfsize). Therefore, Total-FLOPs = 2Pnon-emb + 2nlayersT dq we adopt the following three approaches to accelerate inference: Increasing the MLP-to-Attention ratio reduces the term 2T dq, thereby lowering the total FLOPs. Increasing the hidden size reduces the term 2T dq, thereby lowering the total FLOPs. 25 Preprint MORE LARGE-SCALE TRAINING RESULTS In this section, we first show the detailed result over downstream tasks of large-scale models in Table 6 and Table 7. Table 6: Detailed Results on Downstream Tasks for 1B Models: In this table, we show detailed results of 1B models over 9 downstream tasks. Downstream Tasks LLaMA-3.2-1B Panda-1B Surefire-1B Arc-Easy Arc-Challenge LAMBADA HellaSwag OpenBookQA PIQA SciQ WinoGrande COQA Avg. 58.8 29.8 52.8 56.9 32.0 73.6 84.8 57.1 48.7 54. 60.9 28.9 55.1 58.4 33.2 75.2 87.2 58.6 55.3 57.0 59.7 30.2 52.0 56.6 32.0 73.0 84.9 57.5 52.7 55.4 Table 7: Detailed Results on Downstream Tasks for 3B Models: In this table, we show detailed results of 3B models over 9 downstream tasks. Downstream Tasks LLaMA-3.2-3B Panda-3B Surefire-3B Panda-3B Arc-Easy Arc-Challenge LAMBADA HellaSwag OpenBookQA PIQA SciQ WinoGrande COQA Avg. 66.4 33.3 60.6 66.7 38.4 76.8 89.4 62.5 63.3 61. 65.5 35.2 61.8 66.9 38.6 76.9 91.2 63.2 63.4 62.5 67.6 33.9 61.4 67.0 38.6 77.4 92.1 60.5 65.4 62.6 66.8 33.3 61.5 67.8 38.0 76.8 90.5 62.7 64.9 62.5 26 Preprint"
        },
        {
            "title": "J MOE INFERENCE",
            "content": "In this section, we examine how the Mixture-of-Experts (MoE) architecture affects inference efficiency. Figure 17 indicates that larger hidden sizes and higher Active-Experts-to-Attention ratios improve the inference throughput of MoE models, consistent with observations in dense models. Figure 17: Active-Experts-to-Attn on Inference Throughput: (left) 3B-A1.1B model variants; (center) 5.3B-A1.7B model variants; (right) 8.3B-A1.5B model variants. We study the effect of the Active-Experts-to-Attention ratio on inference throughput by fixing the total number of active parameters, setting GQA to 4, and using batch size of 2048 to reduce MoE inference variance in this figure. All evaluations are performed using the vLLM framework Kwon et al. (2023) on single NVIDIA Ampere 40GB A100 GPU with 1024 input and 256 output tokens."
        }
    ],
    "affiliations": [
        "Amazon Web Services",
        "UW-Madison"
    ]
}