{
    "paper_title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement",
    "authors": [
        "Teng Hu",
        "Zhentao Yu",
        "Zhengguang Zhou",
        "Jiangning Zhang",
        "Yuan Zhou",
        "Qinglin Lu",
        "Ran Yi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent advances in video generation, existing models still lack fine-grained controllability, especially for multi-subject customization with consistent identity and interaction. In this paper, we propose PolyVivid, a multi-subject video customization framework that enables flexible and identity-consistent generation. To establish accurate correspondences between subject images and textual entities, we design a VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding. To further enhance identity preservation and subject interaction, we propose a 3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings. Moreover, we develop an attention-inherited identity injection module to effectively inject fused identity features into the video generation process, mitigating identity drift. Finally, we construct an MLLM-based data pipeline that combines MLLM-based grounding, segmentation, and a clique-based subject consolidation strategy to produce high-quality multi-subject data, effectively enhancing subject distinction and reducing ambiguity in downstream video generation. Extensive experiments demonstrate that PolyVivid achieves superior performance in identity fidelity, video realism, and subject alignment, outperforming existing open-source and commercial baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 8 4 8 7 0 . 6 0 5 2 : r PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement Teng Hu1 Zhentao Yu2 Zhengguang Zhou2 Jiangning Zhang3 Yuan Zhou2 Qinglin Lu2 Ran Yi1 1Shanghai Jiao Tong University 2Tencent Hunyuan 3Zhejiang University Figure 1: PolyVivid can generate high-quality customized videos from multiple subject images and text prompt, which ensures high subject similarity and good subject interaction specified by the text."
        },
        {
            "title": "Abstract",
            "content": "Despite recent advances in video generation, existing models still lack fine-grained controllability, especially for multi-subject customization with consistent identity and interaction. In this paper, we propose PolyVivid, multi-subject video customization framework that enables flexible and identity-consistent generation. To establish accurate correspondences between subject images and textual entities, we design VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding. To further enhance identity preservation and subject interaction, we propose 3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings. Moreover, we develop an attention-inherited identity injection module to effectively inject fused identity features into the video generation process, mitigating identity drift. Finally, we construct an MLLM-based data pipeline that combines MLLM-based grounding, segmentation, and clique-based subject consolidation strategy to produce high-quality multi-subject data, effectively enhancing subject distinction and reducing ambiguity in downstream video generation. Extensive experiments demonstrate that PolyVivid achieves superior performance in identity fidelity, video realism, and subject alignment, outperforming existCorresponding author. Preprint. Under review. ing open-source and commercial baselines. For more details, please refer to https://sjtuplayer.github.io/projects/PolyVivid."
        },
        {
            "title": "Introduction",
            "content": "In recent years, the field of video generation has witnessed remarkable progress [40, 23, 16, 25, 15], with the emergence of numerous open-source and commercial video-generation models. These advancements have significant real-world implications, ranging from content creation in the entertainment industry to applications in artist design, education, advertising, etc. However, despite these achievements, current video-generation models suffer from notable limitation insufficient controllability. It remains challenging for these models to generate videos precisely tailored to users specific requirements, which restricts their potential applications in various scenarios. Video customization aims to generate videos featuring user-specified subjects. Existing approaches fall into two categories: (1) Instance-specific methods that fine-tune the model for each subject identity [43, 42, 19, 13], which are timeand resource-intensive; and (2) End-to-end methods that extract identity features from subject images and inject them into the generation process. While recent works like ConsisID [47] and MovieGen [33] show promise, they are limited to single-human scenarios and cannot handle arbitrary subject types or multiple identities. To support multi-subject video customization, recent methods like ConceptMaster [17], Video Alchemist [7], Phantom [27], SkyReels-A2 [10], and VACE [20] extend single-subject frameworks by incorporating multiple image conditions. However, they still struggle with three key challenges: (1) learning precise correspondences between subject images and textual entities to model correct actions and interactions; (2) maintaining identity consistency across multiple subjects; and (3) resolving semantic ambiguity during data construction for accurate text-image alignment. To address the above challenges, we propose PolyVivid, novel multi-subject video customization framework that enables flexible, controllable, and identity-consistent video generation. To establish accurate correspondences between subject images and their textual descriptions, we first design VLLM-based text-image fusion module. By leveraging the semantic understanding capabilities of Vision Large Language Models (VLLMs), this module encodes subject images into the textual embedding space, enabling the model to correctly ground each image to its corresponding entity in the prompt. To further enhance identity preservation and enable richer subject interactions, we propose 3D-RoPE-based identity-interaction enhancement module. This module enables structured bidirectional information flow between the text embeddings from VLLM and the subject image embeddings encoded by pretrained VAE, enhancing identity information in text embeddings and injecting interaction semantics into the image embeddings. Finally, to efficiently inject both the identity-enhanced text embeddings and the interaction-enhanced image embeddings, we propose an attention-inherited identity injection module, which leverages the pretrained MM-Attentions multimodal processing capability to construct new condition injection module. In this way, each frame can receive sufficient conditioning information, ensuring consistent subject appearance and preventing identity drift throughout the video. Additionally, to mitigate confusion between semantically similar entities, such as multiple humans, we develop MLLM-based data construction pipeline that integrates MLLM-based grounding and segmentation with the proposed clique-based subject consolidation module. This pipeline leverages cross-modal cues to enhance subject discriminability and employs clique analysis to filter out inconsistent or transient subjects, further improving the accuracy and robustness of the generated results. PolyVivid has been extensively experimented on multi-subject video customization. We compared it with existing open-source methods and closed-source commercial software, conducting comprehensive comparisons from multiple aspects such as ID consistency, authenticity of the generated videos, and video-text consistency. Extensive experiments show that PolyVivid outperforms all existing methods in multi-subject video customization. The contributions can be summarized in four-fold: We propose PolyVivid, novel multi-subject video customization model that leverages VLLM models to establish the correspondences between the text prompt and subject images, enabling high subject-consistency video generation and complex subject interactions. We propose new 3D RoPE-based identity-interaction enhancement module to enable structured bidirectional information flow between textual and visual modalities. This 2 mechanism enhances identity information in the text embedding and enriches the interaction information in the image embeddings, enabling more effective cross-modal interaction. We design an Attention-inherited identity injection module that utilizes the multimodal processing capabilities of the pretrained MM-Attention to develop novel condition injection module. This approach ensures that each frame receives sufficient conditioning information, maintaining consistent subject appearance and preventing identity drift throughout the video. We propose MLLM-based data construction pipeline that combines MLLM-based grounding, segmentation, and clique-based subject consolidation to improve multi-subject discriminability and reduce subject ambiguity in multi-subject data construction."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Video Generation Model The development of diffusion models [36] has significantly advanced video generation. Early models [2, 11] extended pre-trained text-to-image models for continuous video generation by adding temporal modeling. Recently, works [29, 40, 23, 48, 45] have employed advanced Diffusion Transformers [31, 9], trained on large-scale text-video data, to produce longer and higher-quality videos. However, while many models focus on text-to-video and image-to-video generation, there is still potential for improving fine-grained controllability in video generation. 2.2 Video Customization Instance-specific video customization methods use multiple images of the same subject to fine-tune pre-trained video generation model, training each subject separately. Still-Moving [4] fine-tunes video models with LoRA to create static frame videos, then repeats the image as static video and uses DreamBooth [37] to learn the identity. CustomCrafter [43] repeats the image across frames, embeds it into text space, and fine-tunes the model for better identity learning. CustomVideo [42] and DisenStudio [5] extend customization to multiple subjects by segmenting and combining images, aligning subject identity with text through cross-attention maps. These methods rely on instancespecific optimization, posing challenges for real-time or large-scale video customization. End-to-end video customization methods integrate identity information from target images through additional conditioning networks, which allows for the generalization to various identity inputs. Earlier works focus on preserving facial identity. For example, ID-Animator [13] uses face adapter and facial identity loss to ensure facial ID consistency. ConsisID [47] captures comprehensive ID information by extracting lowand high-frequency details from facial images. MovieGen [33] embeds facial ID information into the text space and uses facial images from different videos to guide generation, reducing facial copying issues. To customize arbitrary objects, VideoBooth [19] incorporates identity information using coarse CLIP features and fine-grained image features. Recent works like ConceptMaster [17], Video Alchemist [7], Phantom [27], SkyReels-A2 [10], VACE [20], and HunyuanCustom [16] extend customization to multiple subjects by linking text prompts to subject images, enabling multi-subject video generation. However, challenges remain in maintaining and interacting with multiple subject IDs due to the complexity of interactions and mutual influence among them."
        },
        {
            "title": "3 MLLM-based Data Construction",
            "content": "In this section, we outline the creation of our multi-subject customization dataset, emphasizing the grounding and segmentation process. Details on data filtering and captioning are in the supplementary materials. MLLM-based Subject Segmentation. For subject extraction in videos, previous methods either use GroundingDINO+SAM [28, 35] to detect object boxes based on text and then segment the boxes with SAM, or employ MLLM-based segmentors like LISA [24] to directly segment the subject parts corresponding to the captions. However, these methods have limitations: (1) The GroundingDINO+SAM approach struggles with fine-grained semantic distinctions, such as differentiating between two people in video, leading to inaccurate caption-to-semantic correspondence. (2) MLLM-based 3 Figure 2: Framework of our PolyVivid: the text prompt and reference image are fused by the VLLMbased text-image fusion module. Then, 3D RoPE-based identity-interaction enhancement module is employed to enhance the text-image interaction. The enhanced image tokens are injected by an MM cross-attention module, which helps preserve the identities while ensuring good subject interaction. segmentation methods can better distinguish fine-grained semantics but suffer from fragmented and low-quality masks due to the scarcity of multimodal segmentation data. To address these issues, we adopt MLLM-based detection+SAM segmentation paradigm for accurate and efficient multi-subject segmentation. Specifically, we use Florence2 [44], MLLM-based detection model, to detect subject location boxes in caption given an image and caption. We then use SAM2 to segment within these boxes, selecting the largest object as the subject. The segmentation is considered valid if the CLIP score between the segmented subject and the corresponding text exceeds certain threshold. Clique-based Subject Consolidation. Finally, to further enhance the accuracy of multi-subject detection and segmentation and prevent any subject from fleetingly appearing in the video, we extract CLIP features for each subject image and construct graph = (V, E), where each node represents subject image. We calculate the feature distance between each pair of subject images, connecting them with an edge if the distance is below certain threshold. We then iteratively detect the maximum clique (a subgraph where every pair of nodes is connected) in the graph. If the number of nodes in the maximum clique exceeds one-third of the total detected frames, we extract it from the graph and continue searching for the next maximum clique, until the maximum clique has fewer nodes than one-third of the total detected frames. Ultimately, we obtain multiple cliques, each representing reference image of subject in the video. This algorithm effectively avoids errors in detection and segmentation in certain frames and removes subjects that appear in only few frames, thereby improving the quality and robustness of the multi-subject generation model."
        },
        {
            "title": "4 Method",
            "content": "PolyVivid is proposed for multi-subject video customization. It generates videos from multiple subject images {I1, I2, , In} and text prompt describing scenes, actions, and interactions, ensuring identity preservation and accurate text-specified interactions. The framework is shown in Fig. 2. First, VLLM-based text-image fusion module encodes both text and subject images into the text space as {zT,T , zT,I }, capturing high-level semantic associations. To address LLaVAs limitation in finegrained identity details, we use pretrained VAE to extract detailed visual identity features from the subject images, obtaining image embeddings zI . Next, an identity-interaction enhancement module with text-image interaction 3D RoPE facilitates mutual information flow between modalities. Image tokens inject identity information into text tokens, while text tokens provide interaction cues to image tokens, resulting in identity-enhanced text tokens and interaction-aware image tokens. Finally, an Attention-Inherited Identity Injection Module injects interaction-enhanced image tokens into the 4 video latent space via an MM cross-attention mechanism to ensure consistent identity preservation, and the identity-enhanced text tokens guide subject-specific interactions during video generation using the pretrained MM-attention. By integrating these components, PolyVivid achieves high-fidelity video generation with good identity consistency and text-aligned multi-subject interactions. 4.1 VLLM-based Text-image Fusion In video customization tasks, the model first needs to learn the relationship between the input text and images to identify which subject image corresponds to which entity in the text, enabling the generation of corresponding subject actions as described in the prompt. However, how to effectively integrate image-text information has been key challenge for previous customization methods. These methods usually take subject image features and text embeddings as two separate conditions, and either lack design for interactive understanding between them (lacks the learning of correspondence between text entities and subject images), or rely on additional newly trained network branches to achieve this interaction, causing the model prone to confusing different subjects within the generated video. To facilitate efficient image-text interaction understanding, PolyVivid leverages the text comprehension capabilities trained in the LLaVA [26] text space by HunyuanVideo [23] and utilizes LLaVAs multimodal interaction understanding to build the connection between the text and images. Given text input and several subject images {I1, , In}, each with corresponding description word {TI,1, , TI,n} in the text, we design structured template to explicitly link each image with its textual entity. This template is processed by LLaVA to learn multi-modal correlations between text and image identities. LLaVA is multi-modal model trained for visual question answering and dialogue tasks. It uses conversation-style prompt where images and texts are interleaved, employing pretrained vision encoder (i.e., CLIP-ViT) to extract image features, which are integrated with text tokens through causal language modeling. Our structured template appends identity prompts after the input text prompt, associating each subject word with its corresponding subject image, formatted as: (cid:124)(cid:123)(cid:122)(cid:125) Text prompt <SEP> The TI,1 looks like <image 1>. (cid:125) (cid:124) (cid:123)(cid:122) Identity prompt 1 The TI,2 looks like <image 2>. (cid:123)(cid:122) (cid:125) (cid:124) Identity prompt 2 (1) where <SEP> is the special token marking the end of dialogue round; <image i> is the token for subject image i. For example, with the text prompt \"A man is playing guitar\", the resulting template is \"A man is playing guitar. <SEP> The man looks like <image 1>. The guitar looks like <image 2>\". When the template is input to LLaVA, each <image i> token is replaced with visual tokens extracted from the corresponding image using CLIP image encoder, typically resulting in 24 24 tokens per image. This structured template is then processed by LLaVAs autoregressive language model to produce joint multi-modal embeddings. Due to the much longer sequence of image tokens compared to text tokens, directly appending them can cause the model to be overly influenced by visual content, disrupting textual understanding. To address this, the insertion of <SEP> acts as soft delimiter, reducing interference between text and image parts while allowing LLaVA to correctly associate subjects with their visual references. After LLaVA processes the multimodal image-text interaction, the output is fused text embeddings zT , which can serve as text inputs for the video generation model to synthesize customized videos that reflect the specified subjects and contextual information. 4.2 Identity-Interaction Enhancement by Text-image Interaction The LLaVA model used in our text-image fusion (Sec. 4.1, as multimodal understanding framework, is primarily designed to capture high-level semantic correlations between text and imagessuch as category, color, and shapewhile often overlooking finer-grained details like textural cues and detailed visual attributes. However, in the context of video customization, these fine details are crucial for accurate identity preservation, making the LLaVA branch alone insufficient. To complement this, we leverage the pretrained VAE from the base video generation model (HunyuanVideo) to encode subject images into image embeddings zI , which effectively retain identity-specific information. So far, we have obtained text embedding zT from LLaVA (Sec. 4.1) that captures interaction-related semantics but lacks detailed identity information, and an image embedding zI that captures identity features but lacks subject interaction context. To compensate for these missing information, we propose text-image interaction module based on 3D-interacted RoPE, designed to enhance identity features in zT and enhance interaction semantics in zI . 5 Text-image Interaction Module. Our base video generation model (HunyuanVideo) is built upon Multimodal Attention Framework, where each model block contains MM-attention module that integrates text and video embeddings to establish cross-modal interactions and achieve video-text alignment. Since subject image can be viewed as single-frame video, we repurpose the MMattention mechanism to facilitate interaction between text and image embeddings. This enables the injection of identity information from image embedding zI into text embedding zT and, conversely, the infusion of semantic interaction cues from text embedding zT into image embedding zI . Specifically, the MM-attention module in the base model contains the Query, Key, and Value matrices for both video embedding and text embedding , i.e., {q,k,v} for text embedding. To effectively process the image and text embeddings, we adopt LoRA [14]-based approach to fine-tune the Query, Key, Value matrices, and feed-forward network (FFN) weights, enabling efficient and effective adaptation of the MM-attention layers for text-image fusion: {q,k,v} for video embedding, and , z = -Attn({ ˆW (zI ), ˆW ˆzI , ˆzT = ˆF (z (zI ), ˆW zT }, { ˆW ˆF (z ), zT }, { ˆW ), (zI ), ˆW zT }), (2) where { ˆW {V,T } , ˆW {V,T } Attention and LoRA module. , ˆW {V,T } ˆF } are the weights combined by the pretrained MM- , Text-image Interaction 3D-RoPE. Our text-image interaction module takes both text embedding and image embedding as inputs, aiming to enhance identity information in the text embedding and enrich interaction semantics in the image embedding. However, directly integrating these modalities through standard attention mechanism often fails to capture their mutual correspondence effectively, as text tokens are organized as one-dimensional sequences, while image tokens exhibit two-dimensional spatial structure. This mismatch hinders the alignment and fusion of information between modalities, leading to ineffective interaction modeling. To overcome this challenge, we propose text-image interaction 3D-RoPE, which facilitates structured and fine-grained positional encoding for both modalities, in order to bind text tokens and image tokens of the same subject. This design enables more effective cross-modal interaction while preserving the intrinsic semantics of each modality. Specifically, the text embedding zT consist of two types of tokens: zT,T and zT,I , where zT,T is derived from the input text prompt and zT,I is derived from the input subject images. The text tokens are represented as: zT = {z1 (cid:124) , zm1+1 T,I (cid:124) , zm2+1 T,T (cid:124) , zm3+1 , , zm3 T,I T,T (cid:124) (cid:125) (cid:123)(cid:122) text T,T , , zm1 T,T (cid:123)(cid:122) (cid:125) text , , zm4 }. T,I (cid:125) (cid:123)(cid:122) <image 2> , , zm2 T,I (cid:123)(cid:122) (cid:125) <image 1> (3) We denote the text tokens zT,T encoded from the text prompt as <text> tokens, and the text tokens zT,I encoded from the subject image as <image> tokens. In addition, the image tokens zI refer to the tokens extracted from the subject image by the VAE encoder. The differences between these two kinds of image tokens are that: The <image> tokens zT,I (obtained from LLaVA) reside in the text space and capture rich interaction information derived from the text, while the image tokens zI (obtained from VAE encoder) retain detailed identity features extracted from the subject image. Our text-image interaction 3D-RoPE is shown in Fig. 2. To preserve the source text information, it is crucial to maintain the sequential relationship in zT,T . Therefore, we assign the <text> tokens in zT,T with position encodings of 3D RoPE, setting the spatial dimension to (0, 0), and assign them sequentially along the temporal dimension. Take the first subject as an example, the RoPE position indices for its <text> tokens are set as: IdxRoP E(zi = 1, , m1. T,T ) = (i, 0, 0), (4) This method preserves the original sequential relationship of zT,T along the temporal axis. To model the interaction between <text> tokens and <image> tokens, we assign the <image> tokens with 3D RoPE of temporal index m1 + 1 and expanded along the spatial axis, centering them at (0, 0) in the spatial dimensions. The RoPE position indices for the first subjects <image> tokens are set as: IdxRoP E(zm1+i 2 where and are the width and height of the encoded image, respectively. Since the same subjects <text> tokens and <image> tokens RoPE positions are close in temporal dimension, their position encodings are close, thereby stronger correlations are more likely to be obtained in MM-Attention. ) = (m1 + 1, , mod = 1, , wh. 2 ), T,I (5) 6 Figure 3: Comparison of the condition injection strategies for MM-DiT. For the image tokens zI , we assign them with 3D RoPE of temporal index m1 + 2 and spatial indices aligned with those of the <image> tokens. This alignment facilitates efficient pixel-by-pixel interaction between the two sets of image tokens. The RoPE position indices for the first subjects image tokens zI are set as: IdxRoP E(zi ) = (m1 + 2, 2 , mod 2 ), = 1, , wh, (6) For multi-subject inputs, we expand them iteratively along the time axis, with the second subjects <text> tokens starting from (m1 + 3, 0, 0). The relationships among <text>, <image>, and image tokens remain consistent as described in Eq. (4) to Eq. (6). This text-image 3D RoPE effectively preserves the sequential relationship between <text> and <image> tokens. Additionally, it enables efficient pixel-by-pixel interaction between <image> tokens and image tokens, enhancing the identity of the <image> tokens. Furthermore, since image tokens are embedded between text tokens through this 3D RoPE, they can efficiently interact with text tokens, thereby enriching the interaction information in image tokens. With this text-image interaction 3D-RoPE, it enables structured bidirectional information flow between text and image tokens via Eq. (2), resulting in identity-enhanced text tokens ˆzT and interaction-enhanced image tokens ˆzI . 4.3 Attention-inherited Identity Injection With the identity-enhanced text tokens ˆzT and interaction-enhanced image tokens ˆzI obtained in Sec. 4.2, the remaining challenge is how to effectively inject both types of tokens into the video generation process to produce subject-consistent videos aligned with the text-described context. Problems in Current Controllable Generation Methods. Our model is built upon HunyuanVideo, which is based on MM-DiT, and we consider two common condition injection strategies used in MM-DiT: 1) Token concatenation (Fig. 3 (a)), as used in OmniControl [38], involves concatenating the condition tokens with the latent video tokens and using MM-Attention to learn their interaction. 2) Adapter-based methods (Fig. 3 (b)), such as IP-Adapter [46], extract condition features via an external image encoder and inject them into the pretrained model through cross-attention layers. Both approaches are originally designed for image generation, and applying them directly to video generation presents challenges. In token concatenation, appending image tokens before or after video tokens can lead to temporal imbalance, i.e., frames distant from the condition image receive weaker guidance, resulting in identity degradation. Adapter-based methods struggle to inject information effectively into MM-DiT due to the large dimensionality of its feature space (considering the long token sequence of video); the adapters features often misalign with the pretrained models internal representations, hindering effective conditioning. Attention-inherited Identity Injection. To overcome the limitations of existing controllable generation methods, we propose novel Attention-Inherited Identity Injection Module. This module uses parameters from the pretrained MM attention module in the base model to efficiently inject identity by fusing interaction-enhanced image tokens with video tokens (Fig. 3 (c)). We reparameterize the Key and Value matrices of the video tokens using LoRA module to incorporate image token 7 Figure 4: Comparison on multi-subject video customization. information, and reparameterize the Query matrices using another LoRA module to maintain videospecific representations. This setup constructs multi-modal cross-attention mechanism where image tokens ˆzI are injected into the video token stream z, ensuring effective identity injection. This design treats all video frames equally, eliminating disparities between earlier and later frames of the token concatenation strategy, thus mitigating identity degradation over time. To stabilize early-stage training, we use zero-initialized fully-connected layer to project the cross-attention output, reducing the impact of randomly initialized attention weights. The identity injection process is formulated as: = Cross-Attn( ˆW (z), ˆW k (ˆzI ), ˆW (ˆzI )), ˆz = C( ˆF (z)), (7) (8) ˆF } are weights combined by the pretrained MM-Attention and LoRA where { ˆW module, and C() is zero-initialized fully connected layer. ˆz is identity-enhanced video tokens. , ˆW , , ˆW After the identity injection, we utilize the original MM-Attention module from the pretrained model to establish the connection between the identity-enhanced text tokens ˆzT and the identity-enhanced video tokens ˆz. This allows the text information to be effectively integrated into the video tokens without compromising identity integrity, thereby supporting both strong identity preservation and vivid subject interaction generation."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implementation Details Baselines. We compare PolyVivid with the state-of-the arts video customization methods, including commercial products (Vidu-2.0 [39], Keling-1.6 [21], Pika [32], and Hailuo [12]) and open-sourced methods (Skyreels-A2 [10] and VACE-1.3B [20]). For each model, we generate 100 videos, which are employed to compute the quantitative metrics. More implementation details are presented in the supplementary material. 5.2 Comparison Results on Multi-subject Customization Qualitative comparisons. We conduct experiments on four types of multi-subject customized video generation to evaluate the effectiveness of our model: (1) Rigid human-object interaction (e.g., person holding an object); (2) Non-rigid human-object interaction (e.g., person wearing flexible clothing); (3) Human-human interaction; and (4) Object-object interaction. In Fig. 4 (with Rigid Table 1: We compare PolyVivid with state-of-the-art video customization methods across multiple metrics. Bold and underline represent the best and second best results, respectively. Metric Face-sim DINO-sim CLIP-B CLIP-L FVD Temporal VACE-1.3B [20] SkyReels-A2 [10] Keling-1.6 [21] Vidu-2.0 [39] Pika [32] PolyVivid (Ours) 0.433 0.554 0.534 0.532 0.546 0.642 0.598 0.619 0.554 0.588 0.548 0.623 0.335 0.332 0.330 0.336 0.310 0.336 0.280 0.276 0.280 0.282 0.263 0.281 1171.42 1379.65 1049.70 1083.35 980.49 959.74 0.966 0.943 0.934 0.970 0.942 0. Table 2: Quantitative Ablation Study. LLaVA Text-Img Interaction Text-Img 3D-RoPE IDinjection LLaVA + Adapter LLaVA + Token-Concatenation Facesim 0.381 0.345 0.584 0.601 0.642 0.401 0.628 DINOsim 0.521 0.496 0.581 0.605 0.623 0.532 0.615 CLIP-B CLIP-L FVD Temporal 0.345 0.334 0.330 0.340 0.336 0.338 0.328 0.291 0.280 0.279 0.286 0.281 0.285 0. 1150.48 1052.34 1052.14 965.62 959.74 1020.35 980.56 0.950 0.961 0.965 0.963 0.964 0.952 0.960 human-object interaction and Human-human interaction included in the #Suppl), we compare our model with the state-of-the-art methods. It can be seen that Pika often produces blurry outputs when handling complex interactions. Vidu and Keling struggle to dress the human subjects in the specified clothing. Vidu and VACE confuse animal features, generating tiger with the shape of giraffe or giraffe with the shape of tiger. SkyReels-A2 introduces noticeable artifacts and inconsistent transitions between frames, and struggles to accurately capture the relative size of different animals (the generated tiger is bigger than the giraffe). Furthermore, all baseline methods except Keling suffer from significant identity degradation, especially in humans. In contrast, our PolyVivid generates videos with high identity consistency while effectively modeling complex interactions between multiple subjects as specified by the text prompts. Quantitative Comparisons. To further demonstrate the effectiveness of our approach, we construct benchmark test set containing 100 object pairs, each associated with corresponding interaction text prompt. We apply each baseline method to generate 100 videos and evaluate the results using comprehensive set of quantitative metrics, including face and object similarity, text-video alignment, and overall video quality. The comparative results are summarized in Tab. 1. As shown in the table, PolyVivid achieves the highest similarity scores for both face and object identity (Face-sim and DINO-sim), highlighting its strong capability in preserving key appearance features across video frames. In terms of text-video alignment, our method obtains the best and second-best CLIP scores among all competitors, suggesting that the generated content accurately reflects the intended semantics of the text prompts. Furthermore, PolyVivid yields the lowest FVD score, indicating superior video quality and diversity. It is worth noting that the FVD is computed against reference set of 1,500 high-quality 4K videos, further underscoring the realism of our results. While our method ranks third in temporal consistency, we observe that certain videos generated by Vidu and VACE remain mostly static, which slightly inflates their temporal consistency scores. Despite this, our method still demonstrates strong temporal coherence. In summary, PolyVivid not only delivers the best performance in keeping identities across humans and objects, but also achieves strong semantic alignment and generation quality, validating its effectiveness for multi-subject video customization. 5.3 Ablation Study In this section, we first compare our model with two existing condition injection strategies: (1) adapter-based injection and (2) token concatenation-based injection. We then conduct an ablation study on four key components of our framework: (1) the text-image fusion module based on LLaVA, (2) the text-image interaction module, (3) the text-image interaction 3D-RoPE, and (4) the identity injection module. Quantitative results are shown in Tab. 2. The adapter-based model struggles to capture identity information, yielding only marginal improvements over the baseline with LLaVA alone. The token concatenation approach maintains identity better, but at the cost of reduced text9 image alignment, as reflected by lower CLIP score. Individually, the LLaVA fusion and text-image interaction module do not provide strong identity preservation, but their combination leads to notable improvement. Incorporating our proposed text-image interaction 3D-RoPE further enhances both identity consistency and text alignment, demonstrating its effectiveness for text-image interaction. Finally, the identity injection module significantly boosts identity preservation and achieves the best FVD score, confirming its effectiveness in subject-consistency video generation. By integrating all proposed modules, our method achieves strong identity preservation, accurate text-video alignment, and high-quality video generation."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present PolyVivid, novel multi-subject video customization framework that addresses the key limitations of existing methods in controllability, identity consistency, and complex subject interaction. By incorporating VLLM-based vision-language fusion module, 3D-RoPEbased identity-interaction enhancement module, and an attention-inherited identity injection module, our model effectively bridges the gap between text and image modalities while preserving subject identities throughout video generation. Furthermore, our proposed multi-subject data construction pipeline enhances the ability to distinguish semantically similar entities, ensuring reliable multisubject customization. Extensive experiments demonstrate that PolyVivid significantly outperforms prior state-of-the-art methods in identity consistency, text alignment, and video realism, achieving superior performance in multi-subject video customization, which opens up new possibilities for controllable and high-fidelity video generation in real-world applications."
        },
        {
            "title": "References",
            "content": "[1] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [3] J. Carreira and A. Zisserman. Quo vadis, action recognition? new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 62996308, 2017. [4] H. Chefer, S. Zada, R. Paiss, A. Ephrat, O. Tov, M. Rubinstein, L. Wolf, T. Dekel, T. Michaeli, and I. Mosseri. Still-moving: Customized video generation without customized video data. ACM Transactions on Graphics (TOG), 43(6):111, 2024. [5] H. Chen, X. Wang, Y. Zhang, Y. Zhou, Z. Zhang, S. Tang, and W. Zhu. Disenstudio: Customized multi-subject text-to-video generation with disentangled spatial control. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 36373646, 2024. [6] T.-S. Chen, A. Siarohin, W. Menapace, E. Deyneka, H.-w. Chao, B. E. Jeon, Y. Fang, H.-Y. Lee, J. Ren, M.-H. Yang, et al. Panda-70m: Captioning 70m videos with multiple crossmodality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. [7] T.-S. Chen, A. Siarohin, W. Menapace, Y. Fang, K. S. Lee, I. Skorokhodov, K. Aberman, J.-Y. Zhu, M.-H. Yang, and S. Tulyakov. Multi-subject open-set personalization in video generation. arXiv preprint arXiv:2501.06187, 2025. [8] J. Deng, J. Guo, N. Xue, and S. Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2019. [9] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 10 [10] Z. Fei, D. Li, D. Qiu, J. Wang, Y. Dou, R. Wang, J. Xu, M. Fan, G. Chen, Y. Li, et al. Skyreels-a2: Compose anything in video diffusion transformers. arXiv preprint arXiv:2504.02436, 2025. [11] Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [12] Hailuo. Hailuo. https://hailuoai.video/, 2025. [13] X. He, Q. Liu, S. Qian, X. Wang, T. Hu, K. Cao, K. Yan, and J. Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. [14] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [15] T. Hu, J. Zhang, R. Yi, Y. Wang, H. Huang, J. Weng, Y. Wang, and L. Ma. Motionmaster: Training-free camera motion transfer for video generation. arXiv preprint arXiv:2404.15789, 2024. [16] T. Hu, Z. Yu, Z. Zhou, S. Liang, Y. Zhou, Q. Lin, and Q. Lu. Hunyuancustom: multimodaldriven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025. [17] Y. Huang, Z. Yuan, Q. Liu, Q. Wang, X. Wang, R. Zhang, P. Wan, D. Zhang, and K. Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. [18] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [19] Y. Jiang, T. Wu, S. Yang, C. Si, D. Lin, Y. Qiao, C. C. Loy, and Z. Liu. Videobooth: Diffusionbased video generation with image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66896700, 2024. [20] Z. Jiang, Z. Han, C. Mao, J. Zhang, Y. Pan, and Y. Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. [21] Keling. Keling. https://klingai.com/cn/, 2025. [22] R. Khanam and M. Hussain. Yolov11: An overview of the key architectural enhancements. arXiv preprint arXiv:2410.17725, 2024. [23] W. Kong, Q. Tian, Z. Zhang, R. Min, Z. Dai, J. Zhou, J. Xiong, X. Li, B. Wu, J. Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [24] X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, and J. Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. [25] S. Liang, Z. Yu, Z. Zhou, T. Hu, H. Wang, Y. Chen, Q. Lin, Y. Zhou, X. Li, Q. Lu, and Z. Chen. Omniv2v: Versatile video generation and editing via dynamic content manipulation, 2025. URL https://arxiv.org/abs/2506.01801. [26] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [27] L. Liu, T. Ma, B. Li, Z. Chen, J. Liu, Q. He, and X. Wu. Phantom: Subject-consistent video generation via cross-modal alignment. arXiv preprint arXiv:2502.11079, 2025. [28] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. 11 [29] Y. Liu, K. Zhang, Y. Li, Z. Yan, C. Gao, R. Chen, Z. Yuan, Y. Huang, H. Sun, J. Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [30] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [31] W. Peebles and S. Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [32] Pika. Pika. https://pika.art/, 2025. [33] A. Polyak, A. Zohar, A. Brown, A. Tjandra, A. Sinha, A. Lee, A. Vyas, B. Shi, C.-Y. Ma, C.-Y. Chuang, D. Yan, D. Choudhary, D. Wang, G. Sethi, G. Pang, H. Ma, et al. Movie gen: cast of media foundation models, 2025. URL https://arxiv.org/abs/2410.13720. [34] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [35] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr, R. Rädle, C. Rolland, L. Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [36] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [37] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [38] Z. Tan, S. Liu, X. Yang, Q. Xue, and X. Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 3, 2024. [39] Vidu. Vidu. https://www.vidu.cn/, 2025. [40] A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, J. Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [41] Q. Wang, Y. Shi, J. Ou, R. Chen, K. Lin, J. Wang, B. Jiang, H. Yang, M. Zheng, X. Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. arXiv preprint arXiv:2410.08260, 2024. [42] Z. Wang, A. Li, L. Zhu, Y. Guo, Q. Dou, and Z. Li. Customvideo: Customizing text-to-video generation with multiple subjects. arXiv preprint arXiv:2401.09962, 2024. [43] T. Wu, Y. Zhang, X. Wang, X. Zhou, G. Zheng, Z. Qi, Y. Shan, and X. Li. Customcrafter: Customized video generation with preserving motion and concept composition abilities. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 84698477, 2025. [44] B. Xiao, H. Wu, W. Xu, X. Dai, H. Hu, Y. Lu, M. Zeng, C. Liu, and L. Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48184829, 2024. [45] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [46] H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. 12 [47] S. Yuan, J. Huang, X. He, Y. Ge, Y. Shi, L. Chen, J. Luo, and L. Yuan. Identity-preserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. [48] Y. Zhou, Q. Wang, Y. Cai, and H. Yang. Allegro: Open the black box of commercial-level video generation model. arXiv preprint arXiv:2410.15458, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Overview In this supplementary material, we offer further details on implementation, present additional experimental results, and provide more comprehensive analyses, structured as follows: Implementation details (Sec. A.2); Multi-modal data curation (Sec. A.3); More multi-subject comparison results (Sec. A.4). More visualization results (Sec. A.5) Limitations and societal impacts (Sec. A.6) A.2 Implementation details Progressive training process. To enhance the efficiency of the training process, we divide it into two distinct stages. The first stage focuses on modeling the identity preservation capability, while the second stage targets the modeling of interaction generation. During the initial stage, the model is trained on single-subject data, concentrating solely on learning the target identity information without the complexity of interactions among multiple subjects. This stage involves 5,000 iterations. Once the model has effectively learned identity preservation, we proceed to the second stage, where the model is trained with multiple subjects as inputs. Here, the objective is to learn the interactions between the given subject images while maintaining their identities. This stage also comprises 5,000 iterations. Additionally, due to the extensive number of parameters in the pretrained HunyuanVideo model [23], each training iteration is time-consuming. To address this, in each stage, we initially train the model at reduced sizes for 1,000 iterations (included in the total 5,000 iterations), allowing the model to efficiently grasp the target objectives in less time. Subsequently, for the remaining iterations, we revert to the standard resolution to ensure the quality of the final output. All training processes are conducted on 256 GPUs, each with more than 80GB of memory, using batch size of 256. Evaluation Metrics. To comprehensively assess the performance of video customization, we adopt several metrics focusing on identity preservation, text-video alignment, and overall video quality: Identity Similarity. We utilize Arcface [8] to extract facial embeddings from both the reference image and each frame of the generated video, and then compute the average cosine similarity to evaluate how well the identity is preserved. Subject Similarity. Each frame is segmented using YOLOv11 [22] to isolate the subject, after which DINO-v2 [30] features are extracted. The similarity between these features and those from the reference is calculated to assess subject consistency. Text-Video Alignment. We employ CLIP-B and CLIP-L [34] to measure the correspondence between the provided text prompt and the generated video, evaluating how accurately the video reflects the textual description. Fréchet Video Distance (FVD). To assess the quality and diversity of the generated videos, we compute the FVD between generated and real videos. Video features are extracted using I3D [3], and the Fréchet Distance is then calculated. Temporal Consistency. Following the approach in VBench [18], we use the CLIP model to compute the similarity between each frame and its adjacent frames, as well as between each frame and the first frame, to evaluate the temporal coherence of the video. Test Dataset. We manually collected 100 images of various objects, covering wide range of categories such as man-made machines, food, goods, and buildings. In addition, we generated 100 human images using an image generation model. These images were then randomly paired to form 100 image pairs. For each pair, we utilized QWen2.5-VL [1] to generate corresponding interaction text prompts. Figure 5: Examples of the test set, which contains images from diverse categories, such as human, animal, man-made machine, food, goods, and building. A.3 Multi-subject data curation In the main paper, we have illustrated the MLLM-based Subject Segmentation stage and the Cliquebased Subject Consolidation. In this section, we give more details for the preprocessing process, including the data source, data filtering and video captioning. We curate large set of high-quality data from open-source datasets, including Panda-70M [6] and Koala-36M [41], as well as our own collected data. Initially, we split the videos by dividing long 15 Figure 6: Comparison on human-object customization. Figure 7: Comparison on human-human and animal-animal customizations. videos into shorter clips. We then perform black border detection, subtitle detection, watermark detection, transition detection, and motion detection on these clips. Videos with black borders are cropped, and those with subtitles, watermarks, transitions, or low motion are removed. Further, we utilize Koala-36M [41] to filter out videos with scores below certain threshold. We then perform structured video captioning on the remaining videos, generating long captions, short captions, and descriptions of background, style, and camera movement for each video. This structured combination is used during training to enhance caption diversity. 16 Figure 8: Comparison on three-subject customization. A.4 More multi-subject comparison results Human-object customization. The ability to generate videos depicting human-object interactions is crucial, with broad applications in fields such as film production and advertising. We present qualitative results of human-object interaction in Fig. 6, where our method is compared against several state-of-the-art approaches, including Pika [32], Keling1.6 [21], Vidu2.0 [39], Skyreels A2 [10], and VACE 1.3B [20]. As shown, Pika, Vidu, and Keling often focus primarily on the object, resulting in the human face disappearing from the generated frames. Skyreels A2, on the other hand, struggles with producing smooth transitions between frames, leading to lower overall video quality. VACE sometimes fails to capture the intended interaction between the human and the object (left example), and occasionally does not preserve the appearance of the specified object (right example). In contrast, our model consistently maintains strong subject consistency for both the human and the object, while also generating natural and coherent interaction motions between them. Human-human & animal-animal customization. We further provide comparative results for human-human and animal-animal customization tasks in Fig. 7. It can be observed that Pika still suffers from incorrect attention, focusing on hands rather than preserving human identities, and introduces artifacts such as generating three giraffes in the right example. Keling copies lighting from the background of the man image, which contradicts the prompt road, and fails to capture the full body of the giraffe, focusing only on the head without depicting the fighting action specified in the prompt, indicating limited prompt adherence. Skyreels A2 fails to represent both subjects and exhibits poor identity preservation. VACE alters the generated human identities and does not follow the fighting prompt in the giraffe example. While Vidu demonstrates relatively better performance, its identity preservation remains suboptimal. In comparison, our model achieves the best identity consistency and prompt adherence, demonstrating superior capability in customized video generation. Three-subject customization. Our model is not limited to two-subject customization. We present additional comparison results for three-subject video customization in Fig. 8. As shown, Pika, Vidu, Skyreels A2, and VACE all exhibit significant identity loss. While Pika and Vidu are able to generate correct interactions that adhere to physical rules, Skyreels A2 and VACE produce unrealistic frames in which the person and tiger appear pasted into the sky, violating physical plausibility. Keling maintains relatively good identity preservation, but there is still room for improvement. In contrast, our model achieves the best identity preservation and is able to generate realistic interactions among multiple subjects while following physical rules, demonstrating our superior capability in multi-subject customization. 17 Figure 9: More results on multi-subject customization. A.5 More visualization results In this section, we present additional visualization results of our model, covering wide range of subject customization scenarios, including human-object interaction, human-scene interaction, humananimal interaction, and animal-animal interaction. We also provide more examples of three-subject customization. The two-subject customization results are shown in Fig. 9. It can be observed that our model is capable of generating natural and realistic interactions between various types of inputs, demonstrating its potential effectiveness in applications such as advertising and movie production. Furthermore, 18 Figure 10: More results on three-subject customization. beyond object interactions, our model can also generate specified subjects within assigned scenes, which is particularly useful for personalized content creation and other creative industries. Next, we showcase more results of three-subject customization in Fig. 10, featuring diverse combinations such as human-animal-animal, human-object-animal, human-animal-scene, and human-objectobject. These results illustrate that our model can effectively handle different combinations of inputs and generate complex interactions among multiple subjects, all while maintaining strong identity preservation. This demonstrates the superior capability of our model in customized video generation for multi-subject scenarios. A.6 Limitations and societal impacts Limitations. Despite the significant advancements introduced by PolyVivid in multi-subject video customization, several limitations remain. First, the quality and controllability of the generated videos are still constrained by the capabilities of the underlying video generation backbone and the pre-trained MLLM and VAE models. Second, while our framework demonstrates strong performance on variety of subject types and interactions, it may encounter difficulties when handling highly complex scenes involving numerous subjects, intricate backgrounds, or fine-grained interactions that require detailed physical reasoning. Finally, although our MLLM-based data curation pipeline 19 improves subject discriminability, it may still be susceptible to errors in grounding or segmentation, especially in cases of occlusion or ambiguous visual cues, potentially affecting the accuracy of subject alignment and interaction modeling. Societal Impacts. PolyVivid enables more flexible and controllable video generation, which can benefit wide range of applications, including creative content production, personalized education, digital marketing, and virtual reality experiences. By allowing users to customize videos with specific subjects and interactions, our framework empowers artists, educators, and businesses to efficiently create tailored visual content."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "Tencent Hunyuan",
        "Zhejiang University"
    ]
}