{
    "paper_title": "MoM: Linear Sequence Modeling with Mixture-of-Memories",
    "authors": [
        "Jusen Du",
        "Weigao Sun",
        "Disen Lan",
        "Jiaxi Hu",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain's ability to maintain robust long-term memory while mitigating \"memory interference\", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE."
        },
        {
            "title": "Start",
            "content": "MoM: Linear Sequence Modeling with Mixture-of-Memories Jusen Du1,2, Weigao Sun1(cid:66), Disen Lan1,3, Jiaxi Hu4, Yu Cheng5(cid:66) 1Shanghai AI Laboratory, 2Nanjing University, 3South China University of Technology, 4The Hong Kong University of Science and Technology (Guangzhou), 5The Chinese University of Hong Kong \" The enemy of memory isnt time; its other memories.\" David Eagleman"
        },
        {
            "title": "Abstract",
            "content": "Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brains ability to maintain robust long-term memory while mitigating \"memory interference\", we introduce novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linearcomplexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recallintensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github. com/OpenSparseLLMs/MoM and is also released as part of https://github.com/ OpenSparseLLMs/Linear-MoE."
        },
        {
            "title": "Introduction",
            "content": "Attention mechanisms have made significant contributions to the field of artificial intelligence, advancing various modalities such as language, viinterns at Shanghai AI Laboratory; (cid:66) corresponding authors (sunweigao@outlook.com, chengyu@cse.cuhk.edu.hk). sion, audio, video, graphs, and even time series (Achiam et al., 2023; Team, 2023). The Transformer (Vaswani, 2017), known for its ability to capture long-range dependencies, has become foundational architecture in this space. However, traditional Transformers encounter computational challenges due to their quadratic time complexity, O(n2), with respect to sequence length n, making it difficult to scale to long sequences. To overcome this limitation, several linear sequence modeling methods have been proposed, including linear attention (Katharopoulos et al., 2020; Qin et al., 2023a; Li et al., 2025), state space modeling (Gu and Dao, 2024; Dao and Gu, 2024), and linear RNNs (Peng et al., 2024; Qin et al., 2024d), which offer O(n) training complexity and O(1) inference complexity. These approaches often reduce the input sequence to fixed-size hidden space, collapsing the information into single \"memory state\". While these methods enhance efficiency, they face two main challenges: limited memory capacity and memory interference. When new information overwrites the single fixed-size memory state, previously stored representations may degrade, which negatively impacts its long-term memory performance on recall-intensive tasks. We argue that the strong performance of Transformer models on recall-intensive tasks arises from their ability to avoid \"memory interference\" by maintaining independent key-value caches for each token, thus offering virtually unlimited memory capacity. In contrast, linear sequence modeling relies on extreme compression, consolidating all the input information into single fixed-size memory state (Katharopoulos et al., 2020; Dao and Gu, 2024). This approach results in limited memory capacity and inherently leads to memory interference issues. Interestingly, the human brain has developed mechanisms that enable large memory capacity while reducing memory interference. Neuroscience studies show that in the hippocampus, theta oscilla5 2 0 2 9 ] . [ 1 5 8 6 3 1 . 2 0 5 2 : r tions (48 Hz) and gamma oscillations (30100 Hz) work together to support neural coding mechanism for multi-item memory (Buzsáki, 2002; Lisman and Jensen, 2013). Specifically, each theta cycle is subdivided into multiple gamma subcycles, and within each gamma subcycle, distinct group of neurons is activated following the \"E%- max\" mechanism (de Almeida et al., 2009). This sequential activation temporally separates different memory items, thus preventing interference. Inspired by these biological insights, we propose new architecture called Mixture-of-Memories (MoM), which aims to strike balance between the explicit token representations in Transformers and the extreme compression found in earlier linear sequence modeling methods. MoM employs multiple independent memory states, with router network that directs input tokens to specific memory states. The input sequence is divided into predefined number of subsequences (phase-specific neural assemblies), which are processed in parallel and fed into the corresponding memory projections (dentate microcircuits) to generate key-value pairs. As the linear sequence modeling layer processes each subsequence using an RNN-like update mechanism, it produces multiple memory states that capture different aspects of the input sequence. The final output is computed as weighted sum of these memories, which we refer to as the mixture-ofmemories. This approach expands memory capacity and eliminates memory interference, enabling MoM to significantly outperform existing linear sequence models that rely on single fixed-size memory state. Our contributions can be summarized as follows: We present MoM, neuroscience-inspired architecture that incorporates multiple independent memory states, significantly enhancing memory capacity and eliminating memory interference, while retaining the efficiency benefits of linear-time training and constantmemory inference. Distinct with existing gating mechanisms, MoM is new paradigm to reduce memory interference by separating the memory states. The overall design of MoM is compatible with existing linear sequence modeling methods, making it straightforward and effective approach to boost task performance. Through empirical evaluation, we show that MoM outperforms state-of-the-art linear sequence modeling techniques across variety of language tasks, particularly on recallintensive tasks. MoM even achieves performance on par with Transformer models, feat that current linear sequence modeling methods struggle to match."
        },
        {
            "title": "2 Preliminary",
            "content": "For notations in this work, we use bold lower-case letters for row vectors (e.g., qt, kt), bold upper-case letters for matrices (e.g., Q, K) and the identical letters represent row in the matrix, e.g., qt is the t-th row of Q. Softmax Attention Given input = [x1, ..., xn]T Rnd,where is the sequence length and xt Rd is the t-th input vector with dimensions. Transformer (Vaswani, 2017) softmax attention computes: = XWQ, = XWK, = XWV , (1) = softmax( QKT A)V , (2) where Q, K, Rnd, Rdd is an attention mask, denotes element-wise production. The training time complexity is O(n2) due to the QKT operation. During inference, Transformers store all the k, calculated as \"KV cache\" which can be viewed as memory states. As the KV cache grows with the input, the Transformers can be considered to separate the memory storage for each input token individually and have an unlimited memory capacity. This also results in time complexity of O(n) per token during inference."
        },
        {
            "title": "Linear Attention",
            "content": "To reduce the time complexity of Transformer attention, various optimization techniques have been proposed. Linear Transformers (Katharopoulos et al., 2020) replace the softmax attention mechanism with dot-product of feature maps ϕ(): ot = (cid:80)n i=1 ϕ(qt)ϕ(ki)T vi i=1 ϕ(qt)ϕ(ki)T , (cid:80)n (3) where qt, kt, vt Rd. While the presence of the denominator may lead to numerical instability (Qin et al., 2024b) and the feature map can utilize an identity function, which we omit for simplicity. In losssuch as introducing gating mechanisms and employing more precise control over memory modifications (Orvieto et al., 2023; De et al., 2024; Beck et al., 2024; Yang et al., 2023; Zhang et al., 2024)some degradation in this compression process is inevitable. Expanding the memory capacity has been shown to mitigate this issue to some extent, with studies indicating that increasing memory capacity can enhance model performance (Qin et al., 2024d; Peng et al., 2024). However, previous approaches that simply increased the size of the RNN state, essentially expanding single memory state, struggled to capture the full spectrum of information within an entire sequence. We propose that this difficulty arises because sequence information is often multifaceted, and single, expanded memory may not be capable of simultaneously capturing multiple aspects of the data. Inputs that introduce new or orthogonal information may interfere with existing memory content when using shared memory. Rather than discarding these inputs through gating mechanisms or overwriting the existing memory state, it may be more effective to consider alternative strategies that allow for the preservation of diverse information without interference."
        },
        {
            "title": "3.2 MoM: Mixture-of-Memories",
            "content": "To address the challenge outlined above, we propose novel approach inspired by biological mechanisms for encoding multi-item memory such as theta-gamma oscillations (Lisman and Jensen, 2013), and concepts from Mixture-of-Experts (MoE) (Shazeer et al., 2017), where different experts handle specific tokens. In this approach, we leverage multiple memory states, each of which is selectively updated by different inputs. This increases the memory capacity and enables the model to retain diverse pieces of information by storing various types of inputs in separate memory states. In our framework, the memory states function similarly to the experts in MoE. However, instead of relying on completely separate networks, these modules are individual RNN states embedded within linear recurrent mechanism. This design allows for the isolation of memory updates while concurrently managing distinct types of information. It is important to note that MoM fundamentally differs from traditional MoE, as we will discuss in Appendix A. Figure 1 provides an overview of the MoM architecture. Below, we introduce the structure of the MoM layer and explain how this Figure 1: Framework of MoM. Each input token selectively activates and updates memory states, leaving non-activated memory states unchanged to avoid interference from current input. Additionally, we introduce continuously activated shared memory. This figure presents the basic memory update mechanism; other mechanisms involving gating or more complex updates follow similar approach. perspective of memory, the formulation can also be written in recurrent format: Mt = Mt1 + kT vt, ot = qtMt, (4) This indicates that linear attention can function as linear recurrent layer with matrix-valued hidden state which we refer to as memory sate and the output is generated by querying the memory state . This represents the ultimate compression of sequence information, condensing the entire sequence into single memory state. Building on the foundational concepts of linear attention and memory perspective, some recent advancements have focused on optimizing memory structure, including gated updates (Yang et al., 2023; Qin et al., 2024e,d) and memory capacity expansion (Peng et al., 2024; Qin et al., 2024d)."
        },
        {
            "title": "3.1 Motivation",
            "content": "Linear sequence models compress the entire sequence data into fixed-size memory state. Despite numerous efforts to minimize information biologically-inspired architecture is implemented in the context of linear sequence modeling."
        },
        {
            "title": "3.2.1 Router Network\nWe use a router to assign inputs to different mem-\nory states. Utilizing the top-k concept, each token\nis routed to the top-k memories based on its impor-\ntance scores. Specifically, we use a simple linear\nlayer to generate these scores for each input token.\nAfter applying a softmax function, we select the\ntop-k scores and normalize them.",
            "content": "scorest = TopK(softmax(xtWg)) Rk, (5) gt = Rk, scorest (cid:80) scorest where xt Rd, is the top-k number, Wg RdM is learnable weight, gt is the normalized importance scores of the input xt. (6)"
        },
        {
            "title": "3.2.2 Linear Recurrent Memory Module\nAfter the router network, the input xt is directed\nto top-k linear recurrent modules, meaning that the\ntop-k memories are activated while the others re-\nmain inactive. For each activated memory module,\nindexed by m, we perform the following operation:",
            "content": "1. Key and Value Projections: We project the input xt to km and vm using and : Method Memory Update Rule Mt = Mt1 + kT vt LA Lightning Mt = γMt1 + kT Mt = γMt1 + kT RetNet Mt = (aT HGRN2 Mt = (aT GLA Mt = αtMt1 + βtkT Mamba2 Mt = (I kT DeltaNet G-DeltaNet Mt = αt(I kT TTT Titan vt vt 1)Mt1 + (1 at)T vt 1)Mt1 + kT vt kt)Mt1 + βtkT vt kt)Mt1 + βtkT Mt = Mt1 + βtl(Mt1; kt, vt) Mt = αtMt1 + βtl(Mt1; kt, vt) vt vt Table 1: Memory Update Rules. We demonstrate that several current linear sequence models can be viewed as recurrent models in terms of memory updates, where αt, βt (0, 1) are data-dependent scaler, at is datadependent vector, and γ is data-independent constant. Memory Mixing. After updating the activated memory states, we perform weighted sum of these memory states using the importance scores obtained from Equation(6). Mt = (cid:88) g(m) m Rdd, (10) where Mm is one activated memory and g(m) the importance score of Mm. is We then obtain the output of the MoM by applying query vector qt to the mixed memory Mt: = xtW km , vm = xtW Rd, (7) ot = qt Mt Rd, (11) where weights for kv of the m-th memory module. are learnable projection , 2. Memory Update: We update the activated memory state using km , vm : Finally, the output of the MoM layer is computed by applying an activation function, normalization, and linear transformation:"
        },
        {
            "title": "M m",
            "content": "t = t1 + (km )T vm Rdd, ot = RMSNorm(Swish(ot))Wo Rd, (12) (8) The equation above represents the simplest form of memory update for clarity. Our approach is flexible and does not rely on specific memory update mechanism. To enhance performance, we can incorporate mechanisms such as forget gates (Sun et al., 2023):"
        },
        {
            "title": "M m",
            "content": "t = γM t1 + (km )T vm Rdd, (9) where γ is constant forget gate. More generally, our method can be adapted to incorporate various memory update methods proposed in previous work. Detailed descriptions of these methods are provided in Table 1. Throughout the recurrent process, only subset of memory states is activated and updated at each time step, while memory states that are not routed remain inactive and unchanged. When the input passes through the key-value projection layer, it generates multiple sets of keys and values that are fed into different memory modules. This design enables the model to maintain multiple memory states, each preserving distinct pieces of information. By aggregating the activated memories into comprehensive mixed memory by weighted summation, the query can effectively retrieve information from this mixed memory, which results the \"attention output\" followed by other layers. Scale Model FDA SWDE SQUAD NQ TriviaQA Drop Avg. 340M Params 15B Tokens L=24, d=1024 1.3B Params 100B Tokens L=24, d=2048 Transformer++ RetNet HGRN2 GLA GSA Gated DeltaNet MoM Transformer++ RetNet HGRN2 GLA GSA Gated DeltaNet MoM 46.14 5.90 11.53 11.26 6.36 20.53 30.79 44.32 13.62 12.35 27.61 23.25 30.25 41.14 25.87 9.28 17.34 16.78 16.87 23.24 26.05 32.43 22.59 23.24 30.93 32.80 27.65 34.30 33.22 22.41 24.08 27.85 21.90 28.55 29. 42.59 33.46 33.19 35.04 35.57 34.06 37.08 18.94 6.91 12.67 12.77 14.60 14.98 13.84 24.49 15.43 19.10 22.27 22.96 23.22 24.11 45.97 40.05 43.84 43.90 42.18 44.91 44.79 58.47 53.79 55.27 56.28 57.05 58.23 58.59 20.03 18.59 17.35 17.68 16.72 16.48 20. 21.56 19.79 19.65 19.45 20.65 20.36 21.03 31.70 17.19 21.14 21.71 19.77 24.78 27.59 37.31 26.45 27.13 31.93 32.05 32.30 36.04 Table 2: Results on Recall-Intensive Tasks. All inputs are truncated to maximum length of 2K tokens. MoM significantly outperforms all other linear models across both model sizes. In the 1.3B model, MoM even achieves performance very close to that of Transformer models."
        },
        {
            "title": "4 Experiments",
            "content": "Shared Memory. To enhance our models ability to capture long-term dependencies, we introduce shared memory mechanism. This shared memory has access to the entire sequence information, allowing it to effectively store and retrieve longterm information. By integrating shared memory into our model, we ensure that it can leverage the complete historical context, resulting in significant improvements in performance and robustness. = xtW ks = Mt = , vs t1 + (ks (cid:88) + = xtW )T vs g(m) v Rd Rdd Rdd (13) (14) (15) Hardware-efficient Implementation. Chunkwise parallel form of linear attention is computational optimization strategy that divides the input sequence into smaller chunks to enable partial parallel and recursive computation during model training (Hua et al., 2022; Yang et al., 2023), which is well supported in open-source frameworks. In the implementation of MoM, mixing the memories before multiplying by the query is mathematically equivalent to firstly multiplying each memory by the query and then mixing the results. This property allows us to reuse the efficient Triton-based operator implementation of all previous linear sequence modeling methods, facilitate the hardwareefficient training as well as inference of MoM. Models marked with an asterisk use open-source pretrained weights with identical training configurations."
        },
        {
            "title": "4.1 Experimental Setups",
            "content": "Models. In our experiments, we employ the Gated DeltaNet (Yang et al., 2024) as the memory update mechanism in MoM. The model is configured with four memory states, two of which are activated at each time step, along with shared memory. Baselines. We evaluate MoM against several linear recurrent models and Transformers, including RetNet (Sun et al., 2023), GLA (Yang et al., 2023), Gated DeltaNet (Yang et al., 2024), and Transformer++ (Touvron et al., 2023), which incorporates Rotary Position Embeddings (Su et al., 2024) and GLU (Shazeer, 2020) into the Transformer architecture. To ensure fair comparison, we train all baseline models from scratch using the exact same number of tokens. Training. We follow the training procedure described by Yang et al. (2023), utilizing the SlimPajama dataset (Soboleva et al., 2023) sampled with 100B tokens and tokenized using the Mistral tokenizer (Jiang et al., 2023). We train models from scratch with parameter sizes of 340M and 1.3B, respectively. For the 340M models, we train on 15B tokens with batch size of 0.5M tokens. The warmup tokens count is set to 0.25M. We set the hidden ratio of our model to 3 to keep the overall parameter count approximately the same. For the 1.3B models, we train on 100B tokens with batch size of 2M tokens. We utilized publicly available pretrained weights from Zhang et al. (2024) with exactly same configuration. The warmup tokens count is 1B. We employ AdamW optimizer Scale Model Wiki. ppl Lamb. ppl ARC-e acc ARC-c accn Hella. accn Lamb. acc PIQA acc Wino. acc 340M Params 15B Tokens L=24, d= 1.3B Params 100B Tokens L=24, d=2048 Transformer++ RetNet HGRN2 GLA GSA Gated DeltaNet MoM Transformer++ RetNet HGRN2 GLA GSA Gated DeltaNet MoM 26.88 31.07 27.90 28.78 28.17 26.47 26.00 17.61 18.18 17.32 17.61 16.69 17.14 16.64 76.46 87.11 77.40 79.95 82.50 58.59 51. 19.29 21.97 15.65 19.66 16.02 18.80 14.83 44.91 44.49 45.24 44.53 45.50 46.04 46.13 55.01 57.49 58.33 55.18 58.33 56.82 55.35 25.94 23.04 23.63 22.27 24.23 23.55 24.15 28.07 26.88 28.07 27.56 28.33 27.39 27.99 34.95 33.86 35.61 34.84 35.00 35.18 35. 49.21 48.09 51.93 48.89 50.98 49.77 50.95 26.90 23.93 24.74 24.94 24.02 27.01 28.26 40.95 37.75 42.31 40.03 42.03 39.94 43.43 64.31 63.49 65.45 63.93 64.85 66.05 65.61 70.08 69.37 71.33 69.86 72.25 71.76 71.27 51.07 52.33 54.06 51.38 50.43 50.83 52. 56.27 53.28 52.01 53.91 53.43 51.78 56.83 Avg. 41.35 40.19 41.46 40.32 40.67 41.44 42.11 49.93 48.81 50.66 49.24 50.89 49.58 50.97 Table 3: Results on Common-Sense Reasoning Tasks. The performance of linear models and Transformer models is comparable; however, MoM consistently achieves the best average performance across all model sizes. (Loshchilov et al., 2017; Sun et al., 2024b) with learning rate of 3e-4 with cosine learning rate schedule (Zhou et al., 2020). The weight decay is set to 0.01 and gradient clipping is 1.0."
        },
        {
            "title": "4.2.1 Recall-intensive Tasks",
            "content": "Linear sequence models, due to their limited memory capacity, often exhibit significant performance gap compared to Transformer models, especially in recall-intensive tasks where extensive context is crucial. These tasks highlight notable performance differences among various linear models, making them more accurate benchmark for evaluating linear models capabilities in handling contextual information. To thoroughly assess our models proficiency in such scenarios, we test six recall-intensive tasks following Arora et al. (2024): FDA (Arora et al., 2023), SWDE (Arora et al., 2023; Lockard et al., 2019), SQuAD (Rajpurkar et al., 2018), NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and Drop (Dua et al., 2019). These tasks are designed to challenge models ability to perform context-based retrieval and comprehension. As shown in Table 2, our proposed approach, benefiting from increased memory capacity and memory mixing mechanism, achieves significant improvements over other linear sequence models. Specifically, our model effectively narrows the performance gap with Transformer models. This improvement underscores the advantage of our method in capturing and utilizing long-range dependencies, thereby enhancing performance on tasks that require extensive contextual understanding."
        },
        {
            "title": "4.2.2 Commonsense Reasoning Tasks",
            "content": "As shown in Table 3, we report the language modeling perplexity and zero-shot performance of commonsense reasoning tasks following (Zhang et al., 2024) which includes WikiText (Merity et al., 2016), LAMBADA (Paperno et al., 2016), ARC-easy, ARC-challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), PiQA (Bisk et al., 2020) and WinoGrande (Sakaguchi et al., 2019). The evaluation results are based on the lmevaluation-harness (Gao et al., 2024). Experimental results show that MoM outperforms other linear models and surpassed the Transformer model as well."
        },
        {
            "title": "4.2.3 Long Context Tasks",
            "content": "Assessing performance on long-context tasks is crucial for linear models, as it reflects their ability to handle long-range dependencies effectively. We evaluated our models comprehension of long contexts using the LongBench benchmark (Bai et al., 2024; Contributors, 2023). In Table 4, we present the average results across various categories, including summarization, few-shot learning, synthetic tasks, and code completion, along with the overall mean across all tasks. The complete detailed results are provided in Appendix 7."
        },
        {
            "title": "4.2.4 Mixed Memory vs. Single Memory",
            "content": "To validate the effectiveness of our mixed memory mechanism, we compare our MoM model with mixed memories to baseline model that uses an expanded single memory with the same activated Model Sum FS Syn Code Avg. RetNet HGRN2 GSA Gated DeltaNet MoM 6.30 6.51 7.75 7.14 6.89 15.76 15.50 20.29 18.00 21.26 2.64 2.61 1.92 2.10 2.63 40.52 40.11 42.83 41.52 47.79 13.61 13.02 14.61 13.98 15. Table 4: LongBench Results. All evaluations were done using Contributors (2023). Note: Sum = Summarization, FS = Few-shot, Syn = Synthetic. memory capacity. We adopt the same memory update method as existing linear models and extend it within our MoM framework. For comparison, we adjust the dimensions of projected and in the original model to match the total size of all activated memories in the MoM model. Note that while our MoM model activates multiple memories during retrieval, the mixing process effectively consolidates these into memory capacity equivalent to single memory. In contrast, the expanded single memory approach increases the entire retrieval capacity to encompass the size of multiple activated memories. We evaluate its performance on the recall-intensive tasks in Table 5. The experimental results demonstrated that using multiple mixed memories leads to greater improvement than simply expanding the capacity of single memory. This confirms that mixed memory can effectively reduce interference from different inputs. Assigning inputs specifically to different memories, combined with the use of forget gate, proves to be more effective approach for reducing interference than relying solely on forget gate."
        },
        {
            "title": "4.2.6 Training Loss Comparison\nTo further assess the learning efficiency of MoM,\nwe compared the training loss curves of MoM with\nthose of other baseline models. As depicted in\nFigure 3, MoM consistently maintains the lowest\nloss throughout the entire training phase. Even",
            "content": "Figure 2: Efficiency of MoM. We demonstrate the inference time and GPU memory consumption required to generate 1K tokens at specific sequence lengths. as training nears convergence, MoM continues to exhibit clear advantage over other methods. Figure 3: Training Loss. Loss curves for training 340M models on 15B tokens with fixed random seed of 42."
        },
        {
            "title": "4.2.7 Ablation",
            "content": "To evaluate the influence of memory hyperparameters in our MoM model, we conducted ablation studies using 340 million parameter model trained on 15 billion tokens. Our investigation primarily focused on the impact of the number of memories, the number of activations, and the use of shared memory on model performance. The results of these experiments are presented in Table 6."
        },
        {
            "title": "Linear Recurrent Models",
            "content": "Linear recurrent models, comprising linear attention, linear RNNs, state-space models (SSMs), have garnered significant research interests (Qin et al., 2023b). The advancement of SSMs began with the pioneering work on S4 (Gu et al., 2022), which was later optimized through diagonalized version (Gupta et al., 2022). Despite their strong performance on the LRA benchmark, these models have faced challenges in language modeling mainly Model GLA expanded GLA MoM Gated DeltaNet expanded Gated DeltaNet MoM FDA SWDE SQUAD NQ TriviaQA Drop Avg. 15.08 9.90 18.26 28.07 20.15 21. 24.27 29.15 28.28 29.36 30.03 29.73 13.30 14.16 17.74 15.24 41.65 45. 48.34 45.50 18.74 20.89 19.26 20.41 22.87 23.53 26.32 28.02 Table 5: Comparison Between Mixed Memory and Single Memory. We constructed MoM models using different memory update mechanisms. Separate memory segments yielded better performance compared to simply increasing the memory capacity of single memory. Acc Number of Memories and Activations (all cases include shared memory by default) Memories Activated 2 3 4 8 1 2 2 2 Shared Memory w/ shared memory w/o shared memory 25.05 27.31 27.59 25.56 27.59 26.06 Table 6: Ablation Study. We performed ablation studies on the number of memories and the use of shared memory. The table presents the average results across all recall-intensive tasks. because they rely solely on data-independent processes. As research progressed, constant forgetting gates were introduced, helping to alleviate some interference issues by uniformly managing memory decay (Sun et al., 2023; Gu and Dao, 2024). The next breakthrough involved data-dependent forget gates. These allowed models to dynamically adjust memory updates based on the input data, significantly enhancing performance across various tasks (Qin et al., 2024c,d; Yang et al., 2023; Zhang et al., 2024; Yang et al., 2024; Qin et al., 2024a). Sequence parallelism techniques are well adapted on linear recurrent models (Sun et al., 2024a, 2025) for efficient long context training. There have also been recent advancements in scaling law and testtime regression optimization (Shen et al., 2024; Sun et al., 2024c; Behrouz et al., 2024). Building on these advancements, our MoM model incorporates data-dependent mechanisms that selectively update memory. By efficiently managing interference through tailored memory updates and leveraging increased memory capacity, MoM represents further evolution, improving model expressiveness and performance. Mixture-of-Experts Mixture-of-Experts (MoE) is technique designed to enhance the capacity of deep neural networks while maintaining computational efficiency (Fedus et al., 2022; Rajbhandari et al., 2020; Lepikhin et al., 2020; Tang et al., 2023; Zhu et al., 2024; Qu et al., 2024). MoE achieves this by activating subset of parameters, known as \"experts\", for each input, which reduces the computational costs. Shazeer first integrated MoE into LSTM layers (Shazeer et al., 2017). The Switch Transformer (Fedus et al., 2022) refined this approach by simplifying the gating mechanism to select only one expert per input. Gshard (Lepikhin et al., 2020) further advanced this by using top-2 expert routing strategy to improve performance. Recent MoE models, such as Deepseek-MoE (Dai et al., 2024), introduce shared experts to capture and consolidate common knowledge across different contexts, while designing fine-grained experts to increase combinatorial flexibility."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose Mixture-of-Memories (MoM), novel architecture that enhances memory capacity and eliminates memory interference, inspired by the brains mechanisms for long-term memory retention. By leveraging multiple independent memory states, MoM significantly improves performance on recall-intensive tasks while maintaining the efficiency advantages of linear-time training and constant-memory inference. Instead of simply discarding tokens as done in gating mechanisms, our memory separation paradigm provides more effective way to preserve sequence information. Our experimental results demonstrate that MoM outperforms existing linear sequence modeling methods, particularly on tasks requiring strong recall, and achieves performance comparable to Transformer models. This makes MoM promising approach for applications need strong efficiency and recall-intensive performance, paving the way for sparse solutions in sequence modeling."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher Ré. 2024. Simple linear attention language models balance arXiv preprint the recall-throughput arXiv:2402.18668. tradeoff. Simran Arora, Brandon Yang, Sabri Eyuboglu, Avanika Narayan, Andrew Hojel, Immanuel Trummer, and Christopher Ré. 2023. Language models enable simple systems for generating structured views of heterogeneous data lakes. Preprint, arXiv:2304.09433. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. Longbench: bilingual, multitask benchmark for long context understanding. Preprint, arXiv:2308.14508. Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. xlstm: Extended long shortterm memory. arXiv preprint arXiv:2405.04517. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. 2024. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. In ThirtyFourth AAAI Conference on Artificial Intelligence. György Buzsáki. 2002. Theta oscillations in the hippocampus. Neuron, 33(3):325340. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457. Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. 2024. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427. Licurgo de Almeida, Marco Idiart, and John Lisman. 2009. second function of gamma frequency oscillations: an e%-max winner-take-all mechanism selects which cells fire. Journal of Neuroscience, 29(23):74977503. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2024. Mamba: Lineartime sequence modeling with selective state spaces. Preprint, arXiv:2312.00752. Albert Gu, Karan Goel, and Christopher Ré. 2022. Efficiently modeling long sequences with structured state spaces. Preprint, arXiv:2111.00396. Ankit Gupta, Albert Gu, and Jonathan Berant. 2022. Diagonal state spaces are as effective as structured In Advances in Neural Information state spaces. Processing Systems, volume 35, pages 2298222994. Curran Associates, Inc. OpenCompass Contributors. 2023. Opencompass: universal evaluation platform for foundation https://github.com/open-compass/ models. opencompass. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. In In2022. Transformer quality in linear time. ternational conference on machine learning, pages 90999117. PMLR. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. 2024. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Tri Dao and Albert Gu. 2024. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR. Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, et al. 2023a. Transnormerllm: faster and better large language model with improved transnormer. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453 466. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668. Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. 2025. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313. John Lisman and Ole Jensen. 2013. The theta-gamma neural code. Neuron, 77(6):10021016. Colin Lockard, Prashant Shiralkar, and Xin Luna Dong. 2019. OpenCeres: When open information extraction meets the semi-structured web. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 30473056, Minneapolis, Minnesota. Association for Computational Linguistics. Ilya Loshchilov, Frank Hutter, et al. 2017. Fixing weight decay regularization in adam. arXiv preprint arXiv:1711.05101, 5. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. Preprint, arXiv:1609.07843. Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. 2023. Resurrecting recurrent neural netIn International Conworks for long sequences. ference on Machine Learning, pages 2667026698. PMLR. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, et al. 2024. Eagle and finch: Rwkv with matrixvalued states and dynamic recurrence. arXiv preprint arXiv:2404.05892. Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. 2023b. Scaling transnormer to 175 billion parameters. arXiv preprint arXiv:2307.14995. Zhen Qin, Xuyang Shen, Dong Li, Weigao Sun, Stan Birchfield, Richard Hartley, and Yiran Zhong. 2024a. Unlocking the secrets of linear complexity sequence model from unified perspective. arXiv preprint arXiv:2405.17383. Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. 2024b. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658. Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. 2024c. Various lengths, constant speed: Efficient language modeling with lightning attention. arXiv preprint arXiv:2405.17381. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. 2024d. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904. Zhen Qin, Songlin Yang, and Yiran Zhong. 2024e. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36. Xiaoye Qu, Daize Dong, Xuyang Hu, Tong Zhu, Weigao Sun, and Yu Cheng. 2024. Llama-moe v2: Exploring sparsity of llama from perspective of mixture-of-experts with post-training. arXiv preprint arXiv:2411.15708. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 16. IEEE. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for squad. Preprint, arXiv:1806.03822. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Winogrande: An adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641. Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202. Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. 2024. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. 2023. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, et al. 2024. Gated slot attention for efficient linear-time sequence modeling. arXiv preprint arXiv:2409.07146. Beitong Zhou, Jun Liu, Weigao Sun, Ruijuan Chen, Claire Tomlin, and Ye Yuan. 2020. pbsgd: Powered stochastic gradient descent methods for accelIn IJCAI, pages erated non-convex optimization. 32583266. Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. 2024. Llama-moe: Building mixture-of-experts from llama with continual pre-training. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1591315923. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, and Yiran Zhong. 2024. Scaling laws for linear complexity language models. arXiv preprint arXiv:2406.16690. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063. Weigao Sun, Disen Lan, Yiran Zhong, Xiaoye Qu, and Yu Cheng. 2025. Lasp-2: Rethinking sequence parallelism for linear attention and its hybrid. arXiv preprint arXiv:2502.07563. Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Linear atarXiv preprint Yu Qiao, and Yiran Zhong. 2024a. tention sequence parallelism. arXiv:2404.02882. Weigao Sun, Zhen Qin, Weixuan Sun, Shidi Li, Dong Li, Xuyang Shen, Yu Qiao, and Yiran Zhong. 2024b. Co2: Efficient distributed training with full communication-computation overlap. arXiv preprint arXiv:2401.16265. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. 2024c. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621. Xiaqiang Tang, Weigao Sun, Siyuan Hu, Yiyang Sun, and Yafeng Guo. 2023. Ms-net: multi-path sparse model for motion prediction in multi-scenes. IEEE Robotics and Automation Letters. InternLM Team. 2023. Internlm: multilingual language model with progressively enhanced capabilities. Qwen Team. 2024. Qwen1.5-moe: Matching 7b model performance with 1/3 activated parameters\". Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971."
        },
        {
            "title": "A Comparison between MoM and MoE",
            "content": "For the benchmark, we tested on these tasks: While our approach to implementing the Mixtureof-Memories (MoM) draws inspiration from the Mixture-of-Experts (MoE) framework, there are significant differences that distinguish our method from traditional MoE implementations. Purpose: The MoE was introduced to scale up the number of parameters without significantly increasing computational resources. It address the limitations of dense models in scaling both parameters and computational demands through sparse activation. However, MoM is designed to expand the memory capacity of linear attention models while preserving their linear time complexity. By sparsely activating memories and using weighed summation to create mixed memory, MoM effectively address the challenge of forgetting historical information in linear attention. Moreover, by separating the memory into distinct states, MoM reduces interference between different pieces of information. Structure: In conventional MoE, each expert is separate neural network within the feedforward network (FFN) layer (Team, 2024). In contrast, in MoM, each memory is an RNN state with unique key-value projection weights to generate different key-value pairs. MoE operates during the channel mixing phase, where each token is processed independently by selected experts. On the other hand, MoM functions during the token mixing phase, where each memory processes different segments of the sequence, preserving inter-token relationships."
        },
        {
            "title": "B Dataset and Benchmark",
            "content": "We pretrained model on SlimPajama dataset. For 340M models, we train on sample of 15B tokens. For 1.3B models, we train on sample of 100B tokens. SlimPajama (Soboleva et al., 2023) is high-quality, optimized subset of the RedPajama dataset, designed for large-scale language model training. It includes diverse text sources such as Common Crawl, Wikipedia, books, and GitHub code, with primary focus on English. The dataset is cleaned, deduplicated, and optimized for efficiency and performance. WikiText (Merity et al., 2016): dataset consisting of high-quality Wikipedia articles, primarily in English, designed for language modeling tasks with 62 test samples. The text covers wide range of topics, including history, science, and culture, and is authored by Wikipedia contributors, who come from diverse demographic backgrounds. LAMBADA (Paperno et al., 2016): An English-language dataset for evaluating contextual understanding in language models with 5153 test samples. It consists of narrative texts sourced from books, requiring models to predict the final word of passage. The data reflects mix of literary styles and author demographics. ARC-Easy & ARC-Challenge (Clark et al., 2018): set of multiple-choice science questions in English, sourced from standardized exams and educational materials with 2376 and 1172 test samples. The dataset represents the domain of elementary and high school science, with questions authored by educators and test designers. ARC-Easy includes straightforward questions, while ARCChallenge contains more difficult ones that require advanced reasoning. HellaSwag (Zellers et al., 2019): An Englishlanguage dataset designed for commonsense reasoning, where models must choose the most plausible continuation of sentence. The text is derived from activity descriptions (e.g., WikiHow), covering everyday scenarios. The dataset was constructed adversarially to be challenging for language models. It has 10003 test samples. PiQA (Bisk et al., 2020): dataset focused on physical commonsense reasoning in English with 3084 test samples. The text consists of everyday tasks and scenarios, requiring models to determine the most practical way to perform an action. The data is sourced from crowdsourced descriptions, reflecting broad range of common human experiences. WinoGrande (Sakaguchi et al., 2019): large-scale English dataset for commonsense reasoning, based on the Winograd Schema Challenge with 1267 test samples. It tests pronoun resolution in ambiguous contexts, with sentences sourced and refined through crowdsourcing. The dataset aims to reduce annotation biases by diversifying sentence structures and topics. discrete reasoning, such as arithmetic and logical operations, over passages. The dataset is sourced from Wikipedia and challenges models to go beyond simple span extraction by performing multi-step reasoning. It has 2087 test samples. LongBench (Bai et al., 2024) is bilingual (Chinese and English) benchmark for evaluating long-context understanding in large language models. It covers 21 tasks across six categories, including QA, summarization, and code completion with 4750 test samples. All datasets used in this work are publicly available and have been released by their original creators, who are responsible for ensuring privacy protection. These datasets are used in accordance with their respective licenses and intended purposes. No modifications or derivative datasets have been created."
        },
        {
            "title": "C Experiments Details",
            "content": "Our experiments were conducted using 32 NVIDIA A800 GPUs. Training the 340M parameter model required approximately 10 hours, while the 1.3B parameter model took around 6 days. All models were trained and evaluated using fixed random seed of 42 to ensure reproducibility. The reported results are based on single run, without aggregation over multiple random seeds. FDA (Arora et al., 2024, 2023): An Englishlanguage dataset includes 100 randomly sampled PDF files of FDA 510(k) pre-market notification submissions, each with 16 manually annotated attributes like device classification and predicate device codes. It has 1102 test samples. SWDE (Arora et al., 2024, 2023; Lockard et al., 2019): An English dataset dataset is used for evaluating information extraction systems. It comprises data from 8 movie websites (e.g., IMDB, Rotten Tomatoes) and 5 university websites (e.g., US News), with each site containing 1063 to 2000 annotated web pages and 1111 test samples. SQuAD (Rajpurkar et al., 2018): reading comprehension dataset with English questions based on Wikipedia articles with 2984 test samples. Answers are text segments from the articles or the questions can be unanswerable. SQuAD2.0 adds over 50,000 unanswerable questions to the original 100,000 from SQuAD1.1, requiring systems to both answer questions and identify when no answer is supported. NQ (Kwiatkowski et al., 2019): large-scale English dataset for open-domain question answering, where questions are sourced from real Google search queries. Answers are extracted from Wikipedia articles, reflecting general knowledge across various domains. It has 3157 test samples TriviaQA (Joshi et al., 2017): An Englishlanguage question-answering dataset consisting of trivia-style questions with 1688 test samples. The dataset includes both humangenerated questions and web-sourced evidence, covering broad range of topics such as history, science, and entertainment. DROP (Dua et al., 2019): reading comprehension benchmark in English that requires Model SQA MQA Sum FS Syn Code Zh-Avg En-Avg Avg. RetNet HGRN2 GSA Gated DeltaNet MoM 9.23 7.38 8.21 8.52 8.14 7.23 6.02 6.63 6.61 7.11 6.3 6.51 7.75 7.14 6.89 15.76 15.5 20.29 18 21. 2.64 2.61 1.92 2.1 2.63 40.52 40.11 42.83 41.52 47.79 15.44 14.28 15.06 14.19 17.33 13.5 13.12 15.2 14.63 15.71 13.61 13.02 14.61 13.98 15.64 Table 7: Complete Results of LongBench. (SQA: Single-doc QA, MQA: Multi-doc QA, Sum: Summarization, FS: Few-shot learning, Syn: Synthetic) FDA SWDE SQUAD NQ TriviaQA Drop Avg. Number of Memories Memories Activated 2 3 4 8 1 2 2 2 20.53 23.71 30.79 19.71 23.15 29.52 26.05 24.27 29.56 28.69 29.63 30. 13.72 15.55 13.84 15.68 43.96 46.09 44.79 46.15 19.36 20.32 20.41 17.39 25.05 27.31 27.59 25.56 Shared Memory w/ shared memory w/o shared memory 30.79 25.61 26.05 27.09 29.63 28.35 13.84 14.32 44.79 41.94 20.41 19. 27.59 26.06 Table 8: Complete Ablation Study Results. The complete results of the ablation studies on recall-intensive tasks. Wiki. ppl Lamb. ppl ARC-e acc ARC-c accn Hella. accn Lamb. acc PIQA acc Wino. acc Avg. Number of Memories Memories Activated 2 3 4 8 1 2 2 2 26.84 25.86 26.00 26.40 66.26 52.78 51.25 52.30 46.25 45.62 46.13 45. 23.21 23.46 24.15 24.74 35.54 36.25 35.91 35.93 25.40 28.84 28.26 28.39 64.69 65.13 65.61 64.91 51.30 53.67 52.57 49.49 41.07 42.16 42.10 41. Shared Memory w/ shared memory w/o shared memory 26.00 27.74 51.25 85.22 46.13 45.33 24.15 24. 35.91 34.91 28.26 23.52 65.61 64.09 52.57 50.28 42.10 40.38 Table 9: Complete Ablation Study Results. The complete results of the ablation studies on recall-intensive tasks."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Shanghai AI Laboratory",
        "South China University of Technology",
        "The Chinese University of Hong Kong",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}