{
    "paper_title": "PORTool: Tool-Use LLM Training with Rewarded Tree",
    "authors": [
        "Feijie Wu",
        "Weiwu Zhu",
        "Yuxiang Zhang",
        "Soumya Chatterjee",
        "Jiarong Zhu",
        "Fan Mo",
        "Rodin Luo",
        "Jing Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current tool-use large language models (LLMs) are trained on static datasets, enabling them to interact with external tools and perform multi-step, tool-integrated reasoning, which produces tool-call trajectories. However, these models imitate how a query is resolved in a generic tool-call routine, thereby failing to explore possible solutions and demonstrating limited performance in an evolved, dynamic tool-call environment. In this work, we propose PORTool, a reinforcement learning (RL) method that encourages a tool-use LLM to explore various trajectories yielding the correct answer. Specifically, this method starts with generating multiple rollouts for a given query, and some of them share the first few tool-call steps, thereby forming a tree-like structure. Next, we assign rewards to each step, based on its ability to produce a correct answer and make successful tool calls. A shared step across different trajectories receives the same reward, while different steps under the same fork receive different rewards. Finally, these step-wise rewards are used to calculate fork-relative advantages, blended with trajectory-relative advantages, to train the LLM for tool use. The experiments utilize 17 tools to address user queries, covering both time-sensitive and time-invariant topics. We conduct ablation studies to systematically justify the necessity and the design robustness of step-wise rewards. Furthermore, we compare the proposed PORTool with other training approaches and demonstrate significant improvements in final accuracy and the number of tool-call steps."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 0 2 0 6 2 . 0 1 5 2 : r PORTool: Tool-Use LLM Training with Rewarded Tree Feijie Wu1, Weiwu Zhu2, Yuxiang Zhang2, Soumya Chatterjee2, Jiarong Zhu2, Fan Mo2, Rodin Luo2, Jing Gao1 1Purdue University, 2Apple Work done during an internship at Apple. Current tool-use large language models (LLMs) are trained on static datasets, enabling them to interact with external tools and perform multi-step, tool-integrated reasoning, which produces tool-call trajectories. However, these models imitate how query is resolved in generic tool-call routine, thereby failing to explore possible solutions and demonstrating limited performance in an evolved, dynamic tool-call environment. In this work, we propose PORTool, reinforcement learning (RL) method that encourages tool-use LLM to explore various trajectories yielding the correct answer. Specifically, this method starts with generating multiple rollouts for given query, and some of them share the first few tool-call steps, thereby forming tree-like structure. Next, we assign rewards to each step, based on its ability to produce correct answer and make successful tool calls. shared step across different trajectories receives the same reward, while different steps under the same fork receive different rewards. Finally, these step-wise rewards are used to calculate fork-relative advantages, blended with trajectory-relative advantages, to train the LLM for tool use. The experiments utilize 17 tools to address user queries, covering both time-sensitive and time-invariant topics. We conduct ablation studies to systematically justify the necessity and the design robustness of step-wise rewards. Furthermore, we compare the proposed PORTool with other training approaches and demonstrate significant improvements in final accuracy and the number of tool-call steps. Date: October 31,"
        },
        {
            "title": "Introduction",
            "content": "As representative paradigm for synergizing reasoning and action, the ReAct framework (Yao et al., 2023b) activates large language models (LLMs) to combine reasoning (step-by-step analysis) and acting (tool calls), achieving strong problem-solving performance. Early works (Yao et al., 2023a,b; Wei et al., 2022; Schick et al., 2023; Wu et al., 2025) enable this capability primarily through prompt engineering, in which carefully crafted demonstrations guide the model to alternate between generating reasoning traces and invoking tools. However, these approaches provide reasoning within generic context rather than learning from actual interactions with external tools, thereby limiting their effectiveness in solving complex, multi-step tasks. This limitation highlights the importance of exploring how to augment LLMs reasoning and acting capabilities through interactive tool use. Reinforcement learning (RL) has emerged as promising direction to address this gap, leading to growing number of research works (Qian et al., 2025; Zhang et al., 2025; Dong et al., 2025; Zeng et al., 2025; Jin et al., 2025; Li et al., 2025; Singh et al., 2025; Xue et al., 2025; Feng et al., 2025a; Wei et al., 2025; Lin and Xu, 2025). Most existing works concentrate on narrow set of tools, such as web search or program compilers, which restricts the trained models applicability to wide range of function-like tools with rigid formatting requirements and specialized functionalities. ToolRL (Qian et al., 2025) and Tool-N1 (Zhang et al., 2025) broaden the scope by training LLMs to interact with diverse tool schemas. Their datasets consist of structured inputs, including queries, tool descriptions, and, if available, historical tool-call trajectories, paired with the expected tool calls for subsequent steps. Both methods adopt group-relative policy optimization (GRPO) (Shao et al., 2024) for fine-tuning, where the model generates various tool-call steps for each input and receives rewards based on consistency with the expected tool calls, thereby reinforcing tool-use capability. 1 These two approaches, however, face several limitations because they assume the existence of groundtruth tool calls at every intermediate step. First, constructing such datasets is economically impractical in real-world settings, as it requires substantial human effort to understand tool execution mechanisms and annotate each step. Second, this design encourages the model to imitate predefined tool calls. While effective for producing generic routines that solve common queries, it diminishes the models ability to explore alternative trajectories. As result, potentially valid solutions are either ignored or penalized for diverging from the expected calls. This limits the models ability to handle In real-time queries, as illustrated in Figure 1. this example, model trained by ToolRL (Qian et al., 2025) attempts to finish the task by following generic tool-call routine. However, the tool raises an error, leading the model to generate no-answer response without considering whether it misunderstood the users intent. In this work, we begin by collecting set of real user queries that require both factual and real-time information, along with suite of executable tools. Instead of providing generic tool-call routine, we build an agentic system that consists of tool-use agent and an evaluation agent: The tool-use agent enables the LLM to interact with external tools to generate complete tool-call trajectories, while the evaluation agent assesses the correctness of each trajectory based on its final answer to the input query. With this system, we follow the existing tool-use training works (Jin et al., 2025; Qu et al., 2025; Dong et al., 2025) to reinforce the LLM on the tool-use agent through GRPO. Specifically, the model generates multiple tool-call trajectories independently, and GRPO computes the advantages across the trajectory level, assigning identical advantages to all tokens within each trajectory. Figure 1 Training by labeled static tool-call trajectories cannot handle real-time query. The example is generated from ToolRL (Qian et al., 2025). Apparently, GRPO neglects the varying contributions of each step to the final result. In particular, even an incorrect trajectory may contain informative intermediate steps that, if recombined differently, could bring correct answer. However, such potential is lost when all steps are assigned with uniform rewards (Zeng et al., 2025). Conversely, certain steps within correct trajectory may have no influence on the final result. These observations emphasize the need to quantify each steps importance with the help of the evaluation agent. Toward the goal, we propose PORTool, Policy Optimization algorithm with Rewarded Tree for tool-use LLMs. PORTool introduces tree rollout, in which certain tool-call steps branch into multiple trajectories and eventually lead to different subsequent tool calls. After evaluating each trajectory, the evaluation agent assigns reward to each step that considers both the correctness of the final answer and its formatting compliance. Correctness is weighted more strongly in the reward signals, encouraging the model to favor step that yields the correct answer, even though its format is incorrect. Notably, step shared by various trajectories is assigned an identical reward, while different steps under the same forks receive distinct rewards. Finally, with these step-wise rewards, the tool-use agent computes fork-relative advantages that reflect the most effective steps within each fork. These are combined with trajectory-relative advantages that characterize the holistic performance of each trajectory to train the LLM, enabling it to generate more effective tool-call steps. Figure 1 illustrates the superiority of the proposed PORTool, where the trained model accurately understands the users intent, takes appropriate step-by-step tool calls, and provides reliable response. Contributions. Throughout this work, our contributions are highlighted as follows: To the best of our knowledge, this is the first work to reinforce the LLM for tool use by interacting with numerous executable tools, demonstrating evolved and dynamic outputs based on real-time information. 2 We propose an algorithm, PORTool, that integrates an innovative tree rollout strategy and designs reward function that quantifies the importance of each step. Moreover, we theoretically figure out an optimal coefficient setting for combining step-relative and trajectory-relative advantages while reinforcing an LLM for tool use. We conduct comprehensive ablation studies to validate both the necessity and effectiveness of the proposed reward function design. In addition, we compare the proposed PORTool with other policy optimization algorithms and demonstrate superiority in terms of final accuracy and the number of tool-call steps."
        },
        {
            "title": "2 Related Works",
            "content": "RL algorithms for enabling LLMs with reasoning. The rapid development of LLMs highlights the pivotal role of RL algorithms in training (Schulman et al., 2017; Rafailov et al., 2023; Ahmadian et al., 2024; Yu et al., 2025; Wu et al., 2024). Recent breakthroughs such as DeepSeek-R1 (Guo et al., 2025) disclose that RL algorithms can endow LLMs with chain-of-thought reasoning abilities (Wei et al., 2022) by assigning verifiable or rule-based rewards (Zhu et al., 2025; Yeo et al., 2025; Havrilla et al., 2024). This insight has spurred wave of new policy optimization approaches (Yu et al., 2025; Zheng et al., 2025; Ahmadian et al., 2024; Liu et al., 2025; Chu et al., 2025b). In multi-step tool-use scenarios, these approaches assign single reward to the entire trajectory, which obscures the varying significance of individual steps and fails to recognize how intermediate tool calls shape the final outcome. To tackle the limitation, GiGPO (Feng et al., 2025b) assigns rewards at the step level and normalizes them across actions within the same state when training the policy model. However, this framework does not extend to our scenario, as the state information is not explicitly defined. Among those RL works, the most related concurrent work is ARPO (Dong et al., 2025). ARPO proposes an entropy-based adaptive rollout mechanism that generates multiple rollouts forming tree structure. Specifically, forks are created when significant probability deviation emerges between the initial and current steps. Leveraging this tree structure, ARPO applies GRPO to reinforce LLMs for tool use. In contrast, our proposed PORTool is more efficient in rollout generation and explicitly quantifies each steps contribution to the final outcome. As demonstrated in Section 4.2, ARPOs heavier rollout process and lack of step-wise attribution lead to inferior performance compared to the proposed PORTool method. Tool-use LLM Training. Beyond RL-based approaches, another widely adopted strategy for equipping LLMs with tool-use capability is supervised fine-tuning (SFT) (Schick et al., 2023; Qin et al., 2023). These methods typically rely on large, high-quality human-annotated datasets such as ToolACE (Liu et al., 2024a), xLAM (Liu et al., 2024b; Prabhakar et al., 2025), and others (Chen et al., 2023, 2024; Acikgoz et al., 2025). However, models trained solely through SFT often face limitations in generalization, exploration, and adaptability (Chu et al., 2025a). In our work, we eliminate human efforts in labeling valid trajectories for all queries and introduce an RL algorithm to encourage LLMs to discover proper trajectories through trial and error."
        },
        {
            "title": "3 PORTool: Policy Optimization with Rewarded Tree",
            "content": "A tool-use LLM is equipped with tool-integrated reasoning (TIR), in which query is solved by interleaving tool-configuration reasoning and tool outputs. At each step, the model analyzes the current query in light of previous tool outputs, decides which tools to invoke, and generates the corresponding calls. The returned outputs are incorporated into the models context, enabling it to refine its reasoning and progress to the next step. In this approach to solving given query, sequence of interdependent steps is formed and is known as tool-call trajectory. In this section, we introduce our design that enables an LLM with the tool-use capability. Therefore, we first formulate an RL objective function that can achieve the goal in Section 3.1. Building on this objective, Section 3.2 introduces PORTool, the proposed method that reinforces an LLM to utilize external tools."
        },
        {
            "title": "3.1 Problem Formulation\nDataset and Tool set. We consider training an LLM with RL using both a dataset and a tool set:",
            "content": "Dataset Q: The dataset includes mixture of time-invariant queries with fixed factual answers (e.g., \"Who is younger, Trump or Biden?\") and time-sensitive queries requiring real-time information (e.g., \"Which of these stocks are above 350 EUR: Apple, Tesla, or Microsoft?\"). Notably, some queries admit multiple valid tool-call trajectories. For instance, the stock-price query can be solved either by (i) retrieving the three stock prices in USD and converting the 350 EUR threshold into USD, or (ii) converting the stock prices into EUR and directly comparing them against 350 EUR. Tool set : The tool set consists of numerous callable functions, each identified by unique name and associated with schema of parameters specifying format and validity constraints. tool processes the given parameters and returns structured outputs; violations of input requirements result in explicit error messages, while incorrect configurations would yield unexpected or incomplete results. Agentic System. The tool-use LLM is optimized within an autonomous agentic system that comprises two distinct roles: tool-use agent responsible for generating tool-call trajectories, while an evaluation agent for verifying their correctness. Tool-use Agent πθ: This agent loads the LLM to be optimized. Each query is resolved through multiple steps: Starting from an input query, the agent produces <think> ... </think> block containing its reasoning traces, followed by <tool_call> ... </tool_call> block specifying one or more tool calls that can be executed simultaneously and independently. Based on the generated tool calls, the agent invokes relevant tools and obtains their responses, which extend the input context to the LLM. The agent conditions on the new input context to generate another step of reasoning traces and tool calls. This process executes repeatedly until one of the following conditions is met: (i) the query is resolved, signaled by invoking the response_gen tool that includes the final answer; or (ii) the query remains unresolved but the maximum number of tool-call steps is reached. Evaluation Agent Rout(q, τ ): This agent uses more powerful LLM (e.g., GPT) to assess the correctness of the final answer. It generates high-quality reference answer to the query through its own reasoning and tool-use capabilities, prioritizing the use of existing knowledge within the trajectory τ . The evaluation returns binary judgement (\"true\" or \"false\") indicating whether the tool-use agents final response is correct. In special cases, it may instead output unable_to_answer when the tool-use agent provides valid reasoning but cannot reach an answer due to inherent limitations (e.g., tools fail to return the required information but comply with their formatting requirements). This case differs from wrong answer because the tool-call trajectory would be valid to produce correct answer when the necessary information is available. Problem Formulation. Consider sampling query from the given dataset Q. The tool-use agent is responsible for generating tool call trajectory τ = {q, (s1, a1), . . . , (sT , aT )}. Specifically, for all [T ], st πθ(stq, U, {(si, ai)}i<t) is the generated content by the LLM for the t-th step, which includes both reasoning traces and tool calls; at = (st) is the responses for the tool calls within st (or the error messages if there exists any error in st). Let us consider that query would create different tool-call trajectories (n 1), denoted as {τj}j[n], where τj = {q, (sj,1, aj,1), . . . , (sj,Tj , aj,Tj )}. Therefore, we formulate an objective function by maximizing J(θ) = qQ, {τj }n j=1πθold (τ q,U ) 1 (cid:88) j=1 1 τj Tj (cid:88) sj,t (cid:88) t=1 o=1 fθ(sj,t,o) , (1) where τj indicates the total length of generated content, i.e., τj = (cid:80) tool-call step sj,t; and t[Tj ] sj,t; sj,t,o is the o-th token of fθ(sj,t,o) (cid:16) = min ρθ(sj,t,o)A(sj,t,o), clip1+ε 1ε (cid:17) (cid:0)ρθ(sj,t,o)(cid:1)A(sj,t,o) , 4 Figure 2 Overview of PORTool Workflow where A(sj,t,o) indicates the advantage of the token sj,t,o, and ρθ(sj,t,o) is defined as ρθ(sj,t,o) = πθ(sj,t,oq, U, {(sj,i, aj,i)}i<t, sj,t,<o) πθold (sj,t,oq, U, {(sj,i, aj,i)}i<t, sj,t,<o) . Limitations of GRPO in tool-use LLM training. Different from the objective formulated by GRPO (Guo et al., 2025), Problem 1 eliminates KL-divergence term used to align with reference model. This change provides greater flexibility, enabling the LLM to follow desired output formats and various tool-call schemas through structured reward mechanism (Qu et al., 2025). In the above objective function, critical aspect is the estimation of the token-level advantage A(sj,t,o), which quantifies the relative contribution of each token to the final outcome. Existing methods such as GRPO (Guo et al., 2025; Shao et al., 2024) simplify this by assigning uniform advantage across all tokens, thereby neglecting the varying influence of different steps within trajectory. This limitation primarily arises from the GRPO design where tool-call trajectories are generated independently. As result, it becomes difficult to determine whether an alternative action at given step could still recover correct answer when the trajectory as whole deviates from the desired outcome."
        },
        {
            "title": "3.2 Algorithm Description",
            "content": "In this section, we introduce PORTool to overcome the limitations of GRPO by capturing the varying importance of each step during training. As illustrated in Figure 2, PORTool generates and organizes tool-call trajectories into tree structure rather than treating them independently. This structure allows rewards to be assigned at each step, reflecting both (i) the correctness of trajectory outcomes and (ii) compliance with formatting requirements and tool-specific schemas. By integrating these step-wise rewards, the optimization of the formulated objective (Problem 1) enables the model to gradually develop robust tool-use capabilities. This three-step training procedure is repeated across multiple rounds until convergence. The rest of the section elaborates on these three steps, including the rollout design, the construction of the reward function, and the optimization strategy for solving Problem 1. Step 1: Tree Rollouts. Given as input query sampled from the dataset Q, the tool-use agent generates tool-call trajectories, with each fork constrained by maximum branching factor , thereby forming tree structure. The process begins by generating rollouts for the first tool-call step. At each subsequent step, completed trajectories are collected into set completed, while the remaining incompleted trajectories are stored in incompleted. Let nc = completed denote the number of completed trajectories. For each incompleted trajectory, the agent produces candidate forks and invokes external tools to obtain their responses. From these candidates, exactly nc are selected to continue expansion, while the rest are discarded. The agent terminates the iterative process once either (i) all trajectories are completed (nc = n) or (ii) the maximum tool-call depth is reached. This guarantees that exactly trajectories are generated for each query, where some may share common prefixes of tool-call steps before diverging. Algorithm 1 summarizes the rollout process in pseudocode form. The generated trajectories naturally induce tree structure for each query. To formalize this, let m(sj,t) be mapping function that returns the set of trajectory indices that pass through the same step sj,t. For each step sj,t, we define the child set C(sj,t) as the collection of trajectory indices branching from sj,t, corresponding to 5 distinct (t + 1)-th steps. By construction, for any k1, k2 C(sj,t) with k1 = k2, it holds that sk1,t+1 = sk2,t+1. However, it is possible for two distinct trajectories k1, k2 m(sj,t) to satisfy sk1,t+1 = sk2,t+1. Therefore, the relation between two sets is C(sj,t) m(sj,t). Step 2: Reward Computation. With the generated trajectories, we compute rewards from two complementary perspectives: outcome Rout and formatting Rf m. For trajectory τj, the reward at its t-th step sj,t is formulated as R(sj,t) = (cid:16)(cid:8)γTktRout (q, τk) + Rf (sj,t)(cid:9) km(sj,t) (cid:17) , (2) where γ [0, 1] is decay factor ensuring that steps closer to the final result have greater influence, thereby favoring trajectories that solve the task with fewer steps (Feng et al., 2025b). The operator is chosen adaptively: (i) = max if the rewards of all sibling nodes are not identical, i.e., there exists pair k1, k2 C(sj,t1) such that R(sk1,t) = R(sk2,t); and (ii) = avg otherwise. This formulation encourages the model to prioritize high-quality tool-call steps that yield higher outcome rewards while maintaining proper formatting. When all sibling steps under fork achieve equivalent optimal performance, the averaging rule prefers the trajectory with higher probability and efficiency of leading to correct final answer. Building upon the reward formulation (i.e., Equation 2), we elaborate on the design of outcome and formatting rewards. Outcome reward Rout: As defined in Section 3.1, the outcome reward (a.k.a. trajectory-based reward) is determined by the evaluation agent, which labels each trajectory as true, false, or unable_to_answer, corresponding to {+1, 1, 0}, respectively. Formatting reward Rf m: For each step sj,t, we evaluate whether the generated output adheres to the required structural format as described in the system prompt (Appendix D). According to the description of the tool-use agent, the LLM is supposed to first produce reasoning block, followed by tool-call block. Thus, we design concrete rubric for assigning formatting rewards: 1. Reasoning: If the generated response includes <think> ... </think> block, the step receives reward of +0.2. 2. Formatting: Formatting rewards are activated only when reasoning block is present. Each tool call must be invoked within <tool_call> ... </tool_call> block and follow valid JSON format containing list of tool calls. We assign rewards based on the following criteria: If the tool-call indicator <tool_call> ... If the enclosed content can be parsed as valid JSON, reward an additional +0.1. If every tool call includes both required fields, namely \"name\" of string type and \"arguments\" of </tool_call> is included, reward +0.1. dictionary format, reward an additional +0.05. Finally, the success of tool invocation is assessed, but only when the above requirements are met. Each tool call is evaluated independently. maximum reward of +0.55 is given if all tool calls succeed, with partial credit assigned in proportion to the fraction of successful tool calls. failed tool call usually does not comply with the tool-call schemas. Based on the definitions above, the outcome reward takes values Discussion: Range of formatting rewards. in {1, 0, +1}, while the formatting reward lies in the range [0, 1]. If these ranges were left unadjusted, undesirable cases could arise where step with an incorrect outcome but perfect formatting outweighs step with correct outcome but imperfect formatting. To prevent this, we rescale the formatting reward so that achieving correct final answer is always more important than adhering to formatting. Specifically, by assuming the decay factor has an upper bound Γ and the problem must be solved within at most Tmax steps, we restrict the formatting reward to [ΓTmax/2, ΓTmax/2]. In this work, we set Γ = 0.9 and Tmax = 5, yielding rescaled formatting reward range of [0.25, 0.25]. This design ensures that formatting influences the model training but never overrides the impact of correctness. In the final stage, we compute token-level advantages to guide the optimization of Step 3: Policy Optimization. the policy model (i.e., LLM). Two complementary forms of advantage are considered: (i) trajectory-relative 6 advantage Atrj(τj), which evaluates the relative quality of complete trajectory among the candidates based solely on the outcome reward; and (ii) fork-relative advantage Afork(sj,t), which evaluates the relative quality of step compared with its sibling tool-call steps in the rollout tree. Mathematically, denote norm(, ) as z-score normalization function applied to value with respect to set of values, and these two advantages are formulated for Atrj(τj) = norm(cid:0)Rout(q, τj), {Rout(q, τk)}k[n] (cid:1), Afork(sj,t) = norm(cid:0)R(sj,t), {R(sk,t)}kC(sj,t1) (cid:1). For all tokens within step sj,t, the advantage is thereby defined as A(sj,t,o) = ω1 m(sj,t) (cid:88) km(sj,t) Atrj(τk) + ω2Afork(sj,t), (3) where ω1 and ω2 are weighting coefficients balancing trajectory-level and fork-level influences. This formulation prevents shared step sj,t that has various advantages across different trajectories. The trajectory-relative advantage provides coarse-grained measure of the overall effectiveness of tool-call trajectory, offering advantage estimation for every generated token. In contrast, the fork-relative advantage offers fine-grained signal for distinguishing the best choice at fork with several competing candidates. We optimize the policy model πθ in Problem 1 by applying the formulated advantage (i.e., Equation 3) so that it gradually learns to generate meaningful tool-call step based on the input context (including user query and historical tool responses) through trial and error. The coefficient setting plays crucial role in training, as it determines Discussion: Setting for ω1 and ω2. the balance between complete trajectory quality and individual tool-call step optimality. One may follow Feng et al. (2025b) to set both to 1. However, since these two terms capture distinct aspects of tool-call generation, equal weighting may introduce objective inconsistency, leading to suboptimal convergence. To mitigate this issue, we rescale the fork-relative advantage term so that it concentrates on optimizing each tool-call step conditioned on its input context, without overshadowing the broader trajectory-level objective. formal definition of the coefficient setting is provided in Theorem 3.1, and Appendix provides the corresponding analysis. Theorem 3.1 By setting ω1 = 1 and ω2 = τj m(sj,t) sj,t C(sj,t1) nf orks(q) , where nf orks(q) is defined for the size of set {sk,t C(sk,t) > 1, [n], [Tk]}, which means the number of forks under the tree rollout for the query q. Then, we have the J(θ) = JGRP O_trj(θ) + JGRP O_f ork(θ)."
        },
        {
            "title": "4.1 Setup\nDatasets and Models. We construct a dataset of 2,701 complex queries spanning both time-invariant and\ntime-sensitive cases as mentioned in Section 3.1. Among these, 2,560 queries are used for training and 141 are\nreserved for evaluation. In this section, the tool-use agent loads and trains Qwen-2.5-7B-Instruct (Bai et al.,\n2023) and Qwen-3-1.7B (Yang et al., 2025) with the proposed PORTool and other state-of-the-art algorithms.\nThe evaluation agent is loaded with GPT-4o (Achiam et al., 2023) to automatically assess trajectory outcomes.",
            "content": "Tools. To support the queries in the dataset, we develop comprehensive suite of 17 tools, including response_gen, which serves as the final module that aggregates retrieved evidence and synthesizes the answer. The toolkit encompasses three categories: real-time tools (e.g., weather_search, which provides current conditions and forecasts), factual tools (e.g., math_calculation, which evaluates mathematical expressions), and hybrid tools combining both functionalities (e.g., conversion_calculation, which performs unit conversions such as currency and measurement). Detailed specifications of each tool, including their functionalities and schemas, are provided in Appendix C.1. 7 Method Qwen-2.5-7B-Instruct + GRPO + GRPO_fm + DAPO + ARPO + PORTool Qwen-3-1.7B + GRPO + GRPO_fm + DAPO + ARPO + PORTool Accuracy (%) 29.79 1.42 54.65 1.17 58.64 1.83 53.22 1.88 56.29 1.19 64.07 1.36 7.09 0.54 27.27 1.48 31.94 1.45 28.83 1.76 25.60 0.48 43.74 2.07 # Tool-call Steps 4.76 3.77 3.62 3.70 3.68 3.22 5.78 4.89 4.51 4.73 5.18 3.70 Unanswerable Rate (%) 49.83 22.90 20.64 21.77 21.42 12.77 91.49 55.11 52.48 59.72 64.21 31.91 Formatting Reward 0.524 0.744 0.799 0.746 0.774 0.862 0.238 0.478 0.578 0.533 0.398 0.784 Table 1 Performance comparisons among different RL training methods on the evaluation dataset Baselines. We compare the proposed PORTool against several representative RL algorithms, including GRPO (Guo et al., 2025), DAPO (Yu et al., 2025), and ARPO (Dong et al., 2025). GRPO and DAPO generate multiple independent tool-call trajectories for each input query, and we assign an outcome reward to each trajectory. In addition, we introduce variant, GRPO_fm, which extends GRPO by incorporating formatting reward, computed as the average step-level formatting score across trajectory, into the final outcome reward. For ARPO, we follow its original algorithmic design for tool-call trajectory generation and reward assignment, using the outcome reward directly as the trajectory-level reward. Evaluation Metrics. We evaluate the performance of the tool-use agent across four dimensions: (i) Accuracy, as judged by the evaluation agent on whether generated tool-call trajectory is correct; (ii) Average number of tool-call steps, measuring the efficiency of query resolution by counting the number of tool-call steps per trajectory (capped at six steps); (iii) Unanswerable rate, defined as the proportion of queries for which the agent fails to produce an answer within six tool-call steps; and (iv) Average formatting reward, ranging from [0, 1], which quantifies the averaged formatting reward of query. To mitigate the stochasticity of LLM-based evaluation, each trajectory is independently assessed five times, and the final accuracy is reported based on majority voting across these evaluations. The accompanying error range reflects the variance introduced by repeated LLM judgments. Implementations. During training, each query generates eight tool-call trajectories. With different RL methods, the tool-use agent trains its model for 20 epochs. Additional implementation details, such as hyperparameter settings, are provided in Appendix C."
        },
        {
            "title": "4.2 Comparison with Other Baselines",
            "content": "Table 1 summarizes the performance of the proposed PORTool compared with several RL baselines under two base models: Qwen-2.5-7B-Instruct and Qwen-3-1.7B. They represent two common types of LLMs: The instruction model, Qwen-2.5-7B-Instruct, follows prompts faithfully and produces well-formatted content, while the reasoning model, Qwen-3-1.7B, generates intermediate thinking processes before producing answers. However, each base model exhibits inherent limitations. Although Qwen-2.5-7B-Instruct complies with the required template, its low accuracy and high unanswerable rate reveal weaknesses in its reasoning and tool-use capabilities. For Qwen-3-1.7B, its low formatting reward indicates that most generated responses do not include <tool_call> ... </tool_call> block, making them fail to invoke tools. Despite being equipped with reasoning capability, Appendix H.1 highlights its insensitivity to formatting errors, thereby preventing correction and leading to extremely low accuracy and high unanswerable rate. Under Qwen-2.5-7B-Instruct, the proposed PORTool substantially enhances performance across all evaluation metrics. It doubles the number of correctly answered queries compared with the base model while 8 (a) Outcome Reward (b) Formatting Reward (c) Tool-call Steps Figure 3 Comparison of different decay factors γ of Equation 2. reducing the proportion of unanswerable queries to roughly one quarter. Meanwhile, PORTool requires fewer tool-call steps per query, indicating that it enables the tool-use agent to invoke tools more precisely and efficiently. Compared with the best baseline, GRPO_fm, the proposed PORTool demonstrates superiority with 5% gain in accuracy, an 8% reduction in the unanswerable rate, and 11% fewer tool-call steps. Besides, PORTool achieves higher formatting rewards, reflecting stronger structural compliance and fewer tool-call or formatting errors. These results confirm that PORTool enables an LLM with both more reliable reasoning and more efficient tool-use behavior than existing RL approaches. Under Qwen-3-1.7B, the proposed PORTool showcases similar improvements over both the base model and existing approaches, demonstrating that its advantages generalize across different backbones. Notably, PORTool attains the highest formatting reward, indicating that most tool calls are executed successfully for most queries. Therefore, compared with other baselines, PORTool requires fewer corrective attempts for failed tool calls, thereby reducing redundant recovery steps that often exceed the step limit for producing an answer. These results suggest that PORTool effectively strengthens the models error-correction capability and progressively guides it toward generating error-free trajectories for enhancing tool-integrated reasoning."
        },
        {
            "title": "4.3 Discussions",
            "content": "This section presents series of empirical studies to evaluate the design robustness of the proposed PORTool. We investigate key components, including the choice of reward function, the decay factor γ in the step-wise reward formulation (Equation 2), and the coefficient settings (ω1, ω2) in the advantage function (Equation 3). 4.3.1 Effect of the Decay Factor γ The decay factor γ in the step-wise reward function (i.e., Equation 2) plays crucial role in blending outcome reward with the formatting reward. Figure 3 illustrates how outcome reward, formatting reward, and the number of tool-call steps evolve throughout training. When γ = 0, both outcome and formatting rewards fall behind other γ settings. As defined in the step-wise reward formulation, setting γ = 0 disregards trajectory correctness, causing the fork-relative advantage to rely solely on formatting reward. This mismatches the objective of trajectory-relative advantage, which focuses on trajectory correctness. Consequently, trajectories with incorrect reasoning but perfect formatting may be over-rewarded, while correct but imperfectly formatted trajectories are penalized, leading to degraded performance and slower convergence. As mentioned in Section 3.2, setting γ 0.9 exhibits synergistic effect, where the step-wise reward emphasizes trajectory correctness without being dominated by the formatting reward. This behavior is consistent with the goal of trajectory-relative advantage. To determine the optimal γ for the proposed PORTool, we conduct experiments across multiple configurations. Among them, γ = 0.95 yields the highest outcome and formatting rewards while requiring the fewest tool-call steps. In contrast, γ = 1 demands more tool-call steps to reach correct answer because the step-wise reward fails to discriminate which step efficiently resolves query. Meanwhile, γ = 0.9 underweights the outcome reward component, resulting in degraded performance. These observations suggest the existence of an optimal γ such that the step-wise reward appropriately balances trajectory efficiency and correctness with formatting precision, leading to efficient training and tool-use behaviors. (a) Outcome Reward (b) Formatting Reward Figure 4 Comparison of different advantage settings. (a) Outcome Reward (b) Format Reward (c) Evaluation Accuracy Figure 5 Comparison of different designs of G() in Equation 2. 4.3.2 Effect of the Advantage Coefficients The coefficients ω1 and ω2 play pivotal role in using the trajectory-relative and fork-relative advantages in Equation 3 for policy optimization. Figure 4 compares four configurations: the proposed PORTool adopts the coefficient setting derived from Theorem 3.1; trajectory-only uses ω1 = 1 and ω2 = 0; fork-only applies ω1 = 0 with ω2 defined as in Theorem 3.1; and no scale sets both coefficients to one (ω1 = ω2 = 1). Among these four settings, the overall performance follows the trend PORTool > fork-only > no scale > trajectory-only in both outcome and formatting rewards. Specifically, the trajectory-only method merely captures trajectory correctness and assigns identical advantage to all tokens, lacking the granularity to distinguish between meaningful and uninformative steps. This limitation explains the substantial performance gap between the baselines and the proposed PORTool. In contrast, the fork-only approach performs much better, since the step-wise reward captures the trajectory correctness and thus overlaps the effect of trajectoryrelative advantage. However, it slightly falls behind PORTool because it cannot exploit the steps without sibling nodes during training. Last but not least, as discussed in Section 3.2, setting both coefficients to one introduces objective inconsistency, which in turn degrades performance. This observation is verified by by experimental results, where the no scale consistently underperforms compared to the proposed PORTool. 4.3.3 Effect of Reward Function R(sj,t) Design In addition to the decay factor γ, another key component influencing model performance is the design of the step-wise reward function R(sj,t) in Equation 2. Section 3.2 outlines the motivation behind the proposed design, which we further validate through empirical comparisons with several alternatives. Specifically, we evaluate three variants: (i ) max, where the aggregation function G() is replaced with = max; (ii ) avg, where = avg; and (iii ) mix2, defined as R(sj,t) = + Rf (sj,t). In mix2, the outcome reward is averaged when sibling steps achieve the same optimal outcome reward. In contrast, PORTool imposes stricter consistency criterion, requiring both the optimal outcome and formatting rewards to align before averaging across various outcome rewards. (cid:16)(cid:8)γTktRout (q, τk)(cid:9) km(sj,t) (cid:17) Figure 5 compares the outcome reward, formatting reward, and evaluation accuracy across different reward function designs. In general, the proposed PORTool consistently outperforms the three alternatives across all metrics, demonstrating the robustness of the proposed reward formulation. Among the variants, avg performs worst. This is because it may fail to guide the model to the step that yields the correct solution, resulting in degraded learning effectiveness. This limitation also explains the inferior performance of mix2, which may 10 lead the model to take tool-call steps that fail to comply with formatting requirements, even though the steps eventually yield correct outcomes. Consequently, the trained LLM would generate incorrect tool calls and weaken tool-integrated reasoning efficacy. Therefore, it is essential to enforce formatting reward consistency before applying averaging, as implemented in the proposed PORTool, to achieve stable and effective training."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose an RL method PORTool for enabling an LLMs with tool-use capabilities within an autonomous agentic system comprising tool-use agent and an evaluation agent. For each round of training, the method consists of three steps: First, following the tree-rollout strategy, the tool-use agent generates multiple tool-call trajectories for given query, forming tree. Second, the evaluation agent computes trajectory-based rewards, reflecting the correctness of complete trajectory, and assigns step-wise rewards, quantifying each step importances based on formatting compliance and the efficiency of yielding correct answers. Third, using these step-wise and trajectory-based rewards, the tool-use agent computes fork-relative and trajectory-relative advantages, which are adopted to train the LLM. Empirical studies show that PORTool consistently outperforms existing baselines and demonstrate the robustness of the proposed design."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Emre Can Acikgoz, Jeremiah Greer, Akul Datta, Ze Yang, William Zeng, Oussama Elachqar, Emmanouil Koukoumidis, Dilek Hakkani-Tür, and Gokhan Tur. Can single model master both multi-turn conversations and tool use? coalm: unified conversational agentic language model. arXiv preprint arXiv:2502.08820, 2025. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881, 2024. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025a. Xiangxiang Chu, Hailang Huang, Xiao Zhang, Fei Wei, and Yong Wang. Gpg: simple and strong reinforcement learning baseline for model reasoning. arXiv preprint arXiv:2504.02546, 2025b. Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, et al. Agentic reinforced policy optimization. arXiv preprint arXiv:2507.19849, 2025. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025a. Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025b. Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions. 11 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025. Heng Lin and Zhongwen Xu. Understanding tool-integrated reasoning. arXiv preprint arXiv:2508.19201, 2025. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920, 2024a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, et al. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. arXiv preprint arXiv:2406.18518, 2024b. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, et al. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601, 2025. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8):198343, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model, May 2023. URL http://arxiv.org/ abs/2305.18290. arXiv:2305.18290 [cs]. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 12 Yifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, and Li Du. Autotir: Autonomous tools integrated reasoning via reinforcement learning. arXiv preprint arXiv:2507.21836, 2025. Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Lu Su, and Jing Gao. Towards federated rlhf with aggregated client preference for llms. arXiv preprint arXiv:2407.03038, 2024. Feijie Wu, Zitao Li, Fei Wei, Yaliang Li, Bolin Ding, and Jing Gao. Talk to right specialists: Routing and planning in multi-agent system for question answering. arXiv preprint arXiv:2501.07813, 2025. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Xiaosen Zheng, Zejun Ma, and Bo An. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. arXiv preprint arXiv:2509.02479, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023a. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023b. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Siliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, and Mingyi Hong. Reinforcing multi-turn reasoning in llm agents via turn-level credit assignment. arXiv preprint arXiv:2505.11821, 2025. Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Exploring tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025."
        },
        {
            "title": "A Algorithm for Tree Rollout",
            "content": "Algorithm 1 Tree Rollout Input: Query q, Number of Rollouts n, Maximum number of forks , Maximum depth Tmax, Tool-use LLM πθ, Tool set Required: IsCompleted(τ ) is function to justify whether the trajectory τ is completed, i.e., the last step of trajectory τ includes tool call response_gen. Required: DupAndSamp(traj, f, ) duplicates every element in the set traj for times, forming new set traj, selects elements from the new set and returns them. 1: completed [] Initialize trajectories with the first tool-call step 2: traj 3: for all {1, . . . , n} do sj,1 πθ(sq) 4: aj,1 (sj,1) 5: τj [q, (sj,1, aj,1)] 6: traj traj {τj} 7: 8: end for 9: 1 Iterative expansion with branching factor while keeping exactly trajectories 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 10: for {2, . . . , Tmax} do for τ traj do 11: 12: if IsCompleted(τ ) then completed.append({τ }) traj traj {τ } end if end for nc completed if nc = then Break end if traj DupAndSamp(traj, f, nc) for all τ traj do st πθ(sq, τ ) at (st) τ.append((st, at)) end for 25: 26: 27: 28: end for 29: completed.append(traj) 30: return completed Proof of Theorem 3.1 Fork set and notation. Let C(s) be the index set of children of step s, and m(s) map step to the index set of its descendant steps whose tokens we average when we expand beyond (both return sets of indices). Define the set of forked steps = { sj,t : C(sj,t) > 1, [n], [Tj] }, and write nforks(q) = for query q. 14 Token-level GRPO losses. Recall the objective in equation 1: J(θ) = 1 (cid:88) j=1 1 τj Tj (cid:88) sj,t (cid:88) t=1 o=1 fθ(sj,t,o) , with the standard PPO/GRPO per-token term fθ(s) (cid:16) = min ρθ(s)A(s), clip1+ε (cid:17) 1ε(ρθ(s))A(s) . We instantiate an additive token loss (a standard multi-objective construction) as fθ(s) = (trj) θ (s) + (fork) θ (s), where (trj) θ (s) (cid:16) = min ρθ(s)Atrj(s), clip1+ε (cid:17) 1ε(ρθ(s))Atrj(s) , and, for rescaled fork advantage Afork to be determined, (fork) θ (s) (cid:16) = min ρθ(s)Afork(s), clip1+ε (cid:17) 1ε(ρθ(s))Afork(s) . By construction, J(θ) = JGRPO_trj(θ) + JGRPO_fork(θ), with JGRPO_trj and JGRPO_fork the expectations of the empirical sums formed with (trj) tively. We set ω1 = 1 in Atrj. θ and (fork) θ respecFork-wise empirical objective. children and their tokens: Independently, define the fork-wise objective by averaging over forks, then over JGRPO_fork(θ) = (cid:124) 1 (cid:88) sj,tF 1 C(sj,t) (cid:88) kC(sj,t) 1 sk,t+1 sk,t+1 (cid:88) o=1 (fork) θ (sk,t+1,o) , (cid:123)(cid:122) emp (θ;{τj }) L(fork) (cid:125) , Reindexing lemma (disjoint union). For any fork sj,t F, m(sj,t) = (cid:91) kC(sj,t) m(sk,t+1), m(sk,t+1) m(sk,t+1) = for = k. Consequently, (cid:88) kC(sj,t) 1 sk,t+1 sk,t+1 (cid:88) o=1 (fork) θ (sk,t+1,o) = (cid:88) sk,t+1 (cid:88) (fork) θ (sk,t+1,o) km(sj,t) o=1 m(sk,t+1) sk,t+1 This is exactly the equalities used informally in your draft, now stated as lemma. Matching normalizers and the choice of ω2. Insert the lemma into L(fork) emp : L(fork) emp (θ; {τj}) = 1 (cid:88) (cid:88) sk,t+1 (cid:88) (fork) θ (sk,t+1,o) sj,tF km(sj,t) o=1 C(sj,t) m(sk,t+1) sk,t+1 . 15 To embed this into the per-trajectory/token averaging of equation 1, observe that token su,o appearing as child of forked parent step sj,t1 receives weight 1 (cid:124)(cid:123)(cid:122)(cid:125) fork avg 1 C(sj,t1) (cid:124) (cid:125) (cid:123)(cid:122) children avg 1 m(sj,t) (cid:125) (cid:123)(cid:122) (cid:124) descendants split 1 sj,t (cid:124) (cid:123)(cid:122) (cid:125) token avg . In contrast, in equation 1 the empirical averaging for any token of trajectory is 1 τj . Therefore, to write the fork contribution in the same empirical form as equation 1, we absorb the ratio of these weights into the advantage via where Afork(sj,t,o) = ω2 Afork(sj,t,o), ω2 = τj m(sj,t) sj,t C(sj,t1) nforks(q) . (When sj,t is not child of fork, take Afork(sj,t,o) = 0.) With this choice, L(fork) emp (θ; {τj}) = 1 (cid:88) j= 1 τj Tj (cid:88) sj,t (cid:88) t=1 o=1 (fork) θ (sj,t,o). Combining with the trajectory term (where we set ω1 = 1), Lemp(θ; {τj}) = 1 (cid:88) j= 1 τj (cid:88) (cid:16) t,o (trj) θ (sj,t,o) + (fork) θ (cid:17) (sj,t,o) , and taking expectations yields J(θ) = JGRPO_trj(θ) + JGRPO_fork(θ). (i) The additivity fθ = (trj) is the usual way to combine multiple GRPO/PPO-style Remarks. objectives; we do not replace by single sum inside one min() (which would not in general be additive). (ii) The scaling ω2 is the unique choice (up to constant that cancels if applied to both numerator and denominator) that aligns the fork averaging with the per-trajectory/token averaging in equation 1. θ + (fork) θ"
        },
        {
            "title": "C Experimental Setup",
            "content": "C.1 Tool Descriptions In this work, we integrate set of structured function-based tools to support experiments involving real-time information retrieval, knowledge reasoning, and contextual decision-making. Each tool is defined with specific schema to ensure precise query formulation and reliable execution. Below, we describe the functionalities of the tools used: News Search (news_search): Provides access to news headlines and articles related to current events, given query string. Weather Search (weather_search): Returns current weather conditions and forecasts for specified location. The tool supports time-bounded queries with start and end times in ISO 8601 format, and allows adjustable precision (e.g., hour, day). Flights Search (flights_search): Retrieves airline flight information, including schedules, status, arrival, and departure data. It supports flexible specifications such as carrier codes, flight numbers, departure/arrival locations, and temporal windows. 16 Stocks Search (stocks_search): Fetches real-time and historical stock price information using stock or market names as input queries. Sports Search (sports_search): Provides information on sports teams, athletes, schedules, and standings based on query intent. Maps Search (maps_search): Identifies businesses, landmarks, and addresses, and computes estimated travel times. It allows specification of anchor locations, estimated time of arrival (ETA) type, and ETA constraints. Conversion Calculation (conversion_calculation): Handles measurement and currency unit conversions. The tool accepts structured inputs specifying the original units and values, the target units, and optionally, the substance to support weightvolume conversions. Math Calculation (math_calculation): Executes mathematical computations directly based on textual math query. Get Current Context (get_current_context): Retrieves contextual information such as the current time and/or the users current location. Timestamp Comparator (timestamp_comparator): Compares two ISO 8601 timestamps using operators such as <, >, or ==. Time zone information is required for both timestamps. Sort List (sort_list): Sorts list of objects by specified numeric property, in either ascending or descending order. Holiday Search (holiday_search): Returns holiday information for specified date, period, or country. It supports filtering by holiday type (e.g., public, bank, cultural). Knowledge Search (knowledge_search): Retrieves factual knowledge from structured knowledge graph about entities such as people, places, or organizations. Queries can be refined using entity IDs or labels. Filter List (filter_list): Filters list of objects by applying condition (e.g., ==, >, <) to specified property field. Timestamp Converter (timestamp_converter): Converts timestamp from one time zone to another, supporting both canonical time zone identifiers and location-based descriptions. Timestamp Interval Calculator (timestamp_interval_calculator): Computes new timestamp by adding or subtracting an interval (in ISO 8601 duration format) from given reference timestamp, with explicit control over the operation type (add/subtract). These tools provide modular framework for handling diverse queries spanning news, weather, transportation, finance, sports, temporal reasoning, and general knowledge. Their structured inputoutput schemas enable seamless integration with learning agents, ensuring consistency and reliability in multi-step reasoning and tool-use experiments. C.2 Implementation Our implementation is built upon the VeRL framework (Sheng et al., 2024), with rollouts executed using VLLM (Kwon et al., 2023). All experiments are conducted on single node equipped with eight NVIDIA H100 GPUs. The training batch size is set to 512, and in each update round, 128 samples are used to optimize the policy model. During training, text generation adopts temperature of 1.0, top-p of 1.0, and top-k of -1, while validation employs suggested configuration (Qwen-2.5-7B-Instruct: tempature is 0.7, top-p is 0.8, and top-k is -1; Qwen-3-1.7B: tempature is 0.6, top-p is 0.95, and top-k is 20). Each reasoning step is capped at 2,048 tokens, and eight rollouts are generated per query. For time-sensitive queries, the reference location is fixed to Cupertino, California (USA), and execution timestamps are recorded with second-level precision to ensure reproducibility. Prior to reinforcement learning, supervised fine-tuning (SFT) is performed for five epochs. All baselines are trained for 20 epochs (equivalent to 100 steps in total), and we report the average performance of the best three checkpoints during training. Unless otherwise specified, LoRA fine-tuning is 17 Category Hyperparameter Data Configuration Train Batch Size Max Prompt Length (Total) Max Response Length (Per Response) Optimization Learning Rate PPO Mini Batch Size KL Loss Used Rollout Configuration Rollout Name GPU Memory Utilization Number of Rollouts Training & Logging Total Epochs 512 30720 1024 1e-6 128 False vllm 0.4 8 20 Table 2 Training and rollout hyperparameter configuration. Method ToolRL PORTool Accuracy (%) # tool-call steps Unanswerable Rate (%) Formatting Reward 60.99 1.01 64.07 1.36 0.821 0.862 14.18 12.77 3.43 3.22 Table 3 Comparison with ToolRL checkpoint on Qwen-2.5-7B-Instruct adopted with rank = 16 and α = 8. summary of detailed training and rollout configurations is provided in Table 2. C.3 Comparison with ToolRL Table 3 presents comparison between ToolRL and the proposed PORTool. Specifically, we adopt the publicly available ToolRL checkpoint built on Qwen-2.5-7B-Instruct, load it into our tool-use agent, and adapt it to the same input template and tool set used in our experiments. Although ToolRL performs worse than the proposed PORTool, it still surpasses all baseline approaches reported in Table 1. Notably, ToolRL is trained on dataset different from ours but still demonstrates remarkable performance, indicating its strong generalization ability. This advantage can be largely attributed to the availability of ground-truth intermediate steps during its training, whereas its performance remains uncertain when only final answers are provided. The latter setting poses more challenging scenario, as ground-truth supervision for intermediate tool-call steps is unavailable."
        },
        {
            "title": "D System Prompt",
            "content": "You are helpful dialogue assistant that solves user tasks exclusively by invoking the appropriate tools. Do not answer from your own knowledge, and you must strictly follow the provided tool descriptions. You have no internal factual knowledge about games, events, locations, coordinates, dates, capitals, monuments, historical places, or time, and you never perform mathematical operations, computations, comparisons, or sorting on your own. All such information or operations must come from the correct tool. Some tools already have access to the users location, so you must not request it explicitly. When entities appear in query, ensure they are correctly preserved and passed into the final tool call, since the tool output determines the user-facing result. Once you have decomposed the query and gathered the necessary tool outputs, you must always invoke the \"response_gen\" tool with the final answer and never produce direct answer System Prompt Available tools: {tool_json} Output Format: <think> Your thoughts and reasoning in one or two sentences </think> <tool_call> [ {\"name\": \"Tool name\", \"arguments\": {\"argument name\": \"argument content\", \"...\": \"...\"}}, {\"name\": \"...\", \"arguments\": {\"...\": \"...\", \"...\": \"...\"}} ] </tool_call> Important Notes: 1. Every output must include both: (i) one <think> ... or two sentences; and (ii) one <tool_call> ... JSON array. </think> block outlining your thoughts and reasoning in one </tool_call> block containing at least one tool call, enclosed in 2. You may invoke multiple tool calls simultaneously inside the JSON array. Each tool call must be JSON object with name field and an arguments field containing dictionary of arguments. 3. Refer to the previous dialogue records in the history, including the users queries, previous tool calls and responses. 4. When the users task is fully resolved, you must invoke the response_gen tool in the <tool_call> ... </tool_call> block with the required format. In this case, only response_gen should be invoked and no other tools. If other tools are still required, do not invoke response_gen until the task is completed. Tool Schema: get_current_context Description: Retrieves specific aspects of the users request context, such as current location and current time. Parameters: requested_context (array of strings) list of context fields to retrieve, encompassing \"current_location\" and/or \"current_time\" only. Required: { requested_context } Type: object 19 Description. finds businesses, landmarks, and addresses and calculates travel times Parameters. query (string). Users search intent. The query should follow one of these patterns: 1. {navigate me to/find/call/phone number/opening hours/how far/eta}{poi/category/menu items/address} Tool Schema: maps_search {near/on/around/in/from/to [anchor_location]} 2. find {poi/category} near [anchor_location] 3. {how far/eta} from [anchor_location] to [targeted_location] 4. is [anchor_location] open {now/at time/on day} All [anchor_location] and [targeted_location] must be concrete and unambiguous. For queries based on the users current location, [anchor_location] or [targeted_location] in the above patterns may be filled with \"near me\", \"from here\", or \"from my location\", and do not include the anchor_location parameter. If query contains vague location (e.g., \"McDonald near me\"), it is forbidden. Instead, first issue \"find [location]\" to resolve candidates, then always select the first candidate and rewrite the query accordingly (e.g., \"is McDonald at 12th Ave open now\", \"how far is McDonald at 12th Ave from here\"). Do not output vague queries like \"is McDonald near me open now\" or \"how far is McDonald\". anchor_location (string). Reference text describing the area or place mentioned in the query (e.g., \"4th Street\", \"Central Park\"). Leave this empty if the query is related to the users current location. eta_type (string). Type of ETA query. Allowed values include \"departure\" and \"arrive\". eta_time (string). Time constraint associated with eta_type. Use ISO 8601 format. Required. { query } Type. object Tool Schema: knowledge_search Description. Returns passages retrieved from the web and the knowledge graph. The tool retrieves top dynamically ranked passages that answer the query. In addition to decomposed queries, it can also address complex queries by sourcing multiple passages. Note: Web passages are less effective at providing real-time and location-sensitive answers. Parameters. query (string). The natural language query to search on the web. entity_labels (array of strings). List of entity labels used to retrieve relevant facts from the knowledge graph. Required. { query } Type. object Description. Performs mathematical calculations using function-call syntax. Important: Use only raw numbers in expressions (no currency symbols, units, or % signs). The tool returns numeric results without formattingadd appropriate context (currency, units, etc.) in your response based on the users question. Tool Schema: math_calculation Parameters. expression (string). Mathematical expression using function-call syntax. add(a, b, ...), subtract(a, b), multiply(a, b, ...), divide(a, b), avg(a, b, Available functions include: ...), modulus(a, b), isDivisible(a, b), power(base, exponent), root(number, degree), factorial(n), gcd(a, b, ...), lcm(a, b, ...), factor(n), primeFactor(n), random(min, max), log(number, base), percentOf(value, percent), fromPercent(percent), percentDiscount(value, percent discount), decimalToFraction(decimal), fractionToDecimal(fraction). Constants: pi, e, phi, tau. Nested functions may be used for complex expressions. toPercent(decimal), Required. { expression } Type. object 20 Description. Measurement and currency unit conversions. Parameters. query (string). The query that contains the conversion to perform. substance (string). The substance the user wants to include as part of the conversion is used to facilitate weight-to-volume and volume-to-weight conversions. Tool Schema: conversion_calculation from (array of objects). The units and their respective amounts to convert. Each object must include: unit (string): The unit to convert. value (number): The amount or value of the unit. the single target unit for the conversion. to_units (array of strings). The units the user wants to convert to. This array must contain only one element, representing Required. { query } Type. object Description. Calculates new ISO 8601 timestamp by adding or subtracting time interval from given ISO 8601 timestamp. Tool Schema: timestamp_interval_calculator Parameters. from_timezone (string). The time zone corresponding to the from_timestamp. from_location (string). The location corresponding to the from_timestamp. from_timestamp (string). Input timestamp in ISO 8601 format (YYYY-MM-DDThh:mm:ss). Do not include the time zone offset, as it is assumed to be in from_timezone. interval (string). Time interval to be added or subtracted, in ISO 8601 duration format (P[n]Y[n]M[n]DT[n]H[n]M[n]S). operation (string). The operation to perform. Allowed values: add, subtract. Required. { from_timestamp, interval, operation } Type. object Tool Schema: response_gen Description. Response generation. Parameters. response (string). Response to be generated. query (string). User query or rewritten query. context (string). Context for response generation. items (array of objects). Items with entities and facts. Required. { response } Type. object 21 Example I: which team has better performance in the last three seasons: the steelers, ravens, or eagles"
        },
        {
            "title": "DAPO",
            "content": "I encountered an issue retrieving the specific performance data for the Steelers, Ravens, and Eagles for the last three seasons. Please provide more specific details or context for the sport and league you are interested in. There seems to be an issue with retrieving the football standings for the Steelers, Ravens, and Eagles. Please try again later or use different source for this information. The performance data for the Steelers, Ravens, and Eagles in the last three seasons is not available in the expected format. Based on the information provided, we cannot determine which team has the better performance. PORTool Based on the data extracted, the Raptors (Ravens) had positive performance in the last three seasons. They won 12 out of their final 15 games and finished the season with record of 12-5. On the other hand, the Steelers had record of 4-2 after snapping their losing streak in Week 6. The Eagles, however, faced an embarrassing collapse, losing five out of their last six games. Therefore, in the last three seasons, the Ravens had better performance compared to the Steelers and Eagles. Table 4 Generated results across different methods with Qwen-2.5-7B E.1 PORTool Query: Which team has better performance in the last three seasons: the Steelers, Ravens, or Eagles? {\"name\": {\"standings\": \"No data is available\"} {\"requested_info\": \"PERFORMANCE\"}}}} \"sports_search\", \"arguments\": {\"query\": Step 1. To determine which team has better performance in the last three seasons, need to retrieve the relevant team standings for the Steelers, Ravens, and Eagles. Tool Call: last 3 seasons\", \"use_case\": Tool Response: {\"result\": Tool Call: {\"name\": 3 seasons\", \"use_case\": Tool Response: {\"result\": Tool Call: {\"name\": 3 seasons\", \"use_case\": Tool Response: {\"result\": \"sports_search\", \"arguments\": {\"query\": {\"standings\": {\"requested_info\": \"sports_search\", \"arguments\": {\"query\": {\"standings\": {\"requested_info\": \"No data is available\"} \"No data is available\"} \"Ravens standings last \"Eagles standings last \"Steelers standings \"PERFORMANCE\"}}}} \"PERFORMANCE\"}}}} 22 Step 2. The responses from the sports_search tool are not providing the necessary team performance data. will try using the knowledge_search tool to find the performance data of the Steelers, Ravens, and Eagles in the last three seasons. Tool {\"name\": performance last 3 seasons\", \"entity_labels\": Tool Response: [1] Title: 2024 Steelers Season Recall: \"knowledge_search\", \"arguments\": {\"query\": [\"Steelers\"]}} Losing streak snapped against Las Vegas ... \"Steelers Call: [\"Ravens\"]}} \"knowledge_search\", \"arguments\": {\"query\": [2] Title: The latest steelers offensive struggles surface news, videos & comments NewsBreak ... Tool Call: {\"name\": last 3 seasons\", \"entity_labels\": Tool Response: [1] Title: 2024 Baltimore Ravens season ... [2] Title: Baltimore Ravens Playoff History 1996 - 2025 ... Tool Call: {\"name\": last 3 seasons\", \"entity_labels\": Tool Response: [1] Title: Eagles collapse to end 2023 season was \"total misrepresentation\" of team according to Lane Johnson Sporting News ... [2] Title: Eagles SOS: AFLs perennial strugglers granted assistance package in revival bid ... \"knowledge_search\", \"arguments\": {\"query\": [\"Eagles\"]}} \"Eagles performance \"Ravens performance {\"name\": Step 3. The knowledge_search tool responses provide information about the performance of each team in the last three seasons. Based on the provided data, can now determine which team has performed better. Tool Call: \"Based on the data extracted, the Raptors (Ravens) had positive performance in the last three seasons. They won 12 out of their final 15 games and finished the season with record of 12-5. On the other hand, the Steelers had record of 4-2 after snapping their losing streak in Week 6. out of their last six games. better performance compared to the Steelers and Eagles.\"}} The Eagles, however, faced an embarrassing collapse, losing five Therefore, in the last three seasons, the Ravens had \"response_gen\", \"arguments\": {\"response\": Figure 6 Step-by-step reasoning and tool usage under PORTool. Tool calls are shown in blue and tool responses in green. Formatting edition for presentation clarification. 23 Example II: Whats 70 days from march 21 F.1 ToolRL Query: whats 70 days from march 21 \"timestamp_interval_calculator\", \"arguments\": Step 1. To find the date 70 days from March 21, need to add 70 days to March 21. Tool Call: {\"name\": \"2023-03-21\", \"interval\": Tool Response: ERROR: invalid timestamp: date-time with or without timezone (e.g., 2006-01-02T15:04:05Z07:00, 2006-01-02 15:04:05), 12-hour AM/PM with or without seconds (e.g., 03:04:05 PM, 03:04PM), and 24-hour formats (e.g., 15:04:05, 15:04) {2023-03-21}. supported time formats include ISO 8601 \"P70D\", \"operation\": \"add\"}} {\"from_timestamp\": Step 2. The timestamp calculator tool returned an error, possibly due to the timestamp format being recognized as incorrect. However, can directly calculate the date 70 days from March 21 using simple addition. Tool Call: {\"name\": \"2023-03-21\", \"interval\": Tool Response: ERROR: invalid timestamp: date-time with or without timezone (e.g., 2006-01-02T15:04:05Z07:00, 2006-01-02 15:04:05), 12-hour AM/PM with or without seconds (e.g., 03:04:05 PM, 03:04PM), and 24-hour formats (e.g., 15:04:05, 15:04) {2023-03-21}. supported time formats include ISO 8601 \"timestamp_interval_calculator\", \"arguments\": \"P70D\", \"operation\": {\"from_timestamp\": \"add\"}} Step 3. Despite the error, the date can be calculated manually since the tool uses the current year, and the timestamp format is valid for the task. 70 days from March 21 is April 20. Tool Call: 21 is April 20.\"}} \"response_gen\", \"arguments\": {\"response\": \"70 days from March {\"name\": Figure 7 Step-by-step reasoning and tool usage under ToolRL. The generated answer is incorrect. F.2 PORTool Query: whats 70 days from march 21 \"timestamp_interval_calculator\", \"arguments\": Step 1. To find the date 70 days from March 21, need to add 70 days to March 21. Tool Call: {\"name\": \"2023-03-21\", \"interval\": \"from_location\": \"\"}} Tool Response: ERROR: invalid timestamp: date-time with or without timezone (e.g., 2006-01-02T15:04:05Z07:00, 2006-01-02 15:04:05), 12-hour AM/PM with or without seconds (e.g., 03:04:05 PM, 03:04PM), and 24-hour formats (e.g., 15:04:05, 15:04) {2023-03-21}. supported time formats include ISO 8601 \"add\", \"from_timezone\": \"P70D\", \"operation\": \"\", {\"from_timestamp\": 24 Step 2. It seems there was an error with the timestamp format. will retry using the correct format. Tool Call: {\"name\": {\"from_timestamp\": \"timestamp_interval_calculator\", \"arguments\": \"2023-03-21T00:00:00\", \"interval\": \"\", \"from_location\": \"\"}} Tool Response: {\"calculatedTime\": \"Tuesday 2023-05-30T00:00:00-07:00 Week_number 22 Day_number 150\", \"timezoneLocal\": \"America/Los_Angeles\"} \"add\", \"from_timezone\": \"P70D\", \"operation\": Step 3. The date 70 days from March 21, 2023 is May 30, 2023. Tool Call: {\"name\": from March 21, 2023 is May 30, 2023.\"}} \"response_gen\", \"arguments\": {\"response\": \"The date 70 days Figure 8 Step-by-step reasoning and tool usage under PORTool with Qwen-2.5-7B backbone. While the method generates correct answer, it also adds meaningless year, 2023. From the evaluation agents perspective, this answer is labeled as true. Example III: How many hours to tomorrow G.1 PORTool with Qwen-2.5-7B Query: how many hours to tomorrow Call: {\"name\": Step 1. To determine how many hours are until tomorrow, need to calculate the time difference between now and the start of tomorrow. Tool \"now\", \"timezone1\": \"comparison_operator\": Tool Response: ERROR: invalid format for timestamp1: The value must be in valid ISO 8601 format. Valid templates include: without timezone, YYYY-MM-DDThh:mm:ssZ for UTC time, and YYYY-MM-DDThh:mm:ss+hh:mm / YYYY-MM-DDThh:mm:ss-hh:mm for times with timezone offsets (ahead or behind UTC time). YYYY-MM-DD for date, YYYY-MM-DDThh:mm:ss for local time \"UTC\", \"timestamp2\": \"tomorrow\", \"timezone2\": \"UTC\", \"timestamp_comparator\", \"arguments\": {\"timestamp1\": \">\"}} Call: Step 2. It seems there was an error because the now value was not provided in the correct ISO 8601 format. need the current time in ISO 8601 format to make the comparison. Tool [\"current_time\"]}} Tool Response: {\"current_time\": {\"date_time\": Day_number 270\", \"time_zone\": \"get_current_context\", \"arguments\": {\"requested_context\": \"Saturday 2025-09-27T02:47:20-07:00 Week_number 39 \"America/Los_Angeles\"}} {\"name\": <think> have the current time in ISO 8601 format now. can 3. Step calculate the time difference between now and the start of tomorrow. :UIControlStateNormal [ {\"name\": {\"from_timezone\": \"America/Los_Angeles\", \"from_location\": \"from_timestamp\": \"2025-09-27T02:47:20-07:00\", \"interval\": \"add\"}} ] </tool_call> Tool Response: ERROR: Invalid output format. </think> field; and (ii) the <tool_call> ... valid output must include both: </tool_call> field. </think> \"Los Angeles\", \"P1D\", \"operation\": \"timestamp_interval_calculator\", \"arguments\": (i) the <think> ... Call: Step 4. Oops, it seems there was an error with the tool execution. Let me try calculating the hours to tomorrow directly using the correct format. Tool \"2025-09-27T02:47:20-07:00\", \"timezone1\": \"timestamp2\": \"2025-09-28T00:00:00-07:00\", \"timezone2\": \"comparison_operator\": Tool Response: {\"comparisonResult\": true, \"timeDifference\": \"21 hours, 12 minutes, 40 seconds\"} \"timestamp_comparator\", \"arguments\": \"America/Los_Angeles\", \"America/Los_Angeles\", {\"timestamp1\": {\"name\": \"<\"}} Step 5. There are 21 hours and 12 minutes and 40 seconds until tomorrow. Tool Call: \"response_gen\", \"arguments\": {\"response\": and 12 minutes and 40 seconds until tomorrow.\"}} {\"name\": \"There are 21 hours Figure 9 Step-by-step reasoning and tool usage under PORTool with the Qwen-2.5-7B backbone. 26 G.2 PORTool with Qwen-3-1.7B Query: how many hours until tomorrow Step 1. Okay, the user is asking, \"how many hours until tomorrow.\" Let me think about how to approach this. First, need to determine the current time to calculate the time until midnight. The user mentioned \"tomorrow,\" so should check the current date and time. Since the users location isnt specified, Ill assume theyre in the same time zone as the system. Next, need to figure out the time difference between now and midnight of the next day. If its already past midnight, the answer would be 0 hours. If its before midnight, subtract the current time from 24 hours. Wait, the tools available include weather_search, which can provide current weather conditions. But the users question is about time, not weather. However, the weather_search tool might have parameter for start_datetime or end_datetime. But the user isnt asking for weather data; they want the time until tomorrow. Hmm, maybe the user just wants the time until midnight tomorrow. Since the exact time isnt provided, need to assume the current time. But the tools dont have function to get the current time directly. The get_current_context tool can retrieve current location and time. Wait, the get_current_context function can retrieve current time. So should use that to get the current time. Then calculate the time until midnight tomorrow. Let me outline the steps: 1. Use get_current_context to get the current time. 2. Calculate the time until midnight of the next day. 3. Return the result. But the users question is straightforward. If the current time is, say, 10:00 AM, then the time until tomorrows midnight would be 14 hours. If its 11:00 PM today, then 13 hours until midnight tomorrow. But since the exact current time isnt provided, need to use the tool to get the current time. Once have that, can compute the difference. So the correct approach is to call get_current_context to get the current time, then calculate the hours until tomorrows midnight. The tool will return the current time, and can use that to determine the answer. Tool [\"current_time\"]}} Tool Response: {\"current_time\": {\"date_time\": Day_number 278\", \"time_zone\": \"get_current_context\", \"arguments\": {\"requested_context\": \"Sunday 2025-10-05T16:52:30-07:00 Week_number 40 \"America/Los_Angeles\"}} {\"name\": Call: Step 2. Okay, the user asked, \"how many hours until tomorrow.\" need to figure out the time difference between now and midnight of the next day. First, used the get_current_context tool to get the current time. The response shows the current time is Sunday, October 5, 2025, at 16:52:30 in Los Angeles. The time zone is America/Los_Angeles. Now, need to calculate the time until midnight of tomorrow. Since today is Sunday, tomorrow is Monday. The current time is 4:52 PM. Midnight tomorrow would be 12:00 AM. The time difference is 8 hours (from 4:52 PM to 12:00 AM is 8 hours). But wait, the current time is 4:52 PM, so from 4:52 PM to 12:00 AM is 7 hours and 8 minutes. Wait, let me check again. From 4:52 PM to 12:00 AM is 7 hours and 8 minutes. But the user might want the exact number of hours. So 7 hours and 8 minutes. But the tool might not have the exact time, so need to calculate it based on the current time. Step 2 (cont.). Wait, the current time is 4:52 PM. The time until midnight is 8 hours and 8 minutes. Wait, 4:52 PM to 12:00 AM is 7 hours and 8 minutes. Wait, no. From 4:52 PM to 12:00 AM is 7 hours and 8 minutes. But the exact calculation would be 8 hours if we consider the current time is 4:52 PM, then 12:00 AM is 7 hours and 8 minutes later. So the answer is 7 hours and 8 minutes. But the user might want the answer in hours, so 7.1333 hours, but the tool might not have the exact time. Alternatively, maybe the tool can calculate it, but since the tool only returns the current time, need to compute it manually. But since the users question is straightforward, and the tool gives the current time, can compute the hours until midnight tomorrow. The current time is 4:52 PM, so 8 hours until midnight. Wait, 4:52 PM to 12:00 AM is 7 hours and 8 minutes. So the answer is 7 hours and 8 minutes. But the user might want it in hours, so 7.1333 hours. However, the exact answer depends on the current time. Since the current time is 4:52 PM, the time until midnight is 7 hours and 8 minutes. So the answer is 7 hours and 8 minutes. But the user might prefer it in hours as decimal or rounded. Tool Call: and 8 minutes until midnight tomorrow.\"}} \"response_gen\", \"arguments\": {\"response\": \"There are 7 hours {\"name\": Figure 10 Step-by-step reasoning and tool usage under PORTool with the Qwen-3-1.7B backbone. Example IV: if youre nine years old and in 2030 how old will you be"
        },
        {
            "title": "GRPO\nARPO\nDAPO\nPORTool",
            "content": "If you are currently 9 years old in 2023, you will be 16 years old in 2030. NA NA If youre nine years old in 2025, you will be 14 years old in 2030. Table 5 Generated results across different methods with Qwen-3-1.7B H.1 Qwen-3-1.7B Query: if youre nine years old and in 2030 how old will you be Step 1. <think> Okay, the user is asking, \"if youre nine years old and in 2030 how old will you be?\" Let me break this down. First, the user is probably trying to calculate their age in 2030 based on their current age. They mentioned theyre nine years old now. So, if today is 2023, then in 2030, they would be 9 + (2030 - 2023) = 9 + 7 = 16 years old. But wait, the user didnt specify the current year. Hmm, but since the question is about 2030, maybe theyre assuming the current year is 2023. However, without knowing the exact current year, its tricky. But the user might be using the current year as 2023, so the answer would be 16. But the tools available dont include math calculation for age. The math_calculation tool can handle basic operations. So can use that. The expression would be add(9, 2030 - 2023). But the user might not know the current year. However, since the question is in 2030, the answer is straightforward. The tool can handle it. So the tool call would be math_calculation with the expression \"add(9, 2030 - 2023)\". The result would be 16. Then, the response_gen tool would generate the answer. </think> [ {\"name\": \"math_calculation\", \"arguments\": ]</tool_call> [ {\"name\": \"response_gen\", \"arguments\": {\"query\": 2030 how old will you be\", \"items\": [{\"tool\": 2030 - 2023)\", \"fact\": are 9 years old in 2023, you will be 16 years old in 2030.\"}} ] Tool Response: ERROR: Invalid output format. <think>...</think> field; and (ii) the <tool_call>...</tool_call> field. \"The answer is 16\"}], \"context\": \"\", \"response\": valid output must include both: \"if youre nine years old and in \"math_calculation\", \"name\": \"add(9, 2030 - 2023)\"}} {\"expression\": (i) the \"add(9, \"If you 29 Step 2. <think> Okay, the user provided an error message saying the output format is invalid. Let me check what went wrong. The previous response had JSON array with \"response_gen\" tool call. The error mentioned that the output must include both the \"itionally\" and \"<tool_call>...</tool_call>\" fields. Wait, maybe the formatting wasnt correct. Let me look at the example. In the first example, the user had JSON array with \"response_gen\" object inside. The error might be because the \"response\" field in the \"response_gen\" tool wasnt properly formatted. Also, the \"items\" array might need to be structured correctly. Wait, the users previous attempt had the \"response_gen\" tool with \"items\" containing tool call. But the error might be that the \"response\" field in \"response_gen\" is missing or not properly structured. Also, the \"items\" array might need to have the tool calls properly nested. need to make sure that the \"response_gen\" tools \"response\" field is string and that the \"items\" array contains the necessary tool calls. Let me reformat the response to comply with the required structure. </think> [ {\"name\": \"response_gen\", \"arguments\": {\"response\": 2023, you will be 16 years old in 2030.\", \"query\": 2030 how old will you be\", \"context\": \"name\": Tool Response: ERROR: Invalid output format. <think>...</think> field; and (ii) the <tool_call>...</tool_call> field. valid output must include both: \"if youre nine years old and in \"add(9, 2030 - 2023)\", \"fact\": \"If you are 9 years old in \"The answer is 16\"}]}]} ] \"math_calculation\", \"\", \"items\": [{\"tool\": (i) the Figure 11 An example of Qwen-3-1.7B. In its second step, the model cannot identify the formatting error. H.2 GRPO Query: if youre nine years old and in 2030 how old will you be Step 1. Okay, the user is asking, \"if youre nine years old and in 2030 how old will you be?\" Let me break this down. First, the user is probably trying to calculate their age in 2030 based on their current age. They mentioned theyre nine years old now. So, if today is 2023, then in 2030, they would be 9 + 7 = 16 years old. But the user hasnt specified the current year, so need to consider the current date. Wait, but the tools available dont include way to calculate age based on current date. The tools like weather_search, stocks_search, etc., are specific to their functions. Since theres no tool for date calculations, cant use any of the provided functions here. The users question is simple arithmetic problem that doesnt require any of the tools. So, should answer directly without using any tools. But according to the instructions, must use the tools if possible. However, since the question is straightforward and doesnt require external data, cant invoke any tools. Therefore, need to respond with the answer based on the current date, but since the current date isnt provided, cant calculate it precisely. But the user might assume the current year is 2023, so 2023 + 7 = 2030, making the age 16. However, without knowing the exact current date, this is an assumption. But the tools dont support this, so the answer must be given directly. Tool Call: {\"name\": 9 years old in 2023, you will be 16 years old in 2030.\"}} \"response_gen\", \"arguments\": {\"response\": \"If you are currently Figure 12 Step-by-step reasoning and tool usage under GRPO. The generated answer is incorrect because it identifies the current year as 2023. 30 H.3 PORTool Query: if youre nine years old and in 2030 how old will you be Step 1. Okay, the user is asking, \"if youre nine years old and in 2030 how old will you be?\" Let me break this down. First, the user is probably asking about their own age in 2030, but they mentioned \"you\" which might be typo. But assuming they mean their age, the answer is straightforward. If theyre nine now, in 2030 theyll be nine plus the number of years until 2030. But wait, the current year is 2023, right? So from 2023 to 2030 is 7 years. So 9 + 7 = 16. But the user might not know the current year. However, the tools available dont include way to calculate age based on current date. The tools like math_calculation can handle arithmetic, but the users question is simple addition. Since the current year isnt provided, cant calculate it accurately. But the user might expect simple answer. However, the tools require specific parameters. Since the users question is straightforward math problem, can use math_calculation with the expression \"9 + 7\" if the current year is 2023. But the tool needs the current year. Since dont have that, maybe should inform the user that the current year is 2023 and the answer is 16. But the tools dont have way to get the current year. Wait, the get_current_context tool can retrieve current location and time. If call that, can get the current year. Let me check the available tools again. The get_current_context tool can retrieve current_location and current_time. So, should use that to find the current year. Then calculate 2023 + 7 = 2030, so 9 + 7 = 16. But the user might not know the current year. However, the tools cant provide that info unless the users location is known. Since the user hasnt provided their location, cant get the current year. Therefore, need to inform the user that the current year is 2023 and the answer is 16. But the tools dont have way to get the current year unless the users location is known. So, the correct approach is to use get_current_context to find the current year, then compute the age. But since the users question is simple math problem, maybe the answer is 16, but need to use the tools to get the current year first. However, the tools might not have the current year. So, Ill proceed to use get_current_context to find the current year, then calculate the age. Tool [\"current_time\"]}} Tool Response: {\"current_time\": {\"date_time\": Day_number 278\", \"time_zone\": \"get_current_context\", \"arguments\": {\"requested_context\": \"Sunday 2025-10-05T18:28:16-07:00 Week_number 40 \"America/Los_Angeles\"}} {\"name\": Call: Step 2. Okay, the user asked, \"if youre nine years old and in 2030 how old will you be?\" Let me think. The current time is October 5, 2025, according to the tool response. So, if today is 2025, then in 2030, the user would be 9 + 5 = 14 years old. Wait, but the user said \"in 2030,\" so from 2025 to 2030 is 5 years. So 9 + 5 = 14. But the user might have meant their own age, but they said \"you\" which could be typo. However, the tools dont have way to calculate age unless the current year is known. The get_current_context tool was used to find the current time, which is 2025. So the answer is 14. But the user might not know the current year, so need to explain that the current year is 2025 and the answer is 14. But the tools cant provide the current year unless the users location is known. Since the tool response shows the current time as 2025, can use that to calculate the age. So the final answer is 14. Tool Call: {\"name\": \"If youre nine years old in 2025, you will be 14 years old in 2030.\"}} \"response_gen\", \"arguments\": {\"response\": Figure 13 Step-by-step reasoning and tool usage under PORTool with Qwen-3-1.7B backbone."
        }
    ],
    "affiliations": [
        "Apple",
        "Purdue University"
    ]
}