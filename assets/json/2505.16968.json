{
    "paper_title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
    "authors": [
        "Ahmed Heakl",
        "Sarim Hashmi",
        "Gustavo Bertolo Stahl",
        "Seung Hun Eddie Han",
        "Salman Khan",
        "Abdulrahman Mahmoud"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA $\\leftrightarrow$ HIP) and assembly-level (Nvidia SASS $\\leftrightarrow$ AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on \\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}}, with code at \\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 2 8 6 9 6 1 . 5 0 5 2 : r CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark Ahmed Heakl1 Sarim Hashmi1 Gustavo Bertolo Stahl1 Seung Hun Eddie Han1 Salman Khan1,2 Abdulrahman Mahmoud1 1MBZUAI 2Australian National University https://github.com/GustavoStahl/CASS (cid:18) https://huggingface.co/datasets/MBZUAI/cass"
        },
        {
            "title": "Abstract",
            "content": "We introduce CASS, the first large-scale dataset and model suite for crossarchitecture GPU code transpilation, targeting both source-level (CUDA HIP) and assembly-level (Nvidia SASS AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on HuggingFace, with code at GitHub."
        },
        {
            "title": "Introduction",
            "content": "Graphics Processing Units (GPUs) are foundational to modern machine learning and scientific computing workloads due to their high-throughput parallelism. Nvidias Compute Unified Device Architecture (CUDA) [1] has become the dominant programming model for GPU acceleration, but its tight coupling to proprietary hardware introduces severe vendor lock-in: CUDA code cannot run on non-Nvidia GPUs due to incompatible instruction set architectures (ISAs) [2]. As result, organizations with large CUDA-based codebases face steep engineering costs when migrating to alternative platforms. Meanwhile, AMD GPUs, offering potential favorable performance-perdollar [3, 4], are increasingly adopted across both data centers and consumer devices [5], creating growing need to execute legacy CUDA programs on AMD hardware without full rewrites in software [6]. In response, AMD introduced the Heterogeneous-computing Interface for Portability (HIP) [7], C++ GPU API built into the ROCm stack [8], designed to mirror CUDAs functionality while supporting cross-platform development. HIP enables unified codebase for both Nvidia and AMD GPUs. Tools like HIPIFY [9], static translator, assist migration by converting CUDA-specific constructs into Equal contribution. Preprint. their HIP equivalents, streamlining adoption of the ROCm stack. However, HIPIFY only operates at the source level and cannot execute precompiled CUDA binaries. Furthermore, it exhibits high failure rate when converting CUDA programs, highlighting the need for more reliable and lower-level transpilation approaches [10]. Translating GPU assembly across vendors is hindered by divergent ISAs and compilation pipelines. Nvidia employs proprietary toolchain centered on nvcc, producing PTX and low-level SASS [1], while AMD uses GCN/RDNA architectures compiled via the open-source ROCm stack using hipcc [8] (Figure 2 provides detailed breakdown of the alternative stacks). Bridging this gap at the assembly level is critical for democratizing the hardware computing landscape, transfer of hardware-specific optimizations across vendors, and enabling automation beyond source-level rewrites, especially for legacy CUDA codebases rich in low-level tuning. Our model introduces the first foundation for Nvidia-to-AMD assembly and source translation, focusing on correctness and alignment. While not optimization-aware yet, it paves the way for future systems that preserve and adapt performance-critical patterns across GPU backends. To address the lack of cross-architecture GPU translation datasets, we introduce CASS (CUDAAMD ASsembly and Source Mapping), large-scale corpus of 70k semantically aligned CUDAHIP source pairs and their corresponding host (CPU x86 ISA) and device (GPU) assemblies for Nvidia (SASS) and AMD (RDNA3) platforms. Each sample comprises functionally equivalent low-level code across vendors, verified through successful compilation and execution, enabling instruction-level analysis across execution boundaries. Unlike generic code corpora like The Stack [11], which lack GPU-aligned and compilable content, CASS provides complete source and binary representations across both GPU compute stacks. To construct CASS, we developed fully open-source pipeline that scrapes, synthesizes, translates (via HIPIFY [9]), compiles, and aligns GPU code. We evaluate CASS along two dimensions: (1) instruction coverage, capturing diverse SASS and RDNA3 opcodes; and (2) domain coverage, spanning real-world compute kernels from ML, graphics, and HPC. CASS is the first dataset to enable sourceand assembly-level translation research for GPU architectures. To validate the utility of our dataset, we introduce the CASS model family, suite of domain-specific large language models fine-tuned for both source and assembly-level GPU code translation. These models are trained on our curated corpus and demonstrate significant improvements over SoTA proprietary systems such as GPT-4o [12], Claude-3.7 [13], and traditional tools like HIPIFY [9]achieving 95% accuracy in source-level translation and 37.5% in assembly translation. To ensure rigorous evaluation, we further contribute CASS-Bench, the first benchmark tailored to cross-architecture GPU transpilation. It spans 16 diverse GPU domains with execution-verified source and assembly pairs, providing standardized testbed for future work in low-level translation and performance-aware code generation. Our contributions are summarized as follows: CASS Dataset. We introduce CASS, the first large-scale dataset for GPU transpilation, containing 70k semantically aligned Nvidia AMD pairs at both the source (CUDA HIP) and assembly levels (SASS RDNA3), covering 16 real-world GPU domains. CASS-Bench. We contribute the first evaluation benchmark for cross-architecture GPU translation, with 40 curated tasks across 16 domains, including functionally verified outputs and aligned CUDA/HIP source and SASS/RDNA3 assembly. CASS Models. We release domain-specialized CASS LLMs trained for cross-architecture code translation. Our 7B model achieves 95% source and 37.5% assembly accuracy, outperforming GPT-4o and Claude (0%) on CASS-Bench. Crucially, 85% of translated assemblies preserve execution runtime and memory compared to native, confirming semantic and performance fidelity. CASS Dataset Pipeline. We designed scalable pipeline for scraping, synthesizing, transpiling, and compiling CUDA/HIP code into aligned host and device assemblies across Nvidia and AMD GPUs. The rest of the paper is organized as follows: 2 reviews prior work on Nvidia-to-AMD and assembly translation. 3 describes our data collection, conversion, and filtering pipeline. 4 analyzes dataset structure and coverage. 5 outlines model training and evaluation, with results and ablations in 6. Finally, 7 lists limitations and future work, followed by 8 concluding remarks. 2 Table 1: Comparison of Domain/Characteristics across Different Datasets Babel Stream[23] ComputeEval NVIDIA[19] Rodinia Bench[20] Poly Bench[22] SHOC [21] Domain/ Characteristics CUDA (source) SASS (assembly) RDNA3 (assembly) OpenCL (source) Ours"
        },
        {
            "title": "2 Related Works",
            "content": "In this section, we describe prior work in GPU translation efforts (2.1), assembly-level transpilation (2.2), and related benchmarks (and their shortcomings) in the space (2.3) 2.1 Translating from Nvidia to AMD The fragmentation of GPU software ecosystems has driven the need for robust CUDA-to-HIP translation tools. HIPIFY [14] statically converts CUDA source code into HIP, enabling ROCm compatibility via direct syntax substitution. Operating at lower abstraction, CuPBoP-AMD [15] translates NVVM IR to HIP-compatible LLVM IR using the LLVM toolchain [16, 17], offering more flexible intermediate-level interoperability. Earlier, GPU Ocelot [18] explored dynamic binary translation, recompiling CUDA to AMD/x86 ISAs at runtime. Although innovative, it was limited by poor scalability and high overhead, making it impractical for modern GPU workloads. All these tools have lacked consistent updates to keep up with CUDA advances, suffer from usability issues, and operate only at the source level. More recently, ZLUDA [6] introduced runtime system for executing unmodified CUDA binaries on AMD GPUs without source access by intercepting CUDA APIs and translating PTX/SASS into AMD-compatible code via LLVM. Originally targeting Intel, it now supports AMD RDNA3 through runtime patching. ZLUDA operates at the LLVM IR level rather than the hardware assembly. While reasonable level in the stack to target, ZLUDA would not be able to benefit from low-level, backend Nvidia optimizations (operating below the PTX level), and is limited to the AMD stacks backend optimizations. In our work, we target assembly-to-assembly translation, in an effort to leverage hardware-specific optimizations below the intermediate representation (IR) level, that may be missing altogether in the corresponding AMD codebase. 2.2 Assembly-to-Assembly Translation Translating assembly across ISAs is challenging due to divergent instruction sets and execution models. Recent work employs language models for this task, including CRT [24], lightweight transpiler from x86 assembly (CISC) to ARM (RISC), and Guess & Sketch [25], which integrates language models with symbolic reasoning to translate between ARMv8 and RISC-V. These recent successes open the door for assembly-to-assembly translation in the unexplored GPU-to-GPU space. key contributing factor to their success is the large CPU-centric dataset enabling training from one ISA to another. Given the lack of such rich dataset in the GPU space, primary goal of this work is to enable such an exploration and transpilation across GPU vendors, democratizing compute in the critical GPU and ML-acceleration landscape, where Nvidia/CUDA currently dominate the market. 2.3 Datasets and Benchmarks for CUDA and HIP As shown in table 1, existing benchmarks in the GPU space generally focus on runtime performance, do none target the assembly level, and do not have paired/aligned data across Nvidia/AMD codebases. ComputeEval [19] includes only CUDA code for hardware evaluation. Rodinia [20] and SHOC [21] provide heterogeneous benchmarks using CUDA/OpenCL/OpenMP but omit AMD code and assembly. PolyBench [22] evaluates compilers with CUDA/OpenCL kernels, yet lacks assembly-level or AMD support. BabelStream [23] benchmarks HIP/CUDA/OpenCL memory bandwidth but excludes assembly and domain diversity. Hetero-Mark [26] targets CPUGPU workloads where GPU code is minimal. The Stack [11, 27] dataset nearly 200k CUDA files but no AMD coverage or aligned 3 Figure 1: CASS Pipeline: We collect CUDA code from public repositories and synthesize additional samples via prompt-based LLM generation. After filtering and deduplication, all CUDA files are translated to HIP using HIPIFY, then compiled to extract host and device assembly. Matched outputs form the CASS dataset with aligned source and assembly pairs across Nvidia and AMD stacks. assembly. In contrast, CASS uniquely offers 70k semantically aligned CUDAHIP source and SASSRDNA3 assembly pairs across both host and device, enabling instruction-level analysis and forming the first dataset purpose-built for cross-vendor GPU assembly translation. To the best of our knowledge, no existing dataset provides paired sourceand assembly-level NvidiaAMD code, hindering effective training and benchmarking."
        },
        {
            "title": "3 Methods",
            "content": "This section outlines the end-to-end methodology behind CASS-Instruct, including data collection, code conversion, and compilation for Nvidia and AMD GPUs. We built the low-level assembly corpus from high-level CUDA code using three strategies: scraping public repositories, generating synthetic samples, and applying targeted code generation frameworks. 3.1 CUDA Code Scraping We leveraged the Stackv2 dataset [27] to extract CUDA source files. This dataset, curated from vast array of public code repositories, offers deduplicated and license-compliant samples, facilitating the assembly of diverse corpus of GPU-oriented code. To maximize the number of compiled files in the later stage, we used the datasets metadata to identify and download the top 200 repositories with the highest number of CUDA files. This repository-level download preserved the original directory structure and relative imports, as shown in Figure 1, and improved compilation success by 23.7% compared to isolated file scraping. After extraction, we applied additional filtering to remove overly long files (> 7k lines), trivially short files (<10 lines), naive boilerplate samples (e.g., Hello World), and files lacking CUDA kernel definitions. This process resulted in final set of 24k usable samples. 3.2 Synthetic Data Generation We employed coding-oriented large language model (Qwen2.5-Coder32B) to synthesize CUDA kernel code using our variable-augmented persona strategy. The process begins by defining set of natural language prompt templates with variable placeholders. For example, template might read: Generate CUDA kernel for cloth simulation with {size}X{size} grid. Optimize for {optimization}. To fill these templates, we prepared predefined lists of variable values. For instance, {size} was instantiated with values such as 32, 64, and 128, while {optimization} was sampled from options like \"memory bandwidth\", \"register usage\", and \"multi-GPU scaling\". This allowed us to systematically generate broad range of prompts, each specifying different values for the placeholders in the templates. Refer to the Appendix A.5 for full details. These prompts were then passed to the LLM, which generated CUDA source files accordingly. While this method introduced some functional inconsistencies that required significant post-generation 4 filtering (syntactic errors, missing definitions, or invalid memory operations), it enabled the creation of rich and diverse CUDA samples. In total, we generated 85k CUDA samples, of which 49.1% compiled successfully, yielding final set of 46.3k valid files. 3.3 Transpilation and Compilation After collecting CUDA files from the previous stages, we performed deduplication to ensure all samples are unique in our dataset. We then used AMDs Hipify tool [9] to convert CUDA source files by replacing CUDA-specific API calls with HIP equivalents. Files that failed conversion (approx. 43.9%) were discarded. Once CUDAHIP pairs were available, we compiled them to host and device assemblies using -Os compilation flag to reduce code size, achieving 9.3% average token reduction compared to O3. Given the architectural divergence of the two stacks (see figure 2), their compilation pipelines differed substantially, requiring significant effort to engineer and standardize our described workflow. In figure 2, key distinction between the CUDA and HIP compilation pipelines lies in how they manage host and device assembly separation. In ROCm, the device binary is typically embedded into the host binary during the BitCode-to-assembly transition. We modified this behavior by deferring insertion until after host assembly was converted to object code, enabling: (1) independent extraction of pure host and device assemblies, and (2) selective recombination for controlled translation and evaluation. Conversely, Nvidia provides no access to its binary injection process, device and host assemblies remain intertwined, with no official method for extraction or reintegration [28]. Since our goal was to support host-to-host and device-to-device transpilation, recombination on the CUDA side was unnecessary. Instead, we developed regex-based filtering pipeline to disentangle host and device assembly sections during CUDA compilation. After compiling both stacks to SASS and RDNA3, we retained only samples that compiled successfully on both Nvidia and AMD pipelines, accounting for asymmetric failures. The final dataset includes matched CUDAHIP source pairs, SASSRDNA3 device assemblies, and host assemblies. We got 64k samples from these steps. 3.4 OpenCL Pipeline OpenCL stands as an independent pipeline in generating Nvidia to AMD mapping datasets outside of the CUDA/HIP framework. In other words, it alsos compiling down to the assembly level without going through the aforementioned stacks, operating as single source\" for GPU code deveolpment [29]. Approximately 6k OpenCL code snippets were collected from the Stack dataset and compiled down to the device assemblies. On the Nvidia stack, wrapper C++ function was used to encapsulate the clBuildProgram library provided by OpenCL [30] and convert them into PTX, after which the CUDA stack was used to compile them down to assemblies. On the AMD stack, clang was used to directly transpile the OpenCL files into device assemblies whilst forcing it to emit intermediate LLVM during this process [17]. In total, these pipelines produced 70k aligned assembly samples, with the final distribution detailed in Table 2. All compilations were performed on an Nvidia A100 PCIe machine for the CUDA stack (SASS sm85 ISA) and on AMD Radeon RX 7900 XT GPUs (RDNA3 ISA) for the AMD stack."
        },
        {
            "title": "4 CASS-Instruct and CASS-Bench Datasets",
            "content": "The final instruction training dataset (CASS-Instruct) comprises 70,694 samples spanning broad range of domains as seen in Figure 3, with primary focus on GPU compute and GPU-related data structures. The dataset also includes corresponding CUDA and HIP source code alongside their compiled assembly representations. All samples have been verified to compile successfully and have pairwise source/assembly alignments. 5 Figure 2: The Nvidia (left) and AMD (right) stacks illustrate the compilation process for CUDA and HIP. Blue denotes device-side steps; green denotes host-side steps. Nvidias stack is opaque; accessing device assembly (SASS) requires first compiling to binary, then using cuobjdump. In contrast, AMDs process is transparent, allowing direct inspection and modification of device assembly (RDNA3) before host integration. Figure 3: CASS coverage across dataset and benchmark (left) domain distribution of training samples (right) category distribution in CASS-Bench. 4.1 Dataset Analysis CASS reveals pronounced structural divergence between CUDA and HIP at both source and assembly levels, underscoring the inherent complexity of cross-architecture GPU transpilation. We analyze this by looking at the length of the assembly files, their syntactic similarity, and opcode diversity. Length of Assembly Files. Figure 4 (left) shows that AMD device assembly is, on average, twice as long as Nvidias in both synthetic and Stack subsets, while Nvidias device assembly exceeds HIP device assembly by 50% in the OpenCL set. We found an exponential relationship between source complexity and assembly size, with CUDA producing more verbose outputs than HIP for equivalent code. This highlights the growing difficulty of assembly-level translation as code complexity scales. See appendix A.4.1 for full details. 6 (a) (b) Figure 4: Comparison of structural and syntactic patterns in CASS: (a) verbosity across subsets and backends; (b) syntactic similarity of translated code. Code Efficiency and Analysis. Assembly accuracy varies across domains 0% in math, data structures, and graph tasks, 2550% in linear algebra and memory operations, and up to 100% in physics simulationshighlighting the challenge of preserving low-level semantics. Despite this, the translated code closely matches the original in execution: memory usage deviates by less than 0.3%, and execution time stays within 11.8%, with over 85% of samples falling within 5.6% for both metrics. Syntax Similarity. As illustrated in figure 4 (right), the CHRF [31] score indicates that HIP and CUDA assembly exhibit low syntactic similarity for both device and medium similarity to host code, particularly in the OpenCL and Stackv2 subsets. In contrast, the source code translations, especially in the synthetic subset, show high overlap, highlighting that surface-level syntax is better preserved in the source code than in the compiled assembly representations. Table 2: Dataset composition by source and size Opcode Diversity. We noticed that tensor operations dominate both CUDA and HIP assembly, especially in device code, with memory-related instructions such as mov and call appearing most frequently (refer to appendix A.4). Additionally, HIP opcodes like s_mov_b32 and v_add_co_u32 are used extensively reflecting low-level vector and memory operations unique to AMDs ISA, while Nvidia is dominated by its own variant of common instructions such as movq, call, and jmp, with greater host-side integration (refer to appendix A.4). Both stacks share common control and memory ops (e.g., mov, test), but HIP provides finer-grained access to GPU internals, revealing deeper visibility into parallelism. The synthetic subset emphasizes memory-oriented instructions, aligning with LLM-driven template optimizations. Figure 6 further shows t-SNE clusters of opcode embeddings (via BERTCoder), suggesting that despite backend differences, Nvidia and AMD share semantically aligned opcode distributions across device and host levels. Synthetic Stack OpenCL Total 85k 124k 6k 40.k 24k 6k 70k Collected Final Dataset 4.2 CASS-Bench CASS-Bench is 40-sample evaluation suite spanning 16 GPU-centric domains, each represented by 15 curated prompts. For each, we (1) used Claude-3.7 to generate CUDA implementation; (2) compiled and executed on Nvidia hardware to obtain reference outputs; then (3) prompted Claude-3.7 to generate the corresponding AMD code. If outputs mismatched due to compilation errors, formatting differences or random generators variance, the AMD code was regenerated. Only samples with manually verified output equivalence were included. All final NvidiaAMD pairs were processed using our pipeline (Section 3) to extract aligned host and device assembly. Figure 3 (right) shows the category distribution. 7 Table 3: Performance of different models on our CASS-Bench. Bold cells refer to the best results. Source-to-Source Accuracy (%) Assembly Accuracy (%) Model ZLUDA [6] T Hipify [9] L GPT-4o [12] Gemini-2.0-Flash [38] Claude-3.7 [13] Qwen2.5-Coder-32B [32] CASS-1.5B CASS-3B CASS-7B"
        },
        {
            "title": "5 Experiments",
            "content": "2.5% 0% 0% 0% 25.0% 12.5% 20.0% 37.5% 27.5% 87.5% 90.0% 80.0% 90.0% 85.0% 90.0% 92.5% 95.0% We evaluate the CASS dataset by instruction-supervised fine-tuning the Qwen2.5-Coder [32] models at various parameter scales. Two variants are developed: one for assembly translation and another for source translation. We benchmark these models against both proprietary and open-source baselines, including larger-scale systems. Instruction Supervised Finetuning. To ensure that input samples fit within the 16K-token context window of the LLM, we normalized CUDA assembly code by removing redundant whitespace and comments, which reduced token count by roughly 15%. No preprocessing was applied to HIP assembly code due to its sensitivity to whitespace changes. We fine-tuned the Qwen2.5-Coder [32] models at 1.5B, 3B and 7B parameter scales on 4xA100 GPUs, using batch size of 4, gradient accumulation of 32 (effective batch size of 512) and learning rate of 1105. The relatively aggressive learning rate was selected due to the datasets distributional divergence from the models pretraining corpus. Training employed DeepSpeed [33] with optimizer state sharding to maximize hardware efficiency, achieving 98% GPU utilization. Additionally, we incorporated Liger Kernel [34] and Paged Adam optimizer [35] to accelerate training and manage memory more effectively. We utilized LLaMAFactory [36] to implement all of these optimizations. All models were trained with 16K-token context window. At inference time, we applied RoPE [37] extrapolation to support up to 32.7K tokens. Inference was efficient, requiring approximately 56 seconds per 16K-token sample. Evaluation Protocol. For both source and assembly transpilation, the LLM-generated code (HIP source or host/device assembly) was compiled and executed. The resulting outputs were then compared against the ground truth from CASS-Bench to verify functional correctness."
        },
        {
            "title": "6 Results",
            "content": "Assembly-to-Assembly Performance. Table 3 reports CASS-Bench results across LLMs and tools. All baselines, including proprietary and large open models, failed with 0% accuracy, except Qwen2.5Coder-32B, which reached 25% . ZLUDA, runtime-level system, achieved only 2.5% assembly accuracy despite operating directly on compiled binaries which be attributed to its compatibility with RNDA1. In contrast, our CASS models reached up to 37.5%, highlighting that our dataset imparts essential assembly-level knowledge absent from existing tools and models. Source-to-Source Performance. To further validate the utility of the dataset, we also evaluated source transpilation performance as shown in table 3. This task aligns more closely with some of the pretraining objectives of many proprietary models, as reflected in their relatively strong performance (ranging from 80% to 90%). Nonetheless, even the smallest CASS model (1.5B) significantly outperformed all baselines, achieving 90% accuracy. The 7B variant showed an outstanding state-of8 Table 4: Ablation study on the impact of different data types. Experiment Source Accuracy Assembly Accuracy Impact Stack subset [27] +Synthetic +OpenCL [29] +RoPE Extrapolation [37] 87.5% 95.0% 95.0% 95.0% 17.5% 30.0% 32.5% 37.5% - +12.5% +2.5% +5.0% the-art performance of 95% accuracy. Although our CUDA dataset was entirely translated by Hipify and we retained only semantically aligned samples, our model surpassed Hipify by 7.5%. Figure 5: Source and assembly-level accuracy across categories. Figure 6: and HIP assembly embeddings. t-SNE projection of CUDA Ablation Study. Table 4 shows that using only Stack data yields 17.5% assembly accuracy. Adding synthetic data improves it by +12.5%, highlighting its role in learning low-level patterns. OpenCL adds +2.5%, providing complementary coverage, while RoPE extrapolation pushes accuracy to 37.5% by extending context capacity."
        },
        {
            "title": "7 Limitations and Future Work",
            "content": "Despite achieving state-of-the-art assembly transpilation, current performance is inadequate for production due to limited accuracy in complex or underrepresented domains. Expanding category diversity is essential to address this. The dataset currently covers only one host/device pair per vendor (RTX 4090 and RX7900), limiting generalizability across GPU architectures with varying ISAs. Broader architectural representation is needed to support real-world deployment. Finally, dataset size was minimized to fit within 16K-token context windows, excluding many vendor-specific low-level optimizations. Incorporating these will require future models with larger context capacity or more advanced chunking and attention strategies."
        },
        {
            "title": "8 Conclusion",
            "content": "We present CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, encompassing 70k aligned pairs of source and assembly code for both Nvidia and AMD platforms. Our dataset uniquely bridges both source-to-source (CUDA to HIP) and assembly-toassembly (SASS to RDNA3) mappings, addressing critical gap in low-level code portability. To validate its effectiveness, we train the CASS model family, which achieves 95% accuracy in source translation and 37. 5% in assembly translation, substantially outperforming both proprietary and open-source baselines. Furthermore, our transpiled code preserves functional behavior: over 85% of samples match native execution in both memory usage and runtime. We also introduce CASS-Bench, purpose-built evaluation suite spanning 16 GPU-centric domains. All models, data, and benchmarks are released as open-source resources, establishing foundation for future research in compiler tooling, hardware interoperability, and performance-aware code generation."
        },
        {
            "title": "References",
            "content": "[1] Mark Harris. An even easier introduction to cuda. NVIDIA Developer Blog, 2024. [2] NVIDIA Corporation. Turing Compatibility Guide for CUDA Applications, 2021. Version 11.4.2. [3] AMD. Gaming gpu benchmarks. https://www.amd.com/en/products/graphics/ gaming/gaming-benchmarks.html, 2024. Accessed: 2025-05-15. [4] The Verge. the beats amd-radeon-rx-9070-xt-review-benchmarks-price, 2024. 15. price. rx 9070 that review: https://www.theverge.com/gpu-reviews/624423/ 2025-05performance Accessed: xt radeon"
        },
        {
            "title": "Amd",
            "content": "[5] Financial Times. Nvidias rivals take aim at its software dominance. 2024. Accessed: 2025-0514. [6] Andrzej Janik. Zluda: Cuda on non-nvidia gpus. https://github.com/vosen/ZLUDA, 2024. Accessed: 2025-04-28. [7] AMD. HIP: Heterogeneous-computing Interface for Portability. https://github.com/ ROCm-Developer-Tools/HIP, 2024. Accessed: 2025-04-30. [8] Advanced Micro Devices (AMD). Amd rocm 6: Open software platform for gpu computing. Technical report, Advanced Micro Devices, Inc., 2024. [9] Advanced Micro Devices, Inc. HIPIFY Documentation, 2025. Accessed: 2025-04-28. [10] Anwar Hossain Zahid, Ignacio Laguna, and Wei Le. Testing gpu numerics: Finding numerical differences between nvidia and amd gpus. In SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 547557. IEEE, 2024. [11] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. [12] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [13] Anthropic. Claude 3.7 sonnet and claude code, February 2025. Accessed: 2025-05-14. [14] AMD ROCm Documentation. HIP Porting Guide, 2024. Accessed: 2025-01-29. [15] Jun Chen, Xule Zhou, and Hyesoon Kim. Cupbop-amd: Extending cuda to amd platforms. In Proceedings of the SC23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis, pages 10931104, 2023. [16] Chris Lattner and Vikram Adve. LLVM: compilation framework for lifelong program analysis and transformation. In Proceedings of the International Symposium on Code Generation and Optimization, pages 7586, San Jose, CA, USA, 2004. IEEE Computer Society. [17] The Clang Team. Clang: language family frontend for llvm, 2025. Accessed: 2025-01-29. [18] Gregory Diamos, Andrew Kerr, and Sudhakar Yalamanchili. Gpuocelot: dynamic compilation framework for ptx. https://github.com/gtcasl/gpuocelot, 2009. Accessed: 2025-0428. [19] NVIDIA. Computeeval: Evaluating large language models for cuda code generation. https: //github.com/NVIDIA/compute-eval, 2024. Accessed: May 2025. [20] Shuai Che, Michael Boyer, Jiayuan Meng, David Tarjan, Jeremy Sheaffer, Sang-Ha Lee, and Kevin Skadron. Rodinia: benchmark suite for heterogeneous computing. In 2009 IEEE international symposium on workload characterization (IISWC), pages 4454. Ieee, 2009. 10 [21] Anthony Danalis, Gabriel Marin, Collin McCurdy, Jeremy Meredith, Philip Roth, Kyle Spafford, Vinod Tipparaju, and Jeffrey Vetter. The scalable heterogeneous computing (shoc) benchmark suite. In Proceedings of the 3rd workshop on general-purpose computation on graphics processing units, pages 6374, 2010. [22] Scott Grauer-Gray, Lifan Xu, Robert Searles, Sudhee Ayalasomayajula, and John Cavazos. Auto-tuning high-level language targeted to gpu codes. in 2012 innovative parallel computing (inpar). IEEE, Piscataway, NJ, USA, pages 110, 2012. [23] Tom Deakin, James Price, Matt Martineau, and Simon McIntosh-Smith. Gpu-stream v2. 0: Benchmarking the achievable memory bandwidth of many-core processors across diverse parallel programming models. In High Performance Computing: ISC High Performance 2016 International Workshops, ExaComm, E-MuCoCoS, HPC-IODC, IXPUG, IWOPH, PË† 3MA, VHPC, WOPSSS, Frankfurt, Germany, June 1923, 2016, Revised Selected Papers 31, pages 489507. Springer, 2016. [24] Ahmed Heakl, Chaimaa Abi, Rania Hossam, and Abdulrahman Mahmoud. From cisc to risc: language-model guided assembly transpilation. arXiv preprint arXiv:2411.16341, 2024. [25] Celine Lee, Abdulrahman Mahmoud, Michal Kurek, Simone Campanoni, David Brooks, Stephen Chong, Gu-Yeon Wei, and Alexander Rush. Guess & sketch: Language model guided transpilation. arXiv preprint arXiv:2309.14396, 2023. [26] Yifan Sun, Xiang Gong, Amir Kavyan Ziabari, Leiming Yu, Xiangyu Li, Saoni Mukherjee, Carter McCardwell, Alejandro Villegas, and David Kaeli. Hetero-mark, benchmark suite for cpu-gpu collaborative computing. In 2016 IEEE International Symposium on Workload Characterization (IISWC), pages 110. IEEE, 2016. [27] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. [28] NVIDIA Corporation. CUDA Binary Utilities, 2025. https://docs.nvidia.com/cuda/ cuda-binary-utilities/index.html. [29] The Khronos Group. Opencl guide. https://github.com/KhronosGroup/OpenCL-Guide, 2025. Accessed: 2025-05-14. [30] Khronos Group. clbuildprogram - opencl 3.0 reference pages. https://registry.khronos. org/OpenCL/sdk/3.0/docs/man/html/clBuildProgram.html, 2020. Accessed: 202505-14. [31] Maja Popovic. chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the tenth workshop on statistical machine translation, pages 392395, 2015. [32] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [33] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System In optimizations enable training deep learning models with over 100 billion parameters. Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. [34] Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels for llm training. arXiv preprint arXiv:2410.10989, 2024. [35] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [36] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. 11 [37] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [38] Demis Hassabis and Koray Kavukcuoglu. Introducing gemini 2.0: our new ai model for the agentic era, December 2024. Accessed: 2025-05-14."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Evaluation on ZLUDA To assess ZLUDAs ability to execute CUDA code on AMD GPUs, we designed two-track evaluation strategy targeting both source-level and binary-level workflows (the latter being akin to assembly-level translation). In the source-to-source setting, we leveraged access to the original CUDA source files to manually compile them into PTX using nvcc. These PTX files were then ingested by ZLUDA, which translated them into AMD-compatible LLVM IR before lowering them into native executables targeting RDNA3 hardware. In the assembly-to-assembly setting, we instead compiled the CUDA source into complete executable and invoked it directly. ZLUDA intercepted the CUDA runtime calls, dynamically translated the embedded PTX or SASS, and executed the resulting code on the AMD backend. This dual strategy allowed us to assess both ZLUDAs static translation capabilities and its runtime interoperability under realistic execution conditions. A.2 Hardware used All experiments were conducted on two distinct machines to generate architecture-specific outputs. For AMD-related compilation and execution, we used workstation equipped with an Intel i714700KF CPU and an AMD Radeon RX 7900 XT GPU. For NVIDIA-related outputs, we used server with an AMD EPYC 9654 CPU and an Nvidia A100 (80GB) GPU. Although the Nvidia system features high-end data center GPU, all CUDA code was compiled targeting the compute capabilities of standard consumer-grade GPU (e.g., RTX 4090) to maintain parity with the AMD hardware. Furthermore, to ensure consistency and reproducibility across platforms, all file generation was performed within Docker containers tailored to each architecture. Figure 7: Relationship between source and assembly-level LoC in the CASS dataset. Scatter plot comparing source code lines of code (LoC) to the corresponding assembly LoC for both CUDA and HIP backends across the Stackv2 and Synthetic subsets. Trend lines and density contours illustrate that CUDA typically produces more verbose assembly output than HIP for equivalent source sizes. Figure 8: Opcode Category Distribution by Dataset and Code Type. Stacked bar chart showing the distribution of assembly instructions across 10 opcode categories for device and host code in the Synthetic, Stackv2, and OpenCL subsets. Each bar represents (dataset, code type) pair, illustrating the functional composition of the code across memory, tensor, control flow, synchronization, and other operations. (a) HIP assembly opcodes. (b) CUDA assembly opcodes. Figure 9: Most frequent opcodes in HIP and CUDA assembly. Word clouds depicting the most common opcodes in HIP and CUDA assembly files. The size of each opcode reflects its relative frequency in the compiled dataset, highlighting structural and architectural differences between the two backends. Figure 10: Accuracy vs. training steps for source/assembly across CASS model scales (1.5B, 3B, 7B). Figure 11: Comparison of memory usage (left) and execution time (right) between predicted and ground truth HIP programs, measured via compilation and runtime profiling. A.3 CASS Domain Coverage To obtain the domain-level breakdown shown in Figure 3, we developed static analysis pipeline that categorizes each source file based on its content. The classification is performed by matching the files text against curated sets of domain-specific keywords corresponding to seven high-level categories: general compute, simulation, data structure, machine learning, graphics, cryptography, and scientific computing. Each keyword set includes terms commonly associated with the respective domain; for example, the machine learning category includes terms such as neural, gradient, and activation, while cryptography includes hash, encrypt, and signature. For given file, the domain with the highest keyword match count is assigned. If no keywords are matched, default label (e.g., general compute) is applied. After all files are processed, their assignments are aggregated to produce the final domain distribution. This process provides simple yet straightforward and interpretable way of grouping source files by their functional domain. A.4 Extra Data Analysis A.4.1 Length of Assembly Files As shown in the figure 7 We found an exponential relationship between source complexity and assembly size, with CUDA producing more verbose outputs than HIP for equivalent code. This highlights the growing difficulty of assembly-level translation as code complexity scales. A.4.2 Code efficiency As shown figure 5, assembly accuracy is inconsistent, 0% in Math, Data Structures, and Graph, 2550% in linear algebra and memory operations. This reflects the challenge of low-level semantic preservation. Only physics simulation achieves 100%, likely due to simpler or repetitive control flows. As shown in figures 11, the translated code exhibits tight fidelity to the ground truth. Memory usage deviates by less than 0.3% for all files, with 18 files using more memory (max +0.3%) and 22 using less (min 0.3%). Execution time differences are similarly small: 11 files are slower (max +11.8%), 8 are faster (min 10.0%), and the rest are unchanged. Over 85% of samples fall within 5.6% across both metrics, confirming that our model preserves both memory and runtime efficiency during assembly translation. Each test was executed 20 times, and the reported values reflect the average across runs to mitigate noise and ensure statistical reliability. A.4.3 Opcode Diversity Taking deeper dive into the low-level instructions representation shown in figure 9, few extra insights can be drawn. In the HIP case, many opcodes, such as s_mov_b32, v_add_co_u32, and s_waitcnt, come directly from AMDs GPU instruction set. These reflect fine-grained control over the hardware, including scalar and vector operations and synchronization. On the other hand, the CUDA assembly is mostly made up of x86-64 instructions like movq, call, jmp, and pushq, which 15 are typically used on the CPU. This suggests that the CUDA output includes more host-side code or that GPU instructions are hidden behind higher level of abstraction. Still, both stacks share common instructions like mov and test, showing that some basic control and memory operations are similar. In general, HIP provides more visibility into what the GPU is doing, while CUDA hides many of those low-level details behind more unified host-device model. A.5 Sythetic Generation To generate large-scale, diverse CUDA programs, we design multiprocessing Python pipeline that interacts with locally hosted large language model via chat-based API. The pipeline leverages wide array of handcrafted prompt templates, each parameterized with variables such as problem size, optimization target, algorithm type, and architectural features (see Appendix A.5.1). At runtime, these templates are instantiated with randomly sampled values from curated sets covering domains like matrix operations, graph algorithms, scientific computing, machine learning, and sparse computation (see Table 5). Each worker process independently generated prompts, sends them to the model, extracts valid CUDA code from the response, and saves the output in structured format. Robust faulttolerance mechanismsincluding retry logic, output validation, and file existence checksensure resilience to model failures and concurrent access. The system supports parallel generation with controlled API concurrency and automatic resumption from previous checkpoints, enabling scalable and efficient generation of compilable CUDA code samples suitable for downstream benchmarking or training. Table 5: Representative values for prompt placeholders used in the synthetic code generation. Placeholder Example Values 64, 1024, 16384 1, 3, 6 memory coalescing, shared memory usage, warp-level programming sum, histogram, L2 norm matrix multiplication, radix sort, BFS 1, 5, 13 adjacency matrix, CSR, edge list Verlet integration, leapfrog, Runge-Kutta conjugate gradient, Jacobi, multigrid finite difference, spectral, Crank-Nicolson SVD, LU, eigenvalue decomposition 2, 6, 12 64, 512, 2048 CSR, ELL, HYB Barnes-Hut, brute force, particle mesh Gaussian, Sobel, Gabor 3, 7, 15 720p, 1080p, 4K {size} {dimension} {optimization} {operation} {algorithm} {radius} {graph_format} {md_algorithm} {linear_solver} {numerical_method} {factorization_method} {conv_layer_count} {neuron_count} {sparse_format} {nbody_algorithm} {filter_type} {filter_size} {resolution} {segmentation_algorithm} watershed, region growing, U-Net {signal_transform} {optimization_algorithm} Adam, simulated annealing, particle swarm {crypto_algorithm} {cracking_method} {hash_algorithm} {data_structure} {collision_strategy} AES, RSA, Argon2 brute force, dictionary attack, rainbow table SHA-256, BLAKE3, Bcrypt binary tree, hash table, bloom filter linear probing, cuckoo hashing, separate chaining FFT, wavelet, Hilbert 16 A.5.1 Prompt Templates for Synthetic CUDA Code Generation Basic Operations 1. Implement CUDA kernel for {size}D FFT (Fast Fourier Transform). Optimize for { optimization}. 2. Generate CUDA implementation for {size}D stencil computation with radius { radius}. Optimize for {optimization}. 3. Write CUDA kernel for parallel reduction to compute the {operation} of an array of size {size}. Focus on {optimization}. 4. Create CUDA implementation for convolution operation with {size}x{size} filter. Focus on {optimization} optimization. 5. Generate CUDA kernel for matrix multiplication of two matrices and of size {size}x{size}. Include error handling and optimize for {optimization}. Graph Algorithms 1. Write CUDA implementation for graph coloring of graph with {size} nodes. Focus on {optimization}. 2. Implement CUDA kernel for community detection in graph with {size} nodes using the {community_algorithm} algorithm. 3. Implement CUDA kernel for graph processing that computes {algorithm} on graph with {size} nodes. Optimize for {optimization}. 4. Generate CUDA kernel for finding strongly connected components in directed graph with {size} nodes. Optimize for {optimization}. 5. Create CUDA implementation for breadth-first traversal on graph with {size} nodes stored in {graph_format}. Optimize for {optimization}. Scientific Computing 1. Write CUDA implementation for {size}D fluid simulation using {method}. Focus on {optimization}. 2. Create CUDA kernel for Monte Carlo simulation of {size} paths for option pricing. Focus on {optimization}. 3. Implement CUDA solver for {size}x{size} sparse linear system using { linear_solver}. Focus on {optimization}. 4. Generate CUDA implementation for {size}D heat equation solver using { numerical_method}. Optimize for {optimization}. 5. Create CUDA kernel for molecular dynamics simulation of {size} particles using {md_algorithm}. Optimize for {optimization}. Machine Learning 1. Generate CUDA kernel for k-means clustering of {size} data points in {dimension }D space. Optimize for {optimization}. 2. Implement CUDA kernel for {size}x{size} matrix factorization using { factorization_method}. Optimize for {optimization}. 3. Create CUDA implementation for computing attention mechanism in transformer with {size} tokens. Focus on {optimization}. 4. Implement CUDA kernel for backpropagation in convolutional neural network with {conv_layer_count} conv layers. Optimize for {optimization}. 5. Write CUDA implementation for training neural network with {layer_count} layers and {neuron_count} neurons per layer. Focus on {optimization}. 17 Sparse Operations 1. Generate CUDA kernel for sparse FFT computation. Optimize for {optimization}. 2. Implement CUDA kernel for sparse tensor operations with {size} non-zero elements. Optimize for {optimization}. 3. Write CUDA implementation for sparse convolution with {size}x{size} filter on sparse input. Focus on {optimization}. 4. Create CUDA implementation for sparse matrix-matrix multiplication in { sparse_format} format. Focus on {optimization}. 5. Generate CUDA kernel for sparse matrix-vector multiplication where the matrix has approximately {size} non-zero elements. Optimize for {optimization}. Simulation 1. Generate CUDA kernel for cloth simulation with {size}x{size} grid. Optimize for {optimization}. 2. Write CUDA implementation for raytracing of scene with {size} objects. Focus on {optimization}. 3. Create CUDA implementation for {algorithm} of {size} particles in {dimension} space. Focus on {optimization}. 4. Create CUDA implementation for fluid-structure interaction with {size} boundary elements. Focus on {optimization}. 5. Implement CUDA kernel for N-body simulation of {size} particles using { nbody_algorithm}. Optimize for {optimization}. Image and Signal Processing 1. Create CUDA implementation for feature extraction from {size}x{size} images. Focus on {optimization}. 2. Generate CUDA kernel for image segmentation using {segmentation_algorithm}. Optimize for {optimization}. 3. Write CUDA implementation for real-time video processing of {resolution} frames . Focus on {optimization}. 4. Implement CUDA kernel for signal processing with {size}-point {signal_transform }. Optimize for {optimization}. 5. Implement CUDA kernel for image filtering using {filter_type} filter of size { filter_size}x{filter_size}. Optimize for {optimization}. Optimization Algorithms 1. Implement CUDA kernel for simulated annealing with {size} states. Optimize for {optimization}. 2. Generate CUDA kernel for genetic algorithm with population size {size}. Optimize for {optimization}. 3. Write CUDA implementation for {optimization_algorithm} with {size} variables. Focus on {optimization}. 4. Write CUDA implementation for gradient descent optimization with {size} parameters. Focus on {optimization}. 5. Create CUDA implementation for particle swarm optimization with {size} particles in {dimension}D space. Focus on {optimization}. 18 Cryptography and Security 1. Generate CUDA kernel for homomorphic encryption operations. Optimize for { optimization}. 2. Write CUDA implementation for secure hashing using {hash_algorithm}. Focus on { optimization}. 3. Generate CUDA kernel for {crypto_algorithm} encryption/decryption. Optimize for {optimization}. 4. Create CUDA implementation for blockchain mining with difficulty {size}. Focus on {optimization}. 5. Implement CUDA kernel for password cracking using {cracking_method}. Optimize for {optimization}. Data Structures 1. Create CUDA implementation for priority queue with {size} elements. Focus on { optimization}. 2. Create CUDA implementation for {data_structure} with {size} elements. Focus on {optimization}. 3. Implement CUDA kernel for operations on B-tree with {size} nodes. Optimize for {optimization}. 4. Generate CUDA kernel for skip list operations with {size} elements. Optimize for {optimization}. 5. Write CUDA implementation for hash table with {size} buckets using { collision_strategy}. Focus on {optimization}. A.5.2 Qualitative Comparison with Other LLMs We highlight several cases where CASS-7B outperforms existing LLMs such as Claude, QwenCoder, and GPT-4o in faithfully transpiling CUDA to HIP. For example, in one instance, CASS-7B correctly transpiled the CUDA code while preserving the exact string constants from the original program, including the label CUDA in the output format string. Maintaining these strings is essential for preserving the intended user-facing behavior, particularly in logging or debugging scenarios where clarity and consistency matter. In contrast, Claude, Qwen-Coder, and GPT4o unnecessarily altered the string to say HIP, despite the output still originating from CUDA kernel. This substitution introduces semantic error, as the original string refers to CUDA, not HIP, and should remain unchanged. CASS-7B printf(\"tanh(%f) = %f CUDA vs %f (CPU)n\", h_input[idx], h_output[idx], tanh(h_input[idx])); Claude, Qwen-Coder, GPT4o printf(\"tanh(%f) = %f (HIP) vs %f (CPU)n\", h_input[idx], h_output[idx], tanh(h_input[idx])); In another example, CASS-7B retained the classical CUDA-style kernel launch syntax using triple angle brackets (<<<...>>>), while also ensuring that the generated code remained compilable by correctly including the required HIP header <hip/hip_runtime.h>. This demonstrates high degree of structural fidelity to the source code, which is especially important for developers familiar with standard CUDA conventions. In contrast, other models such as Claude and Qwen-Coder replaced the launch expression with the HIP-specific macro hipLaunchKernelGGL, which, while functionally valid, deviates from the original representation. More critically, they failed to include the necessary HIP header, rendering the output uncompilable. This example highlights how CASS-7B goes beyond syntactic accuracy to produce code that is both faithful to the original structure and immediately usable in real compilation setting. 19 CASS-7B #include <hip/hip_runtime.h> #include <iostream> ... add<<<(N + 255) / 256, 256>>>(d_a, d_b, d_c, N); Claude, Qwen-Coder #include <iostream> ... hipLaunchKernelGGL(add, (N + 255) / 256, 256, 0, 0, d_a, d_b, d_c, N); Lastly, when verifying numerical correctness, CASS-7B preserved the original logging behavior by correctly emitting output to std::cout, as in the source code. This choice maintains consistency with the original programs semantics, especially in distinguishing between standard output and error streams; important in contexts where output may be redirected or parsed. In contrast, GPT-4o unnecessarily altered the output stream to std::cerr, which, while syntactically valid, changes the runtime behavior of the program. Such change could lead to unexpected side effects in downstream tools or logging pipelines. This example further demonstrates CASS-7Bs attention to both structural and behavioral fidelity in its translations. CASS-7B std::cout << \"Error at element \" << << \": \" << h_output[I] << \" vs. expected \" << h_reference[i] << std::endl; GPT4o std::cerr << \"Error at element \" << << \": \" << h_output[i] << \" vs expected \" << h_reference[i] << std::endl;"
        }
    ],
    "affiliations": [
        "Australian National University",
        "MBZUAI"
    ]
}