{
    "paper_title": "From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning",
    "authors": [
        "Le Zhuo",
        "Liangbing Zhao",
        "Sayak Paul",
        "Yue Liao",
        "Renrui Zhang",
        "Yi Xin",
        "Peng Gao",
        "Mohamed Elhoseiny",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent text-to-image diffusion models achieve impressive visual quality through extensive scaling of training data and model parameters, yet they often struggle with complex scenes and fine-grained details. Inspired by the self-reflection capabilities emergent in large language models, we propose ReflectionFlow, an inference-time framework enabling diffusion models to iteratively reflect upon and refine their outputs. ReflectionFlow introduces three complementary inference-time scaling axes: (1) noise-level scaling to optimize latent initialization; (2) prompt-level scaling for precise semantic guidance; and most notably, (3) reflection-level scaling, which explicitly provides actionable reflections to iteratively assess and correct previous generations. To facilitate reflection-level scaling, we construct GenRef, a large-scale dataset comprising 1 million triplets, each containing a reflection, a flawed image, and an enhanced image. Leveraging this dataset, we efficiently perform reflection tuning on state-of-the-art diffusion transformer, FLUX.1-dev, by jointly modeling multimodal inputs within a unified framework. Experimental results show that ReflectionFlow significantly outperforms naive noise-level scaling methods, offering a scalable and compute-efficient solution toward higher-quality image synthesis on challenging tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 0 8 0 6 1 . 4 0 5 2 : r From Reflection to Perfection: Scaling Inference-Time Optimization for Text-to-Image Diffusion Models via Reflection Tuning Le Zhuo1, 4*, Liangbing Zhao2*, Sayak Paul3, Yue Liao1, Renrui Zhang1, Yi Xin4 Peng Gao4, Mohamed Elhoseiny2, Hongsheng Li1 1CUHK MMLab, 2KAUST, 3Hugging Face, 4Shanghai AI Lab Project page: https://diffusion-cot.github.io/reflection2perfection Figure 1. Overall pipeline of the ReflectionFlow framework with qualitative and quantitative results of scaling compute at inference time."
        },
        {
            "title": "Abstract",
            "content": "Recent text-to-image diffusion models achieve impressive visual quality through extensive scaling of training data and model parameters, yet they often struggle with complex scenes and fine-grained details. Inspired by the selfreflection capabilities emergent in large language models, we propose ReflectionFlow, an inference-time framework enabling diffusion models to iteratively reflect upon and refine their outputs. ReflectionFlow introduces three complementary inference-time scaling axes: (1) noise-level scaling to optimize latent initialization; (2) prompt-level scaling for precise semantic guidance; and most notably, (3) reflection-level scaling, which explicitly provides actionable reflections to iteratively assess and correct previous generations. To facilitate reflection-level scaling, we construct GenRef, large-scale dataset comprising 1 million triplets, each containing reflection, flawed image, and an enEqual Contribution Corresponding Author hanced image. Leveraging this dataset, we efficiently perform reflection tuning on state-of-the-art diffusion transformer, FLUX.1-dev, by jointly modeling multimodal inputs within unified framework. Experimental results show that ReflectionFlow significantly outperforms naive noise-level scaling methods, offering scalable and compute-efficient solution toward higher-quality image synthesis on challenging tasks. All code, checkpoints, and datasets are available at https://diffusioncot.github.io/ reflection2perfection. 1. Introduction Recent advances in text-to-image (T2I) diffusion models [1, 12, 16, 26, 53, 59, 67] have led to remarkable progress in image synthesis. With the scaling of training resources, their capability to create high-resolution and photorealistic images steadily improves. Despite their success, their performance across various domains remains inconsistent, especially when tasked with generating complex human poses, multiple-object compositions, or scenes with complicated lighting and shadows. To alleviate these limitations, it is necessary to exponentially scale the training compute, model parameters, and data size, according to established scaling laws at training time [22]. This motivates us to rethink the prevailing paradigm: instead of continuously scaling pretrained models, how to effectively exploit the full capabilities of existing diffusion models during inference? We hypothesize that while T2I models may struggle to generate desirable images within fixed compute budget, their performance can be significantly improved through an iterative refinement process. This idea of leveraging additional computational resources to enhance performance during inference has been validated in large language models (LLMs) [48]. Modern LLMs have demonstrated the ability to improve their outputs by reflecting on intermediate outputs and subsequently refining their responses [25, 35, 46], leading to superior performance on complex tasks such as math problem solving and code generation. However, current inference-time optimizations for diffusion models [34, 47, 60] focus on parallelized noisespace search strategies, leaving sequential self-refinement paradigm that could enable diffusion models to reflect upon and correct their own mistakes largely unexplored. In this paper, we introduce ReflectionFlow, novel framework for diffusion inference-time self-refinement models leveraging iterative reflection. Our approach explores three dimensions for scaling inference-time computation: (1) noise-level scaling, which searches for better noise initialization; (2) prompt-level scaling, which optimizes input prompt for precise semantic guidance; and (3) reflection-level scaling, which constructs explicit reflections to iteratively assess and correct previously generated outputs. Integrating these dimensions enables diffusion models to flexibly exploit additional computational resources at inference-time, continuously improving the image quality through an iterative process as illustrated in Fig. 1. fundamental challenge underlying our approach is: how to empower T2I diffusion models with self-refinement capabilities? Currently, no existing diffusion model is able to accurately interpret reflection prompts and leverage previously generated images for iterative refinement. Reflecting on the intrinsic similarity between self-refinement and image editing tasks, we observe that both involve generating new output, jointly conditioned on textual and visual contexts. Inspired by the training paradigm of efficiently adapting diffusion priors to image editing [7, 55], we hypothesize that adopting similar methods could enable diffusion models to achieve self-refinement. However, another critical challenge arises: the absence of dedicated datasets specifically designed for reflection-guided refinement. Motivated by this insight, we propose GenRef, the first large-scale image reflection dataset comprising 1 million reflection triplets in multiple domains. To ensure the validity of these triplets (e.g., flawed images accurately capture potential errors, the reflections offer effective and actionable editing suggestions, and the good images exhibit sufficiently higher quality), we design scalable, fully automatic data construction pipeline with four distinct data sources, leveraging verifiable objectives, ensemble reward models, and diverse rollout strategies. We further create chainof-thought (CoT) reflection subset, named GenRef-CoT, providing 227K high-quality progressive annotations from GPT-4o [21] and Gemini 2.0 [52]. We then employ an efficient reflection tuning tailored for T2I diffusion transformers, such as FLUX.1-dev [26]. In our approach, the original prompt, reflection prompt, flawed image, and high-quality image are concatenated into single unified sequence to perform joint multimodal attention, thereby eliminating the need for additional modules [63, 65]. Note that beyond laying the foundation for reflection tuning, our carefully curated dataset can also serve broader applications, such as general preference tuning [54] and reward modeling [32]. Through experiments, ReflectionFlow demonstrates the potential for enhancing image generation quality without requiring additional large-scale training. Compared to naive noise-level scaling, ReflectionFlow achieves significantly better performance under identical inference budgets, with performance consistently improved using better verifiers. Additionally, we explore the flexible trade-off between search width and reflection depth, as well as overall inference budgets. Further evaluation shows that our approach achieves particularly substantial gains on challenging prompts. Qualitative analyses illustrate how our model iteratively reflects on and corrects its outputs, progressively converging to superior solutions. In summary, (1) we propose ReflectionFlow, reflectionbased inference-time scaling framework for diffusion models, integrating three scaling axes, i.e., noise-, prompt-, and reflection-level; (2) we propose scalable pipeline for collecting high-quality image reflection data and introduce GenRef, the first large-scale dataset containing 1 million triplets, along with an additional 227K CoT reflection annotations; (3) we employ an efficient reflection tuning method for diffusion transformers with tailored training strategy; (4) extensive experiments demonstrate that ReflectionFlow substantially boosts performance via scaling inference-time compute, unlocking reasoning capability. 2. Related Work Text-to-Image Diffusion Models. Text-to-image diffusion models have rapidly advanced in terms of model architecture and training strategies. In terms of architecture evolution, the community has transitioned from the prevalent UNet diffusion models [44] towards diffusion transformers (DiT) [37]. Regarding training strategies, earlier complex 2 hand-designed diffusion schedules [19, 50] have given way to simpler flow-based formulations [30, 33], significantly enhancing training efficiency through techniques such as multi-resolution progressive training [9]. Recently, scaling up both the training datasets and model parameters has led to the emergence of various large-scale, flow-based diffusion transformer models [12, 14, 26, 41, 59, 67]. Diffusion Inference-Time Enhancement. Building upon the powerful pretrained diffusion models, recent research has increasingly focused on unleashing their potential at inference time. One line of research has observed that the initial noise significantly impacts generation quality [40, 66], prompting methods to identify superior initialization strategies [2, 34, 40, 66]. Another research direction aims to improve the iterative sampling procedure of diffusion models [3, 47, 64], notably through denoising and inversion [49]. Additionally, recent studies [28, 59] demonstrate that augmenting input prompts can substantially improve visual fidelity and text-image alignment. While existing methods focus on parallel single-pass generation, our work proposes sequential generation then refinement procedure, integrating both parallel and sequential inference-time scaling into unified framework. Scaling Inference-Time Compute. Recent studies on LLMs have provided valuable insights into inference-time scaling laws. primary line of investigation [13, 62] has explored search algorithms, such as best-of-N and beam search, with verifiers to select higher-quality outputs. Another prominent direction focuses on enabling LLMs to refine their own outputs. For instance, techniques [23, 35, 46] such as zero-shot prompting have been employed to elicit self-reflection from models, enabling them to iteratively enhance their outputs. Furthermore, supervised fine-tuning (SFT) and reinforcement learning (RL) approaches [11, 17, 25, 42, 56] have also been introduced to explicitly train models for reflective self-improvement. One recent work [16] investigated RL, test-time scaling, and reflection on autoregressive image generation models, providing preliminary insights into this field. Meta also presented comprehensive framework [48] that systematically unifies these directions, which thoroughly investigates the tradeoffs between the pretraining scale and the computation of inference time, significantly inspiring our approach. 3. Method 3.1. Problem Formulation We first formalize the iterative refinement framework for the text-to-image (T2I) generation task. Given textual prompt and T2I generator Gθ, we initially generate an image x0 Gθ( z, y) from random noise (0, I). Subsequently, during each refinement iteration i, we introduce corrector model Cϕ to produce an improved image xi Cϕ( z, y, xi1, ri1), conditioned on the previous iterations image xi1, the original textual prompt y, and textual reflection ri1 prompt describing previous shortcomings and improvement directions. Specifically, we utilize external evaluation modules such as reward models [18, 61] and multimodal large language models (MLLMs) [4, 21] to assess the quality of generated image at iteration 1 and produce an instructive reflection ri1 R( xi1, y). Through this iterative refinement process, the original T2I generation task is reformulated into sequential generationand-refinement paradigm, expressed mathematically as: Gθ(xN z, y) = Gθ(x0 z, y) (cid:89) Cϕ(xi z, y, xi1, ri1). i=1 (1) central challenge in this iterative refinement approach lies in effectively training the corrector model Cϕ to recognize and rectify its own errors based on textual reflections. Inspired by recent advances [7, 55] in image editing, we observe that the objective of our corrector Cϕ defined in Equation 1 closely resembles the general image editing problem, i.e., transforming an input image into desired target based on textual instructions. Thus, we conceptualize the self-correction task, guided by textual reflections, as generalized editing problem. By constructing dedicated reflection dataset (Section 3.2) and performing efficient reflection tuning (Section 3.3), we enable foundational T2I diffusion model to effectively learn to refine and iteratively improve its own generations (Section 3.4). 3.2. Reflection Dataset The key to enabling T2I diffusion model to learn selfrefinement lies in constructing an appropriate dataset. However, there is currently lack of suitable open-source datasets specifically curated for image refinement tasks guided by textual feedback. To bridge this gap, we introduce GenRef, the first large-scale T2I refinement dataset comprising 1 million triplets of the form (flawed image, high-quality image, reflection) collected across multiple domains using our scalable pipeline, as illustrated in Fig. 2. Additionally, we gather 227K chain-of-thought (CoT) image reflections annotated by closed-source APIs [21, 52], which provide detailed pairwise analyses, preference annotations, and reflections. These annotations form the foundation for training dedicated MLLM verifier capable of reward modeling and reflection generation, thereby being beneficial beyond the task of T2I self-refinement. We provide samples of our datasets in Appendix A. Principles. Inspired by recent advances in self-refinement dataset construction from the LLMs literature [38], we first establish several guiding principles that our dataset must satisfy: (1) the flawed images should comprehensively Figure 2. Construction pipelines and statistics of our GenRef dataset. We collect our reflection triplets (flawed images, enhanced images, textual reflections) from four distinct data sources, including: rule-based data, reward-based data, long-short prompt data, and editing data. cover common errors encountered by the generator Gθ during inference, (2) the high-quality images should clearly exhibit substantial quality improvements relative to their corresponding flawed images evaluated in diverse aspects, and (3) the textual reflections should accurately describe the observed shortcomings and provide actionable guidance for refinement. Guided by these principles, we develop an automated and scalable pipeline for constructing the dataset, spanning four distinct domains below. Rule-based Data. Recent studies, such as DeepseekR1 [11], have demonstrated that high-quality, rigorously verified datasets constructed via rule-based verifiers sigInspired by these nificantly enhance model capabilities. methodologies, we first employ GPT-4o [21] to brainstorm diverse list of common object categories along with their associated attributes, such as colors and spatial positions. We subsequently composite these objects and attributes using rule-based methods to construct unique textual prompts. We rigorously filter these prompts to guarantee there is no overlap with test samples from existing benchmarks [15]. Next, we utilize FLUX.1-dev [26] to generate 10 candidate images per prompt. Each candidate is verified by Grounded SAM [43] to detect and localize individual objects, subsequently evaluating attributes such as color and quantity with respect to the provided prompt. Based on these evaluations, each image is assigned either binary correctness label or continuous correctness score ranging between 0 and 1, along with explicit identification of the reasons for any detected errors. Afterward, we estimate the difficulty of each prompt defined by the fraction of correct samples. We then bin the difficulty into three quantiles, where prompts yielding predominantly correct or incorrect images, indicating insufficient or excessive difficulty, are discarded. For the remaining challenging prompts, we constructed image refinement pairs by randomly pairing the highest-scoring images with the lowest-scoring images. Reward-based Data. While the rule-based triplets primarily emphasize object-centric text-image alignment, aesthetic quality, visual fidelity, and alignment with general user prompts are equally essential for universal image refinement model. To address this, we collect diverse set of general-purpose prompts, including both image captions and real user-generated prompts. For each prompt, we generate 10 candidate images and assess each generated image based on an ensemble scoring strategy utilizing multiple reward models, e.g., HPSv2 score [57], CLIP score [18], and PickScore [24], to comprehensively evaluate their overall quality and alignment with the provided prompts. We con4 cinations as illustrated in Fig. 3. Therefore, we leverage closed-source model APIs [21, 52] and design CoT image reflection annotation pipeline, enabling the models to step-by-step analyze image pairs. Specifically, we concatenate two images into an image grid and provide it as input together with the prompt. The model first identifies key differences between the two images, then makes judgment regarding image preference, and finally produces concise reflection instructing how to correct the identified flaws in the lower-quality image. The detailed CoT prompts are provided in Fig. 20 in the Appendix. Through this CoT-based annotation approach, we observe significant improvement in the accuracy and reliability of generated reflections. Moreover, intermediate results from the reasoning process, such as the explicit image preferences, can also serve as valuable annotations for reward model training, as discussed in the next paragraph. We annotate approximately 270K CoT reflections using this annotation pipeline, and after careful filtering and quality control, we obtain final dataset of 227K high-quality CoT reflections, named GenRef-CoT. Subsequently, we finetune the Qwen2.5-VL-7B model on this curated reflection dataset, enabling it to annotate the full dataset comprehensively1. Fig. 3 illustrates qualitative comparison among reflections generated by the original Qwen2.5-VL-7B, our fine-tuned reflector, and GPT-4o, clearly demonstrating that our fine-tuned model produces substantially improved and more accurate reflections compared to the original model. We also visualize the word cloud of reflections in Fig. 2, which shows the pattern of executable instructions. Verifier Post-processing. To further ensure the quality of our dataset, particularly regarding the quality gap between paired images, we train an image reward model (verifier) to quantitatively evaluate image quality. Specifically, we leverage the intermediate image preference annotations from GenRef-CoT and selected pairs whose preferences align consistently with the image pair annotations in GenRef, serving as confident, high-quality preference Inspired by recent advancements in reward modeldata. ing for video generation [32], we adopt the BradleyTerry (BT) pairwise comparison approach [6] to train our image reward model. The BT framework utilizes pairwise loglikelihood loss to explicitly model the reward gap between preferred and non-preferred image pairs, defined as: LBT = E(y,xw,xl)D [log σ (rη(xw, y) rη(xl, y))] , (2) where denotes the input prompt, (xw, xl) represents the preferred and non-preferred image pair respectively, rη is the learnable reward model, and σ() refers to the logistic sigmoid function. Leveraging this verifier, we conduct rigorous post-processing step on our dataset, applying multi1Appendix shows few samples from this dataset. Figure 3. Comparisons of textual reflection generated by original Qwen2.5-VL-7B, our fine-tuned image reflector, and GPT-4o. struct refinement pairs by randomly pairing images from the top-k and bottom-k subsets for each prompt. Long-Short Prompt Data. Recent studies [5, 8, 45] indicate that, for the same intended content, generated image quality consistently improves as textual prompts become more detailed and descriptive. Motivated by this observation, we fine-tune an MLLM specifically to condense detailed prompts annotated by GPT-4o into shorter, more concise versions. Images generated from these two prompt variants naturally form corresponding image pairs, with the detailed prompt-generated images serving as higher-quality instances. To ensure the quality and effectiveness of these pairs, we also generate multiple samples in parallel and employ the ensemble-reward scoring approach for filtering. Editing Data. Finally, we augment our dataset with existing image editing datasets to further enhance its diversity and richness. Image editing data inherently provides paired images accompanied by textual editing instructions. By treating the caption of the edited image as the input prompt, and the source and edited images as flawed and high-quality counterparts respectively, we seamlessly integrate editing data into our refinement dataset paradigm. Although the editing domain differs from our primary image refinement tasks, the precise and explicit nature of editing instructions can effectively enhance the models ability to follow textual guidance and understand precise correspondences between source and target images. Concretely, we select highquality editing samples from the OmniEdit dataset [55] and generate detailed synthetic captions for each edited image using our in-house captioning model. Reflection Annotation. After constructing diverse image pairs, it is essential to annotate them with textual reflections that explicitly describe how to transform flawed image into its corresponding higher-quality counterpart. We experiment with various MLLMs and observed that even the current state-of-the-art open-source model, Qwen2.5VL [4], tends to generate inaccurate reflections when prompted in zero-shot manner, exhibiting severe hallu5 ple criteria to filter out lower-quality data samples. Furthermore, the trained verifier can also be utilized for inferencetime scaling, which we elaborate in detail in Section 4.2. 3.3. Reflection Tuning After having constructed suitable dataset, we turn our attention to efficiently training corrector model Cϕ that improves the quality of generated images. Analogous to image editing tasks, we treat self-refinement as conditional generation problem and employ an efficient fine-tuning strategy tailored for pretrained diffusion transformers (DiTs), without introducing any additional modules. Efficient Fine-tuning for DiT. The MMDiT architecture [12] has become de-facto for the recent T2I generation models. In these models, image tokens and textual embeddings are concatenated into unified sequence, allowing joint multimodal attention within each transformer block. Inspired by recent advances in conditional generation [29, 51, 58], we similarly concatenate textual inputs y, the flawed image xl, and the refined image xw into single fused sequence, enabling multimodal attention: MMAttention(z) = softmax (cid:19) (cid:18) QK V, (3) where the Q, K, and are linearly projected from the concatenated token sequence = [y; xl; xw], with being concatenation of the original input prompt and the reflection prompt. This unified attention mechanism naturally facilitates bidirectional information exchange across multiple modalities without requiring specialized modules such as ControlNet [65] or IP-Adapter [63]. Under this framework, the flawed image xl serves as conditioning information and thus does not require applying the noise schedule and can be further downsampled to lower resolutions (from 1024 to 512) to boost computational efficiency during both training and inference. Consequently, we can directly apply standard diffusion (or flow-matching) loss [30, 33] on the refined target image xw: = Et,ϵ,xt (cid:2)Cϕ(xt, t, y, xl) (xw ϵ)2 2 (cid:3) dt, (4) where the noise ϵ (0, I) is sampled from standard Gaussian distribution and xt is sampled from p(xtϵ, xw). Training Strategy. We observe that directly training on our proposed dataset could cause distributional drift from the pretrained base model, subsequently degrading the quality of generated images. To mitigate this issue, we design specialized training strategy to enhance model robustness and maintain alignment with the pretrained distribution. First, we randomly drop the original prompt, reflection prompt, and flawed input image with certain probabilities during training. The reflection prompt and flawed image Figure 4. Illustrations of three different inference-time scaling dimensions for text-to-image diffusion models. are dropped simultaneously with relatively high probability. In this way, the training objective effectively reverts to standard T2I generation, ensuring the model does not deviate excessively from its pretrained distribution. Additionally, we adopt task warm-up approach to dynamically adjust the sampling probabilities of multiple data domains throughout the training process. Initially, editing data is sampled with higher probability, facilitating rapid acquisition of accurate instruction-following capabilities. As training progresses, we gradually increase the sampling probabilities of the other three data domains. This gradual adjustment effectively improves the models overall performance and compensates for potential visual quality degradation inherent in editing data by leveraging high-quality synthetic datasets. 3.4. Test-Time Scaling via Iterative Refinement Leveraging the trained corrector model, we aim to maximize the generative capability of the diffusion model at inference time. In this section, we propose revisiting test-time scaling for T2I diffusion models along three distinct yet complementary dimensions: noise-level scaling, reflectionlevel scaling, and prompt-level scaling, as illustrated in Fig. 4. These three dimensions seamlessly integrate within our proposed ReflectionFlow framework. Noise-Level Scaling. We first introduce noise-level scaling, inspired by recent works focused on noise-space optimization [2, 40, 66]. These approaches aim to identify superior initial noise or intermediate noisy images through various search strategies. Within our framework, we define the number of different initial noise samples explored per generation round as the search width . By increasing , one can fully explore the diversity embedded in the diffusion models learned distribution. However, since the generation processes for these initial noise are independent and heavily reliant on feedback from task-dependent verifier, simply scaling up can result in diminishing returns in terms of computational efficiency. Reflection-Level Scaling. Reflection-level scaling builds 6 Methods Samples Overall Single Two Counting Colors Position Attribution SDXL [39] DALLE 3 [5] SANA-1.5 4.8B [60] Lumina-Image 2.0 [53] SD3 [12] Playground v3 [31] Janus-Pro-7B [10] SANA-1.0-1.6B [59] + Noise Scaling [34] + Reflect-DiT [27] FLUX.1-dev [26] + Noise Scaling [34] + Noise & Prompt Scaling + ReflectionFlow 1 1 1 1 1 1 1 20 20 1 32 32 32 Text-to-Image Models 0.55 0.67 0.72 0.73 0.74 0.76 0.80 0.98 0.96 0.99 0.99 0.99 0.99 0.99 0.74 0.87 0.85 0.87 0.94 0.95 0. Inference Time Scaling 0.66 0.80 0.81 0.67 0.85 0.87 0.91 0.99 1.00 0.98 0.99 1.00 0.99 1.00 0.77 0.93 0. 0.81 0.96 0.94 0.98 0.39 0.47 0.77 0.67 0.72 0.72 0.59 0.62 0.79 0.80 0.75 0.91 0.85 0.90 0.85 0.83 0.87 0.88 0.89 0.82 0.90 0.88 0.91 0. 0.80 0.91 0.91 0.96 0.15 0.43 0.34 0.34 0.33 0.50 0.79 0.21 0.55 0.66 0.21 0.52 0.80 0.93 0.23 0.45 0.54 0.62 0.60 0.54 0.66 0.47 0.62 0. 0.48 0.78 0.71 0.72 Table 1. Quantitative comparisons of our ReflectionFlow framework against standard text-to-image models and different inference-time scaling approaches evaluated on the GenEval benchmark. The notation denotes results obtained using only noise-level scaling, which is equivalent to the random search strategy introduced in Ma et al. [34]. upon our trained corrector model by iteratively refining previously generated images to progressively improve their quality. We define the number of iterative refinement rounds as the reflection depth . Specifically, each refinement iteration follows the process defined in Section 3.1, where the images generated in the previous iteration are refined iteratively with reflection. If the corrector model is effective, scaling the reflection depth can significantly enhance the models overall performance. Prompt-Level Scaling. Finally, considering that T2I diffusion models rely not only on noisy image inputs but also on user-provided textual prompts, we design the prompt evolving process in our test-time scaling framework. We find that prompt expansion can substantially improve generation quality, especially for concise prompts. At each iterative round, we leverage an MLLM to refine the textual prompt based on the original user prompt, the previously generated images, and their evaluation scores. This prompt refinement procedure, performed without explicit gradient information, can produce precise and effective prompts for subsequent image generation iterations. ReflectionFlow. Integrating all the aforementioned components, we propose ReflectionFlow framework, referring to Appendix for detailed algorithm. Specifically, we first employ generator, implemented via offloading of LoRA weights, to produce an initial set of candidate images. Subsequently, at iteration i, we utilize an MLLM verifier to comprehensively evaluate and rank the images generated in the previous iteration across multiple dimensions. Based on these evaluation scores and previously generated images, the MLLM generates textual reflections aimed at correcting identified errors and then refines the user prompts. These reflections and improved prompts jointly serve as inputs to our trained corrector model with LoRA, producing refined set of images for the next iteration. Finally, we use the verifier to select the best image for each refinement chain and then select the best image across all chains. Our ReflectionFlow allows flexible adjustment of both the search width (number of parallel chains) and reflection depth (number of iterative refinement rounds). This flexibility empowers users to effectively balance performance and computational efficiency according to the requirements and constraints of various downstream tasks. 4. Experiments 4.1. Setup Training Details. We select FLUX.1-dev2 [26], 12Bparameter flow-based diffusion transformer for text-toimage generation, as our base generator. To train the corrector model, we utilize our GenRef dataset, resizing and cropping all target images to 1024 1024 and all conditional images to 512 512. To enable efficient fine-tuning, we employ LoRA [20] with rank of 256. The model is trained using batch size of 64 and optimized using the Prodigy optimizer [36], with safeguard warmup and bias 2https : / / huggingface . co / black - forest - labs / FLUX.1-dev 7 Figure 5. Left: The choice of verifier significantly impacts the effectiveness of inference-time scaling methods. Middle: By efficiently scaling the inference-time budget, ReflectionFlow achieves substantial performance improvements, requiring 10 times fewer samples compared to naive noise-level scaling. Right: ReflectionFlow demonstrates notably greater performance gains on challenging samples. correction enabled, following practices outlined in OminiControl [51]. The entire fine-tuning procedure is conducted over 6,000 optimization steps on 8 NVIDIA A100 GPUs. For our reflection generator, we utilize Qwen2.5-VL7B [4] as the backbone model and apply full fine-tuning with learning rate of 1 106 for total of 45,000 steps. Considering that the image reward modeling task is comparatively easier, we select Qwen2.5-VL-3B [4] as the backbone of our verifier. Following the configuration of VideoAlign [32], we optimize the verifier using the Bradley-Terry (BT) loss, fine-tuning LoRA [20] parameters of the language model and fully updating parameters of the vision encoder. Training is conducted for 10,080 steps with learning rate of 2 106. Evaluation. We evaluate ReflectionFlow framework on GenEval benchmark [15], following the official evaluation protocol. The GenEval dataset comprises 553 prompts, with four images generated per prompt. All images are generated at resolution of 1024 1024, guidance scale of 3.5, and 30 sampling steps. We use the SANA verifier3 introduced in SANA-1.5 [60] to assess generated images in our main experiments, and use our fine-tuned Qwen2.5-VL-7B to produce reflection. We also test additional verifiers, including our verifier and GPT-4o, which is included in our verifier comparison experiments. The detailed GPT-4o prompt is provided in the Appendix. 4.2. Main Results We first validate the effectiveness of our proposed ReflectionFlow framework on the GenEval benchmark. Specifically, we progressively introduce three complementary scaling dimensions defined in our method: noise-level scaling, prompt-level scaling, and reflection-level scaling. We compare ReflectionFlow with the baseline FLUX.1-dev and sev3https://huggingface.co/Efficient-Large-Model/ NVILA-Lite-2B-Verifier eral state-of-the-art text-to-image models. Our main experiments are done under the setting of 32 samples. ReflectionFlow Boosts Generation Quality. Tab. 1 summarizes the quantitative results. Starting from the baseline FLUX.1-dev model (score of 0.67), we observe that introducing noise-level scaling significantly improves the score to 0.85. Further incorporating prompt-level scaling provides slightly improvement, reaching score of 0.87. Finally, when we integrate three levels of scaling together, ReflectionFlow can achieve substantial leap in performance, reaching an overall score of 0.91. These results demonstrate that naive noise-level scaling alone is inefficient without explicit guidance, while dynamically refining prompts along generation facilitates clearer understanding of complex semantic attributes, such as spatial arrangements. The substantial performance gain achieved by incorporating reflection-level scaling further confirms that textual reflections from verifiers provide valuable feedback, enabling the corrector model to iteratively and reliably refine its outputs. Moreover, we compare our method with concurrent reflection-based work, Reflect-DiT [27]. From Fig. 5, either SANA verifier or our verifiers 16 samples results are better than Reflect-DiTs performance of 0.81, which is achieved with 20 samples per prompt. This comparison highlights the efficiency and effectiveness of our ReflectionFlow framework, which can be attributed to the better dataset, reflection model training, as well as the overall inference-time scaling framework. 4.3. Ablation Studies Exploring Different Verifiers. To explore the potential of ReflectionFlow under various verifiers, we performed experiments using three distinct verifier settings: GPT-4o verifier, our fine-tuned verifier, and SANA verifier, which is specifically designed for the GenEval benchmark. Additionally, we utilized the oracle results from the GenEval 8 Figure 6. Visualization of complex reasoning. Starting from initially incorrect generations (the first image), ReflectionFlow iteratively reflects on and corrects errors, progressively producing images that accurately align with the provided prompts and reflection instructions . evaluation pipeline to estimate the upper bound of current performance. The results are shown in Fig. 5. When using GPT-4o as the verifier, the performance curve quickly approaches its limit with only 32 samples per prompt. In contrast, when using our verifier trained with BT loss, we observe steady increase in performance as the number of samples per prompt increases. Notably, this configuration does not yet reach the limit of inference-time scaling, indicating further potential for improvement. Furthermore, by adopting the SANA verifier, which was designed for the GenEval dataset, we achieve even higher performance. Specifically, with 32 samples per prompt, our ReflectionFlow achieves GenEval score of 0.91. For comparison, using the oracle results from the GenEval benchmark as an upper bound, we find that the model can achieve score of up to 0.98, demonstrating the significant headroom for performance improvements with better verifiers. These results show the strong inherent abilities of our ReflectionFlow, as its performance consistently improves with increasingly better verifiers with little sign of saturation. While the choice of verifier plays critical role in unlocking this potential, the steady performance gains observed across different verifier settings highlight the robustness and scalability of our model. Scaling Inference-time Budgets. We then investigate how the allocated inference-time budget influences the performance of our ReflectionFlow framework. Specifically, we fix the search width to 2 and progressively increase the reflection depth as we scale up the total budget. This allows us to explicitly examine the capability of ReflectionFlow to iteratively reflect and correct its previous mistakes. Results shown in Fig. 5 demonstrate that ReflectionFlow rapidly improves performance as the budget increases from 1 to 4, after which the improvement slows down, ultimately reaching final GenEval score of 0.91 with 32 samples for each prompt. The baseline approaches, including Noise Width Depth Overall Position Attribution 16 8 4 2 1 1 2 4 8 16 0.74 0.76 0.77 0.78 0.78 0.56 0.58 0.56 0.57 0. 0.42 0.51 0.51 0.55 0.51 Table 2. Ablation studies on width and depth sizes in inference. Scaling and Noise & Prompt Scaling consistently underperform relative to ReflectionFlow across all budgets, indicating the limited effectiveness of simpler methods. We capped our evaluation at 32 samples for each prompt; however, the upward trajectory of ReflectionFlows performance suggests that it has not yet reached its full potential, leaving room for further improvement with larger budgets. Exploring Iterative Refinement Strategies. We systematically investigate different exploration strategies within the ReflectionFlow framework. Specifically, we conduct ablation experiments by varying two key hyperparameters: search width , which denotes the number of candidate images generated at each iteration, and reflection depth , representing the number of iterative refinement rounds. Given fixed computational budget of = 16, we explore three different refinement strategies: (1) Sequential, generating one candidate per refinement step; (2) Parallel, generating multiple candidates simultaneously per iteration; and (3) Combine, balancing both depth and breadth by moderately expanding candidate branches per iteration. We use GPT-4o as the verifier. Tab. 2 clearly shows that the sequential strategy (N1M16) achieves the highest overall performance of 0.78, outperforming the parallel strategy (N16M1), which yields lower score of 0.74. Comparing various combined strategies, such as N2M8, N4M4, and N8M2, we consistently observe that strategies with greater refinement depth tend to yield better performance than those with wider branching. These observations suggest that ReflectionFlow framework exhibits effective reflection and self-correction capabilities, yet the refinement process is relatively unstable, requiring multiple sequential iterations to progressively identify and correct errors. Increasing the refinement depth allows the model to continuously reason and rectify previous mistakes, ultimately converging to improved results. 4.4. Analysis and Discussion Reflection Capability for Difficult Tasks. Previous studies in LLMs demonstrate that inference-time scaling, particularly via longer reasoning and reflection chains, improves performance on more challenging tasks [35, 46]. Inspired by these observations, we explore whether ReflectionFlow framework exhibits similar behavior in diffusion models. Leveraging the difficulty estimated in Sec. 3.2, we divide the prompts from GenEval benchmark into three difficulty levels according to their initial correctness: hard prompts with correctness between 0 and 0.3, medium prompts with correctness between 0.4 and 0.7, and easy prompts with correctness between 0.8 and 1.0. Fig. 5 summarizes the performance of ReflectionFlow across these three difficulty levels."
        },
        {
            "title": "ReflectionFlow achieves the largest",
            "content": "improvement on hard prompts, substantially increasing correctness from 0.10 to 0.81. Medium prompts exhibit moderate improvement, increasing correctness from 0.55 to 0.85, whereas easy prompts show minimal change, slightly increasing from 0.95 to 0.97. These results clearly indicate that ReflectionFlow shares similar property with reflection-based scaling strategies in LLMs, where deeper iterative reflection is particularly beneficial for challenging tasks. This demonstrates the potential for future work to dynamically allocate inference-time compute based on prompt difficulty, leveraging the flexibility of ReflectionFlow. Qualitative Examples. To intuitively illustrate the effectiveness of ReflectionFlow, we present qualitative examples of generated images and corresponding reflection processes. As shown in Fig. 6, initial generations by our baseline often fail to capture detailed nuances or complex relations described in prompts. Through our iterative reflection, the model progressively identifies and corrects these issues, resulting in improved final outputs. Moreover, our method naturally produces interpretable reflection chains, which are similar to chain-of-thought reasoning in LLMs, demonstrating how the model explicitly reasons about and resolves visual inconsistencies, stylistic mismatches, and compositional inaccuracies step by step. Please refer to the Appendix for more qualitative results. 5. Conclusion In this work, we present an inference-time framework, ReflectionFlow, that equips text-to-image diffusion models with iterative self-refinement capabilities. Central to our approach is the construction of GenRef, large-scale image reflection dataset consisting of one million triplets of flawed images, high-quality images, and textual reflections. By formulating self-refinement as generalized image editing problem and employing efficient fine-tuning strategies for pretrained diffusion transformers, ReflectionFlow effectively enhances performance without deviating from pretrained distributions. Furthermore, our framework integrates inference-time scaling across noise, reflection, and prompt dimensions, providing flexibility to balance computational efficiency and generation quality. We believe our work offers promising direction for advancing selfrefining generative models, contributing to more reliable and adaptive visual generation systems."
        },
        {
            "title": "References",
            "content": "[1] Eslam Abdelrahman, Liangbing Zhao, Vincent Tao Hu, Matthieu Cord, Patrick Perez, and Mohamed Elhoseiny. Interactive structured image generation Toddlerdiffusion: arXiv preprint with cascaded schr odinger bridge. arXiv:2311.14542, 2023. 1 [2] Donghoon Ahn, Jiwon Kang, Sanghyun Lee, Jaewon Min, Minjae Kim, Wooseok Jang, Hyoungwon Cho, Sayak Paul, SeonHwa Kim, Eunju Cha, et al. noise is worth diffusion guidance. arXiv preprint arXiv:2412.03895, 2024. 3, 6 [3] Lichen Bai, Shitong Shao, Zikai Zhou, Zipeng Qi, Zhiqiang Xu, Haoyi Xiong, and Zeke Xie. Zigzag diffusion samarXiv preprint pling: The path to success is zigzag. arXiv:2412.10891, 2024. 3 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 3, 5, 8 [5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2023. 5, 7 [6] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. 5 [7] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1839218402, 2023. 2, 10 [8] Agneet Chatterjee, Melech Ben Gabriela Stan, Estelle Aflalo, Sayak Paul, et al. Getting it right: Improving spatial consistency in text-to-image models. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. 5 [9] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. Proceedings of the European Conference on Computer Vision (ECCV), 2024. 3 [10] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling, 2025. 7 [11] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3, 4 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML), 2024. 1, 3, 6, 7 [13] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. [14] Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Rongjie Huang, Shijie Geng, Renrui Zhang, et al. Lumina-t2x: Scalable flow-based large In diffusion transformer for flexible resolution generation. Proceedings of the International Conference on Learning Representations (ICLR), 2025. 3 [15] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. 4, 8 [16] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. 1, 3 [17] Alexander Havrilla, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. Glore: When, where, and how to improve llm reasoning via global and local refineIn Proceedings of the International Conference on ments. Machine Learning (ICML), 2024. 3 [18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation In Proceedings of the 2021 metric for image captioning. Conference on Empirical Methods in Natural Language Processing, pages 75147528, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. 3, 4 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 2020. 3 [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. Proceedings of the International Conference on Learning Representations (ICLR), 2022. 7, 8 [21] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 3, 4, 5 [22] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 2 [23] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. Advances in Neural Information Processing Systems (NeurIPS), 2023. 3 [24] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [25] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language arXiv models to self-correct via reinforcement learning. preprint arXiv:2409.12917, 2024. 2, 3 [26] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, 2, 3, 4, 7 [27] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Reflect-dit: Inference-time scaling for text-to-image diffusion transformers via in-context reflection. arXiv preprint arXiv:2503.12271, 2025. 7, 8 [28] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 3 [29] Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming Cheng. Visualcloze: universal image generation framework via visual in-context learning. arXiv preprint arXiv:2504.07960, 2025. 6 [30] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative In Proceedings of the International Conference modeling. on Learning Representations (ICLR), 2023. 3, 6 [31] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models, 2024. [32] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Wenyu Qin, 11 Menghan Xia, et al. Improving video generation with human feedback. arXiv preprint arXiv:2501.13918, 2025. 2, 5, 8 Principled recaptioning improves image generation. arXiv preprint 2310.16656, 2023. 5 [33] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. 3, 6 [34] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, YuChuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. 2, 3, 7, [35] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems (NeurIPS), 2023. 2, 3, 10 [36] Konstantin Mishchenko and Aaron Defazio. Prodigy: an In Proceedexpeditiously adaptive parameter-free learner. ings of the International Conference on Machine Learning (ICML), pages 3577935804, 2024. 7 [37] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2022. 2 [38] Guilherme Penedo, Hynek Kydlıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. In Advances in Neural Information Processing Systems (NeurIPS), pages 3081130849, 2024. 3 [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 7 [40] Zipeng Qi, Lichen Bai, Haoyi Xiong, and Zeke Xie. Not all noises are created equally: Diffusion noise selection and optimization. arXiv preprint arXiv:2407.14041, 2024. 3, 6 [41] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, et al. Luminaimage 2.0: unified and efficient image generative framework. arXiv preprint arXiv:2503.21758, 2025. [42] Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 [43] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 4 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synIn Proceedings of the thesis with latent diffusion models. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 2 [45] Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. picture is worth thousand words: [46] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems (NeurIPS), 2023. 2, 3, 10 [47] Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. general framework for inference-time scaling and steering of diffusion models. arXiv preprint arXiv:2501.06848, 2025. 2, 3 [48] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more arXiv preprint effective than scaling model parameters. arXiv:2408.03314, 2024. 2, [49] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. 3 [50] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equaIn Proceedings of the International Conference on tions. Learning Representations (ICLR), 2021. 3 [51] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and uniarXiv preprint versal control for diffusion transformer. arXiv:2411.15098, 3, 2024. 6, 8 [52] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 3, 5 [53] Alpha VLLM. Lumina-image 2.0 : unified and efficient image generative model. https://github.com/ Alpha-VLLM/Lumina-Image-2.0/, 2025. 1, 7 [54] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 82288238, 2024. 2 [55] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. 2, 3, 5, [56] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. 3 [57] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 4 12 [58] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 6 [59] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. Proceedings of the International Conference on Learning Representations (ICLR), 2025. 1, 3, 7 [60] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. 2, 7, [61] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textIn Advances in Neural Information to-image generation. Processing Systems, pages 1590315935. Curran Associates, Inc., 2023. 3 [62] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems (NeurIPS), 2023. 3 [63] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 6 [64] Haotian Ye, Haowei Lin, Jiaqi Han, Minkai Xu, Sheng Liu, Yitao Liang, Jianzhu Ma, James Zou, and Stefano Ermon. Tfg: Unified training-free guidance for diffusion models. Advances in Neural Information Processing Systems (NeurIPS), 2024. 3 [65] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 38363847, 2023. 2, 6 [66] Zikai Zhou, Shitong Shao, Lichen Bai, Zhiqiang Xu, Bo Han, and Zeke Xie. Golden noise for diffusion models: learning framework. arXiv preprint arXiv:2411.09502, 2024. 3, [67] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 1,"
        },
        {
            "title": "Appendix",
            "content": "A. Dataset Preview In the Figures 7, 8, and 9, we provide samples from our GenRef dataset. We divide these figures with respect to the subsets we used during data curation, except for the edit samples that were sourced from the OmniEdit dataset [55]. For each image, we provide the prompt and reflection pairs. The red and green borders indicate the starting and final images, respectively. For best viewing experience, we recommend zooming in. (a) (b) (c) Figure 7. Samples from the pool of reward-based data. (a) Prompt: surreal digital illustration of floating island with large, lush tree and cascading waterfalls, set against twilight sky with crescent moon, surrounded by misty mountains and vibrant, ethereal colors. Reflection: Remove the sun in the background. Add more mist around the mountains. (b) Prompt: surreal digital artwork depicting clear glass resting on rocks, containing miniature landscape with river, pine trees, and sunset, surrounded by pink flowers, creating dreamlike, photorealistic scene with vibrant colors and intricate details. Reflection: Add more rocks around the glass. Add more pink flowers around the glass. (c) Prompt: young woman with long red hair ice skates gracefully on frozen lake, surrounded by snowcovered mountains and evergreen trees, creating serene and ethereal winter scene. Reflection: Change the womans outfit to white sweater and skirt. Make the woman skate more gracefully. (a) (b) (c) Figure 8. Samples from the pool of rule-based data. (a) Prompt: photo of two carpet cleaners. Reflection: Replace the vacuum cleaners with professional-grade carpet cleaning machines and adjust the posture of the individuals to face forward while holding the machines. (b) Prompt: photo of white blanket and red measuring spoon. Reflection: Change the texture of the blanket to smooth, woven pattern instead of fluffy one. (c) Prompt: photo of four colored pencils. Reflection: Remove two pencils from the group to leave only four pencils visible. Figure 10 shows samples from the dataset created for fine-tuning our Qwen model, as described in Section 3.2. The green and red borders denote the correct and incorrect images (as deemed by closed-source APIs), respectively. (a) (b) (c) Figure 9. Samples from the pool of long-short prompt data. (a) Prompt: Portrait of middle-aged man with beard, seated indoors, looking slightly to the right. He wears dark blue shirt and is positioned in the lower left of the frame. His right hand holds newspaper, partially visible in the foreground. The background features rustic wooden walls with warm, weathered texture, and wooden mirror frame is partially visible on the right. The lighting is soft and diffused, casting gentle shadows on his face, creating contemplative mood. The color palette is muted with earthy tones, emphasizing cozy, intimate atmosphere. The composition is balanced, with shallow depth of field that keeps the focus on the mans expression. Photorealistic, cinematic, warm, introspective, visually balanced. Reflection: Change the suit jacket into dark blue shirt. Remove the gray sweater vest. (b) Prompt: Studio portrait of young woman with fair skin and long, wavy red hair, centered against dark grey background. She gazes directly at the camera with neutral expression, her lips painted vibrant red. Her right hand is raised to her chin, with fingers gently touching her cheek. She wears crisp white blouse with statement necklace featuring large, dark blue gemstones. The lighting is soft and even, highlighting her freckles and the texture of her hair. High contrast, sharp focus, professional studio photography, neutral color palette, elegant and poised, classic portrait composition. Reflection: Paint the lips vibrant red. Replace the necklace with one that features large, dark blue gemstones. (c) Prompt: striking portrait of an elderly woman dressed as superhero in an urban setting. She stands confidently in the foreground, wearing red helmet with visor, green eye mask, and red and blue superhero costume with cape. Her right hand is raised, adorned with silver glove, while her left hand rests on her hip, also gloved. can of food is strapped to her left arm. The background features city street with yellow traffic light and bus with an American flag on its side, parked on the right. brick building with the sign BRINKLEYS is visible behind her. The scene is set in bustling city environment with blurred buildings and taxi in the distance. Photorealistic, high contrast, dramatic lighting, vibrant color palette, sharp focus on the subject, urban superhero theme, dynamic composition, slightly desaturated background, cinematic feel. Reflection: Change the womans outfit to red and blue superhero costume with cape. Add red helmet with visor, green eye mask, and silver glove to the womans hand. 15 (a) (b) (c) Figure 10. Samples from the pool of CoT (chain-of-thought) data. (a) Prompt: Dimly lit photograph of an empty booth in cozy, vintagestyle diner. Red leather benches form an L-shape around pair of adjoining hexagonal marble tables in the left center against the back wall. The tables, with dark veining and brown wooden edges, sit atop black metal bases. The wall shows textured, weathered green paint over brick surface, subtly shadowed. In the foreground, waist-high partition with vertical ribbed glass edges frames the scene on the right. Flooring includes worn terrazzo tiles with visible dust. Warm, ambient lighting creates an inviting yet mysterious atmosphere. Photorealistic, shallow depth of field, warm tones, nostalgic, intimate mood, reminiscent of mid-20th-century design. Analysis: The left image better matches the prompt. It features dimly lit, cozy atmosphere with red leather benches forming an L-shape around two adjoining hexagonal marble tables. The wall has textured green paint over brick surface, and the terrazzo flooring appears worn and dusty. The right image deviates from the prompt as the benches form U-shape instead of an L-shape, the wall includes wooden paneling not mentioned in the prompt, and the lighting is less dim and warm, reducing the nostalgic and intimate mood. Instruction: Change the U-shaped bench arrangement to an L-shape. Remove the wooden paneling on the walls and replace it with textured green paint over brick surface. (b) Prompt: minimalist, flat-design illustration of stylized Arabic calligraphy forming coffee pot with steam, set against warm brown background, featuring cream color palette and subtle shadows for depth. Analysis: The left image accurately portrays stylized Arabic calligraphy forming coffee pot and steam, while the right image only contains Arabic calligraphy around regular coffee pot. The prompt specifically calls for the Arabic calligraphy to form the pot and steam. Instruction: Transform the Arabic calligraphy into the form of coffee pot with steam. Remove the existing regular coffee pot. (c) Prompt: man in red and yellow plaid shirt sits on floral sofa in warmly lit, cozy living room with natural light, bookshelf, and floral lamp, creating relaxed and intimate atmosphere. Analysis: The right image is better because it depicts man in red and yellow plaid shirt seated on floral sofa within warmly lit, cozy living room, effectively conveying relaxed and intimate setting as described in the prompt. The left image features man in plaid shirt, but the colors are not quite right, and the setting is not as warm or cozy, and the lighting is not right. Instruction: Change the shirt color to red and yellow plaid. Add natural light to give the setting warm and cozy feel. 16 B. Qualitative Results In Figure 11, we list some qualitative results of our ReflectionFlow framework. This includes the detailed process of our three scaling steps, giving more nuanced understanding of our framework. Figure 11. Qualitative results involving complex reasoning. 17 Initial image set Generate initial image Score previous images Generate reflection Refine with corrector C. Algorithm Process The proposed ReflectionFlow framework is as follows: Algorithm 1 The proposed ReflectionFlow framework Require: prompt y, generator Gθ, corrector Cϕ, verifier , scaling width , scaling depth Ensure: High-quality image that best realizes user intent 1: X0 2: for = 1 to do 3: Sample zj (0, I) xj 0 Gθ(y, zj) X0 X0 {xj 0} 5: 6: end for 7: for = 1 to do 8: 9: si (Xi1, y) Xi for = 1 to do (xj , yj rj i1, y, si) xj Cϕ(yj , rj , xj i1) Xi Xi {xj } 4: 10: 11: 12: end for 13: 14: 15: end for 16: return arg maxx(cid:83)M i=0 (cid:83)N j=1 xj (x, y) D. Prompts We provide all the prompts we used throughout this work. These prompts were inspired by Figure 16 from [34]. D.1. Verifier Prompts D.1.1. Single Object Below For each score, include short The keys in the JSON object should be: Key Evaluation Aspects and Scoring Criteria: You are multimodal large-language model tasked with evaluating images generated by text-to-image model. Your goal is to assess each generated image based on specific aspects and provide detailed critique, along with scoring system. The final output should be formatted as JSON object containing individual scores for each aspect and an overall score. object completeness, detectability, occlusion handling, and overall score. is comprehensive guide to follow in your evaluation process: 1. For each aspect, provide score from 0 to 10, where 0 represents poor performance and 10 represents excellent performance. explanation or justification (1-2 sentences) explaining why that score was given. The aspects to evaluate are as follows: a) Object Completeness: Assess the structural integrity of the object (no defects/deformations), detail clarity and legibility. Score: 0 (severely fragmented) to 10 (perfectly intact). b) Detectability: Evaluate the distinction and visual saliency of objects and backgrounds using contrast analysis. Score: 0 (camouflaged) to 10 (immediately noticeable). c) Occlusion Handling: Assess whether there is unreasonable occlusion (natural occlusion needs to keep the subject visible). Score: blockage/natural and reasonable blockage). 2. After scoring each aspect individually, provide an overall score, representing the models general performance on this image. based on the importance of each aspect to the prompt or an average of all aspects. 0 (key parts are blocked) to 10 (no"
        },
        {
            "title": "This should be a weighted average",
            "content": "Overall Score: Figure 12. Verifier prompt for images with single object. 19 D.1.2. Two Objects The keys in the JSON object should Key Evaluation Aspects and Scoring Criteria: Your evaluation should focus on these aspects: You are multimodal large-language model tasked with evaluating images generated by text-to-image model. Your goal is to assess each generated image based on specific aspects and provide detailed critique, along with scoring system. The final output should be formatted as JSON object containing individual scores for each aspect and an overall score. separation clarity, individual completeness, relationship accuracy, and be: overall score. Below is comprehensive guide to follow in your evaluation process: 1. from 0 to 10, where 0 represents poor performance and 10 represents excellent performance. For each score, include short explanation or justification (1-2 sentences) explaining why that score was given. The aspects to evaluate are as follows: a) Seperation Clarity: Assess the spatial separation and boundary clarity of two objects. Score: 0 (fully overlapped) to 10 (completely separate and clearly defined boundaries) b) Indivisual Completeness: Evaluate each objects individual integrity and detail retention. Score: 0 (both objects are incomplete) to 10 (both objects are complete). c) Relationship Accuracy: Assess the rationality of size proportions. (wrong proportions) to 10 (perfectly in line with physical laws). 2. score, representing the models general performance on this image. be weighted average based on the importance of each aspect to the prompt or an average of all aspects. Overall Score: After scoring each aspect individually, provide an overall"
        },
        {
            "title": "This should",
            "content": "Score: For each aspect, provide score 0 Figure 13. Verifier prompt for images with two objects. 20 D.1.3. Counting Below Your evaluation The keys in the JSON object should be: Key Evaluation Aspects and Scoring Criteria: You are multimodal large-language model tasked with evaluating images generated by text-to-image model. Your goal is to assess each generated image based on specific aspects and provide detailed critique, along with scoring system. The final output should be formatted as JSON object containing individual scores for each aspect and an overall score. count accuracy, object uniformity, spatial legibility, and overall score. is comprehensive guide to follow in your evaluation process: should focus on these aspects: 1. from 0 to 10, where 0 represents poor performance and 10 represents excellent performance. For each score, include short explanation or justification (1-2 sentences) explaining why that score was given. The aspects to evaluate are as follows: a) Count Accuracy: Assess the number of generated objects matches the exact prompt. b) Object Uniformity: Evaluate the consistency of shape/size/color among same kind of objects. Score: 0 (same kind but total different shape/size/color) to 10 (same kind and same shape/size/color). c) Spatial Legibility: Evaluate the plausibility and visibility of object 0 (heavily overlapped) to 10 distribution (no excessive overlap). (perfect displayed and all easily seen). 2. score, representing the models general performance on this image. be weighted average based on the importance of each aspect to the prompt or an average of all aspects. Overall Score: After scoring each aspect individually, provide an overall Score: 0 (number wrong) to 10 (number correct). For each aspect, provide score"
        },
        {
            "title": "This should",
            "content": "Score: Figure 14. Verifier prompt for images for counting. 21 D.1.4. Colors The keys in the JSON object should Key Evaluation Aspects and Scoring Criteria: Your evaluation should focus on these aspects: color fidelity, textttcontrast effectiveness, multi object consistency, and You are multimodal large-language model tasked with evaluating images generated by text-to-image model. Your goal is to assess each generated image based on specific aspects and provide detailed critique, along with scoring system. The final output should be formatted as JSON object containing individual scores for each aspect and an overall score. be: overall score. Below is comprehensive guide to follow in your evaluation process: 1. from 0 to 10, where 0 represents poor performance and 10 represents excellent performance. For each score, include short explanation or justification (1-2 The aspects to evaluate are as sentences) explaining why that score was given. follows: a) Color Fidelity: Assess the exact match between the object color and the input prompt. b) Contrast Effectiveness: Evaluate the difference between foreground and background colors. Score: 0 (similar colors, difficult to distinguish) to 10 (high contrast). c) Multi-Object Consistency: Assess color consistency across multiple same kind of objects. Score: 0 (same kind of objects with total different colors) to 10 (same kind with same color). 2. score, representing the models general performance on this image. be weighted average based on the importance of each aspect to the prompt or an average of all aspects. Overall Score: After scoring each aspect individually, provide an overall Score: 0 (color wrong) to 10 (color correct)."
        },
        {
            "title": "This should",
            "content": "For each aspect, provide score Figure 15. Verifier prompt for images for colors. 22 D.1.5. Position The keys in the JSON object should be: Key Evaluation Aspects and Scoring Criteria: You are multimodal large-language model tasked with evaluating images generated by text-to-image model. Your goal is to assess each generated image based on specific aspects and provide detailed critique, along with scoring system. The final output should be formatted as JSON object containing individual scores for each aspect and an overall score. position accuracy, occlusion management, perspective consistency, and overall score. Below is comprehensive guide to follow in your evaluation process: evaluation should focus on these aspects: 1. from 0 to 10, where 0 represents poor performance and 10 represents excellent performance. For each score, include short explanation or justification (1-2 The aspects to evaluate are as sentences) explaining why that score was given. follows: a) Positional Accuracy: Assess the matching accuracy between spatial position and prompt description. Score: 0 (totally wrong) to 10 (postion correct) b) Occlusion Management: Evaluate position discernibility in the presence of occlusion. Score: 0 (fully occlusion) to 10 (clearly dsiplay the relationship). c) Perspective Consistency: Assess the rationality of perspective relationship and spatial depth. Score: 0 (perspective contradiction) to 10 (completely reasonable). 2. score, representing the models general performance on this image. be weighted average based on the importance of each aspect to the prompt or an average of all aspects. Overall Score: After scoring each aspect individually, provide an overall For each aspect, provide score"
        },
        {
            "title": "This should",
            "content": "Your Figure 16. Verifier prompt for images for positions. 23 D.1.6. Color Attribution The keys in the JSON object should be: Key Evaluation Aspects and Scoring Criteria: You are multimodal large-language model tasked with evaluating images generated by text-to-image model. Your goal is to assess each generated image based on specific aspects and provide detailed critique, along with scoring system. The final output should be formatted as JSON object containing individual scores for each aspect and an overall score. attribute binding, contrast effectiveness, material consistency, and overall score. Below is comprehensive guide to follow in your evaluation process: evaluation should focus on these aspects: 1. from 0 to 10, where 0 represents poor performance and 10 represents excellent performance. For each score, include short explanation or justification (1-2 sentences) explaining why that score was given. The aspects to evaluate are as follows: a) Attrribute Binding: Correct binding of colors to designated objects (no color mismatches). Score: 0 (color mismatch) to 10 (correct binding). b) Contrast Effectiveness: Evaluate the difference between foreground and background colors. Score: 0 (similar colors, difficult to distinguish) to 10 (high contrast). c) Material Consistency: Assess the coordination of color and material performance. Score: 0 (material conflicts) to 10 (perfect harmony). 2. score, representing the models general performance on this image. be weighted average based on the importance of each aspect to the prompt or an average of all aspects. Overall Score: After scoring each aspect individually, provide an overall For each aspect, provide score"
        },
        {
            "title": "This should",
            "content": "Your Figure 17. Verifier prompt for images for color attribution. D.2. Reflection Prompt A. Prompt Following Instructions: Structure and Focus Areas: Focus strictly on this aspect: Detailed Requirements for Each Aspect: You are an expert assistant for generating image improvement instructions. Analyze the original prompt, the updated prompt to generate the image, the evaluation of the generated image, and the generated image, give instructions to create specific technical directions following these guidelines: 1. 2. Examine the original prompt sentence by sentence. between the bad image and prompt specifications. Remove, Replace, Reposition, Adjust, to modify the image. locations and modification commands. confirm. 3. Following:n-n Each instruction must start with hyphen and complete command. Include spatial references and implementation details. required improvements. Never include explanations or examples. 4. artist. elements. Use imperative verb forms exclusively. without assumptions."
        },
        {
            "title": "Specify precise\nNever use vague terms like ensure or",
            "content": "Content Principles: Every instruction must be directly executable by an Format Specifications: Use exact section headers without markdown:1. Prompt"
        },
        {
            "title": "Describe only missing or incorrect",
            "content": "Prioritize critical errors first. Use direct action verbs: Add,"
        },
        {
            "title": "Omit sections with no",
            "content": "Prompt Following. Figure 18. Prompt for generating reflection instructions. 24 D.3. Refine Prompt You are multimodal large-language model tasked with refining users input prompt to create images using text-to-image model. current prompt, batch of images generated by the prompt, reflection prompt about the generated images and their corresponding assessments evaluated by multi-domain scoring system, your goal is to refine the current prompt to improve the overall quality of the generated images. and drawbacks of current prompt based on the given images and their evaluations. Consider aspects like subject, scene, style, lighting, tone, mood, camera style, composition, and others to refine the current prompt. Do not alter the original description from the original prompt. with the reflection prompt. Directly output the refined prompt without any other text. The refined prompt should not contradict You should analyze the strengths Given original prompt, Some further instructions you should keep in mind: 1) The current prompt is an iterative refinement of the original prompt. 2) In case the original prompt and current prompt are the same, ignore the current prompt. 3) In some cases, some of the above-mentioned inputs may not be available. For example, the images, the assessments, etc. do your best, analyze the inputs carefully, and arrive at refined prompt that would potentially lead to improvements in the final generated images. 4) When the evaluations are provided, please consider all aspects of the evaluations very carefully. In such situations, you should still Figure 19. Prompt for refinement. D.4. Chain-of-Though Image Reflection Prompt You are multimodal analysis assistant. Given prompt and two generated images (left and right), your task is to analyze and compare both images with respect to their alignment to the given prompt, decide which image better matches the prompt, then generate concise editing instructions to modify the inferior image to become the superior image. Follow the step-by-step instructions below: Analyze both images carefully and identify key differences regarding: 1. missing elements, incorrect object attributes (color, size, position, number, etc.), incorrect spatial or logical relationships between objects, presence of unnecessary elements, etc. Based on the analysis, determine which image better aligns with the prompt. 2. Output \"left\" if the left image is better; output \"right\" if the right image is better. 3. inferior image to closely match the better image. Generate exactly one most important editing instruction that will modify the Follow these guidelines: (1) Use concise, accurate, actionable imperative sentences. (2) **DO NOT explicitly mention specific images in your response, like \"the left image\", \"the right image\", or similar words!** (3) Avoid vague or redundant instructions, such as \"ensure\" or \"verify\". (4) Example instructions: - \"Add dirt road in the foreground extending into the background.\" - \"Remove cluster of white, fluffy cotton grass plants in the foreground on the rocky shore.\" - \"Swap the vampire with woman with long, wavy blonde hair.\" - \"Make the image look like its from an ancient Egyptian mural.\" - \"Turn the color of golden shield to gray.\" Format your final response strictly in JSON format: json { \"Analysis\": \"<detailed analysis of key differences between the two images in relation to the prompt>\", \"Result\": \"<left/right>\", \"Instructions\": \"<instruction>\" } Figure 20. Prompt for generating chain-of-thought image reflection annotations."
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "Hugging Face",
        "KAUST",
        "Shanghai AI Lab"
    ]
}