{
    "paper_title": "VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models",
    "authors": [
        "Jeongho Ju",
        "Daeyoung Kim",
        "SunYoung Park",
        "Youngjune Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate a step-by-step training strategy that allows a model learn both linguistic and visual information while preserving the backbone model's knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 2 ] . [ 1 3 0 1 9 1 . 1 1 4 2 : r VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models Jeongho Ju Daeyoung Kim SunYoung Park Youngjune Kim NC Research, NCSOFT {jeongho, daeyoungk, sun0park, youngjune}@ncsoft.com"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we introduce an open-source Korean-English vision-language model (VLM), VARCO-VISION. We incorporate step-by-step training strategy that allows model learn both linguistic and visual information while preserving the backbone models knowledge. Our model demonstrates outstanding performance in diverse settings requiring bilingual image-text understanding and generation abilities compared to models of similar size. VARCO-VISION is also capable of grounding, referring, and OCR, expanding its usage and potential applications for real-world scenarios. In addition to the model, we release five Korean evaluation datasets, including four closed-set and one openset benchmarks. We anticipate that our milestone will broaden the opportunities for AI researchers aiming to train VLMs. VARCO-VISION is available at https://huggingface.co/NCSOFT/VARCO-VISION-14B."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in Large Language Models (LLMs) have increased attention on handling multimodality, providing robust backbones for Vision-Language Models (VLMs). Incorporating high-performing LLMs in VLMs demonstrated significant improvements across various visual tasks requiring text understanding, reasoning, and generation abilities [1, 2, 4, 6, 7, 14, 16, 20, 24, 25, 27, 30, 31, 33, 34]. The development of Multimodal Large Language Models (MLLMs) can further widen the usage of AI models and enhance user experiences to great extent. Accordingly, the AI community, including both academia and industry, is committing substantial time and resources to train MLLMs [18, 39, 41] and establish evaluation frameworks [8, 38]. Although numerous multimodal models and benchmark datasets are being rapidly developed, they focus primarily on major languages such as English and Chinese [1, 6, 7, 14, 16, 19, 24, 34]. On the other hand, only handful of open-source and commercial MLLMs are available for low-resource languages. This may cause heavy reliance on proprietary model APIs for users, instead of fostering research environment. As of now, even in South Korea where huge AI community exists, there is limited selection of Korean-supported models and datasets. While open-source Korean datasets for simple vision-text tasks like Visual Question Answering (VQA) or Optical Character Recognition (OCR) can be found [12, 13], assessing the models general performance remains challenging. In this work, we present (1) strong English-Korean VLM called VARCO-VISION-14B (VARCOVISION for short) and (2) five Korean benchmarks. VARCO-VISION is trained for four distinct phases with the final preference optimization stage. To evaluate the models overall comprehension and generation abilities, we translated three closed-set (MMBench [22], SEED [17], MMStar [5]) and one open-set (LLaVA-W [21]) English benchmarks, and human-validated the datasets to ensure quality. These authors contributed equally to this work. Authors are listed in alphabetical order. Corresponding author"
        },
        {
            "title": "Technical Report",
            "content": "Figure 1: VARCO-VISION Application Examples: Visual Question Answering (VQA), Optical Character Recognition (OCR), Referring, and Grounding. Our model excels at both Korean/English vision-text and text-only tasks. Please see for more detailed examples. The Korean closed-set benchmarks, K-MMBench, K-SEED, and K-MMStar, are multiple-choice question answering (MCQA) tasks, which allow objective evaluation of MLLMs. K-LLaVA-W is based on LLaVA-Bench-in-the-wild (LLaVA-W) dataset with LLM-based automatic evaluation, and measures the Korean generation skill of model compared to GPT [2] model. Aside from these four benchmarks, we introduce novel closed-set Korean benchmark (K-DTCBench) on understanding documents, tables, and charts. This benchmark is designed from scratch to specifically assess the capability of VLMs to process diverse image types. We report performances of our own model as well as other baselines on English and Korean benchmarks to test the models bilingual proficiency and multimodal capabilities. The models were also evaluated on text-only benchmarks, as one of our primary goals is to develop VLM with strong language proficiency. In an overall quantitative evaluation encompassing both multimodal and text-only schemes, VARCO-VISION-14B not only surpasses other models of similar size in performance but also achieves scores comparable to proprietary models. We observe that the preference optimization phase significantly increases the readability and fluency of model outputs, leading to improved performance in K-LLaVA-W and text-only benchmarks. Furthermore, our model demonstrates proficient grounding and referring capabilities, which indicate its substantial potential for practical application. By releasing VARCO-VISION and five Korean vision-text benchmarks, we are looking forward to promoting more open AI community and expanding opportunities for researchers. Our contributions can be summarized as follows: 1. English-Korean Bilingual Model: We release powerful 14B bilingual vision-language model, VARCO-VISION, that outperforms other models of similar scale. Despite being VLM, it achieves high scores on language benchmarks in both Korean and English, demonstrating strong language capabilities. 2. High-quality Korean Multimodal Benchmarks: Based on widely recognized English multimodal benchmarks, we devise four closed-set and one open-set Korean benchmarks to evaluate VLMs bilingual proficiency. 3. Gradual Four-step Training: We train our model in four stages with different objectives, so that it can absorb visual and linguistic capabilities progressively without losing the pre-trained backbone models prior knowledge. As result, our model exhibits outstanding performance across various benchmarks. 4. Grounding, Referring, and OCR: VARCO-VISION is capable of grounding, referring, and OCR tasks in both Korean and English, showing its high potentials for applications in real-world scenarios."
        },
        {
            "title": "2.1 Model Architecture",
            "content": "VARCO-VISION-14B consists of three main components: vision encoder, projector, and Large Language Model (LLM). We leverage Qwen-2.5-14B-Instruct [32] as the language foundation model and SigLIP [37] as the vision encoder. The overall model architecture and visual representation processing method follow LLaVA-OneVision [16]. In this work, we concentrate on training VARCOVISION with single-image examples. Special tokens are added in the tokenizer for specific usages, such as OCR, grounding, and referring. The added special tokens are: <gro> for grounding tasks <ocr> for OCR tasks <char> </char> for indicating text phrase <obj> </obj> for indicating an object <bbox> </bbox> for representing bounding box <delim> for representing multiple location points for one object or text The specific examples for each of these tokens are illustrated in the Appendix B."
        },
        {
            "title": "2.2 Training Strategy",
            "content": "Our training pipeline aims to teach the model to gradually absorb and integrate both visual and linguistic understanding capabilities. Extending LLaVA-OneVisions training framework, we train our model in four stages. To preserve the models language proficiency throughout the training process, we incorporate text-only data from Stages 2 to 4. Stage 1. Feature Alignment Pre-training: We optimize the randomly initialized MLP projection layers while keeping other components frozen. This pre-training stage lets the model learn the mapping between the vision encoder and LLM. We use image-caption pairs as the training dataset to facilitate basic alignment between the two modalities. Stage 2. Basic Supervised Fine-tuning: All model layers are fully trained on six different tasks: basic instruction-following, OCR, grounding/referring, caption description, document/table/chart/mathematical contents, and text-only examples. The model can acquire fundamental vision-language capabilities from Stage 2. Stage 3. Advanced Supervised Fine-tuning: Advanced supervised fine-tuning is similar to Stage 2, but differs in that training tasks demand much more complex problem-solving abilities. The model is trained to enhance its reasoning and instruction-following skills across range of tasks, from detailed image analysis to multi-step reasoning. Stage 4. Preference Optimization: During the final stage, we focus exclusively on training the LLM layers to improve response alignment and generation capabilities. By applying Direct Preference Optimization (DPO) [28], we refine various aspects of the model responsesincluding but not limited to consistency, safety, and task-specific performance."
        },
        {
            "title": "3.1 Korean Evaluation Benchmarks",
            "content": "In this section, we explain how we devise five Korean multimodal benchmarks. This paper is the first to release open-source Korean evaluation benchmarks for general comprehension and Korean generation capabilities. Based on the widely recognized benchmarks, MMBench [22], SEED [17], MMStar [5], and LLaVA-W [21], we translate the datasets into Korean and curate the translated outputs to create high-quality benchmarks. K-DTCBench is Korean benchmark with documents, tables, and charts that we constructed from scratch. Four of the datasets (K-MMBench, K-SEED, K-MMStar, and 3 K-DTCBench) are closed-set multiple-choice question answering tasks, and K-LLaVA-W is an open-set freeform answer generation task with automatic LLM evaluation."
        },
        {
            "title": "3.1.1 Closed-set Dataset",
            "content": "For VLM evaluation, it is common to employ multiple-choice question answering benchmarks to test fundamental abilities in processing image-text information. The three selected English benchmarks (MMBench, SEED, and MMStar) are composed of various evaluation dimensions and were thoroughly curated, making them suitable candidates for reference benchmarks. When using these benchmarks, we utilize the latest GPT API 3 to translate the datasets and enhance the outputs with the help of human annotators. Post-editing was necessary to improve the fluency and accuracy of translation due to translationese and localization issues. K-MMBench4: The original English MMBench is comprised of 20 ability dimensions, such as identity reasoning, image emotion, and attribute recognition. We use questions and ground truth answers from the dev subset of English MMBench. K-SEED5: SEED-Bench consists of images and videos, and evaluate VLMs with regard to 12 dimensions. From English SEED-Bench, we select the first 20 percent of questions requiring images. K-MMStar6: MMStar is vision-indispensable benchmark of 1500 image-oriented questions. We observe that there are unanswerable cases (e.g., multiple images required to answer the question but only have single image, vague questions or options) in the original MMStar dataset. Thus, we modify or re-create the questions to ensure that they can be answered within single image. The examples of K-MMStar can be found in the Appendix A.1. K-DTCBench7: K-DTCBench is newly developed benchmark featuring both computergenerated and handwritten images of three different types (documents, tables, and charts), all written in Korean. It consists of 80 questions for each image type and two questions per image, summing up to 240 questions in total. This benchmark is designed to evaluate whether VLMs can process images in different formats and be applicable to diverse domains. All images are generated with made-up values and statements for evaluation purposes only. We scanned handwritten documents/tables/charts, or created digital objects with matplotlib library to build K-DTCBench. The proportions of digital and handwritten images are equal, each constituting 50%."
        },
        {
            "title": "3.1.2 Open-set Dataset",
            "content": "While it is important for VLMs to predict correct answers for given questions, generating fluent outputs is also significant task for models. However, to the best of our knowledge, there are currently no Korean benchmarks available for assessing the quality of models generation capability. Therefore, we adopt LLaVA-Bench-in-the-wild (LLaVA-W) benchmark and translate-validate the benchmark as we did for the closed-set benchmarks. LLaVA-W contains 24 images of various domains and 60 daily-life questions. Since our goal was to build benchmark exclusively focused on Korean, we change the English texts in images into Korean for localization. Figure 4 shows the examples of LLaVA-W and K-LLaVA-W8. Given an image and question related to the image, models need to generate open-ended answers. The captions of the images are only used during evaluation. For evaluation, our benchmark follows the pipeline of LLaVA-Ws LLM automatic evaluation 9, but with little twist in the JudgeLLM prompts. We convert English prompts into Korean and added instruction details as shown in the Appendix A.3.2. Based on the provided caption, question, and 3gpt-4o-2024-08-06 4https://huggingface.co/datasets/NCSOFT/K-MMBench 5https://huggingface.co/datasets/NCSOFT/K-SEED 6https://huggingface.co/datasets/NCSOFT/K-MMStar 7https://huggingface.co/datasets/NCSOFT/K-DTCBench 8https://huggingface.co/datasets/NCSOFT/K-LLaVA-W 9https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/main/lmms_eval/tasks/ llava-in-the-wild 4 models generated output, JudgeLLM measures the models helpfulness, relevance, accuracy, level of detail, and Korean generation capability. The final K-LLaVA-W score of the target model is calculated as the ratio of the target models JudgeLLM score to the baseline models JudgeLLM score."
        },
        {
            "title": "3.2 Benchmark Results",
            "content": "We leverage four types of benchmarks to fully evaluate VARCO-VISIONs performance on diverse dimensions. To assess the models ability to understand and generate both languages, ten benchmarksfive each for Korean and Englishare used for evaluation. We also use text-only benchmarks to test the effectiveness of our training strategy to incorporate text datasets throughout Stages 2 to 4. In addition to comparing with other VLMs, we evaluate our model on OCR tasks and compare its performance with that of OCR-focused models."
        },
        {
            "title": "3.2.1 Korean Benchmarks",
            "content": "Model VARCO-VISION-14B Pangea-7B [36] Pixtral-12B [3] Molmo-7B-D [7] Qwen2-VL-7B-Instruct [33] LLaVA-OneVision-7B [16] Qwen2-VL-72B-Instruct [33] LLaVA-OneVision-72B [16] GPT-4o-mini [2] GPT-4V [2] GPT-4o [2] KOREAN BENCHMARKS Image Understanding (MCQA) Generation (Image-based) K-MMB (dev) K-SEED K-MMSTAR K-DTCBench K-LLaVA-W 82. 71.64 57.47 63.83 78.26 76.28 84. 88.01 74.48 77.92 81.01 75.39 73. 46.44 69.53 74.08 73.21 78.25 77. 73.30 71.66 76.98 57.33 35.00 23. 47.40 50.67 54.00 63.53 62.66 42. 35.20 56.20 84.58 48.33 27.50 45. 75.00 52.91 88.75 60.83 74.58 47. 85.80 84.74 69.70 82.00 63.90 62. 48.80 97.40 84.10 101.90 98.90 103. Table 1: Model Comparison on Korean Benchmarks. The models in the first upper block are opensource models with similar scale, and the second block are open-source 72B models. The last block shows the performance of proprietary GPT models. We primarily compare VARCO-VISION with the models mentioned in the first block, as they are similar in size to our model. As model trained with strong emphasis on Korean linguistic ability, VARCO-VISION excels in all MCQA benchmarks compared to the models with similar size. In general comprehension tasks like K-MMBench and K-SEED, Qwen2-VL-7B-Instruct [33] and LLaVA-OneVision-7B [16] are next to our model, showing slightly lower performance overall. While models have similar scores in KMMBench, K-SEED, and K-MMStar, we observe significant variation in the performance of models in K-DTCBench while VARCO-VISION achieving dominant performance. This suggests that Korean documents, tables, and charts were not adequately trained by other open-source models. Moreover, VARCO-VISION reaches competitive performance to large-scale open-source and proprietary models, gaining higher scores than GPT-4o-mini and GPT-4V for MCQA tasks. In K-LLaVA-W where fluent Korean generation is key value, only two models among models under 20B, VARCO-VISION-14B and Pixtral-12B [3], obtain score over 80. On the other hand, Qwen2-VL-72B-Instruct [33] and GPT models achieve scores around 100, implying that model scale may affect the response quality in longer text generation. 10We used gpt-4o-2024-08-06 for both JudgeLLM and the baseline model. The original LLaVA-W provides GPT responses. However, instead of translating the responses in LLaVA-W, we ran the latest GPT model to obtain high-quality responses."
        },
        {
            "title": "3.2.2 English Benchmarks",
            "content": "Model VARCO-VISION-14B Pangea-7B [36] Pixtral-12B [3] Molmo-7B-D [7] Qwen2-VL-7B-Instruct [33] LLaVA-OneVision-7B [16] Qwen2-VL-72B-Instruct [33] LLaVA-OneVision-72B [16] GPT-4o-mini [2] GPT-4V [2] GPT-4o [2] ENGLISH BENCHMARKS Image Understanding (MCQA) OCR MMBv1.1 (dev) SEED (image) MMStar (val) MMMU (val) OCRBench (test) 84. 76.23 72.98 72.05 80.95 80.80 86. 85.44 76.31 79.41 81.73 76.73 74. 74.34 74.36 76.45 76.41 77.86 77. 72.80 73.00 76.70 63.33 43.26 48. 52.73 60.00 61.33 67.60 65.33 54. 56.00 64.70 51.33 43.55 49.00 45. 54.10 47.67 56.66 56.80 60.00 62. 69.90 820 620 682 708 630 877 741 785 656 Table 2: Model Comparison on English Benchmarks. MMBench [22], SEED [17], MMStar [5], and MMMU [35] are multi-choice question answering tasks. MMBench and SEED are for comprehension evaluation, whereas MMStar is focused more on vision-indispensible reasoning. MMMU tests collegelevel subject knowledge of VLMs. OCRBench [23] is specialized benchmark in OCR for VLMs, composed of 1000 question-answer pairs. The values in OCRBench indicate the number of questions correctly answered by models. Our goal in training lies in boosting Korean and English proficiency, thus evaluating on English benchmarks was necessary to investigate our models performance. In English MCQA benchmarks, VARCO-VISION gains higher performances over other models under 20B in all benchmarks, except for MMMU. However, we notice that all models generally demonstrate sufficient performance in English benchmarks in contrast to their performances in Korean benchmarks. The results suggest that training schemes for the majority of the models in Table 2 prioritized learning English. In OCRBench, our model shows superior performance over other VLMs, spotlighting the effectiveness of Stages 2 and 3, where the model is exposed to OCR, grounding, and referring tasks."
        },
        {
            "title": "3.2.3 Text-only Benchmarks",
            "content": "TEXT-ONLY BENCHMARKS Model VARCO-VISION-14B Pangea-7B [36] Pixtral-12B [3] Molmo-7B-D [7] Qwen2-VL-7B-Instruct [33] LLaVA-OneVision-7B [16] Qwen2-VL-72B-Instruct [33] LLaVA-OneVision-72B [16] EXAONE 3.0 7.8B Inst.(LLM) [29] GPT-4o-mini [2] GPT-4V [2] GPT-4o [2] Korean English LogicKor KoMT-Bench MT-Bench 8. 5.06 7.71 2.64 4.62 2.23 7. 8.22 8.62 9.14 8.66 9.57 8. 5.06 8.11 3.58 4.54 3.52 7. 7.87 8.92 8.88 9.25 9.24 8. 7.29 8.40 6.93 7.13 7.52 8. 8.78 9.01 9.09 9.41 9.30 Table 3: Model Performance on Korean and English Text-only Benchmarks. MT-Bench [40] is an English multi-turn dialogue benchmark, and KoMT-Bench [29] is built by translating MT-Bench. LogicKor11 consists of multi-turn Korean dialogues across six categories. 6 We evaluate the models on text-only benchmarks to investigate whether our strategy of including text-only datasets during training stages contributed to textual understanding of VARCO-VISION. Two Korean language benchmarks (LogicKor11 and KoMT-Bench [29]) and one English benchmark (MT-Bench [40]) are employed for text-only evaluation. Our model outperforms other VLMs in text-only benchmarks, even when compared to 72B open-source models. We believe that applying preference optimization in the final training phase boosted the overall quality of model responses, resulting in outstanding performance in all three text-only benchmarks. VARCO-VISION achieves scores comparable to that of EXAONE 3.0 7.8B, which is Korean-English language model only trained on textual inputs. It reflects our models capability to handle textual inputs as much as other bilingual language models."
        },
        {
            "title": "OCR",
            "content": "CORD ICDAR2013 ICDAR2015 VARCO-VISION-14B EasyOCR12 Pororo [9] PaddleOCR13 CLOVA OCR14 82. 79.56 78.73 92.71 95.32 94.42 84. 84.29 92.01 94.39 72.95 57.90 64. 73.73 84."
        },
        {
            "title": "Average",
            "content": "83.35 74.14 75.89 86.15 91.26 Table 4: OCR Benchmark Performance. EasyOCR, Pororo, and PaddleOCR are open-source OCR models. CLOVA OCR is proprietary OCR model. PopEval [15] was used as the metric for all benchmarks. We compare our models OCR ability to other well-known OCR models using the PopEval [15] metric. In contrast to OCRBench, CORD [26], ICDAR2013 [10], and ICDAR2015 [11] are more complex tasks that require models to generate both textual content and spatial locations of texts within images. Despite the fact that other models in Table 4 are OCR expert models while VARCO-VISION is not, its performances across these benchmarks are remarkable. We find that VARCO-VISION holds the potential for use in wide range of applications, not limited to specific tasks."
        },
        {
            "title": "4 Discussion and Future Work",
            "content": "Throughout the process of model training and evaluation, we notice that benchmark performances do not fully reflect models true capability. MMBench[22] shows the possibility of choice biases in MLLMs that they tend to prefer certain choice in MCQA tasks, and apply Circular Evaluation to regularize the problem. Nevertheless, the majority of multi-choice question answering tasks still follow naive evaluation techniques. In addition, there are relatively small number of benchmarks aiming to evaluate the models generation capability compared to benchmarks with short answers [19]. Although model might achieve high performance in MCQA tasks, it may produce low-quality answers for tasks that require long responses. We seek significant future developments remain to be made in terms of MLLM benchmarks. In this work, we focus on training VARCO-VISION on single-image scenarios, and the evaluation results show that our model excels in single-image benchmarks. However, MLLMs need to process multi-image scenarios (including videos) and sounds to be applicable in various domains. We plan to expand VARCO-VISIONs modalities to video and audio in the near future, and broaden the horizon of our models usage. Besides modality expansion, we are currently training an advanced, localized model with diverse images taken in Korea. With the improved capability to understand Korean culture, 11https://lk.instruct.kr 12https://github.com/JaidedAI/EasyOCR 13https://github.com/PaddlePaddle/PaddleOCR 14https://www.ncloud.com/product/aiService/ocr 7 the future model is expected to be applied in real-world tasks, such as multimodal search, multimodal retrieval-augmented generation, and visual agents."
        },
        {
            "title": "5 Conclusion",
            "content": "We present an open-source Korean-English vision-language model, VARCO-VISION-14B, and five Korean benchmarks. Although extensive research and numerous models have been developed for MLLMs, this work is the first to release both bilingual VLM and benchmarks supporting Korean. Our model achieves remarkable results in both Korean and English benchmarks among other open-source models of similar scale. VARCO-VISION does not only excel in vision-text benchmarks, but also in text-only and OCR benchmarks. We find that this milestone may expand opportunities for many researchers and enable breakthroughs in training bilingual VLMs."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT(MSIT, Korea)&Gwangju Metropolitan City."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. [4] Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. [5] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models? In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=evP9mxNNxJ. [6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [7] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [8] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2024. URL https://arxiv.org/abs/ 2407.11691. [9] Hoon Heo, Hyunwoong Ko, Soohwan Kim, Gunsoo Han, Jiwoo Park, and Kyubyong Park. Pororo: Platform of neural models for natural language processing. https://github.com/ kakaobrain/pororo, 2021. 8 [10] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. Icdar 2013 robust reading competition. In 2013 12th international conference on document analysis and recognition, pages 14841493. IEEE, 2013. [11] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, In 2015 13th international Shijian Lu, et al. conference on document analysis and recognition (ICDAR), pages 11561160. IEEE, 2015. Icdar 2015 competition on robust reading. [12] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV), 2022. [13] Jin-hwa Kim, Soohyun Lim, Jaesun Park, and Hansu Cho. Korean Localization of Visual Question Answering for Blind People. In AI for Social Good workshop at NeurIPS, 2019. [14] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding vision-language models: insights and future directions. arXiv preprint arXiv:2408.12637, 2024. [15] Hong-Seok Lee, Youngmin Yoon, Pil Hoon Jang, and Chankyu Choi. Popeval: character-level approach to end-to-end evaluation compatible with word-level benchmark dataset. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 12071213. IEEE, 2019. [16] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [17] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. [18] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [19] Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, Ying Tai, Wankou Yang, Yabiao Wang, and Chengjie Wang. survey on benchmarks of multimodal large language models, 2024. URL https://arxiv.org/abs/ 2408.08632. [20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 3489234916. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf. [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [22] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216233. Springer, 2025. [23] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. [24] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 9 [25] Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models, 2024. URL https://arxiv.org/abs/2403.03003. [26] Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo, and Hwalsuk Lee. Cord: consolidated receipt dataset for post-ocr parsing. In Document Intelligence Workshop at Neural Information Processing Systems, 2019. [27] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world, 2023. URL https://arxiv.org/abs/2306.14824. [28] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [29] LG Research, Soyoung An, Kyunghoon Bae, Eunbi Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Yeonjung Hong, Junwon Hwang, Hyojin Jeon, et al. Exaone 3.0 7.8 instruction tuned language model. arXiv preprint arXiv:2408.03541, 2024. [30] DongJae Shin, HyeonSeok Lim, Inho Won, ChangSu Choi, Minjun Kim, SeungWoo Song, HanGyeol Yoo, SangMin Kim, and KyungTae Lim. X-LLaVA: Optimizing bilingual large vision-language alignment. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 24632473, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-naacl.158. URL https://aclanthology.org/2024.findings-naacl.158. [31] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [32] Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https:// qwenlm.github.io/blog/qwen2.5/. [33] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [34] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [35] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [36] Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. Pangea: fully open multilingual multimodal llm for 39 languages. arXiv preprint arXiv:2410.16153, 2024. [37] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. [38] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. URL https://arxiv.org/abs/2407.12772. [39] Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/2408. 05517. [40] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [41] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and In Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372."
        },
        {
            "title": "A Korean Multimodal Benchmarks",
            "content": "In this section, we provide examples of Korean vision-text benchmarks (K-MMStar, K-DTCBench, and K-LLaVA-W). Since these three benchmarks required more than simple translation, we elaborate on benchmark construction with specific examples. A.1 K-MMStar Figure 2: K-MMStar Example K-MMStar has three different types of questions. We noticed that there are unanswerable or vague questions in the original MMStar, thus modified the question or created new one. Figure 2 shows the examples of questions. Type 1 refers to cases where direct translations from English to Korean are sufficient for Korean version questions. In Type 2, the question from MMStar asks about which sport is being played in the image. However, the options include both football and soccer, either of which can be correct depending on British English or American English conventions. Therefore, we changed the options to 축구 (soccer) and 미식축구 (American football) for clarity. In Type 3 example, it has <image 1>, <image 2>, and <image 3> in the options but MMStar did not provide images corresponding to the options. Hence, we re-created new question about the image. A.2 K-DTCBench K-DTCBench is developed from scratch with synthetic images. Each image has question and four options to choose from. 12 Figure 3: K-DTCBench Example A.3 K-LLaVA-W A.3.1 Example In K-LLaVA-W, we changed the English text into Korean text for images with texts. If an original LLaVA-W image did not contain any text, we left it unchanged to preserve its authenticity. In Figure 4, we changed MONDAY. JUST... MONDAY into 월요일. 단지... 월요일. Figure 4: K-LLaVA-W Example 13 A.3.2 K-LLaVA-W JudgeLLM Prompt Figure 5: K-LLaVA-W Evaluation Prompt. We translated the LLaVA-W prompts and added specific guidelines in the JudgeLLM prompt."
        },
        {
            "title": "B Application Examples",
            "content": "Figure 6: Text Recognition and Analysis Example (English) 15 Figure 7: Mathematical Reasoning Example (Korean) 16 Figure 8: Information Extraction and Calculation Example (Korean) Figure 9: Grounding Example (Korean) 18 Figure 10: Grounding Example (English) 19 Figure 11: Referring Example (Korean) Figure 12: OCR Example 21 Figure 13: OCR Example 22 Figure 14: Summarization Example (English) Figure 15: Text Recognition Example (Korean)"
        }
    ],
    "affiliations": [
        "NC Research, NCSOFT"
    ]
}