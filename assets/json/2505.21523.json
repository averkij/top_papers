{
    "paper_title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models",
    "authors": [
        "Chengzhi Liu",
        "Zhongxing Xu",
        "Qingyue Wei",
        "Juncheng Wu",
        "James Zou",
        "Xin Eric Wang",
        "Yuyin Zhou",
        "Sheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 3 2 5 1 2 . 5 0 5 2 : r More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models Chengzhi Liu*1,3, Zhongxing Xu*1, Qingyue Wei2, Juncheng Wu1, James Zou2, Xin Eric Wang1,3, Yuyin Zhou1, Sheng Liu2 1UC Santa Cruz, 2Stanford University, 3UC Santa Barbara chengzhi@ucsb.edu, zxu283@ucsc.edu, sl5924@nyu.edu"
        },
        {
            "title": "Abstract",
            "content": "Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, we observe that this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more on language priors. Attention analysis reveals that longer reasoning chains reduce focus on visual inputs, contributing to hallucination. To systematically study this phenomenon, we introduce RH-AUC, metric that quantifies how models perception accuracy changes with reasoning length, enabling evaluation of whether the model preserves visual grounding while reasoning. We also release RH-Bench, diagnostic benchmark covering diverse multimodal tasks, designed to jointly assess the balance of reasoning ability and hallucination. We find that (i) larger models generally exhibit better balance between reasoning and perception; (ii) reasoning and perception balance depends more on the types and domains of the training data than its volume. Our findings highlight the need for evaluation frameworks that account for both reasoning quality and perceptual reliability."
        },
        {
            "title": "Introduction",
            "content": "Large reasoning models scale test-time computation to improve complex reasoning. These models [6, 7, 26, 2] generate longer outputs and engage in deeper reasoning before producing final answers, resulting in more comprehensive solutions for complex mathematical and scientific problems. This paradigm has been extended to multimodal large language models: non-reasoning base models are supervised finetuned (SFT), or finetuned with reinforcement learning (RL) to obtain strong reasoning ability [29, 49, 8, 42, 47], demonstrating exceptional capabilities in multimodal reasoning tasks, particularly in domains like mathematical problem solving. Most existing studies on multimodal reasoning models focus on enhancing reasoning performance, with limited attention paid to perception-focused tasks. As illustrated in Figure 1a, although the reasoning model generates an extended reasoning chain in visual question answering, its answer is largely driven by language priors rather than visual evidence, leading to hallucination. Our empirical study reveals consistent and significant finding: although reasoning models can generate more detailed reasoning chains, they introduce more hallucinations in perception-focused tasks than the non-reasoning counterparts, as shown in Figure 1b. Through attention analysis, we investigate the decrease of attention on visual tokens in multimodal reasoning models, which exacerbates visual hallucinations. The reasoning model allocates significantly less attention to visual tokens compared to its non-reasoning counterpart, while directing more attention to the instruction tokens. This bias increases reliance on language priors and amplifies hallucination risk. Moreover, the extension of the reasoning chain further weakens the visual attenPreprint. Under review. Figure 1: (a) Example of outputs from reasoning model and non-reasoning model on perception task. Red highlights indicate visual hallucination. Multimodal reasoning models are generally more prone to amplifying hallucinations during the reasoning process compared to their non-reasoning counterparts. (b) Performance of different models on reasoning and perception tasks in the RH-Bench dataset. Better performing models are positioned in the upper right corner. Baseline non-reasoning models of varying scales typically exhibit weaker reasoning capabilities and fewer hallucination, whereas reasoning models display the opposite trend. tion allocation, leading to an increase in hallucinations, as the model becomes more dependent on language-based reasoning rather than visual evidence. Based on these findings, we further investigate the impact of reasoning chain length on model reasoning and hallucination. The results indicate that the influence of reasoning chain length on reasoning-hallucination exhibits non-monotonic relationship. Additionally, the optimal reasoning range differs across tasks, while traditional evaluation metrics, such as accuracy and hallucination rate, are inadequate for capturing the dynamic balance between reasoning and visual grounding. To address this, we introduce RH-AUC, new metric designed to assess the balance between reasoning and hallucination in multimodal reasoning models. This metric is computed by calculating the area under the curve formed by reasoning performance and hallucination performance at different reasoning lengths, with higher values indicating better balance. Alongside this metric, we release RH-Bench, diagnostic benchmark containing 1,000 samples across various reasoning and perception tasks, with each task featuring both multiple-choice questions and open-ended questions. Through the evaluation of RH-Bench, we observe three key findings: (i) Larger models typically demonstrate better reasoning and hallucination balance. (ii) RL-only training models promote more adaptive reasoning, resulting in better balance between reasoning and hallucination compared to SFT+RL. (iii) Reasoning-Hallucination balance is more influenced by the types and domains of the training data than by its volume. To sum up, our contributions are listed as follows: We observe that multimodal reasoning models are more prone to hallucinations than their nonreasoning counterparts in perception tasks, which can be attributed to decline in visual attention allocation. Longer reasoning chains further diminish visual attention. We reveal that the relationship between reasoning chain length and the models reasoning and perception performance is non-monotonic, with the optimal length varying across tasks. We introduce the new RH-AUC metric and the RH-Bench diagnostic dataset to systematically evaluate the balance between reasoning and hallucination across varying reasoning lengths in multimodal reasoning models."
        },
        {
            "title": "2 Multimodal Reasoning Can Amplify Visual Hallucination",
            "content": "In this section, we begin by investigating whether multimodal reasoning models introduce more hallucination in perception-focused tasks. Specifically, we compare 8 recent multimodal reasoning models against their backbone non-reasoning-based counterparts across multiple hallucination benchmarks, including MMVP [35], MMEval-Pro [11],VMCBench [54],Bingo [5],MMHAL [32]. 2.1 Hallucination Increases Consistently Compared to Base Models To systematically assess the impact of multimodal reasoning on visual grounding, we evaluated eight reasoning-augmented models against their non-reasoning Qwen2.5-VL backbones on five 2 hallucination datasets. As shown in Figure 2, all reasoning models trace markedly smaller radar areas than their baselines, indicating uniformly higher hallucination rates on perception-focused tasks. This deficit remains consistent at both the 3 and 7 scales, demonstrating that the elevated hallucination rate stems from the reasoning paradigm itself rather than model size. Figure 2: Comparison of reasoning and non-reasoning models on five perception benchmarks. Results are shown for 3B models (left) and 7B models (right). Higher scores indicate lower hallucination. 2.2 Does Training Paradigm Matter? Comparison Between RL and SFT+RL Current multimodal reasoning models typically adopt one of two training regimes: (1) pure reinforcement learning (RL-only) or (2) supervised fine-tuning followed by reinforcement learning (SFT+RL). Figure 3 shows consistent performance hierarchy across four perception benchmarks: The Qwen2.5-VL baseline achieves the highest scores, followed by RL-only fine-tuning, with the SFT+RL pipeline performing the worst. This pattern highlights the robustness of baseline model in visual grounding and indicates that subsequent RL or hybrid fine-tuning weakens this robustness, with the supervised-preceded RL strategy leading to the most significant performance degradation. Figure 3: Performance across four perception benchmarks comparing Base, RL, and SFT+RL. 2.3 Case Study of Hallucinations in Reasoning Models Figure 4 presents two representative hallucination patterns observed in multimodal reasoning models, arising from visual misrecognition and reasoning bias, respectively. In Figure 4a, the reasoning model fails to identify fine-grained visual cues and miscounts four individuals as three, reflecting localized deficiency in visual perception. In Figure 4b, the reasoning model increasingly relies on linguistic priors during the reasoning process while overlooking early visual evidence, ultimately generating an incorrect response. In contrast, the baseline model exhibits lower hallucination rate under identical inputs. These observations raise crucial question: why do multimodal reasoning models, despite their strong reasoning performance, exhibit weakened visual grounding? In the next section, we provides an in-depth analysis based on the internal attention mechanisms of the reasoning models. Figure 4: Two common types of hallucination patterns observed in multimodal reasoning models. (a) corresponds to hallucinations caused by visual misrecognition, while (b) reflects hallucinations arising from reasoning biases. Hallucinated spans are highlighted in red. Takeaway 1: Reasoning Models Amplify Visual Hallucinations Across training paradigms and model scales, multi-modal reasoning models exhibit consistent drop in accuracy and rise in hallucination rates on general visual benchmarks."
        },
        {
            "title": "3 Why Reasoning Models Amplify Hallucinations?",
            "content": "Many previous studies have investigated the role of attention mechanisms in hallucination, identifying insufficient attention allocation as potential key factor contributing to hallucinations[12, 13, 46]. In this section, we conduct an attention based analysis to explore the underlying causes of hallucination amplification in multimodal reasoning models. Section 3.1 indicates that hallucinations may result from limited attention allocated to visual inputs, while Section 3.2 shows that longer reasoning chains further weaken the models visual focus. 3.1 Hallucination Resulting from Weak Visual Attention We conduct comparative analysis of the attention distributions over visual, instruction, and system tokens across all layers in the reasoning and non-reasoning models. As shown in Figure 5a, the reasoning model consistently assigns low attention to visual tokens, with further decrease observed in deeper layers, indicating limited ability to integrate visual evidence. Meanwhile, more attention is shifted to instruction tokens, reflecting heightened reliance on linguistic priors. In contrast, the non-reasoning maintains relatively high and stable level of visual attention from shallow to intermediate layers. The visual attention heatmap in Figure 5b further supports this observation: while the non-reasoning model progressively focuses on semantically salient regions, the reasoning model exhibits sparse and dispersed attention, failing to consistently engage with key visual areas. This phenomenon indicates that the weakening of visual attention undermines the reasoning models ability to achieve effective visual grounding, exacerbating the occurrence of hallucinations. Figure 5: Attention allocation and visual grounding between reasoning and non reasoning models. The reduction of visual attention in reasoning models amplifies visual hallucinations. 4 3.2 Visual Focus Declines with Longer Reasoning Chains As shown in Figure 6, we visualize the attention distributions of the reasoning model under two reasoning modes: normal thinking and overthinking. As the reasoning chain length increases, the heatmaps clearly reveal systematic shift in the models attention focus: under the overthinking mode, attention to visual tokens significantly decreases, while attention to instruction tokens intensifies. This pattern indicates that longer reasoning chains cause the model to increasingly rely on linguistic cues rather than grounded visual evidence. For instance, when asked whether gray wall is present, the model under normal thinking correctly identifies the gray well and provides correct response. In contrast, under over-reasoning conditions, the model exhibits further diminished attention to visual tokens, with increased focus directed toward the end of the user instruction. This suggests that longer reasoning chains tend to further exacerbate the degradation of the models visual grounding, potentially leading to an increase in hallucinations. Figure 6: Attention shift in the reasoning model under different reasoning length. In normal thinking, the model generates outputs as typically expected, while in overthinking, the reasoning length is adjusted using Latent State Steering (Section 4.1). Longer reasoning chains further exacerbate the degradation of attention to visual information and focus toward linguistic priors."
        },
        {
            "title": "4 Effects of Reasoning Length on Reasoning-Hallucination Balance",
            "content": "In this section, we explore the impact of reasoning length on the balance between hallucination and reasoning. Section 4.1 provides an overview of the proposed control strategy: latent state steering as well as techniques that are previously used in the literature[25]: budget forcing, and test time scaling. In Section 4.2, we explore the optimal generation length for various benchmarks and analyze the trade-off between hallucination and reasoning performance as reasoning length varies. 4.1 Overview of Reasoning Length Control Straregies To systematically control the reasoning length in reasoning models, we adopt three strategies: (1) Token Budget Forcing: hard constraint on reasoning length is enforced by predefining generation budget at decoding time, directly limiting the number of tokens allocated for the reasoning. (2) Test Time Scaling: Reasoning is incrementally extended during inference through staged generation. The model first produces partial reasoning under 4096-token constraint and halts midway. It is then prompted to continue by appending simple token (\"Wait\"), enabling soft extension of reasoning while preserving contextual coherence. (3) Latent State Steering: Inspired by recent works on latent space steering for behavior control in large language models [18, 17, 1, 23], we propose method to steer the model toward generating reasoning traces of varying lengths. Specifically, we extract steering directions from the post-attention hidden states by computing the difference of latent states between long and short reasoning trajectories. These direction vectors are obtained and applied across all layers of the text decoder, with scaling factor controlling both the magnitude of guidance on the reasoning length. Specifically, we collect responses from the test benchmark and categorize them into long reasoning traces Rlong and short reasoning traces Rshort based on token length. The query and reasoning steps for each sample are input into the model, from which hidden representations Sℓ are extracted at each layer. Sℓ(q, t) denotes the hidden representation at layer ℓ for token position in the response to query q. We 5 compute the average hidden representation over reasoning tokens, where Hi represents the set of token positions within the reasoning span. The average representation is then calculated across the long and short reasoning traces to obtain layerwise embeddings: 1 1 Rshort Hi 1 Rlong Sℓ(q, t), ℓ 1 Hi Sℓ(q, t) short = long = (cid:88) (cid:88) (cid:88) (cid:88) ℓ (1) qRlong tHi qRshort tHi The reasoning length direction at layer ℓ is defined as the difference between the long and short embeddings, denoted as dℓ, which captures the variation in the models representation resulting from different reasoning chain lengths. To adjust the hidden representation based on this direction, We introduce parameter α [0.15, 0.15] to dynamically control the reasoning length and its magnitude. As α increases, the length of the reasoning chain extends, as shown below: dℓ = ℓ long ℓ short, ℓ steering = ℓ + αdℓ. (2) These strategies are applied to five representative multimodal reasoning models and evaluated on six benchmark datasets, covering both reasoning and perception tasks. In Figure 7, we present two benchmarks for both tasks. All implementation details and results are provided in Appendix C. Figure 7: Reasoning-Hallucination balance of multimodal reasoning models under varying reasoning lengths. Thinking lengths are controlled within [0600] tokens for reasoning and [0300] for hallucination, corresponding to the longer chains required for reasoning and shorter for hallucination. 4.2 Dynamic Balance Between Reasoning and Hallucination Non-monotonic Effect of Reasoning Length on Reasoning and Perception Performance. The relationship between reasoning length and model performance typically exhibits non-monotonic pattern under reasoning and perception tasks. Across various length control strategies, consistent trend emerges: moderate reasoning depth tends to yield optimal performance, whereas overly short or excessively long reasoning chains often lead to decline in accuracy. As shown in Figure 7, we employ the Latent State Steering method adjusts the thinking step for reasoning and perception tasks. It is evident that as the thinking length increases, the models performance across tasks generally follows rising-then-falling trajectory. This indicates that enhanced reasoning does not linearly improve model performance, but instead follows dynamic trade-off pattern. Task-Specific Variability of Optimal Reasoning Intervals. While most tasks exhibit non-monotonic relationships between reasoning length and performance, we further observe that the optimal reasoning length varies significantly across tasks. Figure 7 reveals that reasoning benchmarks such as MathVista [21] tend to benefit from longer reasoning chains, whereas perception and hallucinationoriented tasks such as MMHalu achieve their best performance at shorter or moderate lengths. This indicates that the balance between reasoning depth and performance is task-specific, and unified length control strategies are unlikely to be effective across all task types. Impact of the Zerothink Condition. Zerothink retains the reasoning structure but lacks substantive content. As shown in Figure 7, this setting leads to consistent drop in model performance on both reasoning and perception benchmarks, notably lower than results under normal reasoning lengths. These results indicate that the absence of reasoning content diminishes the reasoning models performance in both perception and reasoning. Limitation of Conventional Metric. Conventional metrics like reasoning accuracy and hallucination rate, when computed at fixed generation length, fail to capture the dynamic balance between deeper 6 reasoning and perception. Figure 7 shows that reasoning and perception often peak at different reasoning lengths, making it misleading to evaluate models using single-point metrics or simple averages between reasoning and hallucination performance. For instance, short reasoning trace may yield lower hallucination rate but poor reasoning depth, while longer trace may improve reasoning at the cost of increased hallucination, yet both scenarios could yield the same average score. To capture this evolving balance, in the next section, we propose an AUC-style metric that summarizes the balance curve between reasoning and perception fidelity across various reasoning lengths. This provides more faithful and holistic measure of performance, revealing both the models optimal balance and its stability across varying generation lengths. Takeaway 2: Moderate Reasoning Length Strikes the Best Reasoning-Hallucination Balance Reasoning length exerts non-monotonic effect on model performance: both insufficient and excessive reasoning degrade accuracy, and the optimal length is task-dependent."
        },
        {
            "title": "5 Evaluation on the Reasoning-Hallucination Balance",
            "content": "To comprehensively quantify the balance between reasoning and hallucination in multimodal large reasoning models at different reasoning depths, we introduce new metric RH-AUC. This metric captures how hallucination risk evolves with reasoning depth while also reflecting the cumulative effects of reasoning and perception. Additionally, we present RH-Bench, new diagnostic dataset of 1000 samples, designed for the integrated evaluation of reasoning and perception tasks, offering robust basis for analyzing reasoning ability and perceptual hallucinations. 5.1 Setup Benchmark Overview. RH-Bench consists of two types of tasks: reasoning and perception, with each task including two types of questions: multiple-choice and open-ended. The reasoning task includes 500 samples sourced from MathVision [37], MathVista [21], MMMU [48], and ScienceQA [22], while the visual reasoning task includes 500 samples from MMhalu, MMVP, HallusionBench, and VMCBench. Both task types use accuracy as the evaluation metric. For multiple-choice questions, evaluation is based on matching the final options. For open-ended questions, both tasks are evaluated using GPT-4o. The reasoning task determines whether the generated response is consistent with the correct answer, whereas the visual task evaluates the generated response against the correct answer, assigning score within the range of 0 to 6. Responses with score below 3 are classified as hallucinations. All sample ground-truth and evaluation answers have undergone manual inspection. Method Paradigms Perception Reasoning Training Data RH-AUC LLM-R1-3B Curr-ReFT-3B Ocean-R1-3B R1-OneVision-7B ThinkLite-VL-7B OpenVLThinker-7B MM-Eureka-7B MM-R1-7B Ocean-R1-7B RL SFT+RL RL SFT+RL RL SFT+RL RL RL RL Acc.(%) Length Acc.(%) Length Perc. Reas. 40k 3k 63k 77k 8k 25k 15k 6k 63k 391.8 472.61 414.5 457.3 435.4 460.1 450.5 430.0 262. 121.9 133.7 131.2 162.9 110.4 187.7 139.6 139.6 90.4 48.7 50.6 52.8 55.7 63.3 59.2 62.0 60.3 62.3 43.8 42.5 45.6 44.2 50.4 48.9 54.0 54.0 51.8 65k 6k 20k 80k 62k 25k - - 20k 0.46 0.47 0.53 0.46 0.52 0.54 0.55 0.57 0.63 Table 1: Comparison of model performance on RH-Bench, including task-specific accuracy and RHAUC scores. Perc. and Reas. respectively denote training data for visual perception and reasoning. RH-AUC We define reasoning length as , which controls the extent of the models generated reasoning trace. For each length , we compute RT , which represents the reasoning performance at length , and HT , representing performance on hallucination at the same length. By evaluating the model at multiple lengths on the RH-bench benchmark, we obtain series of (RT , HT ) pairs that form balance curve between reasoning and perceptual hallucination. To compute the area under this curve, we first sort the pairs in ascending order of reasoning performance RT . Let 7 the sorted indices be denoted as (0), (1), . . . , (n1), such that RT (0) RT (1) RT (n1). To ensure comparability across models, both RT and HT are min-max normalized to the range [0, 1]. The RH-AUC is then computed using the trapezoidal rule as: RH-AUC = n2 (cid:88) i=0 RT (i+1) RT (i) 2 (HT (i+1) + HT (i) ) , (3) where is the number of evaluated reasoning lengths. higher RH-AUC indicates model that better balances reasoning and hallucination across different reasoning lengths. Figure 8: (a) Accuracy trends on the RH-Bench reasoning task across different reasoning lengths for 3B and 7B models. Larger models typically exhibit more stable performance across varying reasoning lengths. (b) Comparison of SFT+RL and RL-only training paradigms in terms of RH-AUC, with arrow directions indicating the increase in reasoning length for SFT+RL relative to RL-only. RL-only training tends to generate more concise reasoning chains, leading to better perception hallucination balance. (c) Case study comparing RL-only and SFT+RL models. SFT+RL models often introduce rigid imitation reasoning paths, which limit the flexibility of visual reasoning. 5.2 Result Diagnosis We conduct an in-depth analysis of model performance based on the evaluation results from the RH-Bench diagnostic dataset, investigating the influence of three key factors: model scale, training paradigm, and training dataset on the reasoning-hallucination balance. Model Scaling. As shown in Table 1, the 7B model generally outperforms the 3B model in RH-Bench, demonstrating higher RH-AUC, primarily due to its larger parameter size and stronger representational capacity. As illustrated in Figure 8a, the larger model maintains higher stability, especially under longer reasoning chains, while the smaller models show noticeable decline in performance. This suggests that larger models typically exhibit better robustness and adaptability. Training Paradigms. comparison between the two-stage SFT+RL-trained model and the RL-only trained model reveals that RL demonstrates stronger balance between reasoning and perception. For example, in Figure 8c, although the OpenVLThinker model maintains longer reasoning chain, the introduction of redundant reasoning interferes with visual perception, leading to an incorrect inference that the shoe is already tied. In contrast, the RL-only model, Ocean-R1, uses shorter reasoning chain, enabling it to more efficiently capture key visual features and avoid unnecessary complex reasoning steps. This advantage is particularly evident at different reasoning length, as shown in the average RH-AUC in Figure 8b, which is significantly higher for the RL-only model compared to SFT+RL. This phenomenon suggests that although SFT helps the model learn reasoning formats, it may introduce rigid imitation reasoning paths, limiting the models adaptability to dynamic tasks and ultimately resulting in redundant reasoning. In contrast, RL encourages the model to generate more adaptive reasoning behaviors, enhancing the integration of reasoning and perception. Training Dataset. The diversity and quality of training data play crucial role in the reasoninghallucination balance of models. Through statistical analysis of the multimodel reasoning models training data and comparison with the results in Table 1, we have observed some interesting phenomena: (1) More visual perception data does not necessarily improve the balance between reasoning and perception. Increasing the training samples of visual perception data can enhance the balance of reasoning models to certain degree. For example, the ThinkLite-VL model, supported by large scale visual perception data, demonstrates strong hallucination and reasoning balance. Similarly, the Ocean-R1 model adopts two-stage training strategy, first enhancing reasoning ability and then strengthening visual perception, achieving the highest RH-AUC on RH-bench. However, this phenomenon is not consistent. For example, despite the R1-OneVision model utilizing large amount of visual perception data, it demonstrates weaker balance between reasoning and perception, which may be attributed to the limitations of its training paradigm design. (2) Perception and Reasoning balance can be achieved through training on domain-specific data. Training on domain-specific data helps enhance the balance of the reasoning model. For example, the MM-Eureka model, trained on larger mathematical dataset, shows higher RH-AUC, proving its effectiveness in balancing reasoning and perception. Similarly, despite being trained on only 6k mathematical data, the MM-R1 model still performs well on RH-bench. This highlights the potential of domain-specific data to stimulate the balance capabilities of reasoning models, even with smaller datasets. (3) The size of the training data is not always guarantee for the reasoning-perception balance. The traing data size does not always directly correlate with the models balance capability. For example, both the LLM-R1, trained on over 60k visual perception samples, and the R1-OneVision, with dataset of 150k samples, exhibit inadequate reasoning-hallucination balance, with the RH-AUC of only 0.46."
        },
        {
            "title": "6 Related Work",
            "content": "Multimodal Reasoning Tasks. Multimodal reasoning requires integrating information across modalities to solve complex problems. It is generally categorized into general reasoning and domainspecific reasoning. General reasoning typically occurs in natural image scenarios, where models must combine visual perception with knowledge and commonsense. Representative benchmarks include multiple-choice datasets such as MMMU [48], MMVP [35], MMBench [19], MMStar [3], MMEval-Pro [11], and VMCBench [54], as well as open-ended evaluations like Bingo [5], MMHALBench [11], POPE [15], CHAIR [28], and HallusionBench [10]. Domain-specific reasoning focuses on technical tasks within particular domains. For mathematical reasoning, benchmarks such as MathVista [21], MATH-Vision [37], MM-Math [30], WeMath [27] evaluate models ability to solve math problems grounded in visual contexts. For physical reasoning, datasets like PhysBench [4] and CRAVE [31] test understanding of physics and commonsense reasoning from visual inputs. Reinforcement Learning in MLLMs. Recent approaches enhance the reasoning capabilities of multimodal large models by incorporating chain-of-thought supervision during supervised finetuning or reinforcement learning [55, 51, 41, 34, 43, 45]. Methods like RLHF-V [47], LLaVAReasoner [53], and Insight-V [9] leverage large-scale CoT-style datasets and preference optimization to improve model reasoning. Following DeepSeek-R1, the GRPO (Guided Reinforcement Preference Optimization) algorithm has become standard paradigm in training multimodal large reasoning models [20, 50, 40, 36, 14, 38]. Some models, such as R1-OneVision [44], Reason-RFT [33], and R1VL [52], follow two-stage SFT + RL pipeline, while others like Ocean-R1 [16], ThinkLite-VL [39], and MM-Eureka [24] apply rule-based reinforcement learning directly at scale."
        },
        {
            "title": "7 Conclusion",
            "content": "In conclusion, this paper investigates the balance between reasoning and hallucination in multimodal reasoning models, with focus on how reasoning chain length and visual attention allocation impact performance. While longer reasoning chains enhance performance on complex tasks, they also exacerbate hallucinations by diminishing visual attention and increasing reliance on language priors. To address these challenges, the paper introduces the RH-AUC metric and the RH-Bench benchmark, which provide systematic method to evaluate the balance between reasoning ability and hallucination risk. The findings reveal that reasoning-augmented models are more prone to hallucinations, highlighting the importance of developing evaluation frameworks that assess both the quality of reasoning and the accuracy of perception. Limitation. Although our study provides comprehensive analysis of visual hallucinations in multimodal reasoning models, it also has several limitations. First, our evaluation is limited to models built on the Qwen2.5-VL backbone, which may constrain the generalizability of our findings to architectures with different modalities or pretraining objectives. Second, our analysis of the influence of training data is based solely on technical reports and publicly available documentation of existing models, without conducting controlled retraining experiments. Therefore, our conclusions are observational and may not fully capture causal effects."
        },
        {
            "title": "References",
            "content": "[1] Zouying Cao, Yifei Yang, and Hai Zhao. Nothing in excess: Mitigating the exaggerated safety for llms via safety-conscious activation steering. arXiv preprint arXiv:2408.11491, 2024. [2] Edward Y. Chang, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. CoRR, abs/2502.03373, 2025. [3] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [4] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. [5] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287, 2023. [6] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [7] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. [8] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. [9] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. 10 [10] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. [11] Jinsheng Huang, Liang Chen, Taian Guo, Fu Zeng, Yusheng Zhao, Bohan Wu, Ye Yuan, Haozhe Zhao, Zhihui Guo, Yichi Zhang, et al. Mmevalpro: Calibrating multimodal benchmarks towards trustworthy and efficient evaluation. arXiv preprint arXiv:2407.00468, 2024. [12] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1341813427, 2024. [13] Fushuo Huo, Wenchao Xu, Zhong Zhang, Haozhao Wang, Zhicheng Chen, and Peilin Zhao. Self-introspective decoding: Alleviating hallucinations for large vision-language models. In The Thirteenth International Conference on Learning Representations, 2025. [14] Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, and Jian Zhang. Q-insight: Understanding image quality via visual reinforcement learning. arXiv preprint arXiv:2503.22679, 2025. [15] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292305, Singapore, December 2023. Association for Computational Linguistics. [16] Ming Lingfeng, Li Yadong, Chen Song, Xu Jianhua, Zhou Zenan, and Chen Weipeng. Ocean-r1: An open and generalizable large vision-language model enhanced by reinforcement learning. https://github.com/VLM-RL/Ocean-R1, 2025. Accessed: 2025-04-03. [17] Sheng Liu, Haotian Ye, Lei Xing, and James Zou. In-context vectors: Making in context learning more effective and controllable through latent space steering. arXiv preprint arXiv:2311.06668, 2023. [18] Sheng Liu, Haotian Ye, and James Zou. Reducing hallucinations in large vision-language models via latent space steering. In The Thirteenth International Conference on Learning Representations. [19] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [20] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [21] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. [22] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022. [23] Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, and René Vidal. Pace: Parsimonious concept engineering for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 11 [24] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [25] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [26] OpenAI. Learning to reason with LLMs. 2024. [27] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large multimodal model achieve human-like mathematical reasoning? CoRR, abs/2407.01284, 2024. [28] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 40354045, 2018. [29] Haozhan Shen, Zilun Zhang, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model. 2025. Accessed: 2025-02-15. [30] Kai Sun, Yushi Bai, Ji Qi, Lei Hou, and Juanzi Li. MM-MATH: Advancing multimodal math evaluation with process evaluation and fine-grained classification. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 13581375, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [31] Shangkun Sun, Xiaoyu Liang, Bowen Qu, and Wei Gao. Content-rich aigc video quality assessment via intricate text alignment and motion-aware consistency. arXiv preprint arXiv:2502.04076, 2025. [32] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. [33] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [34] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [35] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. [36] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vlrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [37] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [38] Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. [39] Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025. 12 [40] Wenyi Xiao, Leilei Gan, Weilong Dai, Wanggui He, Ziwei Huang, Haoyuan Li, Fangxun Shu, Zhelun Yu, Peng Zhang, Hao Jiang, et al. Fast-slow thinking for large vision-language model reasoning. arXiv preprint arXiv:2504.18458, 2025. [41] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [42] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2024. [43] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning systems? arXiv preprint arXiv:2501.11284, 2025. [44] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [45] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [46] Hao Yin, Guangzong Si, and Zilei Wang. Clearsight: Visual signal enhancement for object hallucination mitigation in multimodal large language models. arXiv preprint arXiv:2503.13107, 2025. [47] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1380713816, 2024. [48] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [49] Liu Yuqi, Peng Bohao, Zhong Zhisheng, Yue Zihao, Lu Fanbin, Yu Bei, and Jia Jiaya. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. 2025. [50] Liu Yuqi, Peng Bohao, Zhong Zhisheng, Yue Zihao, Lu Fanbin, Yu Bei, and Jia Jiaya. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement, 2025. [51] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, et al. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024. [52] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [53] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. [54] Yuhui Zhang, Yuchang Su, Yiming Liu, Xiaohan Wang, James Burgess, Elaine Sui, Chenyu Wang, Josiah Aklilu, Alejandro Lozano, Anjiang Wei, et al. Automated generation of challenging multiple-choice questions for vision language model evaluation. arXiv preprint arXiv:2501.03225, 2025. [55] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405, 2024."
        }
    ],
    "affiliations": [
        "Stanford University",
        "UC Santa Barbara",
        "UC Santa Cruz"
    ]
}