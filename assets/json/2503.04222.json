{
    "paper_title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion",
    "authors": [
        "Ziyi Yang",
        "Fanqi Wan",
        "Longguang Zhong",
        "Canbin Huang",
        "Guosheng Liang",
        "Xiaojun Quan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along with two ultra-compact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop a specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0."
        },
        {
            "title": "Start",
            "content": "FUSECHAT-3.0: PREFERENCE OPTIMIZATION MEETS HETEROGENEOUS MODEL FUSION Ziyi Yang, Fanqi Wan, Longguang Zhong, Canbin Huang, Guosheng Liang, Xiaojun Quan School of Computer Science and Engineering, Sun Yat-sen University, China yangzy39@mail2.sysu.edu.cn, quanxj3@mail.sysu.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce FuseChat-3.0, suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Our source models include the powerful Gemma-2-27B-it, Mistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct. For target models, we focus on three widely-used smaller variantsLlama-3.18B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instructalong with two ultracompact options, Llama-3.2-3B-Instruct and Llama-3.2-1B-Instruct. To leverage the diverse capabilities of these source models, we develop specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of two key stages: (1) supervised fine-tuning (SFT) to align the target and source model distributions, and (2) Direct Preference Optimization (DPO) to apply preferences from multiple source LLMs to fine-tune the target model. The resulting FuseChat-3.0 models exhibit significant performance gains across tasks such as instruction following, general knowledge, mathematics, and coding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves an average improvement of 6.8 points across 14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and 30.1 points on the instruction-following benchmarks AlpacaEval-2 and Arena-Hard, respectively. Our code, models, and datasets are available at https://github.com/SLIT-AI/FuseChat-3.0. 5 2 0 2 6 ] . [ 1 2 2 2 4 0 . 3 0 5 2 : r Figure 1: Results of FuseChat-3.0 across various benchmarks, using Llama-3.1-8B-Instruct as the target LLM. Corresponding author."
        },
        {
            "title": "Preprint",
            "content": ""
        },
        {
            "title": "INTRODUCTION",
            "content": "Combining the strengths of multiple large language models (LLMs) provides powerful means to enhance performance, robustness, and generalization across diverse tasks by leveraging the unique expertise and knowledge each model offers. Individual LLMs, particularly those constrained by size or training data, may perform well in specific areas but struggle in others due to specialization gaps. For instance, one model might excel at generating creative content but lack precision in technical explanations, while another delivers technical accuracy but struggles with conversational fluency. By integrating multiple models, their collective strengths can bridge these gaps, leading to improved overall performance. This collaborative approach also improves robustness, as the system can compensate for individual model errorswhen one model underperforms, others can intervene to support the response. Furthermore, this integration enhances task generalization by exposing the system to diverse patterns, allowing it to adapt more effectively to new or unseen challenges. Various strategies have been developed to achieve this, each with unique trade-offs. Ensemble methods (Jiang et al., 2023b; Wang et al., 2025) enhance performance and robustness by combining predictions from multiple models. However, they require all models to remain active during inference, leading to substantial computational and memory costs. LLM routing (Ding et al., 2024; Hu et al., 2024; Ong et al., 2025) offers more efficient alternative: router selects the most appropriate LLM to handle each query. While this balances effectiveness and efficiency, it requires training new router for each task, limiting its generalization to unseen tasks. Model merging (Wortsman et al., 2022) integrates models with identical architectures into unified parameter set, improving robustness and generalization but limiting applicability to homogeneous model families. Explicit model fusion (EMF) methods (Wan et al., 2024a;b) use knowledge distillation to transfer knowledge from multiple source models to single target model, often through probabilistic distribution matrices. While adaptable to different model structures and sizes, EMF faces challenges like vocabulary alignment and distribution merging, which can complicate the fusion process and introduce errors. In this paper, we present FuseChat-3.0, suite of large language models (LLMs) developed by harnessing the strengths of heterogeneous source LLMs into more compact target models. Our approach enables target LLMs to learn directly from the generated outputs of diverse open-source LLMsa process termed implicit model fusion (IMF) (Yang et al., 2025). IMF leverages the rich information embedded in outputs from stronger LLMs through various mechanisms. For example, these outputs can be used to fine-tune weaker models (Tian et al., 2024; Kang et al., 2024). In the domain of preference learning, methods like Zephyr (Tunstall et al., 2024) and TÃ¼lu 3 (Lambert et al., 2024) utilize GPT-4 to rank responses generated by multiple LLMs, thereby creating preference datasets for Direct Preference Optimization (DPO) training (Rafailov et al., 2023). Similarly, WRPO constructs preference pairs by combining responses from both target and source models. However, generating positive and negative examples from different models can introduce reward annotation bias and variance, potentially undermining optimization. In contrast, our method addresses these issues by constructing each preference pair from the best and worst responses generated by the same source model. This crucial distinction eliminates the reward bias caused by heterogeneous response styles, prevents reward hacking, and delivers more controlled preference signals. As illustrated in Figure 2, our IMF framework follows three-stage process. First, during the data construction stage, we generate multiple responses from various source models for each prompt. These responses are then evaluated using an external reward model for instruction-following tasks, while mathematics and coding responses are verified through rule-based methods. Next, we introduce supervised fine-tuning (SFT) stage to tackle the challenges posed by distribution shifts when applying preference learning directly to heterogeneous LLM outputs (Xu et al., 2024a; Tajwar et al., 2024; Zhou et al., 2024). Specifically, this stage fine-tunes the target models using the optimal response from the source models for each prompt. Finally, building on the SFT initialization, the Direct Preference Optimization (DPO) stage incorporates controlled preference signals from samesource response pairs to further fine-tune the target model. This structured approach enhances model robustness while reducing bias and variance caused by heterogeneous response data. To demonstrate the effectiveness of FuseChat-3.0, we conduct experiments across diverse range of models and benchmarks. We utilize four prominent open-source LLMs, with parameter sizes ranging from 27B to 123B, as source models. For target models, we select five smaller, widely used models with sizes between 1B and 9B parameters. FuseChat-3.0 is evaluated on 14 well-established instruction-following, general knowledge, benchmarks spanning four core capability domains:"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of our proposed FuseChat-3.0 framework for implicit model fusion. mathematics, and coding. The results demonstrate that FuseChat-3.0 consistently outperforms its corresponding target LLMs, highlighting its effectiveness in facilitating the implicit fusion of heterogeneous LLMs across diverse tasks. Notably, when Llama-3.1-8B-Instruct is used as the target LLM, our fusion approach achieves an average improvement of 6.8 points across the 14 benchmarks. Moreover, we observe remarkable gains of 37.1 points on AlpacaEval-2 and 30.1 points on ArenaHard, setting new state-of-the-art performance for 8B-parameter LLMs. These results emphasize the robustness and efficacy of our approach in enhancing the capabilities of smaller target models."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Collective LLMs With the prosperity of various open-source LLMs, it has become natural to explore ways to combine the strengths of these heterogeneous models to create more powerful systems. This section reviews existing approaches to combine heterogeneous LLMs, categorizing them into four main strategies: ensemble, routing, model merging, and model fusion methods. Ensemble methods enhance model performance by aggregating the outputs of multiple LLMs. LLMBlender (Jiang et al., 2023b) employs pairwise ranker to select the top-K outputs from various LLMs, which are then aggregated and refined by sequence-to-sequence blender model. Mixtureof-Agents (MoA) (Wang et al., 2025) organizes LLM agents in hierarchical structure, where each layer refines and combines the outputs from the preceding layer. Beyond sequence-level aggregation, Xu et al. (2024b) introduced token-level ensembling method that merges LLM distributions at each decoding step using global alignment matrix. Although effective, ensemble methods are computationally expensive due to the simultaneous activation of multiple LLMs during inference. Routing methods optimize efficiency by selectively invoking specific LLMs based on input characteristics, thereby reducing the computational cost of ensemble approaches. Hybrid-LLM (Ding et al., 2024) trains BERT-based router on synthetic preference labels generated using BARTScore (Yuan et al., 2021) to dynamically assign queries to either small or large model, depending on predicted query complexity and the desired output quality. RouteLLM (Ong et al., 2025) adopts similar approach but utilizes real human preference labels from Chatbot Arena (Chiang et al., 2024) to train the router. While these methods reduce costs and maintain high-quality responses with minimal latency, their reliance on task-specific router training limits their ability to generalize to new scenarios. Model merging methods (Wortsman et al., 2022) aim to directly combine the parameters of multiple models into single unified model. While enhancing robustness and generalization, they are limited to homogeneous model families. Xu et al. (2024c) introduced strategy for merging models with differing depths and widths by aligning cohesive layer groups and applying elastic neuron zipping to project weights into common space. While this approach allows for the merging of heterogeneous models, it encounters scalability challenges due to high computational complexity with large models. Model fusion methods seek to consolidate the capabilities of multiple source models into single target model and can be categorized as either explicit or implicit. Explicit model fusion (EMF) techniques, such as FuseLLM (Wan et al., 2024a) and FuseChat (Wan et al., 2024b), transfer knowledge from multiple source LLMs to target LLM through multi-teacher knowledge distillation. FuseChat implements two-stage process: first, it performs pairwise fusion between each source model and pivot model; next, it merges the resulting homologous models in parameter space. While EMF methods adapt to various architectures and model sizes, they face challenges like vocabulary alignment and merging distribution matrices. To address these issues, WRPO (Yang et al., 2025) introduces implicit model fusion (IMF), which uses source model responses as auxiliary signals"
        },
        {
            "title": "Preprint",
            "content": "during on-policy preference optimization. WRPO progressively shifts optimization from the target models on-policy outputs to high-reward off-policy responses from the source models. Preference Alignment Aligning large language models (LLMs) with human preferences is crucial for their effectiveness. Reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022) is widely used approach for this purpose. However, RLHF relies on complex reinforcement learning techniques like proximal policy optimization (PPO) (Schulman et al., 2017), which can be challenging to implement and prone to instability during training. To address these challenges, Direct Preference Optimization (DPO) (Rafailov et al., 2023) offers simplified alternative by directly optimizing the policy model based on closed-form implicit reward function derived from human preference data. This approach simplifies implementation, reduces computational costs, enhances training stability, and maintains strong alignment with human preferences."
        },
        {
            "title": "3 FUSECHAT-3.0 DATASET",
            "content": "The data construction process is pivotal in enabling the implicit model fusion (IMF) technique utilized by FuseChat-3.0. This section outlines the methods used for prompt selection, response sampling, and data construction, highlighting the rationale behind our design choices. 3.1 PROMPT SELECTION Our primary objective is to curate diverse dataset that enhances the comprehensive capabilities of target LLMs across multiple domains, including instruction following, mathematics, coding, and Chinese language proficiency. To accomplish this, we strategically select data from reputable opensource community datasets and implement targeted filtering and preprocessing techniques. Below is an overview of the key datasets and filtering criteria employed: Instruction Following: The dataset is selected from multiple sources, including UltraFeedback (Cui et al., 2024), Magpie-Pro-DPO (Xu et al., 2025), and HelpSteer2 (Wang et al., 2024c). To maintain focus on general conversation, we systematically excluded code and mathematicsrelated data, ultimately assembling 80,907 distinct samples. Mathematics: We incorporate mathematical prompts from OpenMathInstruct-2 (Toshniwal et al., 2025), which yields approximately 52,000 unique entries with validated solutions. Coding: Our coding dataset is constructed from LeetCode1 problems and Self-Oss-InstructSC2 (Lozhkov et al., 2024). We specifically select problems that include comprehensive test cases for validation purposes, resulting in final collection of 16,005 samples. Chinese Language: The Chinese language dataset is created by integrating Alpaca-GPT4Zh (Peng et al., 2023) and Magpie-Qwen2-Pro-Zh (Xu et al., 2025). After filtering out code and mathematics-related content, we obtain approximately 10,000 high-quality samples that specifically focus on Chinese dialogues. 3.2 RESPONSE SAMPLING For each prompt within the curated datasets, we sample responses primarily from four prominent source LLMs: Gemma-2-27B-it (Team et al., 2024), Mistral-Large-Instruct-2407 (Jiang et al., 2023a), Qwen-2.5-72B-Instruct (Yang et al., 2024a), and Llama-3.1-70B-Instruct (Dubey et al., 2024). The response sampling strategy varies based on the specific domain: Instruction Following: For each prompt, we sample five responses from each source model. Mathematics: We maintain the same sampling methodology as in the Instruction Following category. Additionally, we incorporate responses generated by Llama-3.1-405B-Instruct (Dubey et al., 2024) and Qwen-2.5-Math-72B-Instruct (Yang et al., 2024b) to enhance the correctness of our mathematical solution pool. Coding: To ensure broader range of coding solutions, we collect eight distinct responses from each source model per prompt. Chinese Language: We utilize Qwen-2.5-72B-Instruct for response generation in this category given its specialized optimization for the Chinese language. 1https://huggingface.co/datasets/greengerong/leetcode"
        },
        {
            "title": "Preprint",
            "content": "We use vLLM2 (Kwon et al., 2023) as our inference back-end and sample responses multiple times with different random seeds. The sampling parameters for the different source LLMs are detailed as follows: For Gemma-2-27B-it, Mistral-Large-Instruct-2407, and Llama-3.1-70B-Instruct, we use temperature of 0.8 and top-p value of 0.95. For Qwen-2.5-(Math)-72B-Instruct, we set the temperature to 0.7, the top-p value to 0.8, and apply repetition penalty of 1.05."
        },
        {
            "title": "3.3 PREFERENCE PAIRS",
            "content": "Unlike previous methods (Tunstall et al., 2024; Lambert et al., 2024; Yang et al., 2025), which construct preference pairs from different models with varying output styles, FuseChat-3.0 leverages the best and worst responses generated by the same source model to construct each preference pair and optimize the target model. This intra-model pairing eliminates the reward bias associated with heterogeneous response styles, prevents reward hacking, and offers more controlled preference signal. The data construction process varies based on the specific domain. For instruction-following and conversational data, we employ an external reward model to evaluate the sampled responses. In contrast, responses in mathematics and coding domains are verified through rule-based systems. We show the specific procedures for each domain as below: Instruction Following: To assign reward model (RM) scores to the five responses generated by each source model, we employ ArmoRM-LLaMA3-8B-v0.1 (Wang et al., 2024a) for annotation. We then divide the annotated data into SFT and DPO datasets using 4:6 ratio. During the SFT phase, we select the highest RM-scoring response for each prompt across all source models for initial training. In the DPO phase, preference pairs are constructed by pairing responses from the same source model. Responses with the highest RM scores within each model are labeled as positive samples, while those with the lowest scores are treated as negative samples. We then use pairs where the positive responses have the highest RM scores to fine-tune the target model. To ensure the quality of preference signals, we impose constraint on the RM score difference between positive and negative samples when constructing preference pairs, limiting it to the range of 0.01 to 0.1. This measure aims to maintain distinguishability between chosen and rejected samples, thereby minimizing potential interference to the optimization process caused by small differences or annotation noise, and thus improving the robustness and convergence of training. Mathematics: Responses from all source models are initially evaluated for correctness by comparing extracted answers against gold labels. This assessment is complemented by RM scores provided by ArmoRM-LLaMA3-8B-v0.1, ensuring both factual accuracy and adherence to stylistic preferences. We then strategically partition the dataset into SFT and DPO partitions. The SFT phase incorporates responses that are both correct and exhibit the highest RM scores. This selection strategy ensures that the fine-tuning process is grounded in high-quality responses closely aligned with the desired task outcomes. For the DPO phase, we construct paired samples from the same source model, as described above. Positive samples consist of correct answers with the highest RM scores, while negative samples comprise incorrect answers with the lowest RM scores. Coding: We employ verification method comprising correctness scores and RM scores for coding evaluation. Correctness scores assess whether the generated code passes both static analysis checks and provided test cases, ensuring functional accuracy. The RM scores are used for preference evaluation, gauging the overall quality of responses based on predefined criteria. During the SFT phase, we select responses that not only pass all test cases but also achieve the highest RM scores. This rigorous selection ensures that the model is fine-tuned on exemplary code meeting both correctness and preference standards. In the DPO phase, we compare positive sampleshighscoring responses that pass all evaluationswith negative sampleslow-scoring responses that fail the tests, both originating from the same source model. This comparison aims to optimize the models ability to prefer higher-quality and functionally correct code during training. Critically, we exclude instances where all model responses fail to meet the testing criteria. This exclusion is implemented to maintain the integrity of the evaluation process, as such cases do not provide meaningful data for assessing and improving the models performance. Chinese Language: We exclusively utilize responses sampled from Qwen-2.5-72B-Instruct during the SFT phase, due to its strong performance in the Chinese language. This focused approach aims to maximize the transfer of high-quality Chinese language capabilities to the target model. Since no suitable reward models were available for this dataset, we opted to omit the DPO phase. 2https://github.com/vllm-project/vllm"
        },
        {
            "title": "Preprint",
            "content": "Table 1: The constitution of FuseChat-3.0 dataset in SFT phase and DPO phase. As no suitable reward models were available for Chinese, we used all samples for SFT and omitted the DPO phase. Category Dataset Instruction Following UltraFeedback Magpie-Pro-DPO HelpSteer2 Count #DSFT 51,098 20,374 9,435 20,439 8,149 3,774 Mathematics OpenMathInstruct-2 51, 40,188 Coding Chinese Language LeetCode Self-Oss-Instruct-SC2 3,113 12,892 Alpaca-GPT4-Zh Magpie-Qwen2-Pro-Zh 2,471 7,481 1,877 10,160 2,471 7,481 #DDPO 30,659 12,225 5,661 11, 1,236 2,732 0 0 Total 158,667 94,539 64, Our final dataset consists of 158,667 entries, with 94,539 allocated to the SFT phase (DSFT) and 64,128 preference pairs for the DPO phase (DDPO). summary of the dataset composition is provided in Table 1. Refer to Appendix for further details on the open-source models and datasets."
        },
        {
            "title": "4 TRAINING RECIPE",
            "content": "This section details the two-stage training pipeline designed for FuseChat-3.0. The first stage involves supervised fine-tuning (SFT) to address distributional discrepancies between the target and source LLMs. Building on the SFT initialization, the second stage employs preference learning, specifically Direct Preference Optimization (DPO), to learn preferences from multiple source LLMs. 4.1 SUPERVISED FINE-TUNING Given prompt xi and its corresponding output yi of length from the fine-tuning dataset DSFT, we denote the sequence preceding the t-th token in the output as yi,<t = (yi,1, yi,2, . . . , yi,t1). The SFT objective for language model with parameters Î¸ is to minimize the negative log-likelihood: LSFT(Î¸) = E(xi,yi)DSFT (cid:34) (cid:88) t= log pÎ¸(yi,t yi,<t, xi) , (1) (cid:35) where pÎ¸(yi,t yi,<t, xi) represents the models predicted probability for the t-th token yi,t in yi, conditioned on the prompt xi and the preceding tokens yi,<t. In our experiments, we use the Llama-Factory library3 (Zheng et al., 2024) to implement the finetuning. For all target models, we perform fine-tuning for 3 epochs, with batch size of 128 and maximum sequence length of 2048 tokens. cosine learning rate schedule with warmup ratio of 0.1 is employed. The learning rates for different models are shown in Table 2. 4.2 DIRECT PREFERENCE OPTIMIZATION The DPO objective directly optimizes policy to align with human preferences by leveraging supervised learning objective on human-labeled preference data. DPO reformulates the reward function to yield closed-form solution for the optimal policy Ï. The reparameterized optimal reward function r(x, y) is denoted as: r(x, y) = Î² log Ï(y x) Ïref(y x) where Z(x) is the partition function, Ïref denotes the reference policy, and Î² is hyperparameter controlling the deviation from Ïref. Given preference dataset DDPO consisting of triplets (x, yw, yl), where yw and yl are the preferred and dispreferred completions for given prompt x, the preference probability for yw over yl is modeled using the Bradley-Terry model (Bradley & Terry, 1952): + Î² log Z(x), (2) p(yw yl x) = Ï (cid:18) Î² log Ï(yw x) Ïref(yw x) Î² log Ï(yl x) Ïref(yl x) (cid:19) . (3) 3https://github.com/hiyouga/LLaMA-Factory"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Learning rates and hyperparameters for different target models during the SFT and DPO stages. Target Model SFT Learning Rate DPO Learning Rate DPO Î² DPO Loss Type Llama-3.1-8B-Instruct Qwen-2.5-7B-Instruct Gemma-2-9B-it Llama-3.2-(3/1)B-Instruct 5 106 2 106 2 106 5 106 8 107 3 107 5 107 1 10 10 0.01 0.01 10 LLN-DPO LDPO LDPO LLN-DPO For parameterized policy ÏÎ¸, the corresponding maximum likelihood objective is: (cid:20) (cid:18) LDPO(ÏÎ¸; Ïref) = E(x,yw,yl)DDPO log Ï Î² log ÏÎ¸(yw x) Ïref(yw x) Î² log ÏÎ¸(yl x) Ïref(yl x) (cid:19)(cid:21) . (4) In this work, we explore the application of length normalization during DPO training, variant proposed in SimPO (Meng et al., 2024) and proven to be effective in TÃ¼lu 3 (Lambert et al., 2024). Specifically, the length-normalized DPO objective is defined as: LLN-DPO(ÏÎ¸; Ïref) = E(x,yw,yl)DDPO (cid:20) log Ï (cid:18) Î² yw log ÏÎ¸(yw x) Ïref(yw x) Î² yl log ÏÎ¸(yl x) Ïref(yl x) (cid:19)(cid:21) , (5) where yw and yl denote the lengths of the preferred and dispreferred completions, respectively. In our experiments, we utilize the alignment-handbook4 as the training framework for DPO. All postSFT target models undergo training for one epoch with batch size of 128 and maximum sequence length of 2048. cosine learning rate schedule with warmup ratio of 0.1 is used. Checkpoints are saved every 100 steps, and the best checkpoint from the last two is selected. Hyperparameter configurations for different models are detailed in Table 2."
        },
        {
            "title": "5 EVALUATION",
            "content": "5.1 EVALUATION BENCHMARKS To showcase the superior performance of FuseChat-3.0, we conduct comprehensive evaluation across multiple domains, including instruction following, question answering, general reasoning, mathematics, and coding. Specifically, we utilize 14 well-established benchmarks, which are categorized into four distinct groups: Instruction-Following Tasks: AlpacaEval-2 (Li et al., 2023), Arena-Hard (Li et al., 2024), MT-Bench (Zheng et al., 2023), AlignBench v1.1 (Liu et al., 2023). General Tasks: LiveBench-0831 (White et al., 2025), MMLU-Pro (Wang et al., 2024b), MMLUredux (Gema et al., 2024), GPQA-Diamond (Rein et al., 2023). Mathematics Tasks: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AMC 23 (Yang et al., 2024b). Coding Tasks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), LiveCodeBench 2408-2411 (Jain et al., 2025). Further details about these benchmarks are provided in Appendix B. 5.2 OVERALL RESULTS In Table 3 and Table 4, we present the overall results of our FuseChat-3.0, with Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, Llama-3.2-1B-Instruct, Qwen-2.5-7B-Instruct, and Gemma-2-9B-it as the target models. Based on the experimental results, we identify several key insights. Firstly, when using Llama-3.1-8B-Instruct as the target model, our FuseChat-3.0 achieves an average performance improvement of 6.8 points across 14 benchmarks compared to the original Llama-3.1-8BInstruct model. Notably, it shows significant gains of 37.1 and 30.1 points on the instruction-following test sets AlpacaEval-2 and Arena-Hard, respectively, establishing new state-of-the-art performance 4https://github.com/huggingface/alignment-handbook"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Overall results of FuseChat-3.0, with Llama-3.1-8B-Instruct, Llama-3.2-3B-Instruct, and Llama-3.21B-Instruct as target models. Bold denotes the best performance on each benchmark. Category Benchmark Llama-3.1-8B-Instruct Llama-3.2-3B-Instruct Llama-3.2-1B-Instruct Base SFT FuseChat Base SFT FuseChat Base SFT FuseChat Instruction Following General Mathematics Coding AlpacaEval-2 (LC %) Arena-Hard (WR %) MT-Bench AlignBenchv1.1 LiveBench0831 MMLU-Pro (0 shot, CoT) MMLU-redux (0 shot, CoT) GPQA-Diamond (0 shot, CoT) GSM8K (0 shot, CoT) MATH (0 shot, CoT) AMC 23 (0 shot, CoT) HumanEval (0 shot) MBPP (0 shot) LiveCodeBench2408-2411 Average 28.3 28.1 8.4 4.6 27.6 50.0 67.2 33. 85.9 50.7 25.0 69.5 75.4 12.3 40.5 41.3 38.7 8.5 6.3 30.2 47.8 68.4 37.9 87.0 54.7 30. 69.5 71.4 12.6 43.2 65.4 58.2 9.0 6.7 32.0 49.2 69.2 34.9 88.0 55.2 37.5 71.3 72.0 13. 47.3 21.4 16.6 6.9 3.8 23.4 39.3 58.5 29.8 82.0 51.4 22.5 61.0 68.5 8.3 35. 31.1 21.3 7.3 5.5 24.5 40.3 58.2 33.3 82.8 52.9 20.0 62.8 67.5 7.1 36.8 54.0 30.2 7.7 5. 24.9 40.3 59.0 33.8 82.0 53.1 35.0 60.4 67.5 9.0 40.2 9.7 5.1 4.7 2.9 14.0 22.3 43.7 21. 46.3 32.7 17.5 39.6 49.5 - 23.8 14.0 6.0 5.2 3.9 13.9 21.5 40.3 25.3 55.6 34.7 15. 36.6 42.1 - 24.2 25.3 8.6 5.7 4.3 15.8 21.3 41.6 24.2 54.5 33.6 20.0 40.2 46.6 - 26.3 Table 4: Overall results of our FuseChat-3.0 framework, with Qwen-2.5-7B-Instruct and Gemma-2-9B-it served as target models. Bold indicates the best performance on each benchmark and of each target model. Category Benchmark Qwen-2.5-7B-Instruct Gemma-2-9B-it Base SFT FuseChat Base SFT FuseChat Instruction Following General Mathematics Coding AlpacaEval-2 (LC %) Arena-Hard (WR %) MT-Bench AlignBenchv1.1 LiveBench0831 MMLU-Pro (0 shot, CoT) MMLU-redux (0 shot, CoT) GPQA-Diamond (0 shot, CoT) GSM8K (0 shot, CoT) MATH (0 shot, CoT) AMC 23 (0 shot, CoT) HumanEval (0 shot) MBPP (0 shot) LiveCodeBench2408-2411 Average 33.2 50.7 8.4 7.5 35.4 54.1 75.1 34.9 91.7 75.0 52. 85.4 80.2 15.8 50.0 34.2 45.2 8.5 7.4 33.7 51.7 72.7 38.4 92.3 72.7 45.0 81.7 84.1 17. 48.9 63.6 61.4 9.0 7.6 33.2 53.0 74.4 33.8 91.7 73.6 57.5 79.9 83.1 18.9 52. 51.1 40.8 8.5 7.0 31.6 50.5 72.8 39.4 88.5 49.6 20.0 67.1 75.1 11.9 43.9 49.8 44.5 8.7 7. 33.3 52.5 72.8 33.3 90.5 58.0 27.5 65.9 70.6 11.0 44.7 70.2 63.4 8.6 7.4 33.2 52.9 73.7 35. 91.0 57.8 35.0 64.0 71.7 10.1 48.2 for 8B LLMs. Moreover, FuseChat-3.0 improves performance in general knowledge tasks by an average of 1.6 points, and in mathematics by an average of 6.3 points, demonstrating comprehensive capability enhancement. However, we observe slight decline of 0.3 average points in coding tasks, which may be attributed to the relatively limited coding data in our training set. Secondly, the consistent improvements across different model sizes, ranging from 1B to 8B, for the Llama-3 series demonstrate the scalability of FuseChat-3.0. Notably, when applied to Llama-3.2-3BInstruct, FuseChat-3.0 achieves an average score of 40.2, nearly matching Llama-3.1-8B-Instruct, which is 2.7 times larger. This highlights the potential of FuseChat-3.0 to enable smaller models to perform comparably to their larger counterparts, offering significant efficiency gains. Finally, when applied to the stronger target model, Qwen-2.5-7B-Instruct, FuseChat-3.0 achieves an average improvement of 2.9 points, despite the higher baseline performance. While the relative improvement is smaller compared to the Llama-3 models, FuseChat-3.0 still boosts performance across several benchmarks. For instance, on AlpacaEval-2, FuseChat-3.0 boosts the score by 30.4 points. Similarly, on Arena-Hard, it achieves an improvement of 10.7 points. When using Gemma-2-9B-it as the target model, FuseChat-3.0 also delivers an average performance gain of 4.3 points. These results validate FuseChat-3.0 as an effective and scalable framework for enhancing capabilities through implicit knowledge fusion, achieving significant performance gains across model architectures and scales without requiring architectural modifications or massive computational resources."
        },
        {
            "title": "5.3 COMPARISON WITH TÃLU 3",
            "content": "We conduct comparative analysis between our FuseChat3.0 framework and AllenAIs recently introduced TÃ¼lu 3 (Lambert et al., 2024). TÃ¼lu 3 employs an extensive training pipeline, which includes leveraging 22 models for data synthesis, GPT-4o for preference annotation, training dataset exceeding 1.2 million data points, and three-stage training pipeline including SFT, DPO, and PPO. As shown in Figure 3, our FuseChat-Llama-3.1-8BInstruct model outperforms Llama-3.1-Tulu-8B across all evaluation categories. FuseChat-3.0 achieves an overall score of 47.3, exceeding TÃ¼lu 3 by 7.1 points. The performance gap is most pronounced in the instruction-following category, where FuseChat leads by 11.4 points. Superior performance is also demonstrated in the general, mathematics, and coding categories. These results suggest that while TÃ¼lu 3 benefits from sophisticated training pipeline and large dataset, our FuseChat-3.0 framework achieves better performance through efficient implicit model fusion. By rigorously curating each preference pair from the best and worst responses generated by the same source model, FuseChat-3.0 effectively mitigates reward bias and variance inherent in TÃ¼lu 3s approach, which constructs each preference pair using different models. These more controlled preference signals enable FuseChat-3.0 to achieve high performance with significantly lower computational cost during the preference optimization process. Figure 3: Comparison between FuseChatLlama-3.1-8B-Instruct and Llama-3.1-Tulu8B across different domains. Domain scores are obtained by averaging the scores of the benchmarks within that domain. 5.4 EFFECTIVENESS OF LENGTH-NORMALIZED DPO We investigate the impact of length normalization on the DPO stage of our FuseChat-3.0 and present the results in Figure 4. When using Llama-3.1-8B-Instruct as the target model, we show that length-normalized DPO (LN-DPO) generally leads to performance gains compared to both the standard DPO approach and the base model. Specifically, FuseChatLN-DPO achieves the highest overall score of 47.3, surpassing FuseChatDPOs 46.2 and the base models 42.6. The most significant improvements due to length normalization are observed in the mathematics and coding categories. We attribute these substantial gains to the fact that length normalization effectively mitigates the length bias inherent in preference pairs. By reducing the models tendency to favor longer responses, this technique ensures that the optimization process prioritizes correctness and completeness over unnecessary verbosity, which is particularly crucial in mathematics and coding domains requiring precise solutions. While LN-DPO also yields improvements in other categories, these gains are less substantial. These findings further reinforce that length normalization is valuable technique for enhancing the performance of DPO-trained models, including FuseChat-3.0. Figure 4: Comparison between lengthnormalized DPO and vanilla DPO in FuseChat3.0. Base denotes Llama-3.1-8B-Instruct."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduce FuseChat-3.0, suite of large language models (LLMs) designed to integrate the strengths of heterogeneous source LLMs into more compact and efficient target models. To harness the diverse capabilities of these source models, we develop specialized data construction protocol tailored to various tasks and domains. The FuseChat-3.0 training pipeline consists of supervised fine-tuning (SFT) to align the target model with source model distributions, and Direct Preference Optimization (DPO) stage to incorporate preferences from multiple source LLMs for further refinement. By leveraging four powerful open-source LLMs as source models and finetuning five smaller models as targets, FuseChat-3.0 consistently achieves substantial performance gains across 14 established benchmarks. These results highlight its effectiveness in transferring and integrating the diverse strengths of heterogeneous LLMs into smaller, more efficient models."
        },
        {
            "title": "REFERENCES",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39:324, 1952. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. In Proceedings of the 41st International Conference on Machine Learning, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 2017. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. UltraFeedback: Boosting language models with high-quality feedback. In Proceedings of the 41st International Conference on Machine Learning, 2024. Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor RÃ¼hle, Laks V. S. Lakshmanan, and Ahmed Hassan Awadallah. Hybrid LLM: Cost-efficient and quality-aware query routing. In The Twelfth International Conference on Learning Representations, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yann Dubois, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple debiasing of automatic evaluators. In First Conference on Language Modeling, 2024. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we done with mmlu? arXiv preprint arXiv:2406.04127, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b. Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, and Shriyash Kaustubh Upadhyay. Routerbench: benchmark for multi-llm routing system. arXiv preprint arXiv:2403.12031, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2025."
        },
        {
            "title": "Preprint",
            "content": "Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and William El Sayed. Mistral 7B. arXiv preprint arXiv:2310.06825, 2023a. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. LLM-Blender: Ensembling large language models with pairwise ranking and generative fusion. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1416514178, 2023b. Minki Kang, Seanie Lee, Jinheon Baek, Kenji Kawaguchi, and Sung Ju Hwang. Knowledgeaugmented reasoning distillation for small language models in knowledge-intensive tasks. Advances in Neural Information Processing Systems, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model In Proceedings of the 29th Symposium on Operating Systems serving with pagedattention. Principles, pp. 611626, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. TÃ¼lu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaEval: An automatic evaluator of instruction-following models, 2023. Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743, 2023. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Yu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple preference optimization with referencefree reward. In Neural Information Processing Systems, 2024. Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, Waleed Kadous, and Ion Stoica. RouteLLM: Learning to route LLMs from preference data. In The Thirteenth International Conference on Learning Representations, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct Preference Optimization: Your language model is secretly reward model. In Neural Information Processing Systems, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023."
        },
        {
            "title": "Preprint",
            "content": "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of LLMs should leverage suboptimal, on-policy data. In Proceedings of the 41st International Conference on Machine Learning, 2024. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, LÃ©onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre RamÃ©, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, and N. Chawla. TinyLLM: Learning small student from multiple large language models. arXiv preprint arXiv:2402.04616, 2024. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating AI for math with massive open-source instruction data. In The Thirteenth International Conference on Learning Representations, 2025. Lewis Tunstall, Edward Emanuel Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, ClÃ©mentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander Rush, and Thomas Wolf. Zephyr: Direct distillation of LM alignment. In First Conference on Language Modeling, 2024. Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large language models. In The Twelfth International Conference on Learning Representations, 2024a. Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang, and Wei Bi. FuseChat: Knowledge fusion of chat models. arXiv preprint arXiv:2402.16107, 2024b. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1058210592, 2024a. Junlin Wang, Jue WANG, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances In The Thirteenth International Conference on Learning large language model capabilities. Representations, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer 2: Open-source dataset for training top-performing reward models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024c. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sreemanti Dey, Shubh-Agrawal, Sandeep Singh Sandha, Siddartha Venkat Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and In The Micah Goldblum. Livebench: challenging, contamination-free LLM benchmark. Thirteenth International Conference on Learning Representations, 2025. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Proceedings of the 39th International Conference on Machine Learning, 2022."
        },
        {
            "title": "Preprint",
            "content": "Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is DPO superior to PPO for LLM alignment? comprehensive study. In Proceedings of the 41st International Conference on Machine Learning, 2024a. Yangyifan Xu, Jinliang Lu, and Jiajun Zhang. Bridging the gap between different vocabularies for llm ensemble. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 71337145, 2024b. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned LLMs with nothing. In The Thirteenth International Conference on Learning Representations, 2025. Zhengqi Xu, Han Zheng, Jie Song, Li Sun, and Mingli Song. Training-free heterogeneous model merging. arXiv preprint arXiv:2501.00061, 2024c. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b. Ziyi Yang, Fanqi Wan, Longguang Zhong, Tianyuan Shi, and Xiaojun Quan. Weighted-reward preference optimization for implicit model fusion. In The Thirteenth International Conference on Learning Representations, 2025. Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 2021. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. In NeurIPS Datasets and Benchmarks Track, 2023. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 400410, 2024. Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, and Chenguang Zhu. WPO: Enhancing RLHF with weighted preference optimization. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 83288340, 2024."
        },
        {
            "title": "Preprint",
            "content": "A DETAILS OF OPEN-SOURCE MODELS AND THE DATASET In Table 5, we provide the Huggingface repository names and links of the target LLMs, source LLMs, reward model, and the source of training datasets used in our experiments. Table 5: Details of open-source models and datasets used in our experiments. Name Huggingface ID Llama-3.1-8B-Instruct Gemma-2-9B-IT Qwen-2.5-7B-Instruct Llama-3.2-3B-Instruct Llama-3.2-1B-Instruct Target LLMs meta-llama/Llama-3.1-8B-Instruct google/gemma-2-9b-it Qwen/Qwen2.5-7B-Instruct meta-llama/Llama-3.2-3B-Instruct meta-llama/Llama-3.2-1B-Instruct Source LLMs Mistral-Large-Instruct-2407 Gemma-2-27B-it Qwen-2.5-72B-Instruct Llama-3.1-70B-Instruct Mistral-Large-Instruct-2407 google/gemma-2-27b-it Qwen/Qwen2.5-72B-Instruct meta-llama/Llama-3.1-70B-Instruct Reward Model ArmoRM-LLaMA3-8B-v0.1 RLHFlow/ArmoRM-Llama3-8B-v0.1 Datasets UltraFeedback Magpie-Pro-DPO HelpSteer2 OpenMathInstruct-2 LeetCode Self-Oss-Instruct-SC2 Alpaca-GPT4-Zh Magpie-Qwen2-Pro-Zh princeton-nlp/llama3-ultrafeedback-armorm Magpie-Align/Magpie-Llama-3.1-Pro-DPO-100K-v0.1 nvidia/HelpSteer2 nvidia/OpenMathInstruct-2 greengerong/leetcode bigcode/self-oss-instruct-sc2-exec-filter-50k llamafactory/alpaca_gpt4_zh Magpie-Align/Magpie-Qwen2-Pro-200K-Chinese"
        },
        {
            "title": "B DETAILS OF EVALUATION BENCHMARKS",
            "content": "AlpacaEval-2 (Li et al., 2023) comprises 805 instructions from five different datasets and assesses models using two metrics: length-controlled (LC) win rate and raw win rate (WR) (Dubois et al., 2024). GPT-4-Preview-1106 serves as both the baseline model and the evaluator for the other models. Arena-Hard (Li et al., 2024) is challenging instruction-following benchmark that closely aligns with the human preference ranking from Chatbot Arena (Chiang et al., 2024), crowd-sourced platform for evaluating LLMs. It spans 250 high-quality topic clusters including 500 well-defined technical problem-solving queries. We report the win rate against GPT-4-0314 using GPT-4-Preview-1106 as the judge model. MT-Bench (Zheng et al., 2023) contains 80 multi-turn dialogues across eight categories, including writing, roleplay, reasoning, math, coding, extraction, STEM, and humanities. Each response is evaluated by GPT-4 on scale from 1 to 10, with the average score reported for each dialogue turn across the 80 dialogues. We use GPT-4-0613 as the judge model following the official setting. AlignBench v1.1 (Liu et al., 2023) is comprehensive multi-dimensional benchmark for evaluating LLMs alignment in Chinese. It contains 683 high-quality samples spanning 8 main categories, namely fundamental language ability, advanced Chinese understanding, open-ended questions, writing ability, logical reasoning, mathematics, task-oriented role play, and professional knowledge. We employ GPT-4-0613 to analyze and subsequently grade the responses. LiveBench-0831 (White et al., 2025) mitigates test set contamination through monthly question updates and the use of recently released data sources. It features verifiable, objective ground-truth answers for automatic and accurate scoring, eliminating the need for LLM-based evaluation. MMLU-Pro (Wang et al., 2024b) is an enhanced version of the MMLU (Hendrycks et al., 2021a) dataset, designed to address issues such as noisy data and reduced difficulty due to advances in model capabilities and increased data contamination. MMLU-Pro increases challenge levels by expanding"
        },
        {
            "title": "Preprint",
            "content": "multiple-choice options from 4 to 10, requiring reasoning across more questions, and incorporating expert-reviewed annotations for improved quality and reduced noise. MMLU-redux (Gema et al., 2024) is re-annotated subset of the MMLU (Hendrycks et al., 2021a) dataset created through manual assessment from 14 human experts. GPQA-Diamond (Rein et al., 2023) is challenging knowledge benchmark crafted by PhD-level domain experts in biology, physics, and chemistry. The dataset contains questions that are straightforward for experts but difficult for laypersons. We evaluate the highest quality diamond set comprising 198 questions. GSM8K (Cobbe et al., 2021) is set of grade-school math word questions that evaluates mathematical reasoning capabilities. MATH (Hendrycks et al., 2021b) is dataset of math problems ranging in difficulty from middle school to high school competition level. It tests wide range of mathematical skills, including algebra, calculus, number theory, and probability. AMC 235 (Yang et al., 2024b) refers to the 2023 American Mathematics Competition, featuring 25 multiple-choice questions that test advanced high school mathematics, including trigonometry, advanced algebra, and elements of calculus. HumanEval (Chen et al., 2021) evaluates code generation capabilities by presenting models with function signatures and docstrings and requiring them to implement the function body in Python. MBPP (Austin et al., 2021) is dataset of simple programming problems designed to assess the ability of models to generate short Python code snippets from natural language descriptions. LiveCodeBench 2408-2411 (Jain et al., 2025) is benchmark designed to evaluate coding capabilities using an evolving set of contamination-free problems sourced from platforms including LeetCode6, AtCoder7, and CodeForces8. We evaluate the subset comprising 160 problems published between August 2024 and November 2024. 5https://huggingface.co/datasets/AI-MO/aimo-validation-amc 6https://leetcode.com 7https://atcoder.jp 8https://codeforces.com"
        }
    ],
    "affiliations": [
        "School of Computer Science and Engineering, Sun Yat-sen University, China"
    ]
}