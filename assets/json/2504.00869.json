{
    "paper_title": "m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models",
    "authors": [
        "Xiaoke Huang",
        "Juncheng Wu",
        "Hui Liu",
        "Xianfeng Tang",
        "Yuyin Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time scaling has emerged as a powerful technique for enhancing the reasoning capabilities of large language models. However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, a simple yet effective approach that increases a model's medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new state-of-the-art performance, while our 32B model rivals previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts, helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Our case-by-case analysis identifies insufficient medical knowledge as a key bottleneck that prevents further performance gains through test-time scaling. We find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvements, particularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical knowledge, other than increased reasoning depth alone, is essential for realizing the benefits of test-time scaling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 9 6 8 0 0 . 4 0 5 2 : r Preprint. Under review. m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models Xiaoke Huang1, Juncheng Wu1, Hui Liu2, Xianfeng Tang2, Yuyin Zhou1 1 UC Santa Cruz https://github.com/UCSC-VLAA/m1 2 Amazon Research Figure 1: Test-time scaling of m1 series. Each plot shows accuracy (%) vs. reasoning token budget for different m1 model variants on various medical QA datasets. All models improve steadily as the thinking length increases, with the 32B model reaching the best accuracy. The linear regression lines are marked in dot line with their 95% confidence interval."
        },
        {
            "title": "Abstract",
            "content": "Test-time scaling has emerged as powerful technique for enhancing the reasoning capabilities of large language models (LLMs). However, its effectiveness in medical reasoning remains uncertain, as the medical domain fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes. In this paper, we provide the first comprehensive investigation of test-time scaling for medical reasoning and present m1, simple yet effective approach that increases models medical reasoning capability at inference. Our evaluation across diverse medical tasks demonstrates that test-time scaling (by increasing the thinking token budget) consistently enhances medical reasoning, enabling lightweight fine-tuned models under 10B parameters to establish new stateof-the-art performance, while our 32B model achieves results comparable to previous 70B-scale medical LLMs. However, we identify an optimal reasoning token budget of approximately 4K, beyond which performance may degrade due to overthinking. Budget forcing, which extends test-time computation through iterative prompts (e.g., appending Wait), helps models double-check answers but does not necessarily improve the overall medical QA performance and, in some cases, even introduces errors into previously correct responses. Taken together, our case-by-case analysis further identifies insufficient medical knowledge as key bottleneck that prevents further performance gains through test-time scaling. To overcome this constraint, we find that increasing data scale, improving data quality, and expanding model capacity consistently enhance medical knowledge grounding, enabling continued performance improvementsparticularly on challenging medical benchmarks where smaller models reach saturation. These findings underscore fundamental differences between medical and mathematical reasoning in LLMs, highlighting that enriched medical 1 Preprint. Under review. knowledge, other than increased reasoning depth alone, is essential for fully realizing the benefits of test-time scaling."
        },
        {
            "title": "Introduction",
            "content": "Test-time scaling has emerged as promising direction to enhance LLM reasoning by enabling models to think more during inference (Yang et al., 2025). OpenAIs o1 (Jaech et al., 2024) demonstrated that significantly extending an LLMs chain-of-thought can yield remarkable gains in problem-solving ability in both STEM fields and the medical domain (Muennighoff et al., 2025; Xie et al., 2024), but the exact methodology was not disclosed, spurring many replication efforts. Among the most successful replication attempts is the open-source s1 method (Muennighoff et al., 2025), which achieved remarkable results through surprisingly simple approach. By fine-tuning 32B parameter model on just 1K carefully curated examples with reasoning traces and implementing an inference control mechanism via Wait token, s1 enabled the model to effectively double-check its work. This simple approach produced state-of-the-art results on challenging mathematical benchmarks, e.g., outperformed OpenAIs o1-preview by up to 27%. Despite these advances, applying test-time scaling to the medical domain remains largely underexplored (Jiang et al., 2025). The medical domain presents unique challenges for LLMs: questions often involve multi-step logical reasoning, accurate recall of medical knowledge, and careful consideration to avoid unsafe or harmful answers (Chen et al., 2024b). As the medical field fundamentally differs from mathematical tasks in terms of knowledge representation and decision-making processes, the effectiveness of test-time scaling for medical reasoning remains uncertain. While advanced proprietary models like GPT-4 (Hurst et al., 2024) and Med-PaLM (Singhal et al., 2022) have achieved expert-level scores on medical exams (Zhang et al., 2024), opensource medical LLMs still struggle to reliably solve complex medical problems. Improving reasoning in these models is critical, as healthcare applications demand not just factual accuracy but robust diagnostic and therapeutic reasoning capabilities. Existing medical reasoning LLMs such as HuatuoGPT-o1 (Chen et al., 2024b) typically rely on computationally intensive methods like reinforcement learning with verification mechanisms. This raises key question: Can simple test-time scaling strategy, with minimal fine-tuning, also unlock strong medical reasoning? In this paper, we answer in the affirmative by presenting m1, lightweight methodology that adapts the test-time scaling paradigm to medical QA tasks. Our approach is straightforward: we curate high-quality set of medical questions with detailed step-by-step solutions (only 1K / 23K examples), fine-tune open LLMs on this data, and at inference use test-time controls to ensure the model fully thinks through problems before answering. Figure 1 illustrates the outcome: as we allow the model to generate longer chains of thought (x-axis increasing), accuracy on various medical benchmarks consistently improves for our m1 models. Notably, even our 7B-parameter model fine-tuned on 1K examples shows significant gains with more reasoning steps, and our 32B model achieves the highest scores across the board. To better understand the impact of test-time scaling on medical reasoning in LLMs, we conduct fine-grained study, systematically examining the effects of thinking budgets, inference techniques, data curation, and model capacity. While increasing the token budget consistently improves performance, we identify an optimal reasoning threshold of approximately 4K tokens, beyond which accuracy declines due to overthinking. In addition to increasing the token budget (Figure 1), reasoning can also be extended through budget forcing, wherein the model iteratively prolongs its thought process during inference (Muennighoff et al., 2025). However, unlike in mathematical reasoningwhere iterative refinement often enhances accuracyforcing additional reasoning in medical QA yields limited benefits and, in some cases, even degrades performance. This occurs when models with erroneous knowledge reconsider correct responses during extended reasoning, ultimately arriving at incorrect conclusions. 2 Preprint. Under review. closer analysis of failure cases reveals that this bottleneck stems from deficiencies in essential medical knowledge, which cannot be resolved merely by increasing the thinking budget. Consequently, extending the reasoning window reaches fundamental limit, and budget-forcing techniques offer negligible benefits, as models lacking foundational knowledge remain anchored to incorrect assumptions. Even with additional reasoning steps, these models still struggle to retrieve accurate information. In such cases, improving data quality and increasing model capacity provide more effective avenues for improvement. Our thorough ablation of data filtering strategies, dataset size, and model scaling demonstrates that when scaling thinking budget reaches its bottleneck, further performance gains can be achieved by enhancing data quality and scaling the model. Specifically, larger, difficultyfiltered, and diversity-sampled datasets consistently improve performance, while larger models further enhance scalability. This is because larger-capacity models or those finetuned on more extensive, high-quality datasets inherently possess richer medical knowledge, leading to higher accuracy. In conclusion, test-time scaling alone is insufficient for enhancing medical reasoning in LLMsit needs to be complemented by scaling model size and improving knowledge grounding through high-quality data. Empirically, m1 exhibits substantial performance despite its minimalism. Our 7B model finetuned on 23K examples (m1-7B-23K) attains new state-of-the-art accuracy of 60.32% among in-domain and out-domain medical exam datasets, surpassing previously established specialized models of similar scale such as HuatuoGPT-o1-7B/8B (trained with complex RL on 40K instances) (Chen et al., 2024b) and UltraMedical-8B (trained on hundreds of thousands of medical instructions) (Zhang et al., 2024). Furthermore, our larger 32B model trained with only 1K fine-tuning samples (m1-32B-1K) achieves performance comparable to 2X bigger resource-exhausted models (around 70B parameters with high training costs), underscoring the efficiency of our test-time scaling approach. All data, code, and models are publicly available to encourage future exploration in optimizing inference strategies in clinical AI applications."
        },
        {
            "title": "2 Related Works",
            "content": "Test-time scaling for LLMs. There is growing interest in techniques that enhance an LLMs reasoning without altering its weights, by allocating more computation at inference time (Jaech et al., 2024; Meng et al., 2023; Hu et al., 2025). basic form is chain-of-thought prompting, e.g. instructing the model to think step by step, which often improves performance on complex tasks (Wei et al., 2022). More explicit approaches include generating multiple solutions and using majority voting or self-consistency to pick an answer, or employing search-based strategies with verifiers and lookahead. These methods trade extra inference passes for accuracy gains. In contrast, sequential test-time scaling keeps single reasoning thread but makes it longer. OpenAIs o1 model hinted at the power of simply extending the reasoning length (Jaech et al., 2024). Muennighoff et al. (2025) formalized this by fine-tuning an LLM to utilize special Wait tokens, which allow controlling response length during inference. Their budget forcing method (described below) proved more effective than parallel voting strategies. Other recent research has proposed optimizing the allocation of test-time compute, for example finding an optimal stopping length per problem to avoid overthinking (Yang et al., 2025). Our work builds directly on the simple test-time scaling idea (Muennighoff et al., 2025; Aggarwal & Welleck, 2025) we apply it to new domain (medicine) and confirm its benefits in very different setting. We focus on single-trace sequential reasoning, noting that it is complementary to orthogonal advances like tool use or retrieval augmentation. Medical LLMs. The success of GPT-4 in medical exams (Zhang et al., 2024) has spurred numerous open efforts to train medical domain LLMs. Early approaches centered on domain-specific pre-training: e.g. Wu et al. (2024) and Qiu et al. (2024) pre-trained Llama models on medical text corpora (MIMIC-III (Johnson et al., 2016), PubMed 1, etc.) to inject medical knowledge. While this improves knowledge recall, the gains on reasoning-heavy 1https://pubmed.ncbi.nlm.nih.gov/ 3 Preprint. Under review. Figure 2: An overview of our data curation and training pipeline. We start with 196K raw medical QA examples, apply difficulty filtering (retaining 37K that Qwen2.5-7BInstruct (Yang et al., 2024) or its 32B version cannot solve), then use DeepSeek-R1 (Guo et al., 2025) to generate reasoning and keep correct solutions (m23K). We perform diversity sampling to select 1K high-quality subset (m1K). These datasets are used to fine-tune base models (Qwen2.5 7B and 32B Instruct) via Supervised Fine-Tuning (SFT), resulting in the m1 models (m1-7B-1K, m1-7B-23K, m1-32B-1K). tasks were limited (Jiang et al., 2025; Wu et al., 2025). More recent projects emphasize instruction tuning and reinforcement learning specialized for medicine. For example, OpenBioLLM was fine-tuned with expert-validated instructions and Direct Preference Optimization, and reportedly outperformed GPT-4 and Med-PaLM-2 on several biomedical QA benchmarks (Pal & Sankarasubbu, 2024). Med42 is another open model suite that achieved impressive results, even exceeding GPT-4.0 on many multi-choice medical QA tasks (Christophe et al., 2024). To push reasoning ability further, some works incorporate explicit reasoning supervision or verification. HuatuoGPT-o1 introduced verifiable medical problem-solving: they constructed 40K problems with known solutions and used twostage training (SFT + RL with verifier) to train 70B model (Chen et al., 2024b). This model achieved new state-of-the-art results on medical reasoning benchmarks, outperforming both general and prior medical LLMs (Zhang et al., 2024). UltraMedical built massive dataset of 410K mixed manual/synthetic instructions for biomedicine (Jiang et al., 2025), and fine-tuned Llama-3 models with supervised and preference learning. The 70B UltraMedical model reached 86.5% accuracy on MedQA, nearly matching Med-PaLM2 (Singhal et al., 2025) and GPT-4 (Achiam et al., 2023). In contrast, our approach remains lightweight as we do not introduce new RL or verification components, and our data size (1K23K) is tiny, yet through test-time scaling, we achieve competitive results with these state-of-the-art models. We hope this encourages more exploration of inference-time techniques as an efficient alternative for domain-specific LLMs."
        },
        {
            "title": "3 Method",
            "content": "Our approach consists of three parts: 1) Data curation: selecting and generating highquality set of medical QA examples with detailed reasoning (Section 3.1), 2) Model training: Supervised Fine-Tuning (SFT) of base LLMs on this data (Section 3.2), and 3) Inference: test-time control of the models reasoning length (Section 3.3). Figure 2 provides an overview of the full pipeline. 3.1 Data curation Initial collection. To construct the training data for m1 through multi-step refinement process, we begin with large pool of approximately 196K medical QA samples com4 Preprint. Under review. piled from public datasets: MedMCQA (Pal et al., 2022), MedQA-USMLE (Jin et al., 2021), HeadQA (Vilares & omez-Rodrıguez, 2019), and PubMedQA (Jin et al., 2019). These include multiple-choice questions from medical exams as well as open-ended research questions. All samples are decontaminated against the evaluation data in Section 4.1. More details are presented in Appendix A. Difficulty filtering. Following (Muennighoff et al., 2025), we identify subset of solvable yet non-trivial problems by performing difficulty filtering using two strong base models. Specifically, we use Qwen2.5-Instruct (Yang et al., 2024) (an open general LLM) of 7B and 32B parameters to attempt each question. We filter question if either Qwen-7B or Qwen-32B answers it correctly. This heuristic retains questions that are challenge to solve, eliminating those that are too easy for either models. Difficulty filtering pruned the dataset from 196K down to 37K samples. Thinking generation. We employ DeepSeek-R1 (Guo et al., 2025), state-of-the-art open reasoning LLM, to generate chain-of-thought and final answer for each of the 37K questions. DeepSeek-R1 was chosen for its robust reasoning capability (its comparable to OpenAIs o1 (Jaech et al., 2024) in multi-step problem solving). For each question, we prompt DeepSeek-R1 to produce detailed solution explanation ending in definitive answer. We then apply solution validation: we only keep those instances where DeepSeek-R1s final answer is correct (matching the ground-truth). This yields set of 23K high-quality thinking - answers, where each question now paired with verified-correct reasoning process. This step ensures our training data predominantly consists of valid reasoning, while incorrect chains are discarded. More details are presented in Appendix and B.1. Diversity sampling. We design diversity sampling strategy to construct well-balanced and enriched subset for training our primary model. This process highlights two key components: domain balance and dataset balance. First, we ensure domain balance by annotating each sample with Medical Subject Headings (MeSH) categories 2, enabling systematic coverage across medical specialties (e.g., cardiology, neurology) and question types. Second, we address dataset imbalance through stratified sampling, first selecting domains, then source datasets, and finally individual samples (see Appendix Tables 3 and 4 for distributions). We perform stratified sampling at the dataset level to address the imbalance in sample counts across datasets (Appendix, Table 3, 4). Specifically, we first sample domain, then sample dataset, and finally roll-out sample. The process is repeated until there are 1K samples (m1K), which will be served as our core training set for m1. The remaining 23K difficult samples (m23K) can be used to augment training or for ablations. We provide summary statistics of the final data in Appendix A. 3.2 Model Training We fine-tune three model variants, corresponding to two model sizes (7B and 32B) and two training set sizes (1K and 23K). For each, we use the pre-trained Qwen2.5-Instruct model as the initialization. Qwen2.5 is recent high-performance open LLM (Yang et al., 2024); using it as our base ensures strong general language ability and allows us to focus on injecting medical reasoning. We format each training example in question reasoning answer style. This format teaches the model to produce coherent reasoning process and then give the answer. Using this data, we perform SFT for each model: m1-7B-1K: Fine-tuned on the 1K m1K dataset using the Qwen2.5-7B-Instruct. This represents the minimal training scenario. m1-7B-23K: Fine-tuned on the full 23K filtered dataset using Qwen2.5-7B-Instruct. This lets us examine the effect of more training data (23K vs 1K) at the same model size. m1-32B-1K: Fine-tuned on the 1K dataset using the larger Qwen2.5-32B-Instruct. This shows the effect of larger model with minimal data. 2https://www.ncbi.nlm.nih.gov/mesh/ 5 Preprint. Under review. Model MedMC MedQA PubMed MMLU-P GPQA Lancet MedB (4) MedB (5) MedX NEJM Avg. In-Distribution Test Out-of-Distribution Test MedLlama3-8B-v1 MedLlama3-8B-v2 OpenBioLLM-8B MMed-8B MMedS-8B MMed-8B-EnIns Med42-8B UltraMedical-8B-3 UltraMedical-8B-3.1 HuatuoGPT-o1-7B HuatuoGPT-o1-8B Qwen2.5-7B-Instruct +CoT m1-7B-1K m1-7B-23K Qwen2.5-72B-Instruct +CoT Med42-70B OpenBioLLM-70B UltraMedical-70B-3 HuatuoGPT-o1-70B HuatuoGPT-o1-72B Qwen2.5-32B-Instruct +CoT m1-32B-1K 34.74 59.34 54.63 52.71 47.29 58.09 56.35 59.22 63.78 63.47 63.97 56.56 56.11 58.26 62.54 66.60 66.15 62.28 74.23 72.94 75.23 76.76 64.83 64.33 67.34 55.07 59.39 55.30 54.28 57.19 60.33 59.78 71.09 75.73 71.56 74.78 61.51 64.49 71.01 75.81 74.55 76.43 51.14 75.10 83.90 86.80 88. 75.26 74.86 83.50 52.70 75.50 70.10 63.40 77.50 63.80 76.00 71.10 79.20 78.60 80.10 71.30 72.60 77.50 75.80 70.80 71.30 78.10 79.30 80.00 81.40 79.90 68.00 68.90 77.60 < 10B LLMs 27.43 55.11 49.32 48.27 33.55 51.60 55.64 61.50 64.30 67.23 63.71 61.17 62.15 65.15 65. 30.77 36.41 41.03 34.87 22.05 45.90 48.21 50.00 48.72 47.95 55.38 42.56 52.56 51.79 53.08 > 10B LLMs 66.06 69.77 54.53 71.92 73.94 76.09 80.46 74.72 74.72 76.94 62.05 63.85 50.77 50.77 58.72 66.67 64.36 63.85 64.87 66. 42.23 52.43 52.43 53.40 55.10 55.34 59.47 61.89 67.23 62.14 64.32 61.17 60.68 64.32 62.62 66.50 65.78 54.61 68.93 75.49 72.82 70.87 66.02 66.75 70.15 38.31 48.38 41.23 41.23 54.22 59.09 44.81 54.22 64.61 52.92 58.44 46.75 50.97 58.77 63.64 57.14 60.06 45.78 58.44 72.08 72.08 77.27 60.39 60.39 73. 33.77 39.94 32.47 35.39 55.84 56.17 46.75 52.27 55.19 50.65 51.95 40.58 42.86 51.95 59.74 53.57 54.22 37.99 54.55 64.61 68.51 73.05 52.92 54.22 67.86 11.04 13.46 14.29 13.73 17.39 18.56 14.63 15.25 17.39 15.11 16.84 12.15 13.18 16.29 19.81 14.91 14.84 16.29 21.33 21.67 26.36 23.53 13.87 14.56 25. 49.25 54.56 54.23 54.39 53.40 62.35 62.69 64.51 66.83 65.17 64.84 59.04 58.54 62.52 64.34 68.99 69.15 56.05 67.83 73.13 74.13 76.29 66.67 66.33 73.13 37.53 49.45 46.50 45.17 47.35 53.12 52.43 56.11 60.30 57.48 59.43 51.28 53.41 57.76 60.32 60.12 61.16 50.75 62.24 67.65 70.01 71.13 60.65 60.99 68. Table 1: Baseline Comparisons. We report accuracy (%) on each evaluation dataset for various models. Our m1 models (in bold) are shown in the 10B group (m1-7B variants) and > 10B group (m1-32B). +CoT indicates using chain-of-thought prompting at inference for that base model. We mark Green color within each parameter groupr: the deeper the color, the higher the accuracy. For header abbreviations, please refer to Section 4.1. 3.3 Inference At inference time, we employ test-time scaling by managing the models generation of the chain-of-thought. Specifically, we define thinking budget: maximum number of tokens the model is allowed to generate before producing final answer. By allocating larger budget, we give the model more thinking space to potentially reason through the problem. If the model would naturally finish its reasoning early, we intervene to use the budget fully. Besides, we apply budget forcing technique to extend the thinking process of the model: when the model outputs end-of-think token indicating the end of thinking before reaching the token budget, we replace it with Wait. and force the model to keep thinking generation. According to (Muennighoff et al., 2025), this method often leads the model to double-check or refine its initial answers for math problems."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Evaluation Settings Datasets. We evaluate on nine medical QA benchmarks, grouped into In-Distribution and Out-of-Distribution tests. We measure accuracy for all datasets. 1) In-Distribution tests, the companied test splits from our training data, include MedMCQA (Pal et al., 2022) (MedMC); MedQA-USMLE (Jin et al., 2021) (MedQA), and PubMedQA (Jin et al., 2019) (PubMed). 2) Out-of-Distribution tests, which not included in training and stylistically distinct, assessing m1s reasoning generalization: medical related questions from MMLU-Pro (Wang et al., 2024) (MMLU-P) and GPQA (Rein et al., 2024), small QA sets from Lancet and the New England Journal of Medicine (NEJM); 4 Options (MEdB (4)) and 5 Options (MedB (5)) splits from the MedBullets platform (Chen et al., 2024a); and MedXpertQA (Zuo et al., 2025) (MedX). 6 Preprint. Under review. LLM baselines. We compare our models against variety of general and specialized medical LLM baselines: 1) general base instruct models Qwen2.5-7B, tested both as-is and with chain-of-thought prompting (+CoT); 2) specialized medical models including MedLlama3 3, OpenBioLLM (Pal & Sankarasubbu, 2024), MMed-Llama (Qiu et al., 2024), Med42 (Christophe et al., 2024), UltraMedical (Zhang et al., 2024); and 3) medical reasoning model HuatuoGPT-o1 (Chen et al., 2024b) undergoes complex RL training. Additionally, we compare to state-of-the-art large open medical LLMs ( 10B), including Med42-70B (Christophe et al., 2024), OpenBioLLM-70B (Pal & Sankarasubbu, 2024), UltraMedical-70B (Zhang et al., 2024), HuatuoGPT-o1-70B/72B (Chen et al., 2024b), and baseline Qwen2.5 models (32B, 72B) with their respective +CoT versions. HuatuoGPT-o1 and UltraMedical employ complex training strategies involving reinforcement learning or expert feedback; therefore, matching or exceeding their performance with our simpler test-time scaling method underscores its effectiveness. 4.2 Results Test-time scaling with different thinking budgets. We first evaluate how increasing the chain-of-thought token budget at inference affects performance on various medical QA datasets. As illustrated by the upward trajectories in Figure 1, our m1 approach gains consistent accuracy improvements as the thinking budget grows, demonstrating the efficacy of simple test-time scaling. Despite simplicity, Table 1 presents that our m1-7B-23K achieves an average accuracy of 60.32% amongst in-distribution and out-of-distribution sets, which exceeds complex RL tuned HuatuoGPT-o1-7B by 2.84%, and matches the large-scale SFTtuned UltraMedical-8B. Notably, beyond 4K tokens, the improvements begin to saturate, indicating limited additional benefit from extremely long reasoning. Larger model capacity helps. When scaling model size from 7B to 32B parameters, we observe more pronounced benefit from test-time scaling as larger models inherently In Table 1, m1-32B-1K consistently outperforms or possess richer medical knowledge. matches even larger (70B+) specialized medical LLMs, demonstrating that pairing larger base model with simple supervised thinking traces and inference-time scaling yields strong results. This trend is also apparent in Figure 1, where the 32B models accuracy curve leads across most datasets as the thinking budget increases. Budget forcing does not help. Unlike mathematical tasks, where prompting the model to repeatedly refine its chain-of-thought can yield further gains, our experiments show diminishing returns from forced re-thinking (Figure 3). Although the model will generate additional intermediate tokens when repeatedly prompted to keep thinking, we see minimal improvement, suggesting that medical reasoning may differ from math domains in how additional iterative reasoning is best leveraged. We analyze such failure cases in the following sections. SFT data ablation. We ablate two key data curation steps used in our SFT process: difficulty filtering and diversity sampling (comprising domain and dataset balancing). As shown in Table 2, at the 1K training scale, models fine-tuned on difficulty-filtered data outperform those trained on randomly sampled data by +0.27% percentage points on average. Adding domain balance further improves performance by 0.43%, and incorporating both domain and dataset balance yields additional gains, reaching up to 56.55% average accuracy. At the 23K scale, overall performance improves substantially, and difficulty filtering alone provides +0.65% accuracy boost on average. These results highlight the critical role of both data quality and scale in enhancing model performance. Failure cases. In this section, we discuss several fail cases where the models fail to perform test-time scaling, underscoring the critical role of accurate knowledge in medical reasoning models. Specifically, our investigation can be distilled into the following key points: 3https://huggingface.co/johnsnowlabs/ 7 Preprint. Under review. Figure 3: Force thinking for different evaluation datasets. Accuracy vs. number of budget forcing times (iterations of injecting Wait) for each m1 model (7B-1K, 7B-23K, 32B-1K). value of 0 means the models first answer is taken without forcing, while higher values mean the model was compelled to reconsider up to that many times (within 2048-token limit). Data Filtering MedMC MedQA PubMed MMLU-P GPQA Lancet MedB (4) MedB (5) MedX NEJM Avg. In-Distribution Test Out-of-Distribution Test Random Hard Random Hard Domain Hard Domain Dataset Random Hard 57.26 56.99 58.88 57.97 60.41 62.01 66.46 68.66 66.38 70.23 71.64 73. 73.60 73.70 74.00 76.10 76.50 75.80 1K Scale 61.63 63.26 64.95 64.23 67.43 65.54 44.62 46.41 45.38 49. 62.62 62.14 63.11 62.14 23K Scale 48.72 50.51 62.38 61.89 54.55 56.82 54.55 57.79 60.06 60. 53.57 44.81 49.68 50.97 14.70 16.36 16.36 17.12 58.54 61.03 61.19 59.20 54.75 55.02 55.45 56.55 54.22 55.19 15.32 18. 61.86 64.34 57.85 58.80 Table 2: Data Filtering Ablation: difficulty filtering (Hard), and Domain and Dataset balance for diversity sampling. Difficulty filtering consistently yields the largest gains across both 1K and 23K training scales, while domain and dataset balancing provide complementary improvements. Notably, scaling up from 1K to 23K substantially boosts accuracy, underscoring the importance of data scale. The same header abbreviations as Table 1. The extent of knowledge is crucial for effective medical reasoning. As illustrated in Figure 4, m1-7B-1K is unable to generate accurate reasoning because it lacks crucial knowledge: Anterior ethmoidal artery belongs to the internal carotid. In contrast, both m17B-23K and m1-32B-1K possess this essential knowledge. Consequently, having substantial amount of accurate knowledge, whether from fine-tuning data or the pre-training model, enhances the models capability for medical reasoning. This is similarly evidenced in Table 1, where m1-7B-23K and m1-32B-1K exhibit significantly better performance. Incorrect knowledge obstructs the reasoning. As demonstrated in Figure 5, even when the model generates the correct answer at first, forcing it to re-think cause it to retrieve faulty information, which results in an incorrect response. Therefore, such erroneous knowledge may lead to unstable reasoning process, highlighting the importance of verifying the accuracy of the training data for medical reasoning models. Test-time scaling fails to rectify incorrect knowledge. In fields like math and coding, scaling up thinking processes can enhance models reasoning by allowing it to conduct self-reflection and identify errors in its previous logic. However, in the medical domain, errors largely stem from misconceptions in knowledge. These are difficult to correct merely by increasing the reasoning budget. As illustrated in Figure 4, despite m1-7B-1K executing the most extended reasoning, its lack of crucial medical knowledge hinders it from arriving at the correct answer. Additionally, in Appendix Figure 7, even when the model is forced to re-think multiple times, it is unable to rectify the inaccurate knowledge. 8 Preprint. Under review. Figure 4: failure case of test-time scaling with the Qwen2.5-7B using 1K reasoning data. Although the m1-7B-1K conducts the longest reasoning, its deficiency in essential medical knowledge prevents it from producing the right answer. On the other hand, both m1-7B-23K and m1-32B-1K effectively resolve the question with relatively brief reasoning procedure. Figure 5: failure case of budget forcing. Initially, the model produces the correct answer, but forcing it to re-think causes the model to retrieve incorrect knowledge, ultimately resulting in an erroneous answer."
        },
        {
            "title": "5 Conclusions",
            "content": "We introduced m1, demonstrating that test-time scaling significantly improves medical reasoning in large language models without requiring extensive fine-tuning. Performance across diverse medical QA benchmarks consistently improved with increased inference9 Preprint. Under review. time reasoning budgets. Crucially, we found that test-time scaling alone cannot remedy fundamental deficiencies in medical knowledge, emphasizing the necessity for high-quality medical data and model scale expansion. m1 achieves strong performance, outperforming more expensive approaches such as HuatuoGPT-o1 and UltraMedical on various benchmarks. Our 7B model trained on 23K data establishes new state-of-the-art in the 10B parameter category, and our 32B model rivals models 2 in size. We release full-stack open-source package including the curated dataset (m1K), fine-tuned model weights, and inference code with budget control, to encourage future exploration in optimizing inference strategies in clinical AI applications."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. Benchmarking large language models on answering and explaining challenging medical questions. arXiv preprint arXiv:2402.18060, 2024a. Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024b. Clement Christophe, Praveen Kanithi, Tathagata Raha, Shadab Khan, and Marco AF Pimentel. Med42-v2: suite of clinical llms. arXiv preprint arXiv:2408.06142, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-reasoner-zero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Shuyang Jiang, Yusheng Liao, Zhe Chen, Ya Zhang, Yanfeng Wang, and Yu Wang. MedS3: Towards medical small language models with self-evolved slow thinking. arXiv preprint arXiv:2501.12051, 2025. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. Alistair EW Johnson, Tom Pollard, Lu Shen, Li-wei Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger Mark. Mimiciii, freely accessible critical care database. Scientific data, 3(1):19, 2016. 10 Preprint. Under review. Chunyang Meng, Shijie Song, Haogang Tong, Maolin Pan, and Yang Yu. Deepscaler: Holistic autoscaling for microservices based on spatiotemporal gnn with adaptive graph learning. In 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 5365. IEEE, 2023. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: largescale multi-subject multi-choice dataset for medical domain question answering. In Conference on health, inference, and learning, pp. 248260. PMLR, 2022. Malaikannan Sankarasubbu Ankit Pal and Malaikannan Sankarasubbu. Openbiollms: Advancing open-source large language models for healthcare and life sciences. Hugging Face repository, 2024. Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards building multilingual language model for medicine. Nature Communications, 15(1):8384, 2024. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. In First Conference on Language Modeling, 2024. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138, 2022. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, et al. Toward expert-level medical question answering with large language models. Nature Medicine, pp. 18, 2025. David Vilares and Carlos omez-Rodrıguez. Head-qa: healthcare dataset for complex reasoning. arXiv preprint arXiv:1906.04701, 2019. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmcllama: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, 31(9):18331843, 2024. Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li, Xiaoxiao Li, and Yuyin Zhou. Medreason: Eliciting factual medical reasoning steps in llms via knowledge graphs. arXiv preprint, 2025. Yunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang Xie, and Yuyin Zhou. preliminary study of o1 in medicine: Are we closer to an ai doctor? arXiv preprint arXiv:2409.15277, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning. arXiv preprint arXiv:2502.18080, 2025. Preprint. Under review. Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, et al. Ultramedical: Building specialized generalists in biomedicine. Advances in Neural Information Processing Systems, 37:2604526081, 2024. Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362, 2025. 12 Preprint. Under review."
        },
        {
            "title": "A Data Statistics",
            "content": "Dataset statistics. Table 3 shows the statistics of all datasets used in the paper. Note that the sample counts across datasets are highly imbalanced. Table 3: The statistics of all datasets used in the paper. Dataset Initial collection +After difficulty filtering +Generating thinking data +Decontamination & deduplication (m23K) Random 23K Random 1K Hard random 1K Hard Domain Balanced 1K Hard Domain Dataset Balanced 1K (m1K) MedQA HeadQA MedMCQA PubMedQA Summation 10,178 2,099 1,628 1,628 1,316 61 78 52 274 2,657 331 209 209 8 10 20 123 182,822 35,270 21,628 21,628 21,831 929 909 924 575 500 116 39 28 2 3 4 28 196,157 37,816 23,504 23,493 23,493 1,000 1,000 1,000 1,000 Token length statistics. As illustrated in Figure 6, the distributions of training token lengths between the m1K/m23K and s1K (Muennighoff et al., 2025) datasets exhibit clear differences. The m1K/m23K dataset shows strongly right-skewed distribution, with most samples having token lengths clustered around 1,000 tokens, quickly diminishing toward lengths beyond 3,000 tokens. In contrast, the s1K dataset displays more uniform and broader distribution, spanning widely from about 2,500 to over 15,000 tokens, with peaks around 5,000 to 10,000 tokens. These contrasting distributions reflect differing data preparation strategies: m1K/m23K focuses on concise medical knowledge without longer thinking steps, whereas s1K includes longer, more detailed reasoning traces suitable for complex multi-step inference tasks. (a) m1K (b) m23K Figure 6: The token length distributions of m1K and s1K (Muennighoff et al., 2025). The 25%/50%/75% quantile is marked in transparent vertical dotted lines. (c) s1K 13 Preprint. Under review. Sample domain statistics. We list the statistics of sample domains for m1K and m23K in Table 4. We use the domain label from MeSH Qualifiers with Scope Notes 4."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Data Generation Details We generate the reasoning traces and answers using the API of deepseek-ai/DeepSeek-R1 model on the SiliconFlow platform 5. We call the API with its default sampling parameters. The API calls are scheduled with curator 6. According to the initial observation on the length of the outputs, we set the limit to 8K as no samples have outputs with length larger than 8K. The prompt is formatted as \"Return your final response within boxed{{}}.n{Question}n{Options}\", thus the answers are enclosed and are easy to be extracted and verified. We perform data decontamination and dedupliation following OpenThoughts project 7. B.2 SFT Details All fine-tuning runs use standard language modeling training with trl library: we optimize the model to minimize the output cross-entropy on the reasoning to answer sequences (teacher-forcing the entire sequence). We use the same training hyperparameters as the s1 paper for consistency (Muennighoff et al., 2025): 5 epochs of training, batch size equals 16, low learning rate of 1e-4 with warmup and cosine decay, modest weight decay of 1e-4, Adam betas of 0.9 and 0,95. The thinking part is enclosed with <im_start>think and <im_start>answer. SFT is performed with trl and transformers libraries. Training m1-7B-1K and m1-7B-23K is extremely fast on the order of minutes on 8 H100 GPUs, and m1-32B-1K can be trained in few hours with 16 H100 GPUs. This underscores the efficiency of our approach: unlike massive instruction tuning efforts that require many days on tens of GPU nodes, our models reach convergence with modest compute. We did not apply any reward modeling or RL in training: the model purely learns to imitate the given chain-of-thought format. B.3 Evaluation Details Datasets. To thoroughly assess both in-domain performance and generalization, we evaluate on eight medical QA benchmarks, grouped as follows: In-Distribution Tests: 1. MedMCQA (Pal et al., 2022) collection of 3.5K multiple-choice questions from Indian medical entrance exams, testing general medical knowledge. 2. MedQA-USMLE (Jin et al., 2021) the USMLE question dataset (NYU MedQA) containing US medical licensing exam MCQs; we use the standard test split. 3. PubMedQA (Jin et al., 2019) dataset of biomedical research questions (factoid paired with abstracts) where the task is to answer yes/no/maybe or short answer. These three were part of our training data pool (though we filtered and sampled from them), so they represent in-domain evaluations. We report accuracy (for MCQ, percentage of correct choices; for PubMedQA, percentage of correct yes/no/maybe). Out-of-Distribution Tests: 4https://www.nlm.nih.gov/mesh/qualifiers scopenotes.html 5https://siliconflow.cn/ 6https://github.com/bespokelabsai/curator/ 7https://github.com/open-thoughts/open-thoughts/tree/main/open thoughts 14 Preprint. Under review. 1. MMLU-Pro (Wang et al., 2024) (Medical) the medical category subset of the Massive Multitask Language Understanding benchmark, which includes professional medicine questions and related subjects. We specifically evaluate on the Professional Medicine section (and report accuracy). We follow the split from (Chen et al., 2024b). 2. GPQA (Medical) (Rein et al., 2024) the biomedical portion of the Graduate-Level Physics/Chemistry/Biology QA dataset GPQA. This dataset contains extremely challenging, Google-proof multiple-choice questions created by experts, requiring high reasoning (we use the biology/medical questions, 150 in total). We follow the split from (Chen et al., 2024b). 3. Lancet & NEJM we compiled two small sets of QA pairs from The Lancet 8 and New England Journal of Medicine 9 (NEJM) clinical case reports (answers verified from the text). These assess how models handle medical literature style questions. 4. MedBullets (Chen et al., 2024a) collection of practice questions from the MedBullets medical education platform. We specifically take subsets of difficulty level 4 and 5 (on 15 scale, 5 being hardest), denoted MedBullets Op4 and MedBullets Op5, about 100 questions each, to serve as challenging test sets. 5. MedXpertQA (Zuo et al., 2025) custom set of 50 expert-written multi-step medical reasoning questions we created for qualitative evaluation (free-form answers). For MedXpertQA we report the percentage of questions answered correctly. These out-of-distribution (OOD) sets were not used in training and often differ in style from our training data (e.g. long clinical vignettes, or extremely tricky edge cases). They allow us to test how well m1s reasoning generalizes. Methods. We compare our models against broad range of baselines, including both general LLMs and specialized medical LLMs: 1. Qwen2.5 Instruct (7B, 32B, 72B) The base instruct models (no medical fine-tuning). We include these to show the starting performance of the underlying models before our fine-tuning. We also test Qwen2.5 with chain-of-thought prompting (+CoT), where we simply prompt it to think step by step at inference, to see if prompting alone can elicit similar reasoning (this baseline uses no additional training). 2. MedLlama3 (8B)10 An 8B instruction-tuned model released by M42 (Johns Hopkins/APL), one of the early open medical LLMs. We list two versions from their releases. 3. OpenBioLLM (8B) (Pal & Sankarasubbu, 2024) The 8B model from Saama AI, fine-tuned with expert-curated medical data. 4. MMed-Llama (8B) (Qiu et al., 2024) multilingual medical model from MedS3 work, which underwent additional pre-training (denoted MMedS or MMed in results). 5. Med42 (8B) (Christophe et al., 2024) The 8B model from the Med42-v2 suite, instruction and preference-tuned on clinical data. 6. UltraMedical (8B) (Zhang et al., 2024) The 8B model from Tsinghuas UltraMedical project (we test both the v3.0 and v3.1 versions if available). 7. HuatuoGPT-o1 (7B & 8B) (Chen et al., 2024b) The smaller versions of HuatuoGPTo1 (the 70B models distilled or intermediate checkpoints) as reported in their paper. 8. Larger models (>10B): We also compare to state-of-the-art open models in the larger size class: Med42-70B (Christophe et al., 2024), OpenBioLLM-70B (Pal & Sankarasubbu, 2024), UltraMedical-70B (Zhang et al., 2024), and HuatuoGPT-o170B/72B (Chen et al., 2024b) (if available). These represent the current best open medical LLMs (some claim parity with GPT-4). 8https://www.thelancet.com/ 9https://www.nejm.org/ 10https://huggingface.co/johnsnowlabs/ Preprint. Under review. It is worth noting that some of these baselines (e.g. HuatuoGPT-o1 (Chen et al., 2024b), UltraMedical (Zhang et al., 2024)) involve complex training regimes (RL or extensive preference tuning), and in cases like Med42 and OpenBioLLM, they incorporate expert feedback. Our approach does not, so beating or matching them would be strong indication of the power of test-time scaling. Inference. We use SGLang as our inference engine. We use bfloat16 precision and greedy sampling (i.e., temperature=0) for inference. fixed seed of 42 is used during inference. The prompt format is: \"{Question}n{Options}n{Instruction}\". We format options as: \"A. nonC. The default instruction is: \"Return boxed{{}}.\" For chain-of-thought inference with baseline LLMs Qwen2.5 7B/32B/72B Instruct, we update the instruction to: \"Let's think step by step. Return your final response within boxed{{}}.\". maybe\". response yesnB. within final your Answer matching. We try to directly extract the answers from \"boxed{{}}\". If the extraction fails, we follow (Chen et al., 2024b) to match answers via regex. If multiply answers are matched, we only choose the first one."
        },
        {
            "title": "C Failure Case of Budget Forcing",
            "content": "We illustrate failure case of budget forcing in Figure 7. Initially, the model arrives at the correct answer with concise and accurate reasoning. However, when forced to continue thinking for longer, the extended reasoning introduces confusion and incorporates incorrect anatomical associations, ultimately leading to the wrong answer. This highlights key limitation of budget forcing in medical QA: more reasoning does not always equate to better reasoning. Figure 7: failure case of budget forcing. 16 Preprint. Under review. Domain Abnormalities Administration & Dosage Adverse Effects Agonists Analogs & Derivatives Analysis Anatomy & Histology Antagonists & Inhibitors Biosynthesis Blood Blood Supply Cerebrospinal Fluid Chemical Synthesis Chemically Induced Chemistry Classification Complications Congenital Cytology Deficiency Diagnosis Diagnostic Imaging Diet Therapy Drug Effects Drug Therapy Economics Education Embryology Enzymology Epidemiology Ethics Ethnology Etiology Genetics Growth & Development History Immunology Injuries Innervation Instrumentation Isolation & Purification Legislation & Jurisprudence Metabolism Methods Microbiology Mortality Nursing Organization & Administration Parasitology Pathogenicity Pathology Pharmacokinetics Pharmacology Physiology Physiopathology Poisoning Prevention & Control Psychology Radiation Effects Radiotherapy Rehabilitation Secondary Standards Statistics & Numerical Data Supply & Distribution Surgery Therapeutic Use Therapy Toxicity Transmission Transplantation Trends Ultrastructure Urine Veterinary Virology Table 4: The statistics of sample domain for m1K and m23k. MedQA PubMedQA HeadQA MedMCQA Summation MedQA PubMedQA HeadQA MedMCQA Summation m23K m1K 24 9 116 2 1 2 17 14 4 45 19 7 0 16 0 0 73 59 3 58 229 25 3 12 164 1 0 12 5 8 8 0 133 66 8 0 17 12 10 0 0 0 15 1 27 3 0 0 5 10 68 4 31 45 92 17 17 20 1 0 0 2 2 5 0 5 5 45 2 1 0 0 0 11 0 12 1 0 2 0 0 2 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 4 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 2 0 2 0 0 0 0 0 0 0 0 3 2 1 0 14 3 2 3 0 0 0 5 1 21 5 2 0 1 2 17 2 2 0 6 1 1 1 3 3 2 0 5 5 3 2 2 1 1 0 0 1 5 3 0 0 5 3 1 0 2 6 1 17 3 0 1 21 0 0 1 0 0 2 0 1 1 7 0 1 0 0 0 0 0 5 10 6 9 2 1 2 10 7 4 7 6 4 0 5 0 0 6 7 3 6 2 7 2 11 1 1 0 7 4 4 5 0 4 4 0 0 10 12 3 0 0 0 1 1 5 3 0 0 5 5 11 3 5 5 5 5 6 0 1 0 0 2 2 5 0 5 5 5 2 1 0 0 0 8 0 6 671 602 1,056 23 11 240 2,348 129 80 406 247 41 6 99 363 346 701 441 94 306 1,928 751 47 119 679 20 22 258 103 276 30 5 796 364 257 74 218 443 168 151 7 205 178 265 385 33 11 115 215 81 1,602 109 347 1,231 1,189 188 149 207 57 28 11 46 69 61 10 757 236 427 31 88 31 1 7 88 2 107 646 590 936 20 10 222 2,328 113 73 360 228 34 1 82 342 341 625 382 90 245 1,681 720 41 107 509 17 20 245 95 265 20 5 657 293 245 72 199 430 157 151 7 204 158 260 358 30 6 111 209 71 1,532 99 315 1,168 1,093 171 131 165 56 28 10 44 67 53 10 749 230 373 29 86 31 1 7 77 2 90 17 1 0 2 0 0 2 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 4 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 2 0 2 0 0 0 0 0 0 0 0 3 2 1 0 2 3 2 3 0 0 0 5 1 7 5 2 0 1 2 3 2 2 0 4 1 1 1 1 3 2 0 5 2 1 2 2 1 1 0 0 1 4 3 0 0 5 3 1 0 2 6 1 5 3 0 1 1 0 0 1 0 0 2 0 1 1 3 0 1 0 0 0 0 0 5 10 9 7 13 10 6 11 6 6 12 8 10 1 8 8 8 3 4 18 3 4 11 3 9 5 11 12 6 2 4 2 5 3 3 5 5 7 10 9 17 7 15 5 9 8 5 6 10 9 4 7 1 6 13 6 7 10 4 23 17 10 6 10 4 10 7 6 3 7 15 11 1 7 6 2 4 21 18 20 16 11 12 24 15 13 20 14 14 6 14 15 13 12 11 22 12 10 24 8 20 10 14 14 14 7 11 9 5 13 9 7 7 19 23 13 17 7 16 10 14 13 8 11 14 15 9 20 10 12 24 15 12 17 6 24 17 11 8 12 12 10 15 12 13 9 17 11 1 7 14"
        }
    ],
    "affiliations": [
        "Amazon Research",
        "UC Santa Cruz"
    ]
}