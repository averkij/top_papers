{
    "paper_title": "ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling",
    "authors": [
        "Qisen Wang",
        "Yifan Zhao",
        "Peisen Shen",
        "Jialu Li",
        "Jia Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is a pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, a training-free method including World State Hyperspace to represent the spatiotemporal constraints of a 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3D-consistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models."
        },
        {
            "title": "Start",
            "content": "ChronosObserver: Taming 4D World with Hyperspace Diffusion Sampling Qisen Wang Yifan Zhao Peisen Shen Jialu Li Jia Li State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University {wangqisen, zhaoyf, psshen, ljl16, jiali}@buaa.edu.cn 5 2 0 2 ] . [ 1 1 8 4 1 0 . 2 1 5 2 : r Figure 1. ChronosObserver results of temporal-synchronized multi-view videos from one single monocular video. The project page is https://icvteam.github.io/ChronosObserver.html."
        },
        {
            "title": "Abstract",
            "content": "Although prevailing camera-controlled video generation models can produce cinematic results, lifting them directly to the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos remains challenging, which is pivotal capability for taming 4D worlds. Some works resort to data augmentation or test-time optimization, but these strategies are constrained by limited model generalization and scalability issues. To this end, we propose ChronosObserver, training-free method including World State Hyperspace to represent the spatiotemporal constraints of 4D world scene, and Hyperspace Guided Sampling to synchronize the diffusion sampling trajectories of Correspondence should be addressed to Yifan Zhao and Jia Li. Website: https://cvteam.buaa.edu.cn. multiple views using the hyperspace. Experimental results demonstrate that our method achieves high-fidelity and 3Dconsistent time-synchronized multi-view videos generation without training or fine-tuning for diffusion models. 1. Introduction Video generation [24, 43] has achieved remarkable progress, particularly with the development of cameracontrolled video generation [5, 10, 48] that can produce cinematic visual results of view-controlling over specified camera trajectories. However, critical challenge toward the 4D world that remains largely unaddressed is the generation of 3D-consistent and high-fidelity time-synchronized multi-view videos. As shown in Fig. 2, straightforward extension of the existing camera-controlled video genera1 Hyperspace, formulated as an incremental representation that models the unified spatiotemporal states of 4D scene to provide compatible constraints throughout generation. Furthermore, we propose Hyperspace Guided Sampling, which leverages spatiotemporal constraints within the hyperspace to control the diffusion generation process by integrating these constraints within the latent space during denoising. For any view and any time, our proposed method achieves 3D-consistent time-synchronized multiview videos generation. Overall, our contributions can be summarized as 1. We propose ChronosObserver, training-free method for generating time-synchronized multi-view videos. 2. We propose World State Hyperspace, an incremental representation that encapsulates the spatiotemporal constraints of 4D world scene. 3. We propose Hyperspace Guided Sampling for guiding the diffusion sampling using the hyperspace to achieve 3D-consistent and high-fidelity generation results. 2. Related Works Video Generation. Video generation models [24, 43] have advanced rapidly, with mainstream approaches including diffusion-based [13, 2931] models [12, 20] as well as recently emerging autoregressive models [9, 46]. These models demonstrate strong capacity for generating highfidelity videos with realistic spatiotemporal visual information. Leveraging the capabilities of foundation models [6, 34, 45], researchers have begun to explore controllable video generation, e.g., motion-controlled [8, 25] and camera-controlled generation [5, 10, 28, 40], leading to increasingly realistic and controllable video content. key insight from recent studies is that video generation models exhibit an emergent understanding of real-world spatial relationships [1, 7] when scaled sufficiently in both parameters and training data. This has motivated growing interest in camera-controlled generation techniques [10, 42, 48]. As result, supplementing and extrapolating 4D visual information through generative approaches has become challenging yet promising research direction. Camera-controlled Video Generation. While existing novel view synthesis methods [18, 26] excel for dense inputs, they often struggle with sparse or monocular inputs due to insufficient visual supervision. This persistent limitation is driving shift towards generative approaches, namely camera-controlled video generation models [3, 14, 23, 36, 37, 41, 47], which leverage diffusion priors to generate visual content at target poses from single image or video conditioned on camera trajectories. Existing methods for camera control follow two main paradigms. Some approaches [5, 10, 11] implicitly encode the target camera poses as network input, but face issues like limited control over pose scale and absence of explicit visual Figure 2. Motivation. Directly lifting camera-controlled video generation method [48] to generate multi-view videos leads to 3D inconsistencies across different viewpoints at the same timestamp. tion method [48] to multi-view synthesis frequently produces 3D inconsistencies. This issue is rooted in independent sampling trajectories across viewpoints, serving as the sparse conditioning constraints, which undermine weaklycoupled conditional distributions for different viewpoints, leading to the failure to enforce unified 4D scene. Although prevailing methods attempt to address this challenge using data augmentation [32, 39] or per-scene test-time optimization [17], these remain constrained by inherent defects, including limited model generalization due to insufficient multi-view data diversity and scalability issues arising from crucial computational requirements. The 3D inconsistencies shown in Fig. 2 underscore fundamental multi-view disjointedness in generation when naively extending existing methods. But critical insight emerges from the observation that these models demonstrate compelling capacity to produce geometrically consistent and high-fidelity content for each individual camera trajectory. This confirms that existing camera-controlled generation models encapsulate the inherent spatiotemporal coherence. Therefore, the central problem lies not in model ability but in achieving cross-view synchronization, as the current paradigm lacks mechanism to enforce coupling across the sampling trajectories of conditional distributions for different viewpoints. This situation shifts the core challenge toward determining how to structure the cross-view sampling trajectories to induce unified 4D scene. Under the constraint of single monocular video input, addressing this question necessitates the design of explicit spatiotemporal constraints that can synchronize the generative process across views, thereby merging independent sampling results into coherent 4D scene. To this end, we propose ChronosObserver, trainingfree method for the challenge of time-synchronized multiview videos generation. First, we propose World State 2 Figure 3. ChronosObserver Pipeline starts from the monocular input video and generates time-synchronized multi-view videos. ChronosObserver incrementally constructs the World State Hyperspace and utilizes it for the Hyperspace Guided Sampling. guidance. In contrast, ViewCrafter [49] first introduces point cloud representations to provide explicit pose guidance. Subsequent works [15, 17, 48] extract dynamic point clouds via monocular depth estimation and reproject them to target poses for reliable control. However, these methods typically operate on single dynamic camera trajectory to achieve cinematic effects. critical challenge lies in generating time-synchronized multi-view videos from sparse visual clues. While essential for taming the 4D world, this task represents key open problem for existing cameracontrolled video generation models. Time-synchronized Multi-view Video Generation. As previously motioned, time-synchronized multi-view videos generation [4, 21, 32, 33, 35, 39, 44] represents critical step toward constructing the 4D world. However, due to challenges in data acquisition, there remains severe scarcity of multi-view dynamic scene datasets. Thus, prevailing works [32, 39] attempt to augment data, like leveraging real-world multi-view static scene data, general video datasets, and synthetic 4D data for training. Despite these efforts, the fundamental issue of data scarcity persists and remains inadequately addressed. To this end, Reangle-A-Video [17] employs the pre-trained video generation model combined with test-time fine-tuning. While this strategy alleviates data dependency, it compromises high inference computational resources. In this work, we propose training-free framework to efficiently and effectively generate high-quality time-synchronized multi-view 4D videos without large-scale datasets for fine-tuning and test-time optimization for diffusion models. 3. Method Towards the challenge of 3D inconsistencies over viewpoints identified in Sec. 1, we propose ChronosObserver, training-free method that enforces unified 4D scene by structuring the diffusion sampling process with explicit spatiotemporal constraints. As shown in Fig. 3, we propose World State Hyperspace to incrementally maintain consistent scene representation and Hyperspace Guided Sampling to synchronize the generation process for different views. Formulation and Initialization. Given the input monocular video with frames = {I0, I1, ..., IT 1}, we first utilize Mega-SAM [22] to predict its depths = {D0, D1, ..., DT 1}, poses = {P0, P1, ..., PT 1}, and the intrinsic K. In this work, we consider two scenarios for the target poses = {Qm}m=1,...,M : internal-view, where the target poses are set to the internal poses of the monocular video; and external-view, where the target poses are set to the external poses specified by the user. Meanwhile, we construct the dynamic trajectory as Q0 = Q1M for providing an additional auxiliary reference. For the internalview scenario, Q0 corresponds to the input video and can be ignored. We aim to generate time-synchronized multiview 4D videos = {I Qm} from the given target poses Q. World State Hyperspace. Existing camera-controlled video diffusion models can individually generate the video of the target pose Qm from the source video and its depths D, poses P, with the guidance of the view-warped visual results of the input video. Specifically, we have the 3 conditional distribution as pθ(I Qm Qm Qm ;r , MQm ;r , Qm), (1) where condition is omitted for simplification, and the ;r , MQm same applies below. Qm ;r are obtained from the view-warping of each frame with the warping function Ψ and the projection function ψ, which can be represented as ;r i, MQm Qm = ψ(ψ1(Ii, Di, K, Pi), K, Qm). ;r = Ψ(I, D, K, P, Qm, i), (2) Figure 4. Intuitive Illustration of ChronosObserver compared to camera-controlled video generation methods. Here, we denote the unprojected points can be represented as = ψ1(Ii, Di, K, Pi)i=0,...,T 1 (3) from the source video as the world state, which serves as the constraint of guiding the trajectory of diffusion sampling. So, the conditional distribution can also be represented as pθ(I Qm Qm P, Qm), (4) where Qm is conditioned by the source video and the world state with the target pose. The generation inverts the diffusion process in the latent space, starting from noise z1 (0, I) and iteratively denoising it to obtain z0, which is decoded into Qm . The denoising step from noisy latent zt to zs (1 > 0) can be represented as zts = Scheduler(zt, ˆϵt = ϵθ(zt, xQm ;r , t), t, s), (5) ;r is the encoded condition of Qm where xQm is the parameterized noise predictor. ;r , MQm ;r , and ϵθ However, we can notice that the produced videos from the existing camera-controlled video diffusion models lack the 3D constraints across different views. Therefore, we propose the concept of the world state hyperspace ζ, which includes the existing constraints for controlling the diffuX , Pα sion sampling. Specifically, the initialized space {Pα Ω} includes the dynamic points Pα and the static points Pα Ω of the Truncated Signed Distance Function (TSDF) [50] from the input and the additional auxiliary reference Q0 with only using Pα , which are denoted as the Base State Hyperspace ζ α = {Pα , Pα Ω}. Similar to the idea of mathematical induction, we temporarily do not consider how to efficiently supplement and use the world states. Instead, we start from the base state hyperspace with 3D consistency, and aim to supplement the generated 3D-consistent Incremental State Hyperspace ζ β , where represents the current status. We assume that we have already generated the video Qm with bullet-time 3D consistency of the target pose Qm and supplement its corresponding state Pβ to the incremental state hyperm = ζ β space ζ β }, which follows Eq. (3) and Qm m1 {Pβ Qm Pβ Qm = ψ1(I Qmi, DQmi, K, Qm)i=0,...,T 1, DQm = PDA(I Qm, DPQm(D, P, Qm)), (6) where DQm is predicted by Prior Depth Anything (PDA) [38] using the generated video Qm and the warped depth DPQm from the depth of the input video. Note that we skip supplementing the states where there are no valid pixels of warped depth from the input frame. The supplemented states are denoted as the incremental state hyperspace. Note that ζ β } with related to Q0 . To this end, we can notice that if we can find way to constrain the sampling trajectory of the camera-controlled video diffusion pθ using the world state hyperspace to generate the incremental video of the target pose Qm, we can generate the time-synchronized multiview 4D videos = {I Qm } which are what we want. 0 is initialized by the generated {Pβ Hyperspace Guided Sampling. As demonstrated above, with the world state hyperspace and the target pose Qm, we need to achieve 3D-consistent sampling on the target pose Qm through guiding the sampling trajectory of the video diffusion model using the state space. Specifically, we can obtain the projected rendered frames and masks for each state in current ζm1 = ζ α ζ β m1 following Eq. (2), which can be represented as ;r = ψ(P, K, Qm). ;r , MQm Qm ω;r , MQm For ease of description, we denote Qm ω;r for base static state, Qm χ;r , MQm for base dynamic state, and χ;r Qm j;r , MQm j;r for j-th incremental dynamic state. Then, we compute the existing dynamic representation by taking the union of the dynamic states, which can be represented as (7) MQm µ; = MQm χ;r ( 1 (cid:95) j=0 MQm j;r ), µ; = MQm Qm χ;r Qm χ;r + 1 (cid:88) (MQm µ;j MQm j;r )I Qm j;r , (8) j=0 µ;m, MQm and Qm µ;m are the current accumulated dynamic representation. We further integrate the dynamic representaTable 1. Quantitative comparison results. Methods ViewCrafter [49] Reangle-A-Video [17] EX-4D [15] TrajectoryCrafter [48] ChronosObserver (Ours) 3D Consistency MEt3R 0.3086 0.2342 0.2514 0.1930 0.1635 VBench Subject Background Motion Temporal Aesthetic Imaging 0.9038 0.8408 0.8267 0.9238 0.9365 0.5909 0.5595 0.5546 0.6149 0. 0.9829 0.9583 0.9769 0.9773 0.9834 0.9792 0.9788 0.9881 0.9886 0.9918 0.9403 0.9070 0.8935 0.9475 0.9582 0.4907 0.5303 0.4873 0.5660 0.5598 tion into the static representation to aid subsequent diffusion sampling, which can be written as (9) ω;r )I Qm ω;r . ω,µ;r, xQm µ;m MQm χ;r , and xQm Mω,µ;r = MQm ω,µ;r = MQm Qm µ;m MQm ω;r , µ;mI Qm µ;m + (MQm After encoding the conditions rendered by the above states, we obtain the state representations in the latent space, ω,µ;r, MQm which are denoted by xQm ω,µ;r for Qm χ;r for j;r , MQm j;r for Qm χ;r , MQm Qm j;r . Besides, we conχ;r , wQm struct latent weights wQm j;r through sequential mask concatenation, and employ wQm ω,µ;r to handle residual uncovered areas, which ensures complete spatial coverage while maintaining normalized weight distribution across all states. Given the pre-trained noise predictor of the cameracontrolled video generation model ϵθ(), we fuse the prediction results of state representations at each step of the denoising loop to perform multi-state representation constraint sampling, which can be represented as ω,µ;rϵθ(zt, xQm ω,µ;r, t) + wQm χ;r ϵθ(zt, xQm χ;r , t) ˆϵt = wQm m1 (cid:88) + wQm j;r ϵθ(zt, xQm j;r , t). (10) j=0 We can further get the denoised output zts similar to the standard diffusion generation process shown in Eq. (5). Intuitive Illustration. We further elaborate on the effectiveness of the proposed world state constraints sampling from the perspective of the conditional distribution of the video diffusion model. As shown in the left part of Fig. 4, the previous camera-controlled video generation models only utilize one single state Pα from the input monocular video, for guiding the diffusion sampling, which can be represented as (11) , Qm), Pα Qm pθ(I QmPα ζ α. Since the projection results obtained by state Pα at different target poses are different, and the sampling processes are independent of each other, the condition distributions corresponding to different target poses are unsynchronized, which in turn leads to 3D inconsistency in the generated time-synchronized multi-view videos. Conversely, our proposed method constructs the world state hyperspace as described above, and then fuses the conditional distributions corresponding to these world states during the video generation process for each target pose, thereby obtaining the sampling results constrained by the hyperspace, as shown in the right part of Fig. 4. Furthermore, due to the self-iterative property of the hyperspace, our time-synchronized multiview video sampling process is autoregressive, namely Qm pθ(I Qm ζ α, ζ β = pθ(I Qm ζ α, ζ β m1, Qm) m1 = ζ β m2 {Pβ Qm1 (12) }, Qm), where ζ β m2 is related to Q0, ..., Qm2 , and Pβ is related to Qm1 . Since we construct the incremental state hyperspace corresponding to the identical 4D scene, and utilize the hyperspace to jointly constrain the sampling trajectory of video generation, we can achieve 3D-consistent time-synchronized multi-view 4D videos. Qm1 4. Experiments 4.1. Experimental Details Dataset. We evaluate our method on our collected 30 monocular videos, which include 20 monocular videos in the wild from DAVIS [27], and 10 monocular videos generated by the Kling video generation model [19]. Each video contains 49 frames with resolution of 384 672. For original videos that are too long, we will truncate and extract frames. For the monocular videos with wide camera trajectories filmed in the wild, we utilize the fixed internal perspective of the videos for inference, which are called Interval-View Scenes. For the monocular videos with relatively static camera trajectories generated by the video generation model, we provide the fixed external perspectives for inference, which are called External-View Scenes. Besides, top-10 cases are extracted through the MEt3R scores of TrajectoryCrafter [48], which are called Hard Scenes. See more dataset details in the Appendix. Implementation. We utilize TrajectoryCrafter [48] as the video generation model of our method. The diffusion sampling step is set to 30, and the Classifier-Free Guidance (CFG) scale is set to 6.0, which is consistent to TrajectoryCrafter. For both Interval-View and External-View Scenes, we extract five perspectives of [0, 12, 24, 36, 48] as our target poses. The code is based on PyTorch and CUDA 5 Figure 5. Qualitative comparisons with other methods (TrajectoryCrafter [48], EX-4D [15], Reangle-A-Video [17], ViewCrafter [49]). 12.1. Experiments are conducted on 48GB 4090. The computational time cost is around 10 minutes per target view. See more implementation details in the Appendix. Metrics. We utilize VBench [16] to evaluate the qualincluding Subject Consistency, ity of generated videos, Background Consistency, Motion Smoothness, Temporal Flickering, Aesthetic Quality, and Imaging Quality. Furthermore, we utilize the multi-view 3D consistency metric MEt3R [2] to evaluate the 3D consistency of generated videos on different sequential pair-wise target views at the same timestamp. See more details of metrics in the Appendix. Baselines. We adopt camera-controlled video generation models and multi-view videos generation models with point cloud rendering input as baselines for fair comparisons, i.e., ViewCrafter [49], Reangle-A-Video [17], EX-4D [15], and TrajectoryCrafter [48], most of which are recently released state-of-the-art methods. See more details for the reproduction of baselines in the Appendix. 4.2. Comparisons with Other Methods Quantitative comparisons with other methods. The quantitative experimental results compared to other methods are shown in Tab. 1. We can see that our method achieves significant improvements in 3D consistency, i.e., MEt3R [2], while also outperforming other methods in most metrics of video generation quality, i.e., VBench [16]. Specifically, our proposed method achieves 15.28% improvement of 3D consistency compared to the state-of-theart method TrajectoryCrafter [48]. Meanwhile, our method achieves 11.38% and 13.17% improvements of subject consistency and imaging quality, respectively, compared to the prevailing time-synchronized multi-view videos generation method Reangle-A-Video [17]. Furthermore, we also evaluate the performance of all methods under different scenesplits, i.e., hard scenes, internal-view scenes, and externalview scenes, which are reported in the Appendix. Qualitative comparisons with other methods. Qualitative results are visualized in Fig. 5, showing selected views [0, 24, 48] and timestamps [0, 24, 48] for each scene in 3 3 grid for each method. Horizontal inspection reveals temporal variations within single view, while vertical alignment assesses 3D consistency across views at each timestamp. The state-of-the-art method TrajectoryCrafter [48] exhibits dynamic content mismatches and 3D inconsistencies across views, attributed to its lack of world state hyperspace guidance during sampling, as discussed in Secs. 1 and 3. Other methods [15, 17] fail to generate plausible content due to fixed mask-gradually-variation patterns or unsuitability for dynamic scenarios [49]. In contrast, our method constructs an incremental world state hyperspace and employs hyperspace-guided sampling, enabling temporally synchronized multi-view video generation with high visual fidelity and robust 3D consistency. Additional qualitative results with extended views and timestamps are provided in the Appendix. 6 Table 2. Quantitative ablation results. I.S.H. means Incremental State Hyperspace. H.G.S. means Hyperspace Guided Sampling. Settings Ours Ours w/o I.S.H. Ours w/o H.G.S. Ours w/o I.S.H., w/o H.G.S. All Scenes Hard Scenes Internal-View Scenes External-View Scenes MEt3R 0.1635 (+0.00%) 0.2611 (+0.00%) 0.1818 0.1828 0.2641 0.1748 0.1870 (-14.37%) 0.3269 (-25.20%) 0.2187 0.2136 (-30.64%) 0.3421 (-31.02%) 0.2291 (-1.15%) (-6.91%) (+0.00%) (-0.55%) (-20.30%) (-26.02%) 0.1268 0.1589 0.1237 0.1826 (+0.00%) (-25.32%) (+2.44%) (-44.01%) Figure 6. Relationship between timestamp and MEt3R. Our method demonstrates higher 3D consistency across timestamps compared to other methods. Figure 7. Relationship between missing visual information ratio and MEt3R. Our method is more effective than other methods in scenarios where more visual information is missing. 4.3. Performance Analysis Does ChronosObserver maintain 3D consistency at every timestamp? We provide comprehensive evaluation of 3D consistency across timestamps as shown in Fig. 6, where 3D consistency is measured by the MEt3R [2] (lower is better) and averaged by different scenes and views. The results demonstrate that our method (red line) consistently maintains more stable consistencies over the entire timestamps compared to the prevailing methods. The MEt3R curve of our proposed method remains persistently lower and exhibits less fluctuation over time than the trend of the other models. The consistent outperformance demonstrates that our method effectively preserves 3D consistency across timestamps. This robustness highlights the state-of-the-art capability of our method in generating time-synchronized multi-view videos, while prevailing methods exhibit significant fluctuations and 3D inconsistencies across timestamps. Can ChronosObserver handle large areas of missing visual information? We visualize the missing visual information ratio and MEt3R [2] (lower is better) for each scene as shown in Fig. 7, where the ratio is computed using the rendering mask of TrajectoryCrafter [48], which can indicate the difficulty of generating visual content for different scenarios in terms of co-visibility between viewpoints. Our method exhibits lower overall trend of MEt3R across various scenarios, i.e., better 3D consistency. Furthermore, the superiority of our proposed method becomes substantially more pronounced as the missing ratio increases. Specifically, the MEt3R trendlines of most prevailing methods exhibit sharper positive slope than ours at high missing ratios, indicating more rapid degradation of other methods in 3D consistency compared to our method. The significant performance gap under conditions of severe information deficiency demonstrates the robustness of our method to variFigure 8. Relationship between viewpoints and MEt3R on external-view scenes. Our method achieves better 3D consistency and stability across extrapolated viewpoints. ous scenarios and highlights its effectiveness in generating 3D-consistent visual content, which is key challenge that existing methods struggle with. Exploration of ChronosObserver on the extrapolation ability. We visualize the pair-wise viewpoints and MEt3R [2] (lower is better) on the external-view scenes as shown in Fig. 8. We can notice that our method achieves an overall better 3D consistency across pair-wise viewpoints compared to other methods. Furthermore, as the degree of extrapolation increases, i.e., the view number increases, although some methods [15, 48] show positive correlation with 3D inconsistency, our method demonstrates better stability and robustness in generating 3D-consistent visual content for extrapolation scenarios. Ablations of ChronosObserver. Detailed ablation studies are conducted to evaluate the contributions of Incremental State Hyperspace (I.S.H.) and Hyperspace Guided Sampling (H.G.S.) across different scene splits, i.e., hard scenes, internal-view scenes, and external-view scenes. The quantitative results of ablations are presented in Tab. 2. The first row demonstrates the performance of our full model 7 Table 3. Exploration on static state, which provides steady improvements in generation quality. Settings Ours w/ Static State Ours w/o Static State 3D Consistency MEt3R 0.1635 0.1787 VBench Subject Background Motion Temporal Aesthetic 0.9918 0.9365 0.9885 0.9235 0.9582 0.9482 0.9834 0.9770 0.5598 0. Imaging 0.6332 0.6144 Table 4. Comparisons on computational time. Methods Reangle-A-Video [17] ChronosObserver (Ours) 3D Consistency Time/View MEt3R 0.2342 0. Hours 0.6756 0.1676 Figure 9. Qualitative comparisons of the ablations. I.S.H. means Incremental State Hyperspace. H.G.S. means Hyperspace Guided Sampling. (Ours). Rows 23 validate the individual effectiveness of both I.S.H. and H.G.S., as removing either component results in measurable performance drop. When both components are ablated simultaneously (last row), substantial performance decline of 30.64% is observed on All Scenes. This cumulative degradation indicates that I.S.H. and H.G.S. offer complementary strengths, and their synergistic integration is critical for ensuring robust 3D consistency in multi-view video generation. The qualitative results of ablations are presented in Fig. 9. We can observe that I.S.H. plays significant role in promoting 3D consistency across different views at the same timestamp and in extrapolating visual content. Furthermore, H.G.S. promotes the guidance of states in the hyperspace for the diffusion sampling process, while removing H.G.S. significantly degrades the visual content generation. Moreover, removing both components simultaneously results in an even more severe degradation in generation quality. Exploration of Static State. Quantitative results of an ablation study investigating the role of base static state Pα Ω within our pipeline are shown in Tab. 3, which indicates that the incorporation of static state yields an overall and steady improvement of generation quality. Qualitative results are provided in Fig. 10. As demonstrated in Sec. 3, the base static state of the dynamic scene is important for generating temporally coherent backgrounds, effectively mitigating structural ambiguities. As shown in the bottom-right corner of the scene presented in Fig. 10, the regions affected by the static state exhibit better static-region visual structural stability compared to the ablated variant. Computational Time Cost. The computational time Figure 10. Exploration of static state, which shows the effect of improving the stability of static background generation. cost compared with the state-of-the-art multi-view video generation method Reangle-A-Video [17] is shown in Tab. 4. We randomly selected one scene each from internalview scenes and external-view scenes for testing. Note that Reangle-A-Video [17] enables gradient checkpointing due to the limitation of VRAM, and the total time of Reangle-AVideo [17] is supplemented by the time of MegaSAM [22] (although relatively short). Then, we divided the total time by the number of target views and reported it as Time/View as shown in Tab. 4, where we further supplement MEt3R for reference. We can notice that, although our method is training-free, it not only requires less time than the stateof-the-art method of time-synchronized multi-view videos generation, but also achieves better generation results. 5. Conclusion We propose ChronosObserver, training-free method for time-synchronized multi-view videos generation, which includes World State Hyperspace for incrementally modeling the spatiotemporal constraints of 4D world scenario, and Hyperspace Guided Sampling for utilizing the constraints within the hyperspace to control the sampling trajectories of the pre-trained camera-controlled video diffusion model, thereby achieving 3D-consistent and high-fidelity results. The limitation of our method is its dependence on the accuracy of monocular depth estimation and depth completion, challenge it shares with other methods that rely on monocular depth estimates to achieve frame warping constraints for guiding video diffusion sampling."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 2 [2] Mohammad Asim, Christopher Wewer, Thomas Wimmer, Bernt Schiele, and Jan Eric Lenssen. Met3r: Measuring multi-view consistency in generated images. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60346044, 2025. 6, 7, 12, 13 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Jiaxu Zou, Andrea Hsin-Ying Lee, Chaoyang Wang, Tagliasacchi, et al. Vd3d: Taming large video diffusion In The Thirteenth Intransformers for 3d camera control. ternational Conference on Learning Representations. 2 [4] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di ZHANG. Syncammaster: Synchronizing multi-camera video generation In The Thirteenth International from diverse viewpoints. Conference on Learning Representations. 3 [5] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647, 2025. 1, 2 [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. [8] Chenjie Cao, Jingkai Zhou, Shikai Li, Jingyun Liang, Chaohui Yu, Fan Wang, Xiangyang Xue, and Yanwei Fu. Uni3c: Unifying precisely 3d-enhanced camera and human motion controls for video generation. arXiv preprint arXiv:2504.14899, 2025. 2 [9] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. In The Thirteenth International Conference on Learning Representations. 2 [10] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: EnIn The abling camera control for video diffusion models. Thirteenth International Conference on Learning Representations, 2025. 1, 2 [11] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and Hongsheng Li. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. 2 [12] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 2 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, pages 68406851, 2020. [14] Chen Hou and Zhibo Chen. Training-free camera control for In The Thirteenth International Confervideo generation. ence on Learning Representations. 2 [15] Tao Hu, Haoyang Peng, Xiao Liu, and Yuewen Ma. Ex-4d: Extreme viewpoint 4d video synthesis via depth watertight mesh. arXiv preprint arXiv:2506.05554, 2025. 3, 5, 6, 7, 12, 13, 14 [16] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 6, 13 [17] Hyeonho Jeong, Suhyeon Lee, and Jong Chul Ye. Reanglea-video: 4d video generation as video-to-video translation. arXiv preprint arXiv:2503.09151, 2025. 2, 3, 5, 6, 8, 12, 13, 14 [18] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4):114, 2023. 2 [19] kling. kling, 2024. 5, 12 [20] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [21] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. Advances in Neural Information Processing Systems, 37:1624016271, 2024. 3 [22] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1048610496, 2025. 3, 8, 13 [23] Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 20162029, 2025. 2 [24] Yue Ma, Kunyu Feng, Zhongyuan Hu, Xinyu Wang, Yucheng Wang, Mingzhe Zheng, Xuanhua He, Chenyang Zhu, Hongyu Liu, Yingqing He, et al. Controllable video arXiv preprint arXiv:2507.16869, generation: survey. 2025. 1, 2 [25] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. 2 [26] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [27] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Computer Vision and Pattern Recognition, 2016. 5, 12 [28] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera conIn Proceedings of the Computer Vision and Pattern trol. Recognition Conference, pages 61216132, 2025. 2 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 2 [30] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. pmlr, 2015. [31] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. [32] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhu, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with deIn Proceedings of the IEEE/CVF coupled video diffusion. International Conference on Computer Vision, pages 13695 13706, 2025. 2, 3 [33] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: ExIn Eutreme monocular dynamic novel view synthesis. ropean Conference on Computer Vision, pages 313331. Springer, 2024. 3 [34] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2 [35] Chaoyang Wang, Peiye Zhuang, Tuan Duc Ngo, Willi Menapace, Aliaksandr Siarohin, Michael Vasilkovsky, Ivan Skorokhodov, Sergey Tulyakov, Peter Wonka, and Hsin-Ying Lee. 4real-video: Learning generalizable photo-realistic 4d video diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1772317732, 2025. 3 [36] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. [37] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2 [38] Zehan Wang, Siyu Chen, Lihe Yang, Jialei Wang, Ziang Zhang, Hengshuang Zhao, and Zhou Zhao. Depth anything with any prior. arXiv preprint arXiv:2505.10565, 2025. 4, 13 [39] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion modIn Proceedings of the Computer Vision and Pattern els. Recognition Conference, pages 2605726068, 2025. 2, 3 [40] Tong Wu, Shuai Yang, Ryan Po, Yinghao Xu, Ziwei Liu, Dahua Lin, and Gordon Wetzstein. Video world models with long-term spatial memory. arXiv preprint arXiv:2506.05284, 2025. 2 [41] FU Xiao, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multientity motion in video generation. In The Thirteenth International Conference on Learning Representations, 2024. 2 [42] Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Trajectory attention for fine-grained video motion control. In The Thirteenth International Conference on Learning Representations. 2 [43] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 57(2):142, 2024. 1, 2 [44] Dejia Xu, Yifan Jiang, Chen Huang, Liangchen Song, Thorsten Gernoth, Liangliang Cao, Zhangyang Wang, and Hao Tang. Cavia: Camera-controllable multi-view video diffusion with view-integrated attention. In Forty-second International Conference on Machine Learning. [45] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations. 2 [46] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion modIn Proceedings of the Computer Vision and Pattern els. Recognition Conference, pages 2296322974, 2025. 2 [47] Meng You, Zhiyu Zhu, Hui Liu, and Junhui Hou. Nvs-solver: Video diffusion model as zero-shot novel view synthesizer. arXiv preprint arXiv:2405.15364, 2024. 2 [48] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocarXiv preprint ular videos via diffusion models. arXiv:2503.05638, 2025. 1, 2, 3, 5, 6, 7, 12, 13, 14 [49] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, 10 and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3, 5, 6, 12, 13, [50] Andy Zeng, Shuran Song, Matthias Nießner, Matthew Fisher, Jianxiong Xiao, and Thomas Funkhouser. 3dmatch: Learning local geometric descriptors from rgb-d reconstructions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 18021811, 2017. 4, 13 11 A. Additional Experimental Results A.1. Video and Website We provide additional experimental results in the https: / / icvteam . github . io / ChronosObserver . html. We highly recommend watching them. A.2. More Quantitative Results More quantitative results compared to other methods. We evaluate the performance of different methods under different scene-splits, i.e., hard scenes, internal-view scenes, and external-view scenes, as shown in the Tab. 5. It can be observed that in most scenarios, our method achieves the best or second-best video generation quality. More importantly, our approach consistently attains the highest level of 3D consistency across different settings, significantly outperforming other approaches. This demonstrates the robustness and effectiveness of our proposed method. More quantitative analysis of ablations. Similar to the 3D consistency analysis conducted in the main manuscript across the temporal dimension, we additionally provide an analysis of 3D consistency at different timestamps for the ablations as shown in Fig. 11. It can be observed that ablating either I.S.H. (Incremental State Hyperspace) or H.G.S. (Hyperspace Guided Sampling) leads to an apparent degradation in overall 3D consistency across timestamps, while the joint ablation of both components results in further deterioration of 3D consistency across timestamps. This further demonstrates that the enhancement in 3D consistency provided by our proposed method is manifested throughout the entire generated video sequence, thereby underscoring the effectiveness of our proposed method. A.3. More Qualitative Results More qualitative results compared to other methods. We provide more qualitative results compared to other methods shown in Fig. 12 for the internal-view scenes and Fig. 13 for the external-view scenes. More qualitative results of comparisons with other methods are provided in the demo video and the local static website of the Supplementary Materials. Similar to the main manuscript, qualitative results are visualized showing selected views [0, 12, 24, 36, 48] and timestamps [0, 12, 24, 36, 48] for each scene in 5 5 grid for each method. Horizontal inspection reveals temporal variations within single view, while vertical alignment assesses 3D consistency across views at each timestamp. The state-of-the-art method TrajectoryCrafter [48] exhibits noticeable dynamic content mismatches and 3D inconsistencies across different viewpoints. These limitations stem primarily from the absence of world state hyperspace guidance during the sampling process, as analyzed in the main manuscript. Without such guidance, the model fails to maintain coherent world scene representation over time, leading to visible artifacts and geometric misalignments across different viewpoints. Other baseline approaches, including Reangle-A-Video [17] and EX-4D [15], also struggle to produce convincing multiview dynamic content. They rely on fixed mask-gradualvariation patterns that lack adaptability to complex scene dynamics. while ViewCrafter [49] is fundamentally designed for static scenes, making it unsuitable for generating temporally coherent video content in dynamic setIn contrast, our method introduces an incremental tings. world state hyperspace construction process, coupled with hyperspace guided sampling, which ensures that the generated time-synchronized multi-view videos remain consistent across all views at each timestamp, thereby achieving time-synchronized multi-view videos generation with high visual fidelity and significantly improved 3D consistency compared to existing methods. More qualitative results of our method. We provide the visualization of the generated time-synchronized multi-view videos from our proposed ChronosObserver in Fig. 14 and Fig. 15. More qualitative results of the generated time-synchronized multi-view videos from our proposed ChronosObserver are provided in the demo video and the local static website of the Supplementary Materials. B. Additional Details B.1. Dataset Details We evaluate our method on collected dataset of 30 monocular video sequences, comprising 20 real-world videos from DAVIS [27] and 10 generated videos by the Kling model [19]. Each video is preprocessed to contain 49 frames at resolution of 384 672, with longer original videos being truncated and frames extracted accordingly. The dataset is fundamentally divided into two categories Interval-View based on camera motion characteristics. Scenes refer to real-world videos from DAVIS [27] with relatively wide camera trajectories, for which inference is performed using the fixed internal perspective of the original viewpoints. External-View Scenes consist of generated videos from Kling [19] with relatively static camera trajectories, where extrapolated external perspectives are provided for inference. Note that we directly utilize the generated videos from the Kling [19] website. Additionally, to evaluate performance under challenging conditions, subset termed Hard Scenes is split by selecting the top10 cases ranked by the MEt3R [2] scores obtained from the state-of-the-art method TrajectoryCrafter [48]. B.2. Implementation Details Our implementation utilizes TrajectoryCrafter [48] served as the foundational camera-controlled video generation model. The diffusion sampling process is configured with Table 5. More Detailed quantitative comparison results. Metrics 3D Consistency MEt3R VBench Subject Background Motion Temporal Aesthetic Imaging All Scenes ViewCrafter [49] Reangle-A-Video [17] EX-4D [15] TrajectoryCrafter [48] ChronosObserver (Ours) ViewCrafter [49] Reangle-A-Video [17] EX-4D [15] TrajectoryCrafter [48] ChronosObserver (Ours) ViewCrafter [49] Reangle-A-Video [17] EX-4D [15] TrajectoryCrafter [48] ChronosObserver (Ours) ViewCrafter [49] Reangle-A-Video [17] EX-4D [15] TrajectoryCrafter [48] ChronosObserver (Ours) 0.3086 0.2342 0.2514 0.1930 0.1635 0.4455 0.3511 0.4011 0.3171 0.2611 0.3117 0.2623 0.2760 0.2096 0. 0.3025 0.1780 0.2023 0.1597 0.1268 0.9038 0.8408 0.8267 0.9238 0.9365 0.8371 0.6655 0.6842 0.8557 0.8964 0.9403 0.9070 0.8935 0.9475 0.9582 Hard Scenes 0.9113 0.8251 0.8399 0.9145 0.9470 Internal-View Scenes 0.9266 0.8778 0.8704 0.9317 0. 0.8871 0.7925 0.7875 0.9076 0.9274 External-View Scenes 0.9373 0.9375 0.9052 0.9562 0.9547 0.9676 0.9653 0.9397 0.9791 0.9772 0.9792 0.9788 0.9881 0.9886 0.9918 0.9668 0.9541 0.9823 0.9791 0.9891 0.9736 0.9728 0.9876 0.9868 0.9919 0.9904 0.9907 0.9891 0.9921 0. 0.9829 0.9583 0.9769 0.9773 0.9834 0.9726 0.9136 0.9653 0.9605 0.9800 0.9784 0.9454 0.9735 0.9725 0.9823 0.9918 0.9841 0.9838 0.9868 0.9856 0.4907 0.5303 0.4873 0.5660 0.5598 0.4528 0.5133 0.4388 0.5441 0. 0.4642 0.5076 0.4326 0.5194 0.5060 0.5436 0.5758 0.5966 0.6591 0.6674 0.5909 0.5595 0.5546 0.6149 0.6332 0.5320 0.4733 0.4392 0.5133 0.5606 0.5518 0.5107 0.4995 0.5850 0.6113 0.6692 0.6570 0.6647 0.6747 0. 30 denoising steps and the CFG scale of 6.0. For both Interval-View and External-View scenes, we uniformly sample five target viewpoints corresponding to indices [0, 12, 24, 36, 48] from the camera trajectory with 49 viewpoints to evaluate multi-view 3D consistency. Our method also follows the order of inferring based on the indices of viewpoints. Since there are currently few depth completion tools for videos, we utilize recently released image depth completion tool called Prior Depth Anything (PDA) [38] in conjunction with flow-based inter-frame smoothing to implement the incremental construction of world states based on the generated videos. The pattern of PDA is set to 500. Besides, the initial incremental state related to the auxiliary reference Q0 is obtained through the re-running of MegaSAM [22], while other incremental states utilize the PDA [38]. TSDF [50] is implemented using Open3D. The framework is implemented in PyTorch and runs on CUDA 12.1, with experiments conducted on an NVIDIA RTX 4090 GPU with 48GB of VRAM. B.3. Experimental Details We provide additional details regarding the experimental setup for the ablations. When performing H.G.S. ablation, we directly project the states in the hyperspace independently to obtain the corresponding rendered video, which is then used as the condition for sampling. The feature fusion weights are calculated as scalars based on mask ratios. Furthermore, for the ablation of the Static State, we supplement the weights of the denoised output of the base dynamic state due to the elimination of the base static state, for the residual uncovered areas. B.4. Metrics Details VBench. We utilize VBench [16] to evaluate the quality of generated videos, including Subject Consistency, Background Consistency, Motion Smoothness, Temporal Flickering, Aesthetic Quality, and Imaging Quality. Dynamic Degree is excluded because this metric is less effective at reflecting the quality of video generation for multi-view videos generation. Since we select 5 viewpoints, each scenario includes 5 evaluation results of VBench [16]. We calculate the average of these metrics for each scenario, and then average them across different scenarios to obtain the final evaluation result. MEt3R. We utilize MEt3R [2] to evaluate the 3D consistency of generated videos from different viewpoints across timestamps. Specifically, for each scenario, we compute the MEt3R metric between every consecutive pair of view13 mains consistent with the original first frame. We deactivate this manual operation due to the task settings for multi-view videos generation. TrajectoryCrafter. We set the CFG of TrajectoryCrafter [48] to 6.0 and the sampling steps to 30, which is the same as our method. Furthermore, we make additional improvements to its pose construction and implement PyTorch-based image warping, all within the same settings as our method to ensure fair comparison. Figure 11. Relationship between timestamp and MEt3R for ablations. points, i.e., 012, 1224, 2436, and 3648, at each timestamp. The MEt3R result for given scenario is then obtained by averaging these values across both the temporal and viewpoint dimensions. So, it also enables additional performance analysis along the temporal and viewpoint dimensions independently, as demonstrated in the main manuscript. The overall MEt3R evaluation result is derived by averaging the results across different scenarios. B.5. Reproduction of Other Methods In this section, we provide reproduction details of different methods. In summary, we make necessary adjustments during the reproduction process, such as aligning the coordinates of poses. Furthermore, the specific details of the different methods are described below. ViewCrafter. Since ViewCrafter [49] supports the resolution of 576 1024 with 25 frames, we first resize the rendered image to the corresponding resolution and perform frame extraction after warping. After obtaining the generated result, we then resize it to the original resolution and pad it to 49 frames for subsequent evaluation. Note that although ViewCrafter [49] forces the first frame of the rendered result to be the original image, namely, the first pose of the camera trajectory remains consistent with the original frame, we deactivate this manual operation due to the task settings for multi-view videos generation. Reangle-A-Video. Since Reangle-A-Video [17] supports the resolution of 480 720, and the running process will automatically resize images to the corresponding resolution, we resize it to the original resolution after obtaining the generated result for subsequent evaluation. Note that since Reangle-A-Video [17] does not release the code of static-view transport generation, we make necessary adjustments to its existing dynamic camera control generation code to ensure it runs successfully. EX-4D. Similar to ViewCrafter [49], EX-4D [15] also forces the first frame of the rendered result to be the original image from the input monocular video in its running process, namely, the first pose of the camera trajectory reFigure 12. Qualitative comparisons with other methods of the internal-view scenes. 15 Figure 13. Qualitative comparisons with other methods of the external-view scenes. 16 Figure 14. Generated time-synchrosized multi-view videos from our method. Figure 15. Generated time-synchrosized multi-view videos from our method."
        }
    ],
    "affiliations": [
        "State Key Laboratory of Virtual Reality Technology and Systems, SCSE, Beihang University"
    ]
}