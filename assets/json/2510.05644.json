{
    "paper_title": "The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP",
    "authors": [
        "Sheriff Issaka",
        "Keyi Wang",
        "Yinka Ajibola",
        "Oluwatumininu Samuel-Ipaye",
        "Zhaoyi Zhang",
        "Nicte Aguillon Jimenez",
        "Evans Kofi Agyei",
        "Abraham Lin",
        "Rohan Ramachandran",
        "Sadick Abdul Mumin",
        "Faith Nchifor",
        "Mohammed Shuraim",
        "Lieqi Liu",
        "Erick Rosas Gonzalez",
        "Sylvester Kpei",
        "Jemimah Osei",
        "Carlene Ajeneza",
        "Persis Boateng",
        "Prisca Adwoa Dufie Yeboah",
        "Saadia Gabriel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite representing nearly one-third of the world's languages, African languages remain critically underserved by modern NLP technologies, with 88\\% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), a comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) a quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) a structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development."
        },
        {
            "title": "Start",
            "content": "The African Languages Lab: Collaborative Approach to Advancing Low-Resource African NLP Sheriff Issaka1 Keyi Wang2 Yinka Ajibola3 Oluwatumininu Samuel-Ipaye3 Zhaoyi Zhang3 Nicte Aguillon Jimenez3 Evans Kofi Agyei4 Abraham Lin5 Rohan Ramachandran3 Sadick Abdul Mumin7 Faith Nchifor3 Mohammed Shuraim6 Lieqi Liu1 Erick Rosas Gonzalez Carlene Ajeneza3 Persis Boateng9 Sylvester Kpei8 Prisca Adwoa Dufie Yeboah10 Jemimah Osei8 Saadia Gabriel1 5 2 0 O 7 ] . [ 1 4 4 6 5 0 . 0 1 5 2 : r 1University of California, Los Angeles 5Carleton University 8Cornell University 3University of Wisconsin - Madison 6Stetson University 9Soka University of America sheriff@cs.ucla.edu 2Georgia Institute of Technology 4University of Cape Coast 7Northwestern University in Qatar 10Columbia University"
        },
        {
            "title": "Abstract",
            "content": "Despite representing nearly one-third of the worlds languages, African languages remain critically underserved by modern NLP technologies, with 88% classified as severely underrepresented or completely ignored in computational linguistics. We present the African Languages Lab (All Lab), comprehensive research initiative that addresses this technological gap through systematic data collection, model development, and capacity building. Our contributions include: (1) quality-controlled data collection pipeline, yielding the largest validated African multi-modal speech and text dataset spanning 40 languages with 19 billion tokens of monolingual text and 12,628 hours of aligned speech data; (2) extensive experimental validation demonstrating that our dataset, combined with fine-tuning, achieves substantial improvements over baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points across 31 evaluated languages; and (3) structured research program that has successfully mentored fifteen early-career researchers, establishing sustainable local capacity. Our comparative evaluation against Google Translate reveals competitive performance in several languages while identifying areas that require continued development."
        },
        {
            "title": "Introduction",
            "content": "The promise of artificial intelligence (AI) and natural language processing (NLP) to democratize information access remains unfulfilled for billions of speakers worldwide. Among the approximately 1To promote accessibility, we provide translations of this abstract in 10 African languages in Appendix B, generated using our fine-tuned models. 7,000 languages spoken globally, fewer than 20 receive substantial attention in NLP research (Magueresse et al., 2020). This technological marginalization particularly affects low-resource languages (LRLs). Without clearly established definition, LRLs are languages that exist at the periphery of the digital transformation, characterized by three critical deficits: (1) scarcity of machine-readable corpora, (2) limited personalized computational technologies and trained language models, and (3) insufficient representation in global research communities (Nigatu et al., 2024; Issaka et al., 2024; Magueresse et al., 2020). While often serving substantial speaker populations, these languages face significant challenges in participating fully in the AI-driven information economy. For Africa, the scale of this crisis is staggering: over 2,000 languages are spoken across Africa (nearly one-third of all languages worldwide). Yet, stunning 88% of African languages are \"severely underrepresented\" or \"completely ignored\" in computational linguistics (Joshi et al., 2020). As illustrated in Figure 1, approximately 814 African languages are in danger of extinction. Countries like Nigeria, Cameroon, and the Ivory Coast have 171, 75, and 65 languages facing the most severe threats, respectively 2. This exclusion has far-reaching consequences, from poor educational and healthcare outcomes to preventing full participation in the digital economy (Laitin et al., 2019; Gessler and von der Wense, 2024). This problem is compounded by severe underrepresentation in the global NLP research com2https://www.ethnologue.com/ munity. Our analysis of mentions of the top 10 global languages versus the top 10 African languages across major academic databases reveals stark imbalance. On average, for every paper discussing African languages in multilingual LLM contexts, there are 20 papers on global languages in Google Scholar (GS), 23 in COnnecting REpositories(CORE), 34 in arXiv, and 70 in The Institute of Electrical and Electronics Engineers (IEEE) (Table 1 and Table 4 in the Appendix). This 20-70x representation gap reinforces self-perpetuating cycle of marginalization where limited research attention leads to poor technological support, which in turn discourages further research investment. Contributing to broader efforts to bridge this systemic technological gap, we present the African Languages Lab (All Lab), an initiative to democratize NLP technology for African languages. Founded in 2020, the All Lab operates through coordinated team of dedicated researchers who combine three innovative elements: 1. Systematic data infrastructure: We developed systematic, quality-controlled data collection framework powered by our \"All Voices\" platform. All Voices is mobile-first platform specifically designed for communitydriven multilingual data collection in lowresource contexts, enabling direct translation between African languages without English intermediation 2. Comprehensive development: dataset Through coordinated collection and validation efforts, we assembled the largest multi-modal dataset for African LRLs, encompassing 19 billion tokens across 40 languages with 12,628 hours of aligned speech data. 3. Empirical validation and capacity building: We demonstrate the effectiveness of our approach through extensive experiments showing average improvements of +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points, while simultaneously developing local research capacity through structured mentorship programs."
        },
        {
            "title": "2 Related Work",
            "content": "The landscape of African NLP research has evolved through three interconnected streams: communitydriven initiatives, advances in multilingual modeling, and the development of evaluation frameworks. We examine how these efforts have shaped current capabilities and identify gaps our work addresses. 2.1 Community-Driven Research Initiatives The development of African NLP has been shaped by complementary community and institutional efforts. Masakhane, comprising over 3,000 Slack members, exemplifies successful communitydriven research, demonstrating that participatory approaches can produce high-quality datasets and models (Orife et al., 2020). Complementing this work, the \"Breaking the Unwritten Language Barrier\" project addresses challenges specific to unwritten and under-documented languages (Adda et al., 2016). Their work on languages like Basaa, Myene, and Embosi has established methodological approaches for speech recognition in LRLs. These community efforts have been supported by institutional initiatives providing essential infrastructure. The Lacuna Fund has enabled dataset development (Rathi et al., 2023), while Metas No Language Left Behind project has contributed architectural innovations for massively multilingual models (Team et al., 2022). Additional infrastructure support has come from Mozillas Common Voice project (Ardila et al., 2020) for speech resources and the AI4D African Language Program (Siminyu et al., 2021) for benchmark development. The Deep Learning Indaba3 has contributed to research capacity building through its convenings, while platforms like Lanfrica have improved resource discoverability and research sharing across the continent (Emezue and Dossou, 2020)."
        },
        {
            "title": "2.2 Advances in African Multilingual NLP",
            "content": "The evolution of multilingual LLMs has shown steady progress in language coverage and capabilities. Early approaches like mBERT (Muller et al., 2021) and XLM-R (Conneau et al., 2020) established initial benchmarks, supporting approximately 100 languages each. Subsequent developments included more focused models like mBART (Liu et al., 2020), mT5 (Xue et al., 2021), and XGLM (Ersoy et al., 2023), which traded broader language coverage for improved performance on specific language sets. The advent of massive LLMs further expanded these capabilities, with models like GPT-3, mGPT (Shliazhko et al., 2024), 3https://deeplearningindaba.com/ IEEE CORE High-Resource Languages Language English Chinese Hindi Spanish Arabic French Bengali Portuguese Russian Urdu GS 14,700 7,710 1,980 4,240 3,150 4,490 943 1,980 2,950 arXiv 323 60 20 29 25 38 9 13 19 3 IEEE CORE 3,095 256 1,694 85 336 41 908 24 616 24 1,037 17 183 8 400 7 611 16 131 9 Language GS Swahili 617 Hausa 261 Yoruba 276 Igbo 203 Amharic 338 Oromo 104 Berber 55 Zulu 175 Fula 20 Malagasy 72 African Languages arXiv 10 1 1 0 2 1 0 1 0 0 3 0 0 0 2 1 0 1 0 0 114 49 59 38 49 21 11 38 7 Table 1: Publication volume analysis comparing top 10 global languages versus top 10 African languages across major academic databases (2020-2024). The disparity reveals 20-70 underrepresentation of African languages in computational linguistics research. Llama (Wendler et al., 2024) have shown capability in handling some African languages, (Robinson et al., 2023; Ojo et al., 2024; Zhu et al., 2024; Dong et al., 2024), though their performance generally does not match that of specialized models; underscoring the need for dedicated resources and architectures."
        },
        {
            "title": "2.3 Benchmarks and Evaluation Frameworks",
            "content": "The development of evaluation frameworks has enabled systematic progress measurement in African NLP across diverse task domains. MasakhaNER provides NER datasets for 10 languages (Adelani et al., 2021), AfriSenti offers sentiment analysis benchmarks in 14 languages (Muhammad et al., 2023), and AFROMT establishes standardized translation benchmarks for 8 languages (Reid et al., 2021). IrokoBench unifies evaluation across natural language inference, mathematical reasoning, and multiple-choice QA in 17 African languages (Adelani et al., 2025). More targeted evaluation resources include NaijaSenti for Nigerian languages (Muhammad et al., 2022) and Kencorpus for Kenyan languages (Wanjawa et al., 2023). These Africa-focused frameworks complement broader initiatives like FLORES200 (Team et al., 2022), the Aya Dataset (Singh et al., 2024b), and Global-MMLU (Singh et al., 2024a). Despite these developments and advances, significant challenges remain in African NLP research (Adebara and Abdul-Mageed, 2022; Issaka et al., 2024). Our work builds upon these foundations while addressing several key limitations in existing approaches, such as robust team coordination, Figure 1: Number of endangered languages in each African country, where darker shading indicates higher number of endangered languages. and BLOOM (Workshop et al., 2023) supporting varying numbers of African languages. Also, Glot500-m (Imani et al., 2023) extends support to 511 languages and the SERENGETI and Cheetah models supports about 517 African languages (Adebara et al., 2023, 2024). Additional progress has come from the Aya model, which demonstrates instruction-following capabilities across 101 languages (Üstün et al., 2024), and specialized models like AfroLM, which focuses on 23 African languages (Dossou et al., 2022). While not specifically trained in African languages, English-centric LLMs such as GPT-4 (OpenAI et al., 2024), Gemini (Team et al., 2025), and cross-initiative knowledge transfer, deduplication of efforts, and intentional skill set development."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Datasets All Voices Platform. To address the fundamental challenge of data scarcity in African languages, we developed All Voices, mobile-first platform that stands as the only solution specifically designed for data collection in any LRL. The platforms innovative approach enables direct translation between LRLs without requiring English as an intermediary, addressing critical gap in the existing data collection infrastructure. In addition, All Voices distinguishes itself through its multimodal capabilities, which support the collection and validation of text and audio data. The platform features an intuitive, user-friendly interface that encourages broad participation, complemented by gamification elements, including global leaderboard system that promotes user engagement. Importantly, All Voices is open and free to everyone, aligning with our mission to democratize language technology development. All Voices contributors provide informed consent for their contributions to be used for research purposes, including dataset creation, model training, and open-source distribution, with full transparency regarding data usage and the right to withdraw consent at any time. The platforms architecture, built using ReactNative 4 and Firebase 5, integrates user authentication and analytics, translation corpus management, and quality control components. Our authentication system provides comprehensive user profiling, tracking contributor demographics and expertise through quantifiable metrics, including successful translations and community validation scores. This system implements OAuth 2.0 authentication and role-based access control to ensure data integrity and user privacy. The translation corpus management system centrally stores both text and audio translations along with their metadata, and protects all data using AES-256 encryption at rest and TLS 1.3 during transmission. Translations undergo peer review requiring both minimum threshold of positive validation (>5 upvotes) and an acceptable error margin (<3 downvotes) to achieve verified status. key innovation is our recursive translation pipeline: verified translations become eligible 4https://reactnative.dev/ 5https://firebase.google.com/ Figure 2: The All Voices platform interface demonstrating its dual functionality: direct text translation from English to Yoruba (left panel) and community-driven translation validation system (right panel). The mobilefirst design enables participation from users with limited technical resources. source material for subsequent translations, creating multiplicative effect in data collection. Data Collection and Processing. Our dataset development methodology combines crowd-sourced translations through All Voices (Figure 2) with carefully curated open-source corpora. We integrate validated translations from our platform with established datasets, including NLLB (Team et al., 2022), CCMatrix (Wenzek et al., 2019), OpenSubtitles (Tiedemann, 2016), MultiCCAligned (ElKishky et al., 2020), ParaCrawl (Bañón et al., 2020), XLEnt (El-Kishky et al., 2021), MultiParaCrawl(Bañón et al., 2020), LinguaToolsWikiTitles (Tiedemann, 2012), and CCAligned (ElKishky et al., 2020). Additionally, we collect new datasets through our community partners. Our data processing implements robust twotier approach combining general normalization with language-specific processing. The general normalization phase addresses universal text artifacts through Unicode normalization, character encoding standardization, and structural cleaning, including HTML removal and symbol standardization. The language-specific processing phase implements specialized handling for African language features, including morphological analysis, computed using modified Tukey method with an adaptive multiplier. This adaptive threshold mechanism automatically calibrates to language-specific characteristics, implementing more stringent filtering for language pairs that exhibit consistent ratios while allowing appropriate flexibility for pairs with inherently higher variability. The resulting validation framework effectively identifies and filters anomalous translations while maintaining sensitivity to legitimate linguistic variations across diverse African language families. The processed data sets are structured according to HuggingFace 6 Dataset specifications, enabling seamless API integration. 3.2 Model Development To evaluate the utility of our dataset and establish baselines for our languages, we experimented with Llama-3.2-1B (Grattafiori et al., 2024). We chose this model as our base because of its demonstrated multilingual capabilities and efficient parameter scaling, making it suitable for LRLs. Critically, while capable of tokenizing, this model has not been explicitly trained on any African language in our dataset. Thus, providing cleaner baseline for measuring the utility of our dataset during finetuning without risk of data contamination. We employed full fine-tuning rather than parameter-efficient methods, as preliminary experiments with Quantization-aware Low-Rank Adaptation (QLoRA) (Üstün et al., 2024) yielded insufficient performance gains for our target languages. Our training pipeline uses supervised learning with standardized instruction template: \"Translate the following English text to X:\", where denotes the target African language. Training leveraged NVIDIA H100 GPUs with the following parameters: batch size of 64 with 4-step gradient accumulation, maximum sequence length of 1024 tokens for both input and output, single epoch training using all available parallel data per language, learning rate of 5.0 105 with cosine scheduling and 0.15 warm-up ratio, and BF16 mixed precision for memory efficiency. During inference, we maintained consistency with batch size 64 while using temperature 0.1, top-p 0.95, top-k 50, and maximum output length of 1024 tokens to balance diversity with quality and reproducibility. 6https://huggingface.co/ Figure 3: End-to-end processing pipeline showing multisource integration, language-specific preprocessing, and statistical validation ensuring dataset quality. script variant normalization, and tone mark standardization, language identification, with custom rule sets developed for specific language families. Next, our translation validation methodology implements robust statistical framework for assessing translation quality through quantitative analysis of character-level distributions. The validation metric employs character ratio analysis between source and target texts, computed as the ratio of target text length to source text length. We analyze these ratios using z-score normalization within languagespecific distribution, enabling the detection of statistical outliers while accounting for natural variations in text length across different language pairs. This approach is augmented with character overlap detection to identify potential artifacts or inappropriate text preservation, particularly crucial for languages sharing similar orthographic features. Also, the threshold determination process implements an adaptive sampling methodology. For each language pair, we establish baseline distributions through initial sampling of 10,000 translation pairs, employing Kernel Density Estimation for robust distribution modeling. This approach effectively captures the non-Gaussian characteristics frequently observed in cross-lingual character distributions. Thresholds are dynamically 3.3 Evaluation Metrics Model performance was evaluated using complementary set of metrics: BiLingual Evaluation Understudy (BLEU) (Wieting et al., 2019), which measures n-gram precision; METEOR (Banerjee and Lavie, 2005), which accounts for word stems and synonyms; COMET (Rei et al., 2020), which leverages multilingual embeddings to assess semantic similarity; and ChrF++ (Wang et al., 2025), which operates on character-level n-grams to better capture morphological variations common in African languages. Additionally, we employed Translation Edit Rate (TER) (Snover et al., 2006), which quantifies the minimum number of edits required to transform the hypothesis into the reference translation (where lower scores indicate better quality), and AfriCOMET (Wang et al., 2024), neural metric specifically trained on African language pairs to better capture language-specific quality nuances. Together, these metrics comprehensively assess translation quality across different linguistic aspects, from surface-level n-gram matching to semantic preservation and post-editing effort. We utilized the FLORES-200 dataset (Team et al., 2022) as our standardized test set, ensuring consistency across all languages and enabling direct comparison with other multilingual systems."
        },
        {
            "title": "4 Results",
            "content": "Our comprehensive data collection yielded 19,012,019,696 tokens of monolingual text and 12,628 hours of aligned audio across 40 African languages  (Table 2)  . Our analysis reveals distinct stratification patterns highlighting the digital divide within African languages themselves. From text perspective (measured in millions of tokens), we observe four distinct tiers: 1. Primary resource languages (>2B tokens): This tier includes Amharic (2.94B), Arabic (2.40B), Yoruba (2.36B), and Afrikaans (2.30B), reflecting sustained digitization efforts and strong institutional support. 2. Established digital languages (12B tokens): Languages such as Hausa (1.54B) and Tigrinya (0.92B) demonstrate robust digital presence, likely owing to consistent documentation and preservation initiatives. (700.39M), and Xhosa (563.07M), show growing digital footprints but still lagging behind the top tiers. 4. Resource-constrained languages (<250M tokens): The majority of languages in our dataset fall into this category, including widely spoken languages such as Bambara (109.49M) and Luganda (121.17M). This tier reflects substantial gaps in textual data availability. For audio resources (measured in hours), the stratification follows different pattern, highlighting distinct set of leading languages: 1. High-resource audio languages (>1,000 hours): Kinyarwanda (3,839.00h), Luganda (1,727.80h), Swahili (1,115.00h), and Arabic (2,721.52h) dominate in audio availability, often due to large-scale speech corpora or broadcast archives. 2. Established audio languages (5001,000 hours): This tier is notably sparse, underscoring the scarcity of mid-scale speech datasets in African languages. 3. Moderate audio languages (100500 hours): Includes Malagasy (325.14h), Twi (227.03h), Bemba (230.30h), and Ewe (147.00h), representing mix of widely spoken languages and those with targeted speech collection efforts. 4. Low-resource audio languages (<100 hours): Many languages, including Kikongo, Rundi, Kanuri, Umbundu, and Fang, have either minimal or no audio data. Overall, our analysis underscores two parallel digital divides: textual divide, where small set of languages capture the majority of tokens, and an audio divide, where different but equally narrow set of languages dominate. Notably, the top three languages by text volume account for disproportionate share of tokens, while the top three by audio hours similarly capture the bulk of recorded speech. This imbalance highlights the urgent need for targeted development of both textual and audio resources, particularly for languages with substantial speaker populations but limited digital presence."
        },
        {
            "title": "4.1 Translation Performance Analysis",
            "content": "3. Emerging digital languages (250M1B tokens): substantial group including Malagasy (839.12M), Somali (751.13M), Swahili Our experimental evaluation across 31 African languages reveals substantial and systematic improvements through fine-tuning, with distinct patterns Language Tokens Hours Tshiluba Language Amharic Arabic Yoruba Afrikaans Hausa Tigrinya Malagasy Somali Swahili Xhosa Zulu Igbo Shona Kinyarwanda Tokens 2,944.95 2,400.00 2,362.70 2,295.09 1,538.84 916.42 839.12 751.13 700.39 563.07 553.67 433.28 428.25 283.40 Kikongo Ewe Language Tokens Hours Hours 114.70 Sesotho 238.00 145.00 Mossi 2,721.52 Oromo 35.00 Chewa 128.30 - Rundi 138.00 239.00 Luganda Tswana 1.00 Bambara 325.14 Lingala 115.40 1,115.00 Twi Fon 123.70 Fula 83.20 25.00 Kikuyu 103.00 Wolof 3,839.00 1,727.80 Berber Krio 111.70 Bemba 30.60 Kanuri 194.30 Umbundu 227.03 Kiluba 18.50 Ngambay 124.00 44.00 Mandinka 183.20 274.61 252.82 230.63 172.61 121.17 118.84 109.49 102.19 86.49 77.27 72.40 66.34 57.46 Fang 54.93 50.59 46.59 31.74 28.86 22.76 8.60 6.18 5.10 2.02 1.03 0.41 0.02 - - - 147.00 19.33 80.00 230.30 - - - - - - Table 2: Dataset composition across our 40 African languages sorted by token count, showing the distribution of tokens (in millions) and hours of audio data. Dashes (-) indicate no audio data available. emerging across language families and resource levels  (Table 3)  . Nine languages were excluded from evaluation due to insufficient training data or absence from the FLORES-200 benchmark. Baseline Performance. The base Llama-3.2-1B model demonstrates limited but non-trivial capability for African languages, revealing interesting patterns of cross-lingual transfer. ChrF++ scores range from 2.00 (Wolof) to 44.76 (Afrikaans), with mean of 8.10, indicating minimal character-level understanding for most languages. COMET scores cluster between 0.16-0.68 (mean: 0.32), suggesting some semantic comprehension despite poor surface realization. Notably, Afrikaans shows exceptional baseline performance (ChrF++ 44.76, BLEU 32.98), leveraging its Germanic roots and Latin script. The extremely low baseline BLEU scores (mean: 2.27) across most languages confirm the models inability to produce accurate n-gram sequences without language-specific training. Fine-tuning Impact. Our dataset enables noticeable performance improvements, with average gains of +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points. The magnitude of improvement correlates inversely with baseline performance, suggesting effective transfer learning rather than simple memorization. Swahili exhibits the largest absolute ChrF++ improvement (+63.27 points), achieving near-parity with Google Translate (72.27 vs 75.81). Sesotho shows remarkable gains across all metrics (+61.79 ChrF++, +51.16 BLEU), while maintaining competitive performance against Google Translate. Languages with minimal baselines, including Fula, Wolof, and Kikongo, demonstrate that even severely underresourced languages benefit substantially from targeted fine-tuning, achieving functional translation capability where none existed before. Comparison with Google Translate. Where available (22 languages), comparison with Google Translate reveals three distinct performance categories. First, languages where our models achieve competitive or superior performance: Yoruba (30.88 vs 21.05 ChrF++), Arabic (31.52 vs 28.46), and notably Twi, where we substantially outperform Google Translate (46.80 vs 31.48). Second, languages where Google maintains clear advantages, particularly in high-resource cases like Kinyarwanda (24.65 vs 70.27) and Swahili (72.27 vs 75.81), reflecting their extensive training data. Third, the languages where Google Translate offers no support, highlighting our contribution to genuinely LRL coverage. Language-Specific Patterns. Three response profiles emerge from our analysis. High-responder languages (Swahili, Sesotho, Hausa) show dramatic improvements exceeding 40 ChrF++ points, suggesting optimal alignment between our dataset characteristics and model architecture. Steady improvers (Igbo, Shona, Somali) demonstrate consistent gains of 25-30 points across metrics, indicating robust but not exceptional adaptation. Challenging cases (Fon, Wolof, Bambara) show limited improvements despite fine-tuning, likely requiring Language Amharic Fula Yoruba Igbo Oromo Swahili Hausa Twi(Asante) Shona Kinyarwanda Ewe Bambara Wolof Luganda Arabic Somali Afrkaans Tigrinya Malagasy Xhosa Zulu Sesotho Chewa Lingala Tswana Fon Kikuyu Tshiluba Mossi Kikongo Bemba Average ChrF++ () Llama1B Ours GT 30.24 - 21.05 45.92 54.93 75.81 54.60 31.48 51.93 70.27 47.86 27.02 - 23.55 28.46 47.32 52.10 23.75 43.06 47.88 64.45 57.01 34.29 38.14 - - - - - - - 44.14 28.69 18.73 30.88 33.42 27.74 72.27 51.77 46.80 36.55 24.65 33.48 15.60 12.01 24.91 31.52 35.64 48.07 24.12 26.29 46.17 38.06 61.79 28.57 33.32 26.37 9.47 25.80 22.10 23.99 20.84 25.79 31.79 3.26 2.97 3.77 5.30 11.30 9.0 7.75 4.0 7.52 5.62 3.05 4.06 2.00 7.08 8.67 7.11 44.76 5.43 9.39 6.58 7.58 7.09 4.76 5.91 8.59 10.57 9.77 15.68 12.82 6.18 3.59 8. COMET () BLEU () Llama1B Ours GT Llama1B Ours GT 16.11 - 11.96 36.09 33.57 54.81 45.49 17.81 18.05 49.60 25.79 13.60 - 3.96 21.41 12.90 27.25 13.00 9.10 14.95 34.57 33.25 12.67 16.82 - - - - - - - 23.76 12.16 5.73 32.33 14.10 5.72 56.23 22.38 27.36 12.06 17.25 31.55 22.50 2.68 3.84 23.36 8.99 18.00 13.65 22.49 12.87 5.90 51.16 13.00 18.21 16.98 6.50 17.92 7.44 9.62 6.15 27.69 17.61 3.83 0.14 0.41 0.65 5.54 0.54 0.57 0.29 0.25 0.37 0.27 0.18 0.16 0.41 0.30 0.41 32.98 2.77 0.32 0.63 0.37 0.14 0.0 0.23 0.69 5.14 0.28 5.856 5.86 0.38 0.25 2.27 0.82 0.55 0.67 0.71 0.70 0.78 0.70 0.71 0.60 0.56 0.26 0.65 0.61 0.64 0.85 0.76 0.86 0.76 0.80 0.75 0.76 0.61 0.62 0.58 0.63 0.60 0.55 0.55 0.51 0.55 0.46 0. 0.38 0.32 0.23 0.27 0.31 0.40 0.39 0.26 0.27 0.28 0.22 0.25 0.30 0.28 0.52 0.37 0.68 0.27 0.42 0.35 0.32 0.28 0.28 0.30 0.28 0.16 0.27 0.31 0.28 0.31 0.31 0.32 0.88 - 0.56 0.72 0.80 0.85 0.80 0.71 0.63 0.67 0.37 0.72 - 0.63 0.89 0.80 0.85 0.83 0.76 0.70 0.73 0.66 0.61 0.65 - - - - - - - 0.72 Africomet () METEOR () TER () Llama1B Ours GT Llama1B Ours GT Llama1B Ours 64.24 72.22 84.31 73.11 119.01 23.77 52.3 49.12 99.15 65.53 93.32 49.33 84.83 74.57 67.00 65.09 48.23 36.55 57.08 71.79 89.52 63.22 72.16 80.32 65.69 29.73 59.38 52.71 81.46 44.51 48.63 65.74 58.40 1151.08 525.95 523.93 77.62 1021.93 993.66 656.31 1318.15 428.45 689.73 766.87 1030.14 980.96 1164.70 442.60 52.61 78.33 856.18 561.279 728.98 1475.22 876.28 955.39 413.87 56.15 943.70 61.49 57.02 318.97 755.92 645. 0.14 0.03 0.01 -0.70 0.13 -0.20 0.01 -0.06 0.01 -0.18 0.06 0.08 0.07 -0.05 0.21 0.03 0.37 0.04 0.13 0.21 0.09 0.14 -0.02 0.00 0.11 0.14 0.03 0.20 0.18 -0.05 0.09 0.04 0.56 0.15 0.60 0.52 0.29 0.62 0.47 0.30 0.47 0.40 0.24 0.22 0.25 0.55 0.70 0.51 0.74 0.43 0.61 0.55 0.62 0.59 0.56 0.12 0.38 0.04 0.03 0.12 -0.03 0.07 0.07 0.38 0.72 - 0.55 0.57 0.66 0.72 0.64 0.36 0.59 0.66 0.33 0.39 - 0.57 0.77 0.62 0.73 0.70 0.45 0.41 0.54 0.65 0.44 0.40 - - - - - - - 0.57 0.01 0.08 0.05 0.06 0.06 0.07 0.07 0.07 0.07 0.04 0.07 0.08 0.07 0.07 0.08 0.07 0.35 0.01 0.06 0.09 0.07 0.08 0.07 0.08 0.07 0.06 0.07 0.11 0.08 0.05 0.10 0.08 0.40 - 0.16 0.43 0.29 0.65 0.53 0.33 0.34 0.48 0.37 0.29 - 0.33 0.55 0.37 0.65 0.23 0.36 0.31 0.35 0.44 0.31 0.45 - - - - - - - 0.39 0.30 0.06 0.27 0.41 0.12 0.57 0.41 0.29 0.28 0.23 0.26 0.20 0.28 0.31 0.52 0.32 0.66 0.16 0.39 0.33 0.41 0.31 0.35 0.23 0.27 0.06 0.09 0.11 0.09 0.16 0.10 0. GT 58.40 - 76.28 52.80 56.92 23.77 64.37 64.24 64.16 30.24 48.69 112.12 - 63.10 67.00 60.75 39.46 78.33 57.08 58.74 51.16 45.66 67.01 54.96 - - - - - - - 58.87 Table 3: Performance comparison between base Llama-3.2-1B (Llama1B), our Finetuned Llama-3.2-1B models (Ours), and Google Translate (GT) across different metrics (ChrF++, COMET, BLEU, Africomet, METEOR, and TER). Higher scores indicate better performance (except for TER, where lower scores are better). specialized tokenization or architectural modifications to address their unique linguistic features. Translation Edit Rate Analysis. The dramatic TER reductions, averaging 580.13 points lower after fine-tuning, provide crucial practical insights. Languages like Swahili achieve TER scores comparable to Google Translate (23.77), indicating production-ready quality requiring minimal postediting. Even languages with modest BLEU improvements show substantial TER reductions, suggesting improved fluency and coherence that traditional metrics may not fully capture. This pattern holds particular significance for scenarios where post-editing cost determines practical viability. Cross-Metric Correlations. While surface metrics (BLEU, ChrF++) show high correlation, the divergence between these and neural metrics (COMET, AfriCOMET) reveals important quality dimensions. Languages like Ewe show minimal COMET improvement (0.04) despite substantial ChrF++ gains (30.43 points), suggesting character-level improvements without the corresponding semantic enhancement. In contrast, Arabic shows strong COMET gains (0.33) with modest improvement in ChrF++, indicating semantic preservation despite surfacelevel challenges. These patterns underscore the importance of multi-metric evaluation for morphologically diverse African languages."
        },
        {
            "title": "5 Conclusion",
            "content": "We have presented the African Languages Lab, research initiative addressing the critical underrepresentation of African languages in NLP through systematic data collection, model development, and capacity building. Our contributions include validated dataset of 19 billion tokens and 12,628 hours of aligned speech across 40 languages, substantial performance improvements averaging +23.69 ChrF++ over baseline models, the All Voices platform, and structured mentorship program developing fifteen early-career researchers. Thus, we demonstrate that the technological marginalization of African languages, while severe, is not intractable. As language technologies increasingly mediate access to information, education, and economic opportunities, ensuring equitable coverage becomes not merely technical challenge but moral imperative. The African Languages Lab demonstrates that this imperative can be met through coordinated research, community engagement, and sustained investment in both technical infrastructure and human capacity, establishing sustainable path forward for the worlds underserved linguistic communities."
        },
        {
            "title": "6 Limitations",
            "content": "6.1 Model Architecture and Scale Constraints Our experiments utilize Llama-3.2-1B as the sole base model, which, while demonstrating the utility of our dataset, may underestimate potential gains achievable with larger-scale architectures. The performance variance across language families, from 63.27 ChrF++ improvement for Swahili to minimal gains for Fon (-1.10), suggests that optimal model selection likely varies by linguistic typology. Additionally, our evaluation of 31 of 40 collected languages reflects FLORES-200 coverage limitations, potentially obscuring insights from the most critically under-resourced languages in our dataset. They also underscore the need for continued investment in computational resources, human expertise, and infrastructure development to support comprehensive technology development for African languages."
        },
        {
            "title": "7 Ethics Statement and Broader Impacts",
            "content": "Developing NLP technologies for LRLs demands rigorous ethical engagement, particularly in contexts shaped by historical exclusion, infrastructural inequities, and linguistic marginalization. Our work at the All Lab is grounded in principled commitment to the public good, community accountability, and equitable technological development. 6.2 Dataset Imbalance and Coverage 7.1 Data Collection Ethics Despite assembling 19 billion tokens, our dataset exhibits 147,000 disparity between the highestresourced (Amharic: 2,944.95M tokens) and lowest-resourced (Fang: 0.02M tokens) languages. This imbalance directly correlates with perforlanguages with >1B tokens mance outcomes: achieve average ChrF++ scores of 45.66, while those with <100M tokens average 24.31. Furthermore, 13 languages lack audio data entirely, limiting multimodal model development. Our validation pipeline, while statistically grounded, operates without native speaker verification for 73% of languages, potentially missing dialectal variations that affect 28% of evaluated translations showing COMET-ChrF++ divergence exceeding 0.3. Our data collection through the All Voices platform operates on principles of voluntary, consent-based participation with full revocability rights. Contributors are informed of their rights, with explicit consent obtained for research purposes including dataset creation, model training, and open-source distribution. The platform implements embedded reporting mechanisms for flagging offensive or culturally inappropriate content, with trained moderators reviewing submissions to maintain quality and cultural sensitivity. We acknowledge that automated validation procedures may miss dialectal nuances or culturally specific meanings, necessitating our ongoing collaboration with native speakers and community experts to expand linguistic coverage and cultural sensitivity."
        },
        {
            "title": "7.2 Data Governance.",
            "content": "The All Voices platform, while innovative, currently operates primarily through mobile interfaces, which may limit participation from communities with different technology preferences or access patterns. The platforms quality control mechanisms, while systematic, may inadvertently favor certain linguistic varieties over others. Overall, these constraints delineate several clear pathways for advancement. One direction is to explore architecture-specific optimizations for morphologically complex languages. Another is to implement active learning strategies that help address data imbalances. It is also important to develop evaluation metrics that are more sensitive to African language typologies. These limitations inform our ongoing work and highlight key areas for future research in African NLP. Our dataset management follows principles of responsible data stewardship. While we aim for maximum openness, certain data components are subject to agreements with contributing communities that restrict fully public release. We maintain managed access framework that provides dataset access to qualified researchers while respecting community rights and contributor agreements. Access requests are evaluated based on research purpose, institutional affiliation, and commitment to ethical use."
        },
        {
            "title": "Development",
            "content": "Our structured research development program has mentored fifteen early-career researchers across four institutions through one-on-one mentorship, project development support, and transitions into extended research roles. This investment in local research leadership establishes sustainable capacity for African NLP development, ensuring that technical advancement aligns with cultural and linguistic expertise. By prioritizing skill development alongside technical innovation, we contribute to sustainable talent pipeline that positions African researchers to lead future developments in their languages. 7.4 Societal Impact and Sustainability Our work directly advances United Nations Sustainable Development Goals in education and inequality reduction through increased digital representation of marginalized languages. The platform enables community-led content creation and facilitates open knowledge transfer, democratizing access to digital tools while preserving linguistic and cultural heritage. With 88% of African languages severely underrepresented or completely ignored in computational linguistics, and 814 languages facing extinction risk, our framework provides critical infrastructure for language preservation."
        },
        {
            "title": "Vision",
            "content": "Guided by Ubuntu philosophyemphasizing inclusivity, interdependence, and opennesswe establish framework for equitable NLP development. Our roadmap encompasses expanding language coverage, optimizing model architectures for lowresource languages, and deepening research collaborations. We acknowledge persistent challenges including limited commercial viability for some LRL technologies and infrastructural constraints, yet our results demonstrate that systematic community engagement can effectively address technological marginalization. Through this comprehensive approach integrating technical innovation, cultural preservation, educational empowerment, and economic inclusion, we provide replicable models for equitable language technology development that can benefit millions of African language speakers while contributing to global linguistic diversity."
        },
        {
            "title": "References",
            "content": "Gilles Adda, Sebastian Stüker, Martine Adda-Decker, Odette Ambouroue, Laurent Besacier, David Blachon, Hélène Bonneau-Maynard, Pierre Godard, Fatima Hamlaoui, Dmitry Idiatov, Guy-Noël Kouarata, Lori Lamel, Emmanuel-Moselly Makasso, Annie Rialland, Mark Van de Velde, François Yvon, and Sabine Zerbian. 2016. Breaking the unwritten language barrier: The bulb project. Procedia Computer Science, 81:814. SLTU-2016 5th Workshop on Spoken Language Technologies for Under-resourced languages 09-12 May 2016 Yogyakarta, Indonesia. Ife Adebara and Muhammad Abdul-Mageed. 2022. Towards afrocentric nlp for african languages: Where we are and where we can go. Preprint, arXiv:2203.08351. Ife Adebara, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. 2024. Cheetah: Natural language generation for 517 african languages. Preprint, arXiv:2401.01053. Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Alcoba Inciarte. 2023. SERENGETI: Massively multilingual language modIn Findings of the Association for els for Africa. Computational Linguistics: ACL 2023, pages 1498 1537, Toronto, Canada. Association for Computational Linguistics. David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel Dsouza, Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen H. Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo, Aremu Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie Yimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Degaga Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin, Tendai Marengereke, and Salomey Osei. 2021. Masakhaner: Named entity recognition for african languages. Transactions of the Association for Computational Linguistics, 9:1116 1131. David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba O. Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, Chiamaka Chukwuneke, Happy Buzaaba, Blessing Sibanda, Godson Kalipe, Jonathan Mukiibi, Salomon Kabongo, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Tadesse Kebede Guge, Tombekai Vangoni Sherman, and Pontus Irokobench: new benchmark Stenetorp. 2025. for african languages in the age of large language models. Preprint, arXiv:2406.03368. Cynthia Jayne Amol, Everlyn Asiko Chimoto, Rose Delilah Gesicho, Antony M. Gitau, Naome A. Etori, Caringtone Kinyanjui, Steven Ndungu, Lawrence Moruye, Samson Otieno Ooko, Kavengi Kitonga, Brian Muhia, Catherine Gitau, Antony Ndolo, Lilian D. A. Wanzare, Albert Njoroge Kahira, and Ronald Tombe. 2024. State of nlp in kenya: survey. Preprint, arXiv:2410.09948. Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. Common voice: massivelymultilingual speech corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 42184222, Marseille, France. European Language Resources Association. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan. Association for Computational Linguistics. Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. 2020. ParaCrawl: Web-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 45554567, Online. Association for Computational Linguistics. Andrew Caines. 2015. The geographic diversity of nlp conferences. MAREK REI, arXiv:1503.06733. Version 2. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. 2024. survey on in-context learning. Preprint, arXiv:2301.00234. Bonaventure F. P. Dossou, Atnafu Lambebo Tonja, Oreen Yousuf, Salomey Osei, Abigail Oppong, Iyanuoluwa Shode, Oluwabusayo Olufunke Awoyomi, and Chris Emezue. 2022. AfroLM: selfactive learning-based multilingual pretrained language model for 23 African languages. In Proceedings of The Third Workshop on Simple and Efficient Natural Language Processing (SustaiNLP), pages 5264, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Ahmed El-Kishky, Vishrav Chaudhary, Francisco GuzmÃn, and Philipp Koehn. 2020. CCAligned: massive collection of cross-lingual web-document In Proceedings of the 2020 Conference on pairs. Empirical Methods in Natural Language Processing (EMNLP 2020), pages 59605969, Online. Association for Computational Linguistics. Ahmed El-Kishky, Adithya Renduchintala, James Cross, Francisco Guzmán, and Philipp Koehn. 2021. XLEnt: Mining large cross-lingual entity dataset with lexical-semantic-phonetic word alignment. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10424 10430, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Chris C. Emezue and Bonaventure F. P. Dossou. 2020. Lanfrica: participatory approach to documenting machine translation research on african languages. ArXiv, abs/2008.07302. Chris Chinenye Emezue, Sanchit Gandhi, Lewis Tunstall, Abubakar Abid, Josh Meyer, Quentin Lhoest, Pete Allen, Patrick Von Platen, Douwe Kiela, Yacine Jernite, Julien Chaumond, Merve Noyan, and Omar Sanseviero. 2023. Afrodigits: community-driven spoken digit dataset for african languages. Preprint, arXiv:2303.12582. Asım Ersoy, Gerson Vizcarra, Tahsin Mayeesha, and Benjamin Muller. 2023. In what languages are generative language models the most formal? analyzing formality distribution across languages. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 26502666, Singapore. Association for Computational Linguistics. Luke Gessler and Katharina von der Wense. 2024. NLP for language documentation: Two reasons for the gap between theory and practice. In Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024), pages 16, Mexico City, Mexico. Association for Computational Linguistics. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Ayyoob Imani, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André Martins, François Yvon, and Hinrich Schütze. 2023. Glot500: Scaling multilingual corpora and language models to 500 languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10821117, Toronto, Canada. Association for Computational Linguistics. Sheriff Issaka, Zhaoyi Zhang, Mihir Heda, Keyi Wang, Yinka Ajibola, Ryan DeMar, and Xuefeng Du. 2024. The ghanaian nlp landscape: first look. Preprint, arXiv:2405.06818. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 62826293, Online. Association for Computational Linguistics. David D. Laitin, Rajesh Ramachandran, and Stephen L. Walter. 2019. The legacy of colonial language policies and their impact on student learning: Evidence from an experimental program in cameroon. Economic Development and Cultural Change, 68(1):239 272. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pretraining for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726742. Alexandre Magueresse, Vincent Carles, and Evan Heetderks. 2020. Low-resource languages: review of past work and future challenges. Preprint, arXiv:2006.07264. Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, Nedjma Ousidhoum, David Ifeoluwa Adelani, Seid Muhie Yimam, Ibrahim Said Ahmad, Meriem Beloucif, Saif M. Mohammad, Sebastian Ruder, Oumaima Hourrane, Pavel Brazdil, Felermino Dário Mário António Ali, Davis David, Salomey Osei, Bello Shehu Bello, Falalu Ibrahim, Tajuddeen Gwadabe, Samuel Rutunda, Tadesse Belay, Wendimu Baye Messelle, Hailu Beshada Balcha, Sisay Adugna Chala, Hagos Tesfahun Gebremichael, Bernard Opoku, and Steven Arthur. 2023. Afrisenti: twitter sentiment analysis benchmark for african languages. Preprint, arXiv:2302.08956. Shamsuddeen Hassan Muhammad, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Idris Abdulmumin, Bello Shehu Bello, Monojit Choudhury, Chris C. Emezue, Anuoluwapo Aremu, Saheed Abdul, and Pavel Brazdil. 2022. Naijasenti: nigerian twitter sentiment corpus for multilingual sentiment analysis. In International Conference on Language Resources and Evaluation. Benjamin Muller, Antonios Anastasopoulos, Benoît Sagot, and Djamé Seddah. 2021. When being unseen from mBERT is just the beginning: Handling new languages with multilingual language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 448462, Online. Association for Computational Linguistics. Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi Fasubaa, Taiwo Fagbohungbe, Solomon Oluwole Akinola, Shamsuddeen Muhammad, Salomon Kabongo Kabenamualu, Salomey Osei, Freshia Sackey, Rubungo Andre Niyongabo, Ricky Macharm, Perez Ogayo, Orevaoghene Ahia, Musie Meressa Berhe, Mofetoluwa Adeyemi, Masabata Mokgesi-Selinga, Lawrence Okegbemi, Laura Martinus, Kolawole Tajudeen, Kevin Degila, Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer, Jason Webster, Jamiil Toure Ali, Jade Abbott, Iroro Orife, Ignatius Ezeani, Idris Abdulkadir Dangana, Herman Kamper, Hady Elsahar, Goodness Duru, Ghollah Kioko, Murhabazi Espoir, Elan van Biljon, Daniel Whitenack, Christopher Onyefuluchi, Chris Chinenye Emezue, Bonaventure F. P. Dossou, Blessing Sibanda, Blessing Bassey, Ayodele Olabiyi, Arshath Ramkilowan, Alp Öktem, Adewale Akinfaderin, and Abdallah Bashir. 2020. Participatory research for low-resourced machine translation: case study in African languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 21442160, Online. Association for Computational Linguistics. Hellina Hailu Nigatu, Atnafu Lambebo Tonja, Benjamin Rosman, Thamar Solorio, and Monojit Choudhury. 2024. The zenos paradox of low-resource languages. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1775317774, Miami, Florida, USA. Association for Computational Linguistics. Jessica Ojo, Kelechi Ogueji, Pontus Stenetorp, and David Ifeoluwa Adelani. 2024. How good are large language models on african languages? Preprint, arXiv:2311.07978. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Iroro Orife, Julia Kreutzer, Blessing Sibanda, Daniel Whitenack, Kathleen Siminyu, Laura Martinus, Jamiil Toure Ali, Jade Abbott, Vukosi Marivate, Salomon Kabongo, Musie Meressa, Espoir Murhabazi, Orevaoghene Ahia, Elan van Biljon, Arshath Ramkilowan, Adewale Akinfaderin, Alp Öktem, Wole Akin, Ghollah Kioko, Kevin Degila, Herman Kamper, Bonaventure Dossou, Chris Emezue, Kelechi Ogueji, and Abdallah Bashir. 2020. Masakhane machine translation for africa. Preprint, arXiv:2003.11529. Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi Shekhar, Mehreen Alam, and Rishemjit Kaur. 2023. Neural machine translation for low-resource languages: survey. ACM Comput. Surv., 55(11). Shashank Rathi, Siddhesh Pande, Harshwardhan Atkare, Rahul Tangsali, Aditya Vyawahare, and Dipali Kadam. 2023. Trinity at SemEval-2023 task 12: Sentiment analysis for low-resource African languages using Twitter dataset. In Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 11611165, Toronto, Canada. Association for Computational Linguistics. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. Comet: neural framework for mt evaluation. Preprint, arXiv:2009.09025. Machel Reid, Junjie Hu, Graham Neubig, and Yutaka Matsuo. 2021. AfroMT: Pretraining strategies and reproducible benchmarks for translation of 8 African languages. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 13061320, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, and Graham Neubig. 2023. Chatgpt mt: Competitive for high- (but not low-) resource languages. Preprint, arXiv:2309.07423. Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Anastasia Kozlova, Vladislav Mikhailov, and Tatiana Shavrina. 2024. mGPT: Few-shot learners go multilingual. Transactions of the Association for Computational Linguistics, 12:5879. Kathleen Siminyu, Godson Kalipe, Davor Orlic, Jade Abbott, Vukosi Marivate, Sackey Freshia, Prateek Sibal, Bhanu Neupane, David I. Adelani, Amelia Taylor, Jamiil Toure ALI, Kevin Degila, Momboladji Balogoun, Thierno Ibrahima DIOP, Davis David, Chayma Fourati, Hatem Haddad, and Malek Naski. 2021. Ai4d african language program. Preprint, arXiv:2104.02516. Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and Sara Hooker. 2024a. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation. Preprint, arXiv:2412.03304. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergün, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Üstün, Marzieh Fadaee, and Sara Hooker. 2024b. Aya dataset: An open-access collection for multilingual instruction tuning. Preprint, arXiv:2402.06619. Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006. study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, pages 223231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, HengTze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian Güra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, Ágoston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff Pitman, Rushin Shah, Emanuel Taropa, Majd Al Merey, Martin Baeuml, Zhifeng Chen, Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, Iñaki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Xavier Garcia, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adrià Puigdomènech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, MingWei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, Sébastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozinska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai Giménez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario Luˇcic, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, Raphaël Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe Sjösund, Sébastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, Léonard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, Adrià Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, Víctor Campos Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, Çaglar Ünlü, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakicevic, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz Kepa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri Gayatri Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton Älgmyr, Timothée Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam Rabinovitch, Pavan Kumar Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan Heimlich Shtacher, Shachi Paul, Oscar Akerlund, François-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora Marinescu, Martin Bölle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei \"Louis\" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah Ó Donnaile, Sébastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya Pranavi Potharaju, Eileen ONeill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, Niccolò Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor Ähdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou, Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Roopali Vij, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, John Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur Bražinskas, Andrei Sozanschi, Matthew Hayes, Héctor Fernández Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante Kärrman, Paweł Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso Castaño, Irene Giannoumis, Wooyeol Kim, Mikołaj Rybinski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, Rémi Leblond, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, Amélie Héliou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, Soheil Hassas Yeganeh, Siim Põder, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher A. ChoquetteChoo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane Rivière, Alanna Walton, Clément Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri LatorreChimoto, Hanna Klimczak-Plucinska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke Ye, Jean Michel Sarr, Melanie Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, Vít Listík, Mathias Carlen, Jan van de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul Müller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia, Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, Chaitanya Krishna Lanka, Derik Clive, Yi Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, Tu Vu, Alek Andreev, Antoine He, Kevin Hui, Sheleem Kashem, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, and Oriol Vinyals. 2025. Gemini: family of highly capable multimodal models. Preprint, arXiv:2312.11805. NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. Preprint, arXiv:2207.04672. Jörg Tiedemann. 2016. Finding alternative translations in large corpus of movie subtitle. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), pages 3518 3522, Portorož, Slovenia. European Language Resources Association (ELRA). Jörg Tiedemann. 2012. Parallel data, tools and interIn Proceedings of the Eight Interfaces in opus. national Conference on Language Resources and Evaluation (LREC12), Istanbul, Turkey. European Language Resources Association (ELRA). Jiayi Wang, David Ifeoluwa Adelani, Sweta Agrawal, Marek Masiak, Ricardo Rei, Eleftheria Briakou, Marine Carpuat, Xuanli He, Sofia Bourhim, Andiswa Bukula, Muhidin Mohamed, Temitayo Olatoye, Tosin Adewumi, Hamam Mokayed, Christine Mwase, Wangui Kimotho, Foutse Yuehgoh, Anuoluwapo Aremu, Jessica Ojo, Shamsuddeen Hassan Muhammad, Salomey Osei, Abdul-Hakeem Omotayo, Chiamaka Chukwuneke, Perez Ogayo, Oumaima Hourrane, Salma El Anigri, Lolwethu Ndolela, Thabiso Mangwana, Shafie Abdi Mohamed, Ayinde Hassan, Oluwabusayo Olufunke Awoyomi, Lama Alkhaled, Sana Al-Azzawi, Naome A. Etori, Millicent Ochieng, Clemencia Siro, Samuel Njoroge, Eric Muchiri, Wangari Kimotho, Lyse Naomi Wamba Momo, Daud Abolade, Simbiat Ajao, Iyanuoluwa Shode, Ricky Macharm, Ruqayya Nasir Iro, Saheed S. Abdullahi, Stephen E. Moore, Bernard Opoku, Zainab Akinjobi, Abeeb Afolabi, Nnaemeka Obiefuna, Onyekachi Raphael Ogbu, Sam Brian, Verrah Akinyi Otiende, Chinedu Emmanuel Mbonu, Sakayo Toadoum Sari, Yao Lu, and Pontus Stenetorp. 2024. Afrimte and africomet: Enhancing comet to embrace under-resourced african languages. Preprint, arXiv:2311.09828. Ruiqi Wang, Jiyu Guo, Cuiyun Gao, Guodong Fan, Chun Yong Chong, and Xin Xia. 2025. Can llms replace human evaluators? an empirical study of llm-as-a-judge in software engineering. Preprint, arXiv:2502.06193. Barack Wanjawa, Lilian Wanzare, Florence Indede, Owen McOnyango, Edward Ombui, and Lawrence Muchemi. 2023. Kencorpus: kenyan language corpus of swahili, dholuo and luhya for natural language processing tasks. Journal for Language Technology and Computational Linguistics, 36(2):127. Chris Wendler, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. Do llamas work in English? on the latent language of multilingual transformers. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1536615394, Bangkok, Thailand. Association for Computational Linguistics. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2019. Ccnet: Extracting high quality monolingual datasets from web crawl data. Preprint, arXiv:1911.00359. John Wieting, Taylor Berg-Kirkpatrick, Kevin Gimpel, and Graham Neubig. 2019. Beyond BLEU: Training neural machine translation with semantic similarity. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4344 4355, Florence, Italy. Association for Computational Linguistics. BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Tasar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, ZhengXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, JanChristoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenˇek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria Castillo, Marianna Nezhurina, Mario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2023. Bloom: 176b-parameter open-access multilingual language model. Preprint, arXiv:2211.05100. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483498, Online. Association for Computational Linguistics. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2024. Multilingual machine translation with large language models: Empirical results and analysis. Preprint, arXiv:2304.04675. Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, WeiYin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya model: An instruction finetuned open-access multilingual language model. Preprint, arXiv:2402.07827."
        },
        {
            "title": "B Abstract Translations",
            "content": "Figure 4: Afrikaans Translation Figure 5: Amharic Translation Source GS (Link) arXiv(Link) IEEE (Link) CORE(Link) High-Resource African Ratio 20.2 33.7 69.6 22.5 42,871 539 487 9,011 2,121 16 7 Table 4: Aggregate paper counts and ratios between high-resource and African languages (2020-2024). The ratio shows the disparity in research visibility, with higher numbers indicating greater inequality in representation. Search term: multilingual large language models Figure 6: Xhosa Translation Figure 8: Shona Translation Figure 7: Swahili Translation Figure 9: Malagasy Translation Figure 10: Igbo Translation Figure 11: Hausa Translation Figure 13: Somali Translation Figure 12: Arabic Translation"
        }
    ],
    "affiliations": [
        "Carleton University",
        "Columbia University",
        "Cornell University",
        "Georgia Institute of Technology",
        "Northwestern University in Qatar",
        "Soka University of America",
        "Stetson University",
        "University of California, Los Angeles",
        "University of Cape Coast",
        "University of Wisconsin - Madison"
    ]
}