{
    "paper_title": "SingLoRA: Low Rank Adaptation Using a Single Matrix",
    "authors": [
        "David Bensaïd",
        "Noam Rotstein",
        "Roy Velich",
        "Daniel Bensaïd",
        "Ron Kimmel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient fine-tuning of large pretrained models. LoRA augments the pre-trained weights of a model by adding the product of two smaller matrices that together form a low-rank matrix update. Recent research has shown that scale disparities between these two matrices often cause unstable training dynamics, leading to suboptimal performance. In this paper, we propose SingLoRA, which reformulates low-rank adaptation by learning the weights update as a decomposition of a single low-rank matrix multiplied by its transpose. This simple design inherently removes inter-matrix scale conflicts, ensuring stable optimization, and roughly halves the parameter count. We analyze SingLoRA within the infinite-width neural network framework, showing that it guarantees stable feature learning by construction. Extensive experiments on multiple tasks validate these benefits. In common sense reasoning, fine-tuning LLama 7B on MNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+ (90.2%) - while using only 60% of their parameter budget. In image generation, fine-tuning Stable Diffusion with SingLoRA significantly improves image fidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to scores of 0.148 and 0.143 for DoRA and LoRA, respectively."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 6 6 5 5 0 . 7 0 5 2 : r SingLoRA: Low Rank Adaptation Using Single Matrix David Bensaïd Technion - IIT Haifa, Israel bensaiddavid@gmail.com Noam Rotstein Technion - IIT Haifa, Israel Roy Velich Technion - IIT Haifa, Israel Daniel Bensaïd University Paris Dauphine Paris, France Ron Kimmel Technion - IIT Haifa, Israel"
        },
        {
            "title": "Abstract",
            "content": "Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient finetuning of large pretrained models. LoRA augments the pre-trained weights of model by adding the product of two smaller matrices that together form lowrank matrix update. Recent research has shown that scale disparities between these two matrices often cause unstable training dynamics, leading to suboptimal performance. In this paper, we propose SINGLORA , which reformulates lowrank adaptation by learning the weights update as decomposition of single low-rank matrix multiplied by its transpose. This simple design inherently removes inter-matrix scale conflicts, ensuring stable optimization, and roughly halves the parameter count. We analyze SINGLORA within the infinite-width neural network framework, showing that it guarantees stable feature learning by construction. Extensive experiments on multiple tasks validate these benefits. In common sense reasoning, fine-tuning LLama 7B on MNLI with SINGLORA achieves 91.3% accuracy surpassing LoRA (89.1%) and LoRA+ (90.2%) while using only 60% of their parameter budget. In image generation, fine-tuning Stable Diffusion with SINGLORA significantly improves image fidelity on DreamBooth, achieving DINO similarity score of 0.151, compared to scores of 0.148 and 0.143 for DoRA and LoRA, respectively."
        },
        {
            "title": "Introduction",
            "content": "Adapting large pretrained models for specialized tasks has emerged as central focus in machine learning research. These efforts aim to leverage the strong generalization capabilities of such models while meeting domain-specific requirements. Rapid scaling of model sizes and datasets has made full fine-tuning computationally prohibitive, driving the development of parameter-efficient fine-tuning (PEFT) methods to reduce computational costs. Among PEFT approaches, Low-Rank Adaptation (LoRA) [6] has gained particular popularity due to its simplicity and effectiveness in various domains. LoRA augments pretrained weights matrices W0 Rdk with low-rank updates, W0 + BA, where Rdr, Rrk and min(d, k). (1) Recent research [4, 19, 20] has identified scale disparities between the matrices and as fundamental challenge in LoRA. Such scale imbalances lead to unstable training dynamics, causing vanishing or exploding gradients and ultimately resulting in suboptimal performance. To address this Preprint. challenge, we introduce SINGLORA , which reformulates the low rank adaptation paradigm using single low-rank matrix Rnr yielding the symmetric update, W0 + AA, (2) This formulation provides two key benefits over traditional LoRA. First, it ensures stable optimization by design, eliminating inter-matrix scale conflicts. Second, it achieves parameter reduction by roughly halving the number of learned parameters. Our empirical results show that these architectural improvements enable SINGLORA to consistently exceed LoRAs performance while using significantly fewer parameters. We analyze SINGLORA in the infinite-width neural network setting [3, 4], showing that, unlike LoRA, it guarantees stable feature learning by construction. Building on this theoretical foundation, we extend the approach to non-square matrices, ensuring its applicability to any neural network, and validate its results through comprehensive experiments across multiple modalities. For example, in comprehension reasoning, fine-tuning LLaMA [14] on MNLI [16] with SINGLORA outperforms LoRA and LoRA+ (91.3% against 89.1% and 90.2% respectively) while using only 60% of their parameter budget. In image generation, SINGLORA increases the image fidelity of Stable Diffusion V1.5 finetuned on Dreambooth [12] by 5.4% compared to LoRA [9]."
        },
        {
            "title": "2 Related Efforts",
            "content": "2.1 LoRA As large multimodal models continue to scale, an increasing number of parameter-efficient finetuning (PEFT) techniques have been developed to facilitate their adaptation to downstream tasks. LoRA has emerged as one of the most popular adapters, offering efficient model adaptation across various domains. It has been studied extensively, resulting in numerous variations. DyLoRA [15] and AdaLoRA [21] focus on rank adjustment during training, with adaptive strategies to optimize rank allocation. LoHA [7] and LoKR [2], propose architectural extensions and, respectively, leverage Hadamard and Kronecker products of two rank approximations to obtain more expressive adapters. Delta-LoRA [23] modifies LoRA by updating pre-trained weights using differences in successive low-rank updates, and LoRA-Drop [22] selects the most impactful LoRA adapters to reduce computational cost. More recently, Weight-Decomposed Low-Rank Adaptation (DoRA) [9] decomposes the pretrained weight matrix into magnitude and direction, before employing LoRA to update the direction. The proposed approach is complementary to these architectural extensions and can be seamlessly integrated with them, further enhancing their effectiveness. 2.2 Stable extensions of LoRA recent line of research has identified fundamental limitations in standard optimizers, such as Stochastic Gradient Descent (SGD) and Adam, to finetune LoRA modules. LoRA+ [4] shows that the matrices and should be optimized with different learning rates to ensure stable learning dynamics. Building on this theoretical foundation, Zhang et al. [20] have proposed to use Riemannian gradient descent and conditioning methods to stabilize the optimization process. Recently, LoRADoneRite [19] identified the multiplicity of possible optimizer updates for single low-rank adapter as the source of instability in LoRAs training. Building on these theoretical insights into optimization stability, we analyze the convergence properties of SINGLORA . We demonstrate that its streamlined low-rank adaptation paradigm naturally promotes stable and robust optimization, without requiring careful learning rate tuning or modifications to classical optimizers."
        },
        {
            "title": "3 LoRA’s Stability Issue",
            "content": "For pretrained weight matrix W0 Rdk, LoRA introduces low-rank update, = W0 + α BA (3) where Rdr, Rrk are trainable matrices with rank min(d, k), and α is scaling factor. During fine-tuning, only and are trained while W0 remains frozen. To preserve the 2 pretrained models behavior at the start of training, is initialized to zero while uses random Gaussian initialization. recent line of work [19, 4, 20] has highlighted that LoRAs training dynamics often suffers from instability issues, particularly as the model width, denoted n, increases. Based on the analysis of LoRA+ [4], we examine these stability challenges in an infinite-width environment [13, 17, 18, 3]. Specifically, we investigate how the learning rate should scale so that the changes in the model output between iterations, denoted , remain both bounded and non-vanishing as the network width grows. We refer to this property as stable features learning, expressed mathematically as = Θn(1). 3.1 Analysis of toy model In this subsection we extend the analysis of the toy model proposed in [4]. For more complete discussion of the theoretical foundations of the stable feature learning theory, we refer the reader to [4, 3]. Consider linear model : Rn Rn with rank-1 LoRA update where we have pretrained weight matrix W0 Rnn and trainable LoRA vectors b, Rn. The model, defined as (x) = (W0 + ba)x, is trained on input-output pairs (x, y), x, Rn with loss = 1 2 (x) y2. is minimized using gradient descent method with learning rate η. Without loss of generality, we assume W0 = 0 by defining = W0x. It holds, aL = (ft1(x) y)(xbt1) and at iteration t, the gradient updates are thus, at = at1 η(ft1(x) y)(xbt1), and bL = (ft1(x) y)(a t1x), and bt = bt1 η(ft1(x) y)(a t1x). (4) (5) To analyze stability, we examine how the network output changes between iterations, ft = ft(x) ft1(x) = (bta bt1a t1) = ηbt12x2[ft1(x) y] (cid:125) (cid:124) (cid:123)(cid:122) δ1 η(a (cid:124) t1x)2[ft1(x) y] (cid:125) (cid:123)(cid:122) δ2 + η2ft1(x) y2(a t1x)(b (cid:124) (cid:123)(cid:122) δ3 t1x)x (cid:125) Having ft = Θ(1) implies that at least one of the terms (δi updates to and significantly impact ft(x), both δ1 implies that the model keeps fixed, effectively training only b, and similarly if only δ2 is effectively trained. When both δ1 shown in [4]. t)i{1,2,3} is Θ(1). To ensure that = o(1) = o(1), only is guaranteed to be Θ(1), as must be Θ(1). This is because δ1 are Θ(1), it follows that δ and δ2 and δ2 3.2 Efficiency Analysis We analyze the scaling behavior with width using the γnotation. For parameter v, we define, = Θ(nγ[v]), (6) where γ[v] represents how scales with n. When applied to vectors, γ[v] indicates that all the entries of respect Eq. 6. For the product of vectors v, Rn, we follow key scaling rule: γ[vw] = γ[v] + γ[w] + 1, where the extra +1 comes from summing terms. As theorized in [17, 18], the initialization scheme of the weights and the learning rate should be adapted as function of the width of the network, n, to ensure efficient learning. We therefore assume that the model is trained with gradient descent procedure with learning rate that respects η = Θ(nc) for some R. is initialized with random Gaussian distribution scaled as Θ(n1/2) (known as Kaiming initialization [5]) and is initialized as 0. Although each component of a0 is Θ(n1/2), by the Central Limit Theorem (CLT), the term 0 = Θ(1), since it represents the sum of independent terms with variance Θ(n1). Starting from initialization where f0(x) = 0, efficient LoRA fine-tuning requires δ1 δ2 = Θ(1) and δ3 translates to the system, = Θ(1), = Θ(1) for all > 1, with ft(x) = Θ(1) for > 1. Using the γ notation, this + 2γ[bt1] + 2 = 0 (δ1 t1x] = 0 (δ2 t1x] = 0 (ft1(x) = Θ(1)) + 2γ[a γ[bt1] + γ[a = Θ(1)) = Θ(1)) (7) Simple calculations with this system yield = 1. Let us analyze the updates at each step t. At initialization, γ[a 0 x] = 0 by the CLT. Using an inductive argument, for > 0, the update equation for is bt = bt1 η(ft1(x) y)(a t1x). Analyzing each term yields, (ft1(x) y) = Θ(1), as we assume that the error of the model is bounded. t1x = Θ(1) by CLT. Multiplying these factors, the overall update to is thus of order of η, i.e. Θ(n1), implying that for > 0, bt = Θ(n1). Similarly for a, at = at1 η(ft1(x) y)(xbt1). Analyzing each term yields, (ft1(x) y) = Θ(1). xbt1 = Θ(1) by the vector product rule, γ[xbt1] = γ[x] + γ[bt1] + 1 = 0. Multiplying these factors, the overall update to is of order Θ(n1), thus maintaining γ[at] = 1/2. Applying the CLT, we finally obtain However, plugging bt = Θ(n1) and x = Θ(1) in the definition of yields ft(x) = Θ(n1), in contradiction with the assumption that ft(x) = Θ(1) for efficient learning. Therefore, LoRA is not stable as updates inherently scale differently with width. This inefficiency motivates the need for alternative approaches, such as using different learning rates for and b, as in LoRA+ [4], or reformulating the low-rank update, as in our research. = Θ(1)."
        },
        {
            "title": "4 SINGLORA : Low-Rank Adaptation with a Single Matrix",
            "content": "In this section, we first present SINGLORA core formulation (4.1), showing how it achieves parameter-efficient adaptation through symmetric low-rank update. We then analyze its training dynamics through simplified model (4.2), proving that, unlike LoRA, our approach guarantees stable feature learning by construction. We establish formal convergence guarantees under standard optimizers like SGD and AdamW (4.3), demonstrating that SINGLORA eliminates the need for specialized optimization techniques. Finally, we extend SINGLORA to non-square weight matrices (4.4), making it applicable across diverse neural architectures. 4.1 SINGLORA formulation SINGLORA reformulates low-rank adaptation by replacing the traditional two-matrix decomposition with single learnable matrix. Given pretrained model with frozen weight matrix W0 Rnn, SINGLORA computes the adapted weights as, W0 + α u(t)AA, (8) where Rnr is low-rank trainable matrix with rank n, α is scaling factor, and u(t) is scalar function controlling the adaptation rate at optimization step t. This formulation provides two key advantages: (1) it eliminates inter-matrix scale conflicts by construction, ensuring stable optimization, and (2) it reduces the parameter count by roughly half compared to standard LoRA. Initialization scheme. To enable effective gradient flow during training, we initialize with Kaiming distribution. To preserve the behavior of the pretrained model at initialization, we require u(0) = 0, ensuring that the model starts from the pretrained weights. In practice, we adopt simple , 1(cid:1), where controls the adaptation rate. ramp function for u(t), namely u(t) = min (cid:0) provides smooth transition from the pretrained weights to the adapted model, allowing for gradual incorporation of task-specific features while maintaining stability during the early training stages. 4.2 Efficiency Analysis of Toy model Similarly to Section 3, we analyze toy example with the proposed single-matrix formulation (using only instead of both and b). Formally, (x) = (W0 + u(t)aaT )x, where W0 Rnn represents the pretrained weights and Rn is the trainable vector. Like in Section 3, we consider single training sample (x, y) with the loss = 1 2 (x) y2 which is minimized using gradient descent procedure with learning rate η = Θ(nc). The gradient aL and its update at step are respectively given by, aL = u(t) [(a t1(ft1(x) y)x] and by at = at1 ηaL. t1x) (ft1(x) y) + Scaling Analysis. For stable feature learning, we require ft(x) = Θ(1). As W0 remains frozen, this means u(t) at (a x) scales as Θ(n2p+1). x) = Θ(1). If at has entries of order Θ(np), then at(a For stability, this implies 2p + 1 = 0, yielding = 1/2. To ensure the stability of the optimization, at should thus maintain entries of order Θ(n1/2). Let us analyze the factor of the gradient update: η = Θ(nc). aL = Θn(1) since (f (x) y) = Θn(1), as we assume the error of the model to be bounded, and aT t1x = Θ(1), aT t1(f (x) y) = Θ(1) by CLT. The gradient update thus scales as Θ(nc) and we require = 1/2 to maintain at with entries Θ(n1/2). Therefore, setting η = Θ(n1/2) ensures stable feature learning. Unlike LoRA where balancing two matrices requires careful learning rate tuning, SINGLORA achieves stable feature learning by design, as we can always set an appropriate learning rate scale to ensure ft = Θ(1). This characteristic guarantees efficient feature learning in the infinite-width limit. 4.3 Transformation Invariance of Low Rank Adapters Here, we extend the analysis of the previous subsection to more general setting, and show that model adapted with SINGLORA and finetuned using standard optimizers, such as SGD, achieves stable feature learning. Observing that pairs of matrices (A1, B1) and (A2, B2) representing the same adapter, i.e. A1B1 = A2B2, can produce different optimizer updates, Yen et al. [19] recently introduced the notion of transformation-invariance for low rank adapters. Definition 1. Transformation-Invariance Let (A1, B1) and (A2, B2) be LoRA adapters satisfying A1B1 = A2B2. An optimizer is transformation-invariant if its updates (δA1, δB1) and (δA2, δB2) satisfy, (A1 + δA1)(B1 + δB1) = (A2 + δA2)(B2 + δB2) (9) To illustrate how the multiplicity in LoRAs parametrization can yield different gradient descent updates, we consider two parameterizations (A1, B1) and (A2, B2) of low-rank adapter related by scaling factor R. Namely, A2 = A1, B2 = 1 B1. (10) A1. Therefore, Defining = A1B1 = A2B2 and applying the chain rule yields A1 = ZB1, A2 = ZB2, where stands for the gradient of the loss of the model with respect to the parameter . We can thus rewrite, A2 = 1 ZB1 = 1 δA1B1 = η A1B1 = η A2B1 = s2 δA2B2, making Eq. 9 unsatisfied in the general case. This example reveals why LoRA exhibits unstable training dynamics when using optimizers that are not transformation-invariant: when the scaling factor is much larger than 1, the matrices and (and their corresponding updates) operate at vastly different scales. This scale mismatch creates fundamental problem: first-order optimizers using single learning rate struggle to achieve stable feature learning, as they cannot simultaneously accommodate both large and small-scale updates effectively. This issue frequently arises during training since LoRAs matrices and are typically initialized with different scales. Theorem 1. Any transformation invariant optimizer applying the same update rule for and achieves efficient feature learning. proof is presented in the appendix. Recent efforts [19, 20] attempt to address the stability issues of LoRA by building dedicated scale-invariant optimizer. In contrast, SINGLORA formulation inherently solves those challenges 5 Dataset Method RoBERTa Acc. (%) Params QQP QNLI MNLI Mean LR 2e4 (2e4, 4e3) 5e4 1e3 4e4 (2e4, 4e3) 5e4 1e3 4e4 (5e5, 4e3) 5e4 1e3 LoRA LoRA+ DoRA Ours LoRA LoRA+ DoRA Ours LoRA LoRA+ DoRA Ours LoRA LoRA+ DoRA Ours 88.5 89.1 89.2 88.9 90.9 92.1 92.1 92. 85.6 86.5 86.4 86.5 88.3 89.2 89.2 89.2 LR 4e4 2e4 0.15M 0.15M (2e4, 4e3) 0.16M 0.075M 5e4 1e 0.15M 0.15M 0.16M 0.075M 0.15M 0.15M (2e4, 4e3) 0.16M 0.075M 5e4 1e3 0.15M 0.15M 0.16M 0.075M GPTAcc. (%) Params 87.9 89.1 89.2 88.8 81.3 82.0 82.2 82.5 84.6 85.6 85.7 85. 1.78M 1.78M 1.78M 0.89M 1.78M 1.78M 1.78M 0.89M 1.78M 1.78M 1.78M 0.89M Table 1: Accuracy of RoBERTa and GPT-2 fine-tuned on GLUE datasets with rank 8 updates. LoRA+ uses learning rates (µA, µB). GPT-2 results on QNLI were not reported in [4]. and SINGLORA can be efficiently tuned with standard deep learning optimizers, such as SGD or Adam [8], without requiring special modifications or careful hyper-parameters tuning. Theorem 2. gradient descent optimizer is transformation-invariant for SINGLORA . proof is presented in the appendix. Theorem 1 and Theorem 2 guarantee the existence of learning rate yielding stable dynamics when training SINGLORA with first-order optimization methods. 4.4 Extension to Non-Square Matrices Our discussion has so far focused on square weight matrices W0 Rnn which are commonly used in the attention layers of transformer architecture. We now extend our approach to rectangular weight matrices W0 Rdindout. Without loss of generality, we assume din < dout. Considering low rank matrix Rdoutr we define Rdinr as truncation of consisting of its first din rows. The adapted layer is then computed by W0 + AA. Training this adapter with standard optimizers preserves the stability and transformation-invariance properties demonstrated for the square case. Theorem 3. The generalization of SINGLORA to non-square matrix preserves the stability and transformation-invariance properties demonstrated for the square case. proof is presented in the appendix."
        },
        {
            "title": "5 On the expressiveness of SINGLORA in Transformer Architectures",
            "content": "In the previous section, we demonstrated that SINGLORA improves the optimization process and yields more stable training dynamics. We now turn to investigating its expressive capacity within the Transformer architecture, which serves as the foundation of most current NLP and vision models. We analyze how SINGLORA symmetric updates affect the key-query interaction in self-attention layers. For input RLd, the attention mechanism is computed as Attention(Q, K, V) = softmax (cid:19) (cid:18) QKT dk V, where = XWq, = XWk and = XWv. (11) 6 Figure 1: Synthetic experiment: convergence plot for LoRA and SINGLORA . When applying SINGLORA , the weight matrices become, Wq = W0 + AqAT and Wk = k + AkAT Examining the key-query interaction QKT , we get QKT = X[W0 qW0T + W0 qAkAT + AqAT W0T + AqAT AkAT ]XT . (12) (13) The crucial insight that emerges is that although SINGLORA uses symmetric updates (AqAT and AkAT ), their interaction in the computation of attention is more general. The product of two )(AkAT symmetric matrices (AqAT ) is not necessarily symmetric unless they commute. Since there is no constraint forcing AqAT and AkAT to commute, SINGLORA can learn general (nonsymmetric) transformations of attention patterns despite its symmetric parameterization and does not fundamentally limit the models ability to learn diverse attention patterns. Synthetic experiment. To empirically verify this property, we implement LoRA and SINGLORA in an attention mechanism, where they learn to approximate target attention pattern given input + AqA X. Attention scores are computed according to 11. For SINGLORA , we define Wq = 0 + ˆBq ˆAq , where both Wq, Wk R128128. For LoRA, we define ˆWq = ˆW 0 + AkA and Wk = 0 and ˆWk = ˆW 0 + ˆBk ˆAk, also with ˆWq, ˆWk R128128. Both approaches are configured to use the exact same number of parameters for fair comparison. We optimize both models using an identical AdamW configuration with learning rate of 104 for 15, 000 iterations, minimizing the loss XWqW 2. As shown in Figure 1, SINGLORA outperforms LoRA in both convergence speed and final approximation accuracy which drops to approximately 105 while LoRA remains around 102, in fewer iterations. We verified these results with 1K different random seeds to sample and X. This experiment shows how SINGLORA maintains expressiveness in attention mechanisms while enhancing optimization dynamics and performance. Z"
        },
        {
            "title": "6 Experiments",
            "content": "We conduct extensive experiments to evaluate SINGLORA in low-rank adaptation for linguistic and visual tasks. Additional experiments and ablative studies on the ramp-up function, and influence of ranks are presented in the appendix. Code will be released upon publication. 6.1 Language Models To evaluate SINGLORA in Natural Language Processing, we consider selected comprehension and reasoning tasks from the General Language Understanding Evaluation (GLUE) benchmark[16]. We 7 Method LoRA LoRA+ DoRA SINGLORA Accuracy # Params 89.1 20M 90.2 20M 90.6 21M 91.3 12M Table 2: Accuracy of Llama tuned on MNLI with SINGLORA and baselines with ranks 8. Figure 2: Accuracy of Llama-7B fine-tuned on MNLI across different learning rates. The plot compares the stability of LoRA and SINGLORA under varying learning rates, demonstrating that SINGLORA accuracy fluctuates by approximately 1%, while LoRAs performance varies by 4.8%. These results highlight the robustness of SINGLORA optimization dynamics. strictly follow the NLPs experimental protocol of LoRA+, which similarly to SINGLORA addresses LoRAs training stability issues and thus serves as key benchmark. To fairly quantify algorithmic differences rather than hyper-parameter tuning advantages, we adopt their training/evaluation codebase, model architectures, modified layers, optimization settings, and training duration. We also compare to DoRA [9], recent state-of-the-art variation of LoRA. RoBERTa-base and GPT-2. We first evaluate each approach based on its ability to fine-tune smaller language models - RoBERTa-base and GPT-2 - on the MNLI, QQP, and QNLI tasks from the GLUE benchmark. Following the LoRA+ setup, we set = α = 8. We use u(t) = min( 103 , 1) where is the training step. Table 1 summarizes the accuracy in these tasks. Accuracies reported for LoRA and LoRA+ are directly taken from the original paper of LoRA+ [4]. Results show that SINGLORA outperforms LoRA, achieving 0.9% mean accuracy improvement for RoBERTa and 1.1% for GPT-2. It also achieves slightly better performance than LoRA+ and DoRA, while using only half the number of trainable parameters of both baselines. Note that while LoRA and LoRA+ explore multiple learning rates (5 for LoRA and 25 for LoRA+) and report results using carefully selected learning rates for each dataset and model, SINGLORA employs single learning rate across all experiments. This indicates that our methods stability reduces the need for extensive hyperparameter tuning, including the exhaustive grid search required for LoRA+. We further analyze the robustness to learning rate variations in Subsection 6.3. Llama 7B. To further validate our approach, we fine-tune large language model (LLM), LLaMA7B, on the MNLI task. Table 2 shows that SINGLORA outperforms LoRA (by more than 2%), LoRA + (by more than 1%) and DoRA, while reducing the number of training parameters by 40%. Since fine-tuning LLMs such as LLaMA is one of the most common applications of low-rank adapters, this result underlines the practical advantages of our approach. 6.2 Image Generation To showcase the versatility of SINGLORA in architectures and modalities, we evaluated its effectiveness in diffusion-based image generation, where LoRA is commonly used to fine-tune models in small-scale datasets of specific subjects. We consider Stable Diffusion V1.5 [11], popular image diffusion model. Dreambooth. We benchmark the approaches on DreamBooth [12], known dataset with 30 classes of objects and animals, each containing 45 training images and 25 evaluation prompts. Following 8 Figure 3: Qualitative comparison of LoRA, DoRA and SINGLORA on Dreambooth. Method CLIP Image CLIP Text DINO Similarity Rank # Params LoRA LoRA+ DoRA SINGLORA SINGLORA 0.677 0.688 0.687 0.677 0.690 0.319 0.315 0.317 0.318 0.317 0.143 0.150 0.148 0.141 0.151 8 8 8 8 0.9M 0.9M 0.9M 0.45M 0.9M Table 3: Performance of Stable diffusion V5 finetuned in Dreambooth with same number of parameters. Similarity of the generated image with the originals was measured using CLIP Image and DINO Similarity. Prompt fidelity is evaluated with CLIP Text. standard practice, we train each model for 400 iterations using the template prompt \"a photo of sks <class>\", where \"sks\" is rare English token. This setup allows the model to learn new object representations while retaining its general capacities. We finetune the query and key projections of the attention matrices in the U-Net component of Stable Diffusion. For all methods, training is conducted for 400 epochs using learning rate of 103. Figure 3 presents qualitative comparison between the methods, illustrating how our adaptation approach enhances subject learning. For instance, note that the shoe in the second row retains its iridescent color, whereas other methods fail to do so. Quantitatively, Table 3 demonstrates that SINGLORA achieves 5.4% improvement in the DINO [1] similarity score compared to DoRA, indicating better object resemblance, while maintaining prompt fidelity measured by the CLIP [10] Text score. Additional image generation experiments are presented in the appendix. 6.3 Stability of SINGLORA To validate the optimization stability analysis of SINGLORA presented in Section 4.3, we compare its performance against LoRA in range of learning rates. Specifically, we fine-tune Llama-7B in MNLI using learning rates ranging from 5 105 to 104. As shown in Figure 2, the precision of SINGLORA fluctuates by approximately 1%, whereas LoRA exhibits larger variation of up to 4.8%. These empirical results validate our theoretical findings, demonstrating that the design of SINGLORA inherently improves learning stability. In practice, this stability translates to simpler hyperparameter tuning, as our method maintains strong performance without requiring extensive searches for an optimal learning rate. Together with previous experiments, these findings highlight that SINGLORA not only offers more efficient and accurate parameterization, but also ensures robust convergence in practical settings."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced SINGLORA , novel formulation of low-rank adaptation that learns and employs single matrix instead of two. Through theoretical analysis, we demonstrated that the proposed design inherently eliminates the inter-matrix scale disparities present in LoRA and guarantees stable feature learning without requiring special optimizers or careful hyperparameter tuning. Extensive experiments on language and vision tasks validated these benefits, consistently demonstrating improved performance with fewer trainable parameters than LoRA and its variants. Since our approach is complementary to various LoRAs variants, suchs as DoRA [9], promising direction is to explore their integration, harnessing their independent strengths to further enhance efficiency and performance."
        },
        {
            "title": "References",
            "content": "[1] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [2] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James Clark, and Mehdi Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650, 2022. [3] Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on deep neural networks training. In International conference on machine learning, pages 26722680. PMLR, 2019. [4] Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation of large models. International Conference on Machine Learning, 2024. [5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [6] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [7] Nam Hyeon-Woo, Moon Ye-Bin, and Tae-Hyun Oh. Fedpara: Low-rank hadamard product for communication-efficient federated learning. International Conference on Learning Representations, 2021. [8] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [9] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. International Conference on Machine Learning, 2024. [10] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [11] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [12] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [13] Samuel Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016. [14] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [15] Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. Findings of the Association for Computational Linguistics, 2022. [16] Alex Wang. Glue: multi-task benchmark and analysis platform for natural language understanding. International Conference on Learning Representations, 2018. 11 [17] Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466, 2022. [18] Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs vi: Feature learning in infinite-depth neural networks. arXiv preprint arXiv:2310.02244, 2023. [19] Jui-Nan Yen, Si Si, Zhao Meng, Felix Yu, Sai Surya Duvvuri, Inderjit Dhillon, Cho-Jui Hsieh, and Sanjiv Kumar. Lora done rite: Robust invariant transformation equilibration for lora optimization. International Conference on Learning Representations, 2025. [20] Fangzhao Zhang and Mert Pilanci. Riemannian preconditioned lora for fine-tuning foundation models. arXiv preprint arXiv:2402.02347, 2024. [21] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation for parameterefficient fine-tuning. International Conference on Learning Representations, 2023. [22] Hongyun Zhou, Xiangyu Lu, Wang Xu, Conghui Zhu, Tiejun Zhao, and Muyun Yang. Lora-drop: Efficient lora parameter pruning based on output evaluation. arXiv preprint arXiv:2402.07721, 2024. [23] Bojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, and Lei Zhang. Deltalora: Fine-tuning high-rank parameters with the delta of low-rank matrices. arXiv preprint arXiv:2309.02411, 2023."
        },
        {
            "title": "A Additional experiments",
            "content": "A.1 Initialization and choice of In LoRA, is initialized using Kaiming distribution while is set to zero. This initialization scheme ensures that the model is equivalent to the pretrained one at the beginning of finetuning (since AB = 0), while maintaining effective gradient flow. For SingLoRA, we adopt similar approach by initializing with Kaiming distribution. To achieve equivalence to the original model at the beginning of finetuning, we rely on simple parametric ramp-up function defined as t/T , where represents the current training step and is hyperparameter, and consider the adapter, u(t)AA (14) To analyze the robustness of this initialization scheme, we conduct an ablation study focusing on the ramp-up function and the hyperparameter . Figure 4 demonstrates that SINGLORA is robust to the choice of , showing comparable of RoBERTas performance in GLUE across wide range of values (from 0.5% to 8% of the total number of training steps). In our main experiments, we set equal to 1% of the total number of training steps. Figure 4: Ablation study on the choice of the hyperparameter . The experiments shows that SINGLORA is robust to wide range of which thus does not require extensive hyperparameter search. A.2 Additional visual experiment: human faces To further analyze the ability of SINGLORA in visual tasks, we follow [20] and benchmark the adapters on dataset that includes 40 human faces. This experiment quantifies the expressive power of adapters and their ability to learn complex details present in human faces. Figure 5 shows subset of the dataset. Following standard practice, we train each adapter for 1500 iterations using the template prompt photo of sks human, where sks is rare English token. Each adapter has the same number of trainable parameters. We finetune the query and key projections of the attention matrices in the U-Net component of Stable Diffusion. Table 4 shows an improvement in the DINO similarity score compared to DoRA, indicating better similarity to the reference image. A.3 Hardware All experiments were performed on single NVIDIA A40 GPU with memory of 48GB. Figure 5: Samples from the dataset used in our experiment. The dataset was automatically generated with state-of-the-art image generation model trained on faces available in https: //this-person-does-not-exist.com/en. Method LoRA DoRA SINGLORA"
        },
        {
            "title": "DINO Similarity\nRank",
            "content": "0.463 8 0.471 8 0.501 16 Table 4: DINO Similarity of Stable Diffusion v1.5 tuned on specific face with the reference face. SINGLORA and baselines uses the same number of trainable parameters."
        },
        {
            "title": "B Proofs",
            "content": "In this section we demonstrate the theorem introduced in Sections 4 and 5. We rely on the following sufficient conditions for transformation invariance (see [20]), (i) (ii) (iii) δA1B1 = δA2B2 A1δB1 = A2δB2 δA1δB1 = δA2δB2 (15) B.1 Theorem 1 Any transformation invariant optimizer applying the same update rule for and achieves efficient feature learning. Here for completeness we present the proof proposed in [20]. Proof: Let A1 = Θ(na), B1 = Θ(nb), = Θ(nc), and η = Θ(nd), where η is the learning rate and denotes the network width. Since = A1B 1 , by the chain rule, we have = = ZB1A 1 . Given the symmetry of the update rule, the updates satisfy: δA1 = Θ(nx+a+(y+1)b+c+d), δB1 = Θ(nx+b+(y+1)a+c+d). Assuming the update rule is invariant under scalar reparameterization, we compare two equivalent decompositions A2 = nsA1 and B2 = nsB1, giving: δA1B1 = δA2B2. From this, it follows: xa + (y + 1)b + zc + = x(a + s) + (y + 1)(b s) + zc + d, which simplifies to: Hence, we deduce: Similarly, we find: xs (y + 1)s = 0 = 1. δA1B1 = Θ(na+(y+1)b+c+d) = Θ(na+b+yc+d). A1δB1 = Θ(na+(y+1)b+c+d) = Θ(na+b+yc+d). 14 Given that these expressions are equal, the update process enables efficient feature learning: δA1B1 = A1δB1 = Θ(1), by selecting proper learning rate η = Θ(nd), where = 1 is fixed and is chosen accordingly. B.2 Theorem 2 gradient descent optimizer is transformation-invariant for SINGLORA . Proof: 1 = A2A We prove that the three sufficient conditions for transformation invariance hold. Proof of (i): Assume two parametrizations of LoRA adapter, A1, A2 Rnr with identical ranks such that A1A 2 . From the Polar Decomposition Theorem, there exists an orthogonal matrix Rrr such that A1 = A2Q. Therefore, defining = A1A 1 and using the chain rule lead to, 1 = ηA1A δA1A = 2ηZA2A 1 = 2ηZA1A 2 = ηA2A 1 = 2ηZA2QQA 2 2 = δA2A 2 . The first sufficient condition for transformation invariance 15 is therefore satisfied. Condition (ii) holds by symmetry with the condition (i). Proof of (iii): δA1 δA1 = δA2 δA2 For the gradients with respect to A, the chain rule gives A1 = ZL A1 A2L = ZL A2 (16) (17) where = AiA there exists matrix such that, is the output of the adapted layer. Using the former relation and the Polar Theorem, A1L = ZL A1 = (ZL A2)Q = A2 Hence, the update for becomes δA1 = η A1 = η A2L = δA2 Q. Now, consider the product of the updates: δA1 δA 1 = (δA2 Q)(δA2 Q) = δA2 (QQ) δA 2 = δA2 δA 2 , since QQ = I. This completes the proof of (ii). B.3 Theorem 3 (18) (19) (20) The generalization of SINGLORA to non-square matrix preserves the stability and transformationinvariance properties demonstrated for the square case. Proof: Recall that for rectangular weight matrix, W0 Rdindout (din < dout), low-rank adapter is defined via matrix Rdoutr, with its truncation Rdinr formed by the first din rows of A. Suppose that there exist two matrices A1, A2 Rdoutr such that their truncations satisfy 1A 1 = 2A 2 . For clarity, divide each Ai as Ai = (cid:21) (cid:20)Xi Yi , = 1, 2, 15 where Xi Rdinr and Yi R(doutdin)r. 2A By definition, the equality 1 = 1A 2 implies, X1X X1Y 1 = X2X 2 1 = X2Y 2 . Using the polar theorem, X2 and Y2 admit polar decomposition: X2 = X1Q Y2 = Y1Q. Where is symmetric orthogonal matrix. Denote the gradient descent updates for Ai as, By equation (22) we have, δAi = (cid:21) (cid:20)δXi δYi , = 1, 2. δA2 = (cid:21) (cid:20)δX1 δY1 . (21) (22) (23) We now demonstrate that the three sufficient conditions for transformation invariance hold. Proof of (i) and (ii): A1 δA 2 . Using equations (22) and (23), we get 1 = A2 δA 2 δA 2 = X1Q (cid:21) (cid:20)δX1Q δY1Q (24) (25) = [X1QQδX = [X1δX 1 X1δY 1 X1QQδY 1 ] (cid:20)δX1 δY1 1 ] = X1 (cid:21) = 1δA The proof for Proof of (iii): δA1 δA 1 = δA2 δA 1 δA1 = 2 δA2 is symmetric. 2 . Using equations (22) and (23), we get (cid:21) δA 2 δA 2 = δX1Q (cid:20)δX1Q δY1Q 1 δX1QQδY 1 ] = [δX1QQδX = [δX1δX 1δA = δA 1 1 δX1δY 1 ] = This completes the proof of iii."
        }
    ],
    "affiliations": [
        "Technion - IIT Haifa, Israel",
        "University Paris Dauphine Paris, France"
    ]
}