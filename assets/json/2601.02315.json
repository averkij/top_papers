{
    "paper_title": "Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping",
    "authors": [
        "Saurabh Kaushik",
        "Lalit Maurya",
        "Beth Tellman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \\href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 ] . [ 1 5 1 3 2 0 . 1 0 6 2 : r Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping Saurabh Kaushik1, Lalit Maurya2, Beth Tellman1 1Center for Sustainability and the Global Environment (SAGE), University of WisconsinMadison, Madison, WI, 53726 USA 2Portsmouth AI and Data Science Centre (PAIDS), School of Computing, University of Portsmouth, Portsmouth, PO1 3HE, UK skaushik8@wisc.edu, lalit.maurya@port.ac.uk, beth.tellman@wisc.edu"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as downstream task, GFMs struggles to outperform the baseline U-Net, highlighting models limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline UNet (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on Prithvi-CAFE Github These authors contributed equally. Geo-Foundation Models (GFMs) are built on selfsupervision techniques, primarily Masked Autoencoders (MAE) [9] [28][33] and contrastive learning [29] [7], to leverage the abundance of unlabeled remote sensing data emerging from diverse sensors [21]. GFMs present an alternative approach from longstanding application specific model to task-agnostic models that can produce reliable maps with sparse labels, owing to GFMs massively pretrained encoders [28][33]. In this quest, more than 50 large vision foundation models have been proposed [21], showing potential in domains such as wildfire, marine, agriculture, urban land cover, forest, and floods using community benchmark datasets, GeoBench [17] and PANGEA [22]. However, interesting observations arise in several domains including flood inundation mapping using Sen1Floods11 data [3]: current GFMs under-perform the baseline U-Net so far. This becomes even more intriguing given detailed experiments and comparisons by [22], highlighting the UNet [26] consistently outperforms all GFMs even in limiteddata scenarios (e.g., 10%, 50%, and 100% of training data). These results are critical for Earth scientists and the machine learning community regarding the utility of such large models with hundreds of millions of parameters demand high computational resources yet still under-perform compared to the baseline U-Net, much lighter model with only 31M parameters. These observations highlight the limitations of large pretrained GFMs in efficiently learning critical local representations compared to much simpler and lighter U-Net models. They underscore ample scope to advance GFM architectures in terms of capturing essential local nuances and supporting any number of input channels, especially in the case of the Prithvi GFM [28], which currently allows only six input channels. This restriction stems from its original pre-training on six Harmonized Landsat and Sentinel-2 (HLS) spectral bands using 4.2M training samples. Such an architectural design limits its ability to generalize across many Earth observation applications that leverage multiIn this pursuit, we opted spectral and multi-modal data. for the Prithvi GFM pretrained encoder, given its popularity in diverse Earth observation applications [28] and its adaptation beyond community-standard datasets like GeoBench [17] and PANGEA [22] for example, in glacial lake mapping [13] and debris-covered glacier mapping [14]. To address these limitations, we propose the PrithviComplementary Adaptive Fusion Encoder (CAFE), which can handle any number of channels. Our approach leverages the strengths of both Transformers and CNNs through complementary channel fusion. We pass six channels (on which Prithvi was pretrained originally) through the Prithvi encoder, and use an adapter-based fast and efficient finetuning method, reducing trainable parameters from 650M to 45.5M. All other channels are processed through CNN residual block and Convolutional Attention Module (CAM). To Integrates Transformer and CNN features, we adopt multiscale and multilevel attention-based fusion approach that preserves the most relevant gobal and longrange information. Mathematically,: Transformer: f1(xS1), CNN: f2(xS2 ), S1 S2 = The fusion module learns joint embedding: = ffuse (cid:0)f1(xS1 ), f2(xS2 )(cid:1) (xS1S2 ) i.e., approximating the full-spectrum feature map through learned cooperation. Our contribution are: We propose simple yet effective segmentation models that maximize critical local nuances while preserving longrange dependencies. We extend Prithvi-GFMs capability beyond six spectral channels to efficiently process any number of channels and produce reliable flood maps. We report state-of-the-art (SoTA) results on the benchmark flood dataset (i.e., Sen1Flood11) through efficient fine-tuning and effective multi-scale, multi-level attention fusion. 2. Related Work 2.1. Geo-Foundational Models GFMs remain the focus of large scientific community aiming to address long-standing limitations of task-specific models, 1) under-utilization of unlabeled remote sensing datasets, 2) exclusion of temporal components in Earth observation, 3) generating reliable maps with sparse labels and 4) underexplored capabilities of multimodal data. As result, we have witnessed the development of 58 remote sensing vision foundation models [21], offering unprecedented opportunities to push the boundaries of Earth observation. Unlike application-specific models, GFMs have proven efficient in various downstream tasks, including classification, semantic segmentation, change detection, and regression across diverse domains[28][33][5] [29][10][12][32] [10]. To assess the performance of these large vision models, the community has curated two primary benchmark datasets GeoBench [17] and PANGEA [22], to evaluate GFM performance across various domains such as forestry, floods, and land use/land cover. discussion of all 58 foundation models is beyond the scope of this article; therefore, we highlight some popular and recent models. Recently, Google released its AlphaEarth Foundation (AEF) model [4], the first of its kind to employ time-continuous embeddings by training on 8,412,511 video sequences collected from nine publicly available satellite sensors, high-quality land cover maps, and text, rather than treating satellite imagery as single timestamp. However, unlike other foundation models such as Clay [5], Prithvi [28], DOFA [33], Panopticon [30], and Galileo [29], the AlphaEarth model is not open-source, limiting its applicability for fine-tuning on specific Earth observation tasks. In contrast, many fully open-access GFMs can be fine-tuned for any Earth observation task, offering greater flexibility for monitoring short-lived changes or specific events. Among these, Clay GFM [5] is one of the largest, pretrained on 70 million image chips from seven diverse remote sensing sensors. Clay is based on an MAE ViT backbone, similar to Prithvi [28], DOFA [33], ScaleMAE [25], and Spectral-GPT [10]. Among these models, Prithvi 2.0 shows promising results on the GeoBench dataset, surpassing other models (DOFA ViT-300, DINO [31], Decur [32], Satlas [2], ScaleMAE [25]) in overall performance. However, while models like DOFA and Clay allow any number of input channel. Prithvis original architecture only supports the six spectral bands on which it was pretrained, limiting its applicability for Earth observation tasks requiring multimodal input data. Nevertheless, given its potential and ease of accessibility via TerraTorch [6], Prithvi remains one of the most popular models adapted for various Earth observation applications beyond community benchmarks (e.g., [14][13]). This motivates the present work to extend Prithvis capabilities through efficient fusion with CNN blocks, enabling support for any input channel and improved understanding of critical local information. Recently, generative models such as TerraMind [12], and DiffusionSat [15] have also been proposed, aiming to generate artificial data to fill gaps and improve performance using synthetically generated complementary data. Vision-language GFMs (VLGFMs) such as RemoteCLIP [20], RSGPT [11], and SkySenseGPT [34] have also emerged, pushing the boundaries of Earth observation by enabling users to interact with geospatial data 2 through natural language prompts. 2.2. Geo-Foundation Models beyond benchmark dataset GFMs have been extensively assessed on benchmark datasets [22] [17]; however, evaluation of GFMs outside benchmarks also bears great significance as it directly highlights the models performance on new domains, datasets, and extensive comparisons with state-of-the-art models. In this quest, Prithvi remains one of the most widely used models for various downstream tasks, including land cover mapping [18], demonstrated fine-tuning Prithvi GFM for land cover mapping and achieved an mIoU of 62.37 compared to baseline U-Net (36.36) and ViT (46.8). Prithvi GFM also found to effecient in glacial lake mapping. UViT [13] U-Net-style Vision Transformer (ViT) leveraging the Prithvi pretrained encoder and enhanced squeeze-andexcitation layers to incorporate multi-sensor data. similar study by [14] reported an 8% mIoU improvement in mapping debris-covered glaciers at global scale compared to U-Net. The successful implementation of Prithvi in completely different domain (Cryosphere) which is underrepresented in benchmark datasets, exhibits the models versatility for downstream tasks. Kostejn et al. [16] proposed the U-Prithvi model for flood segmentation, combining the strength of the pretrained Prithvi encoder with U-Net and evaluating the models performance on the Sen1Floods11 dataset. In U-Prithvi, input is passed through both U-Net and Prithvi encoder and subsequently fed to the decoder with skip connections. The model shows 5% improvement on geographically held-out test sites. 3. Method Here, we explained the architecture of our proposed PrithviCAFE as shown in the Fig 1. The Prithvi-CAFE architecture begins by dividing the input image tensor into two complementary parts: one directed to the Adapted Prithvi transformer and the other to the CNN branch. Selected spectral bands are allocated to the transformer to capture rich spectral features, while the remaining channels are processed by the CNN to extract detailed spatial information (Fig. 1 (a)). The Adapted Prithvi transformer incorporates per-block dynamic prompt adapters (Fig. 1 (e)), which enable efficient fine-tuning of frozen pretrained model using lightweight residual modules, significantly reducing computational cost (Fig. 1 (f)). Outputs from multiple transformer layers are reshaped from token sequences into twodimensional feature maps representing different semantic levels. These are refined through Feature Pyramid Networks that upsample and adjust channel dimensions to create hierarchical multi-scale representations (Fig. 1 (b)). In parallel, the CNN backbone processes its input through several Residual Blocks integrated with Convolutional Attention Modules, which enhance important feature channels and spatial regions. The resulting CNN feature maps are resized to match the transformer outputs, enabling effective fusion. The Multi-Scale Multi-Level Feature Attention Fusion module then adaptively merges contextual transformer features with fine spatial cues from the CNN using attention weighting (Fig. 1 (c)). Finally, the UperNet decoder integrates the fused features through pyramid pooling and lateral connections to generate precise and context-aware segmentation outputs (Fig. 1 (d)). 3.1. Complementary Feature Let the input tensor be denoted as:X RBCHW , where is the batch size, the number of spectral channels, and H, the spatial dimensions. To leverage the pretrained Prithvi transformer effectively, the input channels are split into two complementary subsets: = [XAP XCNN] (1) where XAP is the subset of spectral channels fed to the Adapted Prithvi encoder, and XCNN contains the remaining channels for the CNN pathway. Formally if, IAP = {1, 2, 3, 7, 11, 12}, ICNN = {1, . . . , C} IAP (2) then the input partitions become: XAP = X[:, IAP, :, :], XCNN = X[:, ICNN, :, :]. (3) This complementary feature separation ensures that the transformer processes spectrally-rich features, while the CNN branch captures fine-grained spatial patterns. By routing other informative channels to the CNN, the model digs rich contents and efficiently leverages Prithvis pretrained representations, which is particularly beneficial for multispectral data where some bands contribute differently with regards to local and global context, and balances spectral vs. spatial learning. 3.2. Parallel Backbones: Transformer + CNN Pathways 3.2.1. Transformer Backbone - Adapted Prithvi The transformer backbone employs Dynamic Prompt Adapter for parameter-efficient fine-tuning of Prithvi ViT blocks. In this design, lightweight learnable adapter is positioned at the beginning of each ViT (as shown in Fig. 1 (f)), dynamically adjusting prompts to guide task-specific adaptation while keeping the pretrained Prithvi encoder frozen. This arrangement reduces memory and computational cost, and enables efficient transfer to new datasets without full retraining, while preserving the models ability to capture long-range dependencies. Formally, let blk() denote Prithvi transformer block. The adapted block is defined as: = blk(x + fadapter(x)) (4) Figure 1. The proposed Prithvi-CAFE architecture. (a) The Spectral Selection module divides input images into two spectral branches. (b) The encoder processes these branches using adapted Prithvi blocks and CNN blocks, respectively. (c) The Multi-Scale Multi-Level Feature Attention Fusion (M2FAF) module merges features from both streams. (d) The Decoder integrates them via pyramid pooling and lateral connections to generate the segmentation mask. (e) Adapter modules are attached to each ViT block of Prithvi, as shown in (f). (g) Residual Blocks and (h) Convolutional Attention Modules (CAM) are used to enhance CNN features. fadapter(x) = σ (W2 σ(W1 + b1) + b2) (5) where, W1 Rd32, and W2 R32d. The adapter adds low-rank residual perturbation fadapter(x) (bottlenecked through 32 dimensions for efficiency), enabling the model to adapt representations without retraining the billions of parameters in Prithvi (600M parameters). Adapters thus provide low-rank correction, preserving pretrained distributions while tailoring representations to new datasets. Fine-tuning with adapters reduces GPU memory usage by 5080% compared to full unfreezing. From the transformer outputs, features are extracted at layers {7, 15, 23, 31} correspond to multi-level semantic RBNiCi are reshaped depths. These token maps AP back into 2D spatial tensors using token-to-image (T2I) transformation, where Ci is tokken embedding which is 1280 for Prithvi. The FPN modules apply convolutional upsampling to the selected transformer features to align spatial resolutions and progressively reduce channel dimensions for effective multi-scale fusion. Specifically, FPN1 performs an 8 upsampling using three ConvTranspose2d layers, reducing channels from 1280 640 320 160. FPN2 performs 4 upsampling, reducing channels from 1280 640 320. FPN3 applies 2 upsampling, reducing channels from 1280 640, while FPN4 maintains the spatial size using 1 1 convolution with 1280 channels. These hierarchical refinements enhance spatial detail recovery and ensure consistent feature alignment for downstream fusion and segmentation tasks. This process produces set of multi scale multi level transformer features: FAP = {FAP , FAP 2 , FAP 3 , FAP 4 } (8) AP RBCiHiWi , {7, 15, 23, 31}. (6) 3.2.2. CNN Backbone Each feature map is further refined via Feature Pyramid Networks (FPN) to align spatial resolutions and reduce channel dimensions for multi-scale fusion: The CNN Backbone processes XCNN through four Residual Blocks [8] with CAM. The Residual Block enhances feature learning through residual connections. Each residual block learns: FAP = FPNi(f AP ) (7) = σ(x) + (x; ) (9) 4 where (x; ) consists of two 3 3 convolutional layers followed by batch normalization and ReLU activation, along with shortcut path σ(.) that performs 1 1 convolution when input and output dimensions differ. This design allows the block to learn residual mappings, improving gradient flow and network stability. The CAM (Fig. 1 (h)) improves CNN feature representations by applying channel and spatial attention sequentially. Given an input feature F, channel attention generates weights using global average and max pooling, emphasizing important feature channels. Spatial attention then highlights important regions by combining pooled spatial maps and applying convolution. This dual attention mechanism adaptively focuses on what and where to attend in the feature map, enhancing feature discrimination, representation power, and overall performance in segmentation task. By reducing noise (e.g., atmospheric artifacts), CAM improves feature quality with minimal parameter overhead. The resulting process produces set of hierarchical CNN features: FCN = {FCN 1 , FCN , FCN 3 , FCN 4 } (10) 3.3. Multi-Scale Multi level Feature Attention Fusion (M2FAF) An attention-driven fusion block designed to unify contextual and fine-grained features across scales and levels. Adapted Prithvi features (after FPN) are fused with CNN features using attention-based weighting. After feature extraction, both backbones yield four-scale feature maps, Each CNN feature map is upsampled and projected to match the corresponding transformer feature resolution: ˆFCN = Conv11 (cid:0)Interpolate(FCN , size(FAP ))(cid:1) An attention mask is then computed for each scale: Ai = σ(Conv11([FAP ; FCN ])) (11) (12) where σ denotes the sigmoid activation and [; ] indicates channel concatenation. To stabilize the fusion, bias factor β is applied: attn = attni(1 β) + β, with β [0, 1]. (13) The final multi-scale fused representation is obtained as: , Ff use = attn iFAP i) ˆFCN +(1attn = 1, . . . , 4. (14) Thus, each fused feature Ff use adaptively combines contextual transformer information with spatially-dense CNN cues: Here, attn ), while attn 1 favors transformer features (FAP 0 favors CNN features ( ˆFCN ). 3.4. UperNet Decoder The UperNet Decoder (Fig. 1 (d))combines multi-scale features from backbone for segmentation mask genretaion. 5 It processes four feature maps, from high-resolution detailed maps to low-resolution semantic maps. The Pyramid Pooling Module (PPM) captures global context by pooling the smallest feature at multiple scales and merging them. Lateral connections (L4-L1) reduce channel dimensions of each feature map. Features are then fused progressively from the smallest to the largest, integrating high-level semantic information with fine-grained spatial details. Finally, convolutional layer refines the fused features to produce the final output, resulting in precise and context-aware predictions. 4. Experiments Dataset and Metrics. We evaluated our proposed PrithviCAFE on two high-quality flood datasets: Sen1Floods11 [3] and FloodPlanet [35], given the limited performance of most GFMs in flood mapping, where U-Net consistently outperforms all GFMs. This makes flood mapping an ideal challenge to test the models capability for efficient fusion of long-range information captured by transformers and detailed spatial information extracted by CNNs. Sen1Floods11 consists of 446 hand-labeled image-labels pairs using Sentinel-1 and Sentinel-2 data collected worldwide. This data set is one of the widely used benchmark data sets to evaluate the performance of GFM for flood mapping as downstream task, allowing direct comparison of our models performance with SoTA results. The second dataset, FloodPlanet, consists of 19 major flood events worldwide. This dataset represents one of the highestquality datasets for evaluating deep learning models, since all labels are manually mapped using high-resolution PlanetScope imagery. These high-resolution labels significantly improved model performance (15.6), even when relying on moderate-resolution satellite imagery (10 Sentinel2). Thus, combining these two datasets for evaluation introduces sufficient complexity for flood segmentation tasks. As an evaluation metric, we opted for standard segmentation metrics such as Intersection over Union (IoU) for the foreground class (flood in our case) mean Intersection over Union (mIoU). IoU measures the overlap between the predicted segmentation and the ground truth, calculated as the ratio of their intersection to their union. We also computed mean Dice score, also known as the m-F1 score, which represents the harmonic mean of precision and recall, providing balanced measure of accuracy for imbalanced classes. Implementation Details. To implement Prithvi-CAFE on Sen1Floods11, we used the exact given train, test, and validation split given by [3] to keep the results fairly comparable. FloodPlanet consists of 298 Sentinel-2 images paired with PlanetScope-derived labels. The original data set had inconsistent image sizes close to 320 px; therefore, we resized all images and labels to consistent 320320 before feeding them to the model. Since FloodPlanet does not have predefined split, we performed 4-fold cross-validation, where 70 of the data was used for training, 10 for validation and 20 for testing in each split. This approach provides large number of heterogeneous examples for testing and reduces models bias toward any particular use case. We used cross-entropy loss, the AdamW optimizer for model convergence, and StepLR scheduler. To optimize hyperparameters such as learning rate, optimizer parameters (weight decay), and scheduler parameters (step size and gamma), we ran 20 trials, each with 60 epochs and patience of 10 for early stopping. The best hyperparameters obtained on validation data were used for full model training with patience level of 20 for early stopping. To ensure comparability across experiments, we fixed the random seed to 42, used batch size of 8 and conducted all experiments on an NVIDIA RTX A6000 GPU. 5. Results and Discussion 5.1. Models evaluation on Sen1Floods11 Our proposed Prithvi-CAFE performed very well on the Sen1Flood11 [3] test set, achieving 83.41 IoU water, outperforming most GFMs, including Prithvi-600M (82.50), Prithvi-300M (82.20), TerraMind (82.90), DOFA (81.54), and other recently proposed models including DeepSAR [27] (72.22) and MM UNet [24] (73.84)  (Table 1)  . UNet shows only marginal improvement (0.63) compared to Prithvi-CAFE. In the case of geographically held-out test site (i.e., Bolivia), Prithvi-CAFE surpasses the baseline U-Net by 10.8 IoU, and recently proposed U-Prithvi (which also leverages U-Net with Prithvi encoder) by 1.69, and the original Prithvi by 9  (Table 1)  . PrithviCAFE further outperforms adapter-based methods, achieving mIoU 88.87 compared to ViT Adapter (84.94) and LoRA Adapter (87.57)  (Table 1)  . These findings highlight that our proposed model demonstrates strong spatial In our Fusion transferability compared to other models. Figure 2. The effect of bias factor β on Transformer (semantic) and CNN (spatial) feature fusion mechanism, the bias factor β (Eq. 11) control the stability and reliability of the attention-based fusion between Transformer (semantic) and CNN (spatial) feature streams. We systematically tuned β within the range 0.2 to 0.8 and observed that higher bias values consistently improved fusion In particustability and downstream accuracy  (Fig. 2)  . lar, β = 0.8 yielded the best results on Sen1Floods11, indicating that giving slightly more emphasis to Prithvis transformer-based embeddings, while still allowing the attention module to incorporate complementary CNN spatial details, produces the most robust and discriminative multiscale representations. Performance Table 1. evaluation of Prithvi-CAFE on Sen1Flood11 compared with state-of-the-art methods. References given next to model depict source of the results. dash () indicates that the value was not reported in the respective study. Model Prithvi 2.0 600M [28] Prithvi 2.0 300M [28] TerraMindv1-B MM UNet [24] DeepSARFlood [27] U-Prithvi [16] RemoteCLIP [22] GFM-Swim [22] S12-DINO [22] Spectral GPT [22] DOFA [22] U-Net Base [19] U-Net [16] DeCUR Full FT [23] Prithvi 2.0 300M LoRA [23] Prithvi 2.0 300M ViT Adapter [23] Prithvi-CAFE (Ours) IoU mIoU m-F1 94.80 90.30 82.50 97.60 89.70 82.20 95.01 90.60 82.90 73.84 72.22 89.73 82.22 83.83 72.26 55.18 82.51 72.60 52.36 93.75 88.61 80.25 89.07 81.02 94.20 89.37 81.54 90.80 95.40 84.03 88.84 80.69 86.87 90.04 88.52 97.80 83.41 90.50 Params (M) 650 319 103 83.83 82.51 85 600 410 31 31 25 5.5 20 45. Sen1Flood11 Bolivia Prithvi 1.0 [16] U-Prithvi [16] U-Net Base [19] DeCUR Full FT [23] Prithvi 2.0 300M LoRA [23] Prithvi 2.0 300M ViT Adapter [23] Prithvi 2.0 300M Full FT [23] Prithvi-CAFE (Ours) 72.42 79.68 70.57 81.37 82.89 87.70 82.54 85.84 87.57 84.94 82.07 88.87 96.87 31 25 5.5 20 300 45. 5.2. Models evaluation on FloodPlanet The comparative evaluation on FloodPlanet [35] reveals strong performance by Prithvi-CAFE, achieving the highest IoU-Water (64.70), highest mIoU (68.74), and best mF1 score (81.45) while using only 45.5M trainable parameterssignificantly fewer than large foundation models such as Prithvi-2.0-600M (650M) and Prithvi-2.0-300M (319M), which obtain 61.91 and 62.03 IoU-Water, respectively  (Table 2)  . TerraMind also shows competitive performance with 62.33 IoU, whereas U-Net falls short at 60.14, DOFA at 59.15, TransNorm [1] at 60.19, and UViT (which also uses the Prithvi encoder) at 59.16  (Table 2)  . Prithvi-CAFE consistently outperforms these approaches by substantial margin of 2.57.2 improvement in mIoU and 1.69 points in F1. These findings align with the Sen1Floods11 test results on held-out test site, where Geo-Foundational ModFigure 3. Box plot showing distribution of mIoU per image in four fold cross validation on FLoodPlanet els (GFMs) demonstrated superior generalization to completely unseen locations compared to U-Net  (Table 1)  . The proposed Prithvi-CAFE model excelled in both scenarios, highlighting the combined advantage of pretrained transformer encoder with CNN-derived features. Overall, the results highlight the effectiveness and efficiency of PrithviCAFE for flood segmentation under heterogeneous conditions. The box plot showing distribution of mIoU per imTable 2. Comparison of state-of-the-art models on 4 fold cross validation. Mean values followed by standard deviations in parentheses. Model IoU Water Prithvi 2.0 600M 61.91 (0.04) Prithvi 2.0 300M 62.03 (0.02) 62.33 (0.01) TerraMind 60.14 (0.01) U-Net 59.15 (0.03) DOFA 60.09 (0.04) TransNorm 59.16 (0.04) UViT 64.70 (0.02) Prithvi-CAFE mIoU 65.05 (0.04) 66.05 (0.02) 66.19 (0.01) 64.56 (0.01) 61.52 (0.02) 64.80 (0.04) 63.19 (0.04) 68.74 (0.03) m-F1 76.07 (0.03) 79.83 (0.01) 79.53 (0.01) 75.09 (0.01) 74.22 (0.02) 74.95 (0.04) 72.23 (0.04) 81.45 (0.02) Params (M) 650 319 103 31 116 103 45.5 age across each split demonstrates consistent performance by Prithvi-CAFE, with standard deviation of 0.02, highlighting the models strong generalization capability under heterogeneous conditions  (Fig.3)  . In contrast, other models such as TransNorm exhibit relatively higher standard deviation of 0.04; as the figure indicates, TransNorm performs well on splits 1 and 2 but shows significantly lower performance on splits 3 and 4  (Fig. 3)  . Other large foundation models, such as Prithvi-2.0-600M and TerraMindBase, have standard deviations of 0.03 and 0.14, respectively, TerraMind-Base and U-Net report the lowest standard deviation (0.01) among all. The qualitative analysis also confirms the consistent outperformance of Prithvi-CAFE compared to other models. Fig. 4 depicts several examples where Prithvi-CAFE performs better than the original Prithvi GFM, U-Net and TerraMind. The most interesting observations (Fig. 4 (c), (f), and (h)) highlight cases where other GFMs such as Prithvi2.0-600M, TerraMind, and DOFA capture only the overall flooding pattern, while U-Net captures fine local deFigure 4. Comparative visual analysis of the proposed PrithviCAFE against other models using FloodPlanet data. Numbers in the lower-right corner indicate mIoU. tails. In contrast, our proposed Prithvi-CAFE successfully leverages both long-range and detailed local information, outperforming traditional CNNs and large GFMs. These observations emphasize the effectiveness of our approach in fusing long-range information from the transformer encoder with enhanced local spatial details extracted by parallel CNN blocks. In addition to the final segmentation results, we also visualize the feature embeddings generated by different encoders, including Prithvi-2.0-600M, DOFA, TerraMind, and our Adaptive Prithvi  (Fig. 5)  . The visualization shows that using the efficient and fast fine-tuning method of Adaptive Prithvi, differentiation between the two classes becomes clearly visible while fine-tuning only 7 of the total encoder parameters. Although these embeddings are not the final segmentation masks, they provide reasonable indication of how the encoder distinguishes between classes. These embeddings are subsequently passed to the decoder to generate the final segmentation mask. 5.3. Ablation Study Table 3 summarizes the ablation study conducted on FloodPlanet dataset to evaluate the impact of different modules within our proposed Prithvi-CAFE segmentation framework. The first row shows the baseline configuration with7 to investigate the impact of different CNN configurations within the CNN module of Prithvi-CAFE on segmentation performance  (Table 4)  . The results show clear trend: smaller channel configurations underperform due to insufficient feature representation capacity, while intermediate configurations improve performance but do not fully exploit the models potential. The highest configuration ([128, 256, 512, 1024]) achieves the best performance, matching the optimal full potential of Prithvi-CAFE. 5.4. Modes of failure Overall, the proposed Prithvi-CAFE performed well across heterogeneous testing sites; however, the model still shows some limitations, particularly in cases of dense cloud cover (Fig. S1). Fig. S1 (see Supplementary) explicitly illustrates several cloudy scenarios and misclassifications by all models. Even under these challenging conditions, Prithvi-CAFE performs relatively better than other models. These observations highlight the limitations of large GFMs in capturing detailed local information, with significant misclassification occurring in cloud-shadow regions. In contrast, PrithviCAFE efficiently incorporates long-range dependencies and critical local details, giving it an edge over other models. It is noteworthy that FloodPlanet labels were originally derived from high-resolution PlanetScope imagery (3m), indicating that our model can capture very fine details where other models struggle. To further improve PrithviCAFE, we suggest incorporating SAR data as input, since the model can handle any number of channels. The complementary information provided by SAR is expected to enhance flood mapping capabilities under cloudy conditions. 6. Conclusion Most GFMs struggle to outperform U-Net in the flood segmentation task using the benchmark Sen1Floods11 dataset, highlighting their limitations in capturing critical spatial nuances. To address these challenges, we propose Prithvi-CAFE, which leverages the Prithvi pretrained encoder, fast and efficient fine-tuning method using adapters, and dual-path architecture: the six bands on which Prithvi was originally pretrained are passed to the Transformer block, while the remaining channels are processed through parallel CNN block (Residual + CAM). multi-scale, multi-stage attention-based fusion mechanism then combines long-range information from the Transformer with detailed local information from the CNN block. The proposed Prithvi-CAFE outperforms SoTA improvement results on Sen1Floods11, with significant (6.63 mIoU) observed on geographically held-out test sites. Results on FloodPlanet further confirm the consistent superior performance of Prithvi-CAFE (2-7 mIoU) compared to other GFMs and U-Net. Visual analysis demonstrates that Prithvi-CAFE efficiently retains both long-range and local information, capturing fine details where other GFMs struggle. Our detailed observations also highlight dense Figure 5. Visualization of feature embedding of FloodPlanet data using t-SNE plots. (a) fully fine-tune Prithvi 2.0 encoder (600M), (b) Adaptive Prithvi fine-tuning (only 45.5M parameters are trained) (c) fully fine-tune TerraMind-Base encoder, (e) fully fine-tune DOFA-Base encoder. Table 3. Ablation study on FloodPlanet dataset: effect of different modules of Prithvi-CAFE segmentation performance Prithvi CNN Module M2FAF mIoU mDice w/o Adapter Adapted Res Block CAM 67.06 65.43 66.83 66.91 68. 79.65 76.81 78.53 78.84 81.45 out Prithvi adaptation but including the CNN enhancement and M2FAF modules. This setup achieves moderate performance (67.06 mIoU, 79.65 mDice), highlighting the benefits of convolutional refinement and feature fusion, even without adapter tuning. When only the Prithvi adaptation is enabled (second row), performance decreases, indicating that adaptation alone is insufficient without complementary feature enhancement. Adding the residual block or CAM individually (third and fourth rows) gradually improves the results, showing that each component contributes meaningful spatialchannel refinement. The final configuration, which integrates all modules Prithvi adaptation, residual block, CAM, and M2FAF, achieves the highest performance. We further conducted an ablation study Table 4. Ablation study of CNN Module Channel Configuration Configuration mIoU mDice [32, 64, 128, 256] [64, 128, 256, 512] [128, 256, 512, 1024] 65.03 66.31 68.74 76.45 78.12 81.45 8 cloud cover as limiting factor for model performance."
        },
        {
            "title": "References",
            "content": "[1] Reza Azad, Mohammad Al-Antary, Moein Heidari, and Dorit Merhof. Transnorm: Transformer provides strong spatial normalization mechanism for deep segmentation model. IEEE access, 10:108205108215, 2022. 6 [2] Favyen Bastani, Piper Wolters, Ritwik Gupta, Joe Ferdinando, and Aniruddha Kembhavi. Satlaspretrain: largescale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1677216782, 2023. 2 [3] Derrick Bonafilia, Beth Tellman, Tyler Anderson, and Erica Issenberg. Sen1floods11: georeferenced dataset to train and test deep learning flood algorithms for sentinel-1. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 210211, 2020. 1, 5, 6 [4] Christopher Brown, Michal Kazmierski, Valerie Pasquarella, William Rucklidge, Masha Samsikova, Chenhui Zhang, Evan Shelhamer, Estefania Lahera, Olivia Wiles, Simon Ilyushchenko, et al. Alphaearth foundations: An embedding field model for accurate and efficient global mapping from sparse label data. arXiv preprint arXiv:2507.22291, 2025. 2 [5] Clay Team. Clay foundation model. https://github. com / Clay - foundation / model, 2024. Accessed: 2025-02-21. 2 [6] Carlos Gomes, Benedikt Blumenstiel, Joao Lucas de Sousa Almeida, Pedro Henrique de Oliveira, Paolo Fraccaro, Francesc Marti Escofet, Daniela Szwarcman, Naomi Simumba, Romeo Kienzler, and Bianca Zadrozny. Terratorch: arXiv preprint The geospatial foundation models toolkit. arXiv:2503.20563, 2025. [7] Xin Guo, Jiangwei Lao, Bo Dang, Yingying Zhang, Lei Yu, Lixiang Ru, Liheng Zhong, Ziyuan Huang, Kang Wu, Dingxiang Hu, et al. Skysense: multi-modal remote sensing foundation model towards universal interpretation for earth observation imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2767227683, 2024. 1 [8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770778, 2016. 4 [9] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 1 [10] Danfeng Hong, Bing Zhang, Xuyang Li, Yuxuan Li, Chenyu Li, Jing Yao, Naoto Yokoya, Hao Li, Pedram Ghamisi, Xiuping Jia, et al. Spectralgpt: Spectral remote sensing foundation model. arXiv preprint arXiv:2311.07113, 2023. 2 [11] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, Yu Liu, and Xiang Li. Rsgpt: remote sensing vision language model and benchmark. ISPRS Journal of Photogrammetry and Remote Sensing, 224:272286, 2025. 2 [12] Johannes Jakubik, Felix Yang, Benedikt Blumenstiel, Erik Scheurer, Rocco Sedona, Stefano Maurogiovanni, Jente Bosmans, Nikolaos Dionelis, Valerio Marsocci, Niklas Kopp, et al. Terramind: Large-scale generative multimodality for earth observation. arXiv preprint arXiv:2504.11171, 2025. [13] Di Jiang, Shiyi Li, Irena Hajnsek, Muhammad Adnan Siddique, Wen Hong, and Yirong Wu. Glacial lake mapping using remote sensing geo-foundation model. International Journal of Applied Earth Observation and Geoinformation, page 104371, 2025. 2, 3 [14] Saurabh Kaushik, Lalit Maurya, Elizabeth Tellman, Guoqing Zhang, and Jaydeo Dharpure. Debris covered glacier mapping using newly annotated multisource remote sensing data and geo-foundational model. Science of Remote Sensing, page 100319, 2025. 2, 3 [15] Samar Khanna, Patrick Liu, Linqi Zhou, Chenlin Meng, Robin Rombach, Marshall Burke, David Lobell, and Stefano Ermon. Diffusionsat: generative foundation model for satellite imagery. In ICLR, 2024. 2 [16] Vit Kostejn, Yamil Essus, Jenna Abrahamson, and Ranga Raju Vatsavai. U-prithvi: Integrating foundation model and u-net for enhanced flood inundation mapping. In 13th International Conference on Geographic Information Science (GIScience 2025), pages 181. Schloss Dagstuhl Leibniz-Zentrum für Informatik, 2025. 3, [17] Alexandre Lacoste, Nils Lehmann, Pau Rodriguez, Evan Sherwin, Hannah Kerner, Björn Lütjens, Jeremy Irvin, David Dao, Hamed Alemohammad, Alexandre Drouin, et al. Geobench: Toward foundation models for earth monitoring. Advances in Neural Information Processing Systems, 36: 5108051093, 2023. 1, 2, 3 [18] Devyani Lambhate, Ayush Jain, Kamal Das, Ranjini Bangalore, Johannes Jakubik, and Bianca Zadrozny. Finetuning the geospatial foundation model for land cover mapping. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2024. 3 [19] Wenwen Li, Hyunho Lee, Sizhe Wang, Chia-Yu Hsu, and Samantha Arundel. Assessment of new geoai foundation model for flood inundation mapping. In Proceedings of the 6th ACM SIGSPATIAL International workshop on AI for geographic knowledge discovery, pages 102109, 2023. 6 [20] Fan Liu, Delong Chen, Zhangqingyun Guan, Xiaocong Zhou, Jiale Zhu, Qiaolin Ye, Liyong Fu, and Jun Zhou. Remoteclip: vision language foundation model for remote sensing. IEEE Transactions on Geoscience and Remote Sensing, 62:116, 2024. 2 [21] Siqi Lu, Junlin Guo, James Zimmer-Dauphinee, Jordan Nieusma, Xiao Wang, Steven Wernke, Yuankai Huo, et al. Vision foundation models in remote sensing: survey. IEEE Geoscience and Remote Sensing Magazine, 2025. 1, 2 [22] Valerio Marsocci, Yuru Jia, Georges Le Bellier, David Kerekes, Liang Zeng, Sebastian Hafner, Sebastian Gerard, Eric Brune, Ritu Yadav, Ali Shibli, et al. Pangaea: global and inclusive benchmark for geospatial foundation models. arXiv preprint arXiv:2412.04204, 2024. 1, 2, 3, 6 Saux, Gustau Camps-Valls, and Xiao Xiang Zhu. Neural plasticity-inspired multimodal foundation model for earth observation. arXiv preprint arXiv:2403.15356, 2024. 1, 2 [34] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Skyeyegpt: Unifying remote sensing vision-language tasks via instruction tuning with large language model. ISPRS Journal of Photogrammetry and Remote Sensing, 221:6477, 2025. 2 [35] Zhijie Zhang, Jonathan Giezendanner, Rohit Mukherjee, Beth Tellman, Alexander Melancon, Matt Purri, Iksha Gurung, Upmanu Lall, Kobus Barnard, and Andrew Molthan. Assessing inundation semantic segmentation models trained on high-versus low-resolution labels using floodplanet, manually labeled multi-sourced high-resolution flood dataset. Journal of Remote Sensing, 5:0575, 2025. 5, 6 [23] Francesc Marti Escofet, Benedikt Blumenstiel, Linus Scheibenreif, Paolo Fraccaro, and Konrad Schindler. Finetune smarter, not harder: Parameter-efficient fine-tuning for geospatial foundation models. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 516532. Springer, 2025. 6 [24] Enrique Portales-Julia, Gonzalo Mateo-García, and Luis Gómez-Chova. Understanding flood detection models across sentinel-1 and sentinel-2 modalities and benchmark datasets. Remote Sensing of Environment, 328:114882, 2025. 6 [25] Colorado Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian Clipp, Kurt Keutzer, Salvatore Candido, Matt Uyttendaele, and Trevor Darrell. Scale-mae: scale-aware masked autoencoder for multiscale geospatial In Proceedings of the IEEE/CVF representation learning. International Conference on Computer Vision, pages 4088 4099, 2023. 2 [26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234 241. Springer, 2015. [27] Nirdesh Kumar Sharma and Manabendra Saharia. Deepsarflood: Rapid and automated sar-based flood inundation mapping using vision transformer-based deep ensembles with uncertainty estimates. Science of Remote Sensing, 11: 100203, 2025. 6 [28] Daniela Szwarcman, Sujit Roy, Paolo Fraccaro, Þorsteinn Elí Gíslason, Benedikt Blumenstiel, Rinki Ghosal, Pedro Henrique de Oliveira, Joao Lucas de Sousa Almeida, Rocco Sedona, Yanghui Kang, et al. Prithvi-eo-2.0: versatile multi-temporal foundation model for earth observation applications. arXiv preprint arXiv:2412.02732, 2024. 1, 2, 6 [29] Gabriel Tseng, Anthony Fuller, Marlena Reil, Henry Herzog, Patrick Beukema, Favyen Bastani, James Green, Evan Shelhamer, Hannah Kerner, and David Rolnick. Galileo: Learning global & local features of many remote sensing modalities. arXiv preprint arXiv:2502.09356, 2025. 1, 2 [30] Leonard Waldmann, Ando Shah, Yi Wang, Nils Lehmann, Adam Stewart, Zhitong Xiong, Xiao Xiang Zhu, Stefan Bauer, and John Chuang. Panopticon: Advancing any-sensor foundation models for earth observation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 22042214, 2025. 2 [31] Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying Liu, Conrad Albrecht, and Xiao Xiang Zhu. Ssl4eos12: large-scale multimodal, multitemporal dataset for self-supervised learning in earth observation [software and data sets]. IEEE Geoscience and Remote Sensing Magazine, 11(3):98106, 2023. 2 [32] Yi Wang, Conrad Albrecht, Nassim Ait Ali Braham, Chenying Liu, Zhitong Xiong, and Xiao Xiang Zhu. Decoupling common and unique representations for multimodal self-supervised learning. In European Conference on Computer Vision, pages 286303. Springer, 2024. 2 [33] Zhitong Xiong, Yi Wang, Fahong Zhang, Adam Stewart, Joëlle Hanna, Damian Borth, Ioannis Papoutsis, Bertrand Le 10 A. Appendix This example illustrates the comparative evaluation of models in dense cloudy scene. Our observations highlight PrithviCAFEs relatively better performance, even under challenging cloudy conditions. Figure A.6. Figure illustrate limitation of Prithvi-CAFE in case of dense cloud cover."
        }
    ],
    "affiliations": [
        "Center for Sustainability and the Global Environment (SAGE), University of Wisconsin-Madison",
        "Portsmouth AI and Data Science Centre (PAIDS), School of Computing, University of Portsmouth"
    ]
}