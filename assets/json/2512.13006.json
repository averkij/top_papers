{
    "paper_title": "Few-Step Distillation for Text-to-Image Generation: A Practical Guide",
    "authors": [
        "Yifan Pu",
        "Yizeng Han",
        "Zhiwei Tang",
        "Jiasheng Tang",
        "Fan Wang",
        "Bohan Zhuang",
        "Gao Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 6 0 0 3 1 . 2 1 5 2 : r Few-Step Distillation for Text-to-Image Generation: Practical Guide Yifan Pu1,2, Yizeng Han2, Zhiwei Tang2, Jiasheng Tang2,3, Fan Wang2, Bohan Zhuang2,4, Gao Huang1 1Tsinghua University 2DAMO Academy, Alibaba Group 3Hupan Lab 4Zhejiang University Equal contribution, Corresponding author Diffusiondistillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on strong T2I teacher model, FLUX.1-lite. By casting existing methods into unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill. Date: December 16,"
        },
        {
            "title": "1 Introduction",
            "content": "In recent years, large-scale Diffusion Models (DMs) (Ho et al., 2020; Sohl-Dickstein et al., 2015) have achieved unprecedented success in the field of text-to-image synthesis, with generation quality that rivals or even surpasses human creation (Rombach et al., 2022; Saharia et al., 2022). Models such as Flux (Black Forest Labs et al., 2025), Qwen-Image (Wu et al., 2025) and Imagen (Google DeepMind, 2025), trained on massive image-text datasets, can generate high-fidelity, high-resolution images from complex textual descriptions. However, this remarkable performance comes at significant computational cost. These models rely on an iterative sampling process, progressively converting Gaussian noise into clear image through hundreds of Network Function Evaluations (NFEs) (Song et al., 2020; Ho et al., 2020). This process is not only computationally intensive but also suffers from high latency, severely hindering its application in scenarios requiring real-time feedback or operating in resource-constrained environments, such as interactive design tools, dynamic game content generation, and augmented reality. Therefore, drastically reducing generation latency while maintaining high-quality output has become critical challenge in the field of generative AI. To address the aforementioned challenges, new research direction focused on few-step generation techniques has emerged (Luhman and Luhman, 2021; Wang et al., 2023; Salimans and Ho, 2022). The core objective is to develop models capable of producing high-quality images in very small number of NFEs, typically between 1 and 8. This research direction has spurred variety of innovative methods that, through different technical pathways, attempt to break the trade-off between quality and speed inherent in traditional diffusion models. These methods not only aim to accelerate the inference process of existing models but also explore the fundamental theory of generative models, laying the groundwork for the next generation of efficient generators. Current mainstream few-step generation methods can be broadly categorized into three major paradigms. This report will conduct an in-depth analysis of one or more representative methods from each paradigm: Distribution Distillation: This paradigm leverages few-step and efficient student model to match the output distribution powerful but slow pre-trained teacher model. The student model aims to simulate the teachers output distribution in minimal number of steps. There are two representative methods in this area: Direct Distribution Distillation: Distribution Matching Distillation (DMD) is method that distills multistep diffusion model into an efficient, one-step image generator by minimizing the distribution difference and incorporating regression loss, while its successor, DMDv2, enhances image quality, training efficiency, and flexibility by replacing the costly regression loss with techniques like GAN loss. Recent text-to-image model, like Qwen-Image-Lightning (ModelTC, 2024), adopted this technique. Adversarial Distribution Distillation: Latent Adversarial Diffusion Distillation (LADD) distills slow, pretrained latent diffusion model (the teacher model) into rapid student model capable of generating high-quality images in just one to four steps. The core of this technique (Sauer et al., 2024) is an adversarial game within the latent space, where the student model is trained to produce outputs that fool discriminator, forcing it to match the teachers latent distribution and thereby achieving massive acceleration in generation speed while maintaining high fidelity. Models like SD3-Turbo (Sauer et al., 2024) and Flux.1 Kontext [dev] (Black Forest Labs et al., 2025) use this technique. Trajectory-based Distillation: Trajectory distillation is method for accelerating diffusion models by training student model to predict an entire segment of the teacher models sampling trajectory in fewer steps. Unlike techniques that only match the final output, this approach distills the complete generation process, which allows the student to more faithfully learn complex dynamics like classifier-free guidance and produce high-quality results in very few steps. This report will analyze the latest breakthrough in this area: sCM. Simplified Consistency Models introduces simplified theoretical framework and set of practical techniques to stabilize and scale the training of continuous-time consistency models, enabling them to achieve state-of-the-art performance with as few as two sampling steps in ImageNet. IMM. Inductive Moment Matching enforces consistency at the distribution level by using moment matching to ensure that samples generated from different points along stochastic interpolant path converge to the same target distribution. MeanFlow. Mean-Flow learns the average velocity of flow field through novel identity, enabling stable training and achieving state-of-the-art single-function-evaluation performance. Distillation methods like DMD and LADD have been successfully applied to text-to-image synthesis tasks with significant results. However, the publicly available results for three highly promising new methodssCM, MeanFlow, and IMMare primarily focused on smaller-scale, unconditional image generation tasks (e.g., ImageNet). Their performance, adaptability, and potential advantages in the complex, open-domain task of text-to-image synthesis remain unclear. Therefore, this report aims to achieve three main goals: 1. Provide deep theoretical comparison of these cutting-edge methods, dissecting their fundamental differences in core mechanisms, sources of stability, and scalability. 2. Propose detailed, unified, and feasible experimental plan for adapting these methods (especially sCM and MeanFlow) to the text-to-image synthesis domain for rigorous and fair empirical evaluation. 3. Deliver well-engineered, modular, and reproducible codebase that implements the proposed adaptations, training recipes, evaluation pipelines, and baseline models, facilitating fair comparison and enabling the community to reproduce and extend our experiments."
        },
        {
            "title": "2 A brief overview of each method",
            "content": "This chapter will provide detailed analysis of the three core methods. For each method, we will explain its core mechanism, key innovations, and propose specific, actionable plan to adapt it to the text-to-image synthesis task. This analysis is not just theoretical but aims to provide solid foundation for the subsequent experimental design. 2.1 stabilized Continuous-time Consistency Models (sCM) Consistency Models (CMs) aim to learn function (xt, t) that can map sample xt at any noise level directly to its corresponding clean sample x0 in single step Song et al. (2023). They can be learned through distillation (Consistency Distillation, CD) or trained from scratch (Consistency Training, CT). However, early CMs, especially in the continuous-time setting, were long plagued by severe training instability Luo et al. (2023). sCM introduces comprehensive suite of techniques, including the TrigFlow formulation, architectural improvements, and an adaptively weighted training objective, to resolve the core training instabilities of continuous-time consistency models (CMs)."
        },
        {
            "title": "2.2 MeanFlow",
            "content": "MeanFlow takes distinctly different approach from standard Flow Matching. Standard flow matching models aim to learn the instantaneous velocity v(xt, t) of particles moving along flow field. MeanFlow, instead, chooses to model different physical quantity: the average velocity between two time steps and r. Beyond standard MeanFlow Training (as shown in algorithm 1 Geng et al. (2025)), we further provide MeanFlow Distillation algorithm. In this algorithm, we use flow matching pretrained teacher model to provide the instantaneous velocity in the MeanFlow target. The detailed algorithm is shown in algorithm 2. Algorithm 1 MeanFlow: Training. # fn(z, r, t): function to predict # # x: training batch t, = sample_t_r() = randn_like(x) = (1 - t) * + * = - Algorithm 2 MeanFlow: Distillation. # fn(z, r, t): function to predict # gn(z, t): flow matching teacher # x: training batch t, = sample_t_r() = randn_like(x) = (1 - t) * + * v_teacher = gn(z, t) u, dudt = jvp(fn, (z, r, t), (v, 0, 1)) u, dudt = jvp(fn, (z, r, t), (v, 0, 1)) u_tgt = - (t - r) * dudt error = - stopgrad(u_tgt) loss = metric(error) u_tgt = v_teacher - (t - r) * dudt error = - stopgrad(u_tgt) loss = metric(error)"
        },
        {
            "title": "2.3 Inductive Moment Matching (IMM)",
            "content": "IMM is from-scratch training paradigm. Its core idea is to train the model so that the distribution of samples mapped to target time is consistent regardless of whether they start from time or (with < < t). This inductive consistency along the time axis ensures convergence of the learned mappings. Instead of KL divergence, IMM uses Maximum Mean Discrepancy (MMD) as stable, sample-based measure of distributional difference, which implicitly aligns all moments between distributions."
        },
        {
            "title": "3.1 FM v.s. MeanFlow: Flow Matching as a Special Case of MeanFlow (when r ≡ t)\nFlow Matching (Algorithm 4) can be understood as a specific instance of the more general MeanFlow framework\n(Algorithm 3). In the general MeanFlow formulation, the model learns a average velocity vector field that\ndepends on both the current time t and a reference time r. Flow Matching is precisely recovered when we\nset the reference time r equal to the current time t. In this specialization, the second term of the target\nvanishes entirely. This simplifies the MeanFlow target to utgt = boldsymbolv. Consequently, the network is\ntrained to predict instantaneous velocity directly. This is exactly the objective of standard Flow Matching, as\nshown in Algorithm 4. A direct comparison of the pseudocode in Algorithm 3 and Algorithm 4 illustrates this\nsimplification.",
            "content": "Algorithm 3 MeanFlow: Training. # fn(z, r, t): function to predict # x: training batch t, = sample_two_time() = randn_like(x) = (1 - t) * + * = - Algorithm 4 Flow Matching: Training. # fn(z, t): function to predict # x: training batch = sample_one_time() = randn_like(x) = (1 - t) * + * = - u_pred, dudt = jvp(fn, (z, r, t), (v, 0, 1)) v_pred = fn(z, t) u_tgt = - (t - r) * dudt error = u_pred - stopgrad(u_tgt) loss = metric(error) v_tgt = error = v_pred - v_tgt loss = metric(error)"
        },
        {
            "title": "3.2 FM v.s. sCM: Flow Matching and TrigFlow are interconvertible without re-training",
            "content": "Flow Matching and TrigFlow frameworks are mutually convertible at inference time (Zheng et al., 2025; Chen et al., 2025), enabling models trained in one paradigm to be used with samplers from the other without any retraining. Specifically, pre-trained Flow Matching model, denoted by its velocity field estimator vθ(xt,FM, tFM, y), can be used to denoise sample xt,Trig from TrigFlow process. This is achieved by first transforming the TrigFlow state (xt,Trig, tTrig) into its Flow Matching equivalent (xt,FM, tFM). The time variable is mapped to preserve the signal-to-noise ratio (SNR), and the state variable is rescaled accordingly: tFM = sin(tTrig) cos(tTrig) + sin(tTrig) , xt,FM = 1 cos(tTrig) + sin(tTrig) xt,Trig. (1) Subsequently, the output of the Flow Matching model is used to construct the optimal TrigFlow estimator ˆFθ, which provides the correct update direction for TrigFlow-based solver. This relationship is given by: ˆFθ (cid:0)xt,Trig, tTrig, y(cid:1) = cos(tTrig) sin(tTrig) cos(tTrig) + sin(tTrig) xt,FM + 1 cos(tTrig) + sin(tTrig) vθ(xt,FM, tFM, y). (2) Conversely, this mapping is fully and losslessly reversible, allowing native TrigFlow model Fθ to operate within Flow Matching sampler. Given Flow Matching state (xt,FM, tFM), we first map it to the TrigFlow domain using the inverse transformations for time and state: tTrig = arctan (cid:18) tFM 1 tFM (cid:19) , xt,Trig = xt,FM (cid:112)t2 FM + (1 tFM)2 . (3) The TrigFlow model Fθ processes this transformed input, and its output is then converted back into the velocity field estimate (cid:98)vθ(xt,FM, tFM, y) required by the Flow Matching framework. This output transformation is defined by: (cid:98)vθ(xt,FM, tFM, y) = 1 (cid:112)t2 FM + (1 tFM)2 Fθ (cid:0)xt,Trig, tTrig, y(cid:1) 1 2tFM FM + (1 tFM)2 xt,FM. t2 (4) This bidirectional conversion ensures complete interoperability, allowing practitioners to flexibly combine models and samplers from either framework. 3.3 sCM v.s. MeanFlow: MeanFlow as Synchronization Limit of sCM Under the Flow Matching (FM) parameterization, where the velocity field is directly modeled by neural network, i.e., vθ(xt, t) = Fθ(xt, t), direct comparison between the loss gradients of Stochastic Control Matching (sCM) and our proposed MeanFlow (MF) reveals an insightful connection. The respective gradients with respect to the network parameters θ, denoted as θL, are given by: θL sCM = Et,xt θL MF = Et,xt (cid:20) (cid:28) θFθ(xt, t), vt Fθ (xt, t) dFθ dt (cid:20) (cid:28) θFθ(xt, t), vt Fθ(xt, t) dFθ dt (xt, t) (cid:29)(cid:21) (xt, t) (cid:29)(cid:21) (5) (6) comparison of Eq. (5) and Eq. (6) highlights their nearly identical structure. The sole distinction lies in the regression target provided for the networks output within the inner product. Specifically, sCM employs target network Fθ , parameterized by θ, which is typically an exponential moving average (EMA) of the online networks parameters θ. This technique is widely used to stabilize training by providing more consistent and slowly-evolving target. In stark contrast, MeanFlow utilizes the current online network Fθ for its entire regression target. Consequently, the MeanFlow gradient is mathematically equivalent to the sCM gradient under the condition that the target network is fully synchronized with the online network at every training step, i.e., by setting θ = θ. This interpretation frames MeanFlow not as distinct method, but as specific, simplified variant of sCM that dispenses with the EMA-based target stabilization. This design choice results in more up-to-date, self-referential training dynamic, differentiating its behavior from standard sCM."
        },
        {
            "title": "3.4 CM v.s. IMM: Consistency Models as a Special Case of Implicit Moment Matching",
            "content": "As shown in the Eq.(12) of the IMM paper (Zhou et al. (2025)), the IMM loss is: LIMM(θ) = Ext,x t,xr,x r,s,t (cid:104) (cid:104) w(s, t) (cid:16) ys,t, s,t (cid:17) (cid:16) + ys,r, s,r (cid:17) (cid:16) ys,t, s,r (cid:17) (cid:16) s,t, ys,r (cid:17)(cid:105)(cid:105) , (7) s,t(x s,t = θ s,t(xt), where ys,t = θ s,r (xr), prior weighting function. As is illustrated in Appendix of the original paper, when we set xt = we have θ s,r (x and ys,r = s,t(x definition. The original IMM loss in equation (7) reduce to: r), k(, ) is kernel function, and w(s, t) is , , xr = r) and let k(x, y) = y2, which means ys,t = s,t r)) = 0 by t)) = 0 and k(ys,r, s,r) = k(f θ s,r (xr) = θ s,t(xt), θ s,t(x . So k(ys,t, s,t(xt) = θ t) and θ s,t) = k(f θ s,r(xr), θ t), ys,r = θ s,r = θ s,r (x s,r(x s,r LIMM(θ) = Ext,xr,s,t (cid:104) (cid:104) w(s, t) (cid:16) ys,t, s,r (cid:17) (cid:16) s,t, ys,r (cid:17)(cid:105)(cid:105) . (8) Furthermore, since we use k(x, y) = xy2, k(ys,t, and k(y s,r (xr)2 = θ s,t, ys,r) = θ t) θ s,t(x s,t(xt) θ s,r) = θ s,t(xt)f θ s,t(xt)f θ s,r (xr)2, more simplified expression is r)2 = θ s,r (x s,r (xr)2, LIMM(θ) = Ext,xr,s,t (cid:104) (cid:104) θ 2w(s, t) s,t(xt) θ s,r (xr)2(cid:105)(cid:105) . (9) s,t(xt) gθ(xt, t) , where gθ(xt, s, t) is the diffusion model If is small positive constant, we further have θ parameterized with EDM (Karras et al. (2022)) and we drop as input. If gθ(xt, t) itself satisfies boundary condition at = 0, we can directly take = 0 in which case θ 0,t(xt) = gθ(xt, t). And under these assumptions, and omit the constant 2, our loss becomes LIMM(θ) = Ext,xr,t (cid:104) gθ(xt, t) gθ (xr, r)2(cid:105)(cid:105) (cid:104) w(t) , (10) which is discrete-time consistency model(CM) loss using ℓ2 distance. In short, the IMM loss degenerates to the discrete-time Consistency Model (CM) loss when utilizing single-particle estimate (setting xt = and ), employing the negative squared Euclidean distance as the kernel function (i.e., k(x, y) = y2), xr = and setting the target time to 0."
        },
        {
            "title": "4 Text-to-Image Adaptation with FLUX.1-lite",
            "content": "In this section, we detail the adaptation of the 8B FLUX.1-lite (Freepik (2024)) model for text-to-image generation. We introduce and evaluate two primary methodologies: an adaptation based on simplified Consistency Model (sCM) objective and novel MeanFlow training objective."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "For all subsequent experiments, we utilize 32 Nvidia H20 GPUs for training. The dataset employed is proprietary high-quality text-to-image dataset. While the dataset cannot be publicly released, it ensures consistent and fair comparison across all experiments. We adopt the FLUX.1-lite model with 8 billion parameters as the teacher and aim to distill few-step student model from it. For sCM, we distill the student model for 3,000 iterations, as we observed no further improvement in the GenEval overall score beyond this point. Conversely, for MeanFlow, we report results at 25,000 iterations, as extending the training duration yielded continuous gains in the GenEval score."
        },
        {
            "title": "4.2 Methodological Adaptation for Text-to-Image Generation",
            "content": "4.2.1 sCM Adaptation Normalizing DiT timestep inputs stabilizes training. During the training of sCM using FLUX.1-lite, we observed phenomenon similar to that reported in prior works (Lu and Song (2025); Chen et al. (2025)). Specifically, when the Diffusion Transformers timestep input ranges from 0 to 1000, the gradient norm increases continuously during training, eventually leading to collapse. To mitigate this, we first rescaled the diffusion transformers timestep to the range [0, 1]. We treated the original FLUX.1-lite as the teacher and initialized student model with an identical structure, configured to accept timesteps in [0, 1]. We distilled the teacher into the student using identical images, text prompts, and proportionally scaled timesteps by minimizing the smooth L1 loss between their outputs. This distillation was performed at resolution of 1024 1024 with total batch size of 128 for 120K iterations. The resulting student model achieved performance comparable to the teacher on the GenEval benchmark, as shown in the first two lines of table 1. Guiding the student model with the teachers velocity prediction. Subsequently, we utilized this timestep-rescaled model (t [0, 1]) to train the sCM. Instead of the standard consistency training paradigm, we employed consistency distillation, which uses the classifier-free guided output of the teacher as the velocity target. The training was conducted at resolution of 512 with total batch size of 128 and learning rate of 1e-6. 4.2.2 MeanFlow Adaptation Dual-timestep input mechanism. As MeanFlow models the average velocity between timesteps and t, the diffusion transformer requires an additional time input. The standard FLUX.1-lite encodes single timestep using sinusoidal embeddings and an MLP projection for AdaLN modulation. We replicate this embedding branchinitialized with the same weightsto process the time differential (t r). The outputs of the original branch (encoding t) and the cloned branch (encoding r) are summed before being passed to the AdaLN layers. Teacher Distillation for MeanFlow. While MeanFlow can be formulated as standalone training objective (Algorithm 1), we empirically find it significantly more effective as distillation technique (Algorithm 2). The primary distinction lies in the target velocity. In the standard training formulation, the target = serves as an unbiased but highly stochastic estimator of the true vector field, introducing substantial variance into the optimization of u. In contrast, Algorithm 2 employs pre-trained flow-matching teacher gn to provide the target vteacher = gn(z, t). We utilize the teachers instantaneous velocity as direct guidance signal because the teacher has already converged to the conditional expectation of the vector field, i.e., gn(z, t) E[vz]. Consequently, vteacher represents denoised, deterministic approximation of the optimal transport path. By substituting the noisy raw target with this stable teacher signal, the MeanFlow objective focuses purely on rectifying the curvature of the trajectory via the Jacobian utgt = vteacher (t r) du dt correction term, rather than learning the data distribution from scratch. High-Order Loss Achieves Better Performance. While the original MeanFlow paper Geng et al. (2025) suggests that = 1 or = 0.5 are generally optimal in the loss function = 2γ (where denotes the regression 2 error), we empirically find that setting γ = 2 yields superior performance. This configuration, which effectively minimizes the fourth power of the norm, imposes distinct gradient behavior beneficial for our distillation objective. Quantitatively, this adjustment leads to substantial boost on the vanilla MeanFlow target: the GenEval score increases from 44.04% to 48.65%. Improved CFG for MeanFlow in Text-to-Image. We compare the vanilla MeanFlow (where κ = 0) with the Improved CFG variant introduced in Appendix B.2 of the MeanFlow paper (Geng et al. (2025)). The improved version incorporates mixing scale κ to blend class-conditional and unconditional predictions in the regression target, aligning more closely with standard practices in classifier-free guidance. Experiments show that this modification is beneficial for text-to-image generation, boosting the GenEval score from 48.65% to 51.41%."
        },
        {
            "title": "4.3 Quantitative and Qualitative Results",
            "content": "Table 1 Quantitative comparison on the GenEval benchmark. We evaluate the original FLUX.1-lite, the timestep-rescaled teacher, and the distilled students (sCM and MeanFlow) across varying Numbers of Function Evaluations (NFE). sCM maintains robust performance at NFE=1/2, while MeanFlow requires NFE=4 to match the teachers quality. NFE Single Obj. Two Obj. Counting Colors Position Color Attri. Overall FLUX.1-lite Rescaled 28 28 sCM sCM sCM MeanFlow MeanFlow MeanFlow 4 2 1 4 2 1 98.44 97.81 97.19 94.69 89.69 96.25 61.88 2.81 65.40 63. 56.82 55.81 36.87 64.14 31.31 0.25 29.69 39.06 35.62 38.44 26.56 38.44 18.12 0.31 78.19 74. 76.86 74.20 67.55 69.15 39.89 0.80 13.50 12.50 8.75 10.25 6.50 13.00 7.25 0.25 36.25 33. 38.50 43.50 32.50 27.50 18.50 0.25 53.58 53.58 52.29 52.81 43.28 51.41 29.49 0.78 Table 2 Quantitative results on DPG-Bench. This benchmark evaluates the alignment of global structure, entities, attributes, and relations. MeanFlow achieves teacher-level performance at NFE=4, whereas sCM demonstrates superior stability and alignment at lower step counts (NFE 2). NFE Global Entity Attribute Relation Other Overall FLUX.1-lite Rescaled 28 28 sCM sCM sCM MeanFlow MeanFlow MeanFlow 4 2 4 2 1 86.09 83.93 82.21 82.22 82.33 90.83 79.81 52.44 88.73 87.00 86.28 87.26 82. 86.15 80.50 48.90 86.27 86.0 85.49 85.90 85.48 87.20 83.67 59.83 88.50 88.86 88.41 88.66 83. 88.27 84.13 50.10 80.87 86.54 86.03 86.50 81.74 86.35 80.49 54.39 80.20 79.83 77.85 79.06 75. 80.03 71.09 27.55 Quantitative Comparison. We report the evaluation results on GenEval  (Table 1)  and DPG-Bench  (Table 2)  . First, the Rescaled teacher model exhibits performance nearly identical to the original FLUX.1-lite, validating that normalizing the timestep range to [0, 1] does not compromise generation quality. Comparing the distillation methods reveals distinct trade-off between step efficiency and peak performance. First, sCM excels in extreme few-step regimes. sCM demonstrates remarkable stability at NFE=2 and even NFE=1. On GenEval, sCM (NFE=2) achieves an overall score of 52.81%, effectively matching the teachers 53.58%. Even at single step (NFE=1), it retains respectable score of 43.28%. similar trend is observed on DPG-Bench, where sCM maintains high alignment scores across all step counts. We also find that MeanFlow requires sufficient steps but achieves high fidelity. At NFE=4, it outperforms sCM on DPG-Bench (80.03 vs. 77.85) and nearly matches the teacher, suggesting that its trajectory straightening is highly effective when Figure 1 Qualitative comparison of distillation methods. We visualize samples generated by the teacher (left) and the two student models at NFE={1, 2, 4}. sCM produces structurally coherent images even at single step. In contrast, MeanFlow exhibits generation collapse at NFE=1 (pure noise) and requires NFE=4 to converge to high-fidelity results. given sufficient integration steps. However, performance collapses at NFE=1 and NFE=2 (e.g., 0.78% on GenEval at NFE=1), indicating that the learned vector field, while straight, still requires minimum number of discretization steps to traverse accurately. Qualitative Visualization. These quantitative findings are visually corroborated in Figure 1. For sCM, as shown in the sCM column, the model produces structurally coherent images even at NFE=1 (e.g., the giraffe and the buses are clearly recognizable). Increasing the steps to 2 or 4 primarily refines high-frequency details and textures. The MeanFlow column illustrates the collapse at NFE=1, resulting in pure noise (gray outputs). At NFE=2, the model begins to form semantic content (e.g., the bench appears), but significant artifacts and noise remain (visible in the giraffe and zebra examples). However, at NFE=4, MeanFlow produces images with exceptional sharpness and correct semantics, often surpassing sCM in fine-grained detail (e.g., the fur texture of the giraffe and the reflection on the buses). In summary, sCM is the optimal choice for real-time applications requiring NFE 2, while MeanFlow is preferable for scenarios where slightly higher budget (NFE=4) is acceptable for maximum quality."
        },
        {
            "title": "5 Codebase",
            "content": "Our codebase is built upon the Hugging Face Diffusers library and implements multiple distillation algorithms for accelerating the FLUX.1-lite text-to-image diffusion model. The implementation includes two primary training pipelines: (1) MeanFlow, trajectory distillation approach utilizing Jacobian-vector products (JVPs) to enforce consistency along the flow path with optional classifier-free guidance integration; and (2) simplified Consistency Matching (sCM), which leverages tangent vector matching with learned per-sample variance through TrigFlow reparameterization of the FLUX.1-lite model. The framework employs teacher-student paradigm based on modified FLUX.1-lite MMDiT architecture. We utilize DeepSpeed ZeRO with bfloat16 mixed-precision and gradient checkpointing for efficient distributed training. The pipeline performs on-the-fly encoding via the pretrained AutoencoderKL, operating in 16-channel latent space with 8 downsampling."
        },
        {
            "title": "References",
            "content": "Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas M\"uller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, June 2025. Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Song Han, and Enze Xie. Sana-sprint: One-step diffusion with continuous-time consistency distillation. arXiv preprint arXiv:2503.09641, 2025. Freepik. Flux.1-lite-8b. https://huggingface.co/Freepik/flux.1-lite-8B, 2024. Hugging Face model repository. Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. In NeurIPS, 2025. Google DeepMind. Imagen 4 model card. Model card (PDF), May 2025. https://storage.googleapis.com/ deepmind-media/Model-Cards/Imagen-4-Model-Card.pdf. Published May 20, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022. Cheng Lu and Yang Song. Simplifying, stabilizing & scaling continuoustime consistency models. In ICLR, 2025. Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. ModelTC. Qwen-image-lightning. https://huggingface.co/lightx2v/Qwen-Image-Lightning, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, and et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems (NeurIPS), 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. In ICLR, 2023. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Kaiwen Zheng, Yuji Wang, Qianli Ma, Huayu Chen, Jintao Zhang, Yogesh Balaji, Jianfei Chen, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Large scale diffusion distillation via score-regularized continuous-time consistency. arXiv preprint arXiv:2510.08431, 2025. Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. In ICML, 2025."
        },
        {
            "title": "A Proofs",
            "content": "A.1 Interconversion between TrigFlow and Flow Matching The following derivation is based on the supplementary materials of sana-sprint Chen et al. (2025), with some modifications made for easier understanding. The TrigFlow framework defines the noisy input sample as: xt,Trig = cos(tTrig)x0 + sin(tTrig)z. (11) The Signal-to-Noise Ratios (SNRs) for the Flow Matching and TrigFlow models are defined respectively as: SNR(tFM) = ( 1 tFM tFM )2, SNR(tTrig) = ( cos(tTrig) sin(tTrig) )2 = ( 1 tan(tTrig) )2. (12) To align the models, we match their SNRs. We seek the corresponding time tFM in the Flow Matching framework that satisfies: ( 1 tFM tFM )2 = ( 1 tan(tTrig) )2. Solving this equation yields the interconversion relationship between tFM and tTrig: tFM = sin (tTrig) sin (tTrig) + cos (tTrig) , tTrig = arctan ( tFM 1 tFM ). From this relationship, we can also derive the following three useful identities: cos(tTrig) = cos(arctan ( sin(tTrig) = sin(arctan ( tFM 1 tFM )) = tFM 1 tFM )) = 1 tFM (cid:112)t FM + (1 tFM)2 tFM (cid:112)t2 FM + (1 tFM)2 , , cos(tTrig) + sin(tTrig) = 1 (cid:112)t2 FM + (1 tFM)2 . The conversion relationship between the noisy samples xt,FM and xt,Trig can then be expressed as: xt,FM = (1 tFM)x0 + tFMz = = = = (cid:113) (cid:113) (cid:113) t2 FM + (1 tFM)2 ( 1 tFM (cid:112)t2 FM + (1 tFM) x0 + tFM (cid:112)t2 FM + (1 tFM)2 z), t2 FM + (1 tFM)2 [cos(tTrig)x0 + sin(tTrig)z], t2 FM + (1 tFM)2 xt,Trig, 1 cos(tTrig) + sin(tTrig) xt,Trig, (13) (14) (15) (16) (17) (18) This conversion confirms that the samples from both frameworks can be mapped to the same distribution, fulfilling our objective. Our next goal is to determine the optimal estimator for the TrigFlow model, Fθ, based on the optimal estimator of the Flow Matching model, vθ(xt,FM, tFM, y). We begin by considering an ideal scenario assuming sufficient model capacity. In this optimal setting, the flow matching models solution is: v(xt,FM, tFM, y) = E[z x0xtFM , y], (19) This is the optimal solution as conditional expectation minimizes the Mean Squared Error (MSE) loss. Analogously, the optimal solution for the TrigFlow model is given by: (xt,Trig, tTrig, y) = E[cos (tTrig)z sin (tTrig)x0xtTrig , y]. (20) We now leverage the linearity of conditional expectation to derive the relationship: E[cos (tTrig)z sin (tTrig)x0xtTrig , y]. =E[ (cid:112)t2 1 tFM FM + (1 tFM)2 1 2tFM (cid:112)t2 FM + (1 tFM)2 tFM (cid:112)t2 FM + (1 tFM)2 x0xtFM , y] E[(1 tFM) x0 + tFM zxtFM, y] + 1 2tFM (cid:112)t2 FM + (1 tFM)2 xtFM + FM + (1 tFM)2 t2 (cid:112)t2 FM + (1 tFM)2 E[z x0xtFM , y] = = FM + (1 tFM)2 t2 (cid:112)t2 FM + (1 tFM)2 E[z x0xtFM , y] (21) =[cos(tTrig) sin(tTrig)] xtFM + 1 cos(tTrig) + sin(tTrig) E[z x0xtFM , y] = cos(tTrig) sin(tTrig) cos(tTrig) + sin(tTrig) xt,Trig + 1 cos(tTrig) + sin(tTrig) E[z x0xtFM , y] This derivation ultimately yields the following conversion formulas: (xt,Trig, tTrig, y) = cos(tTrig) sin(tTrig) cos(tTrig) + sin(tTrig) xt,Trig + 1 cos(tTrig) + sin(tTrig) v(xt,FM, tFM, y), (22) v(xt,FM, tFM, y) = 2tFM 1 FM + (1 tFM)2 xt,FM + t2 1 (cid:112)t FM + (1 tFM)2 (xt,Trig, tTrig, y), (23) A.2 sCM v.s. MeanFlow: Equivalence of Gradients This derivation demonstrates that under the Flow Matching parameterization, the training objective gradients for the simplified consistency model (sCM) and simplified version of Meanflow are structurally equivalent. Instead of the TrigFlow, we use the simpler Flow Matching (FM) parameterization for the time-dependent function fθ(xt, t) = xt tFθ(xt, t), which satisfies the boundary condition fθ(x0, 0) = x0. The time derivative of fθ(xt, t) is: dt fθ(xt, t) = vt Fθ(xt, t) dFθ(xt, t) dt , (24) where vt = dxt dt consistency model is (as derived in sCM Lu and Song (2025)): is the velocity of the path xt. Under this parameterization, the loss of continuous-time LsCM = Et,xt (cid:20) θ (xt, t) dfθ (xt, t) dt (cid:21) , where θ denotes the parameters with stopped gradients. This can be written as: LsCM = Et,xt (cid:20)(cid:28) fθ(xt, t), vt Fθ(xt, t) (cid:29)(cid:21) . dFθ dt (25) (26) To find the gradient for optimization, we differentiate this loss with respect to θ. Noting that θfθ(xt, t) = tθFθ(xt, t), this yields: θLsCM = Et,xt (cid:20) (cid:28) tθFθ(xt, t), vt Fθ (xt, t) (cid:29)(cid:21) . dFθ dt (27) On the other hand, the MeanFlow loss is (as derived in MeanFlow Geng et al. (2025)): (cid:104) LMeanFlow(θ) = Et,zt where utgt = vt (t r) (vtzuθ + tuθ) . uθ (zt, r, t) sg (utgt )2 2 (cid:105) , When we set to 0, it becomes: (cid:104) LMeanFlow(θ) = Et,zt where utgt = vt (vtzuθ + tuθ) . uθ (zt, t) sg (utgt )2 2 (cid:105) , (28) (29) After unifying the notation, zt xt, uθ Fθ, and identifying the total derivative term vtzuθ + tuθ dFθ (xt,t) dt (where the stop-gradient operator sg() implies the use of θ), the MeanFlow loss becomes: LMeanFlow(θ) = Et,xt (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) Fθ (xt, t) (cid:18) vt dFθ(xt, t) dt (cid:35) . (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 We differentiate the loss with respect to θ. Omitting the constant factor, the gradient becomes: θLMeanFlow(θ) = Et,xt (cid:20) (cid:28) θFθ(xt, t), vt Fθ(xt, t) (cid:29)(cid:21) . dFθ dt Comparing the resulting gradients: θLsCM = Et,xt (cid:20) (cid:28) tθFθ(xt, t), vt Fθ(xt, t) (cid:29)(cid:21) , dFθ dt θLMeanFlow = Et,xt (cid:20) (cid:28) θFθ(xt, t), vt Fθ(xt, t) (cid:29)(cid:21) . dFθ dt (30) (31) (32) (33) Since θ represents the parameters with stopped gradients, numerically Fθ is equal to Fθ . Consequently, the error signal terms (the second term in the inner product) are numerically identical in both methods. The gradients differ only by the weighting factor present in sCM."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "Tsinghua University",
        "Zhejiang University"
    ]
}