{
    "paper_title": "A Unified Agentic Framework for Evaluating Conditional Image Generation",
    "authors": [
        "Jifang Wang",
        "Xue Yang",
        "Longyue Wang",
        "Zhenran Xu",
        "Yiyu Wang",
        "Yaowei Wang",
        "Weihua Luo",
        "Kaifu Zhang",
        "Baotian Hu",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability."
        },
        {
            "title": "Start",
            "content": "Jifang Wang, Xue Yang, Longyue Wang, Zhenran Xu, Yiyu Wang, Yaowei Wang, Weihua Luo, Kaifu Zhang, Baotian Hu*, Min Zhang Harbin Institute of Technology (Shenzhen), Shenzhen, China 23S151116@stu.hit.edu.cn, {hubaotian,zhangmin2021}@hit.edu.cn 5 2 0 2 9 ] . [ 1 6 4 0 7 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEVAL, unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEVAL utilizes large multimodal models (LMMs) as its core, integrating multi-functional toolbox and establishing fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEVAL (GPT-4o version) achieves high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEVAL surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEVALs capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability1."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large-scale text-to-image (T2I) generative models have enabled the creation of images based on text prompts as well as reference images, i.e. conditional image generation (Kumari et al., 2023; Ruiz et al., 2023; Li et al., 2023b; He et al., 2024). The field is evolving at an unprecedented pace with an increasing number of tasks and models being introduced. Among these, *Corresponding author. 1Our code and models are publicly available at https: //github.com/HITsz-TMG/Agentic-CIGEval. 1 Figure 1: An example of subject-driven image editing with human-annotated low scores. Both traditional metrics and GPT-4o-based VIEScore assign high scores. By integrating GPT-4o with tools, CIGEVAL, our agentic evaluation framework, highlights the glasses object in both images, and finds their different shapes and designs, thereby reaching the correct score. Source and Subject means source image and subject image. text-guided image generation is particularly popular (Ramesh et al., 2022; Chen et al., 2025; Yuan et al., 2025). Expanding beyond text, diverse set of conditions have been employed to steer the diffusion process: text-guided image editing (Brooks et al., 2023), mask-guided image editing (runwayml, 2023), subject-driven image generation and editing (Chen et al., 2023; Guo et al., 2024), multiconcept image composition (Kumari et al., 2023) and control-guided image generation (Qin et al., 2023; Zhang and Agrawala, 2023). Despite the growing number of generative models being developed, significant challenge persists in effectively evaluating AI-synthesized images (Peng et al., 2024). Current evaluation metrics have the following three limitations: (1) Taskspecific: Traditional metrics are narrowly focused and cannot be generalized across different tasks. For example, LPIPS (Zhang et al., 2018) measures the perceptual similarity of pair of images, while CLIP-Score (Hessel et al., 2021) measures the text alignment of one single image. (2) Limited explainability: Assigning single score to generated image without the reasoning process fails to offer comprehensive evaluation. Images can be assessed on multiple dimensions, such as prompt adherence and concept preservation (Fu et al., 2023b). (3) Lack of human alignment: Traditional metrics like DINO (Caron et al., 2021) and CLIP (Radford et al., 2021) often result in huge discrepancies from humans, caused by their image similarity measurement nature. Even based on the powerful large multimodal model (LMM) GPT-4o, as shown in Figure 1, the current state-of-the-art VIEScore (Ku et al., 2024) struggles to capture subtle image nuances and shows low correlation with human judgment in various image editing tasks. To address these issues, we propose CIGEVAL, an autonomous LMM-based agent framework for evaluating conditional image generation. This agent framework can integrate the advanced GPT4o model (OpenAI, 2023) and open-source models (e.g., Qwen2.5-VL (Wang et al., 2024)). Our work is driven by two primary motivations: (1) developing autonomous evaluation agents capable of making independent decisions and judgments without human assistance; (2) enabling relatively smaller models to efficiently perform complex evaluations. To achieve this, we make three key technical contributions. First, we extend the LMMs capability to detect and emphasize subtle differences between highly similar images by curating versatile toolbox, in contrast to previous methods that relied solely on the perceptual capabilities of LMMs. Second, we establish fine-grained evaluation framework, including task decomposition, tool selection and analysis. Third, we synthesize instruction data based on evaluation trajectories for fine-tuning the LMM, where we first employ GPT-4o to execute the stages and then filter the trajectories that align with human evaluations. Experiments on the well-established ImagenHub benchmark (Ku et al., 2023) show that, when using GPT-4o as the underlying LMM, CIGEVAL achieves the state-of-the-art performance across all 7 tasks. It achieves an average Spearman correlation of 0.4625 with human raters, closely matching the human-to-human correlation of 0.47. The primary improvements are observed in tasks involving multiple conditions, such as control-guided image generation and multi-concept image composition, where previous evaluation metrics struggle. Using only 2.3K filtered evaluation trajectories for tuning, CIGEVAL, leveraging 7B open-source LMMs, demonstrates performance surpassing previous GPT-4o-based state-of-the-art methods. Further ablation study shows the importance of each tool and the robustness of our framework. In addition, we conduct preliminary case study on GPT-4os image generation. CIGEVAL assigns scores closely aligned with human annotations and effectively detects subtle flaws in 4o-generated images, especially in tasks involving multiple input images and adherence to specific control signals (e.g., Canny edges, OpenPose). These results suggest that CIGEVAL has substantial promise for achieving human-level performance in assessing synthetic images. Our main contributions are as follows: We introduce CIGEVAL, an LMM-based evaluation agent designed to assess various conditional image generation tasks. Our approach is characterized by its human-aligned, explainable, and unified evaluation method, setting it apart from previous metrics. We evaluate CIGEVAL across 7 conditional image generation tasks, demonstrating that CIGEVAL, based on GPT-4o, outperforms all existing baselines and achieves high correlation with human annotators, closely mirroring the human-to-human correlation. We fine-tune open-sourced 7B LMMs and significantly improve their evaluation performance, surpassing previous GPT-4o-based state-of-the-art method."
        },
        {
            "title": "2.1 Conditional Image Generation",
            "content": "Diffusion models have gained wide attention in AI research for image synthesis (Ho et al., 2020; Dhariwal and Nichol, 2021). In recent years, several new models (Kumari et al., 2023; Ruiz et al., 2023; Li et al., 2023b; Zhang and Agrawala, 2023) have been developed to introduce controllable conditions in image generation. Prevalent tasks in this domain include text-to-image generation (Saharia et al., 2022; Rombach et al., 2022; stability.ai, 2023) (known as text-guided image generation), 2 inpainting (Avrahami et al., 2022; Lugmayr et al., 2022) (referred to as mask-guided image editing) and text-guided image editing (Brooks et al., 2023; Couairon et al., 2022; Wu and la Torre, 2023). Recent works have proposed new tasks, such as subject-driven image generation and editing (Gal et al., 2022; Ruiz et al., 2023; Li et al., 2023b) to inject one specific subject into synthesized image, and multi-concept image composition (Kumari et al., 2023; Liu et al., 2023; Ding et al., 2024), which allows multiple specific subjects into the synthesized image. Additionally, control-guided image generation (Zhang and Agrawala, 2023; Qin et al., 2023; Guo et al., 2024) allows additional conditions alongside the text prompt to guide the image synthesis. Our work employs LMM-based agents to assess all of these discussed tasks."
        },
        {
            "title": "2.2 Synthetic Image Evaluation",
            "content": "A variety of metrics have been introduced to assess AI-generated images. For example, the CLIP score (Hessel et al., 2021) and BLIP score (Li et al., 2022) are commonly used to measure the alignment between the generated image and the text prompt. Metrics like LPIPS (Zhang et al., 2018) and DreamSim (Fu et al., 2023b) focus on assessing perceptual similarity. LLMScore (Lu et al., 2023) and HEIM-benchmark (Lee et al., 2023) assess text-to-image models on multiple fine-grained aspects, including toxicity and safety. However, these metrics predominantly focused on text-to-image generation and remain narrow in scope. There is noticeable lack of effective automatic metrics for other conditional image generation tasks, such as subject-driven image generation and image editing (Ruiz et al., 2023; Li et al., 2023b; Peng et al., 2024). Consequently, some research work (Denton et al., 2015; Isola et al., 2017; Meng et al., 2021; Chen et al., 2023; Sheynin et al., 2023) rely heavily on human evaluation. This dependence highlights the need for more unified, interpretable, and reliable automatic evaluation methods in the field. Our work seeks to bridge this gap by developing an autonomous agentic evaluation framework that closely aligns with human judgment."
        },
        {
            "title": "2.3 Large Multimodal Models as Evaluators",
            "content": "Motivated by the explorations of large language model (LLM)-based evaluators in natural language processing (Zheng et al., 2023; Dubois et al., 2023; Fu et al., 2023a; Cheng et al., 2024b), large multimodal models (LMMs) have been utilized to evaluate responses in visual question answering (Chen et al., 2024a; Xu et al., 2024). In the realm of image evaluation, the GPT-4 series has demonstrated promising capabilities, particularly in assessing text-image alignment (Zhang et al., 2023b; Li et al., 2024). However, these models are not without limitations. comprehensive study on GPT-4os vision abilities have revealed mistakes in fine-grained image evaluation tasks (Yang et al., 2023), such as failing to accurately distinguish differences between similar images (Ku et al., 2024). To address these shortcomings, we enhance the capabilities of LMMs by integrating versatile set of image analysis and editing tools, and by adopting an agentic framework to improve the evaluation of AI-generated images."
        },
        {
            "title": "3 CIGEVAL",
            "content": "In this section, we introduce CIGEVAL, our LMMbased agentic framework designed for evaluating conditional image generation. First, we define seven conditional image generation tasks that are the focus of our study (Sec. 3.1), and then design multi-functional toolbox (Sec. 3.2). Next, we introduce our fine-grained evaluation framework (Sec. 3.3). Finally, we synthesize high-quality trajectory data to fine-tune open-source LMMs (Sec. 3.4)."
        },
        {
            "title": "3.1 Task Definition",
            "content": "To build unified and explainable evaluation metric, we define the image evaluation problem as shown in Equation 1. The function feval takes as input an instruction I, synthesized image O, and set of conditions (e.g. text prompt, subject image, background image, canny-edge, etc). The function feval should produce the intermediate rationale in natural language before generating the final score according to the instruction I: feval(I, O, C) = (rationale, score) (1) Following Ku et al. (2023), we focus on seven primary conditional image generation tasks, each with different sets of conditions C: Text-guided Image Generation: = [p], where is text prompt. The objective is to generate an image that aligns with the text description. Mask-guided Image Editing: = [p, Imask, Isrc], where Imask is binarized mask and Isrc is source image. The aim is to modify Isrc in the masked area according to p. Tool Argument Output Purpose Grounding Image Target Entity yellow alarm clock [219,261,337,370] Highlight Image Region [324,281,381,497] Edited Image Difference Image 1 Image 2 [128,109,164,150] Scene Graph Image dict about objects and attributes Obtain the coordinates of regions from the Image corresponding to the Target Entity. Highlight the listed Region in the Image. Identify the pixel difference between Image 1 and Image 2. Analyzed by LMMs, structured description of the objects, their attributes, and the relationships in Image. Table 1: Tools used in our CIGEVAL framework. Text-guided Image Editing: = [p, Isrc]. This task is similar to Mask-guided Image Editing but does not provide mask. The model must identify the region to edit automatically. Subject-driven Image Generation: = [p, S], where is the image of specific subject. The aim is to generate an image that reflects in relation to the subject S. Subject-driven Image Editing: = [S, p, Isrc], where Isrc is source image, and is the subject reference. The goal is to replace the subject in Isrc with S. Multi-concept Image Composition: = [S1, S2, p, Isrc], where S1 and S2 are images of two subjects. The task is to combine these to create new image according to p. Control-guided Image Generation: = [Icontrol, p], where Icontrol is control signal, such as depth map, canny edge, or bounding box. The aim is to generate an image that follows these lowlevel visual cues. In this paper, following previous work (Mañas et al., 2024; Lin et al., 2024), we investigate the semantic consistency of generated images with the above conditions."
        },
        {
            "title": "3.2 Toolbox",
            "content": "Evaluating image generation that involves multiple conditions can be challenging. Drawing inspiration from prior research (Cheng et al., 2024a; Zhang et al., 2024) we have developed multi-functional toolbox, including Grounding, Difference, Highlight and Scene Graph. Each tool is designed to target specific aspects of image analysis or editing, and outputs either modified image or textual information. Detailed descriptions of each tool can be found in Table 1. Specifically, we implement Grounding with GroundingDino (Liu et al., 2024). Scene Graph uses the same prompting method as CCoT (Mitra et al., 2024) based on GPT-4o. This tool can also function effectively with other open-source LMMs (refer to Sec. 4.4). To assist LMMs in detecting subtle differences between edited images, the Difference tool compares the pixels of two images and identifies the locations of the variations. The Highlight tool emphasizes selected regions by reducing the pixel values of areas outside the highlighted zone to 1/4 of their original values, thereby darkening these areas and accentuating the highlighted region. This tool is typically used after the Grounding and Difference tools have provided the region coordinates."
        },
        {
            "title": "3.3 Framework",
            "content": "In our approach, we conceptualize the image evaluation process as an agent task. As shown in Figure 2, the core of CIGEVAL is well-instructed LMM, which autonomously utilizes tools to assess wide range of conditional image generation tasks. 4 accordance with the setting of Ku et al. (2023), we assume each sub-score weights the same and used min operation to emphasize the importance of meeting all criteria without exception."
        },
        {
            "title": "3.4 Agent Tuning",
            "content": "Previous research has primarily relied on closedsource LMMs to address agentic tasks, mainly due to their superior abilities in tool calling and instruction following (Chen et al., 2024b; Song et al., 2024; Zeng et al., 2024; Xu et al., 2023). As evidenced in Table 3, open-source models significantly underperform compared to GPT-4o. To bridge this gap and empower smaller LMMs as effective evaluators, we aim to perform supervised fine-tuning on 7B models to integrate agentic capabilities into them. To curate high-quality trajectory data, we employ GPT-4o to carry out the evaluation process in Section 3.3. The process begins by providing GPT4o with evaluation instructions and corresponding images. At each turn, the agent receives an observation, formulates plans and thoughts as thought, and invokes relevant tools through action. The tool outputs serve as new observations for the subsequent turn. By iterating the above process, we can construct complete evaluation trajectory including the initial instruction, intermediate steps (i.e., observations, thoughts, actions), and the final scoring result. To ensure the quality of these trajectories, we exclude samples where the discrepancy between predicted scores and human evaluation scores exceeds 0.3. Using 60% of the ImagenHub data, we ultimately gather 2,274 high-quality trajectories for supervised fine-tuning. Using this structured trajectory data, we perform supervised fine-tuning on Qwen2-VL-7B-Instruct and Qwen2.5-VL-7B-Instruct (Wang et al., 2024). Formally, each samples evaluation trajectory is represented as o0, t1, a1, . . . , on1, tn, an, on, where oi, ti, and ai denote the observation, thought, and action at each turn (i) respectively. Specifically, o0 refers to the initial observation consisting of the evaluation instructions and accompanied images, and on denotes the final score. At each turn, based on the preceding trajectory ci = o0, t1, a1, . . . , oi1, the agent aims to generate thought ti and action ai. During the fine-tuning process, we only compute the cross-entropy loss Figure 2: The evaluation process of CIGEVAL regarding the example in Figure 1. CIGEVAL autonomously selects appropriate tools for each decomposed sub-task, and then conducts fine-grained analyses based on the observed tool outputs. The prompts used in this framework are listed in Appendix A. Specifically, we adopt divide-and-conquer scheme for evaluating images generated under multiple conditions. For example, in the subject-driven image editing task in Figure 1, the desired synthesized image will incorporate the object from subject reference while maintaining the background of the source image. Therefore, we break down each evaluation task into several fine-grained subquestions from the listed below: (1) Is the image generation following the prompt? (2) Is the image editing following the instruction? (3) Is the image performing minimal edit without changing the background? (4) Is the object in the Image following the provided subject? (5) Is the image following the control guidance? Then, for each sub-question, CIGEVAL selects the most suitable tool from its toolbox, focusing on the specific aspect of evaluation. For example, Grounding and Highlight are utilized for comparing specific regions of the image, while Scene Graph evaluates background preservation and the extent of over-editing. With these intermediate results, the LMM analyzes the tool outputs and assigns scores in the ReAct format (Yao et al., 2023), ranging from 0 to 10, which are normalized to the [0.0, 1.0] range for comparison with human ratings. These fine-grained scores are aggregated through: = min(α1, ..., αi) (2) where αi represents one of the sub-scores. In 5 for ti and ai while ci is masked: = log (cid:88) i=1 Pr(ti, aici). (3)"
        },
        {
            "title": "4.1 Evaluation Benchmark",
            "content": "ImagenHub (Ku et al., 2023) is standardized benchmark for evaluating conditional image generation models with human raters. The statistics for ImagenHub are presented in Table 2. This large-scale benchmark covers 7 mainstream tasks, 29 models, 4.8K synthesized images, and 14.4K human ratings, making it suitable for calculating correlation scores between automatic evaluation metrics and human raters. list of 29 evaluated models can be found in Appendix B. Each image was assessed by three human raters according to the guidelines of the defined task, and final score in the range [0.0, 1.0] was reported for the average score. Images are scored in two aspects: (1) Semantic Consistency assesses how well the generated image aligns with the given conditions, such as prompts and subject tokens, ensuring coherence and relevance to the specified task criteria. (2) Perceptual Quality evaluates the extent to which the generated image appears visually authentic and conveys sense of naturalness. In this work, we focus on the Semantic Consistency score, leaving the exploration of Perceptual Quality for future research."
        },
        {
            "title": "4.2 Existing Auto-metrics",
            "content": "Here we list some prominent automatic metrics: CLIP-Score (Hessel et al., 2021): This metric computes the average cosine similarities between prompt and generated image CLIP embeddings, making it popular choice for assessing image-text alignment. LPIPS (Zhang et al., 2018) measures the similarity between two images in manner that aligns with human perception. CLIP-I (Gal et al., 2022) calculates the average pairwise cosine similarities between CLIP embeddings of generated and source images. DINO (Ruiz et al., 2023) is computed by the mean cosine similarity computed between the DINO embeddings of ViT-S/16 (Caron et al., 2021) for both synthesized and source images. # Instructions # Images # Human Ratings Text-guided Image Generation (5 models) 197 985 Mask-guided Image Editing (4 models) 179 716 2148 Text-guided Image Editing (8 models) 1432 4296 Subject-driven Image Generation (4 models) 150 600 Subject-driven Image Editing (3 models) 154 462 1386 Multi-concept Image Composition (3 models) 306 918 Control-guided Image Generation (2 models) 150 300 Sum of 7 tasks 1111 4801 14403 Table 2: Statistics of ImagenHub: the number of instructions, evaluated models, synthesized images, and human ratings used in this paper. VIESCORE (Ku et al., 2024) prompts large multimodal models to evaluate generated images in an explainable and fine-grained manner. Based on GPT-4o, it currently represents the state-of-the-art across all seven tasks on ImagenHub. 4."
        },
        {
            "title": "Implementation Details",
            "content": "In all experiments, GPT-4o refers to the model version GPT-4o-2024-05-13, aligning with the original VIESCORE paper (Ku et al., 2024). For the experiments in Sec. 4.4, we evaluate using the entire ImagenHub benchmark. In the ablation study, we randomly select 50 images for each task. For the experiments in Sec. 4.5, we generate training data using 60% of the ImagenHub dataset, as described in Section 3.4, and use the remaining data for testing. We fine-tune the Qwen2-VL-7B-Instruct and Qwen2.5-VL-7B-Instruct models using Megatron-LM. The fine-tuning process employs learning rate of 1e-5 and batch size of 128, with sequence length of 32,768. We use AdamW optimizer with cosine learning scheduler with 3% warm-up steps. 6 Method Textguided IG Maskguided IE Textguided IE Controlguided IG Subjectdriven IG Subjectdriven IE Multiconcept IC Human Raters 0. CLIPScore LPIPS DINO CLIP-I VIESCORE CIGEVAL VIESCORE CIGEVAL VIESCORE CIGEVAL -0.0817 - - - 0.1948 0. 0.4218 0.4347 0.4989 0.5027 0.5390 - -0.1012 - - 0.2037 0.2843 0.3555 0. 0.5421 0.5465 0.4230 - 0.0956 - - 0.5443 - 0.3699 - - 0. - - 0.4160 0.2961 0.4887 - - 0.3022 0.2834 0.5927 - - 0.0979 0.1512 LLaMA3-LLaVA-NeXT-8B 0.0363 0. 0.4001 0.4487 Qwen2.5-VL-7B-Instruct 0.0252 0.2567 0.2836 0.3752 GPT-4o 0.1592 0.2891 -0.1153 -0. 0.1308 0.3704 0.1432 0.2164 0.4264 0.4374 -0.0452 0.4863 0.3328 0.3251 0.2516 0. 0.4062 0.4090 0.4972 0.5402 0.4806 0.4930 0.4800 0.5185 0.4516 0.4931 0.4459 0. Avg. 0.4700 - - - - Table 3: Spearman correlation scores across 7 conditional image generation tasks with different LMMs as backbone. The abbreviations IG, IE and IC stand for Image Generation, Image Editing and Image Composition respectively. - means not applicable."
        },
        {
            "title": "4.4 Main Experiments",
            "content": "For all presented correlations, we applied Fisher Z-transformation to estimate the average Spearman correlation [1, 1] across models and tasks. Metric-to-Human (M-H) correlations. In Table 3, we present the correlations across all tasks, utilizing different backbone models. When using GPT-4o as the underlying LMM, CIGEVAL achieves the state-of-the-art performance across all 7 tasks. It achieves an average Spearman correlation of 0.4625 with human raters, closely matching the human-to-human correlation. The primary improvements are observed in tasks involving multiple conditions, such as control-guided image generation and multi-concept image composition, where previous evaluation metrics struggle. When the underlying LMM is replaced with different open-source models, CIGEVAL consistently outperforms VIESCORE. However, the performance of open-source models is still poor and falls significantly behind GPT-4o. Therefore, we perform agent tuning on these models as described in Sec. 3.4 and report their improved performance in Sec. 4.5. Overall, the experiment demonstrates that CIGEVAL outperforms VIESCORE across variety of image editing and generation tasks, consistently maintaining its edge even when different underlying LMMs are used. Ablation study. To assess the necessity of each tool in CIGEVAL, we conducted an ablation study detailed in Table 4. Since Highlight is often acConfiguration CIGEVAL w/o Grounding w/o Difference w/o Scene Graph Scene Graph with Qwen2.5-VL-7B-Instruct Scene Graph with Qwen2.5-VL-70B-Instruct Avg. 0.7262 0.6376 0.7020 0.6471 0.7120 0.7311 Table 4: Ablation study regarding tools in CIGEVAL (GPT-4o) with different configurations. companied with Grounding and Difference, we do not perform specific ablation on Highlight. The study shows that the complete CIGEVAL configuration achieves the highest average score of 0.7262. When each tool is removed, noticeable drop is observed, underscoring the necessity of every tool for effective performance. On the other hand, when the implementation of the Scene Graph was switched from GPT-4o to an open-source model, the evaluation results remained high with minimal variation. In fact, when replaced with the Qwen2.5-VL-70B, the performance improved further, showcasing the robustness of our agentic framework. Overall, the ablation study underscores that each tool in the CIGEVAL configuration is useful, and their collective integration is crucial for achieving superior performance. Method Textguided IG Maskguided IE Textguided IE Controlguided IG Subjectdriven IG Subjectdriven IE Multiconcept IC Avg. Previous SOTA 0.3081 0. 0.4649 0.5246 0.7105 0.4694 0.5616 0. VIESCORE CIGEVAL + Tuning VIESCORE CIGEVAL + Tuning 0.3457 0.1890 0.4609 0.3699 0.3054 0.4099 0.0158 0.1418 0.2796 -0.1398 0.1974 0. Qwen2.5-VL-7B-Instruct 0.0086 0.4586 0.5916 0.2395 0.3130 0.5876 Qwen2-VL-7B-Instruct 0.1024 0.1438 0.3846 0.3420 0.2615 0.6096 0.1837 0.4485 0.4659 0.1553 0.5096 0. 0.0967 0.5216 0.5458 0.0682 0.1226 0.5975 0.4388 0.4496 0.5778 0.5129 0.5035 0.6691 0.1876 0.3455 0.4631 0.1989 0.2840 0. Table 5: Spearman correlations across 7 tasks with VIESCORE and CIGEVAL based on open-source small LMMs. Previous SOTA here means VIESCORE based on GPT-4o. Figure 3: Case study of text-guided image editing example with low human annotation score."
        },
        {
            "title": "4.5 CIGEVAL with Agent Tuning",
            "content": "The experimental results in Table 5 show the performance of CIGEVAL after agent tuning. Despite utilizing 7B open-source LMMs as the underlying model, Qwen2-VL-7B-Instruct and Qwen2.5-VL7B-Instruct demonstrate 76% and 34% improvement in correlation after fine-tuning, respectively, With only 2,274 filtered evaluation trajectories, the fine-tuned 7B models surpass the previous state-ofthe-art VIEScore based on GPT-4o. This demonstrates the data efficiency of agent tuning and the importance of synthetic data quality."
        },
        {
            "title": "4.6 Case Study",
            "content": "To demonstrate the effectiveness of our CIGEVAL framework and the importance of each tool, we present subject-driven image editing example in Figure 1, text-guided image editing example in Figure 3, and multi-concept image composition example in Figure 5. In the first and third example, by directly prompting in VIESCORE, GPT4o struggles to compare the similarity of specific objects between two images. By grounding and highlighting the focused object (i.e., glasses and flowers), GPT-4o can find the difference in shapes and colors within our framework. In the second example, when discussing the background preservation aspect, VIESCORE considers the over-editing small. However, in our framework, CIGEVAL first calls the Scene Graph tool to get an overall composition of the edited image, and then finds out newly-added tub based on tool outputs, successfully arriving at the correct score. These examples have shown CIGEVALs ability to autonomously select appropriate tools and reach correct conclusions based on the observation, which makes CIGEVAL better evaluator than VIEScore. Preliminary study with GPT-4o image generation. Recently GPT-4o image generation has attracted wide attention. As shown in Figure 4, we extend CIGEVAL with an additional OCR tool and find that our framework assigns appropriate scores to 4o-generated images on OpenAIs official web8 Figure 4: Case study of GPT-4os image generation. Examples are adapted from OpenAIs official website. site2. Furthermore, we test GPT-4o image generation on ImagenHubs various tasks and report the CIGEval scores and human annotations (averaged between two annotators). We have the following three findings: (1) CIGEVAL assigns scores that closely align with human annotations, and effectively detects subtle flaws in GPT-4o-generated images. (2) GPT-4o excels at tasks involving single image as input, such as text-guided image generation and editing, as well as subject-driven image generation, as shown in Figures 9 to 11. (3) GPT-4o struggles with complex tasks that require multiple images and control signals. For example, the subjects in Figures 7 and 8 are not accurately replicated, showing unintended changes in color or shapes. Moreover, consistent with the findings of Yan et al. (2025), we observe that GPT-4o tends to favor color palette dominated by yellow, orange, and warm lighting, as exemplified by the pot in Figure 7 and the man in the rearview mirror in Figure 11. Additionally, the control guidances (e.g., canny edges, OpenPose) are not strictly followed, as seen in Figure 6. 2https://openai.com/index/ introducing-4o-image-generation/"
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose CIGEVAL, unified, explainable and agentic framework for image evaluation across seven popular conditional image evaluation tasks. CIGEVAL utilizes large multimodal models (LMMs) at its core to autonomously select tools for fine-grained evaluation. Experiments show that, when using GPT-4o as the backbone model, CIGEVAL surpasses achieves high correlation of 0.4625 with human raters, closely matching the human-to-human correlation of 0.47. Additionally, we have synthesized 2,274 high-quality evaluation trajectories to incorporate agentic capabilities into smaller LMMs. After agent tuning, the 7B LMMs surpass the performance of the previous state-of-the-art method based on the closed-source GPT-4o. These experimental findings and case studies on GPT-4o image generation suggest that CIGEVAL holds substantial promise for replicating human-like performance in evaluating synthetic images."
        },
        {
            "title": "Limitations",
            "content": "Although CIGEVAL improves the correlation between automatic image evaluators and human raters, there are certain limitations to our approach. 9 First, when using closed-source models APIs, such as GPT-4o, there is risk that AI-generated images resembling real people or photographs may be rejected by GPT-4o for evaluation, potentially affecting the frameworks robustness. Second, our experiments primarily focus on evaluating images consistency with multiple conditions, leaving the assessment of perceptual quality for future research. Due to the lack of more comprehensive benchmark for conditional image generation, we synthesized tuning data and conducted experiments exclusively on ImagenHub. Expanding our experiments to more text-to-image generation and text-based image editing datasets (Peng et al., 2024; Hui et al., 2024) could be beneficial. Finally, the training process currently utilizes only correct trajectory data and discards failed trajectory data. In the future, we aim to refine the CIGEVAL framework to include broader range of tasks and leverage failed data for contrastive training of the model."
        },
        {
            "title": "References",
            "content": "Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1820818218. Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. 2022. Text2live: Text-driven layered image and video editing. In European Conference on Computer Vision, pages 707723. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image 2023. editing instructions. In CVPR. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. Emerging properties in self-supervised vision transformers. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 9630 9640. Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024a. Mllm-asa-judge: assessing multimodal llm-as-a-judge with vision-language benchmark. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William Cohen. 2023. Subject-driven text-to-image generaarXiv preprint tion via apprenticeship learning. arXiv:2304.00186. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. 2025. Janus-pro: Unified multimodal understanding and generation with data and model scaling. Preprint, arXiv:2501.17811. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024b. Agent-FLAN: Designing data and methods of effective agent tuning for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 9354 9366, Bangkok, Thailand. Association for Computational Linguistics. Chuanqi Cheng, Jian Guan, Wei Wu, and Rui Yan. 2024a. From the least to the most: Building plugand-play visual reasoner via data synthesis. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 4941 4957, Miami, Florida, USA. Association for Computational Linguistics. Xiaoxue Cheng, Junyi Li, Xin Zhao, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Kun Gai, and Ji-Rong Wen. 2024b. Small agent can also rock! empowering small language models as hallucination detector. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1460014615, Miami, Florida, USA. Association for Computational Linguistics. Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. 2022. Diffedit: Diffusion-based semantic image editing with mask guidance. In The Eleventh International Conference on Learning Representations. deep floyd.ai. 2023. If by deepfloyd lab at stabilityai. Emily Denton, Soumith Chintala, Rob Fergus, et al. 2015. Deep generative image models using laplacian pyramid of adversarial networks. Advances in neural information processing systems, 28. Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, volume 34, pages 87808794. Curran Associates, Inc. Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, and Chunhua Shen. 2024. Freecustom: Tuning-free customized image generation for multi-concept composition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacafarm: simulation framework for methods that learn from human feedback. Preprint, arXiv:2305.14387. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023a. Gptscore: Evaluate as you desire. ArXiv, abs/2302.04166. 10 Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. 2023b. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. Preprint, arXiv:2306.09344. metrics for conditional image synthesis evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1226812290, Bangkok, Thailand. Association for Computational Linguistics. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. 2022. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations. Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, et al. 2023. Photoswap: Personalized subject swapping in images. arXiv preprint arXiv:2305.18286. Zinan Guo, Yanze Wu, Zhuowei Chen, Lang chen, Peng Zhang, and Qian HE. 2024. PuLID: Pure and lightning ID customization via contrastive alignment. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Zecheng He, Bo Sun, Felix Juefei-Xu, Haoyu Ma, Ankit Ramchandani, Vincent Cheung, Siddharth Shah, Anmol Kalia, Harihar Subramanyam, Alireza Zareian, Li Chen, Ankit Jain, Ning Zhang, Peizhao Zhang, Roshan Sumbaly, Peter Vajda, and Animesh Sinha. 2024. Imagine yourself: Tuning-free personalized image generation. Preprint, arXiv:2409.13346. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. In AdDenoising diffusion probabilistic models. vances in Neural Information Processing Systems, volume 33, pages 68406851. Curran Associates, Inc. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. 2024. Hq-edit: high-quality dataset Preprint, for instruction-based image editing. arXiv:2404.09990. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. 2017. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134. Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. 2023. Imagenhub: Standardizing the evaluation of condiarXiv preprint tional image generation models. arXiv:2310.01596. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2023. Multi-concept In Procustomization of text-to-image diffusion. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941. Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, et al. 2023. Holistic evaluation of textto-image models. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Dongxu Li, Junnan Li, and Steven CH Hoi. 2023a. Blipdiffusion: Pre-trained subject representation for controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 1288812900. PMLR. Tianle Li, Max Ku, Cong Wei, and Wenhu Chen. 2023b. Dreamedit: Subject-driven image editing. arXiv preprint arXiv:2306.12624. Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen, Wanqi Zhong, Chenyang Lyu, Wei Wang, and Min Zhang. 2024. comprehensive evaluation of gpt-4v on knowledge-intensive visual question answering. Preprint, arXiv:2311.07536. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. 2024. Evaluating text-to-visual generation with image-to-text generation. Preprint, arXiv:2404.01291. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. 2024. Grounding dino: Marrying dino with grounded pretraining for open-set object detection. In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part XLVII, page 3855, Berlin, Heidelberg. Springer-Verlag. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. 2024. VIEScore: Towards explainable Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. 2023. Cones 2: Customizable image synthesis with multiple subjects. arXiv preprint arXiv:2305.19327. Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. 2023. LLMScore: Unveiling the power of large language models in text-to-image synthesis evaluation. In Thirty-seventh Conference on Neural Information Processing Systems. Andreas Lugmayr, Martin Danelljan, Andrés Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. 2022. Repaint: Inpainting using denoising diffusion probabilistic models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1145111461. Oscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, and Michal Drozdzal. 2024. Improving text-to-image consistency via automatic prompt optimization. Preprint, arXiv:2403.17804. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2021. Sdedit: Guided image synthesis and editing with In International stochastic differential equations. Conference on Learning Representations. Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. 2024. Compositional Chain-ofThought Prompting for Large Multimodal Models . In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14420 14431, Los Alamitos, CA, USA. IEEE Computer Society. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2023. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6038 6047. OpenAI. 2023. Gpt-4 technical report. Preprint, arXiv:2303.08774. openjourney.ai. 2023. Openjourney is an open source stable diffusion fine tuned model on midjourney images. Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. 2023. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111. Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. 2024. Dreambench++: human-aligned benchmark for personalized image generation. Preprint, arXiv:2406.16855. Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. 2023. Unicontrol: unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion modIn Proceedings of the IEEE/CVF conference els. on computer vision and pattern recognition, pages 1068410695. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2250022510. runwayml. 2023. Stable diffusion inpainting. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. 2023. Emu edit: Precise image editing via recognition and generation tasks. arXiv preprint arXiv:2311.10089. Yifan Song, Weimin Xiong, Xiutian Zhao, Dawei Zhu, Wenhao Wu, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. 2024. AgentBank: Towards generalized LLM agents via fine-tuning on 50000+ interaction trajectories. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 21242141, Miami, Florida, USA. Association for Computational Linguistics. stability.ai. 2023. Stable diffusion xl. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024. 12 Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold. 2023b. Gpt-4v(ision) as generalist evaluator for vision-language tasks. ArXiv, abs/2311.01361. Zhiyuan Zhang, DongDong Chen, and Jing Liao. 2024. Sgedit: Bridging llm with text2image generative model for scene graph-based image editing. ACM Trans. Graph., 43(6). Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. Preprint, arXiv:2409.12191. Chen Henry Wu and Fernando De la Torre. 2023. latent space of stochastic diffusion models for zeroshot image editing and guidance. In ICCV. Zhenran Xu, Senbao Shi, Baotian Hu, Longyue Wang, and Min Zhang. 2024. MultiSkill: Evaluating large multimodal models for fine-grained alignment skills. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 15061523, Miami, Florida, USA. Association for Computational Linguistics. Zhenran Xu, Senbao Shi, Baotian Hu, Jindi Yu, Dongfang Li, Min Zhang, and Yuxiang Wu. 2023. Towards reasoning in large language models via multi-agent peer review collaboration. Preprint, arXiv:2311.08152. Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. 2025. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. Preprint, arXiv:2504.02782. Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. 2023. The dawn of lmms: Preliminary explorations with gpt-4v(ision). Preprint, arXiv:2309.17421. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Huaying Yuan, Ziliang Zhao, Shuting Wang, Shitao Xiao, Minheng Ni, Zheng Liu, and Zhicheng Dou. 2025. FineRAG: Fine-grained retrieval-augmented text-to-image generation. In Proceedings of the 31st International Conference on Computational Linguistics, pages 1119611205, Abu Dhabi, UAE. Association for Computational Linguistics. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2024. AgentTuning: Enabling generalized agent abilities for LLMs. In Findings of the Association for Computational Linguistics: ACL 2024, pages 30533077, Bangkok, Thailand. Association for Computational Linguistics. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. 2023a. Magicbrush: manually annotated dataset for instruction-guided image editing. NeurIPS dataset and benchmark track. Lvmin Zhang and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as perceptual metric. In CVPR. 13 Text-guided Image Editing: MagicBrush (Zhang et al., 2023a), InstructPix2Pix (Brooks et al., 2023), Prompt-to-Prompt (Mokady et al., 2023), CycleDiffusion (Wu and la Torre, 2023), SDEdit (Meng et al., 2021), Text2Live (Bar-Tal et al., 2022), DiffEdit (Couairon et al., 2022), Pix2PixZero (Parmar et al., 2023). Subject-driven Image Generation: DreamBooth (Ruiz et al., 2023), DreamBooth-Lora (Hu et al., 2021), BLIP-Diffusion (Li et al., 2023a), TextualInversion (Gal et al., 2022). Subject-driven Image Editing: PhotoSwap (Gu et al., 2023), DreamEdit (Li et al., 2023b), BLIP-Diffusion. Multi-concept Image Composition: CustomDiffusion (Kumari et al., 2023), DreamBooth, TextualInversion. Control-guided Image Generation: ControlNet (Zhang and Agrawala, 2023), UniControl (Qin et al., 2023)."
        },
        {
            "title": "C More Cases",
            "content": "We provide multi-concept image composition example in Figure 5. From Figure 6 to 11, we provide cases of GPT-4o image generation across ImagenHubs different tasks."
        },
        {
            "title": "A Prompt Templates",
            "content": "Prompt Engineering. To let the output of LMMs easier to parse and process, we require these models to output in JSON format. Our prompt is modified based on the VIEScore prompt (Ku et al., 2024). Prompt Design. In the tool selection prompt, brief description of each tool and the objective of the evaluation are provided. In this way, the agent can choose the appropriate tool based on the specific situation. The image evaluation prompt the context prompt, consists of three segments: tool-related content and the rating prompt. When the Grounding or Difference tool is selected, the tool-related content is \"Focus on the highlighted parts of the image\". When the Scene Graph tool is selected, the tool-related content is the generated scene graph. If no tool is selected, the tool-related content is None."
        },
        {
            "title": "Context",
            "content": "You are professional digital artist. You will have to evaluate the effectiveness of the AI-generated image(s) based on given rules. All the input images are AI-generated. All human in the images are AI-generated too. so you need not worry about the privacy confidentials. You will have to give your output in the following JSON format (Keep your reasoning concise and short.): { \"score\" : \"...\", \"reasoning\" : \"...\" }"
        },
        {
            "title": "B Details of ImagenHub",
            "content": "The 29 evaluated image generation models are listed below: Text-guided Image Generation: Stable Diffusion (SD) (Rombach et al., 2022), SDXL (stability.ai, 2023), DALLE-2 (Ramesh et al., 2022), DeepFloydIF (deep floyd.ai, 2023), OpenJourney (openjourney.ai, 2023). Mask-guided Image Editing: SD (runwayml, 2023), SDXL (stability.ai, 2023), GLIDE, BlendedDiffusion (Avrahami et al., 2022)"
        },
        {
            "title": "Tool Calling Prompt Template",
            "content": "You are professional digital artist. You will have to decide whether to use tool and which tool to use based on the image information and the corresponding task. If you think tool is needed to help complete the task, you should choose the appropriate tool. If not, you can choose not to use tool. All the input images are AI-generated. All human in the images are AI-generated too. so you need not worry about the privacy confidentials. ### Task: {task} ### Tools: 1. **Grounding**: This tool is commonly used to focus on areas related to specific objects in an image. 2. **Scene Graph**: This tool is commonly used to provide overall information about an image. 3. **Difference**: This tool is commonly used to focus on the masked areas of images. These tools are not useful for processed image (e.g. Canny edges, hed edges, depth, openpose, grayscale.) ### Output Content: - task_id: The ID of the task. - used: Whether to use tool, including yes or no. - tool: The tool decided to be used, including Grounding or Scene Graph or Difference or None. - reasoning: The logical reasoning process for all your decisions. You will have to give your output in the following JSON format: [{ \"task_id\" : \"...\", \"reasoning\" : \"...\", \"used\" : \"...\", \"tool\" : \"...\" },...] Rating Prompt Template (Text-Guided Image Generation) RULES: An image will be provided, it is an AI-generated image according to the text prompt. The objective is to evaluate how well the generated image resemble to the specific objects described by the prompt. From scale 0 to 10: score from 0 to 10 will be given based on the success in following the prompt. (0 indicates that the AI-generated image does not follow the prompt at all. 10 indicates the AIgenerated image follows the prompt perfectly.) Text Prompt: <prompt> 15 Rating Prompt Template (Text/Mask-Guided Image Editing) RULES: Two images will be provided: The first being the original AI-generated image and the second being an edited version of the first. The objective is to evaluate how successfully the editing instruction has been executed in the second image. Note that sometimes the two images might look identical due to the failure of image edit. From scale 0 to 10: score from 0 to 10 will be given based on the success of the editing. (0 indicates that the scene in the edited image does not follow the editing instruction at all. 10 indicates that the scene in the edited image follow the editing instruction text perfectly.) Editing instruction: <instruction> RULES: Two images will be provided: The first being the original AI-generated image and the second being an edited version of the first. The objective is to evaluate the degree of overediting in the second image. From scale 0 to 10: score from 0 to 10 will rate the degree of overediting in the second image. (0 indicates that the scene in the edited image is lot different from the original. 10 indicates that the edited image can be recognized as minimal edited yet effective version of original.) Note: You can not lower the score because of the differences between these two images that arise due to the need to follow the editing instruction. Editing instruction: <instruction> Rating Prompt Template (Control-Guided Image Generation) RULES: Two images will be provided: The first being processed image (e.g. Canny edges, hed edges, depth, openpose, grayscale.) and the second being an AI-generated image using the first image as guidance. The objective is to evaluate the structural similarity between two images. From scale 0 to 10: score from 0 to 10 will rate how well the generated image is following the guidance image. (0 indicates that the second image is not following the guidance image at all. 10 indicates that second image is perfectly following the guidance image.) RULES: An image will be provided, it is an AI-generated image according to the text prompt. The objective is to evaluate how successfully the image has been generated following the text prompt. From scale 0 to 10: score from 0 to 10 will be given based on the success in following the prompt. (0 indicates that the image does not follow the prompt at all. 10 indicates the image follows the prompt perfectly.) Text Prompt: <prompt> 16 Rating Prompt Template (Subject-Driven Image Generation) RULES: Two images will be provided: The first image is token subject image. The second image is an AI-generated image, it should contain subject that looks alike the subject in the first image. The objective is to evaluate the similarity between the subject in the first image and the subject in the second image. From scale 0 to 10: score from 0 to 10 will rate how well the subject in the generated image resemble to the token subject in the first image. (0 indicates that the subject in the second image does not look like the token subject at all. 10 indicates the subject in the second image look exactly alike the token subject.) Subject: <subject> RULES: An image will be provided, it is an AI-generated image according to the text prompt. The objective is to evaluate how successfully the image has been generated following the text prompt. From scale 0 to 10: score from 0 to 10 will be given based on the success in following the prompt. (0 indicates that the image does not follow the prompt at all. 10 indicates the image follows the prompt perfectly.) Text Prompt: <prompt> Rating Prompt Template (Subject-Guided Image Editing) RULES: Two images will be provided: The first image is token subject image. The second image is an AI-edited image, it should contain subject that looks alike the subject in the first image. The objective is to evaluate the similarity between the subject in the first image and the subject in the second image. From scale 0 to 10: score from 0 to 10 will rate how well the subject in the generated image resemble to the token subject in the first image. (0 indicates that the subject in the second image does not look like the token subject at all. 10 indicates the subject in the second image look exactly alike the token subject.) Subject: <subject> RULES: Two images will be provided: The first image is input image to be edited. The second image is an AI-edited image, it should contain background that looks alike the background in the first image. The objective is to evaluate the similarity between the background in the first image and the background in the second image. From scale 0 to 10: score from 0 to 10 will rate how well the background in the generated image resemble to the background in the first image. (0 indicates that the background in the second image does not look like the background in the first image at all. 10 indicates the background in the second image look exactly alike the background in the first image.) 17 Rating Prompt Template (Multi-concept Image Composition) RULES: Two images will be provided: The first image is token subject image. The second image is an AI-generated image, it should contain subject that looks alike the subject in the first image, and it is generated based on the text prompt. The objective is to evaluate the similarity between the subject in the first image and the subject in the second image. Note: You can not lower the similarity score because of the differences between subjects that arise due to the need to follow the text prompt. From scale 0 to 10: score from 0 to 10 will rate how well the subject in the generated image resemble to the token subject in the first image. (0 indicates that the subject in the second image does not look like the token subject at all. 10 indicates the subject in the second image look exactly alike the token subject.) Subject: <subject> Text Prompt: <text> RULES: An AI-generated image will be provided. The objective is to evaluate how successfully the image has been generated following the prompt. From scale 0 to 10: score from 0 to 10 will be given based on the success in following the prompt. (0 indicates that the image does not follow the prompt at all. 10 indicates the image follows the prompt perfectly.) Text Prompt: <text> Figure 5: Case study of multi-concept image composition example. Here is the fine-grained score for concept consistency. 18 Figure 6: Case study of GPT-4os image generation. Examples are taken from ImagenHubs control-guided image generation task. Figure 7: Case study of GPT-4os image generation. Examples are taken from ImagenHubs multi-concept image composition task. Figure 8: Case study of GPT-4os image generation. Examples are taken from ImagenHubs subject-driven image editing task. Figure 9: Case study of GPT-4os image generation. Examples are taken from ImagenHubs subject-driven image generation task. 20 Figure 10: Case study of GPT-4os image generation. Examples are taken from ImagenHubs text-guided image generation task. Figure 11: Case study of GPT-4os image generation. Examples are taken from ImagenHubs text-guided image editing task."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology (Shenzhen), Shenzhen, China"
    ]
}