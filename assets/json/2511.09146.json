{
    "paper_title": "DoPE: Denoising Rotary Position Embedding",
    "authors": [
        "Jing Xiong",
        "Liyang Fan",
        "Hui Shen",
        "Zunhai Su",
        "Min Yang",
        "Lingpeng Kong",
        "Ngai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as a noisy feature map, and propose Denoising Positional Encoding (DoPE), a training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with a parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DoPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing a simple yet powerful solution for improving length generalization. Our project page is Project: https://The-physical-picture-of-LLMs.github.io"
        },
        {
            "title": "Start",
            "content": "DoPE: Denoising Rotary Position Embedding Jing Xiong1*, Liyang Fan3*, Hui Shen2, Zunhai Su1, Min Yang3, Lingpeng Kong1, and Ngai Wong1 1The University of Hong Kong 2University of Michigan, Ann Arbor 3Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences Contact: junexiong@connect.hku.hk Project: https://The-physical-picture-of-LLMs.github.io 5 2 0 2 2 1 ] . [ 1 6 4 1 9 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Rotary Position Embedding (RoPE) in Transformer models has inherent limits that weaken length extrapolation. We reinterpret the attention map with positional encoding as noisy feature map, and propose Denoising Positional Encoding (DOPE), training-free method based on truncated matrix entropy to detect outlier frequency bands in the feature map. Leveraging the noise characteristics of the feature map, we further reparameterize it with parameter-free Gaussian distribution to achieve robust extrapolation. Our method theoretically reveals the underlying cause of the attention sink phenomenon and its connection to truncated matrix entropy. Experiments on needle-in-a-haystack and many-shot in-context learning tasks demonstrate that DOPE significantly improves retrieval accuracy and reasoning stability across extended contexts (up to 64K tokens). The results show that the denoising strategy for positional embeddings effectively mitigates attention sinks and restores balanced attention patterns, providing simple yet powerful solution for improving length generalization."
        },
        {
            "title": "Introduction",
            "content": "The position encoding is key component of the large language models (LLMs), influencing interactions among tokens. Attention scores are computed as the dot product of query and key vectors: Attn(i, j) QR , KR . (1) Position encodings are often added to the query and key vectors to incorporate sequence order. Among various techniques, Rotary Position Embedding (RoPE) (Su et al., 2024) is widely used due to its ability to encode relative positions directly within the dot-product operation, where the query and key are rotated as: *Equal contribution Corresponding author QR = Rθ(i)Qi, KR = Rθ(j)Kj. (2) However, previous work such as DAPE (Zheng et al., 2024), which replaces position encoding with additional MLPs on attention scores, and NoPE (Wang et al., 2024), which operates without any positional encoding, both suggest that the classical RoPE schemes may constrain Transformer (Vaswani et al., 2017) performance. In this work, we conceptualize position encoding as noisy feature map via the truncated matrix entropy (Xiong et al., 2024). We measure the noise level of the features and employ parameter-free Gaussian distribution to perform length extrapolation. Specifically, the main contributions of this paper are as follows: We introduce the truncated matrix entropy to identify noisy heads and model the position encoding as parameter-free Gaussian distribution to achieve length extrapolation. We theoretically and empirically identify that the low-frequency alignment of positional encodings used for computing attention score is the fundamental cause of the attention sink and structural sparsity phenomena such as retrieval heads. We find that attention heads with strong extrapolation ability exhibit low-rank structures; keeping their position encodings and removing those of high-rank heads yields up to 10-point improvement without training."
        },
        {
            "title": "2 Background",
            "content": "In this section, we first review the Transformer architecture of Vaswani et al. (2017), with focus on its multi-head attention mechanism. Then, we analyze the role of rotary position embeddings (RoPE) in shaping the attention patterns. 2.1 Multi-Head Self-Attention We consider causal language model implemented as decoder-only Transformer. Given token representations Rnd, we form queries, keys, and values via where common choice is ωf = b2f /dh and θ(m) denotes the vector of per-pair rotation phases at position RoPE rotates queries/keys as QR = R(θi)Qi, KR = R(θj)Kj. (10) = XWQ, = XWK, = XWV , (3) Relative-position property. For any positions i, and vectors q, Rdh, with projection matrices WQ, WK, WV Rd(hdh), (4) where denotes the models hidden dimension, and equal per-head widths dh (so each head has dimension dh and the concatenated width is hdh). Reshape to per-head tensors: Q, K, Rhndh. (5) Let R1nn be the causal mask, with 0 on and below the diagonal and above. Masked attention per head is = softmax (cid:18) QK dh (cid:19) + Rhnn, (6) = AV Rhndh, (7) where denotes the output of self-attention."
        },
        {
            "title": "2.2 Rotary Position Embedding",
            "content": "Most LLMs adopt Rotary Position Embedding (RoPE) (Su et al., 2024) as their default positional encoding mechanism, which has become the de facto standard in contemporary architectures. RoPE encodes token positions by rotating each query/key vector on sequence of two-dimensional planes. This formulation allows attention scores to depend on relative positional offsets while preserving the simple dot-product structure. Definition. Let the per-head width be dh (assume dh is even). Split head into dh/2 complex components by pairing dimensions (2f, 2f +1). indexes the dh/2 two-dimensional subspaces, each corresponding to pair of consecutive feature dimensions used to apply distinct rotation frequency. For an integer position and frequency schedule {ωf }dh/21 (with base b>1), define: =0 R(ϕ) = (cid:20)cos ϕ sin ϕ cos ϕ sin ϕ (cid:21) , (8) Rθ(m) = diag(R(ω0m), . . . , R(ωdh/21m)), (9) R(θi)q, R(θj)k = q, R(θ(ji))k, (11) so the score depends on the relative offset (ji) while preserving the efficiency of the dot product."
        },
        {
            "title": "3 Related Work",
            "content": "Length Extrapolation With RoPE. RoPE has been widely adopted (Su et al., 2024). It is employed in models such as LLaMA2 (Touvron et al., 2023), LLaMA3 (Dubey et al., 2024), and Qwen3 (Yang et al., 2025) where token order is encoded by rotating vectors at positiondependent angles. Moreover, RoPE and its variants are extensively used in wide range of recent large models, including Qwen (Bai et al., 2023), Qwen2 (Team, 2024a), Qwen2.5 (Team, 2024b), Qwen2.5-VL (Bai et al., 2025), Qwen3 (Yang et al., 2025), Mistral (Jiang et al., 2023), Gemma (Team et al., 2024a), Gemma-2 (Team et al., 2024b), and Gemma-3 (Team et al., 2025). However, when the input length exceeds the training length by several times (Peng et al., 2023; Chen et al., 2023; Ding et al., 2024), model performance degrades severely. This degradation is not unique to RoPE; similar issues occur with other positional encodings such as ALiBi (Press et al., 2021) and Kerple (Chi et al., 2022). To address this problem, different approaches have been proposed. For instance, FIRE (Li et al., 2023) alleviates longcontext degradation by introducing learnable positional encodings, where MLPs are used to generate suitable positional representations. In contrast, NTK-based methods (Peng et al., 2023) improve long-context extrapolation by modifying the frequency spectrum, thereby extending the context length and enhancing stability on long sequences. Length Extrapolation Without Positional Encoding. Positional encodings provide sequence awareness and enhance models expressive capacity (Shaw et al., 2018; Yun et al., 2019; Luo et al., 2022). While several studies (Zuo et al., 2024; Haviv et al., 2022; Köcher et al., 2025) have shown that causal decoder-based Transformers can implicitly capture token order information, NoPE (Kazemnejad et al., 2023) further demonstrates that the causal mask itself inherently encodes positional relationships. More recently, data-dependent positional encoding methods, such as DAPE (Zheng et al., 2024) and DAPE v2 (Zheng et al., 2024), have been proposed to enhance length extrapolation by treating positional encodings as input-dependent feature maps of the attention. However, these approaches still rely on learnable parameter matrix to construct the positional encoding."
        },
        {
            "title": "4 Denoising Positional Encoding",
            "content": "4.1 Outlier Features Recent studies (Jin et al., 2025; Qiao and Huang, 2025) reveal that RoPE can induce outlier channels in the query and key representations: small subset of low-frequency rotary bands exhibit abnormally large ℓ2 norms, and give rise to distinctive row/column bright-band patterns in the QK matrix. This subsection formalizes this phenomenon through simple spectral analysis of individual RoPE bands."
        },
        {
            "title": "4.2 Spectral Analysis via the Gram Matrix\nConvention. Let QR, KR ∈ RN ×dh denote the\nRoPE-rotated queries/keys. For RoPE band f , de-\nfine the band-projected matrices",
            "content": "Q = Pf QR, = Pf KR, (12) R2dh where Pf selects coordinates (2f, 2f +1). We work per head; extension to multiple heads is by concatenation. Band-wise Gram Matrix. Consider single RoPE frequency band acting on 2-D subspace via planar rotations. For position j, let the key projected onto this band be (cid:98)kj = βj R(θj) k, βj βmin > 0, (13) where βj denotes the per-position magnitude of the key on this RoPE band (i.e., the ℓ2 norm of its component in the 2-D plane), βmin > 0 is fixed lower bound ensuring nondegeneracy over the visible window, θj is the RoPE phase at position j, and is unit direction in that plane. Assume lowfrequency cone condition (Deshpande et al., 2014) within the positional encoding window: for angles θj there exists unit vector aligned with the mean direction and halfangle γk < π 2 such that u, R(θj)k cos γk for all j. (14) Form the Gram (covariance) matrix on this band: Σk = = (cid:88) j= (cid:88) j=1 (cid:98)kj (cid:98)k R(θj) kkR(θj). β2 (15) Spectral Lower Bound. Let the sum of projected keys be = (cid:80)N j=1 (cid:98)kj and set = S/S. By the Rayleigh quotient and CauchySchwarz, λmax(Σk) xΣkx = = (cid:88) (x, (cid:98)kj)2 j=1 1 (cid:16) (cid:88) j=1 (cid:17) x, (cid:98)kj S2 . (16)"
        },
        {
            "title": "Using the cone condition with x aligned to the",
            "content": "mean direction gives = (cid:13) (cid:13) (cid:13) (cid:88) j=1 βj R(θj)k (cid:13) (cid:13) (cid:13) (cid:88) j=1 βj x, R(θj)k βmin cos γk, (17) and hence the spectral lower bound λmax(Σk) β2 min k2 cos2 γk, σ1(K ) = (cid:112)λmax(Σk) βmin cos γk. (18) An entirely analogous argument for queries matrix on the same band yields σ1(Q ) αmin cos γq, (19) with amplitudes αk αmin > 0 and cone halfangle γq for the query directions. Now consider the attention score submatrix contributed by this band, Af = . (20) K Aligning left/right singular directions of and (up to an angular mismatch ψ between their principal directions) gives the producttype lower bound σ1(Q σ1(Af ) 1 αminβmin ) σ1(K ) cos ψ q cos γQ cos γK cos ψ. (21) Figure 1: Visualization of DoPE where ψ is the angle between the principal left singular direction of and the principal right singular direction of in the band plane, and if ψ := (uQ, uK) denotes the angle between the band-wise mean directions, then by the cone conditions ψ ψ γQ + γK, hence cos ψ cos( ψ + γQ + γK). These inequalities formalize the following physical picture: when RoPE band is sufficiently lowfrequency so that its rotations stay within cone (no phase reversals over the context), the projected keys and queries add coherently in that 2D plane. The resulting Gram matrices acquire dominant eigenvalue that scales like Θ(N ) (hence top )), producing singular values scale like Θ( pronounced principal directionthe largenorm spike observed in practice. When both and cohere on the same band, the score matrix gains an approximately rankone dominant component with large top singular value, which then yields row/columnwise bright bands after the softmax. Applying Lemma A.1 to Af gives (Af )ij αminβmin max i,j k cos γQ cos γK cos ψ, (22) i.e. at least one score entry remains Ω(1) rather than vanishing with ."
        },
        {
            "title": "4.3 Denoising via Truncated Matrix Entropy",
            "content": "Multi-band Matrix Entropy. For attention head let the RoPE-rotated key matrix be kR h, RN dh, and let Pk project onto RoPE frequency band . The band-wise Gram matrix is Σh,f = k h,f = h,f (cid:88) j=1 (cid:98)k(h,f ) (cid:98)k(h,f ) R22. be its trace-normalized form. The matrix entropy of band in head is defined as Hh,f = tr( Σh,f log Σh,f ), 0 Hh,f log 2. (25) Aggregating over all RoPE bands within the head gives Hh = 1 dh/2 dh/2 (cid:88) =1 Hh,f , 0 Hh log 2. (26) small Hh indicates that several RoPE bands collapse to low-rank spectra (dominated by coherent low-frequency spikes), while large Hh implies isotropic covariance and thus denoised, balanced positional encoding within the head. Truncated Matrix Entropy. Following Xiong et al. (2024), we introduce the shorthand for the effective rank of the band Gram matrix so that eh = exp(Hh), ρHh = 1 eh . (27) (28) smaller ρh,f (low spectral entropy) corresponds to more concentrated spectrum and thus larger principal eigenvalue. Combining this with the earlier spectral bound suggests the approximate proportionality λmax(Σh) ρHh. (29) In practice, we are often interested not in the full effective rank, but in its truncated version, which discounts negligible spectral mass. Given eigenvalues λ1 λ2 λr of Σh and threshold {1, 8, 16, 32, 64}, the truncated effective rank is defined as"
        },
        {
            "title": "Let",
            "content": "Σh,f = Σh,f tr(Σh,f ) (23) (24) (cid:32) ρr = exp (cid:88) i=1 λi j=1 λj (cid:80)r log λi j=1 λj (cid:80)r (cid:33) , (30) where denotes the number of the top singular values. This truncated version measures the effective dimensionality of the dominant spectrum, ignoring low-energy tails. For low-frequency RoPE bands, ρr typically collapses to O(1), reflecting near rankone structure, while for denoised or decorrelated representations it approaches the full dimension ( 2 in the band plane), indicating isotropy. Head Selection. To avoid uniformly modifying all attention heads, we perform selection at the head level based on the truncated matrix entropy. smaller ρ indicates that this head exhibits stronger near-rank-one spike structures across multiple frequency bands and is thus more likely to produce bright-band artifacts. Accordingly, we define head-level mask mh = 1[ρ τ ], (31) where the threshold τ can be chosen as quantile (e.g., selecting the lowest-τ heads). Only when mh = 1 do we remove the positional encoding for this head; otherwise, the RoPE positional encoding of that head remains unchanged (all bands are retained). In this way, only heads with low truncated matrix entropy (i.e., more spiky and anisotropic spectra) have their low-entropy frequency bands attenuated or masked, while other heads preserve their original RoPE encoding. This targeted denoising suppresses coherent low-frequency modes responsible for positional artifacts, without degrading overall positional representation capacity. DoPE-by-parts. In practice, denoising can be achieved by selectively attenuating or removing RoPE components associated with low matrix entropy (small ρr h), i.e., frequency bands whose Gram spectra are highly concentrated and thus dominated by single coherent direction. These bands correspond to outlier rotary modes that produce brightband artifacts in the attention map. Formally, for the selected head h, we construct frequency band mask based on the threshold: (cid:104) mh,f = 1 θf θ (cid:105) , (32) where θ is threshold chosen to retain sufficiently isotropic bands while filtering those dominated by low-rank spikes. θ = 2π , (33) where is the training length. The denoised key representation is then obtained by KR,D = dh/2 (cid:88) =1 mh,f Pf KR , (34) and analogously for queries QR,D . This operation removes coherent low-rank positional modes while preserving isotropic components that contribute to balanced attention, and eliminates the persistent bright-band patterns (i.e., attention sinks) in the attention score matrix, achieving denoised and more uniform positional encoding. DoPE-by-all. In this variant, denoising is performed by applying the head-level mask to the entire positional encoding of each head, rather than completely zeroing out the head. Specifically, for each head h, we multiply its RoPE-rotated queries and keys by the scalar mask mh {0, 1}: KR,D = mh KR , QR,D = mh QR . (35) Heads with mh = 0 (i.e., low truncated matrix entropy ρ h) have their entire positional encoding removed, while those with mh = 1 retain their original RoPE positional features. This DoPE-byall strategy masks the positional encoding at the head level in single step, removing anisotropic or low-rank positional modes while preserving the remaining heads balanced representations. DoPE-by-Gaussian. In this variant, denoising is performed by applying the head-level mask to the positional encoding and replacing the removed parts with Gaussian noise. Specifically, for each head h, we define KR,D = mh KR + (1 mh) ϵK,h, (36) QR,D = mh QR + (1 mh) ϵQ,h, (37) where ϵK,h, ϵQ,h (0, σ2I) are Gaussian noise matrices whose variance σ2 matches the empirical variance of the retained RoPE components. Thus, heads with mh = 0 (low truncated matrix entropy) have their positional encoding replaced by isotropic Gaussian samples, while those with mh = 1 retain their original RoPE positional features. This DoPE-by-Gaussian strategy suppresses coherent low-rank positional modes and injects isotropic randomness that restores spectral diversity, acting as stochastic regularization mechanism for the attention representation. Table 1: Summary of experimental configurations and results for denoising strategies. The Indicator column denotes whether the matrix entropy is computed using the Query or Key representations for selecting specific heads. Entropy Type denotes the entropy measure used: Vanilla matrix entropy or Trunc-r (truncated effective rank ρr h,k with threshold r). # Heads indicates the number of attention heads selected for denoising. Criterion refers to the computation stage of entropy: ntk (after applying the NTK positional encoding), pre_ntk (before NTK scaling), or post_rope (after RoPE application). Sort Order specifies the masking direction: ASC removes heads with the lower entropy (i.e., low-entropy heads), while DESC removes heads with the higher entropy (i.e., high-entropy heads). Results are reported for two extrapolation lengths: 24,756 (24k) and 65,536 (64k). This table is filtered to show the top 3 results for the Noisy (64k) and Original (64k) settings, respectively, for each parameter combination. Method Indicator Entropy Type # Heads Criterion Sort Order Noisy (24k) Original (24k) Noisy (64k) Original (64k) Dynamic NTK DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all Query Key Key Query Key Key Key Key Query Query Query Key Query Key Key Key Query Query Query Query Query Query Key Key Key Key Query Key Key Query Query Query Query Full Trunc-32 Trunc-16 Trunc-16 Trunc-16 Trunc-8 Trunc-4 Full Trunc-1 Trunc-1 Full Trunc-32 Trunc-32 Trunc-32 Trunc-32 Trunc-32 Trunc-16 Trunc-8 Trunc-8 Trunc-32 Full Trunc-32 Trunc-32 Trunc-16 Trunc-16 Full Full Trunc-8 Trunc-4 Trunc-1 Trunc-1 Trunc-1 Trunc-1 5 3 5 5 3 1 3 2 5 3 30 25 30 20 25 2 2 3 3 3 5 3 3 3 3 3 1 3 2 5 3 3 post_ntk_query post_ntk_key pre_ntk_key pre_ntk_query pre_ntk_key post_ntk_key post_ntk_key post_ntk_key post_ntk_query post_ntk_query post_ntk_query post_rope_key post_ntk_query post_ntk_key post_ntk_key post_ntk_key post_ntk_query post_ntk_query post_ntk_query post_rope_query post_rope_query post_ntk_query post_ntk_key post_rope_key pre_ntk_key post_ntk_key pre_ntk_query post_ntk_key post_ntk_key post_ntk_query post_ntk_query post_ntk_query post_rope_query DESC ASC ASC ASC ASC DESC DESC DESC ASC ASC ASC ASC ASC ASC ASC ASC DESC DESC DESC ASC ASC DESC ASC DESC ASC DESC ASC DESC DESC DESC ASC ASC DESC 75.417 62.521 84.354 77.417 77.104 77.438 75.250 65.833 73.229 75.167 72.583 44. 76.229 76.604 76.458 76.042 76.104 75.438 75.229 75.271 74.500 74.125 75.438 81.958 65.958 76.583 75.625 73.542 74.917 65.958 75.104 75.000 73.104 46.771 91.896 94.938 94.396 93.708 93.563 93.125 92.229 89.354 90.188 92.938 89.688 76.188 93.063 93.042 92.875 92.854 92.771 92.354 91.771 92.146 92.125 92.479 91.958 93.833 93.771 93.729 93.271 93.250 92.000 89.813 92.354 92.917 90.063 87. 40.417 23.208 40.875 40.604 25.521 41.271 45.667 45.375 44.229 42.208 41.479 44.042 40.312 40.458 40.771 40.188 40.021 42.729 42.521 42.438 40.313 40.125 40.938 40.917 35.354 41.354 39.729 39.333 46.000 45.292 44.292 42.729 41.646 27.000 60.938 36.813 60.896 60.313 46.813 60.021 64.042 61.979 64.292 70.083 69.438 65. 60.375 61.917 61.333 60.625 61.083 60.729 61.104 59.583 62.208 62.146 62.125 61.271 61.063 57.833 58.021 63.146 63.625 62.646 64.146 70.083 69.708 69."
        },
        {
            "title": "5 Experiment\n5.1 Experimental Setup",
            "content": "The \"needle-in-a-haystack\" synthesis task presents particularly challenging problem in the field of natural language processing and information retrieval. The essence of this task is to identify and synthesize highly relevant but sparse information from large volumes of data, where the key insights are hidden amidst vast amounts of irrelevant or less useful content. This challenge is akin to finding \"Needle-in-a-Haystack\", where the desired information is not only rare but often deeply embedded in long, complex documents or across multiple sources. The experiments are divided into two parts: original setups and noisy setups. Original Setups. In the baseline experiments, we insert the needle at various positions within the context under three settings of context length24K and 64K tokens. This design enables us to assess the models capacity to retrieve specific information across different contextual spans. Noisy Setups. In contrast, the noisy experiments are performed under the same two context-length configurations (24K and 64K tokens), wherein attention sink symbols are placed adjacent to the needle to introduce controlled perturbations. This table is filtered to show the top 3 results for the Noisy (64k) and Original (64k) settings, respectively, for each parameter combination. This experimental design enables systematic evaluation of the models robustness and stability under noisy or unreliable data, providing deeper insights into its resilience and potential real-world applicability. In contrast, the noisy experiments are conducted Table 2: Summary of experimental configurations and results for denoising strategies on Qwen2.5-Math-7B extrapolation in the Many-Shot In-Context Learning task. Model is extrapolated from 4K context to 16K context window. Experiments utilize In-Context Learning (ICL) constructed from the nlile/hendrycks-MATH-benchmark dataset. Two experimental settings are evaluated: (1) Needle Insertionproblem inserted at specific depth positions within ICL haystack, with four possible positions (beginning, 1/3, 2/3, end); (2) Skip Needleno problem insertion, baseline performance. Indicator specifies whether denoising is applied to Query or Key representations. Entropy Type denotes the entropy measure used: Full (full matrix entropy Hh,k) or Trunc-ε (truncated effective rank ρ(ε) h,k with threshold ε). # Heads indicates the number of attention heads selected for denoising. Criterion refers to the computation stage of entropy: ntk (after NTK scaling), pre_ntk (before NTK scaling), or post_rope (after RoPE application). Sort Order determines the selection directionDESC (highest entropy) or ASC (lowest entropy). Results are accuracy scores on 100 sampled MATH problems (400 total configurations across 4 insertion positions). Method Indicator Entropy Type # Heads Criterion Sort Order Needle Insert (8K) Skip Needle (8K) Needle Insert (16K) Skip Needle (16K) Zero-shot Baseline Many-shot Baseline DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-parts DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all DOPE-by-all Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Query Trunc-1 Trunc-16 Trunc-1 Trunc-4 Trunc-1 Trunc-4 Trunc-1 Full Full Trunc-16 Trunc-1 Trunc-16 Trunc-4 Trunc-4 Trunc-8 Trunc-1 Trunc-16 Full Full Full Trunc-1 Trunc-4 Trunc-8 Trunc-1 Trunc-1 Trunc-4 Trunc-8 Trunc-16 Full Trunc-1 Trunc-16 Trunc-16 1 1 3 5 5 3 2 1 3 3 1 2 5 3 3 5 5 1 2 3 1 2 2 5 3 5 3 5 3 2 1 3 post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query post_ntk_query ASC ASC ASC ASC ASC ASC ASC DESC DESC ASC ASC ASC ASC ASC ASC ASC ASC DESC DESC DESC ASC ASC ASC ASC ASC ASC ASC ASC DESC ASC ASC ASC 0.430 0.373 0.393 0.380 0.375 0.375 0.318 0.358 0.345 0.388 0.370 0.355 0.388 0.380 0.368 0.360 0.363 0.355 0.365 0.375 0.400 0.388 0.395 0.383 0.383 0.338 0.353 0.375 0.375 0.360 0.393 0.363 0.365 0.353 0.430 0. 0.410 0.360 0.370 0.440 0.440 0.430 0.380 0.400 0.340 0.420 0.410 0.330 0.390 0.420 0.390 0.350 0.380 0.350 0.380 0.390 0.430 0.390 0.390 0.480 0.440 0.440 0.440 0.360 0.350 0.380 0.370 0.340 0.430 0.240 0.228 0.225 0.238 0.225 0.238 0.223 0.258 0.258 0.255 0.248 0.230 0.245 0.220 0.240 0.220 0.245 0.243 0.245 0.258 0. 0.235 0.215 0.225 0.243 0.258 0.220 0.205 0.263 0.258 0.243 0.228 0.253 0.430 0.230 0.250 0.250 0.220 0.190 0.220 0.210 0.240 0.230 0.270 0.260 0.250 0.260 0.260 0.230 0.180 0.240 0.260 0.240 0.230 0.250 0.240 0.240 0.220 0.220 0.210 0.200 0.190 0.240 0.210 0.250 0.250 0.240 Table 3: Ablation study: Performance on 64k extrapolation using attention heads selected at different sequence lengths. Each configuration uses heads identified from sequences of length 24k, 32k, 48k, 56k, and 64k, then evaluates on the 64k task under both Noisy and Vanilla conditions. Scores demonstrate the impact of head selection length on final performance. Method Indicator Entropy Type # Heads Criterion Sort Order Noisy Vanilla Noisy Vanilla Noisy Vanilla Noisy Vanilla Noisy Vanilla Dynamic NTK DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-parts DOPE-by-parts DOPE-by-all DOPE-by-all Key Query Query Query Key Query Trunc-8 Trunc-1 Trunc-16 Trunc-32 Trunc-8 Trunc-1 1 5 2 3 1 5 40.417 60.938 40. 60.938 40.417 60.938 40.417 60.938 40. 60.938 post_ntk_key post_ntk_query post_ntk_query post_rope_query post_ntk_key post_ntk_query DESC ASC DESC ASC DESC ASC 40.896 35.667 41.792 40.313 40.625 37.063 62.438 56.708 61.271 61.688 62.208 61.292 40.417 30.354 41.479 39.333 40.541 32. 63.125 61.979 65.020 66.979 65.229 65.000 28.666 43.604 41.479 39.333 29.604 43.000 60.104 69.166 65.020 66.979 59.979 75.187 28.666 38.020 41.479 39.333 29.604 40.458 60.104 69.854 65.020 66.979 59.979 73.812 45.667 42.208 42.729 40.313 46.000 42. 64.042 70.083 60.729 62.208 63.625 70.083 24k Heads 32k Heads 48k Heads 56k Heads 64k Heads under the same two context-length settings (24K and 64K tokens), but noise tokens such as the startof-sequence symbol (which easily form attention sinks) are inserted after the needle to emulate imperfect conditions. This experimental design enables us to assess the models robustness and stability in the presence of noise or attention sink, thereby offering insights into the relationship between the models attention sink and matrix entropy. Hyperparameters of Head Selection. Head selection is performed globally across all (l h) attention heads, where is the number of layers and is the number of heads per layer (32 layers 32 heads = 1,024 total heads for LLaMA-38B; 28 layers 28 heads = 784 total heads for Qwen2.5-Math-7B). We experiment with selecting 132 heads based on either ascending (ASC, selecting lowest entropy heads) or descending (DESC, selecting highest entropy heads) order. Table 4: Ablation study on attention head identification. The table compares the performance when using different datasets (MATH vs. NIH) to select attention heads for denoising, followed by testing the results in manyshot in-context learning. All experiments are conducted on 8K context with Qwen2.5-Math-7B model. Head selection is performed using Query representations with post-NTK criterion. Results show accuracy scores on MATH problems across two settings: Needle Insertion (answer inserted within ICL haystack) and Skip Needle (a setting where answers are not inserted into the context). Method Entropy Type # Heads Selection Dataset Needle Insert (8K) Skip Needle (8K) Zero-shot Baseline Many-shot Baseline Heads selected using MATH dataset DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-parts DOPE-by-parts DOPE-by-all DOPE-by-all Trunc-4 Trunc-1 Trunc-4 Trunc-1 Trunc-1 Trunc-1 Heads selected using NIH dataset DOPE-by-Gaussian DOPE-by-Gaussian DOPE-by-parts DOPE-by-parts DOPE-by-all DOPE-by-all Full Trunc-16 Trunc-16 Full Trunc-16 Trunc-1 5 1 3 1 5 1 3 1 2 2 5 2 MATH MATH MATH MATH MATH MATH NIH NIH NIH NIH NIH NIH 0.430 0.240 0.375 0.393 0.360 0.388 0.338 0.395 0.365 0.375 0.372 0.350 0.360 0.417 0.430 0.230 0.440 0.410 0.420 0.410 0.480 0. 0.390 0.410 0.330 0.390 0.420 0.390 Entropy can be computed at three different stages in the forward pass. By comparing and analyzing these three stages, we can capture different aspects of positional encoding effects: (1) pre-NTK: entropy is computed on the original query/key representations after the projection layer but before any PE is applied, reflecting the models behavior without positional encoding; (2) post-NTK: entropy is computed after applying Dynamic-NTK scaling to the RoPE base frequency θ, capturing the effect of frequency scaling on the covariance structure; (3) post-RoPE: entropy is computed after the full RoPE rotation has been applied to the query/key representations, measuring the final positional encodings impact on attention patterns. Additionally, entropy can be computed separately for query representations, key representations. This yields six possible entropy computation strategies (3 stages 2 components), plus the option to compute entropy on both query and key jointly. In our experiments, we denote these configurations using the notation Criterion (e.g., post_ntk_query means entropy is computed on query representations after NTK scaling). For the NIH task, head selection is performed using sequences at the target extrapolation length (e.g., heads identified on 64K sequences are used for 64K evaluation, as shown in Table 3). For the many-shot in-context learning task, we compare head selection using entropy scores computed on both the MATH dataset and the NIH dataset to evaluate cross-task transferability  (Table 4)  . All conducted experiments using are SGLang (Zheng et al., 2023) (v0.5.3rc0) with the FlashAttention-3 backend (Shah et al., 2024). Tensor parallelism is enabled for multi-GPU inference when necessary. CUDA graphs are disabled to support dynamic context lengths. 5.2 Main results We conduct experiments under two settings: original setups and noisy setups. Our results are presented in Table 1. Our findings are summarized as follows: (i) The model exhibits sharp performance degradation after introducing attention sink tokens, i.e., under the noisy setting. (ii) Under the shorter context setting (24k tokens), DOPE achieves its best performance when Gaussian noise is added to the positional encodings, improving from the 75.417 baseline to 84.354. This indirectly supports the insight that the layer-wise accumulation of repeatedly applied positional encodings is well modeled by Gaussian distribution. (iii) Truncated matrix entropy and (vallina) matrix entropy exhibit distinctly different patterns. For the truncated variant, we sort values in descending order and prune the low-entropy heads; for the matrix entropy, we sort in ascending order and prune the high-entropy heads. Both strategies perform well, but truncated matrix entropy typically achieves better results. (iv) In extremely sparse regimesfor example, with 64K context lengthusing the truncated matrix entropy with = 1 (which can be regarded as equivalent to the spectral norm, i.e., σmax(Σ)) yields the best results. This indicates that the sparser the setting, the sharper (more sparse) the singular value distribution becomes."
        },
        {
            "title": "5.3 Many-Shot In-Context Learning",
            "content": "We present the models performance under manyshot in-context learning (MICL) scenarios (Agarwal et al., 2024) in Table 2. Experiments are conducted both with test exemplar inserted into the in-context exemplars (needle-in-a-haystack) and without test exemplars (in-context learning). This task not only depends on the models ability to find needle in haystack, but also tests whether the model can identify similar reasoning patterns from the context. In simple terms, we have the following findings. (a) DoPE by Vanilla Matrix Entropy. (b) DoPE by Truncated Matrix Entropy. Figure 2: Comparison of attention weight entropy across all heads and top-16 heads (depth 0). Figure 3: High matrix entropy head (Layer 5, Head 11) Figure 4: Low matrix entropy head (Layer 1, Head 2) Figure 5: High truncated matrix entropy example (Layer 4, Head 12) Figure 6: Low truncated matrix entropy example (Layer 5, Head 11) The Curse of Length. At an appropriate length, MICL can significantly enhance the models reasoning ability. However, when the length extends to 16K, the models final reasoning ability drops significantly. More exemplars does not lead to better performance, indirectly demonstrating that complex reasoning is constrained by the extrapolation length. The Curse of Shortcut. We inserted exemplars of the test samples into the in-context examples. Surprisingly, rather than copying the correct answers in needle-in-a-haystack manner, the models overall performance dropped substantially at the 24K and 64K context lengths. vertical axis (sequence length), compared to the head selected by the matrix entropy shown in Figure 4. This observation also reveals why the truncated matrix entropy can identify heads that perform better in extrapolation than those selected by the full matrix entropy."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented Denoising Positional Encoding (DOPE), parameter-free approach that mitigates low-rank artifacts in Rotary Position Embedding through truncated matrix entropy analysis. By identifying noisy heads and suppressing low-entropy frequency bands, DOPE effectively reduces attention sinks and restores balanced positional representations. Experiments on long-context and noisy settings confirm that DOPE improves extrapolation and reasoning stability across various models. This work highlights that truncated matrix entropy provides simple yet principled direction for enhancing positional encoding and long-sequence generalization in Transformers. 5.4 Matrix Entropy Meets Attention Sink In this section, we directly visualize the attention distributions of the heads identified by the hightruncated matrix entropy. Figure 2 clearly reveals the connection between truncated matrix entropy and the attention distribution, particularly the attention sink. We can conclude from Fig. 2b that when the truncated matrix identify low-entropy heads, they tend to produce severe attention sink (recency bias), while the remaining high-entropy heads correctly allocate attention to the inserted needle. In contrast, in Fig. 2a, the overall high matrix entropy indicates serious attention sink, and although the low-entropy heads generate relatively normal attention distributions, they fail to locate the correct needle position. 5.5 RoPE Induces Low-rankness To demonstrate the low-rank characteristics of the truncated matrix entropy left by the attention heads, this section visualizes the similarity of token representations and the principal component singular value components of attention heads selected by our two distinct entropy metrics: Vanilla Matrix Entropy (denoted Hh) and Top-16 Truncated Matrix Entropy (denoted ρ(16) ). Figures 3 and 6 visualize the cosine similarity between the query vectors of tokens (Y-axis, token position) and the eigenvectors corresponding to their full 128 hiddendimensional eigenvalues (X-axis). This projection onto = 128 basis allows us to observe the effective dimensionality each head utilizes. Our analysis reveals insight: the two entropy metrics identify two functionally distinct attention heads with low-rank structures. Low-rankness. We observe clear low-rankness in the heads selected by the entropy. Figure 4 and Figure 5 shows head keep by its low matrix entropy and high truncated matrix entropy, respectively. Visually, its similirity (color) is mainly concentrated in the first few of the total dimensions. This low-rank head shows that it relies on only few dimensions to perform extrapolation, meaning that heads with good extrapolation performance (retrieval heads) use only small subset of features for retrieval. Periodicity. Figure 5 shows that the similarity distribution of the head selected by the truncated matrix entropy exhibits clear periodicity along the"
        },
        {
            "title": "References",
            "content": "Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, and 1 others. 2024. Many-shot in-context learning. Advances in Neural Information Processing Systems, 37:76930 76966. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, and 1 others. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. 2023. CLEX: Continuous length extrapolation for large language models. In International Conference on Learning Representations. Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky. 2022. KERPLE: Kernelized relative positional embedding for length extrapolation. Advances in Neural Information Processing Systems, 35:83868399. Yash Deshpande, Andrea Montanari, and Emile Richard. 2014. Cone-constrained principal component analysis. Advances in Neural Information Processing Systems, 27. Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. 2024. LongRoPE: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv e-prints, pages arXiv2407. emoZilla. 2023. Dynamically scaled rope further increases performance of long context llama with https://www.reddit.com/r/ zero fine-tuning. LocalLLaMA/comments/14mrgpr/dynamically_ scaled_rope_further_increases/. Reddit post. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, . . . , and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Proceedings of the Thirty-Fifth Conference on Neural Information Processing Systems (NeurIPS) Datasets Benchmarks Track. Albert Q. Jiang, Alexandre Sablayrolles, and 1 others. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, and Yongfeng Zhang. 2025. Massive values in self-attention modules are the key to contextual knowledge understanding. arXiv preprint arXiv:2502.01563. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. 2023. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36:2489224928. Chris Köcher, Alexander Kozachinskiy, Anthony Widjaja Lin, Marco Sälzer, and Georg Zetzsche. 2025. Nope: The counting power of transformers with no positional encodings. arXiv preprint arXiv:2505.11199. et al. Li. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. 2023. Functional interpolation for relative positions improves long context transformers. In International Conference on Learning Representations. Shengjie Luo, Shanda Li, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. 2022. Your transformer may not be as powerful as you expect. Advances in Neural Information Processing Systems, 35:43014315. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. YaRN: Efficient context window extension of large language models. In International Conference on Learning Representations. Ofir Press, Noah Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations. Ye Qiao and Sitao Huang. 2025. Q-roar: Outlier-aware rescaling for rope position interpolation in quantized long-context llms. arXiv preprint arXiv:2509.14391. Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. 2022. Transformer language models without positional encodings still learn positional information. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 13821390. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. 2024. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. 2019. Are transformers universal approximators of arXiv preprint sequence-to-sequence functions? arXiv:1912.10077. Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, and 1 others. 2024. Dape: Data-adaptive positional encoding for length extrapolation. Advances in Neural Information Processing Systems. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody_Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, and 1 others. 2023. Efficiently programming large language models using sglang. Chunsheng Zuo, Pavel Guerzhoy, and Michael Guerzhoy. 2024. Position information emerges in causal transformers without positional encodings via similarity of nearby embeddings. arXiv preprint arXiv:2501.00073. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464468. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, and 1 others. 2025. Gemma 3 technical report. arXiv preprint arXiv:2503.19786. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, and 1 others. 2024a. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, and 1 others. 2024b. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Qwen Team. 2024a. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2. Qwen Team. 2024b. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in Neural Information Processing Systems, 30. Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, and Xiaoling Wang. 2024. Length generalization of causal transformarXiv preprint ers without position encoding. arXiv:2404.12224. Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Lingpeng Kong, and 1 others. 2024. Uncomp: Uncertainty-aware long-context compressor for efficient large language model inference. arXiv preprint arXiv:2410.03090."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Proofs Lemma A.1 (Entry-level lower bound (rectangular)). Let Rmn with largest singular value σ1(M). Then max i,j (M)ij σ1(M) mn . (38) Proof. By the Frobenius/spectral norm relation, (cid:88) M2 = (cid:88) (Mij)2 ( max i,j (Mij))2 mn, max i,j (M)ij i= j=1 MF mn σ1(M) mn , (39) since M2 = (cid:80) σr(M)2 σ1(M)2. Remark A.2. In the square case = = , Lemma A.1 reduces to maxi,j(M)ij σ1(M)/N . Corollary A.3 (Band contribution, rectangular Rmn be the at- / case). Let Ak = tention score submatrix contributed by RoPE band Rn2. Under k, with the cone conditions and amplitude lower bounds αi αmin > 0, βj βmin > 0, we have Rm2 and σ1(Q σ1(K k) αmin k) βmin cos γQ, cos γK , (40) and aligning principal directions up to angle ψ gives σ1(Q σ1(Ak) 1 αminβmin k) σ1(K k) cos ψ mn k cos γQ cos γK cos ψ. (41) Applying Lemma A.1 to Ak yields the entry-level bound max i,j (Ak)ij σ1(Ak) mn k cos γQ cos γK cos ψ, (42) αminβmin i.e. at least one score entry contributed by band remains Ω(1) under the stated assumptions (does not decay with m, n). A.2 Experimental Setup Models. Qwen-1.5-7B (Li, 2023), Qwen2.5Math-7B (Yang et al., 2024) and LLaMA-3-8BInstruct (Grattafiori et al., 2024) are decoder-only transformer models that employ Rotary Positional Embeddings (RoPE) for encoding positional information. Qwen-1.5-7B is trained with maximum context length of 32K tokens, while LLaMA-3-8B is trained with 8K-token context window. To support longer contexts beyond their pre-training limits, we apply RoPE-based extrapolation (e.g., Dynamic-NTK), which rescales RoPE frequencies to improve stability and retrieval performance in extended-context settings. Hyperparameter. All experiments use greedy decoding with temperature set to 0.0 and top-p set to 1.0. For the needle-in-a-haystack (NIH) task on LLaMA-3-8B-Instruct, we set max_new_tokens to 50 with stop conditions including newline characters (<0x0A>) and stop token ID 144. For the many-shot in-context learning (MICL) task on Qwen2.5-Math-7B, we set max_new_tokens to 2048 with stop sequences </s>, <im_end>, <endoftext>, and Problem: to prevent generating additional problems. Context buffers of 200 tokens (NIH) and 2,300 tokens (MICL) are reserved for prompt templates and final questions. For RoPE extrapolation, we apply DynamicNTK scaling (emoZilla, 2023) with the scaling factor computed as α = Ltarget/Loriginal, where Ltarget {24K, 64K, 128K} for NIH experiments and Ltarget = 16K for MICL experiments, while Loriginal corresponds to each models pre-trained maximum position embeddings (32K for Qwen-1.5-7B, 8K for LLaMA-3-8B-Instruct, and 4K for Qwen2.5-Math-7B). For LLaMA3, we additionally evaluate NTK-by-parts (Peng et al., 2023) with low_freq_factor= 1.0 and high_freq_factor= 32.0. The NIH task uses 10 uniformly spaced depth positions (0%, 10%, ..., 100%) for needle insertion at each context length. The MICL task evaluates 100 sampled problems from the MATH dataset (Hendrycks et al., 2021), with needle insertion at four fixed depth positions (0%, 33%, 67%, 100%, corresponding to beginning, 1/3, 2/3, and end) within the in-context examples, yielding 400 total test configurations. For DOPE, Gaussian noise is sampled from (0, 1) with standard deviation σ = 1.0, using fixed random seed (42) to ensure reproducibility. The truncated matrix entropy is computed by retaining the top-k singular values where {1, 4, 8, 16, 32}, with = 1 corresponding to using only the spectral norm σmax(Σ). We also evaluate the full (untruncated) matrix entropy for comparison."
        }
    ],
    "affiliations": [
        "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences",
        "The University of Hong Kong",
        "University of Michigan, Ann Arbor"
    ]
}