{
    "paper_title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
    "authors": [
        "Qi Li",
        "Junpan Wu",
        "Xiang Liu",
        "Yuxin Wang",
        "Zeyu Li",
        "Zhenheng Tang",
        "Yuhan Chen",
        "Shaohuai Shi",
        "Xiaowen Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-world scenario. To close this gap, in this paper, we conduct a comprehensive study of RLLM service. We first perform a pilot study on comparing the serving performance between RLLM and traditional LLM and reveal that there are several distinct differences regarding serving behavior: (1) significant memory usage and fluctuations; (2) straggler requests; (3) adaptive running time; (4) domain preference. Then we further investigate whether existing inference optimization techniques are valid for RLLM. Our main takeaways are that model quantization methods and speculative decoding can improve service system efficiency with small compromise to RLLM accuracy, while prefix caching, KV cache quantization may even degrade accuracy or serving performance for small RLLM. Lastly, we conduct evaluation under real world workload modeled by Gamma distribution to verify our findings. Empirical results of real world workload evaluation across different dataset are aligned with our main findings regarding RLLM serving. We hope our work can provide the research community and industry with insights to advance RLLM inference serving."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 2 7 6 8 1 . 0 1 5 2 : r Reasoning Language Model Inference Serving Unveiled: An Empirical Study Qi Li1,2, Junpan Wu4, Xiang Liu1, Yuxin Wang3, Zeyu Li1, Zhenheng Tang6, Yuhan Chen1, Shaohuai Shi5, Xiaowen Chu1 1 The Hong Kong University of Science and Technology (Guangzhou) 2 Shenzhen International Graduate School, Tsinghua University 3 HKBU 4 University of Wisconsin-Madison 5 Harbin Institute of Technology, Shenzhen 6 The Hong Kong University of Science and Technology lqinfdim@163.com"
        },
        {
            "title": "Abstract",
            "content": "The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-world scenario. To close this gap, in this paper, we conduct comprehensive study of RLLM service. We first perform pilot study on comparing the serving performance between RLLM and traditional LLM and reveal that there are several distinct differences regarding serving behavior: (1) significant memory usage and fluctuations; (2) straggler requests; (3) adaptive running time; (4) domain preference. Then we further investigate whether existing inference optimization techniques are valid for RLLM. Our main takeaways are that model quantization methods and speculative decoding can improve service system efficiency with small compromise to RLLM accuracy, while prefix caching, KV cache quantization may even degrade accuracy or serving performance for small RLLM. Lastly, we conduct evaluation under real world workload modeled by Gamma distribution to verify our findings. Empirical results for real world workload evaluation across different dataset are aligned with our main findings regarding RLLM serving. We hope our work can provide the research community and industry with insights to advance RLLM inference serving. The reproduction details can be found in F."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLM) such as GPT [1], Claude [2, 3], Gemini [4], Llama [5] have emeraged as powerful knowledge bases through pre-training. These models, trained on vast Internet-crawled corpora such as C4 [6], PILE [7] and guided by scaling law [8, 9], have accumulated large-scale knowledge, and exhibited remarkable performance on various knowledge extensive tasks. Despite these advancements, LLMs are criticized for their unsatisfactory capabilities on complex reasoning tasks, e.g., challenging mathematics, and programming tasks. Recently, reasoning large language models (RLLM) like OpenAI o1 [10], DeepSeek R1 [11], Qwen-3 [12] have sparked growing body of research into test time scaling [13, 14] via long chain-of-thought reasoning [15], significantly improving their mathematical reasoning, coding tasks and knowledge reasoning capabilities, e.g., even 1.5B open source RLLM can surpass giant cutting-edge LLMs *Equal Contribution. Corresponding Author. Preprint. like GPT-4o on math tasks [11]. Such achievements make it possible to deploy small to medium RLLM as powerful assistant to light the burden of workload for the staff of small entities or even for person, democratizing the use of cutting-edge RLLMs. Hence, it is desirable for small entity with limited GPU resources to efficiently deploy RLLM with inference engine privately for internal use. Nevertheless, current LLM serving engine, e.g. vLLM [16], LMDeploy [17], Tensor-RT [18], are initially designed for traditional LLM , other than for RLLM. Though optimization techniques for LLM serving (2) have been extensively studied, it remains largely unexplored whether RLLM exhibits distinct serving characteristics from LLM. If so, directly applying existing LLM serving techniques to RLLM may leave sub-optimal serving performance. Thus, it is natural to ask the following critical research question: Is there any distinct difference in serving behaviors between LLM and RLLM? To answer the above question, we perform systematic study of efficient RLLM serving. We first establish the ASU assessment framework (3.2) for assessing RLLM serving. To justify whether there exists distinct difference in serving behavior between RLLM and LLM, we design benchmark suite named ASU-Perf and conduct pilot investigation with it on different scale LLM and RLLM (4). We found that when requests arrive in batches, the serving behavior of RLLMs differ significantly from LLMs, and the main findings can be primarily summarized in the following aspects: (1) RLLM exhibits significant KV Cache fluctuations and usage; (2) long tail distribution of requests running time caused by slow requests; (3) RLLM solves different difficulty level problems with adaptive running time; (4) RLLM excels LLM on math reasoning while on-par on knowledge intensive tasks. To understanding RLLM serving further, we first conduct extensive evaluations with various optimization techniques across diverse benchmarks (5). We find that the model quantization and speculative decoding integrated in serving engine can improve serving efficiency and performance with only small compromising on accuracy of RLLM. However, prefix caching, and KV cache quantization do not always improve serving efficiency. They degrade the accuracy or serving performance for small RLLM, e.g., 7B model. Lastly, we conduct evaluation ( 6) under real world workload modeled by Gamma distribution to verify our findings with different scale language models across different domain. Empirical results of real world workload evaluation indicate that the serving behaviors of RLLM are distinct from the LLM and are aligned with our main findings regarding RLLM serving. We hope our work can provide the research community and industry with insightful perspectives to help advance studies in efficient RLLM serving. To the best of our knowledge, we are the first to dissect the RLLM serving performance. The main contributions of this paper are the following. Conceptually, we propose ASU, framework to assess RLLM serving, which considers accuracy of response, RLLM service-provider side metric, and user side performance metrics together (3). Technically, we introduce ASU-Perf, benchmarking suite for evaluating RLLM serving (3). Empirically, we reveal key differences of serving behaviors between RLLM and LLM: Significant Memory Fluctuations and Usage, Straggler Requests, and Adaptive Running Time (4). We conduct extensive experiments on some RLLM serving optimization techniques (5). We empirically validate our findings in real-world workload and verify their generalization (6)."
        },
        {
            "title": "2 Preliminaries",
            "content": "In this section, we provide preliminaries of RLLM, LLM serving and its metric. For comprehensive introduction of LLM serving optimization and recent advancement, please refer to Appendix E. RLLM and LLM. LLMs have demonstrated remarkable capabilities across various natural language processing tasks. However, standard LLMs often encounter difficulties when faced with complex problems that require multi-step reasoning, planning, and deeper cognitive processes, sometimes referred as System-2 tasks [19]. To address these limitations, RLLMs have emerged, specifically engineered to enhance these deliberative reasoning abilities. key technique employed by RLLMs is the long Chain of Thought (long CoT) prompting strategy [20]. This approach encourages the model to generate extended, explicit step-by-step reasoning pathways, breaking down complex problems into more manageable parts. Unlike standard LLMs that might provide more direct or less detailed answers, RLLMs utilizing long CoT can better navigate the intricacies of tasks, leading to 2 more accurate and justifiable solutions by methodically thinking through the problem. This distinction allows RLLMs to tackle challenges in domains like advanced mathematics, intricate logical puzzles, and long-horizon planning more effectively than their conventional counterparts. LLM Serving. To exploit LLM in real-world scenarios, current practice generally delegates the inference procedure as an individual serving service. The design goal of such serving systems is to accommodate inference output to client users with low latency and high throughput and full use of GPU memories. Unlike the encoder-based language model [21] like BERT [22], LLM first processes input prompts with intensive computation at the prefill stage and then generates output tokens one by one within each iteration at decoding stage, which limited by the memory capacity of the hardware. Traditional serving systems process prompts batch by batch, resulting in ineffective memory utilization. Orca [23] introduces continuous batching schedule at granularity of each token generation iteration to improve throughput of serving system. To handle as much input requests, the memory space for serving system should be efficient yet elaborated managed. Since decoding phase needs to re-use KV values of their prompt tokens which are stored in GPU, vLLM [16], high performance serving engine, introduces PagedAttention with paged memory fragmentation and sharing mechanism , which alleviates memory fragmentation and enables allocation in demand. Considering the prefill is compute-intensive task, while the decode is memory-intensive task, for further improvement, DistServe [24] disaggregates the prefill and decode phase by assign computation of these two stages to different GPUs, which co-optimizes the resource allocation and parallelism tailored for each phase. Serving Performance Metrics. To measure the performance of serving system, there are multiple metrics can be chosen: (1) Time to first token (TTFT) is the time it takes to process the prompt until generate the first token. It measures how long user must wait before seeing the models output; (2) End-to-end request latency (E2E latency) indicates the time it takes from submitting the first token of request to receiving the last token of response, including the time for queueing and batching and network latencies in real-world scenario; (3) Time between tokens (TBT, a.k.a Intertoken latency, ITL) is the average time between the generation of consecutive tokens in sequence; (4) Tokens per second (TPS) of system represents the mean of total output tokens number per second , accounting for all the requests happening simultaneously; (5) Requests per second (RPS) is the average number of requests that can be successfully completed by the system in 1-second period. For More details of LLM benchmarking metrics, please refer to E.2 and related resource [25, 26]."
        },
        {
            "title": "3 Experimental Settings",
            "content": "In this section, we present experimental setups (3.1) and the ASU assessment framework (3.2). 3.1 Setups Here, we list necessary experimental setups. For implementation details, please refer to Appendix G. Language Models. We employ 4 different scale models to assess their serving performance and serving behavior. General LLM : Qwen-2.5-Math 7B [27], Qwen-2.5-14B , Qwen-2.5-32B [28], and meta-llama/Llama-3.3-70B-Instruct [5] and their long-cot tuned counterparts RLLM: DeepSeek-R1Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B, DeepSeek-R1-Distill-Qwen-32B , and DeepSeekR1-Distill-Llama-70B for fair comparison. Evaluating Datasets. We adopt four different widely used datasets to evaluate the performance of RLLM. Since RLLMs are particularly trained for system-2 reasoning tasks [15], we mainly perform benchmarking with mathematical problems. We adopt three different difficulty level math reasoning datasets: GSM8K [29] as easy level, MATH-500 [30, 31] as medium level, AIME-2024 [32] as the hardest level. To further distinguish are there any differences of serving performance and behaviors for RLLM in reasoning math problem or knowledge-based problem , we also used GPQA [33] dataset for knowledge reasoning. More details of these datasets are introduced in G.1. LLM Inference Engine. We employ 2 most adopted open source LLM inference engines, vLLM and SGLang [34] in evaluation. We use OpenAI compatible API of these engines. Evaluation Suite. We employ ASU-Perf, an benchmark suite proposed by us for evaluating LLM and RLLM serving performance with different inference engine. We leverage it in all of evaluation. 3 3.2 The ASU Assessment Framework The adoption of RLLM hinges on whether their are capable of generating value that outweighs their inference costs [35]. Assessing this tradeoff requires metrics that account for both performance and serving costs for both service provider and users. For RLLM service providers and users, the performance metrics they care about differ: providers seek to maximize system throughput, while users expect rapid model responses. In addition, it is essential to ensure response accuracy while optimizing RLLM serving system performance as much as possible. Thus, we propose ASU (Accuracy, Service-end, User-end), trinity framework for assessing RLLM serving performance by together considering response accuracy, RLLM service provider end and user end. For accuracy metric, we employ evaluation own metric for each dataset. For service provider side metrics, we use throughput metric TPS (token per second) . For user-side metrics, we use TTFVT (time to first visible token) , variant of TTFT , since we assume reasoning tokens of RLLM are invisible to users like commercial RLLM like OpenAI o1, and E2E requests running time as metrics. In the next section, we will dive into the characteristic of RLLM serving via detailed experiments."
        },
        {
            "title": "4 Pilot Investigations: Serving LLM v.s. RLLM",
            "content": "In this section, we perform an comprehensive investigation to RLLM and LLM inference serving. Experiments. We involve eight prevailing models in evaluations. For fair comparison, RLLM model we employed is the tuned counterpart of evaluated LLM, e.g., Qwen-2.5-Math-7B and its tuned RLLM counterpart DeepSeek-R1-Distill-Qwen-7B. We conduct evaluation with 7B, 14B, 32B, 70B language models on different inference engines. For comprehensively assessment, we perform evaluation with different token budget and batch size. We use all the datasets described in 3.1. Figure 1: Results of token budget variation across different datasets for 14B and 32B RLLM . Main Results. 1) Results with Different Token Budget: Unlike traditional LLMs, RLLMs engage in deliberate reasoning by generating lengthy chains of thought prior to answer, which significantly increases token consumption. However, as existing LLM services are priced based on token usage, this results in substantially higher costs. To justify the impact of token budget for RLLM serving, we conduct evaluation with varying token budget from 0.5K to 20K across benchmarks. The results are presented in Figure 1. We found that, for the majority of datasets, token budget of 4096 to 8192 can achieved sufficiently good performance. It is worth noting that, as the token budget increases, the performance of RLLMs on the GPQA and AIME24 datasets declined, which may indicate the overthinking problem [36] of RLLM. Please refer to I.1 for full results. 2) Results with Different Batch Size. We also explore the impact of different batch sizes on RLLM serving performance with the same experimental setting. We find that increasing the batch size does not affect model accuracy on various datasets. Nevertheless, it reduces the time required for RLLMs to process the same number of requests, and improves throughput metric TPS, but at the cost of increased average TTFVT. Please refer to I.2 for full results with different batch size. Serving Performance and Behaviors. To investigate RLLM serving behaviors, we analyzed the running logs of the inference serving engine and conducted visualization of the running traces, as shown in Figure 2. As illustrated, RLLMs achieve much higher accuracy on math datasets than same scale LLM, but on-par performance on knowledge reasoning such as GPQA. The full results are presented in I.3. To dissect the difference of serving behavior, we present the running trace in I.4. Main Findings for RLLM Serving Characteristics. Given the above results in pilot studies, we have the following findings in comparison of RLLM and LLM serving behaviors : 4 Figure 2: The serving performance and behavior comparison of batch requests between 7B RLLM and LLM. We can read from this figure that (1) RLLM exhibits significant KV Cache fluctuations than LLM; (2) long tail distribution of requests running time caused by straggler requests; (3) adaptive running time of RLLM; (4) domain preference on math. Please refer to I.3 for more results. Significant Memory Usage and Fluctuations: We observed significant fluctuations in memory utilization of inference engine when serving RLLM . In extreme cases, the usage varied dramatically between 3% and 70%, whereas traditional LLMs typically maintain KV cache usage below 3%. We attribute these fluctuations to the excessive length of the reasoning chains generated by RLLMs, which result in high memory consumption. During inference, the engine must retain KV caches for the reasoning chains until the requests are completed, after which they are discarded. Straggler Requests: When requests arrive at the inference engine in batches, or an RLLM receives multiple requests simultaneously, significant disparities in request difficulty can lead to some requests taking much longer time to complete than others. We denote these slow requests as straggler requests. These straggler requests ends either reaching the token budget or finishing the reasoning process. During this time, only small number of requests remain running in inference engine, resulting in noticeable drop in system throughput and hardware utilization. In contrast, LLMs exhibit much smaller variations in execution time for requests within the same batch. Adaptive Running Time of RLLM: We found that, given the same number of samples with same batch size, the runtime of RLLMs varies significantly across different datasets and is strongly correlated with the difficulty of the tasks. In contrast, traditional LLMs exhibit much smaller runtime differences across datasets, with little sensitivity to task difficulty. When the number of 5 Figure 3: Empirical results of current LLM quantization methods on 7B RLLM. current methods maintain or improve all serving-related metrics with less memory footprint while keep accuracy. samples varies, the runtime of LLMs on each dataset scales approximately linearly with the dataset size, even when there are substantial differences in task difficulty. Domain Preference: RLLMs and LLMs exhibit significant performance differences on the mathematical reasoning , while on-par on knowledge tasks, which align with existing works. Discussion and Analysis for Findings. Based on the working mechanisms of inference engines we employed in benchmarking, we discuss the reason of why the above revealed phenomena occur. 1) Straggler requests: In some mathematical datasets, e.g., Math-500, the difficulty of individual problems varies. We assume that requests arrive in batches, and new requests are sent only after all requests in batch are completed. As easier problems are answered shortly, the few remaining difficult requests continue running in the engine, leading to the entire batchs runtime being extended by these straggler requests. This situation results in reduced system throughput. 2) Memory fluctuation and usage: This issue is caused by the KV Cache management strategy of existing inference engines. Since RLLMs generate more tokens than traditional LLMs, the KV Cache utilization for RLLM is much higher under the same scale model with same precision in the same inference engine. This leads to rapid increase in KV Cache usage, and since current inference engines discard the KV Cache once completing requests, it results in sharp drop in cache usage. 3) Adaptive running time: RLLM generates varying reasoning chain lengths depending on problem difficultymore difficulty lead to longer chains and running time. Hence, RLLMs runtime is typically correlated with problem difficulty, while LLMs generally may not be affected by difficulty. Our findings indicate notable differences in serving RLLMs and LLMs. To enable more effective deployment of RLLMs, we explore some optimization techniques for inference in the next section."
        },
        {
            "title": "5 Observations on RLLM Serving Optimization",
            "content": "In this section, we take closer look at the techniques that may optimize RLLM serving performance. The prerequisite for assessing these optimization techniques is that they must preserve the RLLMs accuracy as much as possible. It holds throughout this section. More results are presented in J. 5.1 Is model weight quantization methods effective in boosting RLLM serving? Model weight quantization (MWQ) refers to the techniques that reduce number of bits for model parameters with the minimal loss in performance. Current LLM quantization methods are mainly 6 Figure 4: Empirical results for KV cache quantization on 14B model across different datasets. fallen into the post-training quantization approaches. For more comprehensive introduction of LLM quantization, please refer to [37] and [38].To investigate the impact of model weight quantization, we employ 4 most adopted (also supported by current open source LLM serving engine) quantization methods for LLM: GPTQ [39] (Int4), AWQ [40] (4-bit), FP8 [41], and Linear 4-bit [42](L4) with BitsAndBytes [43]. We conduct experiments on 7B, 14B RLLM. Main results. The evaluation results of quantized 7B RLLM using different quantization methods are presented in Figure 3. GPTQ-IN4 and FP8 quantization preserve the original model performance on most datasets, incurring only minor degradation of approximately 3% or even perform better, while maintaining or improving all serving-related metrics with less memory footprint. However, GPTQ exhibits substantial performance drop of around 1525% on more challenging mathematical tasks such as AIME24. In contrast, AWQ and L4 maintain performance across all datasets but result in marked reduction in inference efficiency, nearly doubling E2E time and halving throughput. These highlight the limitations of these approaches. The comprehensive results are presented in J.1. Observation 5.1. MWQ methods exert differing impacts on various metrics of RLLM inference . 5.2 Could KV Cache Quantization Lead to Better RLLM Serving Performance? As illustrated in [16], to serve traditional LLM, at least 30% of GPU memory is perserved to store KV cache in the generation process. For RLLM, the demand for KV cache storage would be paramount since its much longer output length ( including chain of thought reasoning ), which makes it evitable for efficient management of memory. KV cache quantization emerges as an appealing approach to this end. We employ two KV cache quantization methods natively supported by vLLM: FP8-E5M2, and FP8-E4M3 [44] for inference serving evaluation. Main results. The results of KV Cache quantization for 14B RLLM are presented in Figure 4. We found that using KV cache quantization effectively accelerates the operation of RLLMs while maintaining performance comparable to the original. Surprisingly, while the 14B or 32B RLLM maintained performance with minimal degradation after KV cache quantization, the 7B RLLM experienced almost complete performance deterioration, as shown in J.2. Furthermore, we observed that KV cache quantization can also improve other metrics such as TTFVT and TPS. Observation 5.2. KV Cache quantization can improve running efficiency for sufficient large RLLM. 5.3 Is Prefix Caching Useful for Contributing Efficient RLLM Serving? Prefix Cache (PC) is cache optimization policy that reuse computed KV values for prefill stage. By using this technique, new prompts that share same prefixes (exactly, same prefix tokens) with previous prompts processed by serving systems can reuse these KV cache. This technique is very useful such as long document query or multi-round conversation where requires multiple recomputation of same text. Empirical studies show that the prefix cache can provide huge performance benefit in such scenarios. To evaluate the utility of prefix cache in RLLM serving, we compare the performance of 4 different RLLMs across all datasets with or without prefix caching enabling in vLLM and SGLang. Main results. The results of PC evaluation on different datasets are shown in Figure 5. We find that for sufficiently large RLLMs (14B and above), prefix caching significantly improves runtime speed Figure 5: Empirical results of comparison for enable or disable prefix caching on 32B RLLM. and serving metrics without compromising performance. However, for 7B models, prefix caching negatively impacts efficiency, leading to increased latency. Detailed results are in J.3. Observation 5.3. PC can accelerate larger RLLMs (14B and above) without performance degrade. 5.4 Does Speculative Decoding Help to Improve RLLM Serving Performance? Speculative decoding (SD) refers to bunch of approaches that improves inter-token latency in memory-bound LLM inference. The initial speculating sampling usually employs faster homogeneous LLM as draft model to generate multiple tokens draft, and then the larger LLM can decide to accept or reject this draft by scoring. The results in [45] show that the overhead of draft model is much smaller than larger LLM forwarding, which makes it feasible to be utilized in real world scenario. Recently, many works in speculative decoding [46] like n-gram matching [47], MLP speculators [48], and Eagle algorithm [49, 50] are proposed. Despite these advancement, current support and compatibility of speculating decoding for RLLM in serving framework is poor. Given this situation, we only assess n-gram matching algorithm for 7B, 14B and 32B RLLM serving with vLLM iframework. The other experimental settings is keeping the same as in 5.1 for fair comparison. Main results. The main results for speculative decoding evaluation of 7B RLLM are listed in Figure 11. See J.4 for full results. We find that speculative decoding improves the inference serving running time of RLLM across all scales, without degrading model performance on benchmarks. However, speculative decoding significantly reduces throughput and degrades the TTFVT metric. Observation 5.4. SD improves the running time of RLLMs and deteriorates metrics like TPS. Summary. This section suggests that many existing LLM inference optimization techniques can be directly applied to RLLMs seamless. However, surprisingly, some of these techniques have the opposite effect on smaller RLLMs, e.g., 7B. We leave the investigation of this phenomenon to future."
        },
        {
            "title": "6 Applying to Real World Workload",
            "content": "In previous section (4), we have shown that the serving behaviors of RLLM is significantly different from the LLM. However, we assumed that the serving engine receives requests simultaneously in batches, with each new batch arriving only after the system has completed processing the previous one. This assumption may be overly idealized and not fully consistent with real-world conditions. Prior works [51, 52, 53] have shown that, in real-world applications, the burstiness of requests received by the serving engine is typically modeled using the Gamma distribution. To validate our insights regarding RLLM serving in 4 under real-world scenarios, we implement workload generator like BurstGPT-Perf [53] that is capable of producing requests following Gamma distribution in our proposed Serve-Pref suite, enabling the generation of streaming, stochastic, and bursty workloads. We then perform empirical studies with it on various scale language models (7B, 14B, 32B) across different datasets to validate our findings. Main Results. As shown in Figure 6, the average KV cache usage rate of RLLM is much higher than LLM. More surprisingly, for RLLM, the utilization of the serving engines KV cache can remain close to 100% for long periods, forcing some new requests to wait in the waiting queue before running. This may significantly prolong request turnaround time in the serving engine, severely degrading user experience. We attribute the persistently high KV cache utilization to the accumulation of numerous stragglers in the system. The running requests in the engine are also much higher when serving 8 Figure 6: KV cache usage of 14B models under real-world workload across different datasets. Figure 7: Num of running requests in the inference engine for 14B models under real-world workload. RLLM compared to LLM, as shown in Figure 7. The above phenomena hold consistently across different datasets, demonstrating the generalizability of our findings. These results demonstrate our findings in 4 remain valid under real-world workloads. Please refer to Appendix for more results."
        },
        {
            "title": "7 Related Work",
            "content": "We introduce necessary related work in this section. More related work can be found in Appendix D. Reasoning Large Language Models. Recent advancement in RLLM , such as OpenAI o1 [10] have demonstrated significant improvement in system-2 tasks such as mathematics and programming via test time scaling , which generates long chain of thought (CoT) reasoning text before answer the question. Compared with chain-of-thought in traditional LLM, the reasoning process of RLLM have the following characteristics: (1) much longer reasoning process; (2) extensive exploration to unreached logic node; (3) backtrack and reflection; (4) aha moment. Recent cutting edge RLLMs such as QwQ [54], Kimi K1.5 [55], Gemini-2.5-flash [56], Seed-think-v.15 [57], Qwen3 [12] have continually improve the performance on complex reasoning dataset. LLM Inference and Serving. Due to the large scale of LLM, they present considerable challenges in efficient serving, undermining the real world utilities of these models. Numerous works have been proposed to alleviate these problems from 6 different views: (1) model parameter memory optimization: model weight quantization like gptq [39], awq [40], FP8 [41], model pruning, model parallelism, CPU offloading ; (2) request scheduling: inter-request scheduling, and intra-request scheduling (3) dynamic memory optimization: KV cache quantization [44], KV cache reuse and dropping [58, 59]; (4) efficient decoding: speculating decoding [45] [49] [50], flash decoding [60] ;(5) system optimization: prefill-decoding disaggregation like [24] [61] [62]; (6) model and algorithm optimization: hard-aware algorithm like flash attention [60], linear attention, mixture of experts."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we systematically investigate the serving performance and behavior of RLLM. We reveal that RLLMs have several different serving behavior compared with traditional LLM, which makes current LLM serving engines struggle to unleash the power of RLLM and fall to reach the optimal performance. Additionally, we further investigate whether existing inference optimization techniques are valid for RLLM. Lastly, we conduct evaluation under real world workload modeled by Gamma distribution, and the results are aligned with our main findings regarding RLLM serving."
        },
        {
            "title": "9 Reproducibility Statement",
            "content": "Below we summarize some critical aspects to facilitate reproducible results: Datasets. The datasets we used are all publicly accessible, which is introduced in G.1. The website for download these data are listed in F. Models. We provide the details about our adopted model and hyperparameters in F. Environment. All experiments are conducted with multiple runs on NVIDIA Tesla RTX409024GB GPUs, RTX A6000-48GB GPUs and NVIDIA A100-PCIE-40GB GPUs with Python 3.11 and PyTorch 2.5."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. [3] Anthropic. Introducing claude 4. online, 2025. [4] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models, 2023. [5] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, and Archie Sravankumar. The llama 3 herd of models, 2024. [6] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. [7] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [8] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [9] Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. [10] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Qwen team. Qwen3: Think deeper, act faster, 2025. [13] Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. 10 [14] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [16] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. [17] LMDeploy Contributors. Lmdeploy: toolkit for compressing, deploying, and serving llm. https://github.com/InternLM/lmdeploy, 2023. [18] NVIDIA. TensorRT. https://github.com/NVIDIA/TensorRT-LLM, 2023. [19] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. [20] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [23] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: distributed serving system for {Transformer-Based} generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521538, 2022. [24] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. {DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 193210, 2024. [25] Nguyen Vinh, Gao Wenwen, Apsey Emily, Kudleppanavar Ganesh, Shah Neelay, and Bermudez Elias. Llm benchmarking: Fundamental concepts, 2025. [26] Nvidia inc. comprehensive guide to nim llm latency-throughput benchmarking, 2024. [27] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. [28] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [29] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [30] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 11 [31] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [32] MAA Committees. Aime problems and solutions, 2024. [33] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [34] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024. [35] Mehmet Hamza Erol, Batu El, Mirac Suzgun, Mert Yuksekgonul, and James Zou. Cost-of-pass: An economic framework for evaluating language models, 2025. [36] Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, Peng Li, Wei Wei, Jing Shao, Chaochao Lu, Yue Zhang, XianSheng Hua, Bowen Zhou, and Yu Cheng. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond, 2025. [37] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. survey on model compression for large language models. Transactions of the Association for Computational Linguistics, 12:15561577, 2024. [38] Ruihao Gong, Yifu Ding, Zining Wang, Chengtao Lv, Xingyu Zheng, Jinyang Du, Haotong Qin, Jinyang Guo, Michele Magno, and Xianglong Liu. survey of low-bit large language models: Basics, systems, and algorithms. arXiv preprint arXiv:2409.16694, 2024. [39] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations, 2023. [40] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. [41] Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters, and Tijmen Blankevoort. Fp8 quantization: The power of the exponent. Advances in Neural Information Processing Systems, 35:1465114662, 2022. [42] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:10088 10115, 2023. [43] bitsandbytes foundation. bitsandbytes, 2022. [44] vllm project. Llm compressor, 2024. [45] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. [46] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 76557671, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [47] vLLM Team. Speculating by matching n-grams in the prompt, 2024. 12 [48] Davis Wertheimer, Joshua Rosenkranz, Thomas Parnell, Sahil Suneja, Pavithra Ranganathan, Raghu Ganti, and Mudhakar Srivatsa. Accelerating production llms with combined token/embedding speculators. arXiv preprint arXiv:2404.19124, 2024. [49] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. In Forty-first International Conference on Machine Learning, 2024. [50] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-3: Scaling up inference acceleration of large language models via training-time test. arXiv preprint arXiv:2503.01840, 2025. [51] Bingyang Wu, Yinmin Zhong, Zili Zhang, Shengyu Liu, Fangyue Liu, Yuanhang Sun, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models. arXiv preprint arXiv:2305.05920, 2023. [52] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph Gonzalez, et al. {AlpaServe}: Statistical multiplexing with model parallelism for deep learning serving. In 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23), pages 663679, 2023. [53] Yuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Yuchu Fang, Yeju Zhou, Yang Zheng, Zhenheng Tang, Xin He, Rui Guo, et al. Burstgpt: real-world workload dataset to optimize llm serving systems. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pages 58315841, 2025. [54] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025. [55] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [56] Google DeepMind. Gemini 2.5: Our most intelligent ai model, 2025. [57] ByteDance Seed, Yufeng Yuan, Yu Yue, Mingxuan Wang, Xiaochen Zuo, Jiaze Chen, Lin Yan, Wenyuan Xu, Chi Zhang, Xin Liu, et al. Seed-thinking-v1. 5: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. [58] Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Yue Liu, Bo Li, Xuming Hu, and Xiaowen Chu. Chunkkv: Semantic-preserving kv cache compression for efficient long-context llm inference, 2025. [59] Xiang Liu, Zhenheng Tang, Hong Chen, Peijie Dong, Zeyu Li, Xiuze Zhou, Bo Li, Xuming Hu, and Xiaowen Chu. Can llms maintain fundamental abilities under kv cache compression? arXiv preprint arXiv:2502.01941, 2025. [60] Dao Tri, Haziza Daniel, Massa Francisc, and Sizov Grigory. Flash-decoding for long-context inference. online, 2023. [61] Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, et al. Memserve: Context caching for disaggregated llm serving with elastic memory pool. arXiv preprint arXiv:2406.17565, 2024. [62] Ruoyu Qin, Zheming Li, Weiran He, Jialei Cui, Feng Ren, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. Mooncake: Trading more storage for less computationa kvcache-centric architecture for serving llm chatbot. In 23rd USENIX Conference on File and Storage Technologies (FAST 25), pages 155170, 2025. [63] OpenAI. Openai o3 and o4-mini system card. Online, 2025. [64] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. [65] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. 13 [66] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. [67] Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari. Llm inference serving: Survey of recent advances and opportunities. arXiv preprint arXiv:2407.12391, 2024. [68] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, and Amir Gholami. Full stack optimization of transformer inference: survey, 2023. [69] Ranran Zhen, Juntao Li, Yixin Ji, Zhenlin Yang, Tong Liu, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Baoxing Huai, and Min Zhang. Taming the titans: survey of efficient llm inference serving. arXiv preprint arXiv:2504.19720, 2025. [70] Malgorzata Lazuka, Andreea Anghel, and Thomas Parnell. Llm-pilot: Characterize and optimize performance of your llm inference services. In SC24: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 118. IEEE, 2024. [71] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. Transactions on Machine Learning Research, 2023. [72] Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, and Deyi Xiong. comprehensive evaluation of quantization strategies for large language models. In Findings of the Association for Computational Linguistics ACL 2024, pages 1218612215, 2024. [73] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language models. In Proceedings of the 41st International Conference on Machine Learning, pages 2848028524, 2024. [74] Qi Li, Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Xinglin Pan, and Xiaowen Chu. Should we really edit language models? on the evaluation of edited language models. In Proceedings of the Thirty-eighth Annual Conference on Neural Information Processing Systems, 2025. [75] Amey Agrawal, Anmol Agarwal, Nitin Kedia, Jayashree Mohan, Souvik Kundu, Nipun Kwatra, Ramachandran Ramjee, and Alexey Tumanov. Etalon: holistic performance evaluation framework for llm inference systems. arXiv preprint arXiv:2407.07000, 2024. [76] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System In optimizations enable training deep learning models with over 100 billion parameters. Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. [77] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [78] Jian Hu, Xibin Wu, Zilin Zhu, Weixun Wang, Dehao Zhang, Yu Cao, et al. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. [79] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. [80] Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, et al. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. [81] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. 14 [82] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, et al. Cachegen: Kv cache compression and streaming for fast large language model serving. In Proceedings of the ACM SIGCOMM 2024 Conference, pages 3856, 2024. [83] Yihua Cheng, Kuntai Du, Jiayi Yao, and Junchen Jiang. Do large language models need content delivery network? arXiv preprint arXiv:2409.13761, 2024. [84] Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, and Junchen Jiang. Cacheblend: Fast large language model serving with cached knowledge fusion. arXiv preprint arXiv:2405.16444, 2024. [85] Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, et al. Phi-4reasoning technical report. arXiv preprint arXiv:2504.21318, 2025. [86] Akhiad Bercovich, Levy Itay, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, et al. Llama-nemotron: Efficient reasoning models, 2025. [87] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, et al. Inference without interference: Disaggregate llm inference for mixed downstream workloads. arXiv preprint arXiv:2401.11181, 2024. [88] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. Splitwise: Efficient generative llm inference using phase splitting. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 118132. IEEE, 2024."
        },
        {
            "title": "Appendix and Supplementary Material",
            "content": "A Use of LLMs Statement Limitation Boarder Impact Extended Related Work D.1 Reasoning Large Language Models . . . . . . . . . . . . . . . . . . . . . . . . . D.2 LLM Inference and Serving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 LLM Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Ecosystem Support for RLLM Serving. . . . . . . . . . . . . . . . . . . . . . . . An Introduction to LLM Serving E.1 Serving Performance . E.2 Serving Metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation and Reproduction Details F.1 Code Base . F.2 Models . F.3 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Hyperparameters Settings for RLLM . . . . . . . . . . . . . . . . . . . . . . . . . F.5 Hyperparameters Settings for LLM . . . . . . . . . . . . . . . . . . . . . . . . . . Experiments Details G.1 Details Evaluation Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Running Device . . G.3 Inference Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Extend Observation H.1 Can Disaggregated Prefilling Improve RLLM Serving Performance ? . . . . . . . . Detailed Empirical Results I.1 Token Budget for Pilot Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.2 Main Results for Pilot Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I. Serving Behaviors for Pilot Study . . . . . . . . . . . . . . . . . . . . . . . . . . I.4 Running Traces Demo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed Empirical Results for RLLM Serving Optimization J.1 Model Weight Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J.2 KV Cache Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J.3 Prefix Caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 18 18 18 18 18 19 19 20 20 22 22 22 22 23 24 24 24 25 25 25 25 25 34 35 35 35 35 J. Speculative Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 Extended Results for Real World Benchmarking"
        },
        {
            "title": "A Use of LLMs Statement",
            "content": "We solemnly declare that the originality of ideas, writing, overall methodology, experiments, and other core contributions in this paper are entirely the work of the authors, with no involvement of any LLMs in the research process. LLMs were used solely for grammar checking and language polishing after drafting this submission."
        },
        {
            "title": "B Limitation",
            "content": "In this work, we systematically investigate the serving performance of RLLM. Despite our comprehensive and thorough experiments, the evaluation of RLLM serving is limited in some extet due to limited support from the current ecosystem. We hope that future improvements in serving engines will enable broader and more comprehensive evaluations. Additionally, our hardware resources were limited, and we aim to extend our evaluations to wider range of hardware platforms in the future."
        },
        {
            "title": "C Boarder Impact",
            "content": "In this paper, we systematically investigate the serving performance of RLLM. We hope our work can provide the research community and industry with insightful perspectives to help advance studies in efficient RLLM serving, help to democratize the use of cutting-edge RLLMs for social good."
        },
        {
            "title": "D Extended Related Work",
            "content": "D.1 Reasoning Large Language Models Recent advancement in RLLM , such as OpenAI o1 [10] have demonstrated significant improvement in system-2 tasks such as mathematics and programming via test time scaling, which generates long chain of thought (CoT) reasoning text before answer the question. Compared with chain-of-thought in traditional LLM, the reasoning process of RLLM has the following characteristics: (1) much longer reasoning process; (2) extensive exploration to unreached logic node; (3) backtrack and reflection; (4) aha moment. Since OpenAIs o1 and o3 [63] are proprietary models, the research community has attempted to replicate their performance. s1 [14] try to achieve test time scaling with only 1k post-training samples. LIMO [64] exploits only 817 curated training samples, improving scores from 6.5% to 57.1% on AIME dataset. DeepSeek R1 [11] is the first open-source RLLM and achieves on-par performance with OpenAI o1. Followed by [65], which aims to fully reproduce R1 by the collaboration of open-source community. Recent cutting edge RLLMs such as QwQ [54], Kimi K1.5 [55], Gemini-2.5-flash [56], Seed-think-v.15 [57], Qwen3 [12] have continually improve the performance on complex reasoning dataset. D.2 LLM Inference and Serving LLM has become cornerstone of deep learning in recent years, reshaping the landscape of AI research. Due to the large scale of LLM, they present considerable challenges in efficient serving, undermining the real-world utilities of these models. Numerous works have been proposed to alleviate these problems from 6 different views: (1) model parameter memory optimization: model weight quantization like gptq [39], awq [40], FP8 [41], model pruning, model parallelism, CPU offloading ; (2) request scheduling: inter-request scheduling, and intra-request scheduling (3) dynamic memory optimization: KV cache quantization [44], KV cache reuse and dropping; (4) efficient decoding: speculating decoding [45] [49] [50], flash decoding [60] ;(5) system optimization: prefill-decoding disaggregation architecture like [24] [61] [62]; (6) model and algorithm optimization: hard-aware algorithm like flash attention [60], linear attention, mixture of expert. Recent advances in LLM inference have yielded variety of specialized frameworks and serving engines that maximize GPU utilization through optimized kernels and memory strategies. Highperformance libraries such as NVIDIAs FasterTransformer [66] and TensorRT-LLM [18], alongside open-source systems like vLLM [16] and SGLang [34], employ different techniques with continuous batching[23], speculative decoding[45], prefill-decode disaggregation[24] and many other methods, ensuring the GPU pipeline remains saturated. Complementing these efforts are dynamic scheduling and memory management schemes that break large KV caches into reusable blocks and selectively merge or preempt operations, allowing much larger batch sizes with minimal overhead. Equally important are multi-way parallelism and algorithmic innovations that further boost throughput and reduce latency. Large models are commonly deployed across GPUs using tensor parallelism (splitting each layers computation), pipeline parallelism (partitioning the model into sequential stages), and data parallel replication. Mixture-of-Experts (MoE) architectures extend this by routing tokens to different expert shards via expert parallelism, with communication optimizations to balance load. On the algorithmic side, parameter-efficient methods such as prompt and prefix tuning adapt frozen models via small soft prompts, speculative decoding [45] uses lightweight draft model to accelerate token generation, and Simple Test-Time Scaling[14] applies budget-forcing at inference to improve reasoning quality. Together, these system-level designs and algorithm-level approaches form cohesive ecosystem that drives state-of-the-art performance in efficient LLM serving. Please see survey papers [67, 68, 69] for comprehensive introduction [70]. D.3 LLM Evaluation Recently, with the rapid development of LLM, there is growing interest in evaluating LLM from different aspects and topics. holistic evaluation framework of language models is proposed [71]. Generally, the technical reports like [28, 12, 11] of LLM provides pre-relase comprehensive evaluation results. The quantization methods for LLM are evaluated in [72] and [73]. In [74], it evaluates the general abilities of post-edit LLM to assess the utility of existing knowledge editing methods. Work [70] and [75] evaluate LLM serving from new perspective. D.4 Ecosystem Support for RLLM Serving. The development of LLMs has greatly benefited from the research community and the open-source ecosystem, including open platforms such as Hugging Face, Github, and Modelscope; open-source LLMs like Llama [5], Qwen [28], and Deepseek R1; open-source LLM infrastructure such as Deepspeed [76], Megatron-LM [77], vLLM [16], OpenRLHF [78], and SGLang [34]; various optimization techniques like Flash-Attention [79], FlashInfer [80], ZeRO [81], and LMCache [82, 83, 84]. The advancement of RLLMs continues this trend. With the open-sourcing of Deepseek R1 [11], large number of open-source RLLMs like Phi-4 reasoning [85], and Llama-Nemotron [86] have emerged, further promoting the democratization of cutting-edge RLLM technology. Although existing LLM serving systems like vLLM, and SGLang provide some level of support for RLLMs, current support and optimization techniques remain significantly limited. Some techniques do not support RLLMs at all, for instance, Eagle speculative decoding currently lacks compatibility with RLLMs, while others fail to offer targeted optimizations and improvements specific to RLLM characteristics. As RLLMs continue to advance rapidly, we call on the research community and industry to collaborate in addressing the issues revealed in this paper."
        },
        {
            "title": "E An Introduction to LLM Serving",
            "content": "The highly increased development of LLMs application arise the demand of effectively using LLM serving systems. In this part, we introduce some optimization methods for serving systems and introduce more serving metrics. For more comprehensive introduction, please refer to [69] and [67]. E.1 Serving Performance Recently, there are lot of researches focus on optimizing the performance of serving system based on LLM architectures characteristics and system-level tricks. Current LLMs are mostly using decode-only architecture, making the KV values of former tokens becomes key information for the next token. Hence, the first useful methods is storing all of KV value in memories(particularly in GPUs), this method significantly improve the efficiency of prefill stage. However, this method had already deployed for language models. For LLM, the most important method proposed first is continuous batching[23]. Continuous batching is processing requests in serving systems in iteration level, compared with former systems process requests in request-level. By using this technique, serving systems dont need to wait until the last request finishes its decoding, but replace requests with new requests once it ends decoding. This method enhance GPUs utilization, reducing waiting time for high-throughput serving systems. Next, considering the difference of prefill and decode that prefill is compute-intense stage which needs more GPU computing resources, while decode is memory-intense stage which needs more GPU memories compared with prefill, Prefill-Decode disaggregation[24] proposed method that process prefill and decode in different GPUs, fully utilizing GPU resources based on the characteristics of the two phases. Despite this, GPU resources are still not fully utilized because the GPU pre-allocates portion of GPU space for requests when storing previous KV cache. However, much of this space isnt effectively used, resulting in significant waste (for example, if request occupies 8 tokens, the GPU allocates 2080 token spaces for decoding this request, but actually only produces 80 tokens, wasting space for 2000 tokens). At the same time, since the GPU allocates and reserves space for requests sequentially, this can lead to memory fragmentation and inefficient resource utilization when requests complete at different times. Paged attention borrows the concept of CPU paging, and in their serving system (vLLM) creates mapping between virtual addresses and actual GPU addresses through virtual pages[16]. E.2 Serving Metric With the high demand of deploying customized LLMs for practical utilization, there is need to measure the cost efficiency of different LLM serving solutions. The cost of serving RLLM depends on how many requests it can handle per second while being responsive to client users and supporting an acceptable level of answer accuracy. To measure the performance of LLM serving system, there are multiple metrics can be chosen: (1) Time to first token (TTFT) is the time it takes to process the prompt until generate the first token. It measures how long user must wait before seeing the models output; (2) End-to-end request latency (E2E latency) indicates the time it takes from submitting the first token of request to receiving the last token of response, including the time for queueing and batching and network latencies in real-world scenario; (3) Time between tokens (TBT, a.k.a Intertoken latency, ITL) is the average time between the generation of consecutive tokens in sequence; (4) Tokens per second (TPS) of system represents the mean of total output tokens number per second , accounting for all the requests happening simultaneously; (5) Requests per second (RPS) is the average number of requests that can be successfully completed by the system in 1-second period. In LLM serving systems, there are many metrics evaluating the performance, In this paper, we use metrics for reference that companies and personal users care most while using RLLM. Ill introduce them here for clear understanding. Throughput: Number of processed requests per second. This is the key metric for users since it directly determines overall system performance. Time to First Token (TTFT): Time from receiving request until the first token is generated (i.e., the prefill stage is completed). This reflects how quickly the serving system handles the prefill stage. Techniques such as continuous batching [23] and paged attention [16] were proposed to optimize this metric. Time to First Visible Token (TTFVT): The time from receiving request until the first token is actually displayed to the user. This metric is specific to RLLMs because some 20 inference systems hide the internal thinking steps and only reveal output once thinking is complete. Since RLLMs often perform prolonged reasoning chain before producing any visible token, TTFVT is typically much larger than TTFT. Time Between Tokens (TBT): Average time between generation of consecutive tokens. For RLLMs, both the thinking stage and decoding stage share this metric. Recent algorithm-level optimizations such as S1 [14] target TBT. In this paper, TBT reflects the real-time per-token responsiveness of the model during interactive generation, capturing both computational and scheduling overhead. KV Cache Utilization: Proportion of total memory occupied by the KV cache during model execution. High utilization enables reuse of KV values by subsequent requests, reducing prefill time. However, excessive utilization triggers frequent evictions, degrading performance. Section 4 analyzes KV cache utilization and its impact on overall performance for RLLMs across datasets of varying difficulty. Tokens per Second (TPS): Total number of tokens generated per second across all active sessions. This combines throughput and per-token speed into one measure of generation capacity. Requests per Second (RPS): Total number of full-request pipelines completed per second. Unlike throughput (which counts raw requests), RPS tracks end-to-end request handling. Model Initialization Latency: Total time from service startupincluding loading model weights, constructing computation graphs, allocating GPU memory, initializing optimizers, and any warm-up stepsuntil the system is ready to handle its first request. For MoE models (such as the DeepSeek model used in this paper) with Tensor Parallelism (TP) and Pipeline Parallelism (PP), this also involves partitioning and distributing parameters across multiple GPUs. This metric helps compare how different serving systems optimize model loading and initialization. End-to-End Latency (E2E Latency): Time from user request submission until receipt of the final token. This metric significantly influences user experience; for enterprises, improving RLLM end-to-end latency is also critical concern."
        },
        {
            "title": "F Implementation and Reproduction Details",
            "content": "In this section, we would like to provide details for reproducing our experimental results. F.1 Code Base Our code and the ASU-Perf suite will be available once this paper accepted. F.2 Models Here, we list all of the model checkpoints used in our experiments. RLLM checkpoints: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B https://hf-mirror.com/deepseek-ai/ DeepSeek-R1-Distill-Qwen-1.5B deepseek-ai/DeepSeek-R1-Distill-Qwen-7B https://hf-mirror.com/deepseek-ai/ DeepSeek-R1-Distill-Qwen-7B deepseek-ai/DeepSeek-R1-Distill-Qwen-14B https://hf-mirror.com/deepseek-ai/ DeepSeek-R1-Distill-Qwen-14B deepseek-ai/DeepSeek-R1-Distill-Qwen-32B https://hf-mirror.com/deepseek-ai/ DeepSeek-R1-Distill-Qwen-32B deepseek-ai/DeepSeek-R1-Distill-Llama-70B https://hf-mirror.com/deepseek-ai/ DeepSeek-R1-Distill-Llama-70B LLM checkpoints: Qwen/Qwen2.5-Math-1.5B https://hf-mirror.com/Qwen/Qwen2.5-Math-1.5B Qwen/Qwen2.5-Math-7B https://hf-mirror.com/Qwen/Qwen2.5-Math-7B Qwen/Qwen2.5-14B https://hf-mirror.com/Qwen/Qwen2.5-14B Qwen/Qwen2.5-32B https://hf-mirror.com/Qwen/Qwen2.5-32B meta-llama/Llama-3.3-70B-Instruct https://hf-mirror.com/meta-llama/Llama-3. 3-70B-Instruct F.3 Datasets Here, we list all of the benchmarking datasets used in our experiments. GSM8K https://hf-mirror.com/datasets/openai/gsm8k MATH-500 https://hf-mirror.com/datasets/HuggingFaceH4/MATH-500 AIME-24 https://hf-mirror.com/datasets/HuggingFaceH4/aime_2024 GPQA https://hf-mirror.com/datasets/Idavidrein/gpqa F.4 Hyperparameters Settings for RLLM The hyperparameters settings for RLLM we employed are as follows: Batch Size: 8, 16, 32 Dataset Capacity: 100, (AIME24 30) Temperature: 0.6, Top-p: 0.95, Top-k: 20, Request Timeout: 1200 sec Experiments Repeat Time: 3 Performance Only Mode: False Reasoning LLM Mode: True CoT Visible (for TTFT): False 22 F.5 Hyperparameters Settings for LLM The hyperparameters settings for LLM we employed are as follows: Batch Size: 8, 16, 32 Dataset Capacity: 100, (AIME24 30) Temperature: 0.7, Top-p: 0.8, Top-k: 20, Request Timeout: 1200 sec Experiments Repeat Time: 3 Performance Only Mode: False Reasoning LLM Mode: False CoT Visible (for TTFT): False"
        },
        {
            "title": "G Experiments Details",
            "content": "G.1 Details Evaluation Datasets We use 4 different datasets in this paper, they are GSM8K, MATH500, AIME24, and GPQA. The details of these datasets are following. GSM8K [29]: The GSM8K dataset is large collection of mathematical problem-solving tasks designed for training and evaluating AI models in the context of elementary schoollevel math. It primarily focuses on grade school math word problems that require multiple steps of reasoning and calculations to solve. MATH500 [31]: challenging dataset consisting of problems from high school math competitions across seven subjects (e.g., Prealgebra, Algebra, Number Theory) and difficulty levels based on AoPS (ranging from 1 to 5). Problems in these competitions range from level 1, the easiest, often found in AMC 8 exams, to level 5, like those in AIME. AIME24 [32]:a dataset from the American Invitational Mathematics Examination, which tests math problem solving across multiple areas (e.g. algebra, counting, geometry, number theory, and probability). Because AIME 2024 contains only 30 examples, we dont considered examples of AIME from other years. GPQA [33]: graduate-level dataset consisting of multiple-choice questions in subdomains of physics, chemistry, and biology. For our experiment, we select the highest quality subset, known as GPQA Diamond (composed of 198 questions). G.2 Running Device All of our experiments are running on three devices: server with 8 RTX A6000 GPUs with 48GB VRAM, server equipped with 8 RTX 4090 GPUs with 24GB VRAM, and server with 8 NVIDIA A100-PCIE-40GB GPUs. G.3 Inference Engine We use vLLM [16] version 0.8.1 and SGLang [34] version 0.4.6.post1. For evaluation, we use OpenAI compatible API /v1/chat/completions. Table 1: Performance Metrics with PD-disaggregated with 7B, 14B model Model Method Dataset Accuracy Running Time Token Per Sec TTFVT Output Tokens 7B 14B GSM8K w/o PD-disa w/ PD-disa GSM8K w/o PD-disa MATH500 MATH500 w/ PD-disa w/o PD-disa AIME2024 AIME2024 w/ PD-disa GPQA w/o PD-disa GPQA w/ PD-disa GSM8K w/o PD-disa w/ PD-disa GSM8K w/o PD-disa MATH500 w/ PD-disa MATH500 w/o PD-disa AIME2024 AIME2024 w/ PD-disa GPQA w/o PD-disa GPQA w/ PD-disa 82 82 65 64 266 23.3 11 20 87 86 65 58 23.3 26.6 20 19 0:58 1:12 5:21 7:57 2:01 2:40 6:52 9:19 6:03 9:04 12:33 22:43 4:14 7:22 14:37 25: 713.3 516.9 492.7 323.2 935.3 711.7 894.2 662.0 163.0 109.7 233.4 134.0 449.9 258.2 403.6 235.6 0.21 0.3 0.23 0.4 0.27 0.4 0.36 0.6 0.31 0.6 0.37 0.7 0.42 0.8 0.55 1.2 41789 42052 158431 154318 112492 113718 368377 369821 59219 59605 175808 182705 114586 114048"
        },
        {
            "title": "H Extend Observation",
            "content": "H.1 Can Disaggregated Prefilling Improve RLLM Serving Performance ? As discussed in Section 2 and paper [24], the process of LLM generates responds to input prompt can be divided into two different phases. The LLM first processes input prompt in the prefill phase, which is computation intensive, to generate the first token of response within one iteration. After it , in the memory bounded decoding phase, LLM generates token one by one in each iteration until reaching the end token. These two phases have distinct different significance. However, many existing serving system co-locate the prefill and decoding at the same device, which may leads to sub-optimal performance and inter-phase interference as revealed in [24]. The disaggregated prefilling architecture was proposed to address this problem. It is first introduced in [24], followed by lines of recent works like [87], [88], [61], [62], notably improving the TTFT and throughput of system. However, current support for disaggregated prefilling is experimental and only available in vLLM. Whats more, the only disaggregated prefilling feature support in vLLM is 1P1D scheme (1 prefilling worker and 1 decoding worker) currently. Hence, we merely perform evaluation with 1P1D on 7B (on two RTX-4090 GPU) and 14B (on two A6000 GPU) models across 4 evaluation datasets. Main results. The results of PD-disaggregation are shown in Table 1. We found that under the 1P1D setup, PD-disaggregation does not improve the serving performance of RLLMs. On the contrary, it leads to decline in system performance metrics. We find that the performance bottleneck of 1P1D serving for RLLMs lies in decoding, while the devices used for pre-filling are largely idle, which leads to suboptimal performance. Additionally, PD-disaggregation requires KV cache transfer between GPUs, and the communication overhead negatively impacts the serving of RLLMs. Observation 6. PD-disaggregation (1P1D) deteriorates RLLM serving metrics compared to mixed PD. Since nearly half of the computing resources are idle."
        },
        {
            "title": "I Detailed Empirical Results",
            "content": "I.1 Token Budget for Pilot Study Full Figures of token budget exploration are listed in Figure 8. I.2 Main Results for Pilot Study We provide full results of RLLM and LLM serving comparison. 7B. RLLM in Table 2, LLM in Table 3 14B. RLLM in Table 4, LLM in Table 32B. RLLM in Table 6, LLM in Table 7 25 Figure 8: Results of token budget variation across different datasets for different scale RLLM . 70B. RLLM in Table 8, LLM in Table 9 I.3 Serving Behaviors for Pilot Study For better presentation, we provide illustration about 14B and 32B model serving visualization in Figure 9 and 10. 26 Table 2: Serving Results of RLLM-7B Model BS Budget Dataset Acc. Running Time TPS TTFT Output Tokens 8 RLLM-7B 16 64 4096 8192 4096 4096 8192 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 80.67 61.33 23.33 15.00 82.33 64.33 38.89 26. 84.30 59.30 20.00 14.67 85.00 62.30 37.78 25.33 80.67 60.67 18.89 14.67 84.67 61.33 35.56 28.00 1m53s 9m34 4m7s 13m29s 1m46s 14m09 8m7s 26m42s 1m1s 5m56s 2m15s 7m40s 1m1s 10m29s 4m35s 15m21s 28s 2m16s 1m22s 3m22s 25s 4m1s 2m46s 6m57s 426.87 302.47 470.37 477.14 443.95 240.55 409.92 410. 791.40 499.45 872.91 838.80 783.22 351.14 736.40 706.20 2.4200 16.5500 40.8200 35.3600 2.3400 20.2900 64.9600 65.9200 2.3900 17.1000 46.4700 39.3300 2.3900 22.2400 73.4100 72.5800 1700.96 1303.67 1413.73 1929.84 1872.10 897.99 1222.58 1567.52 3.7200 25.1500 52.9100 62.5700 3.5800 30.3300 82.1100 102.1000 129629 502833 340730 1124673 126155 601161 592434 126451 505796 346986 1124379 125992 623861 590741 1921405 128915 506862 345118 1131700 125931 624385 600925 1936615 Table 3: Serving Results of LLM-7B Model BS Budget Dataset Acc. Running Time TPS TTFT Output Tokens 8 LLM-7B 16 4096 32 4096 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 69.67 3.30 15.56 3.00 70.00 1.67 18.89 0.04 67.67 1.67 16.67 6.00 1m47s 3m19s 1m34s 3m45s 1m33s 2m13s 50s 2m42s 57s 1m31s 33s 1m34s 394.13 343.65 392.93 324.91 477.32 513.34 699.36 495.68 762.61 748.09 1063.23 754.67 0.0600 0.0713 0.0776 0.1366 0.0991 0.1258 0.1208 0.2114 0.1698 0.2006 0.1971 0.3807 107378 178459 101905 183061 115613 178452 95181 204317 111292 176704 94296 175708 Table 4: Serving Results of RLLM-14B Model BS Budget Dataset Acc. Running Time TPS TTFT Output Tokens 8 RLLM-14B 32 4096 8192 4096 8192 8192 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 87.00 55.00 27.78 16.33 87.67 62.33 47.78 21.00 86.33 60.33 27.78 15.33 86.33 63.67 47.78 22. 85.33 57.33 22.22 15.00 86.33 66.00 50.00 26.67 3m50s 11m49s 4m42s 15m25s 3m30s 17m17s 9m18s 30m34s 2m40s 8m27s 3m11s 10m48s 3m01s 13m25s 6m28s 21m16s 1m38s 5m38s 2m20s 8m18s 2m13s 9m09s 4m22s 15m36s 280.83 277.63 411.29 406.14 303.34 218.77 338.41 341.19 402.48 388.92 594.52 581.23 382.72 291.14 498.79 481. 619.91 568.13 839.99 770.56 486.36 414.53 722.93 658.74 5.9731 20.0256 50.6600 39.0104 6.0476 25.5350 72.4221 67.0687 7.6400 27.2100 61.7600 52.2400 8.1400 33.2600 109.3600 95.8800 10.1400 36.8600 92.3600 78.5500 11.0500 42.8600 140.4300 136.5100 177347 566267 341998 1090910 175210 656437 558966 1842315 173204 563198 335082 1098423 180684 669369 568311 169539 554485 344929 1104010 182233 654295 563372 1818245 28 Table 5: Serving Results of LLM-14B Model BS Budget Dataset Acc. Running Time TPS TTFT Output Tokens LLM-14B 8 16 32 64 4096 4096 8192 4096 8192 4096 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 74.17 44.00 2.22 29.00 74.17 44.67 3.33 29.33 77.33 46.00 3.33 25.33 77.33 47.00 4.44 24. 74.09 47.67 5.56 28.00 74.59 46.67 2.22 28.67 84.00 59.33 23.33 17.00 88.00 68.00 53.33 25.33 1m18s 2m29s 1m47s 2m15s 1m16s 2m23s 1m54s 2m09s 44s 1m57s 1m05s 1m40s 45s 1m53s 58s 2m05s 44s 1m04s 41s 1m17s 52s 1m20s 41s 1m19s 2m15s 7m57s 3m3s 10m22s 2m39s 12m53s 6m7s 20m43s 317.73 286.86 240.82 305.50 337.06 282.50 252.58 228.90 529.30 365.86 388.44 424.31 554.18 385.42 409.73 381.19 615.89 597.29 616.16 403.77 526.30 523.91 620.70 541. 463.17 406.74 630.49 607.50 403.58 299.83 532.62 504.45 0.0603 0.0626 0.0626 0.1222 0.0164 0.0635 0.0619 0.1217 0.1536 0.0913 0.0913 0.2250 0.0850 0.0920 0.9020 0. 0.1350 0.1393 0.1354 0.4789 0.1320 0.1385 0.1445 0.4215 7.3815 25.6479 58.5514 50.8592 7.2181 32.5928 112.0436 86.7966 60286 100818 69874 87836 59314 99443 77607 55856 106980 66109 93954 57030 109010 62213 97573 60318 94561 68309 95110 61833 102168 69811 93025 169635 560880 340172 1098715 174708 667886 577277 29 Table 6: Serving Results of RLLM-32B Model BS Budget Dataset Acc. Running Time TPS TTFT Output Tokens 8 RLLM-32B 16 32 4096 8192 4096 4096 8192 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 91.00 64.00 21.11 20.00 90.33 70.67 45.56 24. 90.67 66.33 25.56 21.67 92.67 68.33 48.89 28.67 91.33 66.67 28.89 18.00 92.33 68.67 50.00 23.00 4m16s 24m58s 9m12s 5m0s 4m11s 36m41s 18m36s 60m31s 2m27s 14m49 5m03s 17m25s 2m30s 25m38s 10m22s 35m30s 1m39s 9m29s 3m01s 11m25s 1m39s 16m6s 6m06s 23m13s 192.12 125.59 215.58 206.07 194.77 97.83 174.28 166. 324.74 201.98 388.52 352.72 324.04 134.29 309.03 283.98 490.92 309.67 631.69 541.78 494.92 213.37 502.69 436.05 5.2715 42.9319 104.0663 73.0046 5.2037 51.2373 150.4045 129.2077 5.4900 44.6300 111.2300 74.0400 5.5700 53.4000 170.1700 137.9600 6.7103 47.9789 119.5658 94.9504 6.4570 60.4897 186.3563 171.4893 130170 537799 349099 1071843 129401 621128 575207 128546 517659 346308 1070143 128060 597129 568936 1778995 129986 504391 335582 1077209 129510 593685 547959 1787777 Table 7: Serving Results of LLM-32B Model BS Budget Dataset Acc. Running Time TPS TTFT Output Tokens 8 LLM-32B 16 32 4096 8192 4096 4096 8192 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 60.67 46.67 6.67 29.00 60.67 44.00 4.44 29. 66.33 49.00 12.22 31.00 62.67 46.03 6.67 23.33 63.00 45.00 7.78 24.33 62.15 44.37 7.78 25.33 7m25s 8m24s 2m43s 7m6s 7m10s 7m42s 3m24s 6m18s 4m03s 4m32s 2m13s 4m23s 4m04s 5m01s 1m53s 3m46s 5m24s 5m44 1m39s 4m06s 3m59s 5m31s 1m52s 4m31s 87.34 103.76 131.04 141.18 89.53 109.18 127.82 152. 130.10 170.74 185.90 210.62 126.56 156.52 200.03 259.97 125.09 148.90 211.36 231.08 129.75 148.39 200.63 246.55 0.1271 0.1339 0.1414 0.2291 0.0867 0.0874 0.0899 0.1141 0.1083 0.1104 0.1092 0.1544 0.1107 0.1115 0.1179 0.1659 0.1902 0.2097 0.2206 0.3894 0.1094 0.1109 0.1113 0.1872 100271 131828 57729 144033 98287 127011 71654 83076 115631 67231 139381 92586 122183 60391 136660 96723 123313 57192 141199 87157 128664 63495 136403 30 Table 8: Serving Results of RLLM-70B Model BS Budget Dataset Acc. Running Time TPS TTFT Output Tokens 8 RLLM-70B 16 32 4096 4096 8192 4096 8192 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 90.00 54.67 32.22 22.67 90.00 62.00 54.44 29.67 88.33 57.00 26.67 23.00 88.00 60.67 55.56 30.67 88.67 56.67 25.56 23.00 89.00 62.33 51.11 32.00 5m22s 30m29s 11m46s 38m11s 5m30s 49m48s 23m18s 75m38s 3m24s 19m45s 7m01s 23m35s 3m32s 32m27s 13m46s 46m01s 2m11s 13m0s 4m26s 15m57s 2m12s 21m36s 8m24s 30m31s 146.65 102.48 162.46 158.29 143.90 78.31 135.50 128.03 230.39 158.60 277.08 253.66 228.88 112.85 221.77 204.78 352.22 238.82 438.58 378.78 352.59 166.26 360.36 307.04 6.6450 48.4212 108.0681 93.9947 6.5438 61.8861 192.8731 159.9508 7.4600 52.3600 140.8100 98.0500 7.7200 69.6400 197.0300 181. 9.7962 66.2581 184.7638 136.1667 9.4598 83.2369 246.0471 228.6145 125391 538966 336881 1052080 125874 677161 561002 1702698 123185 539666 342355 1042191 125291 644132 539132 1659750 122786 534918 343514 1052451 123720 621237 537908 1650647 Table 9: Serving Results of LLM-70B Model BS Budget Dataset Acc. Running Time TPS TTFT Output Tokens 8 LLM-70B 16 32 4096 4096 8192 4096 8192 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 93.00 59.33 30.00 53.33 92.67 59.00 23.33 51.67 91.33 60.00 27.78 47.67 93.67 59.00 27.78 49.00 92.33 60.33 26.56 51.00 93.00 61.00 27.78 53.33 3m03s 10m38s 5m35s 12m18s 2m58s 13m12s 6m1s 11m24s 1m44s 6m35s 3m06s 7m28s 1m52s 6m38s 5m13s 10m26s 1m17s 5m29s 2m37s 5m40s 1m14s 7m7s 3m23s 4m54s 144.56 90.67 107.39 128.53 144.42 78.52 99.04 131.86 243.39 139.78 171.31 201.44 227.03 139.61 124.39 155.10 346.24 176.59 243.93 268.96 357.73 148.03 186.51 306.15 0.2030 0.2283 0.2461 0.4749 0.1143 0.1197 0.1232 0.1847 0.1525 0.1730 0.1678 0.2694 0.1536 0.1704 0.1715 0. 0.6077 0.6520 0.7012 1.5823 0.2325 0.2520 0.2550 0.5346 62746 148624 99826 243123 60838 162129 98349 233596 60890 144651 89789 231586 61434 142362 103021 247291 61987 149612 106146 237672 60831 164985 105595 233070 31 Figure 9: Results of RLLM vs LLM for 14B model size . 32 Figure 10: Results of RLLM vs LLM for 32B model size . 33 I.4 Running Traces Demo INFO : 1 2 7 . 0 . 0 . 1 : 5 3 4 5 8 \"POST / v1 / t / p i HTTP / 1 . 1 \""
        },
        {
            "title": "200 OK",
            "content": "INFO 05 10 1 3 : 0 1 : 4 9 [ g . py : 8 0 ] Avg m h g t : 2 2 3 . 0 e / , Avg e i h g t : 1 6 4 . 5 e / , Running : 16 s , t : 0 s , GPU KV h a : 1 . 0 % , f a h a : 1 1 . 6 % INFO 05 10 1 3 : 0 1 : 5 9 [ g . py : 8 0 ] Avg m h g t : 0 . 0 e / , Avg e i h g t : 6 5 0 . 0 e / , Running : 14 s , t : 0 s , GPU KV h a : 2 . 8 % , f a h r : 1 1 . 6 % INFO 05 10 1 3 : 0 2 : 0 9 [ g . py : 8 0 ] Avg m h g t : 0 . 0 e / , Avg e i h g t : 4 9 5 . 5 e / , Running : 11 s , t : 0 s , GPU KV h a : 3 . 9 % , f a h a : 1 1 . 6 % INFO 05 10 1 3 : 0 2 : 1 9 [ g . py : 8 0 ] Avg m t u u : 0 . 0 e / , Avg e i h g t : 4 0 6 . 1 e / , Running : 8 s , t : 0 s , GPU KV h a : 4 . 0 % , f a h a : 1 1 . 6 % INFO 05 10 1 3 : 0 2 : 2 9 [ g . py : 8 0 ] Avg m h g t : 0 . 0 e / , Avg e i h g t : 3 2 0 . 8 e / , Running : 8 s , t : 0 s , GPU KV h a : 5 . 1 % , f a h r : 1 1 . 6 % INFO 05 10 1 3 : 0 2 : 3 9 [ g . py : 8 0 ] Avg m h g t : 0 . 0 e / , Avg e i h g t : 3 2 0 . 7 e / , Running : 6 s , t : 0 s , GPU KV h a : 4 . 9 % , f a h a : 1 1 . 6 % INFO 05 10 1 3 : 0 2 : 4 9 [ g . py : 8 0 ] Avg m t u u : 0 . 0 e / , Avg e i h g t : 2 7 1 . 3 e / , Running : 5 s , t : 0 s , GPU KV h a : 4 . 9 % , f a h a : 1 1 . 6 % INFO 05 10 1 3 : 0 2 : 5 9 [ g . py : 8 0 ] Avg m h g t : 0 . 0 e / , Avg e i h g t : 1 8 0 . 7 e / , Running : 4 s , t : 0 s , GPU KV h a : 4 . 5 % , f a h r : 1 1 . 6 % INFO 05 10 1 3 : 0 3 : 0 9 [ g . py : 8 0 ] Avg m h g t : 0 . 0 e / , Avg e i h g t : 1 6 4 . 8 e / , Running : 4 s , t : 0 s , GPU KV h a : 5 . 0 % , f a h a : 1 1 . 6 % INFO 05 10 1 3 : 0 3 : 1 9 [ g . py : 8 0 ] Avg m t u u : 0 . 0 e / , Avg e i h g t : 1 7 4 . 4 e / , Running : 4 s , t : 0 s , GPU KV h a : 5 . 7 % , f a h a : 1 1 . 6 %"
        },
        {
            "title": "J Detailed Empirical Results for RLLM Serving Optimization",
            "content": "J.1 Model Weight Quantization Full results of model weight quantization with different models are listed in Table 10 and 11. Table 10: Results of RLLM-7B with Different Quantization Methods Model Method Dataset Acc. Running Time TPS TTFT Output Tokens GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA Budget-4096 36s 3m34s 1m25s 4m48s 2m36s 14m55s 5m02s 17m22s 58s 4m44s 1m49s 6m13s 1m21s 7m58s 3m05s 10m36s Budget-8192 57s 5m54s 2m59s 9m47s 2m35s 25m03s 10m17s 34m54s 1m03s 7m38s 3m42s 12m40s 1m24s 13m01s 6m34s 23m07s 81.67 61.00 16.67 16. 82.67 59.33 21.11 14.33 82.67 64.00 25.56 15.00 82.67 61.00 20.00 15.33 80.33 64.00 31.11 25.33 80.00 66.67 32.22 24.33 83.00 63.00 40.00 27. 80.33 63.67 36.67 31.33 GPTQ AWQ FP8 L4 GPTQ AWQ FP8 L4 1258.52 807.99 1383.83 1342.53 306.10 195.01 390.68 371.37 805.44 589.49 1062.28 1029. 560.40 356.31 630.92 599.34 955.97 591.44 1156.19 1131.34 306.42 141.57 333.68 314.88 774.97 455.87 880.38 856.35 562.97 264.82 499.62 461.52 1.5015 10.8316 28.3623 23. 5.7903 41.2395 104.9071 89.8281 1.8947 13.9700 38.5411 29.9774 3.5042 24.6086 60.3252 51.2468 1.5007 12.3735 44.9320 44.2137 5.6545 54.4308 162.2038 156.5370 2.0614 17.6713 61.7136 57. 3.6269 29.4854 98.4701 94.4508 125477 488868 346988 1127392 128626 501037 347634 1124314 128016 479989 341103 1110873 119344 486082 343397 1109222 135978 600650 618796 126262 618240 610667 1942820 129654 24375 582089 1915156 123986 597768 586687 1879225 RLLM-7B RLLM-7B J.2 KV Cache Quantization Full results of KV Cache quantization with different models are listed in Table 12, 13. J.3 Prefix Caching Full results of KV Cache quantization evaluation with different models are listed in Table 14, 15, 16, 17. J.4 Speculative Decoding The visualizatio for 7B model SD is in Figure 11. Full results of speculative decoding evaluation with different models are listed in this subsection. For RLLM, results are presneted in Table 18, 19, 20, 21. Table 11: Results of RLLM-14B with Different Quantization Methods Model Method Dataset Accuracy Running Time TPS TTFT Output Tokens RLLM-14B RLLM-14B GPTQ AWQ FP L4 GPTQ AWQ FP8 L4 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA Budget-4096 87.67 61.00 22.22 16.33 86.67 61.00 21.11 18.33 89.00 60.67 24.44 14.33 83.67 58.67 26.67 16. 1m55s 5m56s 2m25s 8m09s 1m30s 6m01s 2m30s 8m26s 2m19s 7m16s 2m55s 9m49s 3m27s 9m24s 3m41s 12m44s Budget-8192 84.67 65.33 40.00 25. 86.67 65.33 47.78 26.33 86.00 63.33 51.11 27.33 83.33 63.00 47.78 21.33 1m48s 8m49s 5m03s 16m14s 1m48s 9m22s 5m04s 17m02s 2m14s 11m40s 5m43s 19m27s 2m47s 14m44s 7m58s 28m55s 531.24 535.47 811.91 775.78 617.47 534.74 771.16 750.95 446.80 447.51 665.67 650.24 323.02 326.66 525.41 501.28 569.94 418.08 666.62 638. 585.55 399.52 640.79 621.02 467.49 328.62 535.96 529.1 376.01 248.57 401.73 368.01 5.2007 18.478 49.699 40.463 4.6092 19.725 49.545 42.983 6.5690 24.214 62.497 51. 9.9777 30.626 74.833 65.669 5.2287 22.0500 76.884 74.9600 4.7892 23.658 84.494 77.938 6.5689 28.621 90.392 84.797 9.3994 36.372 123.95 115.88 167860 547482 346219 151523 551164 342303 1106144 170719 560177 343575 1113927 187673 528533 341339 1113251 168441 638458 596659 1831969 155465 647840 575835 1866185 171349 667889 544733 172748 635662 572091 1879105 Figure 11: Results of 7B RLLM with SD enabled . 36 Table 12: Results of RLLM-7B with Different KV Cache Quantization Methods Model Method Dataset Acc. Running Time TPS TTFT Output Tokens RLLM-7B RLLM-7B FP8-E4M3 FP8-E5M2 FP8-E4M3 FP8-E5M2 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA Budget-4096 9.33 4.33 0.00 0.33 2.67 0.33 0.00 0.00 7m5s 7m26s 2m13s 16m27s 9m8s 9m28s 2m50s 9m36s Budget-8192 8.67 4.00 0.00 0.67 2.33 0.33 0.00 0. 14m23s 16m01s 4m18s 7m39s 19m04s 20m02s 5m59s 20m21s 678.93 849.23 936.07 831.28 624.13 731.99 733.58 739.58 634.50 796.17 855.67 914.28 563.56 681.93 682.40 678.99 3.8153 17.1335 23.4623 73.5032 6.5660 22.0233 71.2345 36.1577 5.0929 36.4224 50.5708 33.3658 26.7276 94.8742 91.3258 102.0499 843936 1113415 732374 2428386 1013649 1223241 367838 1631364 2271199 732374 1225852 1919839 2438321 730016 2450781 Table 13: Results of RLLM-14B with Different KV Cache Quantization Methods Model Method Dataset Acc. Running Time TPS TTFT Output Tokens GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA Budget-4096 87.33 63.00 26.67 15.00 82.33 59.33 26.67 15.67 2m17s 7m37s 2m59s 10m03s 2m40s 7m46s 3m02s 10m19s Budget-8192 83.67 66.33 52.22 24.00 85.33 62.67 48.89 26.67 3m03s 12m18s 5m53s 20m23s 2m39s 12m48s 6m04s 20m51s FP8-E4M3 FP8-E5M2 FP8-E4M3 FP8-E5M2 RLLM-14B RLLM-14B 466.06 417.60 640.63 631.30 390.53 394.45 632.26 608.30 357.72 305.61 520.69 511.37 401.41 289.84 513.09 497.93 7.5905 25.1716 63.6228 50.1774 7.6730 25.4353 62.9118 49.3198 7.7833 30.8091 94.5251 89.8416 7.8384 29.0644 92.6786 90.6068 169107 547992 338613 1106576 171445 527039 339940 1094547 180474 653017 545376 1838882 175761 633207 553741 37 Table 14: Results of RLLM-7B without Prefix Cache"
        },
        {
            "title": "Dataset",
            "content": "Acc."
        },
        {
            "title": "Output Tokens",
            "content": "RLLM-7B RLLM-7B GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 79.00 60.67 18.89 14.67 81.33 60.33 38.89 27. Budget-4096 1m26s 6m54s 2m46s 9m24s 564.90 420.14 708.96 686.08 3.5931 22.4207 58.7487 47.7385 Budget-8192 1m14s 11m02s 5m26s 18m42s 622.53 335.91 599.82 583.69 3.4274 26.4863 96.2117 89.0785 130372 498018 349035 1124613 124776 639141 588055 1932823 Table 15: Results of RLLM-14B without Prefix Cache"
        },
        {
            "title": "Dataset",
            "content": "Acc."
        },
        {
            "title": "Output Tokens",
            "content": "RLLM-14B RLLM-14B GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 88.00 57.67 23.33 13.67 87.67 62.33 48.89 23. Budget-4096 2m31s 8m17s 3m12s 10m52s 420.23 404.26 604.58 587.18 8.2604 28.1407 69.2730 56.3880 Budget-8192 2m34s 12m31s 6m22s 21m20s 412.78 292.60 502.43 481.04 8.0207 31.9481 107.6285 91.3230 178175 576686 342347 1114920 171501 655351 569489 1817908 Table 16: Results of RLLM-32B without Prefix Cache"
        },
        {
            "title": "Dataset",
            "content": "Acc."
        },
        {
            "title": "Output Tokens",
            "content": "RLLM-32B RLLM-32B GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 92.67 61.00 25.56 19.00 92.33 70.00 56.67 27. Budget-4096 2m30s 15m30s 5m09s 17m45s 327.11 196.65 379.99 349.79 5.8756 44.8259 113.5958 83.0689 Budget-8192 2m32s 23m14s 10m24s 36m01s 314.79 152.31 302.55 275.68 5.8947 58.6775 174.7540 139.8732 129761 529743 344566 1082837 130188 615489 559504 1752688 38 Table 17: Results of RLLM-70B without Prefix Cache"
        },
        {
            "title": "Dataset",
            "content": "Acc."
        },
        {
            "title": "Output Tokens",
            "content": "RLLM-70B RLLM-70B GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 90.00 54.00 31.11 19.33 90.00 61.33 54.44 31. Budget-4096 3m25s 20m22s 7m05s 24m06s 228.12 153.16 269.74 251.73 8.2671 53.0075 136.8779 110.7695 Budget-8192 3m39s 31m57s 13m52s 46m37s 218.50 115.36 221.07 200.15 8.3275 73.7041 224.2071 177.5976 123955 535884 336872 1055915 126163 634981 544412 1644534 Table 18: Results of RLLM-7B with Different Speculative Decoding Methods Model Budget Dataset Acc. Running Time TPS TTFT Output Tokens GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA L-Step: 2 1m21s 8m26s 4m52s 16m04s 1m21s 13m50s 11m13s 37m16s L-Step: 4 1m32s 8m26s 4m49s 16m54s 1m17s 13m05s 11m13s 37m11s L-Step: 8 1m28s 8m35s 4m57s 16m25s 1m26s 13m15s 10m36s 36m48s 83.33 63.67 18.89 14.33 82.67 61.33 37.78 26.67 82.33 62.00 18.89 15.00 83.67 65.00 37.78 28. 85.67 61.00 21.11 14.67 82.33 60.33 38.89 23.33 4096 8192 4096 8192 8192 RLLM-7B RLLM-7B RLLM-7B 411.47 241.67 266.43 261.39 412.96 176.53 192.58 187.68 380.14 228.30 250.19 240.27 418.62 178.65 180.60 172. 378.44 228.01 248.74 244.13 388.10 174.36 184.35 180.27 3.8268 28.9989 91.1752 73.0445 3.7715 34.7626 156.9481 161.3714 3.9015 30.2225 84.2456 86.0878 4.0124 36.4870 151.0329 152.6671 4.0057 30.6229 92.2684 78.9886 4.0512 36.0669 146.6946 171.9770 84044 342214 226340 720304 83798 415561 383225 1223433 83812 325135 213773 699813 83857 400672 356258 84219 328151 215501 685904 83643 392872 344680 1158190 Table 19: Results of RLLM-14B with Speculative Decoding Method Model Budget L-Step Dataset Accuracy Running Time TPS TTFT Output Tokens RLLM-14B 4096 4 4 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 88.00 59.33 24.44 15.67 85.00 62.67 52.22 23.33 3m3s 11m52s 6m09s 20m42s 2m47s 17m22s 12m17s 43m14s 238.27 187.51 199.02 196.47 255.10 148.10 151.99 146. 11.1347 45.6259 126.3789 105.7676 11.5077 52.2801 194.4668 182.6785 113595 373475 217784 696919 112893 433701 337745 1099344 39 Table 20: Results of RLLM-32B with Speculative Decoding Method Model Budget L-Step Dataset Accuracy Running Time TPS TTFT Output Tokens RLLM-32B 4096 8192 4 4 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 90.33 62.33 25.56 16.67 92.00 68.67 47.78 24. 2m36s 15m45s 8m12s 26m29s 2m32s 26m6s 18m56s 56m02s 218.41 125.10 148.29 147.23 222.34 84.75 99.98 105.96 7.2955 57.7919 157.7257 117.7640 7.3433 77.4161 240.3189 216.8525 84809 333363 211331 665233 84595 376796 331180 1050802 Table 21: Results of RLLM-70B with Speculative Decoding Method Model Budget L-Step Dataset Accuracy Running Time TPS TTFT Output Tokens RLLM-70B 4096 8192 4 4 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 88.67 56.67 32.22 23.33 90.33 59.00 51.11 34.33 3m30s 22m01s 10m25s 33m40s 3m33s 31m32s 21m15s 66m23s 157.68 98.28 121.87 117.38 158.49 77.02 94.05 90.79 10.0978 72.7944 195.0349 159.7265 10.2136 86.9171 298.7859 278.9286 84118 367031 221532 678418 84802 413304 349228 1049010 For LLM, results are presented in Table 22 (7B), 23 (32B). Table 22: Results of LLM-7B with Different Speculative Decoding Methods Model Budget L-Step Dataset Accuracy Running Time TPS TTFT Output Tokens LLM-7B 4096 2 4 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 68.67 2.33 15.56 6.33 66.67 0.67 18.89 3.67 69.33 2.00 21.11 3.00 2m07s 2m40s 1m10s 2m29s 1m47s 2m32s 1m02s 2m24s 1m59s 2m54s 1m06s 2m27s 262.59 299.47 321.47 282.74 304.82 309.60 342.77 297.60 268.42 277.62 313.01 275.03 0.2300 0.3243 0.3449 0.5113 0.2263 0.3160 0.3234 0.5061 0.2257 0.3213 0.3505 0.5067 83610 119825 60913 90423 80208 115604 54837 36045 79618 121082 54877 40 Table 23: Results of LLM-32B with Speculative Decoding Method Model Budget L-Step Dataset Accuracy Running Time TPS TTFT Output Tokens LLM-32B 4096 8192 4 GSM8k MATH500 AIME24 GPQA GSM8k MATH500 AIME24 GPQA 59.67 45.33 6.67 23.67 63.00 45.67 3.33 24.67 4m07s 3m57s 1m19s 3m12s 3m33s 3m33s 1m14s 3m04s 104.92 142.52 170.42 199.91 108.95 146.57 182.46 204.56 0.3605 0.5117 0.5746 0.9070 0.3581 0.5238 0.5354 0. 61457 77136 32802 79295 53146 69419 31939"
        },
        {
            "title": "K Extended Results for Real World Benchmarking",
            "content": "Figure 12: KV cache usage of 7B models under real-world workload across different datasets. Figure 13: KV cache usage of 32B models under real-world workload across different datasets. 42 Figure 14: Num of running requests in the inference engine for 7B models under real-world workload. Figure 15: Num of running requests in the inference engine for 32B models under real-world workload."
        }
    ],
    "affiliations": [
        "HKBU",
        "Harbin Institute of Technology, Shenzhen",
        "Shenzhen International Graduate School, Tsinghua University",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "University of Wisconsin-Madison"
    ]
}