{
    "paper_title": "MINED: Probing and Updating with Multimodal Time-Sensitive Knowledge for Large Multimodal Models",
    "authors": [
        "Kailin Jiang",
        "Ning Jiang",
        "Yuchen Ren",
        "Yuchen Li",
        "Yifan Gao",
        "Jinhe Bi",
        "Yunpu Ma",
        "Qingqing Liu",
        "Xianhao Wang",
        "Yifan Jia",
        "Hongbo Jiang",
        "Yaocong Hu",
        "Bin Li",
        "Lei Liu",
        "Yuntao Du"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs' ability to understand time-sensitive knowledge. To address this gap, we propose MINED, a comprehensive benchmark that evaluates temporal awareness along 6 key dimensions and 11 challenging tasks: cognition, awareness, trustworthiness, understanding, reasoning, and robustness. MINED is constructed from Wikipedia by two professional annotators, containing 2,104 time-sensitive knowledge samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07, while most open-source LMMs still lack time understanding ability. Meanwhile, LMMs perform best on organization knowledge, whereas their performance is weakest on sport. To address these challenges, we investigate the feasibility of updating time-sensitive knowledge in LMMs through knowledge editing methods and observe that LMMs can effectively update knowledge via knowledge editing methods in single editing scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 7 5 4 9 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "MINED: PROBING AND UPDATING WITH MULTIMODAL TIME-SENSITIVE KNOWLEDGE FOR LARGE MULTIMODAL MODELS Kailin Jiang1, Ning Jiang2, Yuchen Ren3, Yuchen Li4, Yifan Gao1, Jinhe Bi5, Yunpu Ma5, Qingqing Liu6, Xianhao Wang1, Yifan Jia7, Hongbo Jiang8, Yaocong Hu4, Bin Li1, Lei Liu1(cid:66), Yuntao Du7(cid:66) 1University of Science and Technology of China 2Northeast Forestry University 3University of Sydney 4Anhui Polytechnic University 5Ludwig Maximilian University of Munich 6Beijing Institute of Technology 7Shandong University 8Xiamen University"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Multimodal Models (LMMs) encode rich factual knowledge via cross-modal pre-training, yet their static representations struggle to maintain an accurate understanding of time-sensitive factual knowledge. Existing benchmarks remain constrained by static designs, inadequately evaluating LMMs ability to understand time-sensitive knowledge. To address this gap, we propose MINED, comprehensive benchmark that evaluates temporal awareness along 6 key dimensions and 11 challenging tasks: cognition, awareness, trustworthiness, understanding, reasoning, and robustness. MINED is constructed from Wikipedia by two professional annotators, containing 2,104 time-sensitive knowledge samples spanning six knowledge types. Evaluating 15 widely used LMMs on MINED shows that Gemini-2.5-Pro achieves the highest average CEM score of 63.07, while most open-source LMMs still lack time understanding ability. Meanwhile, LMMs perform best on organization knowledge, whereas their performance is weakest on sport. To address these challenges, we investigate the feasibility of updating time-sensitive knowledge in LMMs through knowledge editing methods and observe that LMMs can effectively update knowledge via knowledge editing methods in single editing scenarios. Project Page: https://mined-lmm.github.io/"
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Multimodal Models have demonstrated remarkable progress in understanding and reasoning tasks. However, real-world multimodal data often exhibit dynamic and time-sensitive characteristics, such as factual knowledge that evolves and updates continuously. To effectively handle such temporal data, LMMs must not only comprehend static visual and textual content but also incorporate temporal awareness. This capability enables them to track, interpret, and reason about cross-modal changes over time. Current research primarily focuses on temporal awareness in LLMs. Temporal QA benchmarks such as TimeQA (Chen et al., 2021) and TempReason (Tan et al., 2023) evaluate how models perceive time, but more profound challenge lies in whether the model can effectively apply time-sensitive knowledge in continuously evolving scenario. Some studies assess temporal query capabilities through dynamically updated knowledge bases (Kasai et al., 2023) or by examining responses to rapidly changing news (Zhang et al., 2024), while EvoWiki (Tang et al., 2025) leverages real-time Figure 1: We evaluate temporal awareness of time-sensitive knowledge of SOTA LMMs across six capability dimensions. Equal contribution. (cid:66) Corresponding author."
        },
        {
            "title": "Preprint",
            "content": "Wikipedia updates for evaluation. To align with real-world issues such as temporal misalignment, conflicting information, and outdated knowledge. EvolveBench (Zhu et al., 2025) systematically evaluates LLMs ability to leverage temporal knowledge from both cognitive and conscious perspectives. Although progress has been made in temporal reasoning in the text domain, expanding to multimodal scenarios still faces challenges, especially in cross-modal temporal alignment. Recent studies have begun to explore temporal reasoning in LMMs, aiming to capture spatio-temporal dependencies and achieve visual-linguistic temporal alignment. LiveVQA (Fu et al., 2025) evaluates the ability of LMMs in real-time visual knowledge acquisition and updating by constructing large-scale VQA dataset. However, LiveVQA still lacks comprehensive evaluation of practical issues such as temporal misalignment, conflicting information, and outdated knowledge. Without addressing these factors, current evaluations fail to capture the full complexity of temporal reasoning in LMMs. Figure 2: Overview of the construction of MINED. To address this gap, we introduce MINED, novel benchmark designed to evaluate LMMs temporal awareness of time-sensitive knowledge across six key dimensions: ❶ Cognition, which measures LMMs ability to recall and extract internal knowledge and apply it effectively; ❷ Awareness, which tests LMMs ability to detect temporal misalignment between an external context and user query; ❸ Trustworthiness, which assesses the LMMs ability to identify and refuse to answer queries that contain invalid temporal information; ❹ Understanding, which examines the performance of LMMs when confronted with queries containing implicit temporal concepts; ❺ Reasoning, which evaluates the analytical ability of LMMs for temporal reasoning tasks; and ❻ Robustness, measuring the ability of LMMs to correct time comprehension errors. These dimensions collectively provide holistic framework for assessing the temporal competence of LMMs. Constructed from Wikipedia by two professional annotators, MINED comprises 2,104 time-sensitive knowledge samples and 4,208 questions spanning 6 fine-grained knowledge types. We conduct extensive evaluations of 15 widely used LMMs on MINED to assess their temporal understanding capabilities. Experimental results indicate that Gemini-2.5-Pro achieve the highest CEM score of 63.07. However, most open-source LMMs, such as LLaVA-v1.5 (7B) and Qwen-VL (7B), still exhibit notable deficiencies in comprehending time-sensitive knowledge. Evaluated across 6 fine-grained knowledge types, LMMs perform best on organization knowledge but exhibit notable weaknesses in sport knowledge. These findings underscore the need for further improvements in timesensitive knowledge understanding among existing LMMs. To address this challenge, we employ knowledge editing methods to update time-sensitive knowledge that LLaVA-v1.5 (7B) and Qwen-VL (7B) initially failed to answer. Results indicate that knowledge editing methods can effectively update time-sensitive knowledge in single editing scenarios. We propose MINED, novel multi-dimensional benchmark designed to evaluate LMMs temporal awareness of time-sensitive knowledge."
        },
        {
            "title": "Preprint",
            "content": "We perform extensive experiments on 15 widely-used LMMs, the results reveal several limitations for current LMMs in handling temporal multimodal knowledge, establishing foundation for further research on temporal understanding in multimodal systems. We explore the feasibility of knowledge editing methods for updating missing time-sensitive knowledge in LMMs, providing insights for enhancing temporal capabilities for such models."
        },
        {
            "title": "2.1 LARGE MULTIMODAL MODEL",
            "content": "The development of LMMs has transitioned from unimodal models to systems supporting joint vision-language reasoning. Early approaches like CLIP (Radford et al., 2021) used contrastive learning for representation alignment but were limited to recognition. Contemporary architectures typically combine visual encoders, language models, and cross-modal modules. Models such as LLaVA-v1.5 (Liu et al., 2024a), Qwen2.5-VL (Bai et al., 2025), and GPT-4o (OpenAI, 2023) employ projection, end-to-end transformers, or unified architectures for multimodal alignment. Further enhancements in Gemini-2.5-Pro (Gemini Team, 2025) and Kimi-Latest (Kimi Team et al., 2025) improve reasoning and long-context handling through dynamic routing and efficient decoding, significantly boosting performance in visual dialogue, scene understanding, and reasoning."
        },
        {
            "title": "2.2 TEMPORAL REASONING BENCHMARKS",
            "content": "Temporal reasoning denotes models capacity to identify, understand, and infer temporal expressions along with logical temporal relationships such as order, containment, and causality (Bi et al., 2025c). Recent benchmarks like TimeQA (Chen et al., 2021), MenatQA (Wei et al., 2023), TEMPREASON (Tan et al., 2023), and UnSeenTimeQA (Uddin et al., 2025) have been developed to evaluate these capabilities in large language models, focusing on contextual temporal understanding and reasoning. Existing temporal reasoning benchmarks largely ignore time-sensitive knowledge. EvolveBench (Zhu et al., 2025) addresses this gap by evaluating LLMs capacity to leverage temporal knowledge, providing new insights for dynamic knowledge integration. Current studies on temporal reasoning in LMMs are scarce. LiveVQA (Fu et al., 2025) evaluates real-time knowledge acquisition via visual recognition and multi-hop reasoning but overlooks the critical influence of time-sensitive knowledge. This paper introduces MINED that comprehensively evaluates LMMs temporal awareness on timesensitive knowledge. Table 1 shows the comparison between other related benchmarks. Table 1: Overall comparison with existing temporal knowledge benchmarks. P-Agr is Prompt Agreement (Section 4.1)."
        },
        {
            "title": "Benchmark",
            "content": "Multimodal Cog. Awa. Tru. Und. Rea. Rob. P-Agr. TimeQA (Chen et al., 2021) MenatQA (Wei et al., 2023) TempReason (Tan et al., 2023) DyKnow (Mousavi et al., 2024) UnSeenTimeQA (Uddin et al., 2025) EvoWiki (Tang et al., 2025) EvolveBench (Zhu et al., 2025) LiveVQA (Fu et al., 2025) MINED (Ours)"
        },
        {
            "title": "3 MULTIMODAL TIME-SENSITIVE KNOWLEDGE",
            "content": "In this section, we introduce the construction pipeline of the MINED benchmark using Wikipedia data. In Figure 2, each time-sensitive knowledge sample is represented as quadruple (S, H, P, A), where is the subject (e.g., person or visual entity name like Lionel Messi), is the hypernym corresponding to the subject (e.g., Lionel Messis hypernym is footballer), is the property (e.g., the"
        },
        {
            "title": "Preprint",
            "content": "property between Lionel Messi and club is play for), and = [a1, a2, , an] is list of attribute values for that property, which change over time. To construct the foundational data for MINED, we employ two professional annotators to gather time-sensitive knowledge from Wikipedia across six domains: Country, Sport, Company, University, Organization, and Competition. Each data sample is manually verified to ensure high quality. In this benchmark, we set the knowledge cutoff date Tcurrent to June 23, 2025 (corresponding to the benchmark cut-off node in Figure 2)."
        },
        {
            "title": "3.1 BENCHMARK CONSTRUCTION",
            "content": "Dimension 1: Cognition of Time-Sensitive Knowledge. We propose three cognitive levels of varying difficulty to evaluate the ability of LMMs to probe for time-sensitive factual knowledge using their parameters. Given the image of the entity and property , we require the model to probe for the correct knowledge at specific time by leveraging its internal knowledge. Time-Agnostic (T.A) refers to using current or currently to inform the model to provide the latest answer in without giving clear time node. Temporal Interval-Aware (T.I.A) refers to randomly selecting time period (from Tstart to Tend) from the attribute list to prompt the model to provide the corresponding answer. Timestamp-Aware (T.S.A) refers to using random dates between Tstart and Tend to prompt the model to provide corresponding answers. Dimension 2: Awareness of Temporal Misalignment. We evaluate how LMMs handle internal parametric knowledge when external context is temporal misaligned with timestamps in user queries. Future Misaligned Context (F.M.C): We randomly sample past timestamp Tpast from the attribute set for property to construct the query. Subsequently, we provide latest acurrent with and to GPT-4o, instructing it to generate context Ccurrent that elaborately describes the knowledge triple (S, P, acurrent). Under this setting, the temporal information contained in Ccurrent exhibits temporal misalignment with the timestamp Tpast specified in the query, indicating the information is accurate yet futuristic relative to the query timestamp. Past Misaligned Context (P.M.C): User query incorporates the current timestamp Tcurrent. We randomly select past attribute value apast with and to GPT-4o and ask it to generate context Cpast that elaborately describes the knowledge triple (S, P, apast). This configuration evaluates the models capacity to process obsolete information in its responses to user queries. Dimension 3: Trustworthiness of Unanswerable Date. We introduce credibility as third dimension to evaluate whether LMMs produce hallucinations when facing unanswerable date-related queries. Specifically, query is deemed unanswerable if the timestamp provided by the user precedes the earliest record in attribute list for subject and property , or refers to future date. Past Unanswerable Date (P.U.D): We extract the earliest record from attribute list and subtract certain year from it to construct an unanswerable date in the past. For instance, as shown in Figure 2, Lionel Messi had not started his professional career before 2003, so we select time point prior to that year as the past unanswerable date. Future Unanswerable Date (F.U.D): We take the latest record from attribute list and add certain year to construct an unanswerable future date. In Figure 2, Which club will the footballer in the image play for in 2075? is an example based on future unanswerable date. Dimension 4: Understanding of Temporal Concept. This dimension evaluates how effectively LMMs interpret temporal concepts expressed in different formats. In previous evaluations, explicit time formats (e.g., DD Month YYYY) were used to denote temporal information. For implicit temporal expressions, temporal intervals [Tstart, Tend] are defined based on historical events. Implicit Temporal Concept (I.T.C): In Figure 2, the phrase when Jeff Bezos served as CEO of Amazon corresponds to the period from July 5, 1994, to July 5, 2021. Such implicit temporal representations are denoted as Timplicit. Dimension 5: Temporal Reasoning. We propose two tasks to evaluate temporal reasoning in LMMs: ranking task for chronological ordering to assess temporal logic, and calculation task involving time intervals and durations to measure numerical precision."
        },
        {
            "title": "Preprint",
            "content": "Ranking (R.K): Two past events a1 and a2 are randomly selected from attribute list of the tuple (S, P, A). The model is required to determine their correct temporal order by first extracting their timestamps from the input, comparing them, and then providing the final chronological sequence. Calculation (C.A): For two events a1 and a2, date t1 and t2 is randomly selected from their respective time intervals [Tstart, Tend], and the number of days between them, denoted as T, is calculated. Given t1 and T, the task requires the model to perform the necessary computation and infer the correct date corresponding to the target event a2. Dimension 6: Robustness of Time-Sensitive Knowledge. Robustness serves as the final evaluation dimension to assess whether model can effectively identify and self-correct its previous errors when provided with appropriate prompts. Adversarial Temporal Error (A.T.E): We extract knowledge samples for which all LMMs provided incorrect answers across three cognitive subtasks. Using the prompt: Your answer to the original question is wrong. followed by rephrased interrogative form, we examine whether the models can correct their previous errors. Benchmark Analysis: Category Distribution and Key Statistics. In Table 2 and Figure 3, MINED comprises 4,208 questions, spanning 6 dimensions and 6 types of fine-grained knowledge, demonstrating substantial diversity (Bi et al., 2025a). As for quality, the original data of MINED is collected from Wikipedia by two expert annotators, with each entry manually verified to ensure high quality. Regarding MINEDs details, chat templates and case studies, please refer to Appendix B, and G. Table 2: Key Statistics of MINED."
        },
        {
            "title": "Total questions",
            "content": "- Cognition questions - Awareness questions - Trustworthiness questions - Understanding questions - Reasoning questions - Robustness questions Total dimension/subtasks Total fine-grained knowledge types Number of unique images"
        },
        {
            "title": "Number",
            "content": "4,208 1,328 (31.6%) 834 (19.8%) 828 (19.7%) 510 (12.1%) 324 (7.7%) 384 (8.1%) 6/11 6 450 54 13 11.4 2 Figure 3: Subtasks for evaluating each capability dimension."
        },
        {
            "title": "4 EXPERIMENT OF PROBING MULTIMODAL TIME-SENSITIVE KNOWLEDGE",
            "content": "4."
        },
        {
            "title": "EXPERIMENTAL SETUP",
            "content": "Large Multimodal Models. In this paper, we evaluate 15 widely used LMMs on MINED, including: LLaVA-v1.5 (Liu et al., 2024a), Qwen-VL (Bai et al., 2023), , mPLUG-Owl2 (Ye et al., 2023), LLaVA-Next (Liu et al., 2024b), LLaVA-OneVision (Li et al., 2024a), mPlug-Owl3 (Ye et al., 2024), MiniCPM-V2.6 (Yao et al., 2024), Qwen2-VL (Wang et al., 2024), InternVL2.5 (Chen et al., 2024), Qwen2.5-VL (Bai et al., 2025), GPT-4.1 (OpenAI, 2023), Kimi-Latest (Kimi Team et al., 2025), Doubao-1.5-Vision-Pro, Gemini-2.5-Pro (Gemini Team, 2025), Seed-1.6-Vision. Evaluation Protocol: In the evaluation of all subtasks, the model is considered to have correctly responded to the time-sensitive knowledge only when its output exactly matches the corresponding ground truth. Therefore, we evaluate the models outputs using Cover Exact Match (CEM) (Xu et al., 2023) score for each subtask. The models capacity in this dimension is defined as the average CEM score across all subtasks. CEM requires matching models outputs with ground truth. Cd ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 CEMi, CEM = (cid:26)1, 0, ˆy otherwise (1) Where is the number of subtasks in capacity dimension d, CEMi is score of the i-th subtask, and ˆy represent the models output and the ground truth, respectively."
        },
        {
            "title": "Preprint",
            "content": "Prompt Agreement: To mitigate uncertainty from prompt variations, we designed four distinct prompts (Question, Generalization Question,Image, and Generalization Image) for each knowledge. These prompts share the same core meaning but differ in phrasing and are paired to form four unique configurations. The final score is computed by averaging the CEM scores across these prompt variations, strategy we term Prompt Agreement."
        },
        {
            "title": "4.2 ANALYSIS OF MAIN RESULTS\nTable 3: Overall Performance Comparison (%) on MINED. The top two and worst performing\nresults are highlighted in red (1st), yellow (2nd) and blue (bottom) backgrounds, respectively.\nSubscripts M. and I. stand for Mistral-7B and Instruct, respectively.",
            "content": "(Release Time) Models Cog. Awa. Tru. Und. Rea. Rob. T.A T.I.A T.S.A F.M.C P.M.C P.U.D F.U.D I.T.C R.K C.A A.T.E (2023.04) LLaVA-v1.5 (7B) (2023.08) Qwen-VL (7B) (2023.11) mPLUG-Owl2 (7B) (2024.01) LLaVA-NextM. (7B) (2024.08) LLaVA-OV (7B) (2024.08) mPlug-Owl3 (8B) (2024.08) MiniCPM-V2.6 (8B) (2024.09) Qwen2-VLI. (7B) (2024.12) InternVL2.5 (8B) (2025.02) Qwen2.5-VLI. (7B) (2025.02) Kimi-Latest (2025.02) Doubao-1.5-Vision-Pro (2025.03) Gemini-2.5-Pro (2025.04) GPT-4.1 (2025.08) Seed-1.6-Vision 6.96 12.45 10.59 10.69 11.86 9.80 22.16 15.98 20.49 18.33 26.41 35.78 34.25 37.58 37. 9.25 17.30 14.53 14.53 11.34 10.03 21.66 16.72 18.46 16.86 26.60 27.91 56.40 37.94 41.76 16.88 42.09 44.62 41.14 26.79 29.01 55.70 31.96 44.83 41.67 72.43 69.83 84.96 80.91 78.69 Open-source LMMs 7.66 6.04 42.69 33.69 30.93 29.77 38.88 17.90 42.37 40.04 6.40 6.91 38.67 28.87 31.35 28.31 31.35 11.46 38.26 33. 53.99 81.28 11.47 96.74 39.61 97.95 81.52 99.52 98.31 99.64 Closed-source LMMs 68.64 74.36 83.09 78.07 75.95 67.27 70.76 84.30 77.49 80.71 72.10 93.12 80.31 65.22 74.15 50.00 70.17 44.20 90.22 76.21 99.76 97.83 99.76 99.88 99.76 85.39 100.00 97.10 91.30 96. 1.57 3.53 2.16 3.73 3.63 3.14 4.22 4.61 4.22 4.02 7.06 5.29 18.73 8.63 7.55 15.12 25.00 42.90 38.58 51.54 41.98 52.78 49.38 61.73 38.89 45.99 18.52 38.48 15.74 21.60 6.17 17.59 14.20 20.99 8.95 7.10 24.38 14.20 19.14 25.00 42.59 34.57 76.54 59.57 59. 0.39 0.00 6.12 0.00 2.21 3.65 14.45 9.90 0.00 16.86 6.38 12.24 39.58 17.58 32.68 Avg. 15.85 25.67 24.74 34.47 26.77 32.77 40.45 33.76 40.70 39.55 47.35 49.31 63.07 51.82 55.16 We conduct extensive experiments to evaluate 15 widely used LMMs on MINED. Table 3 presents the main results and additional results are in Appendix C. Key observations from Table 3 include: Obs 1: LMMs exhibit improved cognitive performance when queries are framed as timestampaware task. When evaluating the cognitive capacities of LMMs, we present queries conveying identical knowledge in three distinct temporal formats: Time-Agnostic, Temporal Interval-Aware, and Timestamp-Aware. For the knowledge Lionel Messi played for Inter Miami CF, TimeAgnostic, Temporal Interval-Aware, and Timestamp-Aware queries are formulated as follows: Which club does the person in the image currently play for?, Which club did the footballer play for between 2023 and 2024?, and Which club did the footballer play for on 1 January 2024?, respectively. In Table 3, all LMMs perform better on Timestamp-Aware tasks. This phenomenon may stem from the narrower temporal context required: Timestamp-Aware queries only necessitate knowledge retrieval for specific point in time, whereas Time-Agnostic and Temporal IntervalAware tasks demand recalling broader or time period-based information, which is more challenging. Despite this, the top-performing model, Gemini-2.5-Pro, still fails to recall approximately 25% of the knowledge, underscoring the importance of temporal sensitivity in model reasoning. Obs 2: LMMs are vulnerable to temporal misaligned context, especially from past temporal misaligned contexts. Compared to T.S.A. results in Table 3, LMMs performance degrades when queries are accompanied by temporal misaligned context, which impedes correct knowledge recall. For the experiment in Figure 7, we use the same timestamp in the queries, with the only difference being whether the input query included the relevant but temporal misaligned text. We observe that more capable closed-source models and larger open-source models exhibit greater robustness to temporally misaligned context, whereas smaller open-source models suffer significant performance degradation. For instance, Qwen2-VLI. (7B) shows declines of 43.84% on F.M.C and 56.43% on P.M.C. These results indicate that smaller models are more susceptible to misleading temporal context, with past misaligned information having particularly strong negative impact. Obs 3: LMMs are better at rejecting questions with unanswerable future dates than those with past dates. As indicated by P.U.D and F.U.D results in Table 3, most LMMs (except for mPLUG-Owl2 (7B)) are capable of effectively rejecting questions that contain unanswerable dates from either the past or the future. This is likely because such dates are absent from the training data, allowing the models to reject them with greater confidence. Furthermore, LMMs show slightly stronger propensity to reject questions with unanswerable future dates, likely because these represent entirely unseen temporal concepts, resulting in even greater refusal certainty. Surprisingly,"
        },
        {
            "title": "Preprint",
            "content": "both Qwen2-VLI. (7B) (average CEM score of 99.64) and Qwen2.5-VLI. (7B) (average CEM score of 99.70) demonstrate exceptional performance in question refusal, capability potentially attributable to enhanced defensive mechanisms from their instruction tuning process. Obs 4: All LLMs perform terribly on tasks involving implicit temporal concepts. In the I.T.C column of Table 3, all LLMs perform terribly, with even the top-performing model, Gemini-2.5Pro, recalling less than 20% of relevant knowledge. This indicates fundamental deficiency in understanding and utilizing implicit temporal concepts. Obs 5: Open-source LMMs demonstrate stronger performance on simpler ranking task, whereas closed-source LMMs excel in more complex calculation task. Unexpectedly, MiniCPMV2.6 (8B) and InternVL2.5 (8B) achieved the highest performance on ranking task, while models such as GPT-4.1 and Doubao-1.5-Vision-Pro scored below 20% in CEM. Figure 5 further illustrates this phenomenon, showing decline in ranking performance within the Qwen2.5-VLI. series as model size increases 50.3(3B) 38.9(7B) 11.4(72B), potentially due to overthinking. Larger models, despite their enhanced reasoning capabilities, may overcomplicate simple tasks like ranking, leading to reduced effectiveness. In contrast, on more challenging calculation task, closed-source LMMs including Gemini-2.5-Pro and GPT-4.1 demonstrated superior performance. Obs 6: Current LMMs demonstrate limited adversarial robustness against temporal errors. According to the A.T.E results in Table 3, models such as Qwen-VL (7B), LLaVA-NextM. (7B), and InternVL2.5 (8B) fail to correct any prior errors, demonstrating severely limited robustness. Even the top-performing model, Gemini-2.5-Pro, corrects fewer than 40% of errors. These results indicate significant need for improvement in temporal reasoning robustness across current models. Obs 7: More recent LMMs exhibit better temporal awareness performance. Avg. results in Table 3 reveal an approximate trend: more recent LMMs generally achieve superior overall performance, indicating link between temporal awareness and recency of development."
        },
        {
            "title": "4.3 ANALYSIS OF EXPLORATORY RESULTS",
            "content": "In this section, we present further explorations into evaluation of time-sensitive knowledge, yielding the following observations. Exp 1: Fine-grained Knowledge Types. All LMMs show consistent trends in recalling timesensitive knowledge across domains. As shown in Figure 4, LMMs perform better on queries related to organization, company, and country leaders, but worse on athletes and competition champions,likely due to the broader coverage of the former in public knowledge sources. Furthermore, closed-source models outperform open-source variants on university president queries, indicating potential discrepancies in their pretraining corpora. Figure 4: The cognitive capacity of various LMMs across six specific knowledge types when queried with Time-Agnostic tasks. Exp 2: Model Size and Foundation LLM. Observing Figure 5, we have the following findings: (1) Larger model sizes generally lead to improved performance on most tasks, except for R.K, P.U.D, F.U.D, and A.T.E. (2) Even with an identical architecture, LMMs exhibit divergent performance when using different foundation LLMs. For instance, while LLaVA-NextL. (8B) and LLaVANextM. (7B) perform poorly on A.T.E task, LLaVA-NextV. (7B) achieves CEM score of 31.2. Exp 3: Fine-grained Analysis of Time-Agnostic and Temporal Distribution. In the TimeAgnostic task, we further categorize the models outputs into fine-grained labels. Since Prompt Agreement is adopted, each knowledge yields four outputs. If any output contains the most upto-date value from the attribute list A, it is labeled as Latest. If none includes the latest value but at least one contains an outdated answer, it is marked as Outdated. All other cases are categorized as Irrelevant. In Table 4, open-source models not only produce limited number of latest responses but also generate substantial portion of irrelevant responses. In contrast,"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Analysis of impact of different model sizes and foundation LLMs. closed-source models reduce the frequency of irrelevant responses but still exhibit high proportion of outdated responses. These statistical results indicate that significant portion of model-generated responses are either outdated or irrelevant, highlighting pronounced issue of inaccurate timesensitive knowledge. Figure 6 provides an approximate visualization of the temporal distribution of knowledge within LMMs. Closed-source models demonstrate broader temporal coverage. In contrast, the internal knowledge of open-source models is concentrated in more recent time periods, indicating comparative difficulty in recalling information from distant historical contexts. Table 4: Fine-grained analysis of predicted output in Time-Agnostic."
        },
        {
            "title": "Model",
            "content": "Time-Agnostic Lat. Out. Irr. Open-source LMMs LLaVA-v1.5 (7B) LLaVA-NextM. (7B) InternVL2.5 (1B) InternVL2.5 (8B) Qwen2.5-VLI. (7B) 14.90 19.22 14.12 16.08 20. 27.45 36.47 33.73 43.92 56.86 Closed-source LMMs Kimi-Latest GPT-4.1 Seed-1.6-Vision 24.71 28.04 21.57 58.82 53.53 64.31 57.65 44.31 44.31 40.00 23. 16.47 18.43 14.12 Figure 6: Approximating temporal distribution of internal knowledge of LMMs. Exp 4: Error analysis of Awareness of Temporal Misalignment. Table 5 provides detailed error analysis of awareness experiment. The red values in the bracket mean negative effect, while green means positive. Con. to context-based answers, Oth. to other answers, and Irr. to irrelevant ones. Surprisingly, even when provided with relevant context, models still generate responses that are irrelevant to the query or contain incorrect values from attribute list A, rather than leveraging the given context. This finding underscores the need to further investigate how models integrate external information with their internal knowledge. Table 5: Error analysis when provide misaligned context. Model Future Misaligned Context Past Misaligned Context Con. Oth. Irr. Con. Oth. Irr. GPT-4.1 Qwen2-VLI. (7B) LLaVA-NextM. (7B) Qwen2.5-VLI. (72B) 7.94 64.72 52.44 8.79 w/ Misaligned Context 5.61 5.93 4.98 8.16 8.37 11.44 9.11 12.61 10.64 77.21 57.46 12. w/o Misaligned Context 4.83 4.42 5.39 8.01 7.04 6.91 8.29 10.50 GPT-4.1 Qwen2-VLI. (7B) LLaVA-NextM. (7B) Qwen2.5-VLI. (72B) 3.92 (-4.02) 5.51 (-59.21) 7.84 (-44.60) 5.72 (-3.07) 6.78 (+1.17) 23.41 (+17.48) 15.15 (+10.17) 10.06 (+1.90) 8.47 (+0.10) 39.41 (+27.97) 36.23 (+27.12) 12.92 (+0.31) 6.01 (-4.63) 12.18 (-65.03) 12.5 (-44.96) 7.95 (-4.20) 7.47 (+2.64) 20.62 (+16.20) 14.77 (+9.38) 9.58 (+1.57) 8.12 (+1.08) 40.91 (+34.00) 39.29 (+31.00) 13.8 (+3.30) Figure 7: Comparison of performance with and without misaligned context."
        },
        {
            "title": "5 CAN WE UPDATE LMMS WITH TIME-SENSITIVE KNOWLEDGE?",
            "content": "Section 4 reveals that existing LMMs struggle to effectively process time-sensitive knowledge, while also being hampered by substantial amounts of outdated and irrelevant information. Knowledge editing updates factual knowledge in LLMs and LMMs, enabling efficient correction of outdated or inaccurate information without full retraining. Building on prior work (Cheng et al., 2023; Huang"
        },
        {
            "title": "Preprint",
            "content": "et al., 2024; Li et al., 2024b; Zhang et al., 2025; Bi et al., 2025b), we ask: Can LMMs be effectively updated with time-sensitive knowledge? We explore multimodal time-sensitive knowledge editing and updating in real-world scenarios. We observe that LLaVA-v1.5 (7B) and Qwen-VL (7B) perform poorly and are therefore used as outdated models for knowledge editing. Regarding the selection of editing data, we extracted samples from these two models where CEM score is not 100 across five dimensions: cognition, trustworthiness, understanding, reasoning and robustness. Evaluation metric follows the protocol in Section 4.1. For more details, please refer to Appendix F. Methods and Editing Setting: We adopt two categories of multimodal knowledge editing approaches: parameter-modifying, like FT-LLM, FT-VIS, MEND (Mitchell et al., 2022a) and parameterpreserving, like SERAC (Mitchell et al., 2022b), IKE (Zheng et al., 2023). We adopt the following two types of editing settings: ❶ Single editing restores weights after each edit, whereas ❷ lifelong editing examines the cumulative effects of editing entire dataset before evaluating all instances. Table 6: Single Editing Performance Comparison (%) on MINED. The top and worst performing results are highlighted in red (1st) and blue (bottom) backgrounds, respectively."
        },
        {
            "title": "Method",
            "content": "Cog. Tru. T.A T.I.A T.S.A P.U.D F.U.D"
        },
        {
            "title": "Modifying\nParameters",
            "content": "FT-LLM 97.99 85.78 FT-VIS 66.81 MEND"
        },
        {
            "title": "SERAC\nIKE",
            "content": "66.09 85."
        },
        {
            "title": "Modifying\nParameters",
            "content": "FT-LLM 86.55 81.14 FT-VIS 68.13 MEND"
        },
        {
            "title": "SERAC\nIKE",
            "content": "57.16 86.52 93.54 82.92 69.79 67.71 82.40 86.58 79.64 70.47 66.22 78.08 LLaVA-v1.5 (7B) 100.00 79.17 26.62 65.28 47.45 100.00 76.49 18.09 65.12 44.44 Qwen-VL (7B) 100.00 69.92 79.67 100.00 74.27 84. 69.92 72.15 74.56 60.82 92.87 94.88 73.95 71.78 99.38 89.94 80.50 54.93 62.05 91. Und. I.T.C 96.16 78.33 65.71 66.53 75.24 81.81 75.70 64.14 56.44 74. Rea. Rob. R.K C.A A.T.E 96.00 93.33 73. 55.56 59.11 87.50 74.07 65.74 62.96 68.75 97.81 88.60 69.74 67.54 91.23 88.98 80.19 50. 52.17 92.75 100.00 99.64 100.00 28.67 99.19 100.00 100.00 100.00 18.36 92."
        },
        {
            "title": "Avg",
            "content": "97.15 86.57 62.72 61.59 76.02 91.25 79.49 70.90 57.76 79.63 Single Editing Shows Strong Effectiveness: By observing Table 6, we make the following observations: ❶ FT-LLM demonstrates strong performance as knowledge updating method, achieving superior results across all evaluated tasks. ❷ In contrast, both the SERAC and MINED exhibit comparatively weaker performance, demonstrating limited effectiveness in knowledge updating tasks. ❸ Exception of SERAC, all methods achieve excellent performance on A.T.E task, demonstrating the strong robustness of current knowledge editing approaches. ❹ Knowledge updating significantly enhances the models performance on complex I.T.C and C.A tasks. Table 7: Lifelong Editing Performance on MINED. All results are base on LLaVA-v1.5 (7B). Red and green values mean negative and positive effects relative to data in Table 6, respectively."
        },
        {
            "title": "Method",
            "content": "FT-LLM FT-VIS"
        },
        {
            "title": "SERAC",
            "content": "T.A 31.03 (-66.96) 12.64 (-73.14) 53.74 (-12.35) Cog. T.I.A 32.29 (-61.25) 12.50 (-70.42) 53.33 (-14.38) Tru. T.S.A P.U.D F.U.D 25.89 (-66.98) 2.17 (-92.71) 70.08 (-1.70) 100.00 (+0.00) 73.61 (-5.56) 65.97 (+0.69) 98.97 (-1.03) 78.55 (+2.06) 66.41 (+1.29) Und. I.T.C 9.33 (-86.83) 6.45 (-71.88) 5.87 (-60.66) Rea. R.K C.A 60.44 (-35.56) 16.00 (-77.33) 42.67 (-12.89) 27.63 (-70.18) 10.96 (-77.64) 61.84 (-5.70) Rob. A.T.E 100.00 (+0.00) 100.00 (+0.36) 41.22 (+12.55)"
        },
        {
            "title": "Avg",
            "content": "53.95 (-43.20) 34.76 (-51.81) 51.24 (-10.35) Lifelong Editing Still Needs Improvement: By observing Table 7, we make the following observations: ❶ Except for P.U.D, F.U.D and A.T.E tasks, knowledge updating performance of FT-LLM, FT-VIS and SERAC has experienced varying degrees of loss. ❷ SERAC maintains excellent performance in lifelong editing scenario, with only 10.35% loss. Its memory-based architecture mitigates catastrophic forgetting through explicit caching, maintaining robust performance in lifelong editing. ❸ Performance of SERAC in A.T.E has been improved by 12.55%, which may be due to lifelong editing making SERAC better suited for robustness tasks."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "During the development process, we recognize the ethical implications of deploying LMMs. Ensuring the integrity and reliability of multimodal time-sensitive knowledge is crucial for avoiding the spread of outdated and distorted information. Our research reveals the key limitations of existing LMMs in handling multimodal time sensitive knowledge, while verifying the reliability of knowledge editing methods in updating outdated multimodal time sensitive knowledge. Provided valuable insights for improving the reliability of LMMs."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure the reproducibility of our findings, we will release our complete source code and MINED dataset on Hugging Face upon completion of the review process. Furthermore, all open-source models used in our experiments are downloaded from Hugging Face, ensuring that other researchers can access the identical model weights used in our study. We hope these measures will enable other researchers to verify and reproduce our results."
        },
        {
            "title": "REFERENCES",
            "content": "Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arxiv 2023. arXiv preprint arXiv:2308.12966, 1(8), 2023. 5 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. In Proceedings of the Conference on Vision-Language Research 2025, 2025. arXiv preprint arXiv:2502.13923. 3, 5 Jinhe Bi, Yifan Wang, Danqi Yan, Aniri, Wenke Huang, Zengjie Jin, Xiaowen Ma, Artur Hecker, Mang Ye, Xun Xiao, Hinrich Schuetze, Volker Tresp, and Yunpu Ma. Prism: Self-pruning intrinsic selection method for training-free multimodal data selection, 2025a. URL https://arxiv.org/ abs/2502.12119. 5 Jinhe Bi, Yujun Wang, Haokun Chen, Xun Xiao, Artur Hecker, Volker Tresp, and Yunpu Ma. Llava steering: Visual instruction tuning with 500x fewer parameters through modality linear representation-steering, 2025b. URL https://arxiv.org/abs/2412.12359. 9 Jinhe Bi, Danqi Yan, Yifan Wang, Wenke Huang, Haokun Chen, Guancheng Wan, Mang Ye, Xun Xiao, Hinrich Schuetze, Volker Tresp, et al. Cot-kinetics: theoretical modeling assessing lrm reasoning process. arXiv preprint arXiv:2505.13408, 2025c. 3 Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. Rq-rag: Learning to refine queries for retrieval augmented generation. arXiv preprint arXiv:2404.00610, 2024. Wenhu Chen, Xinyi Wang, and William Yang Wang. dataset for answering time-sensitive questions. arXiv preprint arXiv:2108.06314, 2021. 1, 3 Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen,"
        },
        {
            "title": "Preprint",
            "content": "Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 1, 2024. 5 Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and Ningyu Zhang. Can we edit multimodal large language models? In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, 2023. 8 Mingyang Fu, Yuyang Peng, Dongping Chen, Zetong Zhou, Benlin Liu, Yao Wan, Zhou Zhao, Philip S. Yu, and Ranjay Krishna. Seeking and updating with live visual knowledge. arXiv preprint arXiv:2504.05288, 2025. 2, 3 Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 3, 5 Han Huang, Haitian Zhong, Tao Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. Vlkeb: large vision-language model knowledge editing benchmark. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. 8 Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. RealTime QA: whats the answer right now? Advances in Neural Information Processing Systems 36 (NeurIPS 2023), 2023. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 3, 5 Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. In Proceedings of the Conference on Vision-Language Research 2024, 2024a. arXiv preprint arXiv:2408.03326. 5 Jiaqi Li, Miaozeng Du, Chuanyi Zhang, Yongrui Chen, Nan Hu, Guilin Qi, Haiyun Jiang, Siyuan Cheng, and Bozhong Tian. MIKE: new benchmark for fine-grained multimodal entity knowledge editing. In Findings of ACL, pp. 50185029, 2024b. 9 Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024a. 3, 5 Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge. In Proceedings of the Conference on Vision-Language Research 2024, 2024b. 5 Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher Manning. Fast model editing at scale. In International Conference on Learning Representations, 2022a."
        },
        {
            "title": "Preprint",
            "content": "Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Memorybased model editing at scale. In International Conference on Machine Learning, 2022b. 9 Seyed Mahed Mousavi, Simone Alghisi, and Giuseppe Riccardi. Dyknow: Dynamically verifying In Findings of the Association for Computational time-sensitive factual knowledge in llms. Linguistics: EMNLP 2024, pp. 80148029, 2024. 3 OpenAI. Gpt-4 technical report. In Proceedings of the Conference on Artificial Intelligence Research 2023, 2023. arXiv preprint arXiv:2303.08774. 3, Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, 2021. 3 Qingyu Tan, Hwee Tou Ng, and Lidong Bing. Towards benchmarking and improving the temporal reasoning capability of large language models. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023, 2023. 1, 3 Wei Tang, Yixin Cao, Yang Deng, Jiahao Ying, Bo Wang, Yizhe Yang, Yuyue Zhao, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang, and Yong Liao. Evowiki: Evaluating llms on evolving knowledge. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, pp. 948964, 2025. 1, 3 Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son, Eduardo Blanco, Steven R. Corman, and Chitta Baral. Unseentimeqa: Time-sensitive question-answering beyond llms memorization. arXiv preprint arXiv:2407.03525, 2025. 3 Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. In Proceedings of the Conference on Vision-Language Research 2024, 2024. arXiv preprint arXiv:2409.12191. 5 Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, and Kang Liu. MenatQA: new dataset for testing the temporal comprehension and reasoning abilities of large language models. Findings of the Association for Computational Linguistics: EMNLP 2023, 2023. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023. 5 Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 5 Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 5 Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the Conference on Artificial Intelligence Research 2023, 2023. arXiv preprint arXiv:2311.04257. 5 Junzhe Zhang, Huixuan Zhang, Xunjian Yin, Baizhou Huang, Xu Zhang, Xinyu Hu, and Xiaojun Wan. MC-MKE: fine-grained multimodal knowledge editing benchmark emphasizing modality consistency. Findings of the Association for Computational Linguistics, ACL 2025, 2025."
        },
        {
            "title": "Preprint",
            "content": "Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, and Tat-Seng Chua. Analyzing temporal complex events with large language models? benchmark towards temporal, long context understanding. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. 1 Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. Can we edit factual knowledge by in-context learning? In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 48624876, Singapore, December 2023. Association for Computational Linguistics. 9 Zhiyuan Zhu, Yusheng Liao, Zhe Chen, Yuhao Wang, Yunfeng Guan, Yanfeng Wang, and Yu Wang. Evolvebench: comprehensive benchmark for assessing temporal awareness in llms on evolving knowledge. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, pp. 1617316188, 2025. 2,"
        },
        {
            "title": "B MORE DETAILS ABOUT MINED",
            "content": "B.1 MINED QUALITY AND EVOLVABILITY . . . . . . . . . . . . . . . . . . . . . B.2 MINED DETAILED QUANTITY . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C MORE EXPERIMENTAL RESULTS ABOUT MINED",
            "content": "C.1 MORE MAIN RESULTS ABOUT MINED . . . . . . . . . . . . . . . . . . . . . . . C.2 MORE MODEL SIZE RESULTS ABOUT MINED . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E CASE STUDIES ABOUT MINED",
            "content": "F UPDATING TIME-SENSITIVE KNOWLEDGE VIA KNOWLEDGE EDITING F.1 EDITING SETTING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 KNOWLEDGE EDITING METHODS AND PARAMETERS . . . . . . . . . . . . . . F.3 EDITING QUANTITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 15 16 16 17 18 22 22 22 23 MORE DETAILS ABOUT CHAT TEMPLATES AND QUANTITATIVE EXAMPLES"
        },
        {
            "title": "A THE USE OF LARGE LANGUAGE MODELS IN MINED",
            "content": "In this section, we elaborate on the precise role of large language models within MINED, as detailed below. Usage 1: MINEDs construction. In the dimension of Awareness of Temporal Misalignment (Section 3.1), GPT-4o is employed to generate contextual content related to temporal misalignment. This approach is consistent with current academic research norms. Usage 2: MINEDs evaluation. In Section 4.2, we evaluate performance on MINED using KimiLatest, Gemini-2.5-Pro, Doubao-1.5-Vision-Pro, Seed-1.6-Vision and GPT-4.1, following standard benchmarking practices. Usage 3: Paper grammar polishing. The paper is initially drafted by human authors and subsequently polished for grammar using large language model. It is not generated entirely by AI. This practice aligns with current academic norms."
        },
        {
            "title": "B MORE DETAILS ABOUT MINED",
            "content": "B.1 MINED QUALITY AND EVOLVABILITY Owing to the time-sensitive nature of MINED, we will perform quarterly updates to endow the benchmark with evolvability. Unlike conventional benchmarks that merely replace outdated data, MINED offers fundamentally distinct form of evolution. It not only evaluates model performance on time-sensitive knowledge but also probes models internal knowledge boundaries (in Section 4.3). To this end, we design an efficient pipeline to update the attribute list of each knowledge entry every quarter. This pipeline enables continuous renewal of knowledge, persistent evaluation of model knowledge boundaries, and provides the community with dynamic and evolving evaluation resource. We outline MINEDs update pipeline: (1) Leveraging existing MINED subject data, we retrieve corresponding Wikipedia text data offline (e.g., searching Lionel Messi). (2) For club affiliation information, we extract information from Wikipedias career sections using GPT-4o with strict parsing rules(the career field contains Lionel Messis club affiliation information). (3) Newly extracted club data is compared against MINEDs current records, triggering updates when discrepancies occur. This efficient pipeline ensures automated, continuous MINED updates, providing the community with an evolving evaluation resource. Combined with this automated update pipeline, our proposed MINED benchmark can not only evaluate current state-of-the-art LMMs, but also be used to evaluate newly emerging and more powerful LMMs in the future. B.2 MINED DETAILED QUANTITY Table 8: The detailed quantity of time-sensitive knowledge for each task Cog. Awa. Tru. Und. Rea. Rob. T.A T.I.A T.S.A F.M.C P.M.C P.U.D F.U.D I.T.C R.K C.A A.T.E"
        },
        {
            "title": "Sum",
            "content": "255 172 237 236 181 207 255 81 81"
        },
        {
            "title": "C MORE EXPERIMENTAL RESULTS ABOUT MINED",
            "content": "C.1 MORE MAIN RESULTS ABOUT MINED In this section, we present the complete experimental results on MINED. To further validate the reliability of our conclusions, we also employed the F1-Score as an additional evaluation metric. The F1-Score is metric for assessing model performance by quantifying the word-level similarity between models output and the ground truth answer. It is the harmonic mean of Precision and Recall (Chan et al., 2024). To calculate it, we first represent both the ground truth and the prediction as sets of words. Let the ground truth be W(yq) = {y1, . . . , ym} and the models prediction be W( ˆY ) = {ˆy1, . . . , ˆyn}. The number of common words between these sets, known as the overlap U( ˆY , yq), is computed using an indicator function 1[]: U( ˆY , yq) = (cid:88) 1[t W( ˆY )] (2) tW(yq) Precision, P( ˆY , ), is the fraction of relevant words among the predicted words. It is formally defined as: Recall, R( ˆY , ), is the fraction of ground truth words that the model successfully identified. It is defined as: P( ˆY , ) = U( ˆY , yq) W( ˆY ) R( ˆY , ) = U( ˆY , yq) W(yq) (3) (4) Table 9: Complete F1-Score Performance Comparison (%) on MINED. The top two and worst results are highlighted in red (1st), yellow (2nd) and blue (bottom) backgrounds, respectively. Subscripts L, , and stand for LLaMA3-8B, Mistral-7B, Vicuna-7B and Instruct, respectively. (Release Time) Models Open-source LMMs (2023.04) LLaVA-v1.5 (7B) (2023.08) Qwen-VL (7B) (2023.11) mPLUG-Owl2 (7B) (2024.01) LLaVA-NextL. (8B) (2024.01) LLaVA-NextM. (7B) (2024.01) LLaVA-NextV. (7B) (2024.08) LLaVA-OV (7B) (2024.08) mPlug-Owl3 (8B) (2024.08) MiniCPM-V2.6 (8B) (2024.09) Qwen2-VLI. (7B) (2024.12) InternVL2.5 (1B) (2024.12) InternVL2.5 (2B) (2024.12) InternVL2.5 (4B) (2024.12) InternVL2.5 (8B) (2025.02) Qwen2.5-VLI. (3B) (2025.02) Qwen2.5-VLI. (7B) (2024.12) InternVL2.5 (26B) (2024.12) InternVL2.5 (38B) (2024.12) InternVL2.5 (78B) (2025.02) Qwen2.5-VLI. (72B) Closed-source LMMs (2025.02) Kimi-Latest (2025.03) Doubao-1.5-Vision-Pro (2025.03) Gemini-2.5-Pro (2025.04) GPT-4.1 (2025.08) Seed-1.6-Vision T.A 7.89 14.56 13.40 9.39 13.37 13.89 14.22 9.94 24.11 19.20 4.53 6.67 21.02 21.71 19.55 21.59 23.85 29.71 30.44 32.42 28.55 36.87 35.21 37.26 38.50 Cog. T.I.A 11.44 20.30 17.05 16.68 18.74 18.34 15.24 14.07 25.91 21.34 2.65 7.29 17.35 23.29 16.39 22.29 26.20 32.50 35.91 36.97 31.63 34.33 58.86 43.42 48.55 Awa. Tru. T.S.A F.M.C P.M.C P.U.D F.U.D Und. I.T.C Rea. Rob. R.K C.A A.T.E Avg. 53.99 80.00 11.19 99.64 96.74 81.16 39.61 97.60 81.52 99.52 97.95 96.74 98.43 98.31 40.10 99.64 9.49 8.81 44.21 38.20 32.05 22.54 34.84 20.86 34.63 14.71 3.06 4.98 31.36 42.64 14.61 38.83 Model size under 10B 10.60 7.66 48.26 47.51 37.34 27.60 35.12 21.87 41.37 21.92 3.48 5.96 34.06 47.38 15.20 45.77 Model size under 65B 54.07 68.91 Model size under 100B 74.59 75.32 81.16 91.67 73.79 73.56 52.18 62. 97.22 92.63 50.00 69.40 44.20 99.88 90.22 87.92 76.21 99.76 97.83 99.76 98.43 95.89 99.28 99.88 57.25 99.76 99.52 99.15 97.58 97.95 1.95 4.94 3.34 3.47 4.43 3.99 4.86 3.27 5.81 6.09 1.19 2.04 4.26 6.00 5.28 5.74 6.52 5. 7.75 7.78 73.19 78.39 86.37 82.47 79.85 71.16 74.61 86.67 82.02 83.59 72.10 93.12 75.50 64.44 74.15 85.27 100.00 93.77 91.30 96.86 8.45 6.21 17.39 10.11 9. 16.88 47.09 50.94 46.39 46.59 39.15 31.91 33.09 58.78 37.49 4.86 10.21 35.32 49.14 25.16 47.47 62.74 73.72 75.35 76.21 76.34 76.52 87.06 84.93 82.83 15.33 23.13 43.40 36.08 38.85 32.23 52.56 41.53 53.67 50.27 42.35 13.77 47.74 62.11 50.58 39.22 27.71 32. 12.80 11.91 46.48 19.71 39.72 16.77 22.00 6.38 18.96 16.59 10.85 24.23 15.25 14.73 7.62 27.74 18.40 3.85 5.27 22.07 24.52 16.46 28.35 25.33 32.82 43.09 38.07 47.12 38.63 81.21 62.03 62. 0.39 0.00 6.12 0.13 0.00 31.25 2.21 3.65 14.45 9.90 0.00 0.78 1.56 0.00 9.38 22.29 8.33 11.33 8.33 5.73 6.38 12.24 31.94 17.58 31.05 16.76 26.80 27.15 37.11 36.60 33.94 29.23 32.11 42.35 36.24 23.85 22.69 37.50 43.18 24.54 42.81 43.97 49. 49.16 49.78 49.70 51.88 63.07 53.85 57.20 According to the results in Table 9, we found that the conclusion drawn when using F1-Score as the evaluation metric is consistent with the conclusion drawn when using CEM as the evaluation metric, highlighting the reliability of our results and observations."
        },
        {
            "title": "Preprint",
            "content": "Table 10: Complete CEM Performance Comparison (%) on MINED. The top two and worst results are highlighted in red (1st), yellow (2nd) and blue (bottom) backgrounds, respectively. Subscripts L, , and stand for LLaMA3-8B, Mistral-7B, Vicuna-7B and Instruct, respectively. (Release Time) Models Open-source LMMs (2023.04) LLaVA-v1.5 (7B) (2023.08) Qwen-VL (7B) (2023.11) mPLUG-Owl2 (7B) (2024.01) LLaVA-NextL. (8B) (2024.01) LLaVA-NextM. (7B) (2024.01) LLaVA-NextV. (7B) (2024.08) LLaVA-OV (7B) (2024.08) mPlug-Owl3 (8B) (2024.08) MiniCPM-V2.6 (8B) (2024.09) Qwen2-VLI. (7B) (2024.12) InternVL2.5 (1B) (2024.12) InternVL2.5 (2B) (2024.12) InternVL2.5 (4B) (2024.12) InternVL2.5 (8B) (2025.02) Qwen2.5-VLI. (3B) (2025.02) Qwen2.5-VLI. (7B) (2024.12) InternVL2.5 (26B) (2024.12) InternVL2.5 (38B) (2024.12) InternVL2.5 (78B) (2025.02) Qwen2.5-VLI. (72B) Closed-source LMMs (2025.02) Kimi-Latest (2025.02) Doubao-1.5-Vision-Pro (2025.03) Gemini-2.5-Pro (2025.04) GPT-4.1 (2025.08) Seed-1.6-Vision T.A 6.96 12.45 10.59 8.24 10.69 11.47 11.86 9.80 22.16 15.98 6.96 5.59 18.63 20.49 17.65 18.33 21.96 28.43 29.31 29.22 26.41 35.78 34.25 37.58 37. Cog. T.I.A 9.25 17.30 14.53 12.21 14.53 14.83 11.34 10.03 21.66 16.72 3.49 5.52 13.66 18.46 13.66 16.86 21.37 27.47 28.63 31.10 26.60 27.91 56.40 37.94 41. Awa. Tru. T.S.A F.M.C P.M.C P.U.D F.U.D Und. I.T.C Rea. Rob. R.K C.A A.T.E Avg. 53.99 81.28 11.47 99.64 96.74 81.16 39.61 97.95 81.52 99.52 97.95 96.74 98.43 98.31 40.10 99.64 6.40 6.91 38.67 31.63 28.87 17.82 31.35 28.31 31.35 11.46 3.31 3.18 28.31 38.26 11.88 33.98 Model size under 10B 7.66 6.04 42.69 41.10 33.69 23.62 30.93 29.77 38.88 17.90 3.92 4.03 31.36 42.37 12.08 40.04 Model size under 65B 49.79 65.78 Model size under 100B 69.92 70.44 81.16 91. 70.86 69.34 49.72 59.81 97.22 92.63 50.00 70.17 44.20 99.88 90.22 87.92 76.21 99.76 97.83 99.76 98.43 95.89 99.28 99.88 57.25 99.76 99.52 99.15 97.58 97. 1.57 3.53 2.16 2.35 3.73 2.55 3.63 3.14 4.22 4.61 2.35 0.88 3.04 4.22 3.73 4.02 5.00 4.31 5.98 6.18 68.64 74.36 83.09 78.07 75.95 67.27 70.76 84.30 77.49 80.71 72.10 93.12 80.31 65.22 74. 85.39 100.00 97.10 91.30 96.86 7.06 5.29 18.73 8.63 7.55 16.88 42.09 44.62 39.03 41.14 34.39 26.79 29.01 55.70 31.96 7.28 9.07 32.91 44.83 21.41 41.67 59.39 70.15 70.25 71.41 72.43 69.83 84.96 80.91 78. 15.12 25.00 42.90 35.19 38.58 31.17 51.54 41.98 52.78 49.38 45.06 13.27 47.53 61.73 50.31 38.89 26.85 31.79 11.73 11.42 45.99 18.52 38.48 15.74 21.60 6.17 17.59 14.20 8.33 20.99 10.80 8.95 7.10 24.38 14.20 3.40 4.32 20.06 19.14 13.58 25.00 20.99 28. 38.58 34.88 42.59 34.57 76.54 59.57 59.57 0.39 0.00 6.12 0.13 0.00 31.25 2.21 3.65 14.45 9.90 0.00 0.78 1.56 0.00 9.38 16.86 8.33 11.33 8.33 5.73 6.38 12.24 39.58 17.58 32. 15.85 25.67 24.74 34.34 34.47 31.54 26.77 32.77 40.45 33.76 24.74 21.75 35.89 40.70 22.82 39.55 41.83 47.23 46.58 47.21 47.35 49.31 63.07 51.82 55.16 C.2 MORE MODEL SIZE RESULTS ABOUT MINED Figure 8: Analysis of impact of different model sizes about InternVL2.5 series."
        },
        {
            "title": "D EXPERIMENT RESOURCES ABOUT MINED",
            "content": "PROBING TIME-SENSITIVE KNOWLEDGE Regarding the validation experiments of LMMs on MINED, for models with parameter sizes of 38B or less, we conduct experiments on 4 NVIDIA A100 PCIEs machines (40 GiB each); For models with parameter sizes greater than 38B, we conduct experiments on 4 NVIDIA H100 (96 GiB each). EDITING TIME-SENSITIVE KNOWLEDGE We conduct knowledge editing experiment on one H100 (96 GiB each) regarding LMMs."
        },
        {
            "title": "E CASE STUDIES ABOUT MINED",
            "content": "Figure 9: Case study of Time-Agnostic. Figure 10: Case study of Timestamp-Aware."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Case study of Temporal Interval-Aware. Figure 12: Case study of Future Misaligned Context. Figure 13: Case study of Past Misaligned Context."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Case study of Past Unanswerable Date. Figure 15: Case study of Future Unanswerable Date. Figure 16: Case study of Implicit Temporal Concept."
        },
        {
            "title": "Preprint",
            "content": "Figure 17: Case study of Ranking. Figure 18: Case study of Calculation. Figure 19: Case study of Adversarial Temporal Error."
        },
        {
            "title": "Preprint",
            "content": "F UPDATING TIME-SENSITIVE KNOWLEDGE VIA KNOWLEDGE EDITING F.1 EDITING SETTING We conduct experiments on single editing and lifelong editing. In single editing, after performing an editing operation on each knowledge instance, we immediately evaluate the model and restore its weights to pre-editing states, thus ensuring evaluations measure the impact of individual edits. For lifelong editing, we first edit all knowledge instances in the dataset and then comprehensively evaluate the modified model. The complete workflow is shown in Figure 20 Figure 20: Analysis of impact of different model sizes and foundation LLM. F.2 KNOWLEDGE EDITING METHODS AND PARAMETERS We have provided detailed introduction to the multimodal knowledge editing method and specific parameters below. FT FT method optimizes selected model parameters via gradient descent. An AdamW optimizer is employed to restrict gradient computation and updates exclusively to target fine-tuning parameters. FT-LLM Models LLaVA-v1.5 (7B) Qwen-VL (7B) Steps 10 15 Edit Layer 31st layer of Transformer Module 31st layer of Transformer Module"
        },
        {
            "title": "Optimizer Edit LR\nAdamW\nAdamW",
            "content": "1e4 1e4 FT-VIS Models LLaVA-v1.5 (7B) Qwen-VL (7B) Steps 10 15 Edit Layer mm projector 47th layer of ViT Module"
        },
        {
            "title": "Optimizer Edit LR\nAdamW\nAdamW",
            "content": "1e4 1e"
        },
        {
            "title": "MEND",
            "content": "MEND enables targeted parameter adjustments in LLMs of VLMs through lightweight auxiliary networks. These networks apply localized modifications using single input-output pairs while preserving unrelated task performance. The method achieves computational efficiency by exploiting low-rank gradient decomposition to parameterize gradient transformations, scalable to billion-parameter models. Models LLaVA-v1.5 (7B) Qwen-VL (7B) MaxIter 40,000 40,000 Edit Layer layers 29, 30, 31 of Transformer Module layers 29, 30, 31 of Transformer Module"
        },
        {
            "title": "Optimizer\nAdam\nAdam",
            "content": "LR 1e6 1e"
        },
        {
            "title": "SERAC",
            "content": "SERAC integrates scope classifier and retrieval-augmented counterfactual model. The classifier determines input applicability to edited content, routing matched queries to the counterfactual model for memory-augmented generation, while others use the original model. Models LLaVA-v1.5 (7B) Qwen-VL (7B) MaxIter 50,000 20,000 Edit Layer all layers of OPT-125M 31st layer of Qwen-7B"
        },
        {
            "title": "Optimizer\nAdam\nAdam",
            "content": "LR 1e5 1e"
        },
        {
            "title": "IKE",
            "content": "IKE avoids parameter updates by retrieving analogous demonstrations from edited data and injecting knowledge through in-context learning. The method maintains consistency across models by formatting training data as structured prompts: New Fact: question answer Prompt: question answer, which are subsequently embedded for processing. For IKE, text embeddings and similarity-based retrieval are implemented via the all-MiniLM-L6-v2 sentence-transformers model, with the demonstration count fixed at 32 uniformly across models. F.3 EDITING QUANTITY Table 11: Detailed quantity of editing samples for each task. Cog. Tru. Und. Rea. Rob. T.A T.I.A T.S.A P.U.D F.U.D I.T.C R.K C.A A.T.E"
        },
        {
            "title": "Sum",
            "content": "LLaVA-v1.5 (7B) 241 163 220 145 153 161"
        },
        {
            "title": "255\nQwen-VL (7B)\n254",
            "content": "114 78 72 77 70 1504 192"
        },
        {
            "title": "G MORE DETAILS ABOUT CHAT TEMPLATES AND QUANTITATIVE EXAMPLES",
            "content": "Cognition 1: Time-Agnostic System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image, you should answer it using your own knowledge based on todays date. Remember, your answer must contain only the name, with no other words. Question: Which club does the {hypernym} in the image currently {property}? Generalization Question: The {hypernym} in the image currently {property} Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Question: Which club does the person in the image currently play for? Generalization Question: The person in the image currently plays for Cognition 2: Timestamp-Aware System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image, you should answer it using your own knowledge based on the timestamp. Remember, your answer must contain only the name, with no other words. Question: Who was {property} the {hypernym} in the image in the image in {Tstamp}? Generalization Question: In {Tstamp}, {property} the {hypernym} in the image was Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Question: Who was the CEO of the company in the image in 1982? Generalization Question: In 1982, the CEO of the company in the image was"
        },
        {
            "title": "Preprint",
            "content": "Cognition 3: Temporal Interval-Aware System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image, you should answer it using your own knowledge based on the temporal interval. Remember, your answer must contain only the name, with no other words. Question: Who was {property} the {hypernym} in the image from {Tstart} to {Tend}? Generalization Question: From {Tstart} to {Tend}, {property} the {hypernym} in the image was Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Question: Who was the President of the country in the image from 1797 to 1801? Generalization Question: From 1797 to 1801, the President of the country in the image was Awareness 1: Future Misaligned Context System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image and its relevant context, you should answer it using your own knowledge or the knowledge provided by the context. Remember, the provided context may not necessarily be up-to-date to answer the question, and your answer must contain only the name, with no other words. Context: {Future temporal misaligned context} Question: Who was {property} the {hypernym} in the image {Tstamp} Generalization Question: In {Tstamp}, {property} the {hypernym} in the image was Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Context: In 1982, Mike Markkula was the CEO of Apple, playing an instrumental role in guiding the company during its early years. As co-founder and early investor, Markkula helped shape Apples business strategy and oversaw key product developments. Question: Who was the CEO of the company in the image in 1979? Generalization Question: In 1979, the CEO of the company in the image was"
        },
        {
            "title": "Preprint",
            "content": "Awareness 2: Past Misaligned Context System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image and its relevant context, you should answer it using your own knowledge or the knowledge provided by the context. Remember, the provided context may not necessarily be up-to-date to answer the question, and your answer must contain only the name, with no other words. Context: {Past temporal misaligned context} Question: Who was {property} the {hypernym} in the image {Tstamp} Generalization Question: In {Tstamp}, {property} the {hypernym} in the image was Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Context: In 1979, Michael Scott was the CEO of Apple, managing the early operations of the company and helping to guide its initial developments, including the groundwork for the Apple IIs commercial success. Question: Who was the CEO of the company in the image in 1982? Generalization Question: In 1982, the CEO of the company in the image was Trustworthiness 1: Past Unanswerable Date System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image, you should answer it using your own knowledge. Remember, please output Unknown only if the answer does not exist. Otherwise, output the name only. Question: Who was {property} the {hypernym} in the image {TP ast nanswerable Date} Generalization Question: In {TP ast nanswerable Date}, {property} the {hypernym} in the image was Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Question: Who was the President of the country in the image in 1823? Generalization Question: In 1823, the President of the country in the image was"
        },
        {
            "title": "Preprint",
            "content": "Trustworthiness 2: Future Unanswerable Date System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image, you should answer it using your own knowledge. Remember, please output Unknown only if the answer does not exist. Otherwise, output the name only. Question: Who was {property} the {hypernym} in the image {TF uture nanswerable Date} Generalization Question: In {TF uture nanswerable Date}, {property} the {hypernym} in the image was Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Question: Who was the President of the country in the image in 2075? Generalization Question: In 2075, the President of the country in the image was Understanding: Implicit Temporal Concept System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image, you should answer the question using your knowledge and reasoning capacity. Remember, your answer must contain only the name, with no other words. Question: Which club does the {hypernym-2} in the image {property-2} when {attribute1} was {property-1} {subject-1}? Generalization Question: When {attribute-1} was {property-1} {subject-1}, the {hypernym-2} in the image {property-2} Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Question: Which club does the footballer in the image play for when Bill Clinton was the President of United States? Generalization Question: When Bill Clinton was the President of United States, the footballer in the image plays for"
        },
        {
            "title": "Preprint",
            "content": "Reasoning 1: Ranking System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image, you should answer the question using your knowledge and reasoning capacity. Remember, your answer must contain only the name, with no other words. Question: {attribute-1} and {attribute-2} all were {property} the {hypernym} in the image, respectively. Can you identify which one the former {property} was? Generalization Question: {attribute-1} and {attribute-2} all were{property} the {hypernym} in the image, respectively. Please identify the former {property} was Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Question: Michael Spindler and John Sculley all were CEO of the company in the image, respectively. Can you identify which one the former CEO of was? Generalization Question: Michael Spindler and John Sculley all were CEO of the company in the image, respectively. Please identify the former CEO of was Reasoning 2: Calculation System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image, you should answer the question using your knowledge and reasoning capacity. Remember, your answer must contain only the name, with no other words. Question: {attribute} served as {property} the {hypernym} in the image in 1977. Can you identify who occupied this position after {TY ear} years? Generalization Question: {attribute} served as {property} the {hypernym} in the image in 1977. Please identify the person occupied this position after {TY ear} years? years was Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Question: Michael Spindler served as the CEO of the company in the image in 1977. Can you identify who occupied this position after 34 years? Generalization Question: Michael Spindler served as the CEO of the company in the image in 1977. Please identify the person occupied this position after 34 years was"
        },
        {
            "title": "Preprint",
            "content": "Robustness: Adversarial Temporal Error System Prompt: You are knowledgeable assistant who can answer factual questions. User Prompt: Given question and image, you should answer the question using your knowledge and reasoning capacity. Given question and image, you should answer it using your own knowledge. Remember, your answer must contain only Yes or No. Question: Your answer to the original question is wrong. Was {attribute} {property} the {hypernym} in the image from {Tstart} to {Tend}? Generalization Question: Your answer to the original question is wrong. Did {attribute} {property} the {hypernym} in the image from {Tstart} to {Tend}? Your answer: Quantitative Example:"
        },
        {
            "title": "Generalization Image",
            "content": "Question: Your answer to the original question is wrong. Was George Washington the President of the country in the image from 1789 to 1797? Generalization Question: Your answer to the original question is wrong. Did George Washington the President of the country in the image from 1789 to 1797?"
        }
    ],
    "affiliations": [
        "Anhui Polytechnic University",
        "Beijing Institute of Technology",
        "Ludwig Maximilian University of Munich",
        "Northeast Forestry University",
        "Shandong University",
        "University of Science and Technology of China",
        "University of Sydney",
        "Xiamen University"
    ]
}