{
    "paper_title": "InductionBench: LLMs Fail in the Simplest Complexity Class",
    "authors": [
        "Wenyue Hua",
        "Tyler Wong",
        "Sun Fei",
        "Liangming Pan",
        "Adam Jardine",
        "William Yang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 3 2 8 5 1 . 2 0 5 2 : r InductionBench: LLMs Fail in the Simplest Complexity Class Wenyue Hua1 Tyler Wong1 Sun Fei2 Liangming Pan3 Adam Jardine4 William Yang Wang1 1University of California, Santa Barbara, 2Independent Researcher 3University of Arizona, 4Rutgers University, New Brunswick February 25,"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting notable deficiency in current LLMs inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/ inductive_reasoning_benchmark."
        },
        {
            "title": "INTRODUCTION",
            "content": "The remarkable progress of large language models (LLMs) in recent years has yielded substantial improvements in their reasoning capabilities. This progress is most evident in benchmarks involving complex mathematics Cobbe et al. (2021); Hendrycks et al. (2021) and coding tasks Jain et al. (2024); Jimenez et al. (2023); Chen et al. (2021); Fan et al. (2023). Beyond these domains, researchers have also explored the logical reasoning abilities of LLMs from various angles, including propositional logic Zhu et al. (2023), first-order logic Han et al. (2022); Parmar et al. (2024), and propositional logic under different contexts Hua et al. (2024). Despite significant progress in model capabilities, existing benchmarks predominantly focus on deductive reasoning, largely overlooking inductive reasoning. The former requires applying explicitly defined premises to derive valid conclusions, whereas the latter requires inferring the underlying principles, rules, or patterns from observations Hawthorne (2004). Both forms of reasoning are essential; inductive reasoning, in particular, is critical in domains such as scientific discovery where researchers seek to characterize natural laws based on empirical data Grünwald (2007); Hansen & Yu (2001) that captures complex phenomena. Figure 1 illustrates the differences between inductive and deductive reasoning. Corresponding authors: wenyuehua@ucsb.edu, william@cs.ucsb.edu. Im very grateful for extensive discussion with Wenda Xu, Xinyi Wang at UCSB. 1 In this paper, we address this gap by introducing InductionBench, rigorous benchmark designed to assess LLMs inductive reasoning abilities by testing whether they can infer stringto-string transformation from finite set of inputoutput pairs. model must hypothesize the underlying relationship between inputs and outputs based on finite set of examples and then extrapolate those rules to unseen strings. The process of discovering the underlying function from limited data reflects the core principles of inductive reasoning. Our benchmark is grounded in the subregular hierarchy Rogers & Pullum (2011); Truthe (2018); Graf (2022); Jäger & Rogers (2012); Heinz (2018) (see Figure 2) for string-to-string mappings Mohri (1997), focusing on input transformations restricted to regular functions. By systematically increasing task complexity across multiple classes in the subregular hierarchy, we gain detailed insights into how effectively LLMs detect, hypothesize, and generalize underlying rules from theoretically sufficient datapoints. Figure 1: Deductive vs. Inductive Reasoning We evaluate multiple state-of-the-art LLMs to understand LLMs inductive reasoning ability and identify factors that increase the difficulty of inductive reasoning tasks for LLMs, such as the length of the minimum-length description, the number of datapoints, and in-context examples. Through extensive experiments, we find that even advanced models such as o3-mini struggle with basic inductive tasks, highlighting significant shortcoming in the current generation of LLMs. More detailed findings are presented in Section 5."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Deductive Reasoning. One major branch of reasoning benchmarks centers on deductive inference, where models apply established premises to derive specific conclusions. Notable examples include ReClor Yu et al. (2020), which evaluates the ability to solve logical reasoning questions resembling those found in standardized tests, and various logic-based benchmarks of increasing complexity, from propositional logic to first-order logic Han et al. (2022); Parmar et al. (2024); Zhu et al. (2023); Hua et al. (2024). These tasks typically require handling structured logical relationships with minimal ambiguity in how premises lead to conclusions. Regular Functions Left Subsequential Right Subsequential ISL Left OSL Right OSL Figure 2: Subregular hierarchy in string-to-string maps Another type of reasoning benchmarks is mathematical problem solving, including elementary arithmetic to advanced competition-level questions. Hendrycks et al. (2021) test both computational skills and the sequential reasoning steps involved in mathematics. Cobbe et al. (2021) covers broad spectrum of topics, including geometry and higher-level problem solving. However, most standard mathematics problem-solving tasks can be framed as deductive reasoning, as they involve applying established axioms, definitions, and theorems in logically valid sequence to derive conclusion. Inductive Reasoning. Despite the diversity of existing benchmarks, inductive reasoning, where models hypothesize and generalize patterns from examples without pre-specified rules, remains comparatively underexplored. Current evaluations of inductive skills have largely been limited to small-scale symbolic regression, artificial language translation, and concept learning Liu et al. 2 (2024b); Lake et al. (2019); Qiu et al. (2023), which, although important in real-world scenarios, often lack three key elements: (1) an explicit analysis of the inherent difficulty of the task (2) guarantee that the provided inputoutput dataset can identify the target function (3) mechanism to evaluate whether models can identify the best possible hypothesis under Occams Razor Blumer et al. (1987); Baker (2007) principle, i.e., description with minimal length Hansen & Yu (2001); Grünwald (2007). Our Contribution. To address these shortcomings, we introduce new benchmark targeting on inductive reasoning skills. Building on subregular hierarchy and corresponding polynomial time and data learnability guarantees, our benchmark, InductionBench, tests how effectively LLMs infer underlying transformation functions from finite datapoints. We also measure the degree to which models produce minimal, non-redundant hypotheses, providing lens into their ability of generalization. Through fine-grained, gradually increasing level of complexity, our evaluations reveal how current LLMs cope with the growing search space. There are several advantages of our benchmark: 1. Automated Evaluation: Because the data is derived from well-defined functions, one can directly compare the models output with the known ground-truth function, eliminating the need for expensive human annotations. 2. Dynamic Data Generation: The dataset is produced randomly based on specific function classes, allowing periodic refreshes to prevent models from relying on memorized examples. 3. Rigorous Assessment of Hypothesis Space: As the function is well-defined, one can control the size of the hypothesis space with precision. This control enables rigorous and systematic evaluation of LLM performance from theoretically grounded perspective."
        },
        {
            "title": "3 COMPUTATIONAL COMPLEXITY IN INDUCTIVE REASONING",
            "content": "InductionBench uses string-to-string transformation/functions as proxy to study inductive reasoning, which has established computational complexity hierarchy(Roche & Schabes, 1997; Engelfriet & Hoogeboom, 2001). We focus on the subregular hierarchy, the hierarchy under regular functions. Though with limited expressive power, our experiments show that these classes already present substantial challenges for LLMs. Specifically, we limit our attention to three classes of deterministic regular functionsLeft OutputStrictly-Local (L-OSL), Right Output-Strictly-Local (R-OSL), and Input-Strictly-Local (ISL), whose positions in the subregular hierarchy are illustrated in Figure 2 Heinz (2018). These classes represent the lowest-complexity tier for string-to-string mappings within the subregular hierarchy. They are proper subclasses of subsequential function class and, more broadly, of weakly-deterministic class and non-deterministic class, which are themselves subsets of the regular function classes. Although we do not elaborate on the complete regular function hierarchy here, it is important to note that the ISL, L-OSL, and R-OSL classes are among the simplest in this framework. Strictly local functions can be seen as operating with fixed amount of look-ahead, similar to Markov processes. They are provably learnable in polynomial time from polynomially sized samples Chandlee et al. (2014); De La Higuera (1997); Chandlee et al. (2015); Jardine et al. (2014). Moreover, prior work has shown that an algorithm exists to learn the unique (up to isomorphism) smallest subsequential finite-state transducer that represents such ISL, L-OSL, R-OSL functions Satta & Henderson (1997); Arasu et al. (2009). This property allows us to evaluate not only whether LLMs can discover the correct patterns but also whether they can identify the simplest or most concise representation consistent with the data."
        },
        {
            "title": "3.1 PRELIMINARY",
            "content": "Before providing the definitions of the three function classes, we first introduce the fundamental mathematical notations and formal definitions underpinning our discussion of string-to-string transformations and their properties. Let Σ be finite alphabet. We denote by Σ the set of all finite strings over Σ, and by Σn the set of all strings over Σ of length at most n. The empty string is denoted as λ. The set of prefixes of 3 string is denoted as PREF(w), defined as {p Σ Σs.t.w = ps}, and the set of suffixes of denoted as SUFF(w), defined as {s Σ Σs.t.w = ps}. The longest common prefix of set of strings is denoted as LCP(S), defined as wS PREF(w) such as wS PREF(w), < p. For any function : Σ Γ and Σ, let the tails of with respect to be defined as TAILSf (w) = {(y, v) (wy) = uv and = LCP(f (wΣ))}. (1) (2) Intuitively, TAILSf (w) collects all possible continuations (y, v) by appending to w. It summarizes how might extend beyond the partial input w. The total number of distinct tails across all strings in Σ provides measure of how many different non-trivial local transformation encodes."
        },
        {
            "title": "3.2 FUNCTION CLASS DEFINITION",
            "content": "Based on the concepts outlined above, we define the three function classes. Definition 1 (ISL) function is ISL if there is such that for all u1, u2 Σ, if SUFF SUFF k1(u2), then TAILSf (u1) = TAILSf (u2). k1(u1) = In simpler terms, this means that the output at each position in the string depends only on the preceding 1 characters of the input, making the transformation Markovian with respect to the input. Below is simple example: Example 3.1 Suppose function : {a, b} {a, b} rewrites each to only if it appears after the input substring ba. In this scenario, we have = 3, and there are two distinct tails: TAILSf (w) = {(λ, λ), (b, a), (bb, ab), (ab, ab) . . . }, Σ such that ba SUFF(w) and TAILSf (w) = {(λ, λ), (a, a), (bb, bb), (ab, ab) . . . }, Σ such that ba / SUFF(w) These tails indicate how the functions behavior shifts depending on whether the immediate context ends in ba. Such context-dependent tails also highlights that ISL functions can be effectively characterized or represented by local input constraints. Definition 2 (L-OSL) function is L-OSL if there is such that for all u1, u2 Σ, if k1(f (u2)), then TAILSf (u1) = TAILSf (u2). SUFF k1(f (u1)) = SUFF In other words, the output at each position in the transformed string depends only on the preceding 1 characters of the output itself, rather than on the input. This property can be understood as form of Markovian process on the output. Below is simple example: Example 3.2 Suppose function rewrites each to λ only if it appears after the output substring ba. In this scenario, we have = 3, and there are two distinct tails: TAILSf (w) = {(λ, λ), (a, a), (b, λ), (bb, λ), (ab, ab), (ba, a), . . . } Σ such that ba SUFF(f (w)) and TAILSf (w) = {(λ, λ), (a, a), (b, b), (bb, bb), (ab, ab), (ba, ba) . . . } Σ such that ba / SUFF(f (w)) While L-OSL depends preceding output symbols to the left, R-OSL functions depends on limited number of future output symbols to the right. Conceptually, one can view R-OSL as analogous to L-OSL, except that the input is processed in reverse order. Although both belong to the broader OSL paradigm, they are incomparable classes: each can express transformations the other cannot. The formal definition of R-OSL follows: 4 Definition 3 (R-OSL) function is R-OSL if there is such that for all u1, u2 Σ, if SUFF 2 )), then TAILSf (u1 1 ) = TAILSf (u1 2 ). 1 )) = SUFF k1(f (u1 k1(f (u1 Intuitively, this class of functions can be viewed as rightward Markovian process on the output. Each output symbol is determined not by the preceding symbols as in L-OSL but by the next 1 symbols that will appear in the output. The three classes, ISL, L-OSL, and R-OSL, are each deterministic and exhibit Markovian behavior, yet remain pairwise incomparable within the broader subregular hierarchy. In this work, we further restrict our attention to functions that involve substitution which replaces one character with another and deletion which maps character to the empty string λ."
        },
        {
            "title": "3.3 LEARNABILITY",
            "content": "The three function classes are identifiable in polynomial time using polynomially sized characteristic sample Chandlee et al. (2014; 2015). In other words, there exists polynomial-time algorithm that, given sufficient data for target function , can produce representation τ that satisfies (w) = τ (w) for every Σ. In other words, once sufficient data is presented, one can reliably recover function equivalent to on all possible inputs. This learnability property underpins the value of these classes as testbeds for inductive reasoning, since the data requirement remains polynomial and successful inference is theoretically guaranteed. We formalize sufficient data as the minimal set of inputoutput pairs needed to learn k-strictly local function , which is known as characteristic sample. Adapting the original definition1 for clarity Chandlee et al. (2014; 2015), we define: Definition 4 (Characteristic Sample) For given k-ISL , the characteristic sample is defined as {(w, w) Σk (w) = w}. For given k-OSL , the characteristic sample is defined as {(w, w) Σk (w) = w}. If provided dataset contains such characteristic sample, learning algorithm can reconstruct representation of that matches its behavior on every string in Σ. Accordingly, in the context of LLMs, we expect that providing this dataset as in-context examples should enable the model to induce the underlying string-to-string mapping."
        },
        {
            "title": "3.4 UNIQUE FUNCTION REPRESENTATION",
            "content": "Beyond verifying that model can accurately discover function from data, we also investigate how succinctly the model describes its inferred rules. This aspect is of both theoretical and practical interest: minimal or most concise representation not only offers interpretability advantages but can also reflect the models capacity for truly generalizable, rather than merely enumerative, learning. One function can be represented or written in non-unique way. For instance, consider an ISL function f1 with = 2 over Σ = {a, b} that maps the input character to when it comes after b, that rewrites each to only if the preceding character is b, while leaving other substrings unchanged. One concise description is: f1(w)= f1(w1)ba1f1(aw2), if w1 ends with and w=w1 aw2 for some w1, w2Σ (3) w, otherwise An alternative yet more verbose description of the same function might redundantly enumerate multiple cases: 1(w)= 1(w1)ba1f 1(aw2), if w1 ends with ab and = w1aw2 for some w1, w2 Σ 1(w1)ba1f 1(aw2) if w1 ends with bb and = w1aw2 for some w1, w2 Σ w, otherwise (4) 1simplified from original definition 5 Although these two representations encode the same function, the second contains repetitive conditions and fails to emphasize that the output of f1 depends solely on the single preceding character instead of the penultimate character. Because these functions admit unique minimal representation (up to isomorphism) Chandlee et al. (2014); Oncina & Garcia (1991), we can directly compare the function produced by an LLM to the ground-truth minimal form. In doing so, we evaluate whether the model not only discovers the correct transformation but also simplifies it to the most parsimonious description possiblean essential indicator of robust inductive reasoning."
        },
        {
            "title": "3.5 RULE-BASED REPRESENTATION",
            "content": "To streamline the generation and parsing of function representations, we employ simplified notation wherein each transformation is written as condition target character output of the target character Bird & Ellison (1994). In this notation, the condition represents the minimal substring needed to trigger transformation, while any input substring not matching this condition remains unchanged. For instance, in the earlier example, this approach permits concise notation b, indicating that the input is mapped to when it comes after b; otherwise, the input string remains unaltered. This concise, rule-based format simplifies both the models output generation (by reducing complex functional descriptions) and our subsequent evaluation, as the applicable transformations can be easily parsible and verified. To demonstrate the simplicity of rule-based representation: given an ISL function f2 with = 2, the input becomes when it comes after and two consecutive as will be reduced to one single a. The minimal function representation is as below: f2(w)= f2(w1)ba1f2(aw2), if w1 ends with and w=w1aw2 for some w1, w2Σ f2(w1)a1f2(aw2), if w1 ends with and w=w1aw2 for some w1, w2Σ w, otherwise (5) In the simplified rule-based format, it can be written as:a λ, b. In summary, f1 can be minimally expressed with single rule, f2 requires two rules."
        },
        {
            "title": "4 BENCHMARK CONSTRUCTION",
            "content": "In this section, we detail how our benchmark is constructed from the previously defined function classes. Each datapoint (D, ) in the benchmark is pair of dataset and function where is set of input-output pairs generated by . Each of ISL, L-OSL, and R-OSL classes can be further subdivided into incremental levels of complexity, determined by three key parameters: (1) the context window size (2) the vocabulary size Σ (3) the minimal representation length of the function, i.e. the minimal set of rules corresponding to the function. Given and Σ, the search space is 2Σk ; given the number of rules additionally, the search space is (cid:0)Σk (cid:1). To rigorously evaluate LLMs inductive capabilities, we systematically vary these parameters across ISL, L-OSL, and R-OSL function classes. In addition, we examine how performance changes with different numbers of inputoutput pairs in the prompt. Although having the characteristic sample present should theoretically guarantee recoverability of the underlying function, our empirical results indicate that the overall number of examples strongly affects performance. While extra data can provide richer information, it also increases context length considerably and heightens processing demands Li et al. (2024). By varying the number of provided datapoints, we further investigate the extent to which the model engages in genuine reasoning and how robust its inductive abilities remain under changing input sizes. Function Generation To systematically create benchmark instances, we first randomly generate functions based on the three parameters: k, Σ, and the number of minimal rules describing by generating the set of rules that can describe . While multiple representations of varying length can describe the same function, each function has unique minimal representation (up to isomorphism). During function generation, we therefore ensure that each function is expressed by minimal, nonredundant rule set. Formally, if is represented by set of rules Rf = {r1, r2, ..., rn} where each ri has the form of ci ui vi (with ci as the condition substring, ui the target character, and vi the transformed output for ui), there are several constraints may be applied to functions belonging to the three classes. Definition 5 (General Consistency) Given represented by set of rules Rf : ri, rj Rf , ci ui / SUFF(cj uj) and cj uj / SUFF(ci ui). General Consistency ensures that the rules do not contradict one another or become redundant when conditions overlap. For instance, function whose rule-based representation of r1 : and r2 : aa is redundant, as the scenarios where r1 is applied is superset of the scenarios where r2 is applied. For another instance, there does not exist deterministic function that can be described by r1 : and r2 : aa λ. Generating rule-based representations for ISL functions needs only satisfy this constraint. Definition 6 (OSL Non-Redundancy Guarantee) Given represented by set of rules : ri Rf , = cj uj, unless rk such that ck vk = i. {sisici} such that rj Rf such that Constraint 2 is specific to the two OSL function classes because we need to make sure that all output conditions in the rule actually surface somewhere in the outputs of some datapoints. If the output condition never actually surface as the output, the rule will never be put into effect. Thereby the above rule basically requires that condition part of all rules can surface, either because it will never be modified by some other rule, or it emerges on the surface because of the application of other rule. For instance, function represented by rules r1 : aa a, r2 : is redundant because r1 will never be applied because the string aa will never surface as output and thus it will never be put into effect; For another instance, function represented by r1 : aa a, r2 : c, r3 : is non redundant because even though into aa string will be modified into ac, but aa will surface in some datapoint because ad will be modified into aa and thus r1 will be able to be applied. Generating the functions following the two constraints, we ensure that the generated function representation is minimal, non-reducible guarantees clear measure of complexity. One additional requirement is imposed to ensure each function indeed requires look-ahead of size k. Specifically: Definition 7 (k-Complexity Guarantee) Given whose designated context window = k1, such that u such that u = k1. This condition guarantees that the function is genuinely k-strictly local (for ISL or OSL), rather than being representable with smaller window size. Consequently, the functions we generate faithfully reflect the intended complexity level. After generating the function , we generate the characteristic sample of input-output pairs. For instance, given function with = 2 and Σ = {a, b}, the characteristic sample is {(a, (a)), (b, (b)), (ab, (ab)), (aa, (aa)), (bb, (bb)), (ba, (ba))}, small set whose size is 6. By expanding this sample set, we can explore whether providing more than the minimal necessary examples aids or hinders the models performance to infer the underlying function. To evaluate how effectively an LLM can induce the underlying function, we include in the prompt (1) the function class, (2) context window k, (3) the alphabet Σ which are information that guarantee learnability of the function. Then given the sample dataset, we request LLMs to produce minimal rule-based description that reproduces the provided sample set, revealing whether it can discover and optimally represent the underlying transformation."
        },
        {
            "title": "5 MAIN EXPERIMENT",
            "content": "Experiment Setting We evaluate using zero-shot chain-of-thought prompting on six SOTA LLMs, including Llama-3.3-70b Dubey et al. (2024) with FP8 quantization, Llama-3.1-405b with FP8 quantization, GPT-4o Hurst et al. (2024), DeepSeek-V3 Liu et al. (2024a), o1-miniJaech et al. (2024), 7 and o3-mini. For all models, we evaluate with all settings including {2, 3, 4}, Σ {2, 3, 4}, number of rules {2, 3, 4}, and sample set size to be 1, 2, 3, 4 times larger than the characteristic sample. For each setting, we randomly generate 10 functions and corresponding input-output sample to calculate the result. As o1-mini and o3-mini perform much better than other models, in addition, we evaluate on two more complex settings with {4, 5}, Σ = 5. Evaluation Metrics For each experiment setting, we leverage three metrics to evaluate performance: Precision, Recall, Compatibility. Let be the unique ground-truth rule set of minimal length for function , be the predicted rule set generated by LLM, be the provided sample set in the context on which we evaluate the correctness of . Precision measures how many of the predicted rules are correct relative to all rules the model generated: RP . captures the proportion of the models rules that align exactly with the groundP truth rules. higher precision indicates fewer unnecessary/redundant rules. Recall measures what fraction of the ground-truth rules the model successfully recovered: RP higher recall reflects the models ability to cover all aspects of the correct transformation. Compatibility measures whether applying the predicted rule set to each input in the sample set yields the correct output: . Compatibility(P, D) = (cid:26)1 0 if (xi, yi) D, (xi) = yi otherwise Compatibility is the most fundamental measure, as it verifies whether the generated function accurately reproduces all observed inputoutput pairs in D. trivial way to achieve perfect compatibility is to include every pair (xi, yi[1]) as an independent rule; however, doing so results in very low precision, indicating failure to capture the underlying generalizable structure of the function. Note that all results presented are expressed as percentages."
        },
        {
            "title": "ISL",
            "content": "L-OSL R-OSL recall precision compatibility recall precision compatibility recall precision compatibility Llama-3.3 70B 10.00 Llama-3.1 405B 10.00 10.00 GPT-4o 13.33 DeepSeek-V3 36.67 o1-mini 73.33 o3-mini 5.32 3.75 2.67 2.46 22.09 59.58 0.00 0.00 0.00 0.00 0.00 10.00 10.00 6.67 13.33 23.33 43.33 66.67 6.33 1.10 3.82 2.73 32.12 69.17 0.00 0.00 0.00 0.00 0.00 10.00 10.00 13.33 16.67 3.33 26.67 63. 10.83 1.85 6.73 0.25 17.58 62.00 0.00 0.00 0.00 0.00 0.00 30.00 Table 1: Zero-shot CoT benchmark result with = 4, Σ = 4, number of rules = 3, sample size = 2 Table 1 showcases model performance under particularly challenging conditions: = 4, Σ = 4 with 3 rules and sample size twice that of the minimal size of characteristic sample (detailed analysis on the impact of sample size is presented in Section 5.2). As seen in the table, compatibility scores collapse to 0 for all models except o3-mini, which also achieves relatively modest compatibility overall. This pattern highlights the difficulty that current LLMs face when required to track slightly broader contexts window even under very limited vocabulary size = 4. Full results are presented in Tables 3, 4, 5 in Appendix. Table 2 further reports the performance of o1-mini and o3-mini under slightly more challenging settings. Although both models generally exhibit non-trivial recall and precision, their compatibility scores consistently remain at or near zero. It is important to note that ISL, L-OSL, and R-OSL are the simplest function classes within the subregular hierarchy of string-to-string mappings. Thus, despite the strong performance of state-of-the-art models on benchmarks in coding Jain et al. (2024), mathematics Mirzadeh et al. (2024), and knowledge-intensive tasks Wang et al. (2024), they falter on this elementary inductive reasoning task. 5."
        },
        {
            "title": "IMPACT OF DIFFERENT FACTORS",
            "content": "Figure 3 shows how five models (Llama3.3-70B, Llama3.1-405B, GPT-4o, DeepSeek-V3, and o1mini) perform on various ISL tasks, organized by three key parameters: the context window size 8 = 5 P C"
        },
        {
            "title": "Settings",
            "content": "o1-mini o3-mini o1-mini o3-mini o1-mini o3-mini rules = 4 rules = 5 rules = 4 rules = 5 rules = 4 rules = 5 rules = 4 rules = 5 rules = 4 rules = 5 rules = 4 rules = 5 25.00 36.00 37.50 42.00 32.50 28.00 57.50 48. 17.50 16.00 45.00 38.00 = 4 P"
        },
        {
            "title": "ISL",
            "content": "12.21 40.41 49.83 58.67 L-OSL 37.34 23.17 58.93 71.38 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 R-OSL 22.33 15.42 43.76 55.17 0.00 0.00 10.00 0.00 10.00 10.00 27.50 20.00 15.00 8.00 22.50 10.00 12.50 18.00 20.00 14.00 9.10 3.14 30.75 38. 29.33 4.61 39.26 23.67 7.63 22.82 50.00 36.17 0.00 0.00 0.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Table 2: o1-mini and o3-mini results (R = Recall, = Precision, = Compatibility) on harder setting with {4, 5}, Σ = 5, sample size = k, the vocabulary size Σ, and the minimal number of rules required to describe the function. The top row of panels presents recall, the middle row presents precision, and the bottom row presents compatibility. We did not include o3-mini here because its performance is way stronger than all other five models and thereby in order to show the impact of various factors, we omit this model for better visual clarity. Based on these figures, the impact of k, Σ, and number of rules become very clear: (1) increasing markedly reduces recall, precision, and compatibility (2) Σ does not impact the performance much (3) the number of minimal rules can substantially affect compatibility with large and Σ. Impact of k. Across all models, moving from = 2 to = 4 markedly reduces recall, precision, and compatibility. This trend underscores how increasing the context window increases the complexity of the underlying ISL functions and making it more challenging for current LLMs to learn the correct transformations. Longer look-ahead requires the model to track additional input context, which can overload its capacity to induce reliable rules. In contrast, enlarging the vocabulary from Σ = 2 to Σ = 4 does not consistently Impact of Σ. degrade performance to the same degree as increasing k. While some models exhibit slight declines in recall or precision with larger alphabet, these effects are neither as uniform nor as pronounced as those induced by bigger Markov window. This finding suggests that the breadth of symbol variation matters less than the depth of sequential dependencies. Impact of the Number of Rules. Notably, the number of minimal rules can substantially affect compatibility. When = 2 and Σ = 2, comparatively small search space, changing the number of rules does not drastically alter compatibility. However, under more demanding scenarios where {3, 4}, the data indicate that adding rules can cause compatibility to plummet. In many cases, having just one rule still yields nontrivial compatibility, whereas introducing second or third rule often overwhelms the models, resulting in compatibility scores near 0. Impact of Examples We further examined whether few-shot prompting could enhance model performance. In our experiments with Llama-3.3 70B, we varied the number of in-context examples (1-shot, 2-shot, and 3-shot) to determine their effect on the models ability to induce the correct function representation. The results indicate that when both the vocabulary size and the context window are small, adding more examples improves performance across the evaluated metrics. However, as the complexity increaseswith larger values of and vocabulary sizesthe benefits of Llama-3.3 70B Llama-3.1 405B DeepSeek-V3 GPT-4o o1-mini Vocab size = 2 100 c 80 60 40 20 100 c 80 60 40 20 Vocab size = 3 Vocab size = 100 c 80 60 40 0 rules=1 rules=3 rules=2 k=2 rules=1 rules=3 rules=2 k=3 Vocab size = rules=3 rules=1 rules=2 k=4 0 rules=1 rules=3 rules=2 k= rules=1 rules=3 rules=2 k=3 Vocab size = 3 rules=3 rules=1 rules=2 k= 0 rules=1 rules=3 rules=2 k=2 rules=1 rules=3 rules=2 k=3 Vocab size = rules=3 rules=1 rules=2 k=4 100 80 40 20 s r 100 80 40 20 l t o 100 80 40 20 s r 100 80 40 20 l t o 100 80 40 20 s r 100 80 40 20 l t o 0 rules=1 rules=3 rules=2 k= rules=1 rules=3 rules=2 k=3 Vocab size = 2 rules=3 rules=1 rules=2 k= 0 rules=1 rules=3 rules=2 k=2 rules=1 rules=3 rules=2 k=3 Vocab size = rules=3 rules=1 rules=2 k=4 0 rules=1 rules=3 rules=2 k= rules=1 rules=3 rules=2 k=3 Vocab size = 4 rules=3 rules=1 rules=2 k= 0 rules=1 rules=2 k=2 rules=3 rules=1 rules=3 rules= rules=2 k=3 rules=3 rules=2 k=4 0 rules=1 rules=2 k=2 rules= rules=1 rules=3 rules=1 rules=2 k=3 rules=3 rules=2 k= 0 rules=1 rules=2 k=2 rules=3 rules=1 rules=3 rules= rules=2 k=3 rules=3 rules=2 k=4 Figure 3: ISL results for five models: Llama3.3-70b, Llama3.1-405b, GPT-4o, DeepSeek-V3, o1-mini additional few-shot examples become negligible. These findings suggest that while few-shot learning is beneficial for simpler settings, its efficacy diminishes in more complex inductive tasks. Experiment results are presented in Tables 6, 7, and 8. ) % ( l t o a A 1 0.8 0.6 0. 0.2 0 ISL k=2 k=3 k=4 0-shot 1-shot 2-shot 3-shot 1 0.8 0.6 0. 0.2 0 L-OSL k=2 k=3 k=4 0-shot 1-shot 2-shot 3-shot 1 0.8 0.6 0. 0.2 0 R-OSL k=2 k=3 k=4 0-shot 1-shot 2-shot 3-shot Number of Shots Number of Shots Number of Shots Figure 4: Impact of Few-shot Examples under different k. Robustness We assess the stability of inductive reasoning by varying the number of inputoutput pairs provided to the model. The x-axis represent S where is the minimal set of examples needed to guarantee learnability of the underlying function. The hypothesis is that if the model were performing genuine logical or inductive reasoning, we would expect performance to remain stable or even improve as more data points become available, since these points should further clarify the underlying function. Figure 5 illustrates how average compatibility decreases steeply as the number of provided inputoutput examples increases. This drop suggests that the LLMs reasoning process is not robustly inductive: rather than refining its hypothesis with additional data, the model appears to become confused or overwhelmed, leading to poorer overall performance. Consequently, these findings highlight the limited robustness of current LLMs inductive reasoning, particularly in scenarios where increasing the available data should theoretically facilitate, rather than hinder, function inference. Moreover, to isolate the influence of context length from the effect of adding genuinely new data, we conduct an additional experiment in which we simply extending the context size by repeating the 10 ) % ( i i m g v ) % ( l t o a A 0. 0.6 0.4 0.2 0 Impact of Sample Size with Different rules Impact of Sample Size with Different 1 0.8 0.6 0.4 0.2 rules = 1 rules = 2 rules = 3 1 2 3 4 1 0.8 0.6 0.4 0.2 k=2 k=3 k=4 1 2 3 4 0.8 0.6 0.4 0.2 0 Impact of Sample Size with Different Vocab Size vocab size=2 vocab size=3 vocab size=4 Sample Size Multiples Sample Size Multiples Sample Size Multiples Figure 5: Impact of Sample Size with Different Sample Size Multiples. Impact of Context Length with Different rules Impact of Context Length with Different Impact of Context Length with Different Vocab Size 1 2 3 5 rules = 1 rules = 2 rules = 3 1 0.8 0.6 0. 0.2 0 k=2 k=3 k=4 1 2 4 5 1 0.8 0.6 0. 0.2 0 1 vocab size=2 vocab size=3 vocab size=4 2 3 Sample Size Multiples 4 5 1 2 3 5 Sample Size Multiples Sample Size Multiples Figure 6: Impact of Context Length with Different Sample Size Multiples. minimal characteristic sample without introducing novel inputoutput pairs. Comparing Figures 5 and 6 reveals that while compatibility does diminish with increased context length (e.g., at multiple of 2), the decline is relatively small when scaling further to multiples of 3, 4, or 5. By contrast, when truly new datapoints are added (and not just repeated), compatibility plummets nearly to zero for multiples of 4 and 5. These results confirm that the primary driver of performance degradation is the inclusion of additional, distinct datapoints rather than simply lengthening the context. Error Type Analysis We further examined the specific types of errors made by LLMs when their predicted functions failed to match the ground-truth dataset. At high level, we distinguish between missing rules (leading to low recall) and wrong rules (leading to low precision). Missing Rules: These refer to ground-truth rules that do not appear in the models predicted rule set. We classify missing rules into three subtypes: 1. Too General. Although certain ground-truth rule : was missed, there exists corresponding predicted rule : u that over-generalizes. Specifically, the condition is proper suffix of c, causing to apply more broadly than intended. 2. Too Specific. The opposite of the above: predicted rule condition is proper extension of c, thus applying too narrowly and failing to match some instances that should have been captured by the ground-truth rule. 3. Completely Missed. No predicted rule over-generalizes or under-generalizes the ground-truth rule; in other words, this pattern is simply absent from the predicted rule set altogether. Wrong Rules: These refer to rules present in the models predicted set that do not exist in the ground truth. We categorize such rules into four types: 1. Too General. The rule : u is overly broad, applying in contexts where the ground truth does not. This typically arises when is proper suffix of some genuine condition and thus fails to capture necessary constraints. 2. Too Specific. The rule narrowly addresses only subset of the intended patterns (e.g., by employing condition that is an extension of the legitimate condition c), thereby missing broader contexts that should have matched. 3. Correct Condition but Wrong Transformation. Here, the predicted rule accurately identifies the correct condition and target input character u, but the transformation is incorrect. 4. Completely Wrong. None of the above criteria apply: the rules condition and transformation are both inconsistent with the ground truth, indicating fundamental misunderstanding. We present breakdown of error types for three models in Figure7: Llama3.3-70B, o1-mini, and o3-mini. Among missing rules, the most common issue is completely missed, where the model fails 11 Figure 7: Error Type Analysis to identify the relevant pattern at all. The second most frequent error is too general, suggesting that the predicted condition is shorter than needed, thereby overgeneralizing the intended behavior. In contrast, too specific errors in this category are relatively rare. Among wrong rules, the majority are completely wrong, followed by notable fraction of too general. Although there is also non-negligible number of too specific errors, these tend to occur when single ground-truth rule (e.g., ab is replaced by multiple subcases (e.g., aab b, bab b, cab b, indicating the model has uncovered individual instances but failed to unify them into concise representation. Finally, correct condition but wrong transformation occurs relatively infrequently, implying that once the model infers the correct condition pattern, it typically produces the correct transformation."
        },
        {
            "title": "5.3 SUMMARY OF FINDINGS",
            "content": "Overall, our experiments reveal four main insights into the inductive reasoning performance of current LLMs: Context window size dominates complexity: Increasing from 2 to 4 significantly degrades recall, precision, and compatibility, underscoring how longer look-ahead windows intensify the complexity of ISL functions. Number of Rules increases difficulty under large hypothesis space: The number of minimal rules required can drastically lower compatibility in more challenging settings with large adn Σ, indicating that managing multiple interacting rules overwhelms many models. 12 Few-shot examples do not help much: Few-shot examples help in simpler configurations but yield diminishing returns as adn Σ growssuggesting that, past certain complexity threshold, additional examples do not compensate for the models limited inductive capacity. Current LLMs are very unrobust: Providing more novel data should theoretically clarify function patterns, yet performance often plummets, reflecting fragility in inductive reasoning. Error analysis shows that missing rules are most frequently completely missed or too general, while wrong rules often end up completely wrong or again too general. Only small fraction are too specific or feature correct condition but wrong transformation, indicating that once models identify the right condition, they typically produce the correct transformation. Taken together, these findings highlight fundamental limits in current LLMs inductive reasoning. Even state-of-the-art models often fail as complexity grows, or when confronted with more data than their inductive mechanisms appear able to systematically absorb."
        },
        {
            "title": "6 LEADERBOARD BASED ON INDUCTIONBENCH",
            "content": "To facilitate straightforward comparisons among different LLMs, we introduce two-part benchmark leaderboard: standard leaderboard and an exploration leaderboard. The standard leaderboard is based completely on the three function classes we talked about, and this leaderboard simply presents an aggregated score to directly reflect LLMs performance. The exploration leaderboard includes slightly new design of function class and we will present the motivation and details below."
        },
        {
            "title": "6.1 STANDARD LEADERBOARD",
            "content": "The standard leaderboard consists of 1,080 questions spanning three classes of deterministic regular functions: ISL, L-OSL, and R-OSL in equal proportion. Specifically, it includes: 360 ISL questions, 360 L-OSL questions, 360 R-OSL questions. Within each function class, we have settings for {2, 3, 4}, Σ {5, 6, 7, 8}, and number of rules {3, 4, 5}. Each unique parameter combination has 10 data points, totaling 360 points per function class. The performance metrics recall, precision, and compatibility are computed on per-setting basis. We then form an overall weighted average to account for variations in function-space size: Definition 8 For given setting characterized by (k, Σ, r), the weight is defined as Σk s=8 (cid:80) s=5 k=4 (cid:80) k= , sk where is the Markov window, Σ is the alphabet size, and is the minimal rule count. For each function class (ISL, L-OSL, R-OSL), we compute weighted recall, precision, and compatibility according to the above scheme and then take the average of these three scores to produce the final leaderboard score for that class. The overall score across all three classes is the average of those class-wise scores. The leaderboard result can be found in https://github.com/Wenyueh/inductive_ reasoning_benchmark. Due to constraint in computation, currently we only present smaller than or equal to 70b. Based on the leaderboard result, the most recent 70b models including the Deepseek-R1 distilled models can only reach near 0 result."
        },
        {
            "title": "6.2 EXPLORATION LEADERBOARD",
            "content": "A key concern in using subregular function classes (e.g., ISL, L-OSL, R-OSL) is that polynomial-time learning algorithms already exist for these classes, potentially allowing trivial hack to achieve artificially high performance. Though we advocate not using the provbly correct algorithm for task solving so that we can genuinely evaluate LLMs inductive reasoning ability, to make sure, we introduce an exploration leaderboard that focuses on Input-Output Strictly Local (IOSL) functions: 13 more speculative class for which no known algorithm can reliably learn the entire function from finite data in finite time. Rationale. Since IOSL lacks proven polynomial-time learning procedure, successful performance here would more credibly reflect genuine inductive reasoning rather than the application of known shortcut algorithm. Furthermore, IOSL functions have not been deeply studied in the literature, offering an opportunity to see whether LLMs can advance this open research area. This is the definition of IOSL: Definition 9 (IOSL) function is IOSL if there is such that for all u1, u2 Σ, if k1(f (u2)), then TAILSf (u1) = SUFF TAILSf (u2). k1(f (u1)) = SUFF k1(u2) and SUFF k1(u1) = SUFF In essence, this condition requires the model to distinguish between input-based and output-based Markovian triggers, making the learned transformation highly non-trivial if no pre-existing algorithm is used. Leaderboard Setup. The IOSL-based leaderboard contains 1,080 datapoints, mirroring the standard leaderboard in overall structure: {2, 3, 4}, Σ {5, 6, 7, 8}, number of rules {3, 4, 5}. For each setting, there are 30 datapoints per setting (for equivalence to the standard leaderboards size). Since IOSL is not known to admit finite-characteristic sample or minimal representation in the same sense as the deterministic classes, we introduce two adaptations for evaluation: 1. Sample Size. We arbitrarily fix the sample size at 2 Σk, as no characteristic sample is theoretically guaranteed. 2. Evaluation Metrics. We focus primarily on compatibility, as recall and precision hinge on the assumption of unique minimal-length description, which may not exist for IOSL. If models generated rule set is compatible with the data, we then check whether its description length is shorter, identical, or longer than our functions reference length. longer description indicates definite failure to produce minimal representation; shorter or equal does not guarantee minimality, but it at least suggests the model avoids obvious redundancy. The leaderboard result can be found in https://github.com/Wenyueh/inductive_ reasoning_benchmark. Again, for now we only present smaller than or equal to 70b, where they all show 0 compatibility. By presenting both standard leaderboard (subregular classes with known learnability) and an exploration leaderboard (IOSL with no established finite-data algorithm), we offer balanced view: models can demonstrate success in theoretically well-understood tasks while also exploring novel, under-constrained function classesthereby reducing the concern that high performance might merely reflect an existing hack."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we introduced systematic benchmark for assessing the inductive reasoning capabilities of LLMs, leveraging both well-studied subregular function classes (ISL, L-OSL, and R-OSL) and more exploratory class (IOSL) for which no known polynomial-time learning algorithm exists. By controlling parameters such as the Markov window size k, the vocabulary size Σ, and the minimal number of rules, we offered precise yet flexible tasks capable of probing models capacity to infer general transformations from limited data. Our findings revealed several significant challenges for current LLMsespecially when required to track deeper dependencies or manage larger search spacesand underscored the fragility of their inductive reasoning under increased context or novel data. Through experiments measuring recall, precision, and compatibility, we demonstrated that factors like the Markov window size and the number of rules more profoundly degrade performance than an expanded alphabet. Moreover, while few-shot prompting showed promise in simpler scenarios, 14 its benefits quickly plateaued in more complex contexts. An error analysis further highlighted how many rules go completely missing or become overgeneralized under stringent settings, indicating that LLMs often fail to synthesize key patterns comprehensively. We also proposed an exploration leaderboard targeting IOSL functions, class beyond established theoretical learnability, to address concerns that performance gains might stem from known polynomialtime algorithms rather than genuine inductive reasoning. This complementary evaluation opens avenues for research on less tractable classes and poses more authentic test of generalization and adaptability. Overall, our results highlight the need for more robust inductive reasoning strategies within current LLM architectures. We hope that our benchmark will help catalyze progress in both theoretical understanding and practical innovations around LLMs inductive capabilities."
        },
        {
            "title": "LIMITATIONS",
            "content": "While our benchmark offers rigorous, theoretically grounded approach to evaluating inductive reasoning in LLMs, current paper is subject to two notable constraints: Synthetic Rather Than Real-World Data. All tasks and evaluations rely on functions generated from carefully controlled parameters rather than naturally occurring texts or real-world datasets. Although this design enables precise measurement of inductive capabilities, it may not fully capture the complexity of practical language use, where ambiguous contexts, noisy inputs, and domainspecific factors can further challenge inference. Restricted Access to the o1 Model. Our investigation into the o1 family of models is hindered by limited availability and computational resources. As result, certain aspects of o1s inductive behavior may remain unexamined, and more exhaustive exploration of variations or fine-tuning strategies for o1 could further illuminate its performance."
        },
        {
            "title": "REFERENCES",
            "content": "Arvind Arasu, Surajit Chaudhuri, and Raghav Kaushik. Learning string transformations from examples. Proceedings of the VLDB Endowment, 2(1):514525, 2009. Alan Baker. Occams razor in science: case study from biogeography. Biology & Philosophy, 22 (2):193215, 2007. Steven Bird and Mark Ellison. One-level phonology: Autosegmental representations and rules as finite automata. Computational Linguistics, 20(1):5590, 1994. Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred Warmuth. Occams razor. Information processing letters, 24(6):377380, 1987. Jane Chandlee, Rémi Eyraud, and Jeffrey Heinz. Learning strictly local subsequential functions. Transactions of the Association for Computational Linguistics, 2:491504, 2014. Jane Chandlee, Rémi Eyraud, and Jeffrey Heinz. Output strictly local functions. In 14th Meeting on the Mathematics of Language, pp. 112125, 2015. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Colin De La Higuera. Characteristic sets for polynomial grammatical inference. Machine Learning, 27:125138, 1997. 15 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Joost Engelfriet and Hendrik Jan Hoogeboom. Mso definable string transductions and two-way finite-state transducers. ACM Transactions on Computational Logic (TOCL), 2(2):216254, 2001. Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, and Yongfeng Zhang. Nphardeval: Dynamic benchmark on reasoning ability of large language models via complexity classes. arXiv preprint arXiv:2312.14890, 2023. Thomas Graf. Diving deeper into subregular syntax. Theoretical Linguistics, 48(3-4):245278, 2022. Peter Grünwald. The minimum description length principle. MIT press, 2007. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022. Mark Hansen and Bin Yu. Model selection and the principle of minimum description length. Journal of the american statistical association, 96(454):746774, 2001. James Hawthorne. Inductive logic. 2004. Jeffrey Heinz. The computational nature of phonological generalizations. Phonological typology, phonetics and phonology, 23:126195, 2018. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Wenyue Hua, Kaijie Zhu, Lingyao Li, Lizhou Fan, Shuhang Lin, Mingyu Jin, Haochen Xue, Zelong Li, JinDong Wang, and Yongfeng Zhang. Disentangling logic: The role of context in large language model reasoning capabilities. arXiv preprint arXiv:2406.02787, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Gerhard Jäger and James Rogers. Formal language theory: refining the chomsky hierarchy. Philosophical Transactions of the Royal Society B: Biological Sciences, 367(1598):19561970, 2012. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Adam Jardine, Jane Chandlee, Rémi Eyraud, and Jeffrey Heinz. Very efficient learning of structured classes of subsequential functions from positive data. In International Conference on Grammatical Inference, pp. 94108. PMLR, 2014. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Brenden Lake, Tal Linzen, and Marco Baroni. Human few-shot learning of compositional instructions. arXiv preprint arXiv:1901.04587, 2019. Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning. arXiv preprint arXiv:2404.02060, 2024. 16 Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Emmy Liu, Graham Neubig, and Jacob Andreas. An incomplete loop: Deductive, inductive, and abductive learning in large language models. arXiv preprint arXiv:2404.03028, 2024b. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229, 2024. Mehryar Mohri. Finite-state transducers in language and speech processing. Computational linguistics, 23(2):269311, 1997. José Oncina and Pedro Garcia. Inductive learning of subsequential functions. Univ. Politécnica de Valencia, Tech. Rep. DSIC II, 31, 1991. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, and Chitta Baral. Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1367913707, 2024. Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. arXiv preprint arXiv:2310.08559, 2023. Emmanuel Roche and Yves Schabes. Finite-state language processing, volume 115. MIT press Cambridge, MA, 1997. James Rogers and Geoffrey Pullum. Aural pattern recognition experiments and the subregular hierarchy. Journal of Logic, Language and Information, 20:329342, 2011. Giorgio Satta and John Henderson. String transformation learning. In 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pp. 444451, 1997. Bianca Truthe. Hierarchy of subregular language families. 2018. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: reading comprehension dataset requiring logical reasoning. arXiv preprint arXiv:2002.04326, 2020. Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, and Xing Xie. Dyval: Dynamic evaluation of large language models for reasoning tasks. In The Twelfth International Conference on Learning Representations, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "Full results on ISL, OSL, and few-shot experiments are presented here. 17 Models Settings = 2 = = 4 recall precision compatibility recall precision compatibility recall precision compatibility Llama-3.3 70B Llama-3.1 405B DeepSeek-V3 GPT-4o o1-mini o3-mini Llama-3.3 70B Llama-3.1 405B DeepSeek-V3 GPT-4o o1-mini o3-mini Llama-3.3 70B Llama-3.1 405B DeepSeek-V3 GPT-4o o1-mini o3-mini rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 60.00 60.00 53.33 30.00 55.00 56.67 90.00 80.00 70.00 60.00 60.00 73.33 50.00 75.00 66.67 100.00 90.00 100.00 70.00 85.00 66.67 20.00 45.00 50.00 70.00 80.00 80.00 50.00 60.00 66.67 80.00 90.00 80.00 100.00 100.00 96.67 60.00 40.00 53.33 40.00 75.00 40.00 80.00 85.00 76.67 50.00 45.00 56.67 80.00 75.00 93.33 100.00 100.00 96.67 55.00 65.00 68.33 25.00 50.00 44.17 60.00 60.00 54.83 43.33 37.50 68.33 45.00 75.00 61.67 100.00 90.00 97.50 60.00 83.33 74.17 10.00 32.58 38.45 65.00 56.00 60.76 33.33 40.42 64.33 80.00 90.00 77.33 100.000 100.000 97. 60.00 40.00 68.33 35.00 52.33 34.33 52.50 57.15 63.12 40.00 29.00 38.62 70.00 63.33 91.67 100.00 100.00 95.00 vocab size = 2 30.00 45.00 30.00 50.00 10.00 10.00 50.00 40.00 40.00 50.00 35.00 36.67 70.00 70.00 43.33 100.00 90.00 83.33 vocab size = 3 20.00 10.00 33.33 20.00 10.00 6.67 70.00 40.00 56.67 50.00 25.00 30.00 50.00 40.0 63.33 100.00 95.00 93.33 vocab size = 30.00 15.00 6.67 10.00 10.00 13.33 50.00 65.00 50.00 50.00 45.00 33.33 60.00 50.00 46.67 100.00 95.00 93.33 60.00 50.00 20.00 30.00 40.00 20.00 60.00 40.00 40.00 40.00 50.00 60.00 50.00 75.00 60.00 100.00 90.00 100.00 60.00 60.00 20.00 10.00 20.00 20.00 60.00 40.00 50.00 50.00 30.00 40.00 80.00 80.00 60.00 100.00 100.00 80.00 60.00 30.00 20.00 30.00 10.00 10.00 60.00 50.00 40.00 40.00 10.00 0.00 80.00 60.00 80.00 100.00 100.00 90.00 23.33 60.00 46.67 35.00 6.67 9.50 32.50 19.89 26.15 22.00 18.43 19.30 45.83 60.00 34.00 100.00 90.67 71.83 20.00 7.50 35.36 10.00 7.50 3.10 45.00 13.23 21.64 18.33 6.00 9.61 43.33 25.11 36.25 95.00 91.67 91. 30.00 11.67 5.00 5.00 5.83 1.54 17.00 18.66 16.05 16.67 12.62 20.60 50.00 29.93 37.72 95.00 91.67 93.33 20.00 30.00 10.00 30.00 0.00 0.00 50.00 10.00 0.00 40.00 20.00 0.00 40.00 40.00 0.00 100.00 90.00 60.00 20.00 0.00 0.00 10.00 0.00 0.00 60.00 0.00 0.00 40.00 0.00 0.00 40.00 10.00 10.00 90.00 80.00 90.00 30.00 0.00 0.00 0.00 0.00 0.00 40.00 20.00 10.00 20.00 0.00 0.00 50.00 10.00 10.00 100.00 80.00 80.00 10.00 15.00 16.67 20.00 10.00 0.00 50.00 15.00 33.33 10.00 15.00 13.33 30.00 50.00 40.00 80.00 85.00 70.00 20.00 5.00 6.67 10.00 0.00 10.00 50.00 25.00 40.00 20.00 30.00 10.00 30.00 55.00 30.00 90.00 75.00 83. 10.00 0.00 10.00 10.00 0.00 10.00 40.00 45.00 13.33 50.00 0.00 10.00 40.00 35.00 36.67 60.00 75.00 73.33 10.00 8.25 8.54 15.00 7.50 0.00 12.33 3.82 10.61 2.00 5.63 2.50 16.67 28.67 38.52 58.33 80.00 63.81 8.33 2.50 3.43 10.00 0.00 5.27 13.93 1.89 5.88 4.17 6.39 1.72 0.00 24.82 20.44 78.33 75.00 85.17 10.00 0.00 5.32 10.00 0.00 3.75 14.58 5.30 2.46 21.67 0.00 2.67 15.00 35.00 22.09 60.00 76.67 59.58 Table 3: Input Strictly Local with sample size = 2 10.00 0.00 0.00 10.00 0.00 0.00 30.00 0.00 0.00 0.00 10.00 0.00 10.00 0.00 0.00 40.00 50.00 20. 10.00 0.00 0.00 10.00 0.00 0.00 50.00 0.00 0.00 10.00 0.00 0.00 0.00 10.00 0.00 70.00 50.00 50.00 10.00 0.00 0.00 10.00 0.00 0.00 40.00 0.00 0.00 20.00 0.00 0.00 10.00 0.00 0.00 60.00 40.00 10."
        },
        {
            "title": "Settings",
            "content": "k = 2 = 3 = 4 recall precision compatibility recall precision compatibility recall precision compatibility Llama-3.3 70B Llama-3.1 405B GPT-4o DeepSeek-V3 o1-mini o3-mini Llama-3.3 70B Llama-3.1 405B GPT-4o DeepSeek-V3 o1-mini o3-mini Llama-3.3 70B Llama-3.1 405B GPT-4o DeepSeek-V3 o1-mini o3-mini rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 50.00 25.00 56.67 70.00 50.00 63.33 30.00 75.00 66.67 100.00 60.00 83.33 90.00 80.00 90.00 100.00 100.00 100.00 50.00 35.00 40.00 60.00 30.00 66.67 40.00 55.00 60.00 80.00 90.00 66.67 100.00 90.00 96.67 100.00 100.00 96.67 50.00 50.00 50.00 60.00 50.00 43.33 40.00 75.00 70.00 100.00 70.00 60.00 90.00 100.00 80.00 100.00 100.00 100. 45.00 25.00 65.00 45.83 33.33 53.83 12.50 63.17 60.00 75.00 44.17 77.67 90.00 80.00 82.50 100.00 100.00 100.00 50.00 33.67 65.00 45.00 20.00 57.83 27.50 46.50 50.00 70.00 60.32 53.50 95.00 83.33 96.00 100.00 100.00 94.17 29.00 50.00 52.50 34.50 29.00 30.73 35.00 45.50 48.22 82.50 50.67 48.36 85.00 93.33 72.25 100.00 100.00 100.00 vocab size = 2 0.00 10.00 6.67 30.00 25.00 10.00 30.00 20.00 30.00 50.00 10.00 20.00 70.00 60.00 66.67 90/00 95.00 86.67 vocab size = 20.00 20.00 20.00 10.00 15.00 20.00 20.00 45.00 33.33 50.00 70.00 23.33 80.00 70.00 70.00 90.00 90.00 96.67 vocab size = 4 20.00 20.00 6.67 10.00 10.00 16.67 60.00 55.00 33.33 50.00 25.00 50.00 50.00 60.00 70.00 100.00 85.00 76.67 50.00 20.00 0.00 70.00 10.00 0.00 30.00 60.00 50.00 70.00 30.00 50.00 90.00 80.00 50.00 100.00 100.00 100.00 50.00 10.00 20.00 40.00 0.00 30.00 40.00 10.00 10.00 60.00 60.00 40.00 90.00 80.00 90.00 100.00 100.00 90.00 30.00 10.00 20.00 30.00 0.00 20.00 30.00 30.00 20.00 80.00 30.00 30.00 90.00 80.00 60.00 100.00 100.00 100. 0.00 8.33 8.33 9.33 11.39 6.67 10.83 7.42 19.00 32.50 15.00 12.92 55.00 60.00 60.67 90.00 100.00 85.00 12.50 6.93 18.33 3.33 13.33 8.39 8.33 25.67 15.95 22.00 13.82 16.42 63.33 49.42 56.15 90.00 75.15 87.50 13.33 15.96 6.00 5.00 3.13 5.83 25.33 20.47 10.77 23.67 8.01 12.81 38.33 36.92 43.04 100.00 81.67 76.67 0.00 10.00 0.00 10.00 0.00 0.00 10.00 0.00 10.00 40.00 10.00 0.00 40.00 50.00 20.00 90.00 100.00 80.00 10.00 10.00 0.00 10.00 0.00 0.00 20.00 0.00 0.00 40.00 20.00 0.00 70.00 40.00 30.00 90.00 70.00 90.00 10.00 0.00 0.00 10.00 0.00 0.00 40.00 10.00 0.00 30.00 0.00 20.00 30.00 20.00 10.00 100.00 70.00 70. 0.00 5.00 13.33 10.00 10.00 6.67 10.00 15.00 10.00 40.00 20.00 23.33 10.00 65.00 50.00 90.00 85.00 56.67 20.00 25.00 10.00 10.00 5.00 10.00 40.00 30.00 20.00 50.00 30.00 30.00 30.00 35.00 50.00 80.00 80.00 63.33 10.00 0.00 10.00 10.00 10.00 6.67 40.00 20.00 13.33 40.00 15.00 23.33 40.00 50.00 43.33 60.00 45.00 66.67 0.00 10.00 12.83 1.67 3.00 5.00 5.00 6.35 4.16 12.78 11.94 13.97 10.00 53.83 54.22 90.00 78.33 50.33 13.33 15.00 2.78 1.11 0.53 2.36 11.00 6.50 6.21 16.11 5.04 7.54 17.50 29.52 33.58 80.00 72.50 68.43 10.00 0.00 6.33 5.00 2.90 1.10 12.50 9.44 3.82 16.11 3.24 2.73 28.33 31.92 32.12 60.00 58.33 69. Table 4: Left Output Strictly Local with sample size = 2 0.00 0.00 0.00 10.00 0.00 0.00 10.00 0.00 0.00 40.00 0.00 0.00 10.00 10.00 10.00 90.00 70.00 40.00 10.00 0.00 0.00 0.00 0.00 0.00 30.00 10.00 0.00 30.00 0.00 10.00 30.00 20.00 0.00 80.00 50.00 40.00 10.00 0.00 0.00 10.00 0.00 0.00 10.00 0.00 0.00 40.00 0.00 0.00 20.00 10.00 0.00 60.00 20.00 10."
        },
        {
            "title": "Settings",
            "content": "k = 2 = 3 = 4 recall precision compatibility recall precision compatibility recall precision compatibility vocab size = 2 Llama-3.3 70B Llama-3.1 405B GPT-4o DeepSeek-V3 o1-mini o3-mini Llama-3.3 70B Llama-3.1 405B GPT-4o DeepSeek-V3 o1-mini o3-mini Llama-3.3 70B Llama-3.1 405B GPT-4o DeepSeek-V3 o1-mini o3-mini rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = 3 40.00 40.00 63.33 20.00 60.00 63.33 50.00 70.00 83.33 60.00 75.00 86.67 50.00 55.00 46.67 90.00 100.00 96.67 40.00 30.00 30.00 20.00 30.00 16.67 60.00 60.00 66.67 80.00 80.00 70.00 60.00 15.00 53.33 90.00 90.00 100. 40.00 45.00 33.33 10.00 15.00 33.33 70.00 80.00 66.67 80.00 70.00 73.33 70.00 70.00 90.00 90.00 100.00 100.00 40.00 40.00 76.67 8.33 51.67 54.72 30.00 61.67 71.83 38.33 51.67 72.56 38.33 38.33 46.67 90.00 100.00 92.67 40.00 60.00 45.00 11.43 20.83 9.11 50.00 41.67 57.83 60.83 63.19 42.95 60.00 15.00 45.83 90.00 83.33 100.00 25.00 47.33 39.50 10.00 12.00 22.67 49.17 78.10 43.00 65.00 59.17 60.17 53.33 66.67 79.17 90.00 100.00 100.00 40.00 30.00 30.00 0.00 40.00 30.00 30.00 60.00 30.00 20.00 40.00 50.00 40.00 30.00 10.00 90.00 100.00 80.00 40.00 10.00 0.00 10.00 10.00 0.00 40.00 30.00 30.00 60.00 50.00 50.00 60.00 10.00 20.00 90.00 70.00 100. 30.00 10.00 10.00 10.00 0.00 0.00 50.00 40.00 0.00 70.00 30.00 40.00 40.00 50.00 50.00 90.00 100.00 100.00 20.00 15.00 23.33 40.00 25.00 20.00 10.00 45.00 43.33 40.00 35.00 40.00 20.00 35.00 33.33 100.00 90.00 86.67 30.00 20.00 26.67 10.00 15.00 10.00 50.00 50.00 40.00 50.00 55.00 23.33 20.00 40.00 30.00 100.00 80.00 83.33 vocab size = 4 40.00 50.00 30.00 0.00 10.00 3.33 60.00 40.00 20.00 50.00 45.00 43.33 30.00 50.00 23.33 100.00 80.00 76.67 10.00 18.33 24.17 18.33 12.50 14.33 5.00 16.93 24.10 14.17 18.33 27.00 5.83 17.50 24.17 95.00 85.00 78. 25.00 10.93 22.67 5.00 4.17 4.75 27.50 27.50 21.88 29.17 26.67 15.28 13.33 25.00 17.63 95.00 61.67 64.11 23.33 45.83 23.85 0.00 13.33 16.67 19.50 16.02 11.02 18.83 27.79 13.45 18.33 47.50 23.33 93.33 75.00 78.33 10.00 10.00 0.00 0.00 0.00 10.00 10.00 10.00 0.00 20.00 0.00 0.00 0.00 0.00 0.00 90.00 60.00 50.00 30.00 0.00 10.00 0.00 0.00 0.00 20.00 20.00 0.00 40.00 20.00 0.00 10.00 0.00 0.00 100.00 30.00 50.00 20.00 10.00 0.00 0.00 0.00 0.00 50.00 0.00 0.00 20.00 20.00 0.00 10.00 20.00 0.00 90.00 60.00 50.00 20.00 20.00 10.00 0.00 10.00 6.67 0.00 30.00 20.00 20.00 25.00 23.33 20.00 15.00 10.00 70.00 45.00 46. 30.00 25.00 10.00 20.00 10.00 0.00 50.00 30.00 13.33 40.00 40.00 20.00 20.00 30.00 13.33 50.00 60.00 43.33 10.00 5.00 10.00 0.00 5.00 13.33 50.00 20.00 16.67 60.00 10.00 3.33 40.00 40.00 26.67 50.00 70.00 63.33 10.00 18.67 7.83 0.00 6.25 4.50 0.00 15.83 10.00 7.00 13.00 3.68 11.67 8.70 4.41 50.00 41.67 46.67 7.26 19.17 4.58 4.17 3.41 0.00 15.33 8.82 6.33 9.42 10.10 3.56 13.33 19.50 8.43 40.00 59.50 41.83 10.00 1.67 10.83 0.00 0.26 1.85 23.10 10.00 6.73 19.77 0.88 0.25 40.00 34.00 17.58 50.00 69.17 62.00 Table 5: Right Output Strictly Local with sample size = 10.00 10.00 0.00 0.00 10.00 0.00 0.00 20.00 0.00 10.00 20.00 0.00 10.00 0.00 0.00 30.00 20.00 30.00 0.00 10.00 0.00 0.00 0.00 0.00 20.00 0.00 0.00 20.00 0.00 0.00 10.00 0.00 0.00 30.00 40.00 20.00 10.00 0.00 0.00 0.00 0.00 0.00 10.00 0.00 0.00 40.00 0.00 0.00 40.00 0.00 0.00 50.00 40.00 30."
        },
        {
            "title": "Settings",
            "content": "k = 2 = 3 = 4 recall precision compatibility recall precision compatibility recall precision compatibility rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = rules = 1 rules = 2 rules = 3 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 60.00 60.00 70.00 80.00 60.00 65.00 85.00 60.00 53.33 76.67 86.67 90.00 70.00 90.00 70.00 40.00 85.00 90.00 65.00 65.00 66.67 70.00 76.67 60.00 60.00 40.00 70.00 60.00 40.00 60.00 80.00 70.00 53.33 66.67 66.67 60.00 55.00 60.00 70.00 80.00 65.00 70.00 85.00 65.00 68.33 83.33 86.67 93.33 60.00 90.00 70.00 40.00 83.33 95.00 65.00 75.00 74.17 69.17 76.67 60.83 60.00 40.00 57.00 60.00 40.00 60.00 90.00 70.00 68.33 70.83 73.33 71. vocab size = 2 30.00 50.00 70.00 60.00 45.00 40.00 45.00 35.00 30.00 43.33 26.67 46.67 23.33 35.00 50.00 55.00 60.00 53.00 56.67 36.67 46.67 57.67 28.33 52.50 vocab size = 3 20.00 50.00 30.00 40.00 10.00 10.00 30.00 5.00 33.33 20.00 33.33 23.33 20.00 50.00 20.00 35.00 7.50 13.33 28.33 3.33 35.36 22.50 43.33 31. vocab size = 4 30.00 50.00 30.00 20.00 15.00 15.00 15.00 15.00 6.67 30.00 30.00 20.00 30.00 31.67 25.00 15.00 11.67 23.33 12.50 18.33 5.00 47.50 55.00 39.24 60.00 60.00 70.00 80.00 50.00 60.00 80.00 40.00 20.00 60.00 60.00 70.00 60.00 90.00 70.00 40.00 60.00 80.00 40.00 30.00 20.00 40.00 50.00 10.00 60.00 40.00 60.00 60.00 30.00 30.00 60.00 70.00 20.00 30.00 30.00 10. 20.00 40.00 60.00 60.00 30.00 30.00 20.00 10.00 10.00 0.00 0.00 20.00 20.00 50.00 20.00 40.00 0.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 30.00 50.00 30.00 20.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10.00 10.00 20.00 10.00 15.00 20.00 25.00 20.00 16.67 20.00 13.33 16.67 20.00 30.00 10.00 30.00 5.00 5.00 5.00 5.00 6.67 10.00 10.00 16.67 10.00 20.00 10.00 10.00 0.00 0.00 15.00 10.00 10.00 3.00 3.33 0. 10.00 5.00 10.00 5.00 8.25 18.33 23.33 20.00 8.54 18.33 16.67 21.17 8.33 30.00 10.00 18.33 2.50 2.50 2.00 2.50 3.43 13.33 13.33 19.76 10.00 13.33 5.00 10.00 0.00 0.00 10.67 10.00 5.32 5.00 3.33 0.00 10.00 0.00 10.00 10.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 10.00 30.00 10.00 20.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10.00 20.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. Table 6: Input Strictly Local with sample size = 2 with few-shot example"
        },
        {
            "title": "Settings",
            "content": "k = 2 = 3 = 4 recall precision compatibility recall precision compatibility recall precision compatibility rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = rules = 1 rules = 2 rules = 3 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 50.00 80.00 80.00 80.00 25.00 80.00 85.00 75.00 56.67 80.00 80.00 80.00 50.00 100.00 70.00 80.00 35.00 80.00 50.00 70.00 40.00 70.00 83.33 70.00 50.00 50.00 60.00 20.00 50.00 55.00 55.00 10.00 50.00 56.67 66.67 3.33 45.00 80.00 75.00 80.00 25.00 85.00 85.00 80.00 65.00 80.00 75.00 80.00 50.00 100.00 70.00 75.00 33.67 76.67 55.00 70.00 65.00 66.67 90.00 78.33 29.00 50.00 60.00 20.00 50.00 56.67 50.00 20.00 52.50 68.33 67.50 3. vocab size = 2 0.00 40.00 30.00 20.00 10.00 30.00 20.00 25.00 6.67 40.00 33.33 33.33 0.00 33.33 25.00 15.00 8.33 30.83 21.67 20.83 8.33 42.00 48.33 39.17 vocab size = 3 20.00 40.00 30.00 20.00 20.00 30.00 30.00 20.00 20.00 30.00 33.33 23.33 12.50 23.33 23.33 20.00 6.93 30.33 38.33 30.00 18.33 24.83 40.83 44. vocab size = 4 20.00 50.00 10.00 30.00 20.00 20.00 35.00 30.00 6.67 20.00 16.67 23.33 13.33 50.00 10.00 25.00 15.96 28.33 55.00 33.33 6.00 36.25 26.67 40.83 50.00 80.00 70.00 80.00 25.00 70.00 80.00 60.00 0.00 80.00 70.00 80.00 50.00 100.00 70.00 80.00 10.00 80.00 30.00 70.00 20.00 40.00 60.00 30.00 30.00 50.00 60.00 20.00 10.00 30.00 20.00 0.00 20.00 20.00 50.00 0. 0.00 30.00 30.00 20.00 10.00 10.00 10.00 10.00 0.00 0.00 0.00 0.00 10.00 40.00 20.00 20.00 10.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 10.00 50.00 10.00 30.00 0.00 0.00 10.00 10.00 0.00 0.00 0.00 0.00 0.00 20.00 30.00 20.00 5.00 30.00 25.00 20.00 13.33 10.00 13.33 16.67 20.00 0.00 10.00 20.00 25.00 10.00 25.00 20.00 10.00 10.00 30.00 16.67 10.00 20.00 20.00 20.00 0.00 0.00 5.00 10.00 10.00 16.7 6.67 3. 0.00 5.00 13.33 15.00 10.00 19.00 27.90 20.83 12.83 11.67 28.33 26.67 13.33 0.00 5.00 20.00 15.00 15.00 21.67 35.00 2.78 20.30 43.33 18.33 10.00 15.00 20.00 20.00 0.00 0.00 10.00 20.00 6.33 22.83 15.00 3.33 0.00 20.00 10.00 20.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 10.00 0.00 0.00 20.00 0.00 0.00 0.00 0.00 0.00 10.00 0.00 0.00 10.00 20.00 20.00 20.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. Table 7: Left Output Strictly Local with sample size = 2 with few-shot example"
        },
        {
            "title": "Settings",
            "content": "k = 2 = 3 = 4 recall precision compatibility recall precision compatibility recall precision compatibility rules = 1 rules = 2 rules = 3 rules = 1 rules = 2 rules = rules = 1 rules = 2 rules = 3 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 0-shot 1-shot 2-shot 3-shot 40.00 60.00 60.00 60.00 40.00 60.00 70.00 90.00 63.33 60.00 60.00 60.00 40.00 70.00 80.00 70.00 30.00 65.00 60.00 65.00 30.00 80.00 73.33 80.00 40.00 60.00 80.00 70.00 45.00 80.00 65.00 75.00 33.33 66.67 83.33 76.67 40.00 60.00 60.00 60.00 40.00 60.00 80.00 90.00 76.67 60.00 60.00 60.00 40.00 65.00 75.00 65.00 60.00 70.00 70.00 70.00 45.00 80.83 71.67 80.83 25.00 60.00 68.33 70.00 47.33 70.00 65.00 75.00 39.50 71.83 86.50 80. vocab size = 2 20.00 50.00 40.00 50.00 15.00 40.00 45.00 45.00 23.33 40.00 50.00 43.33 10.00 40.00 35.00 40.00 18.33 42.50 43.33 41.67 24.17 47.83 48.33 46.67 vocab size = 3 30.00 40.00 50.00 40.00 20.00 30.00 50.00 30.00 26.67 30.00 26.67 30.00 25.00 35.00 50.00 35.00 10.93 45.00 44.17 45.00 22.67 37.59 34.50 37. vocab size = 4 40.00 50.00 60.00 40.00 50.00 45.00 45.00 45.00 30.00 23.33 26.67 40.00 23.33 31.67 33.33 19.50 45.83 61.67 52.50 48.33 23.85 30.83 37.50 51.67 40.00 60.00 60.00 60.00 30.00 50.00 60.00 90.00 30.00 60.00 60.00 60.00 40.00 60.00 70.00 60.00 10.00 40.00 40.00 40.00 0.00 40.00 30.00 40.00 30.00 60.00 80.00 70.00 10.00 40.00 40.00 70.00 10.00 30.00 50.00 50. 10.00 50.00 40.00 50.00 10.00 20.00 30.00 30.00 0.00 0.00 20.00 10.00 30.00 40.00 50.00 40.00 0.00 10.00 10.00 10.00 10.00 0.00 0.00 0.00 20.00 50.00 30.00 40.00 10.00 10.00 20.00 20.00 0.00 0.00 0.00 10.00 20.00 30.00 30.00 10.00 20.00 30.00 20.00 15.00 10.00 20.00 16.67 16.67 30.00 20.00 30.00 20.00 25.00 15.00 20.00 15.00 10.00 13.33 33.33 13.33 10.00 10.00 20.00 20.00 5.00 5.00 0.00 15.00 10.00 3.33 13.33 13. 10.00 18.33 25.00 2.50 18.67 33.33 27.50 15.00 7.83 24.17 18.33 25.00 7.26 15.00 18.33 15.00 19.17 11.67 20.00 11.67 4.58 11.00 47.00 11.00 10.00 10.00 20.00 20.00 1.67 10.00 0.00 30.00 10.83 2.50 28.83 20.00 10.00 10.00 20.00 10.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10.00 30.00 10.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10.00 10.00 20.00 20.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. Table 8: Right Output Strictly Local with sample size = 2 with few-shot example"
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "Rutgers University, New Brunswick",
        "University of Arizona",
        "University of California, Santa Barbara"
    ]
}