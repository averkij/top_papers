{
    "paper_title": "Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes",
    "authors": [
        "Chenye Meng",
        "Zejian Li",
        "Zhongni Liu",
        "Yize Li",
        "Changle Xie",
        "Kaixin Jia",
        "Ling Yang",
        "Huanghuang Deng",
        "Shiying Ding",
        "Shengyuan Zhang",
        "Jiayi Li",
        "Lingyun Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct a hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in a tree structure. Building on this, we propose a two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 ] . [ 1 0 0 3 4 0 . 1 0 6 2 : r Beyond Binary Preference: Aligning Diffusion Models to Fine-grained Criteria by Decoupling Attributes Chenye Meng1 Ling Yang3 Zejian Li1 Zhongni Liu2 Yize Li1 Huanghuang Deng Shiying Ding1 Lingyun Sun1 Changle Xie1 Shengyuan Zhang1 Kaixin Jia1 Jiayi Li4 1 Zhejiang University 3 Peking University 2 University of Electronic Science and Technology of China 4 University of Nottingham Ningbo China 1 {zejianlee,mengcy}@zju.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Post-training alignment of diffusion models relies on simplified signals, such as scalar rewards or binary preferences. This limits alignment with complex human expertise, which is hierarchical and fine-grained. To address this, we first construct hierarchical, fine-grained evaluation criteria with domain experts, which decomposes image quality into multiple positive and negative attributes organized in tree structure. Building on this, we propose two-stage alignment framework. First, we inject domain knowledge to an auxiliary diffusion model via Supervised Fine-Tuning. Second, we introduce Complex Preference Optimization (CPO) that extends DPO to align the target diffusion to our non-binary, hierarchical criteria. Specifically, we reformulate the alignment problem to simultaneously maximize the probability of positive attributes while minimizing the probability of negative attributes with the auxiliary diffusion. We instantiate our approach in the domain of painting generation and conduct CPO training with an annotated dataset of painting with fine-grained attributes based on our criteria. Extensive experiments demonstrate that CPO significantly enhances generation quality and alignment with expertise, opening new avenues for fine-grained criteria alignment. 1. Introduction In the new era of generative AI, evaluation has become more important than training [49]. The quality and nature of evaluation and data fundamentally define the upper limit of models capabilities. Recent post-training strategies, such as Reinforcement Learning from Human Feedback (RLHF) including DPO [32] and GRPO [34], have demonstrated significant efficacy in enhancing generative models. However, these prevailing frameworks fundamentally depend on the scores of reward models or binary huFigure 1. Existing methods rely on coarse-grained, scalar or binary image-level reward signals. In contrast, our method leverages human expert knowledge for fine-grained attribute decoupling, guiding the model directly from the noise space to approach positive and avoid negative directions. man preferences of winning and losing samples they are optimized for  (Fig. 1)  . Such simplified, coarse evaluation criteria lead to substantial gap when compared to the complex and nuanced patterns of human cognition in real world. follow such uniHuman evaluation does not 1 dimensional or regularized process. Consistent with existing research, we summarize three features of human expert evaluation: (1) Multi-dimensional, assessing multiple dimensions simultaneously (such as composition, color relations and brushwork in paintings); (2) Discrete, employing symbolic labels rather than continuous scores; and (3) Nonequilibrium, meaning the applicable set of evaluation labels dynamically shifts with samples. This highlights critical insight: positive (Apos) and negative (Aneg) attributes are not merely opposites. Their relationship is complex. They may be mutually exclusive in some cases, while in others they can coexist within the same sample. Existing post-training frameworks, which typically optimize single utility function, are ill-equipped to process such complex signals. We argue that an evaluation paradigm aligned with fine-grained human cognition can provide more specific, interpretable guidance, leading to enhanced generation quality and controllability. To bridge this chasm, we go beyond binary preferences and propose new evaluation paradigm. We construct hierarchical, multi-dimensional evaluation criterion with domain experts. We instantiate our approach in the domain of painting generation, developing domainspecific knowledge system comprising 7 root dimensions (e.g., Composition, Color Relations) and 246 pairs of positive/negative attributes. To operationalize this system, we build domain-expert agent that annotates 10,277 collected images of paintings, transforming expert evaluation into discrete, symbolic semantic labels that explicitly identify coexisting positive and negative attributes (Apos, Aneg). Building on this fine-grained feedback, we propose novel two-stage post-training strategy. In the first stage, we inject domain knowledge into pre-trained model via Supervised Fine-Tuning, yielding an expert model θ1 sensitive to these complex attributes. In the second stage, we introduce Complex Preference Optimization (CPO), novel preference learning algorithm to train the final generative model with decoupling attributes learned in the expert model and. Given noisy sample from the training set, θ1 provides an ideal noise prediction zw (winner) mainly conditioned on Apos and non-ideal zl (loser) mainly on Aneg. By assuming the winner prediction guides the noisy training sample to winning output and vice versa, we perform preference optimization that steers the final trained model toward Apos yet away from Aneg. In this case, the trained model generates images aligned with domain-specific evaluation criteria given only the content prompt without specified complex positive attributes. In practice, we observe instability of preference optimization and propose new stabilizing strategy. The instability is manifested by that the term on losing samples dominates the training while that on winning samples fails to converge consistently. We attribute this phenomenon to the behavior of minimizing negative squared error, and thus propose new stretegy that translates the loss term for the losing samples. The translation restricts the norm of backward gradients but remain the gradient direction as the original loss. Our strategy encourages balance between the gradients of winning and losing samples. Extensive experiments demonstrate that our approach significantly enhances generation quality and alignment with expert preferences. Our stabilizing strategy boosts training by over 10 times faster compared to the counterpart with the original loss. Our work validates the merit of finegrained evaluation and sheds light on future post-training paradigms. In summary, our contributions are as follows: We extend the simplified binary preferences and propose new, human-aligned evaluation criteria based on multidimensional, discrete, and non-equilibrium expert criteria. We instantiate this criterion and develop domainexpert agent to create fine-grained dataset with positive and negative attributes. We propose novel two-stage post-training strategy, dubbed Complex Preference Optimization (CPO), which aligns diffusion model by decoupling the positive and negative attributes inside generated samples. We introduce new stability strategy, resolving optimization instabilities by balancing gradients from the postive and negative samples. 2. Related Work Preference optimization dataset. The efficacy of prefis constrained by the feedback sigerence alignment nals granularity. Foundational datasets, including Pick-aPic [17], ImageReward [46],HPS [29, 44], and LAIONAesthetic [33] establish the field by collecting large-scale binary preferences (winning/losing) or monolithic aesthetic scores (e.g., 1-10). However, these simplified evaluation criteria result in pronounced discrepancy between the feedback signal and the complex, fine-grained human evaluation. This limitation is gaining recognition, evidenced by the emergence of RichHF-18k [26] and VisionReward [45]. They assess human preferences along multiple dimensions, yet the evaluation remains at coarse level. Direct preference optimization. Traditional Reinforcement Learning from Human Feedback (RLHF) [2, 30] typically requires the explicit training of reward model [3, 7, 8, 39, 53]. To reduce the overhead, Direct Preference Optimization (DPO) [32] is introduced for language models as stable, RL-free objective, which is successfully adapted to vision by Diffusion-DPO [40]. Subsequent studies primarily focus on refining the optimization process rather than the feedback signal itself. This includes process-guided and step-supervised methods such as SPO [27], D3PO [47], and Dense Reward View [48]; inversion-based approaches such as Inversion-DPO [25] and InPO [28] that enable effi2 cient latent tuning; and trajectory-level optimization methods such as Diffusion-Sharpening [37]. Recently, Negative Preference Optimization (NPO) [51] is explored for unlearning bad concepts in language models. Building upon this idea, Diffusion-NPO [41] and Self-NPO [42] extend the framework to the visual domain by explicitly training negative preference model on switched data pairs. Nevertheless, these methods are all based on coarse-grained scalar or binary reward, and some require the training of an auxiliary negative preference model. Multi-objective optimization. Recent research addresses the one-preference-for-all problem by advancing toward multi-objective optimization, which aims to balIn language modelance conflicting monolithic rewards. ing, MODPO [55] produces Pareto front of models trading off objectives such as helpfulness and harmlessness. This paradigm is extended to vision by CaPO [21], which aligns diffusion models with multiple distinct rewards. Parrot [22] and Preference-Guided Diffusion [1] also pursue Paretooptimal solutions. However, they operate at an aggregated reward to balance different rewards and thus fail to exploit fine-grained attribute information within images. 3. Domain-specific Fine-grained Evaluation Prevailing preference optimization frameworks [6, 20, 32, 40] are founded on simplified evaluation paradigms. They collapse complex, multi-dimensional human evaluation into uni-dimensional signal, such as scalar reward or binary preference. This simplification widens the chasm between the simplified feedback and the granular, complex nature of real-world human cognition [9, 19, 38]. This fundamental limitation of the signal structure inherently restricts the potential for fine-grained model improvement. To bridge this chasm, we first develop new evaluation paradigm imitating expert evaluation. We choose painting generation as our focused domain but our proposed paradigm and method can be easily extended to other scenarios without loss of generality. Collaborating with painting experts, we construct 5-level knowledge hierarchy for evaluation, which comprises 7 root dimensions (including Composition, Color Relations, etc.) and 246 manuallydefined, well-organized pairs of positive/negative attributes. Please refer to our SM for details. We reveal that human evaluation has three features. (1) The evaluation is Multi-dimensional, and experts assess multiple attributes simultaneously. Notice that each of our 7 root dimensions has separate multi-level sub-dimensions to organize attributes. The Composition defines composition category, visual guidance, image richness, visual equilibrium and visual rhythm as sub-dimensions. Again, each sub-dimension has its own children dimensions. Therefore, Multi-Dimension here is also hierarchical. (2) The evaluation language is Discrete; experts tend to employ multiple attributes rather than continuous scores for fine-grained evaluation. (3) The evaluation is Non-Equilibrium, and the applicable set of attributes dynamically shifts with the images content and style. For example, in the sub-dimension of composition category, we have composition of symmetry, asymmetry and geometry as children dimensions. One painting may be of axis-symmetric as leaf attribute of symmetry and also of circular composition as in geometry. However, the painting may fail to break the shape of the circle and thus suffer from negative attribute of close circle without shape breaking. Another painting may be centersymmetric and of radial composition simultaneously, while it may suffer from another negative attribute of ambiguous center because the center to display radial composition is not clear enough. This example shows the applicable attributes vary across different samples (non-equilibrium). Two phenomena pose new challenges to existing posttraining methods. First, negative (Aneg) attributes co-exist with the positive (Apos) in one single painting sample. This requires post-training method to decouple attributes in samples. Second, positive attributes (Apos) can be mutually exclusive when they share the same penultimate subdimension. An example is painting cannot be of upward triangle and circular symmetry simultaneously, and both kinds of symmetry share the same ancestor as geometric composition. This means learned positive attributes vary in each training sample. Existing post-training frameworks to optimize single or multiple utility functions are ill-equipped to process such complex, multi-faceted signals. To operationalize this nuanced understanding, we introduce domain-expert agent that employs DeconstructStructure-Quantify paradigm. This agent leverages our hierarchical knowledge frameworkstructured as 5-level tree with 7 root dimensionsto mimic expert evaluation into prompts. The terminal nodes represent discrete, symbolic semantic labels, explicitly identifying both positive This structure facilitates nonand negative attributes. rather than applying universal equilibrium evaluation: metric, the agent dynamically activates relevant subset of attributes from this extensive knowledge base tailored to the specific image. Utilizing this agent, we annotated 10,277 paintings, creating domain-specific dataset = {(x0, y, Apos, Aneg)}, where x0 is an image of one painting, its prompt, and Apos and Aneg are the sets of positive and negative attributes assigned by the domain-expert agent. By manual investigation, the annotation accuracy is acceptable. Please see SM for more details. 4. Preliminary Diffusion models [13, 36] learn data distributions by reversing gradual noising process. Given clean sample x0 q(x0), forward process progressively adds Gaussian 3 Brushstroke and Texture, Figure 2. The pipeline of our framework. The Domain-Expert Agent decomposes image along 7 dimensions, which are represented as: Color Shape and Posture, relationship, and Edge relationship. Notice that the visualization of the attribute hierarchy in the agent is simplified. The full hierarchy is of 5 levels with 246 attribute pairs in the leaf nodes. Post-annotation, we first conduct SFT to obtain the model θ1. This model is then used to dynamically acquire noise signals that aggregate decoupled attribute information. Subsequently, the aligned model is trained to learn the positive direction while suppressing the negative direction. Perspective and Space, Light and Shadow, Composition, noise to produce sequence x1:T according to q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI), (1) where βt controls the noise schedule. neural network ϵθ(xt, t, c) is trained to approximate the reverse process pθ(xt1xt, c) = (xt1; µθ(xt, t, c), σ2 I), (2) by predicting the injected noise ϵ at timestep with condition c. Training minimizes the expected reconstruction error between true and predicted noise, often expressed as LDM = Ex0,t,c,ϵ (cid:2)ϵ ϵθ(xt, t, c)2 2 (cid:3) . (3) This formulation enables sampling through iterative denoising from pure noise, generating images that are consistent with the given condition. Classifier-Free Guidance (CFG) [12] is cornerstone technique in diffusion models for enhancing conditional control during inference without requiring an explicit classifier. The model is trained to learn both conditional prediction ϵθ(xt, t, c) and an unconditional prediction ϵθ(xt, t, ) by randomly dropping the condition during training. At inference time, the final noise prediction ˆϵ is computed by extrapolating from the unconditional baseline in the direction of the conditional semantics: ˆϵ(xt, t, c) = ϵθ(xt, t, )+ω(ϵθ(xt, t, c)ϵθ(xt, t, )) (4) where ω 0 is the guidance scale. This structure allows for trade-off between sample fidelity (to the condition c) and diversity. Inspired by this, our work leverages similar extrapolation structure to guide the diffusion model in generating outputs that align with positive attributes while avoiding negative attributes. Direct Preference Optimization (DPO) [32] reformulates the reward-learning step of RLHF into direct policy optimization problem. Given preference pairs (c, xw 0), the BradleyTerry model [4] assumes 0 , xl p(xw 0 xl 0c) = σ(r(c, xw 0 ) r(c, xl 0)), (5) where r() is the latent reward. The standard constrained reward maximization is formulated as max pθ Ex0pθ [r(c, x0)] βDKL[pθ(x0c)pref(x0c)], (6) where the hyperparameter β controls regularization. It optimizes conditional generative distribution pθ to maximize the expected reward while regularizing the KL-divergence with respect to reference distribution pref. Noting that the global optimal policy takes the form θ(x0c) pref (x0c) exp (r(c, x0)/β), one can eliminate 4 and obtain direct objective on pθ: 5.1. Domain-specific Knowledge Learning (cid:20) (cid:18) log σ β log c,xw/l 0 = pθ(xw pref(xw 0c) 0c) (7) This loss pushes generative distribution toward preferred outputs while keeping the learned policy not too far from the reference, avoiding potential reward hacking. 0 c) 0 c) pθ(xl pref(xl β log (cid:19)(cid:21) . Extending DPO to diffusion models requires tractable surrogate for the intractable parameterized distribution pθ(x0c), as it requires marginalizing out all possible diffusion paths (x1, . . . , xT ) which lead to x0. To overcome this, Diffusion-DPO [40] reformulates the objective on entire reverse trajectories x0:T rather than just the final samples x0. This yields new theoretical objective: LDiffusion-DPO = (xw 0 ,xl 0)D log σ (cid:18) (cid:104) βExw 1:T pθ(xw 1:T pθ(xl xl pθ(xl pref(xl log 1:T xw 0 ) 1:T xl 0) (cid:105)(cid:19) 0:T ) , 0:T ) log pθ(xw pref(xw 0:T ) 0:T ) (8) Then it uses the ELBO together with an approximation that replaces the intractable reverse posterior by the forward noising process q(x1:T x0). After algebraic simplification and pushing expectations to single timestep t, the training objective reduces to preference-weighted denoising criterion. Writing ϵθ for the models noise prediction and ϵref for the pretrained reference, the practical loss becomes LDiffusion-DPO = 0 ,xl xw 0,t,xtq log σ (cid:16) βT ω(λt)(cid:0)wl(cid:1)(cid:17) , , t)2 2 ϵ ϵref(x (9) with = ϵ ϵθ(x 2. λt = α2 /σ2 represents the signal-to-noise ratio, and ω(λt) denotes weighting function, typically treated as constant [13, 16]. The loss enables preference alignment for diffusion models without extra inference-time cost or unstable RL procedures. , t)2 5. Method Based on above discussion, we find that human evaluation is inherently multi-dimensional, discrete, and nonequilibrium. Existing post-training frameworks [6, 20, 32, 40] for generative models employ simplified signals, insufficient for capturing the intricate, fine-grained evaluation. To address this limitation, we propose novel two-stage learning paradigm  (Fig. 2)  , tailored for injecting and aligning with complex, domain-specific criterion. First, we train the pretrained model to learn the evaluation attributes via Supervised Fine-Tuning (SFT) and thus form domainexpert model. Second, by utilizing our proposed Complex Preference Optimization (CPO), we decouple the learning of positive and negative attributes in the alignment training. Besides, we propose new stabilization strategy. 5 The objective of this stage is to develop an expert model that captures the correlation between training images and attributes defined in our domain-specific preference evaluation. The expert model is text-to-image model parameterized by θ1 and initialized as the pre-trained θ0. Specifically, our training data consists of tuples (x0, y, Apos, Aneg), where x0 is the image, is the content description prompt, and Apos and Aneg are the sets of positive and negative attribute labels, respectively. The learning is conducted with Supervised Fine-Tuning (SFT) to minimize the denoising loss of Eq. (3), where the condition is now union of y, Apos, and Aneg, and ϵ is the sampled ground-truth noise. After fine-tuning, θ1 is aware of domain-specific knowledge. With prompt inputs augmented with Apos and Aneg as auxiliary information during inference, θ1 generates images aligned with explicit textual attribute labels. This model provides the foundation for the subsequent stage of preference learning. 5.2. Complex Preference Optimization This stage performs implicit preference alignment. It trains the final model θ to generate images that conform to the domain-specific positive attributes Apos and eschew the negative Aneg. Here θ is required to use only the content prompt as input. The process decouples bipolar attributes in each sample by distilling knowledge from θ1 into θ. To achieve this, we introduce Complex Preference Optimization (CPO). CPO is built on top of the DiffusionDPO framework [40], an effective off-policy method to align models with human preferences. Instead of static, pre-defined pairs (xw, xl) from preference dataset, CPO leverages the SFT expert model θ1 as dynamic reward oracle. At each denoising step t, noisy sample xt is obtained based on x0. For xt, θ1 generates an ideal (winner) and non-ideal (loser) denoised prediction. These predictions are used to provide fine-grained guidance to θ. Dynamic Process Reward Generation. Using the frozen expert model θ1, we construct two distinct conditional noise predictions at each timestep t, inspired by classifier-free guidance [12]. 1. Winner Noise Prediction (zw) represents the ideal denoising direction. It steers from baseline of negative attributes toward the desired positive attributes. We define the positive conditioning cpos = (y, Apos) and the negative conditioning cneg = (Aneg). The winner noise zw is: zw(xt, t) = (1 ωw)ϵθ1(xt, cneg, t) + ωwϵθ1(xt, cpos, t). (10) 2. Loser Noise Prediction (zl) represents the non-ideal direction. It steers from an unconditional baseline toward the explicitly negative attributes. We define call = propose to approximate pθ(x1:T x0) with pθ1(x1:T zw ) for ˆxw 0 , which applies to ˆxl 0 similarly. Conceptually, since the sampling processes are deterministic, the two approximated reverse trajectories overlap in xt again. Third, for training efficiency, we focus on the training on xt only rather than all intermediate results on the trajectories. Since sampling iteratively with zw guides xt to the winning ˆxw 0 , zw is defined as the target of ϵθ(xt, t) at step t. This is the same for zl. detailed derivation with approximated KL divergence is in SM Sec. S7. By incorporating these defined targets, we formulate the CPO loss LCP to optimize θ: LCP O(θ) = x0D,tU(0,T ),zw zw ϵθ(xt, t)2 log σ(βT ω(λt)( ,zl 2 zw ϵref (xt, t)2 2 2))) 2 zl ϵref (xt, t) (zl ϵθ(xt, t)2 (12) Note ϵθ is only conditioned on the content prompt y. This objective explicitly encourages ϵθ to minimize its error relative to the preferred noise zw and, conversely, to maximize its error relative to the dispreferred noise zl. This mechanism allows the model to implicitly learn the positive attributes and unlearn the negative ones, decoupling attributes without requiring Apos or Aneg at inference time. 5.3. Stabilization of the Optimization Empirically, we observed that training with the standard DPO-style objective suffers from instabilities. We attribute this to the imbalance of winning and losing parts in optimization. The losing term zl ϵθ(xl 2 is innately concave, and the resulting gradient norm grows as the training proceeds. However, the winning term zw ϵθ(xw , t)2 2 is convex instead, and its gradient norm shrinks. Therefore, the gradient norm of the losing term grows disproportionately compared to the winning. Such phenomenon also applies to other methods based on DiffusionDPO. t, t)2 To address this, we stabilize our CPO objective by transforming the original loss with another term. The aim is to ensure the gradient of the losing term is equal to that of the winning term. This ensures that the optimization landscape remains stable and that the gradients from the winner and loser terms are balanced. Specifically, we define zltgt = ϵθ(xt, t) + ϵθ(xt, t) zl ϵθ(xt, t) zl ϵθ(xt, t) zw. (13) Our stabilized objective, LCP OS, is formulated as: x0D,tU(0,T ),zw LCP OS(θ) = βT ω(λt)(zw ϵθ(xt, t)2 + (zltgt ϵθ(xt, t)2 log σ( 2 zw ϵref (xt, t)2 2 2))). 2 zltgt ϵref (xt, t)2 ,zl (14) Figure 3. Illustration of the CPO sampling trajectory. At each timestep t, CPO employs the expert model θ1 to provide deterministic positive and negative noise guidance, directing the trajectory toward virtual winning and losing samples, respectively. Owing to the determinism of the noise trajectory, the final sample x0 can be precisely reconstructed back to xt. Compared with original DPO, this design enables process-level guidance for model training rather than relying solely on the final endpoints, thereby making the training process more efficient. (y, Apos, Aneg) and cnull = (). The loser noise zl is: zl(xt, t) = (1 ωl)ϵθ1(xt, cnull, t) + ωlϵθ1(xt, call, t). (11) Here, ωw and ωl are hyperparameters both greater than 1. CPO Objective The Diffusion-DPO approximates the intractable reverse process from labeled sample to posterior sample trajectory pθ(x1:T x0) with the forward noising process q(x1:T x0). This approximation, while necessary, inevitably introduces errors with stochastic noise ϵw and ϵl drawn from as in Eq. (9). Instead, in CPO we substitute the target noise to zw and zl. The rationale behind this substitution is stated as follows and is illustrated in Fig. 3. First, suppose given noisy xt, deterministic sampling process is conducted again with zw iteratively that biases toward positive away from negative attributes from θ1. The obtained ˆxw 0 would have less negative evaluation than the original x0. This is similar to ˆxl 0 sampled from zl. Therefore, we assume ˆxw 0 is more 0. Second, given ˆxw preferrable than ˆxl 0 as the winning and losing samples for DPO, the posterior trajectory is still intractable for pθ(x1:T x0). Instead of using q, we 0 and ˆxl 6 Table 1. Quantitative results of SDXLand FLUX-based methods on metrics evaluating attribute (#A neg), quality (FID), and preference (the latter four metrics). LCPO and LCPOS denote the training results without and with stabilization. denotes the comparison between our CPO, which does not require training negative model, and NPO, which necessitates additional negative reward training. Method #A neg (avg) FID PickScore HPSv2 ImageReward Aesthetic SDXL SDXL-DPO SDXL-SPO SDXL-CPO (LCPO) SDXL-CPO (LCPOS) SDXL+NPO SDXL-DPO+NPO SDXL-CPO+NPO FLUX FLUX-DPO FLUX-CPO 5.840 5.790 5.770 5.210 5. 5.210 5.630 5.070 5.120 4.400 3.780 89.48 93.12 88.53 88.07 87.37 84.88 86.93 79.13 95.69 104.79 104.71 0.1963 0.2080 0.2081 0.2088 0. 0.2120 0.2084 0.2118 0.2005 0.2113 0.2113 0.2646 0.2906 0.2911 0.2918 0.3039 0.2786 0.2960 0.2989 0.2853 0.3210 0.3212 0.5180 0.9194 0.9200 0.9255 0. 0.8729 0.9992 0.9784 0.8696 1.1516 1.1526 6.210 6.571 6.577 6.581 6.581 6.541 6.539 6.591 6.460 6.864 6.865 In the implementation, we apply stop-gradient (detachment) operation to zltgt. In this case, the direction of the gradient backward to ϵθ(xl t, t) is the same as the original loss but the norm is restricted to zw ϵθ(xw , t). more detailed derivation and analysis can be found in Sec. S5 of the Supplementary Material. This stabilization ensures that the loser terms contribution to the gradient is balanced with that of the winner term. Theoretically, surrogate convex term is used to substitute the original concave term, leading to significantly more robust convergence as shown in empirical results. 6. Experiment 6.1. Dataset and Implement We collect 10,277 diverse publicly available paintings with automated filtering and manual inspection. The dataset is randomly split into 8,221 (80%) / 1,028 (10%) / 1,028 (10%) images for training, validation, and testing. With this dataset, we train our models in two stages. In Stage 1, we perform supervised fine-tuning on the base model using LoRA [14] over the full dataset. Each training instance concatenates the base prompt with its positive and negative labels into single textual input. We use LoRA rank of 16, learning rate of 1e-4, and train for two epochs. In Stage 2, for SDXL, we follow the Diffusion-DPO training configuration [40] to ensure fair comparison, training for 8,221 steps (one epoch). For FLUX, we apply LoRA-based posttraining with reduced rank of 8, keeping the 1e-4 learning rate and setting the LoRA scaling factor β to 0.1. All experiments are conducted on single NVIDIA H800 GPU. 6.2. Evaluation and Baselines We introduce #A neg as new metric, quantifying the presence of negative attributes identified by our domain-expert agent (Sec. 3) and averaged over 300 images. We also conduct evaluation on existing metrics for general image quality, aesthetics, and human preference, including FID [23], PickScore [17], HPSv2 [44], ImageReward [46], and Aesthetic Score [33]. We compare our CPO against baseline methods on both SDXLand FLUXbased models. Baselines include the fine-tuned SDXL [31] and FLUX [18] as well as Diffusion-DPO [41], SPO [27], and their NPOaugmented [28] variants. We also report our non-stabilized objective, SDXL-CPO (LCP O), as an ablation result. 6.3. Quantitative Result As shown in Tab. 1, our CPO demonstrates clear superiority. On the primary SDXL group, our stabilized method SDXLCPO (LCPOS) excels in avoiding negative attributes, significantly reducing #A neg to 5.180. Critically, this reduction does not compromise quality: our method simultaneously secures the best FID (87.37) and joint-highest preference scores. We report the number of negative rather than positive attributes. This is because human expertise is inherently non-equilibrium(Sec. 3), meaning images are assessed under varying criteria. Consequently, high #A pos does not necessarily indicate better image quality. In contrast, the presence of negative attributes is consistently undesirable, making #A neg more reasonable evaluation metric. Compared with CPO, NPO [28] requires additional training of negative reward model. NPO underperforms CPO on most metrics (see the two rows marked with in Tab. 1). When all methods are further trained with NPO, our CPO still demonstrates superior overall performance, showing only lower scores on PickScore and ImageReward. 7 Figure 4. Visual comparison of different baselines and our CPO. #A neg () and PickScore () are annotated in the lower-left and lowerright corners of each image, respectively. CPO outperforms all baselines in both negative-attribute avoidance and preference scoring. CPO generalizes robustly to other architectures. On the FLUX-based model, FLUX-CPO achieves #A neg score of 3.780, dramatic improvement over both the FLUX baseline (5.120) and FLUX-DPO (4.400). It also achieves the best results in preference scores. The increase in FID is unavoidable following fine-tuning, which is also reported in existing research [35] [11] [43] [24], and this effect is particularly pronounced within FLUX [5]. 6.4. Qualitative Result Fig. 4 illustrates the visual performance of our CPO compared to baseline models. Each row shows an input prompt and the corresponding generated images. Images generated by CPO exhibit the fewest negative attributes (marked in red), which is also evident from the last columnour results consistently demonstrate superior composition, color harmony, light and shadow, and brushstroke quality. CPO further tends to achieve higher preference scores, for which we report the PickScore (marked in grey) as an instance. 6.5. Ablation Study We conduct ablations to validate our framework, including the necessity of our fine-grained, attribute-decoupled reward design, the impact of the training data volume, and the effectiveness of the stabilization strategy. Impact of Reward Granularity. As shown in Tab. 2, we compare our 7-dimensional complex reward against two coarser-grained reward structures. Scalar denotes normalizing and averaging the 7 dimensions into single score. And binary denotes simplifying each of the 7 dimensions into winning/losing label. The results clearly indicate that model performance scales directly with the granularity of the feedback signal. Our complex reward performs the best across all metrics. This finding compellingly demonstrates that our proposed complex preference optimization is critical for achieving desired alignment. Impact of Data Proportion. In Tab. 3, we analyze the effect of data volume by training with varying proportions of our attribute-decoupled dataset. The results show significant improvement as the dataset size increases. Effectiveness of stabilization strategy. Fig. 5 plots the values of the winning and losing parts in Eq. (12) (LCPO) over training steps. The winning part is zw ϵθ(xt, t)2 2 zw ϵref (xt, t)2 2 and the losing is similar. Given our stabilization, both parts exhibit markedly smoother and more stable decrease, whereas the loss without stabilization undergoes substantial oscillations. Notice that the winning part is expected to be minimized while the losing is maximized. The joint change of both loss term is known as gradient entanglement [50] and widely observed. Here, the original loss emphasizes more on the unlearning the losing but fail to optimize the winning part. Our stabilization allows the optimization to emphsize on learning the positive attrbutes over unlearning negative ones. The superior performance of our LCPOS over LCPO (refer to Tab. 1) also confirms the efficacy of our stabilization strategy. 7. Conclusion We aim to address the reliance on simplified feedback in preference alignment, introducing hierarchical, fine8 Table 2. Comparison of different reward designs. Scalar and Binary denote scalar scorebased and binary preferencebased optimization, respectively, while Complex represents our fine-grained, attribute-decoupled preference optimization. Reward #AN FID PS HPS IR Scalar Binary Complex 5.840 5.270 5.180 91.99 87.43 87.37 0.1959 0.2080 0.2083 0.2649 0.2921 0.3039 0.5194 0.9296 0.9312 LA 6.239 6.577 6.581 Table 3. Ablation study under different proportions (Prop.) of attribute-decoupled training data. AN, PS, IR, and LA denote #A neg, PickScore, ImageReward, and LAION-Aesthetic. Prop. #AN FID PS HPS IR 5.770 10% 5.750 20% 50% 5.530 100% 5.180 89.39 89.37 86.61 87.37 0.2066 0.2073 0.2074 0.2083 0.2905 0.2914 0.2917 0. 0.9122 0.9266 0.9311 0.9312 LA 6.562 6.566 6.566 6.581 Figure 5. Curves of the win and lose parts of the loss function over training steps. The configuration with stabilization demonstrates significantly greater stability compared to the one without. grained evaluation criterion with positive and negative attributes. Based on this, we propose two-stage alignment with stabilization strategy to learn complex expertise. Experiments demonstrate CPO outperforms existing baselines."
        },
        {
            "title": "References",
            "content": "[1] Yashas Annadani, Syrine Belakaria, Stefano Ermon, Stefan Bauer, and Barbara Engelhardt. Preference-guided diffusion for multi-objective offline optimization. Advances in Neural Information Processing Systems, 2025. 3 [2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, 9 Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862, 2022. 2 [3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. 2 [4] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [5] Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Song Han, and Enze Xie. Sana-sprint: One-step diffusion with continuous-time conIn Proceedings of the IEEE/CVF Insistency distillation. ternational Conference on Computer Vision (ICCV), pages 1618516195, 2025. 8 [6] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017. 3, 5 [7] Luca Eyring, Shyamgopal Karthik, Karsten Roth, Alexey Dosovitskiy, and Zeynep Akata. Reno: Enhancing one-step text-to-image models through reward-based noise optimization. In Advances in Neural Information Processing Systems, pages 125487125519, 2024. 2 [8] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. In Advances in Neural Information Processing Systems, pages 7985879885, 2023. 2 [9] David Freedman, Maximilian Riesenhuber, Tomaso Poggio, and Earl Miller. Categorical representation of visual stimuli in the primate prefrontal cortex. Science, 291(5502): 312316, 2001. 3, 12 [10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 20 [11] Feng He, Zhenyang Liu, Marco Valentino, and Zhixue Zhao. How robust is model editing after fine-tuning? an empirical study on text-to-image diffusion models. arXiv preprint arXiv:2506.18428, 2025. [12] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 4, 5 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, pages 68406851, 2020. 3, 5 [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 7 33rd ACM International Conference on Multimedia, pages 99019910, 2025. 2, 19 [15] Seungjae Jung, Gunsoo Han, Daniel Wontae Nam, and Kyoung-Woon On. Binary classifier optimization for large language model alignment. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18581872, 2025. 20 [16] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in Neural Information Processing Systems, 34:2169621707, 2021. 5 [17] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In Advances in Neural Information Processing Systems, pages 3665236663, 2023. 2, 7, [18] Black Forest Labs. https : / / blackforestlabs.ai/, 2024. Accessed: September 19, 2025. 7 Flux. [19] Helmut Leder, Benno Belke, Andries Oeberst, and Dorothee Augustin. model of aesthetic appreciation and aesthetic judgments. British journal of psychology, 95(4):489508, 2004. 3, 12 [20] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning textarXiv preprint to-image models using human feedback. arXiv:2302.12192, 2023. 3, [21] Kyungmin Lee, Xiahong Li, Qifei Wang, Junfeng He, Junjie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin, Feng Yang, and Yinxiao Li. Calibrated multi-preference optimization for aligning diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 18465 18475, 2025. 3 [22] Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han Zhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Junfeng He, et al. Parrot: Pareto-optimal multi-reward reinforceIn ment learning framework for text-to-image generation. European Conference on Computer Vision, pages 462478. Springer, 2024. 3 [23] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, Minguk Kang, Taesung Park, Jure Leskovec, Jun-Yan Zhu, Fei-Fei Li, Jiajun Wu, Stefano Ermon, and Percy Liang. Holistic evaluation of text-to-image models. In Advances in Neural Information Processing Systems, pages 6998170011, 2023. 7, 16 [24] Zejian Li, Chenye Meng, Yize Li, Ling Yang, Shengyuan Zhang, Jiarui Ma, Jiayi Li, Guang Yang, Changyuan Yang, Zhiyuan Yang, et al. Laion-sg: An enhanced large-scale dataset for training complex image-text models with structural annotations. arXiv preprint arXiv:2412.08580, 2024. 8 [25] Zejian Li, Yize Li, Chenye Meng, Zhongni Liu, Ling Yang, Shengyuan Zhang, Guang Yang, Changyuan Yang, Zhiyuan Yang, and Lingyun Sun. Inversion-dpo: Precise and efficient In Proceedings of the post-training for diffusion models. [26] Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1940119411, 2024. 2 [27] Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, and Liang Zheng. Aesthetic post-training diffusion models from generic preferences with step-by-step preference optimization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1319913208, 2025. 2, 7 [28] Yunhong Lu, Qichao Wang, Hengyuan Cao, Xierui Wang, Xiaoyin Xu, and Min Zhang. Inpo: Inversion preference optimization with reparametrized ddim for efficient diffusion model alignment. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2862928639, 2025. 2, 7, 19 [29] Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1508615095, 2025. 2 [30] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions In Advances in Neural Information with human feedback. Processing Systems, pages 2773027744. Curran Associates, Inc., 2022. 2 [31] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. ArXiv, abs/2307.01952, 2023. [32] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. 1, 2, 3, 4, 5, 18 [33] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems, pages 2527825294, 2022. 2, 7, 16 [34] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 1 feedback to fine-tune diffusion models without any reward In Proceedings of the IEEE/CVF Conference on model. Computer Vision and Pattern Recognition, pages 8941 8951, 2024. 2 [48] Shentao Yang, Tianqi Chen, and Mingyuan Zhou. dense reward view on aligning text-to-image diffusion with preferIn Proceedings of the 41st International Conference ence. on Machine Learning, pages 5599856032. PMLR, 2024. 2 [49] Shunyu Yao. The second half: Ais transition from problemsolving to problem-defining, 2025. Accessed on 11th November, 2025. 1 [50] Hui Yuan, Yifan Zeng, Yue Wu, Huazheng Wang, Mengdi Wang, and Liu Leqi. common pitfall of margin-based language model alignment: Gradient entanglement. In International Conference on Learning Representation, 2025. 8 [51] Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catastrophic collapse to effective unlearning. In First Conference on Language Modeling, 20224. [52] Shengyuan Zhang, Ling Yang, Zejian Li, An Zhao, Chenye Meng, Changyuan Yang, Guang Yang, Zhiyuan Yang, and Lingyun Sun. Distribution backtracking builds faster conIn The Thirvergence trajectory for diffusion distillation. teenth International Conference on Learning Representations, 2025. 20 [53] Xinchen Zhang, Ling Yang, Guohao Li, YaQi Cai, Yong Tang, Yujiu Yang, Mengdi Wang, Bin CUI, et al. Itercomp: Iterative composition-aware feedback learning from model gallery for text-to-image generation. In The Thirteenth International Conference on Learning Representations, 2025. 2 [54] Kaiwen Zheng, Yongxin Chen, Huayu Chen, Guande He, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Direct discriminative optimization: Your likelihood-based visual generative model is secretly gan discriminator. In Forty-second International Conference on Machine Learning, 2025. 20 [55] Zhanhui Zhou, Jie Liu, Jing Shao, Xiangyu Yue, Chao Yang, Wanli Ouyang, and Yu Qiao. Beyond one-preference-fits-all alignment: Multi-objective direct preference optimization. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1058610613, Bangkok, Thailand, 2024. Association for Computational Linguistics. 3 [35] Guibao Shen, Luozhou Wang, Jiantao Lin, Wenhang Ge, Chaozhe Zhang, Xin Tao, Yuan Zhang, Pengfei Wan, Zhongyuan Wang, Guangyong Chen, et al. Sg-adapter: Enhancing text-to-image generation with scene graph guidance. arXiv preprint arXiv:2405.15321, 2024. 8 [36] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. [37] Ye Tian, Ling Yang, Xinchen Zhang, Yunhai Tong, Mengdi Wang, and Bin Cui. Diffusion-sharpening: Fine-tuning diffusion models with denoising trajectory sharpening. arXiv preprint arXiv:2502.12146, 2025. 3 [38] Anne Treisman and Garry Gelade. feature-integration theory of attention. Cognitive psychology, 12(1):97136, 1980. 3, 12 [39] Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex Tseng, Sergey Levine, and Tommaso Biancalani. Feedback efficient online fine-tuning of diffusion models. In Proceedings of the 41st International Conference on Machine Learning, pages 4889248918. PMLR, 2024. 2 [40] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 2, 3, 5, 7, 12, 18, 20 [41] Fu-Yun Wang, Yunhao Shui, Jingtan Piao, Keqiang Sun, and Hongsheng Li. Diffusion-NPO: Negative preference optimization for better preference aligned generation of diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. 3, 7, 12 [42] Fu-Yun Wang, Keqiang Sun, Yao Teng, Xihui Liu, Jiaming Song, and Hongsheng Li. Self-npo: Negative preference optimization of diffusion models by simply learning from itself without explicit preference annotations. arXiv preprint arXiv:2505.11777, 2025. [43] Maorong Wang, Jiafeng Mao, Xueting Wang, and Toshihiko Yamasaki. Reward incremental learning in text-to-image generation. arXiv preprint arXiv:2411.17310, 2024. 8 [44] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 2, 7, 16 [45] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. 2 [46] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for text-to-image generation. In Advances in Neural Information Processing Systems, 2024. 2, 7, 16 [47] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human"
        },
        {
            "title": "Supplementary Material",
            "content": "S1. Description of the Fine-grained Hierarchical Evaluation As referenced in Section 3 of the main paper, our domainspecific fine-grained evaluation system operates based on 5-level knowledge hierarchy comprising 7 root dimensions and comprehensive set of 246 attribute pairs, assessing multiple aspects such as Composition, Color Relationships, and Brushstrokes & Texture. The complete structure is visualized in Fig. S6. This structure underpins that human evaluation is inherently multi-dimensional, discrete, and non-equilibrium. The fine-grained evaluation system serves as the foundational knowledge base for our Complex Preference Optimization (CPO) framework, addressing the limitations of coarse, simplified feedback signals used in prevailing alignment methods. This hierarchical paradigm provides critical advantage over standard text-based evaluation, which relies on monolithic image-level reward signals. The core difference lies in the granularity and bidirectional control. Our system explicitly encodes knowledge spanning seven root dimensions and five hierarchical levels, encompassing specialized subdimensions such as Visual Guidance under Composition and Light Aspect/Quality under Light and Shadow. This fine-grained evaluation scheme enhances the models capacity to perceive and learn domain-specific knowledge. Most importantly, it enables decoupled supervision by providing separate positive (Apos) and negative (Aneg) attribute sets for the same image. This design is essential because negative attributes often coexist with positive onesan image is rarely uniformly good or bad across all aspects. Such granularity and explicit bidirectional control allow CPO to learn complex expert criteria, delivering precise, attribute-level guidance that cannot be achieved by monolithic rewards derived from simple text prompts. S2. Description of Complex Preference Learning Tasks Human cognitive evaluation is inherently not highly regularized process. By collaborating with domain experts to construct the evaluation criteria, we observe that human evaluation is inherently multidimensional, discrete, and non-equilibrium, which is consistent with findings reported in prior research [9, 19, 38]. For example, as illustrated in Fig. S7, an expert evaluating two paintings images may identify one (Fig. S7(a)) as having positive Focal Point Composition but negtive Blurred edges, while another (Fig. S7(b)) exhibits Clear edges yet suffers from Soft and hard light. Besides, the evaluation labels of Fig. S7(c) vary with changes in content and style, reflecting the non-equilibrium of human evaluation. Furthermore, complex, discrepancy and non-equilibrium mean that multidimensional reward functions should not be used for simple scoring, and the multi-dimensional nature is not suited for directly assigning single notion of superiority or inferiority. Therefore, it is essential to develop new paradigm aligning with human evaluation and to formulate corresponding algorithms. S3. User Study To validate whether our proposed CPO (Complex Preference Optimization) method can generate images more aligned with complex human perception than baseline methods (Diffusion-DPO [40], Diffusion-NPO [41]), we design and execute user study. The core purpose of this study is to compare the subjective visual quality of images generated by different models from professional perspective. We first randomly sample 150 prompts from the test set. Subsequently, we use these prompts and feed them separately into the following four trained models: SDXLDPO+NPO, SDXL-CPO+NPO, FLUX-DPO, FLUX-CPO, generating total of 600 images for evaluation. In each trial, participants observe two groups (G1, G2) of images generated from the same prompt. Group G1 (SDXL Base) contain an image generated by SDXL-DPO+NPO and an image generated by SDXL-CPO+NPO. Group G2 (FLUX Base) contain an image generated by FLUX-DPO and an image generated by FLUX-CPO. Participants are asked to base their comparison on 7 pre-defined root dimensions from Domain-Expert Agent knowledge as criteria, and to conduct pairwise comparisons on the image pairs in Group G1 and Group G2, respectively. They have to select the superior image from the two under each dimension (7 comparisons in total). We recruit total of 10 participants, with an age distribution between 20 and 30. All participants have (or are pursuing) professional background in art or design, ensuring they possess the professional judgment ability for the aesthetic standards of oil paintings. The results of the user study are shown in Fig. S8. The data shows that in the SDXL-based comparison (G1 group), 63.5% of the preference is given to the images generated by our SDXL-CPO+NPO. In the FLUXbased comparison (G2 group), the FLUX-CPO method obtain user preference as high as 84.1%. Whether based on the SDXL or FLUX base model, our CPO achieve significantly higher user preference rate in direct comparison with the DPO baseline. This result strongly proves that our proposed method has significant superiority in optimizing complex human preferences and enhancing the subjective perceptual quality of generated images. Notice to Human Subjects. We issued notice to sub12 Figure S6. Illustration of the domain-specific fine-grained evaluation framework. Best viewed magnified on screen. jects to inform them of data collection and use before the experiment: Dear volunteers, thank you for your support of our research. We are researching an image generation algorithm based on Complex Preference Optimization (CPO) and applying it to the generation of oil paintings. All information related to your participation in the study will be displayed in the research records. All information will be processed and stored according to local laws and policies on privacy. Your name will not appear in the final report. When mentioning the data you provide, only the individual number assigned to you will be mentioned. We respect your decision whether to volunteer for this study. If you decide to participate in this study, you can sign this informed consent form. The use of user data has been approved by the Institutional Review Board of the primary authors institution. S5. Additional Explanation on Stabilization"
        },
        {
            "title": "Strategy",
            "content": "S5.1. Effectiveness Analysis To assess the effectiveness of our stabilization strategy, we visualize the evolution of the winning term, the losing term, and the overall loss over training steps under both the withand without-stabilization settings, as shown in Fig. S11. Fig. S11 (b) shows the gap between the positive (winning) and negative (losing) parts. Unlike conventional DPO-style objectives that intentionally enlarge this margin, our method does not aggressively push the positivenegative separation and instead adopts more balanced and stable approach. Classical DPO explicitly aims to maximize this margin, but doing so often comes at the cost of degrading the models fit on both positive and negative samples, as illustrated by the blue curves in Fig. S11 (a), thereby sacrificing the models learning behavior on desirable positive samples. In contrast, we argue that the optimization should also account for how well the model fits the positive samples. As shown by the red curves in Fig. S11 (a), our stabilized training achieves noticeably lower winning-term loss, indicating stronger learning of positive attributes. Ideally, the optimization should move in direction where the model improves its fit on positive samples while deteriorating its fit on negative samples. Although our method represents meaningful step toward this objective, it does not yet fully achieve this ideal separation. We regard this as an important direction for future work. Figure S7. Description of tasks targeted by CPO. Image (a) and (b) are generated from the same prompt, yet each exhibits its own strengths and weaknesses; thus, it is inappropriate to generalize that either image is universally superior. Image (c), generated from different prompt, should be evaluated using criteria distinct from those applied to (a) and (b). S4. More Qualitative Results S5.2. Gradient Analysis Qualitative Results of CPO. Fig. S9 shows the visual performance of different training methods in artistic style generation tasks, including SDXL, DPO, NPO, and CPO combined with NPO (CPO+NPO). The results indicate that CPO+NPO consistently produces the fewest negative attributes across all examples. CPO+NPO also achieves the highest PickScore, clearly outperforming baseline methods. CPO produces images with more natural, precise brushwork, light and shadow, and style consistency, particularly in the swirling sky of Van Goghs style, the halo effect in Monets night scene, and the dramatic lighting in the Baroque portrait. Qualitative Results of Stabilization Strategy. Fig. S10 shows the effect of the stabilization strategy before and after implementation. The results show that the strategy reduces negative attributes across all examples. The stabilization strategy also improves the overall PickScore. In terms of details, the still life shows more coherent light and shadow, the Impressionist figure has better harmony in lighting and skin tone, and the Post-Impressionism harbor displays more stable color blocks and water reflections. To theoretically justify the effectiveness of our stabilization strategy, we analyze the gradient behavior of the proposed objective. Let Lwin = zw ϵθ(xt, t)2 2 and Llose = zl ϵθ(xt, t)2 2 denote the winner and loser terms in the original CPO objective, respectively. The gradient of the original loser term with respect to the model output ϵθ is derived as: ϵθ Llose = 2(ϵθ zl), which directs the optimization to push ϵθ away from the negative prototype zl. its magnitude ϵθ Llose2 = 2ϵθ zl2 grows unbounded as the model successfully unlearns the negative attributes, leading to gradient dominance over the winner term. However, (S15) In our stabilized objective LCP OS, we introduce the surrogate target zltgt. Treating zltgt as fixed target (via stop-gradient), the gradient of the new loser term Lstab = zltgt ϵθ2 2 is: ϵθ Lstab = 2(zltgt ϵθ). (S16) Substituting the definition of zltgt = ϵθ + ϵθzl ϵθzl2 ϵθ 14 SDXL-DPO+NPO SDXL-CPO+NPO FLUX-DPO FLUX-CPO Still life with bottle and fruit, in the style of Expressionism, inspired by Karl Schmidt-Rottluff, bold brushwork, vibrant color, simplified form, textured surface, oil painting Peasants resting under trees, in the style of Rococo, inspired by Watteau, pastoral, rustic, rural life, outdoor gathering, warm light, oil painting Please use the following 7 dimensions as criteria to conduct pairwise comparisons for the image pairs in Group G1 and Group G2, respectively. For each dimension, select the image that performs better: Brushwork and Texture Generation, Edge Relationship Generation, Composition Generation, Light and Shadow Generation, Color Relationship Generation, Perspective and Space Generation, and Shape and Form Generation. Group Model G1 G2 SDXL-DPO+NPO SDXL-CPO+NPO FLUX-DPO FLUX-CPO User Preference 36.5% 63.5% 15.9% 84.1% Figure S8. The result of user study. Top: Qualitative comparison of images generated by different methods using the same prompt. Bottom: Quantitative results from the user study showing preference rates for our CPO methods against DPO baselines across two base models (SDXL and FLUX). zw2, we obtain: ϵθ Lstab = (cid:18) ϵθ zl ϵθ zl2 ϵθ zl ϵθ zl2 (cid:124) (cid:123)(cid:122) (cid:125) Direction (cid:19) ϵθ zw2 ϵθ zw2 (cid:123)(cid:122) (cid:125) (cid:124) Magnitude . (S17) = 2 This derivation reveals two critical properties as shown in Fig. S12: (1) Directional Consistency: The gradient direction aligns with (ϵθ zl), which is identical to the original repulsive force in Llose, ensuring the model continues 15 to unlearn negative attributes. (2) Magnitude Normalization: The gradient norm is rescaled to 2ϵθ zw2. This explicitly matches the magnitude of the winner terms gradient ϵθ Lwin2, guaranteeing balanced optimization landscape throughout the training process. S6. Ablation Study of the Dynamic Process Reward Parameter ω We conduct an ablation study on the guidance strength hyperparameters ωw and ωl in CPO. For simplicity, we set ωw = ωl = ω and evaluate ω {1.0, 1.5, 2.0, 2.5, 3.0} on Figure S9. Visual comparison of different baselines and our CPO. #A neg () and PickScore () are annotated in the lower-left and lowerright corners of each image, respectively. CPO outperforms all baselines in both negative-attribute avoidance and preference scoring. Figure S10. Visual comparison of different baselines and our CPO. #A neg () and PickScore () are annotated in the lower-left and lowerright corners of each image, respectively. CPO outperforms all baselines in both negative-attribute avoidance and preference scoring. the same test set, keeping all other hyperparameters fixed. Evaluation metrics are identical to those in the main paper: #Aneg, FID [23], PickScore [17], HPSv2 [44], ImageReward [46], and Aesthetic Score [33]. Results are shown in 16 (a) Visualization of the winning and losing parts of the loss function. Figure S12. Illustration of the function transformation in the stabilization strategy. It transforms the originally concave losing term into an equivalent convex formulation. The transformed term preserves the direction of the original losing term, but its optimization magnitude is matched to that of the winning term, ensuring stability during training. Table S4. Ablation study under different ω. AN, PS, IR, and LA denote #A neg, PickScore, ImageReward, and LAION-Aesthetic. ω 1.0 1.5 2.0 2.5 3. #AN FID PS HPS IR 5.260 5.230 5.180 4.940 4. 86.61 87.25 87.37 88.91 90.18 0.2179 0.2172 0.2083 0.2171 0.2170 0.2819 0.2837 0.3039 0.2865 0.2871 0.9064 0.9133 0.9312 0.9367 0.9437 LA 6.575 6.584 6.581 6.599 6. (b) Visualization of the overall trend of the loss function. ω = 2.0 as the default parameter, which achieves the best balance between the number of negative attributes (5.18) and multiple human preference metrics. Figure S11. Curves of the separated winning and losing parts of the loss function, together with the overall loss trend, under the withand without-stabilization settings over training steps. The loss used in (b) corresponds to Eq(12) in the main paper. S7. Additional Details on CPO S7.1. Trajectory Description Table S4. Experimental results indicate that as ω increases from 1.0 to 3.0, the average number of negative attributes in the generated images decreases monotonically from 5.26 to 4.87, confirming that enhanced guidance strength effectively suppresses the generation of negative attributes. However, the FID increases from 86.61 to 90.18, indicating that excessively strong guidance may impair the visual quality of the generated images. In terms of human preference evaluation, ImageReward and Aesthetic scores show continuous improvement with increasing ω, while PickScore and HPSv2 achieve an optimal balance at ω = 2.0. Considering the trade-off between negative attribute suppression and visual quality preservation, we ultimately select Further elaborating on Section 5.2, our Complex Preference Optimization (CPO) objective fundamentally addresses core computational difficulty faced by standard Direct Preference Optimization (DPO). Methods like DPO attempt to compare the likelihoods pθ(xwy) versus pθ(xly), which involves computing the probabilities over the entire reverse process pθ(x1:T x0). This calculation is intractable in practice, necessitating approximations by the forward qθ(x1:T x0) that introduce inherent errors and inefficient training. As illustrated in Fig. 3, CPO circumvents this by operating in the latent space and leveraging the auxiliary model θ1 to construct deterministic and controllable preference trajectories. This does not necessarily imply smaller propagation error, but the error becomes controllable and exploitable, thereby enabling more efficient training. For any given real image x0 and its prompt y, the image 17 is first diffused to shared noisy state xt. From this identical starting point xt, our method deterministically samples two reverse trajectories: the positive trajectory zw 1:T and the negative trajectory zl 1:T . The positive trajectory is guided by the ideal fine-grained condition (y and Apos), while the negative trajectory is guided by the undesirable state (y, Apos, and Aneg), representing the attributes we aim to suppress. The central advantage of this construction is that both trajectories are precisely engineered to reconstruct at the same noisy state xt. This shared starting point xt ensures that CPO focuses its optimization effort precisely on the diverging steps immediately following xt, providing deterministic and explicit positive or negative gradient at every time step t. This contrasts sharply with original DPO, which only utilizes the final endpoints xw 0, leaving the intermediary trajectory random and intractable, thereby relying on approximations that inherently introduce uncertainty and inefficiency. 0 and xl S7.2. Mathematical Derivations Diffusion-DPO [40] adapts the Direct Preference Optimization (DPO) [32] framework to the text-to-image diffusion models. The core challenge lies in the intractability of the conditional distribution pθ(x0c) in diffusion models, where x0 is the final generated image and is the text prompt. This is because pθ(x0c) requires marginalizing over all possible diffusion paths x1:T . To address this, Diffusion-DPO leverages the Evidence Lower Bound (ELBO) and reformulate the problem to operate on the full diffusion path x0:T = (x0, x1, . . . , xT ). This leads to new training objective: LDiffusion-DPO = (xw 0 ,xl 0)D log σ (cid:18) (cid:104) βExw 1:T pθ(xw xl 1:T pθ(xl pθ(xl pref(xl log 1:T xw 0 ) 1:T xl 0) (cid:105)(cid:19) 0:T ) . 0:T ) log pθ(xw pref(xw 0:T ) 0:T ) (S18) The loss in Eq. (S18) remains intractable due to the expectation over the reverse process pθ(x1:T x0, c), which involves untrainable path variables. To achieve efficient gradient-based optimization, we make key approximations. Specifically, we substitute the intractable reverse process pθ(x1:T ) with the tractable deterministic trajectories pθ1 (x1:T ). As shown in Fig. 3 (a), given the noise zt at the current timestep t, we can derive the predicted ˆx0 according to the principles of diffusion models: ˆx0 = 1 αt (xt σt zt) . (S19) Given ˆx0 and zt, we can reconstruct xt exactly, thereby making the trajectory pθ1 (x1:T ) accessible. xt = αt ˆx0 + σtzt (S20) By applying this approximation and substituting the loglikelihood ratio with the KL-divergence between the pθ(x1:T ) and pθ1(x1:T ), the loss simplifies to: L(θ) = tU (0,T ),xw pθ1(xw ˆxw 0 ),xl tpθ1 (cid:16) ˆxl xl 0 (cid:17) log σ(βT ( (cid:0)pθ1 + DKL (cid:0)pθ1 DKL (cid:16) DKL pθ (cid:16) (cid:16) (cid:0)xw (cid:0)xw (cid:16) t1 xw t1 xw t1 xl xl (cid:1)(cid:1) 0 (cid:1) pθ , ˆxw (cid:1) pref , ˆxw (cid:17) t, ˆxl t, ˆxl pref (cid:0)xw t1 xw (cid:0)xw t1 xw (cid:1)(cid:17) (cid:0)xl t1 xl (cid:0)xl 0 (cid:17) pθ (cid:1)(cid:1) (cid:1)(cid:17)(cid:17) . 0 pθ1 +DKL t1 xl xl t1 xl (S21) Here we adopt the same strategy as diffusion-DPO [40], using uniformly sampled step U(0, ). Finally, substituting the definitions of the KL-divergence for diffusion models, which relates to the mean-squared error (MSE) of the predicted noise ϵθ7, the final objective for CPO is derived: LCP O(θ) = tU (0,T ),zw zw ϵθ(xt, t)2 (zl ϵθ(xt, t)2 log σ(βT ω(λt)( ,zl 2 zw ϵref (xt, t)2 2 2))) 2 zl ϵref (xt, t)2 (S22) and zl where zw are the noise sampled from the pre-trained expert model θ1, λt is the signal-to-noise ratio, and ω(λt) is weighting function (often constant). This final loss function (Eq. (S22)) directly optimizes the denoising model ϵθ to reduce the noise prediction error for the positive noise (zw ) relative to the reference model ϵref , and conversely, to increase the error for the negative noise (zl t). The term βT ω(λt) acts as dynamic coefficient scaling the preference score. S8. Hallucination in Agent Behaviors To investigate the accuracy of the automatically annotated dataset, we conduct human verification. We randomly select 100 samples with complex annotations from the original dataset. total of 10 participants are invited, with gender ratio of 1:1 and ages ranging from 20 to 30. Participants are required to examine all positive and negative attributes across 7 dimensions for each image and record the attributes that actually appeared to calculate the annotation accuracy and verify the reliability of the automatic annotation results. The calculation is defined as follows: Accuracy = Actual Occurrences Occurrences in Annotations 100% (S23) As shown in Tab. S5, the overall accuracy is 88.71%. The accuracies for individual dimensions are as follows: Color Relationship (96.18%), Perspective and Space (91.93%), Edge Relationship (91.44%), Light and Shadow (94.49%), Brushwork and Texture (89.29%), Composition (88.93%), and Shape and Form (81.96%). Although high level of accuracy has been achieved, there remains slight deviation compared to human judgment. On the one hand, human interpretations of aesthetic attributes inherently involve certain subjectivity, making complete consensus difficult and potentially affecting labeling accuracy. On the other hand, we believe this deviation does not hinder our task construction or algorithmic optimization. Since our proposed CPO method is designed to encourage the model to generate samples exhibiting positive attributes while suppressing those with negative attributes, accurately identifying positive and negative attributes is more critical than achieving exhaustive annotation coverage. Table S5. Verification of annotation accuracy across 7 dimensions. The results are compared against human judgment, with an overall accuracy of 88.71%. Dimension Accuracy (%) Color Relationship Perspective and Space Edge Relationship Light and Shadow Brushwork and Texture Composition Shape and Form Overall 96.18 91.93 91.44 94.49 89.29 88.93 81.96 88.71 S9. Reliability of the SFT Model To evaluate the reliability of the model after first-stage SFT training, we test its performance metrics and IoU scores for Apos and Aneg predictions under three different inference strategies. Specifically, we compared: Configuration (the method for first-stage CPO alignment, placing description and Apos in the prompt and Aneg in the negative prompt), Configuration (placing only and Apos in the prompt), and Configuration (placing y, Apos, and Aneg all in the prompt). Detailed results are presented in Tab. S6. All three configurations achieve high IoU for Apos and low IoU for Aneg, indicating that after SFT, the model can effectively encode Apos while suppressing the expression of Aneg, ultimately generating images that accurately reflect the attribute requirements in the prompt, demonstrating the reliability of SFT. Notably, Configuration yields the highest Apos IoU, the lowest Aneg IoU, and the best overall performance, corroborating the superiority of our CPO approach. Table S6. Quantitative evaluation of our SFT-trained model under three prompting configurations. IoUpos, IoUneg, PS, HPS, IR, and LA denote IoU scores for Apos and Aneg, PickScore, HPSv2, ImageReward, and LAION-Aesthetic Score. Config IoUpos IoUneg FID PS HPS IR LA 0.7780 0.6617 0.6539 0.3928 88.3168 0.1939 0.2592 0.4843 6.1051 0.3975 91.3259 0.1912 0.2467 0.4342 6.0487 0.4253 93.0775 0.1925 0.2551 0.4462 6.0599 S10. Negative Noise Construction Here, we clarify why the direction of our negative noise guidance is derived from (y, Apos, Aneg) rather than solely from Aneg. In our domain-specific fine-grained evaluation , each image is first annotated with its corresponding positive attributes based on the content. However, when an image exhibits local deficiencies, certain positive attributes may not be properly realized; in such cases, the image is additionally annotated with the corresponding negative attributes. In other words, the positive labels encode the complete attribute information of an image, whereas the negative labels only identify which aspects are deficient. For example, if Apos includes compositional attribute such as circular composition, then the associated negative attribute would be absence of shape-breaking elements, since circular composition intrinsically requires such elements. If we were to provide only the negative label aabsence of shape-breaking elements without the accompanying compositional information, the semantics would be incomplete. S11. Differences from and Advantages over Inversion-Based DPO Our proposed Complex Preference Optimization (CPO) framework significantly advances diffusion model alignment beyond existing inversion-based DPO methods, such as DDIM-InPO (InPO) [28] and Inversion-DPO [25], offering key advantages rooted in signal granularity, training efficiency, and optimization stability. The primary distinction lies in the granularity of the alignment signal: existing inversion-based DPO approaches fundamentally rely on maximizing monolithic, coarse preference (binary winner/loser pairs). In contrast, CPO introduces novel, domain-specific evaluation criterion that is hierarchical, multi-dimensional, discrete, and non-equilibrium, allowing it to explicitly decouple positive (Apos) and negative (Aneg) attributes within single sample. This attribute decoupling enables fine-grained guidance, steering the model toward desired characteristics while actively suppressing undesirable ones, capability absent in methods optimizing only for simple preference score or implicit reward derived from inversion. Furthermore, CPO exhibits superior computational efficiency and enhanced training stability. While InversionDPO leverages DDIM inversion to achieve more precise approximation of the diffusion path compared to DiffusionDPO and InPO is highly efficient, aiming for state-of-theart performance in just 400 training steps, CPO offers compelling practical speed gains. For instance, achieving stable convergence for one epoch on the SDXL model with CPO requires approximately 10 GPU hours, representing significant reduction in overhead even compared to optimized inversion-based methods, which, in practice, may require around 138 GPU hours for comparable epoch (Inversion-DPO reports acceleration factors greater than 2 over Diffusion-DPO). Additionally, CPO addresses critical instability inherent in the DPO objective itself by incorporating novel stabilization strategy LCP OS. This strategy specifically counteracts the imbalance where the concave loss term for losing samples dominates the convex loss term for winning samples, resulting in demonstrably smoother and more robust training convergence than the non-stabilized variant (LCP O). In contrast, inversion-based methods focus their stability gains primarily on improving the accuracy of the underlying diffusion process trajectory rather than rectifying this specific gradient entanglement issue in the DPO loss function. S12. Discussion S12.1. The Special Variant of CPO CPO is inherently designed to handle multi-dimensional and decoupled preference signals. It is crucial to examine the relationship between CPO and existing methods when its complexity is reduced. If the attribute system within CPO is constrained to single dimension with onelevel deep, the CPO objective effectively simplifies to form highly similar to the Direct Preference Optimization (DPO) [40]. This is because the core of CPO is built upon optimizing the log-probability difference between the winning and losing samples, an operational structure that mirrors DPO but is adapted for diffusion models via dynamic noise targets (zw, zl). This observation positions CPO as generalized preference optimization framework that extends DPOs binary preference capability to complex, multi-criteria alignment signals within generative models. Furthermore, it is important to distinguish CPO from the Binary Classifier Optimization (BCO) [15] approach. BCO transforms the preference alignment task into binary classification problem, where model is trained to classify preferences based on log-probabilities, and the policy is then optimized using the resulting classification logits. In contrast, CPO remains direct policy optimization method. We do not train an explicit classifier or reward model. Instead, the preference signal is encoded directly into the noise targets, enabling the policy to be updated directly and stably without an auxiliary classification step. This direct preference gradient application differentiates our approach from BCOs classification-mediated optimization strategy. S12.2. The reliability of CPO key design aspect of our two-stage approach is the reliance on the fine-tuned model θ1 to generate the dynamic noise targets, zw (winner) and zl (loser), used in the CPO objective. potential critique is that the final model θ is learning from surrogate representation of preference the knowledge learned by θ1 via Supervised Fine-Tuning (SFT) with attribute promptsrather than directly from the ground-truth fine-grained attributes Apos and Aneg of the original dataset D. We acknowledge this as limitation stemming from the inherent difficulty of performing direct, stable preference optimization on complex, multidimensional, and non-equilibrium signals. However, the utilization of surrogate model is common and often necessary practical trick in modern generative modeling and reinforcement learning. For instance, in Generative Adversarial Networks (GANs) [10], the generator optimizes through gradients provided by the discriminator rather than direct data likelihood. Similarly, diffusion distillation techniques like DisBack [52] and preference optimization methods like DDO [54] utilize an auxiliary model or discriminator as surrogate for knowledge transfer or preference signal. Furthermore, in standard Reinforcement Learning from Human Feedback (RLHF), an explicit reward model is trained from human preference data and subsequently acts as surrogate during the policy optimization stage. In our work, θ1 serves as knowledge-guided surrogate model, injecting and structuring the complex domain expertise such that the decoupled positive and negative attributes can be dynamically translated into quantifiable noise targets zw and zl. Future research will explore more sophisticated techniques to bypass θ1 and achieve direct, stable alignment with raw Apos and Aneg labels. S12.3. The Generalizability of CPO Another critical point is the generalizability of our domainspecific fine-grained evaluation criteria. We instantiate our approach in the painting generation domain with 5-level hierarchy, 7 root dimensions, and 246 pairs of attributes. We emphasize that while the content of the attributes is domain-specific (e.g., Color Relations and Brushstroke for paintings ), the paradigm characterized by being multidimensional, discrete, and non-equilibrium is proposed as universal structure for modeling complex human expertise. The core innovation is in the CPO objective and its ability to process such rich signal, irrespective of the domain. Our method is designed to be easily extensible to other complex generation scenarios, provided similar complex criteria. 20 annotated dataset. If the domain-specific criteria reflect narrow, culturally or demographically homogenous view of good or bad attributes, the resulting aligned model may exhibit reduced diversity, potentially marginalizing minority or unconventional styles. Future work must focus on actively diversifying the expert-defined criteria and the corresponding training data to ensure that CPO promotes universally beneficial and equitable generative models, preventing the entrenchment of single, privileged aesthetic or technical standard. Figure S13. Additional results of failure examples. S13. Failure Cases and Limitation Failure cases. While CPO can generate high-quality images, it remains constrained by the inherent limitations of the base model, and typical failure modes persist. As shown in the Fig. S13, these mainly include: (a) anatomical structural defects (e.g., finger distortion), (b) quantity errors (e.g., abnormal number of rabbit ears), (c) scale anomalies (e.g., excessively long revolver barrel), and (d) spatial misalignment (e.g., incorrect sword placement). Additionally, some samples fail to satisfy specific positive attribute requirements; for example, (b) does not actually meet the abstract characteristics required by abstract geometry. Limitation. As discussed in Sec. S13, CPOs performance remains constrained by the inherent limitations of the base model, occasionally failing to fully satisfy all specified positive attribute requirements. Furthermore, as elaborated in Sec. S5, while our stabilization strategy enhances positive sample fitting, it has yet to achieve the ideal optimization objective of simultaneously improving positive sample fitting and degrading negative sample fitting. These limitations will be prioritized for exploration and resolution in future work. S14. Social Impact CPO and the underlying hierarchical, fine-grained evaluation criteria present substantial positive impact on generative AI by enabling models to align with nuanced human expertise, potentially elevating the quality and controllability of generated content in domains like digital art and design. By shifting the alignment paradigm from coarse, binary preference to multi-dimensional, attribute-decoupled criteria, our method facilitates the integration of complex, domain-specific knowledge into generative models, leading to outputs that are more aesthetically sophisticated and technically sound according to expert standards. This advancement can empower creators by providing tools that adhere to higher, more specific quality benchmarks, thereby raising the overall standard of machine-generated content. However, the technologys effectiveness in instilling expert-defined criteria necessitates consideration of potential risks. The explicit design to favor specific positive attributes Apos and suppress negative ones Aneg could inadvertently introduce or amplify biases present in the expert-"
        }
    ],
    "affiliations": [
        "Peking University",
        "University of Electronic Science and Technology of China",
        "University of Nottingham Ningbo China",
        "Zhejiang University"
    ]
}