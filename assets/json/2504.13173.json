{
    "paper_title": "It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization",
    "authors": [
        "Ali Behrouz",
        "Meisam Razaviyayn",
        "Peilin Zhong",
        "Vahab Mirrokni"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional bias-the natural tendency to prioritize certain events or stimuli-we reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn a mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, we observed that most existing sequence models leverage either (1) dot-product similarity, or (2) L2 regression objectives as their attentional bias. Going beyond these objectives, we present a set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. We then reinterpret forgetting mechanisms in modern deep learning architectures as a form of retention regularization, providing a novel set of forget gates for sequence models. Building upon these insights, we present Miras, a general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. We present three novel sequence models-Moneta, Yaad, and Memora-that go beyond the power of existing linear RNNs while maintaining a fast parallelizable training process. Our experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 3 7 1 3 1 . 4 0 5 2 : r Its All Connected: Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni Google Research {alibehrouz, razaviyayn, peilinz, mirrokni}@google.com Abstract Designing efficient and effective architectural backbones has been in the core of research efforts to enhance the capability of foundation models. Inspired by the human cognitive phenomenon of attentional biasthe natural tendency to prioritize certain events or stimuliwe reconceptualize neural architectures, including Transformers, Titans, and modern linear recurrent neural networks as associative memory modules that learn mapping of keys and values using an internal objective, referred to as attentional bias. Surprisingly, we observed that most existing sequence models leverage either (1) dot-product similarity, or (2) ℓ2 regression objectives as their attentional bias. Going beyond these objectives, we present set of alternative attentional bias configurations along with their effective approximations to stabilize their training procedure. We then reinterpret forgetting mechanisms in modern deep learning architectures as form of retention regularization, providing novel set of forget gates for sequence models. Building upon these insights, we present Miras, general framework to design deep learning architectures based on four choices of: (i) associative memory architecture, (ii) attentional bias objective, (iii) retention gate, and (iv) memory learning algorithm. We present three novel sequence modelsMoneta, Yaad, and Memorathat go beyond the power of existing linear RNNs while maintaining fast parallelizable training process. Our experiments show different design choices in Miras yield models with varying strengths. For example, certain instances of Miras achieve exceptional performance in special tasks such as language modeling, commonsense reasoning, and recall intensive tasks, even outperforming Transformers and other modern linear recurrent models."
        },
        {
            "title": "1 Introduction",
            "content": "Designing efficient architectural backbones for sequence modeling is key to enhance the capability of foundation models in domains ranging from language (Behrouz et al. 2024c; Vaswani et al. 2017a) and computer vision (Dosovitskiy et al. 2020) to computational biology (Wang et al. 2024) and neuroscience (Behrouz et al. 2024a). While Transformers (Vaswani et al. 2017a), mainly due to their in-context learning and ability to learn at scale (Kaplan et al. 2020), have been firmly established as state-of-the-art (SOTA) models in sequence modeling, their quadratic time and space complexity limits their applicability in tasks that require long context modeling (Dalal et al. 2025; Li et al. 2024a; Liu et al. 2024b). Recent efforts aim to overcome Transformer limitations in long-context modeling by designing efficient recurrent alternatives (Behrouz et al. 2024c; Neil et al. 2017; Smith et al. 2022). Unlike Transformers linearly growing memory (i.e., the KV cache), these models compress the context into fixed size memory, demanding improved memory management for comparable performance. To design more effective architectures, studies focus on improving memory capacity and its management by using/designing more expressive: (1) Learning rules: from Hebbian rule (Hebb 2005) to Delta rule (Neil et al. 2017); (2) Forget gates: from LSTMs (Schmidhuber et al. 1997) to Mamba2s (Dao et al. 2024) and then Titans forget gates (Behrouz et al. 2024c); and (3) More expressive memory architectures: from vector-valued memory in RetNet (Sun et al. 2023) and LRU (Orvieto et al. 2023) to neural deep memory in Titans (Behrouz et al. 2024c) and TTT (Sun et al. 2024). At the core of these advancements lies critical question: what is the underlying design framework behind these sequence models, and how can these models be enhanced?. Taking inspiration from the broad definitions of associative memory and learning in neuropsychology literature (Okano et al. 2000), several studies discuss the connection between Transformers 1 and (linear) Recurrent Neural Networks (RNNs) with associative memory (Bietti et al. 2023; Hopfield 1982; Ramsauer et al. 2021). These studies, however, either: (1) lack universal explanation to fully illustrate the underlying learning algorithms, (2) are limited to specific definition of associative memory and lack generalizability, and/or (3) are unable to describe standard, widely used components such as forget gate. Contributions. Inspired by the human cognitive phenomenon of attentional biasthe natural tendency to prioritize certain events or stimuliwe re-conceptualize neural architectures, including Transformers, Titans, and other modern linear recurrent neural networks based on broad definition of associative memory with attentional bias. We define and formalize the concept of attentional bias as the internal memory objective of sequence models (see Section 3) that aims to learn the underlying mapping between inputs (i.e., keys and values). Our formulation reveals that almost all existing sequence models are associative memories that leverage the same type of attentional bias. We reinterpret existing forgetting mechanisms in modern deep learning architectures as form of retention ℓ2-regularization for the attentional bias, and then provide novel set of alternative retention gates (forget gate) for sequence models, providing new insights on how to balance learning new concepts and the retention of previously learned concepts. Building upon our formulation of memory and forget gate, we present Miras1, fundamental framework to design novel sequence modeling architectures by four choice of: (1) Attentional bias (i.e., memory objective), (2) Retention gate, (3) Memory architecture, and (4) Memory learning algorithm (i.e., optimizer). We motivate and discuss several novel design choices, leading to novel architectures beyond existing sequence modeling architectures. Finally, we focus on three novel variants of MirasMoneta, Yaad, and Memorathat are based on attentional biases beyond simple ℓ2-regression objective as well as novel retention gating mechanisms that are more robust than existing ones. We further perform experimental evaluations of these three variants on language modeling, common-sense reasoning, needle-in-haystack, and recall intensive tasks. The results illustrates the superior performance of these variants, outperforming state-of-the-art sequence models. Roadmap. In Section 2, we review literature and discuss relevant concepts that we use through the paper. In Section 3, we present and discuss the broad definition of associative memory with formally defining the concept of attentional bias. We then discuss two viewpointsLearning-Retaining and Follow-the-Regularized-Leader (FTRL)to interpret sequence modeling through the lens of optimization and prove the generality of Learning-Retaining over FTRL. In Section 4, we present our Miras framework and discuss how it unifies modern sequence models. In Section 5, to show the potential of Miras framework, we discuss variety of novel design choices for (1) attentional bias, and (2) retention gate (forget gate). Later in Section 5.3, we present three novel sequence models as the variants of Miras, and then discuss how to train them in parallelizable manner. Finally, our experimental evaluations are reported in Section 6."
        },
        {
            "title": "2 Preliminaries and Background",
            "content": "In this section, we review the related studies and background concepts that we use through the paper. Attention. Attention as the backbone of Transformers is critical component that acts as their associative memory (Bietti et al. 2023). Given input 𝑥 R𝑁 𝑑in, causal attention computes output R𝑁 𝑑in based on Softmax over input dependent key, value, and query matrices: = 𝑥WQ, y𝑖 = 𝑖 𝑗= = 𝑥WK, exp (cid:16) 𝑑in 𝑖 k𝑗 / ℓ=1 exp (cid:16) (cid:205)𝑖 𝑖 kℓ / (cid:17) v𝑗 𝑑in = 𝑥WV, , (cid:17) (1) (2) where WQ, WK, and WV R𝑑in 𝑑in are learnable parameters. While Transformers achieve significant improvements compared to traditional Recurrent Neural Networks (RNNs)such as LSTM (Schmidhuber et al. 1997), their complexity that requires at least 𝑁 𝑑 operators to calculate the output has been the main motivation for researchers to think about alternative architectures. We divide and review the research efforts to design alternative architectures into two groups: (1) Linear shallow memory recurrent models, (2) Deep memory modules. 1 Miras is the translation of Legacy in several languages: such as Persian, Arabic, and Turkish. We choose this name since this framework provides clear steps for future studies to design powerful sequence models based on their task at hand. 2 Figure 1: The overview of Miras framework. Miras is based on four critical choices of (1) memory architecture, (2) attentional bias, (3) retention gate, and (4) memory learning algorithm. In this framework, the memory architecture determines the model capacity to memorize; attentional bias is responsible for modeling the underlying mapping patterns; retention gate determines how to balance learning new concepts and the retention of previously learned concepts; and memory learning algorithm is responsible for memory management. (Linear) Recurrent Models. For many years, non-linear (gated) recurrent neural networks had been the de facto architectural backbones in deep learning (Greff et al. 2016). Their recurrent nature, however, results in non-parallelizable training, making their large scale training infeasible. To this end, in recent years, linear RNNs as alternatives to both Transformers and non-linear RNNs attract much attention mainly due to their parallelizable and linear-time training while maintaining competitive performance (Peng et al. 2025a; Sun et al. 2023; Yang et al. 2024c). Earlier variants of linear RNNs (De et al. 2024; Sun et al. 2023; Yang et al. 2024b), which mostly are based on Hebbian learning rule (Hebb 2005), aim to compress the data into their vector-valued (or matrix-valued) memory (De et al. 2024; Katharopoulos et al. 2020; Liu et al. 2024a; Sun et al. 2023; Yang et al. 2024b). Let M𝑡 R𝑑 𝑛 be the memory (𝑛 = 1 means vector-valued memory), and k, R𝑑 are keys and values (i.e., projection of input 𝑥𝑡 R𝑑 ), simple general formulation for such linear RNNs can be written as: M𝑡 = 𝐴𝑡 M𝑡 1 + v𝑡 𝑡 , (3) where is an arbitrary associative operator and 𝐴𝑡 is data-(in)dependent diagonal matrix or scalar (Yang et al. 2024c). Despite the efficiency that comes with the linear recurrent nature of these models, the memory can overflow mainly due to the additive (without replacement) nature of Hebbian learning rule, resulting in limited memory capacity and limited expressive power in in-context learning tasks. Moreover, the vector-valued memory of these architectures can limited their ability to learn/memorize large context window, mainly due to the limited expressive power of memory to learn the underlying patterns of data (Behrouz et al. 2024c; Sun et al. 2024). To address the above mentioned limitations, recurrent models that use matrix-valued memory with Delta learning rule has gained popularity in recent years (Neil et al. 2017; Schlag et al. 2021; Yang et al. 2024c). Despite significant advantages, even these delta-rule-based recurrent models face theoretical limitations (Irie et al. 2023) with moderate performance in practice (Yang et al. 2024c). Recently, several studies aim to improve the performance of such models by adding scalar or channel-wise forget gate mechanisms (Peng et al. 2025b; Yang et al. 2024a), , using negative eigenvalues (Grazzi et al. 2024), and multiple learning steps (Siems et al. 2025). They, however, still suffer from performance drop in long context, mainly due to the less expressive memory architectures (Behrouz et al. 2024c). 3 Table 1: Overview of recent sequence models in Miras framework perspective. Surprisingly, all models are using the same type of attentional bias and regularization (forget gate). Note that these architectural choices does not uniquely identify the backbone as there are other design choices (e.g., input-dependency, channel-wise parameters, etc.) as well as the use of other components such as attention, convolutions, etc. Note that for attentional bias and retention gate, we are referring to the original design of Miras, discussed in Equation 4 and Remark 1. Model Memory Architecture Attentional Bias Retention Gate Memory Algorithm Memory Write Operation Shallow Memory GD M𝑡 = 𝛼M𝑡 1 + v𝑡 𝑡 Nonparametric M𝑡 = M𝑡 1 {(k𝑡, v𝑡 )} RetNet (2023) Transformer (2017) LA (2021) DFW Lightening Attention (2025) GLA (2024) Mamba (2024) HGRN2 (2024) DeltaNet (2017) Longhorn (2024) TTT-Linear (2024) Gated DeltaNet (2024) RWKV-7 (2025) DeltaProduct (2025) Vector Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix Matrix TTT-MLP (2024) Titans-LMM (2024) Moneta (ours) 2-layer MLP 𝑘-layer MLP 2-layer MLP Dot-Product L2 Dot-Product Dot-Product Dot-Product Dot-Product Dot-Product L1 L2 L2 L2 L2 L2 L2 L2 L𝑝 Yaad (ours) 2-layer MLP Huber - - L2 L2 L2 L2 L2 - - - L2 L2 L2 - L2 L𝑞 Memora (ours) 2-layer MLP L2 KL Implicit GD GD GD GD MGD Deep Memory GD GD GD GD GD GD GD GD GD GD GD M𝑡 = M𝑡 1 + v𝑡 𝑡 M𝑡 = (cid:0)𝛽𝑡𝛼 (cid:1) M𝑡 1 + v𝑡 𝑡 𝑡 M𝑡 = 𝛼M𝑡 1 + v𝑡 𝑡 M𝑡 = Diag(𝛼𝑡 )M𝑡 1 + v𝑡 𝑡 M𝑡 = 𝛼𝑡 M𝑡 1 + v𝑡 𝑡 M𝑡 = Diag(𝛼𝑡 )M𝑡 1 + v𝑡 (1 𝛼𝑡 ) 𝑡 )M𝑡 1 + 𝛽𝑡 v𝑡 M𝑡 = (I 𝛽𝑡 k𝑡 𝑡 𝛽𝑡 k𝑡 𝛽𝑡 M𝑡 = M𝑡 1 + 1+k 1+𝛽𝑡 𝑡 k𝑡 𝛽𝑡 𝑡 k𝑡 M𝑡 = M𝑡 1 𝜂L (M𝑡 1, x𝑡 ) M𝑡 = (cid:0)𝛼𝑡 (I 𝛽𝑡 k𝑡 M𝑡 = diag(𝛼𝑡 ) (cid:0)I 𝛽𝑡 k𝑡 𝑡 M𝑡 = (cid:16) 𝛼𝑡 (cid:206)𝑛 𝑡 )(cid:1) M𝑡 1 + 𝛽𝑡 v𝑡 𝑡 (cid:1) M𝑡 1 + 𝛽𝑡 v𝑡 𝑡 M𝑡 1 + (cid:205)𝑛 𝑖=1(I 𝛽𝑡,𝑖 k𝑡,𝑖 𝑡,𝑖 ) (cid:16) (cid:16) (cid:17) (cid:17) (cid:17) k𝑡 x𝑡 𝑗=1 (cid:206)𝑛 𝑖=𝑗 (I 𝛽𝑡,𝑖 v𝑗,𝑖 𝑗,𝑖 ) GD + Momentum M𝑡 = 𝛼𝑡 M𝑡 1 S𝑡, where S𝑡 = 𝜂𝑡 S𝑡 1 𝜃𝑡 (M𝑡 1; k𝑡, v𝑡 ) M𝑡 = M𝑡 1 𝜂L (M𝑡 1; k𝑡, v𝑡 ) 𝐴𝑡 = 𝛼𝑡𝐴𝑡 1 𝜂𝑡 ℓ𝑝 (𝑊𝑖 1; k𝑡, v𝑡 ),𝑊𝑡 = 𝑊𝑡 = 𝛼𝑡𝑊𝑡 1 (cid:40)𝜂𝑡 ℓ2(𝑊𝑡 1; k𝑡, v𝑡 ) 𝜂𝑡 𝛿𝑡 ℓ1(𝑊𝑡 1; k𝑡, v𝑡 ) Otherwise. (k𝑡 ) v𝑡 𝛿𝑡, 𝐴𝑡 𝐴𝑡 if 𝑞2 𝑞 𝑊𝑡 = Softmax (𝛼𝑡 log(𝑊𝑡 1) 𝜂𝑡 ℓ2(𝑊𝑡 1; k𝑡, v𝑡 )) is using multiple rounds of GD per token. For the sake of clarity, we use L2 for all modified L2-like regularizations. However, in fact, only Titans and RWKV-7 are using L2 retention gate (see Section 4) Deep Memory Module: Titans and Test Time Training. To overcome the limited memory and to extend the effective context length of deep sequence models, more recent studies focus on new generation of architectures with deep memory module (Behrouz et al. 2024c; Sun et al. 2024). These architectures are built on the meta-learning perspective, where the memory is an MLP architecture that is updated using gradient descent (with momentum) (Behrouz et al. 2024c; Sun et al. 2024). Sun et al. (2024) further provide unifying perspective that how linear and softmax attention are respectively parametric and non-parameteric solutions of (kernel) regression loss but consider other modern linear RNNs outside of this class of models. Recently, in concurrent work to ours, Wang et al. (2025) show that with additional simplification of modern RNNs (e.g., RetNet (Sun et al. 2023), Mamba (Dao et al. 2024)) they approximately place in the same class of models that internally optimize regression loss. It, however, still remains unanswered that What is the underlying design framework behind these sequence models that can accurately unify existing architectures? Moreover, the role of forget gates and its alternative choices in modern sequence models is surprisingly less explored."
        },
        {
            "title": "3 Associative Memory, Attentional Bias, and Retention",
            "content": "Associative memory, which is an inseparable component of learning in humans (Terry 2017), has been the inspiration for many artificial neural architectures in the literature (Behrouz et al. 2024c; Hopfield 1982; Neil et al. 2017). These studies, however, define instances of the concept of associative memory, limiting the architecture to specific class of similarity metrics between entities (i.e., keys and values). That is, broadly speaking, associative memory is an operator that maps set of keys 𝐾 to set of values 𝑉 , and so to learn the underlying mapping patterns in data, it requires an objective that targets type of memory and measures the quality of learned mappings: Definition 3.1 (Associative Memory and Attentional Bias). Given set of keys R𝑑𝑘 and values R𝑑𝑣 , associative memory is an operator : V. Learning the mapping of associative memory is based on an objective L, called 4 Attentional Bias, that determines the type of memory and its tendency to prioritize some events: = arg min (M (K); V). (4) few remarks are in order: Remark 1. When we parameterize the memory with parameter 𝑊 , we use (𝑊 , k). In this parameteric setting, the optimization problem in (4) should be performed over the parameter 𝑊 . Furthermore, in the parametric setup, we might use an additional regularization (𝑊 ) to control the retaining of the past data. Remark 2. Learning the mapping between keys and values (Equation 4) is meta-learning problem, in which the attentional bias is optimized in the inner-loop and all other parameters of the neural network (e.g., linear projections, convolutions, etc.) are optimized in the outer-loop. Therefore, the model learns how to store the data into its parameters at test time (Behrouz et al. 2024c; Sun et al. 2024)."
        },
        {
            "title": "3.1 Learning to Memorize and to Retain Through the Lens of Optimization",
            "content": "Definition 3.1 translates the design of neural architecture based on the concept of associative memory to learning the underlying mapping between keys and values, by minimizing an objective L. To optimize Equation 4, one simple approach is to utilize the idea of gradient descent. Specifically, given new pair of keys and values, we update the memory as: 𝑊𝑡 = 𝑊𝑡 1 𝜂𝑡 ℓ (𝑊𝑡 1; k𝑡, v𝑡 ), (5) where, for simplicity, we use the definition ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) := (M (𝑊 ; k𝑡 ), v𝑡 ). Behrouz et al. (2024c) re-interpreter this formulation as momentary surprise metric, where the model memorizes tokens that violates the expectation of the objective (i.e., being surprising to the memory). Although the choice of objective is an important step to fully interpret Equation 5 (which we discuss in detail in Section 5), there are different viewpoints to interpret this update rule in its general format, which later can help us to go beyond existing architectures:"
        },
        {
            "title": "3.2 Viewpoint 1: Online Regression and Follow-The-Regularized-Leader",
            "content": "Equation (5) can be viewed as one step of online gradient descent over the sequence of the loss functions ℓ (𝑊 ; k1, v1), ℓ (𝑊 ; k2, v2), . . . , ℓ (𝑊 ; k𝑡, v𝑡 ), . . . . (6) It is well known that the online gradient descent can be viewed as special case of Follow-The-Regularized-Leader (FTRL) algorithm with special choice of loss functions (Shalev-Shwartz et al. 2012, Chapter 2) and (Hazan et al. 2016). Specifically, assuming 𝑊0 = 0, the update rule in (5) is equivalent to 𝑊𝑡 = arg min 𝑊 𝑡 𝑖=1 𝑊 𝑊𝑖 1, ℓ (𝑊𝑖 1; k𝑖, v𝑖 ) + 1 2𝜂 𝑊 2 2, (7) where the term 𝑊 𝑊𝑖 1, ℓ (𝑊𝑖 1; k𝑖, v𝑖 ) is the local linear approximation of the original loss at time 𝑖 and the second term is regularization term. While the first part (cid:205)𝑡 𝑖=1𝑊 𝑊𝑖 1, ℓ (𝑊𝑖 1; k𝑖, v𝑖 ) measures how well can the memory learn all the past tokens, the second term 2 penalizes the memory update with respect to the size of memory. 2𝜂 𝑊 2 Equation (7) uses linear approximation of the loss function and quadratic regularization. We can, however, in principle use other approximations of the loss function as well as other regularization functions, as used in the past in online optimization (Hazan et al. 2016; Shalev-Shwartz et al. 2012) or in general optimization (Mairal 2015; Razaviyayn et al. 2013). Such changes are the idea behind the development of other optimization algorithms such mirror descent. More specifically, we can generalize the update rule in (7) to the form: 𝑊𝑡 = arg min 𝑊 𝑡 (cid:98)ℓ𝑖 (𝑊 ; k𝑖, v𝑖 ) 𝑖=1 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:124) Attentional Bias (cid:125) + 1 R𝑡 (𝑊 ) 𝜂𝑡 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:123)(cid:122) Memory Stability (cid:125) . (FTRL Viewpoint) 5 In this update rule, the term (cid:205)𝑡 𝑖=1 (cid:98)ℓ𝑖 (𝑊 ; k𝑖, v𝑖 ) aims at memorizing the tokens at test time, while the term R𝑡 (𝑊 ) regularizes the learning dynamics and take the size of the memory into account when updating it by new incoming data. Choosing different loss functions (cid:98)ℓ𝑖 (𝑊 ; 𝑥𝑖 ) and the regularization term 1 R𝑡 (𝑊 ) can lead to different algorithms such as (online) 𝜂𝑡 gradient descent or mirror descent. In this generalization, 𝜂𝑡 to can be data-dependent. Moreover, we will allow imposing constraint on the choice 𝑊 ."
        },
        {
            "title": "3.3 Viewpoint 2: Learning the Latest Token While Retaining Previous Information",
            "content": "Another way to interpret the update rule (5) is to view it as learning from the latest key-value pair (k𝑖, v𝑖 ) (via using its gradient or surprise metric), while staying close to the previous state 𝑊𝑡 1 to retain the previously memorized tokens. Formally, (5) is equivalent to 𝑊𝑡 = arg min 𝑊 𝑊 𝑊𝑡 1, ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) + 1 2𝜂𝑡 𝑊 𝑊𝑡 12 2 The first term locally approximates ℓ (𝑊 ; k𝑡, v𝑡 ) around the previous state 𝑊𝑡 1, while the last term regularizes deviations from 𝑊𝑡 1. This form can generalize to 𝑊𝑡 = arg min 𝑊 (cid:101)ℓ𝑡 (𝑊 ; k𝑡, v𝑡 ) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:125) (cid:123)(cid:122) Attentional Bias + Ret𝑡 (𝑊 ,𝑊𝑡 1) , (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) Retention (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (Learning-Retaining Viewpoint) where the term (cid:101)ℓ𝑡 (𝑊 ; k𝑡, v𝑡 ) is an approximation of ℓ (𝑊 ; k𝑡, v𝑡 ) and minimizing it corresponds to Learning from the new concepts (k𝑡, v𝑡 ). The second term Ret𝑡 (𝑊 ,𝑊𝑡 1) regularizes the changes in 𝑊 to make the learning dynamics stable and to retain previously learned knowledge. This Retention function may have local and global components: Ret𝑡 (𝑊 ,𝑊𝑡 1) = 1 D𝑡 (𝑊 ,𝑊𝑡 1) 𝜂𝑡 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:124) Local Retention (cid:125) + . 1 G𝑡 (𝑊 ) 𝛼𝑡 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:125) (cid:124) Global Retention Here, the term D𝑡 (𝑊 ,𝑊𝑡 1), which is premetric that controls the deviations from 𝑊𝑡 1, aims at retaining previously learned knowledge. The coefficient 𝜂𝑡 can be viewed as meta in-context learning rate, where larger values of 𝜂𝑡 leads to learning more from new concepts, while allowing higher forgetting of previously learned concepts. The second term is global retention that controls the change of the memory with respect to its size. The special instances of the above viewpoint (e.g., without global retention, with implicit closed-form solution, and/or with limited memory structure) have been the motivation behind some of the recent studies such as Liu et al. (2024a)."
        },
        {
            "title": "3.4 Further Discussions on the Two Viewpoints",
            "content": "The (FTRL Viewpoint) and (Learning-Retaining Viewpoint) are connected through the lens of online optimization. For example, as discussed above, by choosing linear approximation of the loss and quadratic regularization/retention, they can both cover online gradient descent update in (5) as special case. One straightforward way to make the connection explicit is by defining the premetric D𝑡 (𝑊 ;𝑊 ) based on the previous loss functions and the regularization, as described in Proposition 3.2 below: Proposition 3.2. Let 𝜂𝑡 = 𝜂 and define ℎ𝑡 (𝑊 ) := (cid:205)𝑡 1 𝑖=1 (cid:98)ℓ𝑖 (𝑊 ; k𝑖, v𝑖 ) + 1 𝜂 𝑅(𝑊 ). Assume = R𝑑 and the function ℎ𝑡 (𝑊 ) is strictly convex in 𝑊 and let Dℎ (, ) be the Bregman divergence defined by function ℎ(), i.e., Dℎ (𝑊 ,𝑊 ) = ℎ(𝑊 ) ℎ(𝑊 ) ℎ(𝑊 ),𝑊 𝑊 . Set Ret𝑡 (𝑊 ,𝑊 ) = Dℎ (𝑊 ,𝑊 ) and (cid:101)ℓ𝑡 (𝑊 ; 𝑥𝑡 ) = (cid:98)ℓ𝑡 (𝑊 ; 𝑥𝑡 ) in (Learning-Retaining Viewpoint). Then, the update rule in (Learning-Retaining Viewpoint) is equivalent to the update rule in (FTRL Viewpoint). We provide the proof in Appendix B. The above proposition shows that (Learning-Retaining Viewpoint) can also explain the approaches obtained by (FTRL Viewpoint), under some mild assumptions. Hence, (Learning-Retaining Viewpoint) may be seen as more general version. This is why we focus on this viewpoint in most of our derivations in the next sections. 6 Remark 3. Given the above viewpoint, we can see that even by using additional global regularization there is no memory erasing or forgetting process (a common term in modern architectures (Behrouz et al. 2024c; Yang et al. 2024a)) but the model might decide to not retain the past state of the memory. Interestingly, this observation also matches the human memory process, where brain does not erase memories but they might become inaccessible due to retrieval failures (Robertson 2002). Therefore, instead of calling it forget gate, later on, we use Retention Gate to refer to this term. Remark 4. As we discuss in Section 4 and summarize in Table 1, most existing modern sequence models are optimizing associative memory objective (attentional bias in Equation 4) using gradient descent. Therefore, to provide further intuition about the connection of existing sequence models as well as their online learning interpretations, we discuss the above two viewpoints that are limited to gradient descent-based update rules. Our initial definition of attentional bias and associative memory in Equation 4, however, is broader and can be optimized by any optimization algorithm (e.g., even Newtons method, or non-parametric solutions)."
        },
        {
            "title": "4 Miras: Learning to Memorize with Robust and Expressive Memory",
            "content": "Building upon our definition of associative memory, attentional bias, and previous viewpoints, we present Miras framework that not only accurately unifies existing backbone architectures but it also provides insights on how to design the next generation of sequence models. As discussed earlier in Section 3, learning an associative memory can be interpreted as meta-learning task, in which the associative memory learns how to compress and store data into its parameters at test time. The architecture of the memory in such tasks is particularly important as in longer contexts, the expressivity of the memory structure can limit its ability to learn the underlying patterns. Therefore, the first choice to design sequence model is the structure of the memory. Given the structure of the memory, parameterized by set of parameters 𝑊 , as discussed earlier, we aim to minimize loss function ℓ (𝑊 ; , ) with retention regularizer Ret() via learning algorithm (e.g., gradient descent). Accordingly, Miras requires four design choices: 1. Memory Structure: This choice specifies the architecture of the memory. For example, this architecture can be vector, linear function, Multilayer Perceptron (MLP) layer, or even more complex structures. We may restrict the choice of 𝑊 to be within certain region, e.g., 𝑊 to lie within an 𝐿2 ball to avoid infinite values or unstable training. 2. Attentional Bias: key choice is the attentional bias objective () in Equation 4. We can even consider different approximations of the loss function, (e.g., (cid:98)ℓ (, ) in (FTRL Viewpoint) or (cid:101)ℓ (, ) in (Learning-Retaining Viewpoint)). The choice of attentional bias determines how memory memorizes the context, maps the inputs, and prioritizes the events. 3. Memory Stability and Retention: Another key choice is the retention regularizer () (e.g., R𝑡 () in (FTRL Viewpoint) and Ret𝑡 () in (Learning-Retaining Viewpoint)). In parametric setups, this choice balances learning with retention of past state. An effective retention gate is key to the good performance in long context tasks. 4. Memory Algorithm: Finally, this choice specifies the learning algorithm that we use to optimize the memory objective. One may use gradient descent, gradient descent with momentum, or any other algorithm (including finding non-parametric solutions). The above choices are major design choices for designing backbone sequence models in neural architectures. There are, however, minor decisions that can distinguish models; i.e., data-dependent or independent parameters, scalar or channel-wise learning rate/retaining gate, etc. Next, we discuss the overview of how existing architectures fit into Miras framework. RNNs with Hebbian Rule. The first generation of modern recurrent architectures (e.g., Linear attention (Katharopoulos et al. 2020), RetNet (Sun et al. 2023), Mamba (Gu et al. 2024), and GLA (Yang et al. 2024b)) are based on Hebbian-like (e.g., gated Hebbian) learning rule (Hebb 2005). We let attentional bias be the dot product similarity. That is, given memory R𝑑 𝑛 and k, R𝑑 , we define ℓ𝑡 := 2M𝑡 k𝑡, v𝑡 and local retention as Ret𝑡 (M, M𝑡 1) = M𝑡 𝛼M𝑡 12 𝐹 . Using Equation Learning-Retaining Viewpoint and gradient descent as the optimizer (i.e., memory learning algorithm), the memory update rule is: M𝑡 = 𝛼M𝑡 1 + v𝑡 𝑡 . When (1) 𝛼 = 1, memory update is equivalent to Linear Attention (LA) (Katharopoulos et al. 2020); (2) 𝛼 is learnable parameter, resulting architecture is either lightening attention (𝑛 > 1) (Li et al. 2025) or RetNet (𝑛 = 1) (Sun et al. 2023); and (3) 𝛼𝑡 are data-dependent learnable parameters, resulting sequence model is Mamba2 (Dao et al. 2024). (8) 7 RNNs with Delta Rule. To improve the memory management and to enhance the memory capacity of the above group, several studies suggest using delta rule (Neil et al. 2017; Schlag et al. 2021) as the learning algorithm in recurrent neural networks (e.g., DeltaNet (Schlag et al. 2021), Longhorn (Liu et al. 2024a), and RWKV7 (Peng et al. 2025b)). In this part, we recall that where R𝑑 𝑛, delta rule is equivalent to optimizing MSE objective M𝑡 k𝑡 v𝑡 2 2 with Ret𝑡 (M, M𝑡 1) = M𝑡 𝛼M𝑡 12 𝐹 as local retention, and stochastic gradient descent as optimizer: (𝜂𝑡 is defined in Equation Learning-Retaining Viewpoint) M𝑡 = 𝛼 (cid:0)I 𝜂𝑡 k𝑡 𝑡 (cid:1) M𝑡 1 + v𝑡 𝑡 . (9) When (1) 𝛼 = 1, memory update is equivalent to DeltaNet (Schlag et al. 2021); and (2) 𝛼𝑡 R𝑚 are data-dependent learnable parameters, resulting sequence model is either Gated DeltaNet (Yang et al. 2024a) (𝑚 = 1), or RWKV7 (Peng et al. 2025b) (𝑚 = 𝑑). Therefore, RNNs with delta rule are special instances of Miras. Beyond Delta Rule. As discussed earlier, while delta rule with its value replacement strategy is more powerful than Hebbian-like learning rules, it suffers from theoretical limitations (Irie et al. 2023) and achieves moderate performance in practice (Yang et al. 2024c). Therefore, several studies have focused on update rules beyond delta rule. Recently, Titans (Behrouz et al. 2024c) suggests using non-linear MSE objective of M𝑡 (k𝑡 ) v𝑡 2 2 with both local and global 2 and optimize it with gradient descent with momentum 2. Therefore, retention of D𝑡 = 𝑊𝑡 𝑊𝑡 12 Titans-LMM is special instance of Miras, where we use the abovementioned attentional bias and retention regularizations, and gradient descent with momentum as the optimizer. 𝐹 and G𝑡 = 𝑊𝑡 2 Another example of such models is Mesa-layer, in which the model uses (cid:205)𝑡 2 as the attentional bias objective with M𝑡 2 2 as the retention regularization. Since these models uses Newtons method to optimize such an objective, they provide more expressive update rule than delta rule. We further discuss set of new learning algorithms beyond delta rule in Section 5. 𝑖=1 M𝑡 (k𝑖 ) v𝑖 2 Attention. As discussed by Sun et al. (2024), softmax attention is non-parameteric solution of ℓ2-MSE loss function (i.e., 𝑊 v2 2) with Nadaraya-Watson estimator. Therefore, softmax attention is an instance of Miras, when we find the non-parameteric solution to the MSE loss with Nadaraya-Watson estimator, without retention."
        },
        {
            "title": "5 Beyond Existing Attentional Biases and Retention Gates",
            "content": "As discussed in the previous section, existing work focus only on linear/quadratic choices for the attentional bias or retention gate. In particular, the loss function 𝐿(M (k𝑡 ), v𝑡 ) is defined as 𝐿(M (k𝑡 ), v𝑡 ) = 𝑐𝑡 (k𝑡 ) v𝑡 2 for some (learnable) constant 𝑐𝑡 in prior work. Also the regularization term 𝑅𝑡 (𝑊 ) or the parametric D𝑡 is considered as quadratic/linear function. In addition, almost all prior work consider 𝑊 to be the entire R𝑑 space. However, in general there could be various choices for all the three aforementioned design choices. To illustrate the potential and flexibility of our designed framework, here, we review some of the potential design choices for attentional bias and retention gate in Miras. For the sake of clarity, we discuss all these attentional bias and memory retention gates based on using gradient descent as the optimizer, and so based on the provided two view points. However, these attentional bias objectives and retention regularizers can be directly used in Equation 4 and optimized by using any other optimization algorithms, resulting in different update rules."
        },
        {
            "title": "5.1 Alternative Attentional Biases",
            "content": "Variant 1: ℓ𝑝 -Attentional Bias. As discussed in the main body, attentional bias defines the similarity metric and measures how well memory can recall the value, given its corresponding key. Although ℓ2 regression loss often is natural choice, it is sensitive to noise in the data. natural extension is to use ℓ𝑝 -norm class of objectives. That is, let be the memory, be the keys, and be the values, we define ℓ𝑝 -attentional bias as: (M (𝑊 , k𝑡 ); v𝑡 ) = (k𝑡 ) v𝑡 𝑝 𝑝, (10) 2The retention gate (forget gate) in Titans is different from Mamba2 and Gated DeltaNet that we discussed above. The main difference comes from the case of full memory erase. While Mamba2 gating removes the entire memory and treats the next token as the first ever seen data, Titans use cold start strategy and use the previous state of the memory to measure the surprise of the incoming token before fully erasing the memory. where 𝑝 R1 and .𝑝 is the 𝑝-norm. Although depending on the distribution of the data, we might want to use different values of 𝑝 (see Section 6), different values of 𝑝 can result in memory architectures with interesting properties. For the sake of simplicity, let memory be matrix, i.e., 𝑊 R𝑚𝑑 and (𝑊 , k𝑡 ) = 𝑊 k𝑡 , the closed form can be derived as: 𝑊𝑡 = 𝑊𝑡 𝜂𝑡 ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) = 𝑊𝑡 𝑝 𝜂𝑡 (cid:0)Sign(𝑊 k𝑡 v𝑡 ) 𝑊 k𝑡 v𝑡 𝑝 1(cid:1) 𝑡 . Let 𝑝 = 1, the recurrence is simplified as: 𝑊𝑡 = 𝑊𝑡 𝜂𝑡 Sign(𝑊𝑡 k𝑡 v𝑡 ) 𝑡 , (11) (12) which means that the memory has only two values of 1 and 1. We call this variation value-less associative memory, in which we store entities (keys) but map them into two extreme class of -1 and +1. Remark 5. One of the critical challenges to use the above update rule is in the backpropagation process, in which Sign() and are non-differentiable and so might cause unstable training. To overcome this issue, we use Sign(𝑥) tanh (𝛼𝑥) , and 𝑥 = 𝑥 2 + 𝜖, as the smooth approximators of these functions. One simple interpretation for such behavior (i.e., value-less memory) is similar to the coping mechanism in humans (Loftus 1993), in which the memory does not store the values for extreme events. This interpretation of protective memory in extreme events motivates our next variant. Variant 2: Huber Loss: Memory with Coping Mechanism. While ℓ2-norm objective is common choice for many statistical and machine learning tasks, it is known to be sensitive to outliers and extreme samples.This sensitivity extends to the use of ℓ2 loss for attentional bias. To address this and drawing motivation from robust regression literature, we suggest utilizing the Huber loss-type (Hastie et al. 2009; Huber 1992) as the attentional bias, thereby reducing the negative impact of the outlier data on the memory learning process. We can apply Huber-type loss in three different ways: The first approach is to define the summation of the Huber loss across different coordinates as the total loss, i.e., ℓ (𝑊 ; k𝑡, v𝑡 ) = 𝑗 (M (𝑊 , k𝑡 ) 𝑗 v𝑡,𝑗 ), where (𝑊 , k𝑡 ) 𝑗 and v𝑡,𝑗 denote the 𝑗-th coordinate of (𝑊 , k𝑡 ) and v𝑡 respectively. The function () : is the Huber loss defined as (𝑎) = (cid:26) 1 2𝑎2 𝛿 (cid:0)𝑎 1 2𝛿 (cid:1) if 𝑎 𝛿 if 𝑎 > 𝛿. (13) Utilizing this attentional bias can lead to various memory update rules. For example, for the matrix form memory (𝑊 , k𝑡 ) = 𝑊 k𝑡, the update rule is given by 𝑊𝑡 = 𝑊𝑡 1 𝜂𝑡 (cid:104)(cid:16) 𝑇 (𝑊 k𝑡 v𝑡 )k 𝑡 (cid:17) (cid:0)I(𝑊 k𝑡 v𝑡 𝛿𝑡 )1(cid:1) + (cid:0)𝛿𝑡 Sign(𝑊 k𝑡 v𝑡 )k(cid:1) (cid:0)I(𝑊 k𝑡 v𝑡 > 𝛿𝑡 )1(cid:1)(cid:105) (14) In this formulation, the parameter 𝛿𝑡 decides the type of the memory used for each block of memory (ℓ2-norm objective or value-less) based on the context, making the memory more robust to outliers. The second approach is to define the Huber-type loss based on the ℓ2 loss over all coordinates, i.e., For simplicity of derivations, assume matrix memory 𝑀 (𝑊 , k𝑡 ) = 𝑊 k𝑡 . Then using gradient descent for updating memory leads the memory update rule ℓ (𝑊 ; k𝑡, v𝑡 ) = (M (𝑊 , k𝑡 ) v𝑡 2). 𝑊𝑡 = 𝑊𝑡 1 𝜂𝑡 (cid:40) (M (𝑊𝑡 1, k𝑡 ) v𝑡 ) k𝑇 𝑡 ( (𝑊𝑡 1,k𝑡 ) v𝑡 ) (𝑊𝑡 1,k𝑡 ) v𝑡 2 k𝑇 𝛿𝑡 𝑡 (𝑊𝑡 1, k𝑡 ) v𝑡 2 𝛿𝑡, if Otherwise. (15) Again, in the form (15), the parameter 𝛿𝑡 decides the type of the memory used (ℓ2-norm objective or normalized version) based on the context, making the memory more robust to outliers. 9 Finally, in the third approach, we present smooth mixture method, in which the memory decides if for an incoming data it is better to use ℓ2 or ℓ1 attentional bias: 𝑊𝑡 = 𝑊𝑡 1 (cid:40)𝜂𝑡 ℓ2 (𝑊𝑡 1; k𝑡, v𝑡 ) 𝜂𝑡 𝛿𝑡 ℓ1(𝑊𝑡 1; k𝑡, v𝑡 ) Otherwise. if (k𝑡 ) v𝑡 𝛿𝑡, (16) The role of parameter 𝛿𝑡 is the same as above. Variant 3: Memory Robust to Value Shifts. Following the robustness requirement discussed in the previous section, we aim to design memory mechanism that exhibits resilience against small shifts in the value parameter. natural approach in this context is to employ robust optimization formulation. Specifically, we define the loss function as the worst-case ℓ2 distance between the predicted memory output and the perturbed true value: (M (𝑊 , k𝑡 ); v𝑡 ) = max 𝜹v𝑡 2 Δ 1 2 (𝑊 , k𝑡 ) (v𝑡 + 𝜹v𝑡 )2 2. (17) This formulation seeks the memory parameters 𝑊 that perform well even under the adverse local perturbation of the true value v𝑡 within an ℓ2 ball of radius Δ. To solve the maximization problem in (17), we find the optimal perturbation 𝜹v 𝑡 . By solving this problem with respect to 𝜹v𝑡 , we arrive at: 𝜹v 𝑡 = Δ (𝑊 , k𝑡 ) + v𝑡 (𝑊 , k𝑡 ) v𝑡 2 Substituting this optimal perturbation back into the loss function (17), we obtain the robust loss: (M (𝑊 , k𝑡 ); v𝑡 ) = 1 2 (𝑊 , k𝑡 ) v𝑡 2 + ΔM (𝑊 , k𝑡 ) v𝑡 2 + 1 2 Δ2. This robust loss function is combination of the standard ℓ2 loss and term proportional to the ℓ2 norm of the error, scaled by the robustness parameter Δ. The value of Δ thus controls the trade-off between fitting the nominal data and ensuring robustness against value perturbations. For simplicity of the derivations, let us consider constant value for Δ, an Euclidean retention gate Ret𝑡 (𝑊 ,𝑊𝑡 1) = 𝑊 𝑊𝑡 12, and an attentional bias term (cid:101)ℓ (𝑊 ; k𝑡, v𝑡 ) = 𝑊 𝑊𝑡 1, ℓ (𝑊𝑡 1; k𝑡, v𝑡 ). Furthermore, to simplify the memory operation, we assume linear matrix memory model (𝑊 , k𝑡 ) = 𝑊 k𝑡 . Under these assumptions, we can derive the memory update mechanism using gradient descent on the robust loss: 𝑊𝑡 = 𝑊𝑡 1 𝜂 (cid:18) (cid:0)M (𝑊𝑡 1, k𝑡 ) v𝑡 (cid:1)k 𝑡 + Δ (𝑊𝑡 1, k𝑡 ) v𝑡 (𝑊𝑡 1, k𝑡 ) v𝑡 2 (cid:19) 𝑡 In this update rule, the parameter Δ, which governs the influence of the robustness term, can also be treated as learnable parameter, allowing the model to adapt its robustness based on the observed data."
        },
        {
            "title": "5.2 Alternative Retention Gates",
            "content": "Variant 1: Memorization Over Scaled Probability Simplex Via 𝑓 -Divergence. common technique in learning to prevent numerical instabilities and exploding values is to restrict the search space to bounded domain. Following this principle, to avoid numerical instabilities, we can constrained the variable 𝑊𝑡 to lie within (scaled) probability simplex. In other words, we can restrict the state to lie in the constraint set = {𝑊 𝑊 1 = 𝑐 and 𝑊𝑗𝑙 0, 𝑗, 𝑙 }. In this set, each matrix 𝑊 can be viewed as measure. Thus, in (Learning-Retaining Viewpoint), we can utilize divergences over measures to define our premetric. For example, we can use 𝑓 -divergence measure (Polyanskiy et al. 2025, Def 4.9), (Csiszar 1967) to define D𝑡 (, ). More specifically, let 𝑓 () be smooth strictly convex function from R+ to with 𝑓 (1) = 0. Then, we can define the 𝑓 divergence between 𝑊 and 𝑊 as D𝑡 (𝑊 ,𝑊 ) = 𝑊 𝑗𝑙 𝑓 (cid:33) . (cid:32)𝑊𝑗𝑙 𝑊 𝑗𝑙 𝑗𝑙 It is known that 𝑓 -divergence is zero if and only if 𝑊 = 𝑊 ; see Polyanskiy et al. 2025, Theorem 2.3. Using the above premetric as the retention gate and setting (cid:101)ℓ (𝑊 ; k𝑡, v𝑡 ) = 𝑊 𝑊𝑡 1, ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) in (Learning-Retaining Viewpoint), we get the update rule 𝑊𝑡 = 𝑊𝑡 1 𝑔 (𝜁𝑡 𝜂𝑡 ℓ (𝑊𝑡 1; k𝑡, v𝑡 )) . Here 𝑔() is the inverse of the mapping 𝑓 , i.e., 𝑔(𝑓 (𝜏)) = 𝜏, 𝜏; the operator denotes the Hadamard (elementwise) product, and 𝜁𝑡 should be chosen such that 𝑊𝑡 1 = 𝑐. Notice that since the function 𝑓 () is strictly convex and smooth, its derivative is strictly increasing and hence 𝑔() is well defined. Conversely, for any strictly monotone function 𝑔(), we can find its inverse function 𝑔1 (which is strictly increasing) and define 𝑓 (𝜏) = const + 𝑔1(𝜏 )𝑑𝜏 . The term const 𝜏 =0 should be chosen such that 𝑓 (1) = 0. Then the update rule in (18) can be interpreted by the 𝑓 -divergence regularization, as explained above. Therefore, one can directly choose continuous monotonically increasing function 𝑔() and use (18) for memory update. (18) Specializing to KL divergence. Let us further make the above update rule explicit by using special function 𝑓 . If we choose 𝑓 (𝜏) = 𝜏 ln(𝜏), then the 𝑓 -divergence becomes the widely used KL divergence measure 𝐷𝑡 (𝑊 ,𝑊𝑡 1) = (cid:205)𝑗𝑙 𝑊𝑗𝑙 log (cid:16) 𝑊𝑗𝑙 (cid:17). In addition, we can also utilize the Shannon entropy as the global retention by regularizing deviations from uniform distribution, i.e., 𝐺𝑡 (𝑊 ) = (cid:205)𝑗𝑙 𝑊𝑗𝑙 log(𝑊𝑗𝑙 ). Combining these choices of the local and global retention gates, we obtain the overall retention gate (𝑊𝑡 ) 𝑗𝑙 Ret𝑡 (𝑊 ,𝑊𝑡 1) = 1 𝜂𝑡 𝑗𝑙 𝑊𝑗𝑙 log (cid:19) (cid:18) 𝑊𝑗𝑙 (𝑊𝑡 ) 𝑗𝑙 + 1 𝛼𝑡 𝑗𝑙 𝑊𝑗𝑙 log(𝑊𝑗𝑙 ) Choosing the attentional bias (cid:101)ℓ (𝑊 ; k𝑡, v𝑡 ) = 𝑊 𝑊𝑡 1, ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) and the above retention gate will lead to the update rule 𝑊𝑡 = arg min 𝑊 𝑊 𝑊𝑡 1, ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) + 1 𝜂𝑡 𝑗𝑙 𝑊𝑗𝑙 log (cid:19) (cid:18) 𝑊𝑗𝑙 (𝑊𝑡 ) 𝑗𝑙 + 1 𝛼𝑡 𝑗𝑙 𝑊𝑗𝑙 log(𝑊𝑗𝑙 ) s.t. 𝑗𝑙 𝑊𝑗𝑙 = 𝑐, 𝑊𝑗𝑙 0, 𝑗𝑙 (19) (20) Attaching the Lagrange multiplier to the first constraint, the KKT conditions implies (ℓ (𝑊𝑡 1; k𝑡, v𝑡 )) 𝑗𝑙 + (cid:18) 1 𝜂𝑡 + (cid:19) 1 𝛼𝑡 (cid:0)1 + log𝑊𝑗𝑙 (cid:1) 1 𝜂𝑡 log (cid:0)(𝑊𝑡 1) 𝑗𝑙 (cid:1) + 𝜇𝑡 = 0, 𝑗, 𝑙 where 𝜇𝑡 should be chosen such that (cid:205)𝑗𝑙 𝑊𝑗𝑙 = 𝑐. Rearranging the terms and defining 𝜆𝑡 = the update rule 1/𝛼𝑡 1/𝛼𝑡 +1/𝜂𝑡 , 𝜂 𝑡 = 1 1/𝛼𝑡 +1/𝜂𝑡 , we get 𝑊𝑡 𝑐 Softmax (cid:0)(1 𝜆𝑡 ) log(𝑊𝑡 1) 𝜂 𝑡 ℓ (𝑊𝑡 1; k𝑡, v𝑡 )(cid:1) (21) where 𝜆𝑡 (0, 1) and 𝜂 R+ are the parameters that can be learned during training. The Softmax operator ensures that the output lies in the set W. Notice that while all above calculations are done for matrix 𝑊 , similar update rule holds for other forms of parameters such as when 𝑊 is neural network (or when the parameter 𝑊 is normalized per slice). Variant 2: Elastic Net Regularization: Hard and Soft Forgetting. Elastic net is powerful and popular tool in regression analysis to balance the feature selection capabilities of LASSO (Tibshirani 1996) and bias reduction properties of Ridge regression (Hilt et al. 1977; Hoerl et al. 1970). It has been widely used in different applications due to its ability to handle high-dimensional data and mitigate the effects of multicollinearity. Given this success, natural question is what happens if we use this regularization scheme in our context. Let us start based on (Learning-Retaining Viewpoint) to design our memorization scheme. In (Learning-Retaining Viewpoint), we discussed that the loss function (cid:101)ℓ𝑡 (𝑊 ; k𝑡, v𝑡 ) is an approximation of the original function ℓ (), measuring our goodnessof-fit. Regularizing this loss with elastic net regularizer, we obtain the approximation (cid:101)ℓ𝑡 (𝑊 ; k𝑡, v𝑡 ) = 𝑊 𝑊𝑡 1, ℓ (𝑊𝑡 1; k𝑡, v𝑡 ). 11 with global retention of G𝑡 (𝑊 ) = 1 𝛼 𝑊 1. To fully specify the update rule of (Learning-Retaining Viewpoint), we also need to specify the premetric functions D𝑡 (, ). For the sake of keeping the update rule simple (and parallelizable), we can choose 2𝛽 𝑊 2 2 + 1 D𝑡 (𝑊 ,𝑊𝑡 1) = 𝑊 𝑊𝑡 12 2. 1 2 These choices of the attentional bias and retention gate leads to the following update rule: where 𝛾 = , 𝜆 = operator is defined as 𝜂𝛽 𝛼 (𝜂+𝛽 ) 𝑊𝑡 = S𝛾 (𝜆𝑊𝑡 1 𝜁 ℓ (𝑊𝑡 1; k𝑡, v𝑡 )) , (22) 𝛽 𝛽+𝜂 , 𝜁 = 𝜂𝜆, and S𝛾 is the soft thresholding operator, applied element-wise. For each element, this S𝛾 (𝑧) = sign(𝑧) max {0, 𝑧 𝛾 } . In other words, for large values of 𝑧, S𝛾 (𝑧) makes 𝑧 closer to zero by 𝛾 amount. If it is already in the 𝛾-vicinity of zero, then it makes it zero (hard forget). Equation (22) can be viewed as combination of soft forgetting (obtained by multiplying 𝑊 by 𝜆 (0, 1), and hard forgetting (if it is smaller than 𝛾). The hyperparameters 𝛾, 𝜆, and 𝜁 can be learned. Notice that since the shrinkage operator is not differentiable, we can approximate it with its smooth approximation. For example, we can use S𝛾 (𝑧) 𝑧 arctan(𝑧/𝛾 ) 𝜋 /2 . Variant 3: Elastic Net Regularization: Forgetting via Soft-thresholding. The elastic net regularizer can also be used in the (FTRL Viewpoint). In particular, in (FTRL Viewpoint), we can set 1 𝜂𝑡 𝑅𝑡 (𝑊 ) = 1 𝜂 𝑊 2 + 1 𝛼 𝑊 1 and use (cid:98)ℓ (𝑊 ; 𝑥𝑖 ) = 𝑊 𝑊𝑖 1, ℓ (𝑊𝑖 1; 𝑥𝑖 ). Assuming initialization at 𝑊0 = 0, these choices of attentional bias and retention gate leads to the update rules: 𝐴𝑡 = 𝐴𝑡 1 𝜂ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) 𝑊𝑡 = S𝜂/𝛼 (𝐴𝑡 ) (23) Here S𝜂/𝛼 () is the soft-thresholding operator with parameter 𝜂/𝛼, which can be smoothly as explained in Variant 1.1. Variant 4: General 𝐿𝑞 Memory Stability. Existing work is based on the retention gate choices D𝑡 (𝑊 ,𝑊𝑡 1) = 𝑊 𝑊𝑡 12 𝐹 or 𝑅(𝑊 ) = 𝑊 2 2. However, one can choose other choices of retention gate. For example, in (FTRL Viewpoint), we can choose 𝐿𝑞 norm as the regularizer 𝑅(𝑊 ). More specifically, for 1 < 𝑞 2, we can set 1 𝜂𝑡 𝑅(𝑊 ) = 1 2𝜂 (𝑞 1) 𝑊 2 𝑞. Using this retention gate and choosing (cid:98)ℓ𝑖 (𝑊 ; k𝑡, v𝑡 ) = 𝑊 𝑊𝑖 1, ℓ (𝑊𝑖 1; k𝑡, v𝑡 ) in (FTRL Viewpoint), leads to the update 𝑖=1 ℓ (𝑊𝑖 1; k𝑡, v𝑡 ); see Shalev-Shwartz et al. 2012, Section 2.6. Here, rule 𝑊𝑡 = 𝜂 denotes the Hadamard (element-wise) product and is the element-wise absolute value operator. Assuming 𝑊0 = 0, this update rule can be recursively written as: 𝑞1 and 𝐴𝑡 = (cid:205)𝑡 , where 𝑝 = 𝐴𝑡 𝐴𝑡 𝑝 2 𝑝 𝑞 𝐴𝑡 = 𝐴𝑡 1 𝜂ℓ (𝑊𝑖 1; k𝑡, v𝑡 ), and 𝑊𝑡 = 𝐴𝑡 𝐴𝑡 𝑝 2 𝑝 . Variant 5: Bregman Divergence as Retention Gate.. Another natural choice is to use Bregman divergence as retention gate, leading to mirror descent-type algorithms. In particular, given smooth strictly convex function 𝑓 () : R, we can define the function 𝐹 (𝑊 ) = (cid:205)𝑗𝑙 𝑓 (𝑊𝑗𝑙 ). Based on this choice of function 𝐹 , we define the Bregman divergence 𝐷𝑡 (𝑊 ,𝑊 ) = 𝐹 (𝑊 ) 𝐹 (𝑊 ) 𝑊 ,𝑊 𝑊 12 as our parametric function. Utilizing this retention gate and choosing (cid:101)ℓ𝑡 (𝑊 ; k𝑡, v𝑡 ) = 𝑊 𝑊𝑡 1, ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) in (Learning-Retaining Viewpoint), we obtain the update rule 𝑊𝑡 = 𝑔 (𝜂ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) + 𝐹 (𝑊𝑡 1)) . Here, 𝐹 is the mapping obtained by applying 𝑓 () (the derivative of 𝑓 ) element-wise to all entries of its input matrix argument. The function 𝑔 is the inverse of the mapping 𝐹 (), i.e., 𝑔(𝐹 (𝑊 )) = 𝑊 . If we choose 𝑓 (𝜏) = 𝜏 2 2 , then 𝐹 (𝑊 ) becomes the identity mapping and so is 𝑔. Therefore, the above update becomes simple gradient descent with no nonlinearity involved in the update rule. However, other choices of 𝑓 () introduces additional nonlinearity in 𝑔(), which can enhance the expressivity of our memory. For example, we can choose the function 𝑓 () so that its derivative becomes the inverse sigmoid function, i.e., 𝑓 (𝜏) = ln (cid:0) 𝜏 (cid:1) with 𝑓 : (0, 1) R. Since 𝑓 () is strictly 1𝜏 increasing, then the function 𝑓 () (and hence 𝐹 ()) is strictly convex. Therefore, the Bregman divergence is well defined. with 𝑔 : (0, 1). Moreover, the inverse of the function 𝑓 () becomes the sigmoid function, i.e., 𝑔(𝜏) = 𝜎 (𝜏) = Then, the update of the memory becomes exp(𝜏 ) 1+exp(𝜏 ) 𝑊𝑡 = 𝜎 (cid:18) ln (cid:19) (cid:18) 𝑊𝑡 1 𝑊𝑡 𝜂ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) (cid:19) , where 𝜎 is the sigmoid function operated element-wise on the entries of 𝑊 , and the division operator 𝑊𝑡 1𝑊𝑡 element-wise. This update rule guarantees that the elements of 𝑊𝑡 remains within the interval (0, 1). is also performed"
        },
        {
            "title": "5.3 Miras’s Variants: Moneta, Yaad, and Memora",
            "content": "In the previous section we discussed different potential choices for attentional bias and retention gate to show the generality and the potential of Miras. In this section, building upon our framework, we present three novel sequence models, each of which designed based on different motivation, and discuss how they can leverage fast parallel training. Moneta. Given 𝑝, 𝑞 R1, we design (𝑝, 𝑞)-Moneta as the variant of Miras as follows: (1) For the choice of memory architecture, we use an MLP with 2 layers with expansion factor of 4 and GELU activation function (Hendrycks et al. 2016). We also use residual connections and layer norm, resulting in (𝑥) = 𝑥 + LN(𝑊1𝜎 (𝑊2𝑥)). (2) We choose ℓ𝑝 -attentional bias (introduced in Equation 11) for Moneta. (3) For the choice of retention gate, we use the hybrid of ℓ𝑞 retention gate 2(𝑞1) 𝑊 2 2. (4) Finally, we use gradient descent as the memory learning algorithm. The above choices, result in the following recurrent formula for the memory module: 𝑞 (see Section 5.2 for details) and the standard ℓ2 regularization 1 𝛽 𝑊 2 1 𝐴𝑡 = 𝛼𝑡𝐴𝑡 1 𝜂𝑡 ℓ𝑝 (𝑊𝑖 1; k𝑡, v𝑡 ), and 𝑊𝑡 = 𝐴𝑡 𝐴𝑡 𝑞2 𝑞 . Notably the gradient can be calculated using: ℓ (𝑊𝑡 1; k𝑡, v𝑡 ) = 𝑝 𝜂𝑡 (cid:0)Sign(𝑊 k𝑡 v𝑡 ) 𝑊 k𝑡 v𝑡 𝑝 1(cid:1) 𝑡 . We use (𝑝, 𝑞) = (3, 4). (24) (25) Yaad. Building upon our discussion on the importance of robust memory that protects itself from extreme events (tokens), we design Yaad based on Huber objective. That is, in Miras, for the choice of memory structure, we follow Moneta and use an MLP with the same architecture as above; for the choice of attentional bias, we use Huber loss (defined in Equation 16); for the choice retention gate, for the sake of simplicity, we use combination of local and global retention as Ret𝑡 (𝑊 ,𝑊𝑡 1) = 1 2, which is equivalent to the forget gate mechanism introduced by Behrouz 2𝜃𝑡 et al. (2024c); and finally, we simply use gradient descent as the memory learning algorithm. Given the above choices, we can write the resulted memory learning process as follows: 𝑊 𝑊𝑡 1 + 1 𝛽𝑡 𝑊 2 𝑊𝑡 = 𝛼𝑡𝑊𝑡 1 (cid:40)𝜂𝑡 ℓ2(𝑊𝑡 1; k𝑡, v𝑡 ) 𝜂𝑡 𝛿𝑡 ℓ1(𝑊𝑡 1; k𝑡, v𝑡 ) Otherwise. if (k𝑡 ) v𝑡 𝛿𝑡, (26) Note that for improving the expressive power, in all architectures, we decouple the learning rate 𝜂 and the retention gate rate 𝛼, resulting in an independent parameter 𝛽𝑡 [0, 1]𝑑 . 13 Figure 2: Visualization of the Mirass variant architecture, their hybrid counterpart with SWA, and block design of Miras layer. Memora. Finally, in Memora, we use the idea of elastic net regularization (i.e., hard and soft retention). To this end, in Miras: (1) For the choice of memory architecture, similar to above variants, we use an MLP (the same architecture as the previous variants). (2) For the choice of attentional bias, we use simple ℓ2 regression loss. (3) For the choice of retention gate we use KL divergence as in Equation 21. (4) Finally, we optimize the memory using gradient descent, resulting in the following update rule: 𝑊𝑡 = Softmax (𝛼𝑡 log(𝑊𝑡 1) 𝜂𝑡 ℓ2(𝑊𝑡 1; k𝑡, v𝑡 )) (27)"
        },
        {
            "title": "5.4 Architecture Backbone and Fast Training",
            "content": "Architectural Backbone. For the architectural backbone, we fully follow recent studies (Behrouz et al. 2024c; Yang et al. 2024a): We replace attention modules with our variants of Miras in Llamas macro architecture with MLPs with SwiGLU(.) activation, rotary positional encodings (RoPE) (Su et al. 2024), and RMSNorm (Zhang et al. 2019). For Miras layer block, we follow the recent modern linear recurrent models (Behrouz et al. 2024c; Yang et al. 2024a), and incorporate 1D depthwise-separable convolution layer (with kernel size of 4) after each of the query, key, and value projections. For the sake of training stability, we also use ℓ2 normalization to and k. The output of Miras layer block is normalized and gated with linear layer (Mehta et al. 2023). Channel-wise Parameters. For learnable parameters of 𝜂𝑡, 𝛿𝑡 and the retention gate of 𝛼𝑡 we use channel-wise parametrization, i.e., 𝜂𝑡, 𝛿𝑡, 𝛼𝑡 R𝑑 . While gaining more expressive power, this parametrization results in significant parameter increase. To mitigate this issue, following Peng et al. (2025b), we use low-rank projections to project the input into R𝑘 and then to R𝑑 , where 𝑘 is hyperparameter (usually 32 or 64). The backbone architecture is illustrated in Figure 2. Hybrid Models. We also evaluate the hybrid version of Mirass variants. For hybrid models, we follow the Samba (Ren et al. 2024) architecture, in which we sequentially combine our Miras layer with Sliding Window Attention (SWA). The illustration of hybrid model Figure 2. Parallelizable Training. While the design of Mirass variant are theoretically well-motivated, their recurrence is non-linear, potentially make their straightforward training slow for large scales. In this section, we build upon the work of Behrouz et al. (2024c) and Sun et al. (2024) to make the training parallelizable. The main idea is to divide the sequence into 14 chunks with size 𝑏 (usually is 16 or 64) and calculate the gradient for all tokens in the current chunk with respect to the last state of the memory in the previous chunk. That is, we use ℓ (M𝑡 ; k𝑡, v𝑡 ) instead of ℓ (M𝑡 1; k𝑡, v𝑡 ), where 𝑡 is the last state in the previous chunk. Given the above trick, we can calculate all gradients at once and make the recurrence inside each chunk linear. However, to fully take advantage of accelerators, we need to reformulate the process as matrix multiplication. For Moneta, for the sake of clarity, assume 𝑞 = 2. We follow the same algorithm as Behrouz et al. (2024c) and expand the recurrence as follows: M𝑡 = 𝛼𝑡 M𝑡 1 𝜂𝑡 ℓ (M𝑡 1; k𝑡, v𝑡 ) 𝑡 = 𝛽𝑡 M0 𝜂𝑖 ℓ (M𝑡 ; k𝑖, v𝑖 ), 𝛽𝑡 𝛽𝑖 𝑖=1 (28) where 𝑡 = 𝑡 mod(𝑡, 𝑏), and 𝛽𝑖 = (cid:206)𝑖 𝑗=1 𝛼 𝑗 . For the sake of clarity, we focus on the first chunk, i.e., 𝑡 = 𝑏 and so 𝑡 = 0, and explain the process for the case that M𝑡 = 𝑊𝑡 is linear. The process for 2-layer MLPs and other chunks is similar. Using ℓ𝑝 loss function, we have: ℓ (𝑊0; k𝑡, v𝑡 ) = 𝑝 (cid:0)Sign(𝑊0k𝑡 v𝑡 ) 𝑊0k𝑡 v𝑡 𝑝 1(cid:1) 𝑡 𝑏 ℓ (𝑊0; ; k𝑖, v𝑖 ) = 𝑝E𝑏 B𝑏 Sign(𝑊 k𝑡 v𝑡 ) (𝑊0 V𝑝 1) K, 𝜂𝑖 𝛽𝑏 𝛽𝑖 𝑖=1 (29) 𝜂2 . . . 𝜂𝑏 (cid:3) and B𝑏 is defined analogously on 𝛽𝑏 𝛽𝑖 where E𝑏 = (cid:2)𝜂1 s. For the sake of stablity in training, we use 𝑥 2 + 𝜖, where 𝜖 > 0 is small number (i.e., 𝜖 = 1𝑒 6). As discussed in Equation 24, the Sign(𝑥) tanh (𝛼𝑥) and 𝑥 = case that 𝑞 2 appears as normalization term on the memory. Similar to Titans (Behrouz et al. 2024c) and TTT (Sun et al. 2024), we do not apply this non-linearity inside each chunk and instead use it at the end of each chunk. For Yaad, the process is very similar to the above. We calculate the gradient of both ℓ1 and ℓ2 loss and use masking based on (k𝑡 ) v𝑡 𝛿𝑡 . For Memora, the update rule has two non-linear part, i.e., softmax and log, making the model hardly parallelizable. To this end, as discussed above, we use its linear version inside each chunk and its non-linear version across chunks. However, using both log and softmax at the end of each chunk removes the effect of log. To this end, we consider lag tokens after each chunk (i.e., tokens with index 𝑖 = 𝑘𝑏 + 1, where 𝑏 is the chunk size and 𝑘 Z+). That is, let M0 be the last state of the memory in previous chunk, we have: and then we use M1 for the next chunk. Again, for the sake of clarity, assume that memory is linear, i.e., M1 = 𝑊1: M1 = Softmax (𝛼1 log(M0) 𝜂1ℓ2(M0; k1, v1)) , ℓ (𝑊1; k𝑡, v𝑡 ) = (𝑊1k𝑡 v𝑡 ) 𝑡 𝑏 𝜂𝑖 𝛽𝑏 𝛽𝑖 𝑖=1 ℓ (𝑊1; ; k𝑖, v𝑖 ) = E𝑏 B𝑏 (𝑊1 V) K, (30) (31) (32) where matrices are defined the same as for Equation 29."
        },
        {
            "title": "6 Experiments",
            "content": "In our experimental evaluations, we aim to answer three main questions: (1) Does different attentional biases results in different architectures in practice? (2) How does different types of retention gates (i.e., retention gate) affect the performance of the model in long context? (3) How do Memora, Moneta, and Yaad perform in downstream tasks compare to baselines? Setup. We train our models with training context window of size 4096 using either FineWeb-Edu dataset (Penedo et al. 2024) (for LM and common-sense reasoning tasks) or C4 dataset (Raffel et al. 2020) (for scaling patterns). We use model 15 Figure 3: Scaling patterns when increasing (Left) model size, (Middle) sequence length (model size = 340M) (3) (Right) sequence length (model size = 760M) on C4 dataset. sizes of 120M, 340M, 760M, and 1.3B parameters. We train small models (120M and 340M) on 15B tokens sampled from the dataset, the medium size model (760M) on 30B tokens, and the large model on 100B tokens. Baseline results are reported by Behrouz et al. (2024c)."
        },
        {
            "title": "6.1 Language Modeling and Common-sense Reasoning",
            "content": "We follow recent studies (Behrouz et al. 2024c; Yang et al. 2024a,c) and first focus on the perplexity in language modeling and also commonsense reasoning tasks. The results for Memora, Yaad, Moneta and also baselines with size of 340M, 760, and 1.3B are reported in Table 2. All of our variants outperforms all the baselines including Transformer++, modern linear recurrent models and hybrid methods. The superior performance compared to hybrid models is particularly important as all of our variants are pure recurrent (attention-free). Among the three variants of Miras, while Moneta achieves slightly weaker performance than Memora, and Yaad, the other two variants are close and depending on the task and model size, the best model can vary."
        },
        {
            "title": "6.2 Scaling Pattern",
            "content": "To evaluate the scaling pattern of models and for comparing them with baseline, in this section, we plot their performance with varying the model size and the context window. Context Length. We first vary the training context length from 2K to 32K for two version of our model with size 340M and 760M. The results are reported in Figure 3 (Middle and Right). All three variants of Miras scales better than state-of-the-art baselines when increasing the context length. We attribute this superior performance to: (1) expressive memory architecture. Contrary to baselines like Mamba2 and GSA that uses vectorand matrix-valued memory, our variants are using 2-layer MLPs with more expressive power to learn from longer sequences. (2) The choice of retention gate and attentional bias: All of our three variants go beyond the standard attentional biases and retention gates. These choices can help the memory to better manage its fixed-size capacity. Model Size. We also report the #FLOPs vs. perplexity of our models and baselines in Figure 3 (Left). All three variants outperforms all baselines given almost the same budget of FLOPs. These results, once again support the importance of powerful memory design."
        },
        {
            "title": "6.3 Needle In Haystack",
            "content": "To evaluate the effective context window of our models and baselines, we use needle-in-haystack task. In this task, we evaluate the model on retrieving piece of information (i.e., the needle) from long distractor texts (i.e., the haystack). We focus on the Single NIAH (S-NIAH) task from RULER benchmark (Hsieh et al. 2024) and evaluate our models and baselines on sequences with length 1K, 2K, 4K, and 8K. The results are reported in Table 3. All our variants outperforms all the baselines with considerable margin. Interestingly, Moneta shows better performance than others when the data is synthetic noise (S-NIAH-PK). This observation validates the effectiveness of 𝑝-norm objective and retention gates as they are more robust to noise. 16 Table 2: Performance of Mirass variants and baselines on language modeling and common-sense reasoning tasks. Hybrid models are marked with . The best results of simple and hybrid models are highlighted. In largest scale, we compare our simple models with even hybrid models and highlight the best results. Model Wiki. ppl LMB. ppl Transformer++ RetNet GLA Mamba DeltaNet TTT Gated DeltaNet Moneta (ours) Yaad (ours) Memora (ours) Transformer++ RetNet Mamba2 DeltaNet TTT Gated DeltaNet Samba Gated DeltaNet-H2 Moneta (ours) Yaad (ours) Memora (ours) Moneta-H (ours) Yaad-H (ours) Memora-H (ours) Transformer++ RetNet Mamba2 DeltaNet Gated DeltaNet Samba Gated DeltaNet-H2 Moneta (ours) Yaad (ours) Memora (ours) 31.52 32.50 28.51 30.83 28.65 27.44 27.01 26.19 26.61 27.16 25.21 26.08 22.94 24.37 24.17 21.18 20.63 19.88 21.18 20.99 22. 18.72 18.59 18.24 18.53 19.08 16.56 17.71 16.42 16.13 15.91 15.52 15.18 15.90 41.08 49.73 43.02 40.21 47.30 34.19 30.94 29.31 29.11 30.44 27.64 24.45 28.37 24.60 23.51 22.09 22.71 20. 21.94 21.57 22.31 20.13 19.80 20.55 18.32 17.27 12.56 16.88 12.17 13.29 12.55 11.47 11.89 12.04 LMB. PIQA Hella. Wino. ARC-e ARC-c acc_n acc acc_n acc acc acc 340M params / 15B tokens 30.76 28.24 28.73 29.94 28.43 30.06 34.11 35.70 34.09 33. 62.98 62.61 64.05 63.79 63.52 63.97 63.08 63.99 64.93 65.21 34.76 34.15 35.96 35.88 35.95 35.71 38.12 39.23 39.86 39.17 50.53 50.91 50.00 49.82 49.63 50.08 51.60 52.04 51.12 51. 760M params / 30B tokens 35.78 34.51 33.54 37.06 34.74 35.54 39.72 39.18 38.02 37.85 38.19 40.59 40.22 39.91 66.92 67.19 67.90 66.93 67.25 68.01 69.19 68.95 69.55 69.14 67. 70.84 69.51 69.06 42.19 41.63 42.71 41.98 43.92 44.95 47.35 48.22 49.16 50.02 49.30 50.13 50.48 49.84 51.95 52.09 49.77 50.65 50.99 50.73 52.01 52.57 53.01 53.93 53. 54.17 53.69 52.88 1.3B params / 100B tokens 42.60 40.52 45.66 42.46 46.65 44.94 48.76 47.88 47.23 48.67 70.02 70.07 71.87 70.72 72.25 70.94 72.19 73.16 72.81 73. 50.23 49.16 55.67 50.93 55.76 53.42 56.88 56.14 56.46 55.99 53.51 54.14 55.24 53.35 57.45 55.56 57.77 59.09 59.02 57.36 45.21 44.27 54.19 49.24 52.68 53.01 55.28 55.96 54.75 53. 60.38 63.17 63.48 64.87 64.53 66.87 66.92 67.01 67.47 67.78 63.57 67.64 68.04 66.90 68.83 67.34 72.47 68.47 71.21 68.81 71.33 72.53 72.14 71.55 24.05 23.62 24.29 24.56 25.37 26.11 26. 27.15 28.64 27.99 32.46 32.78 31.09 31.39 33.81 33.09 33.20 35.49 36.09 36.27 36.15 36.79 36.55 36.12 35.10 33.78 37.88 35.66 38.39 36.17 39.07 40.32 40.05 37. SIQA BoolQ acc acc 36.81 36.79 37.13 35.41 37.96 37.32 34.89 37.29 33.82 34.1 39.51 38.36 40.06 39.88 40.16 39.21 38.98 39.39 40.53 41.01 40.94 40.87 40.28 40. 40.66 40.78 40.20 40.22 40.63 39.96 41.91 41.91 40.73 40.19 58.24 59.72 58.39 60.07 58.79 59.83 59.54 60.22 60.29 59.29 60.37 57.92 58.15 59.02 59.58 59.14 61.24 61.11 63.18 63.34 62. 62.43 61.94 61.75 57.09 60.39 60.13 55.29 60.24 62.11 61.55 61.18 61.86 61."
        },
        {
            "title": "6.4 Ablation Study",
            "content": "In this section we perform ablation studies to validate if different design choices that we discussed through the paper are positively contributing for achieving better results. The Effect of 𝑝 on Performance. We first evaluate the effect of 𝑝 on the performance of Moneta. We vary the value of 𝑝 {1, 1.5, 2, 2.8, 3, 3.2, 4} and context window from 2K to 16K. The results are reported in Figure 4. Interestingly, there is no monotone pattern when increasing the value of 𝑝 and the best performance is achieved when 𝑝 = 3, while 𝑝 = 4 17 Table 3: Performance of Moneta, Yaad, Memora, and baselines on NIAH task from RULER benchmark. The best results with highest accuracy are highlighted. Model S-NIAH-PK S-NIAH-N S-NIAH-W 2K 4K 8K 2K 4K 8K 1K 2K 4K Average Mamba2 DeltaNet Gated DeltaNet TTT Moneta Yaad Memora 98.6 96.8 89.8 98.4 99.4 99.2 99.2 61.4 98.8 91.4 98.8 98.8 98.6 98. 31.0 98.6 90.0 98.0 98.8 94.4 92.6 98.4 47.2 99.2 60.2 99.4 99.8 98.4 55.8 15.4 91.8 36.6 99.4 98.6 99. 14.2 12.8 26.4 10.2 92.8 93.2 93.2 62.2 85.2 86.4 85.8 92.2 91.8 92.4 42.2 46.2 82.6 78.8 88.2 89.6 88. 4.2 20.0 24.4 28.0 70.8 67.4 70.4 52.0 57.9 75.8 66.1 93.5 92.9 92.1 achieves the worst performance. Also, although different values of 𝑝 results in different memory modules with varied performance, the scaling pattern when increasing the context length is almost the same. The Effect of 𝑞 on Performance. Similarly, we evaluate the effect of 𝑞 by varying it in {2, 3, 4, 5}. Interestingly, contrary to 𝑝, the value of 𝑞 can change the scaling pattern when increasing the context length. The main reason for this observation is that the value of 𝑞 determines the retention gate and powerful retention gate can improve the memory management, resulting in better performance. The Effect of Design. To evaluate the architectural design choices, we perform an ablation study on Yaad. The results are in Table 4. The first row, reports the performance of Yaad, while (1) the second row removes the retention (i.e., 𝛽 = 1), (2) third row make 𝛿 input independent, (3) the third row removes ℓ2-loss from the Huber loss, (4) the forth row removes the ℓ1 condition, and (5) the last row replaces the MLP with linear layer. These results indicate that all design choices are contributing to the performance of the model. Table 4: Ablation study on the components of Yaad. Model Yaad - Retention Gate - Input-dependent 𝛿 ℓ2-loss ℓ1-loss linear memory Avg. LM 53.98 50.63 52.19 52.86 53.04 51. Figure 4: The effect of parameters 𝑝 and 𝑞 on the performance with different context length."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we present Miras, general framework that explains the connection of online optimization and test time memorization. Miras framework can explain the role of several standard architectural choices in the literature (e.g., forget gate) and helps design next generation of architectures that are capable of managing the memory better. Building upon our framework, we present three novel sequence models, each of which with its own (dis)advantages. Our experimental evaluations show that all these variants are more powerful than Transformers and linear RNNs, in various downstream tasks. In this work, we present diverse set of variants using Miras. In future, exploring these alternative architectures for different downstream tasks is an interesting future direction."
        },
        {
            "title": "References",
            "content": "[1] Ali Behrouz, Parsa Delavari, and Farnoosh Hashemi. Unsupervised Representation Learning of Brain Activity via Bridging Voxel Activity and Functional Connectivity. In: Forty-first International Conference on Machine Learning. 2024. url: https://openreview.net/forum?id=nOjZfpLyh1. [2] Ali Behrouz, Michele Santacatterina, and Ramin Zabih. Mambamixer: Efficient selective state space models with dual token and channel selection. In: arXiv preprint arXiv:2403.19888 (2024). [3] Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. In: arXiv preprint arXiv:2501.00663 (2024). [4] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of transformer: memory viewpoint. In: Advances in Neural Information Processing Systems 36 (2023), pp. 15601588. [5] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In: Proceedings of the AAAI conference on artificial intelligence. Vol. 34. 2020, pp. 74327439. [6] Leon Bottou and Vladimir Vapnik. Local learning algorithms. In: Neural computation 4.6 (1992), pp. 888900. [7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Ed. by Jill Burstein, Christy Doran, and Thamar Solorio. Minneapolis, Minnesota: Association for Computational Linguistics, June 2019, pp. 29242936. doi: 10.18653/v1/N19-1300. url: https: //aclanthology.org/N19-1300/. [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. In: arXiv preprint arXiv:1803.05457 (2018). Imre Csiszar. On information-type measure of difference of probability distributions and indirect observations. In: Studia Sci. Math. Hungar. 2 (1967), pp. 299318. [9] [10] Róbert Csordás, Christopher Potts, Christopher Manning, and Atticus Geiger. Recurrent Neural Networks Learn to Store and Generate Sequences using Non-Linear Representations. In: Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP. 2024, pp. 248262. [11] Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, et al. One-Minute Video Generation with Test-Time Training. In: arXiv preprint arXiv:2504.05298 (2025). [12] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured [13] state space duality. In: arXiv preprint arXiv:2405.21060 (2024). Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. In: arXiv preprint arXiv:2402.19427 (2024). [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In: arXiv preprint arXiv:2010.11929 (2020). [15] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-time training with masked autoencoders. In: Advances in Neural Information Processing Systems 35 (2022), pp. 2937429385. [16] Xavier Gonzalez, Andrew Warrington, Jimmy Smith, and Scott Linderman. Towards scalable and stable parallelization of nonlinear rnns. In: Advances in Neural Information Processing Systems 37 (2024), pp. 58175849. [17] Riccardo Grazzi, Julien Siems, Jörg KH Franke, Arber Zela, Frank Hutter, and Massimiliano Pontil. Unlocking state-tracking in linear rnns through negative eigenvalues. In: arXiv preprint arXiv:2411.12537 (2024). [18] Klaus Greff, Rupesh Srivastava, Jan Koutnk, Bas Steunebrink, and Jürgen Schmidhuber. LSTM: search space odyssey. In: IEEE transactions on neural networks and learning systems 28.10 (2016), pp. 22222232. [19] Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. In: First Conference on Language Modeling. 2024. url: https://openreview.net/forum?id=tEYskw1VY2. [20] Albert Gu, Karan Goel, and Christopher Re. Efficiently Modeling Long Sequences with Structured State Spaces. In: International Conference on Learning Representations. 2022. url: https : / / openreview . net / forum ? id = uYLFoz1vlAC. 19 [21] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid Structural State-Space Models. In: The Eleventh International Conference on Learning Representations. 2023. url: https://openreview.net/forum?id=g4OTKRKfS7R. [22] Trevor Hastie, Robert Tibshirani, Jerome Friedman, et al. The elements of statistical learning. 2009. [23] Elad Hazan et al. Introduction to online convex optimization. In: Foundations and Trends in Optimization 2.3- (2016), pp. 157325. [24] Donald Olding Hebb. The organization of behavior: neuropsychological theory. Psychology press, 2005. [25] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). In: arXiv preprint arXiv:1606.08415 (2016). [26] Donald Hilt and Donald Seegrist. Ridge, computer program for calculating ridge regression estimates. Vol. 236. Department of Agriculture, Forest Service, Northeastern Forest Experiment . . ., 1977. [27] Arthur Hoerl and Robert Kennard. Ridge regression: applications to nonorthogonal problems. In: Technometrics [28] 12.1 (1970), pp. 6982. John Hopfield. Neural networks and physical systems with emergent collective computational abilities. In: Proceedings of the national academy of sciences 79.8 (1982), pp. 25542558. [29] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. RULER: Whats the Real Context Size of Your Long-Context Language Models? In: First Conference on Language Modeling. 2024. url: https://openreview.net/forum?id=kIoBbc76Sy. Jerry Yao-Chieh Hu, Dennis Wu, and Han Liu. Provably optimal memory capacity for modern hopfield models: Transformer-compatible dense associative memories as spherical codes. In: arXiv preprint arXiv:2410.23126 (2024). [31] Peter Huber. Robust estimation of location parameter. In: Breakthroughs in statistics: Methodology and distribution. [30] Springer, 1992, pp. 492518. [32] Kazuki Irie, Robert Csordas, and Jürgen Schmidhuber. Practical computational power of linear transformers and their recurrent and self-referential extensions. In: arXiv preprint arXiv:2310.16076 (2023). [33] Kazuki Irie, Imanol Schlag, Robert Csordas, and Jurgen Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. In: Advances in neural information processing systems 34 (2021), pp. 77037717. [34] Vidit Jain and Erik Learned-Miller. Online domain adaptation of pre-trained cascade of classifiers. In: CVPR 2011. [35] IEEE. 2011, pp. 577584. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. In: arXiv preprint arXiv:2001.08361 (2020). [36] M. Karami and V. Mirrokni. Lattice: Learning to Efficiently Compress the Memory. 2025. [37] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In: International conference on machine learning. PMLR. 2020, pp. 51565165. [38] Dmitry Krotov. Hierarchical associative memory. In: arXiv preprint arXiv:2107.06446 (2021). [39] Dmitry Krotov and John Hopfield. Dense associative memory for pattern recognition. In: Advances in neural information processing systems 29 (2016). [40] Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. In: arXiv preprint arXiv:2501.08313 (2025). [41] Chengxuan Li, Di Huang, Zeyu Lu, Yang Xiao, Qingqi Pei, and Lei Bai. survey on long video generation: Challenges, methods, and prospects. In: arXiv preprint arXiv:2403.16407 (2024). [42] Xiaoyu Li, Yuanpeng Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. On the expressive power of modern hopfield networks. In: arXiv preprint arXiv:2412.05562 (2024). [43] Yi Heng Lim, Qi Zhu, Joshua Selfridge, and Muhammad Firmansyah Kasim. Parallelizing non-linear sequential models over the sequence length. In: The Twelfth International Conference on Learning Representations. 2024. url: https://openreview.net/forum?id=E34AlVLN0v. [44] Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. Longhorn: State space models are amortized online learners. In: arXiv preprint arXiv:2407.14207 (2024). [45] Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. In: Transactions of the Association for Computational Linguistics 12 (2024), pp. 157173. [46] Elizabeth Loftus. The reality of repressed memories. In: American psychologist 48.5 (1993), p. 518. 20 [47] Carlo Lucibello and Marc Mézard. Exponential capacity of dense associative memories. In: Physical Review Letters [48] 132.7 (2024), p. 077301. Julien Mairal. Incremental majorization-minimization optimization with application to large-scale machine learning. In: SIAM Journal on Optimization 25.2 (2015), pp. 829855. [50] [49] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long Range Language Modeling via Gated State Spaces. In: The Eleventh International Conference on Learning Representations. 2023. url: https://openreview. net/forum?id=5MkYIYCbva. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer Sentinel Mixture Models. In: International Conference on Learning Representations. 2017. url: https://openreview.net/forum?id=Byj72udxe. [51] William Merrill, Jackson Petty, and Ashish Sabharwal. The Illusion of State in State-Space Models. In: Forty-first International Conference on Machine Learning. 2024. url: https://openreview.net/forum?id=QZgo9JZpLq. [52] Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, and Kayvon Fatahalian. Online model distillation for efficient video inference. In: Proceedings of the IEEE/CVF International conference on computer vision. 2019, pp. 35733582. [53] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned neural memory. In: Advances in Neural Information Processing Systems 32 (2019). [54] Tsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. In: Proceedings of the conference. Association for Computational Linguistics. Meeting. Vol. 1. NIH Public Access. 2017, p. 397. [55] Daniel Neil, Jun Haeng Lee, Tobi Delbruck, and Shih-Chii Liu. Delta networks for optimized recurrent network computation. In: International conference on machine learning. PMLR. 2017, pp. 25842593. [56] Hideyuki Okano, Tomoo Hirano, and Evan Balaban. Learning and memory. In: Proceedings of the National Academy of Sciences 97.23 (2000), pp. 1240312404. [57] Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In: International Conference on Machine Learning. PMLR. 2023, pp. 2667026698. [58] Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The LAMBADA dataset: Word prediction requiring broad discourse context. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Ed. by Katrin Erk and Noah A. Smith. Berlin, Germany: Association for Computational Linguistics, Aug. 2016, pp. 15251534. doi: 10.18653/v1/P16-1144. url: https://aclanthology.org/P16-1144/. [59] Guilherme Penedo, Hynek Kydlcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. In: The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024. url: https://openreview.net/forum?id=n6SCkn2QaG. [60] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan S. Wind, Stanisław Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the Transformer Era. In: The 2023 Conference on Empirical Methods in Natural Language Processing. 2023. url: https://openreview.net/ forum?id=7SaXczaBpG. [61] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. In: arXiv preprint arXiv:2404.05892 (2024). [62] Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, et al. RWKV-7\" Goose\" with Expressive Dynamic State Evolution. In: arXiv preprint arXiv:2503.14456 (2025). [63] Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Haowen Hou, Janna Lu, William Merrill, Guangyu Song, Kaifeng Tan, Saiteja Utpala, et al. Rwkv-7\" goose\" with expressive dynamic state evolution. In: arXiv preprint arXiv:2503.14456 (2025). [64] Yury Polyanskiy and Yihong Wu. Information theory: From coding to learning. Cambridge university press, 2025. [65] DL Prados and SC Kak. Neural network capacity using delta rule. In: Electronics Letters 25.3 (1989), pp. 197199. 21 [66] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. HGRN2: Gated Linear RNNs with State Expansion. In: First Conference on Language Modeling. 2024. url: https://openreview.net/ forum?id=y6SqbJfCSk. [67] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. In: Journal of machine learning research 21.140 (2020), pp. 167. [68] Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David Kreil, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. Hopfield Networks is All You Need. In: International Conference on Learning Representations. 2021. url: https: //openreview.net/forum?id=tL89RnzIiCd. [69] Meisam Razaviyayn, Mingyi Hong, and Zhi-Quan Luo. unified convergence analysis of block successive minimization methods for nonsmooth optimization. In: SIAM Journal on Optimization 23.2 (2013), pp. 11261153. [70] Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. In: arXiv preprint arXiv:2406.07522 (2024). [71] Lee Robertson. Memory and the brain. In: Journal of dental education 66.1 (2002), pp. 3042. [72] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd [73] [74] [75] schema challenge at scale. In: Communications of the ACM 64.9 (2021), pp. 99106. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In: International Conference on Machine Learning. PMLR. 2021, pp. 93559366. JH Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets. Accepted for publication in. In: Neural Computation (1992). Jürgen Schmidhuber. Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets. In: ICANN93: Proceedings of the International Conference on Artificial Neural Networks Amsterdam, The Netherlands 1316 September 1993 3. Springer. 1993, pp. 460463. Jürgen Schmidhuber and Sepp Hochreiter. Long Short-term Memory. In: Neural Computation MIT-Press (1997). [76] [77] Mark Schöne, Babak Rahmani, Heiner Kremer, Fabian Falck, Hitesh Ballani, and Jannes Gladrow. Implicit Language [78] [79] [80] [81] [82] Models are RNNs: Balancing Parallelization and Expressivity. In: arXiv preprint arXiv:2502.07827 (2025). Shai Shalev-Shwartz et al. Online learning and online convex optimization. In: Foundations and Trends in Machine Learning 4.2 (2012), pp. 107194. Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, and Riccardo Grazzi. DeltaProduct: Increasing the Expressivity of DeltaNet Through Products of Householders. In: arXiv preprint arXiv:2502.10297 (2025). Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified State Space Layers for Sequence Modeling. In: The Eleventh International Conference on Learning Representations. 2022. url: https://openreview.net/forum? id=Ai8Hw3AXqks. Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified State Space Layers for Sequence Modeling. In: The Eleventh International Conference on Learning Representations. 2023. url: https://openreview.net/forum? id=Ai8Hw3AXqks. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. In: Neurocomputing 568 (2024), p. 127063. [83] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. In: arXiv preprint arXiv:2407.04620 (2024). [84] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. In: arXiv preprint arXiv:2307.08621 (2023). [85] Scott Terry. Learning and memory: Basic principles, processes, and procedures. Routledge, 2017. [86] Robert Tibshirani. Regression shrinkage and selection via the lasso. In: Journal of the Royal Statistical Society Series B: Statistical Methodology 58.1 (1996), pp. 267288. [87] Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco Gori, and Stefano Melacci. On the resurgence of recurrent models for long sequences: Survey and research opportunities in the transformer era. In: arXiv preprint arXiv:2402.08132 (2024). [88] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. In: arXiv preprint arXiv:2302.13971 (2023). 22 [89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In: Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc., 2017. url: https : / / proceedings . neurips . cc / paper _ files / paper / 2017 / file / 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [90] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In: Advances in Neural Information Processing Systems. Ed. by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc., 2017. url: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aaPaper.pdf. Johannes Von Oswald, Maximilian Schlegel, Alexander Meulemans, Seijin Kobayashi, Eyvind Niklasson, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, et al. Uncovering mesa-optimization algorithms in transformers. In: arXiv preprint arXiv:2309.05858 (2023). [91] [92] Ke Alexander Wang, Jiaxin Shi, and Emily Fox. Test-time regression: unifying framework for designing sequence models with associative memory. In: arXiv preprint arXiv:2501.12352 (2025). [93] Yingheng Wang, Zichen Wang, Gil Sadeh, Luca Zancato, Alessandro Achille, George Karypis, and Huzefa Rangwala. [94] [95] [96] Long-context Protein Language Model. In: bioRxiv (2024), pp. 202410. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated Delta Networks: Improving Mamba2 with Delta Rule. In: arXiv preprint arXiv:2412.06464 (2024). Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated Linear Attention Transformers with Hardware-Efficient Training. In: Forty-first International Conference on Machine Learning. 2024. url: https: //openreview.net/forum?id=ia5XvxFUJT. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In: Advances in Neural Information Processing Systems 37 (2024), pp. 115491115522. [97] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can Machine Really Finish Your Sentence? In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Ed. by Anna Korhonen, David Traum, and Lluis Marquez. Florence, Italy: Association for Computational Linguistics, July 2019, pp. 47914800. doi: 10.18653/v1/P19-1472. url: https://aclanthology.org/P19-1472/. [98] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In: Advances in Neural Information Processing Systems 32 (2019). [99] Hao Zhang, Alexander Berg, Michael Maire, and Jitendra Malik. SVM-KNN: Discriminative nearest neighbor classification for visual category recognition. In: 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR06). Vol. 2. IEEE. 2006, pp. 21262136."
        },
        {
            "title": "A Additional Related Work",
            "content": "Modern Linear RNNs. Recent efforts aim to overcome Transformers quadratic cost and limitations in long-context modeling by designing efficient recurrent alternatives (Tiezzi et al. 2024), mainly due to fast inference and training of such models. The first generation of modelssuch as RetNet (Sun et al. 2023), LRU (Orvieto et al. 2023), RWKV (Peng et al. 2023), S5 (Smith et al. 2023), and S4 (Gu et al. 2022)uses data-independent transition matrix mechanism with Hebbian-like update rule. The second generation of such models started to incorporate input-dependent parameters into such linear architectures (e.g., Griffin (De et al. 2024), SSMs (Behrouz et al. 2024b; Dao et al. 2024; Hasani et al. 2023), RWKV6 (Peng et al. 2024)), and/or use more expressive memory updating rule based on delta rule (Liu et al. 2024a; Peng et al. 2025b; Schlag et al. 2021; Yang et al. 2024a,c). The next generation of models, extend the memory architecture to deep models, while using delta-rule-like update rule (Sun et al. 2024), or momentum-based update rule (Behrouz et al. 2024c). Recently, to further enhance the performance of delta-rule-based sequence models, Siems et al. (2025) suggest using multiple gradient descent update per token, resulting in more expressive sequence models in state tracking tasks. In addition to the above fast linear recurrent sequence models, several studies have focused on (interpretable) non-linear RNNs (Csordás et al. 2024; Gonzalez et al. 2024; Karami et al. 2025; Lim et al. 2024; Merrill et al. 2024; Schöne et al. 2025; Von Oswald et al. 2023), and how their training can be faster (Gonzalez et al. 2024; Lim et al. 2024; Schöne et al. 2025). However, due to the recurrent nature of such models, parallelizing them in larger scales is still challenging. Fast Weight Programs. The idea of interpretation of linear layers as the key-value associative memory system backs to Hopfield networks (Hopfield 1982) and then fast weight programs, in which dynamic fast programs are incorporated into recurrent neural networks as writable memory (Schlag et al. 2021; Schmidhuber 1992; Schmidhuber 1993). The two learning rules of Hebbian (Hebb 2005) and delta rule (Prados et al. 1989) are the most popular learning rules for them, which have been extensively explored in the literature (Irie et al. 2021; Munkhdalai et al. 2019, 2017; Schlag et al. 2021; Schmidhuber 1992; Yang et al. 2024a,c). Test Time Training. The key ideas of learning at test time backs to early studies on local learning Bottou et al. 1992, in which each test data is trained on its neighbors before making prediction (Gandelsman et al. 2022; Zhang et al. 2006). Later applying this idea on modern architectures, it has shown promising performance in diverse downstream tasks such as vision tasks (Jain et al. 2011; Mullapudi et al. 2019), video generation (Dalal et al. 2025), etc., mostly due to their ability to mitigate out-of-distribution samples. Hopfield Networks. We build Miras based on the concept of associative memory in its broad form, where we learn an underlying mapping between keys and values. One of the earliest studies that discuss building neural architectures based on associative memory is Hopfield Networks (Hopfield 1982), in which associative memory is defined as the minimizing the energy function required to store keys and values. While traditional Hopfield networks has limited applicability in recent years (mainly due to limited capacity of vector-valued memory and energy function), several recent studies aim to improve their capacity by various techniques (Krotov 2021; Krotov et al. 2016; Li et al. 2024b), including extending the energy function of such models based on exponential kernels (Krotov et al. 2016; Lucibello et al. 2024), and discuss their connection to Transformers (Hu et al. 2024; Ramsauer et al. 2021). Unifying Frameworks. In recent years, there have been growing efforts to understand the underlying mechanism of sequence models and unify (a subset of) them through single perspective. Dao et al. (2024) present SSD framework to connect linear Transformers and (a subset of) linear recurrent models through the lens of associative operators and structured matrices. The SSD framework, however, is limited to models with vector or matrix-valued memory that are updated using Hebbian-like update rules. Later, Liu et al. (2024a) present an online learning perspective on (a subset of) linear recurrent models. While this framework can also explain more expressive recurrent models based on delta rule, it is limited to online learners (i.e., models that optimize their internal associative memory using stochastic optimizers, such as stochastic gradient descent) with matrix-valued memory. Several modern sequence models, such as Transformers (Vaswani et al. 2017b) or Titans (Behrouz et al. 2024c) cannot be expressed in this framework. Sun et al. (2024) further provide unifying perspective on how linear and softmax attention are respectively parametric and non-parameteric solutions of (kernel) regression loss but consider other modern linear RNNs outside of this class of models, mainly due to limiting the objective to be regression loss. Recently, in concurrent work to ours, Wang et al. (2025) also force models to have the same attentional bias objective and show that with additional simplification of modern RNNs (e.g., RetNet (Sun et al. 2023), Mamba (Dao et al. 2024)) they approximately place in the same class of models that internally optimize regression loss. 24 However, this simplification, fully change the understanding of underlying update rules in these models. For example, contrary to Wang et al. (2025), Miras can distinguish models with Hebbian-like update (with dot product similarity) and delta rule update (with regression loss). Furthermore, all presented sequence models in this work (e.g., Moneta, Memora, Yaad) as well as models like HGRN2 (Qin et al. 2024) are placed outside of this class of models, due to their different attentional bias. Proof of Proposition 3.2 Here we present the proof of Proposition 3.2. For the sake of completeness, let us first re-state this Proposition. Proposition 3.2. Let 𝜂𝑡 = 𝜂 and define ℎ𝑡 (𝑊 ) := (cid:205)𝑡 1 𝑖=1 (cid:98)ℓ𝑖 (𝑊 ; k𝑖, v𝑖 ) + 1 𝜂 𝑅(𝑊 ). Assume = R𝑑 and the function ℎ𝑡 (𝑊 ) is strictly convex in 𝑊 and let Dℎ (, ) be the Bregman divergence defined by function ℎ(), i.e., Dℎ (𝑊 ,𝑊 ) = ℎ(𝑊 ) ℎ(𝑊 ) ℎ(𝑊 ),𝑊 𝑊 . Set Ret𝑡 (𝑊 ,𝑊 ) = Dℎ (𝑊 ,𝑊 ) and (cid:101)ℓ𝑡 (𝑊 ; 𝑥𝑡 ) = (cid:98)ℓ𝑡 (𝑊 ; 𝑥𝑡 ) in (Learning-Retaining Viewpoint). Then, the update rule in (Learning-Retaining Viewpoint) is equivalent to the update rule in (FTRL Viewpoint). Proof. Let { (cid:98)𝑊1, (cid:98)𝑊2, . . .} be the sequence of parameters obtained by (FTRL Viewpoint) and { (cid:101)𝑊1, (cid:101)𝑊2, . . .} be the sequence of parameters obtained by (Learning-Retaining Viewpoint). To show both update rules are equivalent, it suffices to show that the above two sequences are the same if they are initialized at the same point. We prove this statement by induction. First of all, since both sequences are initialized at the same point, the induction base is satisfied (i.e. (cid:101)𝑊1 = (cid:98)𝑊1. Now, assume by induction hypothesis that (33) (cid:101)𝑊𝑡 1 = (cid:98)𝑊𝑡 1. To complete the induction, we need to show (cid:101)𝑊𝑡 = (cid:98)𝑊𝑡 . To this end, notice that, by (Learning-Retaining Viewpoint), we have (cid:101)𝑊𝑡 = arg min 𝑊 Using the choice of the Attentional Bias and the Retention function in the Proposition, we obtain (cid:101)ℓ𝑡 (𝑊 , k𝑡, v𝑡 ) + Ret𝑡 (𝑊 , (cid:101)𝑊𝑡 1) (cid:101)𝑊𝑡 = arg min 𝑊 (cid:98)ℓ𝑡 (𝑊 , k𝑡, v𝑡 ) + (cid:98)ℓ𝑖 (𝑊 , k𝑖, v𝑖 ) + 1 𝜂 𝑅(𝑊 ) 𝑡 1 𝑖=1 (cid:98)ℓ𝑖 ( (cid:101)𝑊𝑡 1, k𝑖, v𝑖 ) 1 𝜂 𝑅( (cid:101)𝑊𝑡 1) (cid:98)ℓ𝑖 ( (cid:101)𝑊𝑡 1, k𝑖, v𝑖 ) + 1 𝜂 𝑅( (cid:101)𝑊𝑡 1),𝑊 (cid:101)𝑊𝑡 1 (cid:43) . 𝑡 1 𝑖=1 (cid:42) 𝑡 1 𝑖=1 Ignoring the constant terms and using the induction hypothesis (33), we get (cid:101)𝑊𝑡 = arg min 𝑊 (cid:98)ℓ𝑡 (𝑊 , k𝑡, v𝑡 ) + 𝑡 1 𝑖=1 (cid:98)ℓ𝑖 (𝑊 , k𝑖, v𝑖 ) + 1 𝜂 𝑅(𝑊 ) (cid:42) 𝑡 1 𝑖=1 (cid:98)ℓ𝑖 ( (cid:98)𝑊𝑡 1, k𝑖, v𝑖 ) + 1 𝜂 𝑅( (cid:98)𝑊𝑡 1),𝑊 (cid:98)𝑊𝑡 1 (cid:43) . On the other hand, recall that { (cid:98)𝑊1, (cid:98)𝑊2, . . .} is obtained by (FTRL Viewpoint). Therefore, we have Thus, we have Combining (36) and (35), we obtain (cid:98)𝑊𝑡 1 = arg min 𝑊 𝑡 1 𝑖=1 (cid:98)ℓ𝑖 (𝑊 ; k𝑖, v𝑖 ) + 1 𝜂 R𝑡 (𝑊 ). 𝑡 1 𝑖= (cid:98)ℓ𝑖 (𝑊𝑡 1, k𝑖, v𝑖 ) + 1 𝜂 𝑅(𝑊𝑡 1) = 0. (cid:101)𝑊𝑡 = arg min 𝑊 𝑡 𝑖= (cid:98)ℓ𝑖 (𝑊 , k𝑖, v𝑖 ) + 1 𝜂 𝑅(𝑊 ). This implies (cid:101)𝑊𝑡 = (cid:98)𝑊𝑡 , which completes the proof. 25 (34) (35) (36)"
        },
        {
            "title": "C Experimental Setup",
            "content": "We perform experimental evaluation on the language modeling (Merity et al. 2017; Paperno et al. 2016), common-sense reasoning (Bisk et al. 2020; Clark et al. 2019; Clark et al. 2018; Sakaguchi et al. 2021; Zellers et al. 2019), and long context needle-in-haystack tasks (Hsieh et al. 2024). We compare our models with the state-of-the-art linear recurrent models, Transformers, and hybrid models (recurrent + attention). More specifically we compare with Transformer++ (Touvron et al. 2023), RetNet (Sun et al. 2023), Gated Linear Attention (GLA) (Yang et al. 2024b), Mamba (Gu et al. 2024), Mamba2 (Dao et al. 2024), DeltaNet (Yang et al. 2024c), TTT (Sun et al. 2024), and Gated DeltaNet (Yang et al. 2024a). Table 5: Architectural Details."
        },
        {
            "title": "Token",
            "content": "170M 340M 780M 12 24 24 768 1024 1536 16 16 16 3e-3 1.5e-3 1.25e-3 15B 15B 30B"
        }
    ],
    "affiliations": [
        "Google Research"
    ]
}