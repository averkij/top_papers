{
    "paper_title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers",
    "authors": [
        "Rihui Xin",
        "Han Liu",
        "Zecheng Wang",
        "Yupeng Zhang",
        "Dianbo Sui",
        "Xiaolin Hu",
        "Bingning Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing a key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costly, and sometimes unfeasible. This research delves into the utilization of format and length as surrogate signals to train LLMs for mathematical problem-solving, bypassing the need for traditional ground truth answers.Our study shows that a reward function centered on format correctness alone can yield performance improvements comparable to the standard GRPO algorithm in early phases. Recognizing the limitations of format-only rewards in the later phases, we incorporate length-based rewards. The resulting GRPO approach, leveraging format-length surrogate signals, not only matches but surpasses the performance of the standard GRPO algorithm relying on ground truth answers in certain scenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through systematic exploration and experimentation, this research not only offers a practical solution for training LLMs to solve mathematical problems and reducing the dependence on extensive ground truth data collection, but also reveals the essence of why our label-free approach succeeds: base model is like an excellent student who has already mastered mathematical and logical reasoning skills, but performs poorly on the test paper, it simply needs to develop good answering habits to achieve outstanding results in exams , in other words, to unlock the capabilities it already possesses."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 3 4 9 1 . 5 0 5 2 : r Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers Rihui Xin1, Han Liu1,2, Zecheng Wang1,3, Yupeng Zhang1, Dianbo Sui3, Xiaolin Hu*2, Bingning Wang*1 1Baichuan Inc. 2Tsinghua University 3Harbin Institute of Technology"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costly, and sometimes unfeasible. This research delves into the utilization of format and length as surrogate signals to train LLMs for mathematical problem-solving, bypassing the need for traditional ground truth answers. Our study shows that reward function centered on format correctness alone can yield performance improvements comparable to the standard GRPO algorithm in early phases. Recognizing the limitations of format-only rewards in the later phases, we incorporate length-based rewards. The resulting GRPO approach, leveraging format-length surrogate signals, not only matches but surpasses the performance of the standard GRPO algorithm relying on ground truth answers in certain scenarios, achieving 40.0% accuracy on AIME2024 with 7B base model. Through systematic exploration and experimentation, this research not only offers practical solution for training LLMs to solve mathematical problems and reducing the dependence on extensive ground truth data collection, but also reveals the essence of why our label-free approach succeeds: base model is like an excellent student who has already mastered mathematical and logical reasoning skills, but performs poorly on the test paper, it simply needs to develop good answering habits to achieve outstanding results in exams in other words, to unlock the capabilities it already possesses. https://github.com/insightLLM/rl-without-gt"
        },
        {
            "title": "Introduction",
            "content": "In the dynamic landscape of artificial intelligence, Large Language Models (LLMs) Brown et al. (2020); Chowdhery et al. (2023); Yang et al. (2023); Wang et al. (2025a); Grattafiori et al. (2024) have emerged as transformative force, with models like GPT-o1 Jaech et al. (2024), DeepSeek-R1 DeepSeek-AI et al. (2025), and Qwen3 Yang et al. (2025b) leading the charge. Pre-trained on massive text corpora, these models have demonstrated remarkable proficiency in diverse natural language processing tasks, ranging from text generation and question-answering to language translation and code writing. Their success largely stems from unsupervised pre-training, which empowers LLMs to capture complex semantic and syntactic patterns, enabling effective generalization across various scenarios. Reinforcement Learning (RL) plays crucial role in adapting pre-trained LLMs to specific downstream tasks. Among RL techniques, Proximal Policy Optimization (PPO) Schulman et al. (2017) and its advanced variant, Group Relative Policy Optimization (GRPO) Shao et al. This work was supported by Baichuan-Inc, China. Equal contribution. *Corresponding author. Correspondent: god@bingning.wang. 1 (2024), are commonly employed to optimize LLMs. These methods typically rely on ground truth answers to define rewards, serving as explicit feedback for the model to iteratively refine its solutions. However, obtaining accurate ground truth answers, particularly in the domain of mathematical problem-solving, presents significant challenges. It often demands substantial human effort, time, and resources, and in certain cases, such answers may be scarce or even nonexistent. This limitation has spurred the exploration of alternative training strategies to enable effective RL without relying on explicit ground truth information. Motivated by these challenges, our research endeavors to explore the possibility of training LLMs for mathematical problem-solving using alternative signals instead of ground truth answers. Through systematic experiments and in-depth analysis, we have made several significant discoveries. During the initial 15 steps of RL training, the model predominantly focuses on learning the format of mathematical solutions. This early phase is crucial, contributing approximately 85% of the overall performance improvement during the entire RL training process. Notably, during this period, we observed significant reduction in response length, indicating that the model rapidly eliminates redundant information and converges towards more structured and efficient representation. Experiments revealed that reward function solely considering format correctness achieved the same performance gains as the standard GRPO algorithm, underscoring the potency of format correctness as key signal in the early learning stage. Nevertheless, relying solely on format-based rewards has limitations, causing performance improvement to stall after the initial gains. To overcome this hurdle, we integrated length-based reward into the format-based reward function. Strikingly, our GRPO approach leveraging format-length surrogate signals not only matched but in some cases outperformed the standard GRPO algorithm that relies on ground truth answer information. This is because format and length together act as powerful surrogate signals highly correlated with answer correctness. Format correctness provides necessary optimization target, while the length-based reward penalizes overly long or short responses, prompting the model to refine its content by eliminating incorrect or redundant derivations. Through these findings, our work effectively challenges the necessity of ground truth answers for RL in mathematical problem solving, provides detailed analysis of GRPO training dynamics to reveal the importance of the early format-learning phase and complementary role of length-based rewards, and opens up new possibilities for training LLMs in scenarios where ground truth answers are scarce or unavailable, offering an efficient approach applicable to mathematical reasoning tasks."
        },
        {
            "title": "2 Related Work",
            "content": "RL has been proven effective in enhancing LLM performance. PPO Schulman et al. (2017) and GRPO Shao et al. (2024) are widely used in RL frameworks for LLMs, with detailed introductions provided in Appendix A. Recent research uses scaled-up RL training to enable LLMs to explore reasoning paths for complex problems. For example, DeepSeek-R1 DeepSeek-AI et al. (2025) achieved excellent results in math and coding tasks through largescale RL on an unsupervised base model, without relying on pre-trained reward models or MCTS. Kimi-k1.5 Team et al. (2025) enhances general reasoning via RL, focusing on multimodal reasoning and controlling thinking length. Format reward in RL. DeepSeek-R1 DeepSeek-AI et al. (2025) uses format rewards to structure model outputs. Liu et al. Liu et al. (2025a) noted format rewards dominate early training. Our study isolates the influence of answer rewards and designs format for math reasoning tasks. Experiments show using our format in early RL training matches performance of answer reward training. Length Control in RL. DeepSeek-R1 DeepSeek-AI et al. (2025) found response length and evaluation metrics increase with RL training steps until an \"Aha moment\". Other studies explore length reward functions impacts. Yeo et al. Yeo et al. (2025) observed response lengths decline due to model size and KL divergence penalties. Chen et al. Chen et al. (2025) argued direct length extension training may harm performance. In contrast, our length reward penalizes overly long responses, guiding concise outputs. Experiments show combining length and format rewards outperforms answer rewards."
        },
        {
            "title": "3 Method: Format and Length as Surrogate Signals for Answer",
            "content": "To mitigate the issue of label scarcity in real-world environments, we explore the potential of format and length as powerful \"surrogate signals\" highly correlated with answer correctness. Format correctness in mathematical problem-solving offers necessary but insufficient condition for answer accuracy, providing clear structural optimization target for the model. Meanwhile, the length of the response serves as an indicator of content efficiency and logical compactness, reflecting the quality of the solutions reasoning process. Based on these insights, we develop novel learning framework that integrates format and length rewards into the GRPO algorithm. This framework, centered around optimizing LLMs without relying on explicit ground truth answers, aims to enable effective training by leveraging these surrogate signals to approximate the optimization direction of ground truth answer rewards. 3.1 Format Reward In the context of mathematical problem-solving, correct format is crucial for ensuring the clarity and comprehensibility of the solution. Our format reward mechanism is designed to encourage the model to generate responses that adhere to the standard presentation conventions of mathematical solutions (details in Appendix D). The format reward Rf is defined as binary function: Rf = (cid:26)1 0 if the format is right. else. (1) This reward serves as fundamental signal for the model to learn the structural aspects of mathematical problem-solving in the early stages of training. 3.2 Length Reward To complement the format reward and further refine the content of the models responses, we introduce length reward function. The length of response is critical factor that reflects the efficiency and logical compactness of the solution. An overly short response may lack essential reasoning steps, while an excessively long response might contain redundant or incorrect derivations. Our length reward function is designed to strike balance between promoting comprehensive reasoning and preventing overly long responses that could exceed the models context limits. It is formulated as piecewise function: 1 (cid:0)1 (cid:16) xp 1p 0 p, < 1, Rl = , (cid:17)2 1 2 (2) (cid:1)2 , Let = Lmax , (3) where is the length of the current response and Lmax is the maximum context length. Let (0, 1) be tunable parameter that controls the turning point of the piecewise function, with default value of 0.5. This piecewise function is continuous and differentiable at = p, encouraging response lengths that approach the turning point p. The reward increases smoothly as grows from 0 to p, reaches maximum at = p, and then decreases for > p, thereby penalizing overly long responses. positive length reward can only be obtained when the format is right. Examples with format errors are considered severeno matter how ideal their length may be, they can receive at most 0. Therefore, the final format-length reward can be expressed as: Rfl = (cid:26)Rf + Rl min(0, Rf + Rl) if the format is right. else. (4) 3 By combining the format reward and length reward, we provides an \"surrogate signals\" for the models reinforcement learning, helping to alleviate the issue of label scarcity in real-world environments."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present comprehensive set of experiments designed to demonstrate the practical viability of using format and length as surrogate signals for answer accuracy in GRPO for mathematical reasoning tasks. 4.1 Experimental Setup Reward configurations: We designed series of experiments with distinct reward configurations to assess the effectiveness of our proposed approach. Correctness: This configuration is served as our baseline, which uses the exact match with ground-truth answers as the reward criterion. When the models output precisely aligns with the correct answer, it is assigned reward score of 1; otherwise, it receives 0. We utilized the MARIO_EVAL 1 library to accurately extract answer content from the models output, ensuring reliable evaluation standard. Format-Only: The reward function is as shown in Eq.(1), which is determined solely by the format of the models output. After normalizing the content, we employ SymPy 2, powerful symbolic mathematics library, to validate its mathematical format. Format-Length: The reward function is as shown in Eq.(4), where the format reward is the same as that of Format-Only RL. Datasets: We trained models on two mathematical reasoning datasets: DeepScaleR Luo et al. (2025) and MATH-train. DeepScaleR (17,000 samples) integrates problems from the MATH Hendrycks et al. (2021), AMC (< 2023), AIME (1984-2023), and others, with deduplication and decontamination applied. MATH-train (7,500 samples) is the MATH datasets training split. Evaluation: We evaluated the model on three datasets: MATH500, AIME2024, and AMC2023 with greedy decoding. In addition to analyzing each dataset individually, we also calculated the average scores across all benchmarks to enable direct comparison. Implementation details: We trained the Qwen2.5-Math-7B base model using the GRPO algorithm under the verl 3 framework. For each case in training and evaluation, we used Qwen-Math template (as shown in Appendix C). During training, we used the following hyperparameters: learning rate of 1e-6, batch size of 128, temperature of 0.6, 8 responses per prompt, maximum response length of 3072, and KL coefficient of 0.001. All training was performed on machine with 8 H20 GPUs, and single training run took 6 hours. 4.2 Impact of Format Reward The format-only experiment offers critical insights into the role of format correctness in the training process. During the initial 15 steps, as depicted in Figure 1, the performance of the model trained with format-only reward remarkably aligns with that of the correctness reward setup on both benchmarks. This convergence validates our hypothesis that in the early stages of GRPO, the model predominantly focuses on learning the structural patterns of mathematical solutions. It suggests that format serves as strong initial signal, allowing the model to quickly grasp the essential presentation conventions of mathematical answers, which accounts for approximately 85% of the overall performance improvement in this early phase. However, as the training progresses beyond the 15-step mark, significant divergence emerges. The performance of the format-only model plateaus, barely showing any im1https://github.com/MARIO-Math-Reasoning/MARIO_EVAL 2https://github.com/sympy/sympy 3https://github.com/volcengine/verl Figure 1: Average accuracy on evaluation benchmark training on (a) DeepScaleR and (b) Math-train. (a) (b) provement even after 100 training steps. This stagnation can be attributed to the inherent limitation of relying solely on format as reward signal. While format correctness is necessary condition for answer accuracy, it is not sufficient. Without additional guidance, the model lacks the means to refine the content within the correct format, leading to an inability to further enhance the accuracy of its solutions. This highlights the need for supplementary signals to drive continuous improvement. 4.3 Effectiveness of Format-Length RL Our format-length reward demonstrates notable advantages in mathematical problemsolving without ground truth answers, as shown in Table 1. By using format consistency and response length as surrogate signals, the approach achieves competitive performance against the model trained with correctness reward. Numerically, model trained with format-length reward achieves an average score of 56.8, surpassing the correctness rewards average score of 53.0 when using the DeepScaleR training dataset. In particular, model trained with format-length reward achieved 40 points in AIME2024 using the MATH training dataset.This indicates that leveraging structural and length-based rewards alone can guide the model to generate high-quality solutions comparable to or better than models trained with correctness reward, even without explicit answer supervision. Figure 1 shows the average accuracy curves of GRPO training with different rewards. In Appendix Figures S1 and S2, we present the accuracy curves of each benchmark respectively. It can be seen from these figures that model trained with format-length reward maintains stable performance comparable to the correctness reward baseline throughout the entire training process. The consistent curves validate the reliability of surrogate signals in driving model improvement without ground truth, highlighting the approachs scalability and data efficiency for mathematical reasoning tasks. 4.4 Response Length Dynamics In Figures 2, we respectively show the curves of average response length during GRPO training with different rewards on the DeepscaleR dataset. The model trained with format-length reward demonstrated distinctive dual-phase evolution in response length, which starkly contrasts with the monotonic decrease observed in the models trained with correctness reward and format-only reward. Across all reward configurations, the average response length decreases during the initial 30 training steps. This indicates that the model prioritizes format adherence during this phase. Driven by the dominant format reward signal, which penalizes any deviation from the required answer schema, it prunes redundant content to meet structural constraints. Method Qwen-Math-7B DeepSeek-R1-Distill-7B@3k DeepSeek-R1-Distill-7B@8k Qwen2.5-Math-7B-Instruct LIMR-7B Li et al. (2025) SimpleRL-Zero-7B Zeng et al. (2025) Oat-Zero-7B Liu et al. (2025b) Correctness (baseline) Format-Only Format-Length Label Free AIME2024 MATH500 AMC AVG. 16.7 10.0* 33.3* 16. 50.8 60.1* 88.1* 83.2 42.2 26.2* 68.4* 55.4 36.6 32.1* 63.3* 51. 23.3 (32.5*) 26.7 (40.0*) 40.0 (43.3*) 26.7 / 26.7 26.7 / 26.7 33.3 / 40.0 74.8 (78.0*) 75.4 (80.2*) 78.2 (80.0*) 74.6 / 73.0 72.6 / 72.8 76.8 / 73.0 60.2 (63.8*) 57.8 (70.0*) 61.5 (62.7*) 57.8 / 56.6 55.4 / 53.0 60.2 / 54. 52.8 (58.1*) 53.3 (63.4*) 60.0 (62.0*) 53.0 / 52.1 51.6 / 50.8 56.8 / 55.7 Table 1: Accuracy comparison of different models on benchmark datasets (cyan rows denote our trained models). Results are separated by slash for DeepscaleR and MATH-train datasets (DeepscaleR first, MATH-train second). Results without * are evaluated in our environment (details in Appendix B); * indicates results from Liu et al. (2025b) or the original paper. As training advances from 30 to 100 steps, the length reward mechanism takes the lead, driving strategic expansion of response content. Unlike simplistic length penalties that encourage brevity at the cost of depth, GRPO with format-length reward fosters an optimal equilibrium. It encourages longer thinking processes and discourages unnecessary verbosity. This dynamic mirrors the human problem-solving process, where initial efforts focus on establishing structure, followed by iterative refinement of content. During the final stages, the models response length increases by an average of 14.0%, which correlates with 10.5% improvement in average accuracy training on DeepScaleR, indicating that length serves as proxy for reasoning complexity rather than redundancy. This dual-phase evolution parallels the human learning process encapsulated by the adage Reading thin before reading thick. In the first stage, the model, similar to human summarization, compresses single reasoning process, while in the second stage, it expands and generalizes, exploring more diverse and complex reasoning paths, such as error correction and branch exploration. In contrast, the correctness reward baseline and format-only models, as highlighted by the red box in Figure 2, briefly attempt to explore complex reasoning but ultimately revert to the comfort zone of compressing single reasoning process. Figure 2: Response length during training. The solid lines in the figure represent the original results, while the dashed lines represent the results smoothed with window size of 5. 6 (a) (b) Figure 3: The curves of (a) accuracy, (b) response length, and (c) reflective keyword frequency for cases of different difficulty levels in MATH500 during training. (c) 4.5 Format-Length Rewards Impact Across Difficulty Levels To explore how format-length rewards affect LLMs mathematical problem-solving, we analyzed the MATH500 dataset, which has official difficulty ratings and balanced problem distribution. As depicted in Figure 3a, by the end of the training process, the format-length model outperformed the correctness reward baseline across all difficulty levels. The relationship between response length and reasoning performance further illuminates the mechanism behind these results. As shown in Figure 3b, both models generated longer responses for higher-difficulty problems. The correctness reward baseline model initially showed rapid decrease in output length, which later stabilized, while the format-length model demonstrated mid-stage increase, especially for high-difficulty problems. This increase in length was positively correlated with improved accuracy, indicating that the length reward encourages the model to adopt more comprehensive reasoning strategies, particularly when tackling complex tasks. We delved deeper into the models reasoning process by analyzing the frequency of reflective words in the generated responses (Figure 3c). Reflective words, including those related to verification (wait/verify/check), retrospection (recall/recheck), branch exploration (alternatively), logical turn or contrast (however/but/since), and problem decomposition and step-by-step reasoning (step/step-by-step), represent complex reasoning behaviors. The correctness reward baseline model showed an initial increase in reflective words, which plateaued in the later stages, aligning with its limited performance gains. In contrast, the format-length model exhibited significant rise in reflective words, especially for high-difficulty problems. This indicates that the length signal helps increase the depth of thinking, enabling the model to engage more in complex reasoning behaviors such as verification, retrospection, and problem decomposition. Such enhanced reflective thinking allows the model to better explore different solution paths and logical turns, thereby improving its ability to handle high-difficulty problems. To further validate these findings, we conducted case study by comparing the outputs of the correctness model and format-length model on challenging questions (Appendix Table S1). The format-length model had learned \"step-by-step problem-solving and verification\" pattern, which confirmed the effectiveness of our format-length reward mechanism in balancing response length, reasoning depth, and content quality. Similar to Wang et al. (2025b), we observed that increasing the frequency of reflective language does not necessarily correlate with better model performance. Specifically, models can exhibit over-reflection, characterized by repeatedly switching reasoning paths on complex problems, often leading to failed solutions. This over-reflection is sometimes accompanied by phrase repetition (Appendix Table S2), where models may exploit length rewards through redundancy. We will discuss further in Section 5.3. 7 Method AIME2024 MATH500 AMC2023 Qwen-Math-7B Correctness Format-Only Format-Length 63.3 73.3 66.7 66.7 94.0 94.4 94.0 94.4 92.8 90.4 91.6 92.8 Table 2: Pass@64 results across different methods."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 Rethinking Ground Truth Dependency in Mathematical Reasoning The remarkable performance of our ground truth-free RL approach begs the question: how can RL without explicit answer supervision match the effectiveness of traditional ground truth-based methods? The answer lies in the latent knowledge already encoded within pre-trained language models. Prior to RL fine-tuning, these models have assimilated vast amounts of knowledge from diverse corpora, enabling them to potentially generate correct answersRL merely serves as catalyst to activate this dormant capacity. Our pass@N experiments provide compelling evidence for this mechanism. By generating distinct responses per prompt and assessing the presence of correct answers among them, we observe comparable pass@N scores across four conditions: the pre-trained model, the model fine-tuned by GRPO with correctness, format-only, and format-length rewards. As presented in Table 2, which showcases the pass@64 results, we can see that the performance differences between thes methods are relatively minor. This parity indicates that all RL variants fail to confer new knowledge; instead, they optimize how the model retrieves and structures existing knowledge. In essence, our findings demonstrate that with the right reward designsuch as leveraging format and length cuesRL can effectively stimulate the models internal reasoning processes. As long as the training mechanism activates the models latent cognitive abilities, explicit ground truth answers become an optional component rather than an essential requirement for high-performance RL in mathematical reasoning tasks. 5.2 Format Learning in RL and SFT Since both traditional RL with ground truth rewards and our format-based RL mainly learn answer formatting in the first 15 training steps, key question arises: how does format learning through RL compare with supervised fine-tuning (SFT)? To answer this question, we carried out series of comparative experiments, comparing three different training methods: 1) GRPO training with format-based rewards, 2) offline SFT using ground truth chain-of-thought (CoT) examples, and 3) online SFT. Online SFT serves as middle ground between offline SFT and RL, connecting static supervised learning and the dynamic, feedback-driven RL, which helps us figure out how different training methods affect format learning. We used Qwen2.5-Math-7B as the original model, which we didnt train, to provide baseline for comparison. The GRPO(Correctness) was used as reference to measure the performance of other methods. All experiments were conducted under the setting of sampling from the MATH dataset with temperature of 0.6. In the GRPO training with format-based rewards and online SFT experiments, we adopted an online sampling strategy. During training, we constantly sampled model outputs and applied GRPO or SFT based on whether the format was correct. Specifically, online SFT only used format-correct samples to update parameters. All experiments used batch size of 128 and ran for 100 training steps. As shown in Table 3, the results offer important insights. Under the temperature=0.6 setting, the GRPO training with format-based rewards and online SFT performed very similarly, achieving comparable format accuracy rates and scores on the MATH500 benchmark. On 8 Method Answer Acc Format Acc Qwen2.5-Math-7B GRPO(Correctness) GRPO(Format-Only) offline SFT online SFT 61.7 74.0 70.1 51.3 71.3 87.3 95.0 96.3 88.7 95.0 Table 3: Comparison of format accuracy and answer accuracy across different training methods on the MATH500 benchmark. Note that the Acc is the average accuracy calculated after sampling each question 64 times. Figure 4: Longest duplicate substring ratio of MATH500 evaluation benchmark during training. the other hand, the offline SFT method didnt perform as well, showing lower format accuracy and lower MATH500 scores. These results emphasize the important role of online sampling in making RL more effective for format learning. RL and online SFT can adjust to the quality of real-time outputs, which allows them to optimize answer formatting more efficiently than the static offline SFT. Clearly, the iterative and feedback-driven nature of online training is crucial for quickly improving language models ability to learn formats."
        },
        {
            "title": "5.3 Mitigating Repetition and Reward Hacking",
            "content": "A potential concern with length-based rewards is the risk of reward hacking, where the model generates repetitive content to increase its length. To address this, we employed the longest repeated substring analysis to measure repetition. The longest repeated substring ratio (Figure 4) provides normalized perspective on repetition. At the start of training, both the format-length and correctness models exhibited high levels of repetition, mainly due to incorrect formatting issues, such as stacked instances of boxed. However, this problem was resolved after just 15 training steps. The repetition rate then dropped significantly and remained stable throughout the subsequent training process. These findings demonstrate that the format-length reward mechanism effectively balances response length, reasoning depth, and content quality. By integrating format and length signals, our approach not only improves performance on mathematical reasoning tasks but also mitigates the risks associated with traditional length-based rewards, like repetition and reward hacking. 5.4 Design Principles of Format-Length RL In the context of language model training, truncation refers to the situation where the generated output exceeds the maximum allowable length (e.g., the context window size 9 (b) Figure 5: (a) Response length, (b) clip ratio, and (c) average accuracy of benchmark during training. (a) (c) of the model) and has to be cut off. Truncation is highly undesirable for several reasons. Firstly, it leads to incomplete responses, which can result in the loss of crucial information and logical steps necessary for correct mathematical reasoning. In the case of mathematical problem-solving, truncated answer may omit key derivations or final conclusions, rendering the solution incorrect or meaningless. Secondly, truncation can disrupt the coherence and flow of the reasoning process, making it difficult for the model to build on its own arguments and reach valid conclusion. Prior studies have explored length-based rewards, but their applicability to label-free settings is limited. For example, Yeo et al.Yeo et al. (2025) proposed cosine-shaped length reward coupled with correctness, while Chen et al. Chen et al. (2025) introduced linear length reward: = L/LMax + Rcorrectness. We reproduced this linear reward and the result is in Figure 5. However, it led to rapid surge in response length, exceeding the models context window and causing 52.9% truncation rate by step 54. This high truncation rate severely degraded performance, as the truncated outputs were often incomplete and lacked the necessary logical structure for accurate mathematical reasoning. This outcome underscores the importance of carefully designing length rewards to balance exploration and efficiency, ensuring that the model generates responses of optimal length without incurring excessive truncation. In contrast, our Format-Length approach maintains low truncation rate while achieving superior accuracy. By incorporating length reward that penalizes excessive length before reaching the context limit, our method effectively guides the model to generate concise yet comprehensive responses. This not only prevents reward hacking, where the model might generate overly long or repetitive content to maximize rewards, but also promotes high-quality reasoning, as the model is encouraged to find the most efficient way to express correct mathematical solutions within the given length constraints."
        },
        {
            "title": "6 Conclusion",
            "content": "In this study, we found that format and length can serve as effective surrogate signals for training LLMs in mathematical problem-solving, eliminating the need for ground truth answers. During early RL training, LLMs focus on learning solution formats; format-based reward function alone yields performance gains similar to standard GRPO. Integrating length-based rewards enables the GRPO approach with format-length signals to outperform traditional methods relying on ground truth in some cases. This finding challenges the notion that ground truth answers are essential for LLM training. The format-length signals offer practical, efficient alternative, reducing data collection costs. Applicable across mathematical and logical tasks, this approach opens new avenues for LLM training. Future work will optimize signal utilization and expand application to enhance LLM training efficiency and generalization."
        },
        {
            "title": "7 Limitations",
            "content": "There are aspects of our study that merit further exploration. The evaluation of format and length as surrogate signals was predominantly focused on mathematical problem-solving, 10 leaving open the question of their effectiveness in other complex reasoning domains, such as scientific hypothesis testing or advanced programming challenges. Additionally, our experiments were conducted with specific LLM architectures and training configurations, and the performance of this approach may differ when applied to models with varying pre-training paradigms and scale."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.04697. Alfred V. Aho and Jeffrey D. Ullman. The Theory of Parsing, Translation and Compiling, volume 1. Prentice-Hall, Englewood Cliffs, NJ, 1972. American Psychological Association. Publications Manual. American Psychological Association, Washington, DC, 1983. Rie Kubota Ando and Tong Zhang. framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:18171853, December 2005. ISSN 1532-4435. Galen Andrew and Jianfeng Gao. Scalable training of L1-regularized log-linear models. In Proceedings of the 24th International Conference on Machine Learning, pages 3340, 2007. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Ashok K. Chandra, Dexter C. Kozen, and Larry J. Stockmeyer. Alternation. Journal of the Association for Computing Machinery, 28(1):114133, 1981. doi: 10.1145/322234.322243. Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models, 2025. URL https://arxiv.org/abs/2503.04548. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning et al. Palm: Scaling language modeling with pathways. Research, 24(240):1113, 2023. Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences, 2023. URL https://arxiv.org/ abs/1706.03741. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, 11 Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022. URL https://arxiv.org/abs/2210.10760. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dan Gusfield. Algorithms on Strings, Trees and Sequences. Cambridge University Press, Cambridge, UK, 1997. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025. URL https://arxiv.org/abs/2502.11886. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. There may not be aha moment in r1-zero-like training pilot study. https://oatllm.notion.site/ oat-zero, 2025a. Notion Blog. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025b. URL https://arxiv.org/abs/2503.20783. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. OpenAI. Openai o1 contributions, 2023. learning-to-reason-with-llms/. Accessed: 2023-11-01. URL https://openai.com/index/ Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. 12 Mohammad Sadegh Rasooli and Joel R. Tetreault. Yara parser: fast and accurate dependency parser. Computing Research Repository, arXiv:1503.06733, 2015. URL http: //arxiv.org/abs/1503.06733. version 2. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022. URL https://arxiv.org/abs/2009.01325. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Bingning Wang, Haizhou Zhao, Huozhi Zhou, Liang Song, Mingyu Xu, Wei Cheng, Xiangrong Zeng, Yupeng Zhang, Yuqi Huo, Zecheng Wang, et al. Baichuan-m1: Pushing the medical capability of large language models. arXiv preprint arXiv:2502.12671, 2025a. Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Thoughts are all over the place: On the underthinking of o1-like llms, 2025b. URL https://arxiv.org/abs/2501.18585. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025a. URL https://arxiv.org/abs/2505.09388. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025b. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373. 13 Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. URL https://arxiv.org/abs/1909.08593."
        },
        {
            "title": "A Introducion of PPO and GRPO",
            "content": "A.1 Proximal Policy Optimization PPO is widely-used and highly effective algorithm in the field of RL. At its core, PPO aims to optimize the policy of an agent to maximize the expected cumulative reward over time. The algorithm is based on the policy gradient method, which updates the policy by computing the gradient of the expected reward with respect to the policy parameters. The key idea behind PPO is to balance the trade-off between exploration and exploitation during the policy update process. It does this by introducing clipped surrogate objective function. Let πθ be the policy parameterized by θ , and πθold be the old policy. Given set of trajectories collected from the environment, the objective of PPO is to maximize the following clipped objective function: Eπθold [min (rt(θ)At, clip(rt(θ), 1 ϵ, 1 + ϵ)At)] where rt(θ) = πθ(atst) (atst) πθold (5) (6) is the probability ratio of the new policy πθ to the old policy πθold for taking action at in state st, At is the advantage function that estimates how much better an action is compared to the average action in state st, and ϵ is hyperparameter that controls the clipping range. The clipping operation ensures that the policy update is not too drastic, preventing the policy from diverging significantly from the old policy in single update step. To compute the advantage function At , PPO typically relies on value function estimation combined with Generalized Advantage Estimation (GAE). The value function V(s) parameterized by ϕ, predicts the expected cumulative reward from state s. It is trained via temporal difference (TD) learning to minimize the squared error: LValue(ϕ) = (cid:104)(cid:0)Vϕ(st) (cid:0)Rt + γVϕ(st+1)(cid:1)(cid:1)2(cid:105) , (7) where Rt is the reward given by reward model or reward function and γ is the discount factor. The advantage At is then calculated using GAE, which generalizes multi-step TD errors with tunable parameter λ [0, 1] to balance bias and variance: AGAE(γ,λ) = l=0 (γλ)lδt+l, δt = Rt + γV(st+1) V(st). (8) Here, λ = 0 reduces to single-step TD error, while λ = 1 recovers Monte Carlo advantage estimation. By integrating GAE, PPO efficiently utilizes trajectory data while maintaining stable policy updates. A.2 Group Relative Policy Optimization GRPO is an efficient reinforcement learning algorithm that improves upon PPO by eliminating the need for separate value function. GRPO estimates advantages through grouprelative normalization: for given input query , the behavior policy πθold samples responses {oi}G i=1, then calculates each responses advantage as: AGRPO (oi) = R(oi) mean({R(oj)}G std({R(oj)}G j=1) j=1) , (9) where R(oi) is the reward of response oi ."
        },
        {
            "title": "B Evaluation Details",
            "content": "We used vllm for inference with greedy decoding (temperature = 0) to ensure reproducibility. Since vLLMs batched inference produces different outputs for the same input under different batch sizes, we set the validation batch size to 128 and evaluate each dataset independently to ensure consistency in evaluation. Because we used the Qwen2.5-Math base models with context length of 4k, we set the generation budget for all compared baselines to 3k."
        },
        {
            "title": "C Template",
            "content": "Qwen-Math Template <im_start>system Please reason step by step, and put your final answer within boxed{}. <im_end> <im_start>user {question} <im_end> <im_start>assistant Deepseek-R1 Template conversation between User and Assistant. The User asks question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and the answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. User: {question} Assistant:"
        },
        {
            "title": "D Detailed Form of Format Reward",
            "content": "DeepSeek-R1 DeepSeek-AI et al. (2025) introduced format reward to assess whether the models output aligns with the Deepseek-R1 template(Appendix C) format (i.e., writing the reasoning process within <think> </think> tags and placing the answer within <answer></answer> tags). Our format reward is designed based on Qwen-Math template(in Appendix C), which evaluates whether the models output contains boxed{} and checks whether the content inside the boxed{} is valid mathematical content. specific example is shown below: Right case: boxed{1} boxed{frac{3}{2}} boxed{x^2 + 12y =1} boxed{(0,infty)} Wrong case: boxed{} boxed{x +* 2} boxed{(0,1 }"
        },
        {
            "title": "E Performance Curves of Different Reward Trainings on Benchmarks",
            "content": "See Figure S1 and S2. 2 (a) (b) (c) Figure S1: Accuracy curves on (a) MATH500, (b) AIME2024, and (c) AMC2023 benchmarks training on the DeepScaleR-17k. (a) (b) (c) Figure S2: Accuracy curves on (a) MATH500, (b) AIME2024, and (c) AMC2023 benchmarks training on Math-train dataset."
        },
        {
            "title": "F Case Study",
            "content": "See Table S1 and S2. 3 PROMPT: systemnPlease reason step by step, and put your final answer within boxed{}. user You have seven bags of gold coins. Each bag has the same number of gold coins. One day, you find bag of 53 coins. You decide to redistribute the number of coins you have so that all eight bags you hold have the same number of coins. You successfully manage to redistribute all the coins, and you also note that you have more than 200 coins. What is the smallest number of coins you could have had before finding the bag of 53 coins?nassistantn RESPONSE_FORMAT-LENGTH: To solve this problem, we need to determine the smallest number of coins in each of the original seven bags such that when we add the 53 coins from the new bag, the total number of coins can be evenly distributed among eight bags, and the total number of coins is more than 200.nnLets break down the problem step by step :nn 1. **Define Variables:**n - Let be the number of coins in each of the original seven bags.n - The total number of coins in the original seven bags is 7x .n - After finding the new bag with 53 coins, the total number of coins becomes 7x + 53 .nn 2. **Redistribution Condition:**n - We need to redistribute these coins so that each of the eight bags has the same number of coins.n - Therefore , 7x + 53 must be divisible by 8.nn 3. **Mathematical Formulation:**n - We need to find the smallest such that 7x + 53 equiv 0 pmod{8} .n - This can be rewritten as 7x equiv -53 pmod{8} .n - Since -53 equiv 3 pmod{8} (because -53 + 56 = 3 and 56 is multiple of 8), we have 7x equiv 3 pmod{8} .nn 6. **Calculate the Total Number of Coins:**n - If = 5 , the total number of coins in the original seven bags is 7 times 5 = 35 .n - Adding the 53 coins from the new bag, the total number of coins is 35 + 53 = 88 .nn 7. ** Check the Condition:**n - We need to ensure that the total number of coins is more than 200.n - Since 88 is not more than 200, we need to find the next smallest that satisfies equiv 5 pmod{8} .nn 8. **Next Smallest :**n - The next smallest is = 5 + 8 = 13 .n - If = 13 , the total number of coins in the original seven bags is 7 times 13 = 91 .n - Adding the 53 coins from the new bag, the total number of coins is 91 + 53 = 144 .nn 9. ** Continue Checking :**n - Since 144 is still not more than 200, we continue to the next smallest .n - The next smallest is = 13 + 8 = 21 .n - If = 21 , the total number of coins in the original seven bags is 7 times 21 = 147 .n - Adding the 53 coins from the new bag, the total number of coins is 147 + 53 = 200 .nn 10. ** Final Check :**n - Since 200 is not more than 200, we need to find the next smallest .n - The next smallest is = 21 + 8 = 29 .n - If = 29 , the total number of coins in the original seven bags is 7 times 29 = 203 .n - Adding the 53 coins from the new bag, the total number of coins is 203 + 53 = 256 .nn 11. **Conclusion:**n - The smallest number of coins in each of the original seven bags that satisfies all conditions is = 29 .n - Therefore , the smallest number of coins you could have had before finding the bag of 53 coins is 7 times 29 = 203 .nnThus, the final answer is:n[nboxed{203} n]. RESPONSE_CORRECTNESS: To solve this problem, lets denote the number of coins in each of the original seven bags as . Therefore , the total number of coins you originally had is 7x . After finding the bag of 53 coins, you now have 7x + 53 coins. You redistribute these coins into eight bags, so each bag now has frac7x + 53{8} coins. Since the number of coins in each bag must be an integer, 7x + 53 must be divisible by 8. Additionally, we know that the total number of coins after redistribution is more than 200, so 7x + 53 > 200 . This simplifies to 7x > 147 , or > 21 . We need to find the smallest integer such that 7x + 53 is divisible by 8 and > 21 . Lets iterate through the possible values of starting from 22 and check the conditions.npythonnIterate through possible values of starting from 22nfor in range(22, 1000):n total_coins = 7 * + 53n if total_coins % 8 == 0:n smallest_number_of_coins = 7 * xn breaknnprint(smallest_number_of_coins)n noutputn183nnThe smallest number of coins you could have had before finding the bag of 53 coins is boxed{183}. Ø Table S1: Model output on complex mathematical problems after training on format-length reward and correctness reward. The blue boxes in the figure represent step-by-step problem-solving behavior, while the red boxes indicate verification behavior. Clearly, both types of behavior occur far more frequently in the format-length model than in the correctness model. As for the results, the former produces correct answers, whereas the latter produces incorrect ones. 4 PROMPT: systemnPlease reason step by step, and put your final answer within boxed{}.n user nDenali and Nate work for dog walking business and are paid for each dog they walk. Denali is responsible for 16 dogs and Nate is responsible for 12 dogs. Under the companys new policy, they will be assigned or unassigned new dogs in groups of dogs. The ratio of Denalis pay to Nates pay would be the same if Denali started walking 4x more dogs and Nate stayed at 12 dogs or if of Nates dogs were reassigned to Denali. Find if xneq0.nassistantn, RESPONSE: Lets denote the payment per dog as ( ). The initial ratio of Denalis pay to Nates pay is (frac16p12p = frac43).nnIf Denali starts walking (4x) more dogs and Nate stays at 12 dogs, the new ratio of their pays would be (frac(16 + 4x)p12p = frac16 + 4x12). Since this ratio is equal to (frac43), we can set up the equation:n[nfrac16 + 4x12 = frac43.n]nCross-multiplying gives:n[n3(16 + 4x) = 4 cdot 12,n]n[n48 + 12x = 48,n]n[n12x = 0,n]n[nx = 0.n]n Since (x neq 0), we need to consider the second scenario where (x) of Nates dogs are reassigned to Denali. In this case, the new number of dogs Denali is responsible for is (16 + x) and the new number of dogs Nate is responsible for is (12 - x). The new ratio of their pays would be (frac(16 + x)p(12 - x)p = frac16 + x12 - x). Since this ratio is equal to (frac43), we can set up the equation:n[nfrac16 + x12 - = frac43.n]nCross-multiplying gives:n[n3(16 + x) = 4(12 - x),n]n[n48 + 3x = 48 - 4x,n]n[n7x = 0,n]n[nx = 0.n]nAgain, since (x neq 0), we need to recheck the problem constraints. The only way the ratio can remain the same is if the number of dogs reassigned is such that the ratio of the new number of dogs is the same as the original ratio. Lets solve it again by checking the constraints properly.nnThe correct approach is to solve the equation (frac16 + x12 - = frac43) properly. Lets solve it step by step again.nn[n3(16 + x) = 4(12 - x), n]n[n48 + 3x = 48 - 4x,n]n[n7x = 0,n]n[nx = 0.n]n Since (x neq 0), we need to consider the possibility that the problem might have different interpretation or constraint. Lets recheck the problem statement and constraints. The only way the ratio can remain the same is if the number of dogs reassigned is such that the ratio of the new number of dogs is the same as the original ratio. Lets solve it again by checking the constraints properly. Repeat1: Lets solve it step by step again.nn[n3(16 + x) = 4(12 - x), n]n[n48 + 3x = 48 - 4x,n]n[n7x = 0,n]n[nx = 0.n]n Since (x neq 0), we need to consider the possibility that the problem might have different interpretation or constraint. Lets recheck the problem statement and constraints. The only way the ratio can remain the same is if the number of dogs reassigned is such that the ratio of the new number of dogs is the same as the original ratio. Lets solve it again by checking the constraints properly. Repeat2: Lets solve it step by step again.nn[n3(16 + x) = 4(12 - x), n]n[n48 + 3x = 48 - 4x,n]n[n7x = 0,n]n[nx = 0.n]n Since (x neq 0), we need to consider the possibility that the problem might have different interpretation or constraint. Lets recheck the problem statement and constraints. The only way the ratio can remain the same is if the number of dogs reassigned is such that the ratio of the new number of dogs is the same as the original ratio. Lets solve it again by checking the constraints properly. Table S2: Over-reflection in sentence repetition.The red boxes in the figure indicate reflective words, while the blue boxes represent repeated phrases (with the numbers indicating the frequency of repetition). It can be observed that reflective words appear within the repeated phrases."
        }
    ],
    "affiliations": [
        "Baichuan Inc.",
        "Harbin Institute of Technology",
        "Tsinghua University"
    ]
}