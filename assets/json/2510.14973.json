{
    "paper_title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
    "authors": [
        "Quan Nguyen-Tri",
        "Mukul Ranjan",
        "Zhiqiang Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences, and $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 3 7 9 4 1 . 0 1 5 2 : r a"
        },
        {
            "title": "ATTENTION IS ALL YOU NEED\nFOR KV CACHE IN DIFFUSION LLMS",
            "content": "Quan Nguyen-Tri FPT AI Residency Hanoi, Vietnam quannt40@fpt.com Mukul Ranjan & Zhiqiang Shen VILA Lab, MBZUAI Abu Dhabi, UAE {mukul.ranjan,zhiqiang.shen}@mbzuai.ac.ae Project page: https://vila-lab.github.io/elastic-cache-webpage/"
        },
        {
            "title": "ABSTRACT",
            "content": "This work studies how to adaptively recompute keyvalue (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant MASK tokens primarily act as length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing conservative lower bound on cache change for other tokens. Building on these, we propose Elastic-Cache, training-free, architecture-agnostic strategy that jointly decides when to refresh (via an attention-aware drift test on the most-attended token) and where to refresh (via depth-aware schedule that recomputes from chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: 8.7 on GSM8K (256 tokens), 45.1 on longer sequences, and 4.8 on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput (6.8 on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion large language models (DLMs) (Li et al., 2025) have recently emerged as compelling alternative to autoregressive Transformers (Radford et al., 2018; Achiam et al., 2023), yet their iterative denoising procedure makes inference particularly compute-intensive. In standard implementations, each decoding step recomputes queries, keys, and values (QKV) for every token at every layer, even though the underlying keyvalue (KV) states change only marginally across most steps. This all-tokens, all-layers recomputation incurs substantial latency and memory traffic, ultimately limiting practical deployment. Our goal in this study is to determine how and when to adaptively recompute the KV cache during decoding so as to maximize prediction quality while minimizing wall-clock latency. defining property of diffusion LLM decoding is the progressive unmasking of tokens under lengthand structureaware attention pattern. This induces heterogeneous KV dynamics: shallow layers tend to stabilize quickly as they encode local lexical structure, whereas deeper layers continue to adjust global, semantic dependencies. We formalize this with notion of KV drift: the step-to-step change in cached keys and values, and observe two consistent trends: (i) drift is small for most steps, and (ii) drift grows with layer depth. These trends suggest that indiscriminate recomputation is wasteful, and that targeted refreshes could preserve accuracy while slashing cost. Prior acceleration methods for diffusion (and related) decoders typically refresh the KV cache on fixed schedule, e.g., every iterations without regard to instance difficulty, current attention patterns, or layerwise variability. Such fixed-period policies leave performance on the table: they recompute when nothing has changed and miss updates precisely when rapid semantic revisions occur. Moreover, by treating all layers uniformly, they over-service shallow layers whose representations have already converged, while under-servicing deeper layers where changes matter most. This motivates an adaptive, attention-aware alternative. Equal contribution 1 Our approach is built on three empirical observations. First, distant MASK tokens exert negligible influence on unmasking the current token and behave primarily as length-bias prior; thus, their KV can be block-cached outside the active prediction window to avoid redundant work. Second, KV drift increases with depth, so refreshes should start at learned boundary layer ℓ and apply only to deeper layers, reusing shallow-layer caches. Third, the most-attended token at step typically exhibits the smallest drift, providing conservative lower bound on KV changes across the context. Monitoring this drift yields reliable, low-overhead trigger for deciding whether global refresh is warranted. Based on these ideas, we propose Elastic-Cache, training-free, architecture-agnostic strategy that couples AttentionAware KV Cache Update with Layer-Aware KV Cache Update. The attention-aware module computes lightweight drift statistic on the most-attended token; if the statistic exceeds threshold, refresh is triggered, otherwise cached KVs are reused. The layer-aware module then refreshes only layers ℓ ℓ, while shallow layers retain their caches, and off-window MASK tokens remain block-cached. Together, these mechanisms align recomputation with where and when the models beliefs actually change, minimizing unnecessary QKV work. In contrast to fixed-period baselines, our Elastic-Cache adapts to the input, step, and layer granularity together. It reduces compute by skipping recomputation during stable phases, focuses effort on deeper layers during semantic revisions, and leverages block-wise caching for distant MASK tokens. Conceptually, the method reframes KV management as an attention-guided control problem: attention estimates which tokens matter; drift detects how much the state has changed; and the layer boundary ℓ encodes where updates pay off. This yields practical pathway to low-latency diffusion LLM decoding without modifying training or the base architecture. Our contributions of this work: We diagnose redundancy in diffusion LLM decoding and introduce KV drift as principled signal for adaptive cache management. We propose Elastic-Cache, the first (to our best knowledge) adaptive, layer-aware KV refresh policy for diffusion LLMs that jointly decides when to recompute (attention-aware drift test) and where to recompute (depth-selective updates). We develop block-wise MASK caching to eliminate needless updates outside the prediction window. We provide comprehensive empirical experiments and ablations showing that our Elastic-Cache preserves generation quality while substantially reducing decoding latency across tasks and model scales."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "2.1 MASKED DIFFUSION MODELS Masked Diffusion Models (MDMs), absorbing-state discrete diffusion, build on D3PM (Austin et al., 2021a) and its continuous-time variant (Campbell et al., 2022), replacing tokens with special MASK along forward process (Sahoo et al., 2024; Shi et al., 2024) at timestep t: qt0(xtx0) = (cid:89) i=1 qt0(xi txi 0) = (cid:89) i=1 Cat(xi t; (1 t)δxi 0 + tδMASK) (1) where [0, 1] controls interpolation between the original data x0 (at = 0) and fully masked sequence (at = 1), Cat() denotes the categorical distribution. parametric model pθ learns the reverse denoising; generation starts from all MASK and iteratively unmasks by sampling pθ(xi 0xt). Recent theory (MDLM (Shi et al., 2024; Sahoo et al., 2024), RADD (Ou et al., 2024)) simplifies training from variational bound to reweighted cross-entropy over masked positions: LMDM = (cid:90) 1 0 1 Eqt0(xtx0) (cid:88) log pθ(xi 0xt) dt i:xi t=MASK (2) This formulation scales to LLMs as diffusion language models (DLMs), with LLaDA (Nie et al., 2025b) and Dream7B (Ye et al., 2025) matching autoregressive performance while enabling parallel decoding and flexible infilling. 2.2 KEY-VALUE CACHE IN TRANSFORMERS Transformer-based language models achieve computational efficiency during autoregressive generation through KeyValue (KV) caching (Pope et al., 2023). In causal attention, each layer projects the current hidden state Ht into query, 2 Figure 1: Visualization of our motivation. (a) MASK tokens located near each other receive high attention, while those situated far apart have minimal influence. (b) Over time, the representations in the KV states of cached tokens evolve, with deeper layers experiencing more substantial changes. (c) The changes in attention weights of most-attended tokens exhibit similar patterns to the changes in the KV states of all cached tokens. (d) The KV states of the mostattended tokens have the least changes. key, and value representations using learned projection matrices WQ, WK, WV. At decoding step t, the attention computation for the current token follows: At [t] = softmax (cid:32) Qt [t](Kt [1:t]) dk (cid:33) Vt [1:t], KV cache: (cid:40) [1:t] = concat(Kt1 Kt [1:t] = concat(Vt1 Vt [1:t1], Kt [1:t1], Vt [t]), [t]) . (3) To avoid redundant computation, previous key-value pairs are cached and reused. This caching strategy is effective because in causal attention, previously computed key-value pairs remain invariant throughout decoding (Kt1 [1:t1] = Kt [1:t1]), enabling efficient reuse without affecting model output. KV-Cache in Bidirectional Attention. However, diffusion models employ bidirectional attention where all positions can attend to each other, breaking the invariance property of cached representations. As noted by dKV-Cache (Ma et al., 2025), token representations in diffusion models evolve dynamically during the iterative denoising process, making direct application of traditional KV-cache ineffective. The bidirectional dependencies cause previously computed key-value pairs to become stale as the sequence state changes, requiring careful redesign of caching strategies for diffusion language models."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 OUR FRAMEWORK OVERVIEW AND MOTIVATION Diffusion LLMs differ from autoregressive decoders in that their keyvalue (KV) states evolve across denoising steps due to bidirectional dependencies. Our objective is to adaptively decide when and where to recompute the KV cache to preserve accuracy while minimizing latency. Baseline decoders recompute QKV for all tokens and layers at every step, despite negligible KV changes for most steps and especially in shallow layers (Fig. 1b); deeper layers exhibit larger drift. Rather than fixed-period refreshes (Wu et al., 2025; Ma et al., 2025; Liu et al., 2025), we propose Elastic-Cache, the first (to our knowledge) adaptive, layer-aware KV update policy for diffusion LLMs that jointly optimizes timing and location of recomputation. Our design is driven by three observations. (1) Distant MASK tokens mainly act as length prior and exert minimal influence on the current unmasking, we therefore block-cache their KV beyond the active prediction window (Fig. 1a). (2) KV drift grows with depth, refresh should start at boundary layer and apply only to deeper layers (Fig. 1b). (3) The most-attended tokens typically shows the smallest KV change (Fig. 1d), giving conservative lower bound for others, we use its drift as lightweight trigger for refresh (Fig. 1c). Fig. 2 summarizes the pipeline. To the end, we proposed Elastic-Cache, flexible method for key-value caching in diffusion large language models. Fig. 2 provides visual representation of the overall pipeline of our proposed method. 3.2 SLIDING WINDOW DECODING AND KV CACHING Formally, let = {1, 2, . . . , } represent all positions. At decoding step t, let Dt denote newly decoded positions and Mt denote remaining masked positions, where Mt1 = Mt Dt. Denotes D<t = (cid:83) i=1 as the set of all i{Di}t 3 Figure 2: Illustration of the Key-Value cache method for diffusion LLMs. (a) The fast-dLLM (Wu et al., 2025) block-wise decoding method caches the Key-Value of all tokens outside the current block at each step. The KV cache is updated after completing block of decoding. (b) Our proposed method, Elastic-Cache, caches the key-value of tokens outside sliding window that flexibly moves through the sentence from left to right at each iteration. When the attention weights corresponding to the most-attended tokens (one for each layer) change significantly at layer l, we start recomputing the KV cache from layer + 1 to the last layer. decoded tokens up to time step t. Initially, at = 0 we compute the attention for each layer l: (cid:40) K0,l [I](K0,l [I]) V0,l dk initialize KV cache: [I] = softmax (cid:32) Q0,l V0,l [I], A0,l (cid:33) [I] = K0,l [I] = V0,l [I] [I] . (4) For each subsequence iteration ranging from 1 to , The model perform prediction for newly decoded position Dt and the remaining masked position Mt. To enhance efficiency, we only perform predictions for masked positions that are closest to the left and form sliding window of size β, denoted as Mt [1:β]. We also have that Mt1 β Dt. Weve observed that masked positions within the sliding window tend to pay close attention to each other. Conversely, MASK tokens outside the sliding window receive minimal attention, allowing us to reuse their KV from the cache without significantly altering the prediction of the current masked position within the sliding window. We compute the attention only for the tokens in the sliding window Mt1 at step as follows: β = Mt β = Mt β At,l [Mt1 β ] = softmax Qt,l ( Kt,l [I]) [Mt1 ] β dk Vt,l [I], update KV cache at Mt1 β : Kt,l Vt,l [Mt1 β [Mt1 β = Kt,l = Vt,l [Mt1 β [Mt1 β . (5) ] ] ] ] Although the proposed sliding window decoding and KV cache share some similarities with the KV cache for blockwise decoding in Fast-dLLM (Wu et al., 2025) (as shown in the Fig. 2 for pipeline overview and comparison of both methods), we would like to emphasize the significant advancement of sliding window decoding. It ensures that tokens are close to each other and are predicted together, thereby minimizing the loss of cache for faraway MASK tokens. On the other hand, block-wise decoding may overlook those MASK tokens that are near the end of the block, leading to inefficient predictions due to overly aggressive caching of their surrounding context. 3.3 ATTENTION-AWARE KV CACHE UPDATE The most important novelty of our proposed method is to automatically determine whether to update the KV cache to preserve accuracy while minimizing latency. Our method leverages the awareness of the models attention weights to identify when the KV cache undergoes significant changes. At time step and layer l, we determine the token that receives the most frequent attention from other tokens based on the attention weights corresponding to the current models prediction for Mt β. t,l = arg max kD<t (cid:88) qMt β St,l [q,k], 4 where: St,l [Mt β ] = softmax Qt,l [Mt [I]) β ]( Kt,l dk . (6) Algorithm 1 The Elastic-Cache Algorithm 1: Require: Trained model pθ, Prompt xprompt, Sliding window size β, Update threshold γ, Generation length . 2: Initialize: x0 {xprompt; [MASK], . . . , [MASK]}; length(xprompt) 3: 1; D1 {1, . . . , }; M1 {P + 1, . . . , + }; 4: while Mt = do 5: Mt 6: 7: 8: for = 1, . . . , do if > then [I], Vt,l [I], Kt,l Ht,l , St,l Ht,l+1 [I] attention layer(Qt,l [I] [I] cache update(I); Qt,l [I], Kt,l [I], Vt,l [I], Kt,l [I], Vt,l [I]) [:β]; Qt t1 Mt [Qt] Embedding(xt [I] = linear(Ht,l [I]) = M0 = D1 M1 β Mt ; Ht,0 0 ; [Qt]); β // Cache Update else [Qt], Kt,l [Qt] , St,l [Qt], Kt,l [Qt] cache update(Qt); Qt,l [I], Vt,l [I]) [Qt], Vt,l [I] attention layer(Qt,l Ht,l Ht,l+1 σt,l cosine similarity(St1,l if σt,k < γ then [Qt], Kt,l [T t1]) [T t1], St,l l; Ht,l+ [I] get cached state(Ht,l+1 [Qt] ) end if end if Get the most-attended token: t,l arg maxkD<t (cid:80) qMt β end for Decode new tokens: xt+1, Dt+1 decode(xt, Mt Update state: Mt+1 Mt Dt+1; = (cid:83) β) l{T t,l}L l=1 [Qt], Vt,l [Qt] = linear(Ht,l // Cache Reuse [Qt]) // Start update cache from layer + St,l [q,k] + 1 // State Update 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: end while 23: return xt1 l{T t,l}L Here, we focus solely on the most-attended token among the current decoded tokens D<t. This is because the remaining MASK tokens either fall within the sliding window of predictions or have negligible influence on the unmasking tokens (Fig. 1a). We obtain one most-attended token per layer and compile the set of most-attended tokens, denoted as = (cid:83) l=1. In practice, the most-attended token for layer often overlaps with tokens from other layers, resulting in relatively limited number of most-attended tokens being available at any given time. t, besides being the tokens that have the most influence on the predictions outcome, also signify the least changes among the cached decoded tokens (Fig. 1d). Therefore, we use as lightweight trigger for our cache update mechanism. Without updating all cached tokens, we only frequently update the most-attended tokens to measure the degree of changes for all other cached tokens. Ideally, since have the least change among the decoded tokens, we expect that when change significantly, the rest of the decoded tokens will also change significantly. Therefore, we add t1 to the sliding window at step t: t1 Mt1 . We then measure the changes in attention weights of between the current and previous steps, and 1, using cosine similarity. β = (cid:26) if: σt,l < γ othewise , Cosine Similarity(St1,l [T t1], St,l [T t1]) = St1,l [T t1] St,l [T t1] St,l St1,l [T t1] [T t1] = σt,l. (7) The changes in attention St,l directly affect the output of the current layer or the input of the next layer Ht,l+1. This implies that our cached values are diverging from the actual values, necessitating an update. When layer observes significant changes in attention weights σt,l < γ, we initiate the update of the KV cache for the subsequent layers, starting from + 1 and continuing until the last layer L. To achieve this, we initialize the hidden states of all cached tokens with the states Ht,l+1 , which have been saved and updated using patterns similar to Kt,l+1 [I] Update state: Ht,l+1 [Mt1 β ] = Ht,l+1 [Mt1 β , ] Initialize cache update: Qt,l+1 [I] , Kt,l+1 [I] We then update and overwrite the KV cache using the same process as initially at = 0, as described in Eq. 4. If none of the layers satisfy σt,l < γ, we continue to reuse our KV cache for future predictions. We didnt directly compare the hidden state Ht,l+1 and Ht1,l+1 because their changes depend on various network components. The error in measurement could be amplified by the divergence between the cached value and the actual value (including Key-Value states). 5 [I] and Vt,l+1 . [I] = linear( Ht,l+1 [I] [I] , Vt,l+ ) (8) On the other hand, the changes in attention weights are closely linked to the source of the change in Key-Value states, which is the bidirectional attention mechanism in diffusion LLMs. Intuitively, the changes in attention weights become significant when new decoded tokens receive high attention and alter the attention output computed in the past when they were still masked. Consequently, the changes in attention weights exhibit very similar patterns to the changes in Key-Value states during decoding, as illustrated in Fig. 1b and Fig. 1c. We use the hyper-parameter γ to set the trigger for automatic cache updates. As shown in Fig. 1c, the attention weights cosine similarity landscapes influence this. higher γ results in more frequent and extensive cache updates across multiple layers, while lower γ triggers updates less frequently. This flexibility allows us to effectively manage the trade-off between accuracy and latency. Our overall algorithm is described in Algorithm 1."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Implementation Details. All our runs use single NVIDIA A100 80GB. We evaluate Elastic-Cache on LLaDAInstruct (Nie et al., 2025a), LLaDA-1.5 (Zhu et al., 2025), and multimodal LLaDA-V (You et al., 2025) across MBPP (Austin et al., 2021b), HumanEval (Chen et al., 2021), MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), MathVista (Lu et al., 2023), and MathVerse (Zhang et al., 2024b). Default hyperparameters: attention threshold γ=0.9, parallel-decoding confidence ϵ=0.9, cache block size 32. For fair comparison, we re-run LLaDA (Nie et al., 2025a) and Fast-dLLM (Wu et al., 2025) under the same hardware/software. Evaluation Framework and Metrics. We use lm-eval-harness (Gao et al., 2024). Throughput is tokens/sec averaged until emitting, matching Fast-dLLMs protocol (Wu et al., 2025). Accuracy metrics: GSM8K: 5-shot flexible extract (Cobbe et al., 2021); MATH: 4-shot math verify (minerva math) (Hendrycks et al., 2021); HumanEval0-shot with the Fast-dLLM post-processing (Chen et al., 2021; Wu et al., 2025); MBPP3shot pass@1 (Austin et al., 2021b). For LLaDA-V, we adopt the official pipeline with lmms-eval (Zhang et al., 2024a; Li et al., 2024): MathVista: gpt eval score (Lu et al., 2023); MathVerse: gpt eval score on mathverse testmini vision dominant (Zhang et al., 2024b). Confidence-Aware Decoding. We employ confidence-aware decoding strategies from Fast-dLLM (Wu et al., 2025), which select only tokens with confidence scores exceeding specified threshold (ϵ), instead of unmasking fixed number of tokens per step, as in the baseline Diffusion LLM. This straightforward yet effective approach accelerates Diffusion LLM inference by enabling more tokens to be predicted concurrently at each iteration, contingent upon the models performance. Consequently, we concentrate on comparing the acceleration achieved by the KV caching method under the same decoding strategies. 4.2 PERFORMANCE AND EFFICIENCY EVALUATION Across Tables 1, 2, and 3, our proposed Elastic-Cache delivers substantial throughput gains for diffusion LLMs with minimal accuracy loss. By adaptively updating the cache only when necessary, it achieves speedup of up to 45.1 over the standard baseline. While maintaining accuracy within 12% on MATH and HumanEval, it also achieves higher accuracy on GSM8K and MBPP. Compared to Fast-dLLM (Wu et al., 2025), Elastic-Cache consistently attains greater tokens/sec at better accuracy. As presented in Table 1, on LLaDA-Instruct, Elastic-Cache reaches 90.1 t/s on GSM8K (512 tokens; 25.2 over baseline) at 77.71% accuracy, surpassing Fast-dLLMs 44.0 t/s @ 74.83%. On LLaDA-1.5  (Table 2)  , our approach yields even greater gains, including 45.1 on GSM8K-512, with an accuracy of 81.35% (baseline 81.35%). This observation indicates that Elastic-Cache performs better when the models predictions are more accurate. The reason behind this could be the close relationship between our approach and attention scores. Intuitively, accurate predictions are associated with meaningful attention scores with fewer outliers, which makes our approach operate more smoothly. We also observed that in most settings, Elastic-Cache provides higher throughput for longer generation lengths, whereas this is the opposite for Fast-dLLM (Wu et al., 2025), as it often experiences reduced throughput as the generation length increases. The advantages of our approach stem from the fixed-size sliding window and automatic cache update, which minimizes the dependency of throughput on the generation length. In the multimodal setting (LLaDA-V; Table 3), Elastic-Cache raises MathVerse-256 throughput to 32.3 t/s from FastdLLMs 30.3 t/s while maintaining 29.19% accuracy, demonstrating robustness beyond text-only tasks. The significant improvement of Elastic-Cache compared to the baselines across various settings suggests that our method is broadly applicable and has high scalability potential. 6 Table 1: Comprehensive benchmark results on the LLaDA-Instruct suite. Each cell shows accuracy (top) and decoding throughput in tokens/sec with relative speedup to the LLaDA baseline (bottom, blue: t/s / orange: speedup). Highlighted cells denote the highest throughput and speedup per configuration. The highest accuracy is bolded. Benchmark Gen Length LLaDA LLaDA Fast-dLLM Elastic-Cache Confident-Aware Decoding GSM8K (5-shot) MATH (4-shot) HumanEval (0-shot) MBPP (3-shot) 256 512 256 256 512 256 512 78.01 7.3 (1.0) 77.10 3.6 (1.0) 33.58 9.5 (1.0) 37.20 7.1 (1.0) 40.85 33.3 (1.0) 43.90 17.7 (1.0) 29.80 6.5 (1.0) 15.0 4.7 (1.0) 78.62 22.8 (3.1) 77.33 18.6 (5.2) 33.28 25.8 (2.7) 36.82 24.0 (3.4) 42.07 102.1 (3.1) 43.29 51.6 (2.9) 30.00 23.4 (3.6) 15.0 20.8 (4.4) 77.94 53.7 (7.7) 74.83 44.0 (12.3) 32.50 49.0 (5.1) 35.70 52.8 (7.4) 37.20 99.8 (3.0) 45.73 76.1 (4.3) 25.40 45.1 (7.0) 13.6 44.7 (9.5) 78.24 58.0 (8.2) 77.71 90.1 (25.2) 33.14 48.7 (5.1) 36.60 59.3 (7.9) 40.24 160.5 (4.8) 46.34 100.7 (5.0) 32.2 46.9 (7.3) 15.6 63.0 (13.4) 4.3 ABLATIONS We ablate key choices: 1) Cache update threshold γ, 2) sliding window size β, and 3) prefill and generation length, to expose speed/accuracy trade-offs and justify defaults. Cache Update Threshold (γ). Table 4 illustrates the sensitivity of our proposed method to the parameter γ. As γ is used to control the frequency of cache updates, consistent decrease in γ leads to an increase in throughput. However, there is also trend of decreasing accuracy as throughput increases. The trend is more consistent for the LLaDA-1.5 model, while for LLaDA, the accuracy at peak (γ = 0.9) is higher, but the throughput is lower. Sliding Window Size (β). Fig. 3a shows that our accuracy is stable across various β and close to No-Cache until β 64; beyond that LLaDAs tendency to emit EOS early degrades results (You et al., 2025). Throughput, however, is sensitive to β: larger windows enable more parallel prediction (fewer iterations, lower latency), but overly large β reduces cacheable MASK tokens, raising per-step compute and latency. Sliding Window vs. Block-Wise. When switching Elastic-Cache to block-wise decoding (Fast-dLLM-style) (Fig. 3a), our accuracy is often similar to No-Cache, but short blocks hurt accuracy and throughput diverges. Our sliding window groups nearby MASK tokens that strongly attend to each other, whereas block-wise caching over-aggressively freezes distant MASKs, harming small-block predictions. Our Elastic-Caches automatic cache refresh detects divergent tokens and updates them, preserving accuracy at the cost of some throughput. Prefill and Generation Length. Table 5a and Table 5b provide insights into the impact of prefill length and generation length on the overall speedup. Notably, both Fast-dLLM and Elastic-Cache experience decrease in throughput as the prefill length increases from 3-shot to 8-shot. However, Elastic-Cache exhibits remarkable speedup and consistently high accuracy across different prefill lengths. Moreover, the throughput of Elastic-Cache increases with generation length, highlighting its unique scaling properties. 4.4 ANALYSIS Cache update frequency. Fig. 3b and Fig. 3c illustrate the frequency of cache updates performed by Elastic-Cache under varying hyper-parameters γ and ϵ. The proposed method maintains very low cache update frequency across different values of γ (Fig. 3b). In extreme cases, with γ = 0.95, the cache update frequency increases to only 20% compared to the baseline without cache. Moreover, increasing the models confidence and accuracy (with ϵ, Fig. 3c) enhances Elastic-Caches effectiveness, and reduces the cache update frequency. Tunable SpeedAccuracy Trade-off. The cache update threshold γ directly determines the balance  (Table 4)  . An excessively high γ could lead to unnecessary cache updates, resulting in decrease in speedup without any improvement in accuracy. Conversely, smaller γ value could guarantee speedup while sacrificing accuracy. The optimal value for Table 2: Comprehensive benchmark results on the LLaDA-1.5 suite. Each cell shows accuracy (top) and decoding throughput in tokens/sec with relative speedup to the LLaDA baseline (bottom, blue: t/s / orange: speedup). Highlighted cells denote the highest throughput and speedup per configuration. Benchmark Gen Length LLaDA-1.5 LLaDA-1.5 Fast-dLLM Elastic-Cache Confident-Aware Decoding GSM8K (5-shot) MATH (4-shot) HuamnEval (0-shot) MBPP (3-shot) 256 512 512 256 512 256 512 80.36 6.7 (1.0) 81.35 2.6 (1.0) 33.52 8.5 (1.0) 35.63 5.0 (1.0) 43.29 7.0 (1.0) 40.85 3.2 (1.0) 38.00 2.4 (1.0) 38.20 1.0 (1.0) 80.44 22.5 (3.3) 81.88 17.2 (6.6) 33.60 22.3 (2.6) 35.56 20.3 (4.0) 42.68 17.5 (2.5) 39.63 9.7 (3.1) 80.59 51.2 (7.6) 80.82 36.8 (14.1) 32.74 44.4 (5.2) 33.68 44.4 (8.8) 34.75 18.7 (2.7) 36.59 15.4 (4.8) 38.00 14.2 (5.8) 38.60 11.5 (11.5) 34.60 28.0 (11.6) 36.20 17.8 (17.8) 81.50 58.0 (8.7) 81.35 117.2 (45.1) 33.50 51.0 (6.5) 35.36 74.8 (14.9) 36.59 20.9 (3.0) 37.80 16.8 (5.3) 41.20 32.7 (13.5) 39.00 32.8 (32.8) Table 3: Performance and Speedup Comparison of LLaDA-V on MathVista and MathVerse. Each benchmark presents results from LLaDA-V (base) using Fast-dLLM, and our method. Length 512 MathVista MathVerse Fast-dLLM Elastic-Cache (Ours) Fast-dLLM Elastic-Cache (Ours) 55.9 28.7 54.1 23.7 55.9 29.7 55.8 24. 26.78 30.3 25.5 28.1 29.19 32.3 29.19 30.8 γ to maximize both accuracy and throughput depends on the models prediction. Models with higher accuracy tend to have the best γ value, which is closer to 1.0  (Table 4)  . Scaling Properties. Elastic-Cache scales greatly with the generation length and the power of the base model. Increasing the generation length slows down the baseline performance but speeds up Elastic-Cache (Tables 5b). Moreover, Elastic-Cache effectiveness is highly dependent on the accuracy of the models predictions (Table 1, Table 2, Fig. 3c). This indicates that Elastic-Cache can effectively scale with the size of the model and the size of the training data, as LLMs generally improve when they scale up."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Diffusion Language Models. Classical diffusion models excel in continuous domains: images (Ho et al., 2020; Dhariwal & Nichol, 2021; Rombach et al., 2022), audio (Yang et al., 2023; Huang et al., 2023), and video (Xing et al., 2024; Ho et al., 2022a;b), building on the seminal formulation of Sohl-Dickstein et al. (2015). Adapting diffusion to discrete text has followed Markov/multinomial/continuous-time paths (Austin et al., 2021a; Hoogeboom et al., 2021b;a; Campbell et al., 2022; Sun et al., 2022), refined via score matching, ratio methods, and reparameterization (Meng et al., 2022; Lou & Ermon, 2023; Zheng et al., 2023), with recent work unifying these views (Sahoo et al., 2024; Shi et al., 2024; Ou et al., 2024; Zheng et al., 2024). Early NLP systems validated these ideas (He et al., 2022; Li et al., 2022; Gong et al., 2022) and explored semi-autoregression (Han et al., 2022). Masked diffusion approaching autoregressive quality (Sahoo et al., 2024) enabled scalable models (LLaDA) competitive with LLaMA (Nie et al., 2025a; 2024; Touvron et al., 2023a; Dubey et al., 2024), with further gains from AR adaptation and instruction tuning (Gong et al., 2024; Zhu et al., 2025; Ye et al., 2025). The paradigm now spans multimodal/structured domains (You et al., 2025; Yang et al., 2025; Yu et al., 2025; Wang et al., 2024a;b; Kitouni et al., 2023). Acceleration Techniques for Large Language Models. KV caching underpins efficient transformer inference (Vaswani et al., 2017; Pope et al., 2023), complemented by GQA, RoPE, and modern LLM optimizations (Ainslie et al., 2023; Su et al., 2024; Touvron et al., 2023a;b; Dubey et al., 2024). Diffusion LLMs complicate caching due to bidirectional attention and evolving representations; dedicated methods include Fast-dLLM (Wu et al., 2025), dKV8 (a) Impact of Sliding window and block size (b) Cache update frequency. (c) Confident-aware decoding. Figure 3: Ablation study and analysis of our proposed method. (a) Ablation study of our sliding window mechanism compared to block-wise decoding. (b) Analysis of cache update frequency under varying γ. The blue and orange lines represent accuracy and throughput, respectively. The numbers along the lines indicate the frequency of cache updates, assuming no baseline. (c) Analysis of cache update frequency under confident-aware decoding with varying ϵ. Table 4: Impact of attention threshold on accuracy and speedup under GSM8K (5-Shot) for LLaDA and LLaDA1.5 with generation length of 512. Model No Cache Fast-dLLM γ = 0. γ = 0.7 Elastic-Cache (Ours) γ = 0.8 γ = 0.85 γ = 0.9 γ = 0.95 LLaDA LLaDA-1.5 77.10 3.6 (1.0) 81.35 2.6 (1.0) 74.83 44.0 (12.2) 80.82 36.8 (14.2) 71.57 109.9 (30.5) 76.04 142.7 (54.9) 73.46 108.7 (30.2) 77.63 138.6 (53.3) 74.30 103.9 (28.9) 79.45 131.2 (50.5) 74.68 99.1 (27.5) 80.21 129.9 (50.0) 77.71 91.5 (25.4) 81.35 117.2 (45.1) 76.72 75.5 (21.0) 83.02 98.4 (37.8) Table 5: Comparison between Elastic-Cache and Fast-dLLM when varying few-shots and generation length. (a) Impact of few-shots on Accuracy and Speedup Under GSM8K (generation length of 1024) for LLaDA. (b) Impact of generation length on Accuracy and Speedup Under GSM8K (5-Shot), γ = 0.8 for LLaDA. Model. 3-shot 5-shot 8-shot Model. 512 1024 Fast-dLLM Elastic-Cache 73.77 28.5 (1.0) 75.13 185.3 (6.5) 76.04 25.0 (1.0) 75.21 169.8 (6.8) 75.36 20.8 (1.0) 75.28 143.9 (6.9) Fast-dLLM Elastic-Cache 77.94 53.7 (1.0) 78.24 58.0 (1.1) 74.83 44.0 (1.0) 77.71 91.5 (2.1) 76.04 25.0 (1.0) 75.21 169.8 (6.8) Cache (Ma et al., 2025), and DeepCache (Ma et al., 2024). Orthogonal accelerations exploit parallel/non-AR generation (Gu et al., 2017; Xiao et al., 2023), block-wise diffusion (Arriola et al., 2025), fast sampling (Chen et al., 2023), test-time scaling (Ramesh & Mardani, 2025), and consistency models (Kou et al., 2024). However, most rely on temporal heuristics or fixed thresholds, leaving attention patterns underused. Our Perspective. We close this gap with attention-aware and layer-aware caching for diffusion LLMs: tracking most-attended tokens and depth-varying KV dynamics to guide recomputation, complementary to interval-based (Ma et al., 2025) and confidence-based (Wu et al., 2025) policies and compatible with the broader acceleration toolkit (Ainslie et al., 2023; Su et al., 2024; Touvron et al., 2023a;b; Dubey et al., 2024; Gu et al., 2017; Xiao et al., 2023; Arriola et al., 2025; Chen et al., 2023; Ramesh & Mardani, 2025; Kou et al., 2024)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We presented Elastic-Cache, training-free, architecture-agnostic policy that makes KV caching in diffusion LLMs adaptive along two axes: when to refresh (via an attention-aware drift test) and where to refresh (via depth-selective update starting at learned boundary layer). By block-caching distant MASK tokens, reusing shallow-layer caches, and refreshing only when the most-attended token indicates meaningful state change, Elastic-Cache removes large amounts of redundant QKV work. Across decoding steps, this yields substantial latency reductions with negligible impact on generation quality, addressing key deployment bottleneck for diffusion decoders. Looking ahead, we plan to refine drift thresholds with learned predictors, formalize guarantees linking attention patterns to KV drift, and explore interplay with speculative decoding or other hardware-aware scheduling, extending the same principles to autoregressive LLMs and multimodal diffusion frameworks."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work targets inference-time efficiency for diffusion LLMs and does not introduce new data collection or model training. All evaluations use publicly available datasets and third-party checkpoints under their original licenses, no personally identifiable information is processed. While faster decoding can lower the cost of generation and thus broaden access, it may also amplify misuse. We neither change safety filters nor attempt to bypass alignment constraints of the underlying models. We will document evaluation prompts and tasks, follow the usage policies of model providers, and encourage human oversight for downstream deployments, especially in high-stakes applications."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "Elastic-Cache is training-free and defined by small set of inference hyperparameters: the attention similarity threshold γ, block size and generation length. We will release code, configs, and scripts to reproduce all results: (i) reference implementations of Attention-Aware and Layer-Aware KV updates with ablation; (ii) exact prompts/datasets, metrics, and other criteria; and (iii) environment specs (CUDA/driver, framework versions) and hardware details (GPU type, batch sizes). We report wall-clock latency and accuracy metrics for each setting, and provide logs to our tables/figures from raw traces."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: In Proceedings of the 2023 Training generalized multi-query transformer models from multi-head checkpoints. Conference on Empirical Methods in Natural Language Processing, pp. 48954901, 2023. Marianne Arriola, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models, 2025. URL https://arxiv.org/abs/2503.09573. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021a. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021b. Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Zixiang Chen, Huizhuo Yuan, Yongqian Li, Yiwen Kou, Junkai Zhang, and Quanquan Gu. Fast sampling via derandomization for discrete diffusion models. arXiv preprint arXiv:2312.09193, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 10 Bolin Gao and Lacra Pavel. On the properties of the softmax function with application in game theory and reinforcement learning. arXiv preprint arXiv:1704.00805, 2017. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and Richard Socher. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432, 2022. Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029, 2022. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022b. Emiel Hoogeboom, Alexey Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. arXiv preprint arXiv:2110.02037, 2021a. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in neural information processing systems, 34:1245412465, 2021b. Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models, 2023. URL https://arxiv.org/abs/2301.12661. Ganesh Jawahar, Benoˆıt Sagot, and Djame Seddah. What does bert learn about the structure of language? In ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, 2019. Ouail Kitouni, Niklas Nolte, James Hensman, and Bhaskar Mitra. Disk: diffusion model for structured knowledge. arXiv preprint arXiv:2312.05253, 2023. Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models. arXiv preprint arXiv:2403.00835, 2024. Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of bert. arXiv preprint arXiv:1908.08593, 2019. Bo Li, Peiyuan Zhang, Kaichen Zhang, Fanyi Pu, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Accelerating the development of large multimodal models, March 2024. URL https://github.com/EvolvingLMMs-Lab/lmms-eval. 11 Tianyi Li, Mingda Chen, Bowei Guo, and Zhiqiang Shen. survey on diffusion language models. arXiv preprint arXiv:2508.10875, 2025. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35:43284343, 2022. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025. Aaron Lou and Stefano Ermon. Reflected diffusion models. In International Conference on Machine Learning, pp. 2267522701. PMLR, 2023. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1576215772, 2024. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:3453234545, 2022. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025a. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models, 2025b. URL https://arxiv.org/abs/2502.09992. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of machine learning and systems, 5:606624, 2023. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. Vignav Ramesh and Morteza Mardani. Test-time scaling of diffusion models via noise trajectory search. arXiv preprint arXiv:2506.03164, 2025. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. primer in bertology: What we know about how bert works. Transactions of the association for computational linguistics, 8:842866, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/abs/2112.10752. Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. 12 Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. arXiv preprint arXiv:2211.16750, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. Diffusion language models are versatile protein learners. arXiv preprint arXiv:2402.18567, 2024a. Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. Dplm-2: multimodal diffusion protein language model. arXiv preprint arXiv:2410.13782, 2024b. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie yan Liu. survey on nonautoregressive generation for neural machine translation and beyond, 2023. URL https://arxiv.org/abs/ 2204.09269. Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 57(2):142, 2024. Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation, 2023. URL https://arxiv.org/abs/2207.09983. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b, 2025. URL https://hkunlp.github.io/blog/2025/dream. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large language model with parallel decoding, 2025. URL https://arxiv.org/abs/2505.16990. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024a. URL https://arxiv.org/abs/2407.12772. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024b. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. reparameterized discrete diffusion model for text generation. ArXiv, abs/2302.05737, 2023. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Llada 1.5: Variance-reduced preference optimization for large language diffusion models, 2025. URL https://arxiv.org/abs/2505.19223."
        },
        {
            "title": "CONTENTS",
            "content": "A Theoretical Validation for Elastic-Cache A.1 Notation and Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Background Lemmas and Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Layer-Wise KV Drift Monotonicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Attention Concentration and Drift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Implications for Elastic-Cache . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed Experiment Setup Batch Implementation of Elastic-Cache Use of Large Language Models Sample Response 15 15 16 19 22 22 23 23 14 THEORETICAL VALIDATION FOR ELASTIC-CACHE A.1 NOTATION AND SETUP , Vt,ℓ L: number of transformer layers, indexed by ℓ {1, . . . , L} : total denoising steps, indexed by {0, . . . , } : sequence length d: hidden dimension; dk, dv: key and value dimensions Ht,ℓ Rd: hidden state of token at step t, layer ℓ Kt,ℓ Rdk , Rdv : key and value of token St,ℓ RN : attention weights at step t, layer ℓ D<t: decoded token positions up to step 1 Mt: masked token positions at step Mt q,k : total attention token receives β: sliding window of size β over masked positions := (cid:80) Hi := Ht,ℓ t,ℓ := 1 max := maxi Hi2 : maximum hidden state change Γt,ℓ := αt,ℓ qMt β Ht1,ℓ (cid:80)N i=1 Ht,ℓ 2 : average hidden state drift : change in hidden state t,ℓ maxk=T t,ℓ αt,ℓ 0 : Attention Gap αt,ℓ St,ℓ A.2 BACKGROUND LEMMAS AND ASSUMPTIONS Lemma A.1 (Lipschitz Continuity of Softmax). Based on the Proposition 2 in Gao & Pavel (2017), the softmax function σ : Rn n1 defined by (9) (10) σ(z)i = exp(zi) j=1 exp(zj) (cid:80)n is 1-Lipschitz continuous with respect to the ℓ2 norm: σ(z) σ(z)2 z2 , z, Rn (cid:13) (cid:13) (cid:13)Ht,ℓ (cid:13) (cid:13) (cid:13)2 Assumption A.2 (Bounded Representations). At each layer ℓ and step t: Rℓ Assumption A.3 (Lipschitz Network Components). The projection matrices satisfy (cid:13) Wmax. The feedforward network at layer ℓ is LFFN-Lipschitz continuous. Assumption A.4 (Progressive Unmasking). At each step t, non-empty subset Dt Mt1 is unmasked: D<t increases and Mt = Mt1 Dt. Assumption A.5 (Layer-Wise Representation Dynamics). There exists ℓ {1, . . . , L} and functions fℓ(t) 0 as for ℓ ℓ such that: , (cid:13) (cid:13)Wℓ (cid:13)2 , (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13)Wℓ (cid:13)Wℓ (cid:13) (cid:13)2 Shallow layers (ℓ ℓ): The expected hidden state change for decoded tokens vanishes: For ℓ ℓ: (cid:13) (cid:13) (cid:13) D<t] fℓ(t) 0 Ht1,ℓ (cid:13) (cid:13)Ht,ℓ E[ (cid:13) Deep layers (ℓ > ℓ): The expected change remains bounded away from zero: lim inf tT (cid:13) (cid:13)Ht,ℓ E[ (cid:13) Ht1,ℓ (cid:13) (cid:13) (cid:13)2 D<t] cℓ > 0 This reflects that early layers encode local lexical patterns that stabilize quickly, while deep layers encode semantic relationships that continue evolving (Kovaleva et al., 2019; Jawahar et al., 2019; Rogers et al., 2021). Our experiments validate this (Figure 1b). 15 Assumption A.6 (Attention Concentration). The attention gap is non-negligible fraction of total attention mass: for some constant > 0 independent of N, t, ℓ. Definition A.7 (KV Drift). The KV drift at layer ℓ, step for token is: Γt,ℓ Mt β t,ℓ := (cid:13) (cid:13)Kt,ℓ (cid:13) Kt1,ℓ (cid:13) (cid:13) (cid:13)2 + (cid:13) (cid:13)Vt,ℓ (cid:13) Vt1,ℓ (11) (12) (cid:13) (cid:13) (cid:13)2 Average drift over decoded tokens: t,ℓ := D<t (cid:80) iD<t t,ℓ A.3 LAYER-WISE KV DRIFT MONOTONICITY This theorem formalizes the observation that KV drift increases with layer depth, providing theoretical justification for our layer-aware cache refresh strategy that selectively recomputes deeper layers while reusing shallow-layer caches. Figure 1a empirically validates this monotonicity property. Theorem A.8 (Layer-Wise KV Drift Monotonicity). Under Assumptions A.2A.5, there exists transition layer ℓ {1, . . . , L} such that for sufficiently large (when most tokens are decoded): Et[t,ℓ] Et[t,ℓ ], ℓ ℓ < ℓ Proof. Step 1: Relating KV Drift to Hidden State Drift. The key-value projections at layer ℓ are: Kt,ℓ Vt,ℓ = ℓ = ℓ KHt,ℓ Ht,ℓ By the triangle inequality and Assumption A.3 (W ℓ Kt,ℓ Kt1,ℓ K2, ℓ 2 Wmax): Ht1,ℓ K(Ht,ℓ 2 = ℓ K2Ht,ℓ ℓ WmaxHt,ℓ )2 Ht1,ℓ 2 i 2 Similarly for values: Therefore: Vt,ℓ Vt1,ℓ 2 WmaxHt,ℓ 2 t,ℓ = Kt,ℓ Kt1,ℓ 2 + Vt,ℓ Vt1,ℓ 2 2WmaxHt,ℓ Step 2: Layer Recursion for Hidden States. At layer ℓ, the transformer block computes: Ht,ℓ+1 where the attention output is: = Ht,ℓ + Attnℓ(Qt,ℓ , Kt,ℓ, Vt,ℓ) + FFNℓ(Ht,ℓ + Attnℓ()) Attnℓ(Qt,ℓ , Kt,ℓ, Vt,ℓ) = (cid:88) j=1 i,jVt,ℓ St,ℓ The change in hidden state at layer ℓ + 1 satisfies: Ht,ℓ+1 2 = Ht,ℓ+1 Ht,ℓ Ht1,ℓ+1 2 2 + Attnℓ(t) Attnℓ(t 1)2 + FFNℓ(inputt) FFNℓ(inputt1)2 16 (13) (14) (15) (16) (17) (18) (19) (20) (21) By Assumption A.3, the FFN is LFFN-Lipschitz: FFNℓ(inputt) FFNℓ(inputt1)2 LFFNinputt inputt12 The FFN input is Ht,ℓ + Attnℓ(), so: inputt inputt12 Ht,ℓ 2 + Attnℓ(t) Attnℓ(t 1)2 Therefore: Ht,ℓ+1 2 (1 + LFFN)Ht,ℓ 2 + (1 + LFFN)Attnℓ(t) Attnℓ(t 1)2 Step 3: Bounding Attention Output Change. Denote t,ℓ,i attn := Attnℓ(t) Attnℓ(t 1)2. We decompose: (cid:88) j=1 i,jVt,ℓ St,ℓ (cid:88) j=1 i,j Vt1,ℓ St1,ℓ = (cid:88) j= i,j(Vt,ℓ St,ℓ Vt1,ℓ ) + (cid:88) j= Taking norms and applying triangle inequality: (St,ℓ i,j St1,ℓ i,j )Vt1,ℓ t,ℓ,i attn (cid:88) j=1 St,ℓ i,jVt,ℓ Vt1,ℓ 2 + (cid:88) j=1 St,ℓ i,j St1,ℓ i,j Vt1,ℓ 2 Step 3a: First term (value changes). Since (cid:80) St,ℓ i,j = 1 (attention weights sum to 1): (cid:88) j=1 i,jVt,ℓ St,ℓ Vt1,ℓ (cid:88) 2 i,jWmaxHt,ℓ St,ℓ 2 (by Assumption A.3) j=1 = WmaxE Wmax t,ℓ jSt,ℓ i,: [Ht,ℓ 2] Step 3b: Second term (attention weight changes). By Cauchy-Schwarz: (cid:80) ajbj ((cid:80) aj) maxj bj By Assumption A.2: Vt1,ℓ 2 WmaxRℓ Therefore: (cid:88) j=1 St,ℓ i,j St1,ℓ i,j Vt1,ℓ 2 WmaxRℓ (cid:88) j=1 St,ℓ i,j St1,ℓ i,j By the inequality v1 nv2: (cid:88) j=1 St,ℓ i,j St1,ℓ i,j By Lemma A.1 (softmax is 1-Lipschitz in ℓ2): St,ℓ i,: St1,ℓ i,: St,ℓ i,: St1,ℓ i,: 2 zt,ℓ zt1,ℓ 2 where zt,ℓ = (zt,ℓ i,1, . . . , zt,ℓ i,N ) with zt,ℓ i,j = 1 dk Qt,ℓ Kt,ℓ . 17 (22) (23) (24) (25) (26) (27) (28) (29) (30) Step 3c: Bounding logit changes. For each component: i,j zt1,ℓ zt,ℓ i,j = = 1 dk 1 dk [Qt,ℓ [Qt,ℓ Kt,ℓ Qt1,ℓ Kt1,ℓ ] (Kt,ℓ Kt1,ℓ ) + (Qt,ℓ Qt1,ℓ ) Kt1,ℓ ] (31) By Cauchy-Schwarz and the bounds from Assumptions A.2A.3: [WmaxRℓ WmaxHt,ℓ 2 + WmaxHt,ℓ 2 WmaxRℓ] zt,ℓ i,j zt1,ℓ i,j = Taking ℓ2 norm of the logit vector: 1 dk 2 maxRℓ dk 2W 2 maxRℓ dk [Ht,ℓ 2 + Ht,ℓ 2] max Ht,ℓ 2 zt,ℓ zt1,ℓ 2 2 = (cid:88) j=1 zt,ℓ i,j zt1,ℓ i,j 2 Therefore: (cid:19)2 (cid:18) 2W 2 maxRℓ dk (max Ht,ℓ 2)2 2 2W maxRℓ dk max Ht,ℓ 2 zt,ℓ zt1,ℓ For typical sequences where maxk Ht,ℓ 2 = O( t,ℓ): zt,ℓ zt1,ℓ 2 2W 2 maxRℓ dk t,ℓ Step 3d: Combining. Combining the bounds from Steps 3a-3c: t,ℓ,i attn Wmax t,ℓ + WmaxRℓ t,ℓ 2W 2 maxRℓ dk = Wmax t,ℓ (cid:18) 1 + (cid:19) maxR2 2W 2 dk ℓ Define: Then: Cattn(ℓ) := maxR2 2W 2 dk ℓ = (cid:18) 2 ℓ maxR2 dk (cid:19) t,ℓ,i attn Wmax(1 + Cattn(ℓ)) t,ℓ Step 4: Recursive Bound on Hidden State Drift. Substituting equation 38 into equation 24: Ht,ℓ+1 2 (1 + LFFN)Ht,ℓ 2 + (1 + LFFN)Wmax(1 + Cattn(ℓ)) t,ℓ Taking averages over all tokens: t,ℓ+1 [(1 + LFFN) + (1 + LFFN)Wmax(1 + Cattn(ℓ))] t,ℓ (32) (33) (34) (35) (36) (37) (38) (39) (40) Define the layer-dependent amplification factor: λℓ := (1 + LFFN)[1 + Wmax(1 + Cattn(ℓ))] Then: t,ℓ+1 λℓ t,ℓ Step 5: Layer-wise Accumulation by Induction. By induction on ℓ: t,ℓ t,1 ℓ1 (cid:89) k= λk Since λℓ > 1, drift accumulates multiplicatively across layers. Step 6: Applying Layer-Wise Specialization. By Assumption A.5: Shallow layers (ℓ ℓ): t,ℓ fℓ(t) 0 as Deep layers (ℓ > ℓ): lim inf tT t,ℓ cℓ > 0 By equation 18: E[t,ℓ] = (cid:34) 1 D<t (cid:88) iD<t (cid:35) t,ℓ 2Wmax t,ℓ Therefore, for sufficiently large and any ℓ ℓ < ℓ: This establishes: E[t,ℓ] 2Wmaxfℓ(t) 0 E[t,ℓ ] 2Wmaxcℓ > E[t,ℓ] < E[t,ℓ ], ℓ ℓ < ℓ (41) (42) (43) (44) (45) (46) (47) A.4 ATTENTION CONCENTRATION AND DRIFT Theorem A.9 (Attention Concentration and Drift). Let t,ℓ = arg maxkD<t (cid:80) token at layer ℓ, step t. Under Assumptions A.2A.3, the most-attended token has drift bounded by: qMt β St,ℓ q,k be the most-attended t,ℓ t,ℓ t,ℓ + ϵt where t,ℓ = 1 D<t (cid:80) iD<t t,ℓ is the average drift and ϵt = (cid:16) Rℓ dk (cid:17) . Proof. Step 1: Bounding Attention Weight Changes. We derive how attention weights St,ℓ q,k change when hidden states change. Step 1a: Logit change. The attention logits are zq,k = 1 dk Qq Kk where: Qq = WQHq, Kk = WKHk The change in logits between steps and 1 is: zt,ℓ q,k zt1,ℓ q,k = 1 dk [Qt,ℓ 19 Kt,ℓ Qt1,ℓ (48) (49) (50) Kt1,ℓ ] Using the identity ab ab = a(b b) + (a a)b: = 1 dk [Qt,ℓ (Kt,ℓ Kt1,ℓ ) + (Qt,ℓ Qt1,ℓ ) Kt1,ℓ ] Step 1b: Apply Cauchy-Schwarz inequality. Taking absolute value and applying Cauchy-Schwarz: zt,ℓ q,k zt1,ℓ q,k [Qt,ℓ 2Kt,ℓ Kt1,ℓ 2 1 dk + Qt,ℓ Qt1,ℓ 2Kt1,ℓ 2] Step 1c: Bound projection norms. By Assumption A.2: Ht,ℓ 2 Rℓ for all i, t. By Assumption A.3: WQ2, WK2 Wmax. Therefore: Qt,ℓ Kt,ℓ Kt1,ℓ Qt1,ℓ 2 WQ2Ht,ℓ 2 WK2Ht,ℓ 2 WK2Ht,ℓ 2 WmaxHq2 2 WmaxRℓ 2 WmaxRℓ Ht1,ℓ Kt,ℓ Qt,ℓ 2 WmaxHk2 Substituting these bounds: zt,ℓ q,k zt1,ℓ q,k = 1 dk 2 maxRℓ dk [Hk2 + Hq2] [WmaxRℓ WmaxHk2 + WmaxHq2 WmaxRℓ] Step 1d: Use maximum drift. Since Hi2 max for all i: zt,ℓ q,k zt1,ℓ q,k 2W 2 maxRℓ dk max Step 1e: Compute ℓ2 norm of logit vector. The logit vector for query is zq = (zq,1, . . . , zq,N ) RN . By the previous bound applied to each component: zt,ℓ zt1,ℓ 2 2 = (cid:88) k= (cid:88) k=1 = zt,ℓ q,k zt1,ℓ q,k 2 (cid:19)2 2 max (cid:18) 2W 2 maxRℓ dk maxR2 4W 4 ℓ dk max Taking square root: zt,ℓ zt1,ℓ 2 max 2W 2 maxRℓ dk Step 1f: Apply softmax Lipschitz property. By Lemma A.1 (softmax is 1-Lipschitz in ℓ2 norm): St,ℓ q,: St1,ℓ q,: 2 zt,ℓ zt1,ℓ 2 2W maxRℓ dk max Step 1g: Convert to ℓ norm. Since v2 for any vector v: max St,ℓ q,k St1,ℓ q,k 2W 2 maxRℓ dk 20 max (51) (52) (53) (54) (55) (56) (57) (58) (59) (60) (61) (62) Step 2: Change in Total Attention Received. For token k, the change in total attention received is: αt,ℓ αt1,ℓ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:12) (cid:12) (cid:12) ) (cid:12) (cid:12) (cid:12) (cid:88) (St,ℓ q,k St1,ℓ q,k qMt β (cid:88) St,ℓ q,k St1,ℓ q,k (triangle inequality) qMt β Mt β max max St,ℓ q,k St1,ℓ q,k (bound by max) Using equation 62: αt,ℓ αt1,ℓ Mt β max 2W 2 maxRℓ dk Step 3: Relating to KV Drift. Recall that KV drift is t,ℓ = Kt,ℓ Kt1,ℓ 2 + Vt,ℓ Vt1,ℓ 2. By Assumption A.3: t,ℓ WmaxHi2 + WmaxHi2 = 2WmaxHi2 Therefore: Hi2 t,ℓ 2Wmax . In particular: max maxi t,ℓ Substituting into equation 64: 2Wmax . αt,ℓ αt1,ℓ Mt β 2W 2 maxRℓ dk maxi t,ℓ 2Wmax = Mt β WmaxRℓ dk max t,ℓ Step 4: Stability Constraint and Excess Drift. Suppose t,ℓ has drift t,ℓ t,ℓ = t,ℓ + ε where ε > 0 is excess drift beyond average. Then: αt,ℓ t,ℓ αt1,ℓ t,ℓ Mt β WmaxRℓ dk ( t,ℓ + ε) While tokens with average drift have: αt,ℓ αt1,ℓ Mt β WmaxRℓ dk t,ℓ The differential attention change is: differential = Mt β ε WmaxRℓ dk For t,ℓ to remain most-attended, the gap at step 1 must absorb this differential: Γt1,ℓ differential = Mt β WmaxRℓ dk ε Step 5: Assuming Bounded Attention Gap. Applying the assumption A.6: Mt β Mt β ε WmaxRℓ dk (63) (64) (65) (66) (67) (68) (69) (70) (71) Canceling Mt β (assuming Mt β > 0): Solving for ε: Therefore: WmaxRℓ dk ε ε dk WmaxRℓ = (cid:19) (cid:18) dk Rℓ t,ℓ t,ℓ t,ℓ + (cid:19) (cid:18) dk Rℓ (72) (73) (74) A.5 IMPLICATIONS FOR ELASTIC-CACHE These results provide theoretical justification for our design: Theorem A.8: Deeper layers have larger KV drift, justifying layer-aware refresh starting from ℓ Theorem A.9: Most-attended tokens have minimal drift, validating their use as cache staleness indicators"
        },
        {
            "title": "B DETAILED EXPERIMENT SETUP",
            "content": "Implementation Details. We conduct all the experiments on single NVIDIA A100 80GB GPU to ensure consistent hardware environment. We evaluate our proposed method, Elastic-Cache, on three large scale DLMs: LLaDA-Instruct (Nie et al., 2025a), LLaDA-1.5 (Zhu et al., 2025), and the multimodal LLaDA-V (You et al., 2025). Our evaluation spans both language and multimodal reasoning tasks including MBPP (Austin et al., 2021b), HumanEval (Chen et al., 2021) for coding tasks, MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021) for Maths related tasks and MathVista (Lu et al., 2023) MathVerse (Zhang et al., 2024b) for multimodal mathematical reasoning tasks. The major hyperparameters for Elastic-Cache, unless otherwise specified in ablation studies, are set to attention threshold of γ = 0.9, confidence threshold for parallel decoding of ϵ = 0.9, and cache block size of 32. To establish rigorous and fair comparison for all baseline methods, were re-evaluate all the methods including the original diffusion model LLaDA Nie et al. (2025a) and Fast-dLLM (Wu et al., 2025). This process eliminates confounding variables from hardware or software discrepancies and ensures that all observed performance differences are attributable to the methods themselves. Evaluation Framework and Metrics. Our evaluation protocol comprehensively assesses both inference efficiency and the preservation of model performance across variety of tasks. For standardization and reproducibility, we conduct all task-specific evaluations using the lm-eval-harness library (Gao et al., 2024). We measure inference speed by throughput in tokens per second (t/s), which we calculate as the average number of tokens the model generates over the entire sequence until it produces an end-of-sequence (<eos>) token. We keep our calculation methodology consistent with that of Fast-dLLM (Wu et al., 2025) to ensure comparable speed benchmarks. We measure task-specific performance using established metrics appropriate for each benchmark: for GSM8K (Cobbe et al., 2021), we report 5-shot flexible extract exact match accuracy; for the MATH dataset (Hendrycks et al., 2021), we report the 4-shot math verify score using the minerva math variant; for HumanEval (Chen et al., 2021), we evaluate 0shot accuracy using post-processing script consistent with the Fast-dLLM implementation to ensure fair comparison; and for MBPP (Austin et al., 2021b), we report the 3-shot pass@1 metric. For multimodal evaluation on LLaDAV (You et al., 2025), we utilize an evaluation suite adapted from its official implementation using the lmms-eval framework (Zhang et al., 2024a; Li et al., 2024) to test on the MathVista (Lu et al., 2023) and MathVerse (Zhang et al., 2024b) benchmarks. For MathVista, we report the gpt eval score, and for MathVerse, we report the gpt eval score on the mathverse testmini vision dominant subset. Hyper-parameters: The hyper-parameters used for Elastic-Cache are provided in Table 6. Specifically, For LLaDA and LLaDA-1.5, γ = 0.9 everywhere; β is mostly 16, except GSM8K (β = 32 at 256, 16 at 512) and HumanEval (β = 32 at both 256/512). For LLaDA-V (MathVista/MathVerse), γ = 0.7 and β = 16 for both 256 and 512 token lengths. All tasks are reported at generation lengths 256 and 512. 22 BATCH IMPLEMENTATION OF ELASTIC-CACHE In our experimental setup, we use batch size of one for comparison purposes. However, in real-world deployment scenario, requests can be grouped into batches to take advantage of parallelism and speed up decoding. In this section, we extend our implementation of Elastic-Cache to support batch decoding and compare it to baselines under different batch sizes. Batch implementation of Elastic-Cache is very challenging because the query length varies during caching and cache updates. Specifically, the query positions at time are represented as Qt, while caching is in progress, and when cache updates are being performed. Since Elastic-Cache automatically triggers cache updates, each sample within batch will have different length due to the varying triggers for each sample. This poses significant challenge to parallelism and efficiency of the method. To address this problem, we propose solution that involves rearranging the batch and concatenating all sentences within the batch into single sentence. This approach enables us to compute the sentences in parallel, regardless of their current lengths. We then reimplement the multi-head attention function to compute attention on the concatenated sentence (Algorithm 2). , Vi,t,l ; Batch size B; Algorithm 2 Batch attention computation Qi,t, Ki,t,l 1: Require: Batch samples Qi,t,l 2: Qt,l Cat([Q1,t,l Q1,t, . . . , QB,t,l QB,t]) 3: for = 1, . . . , do 4: Kt,l Cat([Kt,l, {Ki,t,l Vt,l Cat([Vt,l, {Vi,t,l 5: 6: end for 7: At,l = softmax 8: return At,l (cid:16) Qt,l(Kt,l) dk } Qi,t]); } Qi,t]); (cid:17) Vt,l"
        },
        {
            "title": "D USE OF LARGE LANGUAGE MODELS",
            "content": "An LLM was used to help solely polish the writing of the paper, while all method, idea and experiments were prepared and carried out entirely by the authors."
        },
        {
            "title": "E SAMPLE RESPONSE",
            "content": "In the following, we present several examples of actual generation results produced by our approach under different parameter settings. 23 Table 6: The hyper-parameters of Elastic-Cache under various settings. Model Benchmark Gen Length LLaDA LLaDA-1.5 GSM8K (5-shot) MATH (4-shot) Humaneval (0-shot) MBPP (3-shot) GSM8K (5-shot) MATH (4-shot) Humaneval (0-shot) MBPP (3-shot) LLaDA-V Mathvista Mathverse 256 512 256 512 256 512 256 512 256 256 512 256 512 256 512 256 512 256 512 β 32 16 16 16 32 32 16 16 16 16 16 32 32 16 16 16 16 16 16 γ 0.9 0. 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0. 0.9 0.9 0.7 0.7 0.7 0.7 Table 7: Example Qualitative comparison for different γ, LLaDA-1.5 Prompt: store sells 3 types of fruit: apples at $2 each, bananas at $1 each, and oranges at $3 each. If Sarah buys 4 apples, 6 bananas, and 2 oranges, how much does she spend in total? γ = 0.5 To determine how much Sarah spends in total, we need to calculate the cost of each type of fruit separately and then sum these amounts. 1. **Cost of apples:** - Sarah buys 4 apples. - Each apple costs $2. - Total cost for apples = (4 2 = 8) dollars. 2. **Cost of bananas:** - Sarah buys 6 bananas. - Each banana costs $1. - Total cost for bananas = (6 1 = 6) dollars. 3. **Cost of oranges:** - Sarah buys 2 oranges. - Each orange costs $3. - Total cost for oranges = (2 3 = 6) dollars. Now, we sum the costs of all the fruits to find the total amount Sarah spends: 8 (apples) 6 (oranges) = 20 dollars Therefore, Sarah spends total of 20 dollars. 6 (bananas) + + Time: 3.1s γ = 0.7 To determine how much Sarah spends in total, we need to calculate the cost of each type of fruit separately and then sum these amounts. First, lets calculate the cost of the apples: - Sarah buys 4 apples. - Each apple costs $2. - Therefore, the cost of apples is (4 2 = 8) dollars. Next, lets calculate the cost of the bananas: - Sarah buys 6 bananas. - Each banana costs $1. - Therefore, the cost of bananas is (6 1 = 6) dollars. Finally, lets calculate the cost of the oranges: - Sarah buys 2 oranges. - Each orange costs $3. - Therefore, the cost of oranges is (2 3 = 6) dollars. Now, we sum the costs of all the fruits to find the total amount Sarah spends: - Total cost = Cost of apples + Cost of bananas + Cost of oranges - Total cost = (8 + 6 + 6 = 20) dollars So, Sarah spends total of 20 dollars. Time: 3.4s γ = 0.95 To determine how much Sarah spends in total, we need to calculate the cost of each type of fruit separately and then sum these amounts. First, lets calculate the cost of the apples: - Sarah buys 4 apples. - Each apple costs $2. - Total cost for apples = (4 2 = 8) dollars. Next, lets calculate the cost of the bananas: - Sarah buys 6 bananas. - Each banana costs $1. - Total cost for bananas = (6 1 = 6) dollars. Then, lets calculate the cost of the oranges: - Sarah buys 2 oranges. - Each orange costs $3. - Total cost for oranges = (2 3 = 6) dollars. Now, we sum the costs of all the fruits to find the total amount spent: - Total cost = Cost of apples + Cost of bananas + Cost of oranges - Total cost = 8 + 6 + 6 - Total cost = 20 dollars Therefore, Sarah spends total of 20 dollars. Time: 3.5s 24 Table 8: Example Qualitative comparison for different γ, LLaDA-1.5 Prompt: Tom has 24 marbles. He gives 1/3 of them to his sister and 1/4 of the remaining marbles to his brother. How many marbles does Tom have left? 3 of his 24 marbles to his 1 3 24 = 8 So, Tom gives γ = 0.5 To determine how many marbles Tom has left, we will follow these steps: 1. Calculate the number of marbles Tom gives to his sister. 2. Subtract the marbles given to his sister from the original number of marbles. 3. Calculate the number of marbles Tom gives to his brother. 4. Subtract the marbles given to his brother from the remaining marbles. Lets start with the first step: **Step 1: Calculate the number of marbles Tom gives to his sister.** Tom gives 1 sister. marbles to his sister. **Step 2: Subtract the marbles given to his sister from the original number of marbles.** 24 8 = 16 So, after giving 8 marbles to his sister, Tom has 16 marbles left. **Step 3: Calculate the number of marbles Tom gives to his brother.** Tom gives 1 bles to his brother. 1 gives 4 marbles to his brother. **Step 4: Subtract the marbles given to his brother from the remaining marbles.** 16 4 = 12 So, after giving 4 marbles to his brother, Tom has 12 marbles left. Therefore, the number of marbles Tom has left is 12 . 4 of the remaining 16 mar4 16 = 4 So, Tom γ = 0.7 To determine how many marbles Tom has left, we will follow these steps: 1. Calculate the number of marbles Tom gives to his sister. 2. Subtract the marbles given to his sister from the original number of marbles. 3. Calculate the number of marbles Tom gives to his brother. 4. Subtract the marbles given to his brother from the remaining marbles. Lets start with the first step: **Step 1: Calculate the number of marbles Tom gives to his sister.** Tom gives 1 3 of his 24 marbles to his sister. Marbles given to sister = 1 3 24 = 8 **Step 2: Subtract the marbles given to his sister from the original number of marbles.** Remaining marbles after giving to sister = 24 8 = 16 **Step 3: Calculate the number of marbles Tom gives to his brother.** Tom gives 1 4 of the remaining 16 marbles to his brother. Marbles given to brother = 1 **Step 4: Subtract the marbles given to his brother from the remaining marbles.** Remaining marbles after giving to brother = 16 4 = 12 Therefore, the number of marbles Tom has left is 12 . 4 16 = 4 Time: 3.1s γ = 0.95 To determine how many marbles Tom has left, we will follow these steps: 1. Calculate the number of marbles Tom gives to his sister. 2. Subtract the marbles given to his sister from the total number of marbles. 3. Calculate the number of marbles Tom gives to his brother. 4. Subtract the marbles given to his brother from the remaining marbles. **Step 1: Calculate the number of marbles Tom gives to his sister.** Tom gives 1 3 of his 24 marbles to his sister. Marbles given to sister = 1 3 24 = 8 **Step 2: Subtract the marbles given to his sister from the total number of marbles.** Remaining marbles after giving to sister = 24 8 = 16 **Step 3: Calculate the number of marbles Tom gives to his brother.** Tom gives 1 4 of the remaining 16 marbles to his brother. Marbles given to brother = 1 **Step 4: Subtract the marbles given to his brother from the remaining marbles.** Remaining marbles after giving to brother = 16 - 4 = 12 Therefore, the number of marbles Tom has left is 12 . 4 16 = 4 Time: 3.2s Time: 3.3s 25 Table 9: Example Qualitative comparison of responses under different γ. Prompt: Points and are on parabola = 4x2 + 7x 1, and the origin is the midpoint of AB. Find the square of the length of AB. γ = 0.5 Let = (x1, y1) and = (x2, y2). Since the origin is the midpoint of AB, we have: γ = 0.7 Let = (x1, y1) and = (x2, y2). Since the origin is the midpoint of AB, we have: γ = 0.9 Let = (x1, y1) and = (x2, y2). Since the origin is the midpoint of AB, we have: (cid:16) x1 + x2 2 , y1 + y2 (cid:17) = (0, 0). (cid:16) x1 + x2 2 , y1 + y2 2 (cid:17) = (0, 0). (cid:16) x1 + x2 2 , y1 + y2 2 (cid:17) = (0, 0). This implies: This implies: This implies: x1 + x2 = 0 and y1 + y2 = 0. x1 + x2 = 0 and y1 + y2 = 0. x1 + x2 = 0 and y1 + y2 = 0. Since and lie on the parabola = 4x2 + 7x 1, we have: Since and lie on the parabola = 4x2 + 7x 1, we have: Since and lie on the parabola = 4x2 + 7x 1, we have: y1+y2 = (4x2 1+7x11)+(4x2 2+7x21) = 0. Now, using the equations x1 + x2 = 0 and y1 + y2 = 0, we get: and y1 = 4x2 1 + 7x1 1 y1 = 4x2 1 + 7x1 y2 = 4x2 2 + 7x2 1. y2 = 4x2 2 + 7x2 1. and From x1 + x2 = 0, we get x1 = x2. Substituting x1 = x2 into the equations for y1 and y2: Using y1 + y2 = 0, we get: y1 + y2 = (4x2 1 +7x1 1)+(4x2 2 +7x2 1) = 0. Simplifying, we have: 4x2 1 + 7x1 1 + 4x2 2 + 7x2 1 = 0. This simplifies to: 4x 1 + 7x1 + 4x2 2 + 7x2 2 = 0. Thus, the square of the length of AB is: (x1 x2)2 +(y1 y2)2 = 02 +02 = 0. and y1 = 4x 2 + 7x2 1 y2 = 4x2 2 + 7x2 1. Therefore, the square of the length of AB is: Since y1 + y2 = 0, we have: 0 . y1 = y2. Time: 8.1s Thus, y1 = (4x2 2+7x21) = 4x2 27x2+1. Similarly, y2 = 4x2 2 + 7x2 1. Therefore, y1 = y2 = (4x 2 + 7x2 1) = 4x2 2 7x2 + 1. Thus, the square of the length of AB is given by: 2 . Time: 9.3s 4x2 1 + 7x1 + 4x2 2 + 7x2 2 = 0. Since x1 + x2 = 0, we get: 4x2 1 + 7x1 + 4x 2 + 7x2 2 = 0. Now, we need to find the square of the length of AB. The length of AB is given by: AB = (cid:112)(x2 x1)2 + (y2 y1)2. Using the midpoint formula and the properties of the parabola, we can simplify the expression. However, since the problem asks for the square of the length, we can directly use the result. Thus, the square of the length of AB is: 50 . Time: 8.7s (Correct) Table 10: Coding Example Close Elements from typing import List Prompt: def has_close_elements(numbers: List[float], threshold: float) -> bool: \"\"\" Check if in given list of numbers, are any two numbers closer to each other than given threshold. >>> has_close_elements([1.0, 2.0, 3.0], 0.5) False >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3) True \"\"\" FAST-DLLM OURS sorted_numbers = sorted(numbers) for in range(len(sorted_numbers)): sorted_numbers = sorted(numbers) for in range(len(sorted_numbers) - if abs(sorted_numbers[i] - sorted_numbers[i + 1]) < threshold: return True return False 1): if abs(sorted_numbers[i] - sorted_numbers[i + 1]) < threshold: return True Fast-DLLM generated incorrect solution return False Table 11: Coding Example Truncate Number Prompt: def truncate_number(number: float) -> float: \"\"\" Given positive floating point number, it can be decomposed into and integer part (largest integer smaller than given number) and decimals (leftover part always smaller than 1). Return the decimal part of the number. >>> truncate_number(3.5) 0.5 \"\"\" FAST-DLLM if number < 0: OURS if number < 0: raise ValueError(\"The number must raise ValueError(\"The number must be positive.\") be positive\") integer_part = int(number) decimal_part = number - integer_part return decimal_part integer_part = int(number) decimal_part = number - integer_part return decimal_part Both solutions are functionally equivalent with minor differences"
        }
    ],
    "affiliations": [
        "FPT AI Residency Hanoi, Vietnam",
        "VILA Lab, MBZUAI Abu Dhabi, UAE"
    ]
}