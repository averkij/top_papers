{
    "paper_title": "Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning",
    "authors": [
        "Yu-Ang Lee",
        "Ching-Yun Ko",
        "Pin-Yu Chen",
        "Mi-Yen Yeh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Low-Rank Adaptation (LoRA) is the prevailing approach for efficient large language model (LLM) fine-tuning. Building on this paradigm, recent studies have proposed alternative initialization strategies and architectural modifications, reporting substantial improvements over vanilla LoRA. However, these gains are often demonstrated under fixed or narrowly tuned hyperparameter settings, despite the known sensitivity of neural networks to training configurations. In this work, we systematically re-evaluate four representative LoRA variants alongside vanilla LoRA through extensive hyperparameter searches. Across mathematical and code generation tasks on diverse model scales, we find that different LoRA methods favor distinct learning rate ranges. Crucially, once learning rates are properly tuned, all methods achieve similar peak performance (within 1-2%), with only subtle rank-dependent behaviors. These results suggest that vanilla LoRA remains a competitive baseline and that improvements reported under single training configuration may not reflect consistent methodological advantages. Finally, a second-order analysis attributes the differing optimal learning rate ranges to variations in the largest Hessian eigenvalue, aligning with classical learning theories."
        },
        {
            "title": "Start",
            "content": "Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Yu-Ang Lee 1 Ching-Yun Ko 2 Pin-Yu Chen 2 Mi-Yen Yeh 1 3 6 2 0 2 4 ] . [ 1 8 9 9 4 0 . 2 0 6 2 : r Abstract Low-Rank Adaptation (LoRA) is the prevailing approach for efficient large language model (LLM) fine-tuning. Building on this paradigm, recent studies have proposed alternative initialization strategies and architectural modifications, reporting substantial improvements over vanilla LoRA. However, these gains are often demonstrated under fixed or narrowly tuned hyperparameter settings, despite the known sensitivity of neural networks to training configurations. In this work, we systematically re-evaluate four representative LoRA variants alongside vanilla LoRA through extensive hyperparameter searches. Across mathematical and code generation tasks on diverse model scales, we find that different LoRA methods favor distinct learning rate ranges. Crucially, once learning rates are properly tuned, all methods achieve similar peak performance (within 12%), with only subtle rank-dependent behaviors. These results suggest that vanilla LoRA remains competitive baseline and that improvements reported under single training configuration may not reflect consistent methodological advantages. Finally, second-order analysis attributes the differing optimal learning rate ranges to variations in the largest Hessian eigenvalue, aligning with classical learning theories. 1. Introduction Despite the rapidly growing capabilities of pretrained large language models (LLMs), fine-tuning remains fundamental step for adapting these models to specialized applications in diverse domains such as medicine (Anisuzzaman et al., 2025) and finance (Djagba & Saley, 2025). However, modern LLMs typically contain billions of parameters, making full-parameter fine-tuning (Full FT) prohibitively 1Data Science Degree Program, National Taiwan University and Academia Sinica, Taipei, Taiwan 2IBM Research, New York, United States 3Institute of Information Science, Academia Sinica, Taipei, Taiwan. <r12946015@ntu.edu.tw, cyko@ibm.com, pinyu.chen@ibm.com, miyen@iis.sinica.edu.tw>. Preprint. 1 Figure 1. Performance of Qwen3-0.6B fine-tuned on mathematical reasoning tasks across learning rates. Different methods reach similar performance level once the learning rate is properly tuned. expensive in terms of memory and computation. These constraints have motivated sustained research interest in developing parameter-efficient fine-tuning (PEFT) methods, which allow task-specific learning while updating only small fraction of parameters. Even though PEFT methods span diverse design paradigms, ranging from prompt-based approaches (Li & Liang, 2021; Lester et al., 2021) to adapter-based methods (Houlsby et al., 2019; He et al., 2021), low-rank adaptation (LoRA), introduced by Hu et al. (2022), has emerged as the de facto standard. Inspired by the low intrinsic dimensionality observed in pretrained models (Li et al., 2018), Hu et al. (2022) hypothesize that task-specific parameter updates can be well approximated by low-dimensional structures. Consequently, they inject pairs of trainable decomposition matrices into selected layers while keeping the pretrained weights frozen. After training, these learned low-rank adapters can be merged into the original backbone, thereby incurring no additional inference latency. Even with its popularity, LoRA has been shown to underperform Full FT on challenging tasks in programming and mathematics (Biderman et al., 2024). This gap has in turn spurred recent efforts toward advanced LoRA variants (Zhu & Nguyen, 2025), with promising performance improvements reported. On LLaMA (Touvron et al., 2023), for example, Meng et al. (2024a) presented around 10% accuracy improvement on GSM8K (Cobbe et al., 2021) by modifying LoRA initialization strategies, while Liu et al. Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning (2024a) reported substantial gains of 37.2% on commonsense reasoning tasks by separately learning magnitude and directional updates of pretrained weight matrices. Yet, the results in majority of work along this line were obtained with hyperparameters directly inherited from prior studies, or only fine-tuned in narrow range. To be specific, in Figure 2, we collect 42 LoRA publications from major AI conferences and journals over the past three years, and include 10 high-impact or recently released preprints, to investigate whether their training involved tuning key hyperparametersnamely, learning rate, batch size, and rank. The statistics clearly reveal that hyperparameter search is not standard practice in the field, with only one paper simultaneously considering three hyperparameters and fewer than 30% tuning the learning rate, raising questions about the extent to which the reported gains can be attributed to genuine methodological improvements, particularly given the well-known sensitivity of neural networks to training configurations (LeCun et al., 2002; Bengio, 2012). This is especially critical when it comes to LoRA on LLMs, where careful learning rate tuning is demonstrated to be essential for eliciting strong performance, and optimal settings are contingent on both the base model and the target problem (Biderman et al., 2024; Schulman & Lab, 2025; Yan et al., 2025). To address above concern, we select four representative advanced LoRA variants and conduct large-scale hyperparameter search, benchmarking them against vanilla LoRA in head-to-head manner. Under unified evaluation protocol, we surprisingly find that once the learning rate is properly tuned, all methods peak at similar performance levels, exhibiting no systematic advantages over the vanilla LoRA. For example, in Figure 1, where learning rates are tuned across three orders of magnitude for Qwen (Yang et al., 2025) with rank 128, all methods achieve accuracies within narrow 0.43% range. Moreover, different methods operate under disparate learning rate ranges (e.g., 10 difference between PiSSA (Meng et al., 2024a) and LoRA in Figure 1), suggesting that success under single training configuration cannot be taken as evidence of robust and reliable improvements. This phenomenon is not isolated; we consistently observe such performance parity across diverse models, tasks, and LoRA ranks. Notably, within these marginal performance variations, rank-dependent behaviors emerge: some advanced variants may slightly outperform LoRA at higher ranks while lagging behind at lower ones (or vice versa), highlighting the importance of verifying improvements across the entire rank spectrum. By delving into the fundamentals in learning theories (LeCun et al., 1992; Lewkowycz et al., 2020), we provide an explanation for the importance of fine-tuning learning rates during LoRA finetuning and uncover the reasons beFigure 2. Frequency of advanced LoRA-based PEFT studies, categorized by whether learning rate or batch size tuning was applied and whether comparisons with vanilla LoRA across different ranks were conducted. Crucially, only one of the surveyed works simultaneously covered all three dimensions. Refer to Appendix Sec. for detailed data counts. hind different desirable learning rate ranges among various LoRA finetuning methods. Specifically, we demonstrate that PiSSA exhibits significantly larger maximum Hessian eigenvalue compared to vanilla LoRA, which theoretically justifies its requirement for lower learning rate. In summary, the main contributions of this work are: We demonstrate that while different methods require distinct optimal learning rate ranges, they yield comparable performance when configured to their optimal settings. For example, on Qwen3-0.6B, the top-performing method (LoRA) leads the runner-up (DoRA) by only 0.15%, and the least effective method (MiLoRA) by 0.43% (cf. Figure 1). We identify intriguing rank-dependent behaviors. For example, MiLoRA demonstrates some improvement over vanilla LoRA when the rank is 8 on the mathematical reasoning task with Gemma, yet slightly underperforms LoRA when the rank is 256. By analyzing the eigenvalues of the Hessian across LoRA variants, we find that the optimal learning rate is generally negatively correlated with the maximum eigenvalue, aligning with classical learning theories. 2. Related Work 2.1. Systematic Empirical Re-evaluation of Prior Claims Incomplete performance evaluation remains persistent concern (Sculley et al., 2018; Lipton & Steinhardt, 2019). For instance, Melis et al. (2017) revealed that two published improvements to the vanilla LSTM (Graves, 2012), originally attributed to complex network designs, were in fact due to more careful hyperparameter tuning. Under fair tuning protocol, the standard LSTM emerged as the best-performing 2 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Figure 3. Overview of our considered PEFT methods: (a) Vanilla LoRA (Sec. 3.1), (b) three representative initialization variants (Sec. 3.2), and (c) one architectural modification (Sec. 3.3). architecture. In the same vein, Lin et al. (2023) pointed out that simple baselines such as linear SVM (Boser et al., 1992) are competitive with BERT (Devlin et al., 2019)-based methods for text classification, sometimes even outperforming them with clear gap. Such systematic empirical re-evaluation has also been done across diverse machine learning subfields. Examples include not only traditional topics such as image classification (Chatfield et al., 2014), graph neural networks (Shchur et al., 2018), generative adversarial networks (Lucic et al., 2018), recommender systems (Ferrari Dacrema et al., 2019), metric learning (Musgrave et al., 2020), and neural network pruning (Blalock et al., 2020), but also more recent areas like optimizers (Schmidt et al., 2021), reinforcement learning (Eimer et al., 2023), preference optimization (Ahrabian et al., 2025), and model merging (Hitit et al., 2025). While empirical studies benchmarking LoRA with other PEFT methods such as prefix tuning (Li & Liang, 2021) and BitFit (Zaken et al., 2022) exist (He et al., 2021; Hu et al., 2023; Zheng et al., 2024b; Männistö et al., 2025; Belanec et al., 2025), few studies specifically focus on comparing LoRA and its advanced variants. More concerning, training hyperparameters in many of these works were kept fixed without method-specific optimization. Therefore, practitioners are left without clear and reliable guidance when choosing LoRA-based methods. 2.2. LoRA Hyperparameter Tuning Theories regarding LoRAs lack of Lipschitz smoothness (Sun et al., 2024; Malinovsky et al., 2024) and its spurious loss landscape (Liu et al., 2025) point toward its intrinsic sensitivity to hyperparameter variations. Consequently, many research efforts have been invested in finding optimal training setups, such as learning rate (Hayou et al., 2024b), rank (Zhang et al., 2025a), initializations (Hayou et al., 2024a), scaling factor (Kalajdzievski, 2023), dropout (Lin et al., 2024), and adapter placements (Fomenko et al., 2024; Hayou et al., 2025). Despite these insights into individual hyperparameters, establishing unified configuration guidelines remains an ongoing pursuit. For example, recent works have sought to derive practical rules of thumb through extensive, joint evaluations across multiple hyperparameter dimensions (Biderman et al., 2024; Schulman & Lab, 2025). Addressing the computational bottleneck of such extensive searches, Yan et al. (2025) took system-level approach, optimizing hardware resources to maximize training throughput specifically for LoRA hyperparameter tuning. Despite these efforts to optimize LoRA hyperparameters, few studies have examined whether LoRA and its variants require distinct hyperparameter settings. Most relevant to our work is Zhang et al. (2025h), which provided helpful theoretical framework revealing the interplay between LoRA initialization strategies and hyperparameters such as learning rate and scaling factor. Specifically, Zhang et al. (2025h) noted an intriguing phenomenon where LoRA and two of its initialization variants, PiSSA (Meng et al., 2024a) and MiLoRA (Wang et al., 2024b), exhibit performance shifts across learning rates. However, this analysis was limited to two learning rates (2e4 and 2e5) and the observed trends were further shown to depend on the rank configuration (cf. Zhang et al. (2025h, Appendix G, Table 5-6)). In contrast, our work expands this investigation to broader set of recent LoRA variants and conducts comprehensive multivariate hyperparameter tuning to identify the best-performing configuration for each method. Moreover, we also provide intuitions of the underlying factors driving the observed performance differences and trends. 3. Background: LoRA PEFT Methods In this section, we introduce background and notations for five selected PEFT methods, including LoRA and its representative improvements, i.e., PiSSA (Meng et al., 2024a), 3 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning MiLoRA (Wang et al., 2024b), Init[AB] (Li et al., 2025), and DoRA (Liu et al., 2024a) (Figure 3). 3.1. Low-Rank Adaptation Given pretrained neural network layer parameterized by Wpre Rmn, LoRA introduced two trainable matrices: the down-projecting Rrn and the up-projecting Rmr (r min(m, n)). For layer input Rn, the output Rm is computed as: = Wprex + γrBAx, where γr = α serves as rank-dependent scaling factor with α being tunable hyperparameter. At initialization, the two trainable matrices and are set to B0 = 0 and A0 (0, σ2) (i.e., Kaiming initialization (He et al., 2015)), respectively, ensuring that fine-tuning starts exactly from the pretrained checkpoint. 3.2. Initialization Variants line of work has explored better initialization strategies for LoRA. For example, PiSSA and MiLoRA leverage the singular value decomposition (SVD) of Wpre to inform the initialization of and A, while Init[AB] theoretically examines different zero initialization schemes. PiSSA. Aiming to address the potential slow convergence of LoRA, Meng et al. (2024a) proposed to initialize BA with the top-r principal components of the pretrained weight matrix and showed that this approach achieves faster convergence with loss and gradient norm curves similar to those of full fine-tuning. Suppose Wpre admits SVD into (cid:80) σiuivT where σi are singular values in descending order, the LoRA adapter is initialized as: B0A0 = (cid:88) i=1 σiuivT , with B0 = (cid:88) i=1 σiuieT , A0 = (cid:88) i= σieivT , (1) where ei Rr denotes the i-th standard basis vector. To start fine-tuning from the pretrained weights, the residual matrix is defined as Wres = Wpre B0A0 = min(m,n) (cid:88) i=r+ σiuivT , and the forward pass becomes: = Wresx + γrBAx. MiLoRA. Concurrent to PiSSA, Wang et al. (2024b) proposed an analogous approach targeting adaptation to new tasks while maximally retaining the pretrained models knowledge. Instead of using principal components, MiLoRA initializes the low-rank adapters using bottom-r minor components: B0A0 = min(m,n) (cid:88) σiuivT , i=min(m,n)r+1 with B0 and A0 defined analogously to Eq. 1. The residual matrix retains principal components: Wres = Wpre B0A0 = min(m,n)r (cid:88) i= σiuivT . Wang et al. (2024b) showed experimentally that MiLoRA achieves superior downstream performance with less catastrophic forgetting. Init[AB]. Several works have theoretically analyzed the initialization strategies of LoRA (Hayou et al., 2024a; Xu et al., 2025). In particular, Hayou et al. (2024a) confirmed that Init[A] (i.e., randomly initializing only, the default LoRA setting) generally leads to better performance than Init[B] (randomly initializing only). Li et al. (2025) further showed that initializing both matrices (i.e., Init[AB]) can provide even better advantage by balancing stability, training efficiency, and hyperparameter robustness. Specifically, Init[AB] initializes both matrices as B0 (0, σ2) and A0 (0, σ2). Since B0A0 = 0 in this case, the residual matrix Wres is similarly introduced and utilized. 3.3. Architectural Modifications Besides investigating initialization strategies, an even larger body of literature has focused on architecture-level improvements, e.g., VeRA (Kopiczko et al., 2023b), BOFT (Liu et al., 2023b), and GraLoRA (Jung et al., 2025) available in the PEFT library (Mangrulkar et al., 2022). DoRA. Liu et al. (2024a) proposed to learn magnitude and directional updates of Wpre separately. Formally, the modified forward pass becomes: = γr (cid:18) Wpre + BAc (Wpre + BA) x, (2) (cid:19) where and are responsible for directional updates and initialized as vanilla LoRA, while R1n is an additional trainable magnitude vector initialized with m0 = Wprec. Note that .c denotes taking the columnwise norm of matrix, while denotes element-wise multiplication with broadcasting across columns. With only 4 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning marginal increase in trainable parameters introduced by the vector m, DoRA has been shown to consistently outperform LoRA, especially in regimes where the rank is small. We select DoRA among architectural variants as it maintains parameter efficiency similar to vanilla LoRA, unlike the significant reduction in VeRA (from (m + n)r to + per layer), and does not require setting additional architectural hyperparameters (e.g., (m, b) in BOFT or in GraLoRA). 4. Learning Rate Matters, Really 4.1. Motivation For the trainable LoRA parameters across layers, collectively denoted as θ, the update rule of Stochastic Gradient Descent (SGD) at step is: θt+1 = θt ηg(θt), where η is the learning rate. While setting η too large causes the optimization step to overshoot, leading to instability or divergence, value that is too small is insufficient to escape suboptimal local minima or affect convergence rate. To analyze this formally, consider the local geometry characterized by the gradient g(θt) L(θt) and the Hessian H(θt) 2L(θt), where denotes the loss function. According to classical learning theories (LeCun et al., 1992), the optimal learning rate η for efficient learning is intrinsically tied to the curvature of the loss landscape at θ, typically scaling inversely with the Hessians maximum eigenvalue: η 1 λmax(H(θ)) . (3) Crucially, LoRA initialization variants establish specific training starting points θ0, resulting in distinct g(θ0), H(θ0), and subsequent training trajectories compared to vanilla LoRA. Therefore, different methods theoretically require their respective calibrations of η to ensure efficient convergence, motivating our decision to perform learning rate tuning for fair and reliable head-to-head comparison across methods. 4.2. Experimental Setup Since model choices, training configurations, and dataset partitioning vary across papers, we establish unified experimental framework that accommodates all methods fairly. We describe the key components below, with additional implementation details deferred to Appendix B. Pretrained Models. We consider three decoder-only models spanning different scales: Qwen3-0.6B (Yang et al., 2025), Gemma-3-1B (Team et al., 2025), and Llama-27B (Touvron et al., 2023). This selection includes both recently released models (Qwen3, Gemma-3) and an older one (Llama-2) that is widely used in prior art on this subject, enabling us to validate results across LLMs with diverse pretrained capabilities. Fine-tuning Tasks. We train models on two canonical tasks: mathematical reasoning and code generation. For these tasks, we follow the setup of Meng et al. (2024a). In particular, for mathematical reasoning, we use MetaMathQA (Yu et al., 2023) during the training, and GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) for testing; for code generation, we use CodeFeedback (Zheng et al., 2024a) during the training, and HumanEval (Chen, 2021) and MBPP (Austin et al., 2021) for testing. Unless otherwise specified, we report mean accuracy (equivalently, pass@1 for code generation) on the two testing datasets. We also adopt the same data preprocessing: MetaMathQA is subsampled from 395k to 100k samples, and CodeFeedback is filtered to Python-only questions (from 156k to 104k samples). Hyperparameter Settings. We consider batch sizes (B) in {16, 64, 128} and ranks (r) in {4, 8, 16, 32, 64, 128, 256}. The learning rates (η) are swept uniformly on logarithmic scale from 103 to 106, with four values per order of magnitude: 1.124710, 2.000010, 3.556610, 6.324610, yielding up to 16 grid points for the learning rate alone. To maintain the computational feasibility of this study, batch size and rank are tuned only for selected model-task combinations; conversely, learning rates are tuned across all combinations, with ranges defined to ensure inclusion of optimal performance. See Appendix Table 3 for summary of models, tasks, and their corresponding hyperparameter search ranges. Other configurations, such as epoch, adapter placement, and learning rate scheduler, remain fixed across all experiments and are listed in Appendix Table 4. Specifically for the scaling factor γr, we follow Meng et al. (2024a) by setting α equal to in all our experiments. This results in γr = 1 for all r, effectively factoring out the need to tune this hyperparameter (refer to Appendix Sec. for further discussion). 4.3. Results and Observations We begin by discussing results under fixed rank = 128 in Sec. 4.3.1, and then in Sec. 4.3.2, we analyze how the methods perform across different ranks. 4.3.1. SIMILAR PERFORMANCE LEVELS Analogous to the hyperparameter search results shown in Figure 1 for Qwen3-0.6B, Table 1 and Figure 4 present the results for Gemma-3-1B and Llama-2-7B. Through comprehensive hyperparameter searches, we consistently observe across different model scales and tasks that these methods peak at performance levels similar to vanilla LoRA. Specif5 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Methods Batch Size 1.1e2e-5 3.6e-5 6.3e-5 1.1e-4 2e-4 3.6e6.3e-4 1.1e-3 2e-3 3.6e-3 6.3e-3 Learning Rate LoRA DoRA 16 64 9.780.36 11.160.28 13.580.18 15.480.15 18.430.14 20.000. 19.930.65 17.990.55 11.710.49 1.520.19 1.270.59 1.070.27 6.880. 9.120.39 10.790.37 13.230.25 15.650.57 17.540.29 19.730.16 20.460.79 19.830.91 13.330.81 1.480.48 0.000. 128 5.700.34 6.950.23 9.410.44 11.430.40 13.680.77 15.920.45 18.580. 19.600.09 20.320.28 16.952.70 0.090.16 0.000.00 16 64 9.890.24 11.160.51 13.840.41 15.610.11 18.210.45 20.110. 20.960.57 18.340.20 11.900.29 4.890.99 0.930.12 1.160.15 6.720. 9.190.19 10.530.20 13.450.31 15.720.32 17.660.20 19.960.05 20.820.32 19.870.91 13.531.64 1.520.45 0.340.23 5.550.11 7.210.18 9.720.17 11.580.25 13.980.33 16.190.46 18.250.23 19.670.71 20.330. 12.8610.03 0.130.23 0.020.03 Init[AB] 16 64 9.730.35 12.100.14 14.410.49 16.730.37 18.380.53 20.390.38 20.550. 18.340.48 11.940.31 1.480.24 1.160.31 1.450.17 6.510.22 9.150. 11.280.20 13.200.24 15.880.39 17.890.30 20.080.26 20.980.33 19.310.75 13.970.03 2.743.83 0.070.12 128 6.060. 7.050.33 9.530.22 11.810.08 13.980.79 16.460.39 18.360.21 20.370.39 20.660.39 17.850.84 4.407.46 0.000. MiLoRA 16 64 12.440.07 13.770.25 16.280.24 18.450.47 20.040.19 20.630.67 19.400.80 15.720. 10.220.42 2.030.95 1.350.43 1.560.65 8.820.40 11.250.20 13.160.11 15.540.29 17.430.24 19.560.33 20.030. 19.600.78 17.930.90 13.650.07 4.970.40 0.000.00 128 7.320.33 9.570. 11.760.33 13.540.12 16.020.16 18.390.26 19.700.34 19.990.66 19.530.47 16.830.73 7.451.00 0.570.81 PiSSA 16 64 14.300.18 16.100.27 18.310.12 19.900.21 20.610.28 19.090.20 16.100.64 13.250.55 8.410. 4.670.29 2.501.27 0.960.15 11.110.05 13.670.17 15.560.33 18.110.23 19.520.48 20.680.77 20.590.32 19.110.86 15.530. 9.570.72 5.780.37 0.330.46 128 9.420.38 11.800.28 14.400.11 16.230.38 18.600.21 19.610.44 20.650. 19.211.15 16.910.19 13.870.97 6.280.49 1.190.36 Table 1. Performance of Gemma-3-1B on mathematical reasoning task across varying batch sizes and learning rates (r = 128). Results are reported as mean standard deviations over three independent runs. Best results are highlighted in bold, and configurations achieving 18.5% accuracy (i.e., 90% of the maximum) are shaded in green ( ). While all methods achieve comparable peak accuracies, the optimal learning rates vary depending on both the fine-tuning method and batch size. ically, the performance gap across all methods is small: 0.52% for Gemma on math, and 0.43% and 1.75% for Llama on math and code, respectively. It is also important to note that while peak performance is similar, the optimal learning rates vary. In particular, PiSSA requires lower learning rate compared to vanilla LoRA across all model-task combinations, while other methods fall within similar range to LoRA, typically within the same order of magnitude. Beyond the optimal learning rates, closer inspection of the full learning rate spectrum reveals intriguing methodspecific behaviors. For instance, we observe that PiSSA tends to remain effective at larger learning rates where other methods diverge: in Figure 4 at η =1.1103, PiSSA maintains accuracies of 27.83% and 26.90% on math and code tasks, respectively, while other methods collapse to nearzero performance on at least one of the tasks. Note that in Table 1, the joint optimization of learning rate and batch size indicates that tuning the learning rate is significantly more critical than tuning the batch size for obtaining best performance in both LoRA and its variants, consistent with early findings for neural networks (Bengio, 2012). For example, with DoRA, fixing the learning rate at 2105 and tuning only the batch size yields suboptimal maximum accuracy of 11.16%. In contrast, by fixing the batch size to any value in {16, 64, 128} and tuning the learning rate, the model achieves substantially higher performance within the range of 20.521.0%. Moreover, we observe that the optimal learning rate scales proportionally with batch size, aligning with the scaling rule established in SGD literature (Goyal et al., 2017; Hoffer et al., 2017). This offers plausible explanation for the discrepancy between our results and those of Schulman & Lab (2025), where LoRA was reported to suffer from performance degradation with larger batch size of 128; we hypothesize that this degradation likely stems from suboptimal learning rate choice. While recent work has highlighted more nuanced batch size effects (e.g. Marek et al. (2025) demonstrated that smaller batch sizes can be beneficial if the learning rate and optimizer hyperparameters are jointly tuned), we leave the extended study for future work. We further validate our findings across varying number of training samples in Appendix D.1. More numerical results and evaluations of additional model-task combinations, and example model responses are provided in Appendix and F, respectively. 4.3.2. PERFORMANCE COMPARISON ACROSS RANKS Next, we extend our analysis by varying adapter ranks for Gemma-3-1B, as shown in Figure 5. The results indicate that the previously observed performance parity persists across wide range of rank settings, with the maximum performance differences among methods being only 1.67% (Math, = 32) and 2.15% (Code, = 4). Interestingly, however, we observe the relative performance of variants compared to LoRA fluctuates across different ranks within these margins. In particular, PiSSA initially underperforms vanilla LoRA before gradually overtaking it as the rank increases. Taking the math task as an example (Figure 5a), PiSSA exhibits performance deficits of up to 1.67% at low ranks (r 32), 6 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning (a) Mathematical Reasoning (b) Code Generation Figure 4. Performance of Llama-2-7B on mathematical reasoning and code generation tasks across varying learning rates (r = 128, = 128). Notably, PiSSA peaks at lower learning rates but remains effective at larger learning rates on both tasks (e.g., 1.1103), where other methods diverge. (a) Mathematical Reasoning (b) Code Generation Figure 5. Best achievable performance of LoRA and its advanced variants across adapter ranks on Gemma-3-1B (B = 64). With properly tuned learning rates, all methods exhibit similar performance improvement trends as the rank increases, though subtle rank-dependent behaviors emerge. Results are reported with means and standard deviations over three independent runs. but narrows the gap to within 0.11% at = 64 and shifts to slight gain of 0.22% and 0.33% at = 128 and 256, respectively. In contrast, MiLoRA shows an opposite trend, where it tends to outperform vanilla LoRA at lower ranks but fails to sustain this advantage as the rank increases: it achieves 0.8% gain at = 8 and aligns with LoRA at = 64, before eventually falling behind by 0.43% and 0.63% at = 128 and 256, respectively. Figure 5b indicates that this rank-dependent dynamics extend to the coding task. In the case of Init[AB], we observe that it tends to outperform LoRA at medium ranks, e.g., achieving maximal gains of 0.52% on math and 1.26% on code at = 128. Yet, the success does not translate to either lower or higher rank scenarios, where Init[AB] typically performs similarly to vanilla LoRA. Turning to DoRA, we observe performance gains against LoRA specifically in low-rank regimes, peaking at 1.1% on math and 0.95% on code at = 8. While this aligns with the claim that DoRA excels especially in low-rank scenarios, we highlight that the improvements are smaller than those reported in the original work (cf. Liu et al. (2024a, Figure 5)), underscoring the critical importance of learning rate tuning. Furthermore, DoRAs success does not translate uniformly to other low-rank settings (e.g., math at = 4), and it is also worth noting that the parameter overhead introduced by the magnitude vector (m in Eq.2) becomes non-negligible relative to the total trainable parameter counts when the rank is small. Similar performance comparisons across ranks were also conducted on Llama, with results deferred to Appendix Sec. D.2. 5. Understanding the Optimal Learning Rate via Hessian Analysis 5.1. Sharpness-Learning Rate Relationship The Hessian of the loss function has been the subject of numerous studies. Geometrically, its top eigenvalue (denoted as λmax for brevity) at given point represents the maximal curvature of the loss landscape along any direction, com7 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Figure 6. Distributions of the ratios of the top loss Hessian eigenvalues relative to LoRA for Query projection matrices across Transformer layers. Dashed lines indicate the medians. With an adapter rank of = 128, PiSSA consistently exhibits the largest eigenvalues. In contrast, MiLoRA and Init[AB] exhibit eigenvalues slightly larger than those of LoRA on Qwen, yet remain comparable to LoRA on Gemma and Llama. Detailed λmax values and results on other matrix types are provided in Appendix Sec. G.2. monly referred to as sharpness (Dinh et al., 2017; Lyu et al., 2022; Luo et al., 2024). This metric is closely linked to the optimal learning rate, connection that originates from the Gauss-Newton method for convex optimization and further elucidated by LeCun et al. (1992) in the context of neural networks. Specifically, it was shown that an efficient learning rate theoretically falls within 1/λmax η < 2/λmax under quadratic approximation, whereas rates exceeding 2/λmax lead to divergence. More recently, Lewkowycz et al. (2020) identified catapult learning regime characterized by 2/λmax η 12/λmax, in which modern architectures achieve optimal performance. More research has further explored the intricacies of the interplay between λmax and η with consensus that these two quantities exhibit an inversely proportional relationship (Pan et al., 2021; Cohen et al., 2021; Kalra & Barkeshli, 2024). 5.2. Sharpness Analysis in LoRA For our LoRA fine-tuning problem, we leverage the downstream dataset to compute the Hessian matrix of the loss function and focus exclusively on the trainable LoRA parameters (Yang et al., 2023; Zhao et al., 2024b; Yu et al., 2025b). Instead of concatenating LoRA parameters across all layers, we follow standard practices in LLMs to estimate λmax in block-wise manner (Zhang et al., 2024b; Wang et al., 2025a; Ilin, 2025) at the initialization point. Formally, max = λmax(Hl), we calculate the layer-wise metric as λl where Hl represents the Hessian corresponding to parameters θl = {Bl 0}, with indexing through matrix types and Transformer layers. The Lanczos algorithm (Lanczos, 1950) and Hessian-vector Products are utilized to estimate the top eigenvalue without explicitly forming H. Implementation details are provided in Appendix G.1. While DoRA shares LoRAs initialization, implying identical initial Hessians, its unique architectural design may lead to distinct Hessian evolution throughout training, rendering the analysis non-trivial; we thus defer the investigation of DoRA to future work. 0, Al max,t/λQ,i Specifically, let the Hessian for the Query projection matrix in the i-th layer be HQ,i for =PiSSA, MiLoRA, Init[AB], and LoRA, we further denote their corresponding maximum eigenvalues by λQ,i max,t. Then, we normalize the maximum eigenvalues from PiSSA, MiLoRA, and Init[AB] by that from LoRA, and plot the distribution across layers in Figure 6, i.e. λQ,i max,LoRA for =PiSSA, MiLoRA, Init[AB] and = 1, . . . , L. The results reveal that PiSSA initializes trainable parameters in state of significantly higher curvature compared to other methods. This observation corroborates our prior learning rate tuning experiments, explaining why PiSSA requires much lower learning rate. For other initialization variants, however, the eigenvalue magnitudes become more similar to those of vanilla LoRA. Notably on Qwen, Init[AB] and MiLoRA exhibit slightly larger λmax ( 2) compared to LoRA, supporting their lower optimal learning rates by factor of 1.8 and 3.2 in Figure 1, respectively. 6. Conclusion Motivated by the increasing number of LoRA variants and the insufficient hyperparameter tuning in many studies, in this work, we conducted systematic re-evaluation of five LoRA PEFT methods under unified evaluation protocol. Based on the comprehensive hyperparameter experiments, we conclude that improper learning rate gives false sense of LoRA advancements. In our studies, we also pointed out the scenarios where one might be in favor of specific variant that, albeit likely to produce generally comparable performance, marginally outperforms other variants. It is worth noting that these improvements often lack universality, with vanilla LoRA frequently matching or even outperforming them. By elucidating the disparate optimal learning rate ranges through Hessian analysis, we hope our study encourages future PEFT research to adopt more comprehensive hyperparameter search protocol, ensuring reliable advancements in the field. 8 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning"
        },
        {
            "title": "Impact Statement",
            "content": "The objective of this paper is to advance Machine Learning research. Although there may be potential societal consequences of our work, we do not identify any specific ones that must be specifically highlighted here. We acknowledge that this paper is subject to several limitations, primarily due to computational constraints. Specifically, we focused our investigation on decoder-only LLMs up to the 7B parameter scale, restricting our evaluation to mathematical and code generation tasks. Consequently, the scalability of our findings to larger foundation models and their generalization to diverse downstream tasks remain to be verified. We also caution that our results may not extend to untested model architectures or other advanced LoRA variants. We refer the readers to Appendix Sec. for more details."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported in part by National Science and Technology Council, Taiwan, R.O.C., under grant 113-2628E-001 -003 -MY4, and by National Taiwan University and Academia Sinica Innovative Joint Program, under grant ASNTU-114-06. Y.-A. Lee would like to thank the academic training and support received from the Data Science Degree Program at NTU and Academia Sinica, as well as the NTU Overseas Internship Program and IBM Research."
        },
        {
            "title": "References",
            "content": "Ahrabian, K., Lin, X., Patra, B., Chaudhary, V., Benhaim, A., Pujara, J., and Song, X. practical analysis of human alignment with* po. In Findings of the Association for Computational Linguistics: NAACL 2025, pp. 80138021, 2025. Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: visual language model for fewshot learning. Advances in neural information processing systems, 35:2371623736, 2022. Albert, P., Zhang, F. Z., Saratchandran, H., RodriguezOpazo, C., Hengel, A. v. d., and Abbasnejad, E. Randlora: Full-rank parameter-efficient fine-tuning of large models. arXiv preprint arXiv:2502.00987, 2025. Anisuzzaman, D., Malins, J. G., Friedman, P. A., and Attia, Z. I. Fine-tuning large language models for specialized use cases. Mayo Clinic Proceedings: Digital Health, 3 (1):100184, 2025. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Azizi, S., Kundu, S., and Pedram, M. Lamda: Large model fine-tuning via spectrally decomposed low-dimensional adaptation. arXiv preprint arXiv:2406.12832, 2024. Bałazy, K., Banaei, M., Aberer, K., and Tabor, J. Lora-xs: Low-rank adaptation with extremely small number of parameters. arXiv preprint arXiv:2405.17604, 2024. Belanec, R., Pecher, B., Srba, I., and Bielikova, M. Peftbench: parameter-efficient fine-tuning methods benchmark. arXiv preprint arXiv:2511.21285, 2025. Bengio, Y. Practical recommendations for gradient-based training of deep architectures. In Neural networks: Tricks of the trade: Second edition, pp. 437478. Springer, 2012. Biderman, D., Portes, J., Ortiz, J. J. G., Paul, M., Greengard, P., Jennings, C., King, D., Havens, S., Chiley, V., Frankle, J., et al. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024. Bini, M., Girrbach, L., and Akata, Z. Decoupling angles and strength in low-rank adaptation. In The Thirteenth International Conference on Learning Representations, 2025. Blalock, D., Gonzalez Ortiz, J. J., Frankle, J., and Guttag, J. What is the state of neural network pruning? Proceedings of machine learning and systems, 2:129146, 2020. Boser, B. E., Guyon, I. M., and Vapnik, V. N. training algorithm for optimal margin classifiers. In Proceedings of the fifth annual workshop on Computational learning theory, pp. 144152, 1992. Büyükakyüz, K. Olora: Orthonormal low-rank adaptation of large language models. arXiv preprint arXiv:2406.01775, 2024. Cahill, E., Irving, A., Johnston, C., Sexton, J., Collaboration, U., et al. Numerical stability of lanczos methods. Nuclear Physics B-Proceedings Supplements, 83:825827, 2000. Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531, 2014. Chavan, A., Liu, Z., Gupta, D., Xing, E., and Shen, Z. One-for-all: Generalized lora for parameter-efficient finetuning. arXiv preprint arXiv:2306.07967, 2023. Chen, A., Cheng, J., Liu, Z., Gao, Z., Tsung, F., Li, Y., and Li, J. Parameter-efficient fine-tuning via circular convolution. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 20042019, 2025. 9 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Chen, M. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Cohen, J. M., Kaur, S., Li, Y., Kolter, J. Z., and Talwalkar, A. Gradient descent on neural networks typically occurs at the edge of stability. arXiv preprint arXiv:2103.00065, 2021. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115, 2023. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Ding, C., Li, J., Dong, S., Gao, X., He, Y., and Gong, Y. Sulora: Subspace low-rank adaptation for parameterefficient fine-tuning. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 53345349, 2025. Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. Sharp minima can generalize for deep nets. In International Conference on Machine Learning, pp. 10191028. PMLR, 2017. Fan, C., Lu, Z., Liu, S., Gu, C., Qu, X., Wei, W., and Cheng, Y. Make lora great again: Boosting lora with adaptive singular values and mixture-of-experts optimization alignment. arXiv preprint arXiv:2502.16894, 2025. Ferrari Dacrema, M., Cremonesi, P., and Jannach, D. Are we really making much progress? worrying analysis of recent neural recommendation approaches. In Proceedings of the 13th ACM conference on recommender systems, pp. 101109, 2019. Fomenko, V., Yu, H., Lee, J., Hsieh, S., and Chen, W. note on lora. arXiv preprint arXiv:2404.05086, 2024. Gao, Z., Wang, Q., Chen, A., Liu, Z., Wu, B., Chen, L., and Li, J. Parameter-efficient fine-tuning with discrete fourier transform. arXiv preprint arXiv:2405.03003, 2024. Golub, G. H., Underwood, R. R., and Wilkinson, J. H. The lanczos algorithm for the symmetric ax= λ bx problem., 1972. Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Graves, A. Long short-term memory. Supervised sequence labelling with recurrent neural networks, pp. 3745, 2012. Hao, Y., Cao, Y., and Mou, L. Flora: Low-rank adapters arXiv preprint are secretly gradient compressors. arXiv:2402.03293, 2024. Djagba, P. and Saley, A. Y. Exploring large language models for financial applications: Techniques, performance, and challenges with finma. arXiv preprint arXiv:2510.05151, 2025. Hayou, S., Ghosh, N., and Yu, B. The impact of initialization on lora finetuning dynamics. Advances in Neural Information Processing Systems, 37:117015117040, 2024a. Dong, H., Zhu, W., Song, G., and Wang, L. Aurora: Breaking low-rank bottleneck of lora with nonlinear mapping. arXiv preprint arXiv:2505.18738, 2025. Dosovitskiy, A. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Edalati, A., Tahaei, M., Kobyzev, I., Nia, V. P., Clark, J. J., and Rezagholizadeh, M. Krona: Parameter-efficient tuning with kronecker adapter. In Enhancing LLM Performance: Efficacy, Fine-Tuning, and Inference Techniques, pp. 4965. Springer, 2025. Eimer, T., Lindauer, M., and Raileanu, R. Hyperparameters in reinforcement learning and how to tune them. In International conference on machine learning, pp. 91049149. PMLR, 2023. Hayou, S., Ghosh, N., and Yu, B. Lora+: Efficient low rank adaptation of large models. arXiv preprint arXiv:2402.12354, 2024b. Hayou, S., Ghosh, N., and Yu, B. Plop: Precise lora placement for efficient finetuning of large models. arXiv preprint arXiv:2506.20629, 2025. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pp. 10261034, 2015. 10 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Hitit, O. K., Girrbach, L., and Akata, Z. systematic study of model merging techniques in large language models. arXiv preprint arXiv:2511.21437, 2025. Hoffer, E., Hubara, I., and Soudry, D. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. Advances in neural information processing systems, 30, 2017. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pp. 2790 2799. PMLR, 2019. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Hu, Z., Wang, L., Lan, Y., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. In Proceedings of the 2023 conference on empirical methods in natural language processing, pp. 52545276, 2023. Huang, Q., Ko, T., Zhuang, Z., Tang, L., and Zhang, Y. Hira: Parameter-efficient hadamard high-rank adaptation for large language models. In The Thirteenth International Conference on Learning Representations, 2025. Ilin, I. Hessian of perplexity for large language models by pytorch autograd (open source). arXiv preprint arXiv:2504.04520, 2025. Jiang, T., Huang, S., Luo, S., Zhang, Z., Huang, H., Wei, F., Deng, W., Sun, F., Zhang, Q., Wang, D., et al. Mora: High-rank updating for parameter-efficient fine-tuning. arXiv preprint arXiv:2405.12130, 2024. Jung, Y., Ahn, D., Kim, H., Kim, T., and Park, E. Gralora: Granular low-rank adaptation for parameter-efficient finetuning. arXiv preprint arXiv:2505.20355, 2025. Kalajdzievski, D. rank stabilization scaling factor for fine-tuning with lora. arXiv preprint arXiv:2312.03732, 2023. Kalra, D. S. and Barkeshli, M. Why warmup the learning rate? underlying mechanisms and improvements. Advances in Neural Information Processing Systems, 37: 111760111801, 2024. Kang, J. and Yin, Q. Miss: Revisiting the trade-off in lora with an efficient shard-sharing structure, 2025. URL https://arxiv.org/abs/2409.15371. Kopiczko, D. J., Blankevoort, T., and Asano, Y. M. Vera: Vector-based random matrix adaptation. arXiv preprint arXiv:2310.11454, 2023a. Kopiczko, D. J., Blankevoort, T., and Asano, Y. M. Vera: Vector-based random matrix adaptation. arXiv preprint arXiv:2310.11454, 2023b. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient memory management for large language model serving In Proceedings of the 29th symwith pagedattention. posium on operating systems principles, pp. 611626, 2023. Lanczos, C. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. Journal of research of the National Bureau of Standards, 45(4):255282, 1950. LeCun, Y., Simard, P., and Pearlmutter, B. Automatic learning rate maximization by on-line estimation of the hessians eigenvectors. Advances in neural information processing systems, 5, 1992. LeCun, Y., Bottou, L., Orr, G. B., and Müller, K.-R. Efficient backprop. In Neural networks: Tricks of the trade, pp. 950. Springer, 2002. Lee, D., Park, J., Kim, M., and Kwon, J. Abm-lora: Activation boundary matching for fast convergence in low-rank adaptation. arXiv preprint arXiv:2511.19145, 2025. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Lewkowycz, A., Bahri, Y., Dyer, E., Sohl-Dickstein, J., and Gur-Ari, G. The large learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218, 2020. Li, C., Farkhoor, H., Liu, R., and Yosinski, J. Measuring the intrinsic dimension of objective landscapes. arXiv preprint arXiv:1804.08838, 2018. Li, S., Luo, X., Tang, X., Wang, H., Chen, H., Luo, W., Li, Y., He, X., and Li, R. Beyond zero initialization: Investigating the impact of non-zero initialization on lora fine-tuning dynamics. arXiv preprint arXiv:2505.23194, 2025. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 11 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Li, Y., Han, S., and Ji, S. Vb-lora: Extreme parameter efficient fine-tuning with vector banks. Advances in Neural Information Processing Systems, 37:1672416751, 2024. Lialin, V., Shivagunde, N., Muckatira, S., and Rumshisky, A. Relora: High-rank training through low-rank updates. arXiv preprint arXiv:2307.05695, 2023. Liao, B. and Monz, C. 3-in-1: 2d rotary adaptation for efficient finetuning, efficient batching and composability. Advances in Neural Information Processing Systems, 37: 3501835048, 2024. Lin, Y., Ma, X., Chu, X., Jin, Y., Yang, Z., Wang, Y., and Mei, H. Lora dropout as sparsity regularizer for overfitting control. arXiv preprint arXiv:2404.09610, 2024. Lin, Y.-C., Chen, S.-A., Liu, J.-J., and Lin, C.-J. Linear classifier: An often-forgotten baseline for text classification. arXiv preprint arXiv:2306.07111, 2023. Lipton, Z. C. and Steinhardt, J. Troubling trends in machine learning scholarship: Some ml papers suffer from flaws that could mislead the public and stymie future research. Queue, 17(1):4577, 2019. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openreview. net/forum?id=1qvx610Cu7. Liu, S.-Y., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F., Cheng, K.-T., and Chen, M.-H. Dora: Weightdecomposed low-rank adaptation. In Forty-first International Conference on Machine Learning, 2024a. Liu, W., Qiu, Z., Feng, Y., Xiu, Y., Xue, Y., Yu, L., Feng, H., Liu, Z., Heo, J., Peng, S., et al. Parameter-efficient orthogonal finetuning via butterfly factorization. arXiv preprint arXiv:2311.06243, 2023b. Liu, X.-H., Du, Y., Wang, J., and Yu, Y. On the optimization landscape of low rank adaptation methods for large language models. In The Thirteenth International Conference on Learning Representations, 2025. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Liu, Z., Lyn, J., Zhu, W., Tian, X., and Graham, Y. Alora: Allocating low-rank adaptation for fine-tuning large language models. arXiv preprint arXiv:2403.16187, 2024b. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Lucic, M., Kurach, K., Michalski, M., Gelly, S., and Bousquet, O. Are gans created equal? large-scale study. Advances in neural information processing systems, 31, 2018. Luo, H., Truong, T., Pham, T., Harandi, M., Phung, D., and Le, T. Explicit eigenvalue regularization improves sharpness-aware minimization. Advances in Neural Information Processing Systems, 37:44244453, 2024. Lyu, K., Li, Z., and Arora, S. Understanding the generalization benefit of normalization layers: Sharpness reduction. Advances in Neural Information Processing Systems, 35: 3468934708, 2022. Malinovsky, G., Michieli, U., Hammoud, H. A. A. K., Ceritli, T., Elesedy, H., Ozay, M., and Richtárik, P. Randomized asymmetric chain of lora: The first meaningful theoretical framework for low-rank adaptation. arXiv preprint arXiv:2410.08305, 2024. Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, PEFT: State-of-theS., Bossan, B., and Tietz, M. art parameter-efficient fine-tuning methods. https:// github.com/huggingface/peft, 2022. Männistö, J., Attieh, J., and Tiedemann, J. comparative study of peft methods for python code generation. In Proceedings of the Joint 25th Nordic Conference on Computational Linguistics and 11th Baltic Conference on Human Language Technologies (NoDaLiDa/Baltic-HLT 2025), pp. 390396, 2025. Marek, M., Lotfi, S., Somasundaram, A., Wilson, A. G., and Goldblum, M. Small batch size training for language models: When vanilla sgd works, and why gradient accumulation is wasteful. arXiv preprint arXiv:2507.07101, 2025. Melis, G., Dyer, C., and Blunsom, P. On the state of the art of evaluation in neural language models. arXiv preprint arXiv:1707.05589, 2017. Meng, F., Wang, Z., and Zhang, M. Pissa: Principal singular values and singular vectors adaptation of large language models. Advances in Neural Information Processing Systems, 37:121038121072, 2024a. Meng, X., Dai, D., Luo, W., Yang, Z., Wu, S., Wang, X., Wang, P., Dong, Q., Chen, L., and Sui, Z. Periodiclora: Breaking the low-rank bottleneck in lora optimization. arXiv preprint arXiv:2402.16141, 2024b. Musgrave, K., Belongie, S., and Lim, S.-N. metric learning reality check. In European Conference on Computer Vision, pp. 681699. Springer, 2020. 12 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Nikdan, M., Tabesh, S., Crnˇcevic, E., and Alistarh, D. Rosa: Accurate parameter-efficient fine-tuning via robust adaptation. arXiv preprint arXiv:2401.04679, 2024. Shchur, O., Mumme, M., Bojchevski, A., and Günnemann, S. Pitfalls of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018. Paige, C. C. Practical use of the symmetric lanczos process with re-orthogonalization. BIT Numerical Mathematics, 10(2):183195, 1970. Pan, R., Ye, H., and Zhang, T. Eigencurve: Optimal learning rate schedule for sgd on quadratic objectives with skewed hessian spectrums. arXiv preprint arXiv:2110.14109, 2021. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. Qin, P., Zhang, R., and Xie, P. BidoRA: Bi-level optimization-based weight-decomposed low-rank adaptation. Transactions on Machine Learning Research, ISSN 2835-8856. URL https://openreview. 2025. net/forum?id=v2xCm3VYl4. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 35053506, 2020. Renduchintala, A., Konuk, T., and Kuchaiev, O. Tied-lora: Enhancing parameter efficiency of lora with weight tying. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 86948705, 2024. Schmidt, R. M., Schneider, F., and Hennig, P. Descending through crowded valley-benchmarking deep learning optimizers. In International Conference on Machine Learning, pp. 93679376. PMLR, 2021. Schulman, J. and Lab, T. M. Lora without regret. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml. 20250929. https://thinkingmachines.ai/blog/lora/. Sculley, D., Snoek, J., Wiltschko, A., and Rahimi, A. Winners curse? on pace, progress, and empirical rigor. 2018. Shi, S., Huang, S., Song, M., Li, Z., Zhang, Z., Huang, H., Wei, F., Deng, W., Sun, F., and Zhang, Q. Reslora: Identity residual mapping in low-rank adaption. arXiv preprint arXiv:2402.18039, 2024. Si, C., Shi, Z., Zhang, S., Yang, X., Pfister, H., and Shen, W. Unleashing the power of task-specific directions in parameter efficient fine-tuning. In The Thirteenth International Conference on Learning Representations, 2024. Sun, Y., Li, Z., Li, Y., and Ding, B. Improving lora in privacy-preserving federated learning. arXiv preprint arXiv:2403.12313, 2024. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Ramé, A., Rivière, M., et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Valipour, M., Rezagholizadeh, M., Kobyzev, I., and Ghodsi, A. Dylora: Parameter-efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 32743287, 2023. Wang, F., Jiang, J., Park, C., Kim, S., and Tang, J. Kasa: Knowledge-aware singular-value adaptation of large language models. arXiv preprint arXiv:2412.06071, 2024a. Wang, H., Li, Y., Wang, S., Chen, G., and Chen, Y. Milora: Harnessing minor singular components for parameterefficient llm finetuning. arXiv preprint arXiv:2406.09044, 2024b. Wang, J., Wang, M., Zhou, Z., Yan, J., Wu, L., et al. The sharpness disparity principle in transformers for accelerating language model pre-training. arXiv preprint arXiv:2502.19002, 2025a. Wang, Q. and Shen, S. Activation-guided low-rank parameter adaptation for efficient model fine-tuning. IEEE Access, 2025. Wang, S. and Kanwar, P. Bfloat16: The secret to high performance on cloud tpus. Google Cloud Blog, 4(1), 2019. 13 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Wang, S., Yu, L., and Li, J. Lora-ga: Low-rank adaptation with gradient approximation. Advances in Neural Information Processing Systems, 37:5490554931, 2024c. Wang, X., Chen, T., Ge, Q., Xia, H., Bao, R., Zheng, R., Zhang, Q., Gui, T., and Huang, X.-J. Orthogonal subspace learning for language model continual learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1065810671, 2023. Wang, X., Qi, Y., and Xu, B. Losia: Efficient high-rank fine-tuning via subnet localization and optimization. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 67076726, 2025b. Wang, Y., Meng, F., Zhang, X., Jiang, F., Tang, P., and Zhang, M. Hd-pissa: High-rank distributed orthogonal adaptation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 65266539, 2025c. Wu, Z., Arora, A., Wang, Z., Geiger, A., Jurafsky, D., Manning, C. D., and Potts, C. Reft: Representation finetuning for language models. Advances in Neural Information Processing Systems, 37:6390863962, 2024. Xiong, Y. and Xie, X. Oplora: Orthogonal projection lora prevents catastrophic forgetting during parameterefficient fine-tuning. arXiv preprint arXiv:2510.13003, 2025. Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen, Z., Zhang, X., and Tian, Q. Qa-lora: Quantizationaware low-rank adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023. Xu, Z., Min, H., MacDonald, L. E., Luo, J., Tarmoun, S., Mallada, E., and Vidal, R. Understanding the learning dynamics of lora: gradient flow perspective on lowrank adaptation in matrix factorization. arXiv preprint arXiv:2503.06982, 2025. Yan, M., Wang, Z., Jia, Z., Venkataraman, S., and Wang, Y. Plora: Efficient lora hyperparameter tuning for large models. arXiv preprint arXiv:2508.02932, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. parameter-efficient fine-tuning. Advances in Neural Information Processing Systems, 37:7176871791, 2024a. Yang, Y., Zhou, J., Wong, N., and Zhang, Z. Loretta: Low-rank economic tensor-train adaptation for ultra-lowparameter fine-tuning of large language models. arXiv preprint arXiv:2402.11417, 2024b. Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M. W. Pyhessian: Neural networks through the lens of the hessian. In 2020 IEEE international conference on big data (Big data), pp. 581590. IEEE, 2020. Yin, B., Yang, X., and Wang, X. Dont forget the nonlinearity: Unlocking activation functions in efficient fine-tuning. arXiv preprint arXiv:2509.13240, 2025. Yin, F., Ye, X., and Durrett, G. Lofit: Localized fine-tuning on llm representations. Advances in Neural Information Processing Systems, 37:94749506, 2024. Yu, J., Zhang, Y., Wang, B., Lin, P., Liu, Y., and Feng, S. Ssmlora: Enhancing low-rank adaptation with state space model. arXiv preprint arXiv:2502.04958, 2025a. Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Yu, X., Xie, C., Zhao, Z., Fan, T., Xue, L., and Zhang, Z. Prunedlora: Robust gradient-based structured pruning for low-rank adaptation in fine-tuning, 2025b. URL https: //arxiv.org/abs/2510.00192. Yuan, S., Liu, H., and Xu, H. Bridging the gap between lowrank and orthogonal adaptation via householder reflection adaptation. Advances in Neural Information Processing Systems, 37:113484113518, 2024. Zaken, E. B., Goldberg, Y., and Ravfogel, S. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 19, 2022. Zhang, C., Wang, K., and Gu, Y. Beyond low-rank tuning: Model prior-guided rank allocation for effective transfer in low-data and large-gap regimes. arXiv preprint arXiv:2507.00327, 2025a. Yang, A. X., Robeyns, M., Wang, X., and Aitchison, L. Bayesian low-rank adaptation for large language models. arXiv preprint arXiv:2308.13111, 2023. Zhang, F. and Pilanci, M. Riemannian preconditioned lora for fine-tuning foundation models. arXiv preprint arXiv:2402.02347, 2024. Yang, Y., Li, X., Zhou, Z., Song, S., Wu, J., Nie, L., and Ghanem, B. Corda: Context-oriented decomposition adaptation of large language models for task-aware Zhang, H. Droplora: for parameter-efficient fine-tuning. arXiv:2508.17337, 2025. Sparse low-rank adaptation arXiv preprint Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Zhang, H., Huang, B., Li, Z., Xiao, X., Leong, H. Y., Zhang, Z., Long, X., Wang, T., and Xu, H. Sensitivity-lora: Low-load sensitivity-based fine-tuning for large language models. arXiv preprint arXiv:2509.09119, 2025b. Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y. Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507, 2024a. Zhao, Y., Dang, S., Ye, H., Dai, G., Qian, Y., and Tsang, I. W. Second-order fine-tuning without pain for llms: hessian informed zeroth-order optimizer. arXiv preprint arXiv:2402.15173, 2024b. Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., Chen, W., and Yue, X. Opencodeinterpreter: Integrating code generation with execution and refinement. arXiv preprint arXiv:2402.14658, 2024a. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024b. Zhou, H., Lu, X., Xu, W., Zhu, C., Zhao, T., and Yang, M. Lora-drop: Efficient lora parameter pruning based on output evaluation. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 55305543, 2025. Zhu, M. and Nguyen, P. H. survey of lora algorithm variations for language models. In International Conference on Applications of Natural Language to Information Systems, pp. 275290. Springer, 2025. Zi, B., Qi, X., Wang, L., Wang, J., Wong, K.-F., and Zhang, L. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices. arXiv preprint arXiv:2309.02411, 2023. Zhang, J., You, J., Panda, A., and Goldstein, T. LoRA without forgetting: Freezing and sparse masking for lowIn Sparsity in LLMs (SLLM): Deep rank adaptation. Dive into Mixture of Experts, Quantization, Hardware, and Inference, 2025c. URL https://openreview.net/ forum?id=aGOQYJfz6H. Zhang, J.-C., Xiong, Y.-J., Xia, C.-M., Zhu, D.-H., and Zhan, H.-J. Lora2: Multi-scale low-rank approximations for fine-tuning large language models. Neurocomputing, 650:130859, 2025d. Zhang, L., Zhang, L., Shi, S., Chu, X., and Li, B. Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning. arXiv preprint arXiv:2308.03303, 2023a. Zhang, Q., Chen, M., Bukharin, A., Karampatziakis, N., He, P., Cheng, Y., Chen, W., and Zhao, T. Adalora: Adaptive budget allocation for parameter-efficient finetuning. arXiv preprint arXiv:2303.10512, 2023b. Zhang, Q., Chu, C., Peng, T., Li, Q., Luo, X., Jiang, Z., and Huang, S.-L. Lora-da: Data-aware initialization for lowrank adaptation via asymptotic analysis. arXiv preprint arXiv:2510.24561, 2025e. Zhang, R., Qiang, R., Somayajula, S. A., and Xie, P. Autolora: Automatically tuning matrix ranks in lowrank adaptation based on meta learning. arXiv preprint arXiv:2403.09113, 2024a. Zhang, X., Chen, G.-Z., Li, S., Liu, Z., Chen, C. P., and Zhang, T. An orthogonal high-rank adaptation for large language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 1882618844, 2025f. Zhang, Y., Chen, C., Ding, T., Li, Z., Sun, R., and Luo, Z. Why transformers need adam: hessian perspective. Advances in neural information processing systems, 37: 131786131823, 2024b. Zhang, Y., Liu, F., and Chen, Y. Lora-one: One-step full gradient could suffice for fine-tuning large language models, provably and efficiently. arXiv preprint arXiv:2502.01235, 2025g. Zhang, Z., Li, H., Zhang, Y., Gong, G., Wang, J., Liu, P., Jiang, Q., and Hu, J. The primacy of magnitude in low-rank adaptation. arXiv preprint arXiv:2507.06558, 2025h. 15 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning"
        },
        {
            "title": "A Comprehensive Study of Hyperparameters in Prior Work",
            "content": "A.1 Survey Criteria . . . . . . . . A.2 Comprehensive List of Papers . . Fine-tuning Implementation Details B.1 Models . . . . . . . . . . . . . B.2 Hyperparameter Search Ranges . . . . . B.3 Fixed Training Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Data, Code, Libraries, and Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Additional Experiments",
            "content": "D.1 Varying Training Duration . . . . D.2 Varying Adapter Ranks on Llama . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Details of Hyperparameter Search Results",
            "content": "E.1 Qwen3-0.6B . . E.2 Gemma-3-1B . E.3 Llama-2-7B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "G Hessian Computation Details",
            "content": "G.1 Lanczos Algorithm Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Additional Hessian Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "H Limitations and Future Work",
            "content": "17 17 17 21 21 21 22 22 23 23 24 24 25 25 28 31 33 35 16 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning A. Comprehensive Study of Hyperparameters in Prior Work A.1. Survey Criteria To generate the statistics presented in Figure 2, we curated dataset comprising 52 papers, consisting of 42 studies published at major AI conferences or journals, 6 high-impact arXiv preprints (exceeding 40 citations), and 4 recent preprints released within the last six months. For each paper, we examined whether the authors reported performance metrics under learning rate or batch size tuning, and whether comparisons across different ranks were provided. The selection criteria for inclusion were as follows: 1. The primary objective of the proposed method is to enhance fine-tuning effectiveness (i.e., aiming for higher accuracy with equivalent trainable parameter counts or sustained performance with greater parameter efficiency). 2. Vanilla LoRA is explicitly employed as baseline for performance comparison. In assessing hyperparameter tuning, our analysis focuses exclusively on decoder-only LLMs, excluding encoder-only (Devlin et al., 2019), encoder-decoder (Raffel et al., 2020) architectures, Vision Transformers (Dosovitskiy, 2020), and Vision-Language Models (Alayrac et al., 2022), as these lie outside the scope of this work. Consequently, papers lacking experiments on decoder-only LLMs are excluded from our statistics (e.g., Zhang et al. (2023b); Liu et al. (2023b); Xu et al. (2025); Zhang et al. (2025d); Edalati et al. (2025); Yuan et al. (2024)). Moreover, we exclude papers focusing on objectives other than PEFT efficiency, such as parameter-efficient pretraining (Lialin et al., 2023), continual learning (Wang et al., 2023; Zhao et al., 2024a), and quantization (Dettmers et al., 2023; Xu et al., 2023). Given that some studies may tune hyperparameters only for their proposed methods while leaving baselines untuned (e.g., by adopting settings from prior work without modification), we rigorously verified whether the vanilla LoRA baseline underwent tuning. Specifically, we consider the learning rate and batch size to be tuned only if they are evaluated across at least three distinct values. Consequently, studies such as MiLoRA (which compared two sets of hyperparameter setups) or LoRA-GA (Wang et al., 2024c) (which tested learning rate in {1e-5, 5e-5}) do not qualify as tuning under our criteria. For rank, we require comparisons across at least two distinct values. Crucially, if study varies the rank for its proposed method but benchmarks against fixed-rank vanilla LoRA, we do not classify the baseline rank as tuned (e.g., Bałazy et al. (2024); Yuan et al. (2024); Li et al. (2024)). During the data curation process, we observed that verifying specific hyperparameter tuning details can be non-trivial in some cases. The difficulty arises from discrepancies between paper versions (e.g., ablation studies on hyperparameters added to the Appendix post-publication), incomplete descriptions of experimental setups, or underspecified hyperparameter settings. Additionally, we frequently observed papers performing hyperparameter tuning only on smaller encoder-only LLMs (e.g., RoBERTa (Liu et al., 2019)). In strict adherence to our inclusion criteria, we do not categorize these instances as tuned. While these ambiguities complicate binary categorization, we have made every effort to ensure accuracy. We emphasize that the statistics is curated solely to present the current state of hyperparameter tuning practices in LoRA PEFT research, and we welcome future contributions or corrections to further refine this collection. A.2. Comprehensive List of Papers comprehensive list of the reviewed papers is detailed in Table 2. Specifically, arXiv Date marks the release date of the first version on arXiv (denoted as if unavailable). Pub. Date refers to the formal publication date of the venue, where indicates the paper has not yet been formally published. Note that the table is sorted primarily by Pub. Date, followed by arXiv Date. Table 2. Publication dates, venues, and experimental configurations in prior LoRA PEFT studies. The table summarizes decoder-only LLMs and tasks, noting whether the vanilla LoRA baseline involved learning rate (LR) or batch size (BS) tuning and offered comparisons across different Ranks. positive entry () indicates the configuration was provided for at least one model-task combination; denotes otherwise. The symbol denotes workshop paper. Method arXiv Date Pub. Date Venue Decoder-only LLM Fine-tuned Task LR BS Rank DyLoRA (2023) 2022-10 2023-05 EACL GPT-2 Medium GLoRA (2023) 2023arXiv Llama-1-7B Llama-2-7B 17 NLG NLG Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Method arXiv Date Pub. Date Venue Decoder-only LLM Fine-tuned Task LR BS Rank LoRA-FA (2023a) 2023-08 arXiv Laplace-LoRA (2023) 2023-08 2024-05 ICLR VeRA (2023a) 2023-10 2024ICLR Llama-1-7B Llama-2-7B Llama-1-7B Llama-2-7B GPT-2 Medium/Large Llama-1-7B/13B Llama-2-7B/13B BOFT (2023b) 20232024-05 ICLR Llama-2-7B MoRA (2024) 2024-05 Delta-LoRA (2023) 2023-09 arXiv Llama-2-7B/13B arXiv GPT-2 Medium NLG Commonsense Commonsense NLG Instruction Following Instruction Following Math UUID Math Instruction Following NLG Commonsense Math NLG GLUE NLG NLG GLUE Instruction Following NLG Math Code Instruction Following GLUE Instruction Following NLG GLUE Commonsense Summarization Translation NLG Instruction Following Math Commonsense Instruction Following Math Commonsense Instruction Following NLG Math Commonsense Tied-LoRA (2024) 2023-11 2024-06 NAACL GPT-2B-001 Llama-2-7B LoRETTA (2024b) 2024-02 2024-06 NAACL Llama-2-7B/13B/70B AutoLoRA (2024a) 2024-03 2024-06 NAACL GPT-2 Medium ALoRA (2024b) 20242024-06 NAACL GPT2-Large Llama-2-7B RoSA (2024) 2024-01 2024ICML Llama-2-7B LoRA+ (2024b) 2024-02 2024-07 ICML scaled AdamW (2024) 2024-02 2024-07 ICML DoRA (2024a) 20242024-07 ICML GPT-2 Llama-1-7B GPT-2 Medium Mistral-7B-V0.1 Llama-1-7B/13B Llama-2-7B Llama-3-8B FLORA (2024) 2024-02 2024-07 ICML GPT-2 -base/XL FourierFT (2024) 20242024-07 ICML GPT-2 Medium/Large Llama-1-7B/13B Llama2-7B/13B ResLoRA (2024) 2024-02 2024ACL Llama-2-7B PLoRA (2024b) 2024-02 OLoRA (2024) 2024arXiv Llama-1-7B arXiv Mistral-7B LLaMA-2-7B Tiny Llama-1.1B Gemma-2B OPT-1.3B LamDA (2024) 2024-06 2024-11 EMNLP Llama-2-7B Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Method arXiv Date Pub. Date Venue Decoder-only LLM Fine-tuned Task LR BS Rank LoRA-drop (2025) 2024-02 2025-01 COLING Llama-2-7b PiSSA (2024a) 2024-04 2024-12 NeurIPS Llama-2-7B / 13B Llama-3-8B / 70B Mistral-7B-v0.1 Gemma-7B Qwen1.5-7B Yi-1.5-34B DeepSeek-MoE-16B Mixtral-8x7B NLG Summarization GLUE Math Math Code Instruction Following VB-LoRA (2024) 2024-05 2024-12 NeurIPS GPT-2 Medium/Large Llama-2-7B/13B Mistral-7B-v0.1/Gemma-7B NLG Math Instruction Following HRA (2024) 2024-05 2024-12 NeurIPS Llama-2-7B Math CorDA (2024a) 2024-06 2024-12 NeurIPS Llama-2-7B/13B Llama-3-8B Gemma-2-9B LoRA-GA (2024c) 2024-07 2024-12 NeurIPS Llama-2-7B RoAd (2024) 20242024-12 NeurIPS Llama-1-7B/13B Llama-2-7B Llama-3-8B AG-LoRA (2025) 2025IEEE Access Llama-1-7B LoRA-Dash (2024) 2024-09 2025-04 ICLR KaSA (2024a) 2024-09 2025-04 ICLR RandLoRA (2025) 2025-02 2025ICLR DeLoRA (2025) 2025-03 2025-04 ICLR HiRA (2025) 2025-04 ICLR MiLoRA (2024b) 2024-06 2025NAACL SSMLoRA (2025a) 2025-02 2025-04 NAACL MiSS (2025) 2024-09 2025-07 ICML Llama-1-7B Llama-2-7B Llama-3-8B Qwen2.5-7B Llama-1-7B Llama-2-7B Llama-3-8B Qwen2.5-7B GPT-2 Medium Qwen2-0.5B Phi3-3B Llama3-8B Llama-2-7B Llama3-8B Llama-2-7B Llama-3-8B Llama-2-7B Llama-3-8B Qwen2.5-7B GPT-2 Llama-2-7B/13B Llama2-7B/13B Mistral-7B Qwen3-4B Llama-3.2-3B Math Code Instruction Following World Knowledge Math Code Instruction Following Math Commonsense Commonsense Reasoning GLUE Commonsense GLUE Commonsense Instruction Following NLG Commonsense Commonsense Math Commonsense Dialogue Generation Math Commonsense Instruction Following GLUE Math Code Instruction Following Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Method arXiv Date Pub. Date Venue Decoder-only LLM Fine-tuned Task LR BS Rank LoRA-One (2025g) 2025-02 2025-07 ICML Llama-2-7B Init[AB] (2025) 2025-05 2025-07 ICML Llama-3-8B C3A (2025) 20242025-07 ACL Llama-2-7B Llama-3-8B/70B Mistral-7B Mistral-8x7B Math Code Instruction Following Arithmetic Commonsense Math Code Commonsense 2025-07 ACL Llama-2-7B Instruction Following SuLoRA (2025) BiDoRA (2025) 2024-10 2025-08 TMLR GPT-2 Medium HD-PiSSA (2025c) 2025-05 2025-11 EMNLP LoSiA (2025b) 2025-07 2025EMNLP Sensitivity-LoRA (2025b) 2025-09 2025-11 EMNLP OHoRA (2025f) 2025-11 EMNLP Llama-2-7B Llama-3-8B Mistral-7b-v0.1 Gemma 2B Llama-2-7B/13B GPT-2 Large Qwen2.5-7B/32B Llama-3.1-8B Llama-2-7B Llama-3-8B Gemma-7B Llama-3.1-8B-Inst NLG Math Code Math Code Commonsense Instruction Folllowing NLG Instruction Following Math Code Commonsense Instruction Following AuroRA (2025) 2025-05 2025-12 NeurIPS Llama-3-8B Commonsense DropLoRA (2025) 2025-08 PrunedLoRA (2025b) 2025-09 LoRA-DA (2025e) 2025ABM-LoRA (2025) 2025-11 arXiv Llama-2-7B Llama-3-8B arXiv Llama-3-8B arXiv Llama-2-7B Math Code Commonsense Instruction Following Math Commonsense arXiv Llama-2-7B Instruction Following Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning B. Fine-tuning Implementation Details B.1. Models Following standard practice in PEFT research to ensure results purely reflect the training data, we use base versions (not instruction-tuned) for all models (Hu et al., 2022; Meng et al., 2024a; Wang et al., 2024b; Li et al., 2025; Liu et al., 2024a). Specifically, we utilize the official checkpoints hosted on Hugging Face: Qwen3-0.6B-Base1, gemma-3-1b-pt2, and Llama-2-7b-hf3. B.2. Hyperparameter Search Ranges See Table 3 for hyperparameter search ranges for each model-task combination. For all experiments on Qwen and Gemma, we conduct three independent trainings and report the mean and standard deviation. Model Task Rank (r) Batch (B) Learning Rate (η) Qwen3-0.6B Math 8 128 {16, 64, 128} 1.1247e-5 6.3246e-3 2.0000e-6 2.0000e-3 64 Gemma-3-1B Math {4, 8, 16, 32, 64, 128, 256} 64 1.1247e-5 6.3246e-3 {16, 64, 128} 1.1247e-5 6.3246e-3 Llama-2-7B Code {4, 8, 16, 32, 64, 128, 256} Math {8, 32, 128} 1.1247e-5 6.3246e-3 {16, 128} 2.0000e-5 3.5566e-3 Code {8, 32, 128} {16, 128} 2.0000e-5 3.5566e-3 Table 3. Summary of models, tasks, ranks, and hyperparameter search ranges. Learning rates are tuned evenly in logarithmic scale: {1.1247e*, 2.0000e*, 3.5566e*, 6.3246e*} per order of magnitude. B.3. Fixed Training Hyperparameters Except for tunable hyperparameters (i.e., learning rate and batch size), all other training configurations remain fixed and the same for all experiments; the values are summarized in Table 4. Note that these configurations primarily follow PiSSA, thus may differ from those of other considered PEFT methods. For example, MiLoRA, DoRA, and Init[AB] employ linear decay instead of cosine annealing for learning rate scheduling. Additionally, MiLoRA and DoRA use fixed warmup steps (100) rather than 3% of total training steps, and apply dropout rate of 0.05 instead of no dropout. Furthermore, while we place low-rank adapters on all linear layers, these methods exclude output projections (out_proj) or gate matrices (gate_proj) in several of their experiments. Configuration Value Epoch Scaling factor (α) Optimizer LR scheduler Warmup ratio Dropout 1 AdamW (Loshchilov & Hutter, 2017) Cosine annealing with warmup 3% None None Weight Decay Adapter placement Base model precision BFloat16 (Wang & Kanwar, 2019)4 Adapter precision All linear layers Float32 Table 4. Fixed training configurations across all experiments. Scaling factor α equals the LoRA rank r. Adapters are applied to all linear projection layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj). Max sequence length 1https://huggingface.co/Qwen/Qwen3-0.6B-Base 2https://huggingface.co/google/gemma-3-1b-pt 3https://huggingface.co/meta-llama/Llama-2-7b-hf 4Following PiSSAs codebase, normalization layers and gate_proj matrices of the pretrained model are converted back to Float32 after the BFloat16 sampling and before training. 21 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning B.4. Data, Code, Libraries, and Hardware The codebase of Meng et al. (2024a)5 is used to run LoRA, PiSSA, and DoRA (with DoRA implemented via the PEFT package (Mangrulkar et al., 2022)), and we further extend this codebase to support other PEFT methods, including MiLoRA6 and Init[AB]7. Note that while the original LoRA paper used Kaiming Normal initialization, we follow its official implementation and the widely-used PEFT library to use Kaiming Uniform instead in our experiments. The results are expected to be similar (cf. Meng et al. (2024a, Table 2)). Additionally, while Li et al. (2025) also proposed variant, Init[AB+], which does not require Wres and shows no discernible performance difference, we chose to implement the default Init[AB]. For the training data, we directly utilized the preprocessed dataset released by Meng et al. (2024a)8. PyTorch (Paszke et al., 2019) version 2.7.1 is used for implementation. All experiments are conducted on four GPUs (either 4 Nvidia RTX 3090 or 4 Nvidia A6000). We employ DeepSpeed (Rasley et al., 2020) for parallel training and vLLM (Kwon et al., 2023) for parallel inference. During inference, we apply greedy decoding (i.e., temperature set to 0), and utilize the EvalPlus (Liu et al., 2023a) framework to evaluate pass@1 for code generation tasks. All fine-tuning experiments (except those for Llama-2-7B) are conducted with three independent runs and reported with means and standard deviations. The sources of randomness are controlled by explicitly fixing random seeds across Python, NumPy, and PyTorch using the code snippet shown below. def seed_everything ( seed ): random . seed ( seed ) os . environ [ PYTHONHASHSEED ] = str ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = True C. On LoRA Scaling Factor The configuration of the LoRA alpha parameter (α) generally follows two paradigms: (1) setting it to fixed constant across LoRA ranks (typically 32 or 64), or (2) scaling it with the LoRA rank, often following α = or α = 2r, which results in scaling factor γr of 1 or 2, respectively. The configurations adopted for decoder-LLMs in the considered LoRA variants are summarized as follows: PiSSA: γr = 1 (α = for all r). MiLoRA: γr = 2 for vanilla LoRA; γr = 1 for both MiLoRA and PiSSA. Init[AB]: γr = 1 (r = 16 and α = 16). DoRA: γr = 2 (α = 2r for all r). Evidently, the methods considered in this paper adhere to the second paradigm. Consequently, we adopt the setting α = (γr = 1) for all our experiments. We refer readers to several prior studies that have explored the optimal LoRA scaling factor: Kalajdzievski (2023) argued that α should scale with the square root of r, rather than linearly (α r), though the optimal α setup remains unclear. Empirically, Biderman et al. (2024) demonstrated through joint sweeps of α and learning rates that α = 2r is the optimal choice, with α = performing only marginally worse (cf. Biderman et al. (2024, Appendix B.2, Figure S3)). Notably, Zhang et al. (2025h) recently unified the learning rate, scaling factor, and initialization under single theoretical framework, suggesting that tuning the learning rate is theoretically equivalent to tuning the scaling factor (Fan et al., 2025). This further validates our decision to fix the scaling factor in our experiments. 5https://github.com/GraphPKU/PiSSA 6https://github.com/sufenlp/MiLoRA 7https://github.com/Leopold1423/non_zero_lora-icml25 8https://huggingface.co/datasets/fxmeng/pissa-dataset Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning D. Additional Experiments D.1. Varying Training Duration In this section, we examine whether different methods consistently peak at comparable performance levels under varying training durations. Specifically, we vary the training duration by scaling the number of MetaMathQA training samples from 5k up to the full 395k (Figure 7). Figure 7. Best achievable performance of LoRA and its variants across different training sample sizes on mathematical reasoning with Gemma-3-1B (r = 128, = 64). Once the learning rate is properly tuned, all methods exhibit nearly identical improvement trends as the number of training samples increases. Results are reported with mean and standard deviation over three runs. D.2. Varying Adapter Ranks on Llama In Sec. 4.3.2, we analyzed the behavior of LoRA PEFT methods as the adapter rank varies on Gemma across math and code tasks. Here, we present corresponding analysis on Llama in Figure 8, reporting the best performance achieved under joint optimization of learning rate and batch size (B {16, 128}). Similar trends can be observed, where all methods exhibit comparable performance improvement trends as the rank increases. Although under specific tasks or rank settings, one might favor particular variant that marginally outperforms others, it is worth noting again that these improvements often lack universality, with vanilla LoRA frequently matching or even outperforming them. (a) Mathematical Reasoning (b) Code Generation Figure 8. Best achievable performance of LoRA and its advanced variants across adapter ranks on Llama-2-7B (B {16, 128}). 23 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning E. Details of Hyperparameter Search Results Sections E.1, E.2, and E.3 present detailed hyperparameter search results on Qwen, Gemma, and Llama, respectively. Note that Tables 6, 10, and 13 correspond to the detailed numerical results for Figures 1, 4a, and 4b in the main text, respectively. Additionally, Tables 8, 9, and 10 provide the hyperparameter search details across adapter ranks for Appendix Figure 8a, while Tables 11, 12, and 13 correspond to Appendix Figure 8b. E.1. Qwen3-0.6B E.1.1. MATHEMATICAL REASONING Methods Batch Size 1.12e-05 2.00e-05 3.56e-05 6.32e-05 1.12e-04 2.00e3.56e-04 6.32e-04 1.12e-03 2.00e-03 3.56e-03 6.32eLearning Rate LoRA DoRA 16 64 38.941.02 46.491.17 47.780.47 47.850.42 48.130.40 48.900.41 48.410.59 49.050.49 47.640.86 44.000.45 26.5921. 5.035.33 29.530.18 33.020.28 39.150.33 46.490.25 48.160.24 48.390.29 48.950.23 48.990.40 48.730.10 48.140.42 43.920.48 1.280.05 128 22.300.30 30.221.63 33.640.13 40.641.48 47.880.61 48.380.19 48.380.01 48.690.57 48.720.04 48.330.54 31.3226. 1.130.58 16 64 42.031.63 47.360.59 48.100.12 48.100.48 48.290.08 48.800.64 48.600.17 48.700.05 46.300.29 42.670.19 35.930.44 1.310. 38.691.24 37.601.22 40.651.58 47.060.50 48.410.31 48.030.18 49.070.03 48.870.82 48.550.43 47.310.48 44.610.00 1.100.31 128 33.371.56 36.850.98 36.560.41 43.031.11 48.460.54 47.940.30 48.300.24 48.410.72 48.590.10 47.750.47 46.080. 14.9123.56 Init[AB] 16 64 36.532.20 41.671.99 45.471.30 48.070.70 48.280.72 48.660.31 48.530.48 48.180.49 46.790.19 42.320.39 38.650. 20.6727.96 35.780.54 35.151.09 37.850.08 40.041.50 45.031.13 48.340.29 48.530.07 48.450.50 48.680.15 47.110.43 43.130.80 1.360.06 128 31.341.30 32.440.70 36.210.32 35.291.90 41.731.33 47.061.29 48.380.60 48.570.39 48.120.01 48.340. 46.790.22 0.990.52 MiLoRA 16 64 39.420.87 45.090.19 44.760.78 45.920.81 49.160.37 49.360.09 48.930.37 48.080.06 46.090.55 43.390.52 25.5320. 1.470.07 32.251.20 38.331.24 45.720.88 44.081.50 47.320.05 48.690.32 49.400.01 49.060.18 48.900.37 47.020.38 43.981.06 1.070.50 128 30.940.32 33.710.34 35.030.69 40.490.39 44.270.12 48.080.42 48.650.18 49.370.19 49.330.52 48.120. 46.910.06 1.240.33 PiSSA 16 64 47.100.28 44.800.71 46.450.61 48.370.36 48.300.26 47.420.22 45.470.36 42.430.03 38.830.40 33.140. 24.630.98 14.8219.10 44.200.12 44.120.69 47.540.50 48.250.14 48.460.59 48.430.12 47.940.39 47.230.19 43.940.04 40.150.20 35.370.55 18.9015.52 39.492.26 43.640.51 43.610.50 46.420.10 48.510.31 48.240.18 48.060.72 47.700.36 45.560.15 43.540.41 38.850.18 33.360.44 Table 5. Performance of Qwen3-0.6B fine-tuned on mathematical reasoning tasks with rank = 8. Methods Batch Size LoRA DoRA Init[AB] MiLoRA PiSSA 64 64 64 64 Learning Rate 2.00e3.56e-06 6.32e-06 1.12e-05 2.00e-05 3.56e-05 6.32e1.12e-04 2.00e-04 3.56e-04 6.32e-04 1.12e-03 2.00e21.480.53 24.520.44 31.940.58 43.071.16 48.370.19 48.810.14 49.270.34 49.460.56 49.600.18 48.950.20 47.080.22 40.760.96 1.270.15 36.840.86 38.191.74 39.290.36 44.720.96 48.090.74 49.010.44 49.250.33 49.450.25 49.330.45 49.320.71 46.920.28 40.240.62 10.1415.64 33.402.16 35.811.72 41.201.83 47.270.89 49.270.13 49.020.11 48.810.17 49.290.23 48.510.44 47.370.39 44.810.34 39.410.49 0.930.41 33.620.39 38.350.46 43.370.41 48.300.27 49.080.23 48.740.32 49.170.38 49.080.14 48.220.20 46.570.73 43.910.58 38.700. 9.4814.40 38.130.84 44.510.26 48.110.14 48.770.12 49.430.19 49.090.16 48.440.12 47.100.32 43.840.28 39.660.36 34.370.39 27.181.29 17.350.57 Table 6. Performance of Qwen3-0.6B fine-tuned on mathematical reasoning tasks with rank = 128. 24 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning E.2. Gemma-3-1B E.2.1. MATHEMATICAL REASONING Methods Batch Size 1.12e-5 2.00e3.56e-5 6.32e-5 1.12e-4 2.00e-4 3.56e-4 6.32e1.12e-3 2.00e-3 3.56e-3 6.32e-3 Learning Rate LoRA DoRA 16 64 9.780.36 11.160.28 13.580.18 15.480.15 18.430.14 20.000.26 19.930.65 17.990.55 11.710.49 1.520. 1.270.59 1.070.27 6.880.04 9.120.39 10.790.37 13.230.25 15.650.57 17.540.29 19.730.16 20.460.79 19.830.91 13.330.81 1.480.48 0.000. 128 5.700.34 6.950.23 9.410.44 11.430.40 13.680.77 15.920.45 18.580.44 19.600.09 20.320.28 16.952. 0.090.16 0.000.00 16 64 9.890.24 11.160.51 13.840.41 15.610.11 18.210.45 20.110.26 20.960.57 18.340.20 11.900.29 4.890. 0.930.12 1.160.15 6.720.09 9.190.19 10.530.20 13.450.31 15.720.32 17.660.20 19.960.05 20.820.32 19.870.91 13.531.64 1.520.45 0.340. 128 5.550.11 7.210.18 9.720.17 11.580.25 13.980.33 16.190.46 18.250.23 19.670.71 20.330.64 12.8610.03 0.130.23 0.020.03 Init[AB] 16 64 9.730.35 12.100.14 14.410.49 16.730.37 18.380.53 20.390.38 20.550.40 18.340.48 11.940.31 1.480.24 1.160.31 1.450. 6.510.22 9.150.12 11.280.20 13.200.24 15.880.39 17.890.30 20.080.26 20.980.33 19.310.75 13.970.03 2.743.83 0.070.12 6.060.35 7.050.33 9.530.22 11.810.08 13.980.79 16.460.39 18.360.21 20.370.39 20.660.39 17.850.84 4.407.46 0.000. MiLoRA 16 64 12.440.07 13.770.25 16.280.24 18.450.47 20.040.19 20.630.67 19.400.80 15.720.49 10.220.42 2.030.95 1.350.43 1.560. 8.820.40 11.250.20 13.160.11 15.540.29 17.430.24 19.560.33 20.030.59 19.600.78 17.930.90 13.650.07 4.970.40 0.000.00 128 7.320. 9.570.24 11.760.33 13.540.12 16.020.16 18.390.26 19.700.34 19.990.66 19.530.47 16.830.73 7.451.00 0.570.81 PiSSA 64 128 14.300.18 16.100.27 18.310.12 19.900.21 20.610.28 19.090.20 16.100.64 13.250.55 8.410.13 4.670.29 2.501.27 0.960. 11.110.05 13.670.17 15.560.33 18.110.23 19.520.48 20.680.77 20.590.32 19.110.86 15.530.37 9.570.72 5.780.37 0.330.46 11.800.28 14.400.11 16.230.38 18.600.21 19.610.44 20.650.44 19.211.15 16.910.19 9.420.38 Table 7. Performance of Gemma-3-1B fine-tuned on mathematical reasoning tasks with rank=128. 13.870. 6.280.49 1.190.36 E.3. Llama-2-7B E.3.1. MATHEMATICAL REASONING Methods Batch Size 2.00e-05 3.56e-05 6.32e-05 1.12e-04 2.00e-04 3.56e-04 6.32e-04 1.12e-03 2.00e-03 3.56eLearning Rate LoRA DoRA Init[AB] MiLoRA PiSSA 16 128 16 128 16 16 128 16 23.16 16.12 22. 16.07 20.89 15.79 21.12 15.72 22. 24.55 18.60 24.34 18.98 23.36 17. 23.45 18.51 26.30 26.05 21.46 26. 21.57 27.08 20.17 25.61 20.70 28. 28.73 23.49 28.44 23.70 29.25 22. 28.38 22.58 30.12 30.70 25.91 30. 26.16 31.24 25.42 30.59 25.32 31. 32.18 28.21 33.20 28.54 33.30 28. 32.49 26.76 31.62 32.94 30.31 32. 30.20 32.78 30.45 33.22 29.87 30. 32.02 32.28 32.43 32.80 31.34 32. 32.46 31.48 28.76 27.71 32.78 1. 33.62 27.38 32.96 27.56 33.55 0. 18.94 128 31.53 Table 8. Performance of Llama-2-7B fine-tuned on mathematical reasoning with rank=8. 30.61 26.23 28.14 31. 21.60 22.80 31.86 0.00 1.97 1. 0.00 0.04 31.08 0.00 0.26 0. 0.48 25 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Methods Batch Size 2.00e-05 3.56e-05 6.32e-05 1.12e-04 2.00e-04 3.56e-04 6.32e-04 1.12e-03 2.00eLearning Rate 16 128 16 128 128 16 128 16 25.03 19. 25.41 20.60 24.06 17.83 24.53 18. 29.42 27.77 22.25 27.35 22.41 27. 20.96 27.23 21.45 30.68 29.67 24. 29.78 24.02 28.71 23.15 29.23 23. 32.75 32.11 26.29 31.96 26.66 32. 26.62 31.44 26.54 33.66 33.73 29. 34.14 29.71 34.10 28.34 33.97 27. 33.62 33.84 32.19 35.16 31.96 34. 30.79 34.85 30.29 32.66 34.18 33. 33.26 33.41 34.12 33.38 33.85 32. 29.30 27.01 34.62 28.92 34.25 27. 34.59 28.35 34.59 18.36 1.31 0. 0.55 0.00 0.77 0.97 0.62 0. 0.15 128 30.01 Table 9. Performance of Llama-2-7B fine-tuned on mathematical reasoning with rank=32. 32.89 33.42 32. 29.44 23.98 27.02 34.31 0.65 Methods Batch Size 2.00e-05 3.56e-05 6.32e-05 1.12e-04 2.00e-04 3.56e-04 6.32e-04 1.12e-03 2.00e-03 Learning Rate 16 128 128 16 128 16 128 29.21 22.69 29.43 23.27 29.04 22. 28.23 22.14 33.35 31.30 24.95 30. 25.63 31.52 25.03 31.11 24.84 35. 33.25 27.79 33.14 27.87 31.96 28. 33.42 27.88 35.27 35.45 30.74 35. 30.11 34.81 30.47 35.22 30.33 35. 35.91 32.62 36.41 33.00 36.72 31. 36.02 31.29 32.89 35.10 34.85 34. 35.10 35.41 34.78 34.71 33.67 27. 27.41 35.66 1.28 35.57 27.98 35. 28.03 35.23 16.75 0.97 0.00 0. 0.38 1.54 34.45 0.38 0.00 1. 0.00 0.00 0.79 0.00 0.00 0. 0.00 0.00 0.00 128 34.45 Table 10. Performance of Llama-2-7B fine-tuned on mathematical reasoning with rank=128. 27. 34.99 31.59 33.44 29.84 31.64 35. 0.00 LoRA DoRA Init[AB] MiLoRA PiSSA LoRA DoRA Init[AB] MiLoRA PiSSA E.3.2. CODE GENERATION 26 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Methods Batch Size LoRA DoRA Init[AB] MiLoRA PiSSA 16 128 128 16 128 16 128 128 Learning Rate 2.00e-05 3.56e-05 6.32e-05 1.12e-04 2.00e-04 3.56e-04 6.32e-04 1.12e-03 2.00e-03 3.56e-03 27.20 24.50 28. 24.65 26.25 23.00 26.80 22.50 29. 27.45 29.40 25.60 28.45 26.95 27. 25.50 28.60 25.50 29.90 27.70 27. 27.05 28.90 27.25 30.30 25.55 27. 26.40 32.55 30.15 30.00 28.15 31. 29.65 29.90 29.15 28.75 27.20 31. 30.45 31.40 29.30 31.80 29.15 32. 30.50 31.60 29.55 34.05 31.45 33. 30.90 33.15 30.15 34.15 32.05 33. 29.30 30.75 30.10 36.10 30.45 33. 33.15 35.60 33.55 32.25 32.35 29. 33.55 32.65 35.35 32.00 33.15 32. 34.20 33.30 32.40 29.60 31.75 32. 33.35 29.50 33.35 31.20 33.25 29. 33.35 24.35 30.75 0.00 0.00 0. 32.55 0.00 0.00 0.00 0.00 0. 29.35 Table 11. Performance of Llama-2-7B fine-tuned on code generation with rank=8. Methods Batch Size 2.00e-05 3.56e-05 6.32e-05 1.12e-04 2.00e-04 3.56e-04 6.32e-04 1.12e-03 2.00e-03 Learning Rate LoRA DoRA Init[AB] MiLoRA PiSSA 128 16 128 16 128 128 16 28.30 27.60 29.00 26. 27.75 26.10 27.15 25.70 32.10 29. 27.25 29.10 27.15 29.70 27.00 28. 26.50 33.00 30.55 29.25 30.30 29. 31.10 27.75 30.55 26.90 34.00 35. 29.30 34.75 30.30 34.05 29.00 32. 28.90 32.10 37.05 30.85 34.20 31. 35.40 31.60 34.10 31.05 34.45 37. 34.55 36.40 35.25 35.20 35.70 35. 31.40 32.30 36.85 35.45 37.40 35. 33.05 37.00 36.10 33.30 27.75 28. 35.65 29.35 35.15 31.25 36.70 27. 34.75 18.55 0.00 32.20 0.00 0. 0.00 0.00 0.00 0.00 0.00 32.10 Table 12. Performance of Llama-2-7B fine-tuned on code generation tasks with rank = 32. 33.55 30.05 31.75 33.20 29. 32.60 35.00 0.00 Methods Batch Size 2.00e-05 3.56e-05 6.32e-05 1.12e-04 2.00e-04 3.56e-04 6.32e-04 1.12e-03 2.00eLearning Rate LoRA DoRA Init[AB] MiLoRA PiSSA 16 128 16 128 16 16 128 16 30.72 29.37 30. 29.12 30.73 28.32 29.72 28.67 35. 31.87 29.82 32.50 30.17 31.48 30. 32.18 29.20 37.35 34.23 31.18 34. 31.22 34.47 30.15 33.55 29.72 35. 37.55 33.40 36.70 32.80 36.75 32. 36.53 31.93 36.08 37.40 35.70 38. 36.12 38.07 35.83 37.08 34.67 33. 36.68 36.48 35.82 38.12 35.82 36. 35.95 37.42 28.43 29.20 36.68 30. 37.50 30.72 38.43 30.03 37.07 17. 0.00 13.05 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0. 128 35.42 Table 13. Performance of Llama-2-7B fine-tuned on code generation tasks with rank = 128. 36.67 26.90 34.50 34. 31.90 32.33 36.77 0.00 27 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning F. Example Model Responses We examine the responses of Gemma-3-1B fine-tuned on the mathematical reasoning task using LoRA and PiSSA (r = 128, = 16) under various learning rates. Figure 9 presents randomly selected testing example from the MATH dataset, with the corresponding model responses organized in Table 14. In this example, it is easily observed that the two PEFT methods operate in distinct effective learning rate regimes. Specifically, LoRA produces correct reasoning paths within the range of 2 104 to 6.32 104, whereas PiSSA achieves accurate results in slightly lower range of 6.32 105 to 2 104. Moreover, the LLM under PiSSA fine-tuning tends to continue responding reasonably at larger learning rates, as evident when η = 2 103 and 3.56 103, where the model produces answers of 6 and 4, while under LoRA fine-tuning, the model diverges to output repetitive gibberish. These qualitative findings correspond to our discussion in Sec. 4.3.1."
        },
        {
            "title": "Selected MATH Testing Set Input",
            "content": "Below is an instruction that describes task. Write response that appropriately completes the request. ### Instruction: There are eight boys and six girls who are members of the trumpet section in the band. From the members of the trumpet section, quintet is being formed. If the quintet must consist of three boys and two girls, how many quintets are possible? ### Response: Lets think step by step. Figure 9. An example input from the MATH dataset used for testing. Table 14. Qualitative comparison of LoRA and PiSSA reasoning paths across learning rates. LR (η) LoRA Response PiSSA Response 2!6! = 87 1.12 105 We are given that there are 8 boys and 6 girls in the trumpet section. To form quintet, we need to choose 3 boys and 2 girls. So, we need to choose 3 boys from the 8 boys and 2 girls from the 6 girls. This can be done in (cid:0)8 (cid:1) = 8! 2 The answer is: 16 21 = 16 ways. Incorrect 2.00 105 We are given that there are 8 boys and 6 girls in the trumpet section. To form quintet, we need to choose 3 boys and 2 girls. So, the number of quintets that can be formed is 8 6 = 8 . The answer is: 8 Incorrect 3.56 105 We have 8 boys and 6 girls, so there are total of 8 + 6 = 14 members in the trumpet section. To form quintet, we need to choose 3 boys and 2 girls from the 14 members. The number of quintets that can be formed is the number of ways to choose 3 boys and 2 girls from group of 14 members. Using the combination formula, this is (cid:0)14 3!11! = 141312 321 = 105 . 32 = 48 2 = 86 (cid:1) = 14! 3 The answer is: 105 We have 8 boys and 6 girls, so we need to choose 3 boys from the 8 boys and 2 girls from the 6 girls. This can be done in (cid:1) = 8! (cid:0)8 3!3! 6! 3 The answer is: 14 32 = 14 ways. 2!2! = 87 (cid:1) (cid:0)6 2 Incorrect We have 8 boys and 6 girls, so we need to choose 3 boys from the 8 boys and 2 girls from the 6 girls. This can be done in (cid:0)8 3 ways. Therefore, there are 28 quintets possible. The answer is: 28 3!3! = 87 32 = 28 (cid:1) = 8! (cid:1) (cid:0)6 2 Incorrect We have 8 boys and 6 girls in the trumpet section. To form quintet, we need to choose 3 boys and 2 girls from the 8 boys and 6 girls. This can be done in (cid:0)8 3 ways. Therefore, there are 28 possible quintets. The answer is: 28 3!3! = 87 32 = 28 (cid:1) = 8! (cid:1) (cid:0)6 2 Incorrect Incorrect 28 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning (cid:1) = 56. PiSSA Response We need to choose 3 boys from the 8 members of the trumpet section, and 2 girls from the 6 members of the trumpet section. The number of ways to choose 3 boys from 8 is (cid:0)8 3 The number of ways to choose 2 girls from 6 is (cid:0)6 2 Therefore, the total number of quintets is 56 15 = 840 . The answer is: 840 (cid:1) = 15. Correct (cid:1) = 56. We need to choose 3 boys from the 8 available boys, and 2 girls from the 6 available girls. The number of ways to choose 3 boys from 8 is (cid:0)8 3 The number of ways to choose 2 girls from 6 is (cid:0)6 2 Therefore, the total number of quintets is 56 15 = 840 . The answer is: 840 (cid:1) = 15. Correct (cid:1) = 56 ways. First, we choose 3 boys from the 8 available boys. This can be done in (cid:0)8 3 Next, we choose 2 girls from the 6 available girls. This can be done in (cid:0)6 2 Therefore, the total number of quintets is 56 15 = 840 . The answer is: 840 (cid:1) = 15 ways. Correct For the boys, we have 8 choices for the quintet. For the girls, we have 6 choices for the quintet. So, the total number of quintets is 8 6 = 48 . The answer is: 48 Incorrect There are 8 boys and 6 girls, so there are total of 8 + 6 = 14 members in the trumpet section. We need to choose 3 boys from the 8 boys, so there are 8 boys. We also need to choose 2 girls from the 6 girls, so there are 6 girls. Therefore, the total number of quintets is 8 + 3 + 6 = 17 . The answer is: 17 Incorrect LR (η) LoRA Response 6.32 105 We have 8 boys and 6 girls, so we need to choose 3 boys from the 8 boys and 2 girls from the 6 girls. This can be done in (cid:1) = 8! (cid:0)8 3!3! 6! 3 The answer is: 36 2!4! = 876 321 = 6 = 36 . (cid:1) (cid:0)6 2 Incorrect 1.12 104 We have 8 boys and 6 girls, so we need to choose 3 boys from the 8 boys and 2 girls from the 6 girls. This can be done in (cid:1) = 8! (cid:0)8 3 Therefore, there are 56 possible quintets. The answer is: 56 321 = 56 ways. 2!2! = 876 3!3! 6! (cid:1) (cid:0)6 2 (cid:1) = 15 ways. Incorrect 2.00 104 We need to choose 3 boys from the 8 available, and 2 girls from the 6 available. This can be done in (cid:0)8 (cid:1) = 56 ways. 3 Similarly, we need to choose 2 girls from the 6 available, and 3 boys from the 3 available. This can be done in (cid:0)6 2 Therefore, there are 56 15 = 840 possible quintets. The answer is: 840 Correct 3.56 104 We need to choose 3 boys from the 8 available, and 2 girls from the 6 available. The number of ways to choose 3 boys from 8 is (cid:0)8 3 The number of ways to choose 2 girls from 6 is (cid:0)6 2 Therefore, the total number of quintets is 56 15 = 840 . The answer is: (cid:1) = 15. (cid:1) = 56. Correct (cid:1) = 56. 6.32 104 We have 8 boys and 6 girls, so we need to choose 3 boys and 2 girls. The number of ways to choose 3 boys from 8 is (cid:0)8 3 The number of ways to choose 2 girls from 6 is (cid:0)6 2 Therefore, the total number of quintets is 56 15 = 840 . The answer is: 840 (cid:1) = 15. Correct 29 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning LR (η) LoRA Response PiSSA Response There are 8 boys and 6 girls in the band, so there are total of 8 + 6 = 14 members in the band. We want to find the number of quintets that are possible. Since the quintet must consist of three boys and two girls, we need to choose 3 boys out of the 14 members. This can be done in (cid:0)14 3 Therefore, there are 46 14 = 32 possible quintets. The answer is: 32 (cid:1) = 46 ways. Incorrect There are 8 boys and 6 girls, so there are total of 8 + 6 = 14 people in the band. There are 6 girls and 2 girls, so there are total of 6 + 2 = 8 people in the band. There are 8 quintet members in the band. Therefore, the number of quintets possible is 14 - 8 = 6. #### 6 The answer is: 6 Incorrect There are 8 boys and 6 girls, so there are total of 8 + 6 = 14 possible outcomes. Out of the 14 girls, 6 are girls, so there are 14 6 = 18 possible outcomes. Therefore, there are 18 14 = 4 possible outcomes. The answer is: Incorrect We know that the total amount of the water is 1. We also know that the total number of eggs is 1. The total number of boys who has not the number of students is 1. We can set up the equation as follows: Number of students who has not the pool + Number of students who has not the number of students who in the pool...(repetitive gibberish) Incorrect 1.12 103 From the members of the trumpet section, there are 8 boys and 6 girls. From the members of the quintet, we need to choose 3 boys from the 8 boys and 2 girls from the 6 girls. This can be done in (cid:0)8 3 ways. The answer is: 35 3!5! = 876 321 = (cid:1) = 8! Incorrect 2.00 103 Since the number of boys is x, we can set up the equation: Number of boys = Number of boys with the number of boys with the two boys with the two boys with the two boys with the two boys...(repetitive gibberish) 3.56 103 The total number of students is 1, so the number of people is 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...(repetitive gibberish) Incorrect Incorrect 6.32 103 The number of the first circle is 12, 10 = 10. The total number of the number of the number of the number of the number of the number of the number of the first two sides of the first two sides. The area of the first two sides is 10 + 1 = 120 = 100 The total number of the first two sides is 1000. The total number of the number of the first two sides is 1000, so the total number of the first 100000. The answer is 10000000000...(repetitive gibberish) Incorrect 30 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning G. Hessian Computation Details We select the Lanczos algorithm over the Power Iteration method for the presented eigenvalue problem, as the latter converges to eigenvalues in descending order of magnitude, whereas our focus is on probing the algebraically largest eigenvalue of the Hessian. Sec. G.1 explains implementation details of the Lanczos algorithm, and Sec. G.2 provides additional Hessian results. G.1. Lanczos Algorithm Implementation Details Our implementation is built upon several Hessian-related frameworks, such as PyHessian9 (Yao et al., 2020) and LLM-Hessian10 (Ilin, 2025), with several modifications to suit our custom scenario. Algorithm 1 summarizes our implementation of the Lanczos Algorithm for estimating λmax(H). We set the Lanczos iterations = 100 and tolerance ϵ = 5 103. At each Lanczos iteration step, the Hessian-Vector Product (HVP) is applied to calculate Hqk without explicitly forming (Algorithm 2). We strictly ensure that the loss is calculated identically to that in supervised fine-tuning, rendering the resulting curvature information meaningful. In particular, we ensure that (1) the input prompt (i.e., instruction or question) tokens are masked out from the loss calculation, and (2) the loss is averaged over each token instead of each sentence11. To ensure computational feasibility, subset of = 500 training samples from MetaMathQA is selected for loss calculation, and batch size of = 5 is utilized. Figure 10 validates that this sample size is sufficient for reliably estimating the Hessian of the downstream task. Due to the numerical instability of the Lanczos algorithm in finite precision arithmetic (Cahill et al., 2000), we use Float32 precision for both the base model and adapters and incorporate re-orthogonalization steps (Paige, 1970; Golub et al., 1972). Figure 10. Approximately 500 training samples are sufficient for stable Hessian estimation. The figure reports the estimated λmax for PiSSA on the first Query projection matrix of Qwen3-0.6B (r = 128). We track these estimates across varying sample sizes (N ) from the MetaMathQA dataset, using = 10k as the reference baseline. Results represent the mean and standard deviation over 5 randomly selected subsets for each . 9https://github.com/amirgholami/PyHessian 10https://github.com/vectozavr/llm-hessian 11This is the default way of calculating loss during LLM supervised fine-tuning; see https://github.com/huggingface/ transformers/issues/34510 31 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Algorithm 1 Estimating Maximum Eigenvalues of Hessian by Lanczos Iterations Input: LoRA parameters θ = {B0, A0}, downstream dataset D, iterations m, initial vector b, tolerance ϵ. Output: Approximation of the maximum eigenvalue λmax(H). Initialization: Set β0 = 0, q0 = 0, λprev = . Normalize initial vector: q1 = b/b2. Lanczos Iteration: for = 1 to do Compute Hessian-Vector Product: = HVP(θ, D, qk) Compute diagonal element of : αk = Orthogonalize (Gram-Schmidt): = βk1qk1 αkqk Compute off-diagonal element of : βk = v2 Convergence Check: Construct symmetric tridiagonal matrix Tk Rkk using α1:k, β1:k1. eig_vals torch.linalg.eigvalsh(Tk) λcurr = max(eig_vals) if λcurr λprev < ϵ then Return λcurr end if λprev λcurr if βk 0 then Return λcurr end if Normalize: qk+1 = v/βk end for Return λcurr // See Algorithm // The Krylov subspace is invariant Algorithm 2 Hessian-Vector Product (HVP) Calculation Input: LoRA parameters θ = {B0, A0}, downstream dataset D, vector v, sample size , batch size B. Output: The Hessian-Vector product Hv. Sampling: Sample subset of size from D. Initialization: Set accumulator = 0. Set total token counter Ctotal = 0. Batch Processing: for each mini-batch of size from do (x,y)B ℓ(f (x; θ), y) Count supervised tokens in batch: cB = CountTokens(B). Ctotal Ctotal + cB. Forward Pass for Loss Calculation: Compute sum of Cross-Entropy losses over all supervised tokens in B: Lbatch(θ) = (cid:80) Double Backward for HVP: = θLbatch = gv hB = θs Accumulate: + hB end for Normalize: u/Ctotal Return // torch.nn.CrossEntropyLoss(sum) // Implemented via torch.autograd.functional.vhp // Summing up batch-wise contributions // Average over total supervised tokens 32 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning G.2. Additional Hessian Results Figure 11 presents the detailed λmax values of the Query projection matrix for Qwen across Transformer layers, providing layer-wise breakdown of the results shown in Figure 6. We observe intriguing patterns where all methods tend to have high or low values at similar layer locations. For example, at layer 20, λmax = {4.7, 8.5, 8.3, 53.8} for LoRA, Init[AB], MiLoRA, and PiSSA, respectively; whereas at layer 26, the values drop to {0.2, 0.7, 0.7, 2.3}. However, at any given layer, PiSSA consistently exhibits larger λmax compared to LoRA, by around an order of magnitude. Similar trends for other matrix types, i.e., the Key, Value, Gate, Up, and Down projection matrices, are presented in Figures 1217. Figure 11. Heatmap of the top eigenvalues of the Query projection matrix across Transformer layers, i.e., λQ,i max for = 1, . . . , L, for Qwen3-0.6B on mathematical reasoning task (r = 128). All methods exhibit similar distributional patterns, with PiSSA consistently exhibiting significantly larger values compared to vanilla LoRA. Figure 12. Heatmap of the top eigenvalues of the Key projection matrix across Transformer layers, i.e., λK,i Qwen3-0.6B on mathematical reasoning task (r = 128). max for = 1, . . . , L, for Figure 13. Heatmap of the top eigenvalues of the Value projection matrix across Transformer layers, i.e., λV,i Qwen3-0.6B on mathematical reasoning task (r = 128). max for = 1, . . . , L, for 33 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning Figure 14. Heatmap of the top eigenvalues of the Output projection matrix across Transformer layers, i.e., λO,i Qwen3-0.6B on mathematical reasoning task (r = 128). max for = 1, . . . , L, for Figure 15. Heatmap of the top eigenvalues of the Gate projection matrix across Transformer layers, i.e., λGate,i Qwen3-0.6B on mathematical reasoning task (r = 128). max for = 1, . . . , L, for Figure 16. Heatmap of the top eigenvalues of the Up projection matrix across Transformer layers, i.e., λUp,i Qwen3-0.6B on mathematical reasoning task (r = 128). max for = 1, . . . , L, for Figure 17. Heatmap of the top eigenvalues of the Down projection matrix across Transformer layers, i.e., λDown,i Qwen3-0.6B on mathematical reasoning task (r = 128). max for = 1, . . . , L, for 34 Learning Rate Matters: Vanilla LoRA May Suffice for LLM Fine-tuning H. Limitations and Future Work In this paper, we focused our investigation on decoder-only LLMs up to the 7B parameter scale and restricted our evaluation to mathematical and code generation tasks. Hence, the scalability of our findings to larger foundation models and their generalization to diverse downstream tasks remain to be verified. Additionally, the computational costs required for hyperparameter searches precluded an exhaustive search over all training configurations. While key hyperparameters (learning rate, batch size, and rank) were tuned, other secondary training setups, such as learning rate schedulers, warmup steps, and LoRA adapter placements, remained fixed. It is possible that fine-grained tuning of these configurations could yield further performance gains or distinct convergence behaviors. We also highlight that our findings may not extend to untested model architectures (e.g., encoder-only LLMs (Devlin et al., 2019), Vision Transformers (Dosovitskiy, 2020), and Vision-Language Models (Alayrac et al., 2022)) or to all existing advanced LoRA variants. For instance, several methods have originally reported higher peak performance than LoRA under comprehensive learning rate sweepssuch as LoRAOne (Zhang et al., 2025g), which initializes adapters via the SVD of the one-step full gradient, with 2% performance improvement on Llama (cf. Zhang et al. (2025g) Table 3). Moreover, fine-tuned accuracy on standard benchmarks is not the sole criterion for evaluating PEFT algorithms; specific variants may offer distinct advantages in other dimensions, e.g., mitigating catastrophic forgetting of pretrained knowledge (Biderman et al., 2024; Wang et al., 2024b; Yang et al., 2024a; Zhang et al., 2025c; Xiong & Xie, 2025). While the landscape of LoRA variants continues to expand, our results suggest that vanilla LoRA already suffices as competitive baseline, potentially indicating that weight-based low-rank adaptation strategies may be approaching saturation. Looking ahead, we posit that further investigation into alternative adaptation mechanisms may unlock new dimensions of efficiency. Examples of such mechanisms include hidden representation fine-tuning (Wu et al., 2024; Yin et al., 2024) and approaches that adapt non-linear functions within layers (Yin et al., 2025). We leave the exploration of these orthogonal paradigms as future work."
        }
    ],
    "affiliations": [
        "Data Science Degree Program, National Taiwan University and Academia Sinica, Taipei, Taiwan",
        "IBM Research, New York, United States"
    ]
}