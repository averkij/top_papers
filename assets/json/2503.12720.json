{
    "paper_title": "GenStereo: Towards Open-World Generation of Stereo Images and Unsupervised Matching",
    "authors": [
        "Feng Qiao",
        "Zhexiao Xiong",
        "Eric Xing",
        "Nathan Jacobs"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Stereo images are fundamental to numerous applications, including extended reality (XR) devices, autonomous driving, and robotics. Unfortunately, acquiring high-quality stereo images remains challenging due to the precise calibration requirements of dual-camera setups and the complexity of obtaining accurate, dense disparity maps. Existing stereo image generation methods typically focus on either visual quality for viewing or geometric accuracy for matching, but not both. We introduce GenStereo, a diffusion-based approach, to bridge this gap. The method includes two primary innovations (1) conditioning the diffusion process on a disparity-aware coordinate embedding and a warped input image, allowing for more precise stereo alignment than previous methods, and (2) an adaptive fusion mechanism that intelligently combines the diffusion-generated image with a warped image, improving both realism and disparity consistency. Through extensive training on 11 diverse stereo datasets, GenStereo demonstrates strong generalization ability. GenStereo achieves state-of-the-art performance in both stereo image generation and unsupervised stereo matching tasks. Our framework eliminates the need for complex hardware setups while enabling high-quality stereo image generation, making it valuable for both real-world applications and unsupervised learning scenarios. Project page is available at https://qjizhi.github.io/genstereo"
        },
        {
            "title": "Start",
            "content": "GenStereo: Towards Open-World Generation of Stereo Images and Unsupervised Matching Feng Qiao, Zhexiao Xiong, Eric Xing, Nathan Jacobs Washington University in St. Louis 5 2 0 2 7 1 ] . [ 1 0 2 7 2 1 . 3 0 5 2 : r Figure 1. Comparison with diffusion-based methods for stereo image generation on the COCO dataset."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Stereo images are fundamental to numerous applications, including extended reality (XR) devices, autonomous driving, and robotics. Unfortunately, acquiring highquality stereo images remains challenging due to the precise calibration requirements of dual-camera setups and the complexity of obtaining accurate, dense disparity maps. Existing stereo image generation methods typically focus on either visual quality for viewing or geometric accuracy for matching, but not both. We introduce GenStereo, diffusion-based approach, to bridge this gap. The method includes two primary innovations (1) conditioning the diffusion process on disparity-aware coordinate embedding and warped input image, allowing for more precise stereo alignment than previous methods, and (2) an adaptive fusion mechanism that intelligently combines the diffusiongenerated image with warped image, improving both realism and disparity consistency. Through extensive training on 11 diverse stereo datasets, GenStereo demonstrates strong generalization ability. GenStereo achieves state-ofthe-art performance in both stereo image generation and unsupervised stereo matching tasks. Our framework eliminates the need for complex hardware setups while enabling high-quality stereo image generation, making it valuable for both real-world applications and unsupervised learning scenarios. Project page is available at URL. The demand for stereo images continues growing with XR devices, autonomous driving, and robotics. However, acquiring high-quality stereo images remains challenging due to complex camera calibration and environmental constraints, limiting the development of robust stereo matching models that generalize well across diverse scenarios. Real-world datasets often provide only sparse disparity annotations [26] or lack accurate ground truth altogether [8]. Although some datasets offer dense, accurate disparity maps [2, 31], they are typically limited to specific scenarios like indoor scenes. Moreover, capturing realworld stereo data requires complex sensor setups with precise calibration, often constraining baseline distances and scene diversity. Synthetic datasets, while providing precise disparity maps, suffer from domain gaps compared to realworld scenarios [5, 54, 55]. Recent advancements in monocular depth estimation (MDE) models [3, 32, 61] have provided increasingly accurate dense disparity maps, playing crucial role in image generation from single views. Meanwhile, Text-to-Image (T2I) diffusion models, such as Stable Diffusion [33], have made remarkable strides in generating diverse and highquality images from user-provided text prompts. However, these models often struggle to maintain spatial consistency when altering the viewpoint of the generated images. While there has been some research on 3D novel view generation [36, 51], there remains gap in utilizing image gener1 ation models to directly produce stereo images. StereoDiffusion [53] is the first approach to leverage diffusion-based generation for stereo image generation, but it struggles with pixel-level accuracy, as it applies disparity shifts in the latent space and fills occluded regions with blurry pixels that lack meaningful semantic content. On the other hand, Stable Diffusion Inpainting (SD-Inpainting) [33], though not designed for stereo image generation, faces similar challenges by incorrectly filling occluded areas with semanFig. 1 shows that previous tically inappropriate pixels. diffusion-based methods and our methods tested on the COCO dataset [22]. The scale factor γ is set to 0.15, meaning that the maximum disparity is 15% of the image width. We present GenStereo, novel framework that addresses both visual quality and geometric accuracy in stereo image generation. Existing approaches rely on either warpingbased methods [58], which provide geometric accuracy but struggle with semantic consistency in occluded regions, or diffusion-based approaches [53], which maintain better semantic coherence but lack precise geometric control. GenStereo bridges this gap through carefully designed twostream architecture inspired by recent advances in human animation [14] and view synthesis [36], achieving both geometric precision and semantic consistency. Our framework employs multi-level constraint system for geometric precision and semantic coherence, starting with disparity-aware coordinate embedding that provides implicit geometric guidance. This is followed by crossview attention mechanism that enables semantic feature alignment between views, ensuring consistency in challenging areas like occlusions and complex textures. The framework leverages dual-space supervision strategy that operates in both latent and pixel spaces, with an adaptive fusion mechanism that further optimizes pixel-level accuracy. To ensure robust generalization across diverse real-world scenarios, we train our model on diverse dataset combining 11 stereo datasets with varying scenes and baselines. Furthermore, we demonstrate how our high-quality stereo generation framework significantly improves unsupervised stereo matching learning. Previous approaches to unsupervised learning have been limited by either simplified warping and random background filling [58] or conIn contrast, our straints to small-scale static scenes [48]. method enables large-scale training with diverse, photorealistic stereo images that maintain both geometric accuracy and semantic consistency. This advancement represents significant step toward bridging the gap between supervised and unsupervised stereo matching approaches. Our contributions can be summarized as follows: We propose GenStereo, the first unified framework for open-world stereo image generation that addresses visual quality and geometric accuracy, enabling both practical applications and unsupervised stereo matching. We introduce comprehensive multi-level constraint system that combines: (1) disparity-aware coordinate embedding with warped image conditioning for geometric guidance, (2) cross-view attention mechanism for semantic feature alignment, and (3) dual-space supervision with adaptive fusion for pixel-accuracy generation. Extensive experimental validation demonstrating stateof-the-art performance in both stereo image generation and unsupervised stereo matching. 2. Related Work 2.1. Conditional Diffusion Models Diffusion models began with DDPM [13], advancing from slow, probabilistic generation to faster sampling with DDIM [39]. Stable Diffusion [33], pioneering T2I model, further improved efficiency by performing diffusion in compact latent space, combining it with CLIP-based [30] text conditioning for versatile T2I generation. However, Stable Diffusion lacks fine-grained control over specific visual attributes, relying mainly on text prompts without structured guidance. To address these limitations, conditional diffusion models have emerged. ControlNet [62] is key advancement, introducing conditioning mechanisms for structural inputs (such as edges, depth maps, or poses) that maintain flexibility while enabling precise, user-driven synthesis. However, ControlNet still faces challenges in achieving pixel-level accuracy in complex images, such as stereo image generation. Recent research [14, 18, 36, 37, 60] has explored the self-attention properties within T2I models. Text2VideoZero [18] and MVDream [37] generate consistent visuals across video frames or 3D multi-views by sharing self-attention, while Animate-Anyone [14] and MagicAnimate [60] apply similar approach to produce human dance videos through fine-tuned T2I models. GenWarp [36] employs augmenting self-attention with cross-view attention between the reference and target views to generate novel images. This two-stream architecture augments the features of the denoising net. Inspired by the adaptability and control of these self-attention-based architectures, our method builds on their strengths. However, their direct application to stereo image generation faces fundamental challenges. First, they typically dont incorporate disparity information, which is crucial for stereoscopic consistency. Second, achieving the pixel-level accuracy required for comfortable stereo viewing demands specialized architectural considerations beyond existing frameworks. 2.2. Stereo Image Generation and Inpainting Traditional view synthesis approaches, including geometrybased reconstruction [11, 12, 19] and recent NeRF-based methods [48], excel at rendering novel views but are constrained by their requirement for multiple input images of static scenes. While 3D Photography techniques [11, 38] attempt to overcome this limitation through depth-based mesh projection, they often struggle with complex occlusion handling and require sophisticated post-processing pipelines. The evolution of stereo generation methods has progressed from simple warping-based approaches to more sophisticated diffusion-based solutions. MfS [58] introduced basic framework that attempted to handle occlusions by sampling random background patches from the dataset, but this led to semantic inconsistencies and visible artifacts at occlusion areas. While SD-Inpainting [33] introduced more coherent semantic content through its prior learning, it failed to maintain local consistency between inpainted regions and their surrounding context, often producing visible discontinuities in texture and structure. Mono2Stereo [57] made progress by fine-tuning SD-Inpainting specifically for stereo generation, enabling unsupervised stereo matching, though visible artifacts persist in in-painted regions. StereoDiffusion [53] marked significant shift by introducing end-to-end diffusion-based stereo image generation, but its latent-space warping approach without explicit geometric constraints led to compromised pixel-level accuracy. Despite these advances, the challenge of generating photorealistic stereo images that maintain both visual quality and geometric fidelity remains largely unsolved, particularly for diverse real-world scenarios. 2.3. Unsupervised Stereo Matching Traditional unsupervised stereo matching approaches primarily rely on photometric consistency. Methods like [10, 45, 46, 63] leverage photometric losses across stereo images, while others [7, 20, 56] extend this to temporal sequences. parallel line of research explores proxy supervision strategies, either through carefully designed algorithmic supervisors [29, 43, 44] or knowledge distillation from pre-trained networks [1]. Recent domain adaptation approaches [23, 40, 59] attempt to bridge the gap between synthetic and real-world domains or facilitate cross-dataset adaptation. However, these methods often exhibit limited generalization capability beyond their target domains [1], particularly struggling with diverse real-world scenarios. More recent approaches have explored novel directions for stereo generation and matching. MfS [58] pioneered unsupervised stereo matching using MDE as guidance, but it relies on random background sampling for occlusion filling, leading to semantic inconsistencies. While NeRFStereo [48] achieves high-quality results through neural radiance fields, it requires multiple views of static scenes, limiting its practical applications. Mono2Stereo [57] leverages Stable Diffusion for inpainting occluded regions, representing significant advance in generative stereo synthesis, though it still exhibits artifacts in occluded areas where geometric and semantic consistency is crucial. 3. Methods Given left view image Il and its corresponding disparity map ˆDl predicted by an MDE model, our goal is to generate high-quality right view image ˆIr that maintains both visual quality and geometric consistency. As shown in Fig. 2, our GenStereo framework processes the left view as the reference image and synthesizes the right view as the target image. The generated triplet Il, ˆDl, ˆIr not only serves as stereo pair for visualization but also provides training data for unsupervised stereo matching. 3.1. Disparity-Aware Coordinate Embedding Traditional inpainting-based stereo image generation methods often suffer from visible boundaries between warped and inpainted regions. To address this limitation, we propose disparity-aware coordinate embedding scheme inspired by recent advances in coordinate-based generation [27, 36]. Our approach utilizes dual coordinate embeddings: canonical embedding for the left view and its warped counterpart for the right view. Specifically, we first construct canonical 2D coordinate map Rhw2 with values normalized to [1, 1]. This map is transformed into Fourier features [41] through positional encoding function ϕ: Cl = ϕ(X) (1) The resulting Fourier feature map Cl serves as the coordinate embedding for the left view Il. We then generate the right view embedding by warping Cl according to the disparity map: Cr = warp(Cl, Dl), (2) where Dl represents ground-truth disparity during training and predicted disparity ˆDl during inference. These coordinate embeddings (Cl and Cr) are integrated into their respective view features (Fl and Fr) through convolutional layers, establishing strong geometric correspondence while maintaining visual consistency between views. Our approach differs from GenWarp [36] by utilizing disparity maps instead of camera matrices for warping, enabling more precise pixel-level control. We further enhance geometric consistency by incorporating the warped image Iwarp as additional conditioning for the denoising U-Net. 3.2. Cross-View Feature Enhancement To facilitate effective information exchange between views, we adopt two-parallel U-Nets framework pretrained from Stable Diffusion, inspired by GenWarp [36]. However, our work differs in its novel conditioning strategy: the reference U-Net takes concatenated (Il, Cl) as conditions to process the left view, while the denoising U-Net takes concatenated 3 Figure 2. Overview of the GenStereo framework. + denotes feature concatenation. (Iwarp, Cr) as conditions when synthesizing the right view ˆIr. This design ensures that each U-Net receives both image content and corresponding coordinate information for better feature extraction. To incorporate left-view information during right-view generation, we concatenate the reference features with the target features in the attention mechanism. Specifically, we compute cross-view attention as: = Fr, = [Fl, Fr], = [Fl, Fr]. (3) where Fl is derived from the reference U-Net conditioned on (Il, Cl), while Fr comes from the denoising U-Net conditioned on (Iwarp, Cr). This dual-stream attention mechanism allows the model to adaptively balance between semantic consistency from the reference view and geometric accuracy from the warped view, with each stream guided by both image content and coordinate information. 3.3. Training Strategy Mixed training [21, 32, 61] has been proven to be an effective training strategy for domain generalization in both monocular and stereo matching depth estimation. We follow the datasets summarized in [49] and utilize the most widely available public synthetic datasets. Based on our observations, real-world datasets can negatively impact performance, even when there are only minor calibration errors or slight differences in imaging between the two cameras. Tab. 1 shows the comprehensive list of datasets utilized. Datasets containing abstract images are not included, because our model aims to have good generalization capability in the real world. To address dataset imbalance, we employ resampling strategy. Specifically, we sample the smaller datasets multiple times until the number reaches 10% of the largest dataset. As the Stable Diffusion model only accepts images of size 512 512, we apply random square crop and resize them to 512 512. Table 1. Our training data sources. All datasets are synthetic with the size of 684K."
        },
        {
            "title": "Outdoor",
            "content": "#Images TartanAir [55] Dynamic Replica [17] IRS [54] Falling Things [50] VKITTI2 [5] InfinigenSV [15] SimStereo [16] UnrealStereo4K [47] Spring [25] PLT-D3 [42] Sintel [4] 306K 145K 103K 61K 21K 17K 14K 8K 5K 3K 1K"
        },
        {
            "title": "Year",
            "content": "2020 2023 2021 2018 2020 2024 2022 2021 2023 2024 2012 3.4. Pixel Space Alignment Given an RGB image RHW 3, Latent Diffusion Models (LDMs) first encode it into latent representation = E(I), where Rhwc. While this latent space operation significantly reduces computational costs, it may compromise pixel-level accuracy during image generation. The standard LDM objective operates in latent space: Llatent := Ez,ϵ,t (cid:2)ϵ ϵθ(zt, t)2 2 (cid:3) (4) where zt is the noisy latent at timestep t, ϵ is the noise added to create zt, and ϵθ(zt, t) is the networks prediction of this noise. The model learns to denoise by predicting the noise that was added. To maintain pixel-level accuracy, we introduce an additional pixel-space loss by decoding both the predicted and target latent variables: Lpixel = Ez,ϵ,t (cid:2)D(zpred) D(ztarget)2 (cid:3) The final objective combines both spaces: = Llatent + αLpixel (5) (6) where α = 1 balances latent and pixel-space supervision. 4 3.5. Adaptive Fusion Module To achieve seamless integration between generated and warped content, we propose an Adaptive Fusion Module that learns to combine Igen and Iwarp based on local context and confidence. The module predicts spatially-varying fusion weights through lightweight network: = σ (fθ (concat(Igen, Iwarp, ))) , (7) where fθ is 3 3 convolutional layer and σ is the sigmoid activation, ensuring weights in [0, 1]. The final right view is computed as: ˆIr = Iwarp + (1 ) Igen, (8) where denotes element-wise multiplication. This formulation adaptively favors warped content in high-confidence regions (M 1) while relying on generated content in occluded or uncertain areas. The learned weights enable smooth transitions between warped and generated regions, ensuring both geometric accuracy and visual consistency. 3.6. Random Disparity Dropout To simulate the sparsity of real-world disparity maps like KITTIs LiDAR-derived ground truth, we randomly apply disparity dropout to 10% of training samples. For each selected sample, we first generate dropout ratio: Uniform(0, 1) (9) Using this ratio, we create binary mask where each pixel has probability of being dropped: Mrand(i, j) = (cid:40) otherwise 1 0 with probability (10) The final mask combines this random dropout with the warping mask: = Mwarp Mrand (11) where denotes element-wise logical OR. This strategy encourages the model to handle sparse disparity inputs, improving its robustness and generalization to real-world scenarios where dense disparity maps may not be available. 4. Experiments 4.1. Stereo Image Generation Experimental Setup. We fine-tune the pretrained Stable Diffusion UNet v1.5 for 3 epochs across all experiments. For evaluation, we utilize two widely adopted stereo vision benchmarks: Middlebury 2014 [34] and KITTI 2015 [26], neither of which are included in our training set. Middlebury 2014 comprises 23 high-resolution indoor stereo pairs captured with wide-baseline stereo cameras under varying illumination conditions, providing challenging scenarios for stereo generation. KITTI 2015 contains 200 outdoor stereo pairs of street scenes with LiDAR-based sparse disparity ground truth, offering diverse real-world testing scenarios. Following StereoDiffusion [53], we preprocess Middlebury images to 512 512 resolution, while KITTI images are center-cropped before resizing to 512 512 to maintain the most informative regions. We conduct comprehensive comparisons against both traditional and learning-based baselines. Traditional approaches include naıve solutions such as leave blank and image stretching. For learning-based comparisons, we evaluate against state-of-the-art diffusion models adapted for stereo generation: Repaint [24] and SD-Inpainting [33], as well as StereoDiffusion, which specifically targets stereo image generation. To quantitatively assess the quality of generated right-view images ( ˆIr) against ground truth (Ir), we employ three complementary metrics: Peak Signal-toNoise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS) with SqueezeNet backbone. Quantitative Results. Tab. 2 presents the generation performance on Middlebury 2014 and KITTI 2015. The sparsity of ground truth disparities in KITTI limits evaluation accuracy, making pseudo-label disparities more effective for assessing stereo consistency. This sparsity particularly affects inpainting-based methods, as they struggle to infer accurate right-view images in areas with missing depth information, especially for dynamic objects and distant regions. In contrast, Middlebury 2014, which provides dense ground truth disparities, demonstrates smaller performance gap between using ground truth and using pseudo disparities, underscoring the advantage of high-quality disparity annotations for generation. We obtain pseudo disparities from CREStereo [21] pretrained model, where evaluation datasets are not used for training to ensure unbiased evaluation. GenStereo enables the construction of stereo datasets from single camera, utilizing either sparse disparities or pseudo disparities. This capability expands the applicability of stereo generation to scenarios with limited sensor setups, making our approach more versatile in real-world applications where dual-camera systems may be impractical or cost-prohibitive. Qualitative Results. We present qualitative comparisons leave blank, between our method and several baselines: StereoDiffusion, and SD-Inpainting, with ground truth as reference. As shown in Fig. 3, our method demonstrates superior performance on the Middlebury 2014 dataset. While SD-Inpainting struggles with occlusion handling, often generating semantically inconsistent content in occluded regions, and StereoDiffusion exhibits loss of fine details due 5 Table 2. Quantitative results of image generation for Middlebury 2014 and KITTI 2015 datasets. The top three results for each metric are highlighted with first , second , and third background, respectively. Methods Leave blank Stretch 3D Photography [38] RePaint [24] SD-Inpainting [33] StereoDiffusion [53] Ours + GT Ours + Pseudo Middlebury 2014 KITTI PSNR SSIM LPIPS PSNR SSIM LPIPS 11.328+2.483 3.489 14.842+2.714 2.753 14.190+2.464 2.798 15.102+2.909 2.802 15.740 +3.511 3.351 15.456+2.669 3.313 23.699 +5.456 6.033 23.835 +6.842 6.741 0.315+0.230 0.154 0.4320.190 +0.265 0.427+0.238 0.175 0.462+0.268 0.253 0.412+0.236 0.188 0.468 +0.252 0.205 0.866 +0.084 0.141 0.868 +0.096 0.157 0.450+0.097 0.089 0.285+0.112 0.089 0.275+0.073 0.065 0.311+0.088 0.079 0.311+0.098 0.164 0.231 +0.096 0.088 0.064 +0.053 0.034 0.062 +0.063 0.038 12.980+4.919 3.831 14.757+5.375 4.694 14.540+7.256 4.023 15.056+5.366 4.897 9.792+2.809 4.070 15.679 +5.888 5.487 20.439 +7.518 8.541 22.749 +6.722 6.102 0.374+0.286 0.251 0.429+0.287 0.271 0.398+0.270 0.323 0.462+0.268 0.285 0.230+0.236 0.229 0.481 +0.245 0.310 0.746 +0.155 0.274 0.813 +0.110 0.152 0.313+0.222 0.109 0.212+0.145 0.100 0.210+0.099 0.073 0.251+0.128 0.095 0.652+0.144 0.139 0.205 +0.135 0.099 0.108 +0.117 0.048 0.096 +0.068 0. to latent-space warping, our approach successfully maintains both geometric accuracy and visual fidelity. Particularly noteworthy is our methods ability to generate coherent right-view images while preserving pixel-level correspondence with the left images. Additional qualitative results on the KITTI 2015 dataset and more visualizations on other datasets can be found in the supplementary materials. Table 3. Comparison with other diffusion-based stereo image generation methods for unsupervised stereo matching. The first group of methods is trained on PSMNet while the second group is trained on RAFT-Stereo."
        },
        {
            "title": "Methods",
            "content": "KITTI 2012 KITTI 2015 D1-all EPE D1-all EPE Leave blank SD-Inpainting [33] StereoDiffusion [53] Ours Leave blank SD-Inpainting [33] StereoDiffusion [53] Ours 25.219 3.907 15.213 3.802 4.184 4.016 12.756 3.537 3.542 0.894 2.220 0.815 0.952 0.945 1.811 0. 24.568 4.490 5.651 3.933 4.074 4.128 6.360 3.653 2.937 1.059 1.154 0.991 0.957 0.972 1.174 0.943 4.2. Unsupervised Learning Experimental Setup. We conduct two comprehensive experiments to validate the effectiveness of our proposed generation method for unsupervised stereo matching. In the first experiment, we fine-tune stereo matching model pretrained on the SceneFlow dataset using the KITTI 2012 and KITTI 2015 benchmarks. We split the training and evaluation sets into 160/34 images for KITTI 2012 and 160/40 images for KITTI 2015, following [58]. The second experiment evaluates various unsupervised stereo matching approaches by generating stereo images. Specifically, KITTI 2012 [9] has 194 stereo images, KITTI 2015 has 200 stereo images, ETH3D [35] has 27 stereo pairs, and the Middlebury v3 training set [34] has 15 stereo pairs at Quarter and Half resolutions (Midd-Q, Midd-H). This diverse dataset selection allows us to comprehensively assess the generalization capability of our method across different domains and resolutions. We evaluate the performance of various unsupervised stereo matching approaches using the D1-all metric, which measures the percentage of incorrect disparity predictions (error > 3px or 5% of the true disparity), the End-PointError (EPE), and the >npx metric, which denotes the percentage of pixels with an EPE exceeding pixels. To demonstrate the architecture-agnostic nature of our approach, we employ two distinct stereo models: PSMNet, cost-volume-based method, and RAFT-Stereo, an iterative refinement model. For disparity estimation, we utilize the Depth Anything Model v2 (DAMv2) [61], from which we obtain the disparity map. The disparity values are normalized to the range [0, 1] and subsequently scaled by factor γ to enable flexibility generation. Fine-tuning on KITTI. To ensure fair and consistent comparisons across all generation methods, we construct the training datasets using standardized procedure. Specifically, we sample the scale factor γ from the set {0.05, 0.1, 0.15, 0.2, 0.25}. For each selected scale, we employ various generation methods to create the right images based on 160 left images and their corresponding scaled disparity maps for both KITTI 2012 and KITTI 2015. This procedure results in total of 800 stereo pairs. As shown in Tab. 3, higher generation quality leads to improved stereo matching performance. Among the compared methods, SD-Inpainting achieves the closest performance to ours due to its pixel-level warping operation. However, it falls short in providing semantic consistency in occluded areas, which limits its overall effectiveness. Unsupervised Generalization. Following the methodology of constructing the Mono for Stereo (MfS) dataset in [58], we construct our training dataset, MfS-GenStereo, from the same datasets, including ADE20K [64], Mapillary Vistas [28], DIODE [52], Depth in the Wild [6], and COCO 2017 [22]. During the generation process, we randomly select scale factor in the range [0, 0.35] and set the maximum disparity to 256 for PSMNet. As shown in Tab. 4, our method enables unsupervised stereo matching using only left images, achieving superior performance. Figure 3. Qualitative comparison on Middlebury 2014 with ground truth disparity maps. Table 4. Zero-Shot Generalization Benchmark. The first group shows methods trained with PSMNet, while the second group shows methods trained with RAFT-Stereo."
        },
        {
            "title": "Methods",
            "content": "KITTI 2012 KITTI 2015 D1-all EPE >3px D1-all EPE >3px D1-all MfS [58] MfS+DAMv2 [61] Mono2Stereo [57] NeRFStereo [48] Ours NeRFStereo [48] Ours 4.322 4.931 4.077 3.683 3.676 3.787 3.224 1.009 1.063 0.859 0.851 0. 0.855 0.734 4.703 5.313 - 4.507 4.063 4.140 3.455 4.963 5.504 4.507 4.813 3.879 5.356 4.683 1.636 1.401 1.052 1.448 0. 1.469 1.042 5.181 5.737 - 5.041 4.112 5.561 4.897 2.204 1.990 1.531 1.620 1.114 0.830 0."
        },
        {
            "title": "EPE",
            "content": "0.535 0.493 0.397 0.557 0.378 0.301 0.294 Middlebury-Q Middlebury-H >1px D1-all"
        },
        {
            "title": "EPE",
            "content": ">2px D1-all"
        },
        {
            "title": "EPE",
            "content": ">2px 8.170 8.145 - 11.682 5.556 3.370 2.667 9.217 9.447 - 7.619 6.887 5.819 3.450 1.648 1.825 - 1.451 1. 1.003 1.003 12.066 11.902 - 11.040 10.434 8.370 7.952 12.338 12.344 - 8.892 11.334 7.426 4.446 3.242 3.678 - 2.590 2. 1.820 1.514 17.559 17.209 - 12.884 19.387 10.948 9.938 We compare our approach with the original MfS [58] and its enhanced variant using the Depth Anything Model v2 (DAMv2), which generates the right view images by filling the occluded areas with random background patches from the dataset. The experiments indicate that, despite the better depth prior, the synthesis of right-view images remains critical bottleneck in this methodology. Additionally, we compare with Mono2Stereo, which fine-tunes SD-Inpainting for right-view image generation, and NeRFStereo, which generates stereo images by reconstructing the static scenes using Neural Radiance Fields (NeRF), limiting the diversity of its datasets. Our experimental results indicate that our proposed method consistently surpasses these existing approaches in generating high-quality stereo pairs with pixel-level accuracy, demonstrating robust generalization capabilities and significantly enhancing stereo matching performance across diverse datasets. Table 5. Ablation studies. Source # Mixed Datasets Random Drop Coord Embedding Warped Image Pixel Loss Fusion Pseudo Disparities Ground Truth (1) (2) (3) (4) (5) (6) (7) (8) Middlebury 2014 KITTI 2015 PSNR SSIM LPIPS PSNR SSIM LPIPS 23.552 23.699 20.037 22.033 22.697 23.407 23.811 23.835 0.862 0.866 0.745 0.796 0.832 0.855 0.867 0.868 0.0641 0. 0.1214 0.0731 0.0715 0.0644 0.0621 0.0620 16.164 20.439 21.085 21.866 22.313 22.520 22.716 22.749 0.664 0.746 0.777 0.779 0.798 0.805 0.811 0.813 0.1740 0. 0.1113 0.1002 0.1011 0.0974 0.0961 0.0958 4.3. Ablation Study. Random disparity drop. Since the ground-truth disparity maps in real-world datasets (e.g., KITTI) are often derived from LiDAR, resulting in sparse and incomplete point clouds, our random disparity dropout strategy mitigates the models reliance on dense disparity supervision. This enhancement allows the model to generate plausible right images even when only sparse disparity maps are available. This strategy significantly improves performance on datasets with sparse ground-truth annotations, demonstrating better generalization to real-world scenarios. As shown in Tab. 5, comparing rows (1) and (2) highlights the effectiveness of this strategy. Mixed datasets. We train our model on Virtual KITTI 2 (VKITTI2) [5] and combination of mixed datasets, and subsequently evaluate both models. When trained exclusively on VKITTI2, the model achieves comparable performance to the mixed-dataset model on KITTI 2015, owing to the relatively small domain gap between VKITTI2 and KITTI 2015. However, this single-dataset training results in notable performance degradation on other datasets, indicating limited generalization. As shown in Tab. 5, comparing rows (3) and (8) demonstrates that our mixed dataset training strategy significantly improves the models generalization across diverse domains. Coordinate embedding and the warped image. Incorporating the coordinate embedding process effectively encodes spatial information and reinforces the structural relationship between stereo views. Additionally, using the warped image as condition enhances performance by providing pixel-level guidance. The warped image serves as coarse prediction of the right view, helping the model focus on correcting local inconsistencies rather than synthesizing the entire image from scratch. As shown in rows (4), (5), and (6) in Tab. 5, the combination of coordinate embedding and warped image significantly improves generation performance, preserving geometric consistency and reducing artifacts like stitching boundaries. While warped image embedding contributes most to performance, its effectiveness decreases with additional fine-tuning iterations, revealing instability when using warped image alone as conditioning signal. This underscores the importance of combining coordinate embedding and warped image to achieve stable, high-quality stereo image generation. Pixel-level loss. By decoding the latent representations back to the image space, the model learns to minimize discrepancies directly in the observed image space, enforcing pixel-level alignment. As illustrated in rows (6) and (7), the introduction of the pixel-level loss significantly improves the generation performance, highlighting its role in maintaining high-fidelity stereo image generation. Adaptive Fusion. The proposed adaptive fusion module significantly enhances visual quality by dynamically weighting the contributions of both the generated and warped images. comparison of rows (7) and (8) demonstrates that this fusion strategy further refines pixel alignment, resulting in sharper, more structurally coherent, and perceptually accurate reconstructions. The improvement in PSNR is more pronounced than that in SSIM and LPIPS, indicating that the adaptive fusion is particularly effective for pixel-level alignment. 5. Conclusion We presented GenStereo, novel diffusion-based framework for open-world stereo image generation with applications in unsupervised stereo matching. Our method introduces several key innovations, including disparity-aware coordinate embeddings along with warped image embeddings, pixel-level loss, and an adaptive fusion module, to ensure high-quality stereo image generation with strong geometric and semantic consistency. Through extensive experiments across diverse datasets, we show that GenStereo significantly outperforms existing methods in both stereo generation and unsupervised matching. Our ablation studies demonstrate the efficacy of each proposed component. Limitations. Despite promising results, diffusion-based models face inherent challenges with large disparities when generating right-view images due to large unconditioned regions. While our data augmentation such as random cropping and resizing mitigates these issues and performs well in typical stereo setups, future work could explore extending these models to accommodate larger disparities.."
        },
        {
            "title": "References",
            "content": "[1] Filippo Aleotti, Fabio Tosi, Li Zhang, Matteo Poggi, and Stefano Mattoccia. Reversing the cycle: self-supervised deep stereo through enhanced monocular distillation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16, pages 614632. Springer, 2020. 3 [2] Wei Bao, Wei Wang, Yuhua Xu, Yulan Guo, Siyu Hong, and Xiaohu Zhang. Instereo2k: large real dataset for stereo matching in indoor scenes. Science China Information Sciences, 63:111, 2020. 1 [3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 1 [4] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. naturalistic open source movie for optical flow evaluation. In European Conf. on Computer Vision (ECCV), pages 611 625. Springer-Verlag, 2012. 4 [5] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 1, 4, [6] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Singleimage depth perception in the wild. Advances in neural information processing systems, 29, 2016. 6 [7] Cheng Chi, Qingjie Wang, Tianyu Hao, Peng Guo, and Xin Yang. Feature-level collaboration: Joint unsupervised learning of optical flow, stereo depth and camera motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24632473, 2021. 3 [8] Jaehoon Cho, Dongbo Min, Youngjung Kim, Kwanghoon Sohn. leveraging large-scale outdoor stereo dataset. Systems with Applications, 178:114877, 2021. 1 and Deep monocular depth estimation Expert [9] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition, pages 33543361. IEEE, 2012. 6 [10] Clement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel Brostow. Digging into self-supervised monocular In Proceedings of the IEEE/CVF interdepth estimation. national conference on computer vision, pages 38283838, 2019. [11] Peter Hedman, Suhib Alsisan, Richard Szeliski, and Johannes Kopf. Casual 3d photography. ACM Transactions on Graphics (TOG), 36(6):115, 2017. 2, 3 [12] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (ToG), 37(6):115, 2018. 2 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [14] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 2 9 [15] Junpeng Jing, Ye Mao, Anlan Qiu, and Krystian Mikolajczyk. Match stereo videos via bidirectional alignment. arXiv preprint arXiv:2409.20283, 2024. [16] Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, and Mohammed Bennamoun. Active-passive simstereo, 2022. 4 [17] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1322913239, 2023. 4 [18] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. 2 [19] Johannes Kopf, Fabian Langguth, Daniel Scharstein, Richard Szeliski, and Michael Goesele. Image-based rendering in the gradient domain. ACM Transactions on Graphics (TOG), 32(6):19, 2013. 2 [20] Hsueh-Ying Lai, Yi-Hsuan Tsai, and Wei-Chen Chiu. Bridging stereo matching and optical flow via spatiotemporal correspondence. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18901899, 2019. 3 [21] Jiankun Li, Peisen Wang, Pengfei Xiong, Tao Cai, Ziwei Yan, Lei Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Practical stereo matching via cascaded reIn Proceedings current network with adaptive correlation. of the IEEE/CVF conference on computer vision and pattern recognition, pages 1626316272, 2022. 4, 5 [22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 2, [23] Rui Liu, Chengxi Yang, Wenxiu Sun, Xiaogang Wang, and Hongsheng Li. Stereogan: Bridging synthetic-to-real domain gap by joint optimization of domain translation and In Proceedings of the IEEE/CVF constereo matching. ference on computer vision and pattern recognition, pages 1275712766, 2020. 3 [24] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. 5, 6 [25] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andres Bruhn. Spring: high-resolution highdetail dataset and benchmark for scene flow, optical flow and stereo. In Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 4 [26] Moritz Menze and Andreas Geiger. Object scene flow for autonomous vehicles. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3061 3070, 2015. 1, 5 [27] Jiteng Mu, Shalini De Mello, Zhiding Yu, Nuno Vasconcelos, Xiaolong Wang, Jan Kautz, and Sifei Liu. Coordgan: Self-supervised dense correspondences emerge from gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1001110020, 2022. 3 [28] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In Proceedings of the IEEE international conference on computer vision, pages 4990 4999, 2017. [29] Matteo Poggi, Alessio Tonioni, Fabio Tosi, Stefano Mattoccia, and Luigi Di Stefano. Continual adaptation for deep stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):47134729, 2021. 3 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2 [31] Pierluigi Zama Ramirez, Alex Costanzino, Fabio Tosi, Matteo Poggi, Samuele Salti, Stefano Mattoccia, and Luigi Di Stefano. Booster: benchmark for depth from images of specular and transparent surfaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 1 [32] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. 1, 4 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 1, 2, 3, 5, 6 [34] Daniel Scharstein, Heiko Hirschmuller, York Kitajima, Greg Krathwohl, Nera Neˇsic, Xi Wang, and Porter Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In Pattern Recognition: 36th German Conference, GCPR 2014, Munster, Germany, September 2-5, 2014, Proceedings 36, pages 3142. Springer, 2014. 5, [35] Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with highIn Proceedresolution images and multi-camera videos. ings of the IEEE conference on computer vision and pattern recognition, pages 32603269, 2017. 6 [36] Junyoung Seo, Kazumi Fukuda, Takashi Shibuya, Takuya Narihira, Naoki Murata, Shoukang Hu, Chieh-Hsin Lai, Seungryong Kim, and Yuki Mitsufuji. Genwarp: Single image to novel views with semantic-preserving generative warping. arXiv preprint arXiv:2405.17251, 2024. 1, 2, 3 [37] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 2 [38] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 3d photography using context-aware layered depth In Proceedings of the IEEE/CVF Conference inpainting. on Computer Vision and Pattern Recognition, pages 8028 8038, 2020. 3, 6 [39] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. and Stefano Ermon. arXiv preprint [40] Xiao Song, Guorun Yang, Xinge Zhu, Hui Zhou, Zhe Wang, and Jianping Shi. Adastereo: simple and efficient apIn Proceedings of proach for adaptive stereo matching. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1032810337, 2021. 3 [41] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:75377547, 2020. 3 [42] Joshua Tokarsky, Ibrahim Abdulhafiz, Satya Ayyalasomayajula, Mostafa Mohsen, Navya Rao, and Adam Forbes. Plt-d3: high-fidelity dynamic driving simulation dataset for stereo depth and scene flow. arXiv preprint arXiv:2406.07667, 2024. 4 [43] Alessio Tonioni, Matteo Poggi, Stefano Mattoccia, and Luigi Di Stefano. Unsupervised adaptation for deep stereo. In Proceedings of the IEEE International Conference on Computer Vision, pages 16051613, 2017. [44] Alessio Tonioni, Matteo Poggi, Stefano Mattoccia, and Luigi Di Stefano. Unsupervised domain adaptation for depth prediction from images. IEEE transactions on pattern analysis and machine intelligence, 42(10):23962409, 2019. 3 [45] Alessio Tonioni, Oscar Rahnama, Thomas Joy, Luigi Di Stefano, Thalaiyasingam Ajanthan, and Philip HS Torr. Learning to adapt for stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 96619670, 2019. 3 [46] Alessio Tonioni, Fabio Tosi, Matteo Poggi, Stefano Mattoccia, and Luigi Di Stefano. Real-time self-adaptive deep stereo. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 195204, 2019. 3 [47] Fabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger. Smd-nets: Stereo mixture density networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 89428952, 2021. 4 [48] Fabio Tosi, Alessio Tonioni, Daniele De Gregorio, and Matteo Poggi. Nerf-supervised deep stereo. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 855866, 2023. 2, 3, 7 [49] Fabio Tosi, Luca Bartolomei, and Matteo Poggi. surarXiv vey on deep stereo matching in the twenties. preprint arXiv:2407.07816, 2024. Extended version of CVPR 2024 Tutorial Deep Stereo Matching in the Twenties (https://sites.google.com/view/stereo-twenties). 4 [50] Jonathan Tremblay, Thang To, and Stan Birchfield. Falling things: synthetic dataset for 3d object detection and pose thing v2. arXiv preprint arXiv:2406.09414, 2024. 1, 4, 6, 7 [62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2 [63] Yiran Zhong, Yuchao Dai, and Hongdong Li. Selfsupervised learning for stereo matching with self-improving ability. arXiv preprint arXiv:1709.00930, 2017. 3 [64] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. 6 In Proceedings of the IEEE Conference on estimation. Computer Vision and Pattern Recognition Workshops, pages 20382041, 2018. 4 [51] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, JiaBin Huang, and Johannes Kopf. Consistent view syntheIn Proceedings of sis with pose-guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1677316783, 2023. [52] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. 6 [53] Lezhong Wang, Jeppe Revall Frisvad, Mark Bo Jensen, and Siavash Arjomand Bigdeli. Stereodiffusion: Training-free stereo image generation using latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74167425, 2024. 2, 3, 5, 6 [54] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, and Xiaowen Chu. Irs: large naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation. In 2021 IEEE International Conference on Multimedia and Expo (ICME), pages 16. IEEE, 2021. 1, 4 [55] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of In 2020 IEEE/RSJ International Conference visual slam. on Intelligent Robots and Systems (IROS), pages 49094916. IEEE, 2020. 1, 4 [56] Yang Wang, Peng Wang, Zhenheng Yang, Chenxu Luo, Yi Yang, and Wei Xu. Unos: Unified unsupervised optical-flow and stereo-depth estimation by watching videos. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 80718081, 2019. 3 [57] Yuran Wang, Yingping Liang, Hesong Li, and Ying Fu. Mono2stereo: Monocular knowledge transfer for enhanced stereo matching. arXiv preprint arXiv:2411.09151, 2024. 3, [58] Jamie Watson, Oisin Mac Aodha, Daniyar Turmukhambetov, Gabriel Brostow, and Michael Firman. Learning stereo from single images. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 722740. Springer, 2020. 2, 3, 6, 7 [59] Zhexiao Xiong, Feng Qiao, Yu Zhang, and Nathan Jacobs. Stereoflowgan: Co-training for stereo and flow with unsupervised domain adaptation. arXiv preprint arXiv:2309.01842, 2023. 3 [60] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human imIn Proceedings of age animation using diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. 2 [61] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth any-"
        }
    ],
    "affiliations": [
        "Washington University in St. Louis"
    ]
}