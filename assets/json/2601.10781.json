{
    "paper_title": "Future Optical Flow Prediction Improves Robot Control & Video Generation",
    "authors": [
        "Kanchana Ranasinghe",
        "Honglu Zhou",
        "Yu Fang",
        "Luyu Yang",
        "Le Xue",
        "Ran Xu",
        "Caiming Xiong",
        "Silvio Savarese",
        "Michael S Ryoo",
        "Juan Carlos Niebles"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 1 ] . [ 1 1 8 7 0 1 . 1 0 6 2 : r Future Optical Flow Prediction Improves Robot Control & Video Generation Kanchana Ranasinghe,1,2, Honglu Zhou1, Yu Fang,1, Luyu Yang1, Le Xue1, Ran Xu1, Caiming Xiong1, Silvio Savarese1, Michael Ryoo1,2, Juan Carlos Niebles1 1Salesforce AI Research 2Stony Brook University FOFPred.github.io Figure 1. Our proposed model, FOFPred, learns generalizable language-driven future optical flow prediction that supports robot control and video generation. < > is placeholder for text (e.g., Left/Up/Right). For visualization purposes, we sample points from the predicted future optical flow and show their predicted trajectories in this figure. Checkout our website for more visualizations."
        },
        {
            "title": "Abstract",
            "content": "Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, novel languageconditioned optical flow forecasting model featuring unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity dataa highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation Work done during an internship at Salesforce. under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction. 1. Introduction Recent studies have investigated the use of motion representations such as optical flow [67, 96], sparse motion trajectories [7, 89, 102], and direction commands [11, 88, 100] for multiple control and generation tasks. The main idea behind such motion representations, often in the form of future pixel movements like optical flows and trajectories, is to explicitly capture desired dynamics necessary for downstream tasks such as robot control and video generation. In the case of robot control, models with such future motion representations have the capability to explicitly consider future pixel displacement information to infer robot actions [7, 67, 89, 96, 102]. This is in contrast to the conventional Vision-Language-Action (VLA) models taking only RGB-frames and language instructions as input. 1 Similarly, video diffusion models utilizing motion representations enables generation of videos with consistent and detailed movements, and it does it better than the counterparts utilizing language-only instructions [13, 22, 40, 51]. The key is in reliable and generalizable computation of such motion representation, which often involves future forecasting. What has to be provided to the models for many (robot) control and (video) generation tasks is how things should move in the future, not what motion has been observed in the past. This inherently is challenging task: it requires clear formulation of what type of future motion we will be computing, curating large enough training data with strong supervisory signals, and an elegant model capable of fully digesting such (potentially cross-domain) training data while generalizing across domains. In this paper, we formulate the problem as the forecasting of future optical flows conditioned on language instructions, and present new model architecture to forecast them. We also discuss how we can train our model with videos from various sources, ranging from human videos to robot videos. Optical flows are spatially dense motion representations, capturing movements of every pixel in the scene. This better preserves motion details compared to sparse representation, and enables easy use of diffusion models to learn their distributions in the form of images. In contrast to predicting RGB frame sequences, which also capture future motion [33], optical flow eliminates static information irrelevant to motion, leading to more motioncentric representations [67]. We introduce an optical flow forecasting model unifying both Vision-Language Models (VLM) and Diffusion. Fully taking advantage of VLM reasoning capability and pretrained diffusion model image generation ability, we train our VLM-Diffusion model to generate future optical flow images. Our model is trained on web-scale human activity videos with paired captions allowing highly-scalable training. The resulting model, named FOFPred, enables highly generalizable language-driven future optical flow prediction as illustrated in Figure 1. While learning from web videos provides this strong generality, such data is highly unstructured with noisy videos and captions. Most prior work learning future motion representations either avoid training with such data [7, 67, 89, 96, 102] or utilize RGB frame prediction [33, 38]. In [97] where such training data is used, explicit camera motion correction is used to handle noisy videos and VLM backbone enables learning from diverse noisy captions. However, the use of only VLM restricts their future motion prediction to sparse pixel trajectories. In our work, we calculate dense relative optical flow compensating for the camera motion and model this with diffusion, which improves our learning from noisy web videos. Finally, we validate the effectiveness and generality of FOFPred through evaluation on two challenging downstream tasks: language-driven robotic manipulation and language-guided motion video generation. For each task, we attach diffusion policy head or video diffusion head on top of FOFPred and fine-tune the two heads separately for the respective downstream domains. We summarize our key contributions as follows: Unified VLM-Diffusion: Adopting this recent architecture for generalizable future optical flow prediction. Scalable Learning: Establish framework for learning future optical flow prediction from web-scale human activity videos. Cross-Domain Versatility: To the best of our knowledge, the first to utilize VLMdiffusion backbone for robotic manipulation and controlled video generation under language-driven settings. 2. Related Work Unified Model for Control Generation. Recent progress in foundation models has demonstrated powerful trend toward unified model architectures that integrate diverse modalities and achieve highly controlled visual input manipulation. These generalist models [5, 15, 28, 35, 61, 62, 91, 94, 97], often building on transformers and large-scale pre-training (e.g., Gato [68]), establish single frameworks capable of handling hundreds of tasks. In the generative domain, this unification enables advanced control: models like UniDiffuser [5] tackle various image generation and editing tasks within single conditional framework, while BLIP3o [15], OmniGen-2 [91], and MetaQuery [62] demonstrate sophisticated language-conditioned iterative modification and transfer across modalities. Crucially, the development of plug-and-play architectures like ControlNet [105] established the blueprint for adding flexible, low-level spatial controls to pretrained diffusion models. This pioneering work inspired recent unified generative backbones that accept diverse conditioning signals, enabling versatile controllable generation [19, 28, 3436, 41, 53, 78]. This foundation of multi-modal input, unified architecture, and flexible control is key enabler for tackling complex tasks like controlled video [64, 65, 81, 85] and robotic action [49, 55, 66, 84] generation. Motion Representations in Robot Control. Learning motion related information from videos has been widely explored in robot learning [3, 14, 20, 21, 33, 38, 42, 46, 59, 63, 69, 71, 72, 75, 77, 79]. Building on this, the field has increasingly explored vision-language-action (VLA) models to learn policies conditioned on natural language [20, 33, 38, 77], although many of these still rely on extensive action-trajectory annotations or task-specific heuristics. Recent foundation models aim to overcome this by better 2 Figure 2. Overview of Proposed FOFPred: (Left & Center) We present the unified VLM-Diffusion architecture used in FOFPred. Only the DiT module is trained while the VAE and VLM remain frozen. (Right) We illustrate two distinct pipelines constructed with FOFPred for two orthogonal tasks in control and generation. Each task specific head is first finetuned prior to inference on the downstream task. integrating spatio-temporal intelligence: MAGMA [97] utilizes Trace-of-Mark annotations on video movements for action grounding and planning across digital and physical tasks. Other approaches focus on improving world model reasoning before action generation: FlowVLA [109] employs Visual Chain of Thought to explicitly predict optical flow before the next visual frame, disentangling motion dynamics from static appearance for more efficient policy learning, while DreamVLA [106] forecasts compact set of crucial world knowledge, including dynamic regions, depth, and semantics, to guide action planning and establish robust perception-prediction-action (PPA) Independently, optical flow has served as priloop. mary representation in computer vision for motion learning [24, 29, 57, 73, 87], and its extension to trajectories of pixel subsets [7, 89, 96, 102] has found utility in robot control. However, methods relying on localized trajectories [12, 43, 76, 101] often limit their focus to specific object movements, overlooking crucial global information, such as the overall movement of manipulator. In contrast, our FOFPred predicts spatially dense future optical flow leveraging powerful unified model architectures. Motion Control in Video Generation. Controlling motion in video generation using spatial conditioning signals has been widely explored [11, 17, 25, 30, 47, 50, 60, 74, 86, 88, 93, 100, 104, 107, 110]. Some methods define object motion through sparse user-provided trajectories and points, including DragNUWA [100], DragAnything [93], Tora [104], TrackGo [110], and the foundational Controllable Video Generation [30]. Others employ explicit motion models, such as MotionCtrl [88], Motion-I2V [74], and Motion-Conditioned Diffusion Models [17]. Additional techniques achieve control through motion fields (MOFAVideo [60]), image-based precision guidance (Image Conductor [50]), compositional video synthesis (VideoComposer [86]), or generalized animation (AnimateAnything [47]). However, language-based explicit motion control remains underexplored, which we focus on in this work. 3. Method Optical Flow, the apparent displacement of pixels between frame pairs, has long history in computer vision [6, 32]. Estimating the true optical flow given frame pair is wellestablished problem [80]. On the other hand, several recent works explore different task of estimating future optical flow from single frame [2, 23, 82], with some conditioned on language [67, 96]. To distinguish this task of future pixel displacement estimation from single frame, we use the term future optical flow in the following sections. First, we introduce FOFPred, our framework that generates future optical flow, given language instruction and one or more images. This future optical flow generation is learned from video-caption pairs without any pixel-level human annotation. We train unified LLM-diffusion architecture based model to generate sequences of future optical flow frames. In the following sections, we first detail the architecture of our framework, followed by training and inference procedures. Next we describe our extension for robot control, task requiring strong awareness of object and ego motions. Finally we present our second extension for motion control in text-to-video generation. 3.1. Architecture Motivated by recent success of unified model architectures in language conditioned visual generation [15, 62, 91, 94], we construct FOFPred that maps frame sequences and language into future optical flow sequences. We illustrate an architectural overview of FOFPred in Figure 2. Consider scene observations (images) xt Rhwc corresponding to time step and paired natural language caption c. We first utilize an autoregressive transformer 3 based vision-language model as textual feature encoder that encodes both the language caption and visual input xt1, xt to obtain fc. We next encode the visual inputs, xt1, xt with VAE encoder to obtain fv. The textual (fc) and visual (fv) features are passed through MLP layers and input to our diffusion transformer (DiT). The outputs of the diffusion transformer, ˆfy are passed through VAE decoder to obtain ˆy sequences which correspond to future optical flow sequences. For our DiT architecture, we utilize the OmniGen transformer [91], introducing several modifications to enable temporal modeling, given the time dimension of our frameworks inputs and outputs. We first modify the DiT 2D RoPE encoding to handle both input and output frame sequences. Next we update the DiT transformer blocks to perform full spatio-temporal attention in order to model the temporal axes in frame sequences. This design introduces no additional learnable parameters, enabling our framework to directly benefit from image domain pretraining used in [91]. In terms of other components, we use Qwen2.5-VL [4] as our VLM and Flux.1 VAE [44] as our visual encoderdecoder. The two MLP layers ensure channel dimension of both textual and visual features are common. These conditional inputs are simply appended to the DiT input sequence. Further details in Section A. 3.2. Optical Flow Representation In contrast to prior future optical flow generation works [67, 96], we adopt an RGB space representation for optical flow (OF). This allows us to directly benefit from existing powerful VAE models to encode our OF, in contrast to training or finetuning the VAE model [67, 96]. Motivated by [13, 109], we explore representing OF in RGB formats by mapping the polar coordinate values of OF to the HSV color space. We map magnitude, rotation, and linear combination of these two from the polar representation into the three H-S-V channels respectively. We tune the scaling values for these mappings to maximize visual continuity across frames (e.g. no flickering) and to resemble RGB images (e.g. animated graphics). See Section for further details. 3.3. Training Our FOFPred framework is trained end-to-end to predict future optical flow sequences conditioned on image sequences and paired textual captions. During training, the target optical flow sequence, y, is calculated using suitable optical flow calculation algorithm, F, which can access the future frames for the calculation. We use y[i] = F(xi, xi+1) where [i] indexes the i-th frame of each sequence. These optical flow targets are encoded with the VAE encoder to obtain fy which is in turn used with flow matching diffusion loss as the training objective. Let us define the learnable components of our frameFigure 3. Relative Optical Flow Calculation: We illustrate the key stages of the algorithm for calculating optical flow targets for our training. i.e. the two MLPs and DiT, as such that work, D(fc, fv, n) ˆfy where is noise input and ˆfy is predicted tensor output of the DiT module. Given targets fy, corrupted targets fy, and noise f0 we obtain our training objective as, LFM(θ) = {D(fc, fv, fy) (fy f0)2 2 (1) where θ are our learnable parameters. During training, we perform classifier-free guidance dropping the textual condition (fc) or the visual condition (fv). We also partially mask our visual condition (fv) along time (e.g. xt1 is masked) and viewpoint axes. Note that our visual observation, xt, may contain images corresponding to multiple viewpoints, hence the viewpoint masking. Relative Optical Flow Calculation: We construct our framework to train from internet videos of human activities (e.g. taken from mobile phone), where each video may contain both camera (i.e. ego) motion and object motions. We construct our optical flow calculation algorithm, F, accounting for these possible structures of training data. Given current-future frame pair, we first use an off-theshelf optical flow calculation algorithm [80] to calculate flow vectors. Next, motivated by prior work [97], we use homography techniques with deep features [45] to estimate the camera motion. Then we utilize the camera motion estimates with projective geometry to calculate flow vectors relative to the camera. Object motions are disentangled from camera motions in these relative optical flow vectors. We provide further details on this calculation in Section C. We also note how motion in natural videos is not uniformly distributed. To tackle this, given optical flow between frame pairs, we take the top-k percent of flow values as proxy to motion between those frame pairs. We select frame sequences such that the motion between consecutive pairs exceeds specific threshold. The final optical flow targets used to train our framework are calculated for these filtered frame sequences. Further details of our frame selection in Section D. Given the computational cost of our data preprocessing, the initial frame selection utilizes fast optical calculation [56] on low spatial resolution frames. Relative optical flow calculation is performed on original resolution images. Both steps are performed offline as one time pro4 cesses, resulting in similar sampling costs as uniform frame sampling during training. 3.4. Inference Our VLM and VAE encoder act as feature extractors, followed by the two MLP layers projecting these features to common dimension. The DiT module then uses these features as conditional inputs, and together with noise vectors performs iterations of reverse diffusion. The value is varied based on the downstream application. For the DiT component, we perform classifier-free guidance on the text and visual features. 3.5. Downstream Use Case I: Robot Control We first validate the usefulness of our FOFPred framework through language driven robot control or vision-languageaction (VLA). These tasks require robust motion awareness and precise language-based control. Motivated by prior work [33, 67], we integrate the future optical flow predicted by FOFPred into diffusion policy network (DPN) [70]. This DPN processes the predicted future optical flow alongside textual inputs and current state information, mapping these multimodal inputs directly into action vectors that control robotic manipulator. We provide an overview of our robot control pipeline in Figure 2. Given the embodiment awareness necessary for robotic manipulation, we finetune our framework for downstream robotic tasks. Aligning with established methodologies [33, 67], this process comprises two stages: (1) future optical flow prediction finetuning on robotic video-caption data, and (2) action head training on robotic demonstration data. key distinguishing feature of our work is the future optical flow finetuning phase, which explicitly accounts for the two types of viewpoints, fixed external and moving wrist cameras, inherent in robotic data. We achieve this by ensuring cross-view conditioning within the DiT backbone and augmenting our prediction targets (fy) to encompass optical flow from both viewpoint types. These architectural and data handling innovations differentiate our framework from previous robot control approaches [33, 67]. For final robot control, the predicted motions are mapped to robotic actions with the diffusion policy network from [33] modified to condition on our future optical flow predictions. 3.6. Downstream Use Case II: Video Generation We further demonstrate the versatility of FOFPred framework by exploring Text-to-Video (T2V) generation focused on purely natural language-based motion control. While prior motion-guided video generation methods typically rely on low-level visual inputs like pixel-level trajectories or motion masks generated by humans through suitable software interfaces [11, 93, 100], we investigate the use of textual descriptions as direct, human-centric motion control signal. We establish two-stage pipeline by connecting FOFPred with the existing video synthesis model, Go-withthe-Flow (GWTF) [11]. First, FOFPred takes static visual observation (the initial frame) and textual description of the desired future motion as input. Conditioned on these inputs, FOFPred generates sequence of future optical flow frames. This sequence is subsequently interpolated to create dense motion signal. In the second stage, this dense motion and the initial frame are passed to GWTF, which synthesizes the final video sequence, ensuring the output realizes the motion pattern defined in the original text. We provide high level overview of this extended setup in Figure 2. Compared to single-stage T2V methods [39, 83, 98], our approach is computationally more intensive but offers two distinct advantages. It demonstrates superior fidelity to complex textual motion instructions, and the interpretable future optical flow sequence provides crucial, intermediate layer of transparency to the generation process. We also highlight this as an experiment to validate the strength of language-driven future optical flow forecasting ability learned by our framework. 4. Experiments In this section, we first detail our experimental setup followed by evaluations on language-driven robotics manipulation and motion focused text-to-video generation. Finally we provide ablations of our FOFPred framework to validate our design choices. Implementation Details: Our FOFPred framework contains three key components: VLM, VAE, and DiT. We use the Qwen2.5-VL [4] architecture for our VLM and the Flux.1 [44] architecture for our VAE. For both models, we initialize with their official pretrained weights. Both the VLM and VAE weights are kept frozen. Our DiT uses standard diffusion transformer architecture following OmniGen [91] with modifications for video generation as described in Section 3.1. We initialize the DiT (including projector layer MLP) with weights from model pretrained on image editing tasks from [91]. Training Details: We train our FOFPred framework on the human activity data from the train split of SomethingSomething-V2 dataset [26] and the EgoDex dataset [31]. Our training data contains roughly 500,000 video-caption pairs. We train our model on 8xH200 GPUs with global batch size of 4096 for 100 epochs. We note how different frame subsequences are sampled from the same video across epochs based on motion based sampling as explained in Section 3.3. For robotic manipulation, we additionally finetune our model on all available video data (without access to action trajectory labels) on the downstream tasks following prior robotic manipulation works [33, 67, 106]."
        },
        {
            "title": "Training Data",
            "content": "RT-1 [10] Diffusion Policy [18] Robo-Flamingo [48] Uni-Pi [20] MDT [70] Susie [8] GR-1 [92] Vidman [90] RoboUniview [52] LTM [67] DreamVLA [106] VPP [33] FOFPred GR-1 [92] VPP [33] FOFPred 100% ABC 100% ABC 100% ABC 100% ABC 100% ABC 100% ABC 100% ABC 100% ABC 100% ABC 100% ABC 100% ABC 100% ABC 100% ABC 10% ABC 10% ABC 10% ABC 1 53.3 40.2 82.4 56.0 63.1 87.0 85.4 91.5 94.2 97.1 98.2 ith Task Success Rate 3 9.4 2.6 46.6 8.0 24.7 49.0 59.6 68.2 73.4 72.8 89.5 3.8 0.8 33.1 8.0 15.1 38.0 49.7 59.2 62.2 67.2 83.4 2 22.2 12.3 61.9 16.0 42.9 69.0 71.2 76.4 84.2 82.4 94.6 5 1.3 0.0 23.5 4.0 9.1 26.0 40.1 46.7 50.7 60.6 78.1 Avg. Len 0.90 0.56 2.47 0.92 1.55 2.69 3.06 3.42 3.65 3.81 4.44 96.5 98.8(+2.3) 67.2 87.8 90.4(+2.6) 90.9 95.0 (+4.1) 37.1 74.6 77.4(+2.8) 86.6 90.4(+3.8) 19.8 63.2 65.9(+2.7) 82.0 84.6(+2.6) 10.8 54.0 62.8(+8.8) 76.9 78.7(+1.8) 06.9 45.3 46.9(+1.6) 4.33 4.48(+0.15) 1.41 3.25 3.43(+0.18) Table 1. CALVIN Evaluation: Zero-shot long-horizon evaluation on the Calvin ABCD benchmark where agent is asked to complete five chained tasks sequentially based on instructions. We report success rate (%) for each sequential task and the average length of task completion following standard protocol. Improvements over VPP [33] baseline reported for our method. Method Handover Block Handover Mic Pick Diverse Bottles Pick Dual Bottles Place Dual Shoes Average RDT [54] ACT [108] DP [18] DP3 [103] π0 [9] 45 42 10 70 45 VPP [33] FOFPred (ours) 54 61(+7) 90 85 53 100 98 80 87(+7) 2 7 6 52 27 60 67(+7) 42 31 24 60 63 68(+5) 4 9 8 13 15 52 60(+8) 36.6 34.8 20.2 59.0 48.4 61.8 68.6(+6.8) Table 2. RoboTwin Evaluation: We report success rates on the RoboTwin 2.0 benchmark following their official protocol on the easy setting. This environment contains bimanual robot. The five tasks are selected based on necessity of both arms to complete the task. Our proposed FOFPred achieves consistent improvements over the VPP baseline implemented under identical settings. 4.1. Language-Driven Robot Manipulation We evaluate on two robot manipulation benchmarks, CALVIN [58] and RoboTwin 2.0 [16]. The CALVIN benchmark is an open-source, simulated environment designed for long-horizon, language-conditioned tasks. It challenges agents to perform complex, multi-step manipulation sequences using continuous control, with task goal signals specified only via unconstrained natural language instructions. This dataset focuses on developing policies that can generalize to novel language instructions and unseen environments, making it an ideal evaluation task for our framework. For this task, we utilize the CALVIN ABCD setting, where training data is available only for ABC environments, while evaluation is performed zeroshot on the novel environment. Given the multi-step sequential manipulation tasks within this benchmark, the standard evaluation metrics are ith task success rate and the average length of task stages completed. We report this standard metrics from the benchmark used across prior work [33, 67] for our evaluations. The RoboTwin 2.0 framework provides scalable approach for bimanual (dual-arm) robotic manipulation, specifically addressing the limitations of oversimplified simulations for two-arm tasks. We select 5 tasks that specifically require utilizing both arms for manipulation in order to successfully complete the task. This benchmark allows evaluating the generality of our framework across both single and dual arm robotic environments. The standard evaluation metric for this benchmark is success rate which we use in our evaluations, along with the average success rate across all tasks. CALVIN Results: Our results reported in Table 1 demonstrate state-of-the-art performance of our FOFPred framework in long-horizon, zero-shot robot manipulation. All evaluations follow standard protocol [58]. Results for baselines are reported directly from prior work [33, 106]. Train6 Method SSIM PSNR LPIPS FVD KVD MF [99] Seer [27] Dynamicrafter [95] CosHand-I [77] CosHand-A [77] InterDyn [1] InterDyn-R [1] 41.8 61.5 53.1 66.4 68.0 10.71 16.87 14.92 18.60 19.04 58.8 31.3 40.8 26.0 25.2 287.46 204.11 91.18 90.30 19.27 22.22 81.31 31.81 19.24 13.68 1.99 2.09 0.432 0.570 0.633 0. CogVideoX [98] FOFPred (ours) 67.2 68.4(+1.2) 21.51 22.26(+0.75) 30.3 28.5(+1.8) 78.47 75.39(+3.08) 12.46 11.38(+1.08) 0.594 0.662(+0.068) Table 3. Evaluation of Language-Driven Motion Control in T2V: We evaluate the ability of our T2V pipeline for language-driven motion control using the motion heavy SSv2 dataset following prior work [1]. Over the CogVideoX baseline, our FOFPred framework shows consistent improvements in generation quality. ing on the full split for the ABCD setting, our FOFPred framework registers the highest success rate across all five chained tasks, culminating in Task 5 Success Rate of 0.787 and the highest Average Length of task completion at 4.48, marginally surpassing the prior best model, DreamVLA (4.44). This strong performance confirms our methods superior ability to generalize to novel language instructions for sequential, multi-step manipulation. Furthermore, in the data-limited regime (using only 10% of the ABC dataset), our method continues to excel, achieving an average length of 3.43, which is noticeably higher than other models in this setting, highlighting its data efficiency. Similar trends of data efficiency have been observed in prior works utilizing motion representations [23, 67], another benefit of our framework. RoboTwin Results: We report RoboTwin results in Table 2, demonstrating the effectiveness of our FOFPred framework in complex bimanual manipulation tasks. Results for baselines are directly from the official leaderboard [16], with attempted replication following the official codebase. We note that we were unable to replicate the baseline results reported in leaderboard for some tasks (e.g. Handover Mic), but nevertheless report official values from the benchmark leaderboard. We replicate the VPP baseline [33] following their official codebase training under identical settings as our FOFPred framework, enabling direct and fair point of comparison. On our selected task subset, our model achieves an average success rate of 68.6%, significantly outperforming our baseline model, VPP (61.8%), which is trained similarly on web human videos, but with frame prediction instead of motion prediction. Our FOFPred outperforms this baseline consistently across all tasks, demonstrating the strength of motion prediction over frame prediction. In comparison to other baselines, our model notably shows robust performance across tasks (e.g. Place Dual Shoes), highlighting its better generalization ability. We take these results as confirmation on the utility of FOFPred framework for solving complex robotic tasks requiring cooperative bimanual manipulation. 4.2. Motion Control in Video Generation Our second downstream task focuses on text-to-video generation with focus on language-driven motion control aspects. Table 3 presents results for our evaluation on the validation split of SSv2 dataset following prior work [1]. Our evaluation protocol is identical to [1] with results for baselines [1, 27, 77, 95] reported directly from [1]. We implement and evaluate the CogVideoX baseline [98] under identical settings as our FOFPred framework. We achieve consistent improvements over the CogVideoX baseline across all major metrics. Notably, while most prior controllablegeneration baselines (e.g., [1, 77]) rely on auxiliary control signals such as hand or object masks over timeinputs that are not freely available at inference, our framework operates solely from language prompts and visual context. Despite this weaker supervision at test-time, FOFPred attains comparable or superior performance, underscoring its capacity to learn text-conditioned motion representations that generalize without explicit spatiotemporal guidance. We take these results as indication to how motion-aware language grounding alone can yield coherent, controllable dynamics in text-to-video generation. 4.3. Ablations Next we analyze contributions of each design choice. For all experiments in this section, we use only the SSv2 dataset as the pretraining source for faster experimentation. This dataset provides broad range of human activity videos with rich motion diversity, making it suitable testbed for controlled comparisons. For evaluation, we follow the standard CALVIN setup [33, 58], where each evaluation sequence consists of five chained tasks. While our main experiments utilize the full 1000-sequence benchmark to ensure comprehensive assessment, the ablations are performed on reduced subset of 400 sequences to facilitate efficient experimentation without compromising representativeness. The following analyses dissect three key fac7 Pretrain Dataset DROID SSv Train Steps Avg Len 2000 2000 4.04 4.39(+0.35) Table 4. Ablation on Human Web Video Pretraining: The abundance of web videos of human activities and their inexpensive collection in comparison to teleoperated robotic manipulation videos allows training with significantly more human activity data. For fair investigation on impact of human videos, we pretrain our framework for common step count on both DROID [37] and SSv2 dataset. DROID is considered large-scale In-the-Wild robotic manipulation dataset that contains diversity across scenes and actions in its teleoperated demonstration videos. Backbone IE PT Avg Len Diffusion Only VLM-Diffusion VLM-Diffusion 4.01(-0.38) 4.14(-0.25) 4.39 Table 5. Ablation on Backbone Choice: We evaluate our VLMDiffusion architecture against diffusion only backbone following [67] and demonstrate the gains in terms of learning information from web human videos that is useful for robotic tasks. We also ablate the image-editing pretraining we adopt from [91] to demonstrate its utility for motion guidance prediction. tors: pretraining, backbone architecture, and motion disentangling strategy. We evaluate each for its impact on motion guidance prediction through downstream task performance. Pretraining Ablation: Table 4 investigates the effect of pretraining on large-scale web videos of human activities compared to teleoperated robotic demonstrations by using the full DROID dataset [37]. While DROID provides diverse in-the-wild robot manipulation demonstrations, it remains limited in coverage and motion variability relative to human activity videos such as SSv2. When pretrained for the same number of steps, our model exhibits noticeable performance gain (+0.35) when trained on SSv2, confirming that web-scale human motion data significantly improves the models ability to infer language-conditioned motion patterns. This highlights the value of human web videos as scalable supervision source for motion understanding, even though they are noisier and less structured than robot demonstrations. The result supports our hypothesis that large, diverse human video corpora can serve as an effective pretraining substrate for motion guidance learning prior to domain-specific adaptation. Backbone Ablation: Table 5 explores contribution of the unified VLMdiffusion backbone relative to diffusiononly architecture as well as the fine-grained image-editing pretraining (IE-PT) that is enabled by this VLM-diffusion architecture [91]. The diffusion-only baseline achieves indicating its ability to model lowlower performance, Method No Video Pretraining Static Frame Targets Raw Motion Targets Disentangled Motion Targets Avg Len 3.91(-0.48) 4.28(-0.11) 3.89(-0.50) 4.39 Table 6. Ablation on Motion Disentangling Strategy: We examine the importance of motion disentangling for learning language driven dense pixel motion prediction from web videos of human In line with prior work [97], our results demonstrate activities. the difficulty of learning any meaningful signals without suitable disentangling of motion targets using our algorithm. In Row 1, we provide baseline with no video pretraining, followed by pretraining with static frame targets (similar to VPP [33]), raw motion targets (no camera-object motion separation), and our disentangled motion targets (default) in Rows 2-4 respectively. level motion but limited understanding of linguistic context. Incorporating VLMdiffusion backbone without image-editing pretraining improves performance, as the integrated visionlanguage transformer model (VLM) introduces stronger multimodal reasoning and better semantic grounding. The full VLMdiffusion model with imageediting pretraining achieves the highest score, confirming that pretraining on language-driven image transformation tasks facilitates cross-modal alignment between linguistic commands and visual motion cues. Together, these results validate our architectural design: combining VLM and diffusion components benefits generalizable learning of motion guidance prediction from weakly aligned web captions. Ablation on Motion Target Calculation: Table 6 examines how different formulations of motion supervision affect the ability to learn meaningful language-conditioned motion guidance prediction. Removing disentangling and using raw optical flow (Row 3) results in significant performance drop, suggesting that camera-induced motion severely hinders the learning of object-centric motion guidance. Similarly, static frame targets (Row 2), which ignore motion altogether, lead to underfitting of temporal dynamics. In contrast, our disentangled motion targets (Row 4, default) yield the best results, demonstrating that separating object motion from camera motion provides clean, physically meaningful supervision that aligns with linguistic intent. This confirms that our motion disentangling algorithm is critical for grounding the models predictions in genuine object dynamics rather than pixel changes caused by viewpoint shifts. 5. Conclusion We presented FOFPred, new framework for bridging language and motion through dense, pixel-level displacement prediction. Our disentangling algorithm refines noisy web video supervision by separating object and cam8 era motion, yielding cleaner learning signals. The unified VLMdiffusion backbone further enables robust multimodal learning from diverse and noisy caption data. Through extensive experiments across robotics and video generation, we demonstrate that motion-guided representations substantially enhance language-conditioned control and synthesis. We hope this work paves the way toward generalizable, motion-aware world models that understand and act through dynamic visual grounding. 5.1. Limitations and Future Work Our FOFPred model currently contains several limitations. It is sensitive to the text prompt (i.e. slightly different text prompts could lead to wrong predictions). For example, changing text moving from right to left moving left results in wrong predictions for some samples. In addition, the current FOFPred is relatively large model (7B parameters in total) that is expensive to deploy in real-time setting and requires considerable compute (at least 24GB of GPU) for inference. Further discussion in Section F. On the other hand, our model captures diversity of motion patterns quite well and generates meaningful future optical flow in as less as 1 reverse diffusion (i.e. denoising) iteration. Across different seeds for the same frame-caption pair, our model generates diverse distribution of mostly meaningful future optical flow: we attribute this strength to the diffusion based training. On fast convergence (during reverse diffusion), we note how our model is not distilled or trained specifically for this purpose; RGB image generation with identical architecture and training pipeline usually requires at least 20 sampling steps for meaningful generation. We hypothesize that optical flow is less complex distribution compared to natural images (i.e. lies on lower dimensional manifold and contains less high frequency information such as textures or patterns): this allows even single sampling step of reverse diffusion to generate meaningful outputs. In future work, we plan to explore training data augmentation with automated text-label re-phrasing and model distillation into light-weight architectures to address our key limitations. We also aim to further investigate and quantify the diversity aspect of FOFPred, analyze the fast convergence aspect, and explore possibility of real-time inference."
        },
        {
            "title": "Reproducibility Statement",
            "content": "To ensure the reproducibility of our results, we have made our code, trained model weights, and data configurations publicly available (see fofpred.github.io). Our method builds upon open-source pretrained models retrieved from the Hugging Face Hub. Detailed implementation specifics, hyperparameter settings, and experimental protocols are thoroughly documented in the main text and the Appendix."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank the broader research team at Salesforce for support, feedback and guidance. In particular, we thank Shelby Heinecke, Juntao Tan, Srinath Meadusani, Eric Xu, Matthew Fernandez, Jim Jagielski, Joyce Zheng, and Jinxuan Xu for technical feedback and support; Jeanette Berberich and Jennifer McCallion for legal coordination; Janna Remperas, Ian Thomas, Mitra Mitchell, and Alex Dalton for logistical help and encouragement throughout the project."
        },
        {
            "title": "References",
            "content": "[1] Rick Akkerman, Haiwen Feng, Michael J. Black, Dimitrios Tzionas, and Victoria Fernandez Abrevaya. Interdyn: Controllable interactive dynamics with video diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2025. 7 [2] Filippo Aleotti, Matteo Poggi, and Stefano Mattoccia. In IEEE Conf. Learning optical flow from still images. Comput. Vis. Pattern Recog. (CVPR), pages 1519615206, 2021. 3 [3] Shikhar Bahl, Abhinav Gupta, and Deepak Pathak. HumanIn Robotics: Science and to-robot imitation in the wild. Systems, 2022. 2 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. ArXiv, abs/2502.13923, 2025. 4, 5, 15 [5] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. In Proceedings of the 40th International Conference on Machine Learning (ICML), 2023. [6] John L. Barron, David J. Fleet, and Steven S. Beauchemin. Performance of optical flow techniques. International Journal of Computer Vision, 12:4377, 1992. 3 [7] Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, and Shubham Tulsiani. Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation. Eur. Conf. Comput. Vis. (ECCV), 2024. 1, 2, 3 [8] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Rich Walke, Chelsea Finn, Aviral Kumar, and Zero-shot robotic manipulation with Sergey Levine. ArXiv, pretrained image-editing diffusion models. abs/2310.10639, 2023. 6 [9] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, 9 and Ury Zhilinsky. π0: vision-language-action flow model for general robot control. ArXiv, abs/2410.24164, 2024. 6 [10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael S. Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. Robotics science and systems (RSS), 2023. [11] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael S. Ryoo, Paul E. Debevec, and Ning Yu. Go-with-the-flow: Motioncontrollable video diffusion models using real-time warped noise. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1323, 2025. 1, 3, 5, 18 [12] Junuk Cha, Jihyeon Kim, Jae Shin Yoon, and Seungryul Baek. Text2hoi: Text-guided 3d motion generation for hand-object interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15771585, 2024. 3 [13] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Videojam: Joint appearance-motion representations for enhanced motion generation in video models. ArXiv, abs/2502.02492, 2025. 2, 4 [14] Annie Chen, Suraj Nair, and Chelsea Finn. Learning Generalizable Robotic Reward Functions from In-The-Wild Human Videos. In Robotics: Science and Systems, 2021. 2 [15] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal modelsarchitecture, training and dataset. ArXiv, abs/2505.09568, 2025. 2, 3 [16] Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Qiwei Liang, Zixuan Li, Xianliang Lin, Yiheng Ge, Zhenyu Gu, et al. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088, 2025. 6, 7 [17] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, and Ming-Hsuan Yang. Motion-conditioned diffusion model for controllable video synthesis. arXiv preprint arXiv:2304.14404, 2023. 3 sion policy: Visuomotor policy learning via action diffusion. ArXiv, abs/2303.04137, 2023. [19] Minkyoung Cho, Ruben Ohana, Christian Jacobsen, Adityan Jothi, Min-Hung Chen, Morley Mao, and Ethem Can. Tc-lora: Temporally modulated conditional lora for adaptive diffusion control. arXiv preprint arXiv:2510.09561, 2025. 2 [20] Yilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning Universal Policies via Text-Guided Video Generation. arXiv:2302.00111, 2023. 2, 6 [21] Chelsea Finn and Sergey Levine. Deep Visual Foresight for Planning Robot Motion. In IEEE International Conference on Robotics and Automation, 2017. 2 [22] Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, and Lin Shao. Flip: Flow-centric generative planning as In Int. Conf. general-purpose manipulation world model. Learn. Represent., 2025. 2 [23] Ruohan Gao, Bo Xiong, and Kristen Grauman. Im2flow: Motion hallucination from static images for action recognition. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pages 59375947, 2018. 3, 7 [24] Mathias Gehrig, Manasi Muglikar, and Davide Scaramuzza. Dense continuous-time optical flow from event cameras. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(7):47364746, 2024. [25] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, and Deqing Sun. Motion prompting: Controlling video generation with motion trajectories. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 112, 2024. 3 [26] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Frund, Peter N. Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The something something video database for learning and evaluating visual common sense. 2017 IEEE International Conference on Computer Vision (ICCV), pages 58435851, 2017. 5 [27] Xianfan Gu, Chuan Wen, Jiaming Song, and Yang Gao. Seer: Language instructed video prediction with latent diffusion models. ArXiv, abs/2303.14897, 2023. 7 [28] Prerit Gupta, Shourya Verma, Ananth Grama, and Aniket interactive & reactive 3d arXiv preprint Unified multi-modal Bera. motion generation via rectified flow. arXiv:2509.24099, 2025. 2 [29] Tengda Han, Weidi Xie, and Andrew Zisserman. Selfsupervised co-training for video representation learning. ArXiv, abs/2010.09709, 2020. 3 [30] Zekun Hao, Xun Huang, and Serge Belongie. Controllable video generation with sparse trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [18] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffu- [31] Ryan Hoque, Peide Huang, David J. Yoon, Mouli Sivapurapu, and Jian Zhang. Egodex: Learning dexterous 10 manipulation from large-scale egocentric video. ArXiv, abs/2505.11709, 2025. 5 through dense correspondences. ArXiv, abs/2310.08576, 2023. 2 [32] Berthold Klaus Paul Horn, editor. Robot vision. MIT Press, Cambridge, MA, USA, 1986. 3 [33] Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. ICML, 2025. 2, 5, 6, 7, 8 [34] Team Hunyuan3D, Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Penghao Wang, et al. Hunyuan3d-omni: unified framework for controllable generation of 3d assets. arXiv preprint arXiv:2509.21245, 2025. 2 [35] Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Video generative foundation models with multiIn Proceedings of the modal control via full attention. IEEE/CVF International Conference on Computer Vision, pages 1573715747, 2025. 2 [36] Yash Kant, Ethan Weber, Jin Kyu Kim, Rawal Khirodkar, Su Zhaoen, Julieta Martinez, Igor Gilitschenski, Shunsuke Saito, and Timur Bagautdinov. Pippo: High-resolution multi-view humans from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1641816429, 2025. 2 [37] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Ye Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sung Yul Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean-Pierre Mercat, Abdul Rehman, Pannag R. Sanketi, Archit Sharma, C. Blake Simpson, Quang Uyen Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Da Ling Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Muhammad Zubair Irshad, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosa Maria Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph J. Lim, Jitendra Malik, Roberto Martin-Martin, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. Droid: large-scale in-the-wild robot manipulation dataset. ArXiv, abs/2403.12945, 2024. [38] Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Josh Tenenbaum. Learning to act from actionless videos [39] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jia-Liang Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fan Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Peng-Yu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhen Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Z. Xu, Yang-Dan Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. ArXiv, abs/2412.03603, 2024. 5 [40] Mathis Koroglu, Hugo Caselles-Dupre, Guillaume Jeanneret Sanmiguel, and Matthieu Cord. Onlyflow: Optical flow based motion conditioning for video diffusion models, 2024. 2 [41] Akshay Krishnan, Xinchen Yan, Vincent Casser, and Abhijit Kundu. Orchid: Image latent diffusion for joint appearance and geometry generation. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 2 [42] Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart Russell, and Pieter Abbeel. Learning Plannable Representations with Causal InfoGAN. In Neural Information Processing Systems, 2018. 2 [43] Teyun Kwon, Norman Di Palo, and Edward Johns. LanIEEE guage models as zero-shot trajectory generators. Robotics and Automation Letters, 9(7):67286735, 2024. [44] Black Forest Labs, Stephen Batifol, A. Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. ArXiv, abs/2506.15742, 2025. 4, 5, 15 [45] Hoang Le, Feng Liu, Shu Zhang, and Aseem Agarwala. Deep homography estimation for dynamic scenes. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 76497658, 2020. 4 [46] Jangwon Lee and Michael Ryoo. Learning Robot Activities from First-Person Human Videos Using Convolutional Future Regression. In CVPRW, 2017. 2 [47] Guojun Lei, Chi Wang, Rong Zhang, Yikai Wang, Hong Li, and Weiwei Xu. Animateanything: Consistent and controllable animation for video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 3 [48] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chi-Hou Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, and Tao Kong. Visionlanguage foundation models as effective robot imitators. ArXiv, abs/2311.01378, 2023. 6 [49] Xiaoqi Li, Jingyun Xu, Mingxu Zhang, Jiaming Liu, Yan Shen, Iaroslav Ponomarenko, Jiahui Xu, Liang Heng, Siyuan Huang, Shanghang Zhang, et al. Object-centric prompt-driven vision-language-action model for robotic In Proceedings of the Computer Vision manipulation. and Pattern Recognition Conference, pages 2763827648, 2025. 2 [50] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Yuexian Zou, and Ying Shan. Image conductor: Precision control for interactive video synthesis. arXiv preprint arXiv:2406.15339, 2024. 3 [51] Jingyun Liang, Yuchen Fan, Kai Zhang, Radu Timofte, Luc van Gool, and Rakesh Ranjan. Movideo: Motion-aware video generation with diffusion model. In European Conference on Computer Vision, 2024. 2 [52] Fanfan Liu, Feng Yan, Liming Zheng, Chengjian Feng, Yiyang Huang, and Lin Ma. Robouniview: Visual-language model with unified view representation for robotic manipulaiton. ArXiv, abs/2406.18977, 2024. 6 [53] Haiyang Liu, Zhan Xu, Fa-Ting Hong, Hsin-Ping Huang, Yi Zhou, and Yang Zhou. Video motion graphs. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. 2 [54] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. 6 [55] Xiao Liu, Yifan Zhou, Fabian Weigend, Shubham Sonawani, Shuhei Ikemoto, and Heni Ben Amor. Diff-control: stateful diffusion-based policy for imitation learning. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 74537460. IEEE, 2024. [56] Bruce D. Lucas and Takeo Kanade. An iterative image registration technique with an application to stereo vision. In International Joint Conference on Artificial Intelligence, 1981. 4, 17 [57] Ao Luo, Xin Li, Fan Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Flowdiffuser: Advancing optical flow estimation with diffusion models. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1916719176, 2024. 3 [58] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for languageconditioned policy learning for long-horizon robot manipIEEE Robotics and Automation Letters, 7: ulation tasks. 73277334, 2021. 6, 7 [59] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3M: Universal Visual Representation for Robot Manipulation. In Conference on Robot Learning, 2022. 2 [60] Muyao Niu, Xiaodong Cun, Xintao Wang, Yong Zhang, Ying Shan, and Yinqiang Zheng. Mofa-video: Controllable image animation via generative motion field adaptions in frozen image-to-video diffusion model. In European Conference on Computer Vision (ECCV), 2024. 3 [61] Nvidia, Johan Bjorck, et al. Gr00t n1: An open founArXiv, dation model for generalist humanoid robots. abs/2503.14734, 2025. 2 [62] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. ArXiv, abs/2504.06256, 2025. 2, 3 [63] Jyothish Pari, Nur Muhammad Shafiullah, Sridhar Pandian Arunachalam, and Lerrel Pinto. The Surprising Effectiveness of Representation Learning for Visual Imitation. In Robotics: Science and Systems, 2022. 2 [64] Bo Peng, Xinyuan Chen, Yaohui Wang, Chaochao Lu, and Yu Qiao. Conditionvideo: Training-free condition-guided video generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 44594467, 2024. 2 [65] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, MingChang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. In IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2024. 2 [66] Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Korrawe Karunratanakul, Pu Wang, Hongfei Xue, Chen Chen, Chuan Guo, Junli Cao, Jian Ren, and Sergey Tulyakov. Controlmm: Controllable masked motion generation. arXiv preprint arXiv:2410.10780, 2024. 2 [67] Kanchana Ranasinghe, Xiang Li, Cristina Mata, Jong Sung Park, and Michael S. Ryoo. Pixel motion as universal representation for robot control. ArXiv, abs/2505.07817, 2025. 1, 2, 3, 4, 5, 6, 7, [68] Scott Reed, Konrad Zolna, Emilio Parisotto, and et al. generalist agent. Transactions on Machine Learning Research (TMLR), 2022. 2 [69] Juntao Ren, Priya Sundaresan, Dorsa Sadigh, Sanjiban Choudhury, and Jeannette Bohg. Motion tracks: unified representation for human-robot transfer in few-shot imitation learning. ArXiv, abs/2501.06994, 2025. 2 [70] Moritz Reuss, Omer Erdinc Yagmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. ArXiv, abs/2407.05996, 2024. 5, 6 [71] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2Robot: Learning Manipulation Concepts from Instructions and Human Demonstrations. IJRR, 2021. 2 [72] Pratyusha Sharma, Deepak Pathak, and Abhinav Gupta. Third-person visual imitation learning via decoupled hierarchical controller. In Neural Information Processing Systems, 2019. [73] Yash Sharma, Yi Zhu, Chris Russell, and Thomas Brox. Pixel-level correspondence for self-supervised learning from video. ArXiv, abs/2207.03866, 2022. 3 [74] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Y. Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Da, and Hongsheng Li. Motion-i2v: Consistent and controllable imageto-video generation with explicit motion modeling. ACM SIGGRAPH 2024 Conference Papers, 2024. 3 12 [75] Aravind Sivakumar, Kenneth Shaw, and Deepak Pathak. Robotic Telekinesis: Learning Robotic Hand Imitator by Watching Humans on Youtube. In Robotics: Science and Systems, 2022. 2 [76] Daeun Song, Jing Liang, Xuesu Xiao, and Dinesh Manocha. Vl-tgs: Trajectory generation and selection using vision language models in mapless outdoor environments. IEEE Robotics and Automation Letters, 2025. [77] Sruthi Sudhakar, Ruoshi Liu, Basile Van Hoorick, Carl Vondrick, and Richard Zemel. Controlling the world by sleight of hand. ArXiv, abs/2408.07147, 2024. 2, 7 [78] Shikun Sun, Min Zhou, Zixuan Wang, Xubin Li, Tiezheng Ge, Zijie Ye, Xiaoyu Qin, Junliang Xing, Bo Zheng, and Jia Jia. Minimal impact controlnet: Advancing multiIn Int. Conf. Learn. Represent., controlnet integration. 2025. 2 [79] Shao-Hua Sun, Hyeonwoo Noh, Sriram Somasundaram, and Joseph Lim. Neural program synthesis from diverse demonstration videos. In International Conference on Machine Learning, 2018. 2 [80] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In European Conference on transforms for optical flow. Computer Vision, pages 402419. Springer, 2020. 3, 4 [81] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motioneditor: Editing video motion via content-aware diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78827891, 2024. 2 [82] Jacob Walker, Abhinav Kumar Gupta, and Martial Hebert. Dense optical flow prediction from static image. In Int. Conf. Comput. Vis. (ICCV), pages 24432451, 2015. 3 [83] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Xiaofeng Meng, Ningying Zhang, Pandeng Li, Ping Wu, Ruihang Chu, Rui Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, WenChao Zhou, Wente Wang, Wen Shen, Wenyuan Yu, Xianzhong Shi, Xiaomin Huang, Xin Xu, Yan Kou, Yan-Mei Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhengbin Han, Zhigang Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. ArXiv, abs/2503.20314, 2025. [84] Boyang Wang, Nikhil Sridhar, Chao Feng, Mark Van der Merwe, Adam Fishman, Nima Fazeli, and Jeong Joon Park. This&that: Language-gesture controlled video generation for robot planning. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 1284212849. IEEE, 2025. 2 [85] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. 2 [86] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video arXiv preprint synthesis with motion controllability. arXiv:2306.02018, 2023. 3 [87] Yihan Wang, Lahav Lipson, and Jia Deng. Sea-raft: Simple, efficient, accurate raft for optical flow. In Eur. Conf. Comput. Vis. (ECCV), pages 3654. Springer, 2024. 3 [88] Zhouxia Wang, Fanqing Lin, Xintao Wang, Takuhiro Kaneko, Jianfeng Lu, Chuan He, Fengyun Han, Zhibin Wang, Yuxu Wang, Jie Chen, Jianxiong Xiao, Chao Dong, Zhaoxiang Zhang, and Xiaokang Yang. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Proceedings, 2024. 1, 3 [89] Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, and Pieter Abbeel. Any-point trajectory modeling for policy learning. ArXiv, 2023. 1, 2, 3, 17, [90] Youpeng Wen, Junfan Lin, Yi Zhu, Jianhua Han, Hang Xu, Shen Zhao, and Xiaodan Liang. Vidman: Exploiting implicit dynamics from video diffusion model for effective robot manipulation. ArXiv, abs/2411.09153, 2024. 6 [91] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. ArXiv, abs/2506.18871, 2025. 2, 3, 4, 5, 8, 15 [92] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative prearXiv preprint training for visual robot manipulation. arXiv:2312.13139, 2023. 6 [93] Weijia Wu, Jiajun Liang, Jianqiao Zhou, Jian Zhang, Qingsheng Han, Jinlin Liu, Xiangyu Fan, Dongdong Yu, Zaidan Zhang, and Changhua Zhou. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision (ECCV), 2024. 3, 5 [94] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1329413304, 2024. 2, 3 [95] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. DynamiCrafter: Animating In Eur. open-domain images with video diffusion priors. Conf. Comput. Vis. (ECCV), pages 399417, 2024. 7 [96] Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein, Manuela Veloso, and Shuran Song. Flow as the cross-domain manipulation interface. In Conference on Robot Learning, 2024. 1, 2, 3, 4 [97] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, 13 [110] Haitao Zhou, Chuang Wang, Rui Nie, Jinlin Liu, Dongdong Yu, Qian Yu, and Changhu Wang. Trackgo: flexible and efficient method for controllable video generation. arXiv preprint arXiv:2408.11475, 2024. 3 Seonghyeon Ye, Joel Jang, Yuquan Deng, Lars Liden, and Jianfeng Gao. Magma: foundation model for multimodal ai agents. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1420314214, 2025. 2, 3, 4, [98] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-tovideo diffusion models with an expert transformer. ArXiv, abs/2408.06072, 2024. 5, 7, 18, 19 [99] Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot In IEEE Conf. Comput. Vis. text-driven motion transfer. Pattern Recog. (CVPR), 2024. 7 [100] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Ming Gong, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.10640, 2023. 1, 3, 5 [101] Xin Yu, Tianyu Wang, Soo Ye Kim, Paul Guerrero, Xi Chen, Qing Liu, Zhe Lin, and Xiaojuan Qi. Objectmover: Generative object movement with video prior. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1768217691, 2025. 3 [102] Chengbo Yuan, Chuan Wen, Tong Zhang, and Yang Gao. General flow as foundation affordance for scalable robot learning. arXiv preprint arXiv:2401.11439, 2024. 1, 2, 3 [103] Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, and Huazhe Xu. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. In Proceedings of Robotics: Science and Systems (RSS), 2024. 6 [104] Hang Zhang, Jinglin Chen, Chen Ma, and Yuan Lin. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2404.15617, 2024. [105] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 38133824, 2023. 2 [106] Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, He Wang, Zhizheng Zhang, Li Yi, Wenjun Zeng, and Xin Jin. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge. ArXiv, abs/2507.04447, 2025. 3, 5, 6 [107] Kaifeng Zhao, Gen Li, and Siyu Tang. Dartcontrol: diffusion-based autoregressive motion model for real-time text-driven motion control. In International Conference on Learning Representations, 2024. 3 [108] Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with arXiv preprint arXiv:2304.13705, low-cost hardware. 2023. 6 [109] Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, and Haoang Li. Flowvla: Thinking in motion with visual chain of thought. ArXiv, 2025. 3, 4 14 Future Optical Flow Prediction Improves Robot Control & Video Generation"
        },
        {
            "title": "Contributions",
            "content": "KR led the project by implementing the preliminary idea of language-driven OF prediction, building the codebase for experimentation, and performing most of the evaluations. HZ discussed all aspects of the project, contributed to several key design choices (on model architecture, OF calculation, video evaluations), and helped debug several technical issues. YF discussed multiple aspects of the robotic evaluation pipelines, helped implement the RoboTwin evaluations, and performed several robotic baseline evaluations. LY contributed to building the video evaluation pipeline, helped investigate our novelty against prior works, and discussed several aspects of our video generation pipeline. LX proposed using the unified VLM-Diffusion architecture, provided feedback on our data and training pipeline implementations, and discussed several early results. RX helped streamline early exploration of the idea, supported scaling up our training pipeline, and provided several critical feedback on project design. CX & SS provided the strategic vision for the project, oversaw the research environment, and shaped the high-level framing of the research problem. MR set the direction for robotic downstream tasks and discussed multiple aspects of the project idea, scope, and implementation. JN organized the overall project, set the research direction, and discussed all aspects of the project idea, scope, and implementation."
        },
        {
            "title": "Appendix Contents",
            "content": "A. Additional Architectural Details B. Optical Flow Representation C. Optical Flow Calculation D. Motion-Guided Frame Sampling E. Additional Ablations F. Detailed Limitations G. Visualizations 15 16 16 17 18 18 A. Additional Architectural Details We provide additional details of our architecture in this section, focusing on the conditional input processing and the modifications made to the Diffusion Transformer (DiT). To 15 recap, our overall architecture contains 3 main components: the VLM (3B parameters), VAE (83M parameters), and DiT (4B parameters). The DiT (diffusion transformer) module is the key component that we modify and train to construct our FOFPred model. A.1. Conditional Input Processing The core architecture utilizes two distinct conditional features: fc (textual) and fv (visual), which are passed through Multi-Layer Perceptrons (MLPs) to ensure dimensional compatibility with the Diffusion Transformer. Textual Feature Projection: The textual feature fc, obtained from the Qwen2.5-VL [4] Vision-Language Model (VLM), has an initial channel dimension of 2520. This feature is projected via an MLP layer to common channel dimension D. The VLM input consists of the natural language caption paired with the interleaved visual inputs xt1 and xt. Visual Feature Projection: The visual feature fv, obtained from the Flux.1 VAE [44] encoder, has an initial channel dimension of 16. This feature is reshaped using 2 2 grid to obtain 64 = 16 2 2 channel dimension vectors, followed by projected via different MLP layer to the same common channel dimension D. Initial Noise Vector: The noise vector, sampled from the VAE latent space, is also projected into the DiT channel dimension using the same reshaping operation and visual feature project MLP layer. We enforce the condition that the output dimensions of the two MLPs are equal to = 2520 which is the DiT input channel dimension. These projected conditional features, ˆfc and ˆfv, are simply appended to the input noise sequence of the DiT, providing comprehensive context for the diffusion process. A.2. DiT Modifications for Temporal Modeling Our Diffusion Transformer is based on the OmniGen architecture [91], adapted to explicitly handle the temporal nature of our sequence prediction task (future optical flow ˆyt+1:t+4 from inputs xt1, xt). Time-Aware 3D RoPE: We implemented modified RoPE module to handle the required temporal dimension within the sequence. The original OmniGen RoPE provides 3axis position encoding (L, H, ) for text length (L), image height (H), and image width (W ). Our modification interprets these three axes as: Axis 1 (Text/Image Shift): Used to assign unique base position IDs for the text tokens and for each separate image (or image sequence). This axis is now leveraged to encode the temporal offset or frame index for the input sequences (xt1, xt) and the latent noisy output y. Axis 2 and 3 (Image H, ): Encode the spatial positions within each frames patch grid. Specifically, for multiple frames (e.g., xt1 and xt representations), the position shift variable is incremented after processing each frame. This ensures that while text tokens and xt1 tokens receive their respective base position IDs, the tokens corresponding to xt receive unique, subsequent base position ID, which acts as temporal index. Overall, this ensures that the tokens are timestamped with temporal position using RoPE, allowing the transformer to distinguish between and model the relationship across the consecutive time steps in both the input and output frame sequences. Spatio-Temporal Attention: The DiT transformer blocks are updated to perform full spatio-temporal attention over the output tokens. This means the self-attention mechanism is configured to operate over the entire input sequence fin = [ ˆfc, ˆfv, fy], where fy is the latent sequence of the noisy optical flow. This design choice allows the DiT to explicitly capture the motion dynamics and sequence dependencies required for accurate predictions of future optical flow frame sequences. B. Optical Flow Representation The visualization algorithm begins by converting the Cartesian optical flow field, denoted as R2HW with components fx and fy, into polar representation. The magnitude of the flow is computed as = and normalized by scaling factor η = 64.0. This normalized magnitude, ˆM , is clamped to the range [0, 1]. The directional angle is derived using the four-quadrant inverse tangent function, ϕ = arctan2(fy, fx), and is shifted by π to ensure the final angle θ lies within the interval [0, 2π]. + 2 2 (cid:113) Algorithm 1 Relative Optical Flow Calculation Require: 1 Fraw RB2HW : Default optical flow tensors 2 τransac: RANSAC reprojection threshold (default 5.0) 3 s: Sampling stride (default 8) 4 τnoise: Post-compensation noise threshold (default 0.5) Ensure: 5 Fcomp: Camera-motion compensated flow tensors 6 procedure COMPENSATEFLOW(Fraw, s, τransac, τnoise) 7 B, C, H, shape(Fraw) Fcomp List() {(x, y) 0 < W, 0 < H} Sample(G, s); p0 Coords(S) for 0 to 1 do fi Fraw[i] Derive Correspondences Extract(fi, S) p1 p0 + v"
        },
        {
            "title": "Estimate Homography",
            "content": "H FindHomography(p0, p1, RANSAC, τransac) if is valid then Compute Camera Flow & Compensation PerspectiveTransform(G, H) fcam fobj fi fcam Post-Compensation Thresholding if Threshold Enabled then fobj2 fobj[M < τnoise] 0 end if ffinal fobj else ffinal fi end if Append ffinal to Fcomp 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 end for return Stack(Fcomp) 30 31 end procedure C. Optical Flow Calculation Subsequently, these polar coordinates are mapped to the HSV color space to generate an RGB image. The hue channel is assigned the flow angle θ, encoding the direction of motion, while the saturation channel is defined by the normalized magnitude ˆM . The value channel is set to constant maximum intensity of 1 (formulated in the algorithm as ˆM + (1 ˆM )) - this eliminates large color variances across consecutive frames due to outliers. Finally, the resulting HSV tensor is converted into RGB format using differentiable color space transformation. The proposed method (Algorithm 1) compensates for camera motion within dense optical flow fields by leveraging the flow data itself to derive dense correspondences. Rather than relying on computationally expensive sparse feature extraction, we uniformly sample grid points from the source frame and project them into the target frame using the raw flow vectors Fraw. These calculated point pairs serve as inputs for RANSAC-based homography estimation, which computes transformation matrix representing the global camera motion. dense camera flow field, Fcam, is sub16 Algorithm 2 Motion-Aware Frame Selection Require: Video Sequence = {I1, I2, . . . , IN }, Threshold τ , Top-Percentile Ensure: Selected Frame Indices 1 Initialize selected set 2 for 1 to 1 do 3 4 6 Step 1: Spatial Downsampling for efficiency t+1 Resize(It, It+1, 32 32) , low"
        },
        {
            "title": "I low\nt",
            "content": "FLK LucasKanade(I low Step 2: Fast Optical Flow Approximation , low t+1) 7 Step 3: Compute Motion Proxy 8 FLK2 Calculate L2 norm of flow vectors µproxy Percentile(M, k) Extract top-k% value Step 4: Threshold Filtering 10 11 12 if µproxy > τ then {t} 13 end if 14 end for 15 return sequently synthesized by applying to the entire coordinate grid and calculating the displacement vectors. Finally, the object-centric motion is isolated by subtracting the estimated camera flow from the raw flow, such that Fobj = Fraw Fcam. post-processing magnitude threshold is applied to Fobj to suppress residual noise and artifacts arising from imperfect alignment. We reiterate that this one-time process is run offline. For our 500,000 training videos, we are able to complete processing using 4 A100 GPUs in roughly 30 hours. This timing also includes the motion-guided frame sampling (negligible time for this operation compared to relative flow calculation) that we describe in the next section. D. Motion-Guided Frame Sampling Training video prediction models on natural sequences requires careful data curation, as large portions of raw video may contain static scenes or imperceptible motion that contribute little to the learning process. To address this, we employ two-stage filtering pipeline, detailed in Algorithm 2, which acts as computational gate to prioritize high-motion segments before generating expensive ground-truth labels. Calculating dense optical flow (e.g., using RAFT or PWCNet) for every frame pair in large-scale dataset is computationally prohibitive. To circumvent this bottleneck, we utilize lightweight approximation strategy (Lines 46). We first spatially downsample all input frames to resolution of 32 32 pixels. This reduction removes high-frequency textures and compression artifacts while preserving dominant structural motion. On these low-resolution pairs (I low), we apply the Lucas-Kanade method [56]. Unlike deep learning approaches, Lucas-Kanade relies on local least-squares op17 timization, which converges rapidly on small spatial grids (32 32), allowing for high-throughput processing of millions of frames. To robustly distinguish meaningful scene activity from background noise, we derive scalar motion proxy, µproxy, from the raw flow field (Lines 89). We compute the magnitude (L2 norm) of the flow vectors and select the top-k percentile value (k = 10) rather than the mean. This percentile-based approach ensures that the metric is driven by the moving objects within the scene, rather than being diluted by large static background regions. Frame pairs are included in the final training dataset only if this proxy value exceeds an empirical threshold τ (Lines 11 12), ensuring the model focuses on sequences with significant temporal dynamics. The result of this motion-based frame filtering is the elimination of seemingly static regions of the video. In practice, we observe that certain static regions are removed while most frames in other motion-heavy regions remains. The resulting filtered videos still produce coherent video segments without breaking the natural temporal continuity of the motion. The low threshold for motion filtering (we use 5 pixel length as threshold for 256 256 images) is crucial for this. We set this based on empirical observation over randomly selected set of videos. We highlight again that tune the filtering process is important to ensure coherent frame sequences, instead of generating frame sequences that may contain large discontinuities. E. Additional Ablations We provide two new ablations on using dense optical flow as our representation and the significance of the motion signal passed into our robot action policy in Table A.1 and Table A.2 respectively. Dense vs. Sparse Motion: We investigate the impact of dense vs. sparse motion representations on the downstream robot control task in Table A.1, reporting average length on the CALVIN benchmark. Training and evaluation settings are identical to our ablations in the main paper. To isolate the value of our dense representation, we compare our method against two sparse variants: naive baseline where our predicted future optical flows are subsampled to 16 16 grid, and re-implementation of the sparse trajectory model, ATM [89], trained on our dataset. The results demonstrate distinct advantage for spatially dense motion information. Our default dense model achieves an average length of 4.39, significantly outperforming the ATM baseline (2.92) and the sub-sampled variant (1.24). This suggests that the fine-grained, pixellevel dynamics captured by dense optical flow are essential for precise robotic manipulation, whereas sparse representations fail to capture the complete motion context required for complex tasks. Method Motion Avg Len Naive Baseline ATM Ours (default) Sparse Sparse Dense 1.24(-3.15) 2.92(-1.47) 4.39 Table A.1. Dense vs. Sparse Motion: We report the average length metric (Avg Len) on CALVIN benchmark. We demonstrate the significance of our dense motion representation. The first row uses naive sub-sampling of our predicted future optical flows to obtain motion for points on uniform 16 16 grid on the image. This variant uses training and inference identical to ours. The second row re-implements the ATM [89] approach with training on our same data but inference similar to the original work. We highlight the clear improvements arising from our proposed method. Method Motion Input Avg Len No motion input Static visual input Ours (default) 0.02(-4.37) 0.52(-3.87) 4. Table A.2. Motion Input Ablation: We report the average length metric (Avg Len) on CALVIN benchmark. For our robot control extension, we ablate the motion input under two settings. First (row 1) we remove the motion input leaving only the state and text goal inputs. Next (row 2) we replace the motion input with an embedding (from our VAE) of the visual input. Our motion inputs in the form of future optical flow clearly leads to performance improvement. Ablations on Motion Forecast Conditioning: We investigate the usefulness of motion forecasting as opposed to generic visual representations for our downstream robot control task in Table A.2. We compare our full framework against two baselines: one where motion input is entirely removed (note that our VLM input is representation of both images and text), and another where the motion input is replaced by static visual embeddings from our VAE encoder. The results are decisive; the policy fails almost entirely without motion input (0.02) and shows only marginal improvement when provided with the static visual features (0.52). In contrast, conditioning on our predicted future optical flow embeddings yields score of 4.39. This confirms that the predicted optical flow provides unique, critical dynamic information that static visual representations cannot substitute. F. Detailed Limitations We have discussed limitations and future directions of our work in Section 5.1 of the main paper. In this section, we expand our discussion on limitation of our proposed FOFPred model in detail, particularly focused on our base model (i.e. the result of our large-scale pretraining). In Figure A.1, we explore the diversity of our FOFPred Figure A.1. Sensitivity to seed: We visualize 4 FOFPred predictions for the same image and prompt, \"Moving the bowl from left to right\", but using 4 different starting noise vectors for the reverse diffusion process. Notice how the upperright corner conflates the object motion with camera motion instead; however this camera motion does correspond to the object motion described in the provided prompt. In the lower left image, in addition to the desired object motion, we again observe slight amount of corresponding camera motion. base model predictions. Across different seeds, we notice unexpected camera motion sometimes. While often relevant to the provides motion prompt, this does not explicit capture the object motions that we desire from our model. G. Visualizations We next visualize some videos generated with our framework for pairs of first frames and motion-focussed captions obtained from the SSv2 dataset. Figure A.2 presents this qualitative comparison between the ground truth (GT) video, our T2V baseline from CogVideoX [98], and our framework, FOFPred1. The samples are selected from the SSv2 validation split to demonstrate generation capabilities conditioned on the first frame (outlined in green) and specific text goal. As observed across the samples, FOFPred consistently exhibits superior motion adherence compared to the baseline, which frequently struggles to generate significant movement or directionally accurate dynamics (e.g., the Moving glue stick and Moving cycle examples). While FOFPred successfully executes the semantic requirements of the text prompts, we also visualize partial failure case in the second row (Pulling toy car). In this instance, while our model correctly adheres to the motion instruction, our extended pipeline exhibits trade-off in visual fidelity, resulting in slight distortion of the objects appearance compared to the baseline. However, the baseline in this case moves the car in the wrong direction. 1In our work, we explore Text-to-Video (T2V) generation using two-stage pipeline that connects FOFPred with the existing video synthesis model Go-with-the-Flow (GWTF) [11]. GWTF is built on top of CogVideoX [98] and extends it to additionally accept user-provided motion prompt as input. 18 Figure A.2. Visualization of success and failure cases for Text-to-Video (T2V) generation: We visualize some success and failure cases for our framework over the baseline, CogVideoX [98]. Examples are drawn from the SSv2 validation split. We note that our method consistently improves motion adherence over the baseline. However, in some cases our framework distorts the visual appearance of objects although they undergo correct movement (e.g., see toy car in Row 2). Checkout our FOFPred.github.io for more visualizations. For more visualizations of our proposed FOFPred framework, checkout our website FOFPred.github.io."
        }
    ],
    "affiliations": [
        "Salesforce AI Research",
        "Stony Brook University"
    ]
}