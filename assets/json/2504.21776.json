{
    "paper_title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
    "authors": [
        "Xiaoxi Li",
        "Jiajie Jin",
        "Guanting Dong",
        "Hongjin Qian",
        "Yutao Zhu",
        "Yongkang Wu",
        "Ji-Rong Wen",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \\textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \\textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker."
        },
        {
            "title": "Start",
            "content": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability Xiaoxi Li1, Jiajie Jin1, Guanting Dong1, Hongjin Qian2, Yutao Zhu1, Yongkang Wu3, Ji-Rong Wen1, Zhicheng Dou1 1Renmin University of China 2BAAI 3Huawei Poisson Lab {xiaoxi_li, dou}@ruc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose WebThinker, deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates Deep Web Explorer module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an Autonomous Think-Search-and-Draft strategy, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an RL-based training strategy via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker. 5 2 0 2 0 3 ] . [ 1 6 7 7 1 2 . 4 0 5 2 : r Figure 1: Overall performance comparison of WebThinker with other models across two tasks: complex reasoning problem-solving (left) and scientific research report generation (right). Equal contribution. Corresponding author. Preprint. Work in progress."
        },
        {
            "title": "Introduction",
            "content": "Recently, large reasoning models (LRMs) have demonstrated remarkable capabilities across various domains like math, code, and science [35, 50, 5]. However, when confronted with complex information research needs, models that rely solely on internal knowledge struggle to conduct in-depth web information retrieval and to generate comprehensive and accurate scientific reports through multi-step reasoning. Therefore, the deep integration of LRMs reasoning capabilities with web information exploration has become practical demand, which has sparked series of deep research initiatives by OpenAI [36], xAI Grok3 [14], and Google Gemini [12]. The objective of deep research technology is quite revolutionary: enabling users to conduct deep searches, mining, and integration of comprehensive and reliable research information across the internets massive information landscape through simple queries. This approach significantly reduces the time and costs associated with information gathering for researchers in knowledge-intensive fields (e.g., finance, science, engineering). Unfortunately, existing open-source deep search agents typically employ retrieval-augmented generation (RAG) techniques with predefined workflows [1, 63, 43], which limits LRMs ability to explore deeper web information and hinders close interaction between LRMs and search engines (Figure 2 (a) & (b)). Consequently, developing universal, flexible, open-source deep research framework has emerged as critical challenge urgently awaiting resolution in both academic and industrial circles. To address this, we propose WebThinker, an open-source deep research agent entirely powered by reasoning models, as illustrated in Figure 2 (c). It enables LRMs to autonomously conduct web searches and web page navigation to acquire external knowledge during the reasoning process, facilitating complex real-world problem solving. Furthermore, WebThinker allows LRMs to draft reports concurrently with thinking and searching. Once sufficient information is gathered for specific section, the model can draft that part, ultimately producing comprehensive and customized reports tailored to users research questions. To empower LRMs with the capability to deeply explore web information, we design Deep Web Explorer module that enables LRMs to search, navigate pages by clicking interactive elements (like links or buttons), and extract relevant information. Based on the current querys search results, the LRM can initiate follow-up searches and traverse deeper links until it collects all relevant information. To further facilitate scientific report writing, we introduce an Autonomous Think-Search-and-Draft strategy that deeply integrates report writing with the reasoning and search processes of LRMs. Rather than generating the entire report all at once after searching, our approach enables the model to draft and seek necessary knowledge in real-time. To achieve this, we equip LRMs with three specialized tools: (1) drafting content for specific chapters, (2) checking the current report, and (3) editing the report. These tools enable LRMs to autonomously enhance the report by maintaining comprehensiveness, coherence, and adaptability to newly discovered information during reasoning. To further unlock the deep research potential of LRM backbones, we develop RL-based training strategies to optimize end-to-end task performance. We leverage the LRM equipped with our tools to sample large-scale reasoning trajectories from complex tasks [48, 57, 49, 65, 25]. Using the accuracy of reasoning, tool usage, and final outputs, we construct preference pairs for online DPO training [41, 58, 6]. Through iterative, on-policy training, the model progressively improves its ability to perceive, reason, and interact with research tools effectively. We conduct extensive experiments on (1) knowledge-intensive complex reasoning benchmarks, including GPQA [42], GAIA [33], WebWalkerQA [57], and Humanitys Last Exam (HLE) [38] to assess complex problem-solving capabilities, and (2) open-ended reasoning tasks from Glaive [13] to evaluate report quality. As shown in Figure 1, WebThinker consistently outperforms competing approaches on both problem-solving and report generation tasks. In summary, our core contributions are as follows: 1. We introduce WebThinker, deep research agent that autonomously search, deeply explore web pages, and draft research reports, all within its thinking process. Unlike traditional predefined workflow, WebThinker enables the LRM itself to perform actions on its own while thinking, achieving end-to-end task execution in single generation. 2 Figure 2: Comparison of RAG paradigms: (a) Standard RAG workflow, (b) Iterative RAG workflow, and (c) WebThinker, reasoning agent that autonomously search, deeply explore web pages, and draft research reports, all within its continuous thinking process. 2. We propose Deep Web Explorer that empowers LRMs with web search and navigation capabilities to deeply gather, traverse, and extract high-quality information from the web. 3. We introduce an Autonomous Think-Search-and-Draft strategy that enables real-time report writing during the thinking and searching process. 4. We develop RL-based training strategies that iteratively synthesize tool-usage preference data and apply online DPO training to enhance the LRMs tool utilization capabilities. 5. We demonstrate the effectiveness of WebThinker on complex reasoning tasks and scientific report generation tasks with both QwQ-based [50] and DeepSeek-R1-based [5] LRM backbones."
        },
        {
            "title": "2 Related Work",
            "content": "Large Reasoning Models. Large reasoning models (LRMs) enhance test-time performance through extended reasoning, setting them apart from traditional large pre-trained models that scale mainly with model size or data volume [16, 62, 40, 29, 3]. Models like OpenAI-o1 [21], Qwen-QwQ [50], and DeepSeek-R1 [5] demonstrate explicit chain-of-thought (CoT) reasoning [56], resembling human system-2 thinking in tasks such as math and programming [29]. Various strategies have been proposed to achieve o1-like reasoning capabilities [5, 11, 4, 47, 60, 17]. Some approaches introduce intentional reasoning errors during training to help models partially internalize reasoning patterns [39, 64]. Others enhance reasoning ability through distilled training data [34]. More recently, reinforcement learning has been explored as means to develop long CoT abilities in LLMs [5, 4, 60, 17]. However, these methods are constrained by their reliance on static, parameterized architectures that lack access to external world knowledge. This limitation becomes particularly problematic in complex reasoning tasks requiring extensive real-world information. Retrieval-Augmented Generation. Retrieval-augmented generation (RAG) enhances generative models by integrating retrieval mechanisms, enabling access to external knowledge beyond static parameters [24, 66, 28, 68]. Recent advances cover multiple dimensions, including retrieval necessity [46], query reformulation [32, 53], document compression [61, 23, 70], denoising [31, 9], and instruction-following [8, 7, 69]. Moreover, complex workflows such as structured planning [27, 52, 59, 18] and decision-making frameworks [30, 15] have shown notable gains in multi-hop reasoning, planning, and domain-specific tasks [54]. Recent work has also explored integrating o1-style reasoning with retrieval. For example, Searcho1 [26] incorporates an agentic RAG framework and Reason-in-Documents module to merge retrieval with reasoning via prompt engineering. Other studies employ reinforcement learning to train reasoning with search capabilities from scratch [22, 44, 2, 67, 45], showing strong results in Wikipedia-based QA. Nonetheless, these methods fall short in adapting to complex real-world reasoning scenarios and comprehensive report-writing tasks. 3 Figure 3: Overview of the WebThinker framework. It operates in two modes: (1) Problem-Solving Mode equips reasoning models with search tool backed by Deep Web Explorer, enabling thorough web exploration to retrieve relevant information for solving complex real-world problems. (2) Report Generation Mode extends the models capabilities with writing, checking, and editing capabilities, allowing it to iteratively craft comprehensive research reports while thinking and searching."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Problem Formulation We consider complex reasoning task that requires both multi-step reasoning and the utilization of research tools. The objective is to generate comprehensive solution for given task query q, guided by an instruction I. solution comprises logical reasoning chain and final output (which could be an answer or signal indicating completion). WebThinker enables the reasoning model to autonomously invoke tools from an available set during its reasoning process, which can be formalized as the mapping (I, q, ) (R, y). The generation process can be expressed as: (R, I, q, ) = (cid:89)Tr t=1 (cid:124) (Rt R<t, I, q, {Oτ }τ <t) (cid:125) (cid:123)(cid:122) Reasoning with Tools (cid:124) (cid:89)Ty t=1 , (yt y<t, R, I, q) (cid:125) (cid:123)(cid:122) Final Output Generation (1) where Tr is the number of tokens in the reasoning sequence R. The token at position is Rt, and R<t represents all tokens generated before position t. {Oτ }τ <t denotes the outputs of all tool calls made before position t. Similarly, Ty is the length of the output sequence y, with yt being the token at position and y<t indicating all generated output tokens before position t. 3.2 Overview of the WebThinker Framework WebThinker is designed to enhance large reasoning models with deep research capabilities by enabling autonomous web exploration and report generation during the reasoning process. As illustrated in Figure 3, WebThinker operates in two primary modes: Problem-Solving Mode: Empowers the LRM with Deep Web Explorer module. When encountering knowledge gaps, the LRM can autonomously initiate web searches, navigate through web pages by clicking links or buttons, and extract relevant information before continuing its reasoning. This facilitates in-depth information gathering beyond standard shallow search. Report Generation Mode: Implements an Autonomous Think-Search-and-Draft strategy. The LRM can interleave reasoning, information seeking (via the Deep Web Explorer), and report It utilizes specialized tools for drafting, checking, and editing report sections, composition. managed by an assistant LLM, ensuring the final report is comprehensive, coherent, and grounded in the gathered evidence. The subsequent sections provide details of these components and the RL-based training strategies. 4 3.3 Solving Complex Reasoning Tasks with the Deep Web Explorer In the Problem-Solving Mode, WebThinker tackles complex tasks requiring knowledge beyond the LRMs internal parameters. The core component is the Deep Web Explorer tool, Texp , which the LRM can invoke during reasoning (Figure 3). Given task-specific instruction Iq and query q, the LRM generates reasoning chain R, potentially interspersed with calls to the Deep Web Explorer. The final output in this mode is typically direct answer a. The generation process is formalized as: (cid:89)Tr t=1 (Rt R<t, Iq, q, {O(j) (at a<t, R, Iq, q), (R, Iq, q) = exp}j<i(t)) (cid:89)Ta t=1 (2) where = (a1, . . . , aTa ) is the answer sequence. {O(j) Deep Web Explorer calls completed prior to reasoning step t. exp}j<i(t) denotes the set of outputs from all The Deep Web Explorer itself is driven by the LRM, operating under specific instruction Ie. It utilizes two elementary tools: search engine Ts to retrieve web pages based on generated query qs, and navigation tool Tn to interact with elements (e.g., click links) on the currently viewed page(s) D. The explorer, triggered by an information need qs, generates its own internal reasoning chain Re to decide whether to search further or navigate deeper based on the evolving web content Dt it encounters. Its goal is to produce concise output Oexp that addresses the knowledge gap in the main reasoning chain R. The exploration process within the tool is modeled as: (cid:89)Te t=1 (Re,t Re,<t, qs, Dt, Ie) (Oexp Re, qs, D, Ie), (Re, Oexp qs, D, Ie) = (3) where Te is the length of the explorers reasoning chain Re. Dt represents the web content available at step t, which dynamically changes based on search and navigation actions. This hierarchical structure allows the main reasoning process to delegate complex information gathering tasks to the Deep Web Explorer, which can recursively search and navigate the web. 3.4 Generating Comprehensive Reports via Autonomous Think-Search-and-Draft In the Report Generation Mode, the LRM autonomously produces comprehensive reports by interleaving reasoning, searching, and writing. Besides the Deep Web Explorer (Texp) for knowledge acquisition, the LRM utilizes set of report writing tools Twrite = {Tdraft, Tcheck, Tedit}. These tools are implemented by an assistant LLM, separating the complex task orchestration performed by the main LRM from the detailed text manipulation required for report writing. All web pages explored via Texp are accumulated in document memory M. When the main LRM decides to invoke writing tool (e.g., Tdraft), it generates an editing instruction e. The assistant model then receives e, the current report state r, and relevant documents Dtop-k retrieved from M. It produces the updated report content rnew according to: (rnew e, Dtop-k, r) = (cid:89)Trnew t=1 (rnew,t rnew,<t, e, Dtop-k, r), (4) where Trnew is the length of the newly generated/edited report content rnew. The main LRMs role is to orchestrate the overall process: performing reasoning steps, deciding when to explore for more information using Texp, and determining when and how to modify the report using Twrite. The main reasoning process concludes when the LRM generates the EOS token, denoted as yend. This overall process, conditioned on the initial instruction and query q, is formalized as: (R, yend I, q) = (cid:89)Tr t=1 (Rt R<t, I, q, {O(j) exp}j<i(t)) (yend R, M), (5) where {O(j) exp}j<i(t) represents outputs from prior Deep Web Explorer calls. Although the report state is not directly input to the main LRMs generation step (Eq. 5), the LRM implicitly tracks progress and uses tools based on the research status reflected in the reasoning chain and potentially available context. The document memory serves as the knowledge base for the assistant LLM (Eq. 4) executing the writing operations. This iterative design allows the main LRM to maintain high-level control over research direction and report structure, while the assistant LLM handles the detailed writing, ensuring the final report is well-supported by evidence from M. 5 3.5 Improving LRMs with Research Tools via Reinforcement Learning To enhance the LRMs ability to effectively utilize research tools that range from high-level web exploration and report manipulation to elementary search and navigation actions, we employ onpolicy RL training. This focuses on constructing preference data reflecting desired tool usage patterns and applying an iterative online DPO [41, 58, 6] strategy. Preference Data Construction. We generate diverse reasoning trajectories by applying the WebThinker framework to comprehensive set of complex reasoning and report generation datasets: SuperGPQA [48], WebWalkerQA [57], OpenThoughts [49], NaturalReasoning [65], NuminaMath [25] and Glaive [13]. For each query q, our initial LRM produces distinct trajectories {R(i)}n i=1. This self-sampling approach explores varied solution strategies, capturing different tool usage patterns across both the main reasoning chain and the Deep Web Explorers internal reasoning process. Our goal is to identify trajectories that demonstrate not only correctness but also efficient use of the research tools. To achieve this, we establish preference pairs (Rw, Rl), where Rw is the preferred trajectory and Rl is the dis-preferred trajectory. We apply the following criteria iteratively in order of priority to pairs of sampled trajectories (Ri, Rj) for the same task q: 1. Overall Correctness/Quality: If Ri yields correct final answer (for reasoning tasks) or higher quality final report (for report generation tasks), while Rj does not, then Ri is preferred (Rw = Ri, Rl = Rj). This rule takes precedence over all others. 2. Tool Efficiency: If both Ri and Rj reach the correct final answer, the trajectory with fewer total tool calls is preferred. If total_tool_calls(Ri) < total_tool_calls(Rj), then Rw = Ri, Rl = Rj. 3. Thinking Conciseness: If both Ri and Rj are correct and involve the same number of tool calls, the shorter trajectory is preferred when the length ratio exceeds the threshold γ > 1. If len(outputj)/len(outputi) > γ, then Rw = Ri, Rl = Rj. By applying these rules across all valid sampled trajectory pairs for all tasks, we construct collection = {(I, q, Rw, Rl)k} of preference pairs. Iterative Online DPO Training. We utilize the constructed preference dataset to train the LRM using iterative online DPO. The standard DPO loss function aims to increase the likelihood of preferred trajectories Rw while decreasing the likelihood of dispreferred trajectories Rl, relative to reference policy πref: LDPO(πθ; πref) = E(Rw,Rl)D (cid:20) (cid:18) log σ β log πθ(Rw I, q) πref(Rw I, q) β log πθ(Rl I, q) πref(Rl I, q) (cid:19)(cid:21) , (6) where πθ is the policy being trained, β is hyperparameter controlling the deviation from the reference policy, and σ is the sigmoid function. We employ an iterative online scheme: (1) Train πθ on the current preference set using Eq. 6. (2) Use the updated πθ to sample new trajectories for the tasks (exploration). (3) Apply the preference criteria (1-3) to the new trajectories to generate an updated preference set D. (4) Set and πref πθ, then repeat from step (1). This iterative loop allows the LRM to learn from online feedback generated by its own improving policy, progressively refining its complex tool usage strategy through interaction with the task environment in an on-policy fashion."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Tasks and Datasets We evaluate WebThinker on two primary task categories: Complex Reasoning Benchmarks: Assesses the models ability to solve intricate problems requiring multi-step reasoning and external knowledge. We utilize several benchmarks: GPQA [42], PhD-level science question answering dataset covering physics, chemistry, and biology (we use the diamond set comprising 198 questions); GAIA [33], benchmark designed to evaluate general AI assistants on complex information retrieval tasks (we use 103 questions from the text-only validation subset); WebWalkerQA [57], dataset focused on deep web information retrieval, requiring navigation and extraction (we use the test set containing 680 questions); and Humanitys Last 6 Table 1: Main results on challenging research tasks, including PhD-level science QA, general AI assistants, and web exploring benchmarks. We report Pass@1 metric for all tasks. For 32B models, the best results are in bold and the second are underlined. Results from larger or closed-sourced models are in gray color for reference. denotes results from their official releases. Method GPQA (Science QA) GAIA (General AI Assist.) WebWalkerQA Phy. Chem. Bio. Avg. Level 1 Level 2 Level 3 Avg. Easy Med. Hard Avg. Direct Reasoning (w/o Retrieval) Qwen2.5-32B DeepSeek-R1-32B QwQ-32B 52.3 82.5 84.8 Qwen2.5-72B GPT-4o DeepSeek-R1-671B o1-preview 58.1 62.8 90.7 89.4 30.1 41.9 44.1 39.8 46.2 57.0 59.9 Enhancing Reasoning with RAG Workflow 41.9 RAG-Qwen2.5-32B 40.9 w/ Query Planning 41.9 w/ Iterative RAG 46.2 46.2 45. w/ Query Planning w/ Iterative RAG 59.3 61.6 64.0 84.9 87.2 84.9 RAG-QwQ-32B Autonomous Search within Reasoning OpenAI Deep Research Search-o1-32B WebThinker-32B-Base WebThinker-32B-RL - 84.9 87.2 90.7 - 49.5 51.6 50. 68.4 73.7 68.4 57.9 68.4 84.2 65.9 68.4 52.6 57.9 63.2 68.4 73.7 - 73.7 68.4 78.9 43.4 62.6 64.1 49.5 55.6 74.2 73. 52.0 51.0 53.0 64.6 66.2 65.2 - 67.2 68.7 70.7 20.5 23.1 30.8 20.5 23.1 43.6 - 12.8 30.8 35.9 33.3 48.7 51.3 74.3 53.8 53.8 56. 9.6 17.3 15.4 13.5 15.4 26.9 - 11.8 17.3 19.2 36.5 25.0 28.8 69.1 34.6 44.2 50.0 8.3 0.0 25.0 0.0 8.3 8.3 - 8.3 0.0 8.3 8.3 8.3 8.3 47.6 16.7 16.7 16.7 13.6 17.5 22.3 14.6 17.5 31.1 - 11.8 20.4 24.3 32.0 32.0 35.0 67.4 39.8 44.7 48. 3.8 7.5 7.5 9.4 6.7 5.0 11.9 23.1 29.4 30.6 36.9 28.8 29.4 - 43.1 47.5 58.8 2.5 1.4 2.1 7.1 6.0 11.8 10. 14.3 36.4 35.7 26.1 35.7 32.9 - 35.0 41.1 44.6 3.3 4.2 4.6 3.3 4.2 11.3 7.9 11.3 25.0 25.4 33.5 30.8 31.3 - 27.1 39.2 40. 3.1 3.8 4.3 6.3 5.5 10.0 9.9 15.3 30.7 30.9 31.2 32.5 31.5 - 34.1 41.9 46.5 Exam (HLE) [38], an extremely challenging dataset featuring problems across various disciplines (mathematics, biology, physics, computer science, etc.) that demand advanced search and reasoning skills (we sampled 500 questions from its text-only subset for experimental efficiency). Accuracy for these tasks is judged by Qwen2.5-72B-Instruct [40]. Scientific Report Generation Tasks: Evaluates the models capability to synthesize comprehensive research reports for open-ended questions. We use glaiveai/reasoning-v1-20m (Glaive) [13], largescale dataset of general, open-ended reasoning questions covering wide range of subjects, suitable for evaluating research capabilities across various aspects. We sample 30 diverse questions from this dataset as our test set. Reports are evaluated using average scores judged by DeepSeek-R1-671B [5] and GPT-4o [20]. All data used in our experiments can be found in our GitHub repository3. 4.2 Baselines We compare WebThinker against several baseline methods: Direct Reasoning: Models using only internal knowledge without retrieval. Includes opensource models (Qwen2.5-32B/72B-Instruct [40], Qwen2.5-Coder-32B-Instruct [19], QwQ-32B [50], Llama3.3-70B-Instruct [10]) and closed-source models (DeepSeek-R1-671B [5], GPT-4o [20], o1preview [35], o3-mini [37], Gemini-2.0-Flash-Thinking [12]). Retrieval-Augmented Reasoning: Methods using external knowledge from search engines. We test three variants: (1) Standard RAG, retrieving top-10 documents for the original query before generation; (2) RAG w/ Query Planning, decomposing the query into sub-queries, retrieving for each, then generating; and (3) Iterative RAG, which iteratively retrieves information, deciding whether to search further or generate the final answer. Autonomous Search within Reasoning: Systems integrating search actions into the reasoning process. Includes the open-source Search-o1 framework [26] and non-proprietary systems like OpenAI Deep Research [36], Grok3 DeeperSearch [14], and Gemini2.0 Deep Research [12]. 3https://github.com/RUC-NLPIR/WebThinker/tree/main/data 7 Table 2: Main results on Humanitys Last Exam. We report Pass@1 metric for all tasks. For 32B models, the best results are in bold and the second are underlined. Results from larger or closedsourced models are in gray color for reference. denotes results from their official releases. Method Humanitys Last Exam (Extremely Hard Reasoning Tasks) Math Bio/Med Physics CS/AI Human. Chem. Engineer. Other Avg. Direct Reasoning (w/o Retrieval) Qwen2.5-32B DeepSeek-R1-32B QwQ-32B GPT-4o Gemini-2.0-Flash-Thinking DeepSeek-R1-671B o3-mini (Medium) o3-mini (High) 6.0 6.9 12.6 2.4 8.5 9.3 14.0 18. Enhancing Reasoning with RAG Workflow RAG-Qwen2.5-32B w/ Query Planning w/ Iterative RAG RAG-QwQ-32B w/ Query Planning w/ Iterative RAG 4.2 6.0 5.1 7.9 11.2 10.2 Autonomous Search within Reasoning OpenAI Deep Research Search-o1-32B WebThinker-32B-Base WebThinker-32B-RL - 12.1 14.9 16. 4.3 Implementation Details 7.0 8.3 14.0 5.3 7.4 8.6 9.8 11.1 7.0 4.7 7.0 14.0 16.3 14.0 - 11.6 16.3 25. 2.0 3.5 4.0 2.2 5.3 5.8 11.5 14.2 8.2 4.0 4.0 2.0 4.0 4.0 - 2.0 6.0 2.0 3.2 4.5 7.9 1.2 5.8 7.4 8.2 11. 1.6 6.3 4.8 4.8 4.8 7.9 - 7.9 9.5 12.7 10.0 7.3 6.0 2.9 7.1 11.0 6.7 6.2 8.0 4.0 6.0 14.0 12.0 10.0 - 14.0 6.0 18. 0.0 7.4 13.3 1.9 6.5 5.6 10.2 10.2 13.3 6.7 13.3 0.0 6.7 13.3 - 6.7 20.0 26.7 5.3 7.0 5.3 1.3 3.8 10.3 7.7 7. 5.3 5.3 5.3 0.0 0.0 10.5 - 10.5 21.1 15.8 4.4 5.7 4.4 3.5 4.0 7.5 6.5 8.0 0.0 6.7 4.4 4.4 11.1 8.9 - 15.6 15.6 15. 5.4 6.4 9.6 2.6 7.1 8.6 11.1 14.0 4.8 5.6 5.4 7.2 9.6 9.6 26.6 10.8 13.0 15.8 For the backbone reasoning model for WebThinker, we utilize the open-source QwQ-32B [50] in our main results and DeepSeek-R1-Distilled-Qwen-[7B, 14B, 32B] [5] in Figure 5. For assistant models, we use Qwen2.5-Instruct [40] with the same parameters as the backbone LRM. For generation, we use maximum of 81920 tokens, temperature of 0.7, top_p of 0.8, top_k of 20, and repetition penalty of 1.05 across all models. For search, we use the Bing Web Search API, configured for the US-EN region, retrieving the top 10 search results (k=10). Web page content corresponding to the retrieved URLs is fetched using Crawl4AI [51]. Model training consists of 2 iterations of online DPO with maximum sequence length of 32768. For baseline models not specifically trained for o1-like reasoning, we employ Chain-of-Thought (CoT) [56] prompting to guide the reasoning process prior to answer generation. Detailed instructions for all models are provided in Appendix C. All experiments are conducted on 4 nodes of 8 NVIDIA H100-80GB GPUs. 4.4 Results on Complex Problem-Solving Main Results. Tables 1 and 2 present WebThinkers performance on complex reasoning tasks. Key findings include: 1. Limitations of Base LRMs and RAG Workflow: While reasoning models like QwQ-32B outperform standard LLMs (e.g., Qwen2.5-72B on GPQA avg.), their direct reasoning capabilities struggle on knowledge-intensive tasks like GAIA and WebWalkerQA. Although RAG workflows improve performance on these tasks, they fail to show consistent gains on complex HLE tasks due to challenges in integrating with deep reasoning. 2. Autonomous Search Advantage: Incorporating autonomous search within reasoning, as demonstrated by Search-o1, provides notable gains over direct reasoning and basic RAG, particularly on GAIA, WebWalkerQA, and HLE (e.g., Search-o1 avg. 10.8 vs. RAG-QwQ-32B avg. 7.2 on HLE). 3. Superiority of the WebThinker Framework: Our WebThinker framework further enhances performance. The WebThinker-32B-Base model, utilizing the Deep Web Explorer to deepen web exploration, consistently surpasses prior methods like Search-o1 across all benchmarks (e.g., 22.9% and 20.4% improvement on WebWalkerQA and HLE, respectively). 8 Method Glaive (General Research Tasks) Comp. Thorough. Fact. Coherence Avg. Retrieval-Augmented Report Generation 5.3 RAG-Qwen2.5-72B 6.4 RAG-DeepSeek-R1 5.7 6.6 Non-Proprietary Systems Grok3 DeeperSearch Gemini2.0 Deep Research 6.4 8.1 6.1 8. Autonomous Think-Search-and-Draft (Ours) WebThinker-32B-Base WebThinker-32B-RL 8.4 8.3 8.2 8.4 6.4 7.1 7.0 7.7 7.7 7. 6.3 7.1 6.5 7.7 7.8 7.9 5.9 6.8 6.5 7.9 8.0 8. Figure 4: Main results on scientific report generation for general research tasks. Left: Overall performance comparison, reporting average scores evaluated by DeepSeek-R1 and GPT-4o. Right: t-SNE visualization of content embeddings from three randomly sampled topics. Figure 5: Analysis of the performance of WebThinker with DeepSeek-R1-based models across 7B, 14B, and 32B sizes, compared with direct generation and standard RAG approaches. 4. RL Improvement: The WebThinker-32B-RL model, refined through our RL-based training, achieves state-of-the-art results among 32B models on all tested benchmarks. It demonstrates substantial improvements over the Base version (e.g., 8.5% and 21.5% improvement on GAIA and HLE, respectively). Notably, on the challenging HLE benchmark, WebThinker-32B-RL surpasses even the performance of stronger models like o3-mini (High). 4.5 Results on Scientific Report Generation Main Results. Figure 4 (left) shows the performance of WebThinker compared to baselines (RAG and non-proprietary systems) on the Glaive scientific report generation task. Performance is evaluated by DeepSeek-R1 and GPT-4o across Completeness (Comp.), Thoroughness (Thorough.), Factuality (Fact.), and Coherence. WebThinker achieves the highest overall score (8.0), outperforming both RAG baselines and advanced deep research systems such as Gemini-Deep Research (7.9). It particularly excels in Completeness (8.4) and Thoroughness (8.2), while matching the top scores in Factuality (7.7) and Coherence (7.8). These results underscore the effectiveness of the Autonomous Think-Search-and-Draft strategy, which fully unlocks LRMs capabilities to iteratively refine content through dynamic information gathering, resulting in more comprehensive and coherent reports than approaches limited by predefined RAG workflows. Analysis of Information Scopes. To qualitatively assess the generated reports, we visualize their content embeddings using t-SNE for three randomly sampled topics from the Glaive dataset (Figure 4, right). Each point corresponds to the embedding of report generated by one of the compared methods. The visualization reveals distinct clusters for each topic, confirming that all methods produce topic-specific content. Notably, reports generated by WebThinker often form broader sub-clusters within each topic group. This suggests that WebThinker, leveraging its Deep Web Explorer and iterative drafting process, explores and synthesizes information from diverse 9 Table 3: Ablation studies of WebThinker. Left: Performance on complex reasoning benchmarks (GAIA, WebWalkerQA, HLE). Right: Performance on the scientific report generation tasks (Glaive). Method Complex Problem-Solving GPQA GAIA Web. HLE Avg. Method WebThinker-32B-RL 70.7 48.5 46.5 15.8 45.4 WebThinker-32B-RL w/ Offline DPO w/o Training (Base) w/o Link Clicking w/o Deep Web Explorer 69.2 68.7 69.7 63.6 45.6 44.7 42.7 38.8 44.0 41.9 42.6 38.5 14.2 43.2 13.0 42.1 15.2 42.6 12.0 38.3 w/o Training (Base) w/o Deep Web Explorer w/o Report Check & Edit w/o Auto. Report Draft Scientific Report Generation Comp. Tho. Fact. Coh. Avg. 8.3 8.4 7.9 8.1 6.3 8.4 8.2 7.9 8.0 6. 7.7 7.7 7.5 7.6 6.8 7.9 7.8 7.6 6.9 7.0 8.1 8.0 7.7 7.7 6. perspectives and depths compared to other methods. Its autonomous nature enables it to adapt the information gathering and writing process dynamically to the reports evolving requirements, leading to richer and potentially more unique information coverage. 4.6 Adaptability of WebThinker Across Different LRM Backbones. To evaluate WebThinkers adaptability across different LRM backbones, we experimented with DeepSeek-R1 models of varying sizes (7B, 14B, and 32B), as illustrated in Figure 5. To ensure stable tool usage capabilities in the R1-based LRMs, we first conducted cold-start supervised finetuning (SFT) using 7.8k reasoning trajectories collected from the QwQ-32B-based WebThinker, followed by reinforcement learning (RL) training (Section 3.5). Across all model sizes and tasks (GAIA, WebWalkerQA, HLE, Glaive), the R1-based WebThinker models consistently outperformed direct reasoning and standard RAG baselines. For example, with the DeepSeek-R1-7B backbone, WebThinker achieves relative improvements of 174.4% on GAIA and 422.6% on WebWalkerQA over direct generation, and 82.9% on GAIA and 161.3% on WebWalkerQA over standard RAG. Similar substantial gains are observed across other model sizes and tasks. These results demonstrate the general applicability and strong effectiveness of the WebThinker framework in enhancing the deep research capabilities of diverse LRMs. 4.7 Ablation Studies We conducted ablation studies to evaluate the contribution of key WebThinker components, with (1) Impact of RL Training: Iterative online RL significantly results summarized in Table 3. enhances problem-solving performance (Avg. 44.9 vs. 42.1 for the base model and 43.2 for offline DPO), validating the efficacy of the training strategy. However, its impact on report generation is minimal, indicating that the base framework is already highly effective. (2) Importance of Deep Web Exploration: Removing the Deep Web Explorer drastically reduces performance in both problemsolving (Avg. 38.3) and report generation (Avg. 7.7), highlighting its critical role. Additionally, disabling only link-clicking negatively impacts problem-solving (Avg. 42.6), demonstrating the value of exploring beyond initial search results, especially for navigation-intensive tasks. (3) Effectiveness of Report Generation Components: For report generation, removing autonomous drafting results in the most significant quality drop (Avg. 6.6), underscoring the importance of interleaving thinking, searching, and drafting. Similarly, disabling the check-and-edit tools lowers overall quality (Avg. 7.7), particularly coherence (6.9 vs. 7.9), reinforcing the necessity of iterative refinement. These results affirm the effectiveness of WebThinkers design choices."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present WebThinker, framework that equips large reasoning models (LRMs) with deep research capabilities, addressing their limitations in knowledge-intensive real-world tasks such as complex reasoning and scientific report generation. WebThinker enables LRMs to autonomously explore the web and produce comprehensive outputs through continuous reasoning process. Key contributions include the Deep Web Explorer, which empowers LRMs to dynamically search and navigate web content, and the Autonomous Think-Search-and-Draft strategy, which seamlessly integrates reasoning, searching, and report writing. In addition, we introduce RL-based training strategies that further refine the LRMs ability to effectively utilize research tools. Extensive 10 experiments on complex reasoning benchmarks and scientific report generation tasks demonstrate that WebThinker consistently outperforms existing methods and even strong proprietary systems. These findings underscore WebThinkers potential to significantly advance the deep research capabilities of LRMs, paving the way for more powerful and versatile intelligent systems capable of addressing complex real-world challenges. Looking ahead, we plan to extend WebThinker in several directions. First, we aim to incorporate multimodal reasoning capabilities, enabling deep research involving images, videos, and other non-textual information and leveraging the rich multimodal content available online. Second, we will explore advanced tool learning and expansion mechanisms, allowing WebThinker to continuously refine its tool usage strategies through self-improvement and to integrate broader range of external tools. Finally, we intend to investigate GUI-based web exploration, enhancing the models ability to understand and interact with graphical web interfaces and enabling it to tackle more complex interactive tasks."
        },
        {
            "title": "References",
            "content": "[1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023. [2] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning to reason with search for llms via reinforcement learning. CoRR, abs/2503.19470, 2025. [3] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. CoRR, abs/2503.09567, 2025. [4] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models. CoRR, abs/2503.04548, 2025. [5] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [6] Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. CoRR, abs/2406.13542, 2024. [7] Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large language models. CoRR, abs/2406.13542, 2024. [8] Guanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, and Ji-Rong Wen. Toward general instruction-following alignment for retrieval-augmented generation. CoRR, abs/2410.09584, 2024. 11 [9] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen. Understand what LLM needs: Dual preference alignment for retrieval-augmented generation. CoRR, abs/2406.18676, 2024. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [11] Mohamed Amine Ferrag, Norbert Tihanyi, and Mérouane Debbah. Reasoning beyond limits: Advances and open problems for llms. CoRR, abs/2503.22732, 2025. [12] Gemini. Gemini deep research. https://gemini.google/overview/ deep-research, 2025. [13] Glaive. reasoning-v1-20m. reasoning-v1-20m, 2025. https://huggingface.co/datasets/glaiveai/ [14] Grok. Grok 3 beta the age of reasoning agents. https://x.ai/news/grok-3, 2025. [15] Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. Deeprag: Thinking to retrieval step by step for large language models. CoRR, abs/2502.01142, 2025. [16] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. [17] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. CoRR, abs/2503.24290, 2025. [18] Yunhai Hu, Yilun Zhao, Chen Zhao, and Arman Cohan. MCTS-RAG: enhancing retrievalaugmented generation with monte carlo tree search. CoRR, abs/2503.20757, 2025. [19] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. [20] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [21] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [22] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. CoRR, abs/2503.09516, 2025. [23] Jiajie Jin, Yutao Zhu, Yujia Zhou, and Zhicheng Dou. Bider: Bridging knowledge inconsistency for efficient retrieval-augmented llms via key supporting evidence. arXiv preprint arXiv:2402.12174, 2024. [24] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. [25] Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Jiang, Ziju Shen, Zihan Qin, Bin Numi- [https://huggingface.co/AI-MO/NuminaMath-1.5](https: Huang, Kashif Rasul, Longhui Yu, Albert Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. namath. //github.com/project-numina/aimo-progress-prize/blob/main/ report/numina_dataset.pdf), 2024. [26] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. CoRR, abs/2501.05366, 2025. [27] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, and Zhicheng Dou. Retrollm: Empowering large language models to retrieve fine-grained evidence within generation. arXiv preprint arXiv:2412.11919, 2024. [28] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. From matching to generation: survey on generative information retrieval. CoRR, abs/2404.14851, 2024. [29] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song, and Cheng-Lin Liu. From system 1 to system 2: survey of reasoning large language models. CoRR, abs/2502.17419, 2025. [30] Zhuoqun Li, Haiyang Yu, Xuanang Chen, Hongyu Lin, Yaojie Lu, Fei Huang, Xianpei Han, Yongbin Li, and Le Sun. Deepsolution: Boosting complex engineering solution design via tree-based exploration and bi-point thinking. CoRR, abs/2502.20730, 2025. [31] Jingyu Liu, Jiaen Lin, and Yong Liu. How much can RAG help the reasoning of llm? CoRR, abs/2410.02338, 2024. [32] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting for retrieval-augmented large language models. arXiv preprint arXiv:2305.14283, 2023. [33] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: benchmark for general AI assistants. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [34] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, et al. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. [35] OpenAI. Learning to reason with llms. https://openai.com/index/ learning-to-reason-with-llms, September 2024. [36] OpenAI. Introducing introducing-deep-research, 2025. deep research. https://openai.com/index/ [37] OpenAI. Openai o3-mini. https://openai.com/index/openai-o3-mini, January 2025. [38] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Daron Anderson, Tung Nguyen, Mobeen Mahmood, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Jessica P. Wang, Pawan Kumar, Oleksandr Pokutnyi, Robert Gerbicz, Serguei Popov, John-Clark Levin, Mstyslav Kazakov, Johannes Schmitt, Geoff Galgon, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Zachary Giboney, Gashaw M. Goshu, Joan of Arc Xavier, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, John Wydallis, Mark Nandor, Ankit Singh, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Darling Duclosel, Jungbae Nam, Jennifer Zampese, Ryan G. Hoerr, Aras Bacho, Gautier Abou Loume, Abdallah Galal, Hangrui Cao, Alexis C. Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Lianghui Li, Sumeet Motwani, Christian Schröder de Witt, Edwin Taylor, Johannes Veith, Eric Singer, Taylor D. Hartman, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Joshua Robinson, Aleksandar Mikov, Ameya Prabhu, Longke Tang, Xavier Alapont, Justine Leon Uro, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Julien Guillod, Yuqi Li, Joshua Vendrow, Vladyslav Kuchkin, and Ng Ze-An. Humanitys last exam. CoRR, abs/2501.14249, 2025. [39] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. [40] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2024. [41] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [42] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022, 2023. [43] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294, 2023. [44] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. CoRR, abs/2503.05592, 2025. [45] Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. 2025. [46] Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms. arXiv preprint arXiv:2402.12052, 2024. [47] Xinyu Tang, Xiaolei Wang, Zhihao Lv, Yingqian Min, Wayne Xin Zhao, Binbin Hu, Ziqi Liu, and Zhiqiang Zhang. Unlocking general long chain-of-thought reasoning capabilities of large language models via representation engineering. CoRR, abs/2503.11314, 2025. [48] M.-A-P. Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Jun Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling LLM evaluation across 285 graduate disciplines. CoRR, abs/2502.14739, 2025. [49] OpenThoughts Team. Open Thoughts. https://open-thoughts.ai, January 2025. [50] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face, 2024. [51] UncleCode. Crawl4ai: Open-source llm friendly web crawler & scraper. https://github. com/unclecode/crawl4ai, 2024. [52] Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, and Furu Wei. Chainof-retrieval augmented generation. CoRR, abs/2501.14342, 2025. [53] Liang Wang, Nan Yang, and Furu Wei. Query2doc: Query expansion with large language models. arXiv preprint arXiv:2303.07678, 2023. [54] Zhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen, Yu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang, Weinan E, Linpeng Tang, and Wentao Zhang. RARE: retrieval-augmented reasoning modeling. CoRR, abs/2503.23513, 2025. [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. [56] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [57] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal. CoRR, abs/2501.07572, 2025. [58] Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. CoRR, abs/2405.00675, 2024. [59] Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Omnithink: Expanding knowledge boundaries in machine writing through thinking. CoRR, abs/2501.09751, 2025. [60] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing LLM reasoning with rule-based reinforcement learning. CoRR, abs/2502.14768, 2025. [61] Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with compression and selective augmentation. arXiv preprint arXiv:2310.04408, 2023. [62] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024. [63] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. [64] Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems. arXiv preprint arXiv:2408.16293, 2024. 15 [65] Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason Weston, and Xian Li. Naturalreasoning: Reasoning in the wild with 2.8m challenging questions, 2025. [66] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. Retrieval-augmented generation for ai-generated content: survey. arXiv preprint arXiv:2402.19473, 2024. [67] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025. [68] Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, and Philip S. Yu. Trustworthiness in retrieval-augmented generation systems: survey. CoRR, abs/2409.10102, 2024. [69] Yujia Zhou, Zheng Liu, and Zhicheng Dou. Assistrag: Boosting the potential of large language models with an intelligent information assistant. CoRR, abs/2411.06805, 2024. [70] Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrievalaugmented large language models. In Tat-Seng Chua, Chong-Wah Ngo, Ravi Kumar, Hady W. Lauw, and Roy Ka-Wei Lee, editors, Proceedings of the ACM on Web Conference 2024, WWW 2024, Singapore, May 13-17, 2024, pages 14531463. ACM, 2024."
        },
        {
            "title": "Appendix",
            "content": "A Inference Process A.1 Research-Related Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Inference Process of WebThinker . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2.1 Inference for Main Reasoning Process . . . . . . . . . . . . . . . . . . . . A.2.2 Inference for Deep Web Explorer . . . . . . . . . . . . . . . . . . . . . . Datasets B.1 Complex Reasoning Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Scientific Report Generation Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . Instruction Templates C.1 Instructions for WebThinker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Task Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.1 Task Instruction for QwQ-based Models . . . . . . . . . . . . . . . . . . . C.2.2 Task Instruction for DeepSeek-R1-based Models . . . . . . . . . . . . . . C.3 Instructions for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3.1 Evaluation Instruction for Problem-Solving Tasks . . . . . . . . . . . . . . C.3.2 Evaluation Instruction for Report Quality . . . . . . . . . . . . . . . . . . C.4 Additional Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Case Study D.1 Case Study for Complex Problem Solving . . . . . . . . . . . . . . . . . . . . . . D.2 Case Study for Deep Web Explorer . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Case Study for Scientific Report Generation . . . . . . . . . . . . . . . . . . . . . 18 18 18 18 19 19 20 20 20 23 24 24 24 24 26 26 26"
        },
        {
            "title": "A Inference Process",
            "content": "A.1 Research-Related Tools WebThinker utilizes several tools during its inference process: Search Tool: This tool is employed by both the main reasoning process and the Deep Web Explorer. In the main process, it invokes the Deep Web Explorer, while within the Explorer, it calls standard search engine API. The model signals search query by generating <begin_search_query> and <end_search_query> . The processed search results are then returned within <begin_search_result> and <end_search_result> . Link & Button Clicking Tool: This tool is exclusively used by the Deep Web Explorer to interact with web pages, typically invoking web crawler. We use Crawl4AI [51] for fetching web page content. The model triggers this tool by generating <begin_click_link> and <end_click_link> . The content retrieved from the clicked link (potentially summarized) is returned within <begin_click_result> and <end_click_result> . Report Writing Tools: These tools are specific to the Report Generation Mode and are invoked by the main reasoning process. They facilitate drafting and refining the research report. Section Writing: Triggered by section_namencontents_to_write <begin_write_section> <end_write_section> . This command instructs an assistant LLM to write the specified section based on the provided content guidelines and gathered information. It does not return content directly into the main reasoning flow to maintain coherence. Report Checking: Triggered by <begin_check_article> . This command requests the current outline of the report. The system extracts all section and subsection titles to form the outline, which is then inserted into the reasoning context followed by <end_check_article> . This avoids injecting the full report text, which could disrupt the reasoning flow. Report Editing: Triggered by <begin_edit_article> edit_instruction <end_edit_article> . This command instructs an assistant LLM to modify the existing report based on the provided editing instructions. Similar to section writing, it doesnt return content directly into the main flow. A. Inference Process of WebThinker The overall inference process in WebThinker involves two main components: the main reasoning process and the Deep Web Explorer for in-depth information gathering. As outlined in Section 3.2, the main reasoning process operates in two distinct modes: Problem-Solving Mode and Report Generation Mode. Detailed instructions for each component and mode can be found in Section C. A.2.1 Inference for Main Reasoning Process Problem-Solving Mode: In this mode, the LRM receives users question and aims to find direct answer. It primarily utilizes the Search Tool. When the model generates the <end_search_query> token, the generation process pauses. The system extracts the most recent query enclosed within <begin_search_query> and <end_search_query> . Concurrently, an assistant LLM formulates detailed search intent based on the query and the preceding reasoning chain. The system then uses web crawler to fetch the content of the top-k (k=10 in our experiments) retrieved web documents. The original query, the generated search intent, and the fetched web content are passed to the Deep Web Explorer. The Explorer processes this information and returns refined summary relevant to the query and intent, enclosed within <begin_search_result> and <end_search_result> . The main LRM then resumes its reasoning with this new information. This cycle continues until the model generates an End-of-Sequence (EOS) token or reaches the maximum token limit. The final answer is extracted based on predefined answer pattern (e.g., boxed{ANSWER} or ANSWER: {ANSWER}). Report Generation Mode: This mode handles tasks requiring the generation of comprehensive research report. It receives the users question along with research plan (potentially generated by an assistant LLM). The LRM utilizes the Search Tool (invoking the Deep Web Explorer as in Problem-Solving Mode) and the three Report Writing Tools (Section Writing, Report Checking, Report Editing). Similar to the problem-solving mode, generation pauses whenever the model outputs 18 closing tool token ( <end_search_query> , <end_write_section> , <end_edit_article> ) or the opening token for checking ( <begin_check_article> ). For search, the process mirrors the Problem-Solving Mode: invoke the Deep Web Explorer and insert results between <begin_search_result> and <end_search_result> . For section writing and report editing, the corresponding assistant LLM performs the action in the background. Since no direct output is inserted, the main LRM continues its reasoning immediately after generating the end token. For report checking, the system generates the report outline, appends the <end_check_article> token, inserts this into the context, and resumes generation. The model continues this loop of reasoning, searching, and drafting/editing until it generates an EOS token or reaches the maximum token limit. The final, complete report is the accumulated result of the writing and editing actions performed by the assistant LLM throughout the process. A.2.2 Inference for Deep Web Explorer Deep Web Explorer: The Deep Web Explorer operates as sub-process invoked by the main reasoning process when search is needed. Its goal is to deeply investigate the initial search results provided by the main process. It has access to two tools: direct Search Engine call and the Link & Button Clicking Tool. When the Explorer model generates <end_search_query> or <end_click_link> , generation pauses. search action directly calls the search engine API (e.g., Bing) and returns the top-10 results, typically including titles, URLs, and snippets (not the full page content). These results are inserted into the Explorers reasoning context. link click action uses the crawler (Crawl4AI) to fetch the full content of the specified URL. To manage potentially long web pages, an assistant LLM summarizes the fetched content based on generated \"click intent\", ensuring the inserted information is concise and relevant. The summarized result is placed within <begin_click_result> and <end_click_result> . The Explorer continues its internal reasoning processanalyzing results, deciding whether to perform another search, or clicking on promising linkuntil it gathers sufficient information relevant to the initial query and search intent provided by the main reasoning process. It concludes by generating its findings within designated \"Final Information\" pattern, which is then passed back to the main reasoning process as the output within <begin_search_result> and <end_search_result> . This process stops when the Explorer generates an EOS token or reaches its maximum token limit."
        },
        {
            "title": "B Datasets",
            "content": "B.1 Complex Reasoning Tasks For testing, we use the following four widely used datasets: GPQA [42]: Written by experts (holding or pursuing PhDs in relevant fields) in biology, physics, and chemistry, these \"Google-proof\" questions test the models expertise and reasoning in these specific domains. It consists of multiple-choice questions (4 options each); we use the diamond set, totaling 198 questions. GAIA [33]: benchmark dataset designed to evaluate the capabilities of general artificial intelligence (AGI). Compared to GPQA, GAIA focuses more on generality, testing AI performance across diverse tasks such as question answering, reasoning, code generation, and multimodal processing. As our model is text-based and cannot handle other modalities, we use the text-only validation subset, comprising 103 questions. WebWalkerQA [57]: benchmark specifically designed to evaluate the web traversal capabilities of LLMs, simulating human ability to explore websites via clicking to find required information. It includes 680 challenging queries covering multilingual and multi-domain web content. We use the test set for evaluation, totaling 680 questions. Humanitys Last Exam (HLE) [38]: Aims to assess LLMs capabilities at the frontiers of human knowledge. It contains 2500 challenging cross-disciplinary questions covering fields like mathematics, humanities, and natural sciences. The benchmark includes multiple-choice and short-answer questions, all with clear and easily verifiable solutions. Current state-of-the-art models achieve less than 10% accuracy on HLE, highlighting its difficulty and effectiveness in measuring advanced academic capabilities. Due to its large test set, we randomly sample 500 text-only questions for testing. For training, we use the following datasets. We sample approximately 3k data points from these datasets. First, we perform direct generation using QwQ-32B and retain only the questions that cannot be answered correctly through direct generation. Then, for each of these questions, we generate three responses using WebThinker-32B-Base to filter for high-quality preference training data, following the criteria outlined in Section 3.5. SuperGPQA [48]: Designed to evaluate LLM knowledge and reasoning across 285 graduate-level subjects. It contains 26,529 multiple-choice questions covering 72 fields, grouped into 13 broader disciplines, with strong emphasis on STEM subjects (77.2%). WebWalkerQA (Silver) [57]: We use the Silver set of WebWalkerQA, containing 13.6k data points. The description is the same as above. OpenThoughts [49]: Contains 114,000 carefully crafted reasoning examples designed to train AI models for complex logical and mathematical reasoning tasks. Given that we already use NuminaMath as training data, we prioritize sampling data related to STEM and puzzles. NaturalReasoning [65]: Aims to enhance LLM reasoning capabilities through 2.8 million challenging reasoning problems across multiple domains, including STEM (e.g., physics and computer science), economics, and social sciences. NuminaMath [25]: Features problems ranging in difficulty from high school exercises to International Mathematical Olympiad questions, sourced from various online platforms and PDFs. NuminaMath provides high-quality, structured data enabling AI models to learn and replicate expert-level mathematical reasoning. B.2 Scientific Report Generation Tasks For the scientific report generation task, we use glaiveai/reasoning-v1-20m (Glaive) [13] for both training and testing. This dataset is large-scale synthetic collection containing over 22 million general reasoning questions and responses generated using DeepSeek-R1-Distill-Llama-70B [5]. It aims to cover diverse non-code/math topics such as social and natural sciences, education, and creative writing. We sample 1.5k questions for each iterations preference data construction and 30 questions for testing."
        },
        {
            "title": "C Instruction Templates",
            "content": "C.1 Instructions for WebThinker Problem Solving Instruction for WebThinker You are reasoning assistant with the ability to perform web searches to help you answer the users question accurately. You have special tools: - To perform search: write <begin_search_query> your query here <end_search_query>. Then, the system will search and analyze relevant web pages, then provide you with helpful information in the format <begin_search_result> ...search results... <end_search_result>. You can repeat the search process multiple times if necessary. Once you have all the information you need, continue your reasoning. Example: Question: \"Alice David is the voice of Lara Croft in video game developed by which company?\" Assistant thinking steps: - need to find out who voices Lara Croft in the video game. - Then, need to determine which company developed that video game. Assistant: 20 <begin_search_query>Alice David Lara Croft voice<end_search_query> (System returns processed information from relevant web pages) Assistant thinks: The search results indicate that Alice David is the voice of Lara Croft in specific video game. Now, need to find out which company developed that game. Assistant: <begin_search_query>video game developed by Alice David Lara Croft<end_search_query> (System returns processed information from relevant web pages) Assistant continues reasoning with the new information... Remember: - Use <begin_search_query> to request web search and end with <end_search_query>. - When done searching, continue your reasoning. Instruction for Deep Web Explorer You are web explorer analyzing search results to find relevant information based on given search query and search intent. **Guidelines:** 1. **Analyze the Searched Web Pages:** - Carefully review the content of each searched web page. - Identify factual information that is relevant to the **Current Search Query** and can aid in the reasoning process for the original question. 2. **More Information Seeking:** - If the information is not relevant to the query, you could: 1. Search again: <begin_search_query>another search query<end_search_query> 2. Access webpage content using: <begin_click_link>your URL<end_click_link> 3. **Extract Relevant Information:** - Return the relevant information from the **Searched Web Pages** that is relevant to the **Current Search Query**. 4. **Output Format:** - Present the information beginning with **Final Information** as shown below. **Final Information** [Relevant information] **Inputs:** - **Current Search Query:** {search_query} - **Detailed Search Intent:** {search_intent} - **Searched Web Pages:** {search_result} Now please analyze the web pages and extract relevant information for the search query \"{search_query}\" and the search intent. Report Generation Instruction for WebThinker You are research assistant with the ability to perform web searches to write scientific research article. You have special tools: - To perform search: write <begin_search_query> your query here <end_search_query>. Then, the system will search and analyze relevant web pages, then provide you with helpful information in the format <begin_search_result>search results<end_search_result>. - To write section of the research article: write <begin_write_section>section namencontents to write<end_write_section>. Then, the system will completely write the section based on your request and current gathered information. - To check the current article: write <begin_check_article>system returns outline of all current written contents<end_check_article>. - To edit tion<end_edit_article>. Then, the system will edit the article based on your goal and instruction and current gathered information. Your task is to research and write scientific article about: {question} Here is research plan to guide your investigation: {plan} the article: write <begin_edit_article>your detailed edit goal and instrucof of web web from from section searched information information Please follow the research plan step by step: 1. Use web searches to gather detailed information for each point 2. After each search, analyze the results and determine what additional information is needed 3. When you have sufficient information for section, request to write that section 4. Continue this process until the full article is complete 5. Check the current article and edit sections as needed to improve clarity and completeness Example: <begin_search_query>first search query<end_search_query> <begin_search_result>Summary pages<end_search_result> Based on these results, understand X, but still need to investigate Y... <begin_search_query>follow-up search query focusing on Y<end_search_query> <begin_search_result>Summary searched pages<end_search_result> Now have enough information to write the first section... <begin_write_section>IntroductionnThis <end_write_section> have written the introduction. Now need to explore more information to write the next section ... After writing the above sections, need to check the current article to ensure the content is complete and accurate. <begin_check_article>System article<end_check_article> Wait, realize that need to edit ... <begin_edit_article>your edit instruction<end_edit_article> Assistant continues gathering information and writing sections until getting comprehensive information and finishing the entire article. Remember: - Use <begin_search_query>query<end_search_query> to get information from web searches - Use <begin_write_section>section namencontents to write<end_write_section> to call the system to write section in the article - Use <begin_check_article>outline of current article<end_check_article> to check the current written article - Use <begin_edit_article>edit instruction<end_edit_article> to call the system to edit and improve the article - You should strictly follow the above format to call the functions. - Do not propose methods or design experiments, your task is to comprehensively research with web searches. - Do not omit any key points in the article. - When you think the article is complete, directly output \"I have finished my work.\" and stop. Now begin your research and write the article about: {question} introduce written current outline returns should of ... Instruction for Writing Section You are research paper writing assistant. Please write complete and comprehensive \"{section_name}\" section based on the following information. **Potential helpful documents:** {relevant_documents} **Original question:** {question} **Previous thoughts:** {previous_thoughts} **Outline of current written article:** {current_article} **Name of the next section to write:** ## {section_name} **Your task is to comprehensively write the next section based on the following goal:** {task} **Note:** - Write focused content that aligns with the above goal for this section. - No need to mention citations or references. - Each paragraph should be comprehensive and well-developed to thoroughly explore the topic. Avoid very brief paragraphs that lack sufficient detail and depth. - If possible, add markdown tables to present more complete and structured information to users. Please provide the comprehensive content of the section in markdown format. ## {section_name} 22 Instruction for Editing Article You are professional article editor. Please help me modify the article based on the following edit instruction: **Edit instruction:** {edit_instruction} **Current article:** {article} Please output the complete modified article incorporating all the requested changes. **Note:** - Keep all original content that doesnt need modification. (Do not just output the modified content, but output the entire modified article.) - Make all edits specified in the edit instructions. - Output format: markdown ... Please provide the complete modified article in markdown format. Instruction for Search Plan Generation Please help me create detailed plan to search over the web for solving the following question: {query} Your task is to comprehensively gather all relevant information to thoroughly solve the users question. Note: - No need to mention citations or references. - Do not propose methods or design experiments, your task is to research users question with web searches. - Be comprehensive and thorough, do not miss any relevant information. - No more than 8 steps. Please output the plan in numbered steps like: (1) ... (2) ... etc. Directly output the plan, do not include any other words. Instruction for Search Intent Generation Based on the previous thoughts below, provide the detailed intent of the latest search query. Previous thoughts: {previous_thoughts} Please provide the current search intent. Instruction for Click Intent Generation Based on the previous thoughts below, provide the detailed intent of the latest click action. Original question: {question} Previous thoughts: {prev_reasoning} Please provide the current click intent. C.2 Task Instructions The task instruction specifies the description of specific task and the answer output format for particular model. These instructions are directly concatenated after the method instructions (such as those for WebThinker, Search-o1, or RAG approaches). 23 C.2.1 Task Instruction for QwQ-based Models Task Instruction for QwQ-based Models the following question. Please answer boxed{YOUR_ANSWER}. Question: {question} You should provide your final answer in the format C.2.2 Task Instruction for DeepSeek-R1-based Models Task Instruction for DeepSeek-R1-based Models Please answer the following question. Provide your final answer in the format **ANSWER: {YOUR_ANSWER}**. Question: {question} C.3 Instructions for Evaluation In this work, we use LLM-as-Judges to evaluate both complex problem-solving and scientific report generation tasks. The specific instructions are as follows: C.3.1 Evaluation Instruction for Problem-Solving Tasks We use Qwen2.5-72B-Instruct [40] to evaluate all complex problem-solving tasks. An output labeled as \"Correct\" is considered correct, while \"Incorrect\" is considered wrong. In cases where the predicted answer cannot be accurately extracted, we directly use the last five lines of the models output as the predicted answer. This approach helps reduce evaluation inaccuracies caused by formatting issues, case sensitivity, and similar factors, aligning with the official evaluation methods of benchmarks like WebWalkerQA [57] and HLE [38]. Evaluation Instruction for Problem-Solving Tasks You are an evaluation assistant. Please determine if the predicted answer is equivalent to the labeled answer. Question: {question} Labeled Answer: {labeled_answer} Predicted Answer: {pred_answer} Are these answers equivalent? Please respond with \"Correct\" if they are equivalent, or \"Incorrect\" if they are not equivalent. Do not include any other text. C.3.2 Evaluation Instruction for Report Quality For scientific report generation tasks, we use DeepSeek-R1-671B [5] and GPT-4o [20] respectively with the following instruction for evaluation, obtain scores, and take their average. Here, {system_a} to {system_e} are sequentially the reports generated by the systems being evaluated. We perform listwise evaluation to better compare the quality differences between reports generated by different systems. The input order is randomized rather than fixed to reduce bias caused by the models context window. We did not employ human evaluation because the reports generated by each system have distinct features, making it easy for humans to identify their originating system, which introduces significant bias. Model-based evaluation is therefore more impartial. Evaluation Instruction for Report Quality Research Question: {question} Please objectively evaluate the quality of research articles generated by systems A, B, C, and for this question, and provide scores out of 10 for the following criteria: (1) Overall Comprehensiveness: The report should cover content as comprehensively as possible (2) Thoroughness of Discussion: Each section should be discussed thoroughly, not just superficially (3) Factuality: There should be minimal factual errors (4) Coherence: The discussion should stay focused and relevant to the topic Notes: - satisfactory performance deserves around 5 points, with higher scores for excellence and lower scores for deficiencies - You should not easily assign scores higher than 8 or lower than 3 unless you provide substantial reasoning. - You do not need to consider citations in the articles Research article generated by system A: {system_a} {system_b} {system_c} {system_d} {system_e} Research article generated by system B: Research article generated by system C: Research article generated by system D: Research article generated by system E: Research Question: {question} Please objectively evaluate the quality of research articles generated by systems A, B, C, and for this question, and provide scores out of 10 for the following criteria: (1) Overall Comprehensiveness: The report should cover content as comprehensively as possible (2) Thoroughness of Discussion: Each section should be discussed thoroughly, not just superficially (3) Factuality: There should be minimal factual errors (4) Coherence: The discussion should stay focused and relevant to the topic Notes: - satisfactory performance deserves around 5 points, with higher scores for excellence and lower scores for deficiencies - You should not easily assign scores higher than 8 or lower than 3 unless you provide substantial reasoning. - You do not need to consider citations in the articles Please analyze each article and provide the final scores in the following JSON format: json { \"System A\": { \"Overall Comprehensiveness\": , \"Thoroughness of Discussion\": , \"Factuality\": , \"Coherence\": }, \"System B\": { \"Overall Comprehensiveness\": , \"Thoroughness of Discussion\": , \"Factuality\": , \"Coherence\": }, 25 \"System C\": { \"Overall Comprehensiveness\": , \"Thoroughness of Discussion\": , \"Factuality\": , \"Coherence\": }, \"System D\": { \"Overall Comprehensiveness\": , \"Thoroughness of Discussion\": , \"Factuality\": , \"Coherence\": }, \"System E\": { \"Overall Comprehensiveness\": , \"Thoroughness of Discussion\": , \"Factuality\": , \"Coherence\": } } C.4 Additional Notes All instructions presented above are provided as user prompts rather than system prompts. When applying the WebThinker approach (detailed in Section C.1) to models based on QwQ-32B [50] or DeepSeek-R1 series [5], the corresponding task instruction (from Section C.2.1 or C.2.2) is appended after the main WebThinker instruction. For other models, such as Qwen2.5-32B-Instruct [40], Qwen2.5-72B-Instruct [40], and GPT-4o [20], we include the Chain-of-Thought prompt \"You should think step by step to solve it.\" before the question to explicitly encourage step-by-step reasoning prior to providing the final answer, following [55]. For detailed instructions regarding the baseline methods like Search-o1, Standard RAG, etc., you can refer to the appendix of Search-o1 [26] or visit our WebThinker GitHub repository: https: //github.com/sunnynexus/WebThinker."
        },
        {
            "title": "D Case Study",
            "content": "The examples from Tables 4, 5, 6, 7, 8, 9, and 10 demonstrate WebThinkers effectiveness across different capabilities including complex problem solving, deep web exploration, and scientific report generation. D.1 Case Study for Complex Problem Solving WebThinker shows strong reasoning capabilities in complex problem-solving scenarios: In the GAIA dataset example  (Table 4)  , WebThinker correctly identifies that Nemo from \"Finding Nemo\" is clownfish, and methodically searches for nonnative sightings in the USGS database, finding the only documented case at Fred Howard Park, Florida. It then determines the correct zip code (34689). For the WebWalkerQA example  (Table 5)  , WebThinker resolves ambiguity about the \"evening after\" the ACL 2023 awards ceremony by searching for relevant dates and determining that the social event occurred on the same day (July 11) from 7:00 PM to 10:30 PM, not the following day. In the mathematical problem from Humanitys Last Exam  (Table 6)  , WebThinker demonstrates sophisticated reasoning by finding the formula for simplicial volume of surfaces and their products, correctly calculating the simplicial volume of Σ31 Σ17 as 11520. D.2 Case Study for Deep Web Explorer The Deep Web Explorer demonstrates effective information retrieval and integration: 26 For the ASH Annual Meeting deadlines  (Table 7)  , the explorer not only finds the late-breaking abstract submission dates but also discovers the ancillary meeting deadline by clicking on PDF link, compiling comprehensive deadline information with specific dates and requirements. In the CLTS and Aedes mosquito control example  (Table 8)  , the explorer clicks on repository link to find case study integrating Community-Led Total Sanitation with mosquito control in Indonesia, Vietnam, and the Philippines, providing specific outcomes (40% reduction in breeding sites). D.3 Case Study for Scientific Report Generation WebThinkers report generation capability is illustrated in Tables 9 and 10: The model systematically follows research plan for optimizing 3D printed lattice structures, conducting searches on FDM limitations, lattice optimization techniques, and material properties. It methodically writes, checks, and edits sections, addressing duplicate content and ensuring coherent structure. The final report  (Table 10)  presents comprehensive analysis of lattice optimization for robotic nodes, covering technical aspects from printing limitations to material selection, with practical applications and case studies. These examples validate WebThinkers effectiveness across complex problem-solving, deep web exploration, and scientific report generation tasks. Table 4: problem-solving example from main reasoning process of WebThinker on GAIA dataset, with special symbols used in the search queries and search results highlighted in blue box . [...] indicates omitted content. Example # from GAIA Dataset Question: Im researching species that became invasive after people who kept them as pets released them. Theres certain species of fish that was popularized as pet by being the main character of the movie Finding Nemo. According to the USGS, where was this fish found as nonnative species, before the year 2020? need the answer formatted as the five-digit zip codes of the places the species was found, separated by commas if there is more than one place. Labeled Answer: 34689 Model Output by WebThinker Okay, lets tackle this question step by step. The user wants to know the five-digit zip codes of places in the US where the clownfish (the main character from Finding Nemo) has been found as nonnative species according to the USGS, before 2020. First, need to confirm which species is the main character in Finding Nemo. Thats Nemo the clownfish, specifically *Amphiprion ocellaris*. Its important to look for the correct species. Next, the user mentioned the USGS database on nonindigenous aquatic species. need to check this database for records before 2020. The task requires the answer formatted as five-digit zip codes. need to find out if the USGS data provides this level of detail or if Ill need to find the zip codes based on reported locations (like cities or parks). Let me start by searching the USGS for nonnative sightings of *Amphiprion ocellaris* before 2020. <begin_search_query> USGS Amphiprion ocellaris nonnative sightings pre 2020 <end_search_query> <begin_search_result> The only documented nonnative sighting of *Amphiprion ocellaris* (clown anemonefish) in the United States prior to 2020, as reported by the USGS Nonindigenous Aquatic Species (NAS) database, occurred in **Pinellas County, Florida**, specifically at **Fred Howard Park** in the Gulf of Mexico region during **2018**. [...] The provided web pages do not explicitly mention the **five-digit zip code** associated with this location. [...] For precise zip code details, accessing the full NAS database record [...] may be necessary. <end_search_result> Okay, the search result indicates only one sighting: Fred Howard Park in Pinellas County, Florida, in 2018. It explicitly states the zip code isnt provided in the search results. So, need to find the zip code for Fred Howard Park. The result mentions Pinellas County, but the park might be in specific city within the county. Let me search for the parks zip code. <begin_search_query> Fred Howard Park St. Petersburg Florida zip code <end_search_query> <begin_search_result> The zip code for Fred Howard Park in Tarpon Springs, Florida (part of Pinellas County) is **34689**. While the search query references St. Petersburg, the park is officially located in Tarpon Springs [...]. The address consistently listed [...] is **1700 Sunset Drive, Tarpon Springs, FL 34689**. [...] <end_search_result> Ah, the park is in Tarpon Springs, FL, within Pinellas County, and the zip code is 34689. The first search result mentioned this was the *only* documented sighting before 2020. To be absolutely sure, Ill run the first search query again to confirm no other locations were missed. <begin_search_query> Amphiprion ocellaris USGS nonnative sightings before 2020 <end_search_query> <begin_search_result> The USGS Nonindigenous Aquatic Species (NAS) database indicates that *Amphiprion ocellaris* (clown anemonefish) has only one documented nonnative sighting in the United States prior to 2020. This sighting occurred in **Fred Howard Park**, **Pinellas County**, **Florida**, in **2018**. [...] No other locations besides Fred Howard Park are listed [...] as having nonnative sightings of *A. ocellaris* before 2020. The five-digit zip code for Fred Howard Park, Tarpon Springs, Florida, is **34689**. [...] <end_search_result> Confirmed again. The only documented nonnative sighting before 2020 was at Fred Howard Park, zip code 34689. Since the user asked for zip codes separated by commas if theres more than one, and theres only one, the answer is just this single zip code. **Final Answer** 28 Table 5: problem-solving example from main reasoning process of WebThinker on WebWalkerQA dataset, with special symbols used in the search queries and search results highlighted in blue box . [...] indicates omitted content. Example from WebWalkerQA Dataset Question: What was the specific schedule for the social event held on the evening after the ACL 2023 best paper awards ceremony? Labeled Answer: The social event was scheduled from 7:00 PM to 10:30 PM on July 11th. Model Output by WebThinker Okay, so the user is asking about the specific schedule of social event that took place on the evening *after* the ACL 2023 best paper awards ceremony. First, need to figure out when the awards ceremony actually happened. Let me search for that specifically. <begin_search_query> ACL 2023 best paper awards ceremony date <end_search_query> <begin_search_result> The ACL 2023 best paper awards ceremony took place on **July 11, 2023**, according to multiple sources: [...] Web Page 4 [...] explicitly states that the awards were announced on July 11, 2023. [...] Web Page 5 [...] mentions the award presentation occurred on July 11, 2023 [...]. Web Page 6 [...] confirms the awards were announced on July 11, 2023. [...] Web Page 9 [...] also indicates the awards were announced on July 11, 2023. [...] the awards ceremony itself was held on the **11th**. [...] <end_search_result> Okay, the awards ceremony was definitively on July 11, 2023. The question asks for the social event on the evening *after* the ceremony, which would be the evening of July 12. Let me search for the social events schedule to see what was planned. <begin_search_query> ACL 2023 social events schedule <end_search_query> <begin_search_result> The ACL 2023 social events schedule includes the following relevant details: 1. **Social Event on July 11, 2023**: - **Time**: 7:00 PM 10:30 PM (EDT) - **Location**: Steam Whistle Brewing [...] - **Description**: This event occurred on the evening of July 11, immediately following the Best Paper Awards ceremony (which took place earlier that day at 16:0017:30). [...] 2. **No Explicit Social Events on July 12, 2023**: - The evening of July 12, 2023, is not listed as having any official social events in the provided sources. [...] **Note**: The primary social event referenced in the sources is on July 11, not July 12. The Best Paper Awards ceremony was held on July 11, so the subsequent evening event likely refers to the Social Event on that same day. For July 12, only daytime sessions are documented. [...] <end_search_result> Hmm, interesting. The awards were on July 11, and the search results show social event on the evening of July 11, immediately following the awards ceremony (7:00 PM - 10:30 PM). However, the results explicitly state there were no official social events scheduled for the evening of July 12, which is technically the evening *after* the July 11 ceremony. Given the users phrasing \"evening after\", but the lack of any scheduled event on July 12 evening, its highly probable they meant the social event that occurred on the *same evening* as the awards ceremony, which did happen *after* the ceremony concluded earlier that day. The only documented social event connected to the awards ceremony fits this description. Therefore, the specific schedule requested corresponds to the event on July 11. The social event on the evening after the ACL 2023 best paper awards ceremony (held on July 11, 2023) was scheduled from 7:00 PM to 10:30 PM EDT at Steam Whistle Brewing / Canadas Premium Beer. 29 Table 6: problem-solving example from main reasoning process of WebThinker on Humanitys Last Exam dataset, with special symbols used in the search queries and search results highlighted in blue box . [...] indicates omitted content. Example from Humanitys Last Exam Dataset Question: Let Σg denote the oriented closed surface of genus g. Compute the simplicial volume of Σ31 Σ17. Labeled Answer: 11520 Model Output by WebThinker Okay, so need to compute the simplicial volume of Σ31 Σ17, where Σg is an oriented closed surface of genus g. First, should recall the simplicial volume of single surface Σg. For 2, the simplicial volume is Σg = 4(g 1). Let me quickly verify this. <begin_search_query> simplicial volume of closed surface of genus <end_search_query> <begin_search_result> The simplicial volume of closed surface of genus 2 is indeed 4(g 1). This result is derived from the relationship between simplicial volume and the Euler characteristic (χ) of the surface. Specifically, for surface Σg of genus g, the Euler characteristic is χ(Σg) = 2 2g. The simplicial volume is given by Σg = 2χ(Σg), leading to: Σg = 22 2g = 4(g 1). [...] For the product of two closed surfaces of genus g, 2, the simplicial volume Σg Σh has known bounds related to the product of individual volumes, but the exact value requires specific theorems. [...] Sources confirm Σg = 4(g 1). [...] <end_search_result> Okay, the formula Σg = 4(g 1) for 2 is confirmed. Now, the core problem is the simplicial volume of the product Σ31 Σ17. The first search mentioned bounds but not an exact formula. need the specific rule for products of surfaces. Let me search for that. <begin_search_query> simplicial volume of product of surfaces <end_search_query> <begin_search_result> The simplicial volume of the product of two oriented closed surfaces of genus and (each 2) is given by the formula: Σg Σh = 3 2 Σg Σh. Substituting the known simplicial volumes of individual surfaces (Σg = 4(g 1) and Σh = 4(h 1)), this becomes: Σg Σh = 4(g 1) 4(h 1) = 24(g 1)(h 1). 3 2 2 Σg Σh = 24(g 1)(h 1). This result is derived from Michelle Buchers work, leveraging the proportionality principle for manifolds covered by H2 H2. [...] <end_search_result> Ah, perfect. The formula is Σg Σh = 3 Now can substitute = 31 and = 17: Σ31 Σ17 = 24 (31 1) (17 1) Σ31 Σ17 = 24 30 16 Calculating this: 24 30 = 720 720 16 = 11520 So the simplicial volume is 11520. The simplicial volume of an oriented closed surface Σg of genus 2 is 4(g 1). For the product of two such surfaces Σg Σh, the simplicial volume is given by 3 2 times the product of their individual simplicial volumes, which simplifies to 24(g 1)(h 1). Applying the formula for = 31 and = 17: Σ31 Σ17 = 24(31 1)(17 1) = 24 30 16 = 720 16 = 11520. Thus, the simplicial volume of Σ31 Σ17 is 11520 . 30 Table 7: An example from the Deep Web Explorer of WebThinker on finding deadlines for the ASH 66th annual meeting, with special symbols used in the search queries, search results highlighted in blue box , and click actions/results highlighted in red box . [...] indicates omitted content. Due to space limitations, the input \"Searched Web Pages\" in the table are summarized. Example from Deep Web Explorer Inputs to Deep Web Explorer: - Current Search Query: ASH 66th annual meeting deadlines ancillary meetings late-breaking abstracts - Detailed Search Intent: The current search intent is to find the specific deadlines for requesting ancillary meetings and submitting late-breaking abstracts for the 66th ASH Annual Meeting. The search query aims to gather this information from the official ASH website or other reliable sources to ensure accuracy and relevance. - Searched Web Pages: Provided 10 web page snippets from sources like Hematology.org and Confex.com. Pages 1, 3, 5, and 7 (e.g., \"Call for Late Breaking Abstracts - Hematology.org\", \"Call for Abstracts - confex.com\") consistently stated the late-breaking abstract submission period opens October 16, 2024, and closes October 28, 2024, 11:59 p.m. Pacific Time. Page 8 (\"ASH Annual Meeting Abstracts - Hematology.org\") contained conflicting dates mentioning 2025, which were disregarded as likely incorrect for the 66th meeting. None of the provided snippets explicitly mentioned the deadline for ancillary meeting requests. [...] Deep Web Explorer Processing Output Okay, need to find deadlines for ancillary meetings and late-breaking abstracts for the 66th ASH Annual Meeting. From the initial web pages, the late-breaking abstract deadline is confirmed as October 16-28, 2024 (11:59 PM PT) from pages 1, 3, 5, and 7. However, none of the initial pages provide the deadline for ancillary meetings. need to search specifically for that. <begin_search_query> ASH 66th annual meeting ancillary meetings proposal deadline <end_search_query> <begin_search_result> Search results included PDF link (Web Page 1) indicating the ancillary meeting deadline is **November 22, 2024, 11:59 p.m. PST**. Another page (Web Page 4) stated the request form becomes available **July 24, 2024**. Other pages mentioned meeting dates or referred to the 2025 meeting. [...] <end_search_result> The new search results provide the ancillary meeting deadline (Nov 22, 2024) and the date the request form opens (July 24, 2024). Web Page 1 looks like the official guidelines PDF. Let me click that to confirm details. <begin_click_link> https://www.hematology.org/-/media/hematology/files/annualmeeting/2024_ash_ancillary_meeting_guidelines_final.pdf <end_click_link> <begin_click_result> The clicked PDF (Ancillary Meeting Guidelines for the 66th ASH Annual Meeting) confirmed: - Request Form Availability: **Wednesday, July 24, 2024, 11:00 a.m. EDT**. - Submission Deadline: **Friday, November 22, 2024, 11:59 p.m. PST**. - non-refundable processing fee applies, tiered by event type (e.g., $200 for Member/Non-Profit, $250 for Advisory Board, $500 for Investor Meeting). - Space is limited, first-come, first-served, using designated hotels (no convention center space). - Eligibility: Organizer must be affiliated with ASH (exhibitor, sponsor, member, etc.). - Contact for questions: asheventrequests@spargoinc.com. [...] <end_click_result> Okay, now have confirmed information for both late-breaking abstracts (from initial search) and ancillary meetings (from the additional search and click). can now consolidate this information. ## Final Information After reviewing the official ASH resources and related search results, the critical deadlines for the **66th ASH Annual Meeting** are summarized below: ### **Late-Breaking Abstracts Deadlines** - **Open Date**: October 16, 2024 - **Close Date**: October 28, 2024, at **11:59 p.m. Pacific Time** - **Details**: - Requires justification for late submission. - Submission fee: **$125 USD** (non-refundable). - Max **six abstracts** accepted for presentation on **Dec 10, 2024**. ### **Ancillary Meetings Deadlines** - **Form Available**: July 24, 2024, at **11:00 a.m. EDT** - **Submission Due**: November 22, 2024, at **11:59 p.m. PST** - **Requirements**: - Organizer must be ASH-affiliated. - Tiered fees apply ($200/$250/$500 based on type). - Use approved hotels (no convention center space), first-come, first-served. - Must not conflict with official ASH programming. 31 Table 8: An example from the Deep Web Explorer of WebThinker on case studies of CLTS and Aedes mosquito control, with special symbols used in the search queries, search results highlighted in blue box , and click actions/results highlighted in red box . [...] indicates omitted content. Due to space limitations, the input \"Searched Web Pages\" in the table are summarized. Example from Deep Web Explorer Inputs to Deep Web Explorer: - Current Search Query: case studies of CLTS and Aedes mosquito control in rural Southeast Asia - Detailed Search Intent: Find case studies integrating Community-Led Total Sanitation (CLTS) with Aedes mosquito control in rural Southeast Asia. The goal is to identify real-world implementations combining these programs to address waste management and water storage challenges [...] related to dengue fever. - Searched Web Pages: Provided 10 web page snippets from sources like PubMed, NCBI, Parasites & Vectors, ScienceDirect and UNAIR Repository. Several pages (1-4) focused on Aedes mosquito control methods including insecticide resistance and sterile insect techniques. Web Page 6 mentioned \"government-community partnerships\" for vector control in Southeast Asia. Web Page 10 discussed Aedes breeding in water containers in rural areas, though in Cameroon rather than Southeast Asia. None of the snippet information explicitly connected CLTS to Aedes control. [...] Deep Web Explorer Processing Output Okay, need to find case studies connecting CLTS and Aedes mosquito control in Southeast Asia. From the initial web pages, Im not seeing explicit mentions of Community-Led Total Sanitation (CLTS) being integrated with Aedes mosquito control measures. Web Page 6 mentions government-community partnerships for vector control, which is related to community engagement but not specifically CLTS. Let me check the UNAIR repository article for more details. <begin_click_link> https://repository.unair.ac.id/128927/1/Artikel-Effectiveness.pdf <end_click_link> <begin_click_result> The clicked PDF (from UNAIR Repository) detailed study integrating Community-Led Total Sanitation (CLTS) with Aedes mosquito control in rural Southeast Asia (Indonesia, Vietnam, Philippines). The study combined CLTS implementation (community mobilization, latrine construction) with vector control (larvicide distribution, education). Results showed 40% lower prevalence of Aedes breeding sites in villages with the integrated approach compared to controls, demonstrating improved sanitation and reduced mosquito breeding through community engagement. [...] <end_click_result> The UNAIR article provides the exact case study information were looking for, explicitly documenting CLTS integration with Aedes mosquito control in rural Southeast Asia. should also search for additional case studies to provide more comprehensive answer. <begin_search_query> CLTS and dengue prevention in rural Indonesia <end_search_query> <begin_search_result> No additional clear case studies of CLTS explicitly integrated with Aedes control were found in the search results. Most sources discuss either CLTS for sanitation improvements or Aedes control measures separately, but not their intentional integration. The previously found UNAIR Repository article remains the most relevant source documenting this specific integrated approach in Southeast Asia. [...] <end_search_result> The additional search did not yield further explicit case studies beyond the UNAIR Repository article. ## Final Information Based on the analysis of web pages and further exploration, one key case study of CLTS integration with Aedes mosquito control in rural Southeast Asia was identified: **Case Study from UNAIR Repository:** - **Location:** Rural villages in Indonesia, Vietnam, and the Philippines - **Intervention:** CLTS was combined with Aedes control measures including: - Community mobilization and latrine construction - Larvicide distribution for water container treatment - Education on identifying and eliminating breeding sites - Proper waste disposal and water storage practices - **Outcomes:** - 40% reduction in Aedes breeding sites in CLTS-integrated villages - Significant improvements in sanitation facilities - Enhanced community engagement and sustained behavior change While Web Page 6 mentioned government-community partnerships in vector control, and Web Page 10 highlighted breeding site management (though in Cameroon), neither explicitly discussed CLTS integration with Aedes control. The findings suggest that CLTS can be effectively combined with mosquito control by addressing the environmental conditions that enable mosquito breeding, particularly through improved waste management and water storage practices that reduce standing water sources. 32 Table 9: report generation example from the main reasoning process of WebThinker on the Glaive dataset, concerning the optimization of 3D printed lattice structures for robotics. Special symbols highlight search queries/results ( blue box ) and writing/checking/editing actions ( green box ). [...] indicates omitted content. Research Question How can optimize the lattice structure of 3D printed robotic node for maximum strength-to-weight ratio while minimizing the amount of support material required, considering the limitations of FDM printing and the need for integrated connectors for inter-robot communication and power transfer in swarm robotics application? Research Plan (1) Search for \"FDM 3D printing limitations\" to understand the constraints of the printing process. (2) Look up \"lattice structure optimization for 3D printing\" to find general strategies and software tools. [...] (7) Look into \"material properties for 3D printing lattice structures\" for best materials. (8) Find case studies on \"3D printed robotic components\" for real-world examples. Model Output by WebThinker risk sagging **Overhang Angles**: - Overhangs exceeding shallow angles (e.g., 45) Okay, so need to help write scientific article about optimizing 3D printed robotic nodes lattice structure [...] Let me start by following the research plan step by step. <begin_search_query> FDM 3D printing limitations <end_search_query> <begin_search_result> The limitations of FDM (Fused Deposition Modeling) 3D printing technology relevant to optimizing the lattice structure of robotic node include: 1. **Print Orientation**: - FDM prints layer-by-layer, so orientation affects structural integrity and support requirements [...] 2. [...] <end_search_result> [... multiple additional searches performed for each research plan point ...] Now need to start writing the sections based on the gathered info. Let me start with the Introduction section first. <begin_write_section> Introduction The optimization of lattice structures in 3D printed robotic nodes is crucial for achieving high strengthto-weight ratios and minimizing support material, particularly in swarm robotics applications. Fused Deposition Modeling (FDM) printing presents unique challenges such as orientation-dependent anisotropy, limited overhang angles, and material constraints, which must be addressed to ensure structural integrity [...] <end_write_section> [... continues writing each section according to research plan ...] <begin_search_query> material properties for 3D printing lattice structures <end_search_query> <begin_search_result> PLA offers moderate rigidity but struggles with interlayer adhesion. TPEE provides superior energy absorption (3260 kJ/m3) compared to PLA (200.90 kJ/m3). TPU excels in producing flexible lattice structures, with honeycomb architectures demonstrating 30% higher rigidity than gyroid structures [...] <end_search_result> <begin_write_section> Material Selection for Enhanced Performance Selecting the right material is pivotal for optimizing lattice structures. PLA is cost-effective but brittle, requiring careful design to prevent delamination. TPU offers flexibility and superior energy absorption, making it ideal for connectors and flexible nodes [...] <end_write_section> <begin_check_article> # Optimizing Lattice Structures in 3D Printed Robotic Nodes for Swarm Robotics: Balancing Strength-to-Weight Ratio and Minimizing Support Material ## Introduction ## FDM 3D Printing Limitations Impacting Lattice Design ### Print Orientation Effects [...] <end_check_article> [... notices duplicate sections ...] <begin_edit_article> Remove duplicate \"Real-World Applications and Case Studies\" section and ensure each numbered section corresponds correctly to the research plans eight points. Also, reorder sections to follow the research plan sequence precisely. <end_edit_article> [... notices missing conclusion after editing ...] <begin_write_section> Conclusion In conclusion, optimizing the lattice structure of 3D printed robotic nodes [...] <end_write_section> <begin_edit_article> Ensure all sections maintain consistent formatting and cross-reference related concepts throughout the article. Add transition sentences between major sections to improve flow. <end_edit_article> have finished my work. 33 Table 10: An example of report generation by WebThinker: summary outlining the structure and key sections of the final report on optimizing 3D printed lattice structures for swarm robotics. Final Report Summary Optimizing Lattice Structures in 3D Printed Robotic Nodes for Swarm Robotics: Balancing Strengthto-Weight Ratio and Minimizing Support Material Introduction Swarm robotics draws inspiration from social insects. The optimization of lattice structures in 3D printed robotic nodes is crucial for achieving high strength-to-weight ratios and minimizing support material, particularly in swarm robotics applications. Fused Deposition Modeling (FDM) printing presents unique challenges [...] FDM 3D Printing Limitations Impacting Lattice Design Fused Deposition Modeling (FDM) 3D printing is widely adopted but has inherent limitations affecting lattice design, including print orientation effects, overhang limitations, material property restrictions, and layer adhesion challenges [...] Lattice Structure Optimization Techniques Optimizing lattice structures involves balancing strength, weight, and support material reduction. Key approaches include multi-objective frameworks (e.g., combining Design of Experiments (DOE) and Genetic Algorithms (GA)), selecting prominent lattice patterns (e.g., Hexagon, Diamond/Octet, BCC/SC, Fluorite), using appropriate software tools, optimizing printing parameters, minimizing support material, and integrating connectors [...] Key Factors Influencing Strength-to-Weight Ratio Achieving high strength-to-weight ratio is essential. This involves careful material selection, lattice architecture design, applying optimization techniques, performing testing and validation, and balancing competing parameters [...] Reducing Support Material Through Design and Process Adjustments Minimizing support material is essential for cost, efficiency, and quality. Strategies include strategic design modifications, optimizing printing orientation, using advanced support types, manual support placement, stress-guided topology optimization, and integrating connectors [...] Integrating Connectors for Inter-Robot Communication and Power Transfer In swarm robotics, communication and power transfer between nodes are essential. This requires integrating suitable mechanical and electrical connectors (e.g., Box Joint, Dovetail, Snap Fits), considering material selection, integration techniques, and specific design considerations [...] Design Considerations Specific to Swarm Robotics Swarm robotics requires coordination through decentralized interactions. Design must account for communication systems, energy efficiency, mobility, structural integrity, and connector integration [...] Material Selection for Enhanced Performance Selecting appropriate materials (e.g., PLA, TPU, TPEE) is crucial. Considerations include mechanical properties, suitability for lattice structures, potential for closed-cell topologies or hybrid strategies, and specific material parameters and printing techniques [...] Real-World Applications and Case Studies Practical applications demonstrate the effectiveness of these principles, including custom protective covers, modular structures using spatial lattice printing, rapid tooling replacement, and lattice-based nodes for swarm systems [...] Conclusion Optimizing lattice structures for 3D printed swarm robots requires holistic approach addressing FDM limitations, materials, and design. Selecting appropriate materials and lattice patterns enables high strength-toweight ratios while minimizing support. Connector integration enhances functionality. Careful consideration of parameters, orientation, and materials leads to robust, lightweight nodes for coordinated tasks, as shown in case studies [...] Future work should focus on further optimization and exploring new materials and technologies."
        }
    ],
    "affiliations": [
        "BAAI",
        "Huawei Poisson Lab",
        "Renmin University of China"
    ]
}