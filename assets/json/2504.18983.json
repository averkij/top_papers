{
    "paper_title": "MediAug: Exploring Visual Augmentation in Medical Imaging",
    "authors": [
        "Xuyin Qi",
        "Zeyu Zhang",
        "Canxuan Gang",
        "Hao Zhang",
        "Lei Zhang",
        "Zhiwei Zhang",
        "Yang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at https://github.com/AIGeeksGroup/MediAug."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 3 8 9 8 1 . 4 0 5 2 : r MediAug: Exploring Visual Augmentation in Medical Imaging Xuyin Qi12, Zeyu Zhang13, Canxuan Gang4, Hao Zhang5, Lei Zhang5, Zhiwei Zhang6, Yang Zhao1 1La Trobe 2AIML 3ANU 4UNSW 5UCAS 6PSU Equal contribution. Project lead. Corresponding author: y.zhao2@latrobe.edu.au. Abstract. Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix based strategies unclear. To address these challenges, we propose unified evaluation framework with six mix based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at https://github.com/AIGeeksGroup/MediAug. Keywords: Data Augmentation Low-Level Vision Medical Imaging."
        },
        {
            "title": "Introduction",
            "content": "Data augmentation (DA) expands limited datasets with synthetic variations that preserve labels such as flips, rotations and elastic deformations, and it has become indispensable for deep learning [73]. In medical image analysis where scans annotated by experts are scarce, class distributions are skewed and diagnostic cues are subtle, DA directly translates into more reliable screening, triage and treatment planning systems. Two significant challenges remain. First, there is pronounced domain gap between natural photographs, which motivated most modern DA methods, and medical images whose low contrast, high noise and 2 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. dense semantic content can make policies designed for ImageNet distort or remove critical disease features, leaving these methods underexplored in clinical AI [60]. Second, DA studies that focus on medical imaging are often fragmented and limited to single task or model, so it remains unclear which advanced strategies based on mixing images truly improve performance and why [44]. To address these challenges, we investigate whether the state of the art mix based DA policies, originally developed for natural images, can boost medical classifiers. We introduce MediAug, unified pipeline that applies six prominent techniques, including MixUp [99], YOCO [34], CropMix [35], CutMix [97], AugMix [38] and SnapMix [42], to two clinically relevant datasets, namely the brain tumour MRI dataset [7] and the eye disease fundus dataset [24]. We evaluate both ResNet-50 [37] and ViT-B [25] backbones by conducting comprehensive experiments that reveal how augmentation and architecture choices affect performance. We introduce MediAug, comprehensive and reproducible study of advanced DA strategies for medical imaging that offers unified reference and practical guidance for the research community. Our study provides systematic evaluation of MixUp, CutMix, SnapMix, AugMix, CropMix and YOCO across convolutional and transformer backbones, highlighting their respective strengths and limitations. We conducted comprehensive experiments on the brain tumour and eye disease datasets, achieving 79.19% with MixUp on ResNet-50 and 99.44% with SnapMix on ViT-B for brain tumour classification and 91.60% with YOCO on ResNet-50 and 97.94% with CutMix on ViT-B for eye disease classification, indicating that MixUp on ResNet-50 and SnapMix on ViT-B are optimal for brain tumours and YOCO on ResNet-50 and CutMix on ViT-B are optimal for eye diseases."
        },
        {
            "title": "2 Related Work",
            "content": "Visual data augmentation has long been cornerstone in computer vision, in natural images, random crops [47] mitigate overfitting, flips [66] improve generalization, rotations [73] increase feature robustness, and color jitter [73] expands appearance diversity. Noise injection and elastic deformations [66] further enhance resilience to imaging artifacts. In medical imaging, DA enhances image classification [58,68,91,103], object detection [107,105,9,10,108], and semantic segmentation [113,101,80,27,81,104,90,106] performance, where expert annotated scans are scarce and manual labeling costs are high, these techniques reduce reliance on large annotated datasets [57], improve the accuracy of automated screening and triage systems [78], support semi supervised and transfer learning workflows [14] when annotations are limited, assist clinicians by lowering diagnostic costs [26,102], accelerate clinical throughput [70,39], and enable more consistent segmentation of lesions and organs in complex modalities such as MRI and CT [57,67]. Beyond these traditional methods, advanced mix based MediAug: Exploring Visual Augmentation in Medical Imaging 3 Fig. 1: Architecture of MediAug: We enhance medical representation learning via advanced visual augmentation. approaches generate richer training samples by semantically combining multiple images, for example MixUp blends pairs of images and their labels to smooth decision boundaries [99], YOCO applies independent augmentations to subregions to enhance local and global diversity [34], CropMix merges crops at multiple scales to capture multi resolution features [35], CutMix replaces patches between images to preserve spatial context [97], AugMix ensembles diverse augmentation chains to improve model calibration and uncertainty estimation [38], and SnapMix leverages class activation maps to guide semantic mixing and improve fine grained classification [42]. These strategies have demonstrated significant performance gains on natural image benchmarks and offer promising directions for robust medical image analysis."
        },
        {
            "title": "3.1 Overview",
            "content": "Our method enhances medical representation learning by applying advanced visual data augmentation strategies, as illustrated in fig. 1. To begin with, we create augmented versions of each input image with six popular techniques: MixUp [99], YOCO [34], CropMix [35], CutMix [97], AugMix [38], and SnapMix [42], together with the original image as baseline. These images are then processed by two backbone networks: Vit-B [25] pretrained with JointViT [103] and ResNet-50 [37] pretrained with MedConv [68]. classification head outputs disease labels, enabling the backbones to learn robust and transferable medical representations from the augmented data."
        },
        {
            "title": "3.2 MixUp",
            "content": "Mixup [99] is data augmentation technique that improves the generalization ability of neural networks by interpolating images and labels. Let Ia and Ib 4 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. Fig. 2: Augmentation in Eye diseases classification dataset [24]: (a) An image of cataract. (b) MixUp [99]: Original image and contrast-enhanced image is mixed with mixing parameter λ = 0.42. (c) YOCO [34]: splits, flips, and recombines the image for augmentation. (d) CropMix [35]: combines three 25% cropped views using MixUp [99] for augmentation. (e) CutMix [97]: augments by relocating 1/4 cropped region within the image. (f) AugMix [38]: AugMix blends blurring, sharpening, and color adjustments with the original. (g) SnapMix [42]: SnapMix blends interpolated regions, weighted by saliency maps, with the original. represent two randomly selected input images from the training dataset, and ya and yb be their corresponding one-hot encoded labels. mixing coefficient λ is sampled from Beta distribution, denoted as λ Beta(α, α), where α > 0 controls the strength of interpolation. The mixed image and label are computed as: = λIa + (1 λ)Ib, = λya + (1 λ)yb. Ia and Ib are two randomly selected input images, while ya and yb are their corresponding one-hot encoded labels. The parameter λ represents the mixing coefficient, sampled from Beta distribution with parameter α > 0, which determines the interpolation ratio between Ia and Ib. The interpolated image and label are obtained by combining Ia and Ib, as well as ya and yb, respectively, using λ and 1 λ as weights. During training, the loss function is defined as: = 1 (cid:88) i= ℓ(f ( Ii), yi), MediAug: Exploring Visual Augmentation in Medical Imaging 5 Fig. 3: Augmentation in Brain Tumor Classification (MRI) dataset [7]: (a) An image of no_tumor in Testing. (b) MixUp [99]: Contrast-adjusted and mixed (λ = 0.30), in grayscale and pseudo-color. (c) YOCO [34]: The image was resized, contrast-enhanced, flipped, and concatenated. (d) CropMix [35]: It from contrast-enhanced, randomly cropped image using CutMix. (e) CutMix [97]: Original image flipped and mixed using random CutMix enhancement (λ = 0.70). (f) AugMix [38]: It enhances the image with flipping and brightness adjustment. (g) SnapMix [42]: It enhances by blending resized cropped regions with the original image. where denotes the neural network model, ℓ represents the loss function (e.g., cross-entropy), is the batch size, and ( Ii, yi) are the mixed image and label for the i-th training example. In medical imaging, Mixup, as illustrated in panels (b) of figs. 2 and 3, is particularly effective in addressing challenges such as limited labeled data and class imbalance. By interpolating images and labels, Mixup enhances the dataset while maintaining semantic consistency. For example, in lesion classification tasks, Mixup blends regions of interest across different images, helping the model focus on localized features. This technique alleviates overfitting, improves robustness to label noise, and reduces sensitivity to adversarial examples, making it highly valuable for medical image analysis."
        },
        {
            "title": "3.3 YOCO",
            "content": "The YOCO method [34], as illustrated in panels (c) of figs. 2 and 3, is designed for processing medical images RCHW , where represents the number of image channels (e.g., grayscale or RGB), is the height, and is the width of 6 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. the image. The goal of YOCO is to apply data augmentation in way that enhances both local and global diversity while maintaining the structural integrity of the image. data augmentation function a() : RCHW RCHW transforms the input image into an augmented image = a(X). Unlike traditional augmentation methods, YOCO first splits the input image into sub-regions. To achieve this, the image is randomly split into two parts along either the height or width dimension. Specifically, if the random variable (0, 1) (a uniformly distributed random number between 0 and 1) satisfies 0 < 0.5, the image is split along the height, resulting in two sub-images [X1, X2] = cutH(X). Alternatively, if 0.5 < 1, the image is split along the width, resulting in [X1, X2] = cutW(X). The dimensions of these sub-images depend on the splitting direction: - For height splitting, X1, X2 RC 2 , - For width splitting, X1, X2 RCH 2 . After splitting, independent augmentation functions a1() and a2() are applied to the two sub-images X1 and X2, respectively. These augmentations may involve operations such as rotation, flipping, or intensity adjustment. The final augmented image is then reconstructed by concatenating the two augmented sub-images along the original splitting axis: = concat[a1(X1), a2(X2)]. To generalize this approach for more complex scenarios, YOCO can split the image into + 1 parts along the height and + 1 parts along the width, producing (M + 1) (N + 1) sub-images. This is represented as: [Xi,j] = cutM,N (X), = 1, . . . , + 1, = 1, . . . , + 1, where Xi,j denotes the sub-image located in the i-th row and j-th column of the grid. Each sub-image Xi,j is independently augmented using corresponding augmentation function ai,j(). The final augmented image is then reconstructed by concatenating all augmented sub-images: = concatM +1,N +1 i=1,j=1 [ai,j(Xi,j)]. By augmenting sub-regions while preserving global structure, YOCO boosts data diversity, aiding detection of localized features in medical imaging and improving model robustness."
        },
        {
            "title": "3.4 CropMix",
            "content": "CropMix [35] is data augmentation method that improves the generalization ability of neural networks by combining multi-scale random crops to capture diverse features and suppress labeling errors. In medical imaging, this method is particularly effective in addressing challenges such as limited labeled data and complex lesion structures. By generating multiple cropped views of the original image, CropMix enhances data diversity while preserving semantic consistency. MediAug: Exploring Visual Augmentation in Medical Imaging 7 Fig. 4: Eye disease classification dataset [24] with four categories (23.9%-26.0%). Fig. 5: t-SNE visualization of eye diseases [24], showing clustering patterns across cataract, diabetic retinopathy, glaucoma, and normal cases. Let represent the original input medical image, and Ii denote cropped view of I, obtained through random resized cropping (RRC) with crop scale si, where si [smin, smax]. The cropped views Ii are generated to capture both fine-grained and coarse-grained information from the image. To combine these views, mixing coefficient λ is sampled from Beta distribution, denoted as λ Beta(α, α), where α > 0 controls the interpolation strength. The mixed image is computed using the following formula: = λI1 + (1 λ)I2, where I1 and I2 are two randomly selected cropped views of I, and λ determines the mixing ratio. Since all cropped views are derived from the same image, the label of the original image remains unchanged, ensuring semantic consistency: = y. In medical imaging applications, CropMix, as illustrated in panels (d) of figs. 2 and 3, leverages the multi-scale information captured from the cropped views to enhance the models ability to learn localized lesion features while retaining global anatomical structures. For example, in lesion classification tasks, the combination of fine-grained and coarse-grained information helps the model focus on both detailed lesion characteristics and broader contextual cues. By interpolating cropped views and preserving labels, CropMix effectively reduces sensitivity to label noise, alleviates overfitting, and improves robustness, making it highly valuable for medical image analysis. 8 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. Fig. 6: Brain MRI dataset [7] showing Meningioma tumor 28.7%, Glioma 28.4%, Pituitary 27.6%, No tumor 15.3%. Fig. 7: t-SNE visualization of brain tumor categories [7] showing clustering of glioma, meningioma, pituitary, and nontumor cases."
        },
        {
            "title": "3.5 CutMix",
            "content": "CutMix [97] is data augmentation technique that improves the generalization ability of neural networks by combining regions from different images and interpolating their labels. Let IA and IB represent two randomly selected input images from the training dataset, and yA and yB be their corresponding onehot encoded labels. binary mask is used to define the region in IA that is replaced with patch from IB. The mixing ratio λ is sampled from Beta distribution, denoted as λ Beta(α, α), where α > 0 controls the interpolation strength. The mixed image and label are computed as: = IA + (1 ) IB, = λyA + (1 λ)yB. IA and IB are two randomly selected input images, while yA and yB are their corresponding one-hot encoded labels. The binary mask defines the region in IA that is replaced with pixels from IB, and λ represents the mixing ratio, which is proportional to the area of . The mixed image is obtained by combining IA and IB based on , while the mixed label interpolates yA and yB using λ and 1 λ as weights. The binary mask is determined by sampling the bounding box coordinates rx and ry, and dimensions rw and rh. The coordinates rx and ry are sampled uniformly: rx Unif(0, ), ry Unif(0, H), where and are the width and height of the image. The dimensions rw and rh are computed as: rw = 1 λ, rh = 1 λ. MediAug: Exploring Visual Augmentation in Medical Imaging 9 Table 1: Comparative performance on brain tumor classification dataset [7]. Method Backbone Accuracy Precision Recall Sensitivity Specificity F1 Score ROC AUC Baseline AugMix CropMix CutMix MixUp SnapMix YOCO 76.4 85. 75.26 81.55 76.14 82.58 83.18 ResNet-50 ViT-B 84.78 ResNet-50 76.65+0.25 82.35-0.83 76.65+0.51 75.94+0.68 97.51+12.31 84.63-0.15 84.32+1.74 83.39+1.84 ViT-B 71.20-4.06 78.58-4.60 73.35-2.79 73.35-3.05 ResNet-50 99.05+13.85 86.10+1.32 86.06+3.48 85.40+3.85 ViT-B 74.37-2.03 ResNet-50 72.63-2.63 81.82-1.36 74.37-1.77 97.61+12.41 87.57+2.79 87.28+4.70 86.63+5.08 ViT-B ResNet-50 79.19+2.79 84.50+1.32 74.36-1.78 79.19+3.93 98.52+13.32 89.05+4.27 89.02+6.44 89.14+7.59 ViT-B ResNet-50 78.68+2.28 84.21+1.03 78.68+2.54 77.25+1.99 99.44+14.24 90.68+5.90 90.59+8.01 90.83+9.28 ViT-B 74.37-2.03 ResNet-50 72.78-2.48 79.73-3.45 74.37-1.77 95.02+9.82 87.64+2.86 87.63+5.05 87.92+6.37 ViT-B 92.34 72.22 91.69 96.61 82.55 93.94 94.11+1.77 73.72+1.5 91.92+0.23 95.39-1.22 84.32+1.77 94.53+0.59 91.47-0.87 70.85-1.37 90.78-0.91 96.80+0.19 86.00+3.45 95.16+1.22 91.87-0.47 72.73+0.51 91.04-0.65 95.59+1.65 96.41-0.20 87.29+4.74 78.53-13.16 92.77+20.55 76.55-15.79 97.42+0.81 88.98+6.43 96.27+2.33 93.70+1.36 76.72+4.50 92.58+0.89 97.82+1.21 90.62+8.07 96.72+2.78 91.79-0.55 71.92-0.30 91.11-0.58 97.21+0.60 87.54+4.99 95.75+1.81 During training, the loss function is defined as: = 1 (cid:88) i=1 ℓ(f ( Ii), yi), where denotes the neural network model, ℓ represents the loss function (e.g., cross-entropy), is the batch size, and ( Ii, yi) are the mixed image and label for the i-th training example. In medical imaging, CutMix, as illustrated in panels (e) of figs. 2 and 3, is particularly effective in addressing challenges such as limited labeled data and class imbalance. By replacing regions between images and interpolating labels, CutMix enhances the dataset while maintaining spatial and semantic consistency. For example, in lesion detection tasks, CutMix combines regions of interest across different images, helping the model focus on localized features. This technique alleviates overfitting, improves robustness to label noise, and reduces sensitivity to adversarial examples, making it highly valuable for medical image analysis."
        },
        {
            "title": "3.6 AugMix",
            "content": "AugMix [38] is data augmentation method that enhances robustness and uncertainty estimation by mixing diverse augmentations while enforcing consistency. In medical imaging, it effectively addresses challenges such as data corruption and unseen perturbations, ensuring reliable predictions in critical tasks. Let xorig represent the original input medical image, and denote the set of augmentation operations (e.g., rotation, translation, posterization). AugMix generates augmented images by combining multiple augmentation chains, where is the number of chains, and wi is the mixing weight for the i-th chain, sampled from Dirichlet distribution: Dirichlet(α, . . . , α). The augmented image xaug is computed as: 10 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. Table 2: Comparative performance on eye diseases classification dataset [24]. Method Backbone Accuracy Precision Recall Sensitivity Specificity F1 Score ROC AUC Baseline AugMix CropMix CutMix MixUp SnapMix YOCO 87.95 80. 85.78 78.81 85.79 90.77 ResNet-50 78.90 80.36 ViT-B 83.33-2.46 84.55-3.40 83.31-2.47 83.31-7.46 ResNet-50 ViT-B 93.71+13.35 82.13+1.52 81.63+2.82 81.51+2.61 ResNet-50 73.25-17.52 77.15-10.80 73.25-12.53 73.57-12.22 ViT-B 97.32+16.96 83.91+3.30 83.56+4.75 83.55+4.65 ResNet-50 73.25-17.52 77.15-10.80 73.25-12.53 73.57-12.22 97.94+17.58 81.08+0.47 81.04+2.23 80.87+1.97 ViT-B 88.99-1.78 89.50+1.55 88.99+3.21 88.68+2.89 ResNet-50 90.55-0.22 83.88+3.27 83.26+4.45 83.14+4.24 ViT-B 87.69-3.08 88.81+0.86 87.69+1.91 87.32+1.53 ResNet-50 ViT-B 93.93+3.16 81.14+0.53 81.04+2.23 80.76+1.86 ResNet-50 91.60+0.83 91.78+3.83 91.60+5.82 91.37+5.58 97.72+17.36 87.65+7.04 87.56+8.75 87.36+8.46 ViT-B 96.12 85.98 85.98 95.45 79.17 92.99 95.85-0.27 83.53-2.45 94.46+8.48 94.70-0.75 93.91+0.92 81.51+2.34 92.24-3.88 91.13+5.15 72.90-13.08 95.13-0.32 94.54+1.55 83.66+4.49 92.24-3.88 91.13+5.15 72.90-13.08 93.69+0.70 80.96+1.79 94.74-0.71 96.33+10.35 88.75+2.77 97.69+1.57 94.44+1.45 83.47+4.30 95.96+0.51 95.89+9.91 87.59+1.61 96.51+0.39 93.69+0.70 80.99+1.82 94.20-1.25 97.20+11.22 91.51+5.53 97.89+1.77 95.86+2.87 87.52+8.35 97.27+1.82 xaug = (cid:88) i= wi chaini(xorig), where chaini() represents an augmentation chain composed of multiple operations. The final AugMix image xaugmix is interpolated between xorig and xaug, with interpolation weight sampled from Beta distribution: Beta(α, α). The interpolation is defined as: xaugmix = xorig + (1 m) xaug. To enforce consistency across embeddings, AugMix introduces the Jensen-Shannon divergence consistency loss LJS. This loss measures the divergence between the models predicted distributions for the original image xorig and two independently generated AugMix images xaugmix1 and xaugmix2. The loss is defined as: LJS = JS(p(yxorig), p(yxaugmix1), p(yxaugmix2)), where p(yx) is the models predicted distribution for image x, and JS() represents the Jensen-Shannon divergence. In medical imaging, AugMix, as illustrated in panels (f) of figs. 2 and 3, is particularly effective in improving robustness against unseen perturbations and enhancing uncertainty estimation. The augmented image xaug combines diverse augmentation chains to introduce variability while preserving semantic consistency. The AugMix image xaugmix interpolates between the original and augmented images, ensuring the final image remains meaningful. The Jensen-Shannon divergence loss LJS enforces consistency across embeddings, improving calibration and reducing sensitivity to data corruption. This makes AugMix valuable technique for medical imaging tasks such as disease classification, lesion segmentation, and anomaly detection. MediAug: Exploring Visual Augmentation in Medical Imaging 11 Fig. 8: ROC curves with ViT-B on brain tumor dataset [7]. Fig. 9: ROC curves with ResNet-50 on brain tumor dataset [7]."
        },
        {
            "title": "3.7 SnapMix",
            "content": "SnapMix [42] is data augmentation method that leverages class activation maps (CAM) to ensure semantic consistency when mixing images and labels. In medical imaging, SnapMix enhances model performance by focusing on localized features such as lesions, mitigating overfitting, and improving robustness in finegrained tasks like disease classification and lesion detection. Let Ii denote an input medical image with label yi (e.g., disease class or lesion type). The class activation map CAMi for Ii is normalized to sum to 1, producing semantic percent map SPMi that represents pixel-level semantic relevance. region Ri is cropped from Ii, and its semantic ratio SRi is computed as: SRi = (cid:88) pRi SPMi(p), where represents pixel in the cropped region Ri. The semantic ratio SRi quantifies the semantic importance of the region Ri based on the values in SPMi. To create mixed image I, SnapMix combines cropped regions Ra and Rb from two input images Ia and Ib: = Ra + Rb. The mixed label for is computed by weighting the original labels ya and yb of Ia and Ib according to their semantic ratios SRa and SRb: = SRa SRa + SRb ya + SRb SRa + SRb yb. This ensures that the mixed label accurately reflects the semantic contributions of Ra and Rb to the mixed image I. 12 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. Fig. 10: ROC curves with ViT-B on eye diseases dataset [24]. Fig. 11: ROC curves with ResNet-50 on eye diseases dataset [24]. In medical imaging, SnapMix, as illustrated in panels (g) of figs. 2 and 3, is particularly effective for tasks involving fine-grained details, such as lesion segmentation and disease classification. By using CAMs to guide the mixing process, SnapMix ensures that the mixed images retain meaningful semantic information. The semantic ratio SRi captures the importance of specific regions, allowing the method to focus on areas of high diagnostic relevance. This approach mitigates overfitting, enhances model robustness to data variability, and improves generalization to unseen cases, making SnapMix valuable tool in medical image analysis."
        },
        {
            "title": "4.1 Datasets and Evaluation Matrices",
            "content": "Brain Tumor Classification Dataset [7] includes four categories: glioma tumor, meningioma tumor, no tumor, and pituitary tumor. pie chart fig. 6 reveals an imbalanced class distribution. t-SNE visualization fig. 7 shows pituitary tumor forms distinct clusters, while glioma tumor and meningioma tumor partially overlap. No tumor is dispersed and overlaps significantly with other categories, increasing classification difficulty. Eye Diseases Classification Dataset [24] includes four categories: cataract, diabetic retinopathy, glaucoma, and normal. balanced class distribution fig. 4 makes it suitable for multi-class classification. t-SNE visualization fig. 5 shows clear clustering for cataract and diabetic retinopathy, while glaucoma and normal partially overlap, increasing classification difficulty. Evaluation Matrices: Accuracy measures overall correctness. Precision assesses the accuracy of positive predictions, while Recall measures the ability to identify MediAug: Exploring Visual Augmentation in Medical Imaging 13 Table 3: Ablation study on the CutMix interpolation parameter α using the ViTB backbone on the eye diseases dataset, demonstrating CutMix performance across different α values. α value Accuracy Precision Recall Sensitivity Specificity F1 Score ROC AUC α = 1.0 α = 0.8 α = 0.6 α = 0.4 α = 0. 94.74 94.32 93.86 93.12 92.47 80.96 80.37 79.79 78.53 77.05 81.08 80.42 79.76 78.45 76.92 97.94 96.78 95.35 93.21 90.64 80.87 80.15 79.64 78.40 76.95 81.04 80.33 79.82 78.61 77. 93.69 93.21 92.87 92.35 91.83 actual positives. Sensitivity (similar to Recall) focuses on detecting positives, and Specificity evaluates the identification of negatives. F1-Score balances Precision and Recall, and ROC AUC reflects the models ability to distinguish between classes, with higher values indicating better performance. 4."
        },
        {
            "title": "Implementation Details",
            "content": "The datasets were randomly split into Training and Testing sets with an 8:2 ratio, using fixed random seed to ensure reproducibility. Six data augmentation methods were applied exclusively to the training sets, while the testing sets remained untouched to evaluate model generalization. All models were trained for 50 epochs using an initial learning rate of 0.001 and the Adam optimizer. Experiments ran on an NVIDIA A100-SXM4-40GB GPU, CUDA 12.4, Intel Xeon CPU @ 2.20GHz, and 80G memory."
        },
        {
            "title": "4.3 Comparative Study",
            "content": "Tables 1 and 2 present our comparative experiments on augmentation techniques for medical image classification. For brain tumour classification  (Table 1)  , MixUp with ResNet-50 achieves the highest performance gains, increasing accuracy to 79.19%, while SnapMix with ViT-B achieves the highest gains, increasing accuracy to 99.44%, as evidenced by the ROC curves in Figs. 8 and 9. For eye diseases classification  (Table 2)  , YOCO with ResNet-50 achieves the highest performance gains, increasing accuracy by 0.83% to 91.60% and ROC AUC by 1.77% to 97.89%. Meanwhile, CutMix with ViT-B achieves the highest gains, increasing accuracy by 17.58% to 97.94%, as shown in Figs. 10 and 11. Our study indicates that specific combinations excel in particular applications, with MixUp on ResNet-50 and SnapMix on ViT-B optimal for brain tumour classification and YOCO on ResNet-50 and CutMix on ViT-B optimal for eye disease classification."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Tables 3 and 4 present ablation studies on the CutMix interpolation parameter α. Our experiments show that performance consistently improves as α increases 14 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. Table 4: Ablation study on the CutMix interpolation parameter α using ResNet50 backbone on the eye diseases dataset, demonstrating CutMix performance across different α values. α value Accuracy Precision Recall Sensitivity Specificity F1 Score ROC AUC α = 1.0 α = 0.8 α = 0.6 α = 0.4 α = 0.2 91.86 91.32 90.92 90.50 90.21 98.22 97.86 97.43 97.12 96. 91.83 91.25 90.86 90.42 90.15 91.97 91.39 90.98 90.58 90.28 91.67 91.08 90.72 90.25 90.04 97.29 97.05 96.84 96.65 96.42 91.83 91.25 90.86 90.42 90.15 across both ViT-B and ResNet-50 architectures, with the optimal value α = 1.0 yielding the highest scores across all metrics, ViT-B achieving 97.94% accuracy and ResNet-50 reaching 91.83% accuracy. These results underscore the importance of systematic hyperparameter sweeps for mix-based augmentations but as demonstrated in Section 4.3 they only reflect CutMixs internal parameter tuning and do not alter the overall optimal strategies, where MixUp on ResNet-50 and SnapMix on ViT-B remain best for brain tumour classification and YOCO on ResNet-50 and CutMix on ViT-B remain best for eye disease classification. Moreover, this concrete example of CutMix tuning illustrates general approach: the same protocol can and should be applied to fine-tune MixUps interpolation weight, SnapMixs semantic mixing ratios and YOCOs region split configurations, demonstrating that hyperparameter optimization holds practical significance across all mix-based augmentation methods."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work MediAug provides reproducible benchmark for six mix based augmentation methods on brain tumour MRI and eye disease fundus classification. Experiments using ResNet 50 and ViT-B identify MixUp with ResNet 50 and SnapMix with ViT-B as optimal for brain tumour classification and YOCO with ResNet 50 and CutMix with ViT-B as optimal for eye disease classification. CutMix ablation highlights the value of systematic hyperparameter tuning and can be applied to other mix based methods. By offering clear, reproducible guidance on optimal augmentation architecture combinations and tuning protocols MediAug empowers the medical imaging community to develop more robust, generalizable AI models and accelerate the safe and effective integration of deep learning into clinical decision making. MediAug: Exploring Visual Augmentation in Medical Imaging"
        },
        {
            "title": "References",
            "content": "1. Agustin, T., Utami, E., Al Fatta, H.: Implementation of data augmentation to improve performance cnn method for detecting diabetic retinopathy. In: 2020 3rd International Conference on Information and Communications Technology (ICOIACT). pp. 8388. IEEE (2020) 2. Araújo, T., Aresta, G., Mendonça, L., Penas, S., Maia, C., Carneiro, A., Mendonca, A.M., Campilho, A.: Data augmentation for improving proliferative diabetic retinopathy detection in eye fundus images. IEEE access 8, 182462182474 (2020) 3. Babaud, J., Witkin, A.P., Baudin, M., Duda, R.O.: Uniqueness of the gaussian kernel for scale-space filtering. IEEE transactions on pattern analysis and machine intelligence (1), 2633 (1986) 4. Balasubramanian, R., Sowmya, V., Gopalakrishnan, E., Menon, V.K., Variyar, V.S., Soman, K.: Analysis of adversarial based augmentation for diabetic retinopathy disease grading. In: 2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT). pp. 15. IEEE (2020) 5. Berthelot, D., Carlini, N., Cubuk, E.D., Kurakin, A., Sohn, K., Zhang, H., Raffel, C.: Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. arXiv preprint arXiv:1911.09785 (2019) 6. Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., Raffel, C.A.: Mixmatch: holistic approach to semi-supervised learning. Advances in neural information processing systems 32 (2019) 7. Bhuvaji, S., Kadam, A., Bhumkar, P., Dedge, S., Kanchan, S.: Brain tumor classification (mri) (2020). https://doi.org/10.34740/KAGGLE/DSV/1183165, https: //www.kaggle.com/dsv/ 8. Cagli, E., Dumas, C., Prouff, E.: Convolutional neural networks with data augmentation against jitter-based countermeasures: Profiling attacks without preprocessing. In: Cryptographic Hardware and Embedded SystemsCHES 2017: 19th International Conference, Taipei, Taiwan, September 25-28, 2017, Proceedings. pp. 4568. Springer (2017) 9. Cai, G., Cai, Y., Zhang, Z., Cao, Y., Wu, L., Ergu, D., Liao, Z., Zhao, Y.: Medical ai for early detection of lung cancer: survey. arXiv preprint arXiv:2410.14769 (2024) 10. Cai, G., Zhang, R., He, H., Zhang, Z., Ergu, D., Cao, Y., Zhao, J., Hu, B., Liao, Z., Zhao, Y., et al.: Msdet: Receptive field enhanced multiscale detection for tiny pulmonary nodule. arXiv preprint arXiv:2409.14028 (2024) 11. Chatfield, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531 (2014) 12. Chen, P., Liu, S., Zhao, H., Wang, X., Jia, J.: Gridmask data augmentation. arXiv preprint arXiv:2001.04086 (2020) 13. Chen, Y., Li, Y., Kong, T., Qi, L., Chu, R., Li, L., Jia, J.: Scale-aware automatic augmentation for object detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 95639572 (2021) 14. Cheplygina, V., de Bruijne, M., Pluim, J.P.: Not-so-supervised: survey of semisupervised, multi-instance, and transfer learning in medical image analysis. Medical Image Analysis 54, 280296 (2019). https://doi.org/10.1016/j.media. 2019.02.010 16 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 15. Choi, J., Lee, C., Lee, D., Jung, H.: Salfmix: novel single image-based data augmentation technique using saliency map. Sensors 21(24), 8444 (2021) 16. Choi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo, J.: Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 87898797 (2018) 17. Choi, Y., Uh, Y., Yoo, J., Ha, J.W.: Stargan v2: Diverse image synthesis for multiple domains. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 81888197 (2020) 18. Chu, P., Bian, X., Liu, S., Ling, H.: Feature space augmentation for long-tailed data. In: Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXIX 16. pp. 694710. Springer (2020) 19. Cicalese, P.A., Mobiny, A., Yuan, P., Becker, J., Mohan, C., Nguyen, H.V.: Stypath: Style-transfer data augmentation for robust histology image classification. In: Medical Image Computing and Computer Assisted InterventionMICCAI 2020: 23rd International Conference, Lima, Peru, October 48, 2020, Proceedings, Part 23. pp. 351361. Springer (2020) 20. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation strategies from data. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 113123 (2019) 21. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated data augmentation with reduced search space. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. pp. 702703 (2020) 22. DeVries, T., Taylor, G.W.: Dataset augmentation in feature space. arXiv preprint arXiv:1702.05538 (2017) 23. DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552 (2017) 24. Doddi, G.: Eye diseases classification dataset. https://www.kaggle.com/ datasets/gunavenkatdoddi/eye-diseases-classification/data (2023), accessed: 2025-03-28 25. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2021) 26. Esteva, A., Kuprel, B., Novoa, R.A., Ko, J., Swetter, S.M., Blau, H.M., Thrun, S.: Dermatologist-level classification of skin cancer with deep neural networks. Nature 542(7639), 115118 (2017) 27. Ge, J., Zhang, Z., Phan, M.H., Zhang, B., Liu, A., Zhao, Y.: Esa: Annotation-efficient active learning for semantic segmentation. arXiv preprint arXiv:2408.13491 (2024) 28. Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.Y., Cubuk, E.D., Le, Q.V., Zoph, B.: Simple copy-paste is strong data augmentation method for instance segmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 29182928 (2021) 29. Goceri, E.: Medical image data augmentation: techniques, comparisons and interpretations. Artificial Intelligence Review 56(11), 1256112605 (2023) 30. Gong, C., Wang, D., Li, M., Chandra, V., Liu, Q.: Keepaugment: simple information-preserving data augmentation approach. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10551064 (2021) MediAug: Exploring Visual Augmentation in Medical Imaging 31. Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets. Advances in neural information processing systems 27 (2014) 32. Guo, J., Gould, S.: Deep cnn ensemble with data augmentation for object detection. arXiv preprint arXiv:1506.07224 (2015) 33. Haidar, M.A., Rezagholizadeh, M., Ghaddar, A., Bibi, K., Langlais, P., Poupart, P.: Cilda: Contrastive data augmentation using intermediate layer knowledge distillation. arXiv preprint arXiv:2204.07674 (2022) 34. Han, J., Fang, P., Li, W., Hong, J., Armin, M.A., Reid, I., Petersson, L., Li, H.: You only cut once: Boosting data augmentation with single cut. In: International Conference on Machine Learning. pp. 81968212. PMLR (2022) 35. Han, J., Petersson, L., Li, H., Reid, I.: Cropmix: Sampling rich input distribution via multi-scale cropping. arXiv preprint arXiv:2205.15955 (2022) 36. Harris, E., Marcu, A., Painter, M., Niranjan, M., Prügel-Bennett, A., Hare, J.: Fmix: Enhancing mixed sample data augmentation. arXiv preprint arXiv:2002.12047 (2020) 37. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770778 (2016) 38. Hendrycks, D., Mu, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.: Augmix: simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781 (2019) 39. Hiwase, A.D., Ovenden, C.D., Kaukas, L.M., Finnis, M., Zhang, Z., OConnor, S., Foo, N., Reddi, B., Wells, A.J., Ellis, D.Y.: Can rotational thromboelastometry rapidly identify theragnostic targets in isolated traumatic brain injury? Emergency Medicine Australasia 37(1), e14480 (2025) 40. Ho, D., Liang, E., Chen, X., Stoica, I., Abbeel, P.: Population based augmentation: Efficient learning of augmentation policy schedules. In: International conference on machine learning. pp. 27312741. PMLR (2019) 41. Hong, M., Choi, J., Kim, G.: Stylemix: Separating content and style for enhanced data augmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1486214870 (2021) 42. Huang, S., Wang, X., Tao, D.: Snapmix: Semantically proportional mixing for augmenting fine-grained data. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 35, pp. 16281636 (2021) 43. Inoue, H.: Data augmentation by pairing samples for images classification. arXiv preprint arXiv:1801.02929 (2018) 44. Islam, T., Hafiz, M.S., Jim, J.R., Kabir, M.M., Mridha, M.: systematic review of deep learning data augmentation in medical imaging: Recent advances and future research directions. Healthcare Analytics 5(1), 100340 (2024). https:// doi.org/10.1016/j.health.2024.100340 45. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversarial networks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 11251134 (2017) 46. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statistics for optimal mixup. In: International conference on machine learning. pp. 52755285. PMLR (2020) 47. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Advances in Neural Information Processing Systems. vol. 25, pp. 10971105 (2012) 18 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 48. Kuo, C.W., Ma, C.Y., Huang, J.B., Kira, Z.: Featmatch: Feature-based augmentation for semi-supervised learning. In: Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVIII 16. pp. 479495. Springer (2020) 49. Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., Srinivas, A.: Reinforcement learning with augmented data. Advances in neural information processing systems 33, 1988419895 (2020) 50. Li, B., Wu, F., Lim, S.N., Belongie, S., Weinberger, K.Q.: On feature normalization and data augmentation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1238312392 (2021) 51. Li, L., Betti, R.: machine learning-based data augmentation strategy for structural damage classification in civil infrastructure system. Journal of Civil Structural Health Monitoring 13(6), 12651285 (2023) 52. Li, P., Li, X., Long, X.: Fencemask: data augmentation approach for preextracted image features. arXiv preprint arXiv:2006.07877 (2020) 53. Li, Y., Hu, G., Wang, Y., Hospedales, T., Robertson, N.M., Yang, Y.: Differentiable automatic data augmentation. In: Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXII 16. pp. 580595. Springer (2020) 54. Lim, G., Thombre, P., Lee, M.L., Hsu, W.: Generative data augmentation for diabetic retinopathy classification. In: 2020 IEEE 32nd international conference on tools with artificial intelligence (ICTAI). pp. 10961103. IEEE (2020) 55. Lim, S., Kim, I., Kim, T., Kim, C., Kim, S.: Fast autoaugment. Advances in neural information processing systems 32 (2019) 56. Lin, C., Guo, M., Li, C., Yuan, X., Wu, W., Yan, J., Lin, D., Ouyang, W.: Online hyper-parameter learning for auto-augmentation strategy. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 65796588 (2019) 57. Litjens, G., Kooi, T., Bejnordi, B.E., Setio, A.A.A., Ciompi, F., Ghafoorian, M., van der Laak, J.A., van Ginneken, B., Sánchez, C.I.: survey on deep learning in medical image analysis. Medical image analysis 42, 6088 (2017) 58. Luo, Y., Wang, S., Liu, J., Xiao, J., Xue, R., Zhang, Z., Zhang, H., Lu, Y., Zhao, Y., Xie, Y.: Pathohr: Breast cancer survival prediction on high-resolution pathological images. arXiv preprint arXiv:2503.17970 (2025) 59. Luo, Z., Shen, T., Zhou, L., Zhang, J., Yao, Y., Li, S., Fang, T., Quan, L.: Contextdesc: Local descriptor augmentation with cross-modality context. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 25272536 (2019) 60. Ma, Y., Zhang, H., Si, Z., Li, A.: On the transferability of data augmentation approaches from natural images to medical imaging. Medical Image Analysis 81, 102533 (2022). https://doi.org/10.1016/j.media.2022.102533 61. Mansfield, P.A., Afkanpour, A., Morningstar, W.R., Singhal, K.: Random field augmentations for self-supervised representation learning. arXiv preprint arXiv:2311.03629 (2023) 62. Mumuni, A., Mumuni, F.: Data augmentation: comprehensive survey of modern approaches. Array 16, 100258 (2022) 63. Nalepa, J., Marcinkiewicz, M., Kawulok, M.: Data augmentation for brain-tumor segmentation: review. Frontiers in computational neuroscience 13, 83 (2019) 64. Olsson, V., Tranheden, W., Pinto, J., Svensson, L.: Classmix: Segmentationbased data augmentation for semi-supervised learning. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision. pp. 13691378 (2021) MediAug: Exploring Visual Augmentation in Medical Imaging 19 65. Peli, E.: Contrast in complex images. Journal of the Optical Society of America 7(10), 20322040 (1990) 66. Perez, L., Wang, J.: The effectiveness of data augmentation in image classification using deep learning. arXiv preprint arXiv:1712.04621 (2017) 67. Qi, X., Zhang, Z., Handoko, A.B., Zheng, H., Chen, M., Huy, T.D., Phan, V.M.H., Zhang, L., Cheng, L., Jiang, S., et al.: Projectedex: Enhancing generation in explainable ai for prostate cancer. arXiv preprint arXiv:2501.01392 (2025) 68. Qi, X., Zhang, Z., Zheng, H., Chen, M., Kutaiba, N., Lim, R., Chiang, C., Tham, Z.E., Ren, X., Zhang, W., et al.: Medconv: Convolutions beat transformers on long-tailed bone density prediction. arXiv preprint arXiv:2502.00631 (2025) 69. Qin, J., Fang, J., Zhang, Q., Liu, W., Wang, X., Wang, X.: Resizemix: Mixing data with preserved object information and true labels. arXiv preprint arXiv:2012.11101 (2020) 70. Rajpurkar, P., Irvin, J., Zhu, K., Yang, B., Mehta, H., Duan, T., Ding, D., Bagul, A., Langlotz, C., Shpanskaya, K., Lungren, M.P., Ng, A.Y.: Chexnet: Radiologistlevel pneumonia detection on chest x-rays with deep learning. arXiv preprint arXiv:1711.05225 (2017), computer Science Machine Learning 71. Ramé, A., Sun, R., Cord, M.: Mixmo: Mixing multiple inputs for multiple outputs via deep subnetworks. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 823833 (2021) 72. Seo, J.W., Jung, H.G., Lee, S.W.: Self-augmentation: Generalizing deep networks to unseen classes for few-shot learning. Neural Networks 138, 140149 (2021) 73. Shorten, C., Khoshgoftaar, T.M.: survey on image data augmentation for deep learning. Journal of big data 6(1), 148 (2019) 74. Singh, K.K., Yu, H., Sarmasi, A., Pradeep, G., Lee, Y.J.: Hide-and-seek: data augmentation technique for weakly-supervised localization and beyond. arXiv preprint arXiv:1811.02545 (2018) 75. Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.A., Cubuk, E.D., Kurakin, A., Li, C.L.: Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems 33, 596608 (2020) 76. Summers, C., Dinneen, M.J.: Improved mixed-example data augmentation. In: 2019 IEEE winter conference on applications of computer vision (WACV). pp. 12621270. IEEE (2019) 77. Sun, X., Fang, H., Yang, Y., Zhu, D., Wang, L., Liu, J., Xu, Y.: Robust retinal vessel segmentation from data augmentation perspective. In: Ophthalmic Medical Image Analysis: 8th International Workshop, OMIA 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings 8. pp. 189198. Springer (2021) 78. Tajbakhsh, N., Shin, J.Y., Gurudu, S.R., Hurst, R.T., Kendall, C.B., Gotway, M.B., Liang, J.: Convolutional neural networks for medical image analysis: Full training or fine tuning? In: IEEE Transactions on Medical Imaging. vol. 35, pp. 12991312 (2016). https://doi.org/10.1109/TMI.2016.2535302 79. Takahashi, R., Matsubara, T., Uehara, K.: Ricap: Random image cropping and patching data augmentation for deep cnns. In: Asian conference on machine learning. pp. 786798. PMLR (2018) 80. Tan, S., Xue, R., Luo, S., Zhang, Z., Wang, X., Zhang, L., Ergu, D., Yi, Z., Zhao, Y., Cai, Y.: Segkan: High-resolution medical image segmentation with longdistance dependencies. arXiv preprint arXiv:2412.19990 (2024) 20 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 81. Tan, S., Zhang, Z., Cai, Y., Ergu, D., Wu, L., Hu, B., Yu, P., Zhao, Y.: Segstitch: Multidimensional transformer for robust and efficient medical imaging segmentation. arXiv preprint arXiv:2408.00496 (2024) 82. Tang, C., Yang, X., Zhai, G.: Noise estimation of natural images via statistical analysis and noise injection. IEEE Transactions on Circuits and Systems for Video Technology 25(8), 12831294 (2014) 83. Tian, K., Lin, C., Sun, M., Zhou, L., Yan, J., Ouyang, W.: Improving autoaugment via augmentation-wise weight sharing. Advances in Neural Information Processing Systems 33, 1908819098 (2020) 84. Tufail, A.B., Ullah, I., Khan, W.U., Asif, M., Ahmad, I., Ma, Y.K., Khan, R., Kalimullah, Ali, M.S.: Diagnosis of diabetic retinopathy through retinal fundus images and 3d convolutional neural networks with limited number of samples. Wireless Communications and Mobile Computing 2021(1), 6013448 (2021) 85. Uddin, A., Monira, M., Shin, W., Chung, T., Bae, S.H., et al.: Saliencymix: saliency guided data augmentation strategy for better regularization. arXiv preprint arXiv:2006.01791 (2020) 86. Verma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Lopez-Paz, D., Bengio, Y.: Manifold mixup: Better representations by interpolating hidden states. In: International conference on machine learning. pp. 64386447. PMLR (2019) 87. Volpi, R., Morerio, P., Savarese, S., Murino, V.: Adversarial feature augmentation for unsupervised domain adaptation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 54955504 (2018) 88. Wang, J., Saputra, M.R.U., Lu, C.X., Trigoni, N., Markham, A.: Rada: Robust adversarial data augmentation for camera localization in challenging conditions. In: 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). pp. 33353342. IEEE (2023) 89. Wang, K., Fang, B., Qian, J., Yang, S., Zhou, X., Zhou, J.: Perspective transformation data augmentation for object detection. IEEE Access 8, 49354943 (2019) 90. Wu, B., Xie, Y., Zhang, Z., Ge, J., Yaxley, K., Bahadir, S., Wu, Q., Liu, Y., To, M.S.: Bhsd: 3d multi-class brain hemorrhage segmentation dataset. In: International Workshop on Machine Learning in Medical Imaging. pp. 147156. Springer (2023) 91. Wu, B., Xie, Y., Zhang, Z., Phan, M.H., Chen, Q., Chen, L., Wu, Q.: Xlip: Crossmodal attention masked modelling for medical language-image pre-training. arXiv preprint arXiv:2407.19546 (2024) 92. Xie, T., Cheng, X., Wang, X., Liu, M., Deng, J., Zhou, T., Liu, M.: Cut-thumbnail: novel data augmentation for convolutional neural network. In: Proceedings of the 29th ACM International Conference on Multimedia. pp. 16271635 (2021) 93. Xu, Y., Zhang, J., He, R., Ge, L., Yang, C., Yang, C., Wu, Y.N.: Sas: Selfaugmentation strategy for language model pre-training. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 1158611594 (2022) 94. Yang, S., Li, P., Xiong, X., Shen, F., Zhao, J.: Adaaugment: tuningfree and adaptive approach to enhance data augmentation. arXiv preprint arXiv:2405.11467 (2024) 95. Yoo, J., Ahn, N., Sohn, K.A.: Rethinking data augmentation for image superresolution: comprehensive analysis and new strategy. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 83758384 (2020) MediAug: Exploring Visual Augmentation in Medical Imaging 21 96. Yu, X., Tian, Y., Wang, L., Feng, P., Wu, W., Shi, R.: Adaptaug: Adaptive data augmentation framework for multi-agent reinforcement learning. In: 2024 IEEE International Conference on Robotics and Automation (ICRA). pp. 1081410820. IEEE (2024) 97. Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 60236032 (2019) 98. Zhang, B., Allebach, J.P.: Adaptive bilateral filter for sharpness enhancement and noise removal. IEEE transactions on Image Processing 17(5), 664678 (2008) 99. Zhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 (2017) 100. Zhang, J., Zhang, Y., Xu, X.: Objectaug: object-level data augmentation for semantic image segmentation. In: 2021 international joint conference on neural networks (IJCNN). pp. 18. IEEE (2021) 101. Zhang, R., Guo, H., Zhang, Z., Yan, P., Zhao, S.: Gamed-snake: Gradient-aware adaptive momentum evolution deep snake model for multi-organ segmentation. arXiv preprint arXiv:2501.12844 (2025) 102. Zhang, Z., Ahmed, K.A., Hasan, M.R., Gedeon, T., Hossain, M.Z.: deep learning approach to diabetes diagnosis. In: Asian Conference on Intelligent Information and Database Systems. pp. 8799. Springer (2024) 103. Zhang, Z., Qi, X., Chen, M., Li, G., Pham, R., Qassim, A., Berry, E., Liao, Z., Siggs, O., Mclaughlin, R., et al.: Jointvit: Modeling oxygen saturation levels with joint supervision on long-tailed octa. In: Annual Conference on Medical Image Understanding and Analysis. pp. 158172. Springer (2024) 104. Zhang, Z., Qi, X., Zhang, B., Wu, B., Le, H., Jeong, B., Liao, Z., Liu, Y., Verjans, J., To, M.S., et al.: Segreg: Segmenting oars by registering mr images and ct annotations. In: 2024 IEEE International Symposium on Biomedical Imaging (ISBI). pp. 15. IEEE (2024) 105. Zhang, Z., Yi, N., Tan, S., Cai, Y., Yang, Y., Xu, L., Li, Q., Yi, Z., Ergu, D., Zhao, Y.: Meddet: Generative adversarial distillation for efficient cervical disc herniation detection. In: 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). pp. 40244027. IEEE (2024) 106. Zhang, Z., Zhang, B., Hiwase, A., Barras, C., Chen, F., Wu, B., Wells, A.J., Ellis, D.Y., Reddi, B., Burgan, A.W., To, M.S., Reid, I., Hartley, R.: Thin-thick adapter: Segmenting thin scans using thick annotations. OpenReview (2023) 107. Zhao, R., Zhang, Z., Xu, Y., Yao, Y., Huang, Y., Zhang, W., Song, Z., Chen, X., Zhao, Y.: Peddet: Adaptive spectral optimization for multimodal pedestrian detection. arXiv preprint arXiv:2502.14063 (2025) 108. Zhao, Y., Liao, Z., Liu, Y., Nijhuis, K.O., Barvelink, B., Prijs, J., Colaris, J., Wijffels, M., Reijman, M., Zhang, Z., et al.: landmark-based approach for instability prediction in distal radius fractures. In: 2024 IEEE International Symposium on Biomedical Imaging (ISBI). pp. 15. IEEE (2024) 109. Zheng, X., Chalasani, T., Ghosal, K., Lutz, S., Smolic, A.: Stada: Style transfer as data augmentation. arXiv preprint arXiv:1909.01056 (2019) 110. Zhong, L., Li, F.H., Huang, H.Z., Zhang, Y., Lu, S.P., Wang, J.: Aesthetic-guided outward image cropping. ACM Transactions on Graphics (TOG) 40(6), 113 (2021) 111. Zhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data augmentation. In: Proceedings of the AAAI conference on artificial intelligence. vol. 34, pp. 1300113008 (2020) 22 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 112. Zhou, Y., Wang, B., He, X., Cui, S., Shao, L.: Dr-gan: conditional generative adversarial network for fine-grained lesion synthesis on diabetic retinopathy images. IEEE journal of biomedical and health informatics 26(1), 5666 (2020) 113. Zhu, H., Zhang, Z., Pang, G., Wang, X., Wen, S., Bai, Y., Ergu, D., Cai, Y., Zhao, Y.: Doei: Dual optimization of embedding information for attention-enhanced class activation maps. arXiv preprint arXiv:2502.15885 (2025) 114. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: Proceedings of the IEEE International Conference on computer vision. pp. 22232232 (2017) MediAug: Exploring Visual Augmentation in Medical Imaging"
        },
        {
            "title": "A Data Augmentation Methods",
            "content": "1-1-a.Flipping [63,29] is data augmentation technique that generates mirrored version of an image by inverting pixel positions along specified axis, typically horizontal or vertical, as shown in fig. 1. While horizontal flipping is widely used to enhance model robustness to spatial variations, vertical flipping is less common due to the potential semantic inconsistencies between the top and bottom regions of an image. Fig. 1: Flippingis data augmentation technique that generates mirror image by inverting pixel positions across specified axis, typically horizontally, to expand training datasets and improve model generalization. 2 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-1-b.Rotation [29] is data augmentation technique that transforms an image by rotating it around fixed axis within specified range of angles, thereby enhancing model robustness to orientation variations, as shown in fig. 2. While this method is generally effective for tasks like medical imaging, its applicability depends on the rotation degree, as excessive rotation may alter label semantics in certain contexts, such as digit recognition. Fig. 2: Rotation based image augmentation transforms the original image by pivoting it around its centre across an axis within 1-359 degrees while preserving pixel relationships. MediAug: Exploring Visual Augmentation in Medical Imaging 3 1-1-c. Scaling Ratio [29] is data augmentation technique that resizes images along one or more axes using specified scaling factor, which can either be uniform or non-uniform across axes, as shown in fig. 3. This method effectively simulates zoom-in effects when the scaling factor is greater than 1 and zoom-out effects when it is less than 1, enabling models to learn features across varying spatial resolutions. Fig. 3: Scaling Ratio is data augmentation technique that transforms images by applying different scaling factors to the and axes, creating zoom-in, zoomout, or non-uniform distortion effects. 4 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-1-d. Noise injection [82] is data augmentation method that introduces random noise into images to enhance model robustness and generalization by simulating real-world imperfections, as shown in fig. 4. This study proposes novel two-step noise estimation framework, leveraging wavelet and DCT transforms, to accurately infer noise variance and improve the performance of denoising techniques across diverse visual content and noise levels. Fig. 4: Noise injection method combines transform integration and controlled noise addition to accurately estimate variance in natural images, enabling improved denoising performance. MediAug: Exploring Visual Augmentation in Medical Imaging 5 1-1-e. Colour space [11] augmentation involves modifying the colour properties of an image by manipulating its colour channels, such as red, green, and blue (RGB), or by converting between different colour spaces like HSV or grayscale, as shown in fig. 5. This technique enables the generation of diverse image variations while preserving spatial information, though care must be taken with certain transformations, such as grayscale conversion, to avoid potential performance degradation. Fig. 5: Colour space augmentation transforms images by manipulating RGB channels, applying colour shifts, modifying HSV components, isolating individual channels, and converting between different colour spaces while preserving spatial properties. 6 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-1-f. Contrast [65] augmentation is data augmentation technique that adjusts the intensity differences within an image to enhance its visual features and improve model performance under varying lighting conditions, as shown in fig. 6. This study introduces local band-limited contrast definition, considering spatial frequency content, to better understand contrast perception in complex images and its implications for image-processing algorithms. Fig. 6: Contrast is calculated as point-wise measure using frequency decomposition to analyze spatial vision and image processing applications. MediAug: Exploring Visual Augmentation in Medical Imaging 1-1-g. Sharpening [98] is data augmentation method that enhances image sharpness by increasing edge slopes while minimizing noise and avoiding artifacts like overshoot or halo effects, as shown in fig. 7. The adaptive bilateral filter (ABF) achieves this by transforming histograms with adaptive parameters, outperforming traditional methods like unsharp masking in both sharpness and noise reduction. Fig. 7: Sharpening with Adaptive Bilateral Filter increases edge slopes without halos while removing noise, unlike traditional Unsharp Mask techniques. 8 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-1-h. Translation [73,63,29] is data augmentation technique that shifts an image along specified axis using translation vector while preserving the relative positions of its pixels, as shown in fig. 8. This method reduces positional bias by ensuring that models learn spatially invariant features rather than relying on properties tied to specific location within the image. Fig. 8: Translation is data augmentation method that shifts an image along an axis while preserving pixel relationships, helping machine learning models develop spatial invariance and prevent location-specific biases. MediAug: Exploring Visual Augmentation in Medical Imaging 9 1-1-i. Cropping [110] of the image is widely utilized data augmentation technique in computer vision, aimed at refining image composition, enhancing visual aesthetics, and improving the robustness of machine learning models, as shown in fig. 9. Fig. 9: Cropping novel method that extrapolates image content outward to achieve optimal aesthetic composition. 10 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-1-j. Shearing [29] is data augmentation technique that distorts an image by shifting one edge along the horizontal or vertical axis, effectively transforming it into parallelogram, as shown in fig. 10. Controlled by shear angle, this method introduces perspective distortion, allowing models to learn features invariant to geometric transformations. Fig. 10: Shearing transforms an image by sliding one edge along vertical or horizontal axis to create parallelogram effect based on specified angle. MediAug: Exploring Visual Augmentation in Medical Imaging 11 1-1-k. Jitter [8] is data augmentation technique commonly used in machine learning to simulate temporal or spatial misalignments in input data, thereby improving model robustness against variations caused by noise or countermeasures, as shown in fig. 11. In the context of cryptographic implementation analysis, jitter augmentation is particularly effective for addressing misalignment issues in profiling attacks, enabling end-to-end strategies such as Convolutional Neural Networks to achieve high performance without requiring extensive preprocessing or precise feature selection. Fig. 11: Jitter data augmentation: technique that generates synthetic power trace variations with time shifts to train CNN models for robust side-channel attacks against protected cryptographic implementations. 12 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-1-l. Local Augment [59] is data augmentation method designed to enhance local feature descriptors by incorporating contextual information, such as highlevel visual representations and geometric relationships derived from keypoint distributions, as shown in fig. 12. By integrating these cross-modality contexts, this approach improves the robustness and generalization of local features, enabling more effective performance in tasks like geometric matching across diverse scenes. Fig. 12: Local Augment: feature enhancement method that incorporates both visual and geometric contextual information to improve keypoint matching across diverse scenes. MediAug: Exploring Visual Augmentation in Medical Imaging 1-1-m. Kernel Filters [3] such as Gaussian filters play critical role in data augmentation by introducing variations such as blurring and sharpening, thereby enhancing the diversity of training data, as shown in fig. 13. In scale-space filtering, Gaussian kernels are uniquely suited for hierarchical signal analysis due to their ability to maintain consistent extrema behaviour and well-defined zerocrossing contours. Fig. 13: Kernel Filters scale-space transformation method that convolves signals with Gaussian kernels of varying bandwidths to create hierarchical representations through zero-crossing analysis. 14 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-1-n. FixMatch [75] streamlined yet highly effective semi-supervised learning (SSL) algorithm that utilizes high-confidence pseudo-labels derived from weakly-augmented images to guide training on their strongly-augmented counterparts, as shown in fig. 14. Despite its simplicity, FixMatch attains state-ofthe-art performance across various SSL benchmarks, underscoring its efficacy even with extremely limited labelled data. Fig. 14: FixMatch semi-supervised learning method that generates pseudolabels from weakly-augmented unlabeled images and trains the model to predict these labels from strongly-augmented versions of the same images. MediAug: Exploring Visual Augmentation in Medical Imaging 15 1-1-o.Augmentation using intensities [29] involves modifying image properties such as contrast, brightness, blurring, sharpening, or adding noise to enhance model robustness against variations in image quality, as shown in fig. 15. Different types of noise, such as Gaussian, salt-and-pepper, or uniform, are applied by altering pixel values through specific distribution-based sampling methods, simulating diverse imaging conditions. Fig. 15: Augmentation using intensities transforms images by modifying pixel values through brightness/contrast adjustments, blurring, normalization, histogram equalization, sharpening, and the addition of various noise patterns. 16 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-2-a. cutout [23] is simple yet effective regularization technique that involves randomly masking out square regions of input during training, helping to improve the robustness and generalization of convolutional neural networks (CNNs), as shown in fig. 16. By addressing the overfitting issues often faced by CNNs due to their large model capacity, cutout enhances performance and can be seamlessly combined with existing data augmentation methods and regularizers. Fig. 16: cutout data augmentation method randomly masks square regions of input images during training to improve CNN robustness and prevent overfitting. MediAug: Exploring Visual Augmentation in Medical Imaging 17 1-2-b. Hide-and-Seek (HaS) [74] is versatile data augmentation technique that improves visual recognition tasks by randomly hiding patches in training images, encouraging the network to focus on other relevant features when key discriminative regions are obscured, as shown in fig. 17. This method is compatible with any network, requires no modifications during testing, and demonstrates significant improvements in tasks such as object localization, image classification, and semantic segmentation. Fig. 17: Hide-and-Seek (HaS) randomly masks image patches during training to force neural networks to learn diverse features beyond the most discriminative regions. 18 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-2-c. Random Erasing [111] is simple yet effective data augmentation technique that improves the robustness of convolutional neural networks (CNNs) by randomly selecting and erasing rectangular regions in training images with random pixel values, as shown in fig. 18. This method, which is parameterfree and compatible with most CNN-based recognition models, enhances performance across tasks such as image classification, object detection, and person re-identification by reducing overfitting and increasing occlusion robustness. Fig. 18: Random Erasing augments training data by randomly erasing rectangular regions of images with random pixel values to improve CNN robustness to occlusion. MediAug: Exploring Visual Augmentation in Medical Imaging 1-2-d. GridMask [12] is novel data augmentation method that improves performance in various computer vision tasks by systematically removing regions of input images to enhance model robustness, as shown in fig. 19. Compared to existing methods like AutoAugment, GridMask is computationally efficient and achieves superior results on benchmarks such as ImageNet, COCO2017, and Cityscapes. Fig. 19: GridMask systematically removes information in grid pattern to enhance model robustness by forcing neural networks to learn from partially obscured images. 20 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-2-e. FenceMask [52] is novel data augmentation method that simulates object occlusion to balance occlusion and information retention, achieving superior performance in various computer vision tasks, as shown in fig. 20. Extensive experiments on datasets like CIFAR, ImageNet, COCO2017, and VisDrone demonstrate its effectiveness, particularly in fine-grained visual categorization and small object detection. Fig. 20: FenceMask grid-based occlusion data augmentation method that balances information retention with object occlusion for improved performance in computer vision tasks. MediAug: Exploring Visual Augmentation in Medical Imaging 21 1-3-a. Pairing Samples [43] is novel data augmentation technique for image classification that creates new training samples by overlaying two randomly selected images from the dataset, resulting in significant increase in the diversity of training data, as shown in fig. 21. This method has shown substantial improvements in classification accuracy across various datasets and proves particularly effective in tasks with limited training data, such as medical imaging. Fig. 21: Pairing Samples data augmentation method creates new training samples by pixel-wise averaging of randomly selected image pairs, significantly improving classification accuracy across datasets. 22 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-b. Mixup [99] is data augmentation technique that trains neural networks on convex combinations of pairs of examples and their corresponding labels, encouraging linear behavior between training samples, as shown in fig. 22. This method improves generalization, enhances robustness to adversarial examples, reduces label memorization, and stabilizes the training of generative adversarial networks across various datasets and architectures. Fig. 22: Mixup augments training data by linearly interpolating pairs of images and their labels with mixing parameter λ to improve model generalization and robustness. MediAug: Exploring Visual Augmentation in Medical Imaging 23 1-3-c. CutMix [97] is data augmentation technique that enhances model generalization by replacing regions of an image with patches from other images while proportionally mixing their labels, as shown in fig. 23. Unlike traditional regional dropout methods that discard informative pixels, CutMix retains useful information, leading to improved classification, object localization, and robustness across multiple benchmarks. Fig. 23: CutMix augmentation creates new training examples by cutting regions from one image and pasting them onto another while proportionally mixing their class labels. 24 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-d. CropMix [35] is novel data augmentation method designed to generate rich input distribution by cropping an image multiple times at distinct scales, ensuring the capture of multi-scale information, as shown in fig. 24. By mixing these cropped views to form new training data, CropMix enhances performance across diverse vision tasks, including classification, contrastive learning, and masked image modeling, while maintaining computational efficiency. Fig. 24: CropMix augments training data by mixing multiple crops at different scales from the same image to capture richer features and improve model performance. MediAug: Exploring Visual Augmentation in Medical Imaging 1-3-e. YOCO [34] (You Only Cut Once) is novel data augmentation technique that partitions an image into two segments and applies augmentations independently to each segment, as shown in fig. 25. YOCO enhances augmentation diversity, facilitates object recognition from partial information, and achieves significant performance improvements across various neural network architectures and tasks, including CIFAR and ImageNet classification as well as contrastive pre-training. Fig. 25: YOCO This schematic representation illustrates the You Only Cut Once (YOCO) data augmentation technique, which segments input images for independent transformations, thereby enhancing sample diversity and enabling neural networks to recognize objects from partial information with demonstrated performance improvements across multiple architectures. 26 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-f. FMix [36] is novel Mixed Sample Data Augmentation (MSDA) technique that employs random binary masks, generated from low-frequency images in Fourier space, to create augmented samples with diverse shapes, as shown in fig. 26. Building on the strengths of CutMix, FMix improves generalization and prevents memorization without distorting the data distribution, achieving superior performance across various datasets and tasks, including state-of-the-art results on CIFAR-10. Fig. 26: FMix generates augmented training samples by combining image pairs using organically-shaped binary masks derived from low-frequency Fourier space sampling, offering advantages over conventional MixUp and CutMix approaches. MediAug: Exploring Visual Augmentation in Medical Imaging 27 1-3-g. AugMix [38] is data augmentation technique developed to improve the robustness and uncertainty estimates of image classifiers by addressing the challenges posed by unforeseen data distribution changes, as shown in fig. 27. This method is computationally efficient, straightforward to implement and has demonstrated substantial improvements in robustness and uncertainty measures on challenging image classification benchmarks, significantly narrowing the gap to optimal performance in certain scenarios. Fig. 27: AugMix enhances model robustness through combination of diverse augmentation chains, stochastic operations, and consistency regularization via Jensen-Shannon divergence loss between original and augmented predictions. 28 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-h. ManifoldMix [86] is data augmentation method that improves the generalization and robustness of deep neural networks by performing interpolations in hidden representations of the network, as shown in fig. 28. This technique encourages smoother decision boundaries and flatter class representations, leading to improved performance on supervised learning tasks, robustness against adversarial attacks, and better test log-likelihood, all with minimal computational overhead. Fig. 28: ManifoldMix to create smoother decision boundaries and improve model robustness against adversarial examples. interpolates hidden layer representations MediAug: Exploring Visual Augmentation in Medical Imaging 1-3-i. Self-Augmentation [72] is novel approach to learning in few shots that combines self-mix, regional dropout technique that replaces image patches with values of the same image, and self-distillation to enhance generalization by preventing overfitting to dataset-specific structures, as shown in fig. 29. By incorporating auxiliary branches for knowledge sharing and local representation learner to generate synthetic queries and novel class weights, this method achieves state-of-the-art performance on standard few-shot learning benchmarks. Fig. 29: Self-Augmentation combines regional dropout (Self-Mix) with knowledge sharing across network branches (Self-Distillation) to improve few-shot learning generalization. 30 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-j. SalfMix [15] is novel data augmentation method that generates selfmixed images using saliency maps, focusing on improving generalizability by applying mixing strategies to single image, as shown in fig. 30. Additionally, the combination of SalfMix with state-of-the-art two-image augmentation methods, termed HybridMix, achieves superior performance on multiple classification and object detection benchmarks, demonstrating its effectiveness and versatility. Fig. 30: SalfMix The SalfMix technique uses saliency maps to create augmented training data by intelligently combining original and transformed image regions. MediAug: Exploring Visual Augmentation in Medical Imaging 31 1-3-k. Cut-Thumbnail [92] novel data augmentation strategy that embeds reduced version of an image (thumbnail) into random region of the original image, improving both local and global information, as shown in fig. 31. This method achieves significant performance gains across various tasks, with ResNet50 on ImageNet improving by over 2.8%, reaching 79.21% top-1 accuracy. Fig. 31: Cut-Thumbnail data augmentation Method diagram showing how images are processed into reduced thumbnails and reinserted to enhance shape bias in machine learning models. 32 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-l. SaliencyMix [85] novel data augmentation strategy that leverages saliency maps to select representative image patches, which are then mixed with target images to guide the model toward learning more meaningful feature representations, as shown in fig. 32. SaliencyMix achieves state-of-the-art performance on ImageNet classification, reducing top-1 error to 21.26% and 20.09% for ResNet-50 and ResNet-101, respectively, while also enhancing model robustness and object detection performance. Fig. 32: SaliencyMix augmentation identifies salient regions in source image and transfers them to target image. The resulting mixed image is paired with weighted combination of both labels, proportional to the area of the transferred patch. MediAug: Exploring Visual Augmentation in Medical Imaging 33 1-3-m. Puzzle Mix [46] novel mixup-based data augmentation method that explicitly incorporates saliency information and underlying data statistics to generate more informative virtual examples, as shown in fig. 33. By optimizing multi-label objective and saliency-discounted optimal transport objective, Puzzle Mix achieves state-of-the-art generalization and adversarial robustness on CIFAR-100, Tiny-ImageNet, and ImageNet datasets. Fig. 33: Puzzle Mix intelligently combines images by preserving salient regions through alternating optimization of mixing masks and optimal transport to enhance model generalization and adversarial robustness. 34 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-n. SnapMix [42] semantically proportional data mixing augmentation method that leverages class activation maps (CAM) to reduce label noise in fine-grained recognition by ensuring semantic correspondence between mixed images and target labels, as shown in fig. 34. SnapMix consistently outperforms existing mix-based approaches across various datasets and achieves state-of-theart performance in fine-grained recognition tasks. Fig. 34: SnapMix data augmentation technique that uses Class Activation Maps to create semantically proportional mixed images with corresponding mixed labels for improved fine-grained recognition. MediAug: Exploring Visual Augmentation in Medical Imaging 1-3-o. MixMo [71] generalized framework for training multi-input multioutput deep subnetworks by replacing the suboptimal summing operation in prior approaches with more effective binary mixing mechanism inspired by mixed sample data augmentations, as shown in fig. 35. MixMo enhances subnetwork diversity and performance, achieving state-of-the-art results on CIFAR-100 and Tiny ImageNet, while outperforming data-augmented deep ensembles without additional inference or memory overhead. Fig. 35: MixMo framework for neural networks, showing how it improves on previous approaches by using feature mixing (CutMix) between multiple inputs to create stronger, more diverse subnetworks with better performance. 36 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-p. StyleMix [41] the first mixup-based augmentation method that separately manipulates the content and style features of input image pairs to create more diverse and robust training samples, as shown in fig. 36. By dynamically adjusting the degree of style mixing based on class distance, StyleMix enhances model generalization and achieves competitive performance on CIFAR100, CIFAR-10, and ImageNet, while improving robustness against adversarial attacks. Fig. 36: StyleMix data augmentation method separately extracts and recombines content and style features from image pairs based on class distance to create robust training samples. MediAug: Exploring Visual Augmentation in Medical Imaging 37 1-3-q. RandomMix [61] novel family of local transformations based on Gaussian random fields that significantly expand the augmentation space for self-supervised representation learning by allowing transformation parameters to vary across pixels, as shown in fig. 37. RandomMix achieves notable improvements in downstream classification tasks, including 1.7% top-1 accuracy gain on ImageNet and 3.6% improvement on out-of-distribution iNaturalist, highlighting its effectiveness in enhancing representation learning. Fig. 37: RandomMix generates diverse image augmentations by applying pixelspecific transformations via Gaussian random fields that generalize traditional affine and colour transformations. 38 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-r. MixMatch [6] is novel semi-supervised learning algorithm that unifies dominant approaches by generating low-entropy pseudo-labels for augmented unlabeled data and blending labelled and unlabeled examples using MixUp, achieving state-of-the-art results across various datasets, as shown in fig. 38. For instance, it reduces error rates on benchmarks like CIFAR-10 (from 38% to 11% with 250 labels) and STL-10, demonstrating its effectiveness in leveraging unlabeled data. Additionally, MixMatch provides dramatically improved accuracy-privacy trade-off for differential privacy, and an ablation study highlights the key components contributing to its success. Fig. 38: MixMatch unified semi-supervised learning approach that combines data augmentation, low-entropy pseudo-labelling, and MixUp to achieve stateof-the-art results with limited labelled data. MediAug: Exploring Visual Augmentation in Medical Imaging 39 1-3-s. ReMixMatch [5] improves upon the MixMatch semi-supervised learning algorithm by introducing distribution alignment, which aligns the marginal distribution of predictions with ground truth labels, and augmentation anchoring, which enforces consistency across strongly and weakly augmented versions of the same input, as shown in fig. 39. These innovations, combined with learned augmentation policy, make ReMixMatch significantly more data-efficient, achieving 93.73% accuracy on CIFAR-10 with only 250 labelled examples, compared to MixMatchs performance requiring 4,000 examples. Fig. 39: ReMixMatch an advanced semi-supervised learning approach that improves upon MixMatch by incorporating distribution alignment, augmentation anchoring, and learned strong augmentations to achieve superior performance with significantly less labelled data. 40 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-t. Copy-Paste [28] is simple yet effective data augmentation technique for instance segmentation, where objects are randomly pasted onto images, providing substantial performance gains without relying on complex visual context modelling, as shown in fig. 40. This approach not only improves the latest results in COCO (e.g. +0.6 mask AP and +1.5 box AP) but also demonstrates significant benefits in the LVIS benchmark, outperforming the LVIS 2020 Challenge winner by +3.6 mask AP on rare categories. Fig. 40: Copy-Paste simple yet effective data augmentation technique that randomly places segmented objects onto images, significantly improving instance segmentation performance, especially for rare categories. MediAug: Exploring Visual Augmentation in Medical Imaging 1-3-u. Mixed-Example [76] data augmentation generalizes previous methods that combine pairs of examples through linear operations, exploring broader space of augmentation techniques that challenge the necessity of linearity and improve upon prior state-of-the-art, as shown in fig. 41. This generalized approach not only enhances performance but also uncovers novel augmentation strategies, highlighting the limitations of existing theories and suggesting the need for more comprehensive understanding of why such methods are effective. Fig. 41: Mixed-Example data augmentation extends beyond traditional labelpreserving transformations and linear combinations to explore broader space of non-linear example mixing techniques for improved neural network training. 42 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-v. RICAP [79] (Random Image Cropping and Patching) is novel data augmentation technique that enhances training diversity by randomly cropping four images, patching them together, and mixing their class labels, thereby mitigating overfitting and improving model generalization, as shown in fig. 42. Evaluated with state-of-the-art CNNs, RICAP outperforms competitive methods like cutout and mixup, achieving test error of 2.23% on CIFAR-10 and demonstrating superior results on CIFAR-100 and ImageNet. Fig. 42: RICAP combines random crops from four different images into single composite training sample while mixing their class labels to improve CNN generalization performance. MediAug: Exploring Visual Augmentation in Medical Imaging 43 1-3-w. CutBlur [95] is novel data augmentation technique designed for low-level vision tasks, such as super-resolution, which enhances model learning by cutting and pasting low-resolution patches into high-resolution images and vice versa, as shown in fig. 43. This approach enables models to learn not only \"how\" but also \"where\" and \"how much\" to apply super-resolution, leading to consistent performance improvements across various tasks, including denoising and compression artifact removal. Fig. 43: CutBlur augments super-resolution training by strategically exchanging patches between low and high-resolution image pairs to teach models where and how much enhancement to apply. 44 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-x. ResizeMix [69] is simple yet effective data augmentation technique that improves upon cut-based methods like CutMix by resizing source image into smaller patch and pasting it onto another image, preserving more substantial object information, as shown in fig. 44. This approach addresses issues like label misallocation and missing object information, demonstrating superior performance in image classification and object detection tasks while outperforming more complex augmentation methods without additional computational cost. Fig. 44: ResizeMix augments data by resizing source image and pasting it onto target image while preserving complete object information and proportionally mixing their labels. MediAug: Exploring Visual Augmentation in Medical Imaging 45 1-3-y. Classmix [64] is novel data augmentation mechanism for semi-supervised semantic segmentation that generates augmentations by mixing unlabeled samples while leveraging network predictions to preserve object boundaries, as shown in fig. 45. Evaluations on standard benchmarks demonstrate its state-of-the-art performance, supported by extensive ablation studies analyzing various design choices and training regimes. Fig. 45: Classmix creates data-augmented images by using network predictions to generate masks that preserve object boundaries when mixing unlabeled images for semi-supervised semantic segmentation. 46 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 1-3-z. CDA [33] (Contrastive Data Augmentation) is specialized data augmentation strategy designed to enhance contrastive learning by generating semantically consistent positive pairs and diverse negative pairs, as shown in fig. 46. By leveraging techniques such as random cropping, color jittering, and strong augmentations, CDA aims to improve the quality of feature representations and boost performance in tasks like knowledge distillation and self-supervised learning. Fig. 46: CDA enhances knowledge distillation by using intermediate layer representations and contrastive learning to generate high-quality augmented training samples. MediAug: Exploring Visual Augmentation in Medical Imaging 47 1-3-aa. ObjectAug [100] an object-level data augmentation method for semantic segmentation that decouples images into objects and backgrounds, applies augmentations to each object, and restores artifacts via inpainting, as shown in fig. 47. This approach enhances boundary diversity and supports category-aware augmentation, improving segmentation performance. Fig. 47: ObjectAug decouples objects from the background, individually augments them, and reassembles the image to improve semantic segmentation performance. 48 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-1-a. AutoAugment [20] that automatically search for improved data augmentation policies. It consists of two parts: search algorithm and search space. The search algorithm is designed to find the best policy with the highest validation accuracy, as shown in fig. 48. The search space contains many policies that detail various augmentation operations and magnitudes with which the operations are applied. Fig. 48: AutoAugment automatically searches for optimal data augmentation policies by selecting sub-policies with paired operations, each having learned probability and magnitude parameters. MediAug: Exploring Visual Augmentation in Medical Imaging 49 2-1-b. Fast AutoAugment [55] is novel data augmentation algorithm that significantly reduces the search time for effective augmentation policies by employing more efficient density-matching strategy, as shown in fig. 49. Compared to AutoAugment, it achieves comparable performance on diverse image recognition tasks while reducing computational requirements by orders of magnitude. Fig. 49: Fast AutoAugment accelerates data augmentation policy search through efficient density matching between hold-out and augmented data distributions, reducing computational requirements while maintaining performance. Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-1-c. Population-Based Augmentation(PBA) [40] that reduce the time cost of AutoAugment which generates nonstationary augmentation policy schedules instead of fixed augmentation policy, as shown in fig. 50. PBA can match the performance of AutoAugment on multiple datasets with less computation time. Fig. 50: Population-Based Augmentation(PBA) evolves nonstationary policy schedules throughout training using evolutionary algorithms, achieving state-ofthe-art performance with orders of magnitude less computation than previous methods. MediAug: Exploring Visual Augmentation in Medical Imaging 51 2-1-d. RandAugment [21] is streamlined data augmentation method that eliminates the need for separate search phase by significantly reducing the search space, enabling direct application to target tasks, as shown in fig. 51. Its parameterized design allows for adjustable regularization strength across different model and dataset sizes, achieving state-of-the-art performance on benchmarks like CIFAR-10/100, SVHN, ImageNet, and COCO while simplifying implementation and reducing the computational cost. Fig. 51: RandAugment simplifies data augmentation by randomly applying transformations with two tunable parameters, (number of transformations) and (magnitude), across different tasks and model sizes. 52 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-1-e. KeepAugment [30] is novel data augmentation method designed to preserve the fidelity of augmented images by leveraging saliency maps to detect and retain important regions during augmentation, as shown in fig. 52. By generating more faithful training examples, KeepAugment improves upon existing augmentation techniques such as AutoAugment and Cutout, achieving superior performance across tasks like image classification, semi-supervised learning, object detection, and multi-camera tracking. Fig. 52: KeepAugment preserves important image regions during data augmentation by using saliency maps to generate more faithful and informative training examples. MediAug: Exploring Visual Augmentation in Medical Imaging 53 2-1-f. OHL-Auto-Aug [56] (Online Hyper-parameter Learning for Auto-Augmentation) introduces an efficient approach to learning augmentation policy distributions jointly with network training, eliminating the need for separate offline search phase, as shown in fig. 53. This method significantly reduces computational costs, achieving up to 60x and 24x faster search on CIFAR-10 and ImageNet, respectively, while delivering competitive accuracy improvements over baseline models. Fig. 53: OHL-Auto-Aug enables online, joint optimization of augmentation policies with network training, dramatically reducing computational search costs while maintaining model performance. 54 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-1-g. Augmentation-wise Weight Sharing [83] (AWS) introduces an efficient proxy task for evaluating augmentation policies by sharing model weights across different augmentations, significantly improving the speed and reliability of the search process, as shown in fig. 54. This method achieves state-of-the-art results on benchmarks like CIFAR-10 and ImageNet, demonstrating both superior accuracy and efficiency compared to existing auto-augmentation approaches. Fig. 54: Augmentation-wise Weight Sharing method enables efficient augmentation policy search by using shared neural network to evaluate multiple data augmentation policies simultaneously, dramatically reducing search time while maintaining evaluation reliability. MediAug: Exploring Visual Augmentation in Medical Imaging 55 2-1-h. RAD(Reinforcement Learning with Augmented Data) [49] is versatile module that enhances RL algorithms by incorporating general data augmentations, improving both data efficiency and generalization to new environments, as shown in fig. 55. RAD achieves state-of-the-art performance on benchmarks like the DeepMind Control Suite and OpenAI Gym, while also demonstrating superior test-time generalization on OpenAI ProcGen tasks. Fig. 55: RAD(Reinforcement Learning with Augmented Data) enhances RL algorithms by applying diverse data augmentations to visual observations, improving data efficiency and generalization without modifying the core algorithm. 56 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-1-i. MARL(Multi-Agent Reinforcement Learning) [96] is powerful framework for controlling multi-robot systems but suffers from low sample efficiency, limiting its practical applications, as shown in fig. 56. To address this, adaptive methods like AdaptAUG have been proposed to optimize data augmentation strategies, significantly improving sample efficiency and performance in both simulated and real-world multi-robot tasks. Fig. 56: MARL(Multi-Agent Reinforcement Learning) selectively identifies beneficial data augmentations for multi-agent reinforcement learning, enhancing sample efficiency and training stability in multi-robot systems through adaptive augmentation selection guided by theoretical insights. MediAug: Exploring Visual Augmentation in Medical Imaging 2-1-j. Scale-Aware Automatic Augmentation [13] introduces novel scaleaware search space and Pareto Scale Balance metric to efficiently learn augmentation policies tailored for object detection, ensuring scale invariance at both image and box levels, as shown in fig. 57. This approach delivers significant performance improvements across various object detectors and tasks like instance segmentation and keypoint estimation while maintaining lower search costs compared to previous methods. Fig. 57: Scale-Aware Automatic Augmentation addresses object detections scale variation challenge by automatically discovering optimal augmentation policies through specialized search space and Pareto Scale Balance metric that maintains scale invariance across both image and box-level transformations. 58 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-1-k. ADA(Adaptive Data Augmentation) [94] introduces reinforcement learning-based framework to dynamically adjust augmentation magnitudes for individual training samples, addressing the misalignment between static augmentations and the evolving training status of models, as shown in fig. 58. By leveraging dual-model architecture, AdaAugment achieves superior generalization performance across various benchmarks, consistently outperforming existing data augmentation methods in both effectiveness and efficiency. Fig. 58: ADA(Adaptive Data Augmentation) employs dual-model reinforcement learning architecture that dynamically adjusts data augmentation magnitudes based on real-time feedback from the target network, eliminating manual tuning and improving model generalization. MediAug: Exploring Visual Augmentation in Medical Imaging 59 2-1-l. RADA(Robust Adversarial Data Augmentation) [88] introduces targeted approach to data augmentation by perturbing the most vulnerable pixels, enhancing robustness in camera localization under challenging conditions, as shown in fig. 59. This method significantly outperforms existing techniques, achieving up to double the accuracy of state-of-the-art models, even in unseen adverse weather scenarios. Fig. 59: RADA(Robust Adversarial Data Augmentation) is robust adversarial data augmentation method that improves camera localization by identifying and perturbing vulnerable pixels rather than applying general image transformations. 60 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-1-m. PTDA(Perspective Transformation Data Augmentation) [89] introduces novel framework that generates annotated data by simulating images from various camera viewpoints, effectively expanding limited datasets without additional manual labelling, as shown in fig. 60. This approach enhances the performance of deep CNNs, particularly on small or imbalanced datasets, as demonstrated by extensive experiments across multiple benchmarks. Fig. 60: PTDA(Perspective Transformation Data Augmentation) is data augmentation framework that generates new training images by applying perspective transformations to simulate different camera viewpoints without requiring additional manual annotations. MediAug: Exploring Visual Augmentation in Medical Imaging 61 2-1-n. DADA(Differentiable Automatic Data Augmentation) [53] introduces novel approach to data augmentation policy optimization by formulating it as differentiable problem using Gumbel-Softmax and an unbiased gradient estimator, RELAX, as shown in fig. 61. This method significantly accelerates the search process, achieving state-of-the-art efficiency and comparable accuracy across multiple datasets, while also demonstrating its value for pre-training in downstream tasks. Fig. 61: DADA(Differentiable Automatic Data Augmentation) is differentiable data augmentation framework that dramatically accelerates automatic policy search by using Gumbel-Softmax relaxation and the RELAX gradient estimator for one-pass optimization. 62 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-2-a. FeatMatch [48] is novel learned feature-based refinement and augmentation method to produce varied set of complex transformations, as shown in fig. 62. It also can utilize information from both within-class and across-class prototypical representations. Fig. 62: FeatMatch is semi-supervised learning approach that enhances data augmentation by performing complex transformations in feature space using within-class and across-class prototypical representations. MediAug: Exploring Visual Augmentation in Medical Imaging 2-2-b. Moment Exchange [50] is an implicit data augmentation method that leverages the moments (mean and standard deviation) of latent features, traditionally discarded in recognition models, to enhance training by exchanging and interpolating them between training samples, as shown in fig. 63. This approach improves generalization performance across various recognition benchmarks and can be seamlessly combined with existing augmentation techniques for further gains. Fig. 63: Moment Exchange is novel data augmentation technique that improves recognition models by swapping feature statistics (mean and standard deviation) between images while interpolating their labels. 64 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-2-c. Dataset Augmentation in Feature Space [22] introduces domainagnostic approach to augment training data by applying simple transformations, such as noise addition and interpolation, directly in learned feature space rather than the input space, as shown in fig. 64. This method is effective for both static and sequential data, leveraging unsupervised representation learning to enhance generalization across diverse tasks. Fig. 64: Dataset Augmentation in Feature Space is domain-agnostic approach that performs simple transformations like noise addition, interpolation, and extrapolation in learned feature representations rather than input data. MediAug: Exploring Visual Augmentation in Medical Imaging 65 2-2-d. Feature Space Augmentation for Long-Tailed Data [18] addresses class imbalance by generating novel samples for under-represented classes in the feature space, leveraging class-generic and class-specific components extracted via class activation maps, as shown in fig. 65. This approach achieves state-of-the-art performance across multiple long-tailed datasets, effectively enhancing representation for minority classes. Fig. 65: Feature Space Augmentation for Long-Tailed Data decomposes features into class-generic and class-specific components, then generates synthetic samples by fusing class-generic features from data-rich classes with class-specific features from under-represented classes. 66 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-2-e.textit Adversarial Feature Augmentation for Unsupervised Domain Adaptation [87] introduces novel approach that utilizes GAN-based framework to perform feature augmentation, enhancing representations for under-represented classes while enforcing domain invariance, as shown in fig. 66. This method achieves superior or comparable results to state-of-the-art techniques across various unsupervised domain adaptation benchmarks. Fig. 66: Adversarial Feature Augmentation for Unsupervised Domain Adaptation achieves unsupervised domain adaptation by combining domain-invariant feature extractor with GAN-based feature space augmentation to bridge the gap between labelled source and unlabeled target domains. MediAug: Exploring Visual Augmentation in Medical Imaging 67 2-2-f. LDAS(Latent Data Augmentation Strategy) [51] introduces novel approach for generating additional training data in structural health monitoring (SHM) by leveraging conditional variational autoencoder (CVAE) to model and augment the statistical distributions of power cepstral coefficients under various damage conditions, as shown in fig. 67. This method enhances the performance and robustness of damage classification tasks, as demonstrated through numerical simulations and experimental validations. Fig. 67: LDAS(Latent Data Augmentation Strategy) uses conditional variational autoencoder to generate synthetic power cepstral coefficients representing various structural damage conditions, enabling robust damage classification despite limited real-world training data. 68 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-2-g. STaDA(Style Transfer as Data Augmentation) [109] leverages neural style transfer techniques to enrich training datasets by applying artistic styles to images while preserving their semantic content, enhancing variation for image classification tasks, as shown in fig. 68. Experimental results on Caltech 101 and Caltech 256 demonstrate that STaDA, combined with traditional augmentation methods, improves classification accuracy and reduces reliance on large labelled datasets. Fig. 68: STaDA(Style Transfer as Data Augmentation) improves image classification accuracy by applying neural style transfer techniques to expand training datasets while preserving semantic content. MediAug: Exploring Visual Augmentation in Medical Imaging 69 2-2-h. NSTDA(Non-Statistical Targeted Data Augmentation) [62] addresses the challenge of insufficient training data by employing advanced augmentation techniques that enhance the volume, quality, and diversity of datasets in task-specific manner, as shown in fig. 69. This approach integrates modern methods such as neural rendering, 3D graphics modelling, and generative adversarial networks, demonstrating improved performance across various computer vision tasks and datasets. Fig. 69: NSTDA(Non-Statistical Targeted Data Augmentation) improves machine learning models by employing targeted data augmentation and synthesis techniques to overcome limited training data challenges in computer vision applications. 70 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-2-i. SAS(Self-Augmentation Strategy) [93] is novel approach for pretraining language models that utilizes single network to perform both regular pre-training and contextualized data augmentation, eliminating the need for separate generator and reducing computational complexity, as shown in fig. 70. By jointly optimizing Masked Language Modeling (MLM) and Replaced Token Detection (RTD) tasks, SAS simplifies the training paradigm, addresses the challenge of balancing generator-discriminator capacities, and achieves state-ofthe-art performance on GLUE benchmarks with comparable or reduced computational costs. Fig. 70: SAS(Self-Augmentation Strategy) improves language model pre-training by using single network that evolves from MLM-only to joint MLM-RTD training, eliminating the separate generator network needed in previous approaches like ELECTRA. MediAug: Exploring Visual Augmentation in Medical Imaging 71 2-3-a. Generative adversarial networks(GANs) [31] introduce novel framework for estimating generative models by simultaneously training generative model G, which captures the data distribution, and discriminative model D, which distinguishes between real and generated samples, as shown in fig. 71. This adversarial training process, formulated as minimax two-player game, enables the generative model to approximate the training data distribution and the discriminative model to reach an optimal state, with the entire system trainable via backpropagation without requiring Markov chains or approximate inference networks. Fig. 71: Generative adversarial networks(GANs) are machine learning framework where two neural networksa generator and discriminatorcompete to create increasingly realistic synthetic data. Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-3-b. Pix2Pix [45] introduces conditional adversarial networks as generalpurpose framework for image-to-image translation, enabling the model to simultaneously learn both the mapping from input to output images and the loss function required to train this mapping, as shown in fig. 72. This approach demonstrates versatility across diverse tasks, such as photo synthesis from label maps, object reconstruction from edge maps, and image colourization, eliminating the need for task-specific loss function engineering. Fig. 72: Pix2Pix is conditional adversarial network framework for image-toimage translation that simultaneously learns both the mapping function and loss function, enabling diverse applications from edge maps to photos, label maps to scenes, and black/white to colourized images. MediAug: Exploring Visual Augmentation in Medical Imaging 73 2-3-c. CycleGAN [114] is framework for image-to-image translation that tackles the challenge of learning mappings between source domain and target domain without paired training data, as shown in fig. 73. It combines adversarial loss to align image distributions with cycle consistency loss to ensure that translating an image to the target domain and back to the source domain reconstructs the original image. This approach achieves impressive results in tasks such as style transfer, object transfiguration, and photo enhancement, even in the absence of paired datasets. Fig. 73: CycleGAN enables unpaired image-to-image translation through dual generators and cycle consistency, allowing applications like style transfer without requiring matched image pairs. 74 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-3-d. StarGAN [16] is scalable framework for image-to-image translation that addresses the limitations of existing methods, which require separate models for each pair of image domains, as shown in fig. 74. By utilizing unified model architecture, StarGAN enables simultaneous training across multiple datasets and domains, achieving high-quality translations and flexible mappings to any desired target domain, as demonstrated in tasks like facial attribute transfer and expression synthesis. Fig. 74: StarGAN achieves multi-domain image translation with single unified model through domain labels and cycle consistency, enabling flexible facial attribute manipulation using one generator-discriminator network. MediAug: Exploring Visual Augmentation in Medical Imaging 75 2-3-e. StarGAN v2 [17] is unified framework for image-to-image translation that addresses both the diversity of generated images and scalability across multiple domains, overcoming the limitations of existing methods, as shown in fig. 75. Through experiments on CelebA-HQ and the newly introduced AFHQ dataset, StarGAN v2 demonstrates superior performance in visual quality, diversity, and scalability, setting new benchmark for multi-domain image translation. Fig. 75: StarGAN v2 advances image-to-image translation by combining stylebased generator with unified multi-domain framework to produce diverse, highquality outputs across domains using single model. 76 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-3-f. GAN based [29] augmentation leverages Generative Adversarial Networks (GANs) to create realistic synthetic images by training generator and discriminator in tandem, enabling diverse and high-quality data augmentation, as shown in fig. 76. Advanced GAN variants, such as Conditional GAN, Cycle GAN, and Style GAN, enhance image generation by incorporating class labels, domain translation, and fine-grained detail synthesis, addressing challenges such as quality and training stability. Fig. 76: GAN based augmentation uses generative adversarial networks to create synthetic training images through various specialized architectures like conditional, deep convolutional, cycle, and Wasserstein GANs that address quality and stability challenges. MediAug: Exploring Visual Augmentation in Medical Imaging 77 2-3-g. Channel-wise gamma correction [77] is data augmentation technique that applies random gamma correction independently to each colour channel of fundus image, enhancing variability in colour intensity, as shown in fig. 77. This method helps models learn more robust and invariant features, improving performance against global disturbances in retinal vessel segmentation tasks. Fig. 77: Channel-wise gamma correction is independent random gamma transformations applied to each RGB channel to improve retinal vessel segmentation robustness. Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-3-h. Zooming and CLAHE [1] augmentation are effective techniques for enhancing retinal fundus image analysis in diabetic retinopathy classification, as shown in fig. 78. While random zooming introduces variability by simulating different focal levels, CLAHE enhances image contrast, together improving model accuracy, sensitivity, and specificity in deep learning applications. Fig. 78: Zooming and CLAHE is data augmentation approach combining random zoom transformations with contrast enhancement to achieve 98% accuracy in diabetic retinopathy classification. MediAug: Exploring Visual Augmentation in Medical Imaging 79 2-3-i. Blurring and shifting augmentation are data enhancement techniques used to improve the robustness of deep learning models in diabetic retinopathy classification, as shown in fig. 79. Random weak Gaussian blurring reduces image details to simulate real-world noise, while random shifting alters spatial positioning, collectively aiding in training models to handle spatial and visual variability.[84] Fig. 79: Blurring and shifting data augmentation techniques for diabetic retinopathy classification showing differential performance in binary versus multiclass 3D-CNN models. 80 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-3-j. Heuristic augmentation with NV-like structures [2] is data augmentation technique designed to address the scarcity of proliferative diabetic retinopathy (PDR) cases in retinal image datasets, as shown in fig. 80. By synthesizing neovessel-like structures based on their common locations and shapes, this method enhances dataset variability and improves the ability of deep-learning models to detect PDR-related features. Fig. 80: Heuristic augmentation with NV-like structures is data augmentation approach that synthetically generates neovascularization patterns on retinal images to address the underrepresentation of PDR cases in training datasets. MediAug: Exploring Visual Augmentation in Medical Imaging 81 2-3-k. Deep convolutional GAN [4] augmentation leverages generative adversarial networks to synthesize diverse and realistic images for underrepresented classes, such as proliferative diabetic retinopathy, in imbalanced datasets, as shown in fig. 81. This approach enhances data availability and improves classification performance by providing high-quality synthetic samples without affecting other class distributions. Fig. 81: Deep convolutional GAN is generative adversarial network architecture that synthesizes diverse proliferative diabetic retinopathy images to address class imbalance in training datasets, enabling improved classification performance. 82 Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-3-l. Conditional GAN [112] augmentation utilizes generative adversarial networks to synthesize high-resolution fundus images conditioned on grading severity and lesion information, enabling targeted data generation for diabetic retinopathy classification, as shown in fig. 82. By incorporating structural masks and adaptive grading vectors, this approach enhances data diversity and improves model performance on grading and lesion segmentation tasks. Fig. 82: Conditional GAN is generative model that synthesizes high-resolution diabetic retinopathy images with controllable severity grades by incorporating structural masks, lesion patterns, and grading vectors as input conditions. MediAug: Exploring Visual Augmentation in Medical Imaging 83 2-3-m. Style based GAN [54] augmentation leverages generative adversarial networks to synthesize high-quality, class-specific images by learning and transferring fine-grained style features from real images, as shown in fig. 83. This approach addresses data scarcity in medical imaging, particularly for rare disease conditions, and enhances classification performance in diabetic retinopathy detection tasks. Fig. 83: Style based GAN is generative approach that leverages style modulation and dynamic input sampling to create realistic diabetic retinopathy images, addressing class imbalance by synthesizing rare disease cases for improved classifier training. Xuyin Qi, Zeyu Zhang, and Canxuan Gang et al. 2-3-n.StyPath (Style Transfer Pathway) [19] is histological data augmentation technique designed to address variability in tissue stain quality, common challenge in kidney transplant pathology for Antibody Mediated Rejection (AMR) classification, as shown in fig. 84. By leveraging lightweight styletransfer algorithm, StyPath reduces sample-specific bias, improves classification performance, enhances model generalization, and demonstrates faster processing compared to other augmentation methods, as validated through Bayesian performance estimates and expert qualitative analysis. Fig. 84: StyPath enhances AMR classification in kidney transplant histology through lightweight style transfer augmentation that reduces stain variation bias, improving model accuracy and generalization by simulating diverse staining conditions. MediAug: Exploring Visual Augmentation in Medical Imaging 85 2-3-o. Deep CNN Ensemble [32] is variant of the R-CNN model that achieves state-of-the-art performance in object detection by combining complementary deep CNN architectures into an ensemble, as shown in fig. 85. By augmenting the PASCAL VOC training set with curated subset of Microsoft COCO images, the method significantly improves training data diversity and achieves superior results on the PASCAL VOC 2012 detection task. Fig. 85: Deep CNN Ensemble enhances object detection performance by combining multiple complementary CNN architectures with an augmented training set that integrates selected Microsoft COCO images with PASCAL VOC data."
        }
    ],
    "affiliations": [
        "AIML",
        "ANU",
        "La Trobe",
        "PSU",
        "UCAS",
        "UNSW"
    ]
}