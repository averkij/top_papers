{
    "paper_title": "Hierarchical Dataset Selection for High-Quality Data Sharing",
    "authors": [
        "Xiaona Zhou",
        "Yingyan Zeng",
        "Ran Jin",
        "Ismini Lourentzou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows."
        },
        {
            "title": "Start",
            "content": "Hierarchical Dataset Selection for High-Quality Data Sharing Xiaona Zhou1, Yingyan Zeng2, Ran Jin3, Ismini Lourentzou1, 1University of Illinois Urbana-Champaign 2University of Cincinnati 3Virginia Polytechnic Institute and State University xiaonaz2@illinois.edu, zengyy@ucmail.uc.edu, jran5@vt.edu, lourent2@illinois.edu 5 2 0 2 1 1 ] . [ 1 2 5 9 0 1 . 2 1 5 2 : r Abstract The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training, are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), dataset selection method that models utility at both dataset and group levels (e.g., collections, institutions), enabling efficient generalization from limited observations. Across two public benchmarks (DIGIT-FIVE and DOMAINNET), DaSH outperforms state-ofthe-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows. (cid:128) https://plan-lab.github.io/projects/dash"
        },
        {
            "title": "Introduction",
            "content": "Deep learning models have achieved impressive performance across wide range of supervised learning tasks, largely due to their ability to leverage large, high-quality datasets (Alzubaidi et al. 2023; Sun et al. 2017; Mohammed et al. 2025). In many real-world scenarios, however, available data is distributed across multiple heterogeneous sources, such as publicly available dataset repositories or collaborating institutions, with varying degrees of relevance to target task. key challenge in such settings is determining which external datasets, if any, can meaningfully improve model performance (Zhou et al. 2022; Zhang et al. 2022). While practitioners often rely on intuition, domain expertise, or coarse metadata to guide dataset selection, there is little formal understanding of how to model such decisions algorithmically. Most existing approaches to data selection, Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Dataset selection aims to select entire datasets from external sources to improve local model performance. Instance-level methods, such as active learning and subset selection, ignore dataset structure and often select irrelevant or misleading samples. In contrast, DaSH leverages hierarchical grouping to efficiently identify relevant datasets, avoiding noisy sources and achieving higher downstream accuracy. e.g., active learning (Sener and Savarese 2018; Gal, Islam, and Ghahramani 2017; Christen, Christen, and Rahm 2020; Paul, Bappy, and Roy-Chowdhury 2017; Zeng, Chen, and Jin 2023), data valuation (Ghorbani and Zou 2019; Pandl et al. 2021; Tang et al. 2021; Schoch, Xu, and Ji 2022; Kwon and Zou 2022), etc. , operate at the instance level, selecting individual data samples and assuming that all datasets and data sources in the selection pool are uniformly relevant to the task. This assumption fails in multi-source settings, where data is naturally organized into datasets and repositories that vary in relevance, redundancy, and quality. In practice, datasets are typically acquired, licensed, or shared in discrete units, and often originate from common sources such as institutions, simulation pipelines, or web-scale repositories, which induce hierarchical structure over the dataset pool. To address this gap, in this work, we formalize the task of dataset selection: given pool of datasets with unknown relevance to target task, how can we efficiently identify subset of datasets that will improve model performance, without having to exhaustively evaluate all candidates? This setting, illustrated in Figure 1, reflects many real-world constraints, where data is acquired, licensed, or shared in dataset-level units and must be selected under resource, bandwidth, or labeling constraints from multiple sources such as web-scale repositories or partnering institutions. To solve this new task, we propose Dataset Selection via Hierarchies (DaSH), hierarchical Bayesian method that models dataset utility at both the group and dataset levels. Given large pool of candidate datasets, grouped based on dataset origin (e.g., institution or collection) DaSH performs structured exploration to infer both group-level relevance and individual dataset utility via posterior inference over observed model performance. This hierarchical modeling allows DaSH to prioritize informative groups and avoid wasted evaluation on unrelated or harmful sources. Experiments on two benchmarks demonstrate DaSH significantly outperforms state-of-the-art baselines by up to 26.2% in accuracy under low-resource settings. The contributions of this work are: (1) We formalize the task of dataset selection from heterogeneous pool of external datasets, setting common in real-world workflows such as public data acquisition and cross-institutional collaboration, where data is organized into discrete, variably relevant sources. (2) We propose DaSH, the first dataset selection method that models dataset utility through hierarchical inference over groups and datasets, enabling efficient and robust selection under limited feedback. (3) We benchmark DaSH against four state-of-the-art data selection methods across two public datasets, demonstrating consistent performance improvements, improves accuracy by up to 26.2% DIGIT-FIVE and 10.8% on DOMAINNET. Ablation studies show DaSH remains robust to grouping noise and scales effectively to large dataset pools, whereas existing methods frequently select irrelevant or low-utility data samples."
        },
        {
            "title": "2 Related Work\nData Selection. Improving model performance through\nstrategic data selection has been extensively explored across\nvarious paradigms. In active learning, methods aim to min-\nimize labeling costs by iteratively selecting the most infor-\nmative unlabeled instances (Sener and Savarese 2018; Gal,\nIslam, and Ghahramani 2017; Christen, Christen, and Rahm\n2020; Paul, Bappy, and Roy-Chowdhury 2017; Zeng, Chen,\nand Jin 2023; Wang et al. 2023; Coleman et al. 2020). Batch\nactive learning extends this by selecting diverse subsets in\neach iteration to improve efficiency (Kirsch, Van Amers-\nfoort, and Gal 2019; Kaushal et al. 2018). Beyond active\nlearning, data valuation techniques assess the contribution\nof individual points to model performance. Approaches like\nData Shapley (Ghorbani and Zou 2019) and its adaptations\n(Pandl et al. 2021; Tang et al. 2021; Schoch, Xu, and Ji 2022;\nKwon and Zou 2022; Liu et al. 2023; Courtnage and Smirnov\n2021; Wang and Jia 2023; Just et al. 2023; Yoon, Arik, and\nPfister 2020; Kwon and Zou 2023) quantify data utility, guid-\ning the selection of valuable training instances. Additionally,\nsubset selection methods (Killamsetty et al. 2021; Coleman\net al. 2020) focus on constructing representative subsets to\nexpedite learning without compromising accuracy.",
            "content": "However, existing methods largely operate at the instance level and overlook the hierarchical structure often present in real-world settings, where datasets are naturally grouped into repositories, e.g., by source or collection. In contrast, DaSH targets dataset selection, i.e., identify groups of datasets that jointly maximize downstream performance. Empirical results demonstrate that incorporating hierarchical information improves selection efficiency and model robustness. Hierarchical Bandits. Hierarchical bandit algorithms address decision-making problems where actions are structured in hierarchy, enabling efficient exploration and exploitation across multiple levels (Hong et al. 2022; Munos et al. 2014). In recommendation systems, hierarchical bandits have been employed to model user preferences (Yue, Hong, and Guestrin 2012) and item categories (Wang et al. 2018; Zuo et al. 2022), enabling personalized content delivery under resource constraints through adaptive frameworks (Yang et al. 2020; Santana et al. 2020). Beyond recommendation, hierarchical bandits have been applied to intelligent tutoring, decentralized reinforcement learning, and multi-task off-policy learning (Castleman, Macar, and Salleb-Aouissi 2024; Hong et al. 2023; Kao, Wei, and Subramanian 2022). These applications highlight the flexibility of hierarchical formulations in structuring complex decision processes across domains. Concurrently, theoretical advancements have focused on regret minimization and generalization across tasks using hierarchical Bayesian models (Kveton et al. 2021; Hong et al. 2022; Guan and Xiong 2024), offering principled frameworks for exploration under structured priors. Inspired by works in this space, our method tackles the unique setting of dataset selection by introducing hierarchical Bayesian formulation that propagates dataset utility estimates across groups, enabling efficient amortization of training feedback via structured priors, and improving robustness to irrelevant or redundant datasets. To our knowledge, this is the first work to employ hierarchical bandits to dataset selection, with empirical evidence showing large gains in both accuracy and efficiency over non-hierarchical alternatives."
        },
        {
            "title": "3 Method\nProblem Definition. Consider n data groups g =\n{g1, g2, . . . , gn} = {gi}n\ni=1, where each group gi contains\none or more datasets. Let the set of datasets in group gi be de-\nnoted di = {di,j}mi\nj=1, where di,j is the j-th dataset in group i.\nEach dataset may contain an arbitrary number of data points.\nThe full dataset pool is thus D = (cid:83)n\ni=1 di = {di,j}. Given a\nlocal model Mk, the goal is to select a subset ˜Dk ⊆ D from\nexternal sources that maximizes the performance gain over\ntraining on the local data dk alone. Formally, we define:",
            "content": "Acck = max DkD (cid:16) (cid:17) Acc(Mk, Dk) Acc(Mk, dk) , (1) where Acc(Mk, dk) is the performance of local model Mk trained on local data dk, Acc(Mk, Dk) is the performance of Mk after training on selected datasets Dk, and Acck is the performance gain for model Mk. DaSH Initialization To address this selection objective, we introduce DaSH, bi-level hierarchical Bayesian model that captures structured uncertainty across data groups and individual datasets. As depicted in Figure 2, each data group gi is modeled with latent parameter θi encoding its expected utility, and each dataset Figure 2: Overview of the DaSH dataset selection method. Each dataset and its corresponding group are modeled using Gaussian distributions (θi, ˆσ2 ) for datasets and dataset groups, respectively. The selection process involves choosing dataset group, followed by specific dataset within that group. Upon receiving reward, the posterior distributions for the dataset and the dataset group are updated to (µ, σ2) and (θ, ˆσ2) respectively. After training, dataset groups and datasets with higher posterior means are selected as described in Section 3. ) and (µi, σ2 di,j is governed by local parameter θi,j, with corresponding reward observations ri,j(t) at timestep t. We assume normal distributions for both the priors and the reward models, with unknown means and fixed variances. Conditional on θi,j, the reward ri,j(t) is independent of the group-level parameter θi. The generative process is: θi (µi, σ2 θi,jθi (θi, ˆσ2 ri,j(t)θi,j (θi,j, σ2 ), [n] ), [m] ), D(t) = di,j, (2) is the variance of the group prior, ˆσ2 where µi is the mean of the prior distribution for data group gi, σ2 is the variance of the dataset prior θi,j, and σ2 is the variance of the reward observation model. The goal is to iteratively update the posterior distribution of θi and θi,j by incorporating all observed reward values accumulated up to the current time step t. Through this continual update process, DaSH converges towards accurate estimations of the true distributions for both θi and θi,j after number of iterations, as described Algorithm 1 in Appendix. Initialization begins with all dataset groups sharing common prior (µ0, σ2 0) and (θ0, ˆσ2 0). At each time step t, ˆθi is drawn from the normal distributions associated with each dataset group ˆθi (θiri) and the dataset group gi with the largest value is chosen. Given dataset group selection gi, DaSH then draws ˆθi,j from the distributions associated with the datasets within the chosen dataset group, i.e., ˆθi,j (θi,jri,j), and selects the dataset with the largest values, denoted as D(t) = di,j."
        },
        {
            "title": "DaSH Posterior Computation",
            "content": "DaSH receives reward from the chosen dataset and updates the distribution associated with the chosen dataset group and dataset using Eqs. (4) and (7). The posterior distribution of θi after observing reward values ri = {ri,j}, [m], where ri,j = {ri,j(t), D(t) = di,j}, is given by: (cid:90) θi,j (cid:89) j=1 (ri,j; θi,j, σ2 ) )dθi,jN (θi; µi, σ2 ). (3) (θi,j; θi, ˆσ2 From Eq.(3), this yields the closed-form posterior: (θiri) = (cid:32) (cid:32) λ2 µi σ2 + si 2 + σ2 ni ˆσi (cid:33) (cid:33) , λ2 (4) where (cid:32) λ2 = 1 σ2 + 1 2 + σ2 ni ˆσi (cid:33)1 , si = (cid:80)m j=1 ri,j ni . (5) Here, ni is the total number of selections for group gi, and si is the aggregated mean reward across datasets in group i. The posterior mean is precision-weighted average of the prior mean µi and the empirical group mean si. The influence of the prior decays with more observations as λ2 decreases. Since the reward ri,j(t) is conditionally independent of the data group parameter θi, the posterior density of θi,j, after observing rewards ri,j(t) at time step t, is computed by: (θi,j ri,j) (θi,j) (cid:89) t:D(t)=di,j (ri,j(t); θi,j, σ2 ), (6) resulting in the posterior: (θi,j ri,j) = (cid:18) λ2 i,j (cid:18) θi ˆσ2 + si,j ni,j σ2 (cid:19) (cid:19) , λ2 i,j where λ2 i,j = (cid:19)1 (cid:18) 1 ˆσ2 + ni,j σ2 , si,j = ri,j ni,j (7) (8) Here, ni,j is the number of times dataset di,j has been selected and si,j empirical mean of ri,j. Different from the dataset group posterior, the dataset posterior only depends on the rewards received by the dataset. Similar to the dataset group prior mean µi, θi is bias term that influences the decay of the dataset posterior mean. As ni,j , the dataset posterior variance goes to zero, and the dataset posterior mean approaches si,j. Dataset Selection Based on Posterior Distributions We formalize dataset selection using posterior means in two-step process: first selecting dataset group, then dataset within that group. dataset or group is selected if its posterior mean µ exceeds percentile-based threshold, i.e., if µ > 1(x), where 1 is the inverse cumulative distribution function (CDF) over the posterior means, setting the threshold at the x-th percentile. The selection threshold is adaptively chosen based on the specific needs and constraints of the training environment. For example, high percentile (e.g., 90th) indicates stringent criterion, suitable for scenarios with high training costs or where poor data quality significantly impacts model performance. Conversely, lower percentile may be used in exploratory settings or when additional data inclusion costs are minimal. Alternatively, based on the use case, the selection of top-x datasets or dataset groups may be more appropriate. Algorithmic Complexity At each selection step, DaSH performs two sequential operations: (1) inter-group sampling by drawing ˆθi (θi ri) for all groups, and (2) intra-group sampling by drawing ˆθi,j (θi,j ri,j) for the mi datasets in the chosen group. This yields per-step computational cost of O(n + mi). Posterior updates for the chosen dataset and group require constant time per step, as the closed-form updates in Eqs. (4) and (7) avoid iterative optimization. By contrast, flat selection strategy must evaluate all = (cid:80)n i=1 mi datasets at each step, incurring O(D) cost. When groups are large, the hierarchical formulation amortizes exploration: feedback from single dataset selection updates both its dataset-level and group-level posteriors, effectively sharing information across datasets in the same group. This reduces the total number of dataset evaluations required to achieve fixed target accuracy, as consistently demonstrated in our experiments."
        },
        {
            "title": "4 Experiments\nDatasets. We validate DaSH on two widely used benchmarks\nin domain adaptation: DIGIT-FIVE and DOMAINNET (Peng\net al. 2019). Each dataset contains multiple domain-specific\nsubsets for a shared classification task. DIGIT-FIVE in-\ncludes digit images from five domains (MNIST, MNIST-M,\nUSPS, SVHN, and SYN), while DOMAINNET comprises\nobject recognition images across different styles (CLIPART,\nQUICKDRAW, REAL, and SKETCH). Each domain is\ndivided into three disjoint subsets to simulate distributed or\nfederated settings. We use preprocessed versions of these\ndatasets from Schrod et al. (2023), where fixed-size feature\nvectors are extracted from images for training and evaluation.\nTo evaluate the robustness of DaSH across varying dataset\ncompositions, we examine two grouping strategies. In the\nperfect group setting, each group contains three subsets from\nthe same domain (e.g., mn0, mn1, mn2 from MNIST), mod-\neling cases where repositories or institutions curate domain-\nspecific datasets. In the mixed group setting, subsets from\ndifferent domains are combined into groups (e.g., mn1, mn2,\nmm0), modeling cases where datasets from multiple sources\nor domains are aggregated for a shared task and group assign-\nments are noisy or imperfect. Preprocessing steps, group def-\ninitions, and dataset statistics are provided in the Appendix.",
            "content": "(a) DIGIT-FIVE (b) DOMAINNET Figure 3: Accuracy heatmaps of local classifiers after training on different DIGIT-FIVE and DOMAINNET subsets. The first column shows local test accuracy for each subset. The last column indicates the optimal accuracy achievable when training on all available relevant same-domain datasets. Middle columns depict accuracy after augmenting training data with additional subsets from same and different domains. Implementation Details. For DIGIT-FIVE, each local model is lightweight CNN trained on its respective domainspecific subsets (e.g., MNIST, SVHN), while for DOMAINNET, local models are three-layer multilayer perceptrons (MLPs). Local accuracy refers to model performance on its own domain without any additional training. Additional implementation details are provided in the Appendix. Figure 3 summarizes the empirical obtained by training local models on different external datasets. These groundtruth results serve as reference for evaluating the potential benefit of dataset selection. In DIGIT-FIVE, models trained on external datasets consistently underperform compared to their local baselines, indicating strong domain-specific bias. In contrast, DOMAINNET exhibits more favorable crossdomain transfer; for example, training the REAL classifier on subsets from CLIPART yields noticeable performance gains. This distinction underscores the practical relevance of dataset selection in heterogeneous sharing scenarios. Baselines. We compare against existing methods to assess: (1) DaSHs effectiveness in dataset selection relative to stateof-the-art data selection approaches, and (2) its ability to capture dependencies among datasets. Core-sets (Sener and Savarese 2018), which selects representative samples via geometric coverage, such that model learned only on the selected subset are as competitive. FreeSel (Xie et al. 2023a), uses pretrained vision transformer to perform one-pass, supervision-free data selection, with time efficiency close to random selection. ActiveFT (Xie et al. 2023b), which optimizes selection to match the data distribution while preserving diversity. BiLAF (Lu et al. 2024), extends ActiveFT by introducing boundary uncertainty to enable one-shot label-free selection through pseudo-class estimation and iterative refinement. Table 1: Performance comparison on DIGIT-FIVE against baselines (averaged over 5 runs) Best performance is bold. Red downward arrows () indicate absolute drops in accuracy relative to the best-performing method. Method Local Global Core-sets (Sener and Savarese 2018) FreeSel (Xie et al. 2023a) ActiveFT (Xie et al. 2023b) BiLAF (Lu et al. 2024) DaSH Hierarchical MNIST 52.76.5 89.31.1 SVHN 50.93.4 69.71.4 USPS 52.23.2 92.20. MNIST-M SYN 49.42.4 80.21.1 50.95.1 62.82.8 AVG 51.24.1 78.81. 75.72.3 13.8 87.61.2 1.9 58.21.631.3 62.60.526.9 89.50.6 52.82.7 16.4 39.34.0 29.9 53.61.615.6 56.80.412.4 69.23.4 74.03.6 17.2 29.33.1 61.9 59.21.332.0 67.30.523.9 91.20.9 60.82.1 18.1 65.42.2 13.5 48.30.930.6 50.10.528.8 78.90.5 40.81.9 22.1 40.72.9 22.2 41.41.521.5 52.61.010.3 62.91.6 60.82.5 17.5 52.52.7 25.8 52.11.426.2 57.90.620.4 78.31. Table 2: Performance comparison on DOMAINNET against baselines (averaged over 5 runs). Best performance is bold. Red downward arrows () indicate absolute drops in accuracy relative to the best-performing method. Method Local Global Core-sets (Sener and Savarese 2018) FreeSel (Xie et al. 2023a) ActiveFT (Xie et al. 2023b) BiLAF (Lu et al. 2024) DaSH Hierarchical CLIPART QUICKDRAW REAL SKETCH AVG 40.02.4 78.50.6 64.02.1 86.70. 61.01.1 88.40.6 67.51.1 72.30.8 58.11.7 81.61.1 59.10.9 18.2 70.12.1 7.2 67.61.89.7 69.01.68.3 77.30.8 74.10.3 12.3 81.70.8 4.6 78.01.08.3 81.30.55.0 86.31.1 80.10.6 8.3 85.60.7 2.8 83.81.14.6 85.80.52.6 88.40. 67.60.4 4.2 67.21.3 4.6 67.81.14.0 67.80.74.0 71.80.9 70.20.6 10.8 77.71.2 3.3 74.31.36.7 76.00.85.0 81.00.9 In addition, we include two baselines for reference: Local, trained only on local data, and Global, trained on all datasets from the same domain, representing lower and upper bounds. Experimental Results Table 1 reports mean and standard deviation over five independent runs on DIGIT-FIVE subdomains, where we compare DaSH to local and global baselines as well as the four stateof-the-art data selection baselines. Across all five domains, DaSH matches the global model, achieving an average accuracy of 78.3%, which is only 0.5% below the global upper bound (78.8%) and significantly higher than the local lower bound (51.2%). These results indicate that our method is capable of effectively leveraging heterogeneous data sources. Compared to competitive baselines, DaSH exhibits substantial gains. For instance, FreeSel underperforms by over 25.8% on average, and notably degrades performance on SVHN, USPS, and SYN, suggesting that its model-free selection policy does not work well under our problem setting where the selection pool contains irrelevant data. Similarly, ActiveFT and BiLAF fall behind by 26.2% and 20.4%, respectively. Notably, these methods exhibit particularly low accuracy on MNIST-M and SYN, which represent domains with significant distributional divergence from the rest of the datasets. This performance drop suggests that baselines struggle to generalize when the target domain is poorly aligned with the source distribution, highlighting their limitations in handling high domain shift scenarios. In contrast, DaSH consistently maintains top performance with low variance, highlighting its robustness across target domains. Table 2 shows results on DOMAINNET. While performance margins are narrower than in DIGIT-FIVE, DaSH still outperforms all baselines by 3.310.8%. This is likely because all models use features extracted from ResNet-18 backbone that was pretrained on the combined dataset. The shared feature extractor reduces the distributional differences between domains, making the task inherently easier for all methods and diminishing relative gains. Nevertheless, DaSH maintains its advantage, underscoring its effectiveness even when inter-domain variation is minimized."
        },
        {
            "title": "Impact of Hierarchical Grouping",
            "content": "To understand the importance of hierarchical grouping, we compare DaSH against two baseline variants: DaS (flat), non-hierarchical counterpart, and DaSH (mixed), which uses imperfect group assignments. Figure 4 presents Pareto frontiers of accuracy versus selection cost (exploration steps) for each domain in DIGIT-FIVE and DOMAINNET, with marker shapes indicating domains and colors indicating methods. Compared to the non-hierarchical DaS (flat), DaSH consistently delivers equal or higher accuracy at substantially lower selection cost. On DIGIT-FIVE, this translates to savings of 2060 steps per domain without sacrificing accuracy. When compared to DaSH (mixed), the gap is small in most domains, with the mixed variant often lying on or near the Pareto frontier achieved by perfect grouping. This indicates that DaSH is robust to imperfect group assignments, with only modest performance drops in more challenging domains like SYN, CLIPART QUICKDRAW REAL SKETCH MNIST SVHN USPS MNIST-M SYN ) % ( r A 90 80 70 60 90 80 75 70 ) % ( a c 220 200 180 160 120 220 200 180 160 Steps (more fewer) DIGIT-FIVE Steps (more fewer) DOMAINNET Figure 4: Pareto trade-offs between accuracy and selection cost. Each point is methoddomain result (DIGIT-FIVE left, DOMAINNET right). Marker shape encodes the domain, while color distinguishes the methods: DaS (flat), DaSH (mixed), and DaSH. Points toward the upper-right represent better trade-offs (higher accuracy, fewer steps). Across both benchmarks, the upper-right region is occupied by hierarchical variants with DaSH contributing most of the frontier on DIGIT-FIVE and sharing the frontier with DaSH (mixed) on DOMAINNET. Local DaS (flat) DaSH (mixed) DaSH Global 100 80 5 . 9 8 3 . 9 6 . 6 8 7 . 0 8 2 . 2 9 3 . 1 9 5 . 9 8 0 . 0 7 . 9 6 8 . 7 6 4 . 7 6 6 . 5 6 2 . 0 8 7 . 7 4 . 9 6 9 . 7 6 60 7 . 2 5 9 . 0 5 2 . 2 4 . 9 4 r A 9 . 2 6 3 . 7 5 9 . 6 5 4 . 2 9 . 0 5 40 MNIST SVHN USPS MNIST-M SYN Figure 5: Performance under budget constraints. Under limited exploration (15 steps), DaSH and DaSH (mixed) outperform DaS (flat) on 4 out of 5 datasets. Local and Global denote the lower and upper bounds, respectively. QUICKDRAW, and REAL. Overall, these results show that hierarchical grouping not only improves efficiency and accuracy but also maintains strong performance under noisy or partially incorrect group structures."
        },
        {
            "title": "Comparison Under Limited Exploration",
            "content": "We evaluate the ability of each method to identify useful datasets under stringent exploration budgets. Specifically, each method explores each dataset only once, totaling 15 steps across the 15 datasets in DIGIT-FIVE. Figure 5 reports the resulting accuracy for each domain. Under this extreme budget constraint, both DaSH and DaSH (mixed) outperform the non-hierarchical DaS (flat) in 4 out of 5 domains. The gains over DaS (flat) are substantial: +5.2% on MNIST, +6.0% on SVHN, +7.4% on USPS, and +9.0% on SYN. Even with imperfect grouping, DaSH (mixed) closely tracks the performance of perfect grouping, with Table 3: DaSH improves performance even with weak initial model with low accuracy. This table reports accuracy on DIGIT-FIVE when initially trained on 10%, 20%, and 50% of the local training data (Init.), and after using DaSH to select additional datasets for training (DaSH). % Train Name 10% 20% 50% Init. DaSH Init. DaSH Init. DaSH 17.6 MNIST 12.8 SVHN USPS 9.6 MNIST-M 20.6 26.6 SYN 31.5 24.2 13.5 55.1 37.6 23.6 21.2 12.8 28.8 21.4 89.6 21.5 28.6 57.6 24. 36.6 35.6 31.2 44.2 27.4 89.6 66.7 91.4 79.3 41.0 accuracy differences within 12% in most domains. The Local and Global baselines show that hierarchical variants close more than half the gap to the global optimum despite operating under 15-step budget. These results confirm that hierarchical grouping enables efficient, high-quality dataset selection even under severe exploration limits. Effectiveness Under Weak Initialization We additionally investigate whether DaSH can enhance performance when initial local model accuracy is very low. We train initial local classifiers using 10%, 20%, and 50% of the available training data. Table 3 shows consistent accuracy gains across all conditions, even when initial accuracy is as low as 9.6% (USPS), demonstrating DaSHs robustness to significant variations in initial performance before selection. Robustness under Cross-Domain Grouping We evaluate DaSH in an extreme cross-domain grouping scenario, where each group is constructed to contain exactly one dataset from each domain. This setup eliminates the possibility of selecting multiple same-domain datasets within Methods Digit-Five DomainNet SYN SVHN SVHN MNIST-M USPS QUICKDRAW QUICKDRAW REAL REAL SKETCH Core-Sets FreeSel ActiveFT BiLAF DaSH(ours) MNIST SYN SVHN MNIST-M USPS CLIPART CLIPART QUICKDRAW REAL SKETCH SYN SYN SYN SYN SYN CLIPART SKETCH QUICKDRAW CLIPART CLIPART SYN SYN SYN SYN SYN CLIPART SKETCH CLIPART CLIPART REAL MNIST MNIST MNIST MNIST MNIST SKETCH SKETCH SKETCH SKETCH SKETCH Figure 6: Qualitative comparisons on DIGIT-FIVE (target: MNIST) and DOMAINNET (target: SKETCH). Each selected image is labeled by its source domain (above), with green borders indicating correct domain match to the target and red borders indicating mismatch. Unlike prior methods, which frequently select subsets from mismatched domains in the first exploration step, DaSH consistently identifies subsets from the correct domain, even in challenging settings with visually similar categories. Table 4: Robustness of DaSH under cross-domain grouping. Performance on USPS with cross-domain groups, where each group contains exactly one dataset from each domain, removing opportunities to select multiple same-domain datasets. DaSH achieves the robust accuracy while requiring fewer steps than the non-hierarchical variant DaS (flat). Method # Steps Accuracy DaS (flat) DaSH DaSH (cross-domain grouping) 163 140 154 90.92.0 91.20.9 92.20.7 single group, stress-testing the ability of DaSH to perform effective selection when group structure does not align with domain semantics and offers no within-domain redundancy to exploit. As shown in Table 4, DaSH delivers robust accuracy and outperforms the non-hierarchical baseline, DaS (flat), while also requiring fewer selection steps. Our ablation results consistently show that, under different settings, DaSH remains effective, maintaining strong performance with minimal computational overhead."
        },
        {
            "title": "6 Qualitative Analysis",
            "content": "Figure 6 illustrates clear qualitative differences in the selection behavior of each method. Green borders indicate that the selected data instance belongs to the target domain, while red borders indicate domain mismatches. Across both benchmarks, baseline methods such as Core-Sets, FreeSel, ActiveFT, and BiLAF often select subsets from visually similar but incorrect domains. For example, when MNIST is used as the local dataset, most baselines retrieve images that are visually distinct from the target domain. Only FreeSel selects sample from MNIST, which is consistent with its relatively better quantitative performance  (Table 1)  . The rest of the baselines fail to retrieve meaningful samples. In contrast, DaSH effectively selects relevant data. This behavior extends to DOMAINNET, where DaSH maintains domain-consistent selection across diverse categories. These results suggest that DaSH internalizes domain structure more effectively than prior methods, allowing it to identify relevant datasets even under distribution shift and candidate noise, an essential capability for transferability in collaborative data-sharing settings."
        },
        {
            "title": "8 Acknowledgments\nThis research is based on work partially supported\nby the National Science Foundation under award num-\nber CMMI-2331985,\nthe U.S. Defense Advanced Re-\nsearch Projects Agency (DARPA) under award number\nHR001125C0303, and U.S. Army DEVCOM under award\nnumber W5170125CA160. The views and conclusions con-\ntained herein are those of the authors and should not be\ninterpreted as representing the official policies, either ex-\npressed or implied, of NSF, DARPA, the U.S. Army, or the\nU.S. Government. The U.S. Government is authorized to re-\nproduce and distribute reprints for governmental purposes\nnotwithstanding any copyright annotation therein.",
            "content": "and Rahm, E. References Alzubaidi, L.; Bai, J.; Al-Sabaawi, A.; Santamarıa, J.; Albahri, A. S.; Al-Dabbagh, B. S. N.; Fadhel, M. A.; Manoufali, M.; Zhang, J.; Al-Timemy, A. H.; et al. 2023. survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications. Journal of Big Data. Castleman, B.; Macar, U.; and Salleb-Aouissi, A. 2024. Hierarchical Multi-Armed Bandits for the Concurrent Intelligent Tutoring of Concepts and Problems of Varying Difficulty In Deployable RL: From Research to Practice@ Levels. Reinforcement Learning Conference. 2020. Christen, V.; Christen, P.; Informativeness-Based Active Learning for Entity Resolution. In Machine Learning and Knowledge Discovery in Databases: International Workshops of ECML PKDD. Springer. Coleman, C.; Yeh, C.; Mussmann, S.; Mirzasoleiman, B.; Bailis, P.; Liang, P.; Leskovec, J.; and Zaharia, M. 2020. Selection via Proxy: Efficient Data Selection for Deep Learning. In International Conference on Learning Representations. Courtnage, C.; and Smirnov, E. 2021. Shapley-value data valuation for semi-supervised learning. In Discovery Science: 24th International Conference. Springer. Gal, Y.; Islam, R.; and Ghahramani, Z. 2017. Deep Bayesian Active Learning with Image Data. In International Conference on Machine Learning. Ganin, Y.; and Lempitsky, V. 2015. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning. Ghorbani, A.; and Zou, J. 2019. Data Shapley: Equitable Valuation of Data for Machine Learning. In International Conference on Machine Learning. Guan, J.; and Xiong, H. 2024. Improved Bayes Regret Bounds for Multi-Task Hierarchical Bayesian Bandit Algorithms. Advances in Neural Information Processing Systems (NeurIPS). Hong, J.; Kveton, B.; Zaheer, M.; and Ghavamzadeh, M. 2022. Hierarchical Bayesian Bandits. In International Conference on Artificial Intelligence and Statistics (AISTATS). Hong, J.; Kveton, B.; Zaheer, M.; Katariya, S.; and Ghavamzadeh, M. 2023. Multi-task off-policy learning from bandit feedback. In International Conference on Machine Learning. PMLR. Hull, J. J. 1994. database for handwritten text recognition research. IEEE Transactions on Pattern Analysis and Machine Intelligence. Jin, X.; Lan, C.; Zeng, W.; and Chen, Z. 2021. Re-energizing domain discriminator with sample relabeling for adversarial domain adaptation. In IEEE/CVF International Conference on Computer Vision. Just, H. A.; Kang, F.; Wang, T.; Zeng, Y.; Ko, M.; Jin, M.; and Jia, R. 2023. LAVA: Data Valuation Without Pre-Specified Learning Algorithms. In International Conference on Learning Representations. Kao, H.; Wei, C.-Y.; and Subramanian, V. 2022. Decentralized cooperative reinforcement learning with hierarchical information structure. In International Conference on Algorithmic Learning Theory. PMLR. Kaushal, V.; Sahoo, A.; Doctor, K.; Raju, N.; Shetty, S.; Singh, P.; Iyer, R.; and Ramakrishnan, G. 2018. Learning from Less Data: Diversified Subset Selection and Active Learning in Image Classification Tasks. arXiv Preprint arXiv:1805.11191. Killamsetty, K.; Sivasubramanian, D.; Ramakrishnan, G.; and Iyer, R. 2021. Glister: Generalization based data subset selection for efficient and robust learning. In AAAI Conference on Artificial Intelligence. Kirsch, A.; Van Amersfoort, J.; and Gal, Y. 2019. Batchbald: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning. Advances in Neural Information Processing Systems (NeurIPS). Komatsu, T.; Matsui, T.; and Gao, J. 2021. Multi-source domain adaptation with sinkhorn barycenter. In European Signal Processing Conference (EUSIPCO). IEEE. Kveton, B.; Konobeev, M.; Zaheer, M.; Hsu, C.-w.; Mladenov, M.; Boutilier, C.; and Szepesvari, C. 2021. Meta-Thompson Sampling. In International Conference on Machine Learning. Kwon, Y.; and Zou, J. 2022. Beta Shapley: Unified and Noise-reduced Data Valuation Framework for Machine Learning. In International Conference on Artificial Intelligence and Statistics (AISTATS). Kwon, Y.; and Zou, J. 2023. Data-OOB: Out-of-Bag Estimate as Simple and Efficient Data Value. In International Conference on Machine Learning. LeCun, Y.; Bottou, L.; Bengio, Y.; and Haffner, P. 1998. Gradient-based learning applied to document recognition. IEEE. Li, Y.; Yuan, L.; Chen, Y.; Wang, P.; and Vasconcelos, N. 2021. Dynamic transfer for multi-source domain adaptation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition. Liu, Z.; Just, H. A.; Chang, X.; Chen, X.; and Jia, R. 2023. 2D-shapley: framework for fragmented data valuation. In International Conference on Machine Learning. Lu, H.; Xie, Y.; Yang, X.; and Yan, J. 2024. Boundary Matters: Bi-Level Active Finetuning Method. In Advances in Neural Information Processing Systems (NeurIPS). Wang, J. T.; and Jia, R. 2023. Data Banzhaf: Robust Data Valuation Framework for Machine Learning. In International Conference on Artificial Intelligence and Statistics (AISTATS). Wang, L.; Wang, X.; Ji, Q.; Wang, L.; and Jin, R. 2023. Mutual Active Learning for Engineering Regulated Statistical Digital Twin Models. IEEE Transactions on Industrial Informatics. Wang, Q.; Li, T.; Iyengar, S.; Shwartz, L.; and Grabarnik, G. Y. 2018. Online IT Ticket Automation Recommendation Using Hierarchical Multi-Armed Bandit Algorithms. In SIAM International Conference on Data Mining. Xie, Y.; Ding, M.; Tomizuka, M.; and Zhan, W. 2023a. Towards free data selection with general-purpose models. Advances in Neural Information Processing Systems (NeurIPS). Xie, Y.; Lu, H.; Yan, J.; Yang, X.; Tomizuka, M.; and Zhan, W. 2023b. Active finetuning: Exploiting annotation budget in the pretraining-finetuning paradigm. In IEEE/CVF Conference on Computer Vision and Pattern Recognition. Yang, M.; Li, Q.; Qin, Z.; and Ye, J. 2020. Hierarchical Adaptive Contextual Bandits for Resource Constraint Based Recommendation. In The Web Conference. Yao, C.-H.; Gong, B.; Qi, H.; Cui, Y.; Zhu, Y.; and Yang, M.-H. 2022. Federated multi-target domain adaptation. In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). Yoon, J.; Arik, S.; and Pfister, T. 2020. Data Valuation Using Reinforcement Learning. In International Conference on Machine Learning. Yue, Y.; Hong, S. A.; and Guestrin, C. 2012. Hierarchical Exploration for Accelerating Contextual Bandits. In International Conference on Machine Learning. Zeng, Y.; Chen, X.; and Jin, R. 2023. Ensemble Active Learning by Contextual Bandits for AI Incubation in Manufacturing. ACM Transactions on Intelligent Systems and Technology. Zhang, W.; Deng, L.; Zhang, L.; and Wu, D. 2022. survey on negative transfer. IEEE/CAA Journal of Automatica Sinica. Zhou, K.; Liu, Z.; Qiao, Y.; Xiang, T.; and Loy, C. C. 2022. IEEE Transactions on Domain generalization: survey. Pattern Analysis and Machine Intelligence. Zuo, J.; Hu, S.; Yu, T.; Li, S.; Zhao, H.; and Joe-Wong, C. 2022. Hierarchical conversational preference elicitation with bandit feedback. In ACM International Conference on Information & Knowledge Management. Luo, S.; Zhu, D.; Li, Z.; and Wu, C. 2021. Ensemble federated adversarial training with non-iid data. arXiv preprint arXiv:2110.14814. Mohammed, S.; Budach, L.; Feuerpfeil, M.; Ihde, N.; Nathansen, A.; Noack, N.; Patzlaff, H.; Naumann, F.; and Harmouch, H. 2025. The effects of data quality on machine learning performance. Information Systems. Munos, R.; et al. 2014. From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning. Foundations and Trends in Machine Learning. Pandl, K. D.; Feiland, F.; Thiebes, S.; and Sunyaev, A. 2021. Trustworthy Machine Learning for Health Care: Scalable Data Valuation with the Shapley Value. In Conference on Health, Inference, and Learning. Paul, S.; Bappy, J. H.; and Roy-Chowdhury, A. K. 2017. NonUniform Subset Selection for Active Learning in Structured Data. In IEEE/CVF Conference on Computer Vision and Pattern Recognition. Peng, X.; Bai, Q.; Xia, X.; Huang, Z.; Saenko, K.; and Wang, B. 2019. Moment matching for multi-source domain adaptation. In IEEE/CVF International Conference on Computer Vision. Roy, P.; Ghosh, S.; Bhattacharya, S.; and Pal, U. 1807. Effects of degradations on deep neural network architectures. arXiv preprint arXiv:1807.10108. Santana, M. R.; Melo, L. C.; Camargo, F. H.; Brandao, B.; Soares, A.; Oliveira, R. M.; and Caetano, S. 2020. Contextual Meta-Bandit for Recommender Systems Selection. In ACM Conference on Recommender Systems. Schoch, S.; Xu, H.; and Ji, Y. 2022. CS-Shapley: class-wise Shapley values for data valuation in classification. Advances in Neural Information Processing Systems (NeurIPS). Schrod, S.; Lippl, J.; Schafer, A.; and Altenbuchinger, M. 2023. FACT: Federated Adversarial Cross Training. arXiv preprint arXiv:2306.00607. Sener, O.; and Savarese, S. 2018. Active Learning for Convolutional Neural Networks: Core-Set Approach. In International Conference on Learning Representations. Simon, C.; Faraki, M.; Tsai, Y.-H.; Yu, X.; Schulter, S.; Suh, Y.; Harandi, M.; and Chandraker, M. 2022. On generalizing beyond domains in cross-domain continual learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition. Singh, A. 2021. Clda: Contrastive learning for semisupervised domain adaptation. Advances in Neural Information Processing Systems (NeurIPS). Sun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017. Revisiting unreasonable effectiveness of data in deep learning era. In IEEE/CVF Conference on Computer Vision and Pattern Recognition. Tang, S.; Ghorbani, A.; Yamashita, R.; Rehman, S.; Dunnmon, J. A.; Zou, J.; and Rubin, D. L. 2021. Data Valuation for Medical Imaging Using Shapley Value and Application to Large-Scale Chest X-Ray Dataset. Scientific Reports."
        },
        {
            "title": "Reproducibility Checklist",
            "content": "purposes (yes/partial/no/NA) NA 1. General Paper Structure 1.1. Includes conceptual outline and/or pseudocode description of AI methods introduced (yes/partial/no/NA) yes 1.2. Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes/no) yes 1.3. Provides well-marked pedagogical references for lessfamiliar readers to gain background necessary to replicate the paper (yes/no) yes 2. Theoretical Contributions 2.1. Does this paper make theoretical contributions? (yes/no) no If yes, please address the following points: 2.2. All assumptions and restrictions are stated clearly and formally (yes/partial/no) Type your response here 2.3. All novel claims are stated formally (e.g., in theorem statements) (yes/partial/no) Type your response here 2.4. Proofs of all novel claims are included (yes/partial/no) Type your response here 2.5. Proof sketches or intuitions are given for complex and/or novel results (yes/partial/no) Type your response here 2.6. Appropriate citations to theoretical tools used are given (yes/partial/no) Type your response here 2.7. All theoretical claims are demonstrated empirically to hold (yes/partial/no/NA) Type your response here 2.8. All experimental code used to eliminate or disprove claims is included (yes/no/NA) Type your response here 3. Dataset Usage 3.1. Does this paper rely on one or more datasets? (yes/no) yes If yes, please address the following points: 3.2. motivation is given for why the experiments are conducted on the selected datasets (yes/partial/no/NA) yes 3.3. All novel datasets introduced in this paper are included in data appendix (yes/partial/no/NA) NA 3.4. All novel datasets introduced in this paper will be made publicly available upon publication of the paper with license that allows free usage for research 3.5. All datasets drawn from the existing literature (potentially including authors own previously published work) are accompanied by appropriate citations (yes/no/NA) yes 3.6. All datasets drawn from the existing literature (potentially including authors own previously published work) are publicly available (yes/partial/no/NA) yes 3.7. All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing (yes/- partial/no/NA) NA 4. Computational Experiments 4.1. Does this paper include computational experiments? (yes/no) yes If yes, please address the following points: 4.2. This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting (yes/partial/no/NA) NA 4.3. Any code required for pre-processing data is included in the appendix (yes/partial/no) yes 4.4. All source code required for conducting and analyzing the experiments is included in code appendix (yes/partial/no) yes 4.5. All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with license that allows free usage for research purposes (yes/partial/no) yes 4.6. All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes/- partial/no) yes 4.7. If an algorithm depends on randomness, then the method used for setting seeds is described in way sufficient to allow replication of results (yes/partial/no/NA) yes 4.8. This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks (yes/partial/no) yes 4.9. This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics (yes/partial/no) yes 4.10. This paper states the number of algorithm runs used to compute each reported result (yes/no) yes 4.11. Analysis of experiments goes beyond singledimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information (yes/no) yes 4.12. The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank) (yes/partial/no) yes 4.13. This paper lists all final (hyper-)parameters used for each model/algorithm in the papers experiments (yes/partial/no/NA) NA Datasets Our two experimental benchmarks are DIGIT-FIVE and DOMAINNET (Peng et al. 2019), both commonly employed to evaluate domain adaptation models (Schrod et al. 2023; Yao et al. 2022; Simon et al. 2022; Jin et al. 2021; Komatsu, Matsui, and Gao 2021; Li et al. 2021; Luo et al. 2021; Singh 2021), with groupings based on different domain types for the same task. We briefly describe each below: DIGIT-FIVE. The DIGIT-FIVE dataset contains images of handwritten digits (0-9) with variability in writing styles, stroke thickness, and other characteristics. The dataset has five different data subsets: MNIST (clean, grayscale images of handwritten digits in uniform style) (LeCun et al. 1998), MNIST-M (images from MNIST superimposed on complex color backgrounds from BSDS500) (Ganin and Lempitsky 2015), USPS (grayscale images of digits from scanned mail, with variations in scale and stroke thickness) (Hull 1994), SVHN (real-world, full-color images of house numbers with diverse fonts and lighting conditions) (Roy et al. 1807), and SYN (synthetic images of digits manipulated with various font styles and digital effects) (Ganin and Lempitsky 2015). For our experiments, we utilize preprocessed data as provided by Schrod et al. (2023). Images are encoded into vector representations using CNN feature extractor with three convolutional layers followed by pooling layer, trained on the dataset. For each of the DIGIT-FIVE subsets, we randomly sample data points to divide them into three mutually exclusive groups. We refer to each MNIST-derived groups as {mn0, mn1, mn2}, groups derived from MNIST-M as {mm0, mm1, mm2}, from USPS as {us0, us1, us2}, from SVHN as {sv0, sv1, sv2}, and from SYN as {sy0, sy1, sy2}. DOMAINNET. The DOMAINNET dataset (Peng et al. 2019) contains data instances from diverse object categories across six domains: real, clipart, painting, sketch, infograph, and quickdraw, each representing distinct style. For our experiments, we select images from 15 classes across four domains: CLIPART (clip art images), QUICKDRAW (drawings from the game Quick Draw), REAL (photos and real-world images), and SKETCH (sketches of objects). Data pre-processing is similar to that for DIGIT-FIVE. For each of the domain subsets, we randomly sample data points to divide them into three mutually exclusive groups. We refer to groups from CLIPART as {cp0, cp1, cp2}, QUICKDRAW groups as {qd0, qd1, qd2}, groups from REAL as {rl0, rl1, rl2}, and groups from SKETCH as {sk0, sk1, sk2}. To assess DaSHs robustness under different group configurations, we experiment with three distinct settings: Perfect Grouping: Here, groups have clear domain boundaries, each containing three distinct datasets: {mn0, mn1, mn2}, {mm0, mm1, mm2}, {us0, us1, us2}, {sv0, sv1, sv2}, and {sy0, sy1, sy2}. Similarly, DOMAINNET is partitioned into coherent domain-aligned groups: {cp0, cp1, cp2}, {qd0, qd1, q2}, {rl0, rl1, rl2}, and {sk0, sk1, sk2}. Mixed Grouping: We consider mixed groups that contain subsets from different domains. This reflects real-world situations where organizations may contribute data spanning multiple domains. For DIGIT-FIVE, we define the following groups: {mn1, mn2, mm0}, {mm1, mm2, us0}, {us1, us2, sv0}, {sv1, sv2, sy0}, {sy1, sy2, mn0}. For DOMAINNET, the groups are: {cp1, cp2, qd0}, {qd1, qd2, rl0}, {rl1, rl2, sk0}, {sk1, sk2, cp0}. Cross-Domain Grouping: We construct groups such that no group contains datasets from the same domain. This tests whether the method can still make effective selections when group structure does not reflect underlying domain similarity. The DIGIT-FIVE groups are: {mn0, sv0, mm0}, {sv1, mm1, us0}, {mm2, us1, sy0}, {us2, sy1, mn1}, {sy2, mn2, sv2}. Implementation details For the DIGIT-FIVE dataset, the local classifiers consist of single CNN layer. Figure 3 (a) shows the ground truth accuracy heatmap for DIGIT-FIVE, where the first column displays the local accuracy (loc) for each digit classifier on the MNIST ({mn0, mn1, mn2}), MNIST-M ({mm0, mm1, mm2}), USPS ({us0, us1, us2}), SVHN ({sv0, sv1, sv2}) and SYN ({sy0, sy1, sy2}) subgroups while the last column reveals the global accuracy achieved after each classifier is trained on the relevant subsets sampled from its corresponding dataset. For example, the global accuracy of 89.3% for MNIST is achieved by training the local model on {mn0, mn1, mn2}. Training on other datasets yields lower accuracy than the local accuracy, suggesting degradation in performance. Therefore, the optimal performance for MNIST is attained by training on {mn0, mn1, mn2}. The middle columns depict accuracy of local classifiers after additional training on each individual subset. For the DOMAINNET(Peng et al. 2019) dataset, the local classifier consists of three fully connected layers. Figure 3 (b) shows that the CLIPART model exhibits the lowest local accuracy at 40.7%, while the sketch model achieves the highest local accuracy at 67%. In this benchmark, although the local model still gains the most improvement when trained on external sets from the same domain, datasets from other domains also improve model accuracy. For instance, CLIPART datasets {cp0, cp1, cp2} contribute to enhancing local model performance for the REAL dataset group. These characteristics render the DOMAINNET experiments closer to realistic data-sharing settings. The non-hierarchical baseline, DaS (flat), serves as flat DaSH variant to directly compare the utility of hierarchical decomposition. DaS (flat) treats each dataset independently without modeling shared origin or source-level relationships. 0 and ˆσ2 For DaSH and DaS (flat), we set µ0 and θi to 0, and σ2 0 to 2, as prior distributions for all dataset groups and datasets. We set the pre-defined percentile posterior mean threshold to 80 and 60 for the perfect and mixed groups, respectively. At every time step, DaSH decides on dataset to select, retrieves sample, and the local model predicts the samples label. The accuracy of this prediction determines the reward, i.e., ri,j(t) = 1 if ˆy = y, and ri,j(t) = 0 otherwise, where ˆy represents the predicted label and the actual label of the sample. This reward, either 1 for correct prediction or 0 for an incorrect one, serves as the sole feedback for the algorithm to update its prior beliefs. DaSH systematically refines these beliefs in response to the observed reward outcomes. For accurate and efficient dataset selection, we employ K-means clustering to identify representative data points, selecting five points nearest to the centroids in each cluster Algorithm 1: DaSH Dataset Selection 1: Initialize (θiri) group distributions, (θi,jri,j) ) reward distribudataset distributions, and (θi,j, σ2 tions for = 1, . . . , do 2: for = 1, . . . , do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: end for Sample ˆθi(t) (θiri) end for = arg max{ˆθi(t) [n]} {gi is chosen} for = 1, . . . , do Sample ˆθi,j(t) (θi,jri,j) end for = arg max{ˆθi,j(t) [m]} {di,j is chosen} Receive reward ri,j(t) = 1{ˆy = y} Update (θiri) and (θi,jri,j) {Eq. (3) and Eq. (4)} to encapsulate the datasets characteristics. Specifically, for DIGIT-FIVE, which comprises 10 distinct classes, we configure clustering to generate 10 clusters to ensure that the variability inherent in each class is captured effectively. The models priors are updated exclusively using 5 near-centroid points from each cluster. Similarly, for DOMAINNET, we generate 15 clusters corresponding to the 15 classes in the dataset and use 5 near-centroid points from each cluster. Data selection stops when all representative points from particular dataset are selected, indicating that the selection model has identified specific dataset as likely to significantly enhance model performance. The total number of steps required to explore all representative points from all 15 DIGIT-FIVE data subsets is 750 (corresponding to the 15 data subsets, each with 10 clusters and 5 near-centroid points for each cluster). Similarly, the total number of steps required to explore all representative points from DOMAINNET is 1125. However, our experiments verify that the proposed empirical stopping criterion requires significantly fewer steps. DaSH Dataset Selection Algorithm We include the full pseudocode of the proposed DaSH dataset selection algorithm in Algorithm 1, capturing the hierarchical selection process over groups and datasets. The pseudocode corresponds to the framework described in Section 3 Scalability to Larger Dataset Pools We evaluate the scalability of DaSH by expanding the number of candidate datasets within each DIGIT-FIVE group. Specifically, the MNIST, SVHN, USPS, MNIST-M, and SYN groups are augmented to include 10, 12, 11, 9, and 9 datasets, respectively. As shown in Table 5, DaSH continues to identify high-utility datasets and consistently improves downstream accuracy across all domains. Importantly, per-step computational cost remains constant, and the total number of selection steps increases sublinearly with the size of the dataset pool. For instance, SVHN contains 4 more datasets than in the original setting, yet DaSH requires only 2.6 more steps. These results highlight the methods scalability and efficiency in more complex selection settings. Figure 7: DaSH reliably signals the absence of relevant datasets. When no beneficial datasets are present in the pool, the posterior means remain consistently low, even after 600 exploration steps, indicating that DaSH does not overcommit to low-utility sources. Table 5: DaSH scalability ablation on DIGIT-FIVE. Accuracy across five domains with 15 vs. 51 dataset configurations. Larger dataset pools improve performance consistently. Domain MNIST SVHN USPS MNIST-M SYN AVG DaSH (15) DaSH (51) 89.5 93.7 69.2 71. 91.2 92.8 78.9 83.4 62.9 76.5 78.3 83.6 Robustness to Absence of Relevant Sources As in real-world applications where the usefulness of datasets is not known in advance, we further evaluate the behavior of DaSH when no relevant datasets are available in the candidate pool. As illustrated in Figure 7, in this setting, DaSH continues exploration as instructed, but the inferred posterior means across all datasets remain consistently low. This indicates that the method robustly recognizes the lack of beneficial datasets, producing clear signal that no source meaningfully improves downstream performance. DaSH avoids committing to low-utility datasets, demonstrating reliable behavior even in unfavorable selection conditions. Optimality Analysis Let θ = maxdi,j θi,j denote the optimal expected reward and fix arg max di,j θi,j. Define the suboptimality gaps i,j = θ θi,j > 0 for all di,j = d. Under the hierarchical model, DaSH maintains posterior distributions for both the group-level parameters θi and the dataset-level parameters θi,j. Under the Gaussian posterior updates in Eqs. (4) and (7), the corresponding posterior variances λ2 i,j(t) satisfy (t) and λ2 λ2 (t) 0, λ2 i,j(t) 0, so the posterior distributions concentrate around each true θi and θi,j. Thus the probability of selecting suboptimal dataset vanishes, and Pr(cid:0)D(t) = d(cid:1) 1 as . Let RT = (cid:80)T t=1(θ θD(t)) be the cumulative regret. Standard results for Gaussian Thompson Sampling imply that each suboptimal dataset di,j is selected in expectation O(cid:0)(σ2 /2 i,j) log (cid:1) times. Therefore, RT = (cid:88) di,j =d σ2 i,j log , which achieves asymptotically optimal O(log ) regret. Limitations While DaSH is designed for settings where data is organized into discrete, variably relevant datasets grouped by source or origin, our evaluation has focused on publicly available image datasets. This allows for controlled benchmarking but may not capture the full complexity of other data modalities or selection environments. In future work, we plan to extend DaSH to additional domains such as time-series and graph data, where hierarchical structure may arise from different sensors, sources, or collection protocols. Broader Impacts By modeling selection hierarchically, DaSH improves accuracyefficiency trade-offs in realistic deployment scenarios. As such methods become more common, they can streamline large-scale data integration and expand access to high-quality training data. By making dataset selection more robust, efficient, and transparent, DaSH supports ML systems that reflect the constraints and diversity of real-world data ecosystems."
        }
    ],
    "affiliations": [
        "University of Cincinnati",
        "University of Illinois Urbana-Champaign",
        "Virginia Polytechnic Institute and State University"
    ]
}