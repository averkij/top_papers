{
    "paper_title": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis",
    "authors": [
        "Yong Zhong",
        "Zhuoyi Yang",
        "Jiayan Teng",
        "Xiaotao Gu",
        "Chongxuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Concat-ID, a unified framework for identity-preserving video generation. Concat-ID employs Variational Autoencoders to extract image features, which are concatenated with video latents along the sequence dimension, leveraging solely 3D self-attention mechanisms without the need for additional modules. A novel cross-video pairing strategy and a multi-stage training regimen are introduced to balance identity consistency and facial editability while enhancing video naturalness. Extensive experiments demonstrate Concat-ID's superiority over existing methods in both single and multi-identity generation, as well as its seamless scalability to multi-subject scenarios, including virtual try-on and background-controllable generation. Concat-ID establishes a new benchmark for identity-preserving video synthesis, providing a versatile and scalable solution for a wide range of applications."
        },
        {
            "title": "Start",
            "content": "Concat-ID: Towards Universal Identity-Preserving Video Synthesis Yong Zhong1* Zhuoyi Yang2 Jiayan Teng2 Xiaotao Gu3 Chongxuan Li1 1 Gaoling School of AI, Renmin University of China, Beijing, China 2 Tsinghua University 3 Zhipu AI yongzhong@ruc.edu.cn, {yangzy22,tengjy24}@mails.tsinghua.edu.cn, xiaotao.gu@zhipuai.cn, chongxuanli@ruc.edu.cn Project page and code: https://ml-gsai.github.io/Concat-ID-demo/ 5 2 0 2 8 1 ] . [ 1 1 5 1 4 1 . 3 0 5 2 : r Figure 1. Concat-ID produces natural videos for identity-preserving video generation. We select samples for (a) single-identity, (b) multi-identity, and (c) multi-subject scenarios, respectively."
        },
        {
            "title": "Abstract",
            "content": "We present Concat-ID, unified framework for identitypreserving video generation. Concat-ID employs Variational Autoencoders to extract image features, which are concatenated with video latents along the sequence dimension, leveraging solely 3D self-attention mechanisms without the need for additional modules. novel cross-video pairing strategy and multi-stage training regimen are introduced to balance identity consistency and facial editability while enhancing video naturalness. Extensive experiments demonstrate Concat-IDs superiority over existing methods in both single and multi-identity generation, as well as its seamless scalability to multi-subject scenarios, including virtual try-on and background-controllable generation. Concat-ID establishes new benchmark for identity-preserving video synthesis, providing versatile and scalable solution for wide range of applications. 1. Introduction Identity-preserving video generation, which seeks to create human-centric videos of specific identity accurately matching user-provided face image, has recently gained significant attention, as evidenced by the success of commercial tools such as Vidu [23] and Pika [19]. primary challenge in this field is achieving balance between maintaining identity consistency and enabling facial editability. Prior work [9, 13, 28, 30] fails to effectively preserve identity despite utilizing special face encoders and incorporating extra adapters to mitigate cross-modal disparities. To mitigate this limitation, some approaches [4, 29] substitute the spatially aligned reference image in pre-trained image-to-video models [2, 27] with facial images, leading to significant improvement in identity consistency. However, they still face challenges in preventing the replication of facial expressions from the reference image. Moreover, the supplementary modules and parameters introduced by these methods contribute to increased complexity in both model training and inference. Furthermore, academic research on identity-preserving video generation that incorporates multiple identities and subjects remains scarce, leaving it notably behind commercial applications [19, 23]. In this work, we introduce Concat-ID, concise, effective, and versatile framework for identity-preserving video generation. By unifying the model architecture, data processing, and training procedure, Concat-ID not only achieves single-identity video generation but also seamlessly integrates multiple identities and accommodates diverse subjects. Specifically, Concat-ID employs Variational *Work done during the internship at Zhipu. Correspondence to Chongxuan Li. Autoencoders (VAEs) to extract image features, which are then concatenated with video latents along the sequence dimension. This approach leverages solely 3D self-attention mechanisms, which are inherently present in state-of-the-art video generation models, thereby eliminating the need for extra modules and parameters. Furthermore, to effectively balance identity consistency and facial editability while enhancing video naturalness, we develop novel cross-video pairing strategy and multi-stage training regimen. The quantitative and qualitative results, along with the user study (see Sec. 5.2), demonstrate that Concat-ID produces videos with the most consistent identity and superior facial editability across all baselines, for both singleidentity and multi-identity video generation. Moreover, we illustrate that Concat-ID can seamlessly extend to multisubject scenarios, including virtual try-on and backgroundcontrollable generation, while effectively preserving identity (see Sec. 5.3). These findings underscore Concat-IDs capability to scale effectively to diverse subjects, ensuring consistent high performance across various applications. The principal contributions of this work are as follows: We propose Concat-ID, an effective framework for unified identity-preserving video generation across singleidentity, multi-identity, and multi-subject scenarios. Concat-ID uses VAEs to extract image features, which are concatenated with video latents along the sequence dimension, relying solely on 3D self-attention mechanisms. We develop cross-video pairing strategy and multistage training regimen to balance identity consistency and facial editability, while enhancing video naturalness. Concat-ID demonstrates superior identity consistency and facial editability in single and multi-identity scenarios, and seamlessly scales to multi-subject scenarios. 2. Related works The rapid advancement of text-to-video and image-to-video diffusion models [8, 17, 20, 27, 31] has spurred significant interest in fine-tuning these models for downstream tasks, particularly identity-preserving video generation. Tuningbased methods [11, 18] adapt pre-trained video models for each new identity through test-time fine-tuning. Alternatively, tuning-free methods [9, 13, 28, 30] typically leverage face encoders [3, 21] to extract facial features and incorporate additional adapters to mitigate cross-modal discrepancies. Some approaches [4, 25, 29] further enhance identity consistency by integrating face features extracted from Variational Autoencoder (VAE). For instance, ConsisID [29] and Ingredients [4] replace spatially aligned reference images in pre-trained image-to-video models for single-identity and multi-identity generation, respectively. Placing greater emphasis on enhancing video naturalness, Movie-Gen [20] refines the balance between identity consistency and facial editability for single-identity generation 2 4. Concat-ID Given reference image containing human face, our goal is to generate identity-preserving videos based on userprovided text prompts, while also enabling the integration of additional identities or subjects. To address this challenge, we propose Concat-ID, concise, effective, and versatile framework. As illustrated in Fig. 2, we introduce unified architecture for extracting and injecting features from any number of identities and subjects without requiring extra modules and parameters (see Sec. 4.1). To balance identity consistency and facial editability while enhancing video naturalness, we further construct cross-video pairs as training data (see Sec. 4.2) and propose novel multi-stage training strategy (see Sec. 4.3). 4.1. unified architecture We focus on designing unified model architecture for identity feature fusion that is compatible with state-of-theart text-to-video and image-to-video models, and readily extended to multi-identity and multi-subject scenarios. Revisiting the role of VAEs, we recognize their ability to compress conditioning images into the same latent space as Z. Consequently, unlike prior works [9, 13, 28, 30] that introduce extra modules to mitigate distribution differences between image features and Z, our denoising transformer ϵθ can inherently interpret these features due to pre-training on related videos and images. Based on this insight, we adopt the VAE as our feature extractor. Specifically, for reference images {Ii}M i=1, we encode each Ii to obtain the image feature ci = E(Ii) R1HW C, and then concatenate these features with in sequence. Thus, the input to ϵθ is given by: = Concat(Z, c1, c2, , cM ), (1) where Concat(, , ) denotes concatenation along the sequence dimension and R(N +M )HW C. As shown in Fig. 2, this feature injection through concatenation is compatible with any video generation model that utilizes 3D self-attention, which are generally present in state-of-theart video generation models. Since and ci are in the same latent space, ϵθ can seamlessly integrate identity-preserving features without the need for additional modules and parameters to address cross-modal disparities. Concatenating and ci along the channel dimension is another direct method for feature injection, as employed in ConsisID [29] and Ingredients [4]. However, this strategy introduces artifacts (see Fig. 4 and Fig. 5) due to spatial misalignment between face images and video latents. In contrast, by leveraging 3D self-attention mechanism, our sequence concatenation promotes spatial interactions without compromising the quality of any generated frame. Furthermore, it scales efficiently to handle multi-identity and multisubject scenarios (see Fig. 1). For instance, when = 2, Figure 2. The architecture of Concat-ID. We utilize VAE to extract image latents from reference images and concatenate them at the end of the video latents along the sequence dimension. ConcatID relies solely on 3D self-attention, which are commonly present in state-of-the-art video generation models, without introducing additional modules and parameters. through cross-paired data construction. In this work, we explore unified framework capable of handling singleidentity, multi-identity, and multi-subject generation while maintaining crucial balance between consistency and editability, without requiring test-time fine-tuning. 3. Preliminary Existing state-of-the-art text-to-video and image-to-video models [8, 15, 17, 20, 27] generally consist of three main components: 3D variational autoencoder (VAE) E, text encoders , and denoising transformer ϵθ. Given video = {xi}N i=1 with frames, initially compresses the video into latent representation RT HW along the spatiotemporal dimensions, where HW denotes the spatial dimension, represents the channel dimension, and is the temporal dimension. To simplify, we refer to HW as the sequence dimension. The ϵθ then takes the noisecorrupted latent representation as its input, and applies 3D (spatiotemporal) self-attention mechanism [8, 27] to model the distribution of video content. Additionally, 3D relative positional encoding (i.e., 3D-ROPE) is incorporated within the 3D attention module to enhance the models ability to capture both temporal and spatial dependencies in videos. Meanwhile, the text encoder processes the text prompt and encodes it into text representation ctxt. ϵθ typically integrates ctxt either through cross-attention layers [20] or by concatenating it with [27]. mean squared error loss [5, 32] is commonly used to optimize ϵθ. 3 compute the ArcFace cosine similarity [3] between consecutive frames and discard videos if more than 30% of the frames have similarity score below 0.5. The above processes yield 1.3 million videos featuring single identity, and we uniformly select 5 face images per video, defining pre-training pairs Spre = {(Ik , Xk)}i,k where denotes the video index and Ik represents the i-th reference image of Xk. The self-supervised nature of this paired data, where images from the same video serve as labels, inherently limits facial editability. Specifically, models trained on such data may produce frames in which facial expressions unintentionally mirror those of the reference images (see Fig. 7), leading to unnatural content. This issue becomes particularly pronounced when semantic gap exists between the reference images and the text prompts. To enhance facial editability and naturalness, we propose cross-video image-video pairing strategy. Cross-video pairs. The standard process for constructing video clips involves segmenting raw long videos into multiple shorter segments using various algorithms that detect scene transitions, such as motion variations and shot changes. Theoretically, many existing video clips in training sets feature varied facial expressions and head poses of the same person. To construct cross-video pairs where the reference image originates from different video, we calculate the cosine similarity among images {Iv 1}v. For the k-th video, we randomly select an image Ij 1 from {Iv 1}v as its paired reference image, ensuring that 0.7 cos(Ij 1, Ik 1) < 0.9, where the function cos(, ) computes the cosine similarity. The final cross-video pairs Scross include 0.8 million image-video pairs with 0.5 million reference images, indicating reference image can correspond to multiple videos. Personalized image generation can also synthesize reference images with the same identity as given videos but varied facial expressions, as demonstrated in [10, 20]. However, this approach incurs high computational costs, particularly for large-scale image-video pairs. Additionally, existing personalized generation methods [7, 28, 33] often struggle to preserve detailed facial features, which limits their effectiveness. In contrast, as shown in Fig. 3b, our retrieval-based method efficiently gathers large-scale set of real reference images that accurately match the identity of corresponding videos while exhibiting diversity across multiple dimensions, such as facial expressions, hairstyles, lighting conditions, and other identity-irrelevant factors. Trade-off pairs. Similar to the construction of cross-video pairs, for the k-th video, we identify its reference image Ij 1, Ik 1), ensuring that 0.9 cos(Ij 1) < 0.99. This forms our trade-off dataset Strade with 160 thousand videos, improving consistency between reference images and videos compared to cross-video pairs. Additionally, we filter out videos where the facial region oc1 with the smallest cos(Ij 1, Ik (a) The procedure of data processing. (b) Some samples of paired cross-video reference images. (c) Some samples of trade-off pairs. Figure 3. Constructing three types of image-video pairs for single identity: pre-training, cross-video and trade-off pairs. where I1 is face image and I2 represents clothing, our method can extract clothing features c2 using and achieve virtual try-on while preserving the given identity I1. 4.2. Data construction The task of identity-preserving video generation relies on image-video pairs as training data, where an image must depict human face that matches the identity of corresponding videos. To progressively balance identity consistency and facial editability, as illustrated in Fig. 3, we construct three types of image-video pairs for single identity: pre-training pairs Spre, cross-video pairs Scross, and trade-off pairs Strade. Pre-training pairs. To ensure data quality, we filter out videos that are unrelated to humans, contain inconsistent numbers of individuals, or exhibit inconsistencies in identity. Specifically, to retrieve human-related videos from the caption-video pairs, we design human term table that includes various categories such as basic human descriptors, gender, and occupation. We then exclude videos whose captions do not contain any human-related terms. Next, we uniformly sample two frames per second from each video, detect faces using SCRFD [6], and remove videos if more than 30% of the frames have inconsistent numbers of individuals.1 Finally, for frames with the same face count, we 1The common person count across frames is considered the videos person count. cupies less than 4% or more than 90% of the frame area and rank Strade based on the weighted sum of aesthetics scores, optical flow scores, and motion scores [27]. Finally, we retain the top 50,000 videos for training. Notably, trade-off pairs are not strictly necessary and we introduce them to achieve an optimal balance between identity consistency and facial editability. As shown in Fig. 3c, they exhibit greater similarity than cross-video pairs, which can enhance identity consistency in trained models. Moreover, thresholds for cross-video pairing are set empirically. In this section, we detail the data construction process for single identity. However, this procedure can be seamlessly scaled to multi-identity by independently processing each identity within video. Similarly, it can be extended to general subjects by replacing face detectors with openset detectors, such as Grounding DINO [22], and substituting ArcFace cosine similarity with general feature similarity metrics, such as CLIP cosine similarity [21]. Please refer to Appendix for further details on the training data construction for our multi-identity and multi-subject scenarios. 4.3. Training strategy Building on our innovative data construction, we introduce multi-stage training process: pre-training stage, crossvideo fine-tuning, and trade-off fine-tuning. In the pretraining stage, we optimize text-to-video model on Spre to map facial details into generated videos. This selfsupervised training method may constrain certain generated video frames to adhere strictly to the given condition images, potentially degrading the editability of facial expressions and the overall naturalness. The cross-video finetuning on Scross, using image-video pairs derived from different videos, can alleviate this issue. However, we observe that this fine-tuning enhances facial editability at the expense of identity fidelity (see Sec. 5.4). To balance fidelity and editability while ensuring high-degree motion and high artistic quality, we ultimately fine-tune the model on Strade. Throughout all training stages, we proportionally scale, pad, and center-crop images to match the video resolution. To ensure the model focuses on facial regions during training and prevents background leakage during inference, we segment and drop the background of reference images [26]. Additionally, to improve robustness and generalization, we introduce random noise to reference images during training, while omitting this noise during inference. To further differentiate the image latent ci from the video latent and distinguish between different ci, we extend 3D-RoPE to incorporate multiple reference images along the sequence dimension. Owing to the simplicity and efficiency of ConcatID in both data construction and model architecture, our training strategy can seamlessly scale to multi-identity and multi-subject scenarios. Moreover, we establish that singleidentity pre-training facilitates enhanced identity preservation in these downstream tasks (see Tab. 2). 5. Experiments 5.1. Experimental settings Datasets. We evaluate all methods on the ConsistIDBenchmark [29], which consists of 172 reference images and 90 text prompts spanning nine categories. To ensure fair comparison, we exclude reference images present in our training data using combination of automated and manual filtering techniques. Consequently, our evaluation dataset comprises 873 prompt-image pairs, derived from 97 reference images, with one prompt randomly selected from each category for each image. For multi-identity evaluation, we additionally construct 14 distinct pairs of reference images and design 20 textual prompts using ChatGPT [1]. Please refer to Appendix A.1 for further details. Metrics. We evaluate all methods on identity consistency, text alignment, and facial editability. (1) Identity consistency: Following [29], we use FaceSim-Arc (ArcSim) and FaceSim-Cur (CurSim) to assess the average cosine similarity between reference images and generated videos based on ArcFace [3] and CurricularFace [12], respectively. These face recognition models are specifically designed to disentangle identity-related features from identity-unrelated ones. (2) Text alignment: We adopt ViCLIP [24] to compute the similarity between text prompts and generated videos, following [14, 20]. (3) Facial editability: We calculate the cosine distance of CLIP image embeddings [21] (CLIPDist) between reference images and video frames. CLIP effectively captures comprehensive facial features, and thus larger CLIPDist indicates improved facial editability. Implementation details. We use the text-to-video model CogVideoX-5B [27] as our base model. The learning rates are set to 1.0105, 5.0106, and 5.0106 for the first, second, and third training stages, respectively. We finetune all model parameters with linear learning rate decay across all stages. The training data resolution is maintained at 480 720 pixels with 49 frames per video. Text and image prompts are independently dropped with probability of 0.1. Further details are provided in Appendix A.2. Baselines. For comprehensive comparison, we use three representative open-source approaches as baselines. (1) Single-Identity personalization methods: ID-Animator [9] and ConsisID [29]. (2) Multi-Identity personalization methods: Ingredients [4]. ID-Animator, ConsisID, and Ingredients all incorporate additional adapters and auxiliary loss functions to enhance identity consistency. Notably, to the best of our knowledge, Ingredients [4] is the only available open-source method for multiple identities, and no opensource methods currently exist for multiple subjects. Method Identity consistency Text alignment Facial editability ArcSim CurSim ViCLIP CLIPDist Single identity ID-Animator [9] ConsisID [29] Concat-ID (Ours) 0.289 0.432 0.442 0.304 0.451 0.466 Multiple identities Ingredients [4] Concat-ID (Ours) 0.293 0. 0.316 0.514 0.204 0.237 0.242 0.199 0.190 0.297 0.303 0.325 0.407 0.410 Table 1. Quantitative results for single-identity and multiidentity generation. indicates corresponding methods introduce additional adapters and auxiliary loss. Concat-ID achieves superior identity consistency and facial editability while maintaining better or comparable text alignment relative to the baselines. 5.2. Main results We demonstrate the effectiveness of Concat-ID through quantitative metrics, qualitative assessments, and the user study for single-identity and multi-identity generation. Quantitative comparisons. Table 1 presents the quantitative results for single-identity and multi-identity generation. For single-identity generation, ID-Animator performs the worst, exhibiting the lowest ArcSim, CurSim, and CLIPDist scores. This suggests that it achieves the least effective balance between identity preservation and facial editability. Moreover, ID-Animator, ConsisID, and Ingredients incorporate additional adapters and auxiliary loss functions to enhance identity consistency, increasing the complexity of both training and generation processes. In contrast, for both single-identity and multi-identity generation, Concat-ID achieves superior identity consistency simply by concatenating image latents after video latents, highlighting the effectiveness of our architecture. Furthermore, by constructing cross-video pairs, Concat-ID attains higher CLIPSim score than ID-Animator, ConsisID, and Ingredients, demonstrating an optimal balance between identity preservation and facial editability. Qualitative comparisons. Fig. 4 presents qualitative comparisons for single-identity generation. ID-Animator fails to maintain facial characteristics. ConsisID achieves better identity consistency, but some frames replicate facial expressions of reference images. In contrast, Concat-ID mitigates this issue while preserving identity by leveraging advantages of cross-video pairs. For multi-identity generation, as shown in Fig. 5, Concat-ID produces videos that more accurately match identities in given images compared to Ingredients, demonstrating its effectiveness and scalability. To maximize the potential of image-to-video models, ConsisID and Ingredients concatenate the reference image with the first latent frame along the channel dimension. However, this feature injection approach can introduce artifacts in the first generated frame due to spatial misalignment between faces images and generated videos, as evident in the initial frames of all videos. As comparison, ConcatID excels in identity preservation without compromising the quality of any generated frames, highlighting the validity of our concatenation along the sequence dimension. User study. According to both quantitative and qualitative results, we compare Concat-ID with the strongest baseline, ConsisID, through human evaluation. Specifically, we generate 100 videos using 10 reference images and 10 prompts designed by ChatGPT [1] to focus on expression and head pose variation. For each video group, voters answer three questions, selecting the video that: (1) best matches the reference image in facial similarity (identity consistency), (2) best aligns with the facial expressions and head poses described in the prompt (facial motion alignment), and (3) exhibits the most natural and smooth facial motion (facial motion naturalness). With 100 video groups, three types of questions, and three voters participating, we collect total of 900 video comparison results. As shown in Fig. 6, Concat-ID surpasses ConsisID by significant margin in identity consistency and motion alignment and naturalness, demonstrating the effectiveness of our architecture and the advantages of cross-video pair construction. 5.3. Multiple identities and subjects The architecture, data construction, and training strategy of Concat-ID allow for its seamless extension to multi-identity and multi-subject scenarios, sustaining good performance. Multi-identity scenarios. As illustrated in Fig. 1b, when provided with face images of different individuals, ConcatID can generate multi-person videos while preserving their identities, without requiring any additional parameters or modules compared to single-identity generation. Notably, despite being trained on only 40,000 videos, Concat-ID can generate three-identity videos while maintaining distinct identities, leveraging the prior knowledge from twoidentity pre-training and powerful 3D self-attention mechanism that effectively captures both temporal and spatial dependencies. Moreover, Concat-ID determines the spatial position of each identity in the generated videos based on the concatenation sequence of the reference images. Multi-subject scenarios. As illustrated in Fig. 1c, by sequentially concatenating face image with clothing image, Concat-ID enables virtual try-on while preserving both the given identity and intricate clothing details, such as logos and textures. This capability also highlights ConcatIDs potential in simulating interactions between people and objects. Furthermore, the background-controllable identitypreserving generation achieved by Concat-ID demonstrates its ability to manipulate spatial layouts in generated videos by integrating spatially aligned conditions. In this section, we introduce two-identity and threeidentity generation, along with two additional subjects (i.e. clothing and background). Further details on training and 6 Figure 4. Qualitative comparisons for single-identity generation. ID-Animator fails to preserve facial details, while ConsisID replicates the expressions of the reference images, particularly in the third case, where the semantic gap between texts and reference is significant. Concat-ID effectively preserves identity, while simultaneously preventing the direct replication of facial expressions from reference images. data are provided in Appendix B. We posit that ConcatIDs architecture, characterized by its simplicity and effectiveness, coupled with the generalizability of its data construction and training strategy, enables effective scalability to more identities and diverse subjects, ensuring consistent high performance across wider range of applications. 5.4. Ablation study Fig. 7 present the qualitative ablation of Concat-ID. The pre-training stage achieves the best identity consistency but results in low facial editability. For example, facial expressions of some frames in the pre-training stage closely resemble those in reference images. However, the crossvideo stage enhances editability at the expense of identity consistency, aligning with the findings in [20]. In the third stage, Concat-ID further refines the matching threshold of cross-video pairs to better balance identity preservation and facial editability. Leveraging prior knowledge from both pre-training and cross-video fine-tuning, the trade-off stage achieves an optimal balance using only 50,000 videos. 7 Figure 7. Qualitative ablation. Stage I, Stage II, and Stage III indicate the pre-training stage, cross-video stage, and trade-off stage. Method IdentityIdentity-2 ArcSim CurSim ArcSim CurSim No single-identity pre-training Concat-ID (Pre-training) 0.514 0.629 0.535 0.650 0.526 0. 0.550 0.674 Table 2. The effect of single-identity pre-training on multiidentity pre-training. The single-identity pre-training enhances identity consistency in downstream tasks. our training strategy. Moreover, the quantitative analysis in Appendix consistently supports our findings. We also investigate the influence of single-identity pretraining on multi-identity and multi-subject pre-training. Specifically, we conduct comparative analysis of ConcatID with and without single-identity pre-training. Although the two-identity generation is pre-trained on approximately 0.3 million videos, as presented in Tab. 2, single-identity pre-training still results in improved ArcSim and CurSim scores across all identities. This enhancement indicates that single-identity pre-training effectively strengthens identity preservation in downstream tasks. These findings provide empirical support for the scalability of our architecture, data construction methodology, and training strategy. 6. Conclusions In this paper, we introduce Concat-ID, unified framework for identity-preserving video generation. Concat-ID relies solely on 3D self-attention mechanisms, which are commonly used in state-of-the-art video generation models, without introducing additional modules. We also present novel cross-video pairing strategy and multi-stage training regimen to balance identity consistency and facial editability while enhancing video naturalness. Thanks to its arFigure 5. Qualitative comparisons for multi-identity generation. Concat-ID better maintains different identities. Figure 6. Human evaluation. Concat-ID produces more precise and natural videos while effectively preserving identity."
        },
        {
            "title": "These results underscore the effectiveness of each stage in",
            "content": "8 chitecture, data construction, and training strategy, ConcatID achieves effective video synthesis across single-identity, multi-identity, and multi-subject scenarios. Limitations. Due to limitations in the base models capabilities, we do not compare our method with closed-source commercial tools. Currently, we utilize VAEs solely as feature extractors, relying on the models inherent ability to process low-level features. Similar to common video generation models, our approach faces challenges in preserving the integrity of human body structures, such as the number of fingers, when handling particularly complex motions."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 5, 6 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [3] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 2, 4, 5 [4] Zhengcong Fei, Debang Li, Di Qiu, Changqian Yu, and Ingredients: Blending custom phoarXiv preprint Mingyuan Fan. tos with video diffusion transformers. arXiv:2501.01790, 2025. 2, 3, 5, 6, 12 [5] Ruiqi Gao, Emiel Hoogeboom, Jonathan Heek, Valentin De Bortoli, Kevin P. Murphy, and Tim Salimans. Diffusion meets flow matching: Two sides of the same coin. 2024. [6] Jia Guo, Jiankang Deng, Alexandros Lattas, and Stefanos Zafeiriou. Sample and computation redistribution for efficient face detection. arXiv preprint arXiv:2105.04714, 2021. 4 [7] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, Peng Pulid: Pure and lightning id arXiv preprint Zhang, and Qian He. customization via contrastive alignment. arXiv:2404.16022, 2024. 4 [8] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime arXiv preprint arXiv:2501.00103, video latent diffusion. 2024. 2, 3 [9] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 2, 3, 5, 6, 12 [10] Zecheng He, Bo Sun, Felix Juefei-Xu, Haoyu Ma, Ankit Ramchandani, Vincent Cheung, Siddharth Shah, Anmol Kalia, Harihar Subramanyam, Alireza Zareian, et al. Imagine yourself: Tuning-free personalized image generation. arXiv preprint arXiv:2409.13346, 2024. [11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 2 [12] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: adaptive curriculum learning loss for deep In proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 59015910, 2020. 5 [13] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. 2, 3 [14] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 5 [15] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 3 [16] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in neural information processing systems, 33:1210412114, 2020. 12 [17] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, [18] Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. arXiv preprint arXiv:2402.09368, 2024. 2 [19] Pika. Pikascenes. https : / / pika . art / ingredients, 2024. 2 [20] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2, 3, 4, 5, [21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 5 [22] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance arXiv preprint the edge of open-set object detection. arXiv:2405.10300, 2024. 5 [23] Vidu. Reference to video. https://www.vidu.com/ create/character2video, 2024. 2 [24] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 5 [25] Tao Wu, Yong Zhang, Xiaodong Cun, Zhongang Qi, Junfu Pu, Huanzhang Dou, Guangcong Zheng, Ying Shan, and Xi Li. Videomaker: Zero-shot customized video genera10 tion with the inherent force of video diffusion models. arXiv preprint arXiv:2412.19645, 2024. 2 [26] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems, 34: 1207712090, 2021. 5 [27] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 5 [28] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 3, 4 [29] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. 2, 3, 5, 6, [30] Yuechen Zhang, Yaoyang Liu, Bin Xia, Bohao Peng, Zexin Yan, Eric Lo, and Jiaya Jia. Magic mirror: Id-preserved arXiv video generation in video diffusion transformers. preprint arXiv:2501.03931, 2025. 2, 3 [31] Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, and Jun Zhu. Riflex: free lunch for length extrapolation in video diffusion transformers. arXiv preprint arXiv:2502.15894, 2025. 2 [32] Yong Zhong, Min Zhao, Zebin You, Xiaofeng Yu, Changwang Zhang, and Chongxuan Li. Posecrafter: One-shot personalized video synthesis following flexible pose control. In European Conference on Computer Vision, pages 243260. Springer, 2024. 3 [33] Yufan Zhou, Ruiyi Zhang, Kaizhi Zheng, Nanxuan Zhao, Jiuxiang Gu, Zichao Wang, Xin Eric Wang, and Tong Sun. Toffee: Efficient million-scale dataset construction for subject-driven text-to-image generation. arXiv preprint arXiv:2406.09305, 2024."
        },
        {
            "title": "Supplementary material",
            "content": "A. Experimental settings A.1. Datasets We remove reference images from the ConsistID-Benchmark that may appear in our training data using both manual and automated filtering methods. (1) Manual filtering: For each reference image in the ConsistID-Benchmark, we compute its cosine similarity with all training images and identify the most similar one. Human evaluators then determine whether the two images depict the same person. If so, all reference images of the corresponding identity are excluded. (2) Automated filtering: All reference images of an identity are discarded if any training image has cosine similarity greater than 0.45 with one of its reference images. A.2. Implementation details In the first stage, we randomly select one reference image from set of five for each video. Traditional data augmentation techniques, such as flipping, are not used for face images, as they can cause data augmentation leakage [16], leading the model to learn the augmented data distribution rather than the original distribution. For instance, horizontal flipping may result in incorrectly mirrored faces in generated videos. For inference, by default, we set the classifier-free guidance scale to 6, 5, and 5 for the first, second, and third stages, respectively. A.3. Baselines We try our best not to change original settings of baselines to maintain their original capabilities. IDAnimator [9] and ConsisID [29] can produce 16-frame and 49-frame videos at resolution of 480 720, respectively. The multi-identity baseline Ingredients [4] generates 49-frame videos at resolution of 480 720, integrating two distinct identities. B. Multiple identities and subjects B.1. Multi-identity scenarios Through the data construction process of pre-training pairs, we obtain approximately 300,000 videos featuring two identities. For each identity, we determine the sequence order by computing the mean horizontal position of face boxes across all reference images. We discard reference images where the face position does not align with the determined sequence order. Next, we construct cross-video pairs by independently processing each identity within video. Finally, we collect around 8,000 videos, each of which contains identities that have corresponding cross-video reference images. similar strategy is used to construct three-identity training data, resulting in final dataset of approximately 40,000 pre-training videos. For cross-video pairs, we retain videos in which at least two identities have corresponding cross-video reference images, resulting in about 2,000 videos. For multiple identities, the pairing cosine similarity ranges between 0.87 and 0.97. We initialize the model using singleidentity pre-training weights and train it only on the first two stages (i.e., the pre-training stage and cross-pair fine-tuning stage). Our findings indicate that single-identity pre-training facilitates multi-identity convergence and enhances identity consistency. B.2. Multi-subject scenarios We select subset from the cross-video pairs of single-identity, comprising approximately 200,000 videos, where the pairing cosine similarity ranges between 0.87 and 0.97. To achieve virtual try-on, we use Grounded-SAM-22 to detect and segment the clothing of identities. For background-controllable generation, we extract the first frame and use Grounded-SAM-2 to obtain human masks. We then apply SDXL3 to inpaint the masked areas to get bacground images, using randomly selected classification label from YOLO4 as input prompts. We use weights from single-identity pre-training as initialization and apply only random horizontal flip augmentation to clothing images. Additionally, we introduce random noise to both the background and clothing images during training. In multi-subject scenarios, we only train models on the cross-pair fine-tuning stage. To maximize model performance, we independently train different specialized models for specific tasks. Developing comprehensive model capable of addressing multiple tasks simultaneously is left for future work. 2https://github.com/IDEA-Research/Grounded-SAM-2 3https://huggingface.co/diffusers/stable-diffusion-xl-1.0-inpainting-0.1 4https://github.com/ultralytics/ultralytics"
        },
        {
            "title": "Text alignment Facial editability",
            "content": "ArcSim CurSim ViCLIP CLIPDist Concat-ID (Stage I) Concat-ID (Stage II) Concat-ID (Stage III) 0.560 0.185 0.442 0.581 0.200 0. 0.237 0.248 0.242 0.726 0.566 0.675 Table 3. Quantitative ablation. Stage I, Stage II, and Stage III indicate the pre-training stage, cross-video stage, and trade-off stage of Concat-ID, respectively. The second-best result is underlined. Concat-ID in the third stage demonstrates the optimal balance. C. Ablation study Tab. 3 presents the quantitative ablation study of Concat-ID. The pre-training stage achieves the best identity consistency (i.e., ArcSim and CurSim) but has the worst facial editability (i.e., CLIPDist ). However, the cross-video stage significantly improves CLIPDist but degrades ArcSim and CurSim. In the third stage, Concat-ID obtains the second-best results across all metrics, demonstrating that it achieves an optimal balance. These results highlight the superiority of our multi-stage training strategy, which balances the knowledge learned in different stages to achieve optimal performance in the final stage."
        }
    ],
    "affiliations": [
        "Gaoling School of AI, Renmin University of China, Beijing, China",
        "Tsinghua University",
        "Zhipu AI"
    ]
}