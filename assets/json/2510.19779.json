{
    "paper_title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders",
    "authors": [
        "Yuezhou Hu",
        "Jiaxin Guo",
        "Xinyu Feng",
        "Tuo Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at https://github.com/yuezhouhu/adaspec."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 9 7 7 9 1 . 0 1 5 2 : r AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders Yuezhou Hu1, Jiaxin Guo2, Xinyu Feng3, Tuo Zhao3 1 University of California, Berkeley 2 Tsinghua University 3Georgia Institute of Technology yuezhouhu@berkeley.edu jx-guo21@mails.tsinghua.edu.cn {xfeng300,tourzhao}@gatech.edu"
        },
        {
            "title": "Abstract",
            "content": "Speculative Decoding (SD) accelerates large language model inference by employing small draft model to generate predictions, which are then verified by larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target models knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15%). The code is publicly available at https://github.com/yuezhouhu/adaspec."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have revolutionized natural language processing, achieving impressive performance across wide range of tasks. Models like GPT-4 [25] and Llama 3 [11] demonstrate state-of-the-art results in various natural language understanding and generation tasks [2, 27], including highly complex tasks such as summarization [23] and mathematical reasoning [9, 13]. However, as these models grow in size and complexity, their inference becomes increasingly computationally intensive, leading to practical challenges in deployment, including slow generation speeds and significant output latency. To address these shortcomings, current approaches primarily focus on achieving trade-off between efficiency and performance through two main strategies. The first involves compressing the model scale to enhance the capability of smaller models, often using techniques like Knowledge Distillation (KD) [14]. The second approach employs methods such as quantization to enable faster computation. However, these strategies inevitably lead to sacrifice of performance, either due to loss of representational capacity during compression or reduced accuracy resulting from optimization for *Equal Contribution. Work done during an internship at Georgia Tech. Corresponding author. Preprint. speed. As result, there is growing need for methods that can maintain the high performance of LLMs while significantly improving their inference efficiency. Recently, Speculative Decoding (SD) [16, 7] has emerged as promising paradigm for accelerating LLM inference without sacrificing performance. Unlike model compression or quantization, which modify the model architecture or parameters, SD accelerates generation by restructuring the decoding process itself. Specifically, it introduces lightweight draft model that speculatively generates multiple candidate tokens, which are then verified by the larger target model. This paradigm preserves the target models predictive quality while substantially reducing the number of expensive forward passes, offering new efficiencyperformance trade-off. The core of SD lies in the design of the draft model. This model is typically much smaller than the target model, even ranging from one-tenth to one-hundredth of the size, enabling faster token generation while maintaining certain level of capability. Consequently, the actual inference speed-up achieved by SD relies on the draft model closely aligning its predictions with the target models output distribution. Typically, this alignment is achieved by pre-training and fine-tuning both models on the same datasets, yielding pair of homogeneous models from the same family, sharing the same architecture but differing in size. However, training two models on the same datasets does not necessarily produce optimal alignment, especially given the significant scale disparity between the draft and target models. This difference in scale makes the draft model prone to prediction errors. To address this challenge, state-of-the-art methods employ KD techniques to refine the draft model, rather than relying solely on direct fine-tuning [32]. However, optimizing fidelity metric (e.g., forward KL divergence) does not necessarily lead to high acceptance rate. Worse still, it may waste the draft models limited capacity on tokens that are inherently hard to learn and unlikely to be accepted anyway. Additionally, these methods may encounter issues such as the loss failing to converge. Given these challenges, there is critical need for SD-specific training regimes that effectively balance model capacity constraints with prediction accuracy requirements. Fortunately, we observe substantial variation in the difficulty of learning individual tokens during KD, which has critical implications for transferring knowledge from the teacher (target) model to the student (draft) model. Instead of mimicking the full output distribution of the target model, the draft model only needs to produce correct predictions on the subset of tokens that is easy enough to propose. During the process of distillation, we identify subset of hard tokens that pose particular challenges for the student model to learn and to predict accurately, regardless of training efforts. Conversely, other tokens are relatively easy to assimilate. We argue that uniformly emphasizing the loss on both easy and hard tokens may be counterproductive. Attempting to reduce the loss on difficult tokens often comes at the expense of increasing the loss on easy tokens, resulting in suboptimal learning across both categories. To address this issue, we propose novel approach: deliberately excluding hard tokens from the training process. By focusing the loss function exclusively on easy tokens, we can more effectively utilize the limited capacity of the student model, thus achieving better alignment with the teacher model on these tokens. This strategic exclusion of hard-to-learn tokens allows the student model to concentrate its resources on mastering the more accessible aspect of the teachers knowledge, potentially leading to improved overall performance in SD tasks. Our approach thus maximizes the alignment between the draft and target models within the constraints of the draft models capacity. In this study, we propose AdaSPEC, novel Knowledge Distillation method designed to bridge the capacity gap between the draft and target models in SD. AdaSPEC operates in two phases: 1: Reference Model Distillation and Token Filtering: reference model, initialized as copy of the draft model, is distilled using the target model as its teacher. For simplicity, we assume that the target model has been well fine-tuned to downstream tasks of our interests. Here the reference model serves crucial role as token filter. It identifies hard tokensthose that are difficult for smaller models to predict accuratelyby comparing the perplexity differences between the reference and draft models on the training data. 2: Selective Draft Model Distillation: Finally, the draft model undergoes distillation using filtered dataset. The reference model removes the previously identified hard tokens, allowing the draft model to focus its limited capacity on learning to predict the remaining, more manageable tokens accurately. 2 We conduct extensive experiments on wide range of models and downstream tasks, where we benchmark AdaSPEC against DistillSpec and find that AdaSPEC sucessfully pushes the limit of SDacross all tasks and model setups, AdaSPEC consistently achieves higher acceptance rates (up to 15%; see Table 1)."
        },
        {
            "title": "2 Preliminaries",
            "content": "In this section, we provide formal overview of the foundational concepts. We begin with the mathematical framework of SD, followed by description of various evaluation metrics. Finally, we explore language model families and their significance in enabling techniques like SD to bridge performance gaps between models of different sizes. Speculative Decoding. Speculative Decoding [7, 16, 29, 20, 32, 6, 17, 18, 30, 6, 21, 26, 32, 20] is originally proposed to accelerate LLM inference by employing compact draft model to predict potential output sequences in advance and then verified by larger target model. The typical framework of SD is formulated as follows. Let Mp and Mq denote the large target model and the compact draft model, respectively. SD leverages the draft model to autoregressively generate γ tokens {zi}γ i=1 qθ( x) based on the input = [x1, x2, . . . , xt], which includes the prompt and previously generated tokens. The target model then verifies these proposed tokens by evaluating their probabilities {p(zi x, z<i)}γ Both models generate probability distributions p(zi+1 x, z<i) and q(zi+1 x, z<i) for each token = 1, . . . , γ in single forward pass. Using greedy decoding strategy, only the tokens with the highest probabilities are selected for generation or verification. The sampling functions are: i=1 in parallel. Sp(z<i) = arg max zi+1 Sq(z<i) = arg max zi+1 for each = 1, . . . , γ. The complete sampling and verification process is detailed in Appendix A.1 p(zi+1 x, z<i), q(zi+1 x, z<i), (1) (2) Acceptance Rate. The acceptance rate, α, measures the accuracy of the draft model Mq compared to the target model Mp. It is calculated as: α = accept accept + reject . (3) Here, accept and reject are the count of tokens accepted and rejected by Mp, respectively. higher α indicates greater alignment between Mp and Mq, facilitating faster inference in practical scenarios. Block Efficiency and Wall-time Improvement. Block efficiency [7, 16], τ , quantifies the average number of tokens generated per iteration. It is defined as the expected number of accepted tokens per block, with maximum value of γ + 1 for block size of γ. The block efficiency can also be expressed in terms of the acceptance rate α [16]: τ (x) = 1 αγ+1 1 α . (4) This metric evaluates how effectively Mq approximates Mp. The speed-up factor for the total wall-time is given by: Speed-up = τ (x) γc + 1 , (5) where is the cost coefficient, representing the ratio of the time taken by single execution of Mq to that of Mp. Language Model Families. Modern language models are often developed as part of family of models that share the same core architecture but differ in scale, typically measured by the number of parameters or the size of the training dataset. These families, such as Llama 3 [11], BERT [10], and Pythia [5], are designed to enable researchers and practitioners to balance computational efficiency and performance based on specific use cases. Within family, smaller models are generally used for tasks requiring faster inference or lower computational cost, while larger models are leveraged for tasks demanding higher accuracy and richer representations. This structural consistency within family allows for techniques like Knowledge Distillation [14] and Speculative Decoding [7, 16] to transfer knowledge or align predictions effectively between models of varying sizes."
        },
        {
            "title": "3 Method",
            "content": "We introduce AdaSPEC, an adaptive distillation framework for SD that enhances the alignment between target model and smaller draft model through selective Knowledge Distillation. Given target model Mp fine-tuned for specific downstream task, AdaSPEC consists of two key steps: (1) constructing reference model Mref and (2) selectively distilling knowledge from Mp and Mref to the draft model Mq. Step 1: Constructing the Reference Model. The reference model Mref is constructed by distilling Mp on downstream task dataset using the DistillSpec framework [32]. The objective is to minimize the forward KL divergence between the target model and the reference model: LKD = ExD,yP (yx) [K (P (yx)R(yx))] , (6) where represents the input prefix, denotes the generated context, denotes the forward KL divergence, (yx) represents the probability distribution of the target model, and R(yx) corresponds to the probability distribution of the reference model. Step 2: Selective Knowledge Distillation for the Draft Model. To identify learnable tokens for the draft model Mq, we compute token-wise losses based on the predicted distributions of Mref and Mq. Specifically, for each token w, the token-wise KL divergence losses are computed as: Lref (w) = (P (w context)R(w context)) , Ldraft(w) = (P (w context)Q(w context)) , (7) (8) where Q(w context) is the probability predicted for token by the draft model, given the context. Next, we calculate the difference in token-wise losses: L(w) = Ldraft(w) Lref (w). (9) Tokens with higher L(w) represent larger performance gap between Mq and Mref relative to Mp, suggesting that these tokens are not yet well aligned but are highly learnable for the draft model. Accordingly, we select the subset of tokens with larger L(w) values, as they are most promising for improving the alignment between the draft and target models. Specifically, we denote = { L(w) is among the top k100% of all tokens }, [0, 1]. Therefore, the overall loss for training the draft model Mq is: Ldistill = 1 y (cid:88) i=1 [yi S] Ldraft(yi), (10) where I[] is the indicator function that equals 1 if the condition inside the brackets is satisfied, and 0 otherwise. It ensures that only the selected learnable tokens contribute to the loss calculation. The whole filtering process is shown in figure 1. Figure 1: Overview of AdaSPEC distillation process: AdaSPEC selects the most training-effective tokens and distills on these tokens."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate AdaSPEC through comprehensive experiments across diverse domains and conduct detailed ablation studies to analyze its impact on the acceptance rate α. 4.1 Experimental Setup Our experimental framework employs GPT-like decoder-only Transformer models in two distinct configurations, designed to evaluate performance across different parameter scales while maintaining tokenizer consistency for SD: Small-to-Large Model Configuration: draft model Pythia-31M paired with target model Pythia-1.4B [5]. These models share architecture and tokenizer, providing an ideal test case for same-family knowledge transfer. Medium-to-Large Model Configuration: draft model CodeGen-350M paired with target model Phi-2 [24, 1]. While from different families, these models use an aligned tokenizer to ensure token-level consistency, allowing us to evaluate cross-family KD. We test these two configurations on diverse set of five tasks, each representative of specific domain to provide robust evaluation framework for AdaSPEC: GSM8K [9] (A benchmark for multi-step arithmetic reasoning), Alpaca [27] (A comprehensive instruction following dataset), MBPP [3] (A Python programming challenge set for code generation), CNN/Daily Mail [22] (A long-form summarization task), and XSUM [23] (An extreme summarization challenge). Reference Model Training. To ensure consistent starting point and fair comparison, both the draft model and the reference model are initialized from the same pre-trained model. For each task, we first fine-tune the target model on the task-specific dataset to establish strong baseline. The reference model is then trained using the method from DistillSpec [32]. 4.2 Baseline Setup We compare AdaSPEC against DistillSpec [32], the current state-of-the-art method for SD. Although AdaSPEC builds upon DistillSpecs training framework for its reference model, it introduces novel token selection mechanism. To evaluate its effectiveness, we evaluate both methods under two settings: resource-efficient scenario with fixed training duration and scenario optimized for maximum performance: 3-Epoch Setting: Both reference and draft models are trained for exactly 3 epochs, standard practice in LLM fine-tuning that balances task-specific performance with general capability retention [4]. This controlled training duration effectively prevents overfitting while ensuring adequate task adaptation. This setting evaluates model effectiveness under typical resource constraints and provides insights into rapid adaptation scenarios. Optimal-Epoch Setting: Models are trained for variable number of epochs, treated as tunable hyperparameter, to maximize task-specific performance. While this approach may lead to overfitting to the specific task at the expense of performance on other tasks, it allows us to thoroughly evaluate the upper bound of performance. The optimal number of epochs is determined empirically. Specifically, for GSM8K, the number of target epochs is chosen according to validation accuracy, while for the rest of the experiments it is chosen according to validation perplexity. Afterwards, we distill the reference model and pick the one with highest α on validation set. Eventually, this model serves as reference to train our draft model. For robustness, we only select the optimal epoch from 1, 3, 6, 10, 15, 20 and 30 (for XSUM and CNN/Daily Mail we select from 1, 3, 6, 10 for training efficiency). This configuration enables evaluation of both methods under less constrained scenarios, where achieving optimal task performance takes precedence over maintaining general capabilities. While Zhou et al. [32] employs more extensive training schedule in their DistillSpec experiments, our study adopts more resource-efficient approach due to computational constraints. In the OptimalEpoch Setting, we limit training to maximum of 30 epochs, striking balance between performance optimization and computational feasibility. Complete hyperparameter configurations and training specifications for both DistillSpec and AdaSPEC are detailed in Appendix A.2. 5 4.3 Main Results We summarize the main experimental results in Table 1. Table 1: Main experimental results for AdaSPEC compared to DistillSpec under two settings: 3Epoch and Optimal-Epoch. Metrics include acceptance rate (α). Task 3-Epoch (α) Optimal-Epoch (α) Pythia-31M 1.4B CodeGen-350M Phi-2 Pythia-31M 1.4B CodeGen-350M Phi-2 DistillSpec AdaSPEC DistillSpec AdaSPEC DistillSpec AdaSPEC DistillSpec AdaSPEC GSM8K Alpaca MBPP CNN/Daily Mail XSUM 57.58% 44.34% 46.88% 73.05% 47.24% 62.63% 47.25% 47.73% 74.22% 49.11% 79.49% 56.48% 87.36% 79.33% 58.88% 82.79% 58.80% 88.76% 80.63% 59.93% 66.19% 65.41% 49.88% 80.15% 56.11% 68.28% 65.79% 65.12% 80.89% 57.80% 81.49% 58.05% 86.60% 85.01% 66.78% 83.48% 60.36% 87.70% 86.29% 68.19% Acceptance Rate Analysis. We evaluate performance using the acceptance rate α, defined as the proportion of draft-model-generated tokens validated by the target model. As shown in Table 1, AdaSPEC consistently achieves higher acceptance rates than DistillSpec across all tasks and model configurations, demonstrating superior draft-target model alignment. 4.4 Analysis To provide detailed insights into AdaSPECs effectiveness, we conduct in-depth analyses on two representative configurations: Pythia-31M/1.4B on GSM8K (3-Epoch): This configuration examines performance on arithmetic reasoning under constrained training conditions, representing scenarios with limited computational resources and the need for generalization. Since reasoning is typically considered as an additional capability beyond general language modeling, this setup ensures that the model retains its core abilities while effectively handling arithmetic tasks. Pythia-31M/1.4B on CNN/Daily Mail (Optimal-Epoch): This setup investigates extractive summarization with extended training, demonstrating the models ability to optimize for task-specific objectives. In real-world applications, models are sometimes specifically deployed for summarizing long-form contents such as news reports, emails, or web pages, requiring dedicated fine-tuning. Thus, the Optimal-Epoch setting is chosen to maximize the models summarization capabilities. Task-Level Acceptance Rate Distribution. We first analyze the distribution of acceptance rates across tasks for both methods. As illustrated in Figure 2, AdaSPEC demonstrates consistently superior performance compared to DistillSpec. The acceptance rate histograms for both tasks exhibit significant rightward shift under AdaSPEC, indicating more frequent successful draft predictions. This systematic improvement in acceptance rate suggests that AdaSPECs selective distillation approach effectively enhances draft-target model alignment across diverse task contexts. Logit Margin Distributions Across Tokens. Next, we analyze the distribution of top-2 logit margins across tokens for both methods. The logit margin, defined as the difference between the logits of the top-1 and top-2 predicted tokens, serves as measure of prediction confidence. positive margin indicates correct draft model prediction, while negative margin signifies an incorrect prediction that would be rejected in SD. As shown in Figure 2, AdaSPEC demonstrates superior logit margin distributions compared to DistillSpec across both GSM8K and CNN/Daily Mail datasets. AdaSPEC exhibits: Higher frequency and magnitude of positive margins, indicating more frequent and confident correct predictions. Lower frequency and magnitude of negative margins, suggesting less frequent and less severe prediction errors. These patterns demonstrate that AdaSPEC achieves better draft-target model alignment through its selective distillation approach, enabling more effective knowledge transfer from the target model to the draft model. KL-Divergence Distribution Across Tokens. We further analyze the Kullback-Leibler (KL) divergence between draft and target models token prediction distributions on both GSM8K and CNN/Daily Mail datasets. As illustrated in Figure 2, AdaSPEC exhibits consistently lower KL divergence values compared to DistillSpec across both tasks, demonstrated by significant leftward shifts in the distributions. This systematic reduction in KL divergence across different tasks and tokens indicates that AdaSPECs selective distillation approach achieves tighter alignment between draft and target model predictions, corroborating our previous findings on acceptance rates and logit margins. Figure 2: Comparative analysis of AdaSPEC and DistillSpec performance across multiple metrics on GSM8K (a, c, e) and CNN/Daily Mail (b, d, f) datasets: (a-b) Task-level acceptance rate distributions showing AdaSPECs superior performance across tasks. (c-d) Logit margin distributions demonstrating AdaSPECs improved prediction confidence with higher positive margins and lower negative margins. (e-f) Token-level KL divergence distributions indicating better draft-target model alignment for AdaSPEC with consistently lower divergence values. The results demonstrate AdaSPECs more effective knowledge transfer and improved draft-target model alignment compared to DistillSpec across different evaluation metrics. Case Studies. We conduct detailed case studies on GSM8K and CNN/Daily Mail datasets. consistent pattern emerges: AdaSPECs prediction errors form nearly subset of DistillSpecs errors, as illustrated in Figure 3. This pattern demonstrates the general effectiveness of AdaSPECs targeted training approach in improving alignment and reducing inference discrepancies. GSM8K, with its natural division between mathematical and non-mathematical tokens, offers particularly insightful analysis. During training, AdaSPEC predominantly selects mathematics-related tokens for focused learning (see Appendix A.3). During inference, this selective approach translates into significantly improved prediction accuracy for mathematical tokens compared to DistillSpec, as shown in Figure 3. These results demonstrate AdaSPECs ability to identify and prioritize task-critical tokens during training, leading to more precise draft-target model alignment. 4.5 Ablation Study To systematically evaluate the effectiveness of different components in AdaSPEC, we conduct comprehensive ablation studies across the following four key dimensions. All experiments are conducted on GSM8K and MBPP with Pythia 1.4B (target) and Pythia 31M parameters (draft). All models are trained for 3 epochs. Token Selection Mechanism. To evaluate our token selection strategy, we compare models trained on the top 40% of tokens (selected based on KL-divergence margin) against those trained on the bottom 40%. As shown in Table 2, models trained on the top 40% tokens consistently outperform those trained on the bottom 40%, with the latter performing even worse than the reference model. The improvement is particularly pronounced on the MBPP dataset, where token selection yields up to 6% performance gain. These results demonstrate that AdaSPEC effectively enhances model alignment by focusing on more learnable tokens during Knowledge Distillation. Training Method. To demonstrate that AdaSPECs benefits extend beyond Knowledge Distillation, we replace the distillation process for both reference and draft models with direct fine-tuning. Table 3 7 Figure 3: Comparison of prediction errors between AdaSPEC and DistillSpec on GSM8K and CNN/Daily Mail Datasets: Tokens highlighted in blue represent errors made by both methods, while tokens highlighted in red indicate errors unique to the corresponding method. As can be seen, AdaSPECs errors form nearly subset of DistillSpecs errors, demonstrating the effectiveness of AdaSPECs selective training approach in reducing inference discrepancies. Table 2: Ablation study results for token selection strategies. Sub-Strategy GSM8K MBPP Reference α Draft α Reference α Draft α Top 40% Bottom 40% 59.77% 59.77% 63.22% 49.03% 42.22% 42.22% 48.22% 39.75% Table 3: Ablation study results for training methods. MBPP GSM8K Reference α Draft α Reference α Draft α Sub-Strategy Distillation Fine-tuning 59.77% 59.64% 63.22% 63.13% 42.22% 41.42% 48.22% 45.61% Table 4: Ablation study results for distillation methods. Sub-Strategy GSM8K MBPP Reference α Draft α Reference α Draft α KL TVD RKL 59.77% 9.32% 30.22% 63.22% 9.09% 30.05% 42.22% 3.86% 13.17% 48.22% 6.76% 15.61% reveals two key findings: (1) fine-tuned draft models outperform the distillation baseline (reference model) across both datasets, confirming that our token selection process aids model convergence; and (2) fine-tuned draft models achieve up to 4% improvement over their reference counterparts, indicating that our token selection mechanisms benefits generalize beyond distillation to broader training scenarios. Distillation Method. We expand AdaSPEC to more distillation approaches: Reverse KL (RKL) and Total Variation Distance (TVD) [28]. With = 0.4 for all methods, we observe that token selection significantly improves the acceptance rate by 6% on MBPP when using forward KL. However, when using RKL and TVD, the acceptance rate performance degrades. This is primarily attributed to the inherent limitations of RKL and TVD as distillation objectives, which struggle to effectively align the draft and target models in the context of SD. It is worth noting that DistillSpec [32] uses TVD as the distillation function with batch size of 32 and training step of 300,000. This prolonged training process not only requires substantial computational resources but also results in the problem of overfitting. Considering these factors, we ultimately select forward KL divergence as our distillation objective. Token Selection Ratio. To investigate the impact of token selection ratio, we vary and compare the final acceptance rate of the draft model. Results in Fig 4 show that typically, lower values result in better final acceptance rate. To strike balance between training efficiency and performance, we finally choose = 0.4 in most cases. 4.6 Additional Experimental Results Wall Clock Speed-up. To investigate AdaSPECs potential to accelerate end to end decoding in real world setting, we use frontier inference engine vLLM [15] on one single A100 GPU and report speed-up in Table 5. Results show that an expected 1020% speed-up could be easily achieved compared with DistillSpec, demonstrating the effectiveness of our approach. Figure 4: Impact of token selection ratio on acceptance rate for GSM8K: Results show general trend where lower values (0.2-0.4) yield higher acceptance rates. Table 5: Generation speed for AdaSPEC with VLLM on one single A100 GPU. We use greedy decoding and report time to generate sentence and one token. We use Pythia-31M1.4B and the models are trained for 3 epochs. On these tasks, AdaSPEC exhibits 1020% speed-up. Speed (s/sentence) Speed (tokens/s) MBPP GSM8K CNN/DailyMail DistillSpec AdaSPEC DistillSpec AdaSPEC DistillSpec AdaSPEC 0.69 0.57 0.51 0.48 0.76 0.67 149.15 181.67 227.86 241. 248.49 283.50 Integration with Advanced SD Methods. To demonstrate the orthogonality and generalizability of AdaSPEC beyond vanilla speculative decoding (SD), we integrate our method with EAGLE [17], an advanced SD algorithm featuring tree attention and adaptive expansion strategies. Following the standard 3-Epoch training setup on the ShareGPT dataset, we evaluate both training accuracy and generation speed on MT-Bench. As shown in Table 6, AdaSPEC consistently improves both accuracy and decoding efficiency within the EAGLE framework. Table 6: Vicuna-7B-v1.3 [8, 31] with 3Epoch finetuning following original EAGLE recipe. Here, training accuracy refers to firstgenerated-token accuracy in the training set. Eagle Eagle + AdaSPEC Training Accuracy Speed (s/sentence) Speed (tokens/s) 75.3% 8.85 63.48 76.3% 8.06 (-8.9%) 68.21 (+7.45%) Table 7: Acceptance rate of larger model configuration with 3-Epoch GSM8K. GSM8K DistillSpec AdaSPEC 84.43% 86.21% Results on Larger Models. We conduct an additional GSM8K evaluation using combination of the Qwen2.5-0.5B and Qwen2.5-32B models. When trained with 3 epochs, AdaSPEC reaches an acceptance rate of 86.21% while DistillSpec achieves 84.43%, as shown in Table 7. This shows that our approach can easily scale up to larger models. Results on Mixed Dataset. To further validate AdaSPECs ability to work on blended tasks, we mix GSM8K with MBPP in training and validate α separately. Specifically, we first train the target on MBPP and then on GSM8K, each for 3 epochs. The reference and draft models also follow the same process. The Table 8: Performance on mixed datasets. MBPP GSM8K DistillSpec AdaSPEC 69.63% 72.75% 70.69% 78.41% 9 results in Table 8 reveals that AdaSPEC strives to retain the original models capabilities as much as possible, with less forgotten knowledge."
        },
        {
            "title": "5 Dicussion",
            "content": "Size Gap Between Target and Draft Models. In traditional SD settings, the size gap between the draft model and the target model is often within 10x. In this work, we demonstrate that AdaSPEC effectively bridges the performance gap, even when the size difference is substantial up to 64 times in our experiments. By leveraging selective token filtering and Knowledge Distillation, AdaSPEC can enhance token acceptance rates and help maintain generation quality, providing more opportunities to use significantly smaller draft models. Model Size Gap and Performance Gains. From Table 1, we observe that the performance gain of AdaSPEC over DistillSpec becomes more pronounced as the size gap between the reference and target models increases (e.g., from CodeGen-350M Phi-2 to Pythia-31M 1.4B). This trend is consistent across both 3-Epoch and Optimal-Epoch settings. The result aligns well with our motivation: when the capacity discrepancy between models widens, direct Knowledge Distillation tends to suffer from representation mismatch, making it harder for the smaller model to absorb all teacher signals uniformly. AdaSPECs adaptive mechanism mitigates this issue by selectively aligning easier tokens first, effectively narrowing the transfer gap. Consequently, the larger the size difference, the greater the relative improvement AdaSPEC achieves. Connection with Lin et al. [19]. similar token selection method is proposed in Lin et al. [19], which focuses on identifying and prioritizing harder-to-learn tokens (opposite to the motivation of AdaSPEC) during pre-training. Different from their design, our approach focuses on addressing the limited capacity of the draft model in SD. Specifically, we focus on identifying and filtering out challenging tokens, allowing the draft model to concentrate on learning easier-to-predict tokens. Our selective distillation process ensures that the draft model aligns more effectively with the target model on tokens that are more tractable, given its constrained capacity. By doing so, we maximize the draft models limited resources while maintaining high-quality predictions in SD tasks. Thus, the essential difference lies in the distinct objectives of pre-training and Speculative Decoding. Limitations. As preliminary study on selective training for SD, we limit our study on simple loss-related token filter. In future work, one can design more adaptive filtering strategies as well as integrate AdaSPEC with tree-based or multi-step verification frameworks to further improve both speed and quality of LLM inference."
        },
        {
            "title": "6 Conclusion",
            "content": "We present AdaSPEC, novel approach for training more efficient draft models for SD. AdaSPEC introduces selective token filtering based on reference model perplexity gaps, enabling draft models to focus limited capacity on tokens where alignment with the target model is most achievable. Experiments show it outperforms baselines in arithmetic reasoning, instruction following, code generation, and summarization with higher acceptance rates."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Sebastien B, Caio Mendes, Weizhu Chen, Allie Giorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Michael Santacroce, and Yi Zhang. Phi-2: The surprising power of small language models, 12 2023. [2] Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C. Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, Michael Noetel, and Andreas Stuhlmüller. Raft: real-world few-shot text classification benchmark, 2022. URL https: //arxiv.org/abs/2109.14076. [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. 10 [4] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [5] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR, 2023. [6] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. [7] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. [8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/ 2407.21783. [12] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour, 2018. URL https://arxiv.org/abs/1706.02677. [13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. [14] Geoffrey Hinton. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [15] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. URL https://arxiv.org/abs/2309. 06180. [16] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. [17] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. [18] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-2: Faster inference of language models with dynamic draft trees. arXiv preprint arXiv:2406.16858, 2024. [19] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. Rho-1: Not all tokens are what you need, 2024. URL https://arxiv.org/abs/2404.07965. 11 [20] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. Online speculative decoding. arXiv preprint arXiv:2310.07177, 2023. [21] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. Specinfer: Accelerating large language model serving with tree-based speculative inference and verification. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, pages 932949, 2024. [22] Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023, 2016. [23] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization, 2018. URL https://arxiv.org/abs/1808.08745. [24] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022. [25] OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. [26] Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, and Max Ryabinin. Specexec: Massively parallel speculative decoding for interactive llm inference on consumer devices. arXiv preprint arXiv:2406.02532, 2024. [27] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. [28] Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level knowledge distillation, 2023. URL https://arxiv.org/abs/2307.15190. [29] Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 39093925, 2023. [30] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft& verify: Lossless large language model acceleration via self-speculative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1126311282, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.607. [31] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https://arxiv.org/ abs/2306.05685. [32] Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv Kumar, Jean-François Kagy, and Rishabh Agarwal. Distillspec: Improving speculative decoding via knowledge distillation. arXiv preprint arXiv:2310.08461, 2023."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Full Algorithms for AdaSPEC zi = Sq(z<i); + zi Algorithm 1 Greedy Speculative Decoding 1: Input: target model Mp, draft model Mq, input sequence 2: accept 0, reject 0, len(x) 3: Sample γ tokens z1, ..., zγ from Mq autoregressively. 4: for = 1 to γ do 5: 6: end for 7: Verify in parallel 8: [Sp(z<1), Sp(z<2), ..., Sp(z<γ)] [z 9: for = 1 to γ do = zi then 10: reject reject + 1; break 11: 12: 13: 14: 15: end if 16: 17: end for 18: + Sp(z<1); goto 4 end if + zi, accept accept + 1 if zi = <eos> then return x, accept, reject 2, ..., if 1, γ] Train Mp on (via standard LM fine-tuning) to obtain . Algorithm 2 AdaSPEC: Selective Distillation for Speculative Decoding 1: Input: dataset D, target model Mp, draft model Mq, fraction 2: Step 1. Fine-tune Mp: 3: 4: Step 2. Reference Model Training: Initialize Mref Mq. 5: Distill Mref from 6: 7: Step 3. Selective Distillation: 8: (a) Compute losses: For each token and context, on (e.g. forward KL). Lref (w) = (P (w context)R(w context)) , Ldraft(w) = (P (w context)Q(w context)) , L(w) = Ldraft(w) Lref (w). 9: (b) Filter tokens: = { L(w) is among the top k100% of all tokens }, [0, 1]. 10: (c) Distill on filtered set: min Mq Ldistill = 1 y (cid:88) i=1 [yi S] Ldraft(yi) 11: return Mq (draft model in SD) A.2 Implementation Details We use the hyperparameters in Table 9. For 3-Epoch setting, both reference and draft model are distilled for 3 epochs. For Optimal-Epoch setting, the target model is first fine-tuned to maximize performance on validation set. Specifically, for GSM8K, the number of target epochs is chosen 13 according to validation accuracy, while for the rest of the experiments it is chosen according to validation perplexity. Afterwards, we distill the reference model and pick the one with highest α on validation set. Eventually, this model serves as the reference model to train our draft model. For robustness, we only select the optimal epoch from 1, 3, 6, 10, 15, 20 and 30 (for XSUM and CNN/Daily Mail we select from 1, 3, 6, 10 for training efficiency). Note that when performing token selection, we apply the linear scaling rule for learning rate adjustment [12]. Task Hyperparameter Table 9: Experimental hyperparameters. 3-Epoch Pythia 31M1.4B Codegen-350MPhi-2 Optimal-Epoch Pythia 31M1.4B Codegen-350MPhi-2 GSM8K Alpaca MBPP CNN/Daily Mail XSUM Batch size Learning rate Epochs for target model Epochs for reference model Epochs for draft model Filter fraction Batch size Learning rate Epochs for target model Epochs for reference model Epochs for draft model Filter fraction Batch size Learning rate Epochs for target model Epochs for reference model Epochs for draft model Filter fraction Batch size Learning rate Epochs for target model Epochs for reference model Epochs for draft model Filter fraction Batch size Learning rate Epochs for target model Epochs for reference model Epochs for draft model Filter fraction 16 3e-4 3 3 3 0.4 16 3e-4 3 3 3 0.4 8 1e-5 3 3 3 0.4 16 1e-4 3 3 3 0. 16 3e-4 3 3 3 0.4 A.3 AdaSPEC Example Tokens 16 3e-4 3 3 3 0.4 16 3e-4 3 3 3 0.4 8 1e-4 3 3 3 0.4 16 1e-4 3 3 3 0. 16 1e-4 3 3 3 0.4 16 3e-4 6 15 30 0.4 16 3e-4 1 15 30 0.4 8 1e-5 1 30 6 0.4 16 1e-4 1 10 10 0.4 16 3e-4 1 10 10 0. 16 3e-4 3 30 30 0.4 16 3e-4 1 20 15 0.4 8 1e-4 1 10 6 0.4 16 1e-4 1 10 10 0.4 16 1e-4 1 10 10 0.4 Here, we showcase some example tokens (Listing 1) that AdaSPEC selects while training on GSM8K. These selected tokens are typically mathematical related tokens, such as digits and operators. { \"scored\", \"8\", \"in\", \"thus\", \"9\", \"x\", \"1\", \"=\", \"<<\", \"9\", \"*\", \"91\", \"=\", \"19\", \">>\", \"8\", \"19\", \"Em\", \"because\", \"28\", \"28\", \"+\", \"8\", \"28\", \"+\", \"90\", \"18\", \"9\", \"18\", \"18\", \"18\", \"-\", \"8\", \"=\", \"99\", \"99\", \"99\", \"+\", \"100\", \"The\", \" final\", \"answer\", \"100\", \"equal\", \"12\", \"+\", \"7\", \"=\", \"19\", \">>\", \"19\", \"packs \", \"19\", \"5\", \"24\", \"total\", \"(\", \"24\", \"*(\", \"2\", \")=\", \"16\", \">>\", \"16\", \"J\", \"spends\", \"inside\", \"because\", \"-\", \"(\", \"inside\", \"16\", \"iley\", \"3\", \"18\", \" spends\", \"12\", \"In\", \"total\", \"they\", \"+\", \"12\", \"=\", \"<<\", \"8\", \"+\", \"=\", \"20\", \"/\", \"=\", \"10\", \"10\", \"The\", \"earned\", \"final\", \"answer\", \"difference\", \"-\", \"=\", \"13\", \"*\", \"2\", \"26\", \">>\", \"26\", \"twice\", \"26\", \"18\", \"=\", \"26\", \"18\", \"8\", \"The\", \"final\", \"answer\", \":\", \" \", \"8\", } Listing 1: Selected tokens during GSM8k training. A.4 Minimal Code Implementation of AdaSPEC The core of AdaSPEC could be implemented with 100 lines of code. We show it in Listing 2. def compute_loss( self, model, inputs, return_outputs=False, 14 num_items_in_batch=None ): labels = inputs[\"labels\"][:, 1:] outputs = model(inputs) with torch.no_grad(): target_outputs = self.target_model(inputs) ref_outputs = self.ref_model(inputs) logits = outputs[\"logits\"] target_logits = target_outputs[\"logits\"] ref_logits = ref_outputs[\"logits\"] loss_fct = KLDivLoss(reduction=\"none\") shift_logits = logits[..., :1, :].contiguous() shift_target_logits = (target_logits[..., :1, :] shift_ref_logits = (ref_logits[..., :1, :] .contiguous()) .contiguous()) shift_logits = shift_logits.view( 1, model.config.vocab_size) shift_target_logits = (shift_target_logits .view(1, model.config.vocab_size)) shift_ref_logits = (shift_ref_logits mask = labels.ne(IGNORE_INDEX).flatten().unsqueeze(1) .view(1, model.config.vocab_size)) shift_logits = ( torch.masked_select(shift_logits, mask=mask) .view(1, model.config.vocab_size)) shift_target_logits = (torch.masked_select(shift_target_logits, mask=mask) .view(1, model.config.vocab_size)) shift_ref_logits = (torch.masked_select(shift_ref_logits, mask=mask) .view(1, model.config.vocab_size)) = F.softmax(shift_target_logits, dim=1) q_log = F.log_softmax(shift_logits, dim=1) actual = loss_fct(q_log, p) q_log = F.log_softmax(shift_ref_logits, dim=1) ref = loss_fct(q_log, p) actual = actual.sum(dim=1) ref = ref.sum(dim=1) = self.k delta = actual ref mask = delta >= torch.quantile( delta, 1 k, dim=0, keepdim=True) if num_items_in_batch is not None: loss = torch.masked_select(actual, mask=mask).sum() loss = loss / num_items_in_batch else: loss = torch.masked_select(actual, mask=mask).mean() return (loss, outputs) if return_outputs else loss Listing 2: AdaSPEC implementation with PyTorch. 15 A.5 Broader Impact AdaSPEC can be used mainly to improve generation speed of LLMs, which is positive influence to reduce potential electric energy consumption for serving LLMs. However, this technique may also be used for some models that is non-compliance with regulations and ethics, such as models that generate discriminatory contents. A.6 Experiments compute resources Here we list the estimated GPU hours; see Table 10. Table 10: GPU hours of training models on A100 GPUs. Task 3-Epoch Optimal-Epoch Pythia-31M 1.4B CodeGen-350M Phi-2 Pythia-31M 1.4B CodeGen-350M PhiGSM8K Alpaca MBPP CNN/Daily Mail XSUM 1 1 1 60 60 3 3 3 200 200 15 15 15 200 200 50 50 50 700"
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "Tsinghua University",
        "University of California, Berkeley"
    ]
}