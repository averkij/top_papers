{
    "paper_title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods",
    "authors": [
        "Honglin Lin",
        "Zheng Liu",
        "Yun Zhu",
        "Chonghan Qin",
        "Juekai Lin",
        "Xiaoran Shang",
        "Conghui He",
        "Wentao Zhang",
        "Lijun Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 2 ] . [ 1 1 2 8 1 2 . 1 0 6 2 : r MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Honglin Lin1,2, Zheng Liu1,3, Yun Zhu1, Chonghan Qin1,4, Juekai Lin1, Xiaoran Shang1, Conghui He1, Wentao Zhang3, Lijun Wu1 1Shanghai Artificial Intelligence Laboratory, OpenDataLab, 2Shanghai Jiao Tong University, 3Peking University, 4The University of Hong Kong Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source multimodal models still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with detailed, visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art (SOTA) results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover less is more phenomenon via our difficulty-aware filtering strategy: subset of just 7% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities. Further, we conduct comprehensive ablation studies on training strategies and data composition, providing key insights and practical recipes for multimodal reasoning model development. For open-source, we release the full dataset and models to facilitate reproducible research on data-centric strategies for multimodal reasoning. Date: January 30, 2026 Equal Contributions: Honglin Lin, Zheng Liu, Yun Zhu Correspondence: Lijun Wu, lijunwu@pjlab.org.cn Homepage: https://mmfinereason.github.io/ HuggingFace: https://huggingface.co/collections/OpenDataArena/mmfinereason"
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in Vision Language Models (VLMs) have led to substantial improvements in visual reasoning capabilities [55, 60]. State-of-the-art (SOTA) proprietary systems such as GPT-5 [35] and Gemini 3 [13] achieve strong performance on complex multimodal tasks, benefiting from access to massive, carefully curated private datasets. Mounting evidence suggests that the quality and structure of training data plays central role in unlocking strong reasoning abilities [57]. 1 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Figure 1: Average score across mathematical reasoning and multimodal understanding benchmarks. MMFineReason-2B/4B/8B demonstrates strong performance relative to thinking models with significantly more parameters. In the text domain, this data-centric paradigm has proven highly effective. The release of DeepSeekR1 [10] and its distilled variants have further highlighted that constructing high-quality post-training data is key to closing the gap with proprietary models. Recent works [15, 36, 54, 21, 28, 34, 20] have extensively investigated pipelines for building high-quality training data, enabling open models to approach proprietary performance. However, extending this success to the multimodal setting remains challenging. The open-source community still struggles to produce multimodal datasets that match the scale, consistency, and reasoning depth of those used by leading proprietary models. Although recent efforts such as FineVision [46] and LLaVA-OneVision-1.5 [2] have expanded both the quality and quantity of available data, two critical limitations persist: Data imbalance: While VQA data derived from natural images and documents is largely sufficient, high-quality visual reasoning samplesparticularly for STEM diagrams and visual puzzlesremain scarce due to inherent data rarity and high annotation costs [22]. Inconsistent reasoning quality: Unlike the text domain, where distillation from strong teacher models such as DeepSeek-R1 has become standard practice for obtaining high-quality rationales. Multimodal datasets remain fragmented and heterogeneous in annotation style, with limited availability of interpretable, long-form Chain-of-Thought (CoT) supervision [39, 33]. These limitations hinder principled, data-centric research on multimodal reasoning. natural solution is to distill reasoning traces from capable multimodal teacher. The recently released Qwen3-VL series [3] demonstrates strong visual reasoning capabilities approaching proprietary systems, making it promising candidate for scalable data synthesis. Motivated by this, we introduce MMFineReason, an open-source dataset of over 1.8M samples with 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235BA22B-Thinking. As illustrated in Figure 2, we build the dataset in three stages: (1) Data Aggregation and Standardization, where we collect, clean, and unify raw multimodal data from diverse sources 2 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Figure 2: MMFineReason data pipeline and the two-stage training. Illustrating data construction, annotation, selection, mixing, and model training (SFT and RL) in our framework. into canonical schema; (2) Reasoning Distillation, where we generate detailed, visually grounded reasoning traces using the SOTA teacher model; and (3) Data Selection, where we conduct rigorous quality verification and difficulty-based filtering to ensure both correctness and training efficiency. By fine-tuning Qwen3-VL-Instruct on our constructed MMFineReason dataset, we obtain MMFineReason2B/4B/8B models with superior performance. As shown in Figure 1, our models establish new SOTA among open-source models of comparable size. Notably, MMFineReason-4B can surpass Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking. Our contributions can be summarized as follows: We design systematic pipeline integrating data collection, filtering, and CoT distillation to construct MMFineReason-1.8M, the first large-scale multimodal reasoning dataset comprising 5.1B tokens with high-quality annotations distilled from Qwen3-VL-235B-A22B-Thinking. We develop the MMFineReason model family (2B/4B/8B) by fine-tuning Qwen3-VL-Instruct on this dataset. Notably, our models exhibit exceptional scaling efficiency: MMFineReason-4B surpasses Qwen3-VL-8B-Thinking, while MMFineReason-8B outperforms Qwen3-VL-30B-A3BThinking and approaches the performance of Qwen3-VL-32B-Thinking. We demonstrate that training with reasoning-oriented data, such as STEM, puzzles, games, diagrams, yields synergistic effect, driving simultaneous improvements in both specialized reasoning capabilities and general model performance. We adopt difficulty-aware filtering strategy to construct efficient fine-tuning subsets, specifically MMFineReason-123K and MMFineReason-586K. Remarkably, fine-tuning on just 7% of the data selected achieves performance comparable to training on the full dataset. We conduct comprehensive ablation studies on training strategies and data composition, providing key insights and practical recipes for multimodal reasoning model development. 3 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods In summary, MMFineReason serves as high-quality training resource and reproducible framework for building open-source VLMs with robust reasoning capabilities, fostering principled exploration of data-centric strategies in multimodal reasoning."
        },
        {
            "title": "Overall Findings",
            "content": "Data-centric strategies exhibit strong scaling efficiency. With strong pretrained backbone, training on only 5.1B tokens is sufficient to surpass larger open-source models (e.g., Qwen3-VL-30B-A3B-thinking) and achieve performance comparable to advanced closed-source models (e.g., GPT-5-mini-high). severe cross-domain imbalance in data distribution and difficulty. large portion of the current training data is overly simple (67%), while puzzle-style problems are significantly harder and remain underrepresented compared to STEM reasoning data. Difficulty-aware filtering is highly effective. The difficulty-filtered MMFineReason123K dataset achieves performance close to the full dataset using only 7% of the data, demonstrating substantial data efficiency gains. reasoning-oriented data mixing strategy enables simultaneous gains in general and reasoning tasks. Only few amount of general data improves both general-purpose tasks and reasoning benchmarks in synchronized manner. Ultra-high resolution offers diminishing returns for reasoning. Extremely large input resolutions (e.g., 20482) bring limited benefits for reasoning tasks, while general vision tasks still require higher resolutions. Caption augmentation provides marginal benefit once chains-of-thought are mature. When the reasoning chain is already well-formed, concatenating an additional caption brings almost no further improvement."
        },
        {
            "title": "2.1 Multimodal Reasoning Datasets",
            "content": "Multimodal reasoningincluding mathematical problem solving, visual logical reasoning, and chart understandingis key challenge for evaluating the reasoning capabilities of vision-language models. Prior work has demonstrated that data is central factor driving the advancement of model reasoning ability [60, 36]. However, in the multimodal domain, data acquisition and synthesis are considerably more difficult [46]. Many proprietary models rely on large-scale private datasets [13, 35, 3], resulting in significant data gap. In contrast, the open-source community still lacks multimodal reasoning datasets of substantial scale. To support VL reasoning, several datasets have been proposed, such as MathV360K [41] and LLaVACoT [51]. However, these datasets primarily focus on mathematical reasoning and therefore provide limited coverage. Although FineVision [46] attempts large-scale data aggregation, its content is relatively coarse, includes low-quality data sources, and omits many reasoning-oriented datasets. To address these limitations, we conduct the most extensive collection and integration of existing multimodal reasoning data to date, and further provide high-quality reference answers to support research in the open-source community. 4 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods"
        },
        {
            "title": "2.2 Data Recipes for Reasoning Models",
            "content": "Early data pipelines such as LLaVA-Instruct-150K [23] used GPT-4 [1] to generate VQA pairs. ShareGPT4V [7] leveraged GPT-4V [53] and ShareCaptioner to produce large-scale image descriptions, with lengthand content-based filtering to ensure data quality. Other approaches, such as SynthVLM [26] and FUSION [27], further scale up existing datasets by generating synthetic images. Parallel to advancements in the visual domain, significant progress has been made in constructing text-only reasoning data [37, 38, 20]. With the emergence of DeepSeek-R1 [10] and its high-performance distilled models, recent efforts such as OpenR1 [15] and OpenThoughts [36] have built open and transparent data construction pipelines, enabling the open-source community to approach the capability levels of proprietary systems using only public data [16]. Despite this progress, the multimodal field still lacks transparent and reproducible data curation and training pipelines capable of achieving performance parity with closed-source systems. To this end, we introduce MMFineReason, high-quality and fully reproducible data construction pipeline that helps open-source multimodal models progressively narrow the performance gap with closed-source systems at reasonable cost. Importantly, our data construction workflow is entirely based on locally deployed open-source models and does not rely on any closed-source APIs."
        },
        {
            "title": "3 MMFineReason Pipeline",
            "content": "In this section, we detail our dataset construction pipeline. First, we describe the data collection and processing procedures in Section 3.1. Next, we explain our method for distilling responses for curated datasets in Section 3.2. Finally, we present the data selection strategies in Section 3.3."
        },
        {
            "title": "3.1 Data Collection and Processing",
            "content": "Data Collection. We initiate our data construction by aggregating diverse array of multimodal datasets from the open-source community. We first leverage FineVision [46], an extensive collection of visual instruction datasets. We conduct rigorous manual inspection of each candidate source, filtering out datasets unrelated to STEM or reasoning tasks. To expand the coverage of mathematical reasoning, scientific reasoning, and visual games & puzzles, we further incorporate high-quality datasets such as BMMR [48], Euclid30K [19], Zebra-CoT-Physics [18], and GameQA-140K [42]. This strategic expansion ensures more balanced and challenging coverage of scientific and puzzle-solving domains. For comprehensive details and inclusion criteria, please refer to Appendix A. Data Cleaning. We implement comprehensive data cleaning pipeline to guarantee the linguistic consistency, textual cleanliness, and reasoning suitability of the collected samples. The cleaning prompt is detailed in Prompt 11 (see Appendix for representative noise cases). The procedure includes: Language Standardization: To unify the linguistic landscape of the corpus, we translate nonEnglish questions found in datasets like BMMR and Euclid30K into English. Noise Removal: We sanitize question text by removing extraneous artifacts, including webpage links, corrupted characters, formatting residues, problem indices, and score annotations. Instruction Refinement: We reformulate prompts that solicit shallow responses (e.g., directly give the answer) into directives that explicitly encourage analytical thinking (e.g., provide your answer after careful reasoning). This step is crucial for eliciting high-quality reasoning traces during distillation. Task Suitability Filtering: We filter out tasks that fall outside the scope of visual analytical reasoning, such as coding exercises or generation-based drawing tasks, ensuring the dataset remains focused on logical problem-solving. 5 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods We further perform automatic image cleaning [46] by discarding corrupted or unreadable images, resizing those with longest side exceeding 2048 pixels while preserving aspect ratio, and converting all images uniformly to the RGB color space. Data Standardization. The landscape of multimodal datasets is characterized by significant fragmentation and lack of standardization. The heterogeneity in file formats and annotation structures across different sources poses substantial challenge for unified data processing. For datasets that lack explicit ground-truth labels, we employ Qwen3-30B-A3B-Thinking to extract and store the missing answers in the answer field, following the prompt in Prompt 12. To facilitate downstream processessuch as caption generation, reasoning distillation, and correctness evaluationand to enhance accessibility for the research community, we further convert all collected samples into unified canonical schema. Each standardized data entry includes the following fields: Metadata: source, id Raw Data: original question, original answer Input/Output: image, question, answer Augmented Annotations: qwen3vl 235b instruct caption, qwen3vl 235b thinking response Metrics: qwen3vl 4b pass rate, is consistent, consistency analysis The schema fields are defined as follows: source denotes the origin dataset (e.g., Geometry3K), while id serves as the unique sample identifier. The Raw Data fields preserve the original question and original answer exactly as obtained from the source, which are then processed into the standardized Input/Output triplet (image, question, and answer) used for training. Within the Augmented Annotations, qwen3vl 235b instruct caption and qwen3vl 235b thinking - response denote the dense visual description and reasoning steps generated by the teacher model, respectively. The Metrics group includes qwen3vl 4b pass rate, which serves as difficulty proxy based on smaller models performance, alongside is consistent and consistency analysis, which provide automated verification of the generated reasoning against the ground truth."
        },
        {
            "title": "3.2 Data Annotation\nWe employ Qwen3-VL-235B-A22B-Thinking, currently recognized as the SOTA open-source VLM, to\ndistill long-form CoT explanations for each sample. To ensure the rigor and reproducibility of the\nreasoning process, the distillation prompt (see Prompt 14) imposes a systematic four-phase solution\nframework: Comprehensive Information Extraction, Strategic Problem Setup, Rigorous Solution Execution,\nand Solution Validation. Furthermore, it explicitly instructs the model to treat visual elements as integral\ncomponents of the solution rather than supplementary context.",
            "content": "The prompt also enforces unified output template: the model first emits multi-step reasoning trace wrapped in <think>...</think> block, followed by the final solution wrapped in an <answer>...</answer> block to facilitate downstream answer parsing and automatic verification. Additionally, we utilize Qwen3-VL-235B-A22B-Instruct to generate dense image captions, following the guidelines in Prompt 13. Through this systematic annotation and distillation process, we obtain the original MMFineReason-Full dataset, consisting of 2.3M samples with total of 8.8B solution tokens. detailed breakdown of the dataset composition is provided in Table 5. 6 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Figure 3: Consistency analysis across visual instruction tuning datasets. The chart displays the ratio of samples where the predictions generated by Qwen3-VL-235-A22B-Thinking align with the original ground truth answers (Consistent) versus cases of disagreement (Inconsistent)."
        },
        {
            "title": "3.3 Data Selection",
            "content": "Reasoning Quality Filtering to Construct MMFineReason-1.8M. We adopt simple and light way to construct our MMFineReason dataset for training. Specifically, to ensure high-quality and non-redundant reasoning traces, we apply multi-stage filtering procedure to the distilled outputs. Template and Length Validation: We first impose strict structural validation to ensure the usability of the distilled output. Specifically, we filter out any reasoning trace that fails to adhere to the mandated <think>...</think> and <answer>...</answer> output template. Furthermore, to prevent the retention of superficial or trivial rationales, we enforce minimum length constraint, discarding traces that are shorter than 100 words. This stage removes approximately 1.2% of the data based on length and template constraints  (Table 6)  . The detailed statistics of the processed subsets are reported in Table 6. N-gram De-duplication: We detect and remove templated or overly repetitive CoTs using an n-gram overlap criterion. Concretely, we flag CoTs that contain any 50-gram that repeats at least 3 times (i.e., = 50, frequency threshold = 3). Flagged traces are either discarded or re-generated with different random seed to encourage diversity. Correctness Verification: For tasks that have ground-truth answers, we extract the final answer from the <answer> tag and compare it against the answer extracted in Section 3.1, the incorrect CoTs are discarded. The detailed verification protocol is provided in Prompt 15. This process eliminated roughly 20% of instances containing potential hallucinations or incorrect reasoning traces, the consistency ratios across different subsets are summarized in Figure 3. The detailed verification results are shown in Table 7. Following the above data selection pipeline, we obtain high-quality MMFineReason-1.8M dataset with totally 5.1B solution tokens. We further uniformly sample 40k instances from MMFineReason1.8M to construct an RL training subset, while the remaining data are reserved for SFT. Difficulty Filtering for Efficient Training. Given the massive scale of our curated data, training on the entire corpus is computationally suboptimal due to the prevalence of redundant or trivial samples. To address this, we employ difficulty-based filtering strategy [54]. Specifically, we perform inference on each question using Qwen3-VL-4B-Thinking, generating four independent responses. We discard any example where the model provides correct answer in at least one attempt (pass rate = 0). This conservative filtering criterion ensures that we retain only genuinely challenging problems where moderate-sized model fails consistently. By discarding samples that contribute negligible training 7 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Figure 4: Dataset composition of MMFineReason-1.8M. The outer ring represents the proportion of major categories, and the inner ring shows the distribution of specific datasets. Note: To ensure the visual legibility of diverse domains, the segment sizes in this chart are scaled by the square root of N). The actual data distribution is dominated by Mathematics (79.4%), followed by sample counts ( Science (13.8%), Puzzle/Game (4.6%), and General/OCR (2.2%). signals, we direct our computational resources toward challenging data points that actively drive optimization, resulting in faster convergence. Specifically, we derive two challenging subsets from the full MMFineReason-1.8M corpus: MMFineReason-123K and MMFineReason-586K, by retaining samples with pass rate = 0 and pass rate = 1, respectively, which are well suited for efficient SFT and ablation studies. detailed analysis of the difficulty score distribution is presented in Section 4.2."
        },
        {
            "title": "4 MMFineReason Dataset",
            "content": "In this section, we present comprehensive analysis of MMFineReason. We begin by delineating the dataset composition in Section 4.1. Subsequently, we investigate visual diversity in Section 4.3, with specific focus on image category distribution and visual granularity. Finally, Section 4.4 provides statistical analysis of response characteristics and benchmarks our dataset against existing multi-modal reasoning datasets."
        },
        {
            "title": "4.1 Dataset Composition",
            "content": "MMFineReason contains approximately 1.8 million (specifically 1,770,926) high-quality multimodal reasoning samples. Figure 4 summarizes the domain distribution of the dataset, which is strategically weighted towards symbolic and logic-heavy tasks: Mathematics (79.4%), Science (13.8%), Puzzle/Game (4.6%), and General/OCR (2.2%). Mathematics (79.4%). This domain forms the backbone of our reasoning supervision, primarily sourced from the massive MMR1 [17] dataset (1.27M). To ensure diversity in problem types, we integrate WaltonColdStart [45] (42.4K) and ViRL39K [32] (32.0K). We further enhance geometric and symbolic reasoning capabilities by including Euclid30K [19] (22.6K), MMK12 [33] (13.8K), Geo170K [46] (8.9K), and Geo3K [46] (4.7K). Finally, we incorporate specialized subsets including mm-openr1 [29] (4.0K) and the comprehensive WeMath family [39] (Standard 4.5K, Pro 3.3K, and SFT 0.7K). Science (13.8%). Science constitutes significant portion of the dataset, anchored by VisualWebInstruct [46] (157.3K) and BMMR [48] (54.6K). These are complemented by smaller, high-quality 8 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Figure 5: Pass rate distribution across sub-datasets. Datasets are sorted by descending mean pass rate (easiest to hardest). The bubble chart encodes sample proportion via size and color intensity, overlaid with mean pass rate trendline. collections such as TQA [46] (10.4K) and AI2D [46] (10.6K), along with domain-specific subsets like Zebra-CoT [18] (5.9K) and ScienceQA [46] (5.8K). Puzzle/Game (4.6%). This domain targets strategic planning and abstract pattern recognition. It is dominated by GameQA-140K [42] (71.7K) and further enriched by Raven [46] (7.5K), VisualSphinx [12] (1.2K), and PuzzleQA [8] (1.4K). General/OCR (2.2%). In contrast to general-data-heavy training recipes, we adopt reasoningdominant composition. Empirically, the base models visual perception is already robust, and extensive general data often yields diminishing returns for reasoning tasks. Therefore, we include only 38.7K general-purpose samples from LLaVA-CoT [51], serving as regularization set to preserve broad visual and OCR capabilities without diluting the reasoning-centric supervision."
        },
        {
            "title": "4.2 Difficulty Distribution Analysis",
            "content": "Building on the filtration technique for efficient training described in Section 3.3, Figure 5 illustrates the pass rate distribution across various sub-datasets using bubble chart overlaid with mean pass rate line. The datasets are arranged on the x-axis by mean pass rate in descending order (left to right), ensuring visual progression from easiest to hardest, while the y-axis represents the Pass Rate from 0.00 (hardest) to 1.00 (easiest). Bubble size and color intensity denote the proportion of samples at specific pass rate level. Notably, science-oriented sub-datasets such as ScienceQA, AI2D, and TQA exhibit relatively high pass rates. These datasets are generally considered simpler because they feature clean, synthetic diagrams and primarily rely on knowledge derived from primary and secondary school textbooks. Furthermore, they lack the visual complexity and expert depth required by modern standards and are predominantly Multiple Choice Questions, which limits the solution space. Conversely, puzzle and game-based datasets like GameQA-140K, Raven, and VisualSphinx demonstrate the lowest pass rates. These sub-datasets require multi-step abstract reasoning and fine-grained visual discrimination, resulting in significant concentration of samples with low pass rates. Furthermore, we observe that the proportion of samples in the intermediate range remains sparse across all sub-datasets. This is because the reasoning process involved often follows binary success outcome. Unlike tasks where partial understanding might yield closer approximations, these logic and puzzle problems require rigorous, unbroken chain of deduction; single failure in any reasoning step or visual 9 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Table 1: Comparison of token length statistics across datasets. We report the distribution metrics (mean, median, and percentiles) for reasoning chains (CoT) and image captions. The bold values indicate the highest statistics, highlighting the significant reasoning depth of MMFineReason. dataset type count total tokens mean median std dev min max p75 p95 MMFineReason (Ours) CoT CoT OpenMMReasoner CoT HoneyBee MMFineReason (Ours) Caption Caption HoneyBee 1770926 874357 2481229 1770926 5152806394 590096263 2636405079 1079313259 431096653 2909.67 674.89 1062.54 609.46 299.39 2038 180 972 582 2463.83 1477.53 428.00 184.88 157.99 239 26 203 1 21 16316 16483 7190 5187 1321 102 745 494 201 3569 464 1298 688 350 8207 3318 1931 920 Figure 6: Token length analysis of MMFineReason. We present the internal domain distribution of CoT lengths (left), followed by external comparisons of CoT depth (mid) and caption richness (right) against prior works. MMFineReason consistently shows higher token counts, indicating greater complexity. grounding typically leads to completely incorrect answer, thereby polarizing the distribution toward the extremes (0 or 1) and leaving the middle interval empty."
        },
        {
            "title": "4.3 Visual Semantic Analysis",
            "content": "To quantify the visual diversity of our dataset, we adopt caption-based classification strategy. By generating detailed descriptions and categorizing the images based on their semantic content, we provide fine-grained analysis of the visual distribution. Visual Content Analysis. Recent studies [51, 49] demonstrate that detailed image captioning significantly enhances multimodal reasoning capabilities. Building on this insight, we leverage the powerful Qwen3-VL-235B-A22B-Thinking model to generate structured, high-fidelity captions for the entire MMFineReason dataset. As shown in Figure 6c and Table 1, MMFineReason provides significantly denser semantic information, averaging 609 tokens per captionmore than double the length of HoneyBee [4] (299 tokens). Crucially, distinct from HoneyBee which only incorporates captions for subset of its samples (approx. 58%), MMFineReason guarantees 100% caption coverage for all image-question pairs. This comprehensive coverage ensures that every reasoning chain is explicitly supported by fine-grained visual details, providing more consistent and robust foundation for multimodal learning than baselines with partial visual context. Visual Topic Diversity. Leveraging the generated captions as high-level semantic tags, we taxonomize the dataset into distinct STEM/diagrammatic and natural-image categories; the distribution is detailed in Table 2. The corpus is predominantly composed of STEM and diagrammatic content: geometric diagrams, mathematical plots, and logic puzzles collectively account for 75.3% of the full set. This reflects the MMFineReasons emphasis on symbolic and diagram-centric reasoning. While natural images constitute smaller fraction, they exhibit significant internal diversityranging from 10 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Table 2: Image category statistics by group (STEM vs. Natural). Percentages are normalized within each group. STEM / Diagrammatic Image Count Ratio Natural Image Count Ratio Geometric Diagram Mathematical Plot / Chart Puzzle / Logic Diagram Diagram / Flowchart Table / Matrix Textbook Illustration Abstract Mathematical Representation Spatial Reasoning Scene Physics / Mechanics Diagram Biological Structure Experimental Setup Geological / Earth Science Diagram Circuit / Network Diagram Astronomy / Space Visualization Molecular / Chemical Diagram 869,959 332,375 107,276 100,941 76,683 66,715 57,569 35,197 34,386 17,805 10,963 9,587 9,421 6,718 3,735 50.02% Urban / Street Scene 19.11% Indoor / Interior Scene 6.17% Human Portrait / Activity 5.80% Sports / High-Motion Scene 4.41% Document / Text Image 3.84% Animal / Wildlife Scene 3.31% Natural Landscape Scene 2.02% Food / Beverage Item 1.98% Vehicle / Machinery Object 1.02% Product / Still Life Object 0.63% Artwork / Illustration 0.55% Technical / Surveillance / Medical 0.54% 0.39% 0.21% 5,307 4,772 3,789 3,659 2,900 2,125 1,969 1,835 1,460 1,125 325 68 18.09% 16.27% 12.92% 12.47% 9.89% 7.24% 6.71% 6.26% 4.98% 3.84% 1.11% 0.23% urban scenes and documents to astronomical visualizations. This intentional distribution ensures MMFineReason prioritizes fine-grained mathematical reasoning while retaining complementary natural subset to assess generalization beyond synthetic diagrams. We leave the investigation of optimal mixing ratios between these distributions to future work."
        },
        {
            "title": "4.4 Response Analysis",
            "content": "Following the generation protocol described in Section 3.2, we present statistical analysis of the resulting generations, specifically characterizing the distribution of domain-specific response lengths and comparing them with existing recent multimodal reasoning datasets, such as OpenMMReasoner [58] and HoneyBee [4]. Reasoning Depth Comparison to Other Datasets. We quantify reasoning depth by analyzing the CoT token length distributions using the Qwen3-VL tokenizer (Figure 6b and Table 1). MMFineReason exhibits substantially more elaborate reasoning process than existing baselines, achieving an average CoT length of 2,910 tokensapproximately 2.7 longer than HoneyBee (1, 063) and 4.3 longer than OpenMMReasoner (675). Notably, the median token count of MMFineReason (2, 038) is nearly 2.1 that of HoneyBee (972) and over 11.3 that of OpenMMReasoner (180). This disparity indicates that while baselines often provide concise or superficial rationales, MMFineReason consistently delivers extensive, step-by-step derivations. Furthermore, the extended tail of our distribution (Max: 16, 316) underscores the datasets capacity to handle highly complex, multi-stage reasoning tasks requiring deep cognitive traversal. Token Length Distribution of MMFineReason. To evaluate the structural quality and domain adaptability of MMFineReason, we analyze the token length distribution and reasoning density across four curated domains. Using the Qwen3-VL tokenizer on the generated responses (qwen3vl 235b - thinking response), we observe distinct characteristics (visualized in Figure 6a): Puzzle & Game: This domain exhibits the highest average length (4, 810 tokens), reflecting the most intensive reasoning requirements. The distribution is driven by the necessity for rigorous visual-spatial verification. In tasks such as Raven (avg. 7, 745 tokens) and VisualSphinx (avg. 6, 833 tokens), the model must explicitly hypothesize rules and verify candidate options 11 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods sequentially, resulting in dense System-2 reasoning traces that significantly exceed those of other domains. Mathematics: The Mathematics domain demonstrates high information density with an average length of 2, 950 tokens. Distinguished by exceptional symbolic rigor, this category exhibits density of LaTeX markers more than double that of scientific tasks. The structure is distinct from natural language tasks, heavily populated with step-by-step symbolic derivations (e.g., Euclid30K, Geo170k) essential for precise calculation. Science: Averaging 2, 305 tokens across 245k samples, this domain bridges abstract logic and real-world context. The data reflects dual process: the model must ground visual entities (Perception) before applying domain-specific knowledge (Causal Inference). The result is balanced reasoning structure combining substantial textual explanation with moderate symbolic usage. General/OCR: Serving as regularization baseline, this category remains concise (avg. 1, 262 tokens). Primarily derived from LLaVA-CoT, these samples prioritize direct visual grounding over complex logical deduction. This preserves the models System-1 perception capabilities, mitigating reasoning hallucinationthe tendency to over-generate complex rationales for simple perceptual tasks."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we empirically validate the effectiveness of our proposed reasoning datasets, MMFineReason. We begin by detailing the experimental setup in Section 5.1, followed by presentation of the main results in Section 5.2. Section 5.3 analyzes the training dynamics of the Reinforcement Learning (RL) stage. We then examine the impact of our data filtering strategy via ablation studies in Section 5.5. Finally, we investigate the fine-grained contributions of individual sub-datasets in Section 5.6."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Training Details. We use LLaMA-Factory [62] and VeRL [40] as our SFT and RL training frameworks, respectively. The training configurations are summarized in Table 8 and Table 9. We train the Qwen3VL-2B/4B/8B-Instruct models under the same experimental setup. Supervised Fine-Tuning (SFT). As shown in Table 8, we optimize the model using AdamW with learning rate of 1e-5 and cosine decay scheduler. To maximize training throughput and reduce memory fragmentation, we utilize liger kernel and enable sequence packing with length of 32,768. The input images are resized to resolution of 768 768 during this stage to balance efficiency and performance. We train the models for 3 epochs with global batch size of 32. Reinforcement Learning (RL). For the RL stage, we adopt the GSPO algorithm [61] to enhance the reasoning capabilities of the model. As detailed in Table 9, we set the learning rate to 1e-6 with constant scheduler to ensure stable convergence. key component of our setup is the generation of = 16 rollouts for each prompt to estimate the group-dependent baseline, which reduces the variance of the gradient estimator. The training spans 300 steps with batch size of 256. Evaluation Setup. We evaluate our models on VLMEvalKit [11]. To strictly assess the models reasoning reliability, we employ greedy decoding (Temperature = 0) for all benchmarks. Notably, as shown in Table 10, we increase the maximum image resolution to 2048 2048 during inference. More evaluation details are in Appendix B.2. 12 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Baselines. Our baselines fall into three main categories: (1) Closed-source VLMs, including Gemini2.5-Flash [9] and GPT5-mini High [35]; (2) Open-weight VLMs, specifically Qwen3-VL-8B-Thinking, Qwen3-VL-30B-A3B-Thinking and Qwen3-VL-32B-Thinking; and (3) Open-source VLMs. For MMR1 [17] and HoneyBee [4], we fine-tune Qwen3-VL-8B-Instruct on their respective SFT datasets to ensure fair comparison. For OMR-7B [58], we directly report the RL results from the original paper and our evaluation using the officially released checkpoints. All models are evaluated in thinking mode to ensure consistent comparison. Benchmarks & Evaluation. To ensure comprehensive assessment, we evaluate our model across diverse suite of benchmarks spanning three key domains: STEM & Puzzle: MMMUval [56], MathVistamini [31], MathVisiontest [43], MathVersemini [59], Dynamath [63], LogicVista [50], VisuLogic [52], ScienceQA [30]. General VQA: RealWorldQA [47], , MMBench-EN [25], MMStar [6]. Document Understanding: AI2D [14], CharXivreas. [44], CharXivdesc. [44]."
        },
        {
            "title": "5.2 Main Results",
            "content": "Table 3: Comparison of MMFineReason (MFR) models with state-of-the-art models on various multimodal benchmarks. Benchmarks MMMUval MathVistamini MathVisiontest MathVersemini DynaMathtest LogicVistatest VisuLogictest ScienceQA RWQAtest MMBench-EN MMStartest AI2Dtest CharXivreas. CharXivdesc. Avg Closed-source VLMs Open-weight VLMs Open-source VLMs Gemini-2.5 Flash GPT5 mini Qwen3-VL 8B Qwen3-VL 30B-A3B Qwen3-VL 32B MMR1 8B HoneyBee 8B OMR 7B MFR 2B 77.7 79.4 64.3 77.7 75.9 67.3 31.0 97. 76.0 87.0 76.5 88.7 61.7 90.1 75.0 79.0 79.1 71.9 78.8 81.4 71.4 27.2 96.9 79.0 86.6 74.1 88.2 68.6 89. 76.5 74.1 81.4 62.7 77.7 73.2 65.1 27.5 94.8 73.5 85.3 75.3 84.9 53.0 85.9 72.5 76.0 81.9 65.7 79.6 80.1 65.8 26.6 96. 77.4 87.0 75.5 86.9 56.6 86.9 74.5 78.1 85.9 70.2 82.6 82.0 70.9 32.4 97.2 78.4 89.5 79.4 88.9 65.2 90. 77.9 62.8 75.3 48.4 67.3 73.6 54.6 25.4 95.4 71.0 86.9 69.3 83.4 48.8 81.5 67.4 63.1 71.9 37.4 60.9 69.4 47.8 25.9 95. 70.5 87.4 73.3 86.0 47.4 75.8 65.1 57.8 79.5 43.6 63.8 69.1 50.0 24.4 96.8 69.4 85.9 69.0 85.0 46.1 73. 65.3 54.8 74.6 45.3 69.2 71.4 53.8 28.3 94.4 68.2 84.5 67.7 82.5 45.4 74.3 65.3 Ours MFR 4B MFR 8B 69.6 82.2 61.3 78.7 80.6 67.6 29.8 95.8 74.9 88.7 72.8 86.5 58.1 87.7 73. 71.3 81.7 67.1 81.5 83.4 68.5 30.5 97.5 75.6 88.9 75.2 87.9 60.0 90.8 75.7 Table 3 presents comprehensive comparison of our MMFineReason models (MFR) against SOTA open-source models and proprietary vision-language models. Our models establish new SOTA results for their size class. Our MFR-2B model already approaches existing open-source 8B models, the MFR-4B model surpasses Qwen3-VL-8B-Thinking, and our MFR-8B model even outperforms Qwen3VL-30B-A3B-Thinking while approaching the performance of Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Dominance in Mathematical & Logical Reasoning. MFR models exhibit substantial improvements over competing methods. Our MFR-8B surpasses the same-sized Qwen3-VL-8B-Thinking by large margin and outperforms the significantly larger Qwen3-VL-30B-A3B-Thinking on nearly all mathematical benchmarks. On DynaMath, MFR-8B achieves 83.4%, outperforming Qwen3-VL-32B-Thinking at 82.0% and Qwen3-VL-30B-A3B-Thinking at 76.7%. On MathVerse, MFR-8B reaches 81.5%, approaching Qwen3-VL-32B-Thinking at 82.6% while surpassing Qwen3-VL-30B-A3B-Thinking at 79.6%. 13 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Strong Generalization Across Domains. surprising and notable observation is that our MFR models exhibit strong generalization ability, maintaining competitive performance on both general understanding and chart reasoning benchmarks. Specifically, on RWQA, MFR-8B achieves 75.6%, improving over open-source baselines such as MMR1-8B at 71.0% and HoneyBee-8B at 70.5%. On CharXivdesc., MFR-8B achieves 89.9%, approaching Qwen3-VL-32B-Thinking at 90.2% and closedsource models. It is worth noting that our training data contains minimal chart or real-world related samples, yet the enhanced reasoning capabilities generalize effectively to these general domains. Superior Data Efficiency vs. Open-source Baselines. key finding is the significant performance gap between MFR and other open-source baselines. On MathVision, MFR-8B achieves 67.1%, outperforming HoneyBee-8B at 37.4% and OMR-7B at 36.6% by over 30 absolute points. On MathVerse, MFR-8B at 81.5% surpasses MMR1-8B at 67.3% and HoneyBee-8B at 60.9% by large margin. These results demonstrate that the quality of reasoning chains in MFR is far superior to scale-focused strategies. Table 4: Main results of different model scales across various multimodal benchmarks. We compare our MMFineReason (MFR) models against the base Qwen3-VL variants. Inst. and Think. denote Qwen3-VL-Instruct and Qwen3-VL-Thinking, respectively; SFT and RL refer to our MFR-SFT and MFR-Thinking models. Benchmarks MMMUval MathVistamini MathVisiontest MathVersemini DynaMathtest LogicVistatest VisuLogictest ScienceQA 53.4 61.3 31.6 52.1 54.2 35.8 11.5 87.4 RWQAtest 63.9 MMBench-EN 78.4 MMStartest 58.3 AI2Dtest CharXivreas. CharXivdesc. Avg 76.9 26.8 62.3 53. 2B Models 4B Models 8B Models Inst. Think. SFT RL Inst. Think. SFT RL Inst. Think. SFT RL 61.4 73.6 45.9 66.9 66.7 50.0 25.4 88.0 69.5 79.9 68.1 80.4 37.1 70. 63.1 54.6 73.3 40.9 70.4 68.7 52.8 24.7 92.3 67.9 83.2 63.6 78.5 39.0 74.1 63.1 54.8 74.6 45.3 69.2 71.4 53.8 28.3 94. 68.2 84.5 67.7 82.5 45.4 74.3 65.3 67.4 73.7 51.6 46.8 65.3 53.2 19.0 88.0 70.9 83.9 69.8 84.1 39.7 76. 63.5 70.8 79.5 60.0 75.2 74.4 61.1 30.2 94.1 73.2 84.6 73.2 84.9 50.3 83.9 71.1 69.3 80.1 62.4 78.4 79.9 66.7 27.8 96. 71.5 88.7 73.0 86.1 55.9 87.7 73.2 69.6 82.2 61.3 78.7 80.6 67.6 29.8 95.8 74.9 88.7 72.8 86.5 58.1 87. 73.9 69.6 77.2 53.9 62.1 67.7 55.3 22.5 95.4 71.5 84.5 70.9 85.7 46.4 83.0 67.6 74.1 81.4 62.7 77.7 73.2 65.1 27.5 94. 73.5 85.3 75.3 84.9 53.0 85.9 72.5 71.3 81.2 67.6 82.2 82.6 68.7 29.9 95.4 74.1 87.8 74.8 86.5 58.4 89. 75.0 71.3 81.7 67.1 81.5 83.4 68.5 30.5 97.5 75.6 88.9 75.2 87.9 60.0 90.8 75."
        },
        {
            "title": "5.3 Effectiveness of Different Training Stages",
            "content": "Table 4 presents the results of SFT and RL training across different model scales. We observe that each training stage contributes to the final performance. SFT Drives Major Gains in Reasoning. For mathematical and logical reasoning, the largest performance improvements come from SFT. Compared to Qwen3-VL-Instruct, MFR-SFT achieves substantial gains across all model scales. For the 8B model, SFT improves MathVision from 53.90% to 67.56% and LogicVista from 55.30% to 68.68%. Similar trends are observed at smaller scales: the 2B model gains +3.5% on MathVerse and +2.8% on LogicVista after SFT. RL Enhances Generalization. We find that RL training significantly improves generalization on general understanding and chart benchmarks. For the 2B model, RL improves AI2D from 78.47% to 82.51% and CharXivreas. from 38.96% to 45.38%. For the 8B model, RL brings consistent gains on RWQA, SciQA, and CharXivdesc., demonstrating that RL effectively enhances the models ability to generalize beyond the reasoning-focused training distribution. 14 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Figure 7: Performance comparison of MMFineReason against existing open-source datasets across different data scales and model sizes. Bubble size indicates model parameter count (2B, 4B, 8B), while color intensity represents training data volume. Dashed lines denote the performance of strong baselines. MMFineReason demonstrates superior data efficiency, achieving higher accuracy with significantly fewer samples and smaller model parameters compared to MMR1 and HoneyBee. RL Shows Variance on Math Benchmarks. However, we also observe that RL exhibits some variance on mathematical benchmarks. While RL improves DynaMath across all scales, it causes slight drops on MathVision for 4B and 8B models. We hypothesize that since the model has already learned most patterns during SFT, further RL gains require more diverse or challenging data. Exploring more effective RL data strategies remains an important direction for future work."
        },
        {
            "title": "5.4 Scaling Frontiers and Data Efficiency",
            "content": "In this section, we evaluate the effectiveness of our proposed data strategy by benchmarking MMFineReason against existing SOTA open-source datasets and models. As illustrated in Figure 7, our approach demonstrates superior scaling properties across data volume, data quality, and model capacity. Superior Data Quality and Peak Performance. We first examine the impact of data quality by comparing models trained on full datasets under the same parameter scale (8B). MMFineReason-1.8M achieves peak score of 75.7, establishing substantial lead over existing open-source datasets such as MMR1-1.6M (67.4) and HoneyBee-2.5M (65.1). Despite utilizing smaller sample size than HoneyBee (1.8M vs. 2.5M), our model achieves performance gain of +10.6 points. This significant gap highlights that the fine-grained reasoning logic in our dataset provides much denser supervision signals than standard caption-based or coarse-reasoning datasets. Extreme Data Efficiency. Beyond peak performance, MMFineReason exhibits remarkable efficiency in low-data regimes. The most striking finding is that our minimal subset, MMFineReason-123K, already achieves score of 73.3. This result significantly outperforms models trained on the full 15 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Figure 8: Ablation studies on data and training strategies. Left: caption augmentation brings marginal or negative gains on STEM benchmarks, likely redundant with visual cues in long CoT. Right: ultrahigh resolution (20482) shows diminishing returns over 7682/2562 for geometry and chart tasks, though still beneficial for natural images (RWQA). HoneyBee (2.5M) and MMR1 (1.6M) datasets. By utilizing only 5% of the data volume of comparable benchmarks, MMFineReason matches or exceeds their performance. This cross-over effect suggests that rigorous filtering and high-quality rationale construction effectively eliminate the redundancy found in large-scale datasets, allowing for faster convergence with fraction of the computational budget. SOTA Comparison and Parameter Efficiency. Finally, we benchmark our models against leading open-weights and commercial baselines (represented by dashed lines in Figure 7). Our high-quality data enables smaller models to punch above their weight class. Specifically, our 4B model achieves score of 73.9, surpassing the widely-used Qwen3-VL-8B-Thinking (72.5), which demonstrates that superior data quality can effectively compensate for 2 reduction in model parameters. Scaling up to 8B parameters, our model establishes new state-of-the-art for its size class with score of 75.7. It not only outperforms the significantly larger Qwen3-VL-30B-A3B-Thinking (74.5) but also exceeds the performance of the commercial baseline Gemini-2.5-Flash (75.0). These results validate that MMFineReason enables efficient open-weights models to compete directly with, and even surpass, proprietary and significantly larger foundation models."
        },
        {
            "title": "5.5 Ablation Studies",
            "content": "To validate the effectiveness of our proposed data strategy and training settings, we conduct two sets of ablation studies to investigate the impact of caption augmentation strategies and input resolution on model performance. Trade-off of Caption Augmentation. Figure 8 (left) presents the effects of introducing caption augmentation (CapAug), specifically by prepending image captions enclosed within <caption>...</caption> tags to the response. The experiments reveal clear trade-off in model performance. Specifically, CapAug slightly boosts logical reasoning capabilities, with LogicVista improving by 1.1%. This can likely be attributed to captions forcing the model to parse the layout and complete information of images, facilitating better understanding of structure and logical relationships. However, on STEM visual reasoning tasks like MathVista and MathVision, it does not bring significant performance 16 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Figure 9: Performance landscape of distilled sub-datasets. gains. This is likely because the Long CoT already contains sufficient visual information required for reasoning, rendering the caption information redundant. Consequently, we do not adopt CapAug in our final version. Scaling Effect of Input Resolution. The analysis of input resolution (Max Pixels) in Figure 8 (right) shows that simply increasing resolution does not always yield performance gains. Surprisingly, ultra-high resolution (20482) does not outperform medium-to-low resolution settings on multiple benchmarks. For example, on MathVista and CharXiv, the performance of 7682 or even 2562 is superior to that of 20482. Although 20482 achieves slight advantage on MathVision, the overall benefit is limited considering the significantly increased computational cost. This phenomenon suggests that: (1) the core features of many current benchmark problems (especially geometry and charts) do not rely on pixel-level ultra-high definition; and (2) excessive resolution results in overly long visual token sequences, introducing redundancy and increasing the difficulty for the attention mechanism to capture key features. However, notable exception is observed on RealWorldQA, which focuses on general natural images. On this benchmark, scaling up to 20482 yields stable performance gains. This is likely because real-world scenarios often contain fine-grained details, small objects, or dense text embedded in complex backgrounds, which necessitate higher pixel density for precise recognition. In contrast to the sparse and structural nature of diagrams, natural images require high resolution to resolve these subtle visual cues effectively. Balancing performance across domains and computational efficiency, we adopt 7682 as our default setting."
        },
        {
            "title": "5.6 Distilled Sub-Dataset Analysis",
            "content": "The experimental results in Figure 9 characterize how training data propertiesspecifically sample size and domain compositionshape downstream STEM reasoning performance. Across all subsets, 17 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods we observe the following key findings. Diminishing Returns of Scale and the Pareto Frontier. By juxtaposing MMR1 (N 1.5M) with ViRL39K (N 39K), we observe significant non-linear relationship between data volume and reasoning performance. While MMR1 establishes the upper bound with 73.60% accuracy, ViRL39K achieves 72.79% accuracyretaining 98.9% of the relative performance using only 2.4% of the data volume. This constructs new Pareto frontier, indicating that systematically cleaned, reformatted, and verified data (as seen in ViRL39K) can drastically reduce training costs. It further suggests that blind data scaling encounters severe diminishing returns beyond certain threshold in multimodal reasoning tasks. Latent Capability Activation via High-Density Instruction. The most significant outlier in our scaling analysis is WeMath2.0-SFT. Despite comprising negligible 0.05% of the total data volume (N = 814), it achieves reasoning accuracy of 70.98%, effectively matching the performance of datasets three orders of magnitude larger, such as MMR1 (73.60%). This disproportionate efficiency validates the Knowledge-Oriented Chain-of-Thought (KO-CoT) paradigm. We hypothesize that large-scale pre-training endows models with latent domain knowledge, but often leaves them without the specific reasoning syntax required for complex problem-solving. WeMath2.0-SFT functions not as knowledge source, but as high-efficiency catalyst, aligning the models internal representations with structured reasoning paths. This suggests that for foundation models, small set of high-quality, reasoningdense instructions is sufficient to trigger capabilities that otherwise remain dormant in massive, noisy corpora. Not all hard reasoning transfers: puzzle/game datasets lag. Puzzle/game-centric datasets are consistently weaker: GameQA-140K (122.9K, 67.67), Raven (20.3K, 68.08), and PuzzleQA (2.0K, 68.82). Despite their rigorous generation/verification (e.g., engine-derived annotations in GameQA-140K), these tasks emphasize planning-like search, symbolic state transitions, and abstract relational rules that may be mismatched with the dominant evaluation distribution (often math/science QA). Another plausible factor is that many puzzle/game solutions resemble program-execution traces; if the target model is not explicitly trained to internalize algorithmic state updates, these examples contribute less to general multimodal QA performance. Geometry-only corpora underperform, indicating narrow visual grammars. Geo3K (9.0K, 66.38) and Geo170K (11.8K, 67.02) are among the lowest performers, despite being in-domain for diagram reasoning. This highlights critical nuance: geometry datasets can be structurally narrow (limited diagram styles, repetitive constructions, constrained linguistic patterns), which reduces their marginal utility for broad reasoning. Euclid30K performs substantially better (26.7K, 70.79), supporting the hypothesis that formalized reasoning structure and better-designed problem diversity (plane/solid geometry, proof-like steps) are more important than simply adding more geometry instances. Impact of Disciplinary Breadth on Generalization. Comparing GameQA-140K (67.67%) and BMMR (72.94%) highlights the importance of disciplinary diversity in Chain-of-Thought (CoT) data. Although GameQA offers larger volume of samples (140K), its scope is restricted to closed-world game logic. BMMR, despite being smaller (80K), spans over 300 academic disciplines. This breadth allows the model to internalize more generalized reasoning structure, suggesting that for general-purpose vision-language models, diversity in the reasoning domain is stronger driver of performance than the depth of any single task type. 18 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods"
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we view multimodal reasoning primarily as data-centric problem rather than purely model-centric one. MMFineReason shows that strong multimodal reasoning capabilities can be systematically induced through structured data design, without relying on excessive model scaling or proprietary supervision. Beyond the dataset itself, this study establishes scalable framework for reasoning data engineering, covering reasoning supervision, difficulty-aware structuring, and data composition. Results across multiple model scales demonstrate that well-curated reasoning data can deliver substantial performance gains and parameter efficiency, enabling smaller open models to compete with much larger reasoning-oriented systems. Moreover, our findings indicate that reasoningoriented supervision functions as general capability amplifier, benefiting both complex reasoning tasks and broader multimodal understanding. MMFineReason therefore serves as both benchmark resource and reproducible methodology for building open, efficient multimodal reasoning systems. 19 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Xiang An, Yin Xie, Kaicheng Yang, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. URL https://arxiv.org/abs/2509.23661. [3] Shuai Bai et al. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. URL https://arxiv.org/ abs/2511.21631. [4] Hritik Bansal, Devandra Singh Sachan, Kai-Wei Chang, Aditya Grover, Gargi Ghosh, Wen-tau Yih, and Ramakanth Pasunuru. Honeybee: Data recipes for vision-language reasoners. arXiv preprint arXiv:2510.12225, 2025. [5] Mengzhang Cai, Xin Gao, Yu Li, Honglin Lin, Zheng Liu, Zhuoshi Pan, Qizhi Pei, Xiaoran Shang, Mengyuan Sun, Zinan Tang, Xiaoyang Wang, Zhanping Zhong, Yun Zhu, Dahua Lin, Conghui He, and Lijun Wu. Opendataarena: fair and open arena for benchmarking post-training dataset value, 2025. URL https: //arxiv.org/abs/2512.14051. [6] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. [7] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision (ECCV), 2024. [8] Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning skills of language models with abstract visual patterns. ACL, 2024. [9] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [10] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [11] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. [12] Yichen Feng, Zhangchen Xu, Fengqing Jiang, Yuetai Li, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, and Radha Poovendran. Visualsphinx: Large-scale synthetic vision logic puzzles for rl. arXiv preprint arXiv:2505.23977, 2025. [13] Google DeepMind. Gemini 3 technical report. Technical report, Google, 2025. URL https://storage. googleapis.com/deepmind-media/Model-Cards/Gemini-3-Pro-Model-Card.pdf. [14] Tuomo Hiippala, Malihe Alikhani, Jonas Haverinen, Timo Kalliokoski, Evanfiya Logacheva, Serafina Orekhova, Aino Tuomainen, Matthew Stone, and John Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55(3):661688, 2021. [15] Hugging Face. Open r1: fully open reproduction of deepseek-r1. https://github.com/huggingface/ open-r1, 2025. GitHub repository. [16] Yunjie Ji et al. Am-thinking-v1: Advancing the frontier of reasoning at 32b scale. arXiv:2505.08311, 2025. URL https://arxiv.org/abs/2505.08311. arXiv preprint [17] Sicong Leng, Jing Wang, Jiaxi Li, Hao Zhang, Zhiqiang Hu, Boqiang Zhang, Yuming Jiang, Hang Zhang, Xin Li, Lidong Bing, et al. Mmr1: Enhancing multimodal reasoning with variance-aware sampling and open resources. arXiv preprint arXiv:2509.21268, 2025. 20 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods [18] Ang Li, Charles Wang, Deqing Fu, Kaiyu Yue, Zikui Cai, Wang Bill Zhu, Ollie Liu, Peng Guo, Willie Neiswanger, Furong Huang, et al. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746, 2025. [19] Shijie Lian, Changti Wu, Laurence Tianruo Yang, Hang Yuan, Bin Yu, Lei Zhang, and Kai Chen. Euclids gift: Enhancing spatial perception and reasoning in vision-language models via geometric surrogate tasks. arXiv preprint arXiv:2509.24473, 2025. [20] Honglin Lin, Zhuoshi Pan, Qizhi Pei, Xin Gao, Yu Li, Mengzhang Cai, Conghui He, and Lijun Wu. MetaLadder: Ascending mathematical solution quality via analogical-problem reasoning transfer. In Findings of the Association for Computational Linguistics: EMNLP 2025. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025.findings-emnlp.232/. [21] Honglin Lin, Qizhi Pei, Xin Gao, Zhuoshi Pan, Yu Li, Juntao Li, Conghui He, and Lijun Wu. Scaling code-assisted chain-of-thoughts and instructions for model reasoning. arXiv preprint arXiv:2510.04081, 2025. [22] Honglin Lin, Chonghan Qin, Zheng Liu, Qizhi Pei, Yu Li, Zhanping Zhong, Xin Gao, Yanfeng Wang, Conghui He, and Lijun Wu. Scientific image synthesis: Benchmarking, methodologies, and downstream utility. arXiv preprint arXiv:2601.17027, 2026. URL https://arxiv.org/abs/2601.17027/. [23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [24] Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, and Kai Chen. Compassverifier: unified and robust verifier for llms evaluation and outcome reward, 2025. URL https://arxiv.org/abs/2508.03686. [25] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [26] Zheng Liu, Hao Liang, Bozhou Li, Wentao Xiong, Chong Chen, Conghui He, Wentao Zhang, and Bin Cui. Synthvlm: Towards high-quality and efficient synthesis of image-caption datasets for vision-language models, 2025. URL https://arxiv.org/abs/2407.20756. [27] Zheng Liu, Mengjie Liu, Jingzhou Chen, Jingwei Xu, Bin Cui, Conghui He, and Wentao Zhang. Fusion: Fully integration of vision-language representations for deep cross-modal understanding, 2025. URL https://arxiv.org/abs/2504.09925. [28] Zheng Liu, Honglin Lin, Chonghan Qin, Xiaoyang Wang, Xin Gao, Yu Li, Mengzhang Cai, Yun Zhu, Zhanping Zhong, Qizhi Pei, Zhuoshi Pan, Xiaoran Shang, Bin Cui, Conghui He, Wentao Zhang, and Lijun Wu. Chartverse: Scaling chart reasoning via reliable programmatic synthesis from scratch, 2026. URL https://arxiv.org/abs/2601.13606. [29] lmms lab. Multimodal open r1, 2025. URL https://github.com/EvolvingLMMs-Lab/open-r1-multimodal. [30] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [31] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [32] Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-Reasoner: Advancing LLM reasoning across all domains. arXiv:2505.14652, 2025. URL https://arxiv.org/abs/2505.14652. [33] Fanqing Meng et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. In Findings of the Association for Computational Linguistics: EMNLP 2025, 2025. URL https://arxiv.org/abs/2503.07365. [34] Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, Zhenjiang Jin, Guang Liang, Rui Zhang, Wenzheng Zhang, Yuan Qu, Zhifei Ren, Yuefeng Sun, Yuanhong Zheng, Dongsheng Ma, Zirui Tang, Boyu Niu, Ziyang Miao, Hejun Dong, Siyi Qian, 21 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Junyuan Zhang, Jingzhou Chen, Fangdong Wang, Xiaomeng Zhao, Liqun Wei, Wei Li, Shasha Wang, Ruiliang Xu, Yuanyuan Cao, Lu Chen, Qianqian Wu, Huaiyu Gu, Lindong Lu, Keming Wang, Dechen Lin, Guanlin Shen, Xuanhe Zhou, Linfeng Zhang, Yuhang Zang, Xiaoyi Dong, Jiaqi Wang, Bo Zhang, Lei Bai, Pei Chu, Weijia Li, Jiang Wu, Lijun Wu, Zhenxiang Li, Guangyu Wang, Zhongying Tu, Chao Xu, Kai Chen, Yu Qiao, Bowen Zhou, Dahua Lin, Wentao Zhang, and Conghui He. Mineru2.5: decoupled vision-language model for efficient high-resolution document parsing, 2025. URL https://arxiv.org/abs/2509.22186. [35] OpenAI. Gpt-5 system card. gpt-5-system-card.pdf. Technical report, OpenAI, 2025. URL https://cdn.openai.com/ [36] OpenThoughts Team. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. URL https://arxiv.org/abs/2506.04178. [37] Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, H. Vicky Zhao, Conghui He, and Lijun Wu. LEMMA: Learning from errors for MatheMatical advancement in LLMs. Association for Computational Linguistics, 2025. [38] Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming, Xin Gao, Conghui He, and Rui Yan. MathFusion: Enhancing mathematical problem-solving of LLM through instruction fusion. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025.acl-long.367/. [39] Runqi Qiao, Qiuna Tan, Guanting Dong, et al. We-math: Does your large multimodal model achieve humanlike mathematical reasoning? In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025. URL https://arxiv.org/abs/2407.01284. [40] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [41] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 46634680, 2024. [42] Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, et al. Code2logic: Game-code-driven data synthesis for enhancing vlms general reasoning. arXiv preprint arXiv:2505.13886, 2025. [43] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. [44] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. [45] Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, and Weiran Huang. Advancing multimodal reasoning via reinforcement learning with cold start. arXiv preprint arXiv:2505.22334, 2025. [46] Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, and Andres Marafioti. Finevision: Open data is all you need. arXiv preprint arXiv:2510.17269, 2025. [47] xAI. Grok-1.5 vision preview. https://x.ai/news/grok-1.5v, April 2024. [48] Zhiheng Xi, Guanyu Li, Yutao Fan, Honglin Guo, Yufang Liu, Xiaoran Fan, Jiaqi Liu, Jingchao Ding, Wangmeng Zuo, Zhenfei Yin, et al. Bmmr: large-scale bilingual multimodal multi-discipline reasoning dataset. arXiv preprint arXiv:2507.03483, 2025. [49] Jiaer Xia, Yuhang Zang, Peng Gao, Sharon Li, and Kaiyang Zhou. Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning. arXiv preprint arXiv:2505.14677, 2025. [50] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. 22 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods [51] Guowei Xu, Peng Jin, Ziang Wu, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 20872098, October 2025. [52] Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025. [53] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision). arXiv preprint arXiv:2309.17421, 2023. [54] Yixin Ye et al. Limo: Less is more for reasoning. In Proceedings of the Conference on Language Modeling (COLM), 2025. URL https://arxiv.org/abs/2502.03387. [55] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12):nwae403, 2024. URL https://doi.org/10.1093/nsr/ nwae403. [56] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [57] Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, and Dianhui Chu. survey on data selection for llm instruction tuning. Journal of Artificial Intelligence Research, 83, 2025. [58] Kaichen Zhang, Keming Wu, Zuhao Yang, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, and Lidong Bing. Openmmreasoner: Pushing the frontiers for multimodal reasoning with an open and general recipe. arXiv preprint arXiv:2511.16334, 2025. [59] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [60] Yanzhe Zhang et al. Multimodal reasoning with large language models: survey. arXiv:2505.04921, 2025. URL https://arxiv.org/abs/2505.04921. arXiv preprint [61] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization, 2025. URL https://arxiv.org/abs/2507.18071. [62] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. [63] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024. 23 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods"
        },
        {
            "title": "A Data Curation Details",
            "content": "A.1 Statistics Table 5: Dataset composition of MMFineReason. Subsets marked with is inherited from FineVision [46]. Subset Name VisualWebInstruct [46] MMR1 [17] GameQA-140K [42] BMMR [48] LLaVA-CoT [51] WaltonColdStart [45] ViRL39K [32] Euclid30K [19] Raven [46] MMK12 [33] TQA [46] Category Samples Tokens Subset Name Category Samples Tokens Science Math Puzzle Science General Math Math Math Puzzle Science Science 260,556 1,524,033 122,868 80,366 68,838 49,786 36,034 26,690 20,271 15,505 12,263 696,366,646 AI2D [46] Science Math Math Math Science Science 5,924,286,949 Geo170k(qa) [46] 751,735,082 Geometry3k [46] 380,074,912 mmopenr1-8k [29] 132,188,410 Zebra-CoT-Physics [18] ScienceQA [46] 146,912,078 129,740,324 WeMath2-Standard [39] Math 124,091,304 WeMath2-Pro [39] Math Puzzle 192,077,480 VisualSphinx [12] Puzzle Math 66,234,373 26,319,379 WeMath2-SFT [39] PuzzleVQA [8] 12,167 11,771 8,977 7,057 6,610 6,095 5,613 4,334 3,516 1,966 814 29,657,396 25,606,103 16,434,149 27,520,245 31,020,152 9,681,856 16,599,703 18,588,338 28,666,407 7,581,908 4,040, Total Samples: 2,286,130 , Total Tokens: 8,785,423,669 A."
        },
        {
            "title": "Inclusion and Exclusion Criteria",
            "content": "We prioritize using pre-filtered subsets when higher-quality versions of dataset are availablefor example, the FineVision-filtered VisualWebInstruct subset and the OpenMMReasoner-filtered MMR1 subset. We exclude the following categories of data from our collection: Multi-image samples: Data where each instance contains more than one image. Overly simple task data: e.g., CLEVR family and geo170k (align), which provide limited reasoning complexity. Highly specialized imaging domains: e.g., PathVQA, which focuses on medical imagery. Multilingual datasets featuring minor languages within images: e.g., EXAMS-V. A.3 Filteration Details To ensure the high quality and robustness of our training data, we implemented multi-stage data processing pipeline, consisting of basic structural cleaning and advanced quality assessment. The statistics for these processes are summarized in Table 6 and Table 7. Basic Data Cleaning. As shown in Table 6, we first applied rigorous filtering based on sequence length and template validity.The Filt. (Len) step removed samples that exceeded the context window limits or were anomalously short, while the Filt. (Tem) step discarded samples with parsing errors or malformed templates.Overall, the raw datasets exhibited high structural integrity, with retention rates exceeding 95% for most subsets. notably, while large-scale datasets like MMR1 contained higher absolute number of template errors (75,585), the relative loss remained minimal (< 5%).Some specific domains, such as VisualSphinx and Geometry3k, showed slightly lower retention rates ( 92 93%), primarily due to complex formatting requirements inherent to their tasks. 24 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Table 6: Data Cleaning Statistics by Dataset. Filt. (Len) refers to length-based filtering, and Filt. (Tem) refers to template validation errors."
        },
        {
            "title": "Original",
            "content": "Filt. (Len) Filt. (Tem) Remain Rate (%) BMMR Euclid30K FineVision-ai2d merged FineVision-geo170k(qa) FineVision-geometry3k(mathv360k) FineVision-raven FineVision-scienceqa FineVision-tqa FineVision-visualwebinstruct(filt) GameQA-140K LLaVA-CoT MMK12 MMR1 PuzzleQA ViRL39K VisualSphinx WaltonColdStart WeMath2-Pro WeMath2-SFT WeMath2-Standard Zebra-CoT-Physics mmopenr1-8k"
        },
        {
            "title": "Total",
            "content": "84,252 27,021 12,180 12,101 9,716 20,411 6,112 12,560 261,007 123,579 69,006 15,544 1,600,235 1,991 36,242 3,776 51,184 4,531 826 5,683 7,035 7,428 2,372,320 67 3 13 56 271 14 4 5 449 52 168 0 617 0 37 25 17 0 0 2 0 6 3,819 328 0 274 468 126 13 292 2 659 0 39 75,585 25 171 235 1,381 197 12 68 425 365 80,366 26,690 12,167 11,771 8,977 20,271 6,095 12,263 260,556 122,868 68,838 15,505 1,524,033 1,966 36,034 3,516 49,786 4,334 814 5,613 6,610 7,057 1, 84,484 2,286,030 95.39 98.78 99.89 97.27 92.39 99.31 99.72 97.64 99.83 99.42 99.76 99.75 95.24 98.74 99.43 93.11 97.27 95.65 98.55 98.77 93.96 95.01 96.36 25 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Table 7: Pass Rate and Consistency Statistics by Dataset. PR stands for Pass Rate. The last column shows the count of consistent samples."
        },
        {
            "title": "Dataset",
            "content": "Rows PR (Mean) PR (Med) Consistent BMMR Euclid30K FineVision-ai2d merged FineVision-geo170k(qa) FineVision-geometry3k(mathv360k) FineVision-raven FineVision-scienceqa FineVision-tqa FineVision-visualwebinstruct(filt) GameQA-140K LLaVA-CoT MMK12 MMR1 PuzzleQA ViRL39K VisualSphinx WaltonColdStart WeMath2-Pro WeMath2-SFT WeMath2-Standard Zebra-CoT-Physics mmopenr1-8k"
        },
        {
            "title": "Total",
            "content": "80,366 26,690 12,167 11,771 8,977 20,271 6,095 12,263 260,556 122,868 68,838 15,505 1,524,033 1,966 36,034 3,516 49,786 4,334 814 5,613 6,610 7,057 2,286,130 0.5272 0.6976 0.8997 0.6277 0.4369 0.2379 0.9028 0.8285 0.5026 0.4042 0.5747 0.7725 0.7697 0.5137 0.8032 0.1778 0.7643 0.6751 0.6225 0.7398 0.7906 0.5356 0.6973 0.5000 1.0000 1.0000 0.7500 0.2500 0.2500 1.0000 1.0000 0.5000 0.2500 0.7500 1.0000 1.0000 0.5000 1.0000 0.0000 1.0000 1.0000 0.7500 1.0000 1.0000 0.5000 56,543 23,218 10,746 9,130 4,855 7,994 5,862 10,568 162,729 75,002 39,516 14,114 1,293,269 1,445 32,651 1,255 43,233 3,368 670 4,576 6,036 4, 1.0000 1,810,887 26 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Consistency and Difficulty Analysis. Following structural cleaning, we evaluated the semantic quality of the remaining data using Pass Rate (PR) metric and consistency check, as detailed in Table 7. The Pass Rate serves as an indicator of sample clarity or model competence, where higher rate implies that the model successfully processed the sample. We observed clear positive correlation between the Pass Rate and the Consistency Rate: High-Consistency Datasets: Datasets such as ScienceQA and the massive MMR1 demonstrated exceptional quality, with consistency rates of 96.18% and 97.30% respectively, and correspondingly high mean Pass Rates ( 0.8 0.9). This suggests these samples are well-posed and have clear ground truths. Challenging Scenarios: In contrast, abstract reasoning tasks like Raven and VisualSphinx exhibited low mean Pass Rates (< 0.25) and low consistency (< 40%). This disparity highlights the inherent difficulty of these tasks or the presence of ambiguous samples that necessitate more robust filtering strategies. Ultimately, we identified approximately 1.81 million consistent samples (Total Consistent) out of the processed pool, providing solid foundation for stable model training."
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Training Details Table 8: SFT Params. Table 9: RL Params. Table 10: Eval Params."
        },
        {
            "title": "Optimizer\nLearning Rate\nScheduler\nWeight Decay\nEpochs\nWarmup Ratio\nSequence Len\nBatch Size\nPacking\nLiger Kernel\nMax Pixels\nMin Pixels",
            "content": "AdamW 1e-5 cosine 0.0 3 3% 32,768 32 True True 768 768 32 32 Optimizer Learning Rate Scheduler Weight Decay Train Steps Warmup Steps Batch Size Prompt Len Output Len Temperature Rollouts low, high AdamW 1e-6 constant 0.1 300 10 256 8192 16384 1.0 16 3e-4, 4e-4 Engine Precision Temperature Top-p Top-k Max Tokens Rep. Penalty Max Pixels Min Pixels VLLM BF16 0.0 1.0 -1 32768 1.05 2048 2048 32 32 We provide comprehensive overview of our experimental setup, including the hyperparameters for Supervised Fine-Tuning (SFT), Reinforcement Learning (RL). Supervised Fine-Tuning (SFT). As shown in Table 8, the SFT stage utilizes cosine learning rate scheduler with peak learning rate of 1e-5. To optimize training efficiency and memory usage, we employ sequence packing with length of 32,768 and integrate Liger Kernel support. We also accommodate high-resolution inputs by setting the maximum pixel limit to 768 768. Reinforcement Learning (RL). Following SFT, we further align the model using Reinforcement Learning. The detailed hyperparameters are listed in Table 9. In this stage, we adopt constant learning rate of 1e-6 to ensure stability. The training process involves 16 rollouts per prompt with 27 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods KL-divergence penalty controlled by low = 3e-4 and high = 4e-4. The input and output max lengths are set to 8,192 and 16,384 tokens, respectively. B.2 Evaluation Details We conduct comprehensive evaluation using the VLMEvalKit [11]. For evaluation metrics, we replace traditional exact string matching with the compass-verifier [24], which employs an LLM-as-a-Judge to accurately assess response correctness. Regarding rollout settings, following the guidelines from OpenDataArena [5] and the official Qwen documentation, we set the temperature to 0.0, top-p to 1.0, and top-k to 1. We apply repetition penalty of 1.05 and limit the maximum response length to 32768 tokens. Furthermore, we exclude all system prompts during inference. 28 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods"
        },
        {
            "title": "C Prompts",
            "content": "In this section, we provide the full prompts used in our experiments. C.1 Question Cleaning"
        },
        {
            "title": "Question Cleaning Prompt",
            "content": "Task. Clean the given question text by following these steps. Error types 1. Translation Translate the question into English. If the question is already in English, keep it as is. 2. Irrelevant content Remove all irrelevant links, advertisements, signatures, emails, special symbols, or repeated punctuation. Remove Markdown watermarks, unrelated tables, or redundant markings in formulas. Remove question numbers, IDs, or scoring information that appears before the actual question (e.g., Q12., (5 points), 1.). Do not consider an <image> tag at the beginning of question as irrelevant content. 3. Non-answer question Questions that are not actual problem-solving questions, e.g., asking to draw diagram or write code, should be marked under this error type. Questions that are incomplete to the point that they cannot be answered. 4. Low-quality instruction If the question contains instructions that may reduce reasoning quality (e.g., just give the answer, do not think, give me the final answer only), rewrite these into instructions that encourage thoughtful reasoning (e.g., provide clear reasoning process before the final answer). Output rules If the question has no issues (no translation or cleaning needed), output: No Problem If the question has issues, output JSON in the following format: { \"error type\": [\"translation\", \"irrelevant content\"], // only \"translation\", \"irrelevant content\", \"non-answer question\" and \"low-quality instruction\" \"corrected text\": \"Cleaned and translated version of the question (leave empty if non-answer question)\" } Examples Example 1 Input: What is this? https://example.com Output: { \"error type\": [\"translation\", \"irrelevant content\"], \"corrected text\": \"What is this?\" } 29 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Example 2 Input: Please directly answer the question: what are the roots of this equation? Output: { \"error type\": [\"translation\", \"low-quality instruction\"], \"corrected text\": \"Please provide clear reasoning process before giving the roots of this equation.\" } Example 3 Input: {question} Output: No Problem C.2 Answer Extraction"
        },
        {
            "title": "Answer Extraction Prompt",
            "content": "Table 11: Prompt for question cleaning. You are precise math answer extractor. Your task is to read the users question and the provided solution, then extract ONLY the final answer(s). Output EXACTLY one <answer>...</answer> tag containing only the final answer, with no extra text or explanations. Extraction Rules (Follow in order of priority) 1. Top Priority ( ). If final is present, output its INNER CONTENT EXACTLY as written, preserving all LaTeX, symbols, and text. This rule takes precedence over all other rules (including the unit rule). Ensure the extracted content is complete (e.g., balanced braces). 2. Final Result (No Box). If no is found, extract the final explicit numerical or symbolic result (e.g., after final answer is, answer is, Thus, Therefore). 3. LaTeX Preservation. When applying Rule 2, preserve all LaTeX expressions and symbols (e.g., sqrt{...}, , , ). Do NOT convert LaTeX to plain numbers or words. 4. No Simplification. Do NOT convert words to digits, rewrite mixed numbers, or simplify fractions unless they already appear that way in the final result. 5. Unit Stripping (No Box Only). If applying Rule 2 (i.e., no was found), do NOT include units (e.g., cm, dollars, ways). Exception: Always keep the percent sign (%). 6. Multiple Solutions. If the final answer lists multiple distinct values (e.g., = 5 or = 10, the roots are 1 and 1), output them as single, comma-separated string (e.g., \"5, 10\", \"-1, 1\"). 7. Word Answers. If the solutions final answer is definitive word (e.g., Yes, No, True, False, None, Cannot be determined), extract that word. 8. Not Found. If no specific, concise answer (mathematical, expression, or definitive word) can be found, respond with <answer></answer>. Examples Example 1 (Boxed). Solution: ...blah blah... The answer is 10 . Respond with: <answer>10</answer> 30 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Example 2 (Boxed with LaTeX). Solution: ...so the value is 3 2 . Respond with: <answer>frac{sqrt{3}}{2}</answer> Example 3 (Boxed with Units Rule 1 Precedence). Solution: ...the final area is 24 cm2 . Respond with: < answer > 24{cm}2 < /answer > Example 4 (No Box with Units Rule 5 Applies). Solution: ...Therefore, the length is 40 meters. Respond with: <answer>40</answer> Example 5 (No Box with Percent Rule 5 Exception). Solution: ...The total increase was 15.5%. Respond with: <answer>15.5%</answer> Example 6 (Multiple Solutions). Solution: ...the roots of the equation are = 2 or = 5. Respond with: <answer>-2, 5</answer> Example 7 (Word Answer). Solution: ...we can conclude that the statement is False. Respond with: <answer>False</answer> Example 8 (Not Found). Solution: ...this completes the proof by induction. Respond with: <answer></answer> Task template Question: {instruction} Solution: {output tail} Respond with: <answer>...</answer> Table 12: Prompt for Answer Extraction. C."
        },
        {
            "title": "Multimodal Data Annotation Specialist",
            "content": "You are meticulous Multimodal Data Annotation Specialist. Your primary mission is to deconstruct multimodal tasks (consisting of images and text) and translate them into highly structured and comprehensive natural language description. The goal is to create golden reference text that is as unambiguous and detailed as data file, which will be used to evaluate the accuracy of other AI models. Your adherence to the format described below is critical. You will be provided with task that consists of up to two parts: one image, and its corresponding question text. Guiding Principles for Analysis: 1. Category-First, Structure-Always: Your entire analysis begins with correctly identifying the images category. The category list now includes both STEM-style images and natural photographs (see expanded list below). This category dictates the focus of your description. You must then follow the specified markdown structure precisely for your output. 2. Separate What is Seen from What is Inferred: Your description must maintain strict separation between elements explicitly visible in the image and properties inferred from the accompanying 31 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods text. The output format has dedicated sections for this. 3. Comprehensive and Atomic Breakdown: Every single element in the image must be described individually within the Explicit Component Breakdown section. For natural images, this includes: People (pose, clothing, objects held) Everyday objects Scene elements (furniture, roads, sky, vehicles) Background structures Animals, plants, food, tools, etc. Treat each as standalone component. 4. Holistic Synthesis: The image and question text are single unit. Use the text to define roles, identify actions, or extract inferred properties. Instructions for Structuring Your Output You must generate single text block. The response must be structured using markdown with headings, bolded keywords, and bullet points exactly as specified below. For each image provided, create complete descriptive block starting with: ### Image [N]: [Primary Category Name] Required Output Structure: Heading. ### Image [N]: [Primary Category Name] (replace [N] with the image number, and [Primary Category Name] with the category you identify from the list below). Scene Summary. single, concise sentence that describes the overall purpose and content of the image. Explicit Component Breakdown. (This section is for visible elements only.) [Component Name] ([label]): description of the component. The [label] should be the exact text or symbol labeling the component in the image. If there is no label, use None. Repeat for every single visible component: objects, vectors, surfaces, axes, points, everyday objects, people, clothing, background structures, etc. Interactions and Relationships. (This section describes how the explicit components are connected and arranged.) Describe spatial and structural connections (e.g., Person stands next to table B, Button triggers Modal B). Describe logical or physical relationships (e.g., contact, holding, containment, occlusion). Trace directional flows (arrows/process steps) or describe data trends (charts/graphs). Implicit and Inferred Properties. (This section is only for information derived from the question text or domain conventions, not explicitly drawn.) [Component or System Name]: [Inferred Property]. For example, Person A: identified as teacher from question text. [Component or System Name]: [Inferred Property]. For example, Dataset: values normalized to [0, 1]. List every piece of non-visual information here. Identified Ambiguities. (If any part of the image is illegible or unclear, list it here. If none, state None.) [Description of ambiguous element]. Reference Guide: Image Categories Below is the expanded and unified category list. STEM / Diagrammatic Categories Geometric Diagram Spatial Reasoning Scene 32 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Mathematical Plot / Chart Puzzle / Logic Diagram Textbook Illustration Physics / Mechanics Diagram Experimental Setup Astronomy / Space Visualization Molecular / Chemical Diagram Biological Structure Geological / Earth Science Diagram Circuit / Network Diagram Abstract Mathematical Representation Table / Matrix Diagram / Flowchart Natural Image Categories Natural Landscape Scene Urban / Street Scene Indoor / Interior Scene Human Portrait / Activity Sports / High-Motion Scene Animal / Wildlife Scene Product / Still Life Object Vehicle / Machinery Object Food / Beverage Item Document / Text Image Artwork / Illustration Technical / Surveillance / Medical Now, analyze the provided image(s) and question text, and produce the structured natural language description in this exact format: {question} {image} Table 13: Prompt for image captioning. C.4 Long CoT Distillation"
        },
        {
            "title": "Long CoT Distillation Prompt",
            "content": "You are an expert in science and visual reasoning with advanced capabilities in multimodal analysis. Your response will be used as high-quality example to train new AI model. Solve the problem efficiently and clearly by integrating all information from multimodal inputs. Core Principles 33 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods 1. Equal Weight to All Inputs. Information from images (photos, charts, graphs, diagrams, tables, handwritten notes) is as important as text. Never ignore visual elements. 2. Systematic Analysis. Follow rigorous, reproducible approach for every problem. 3. Precision and Accuracy. Double-check all calculations and reasoning steps. 4. Adaptive Reasoning. Choose the most appropriate method based on the specific problem context. Solution Framework Phase 1: Comprehensive Information Extraction Carefully analyze all text content for requirements, constraints, and given values. Thoroughly examine all visual elements, extracting every piece of relevant information. Note measurements, relationships, patterns, and any implicit information. Explicitly connect visual and textual information when they relate to each other. Phase 2: Strategic Problem Setup Compile all extracted information in an organized manner. Clearly state what needs to be found or proven. Identify the most relevant scientific principles and methodologies. Consider what assumptions may be necessary and state them explicitly. Phase 3: Rigorous Solution Execution Present your solution with complete logical flow. Show all mathematical steps with proper notation. When using formulas, present them clearly, substitute values, and then calculate. Reference specific parts of visual inputs when they support your reasoning. Maintain unit consistency throughout all calculations. Keep appropriate precision and significant figures. Phase 4: Solution Validation Verify your answer makes scientific and logical sense. Check that all parts of the question have been addressed. For multiple choice questions, confirm your selection and briefly justify if needed. Ensure dimensional analysis is correct. Key Reminders Visual information is never supplementary it is integral to the solution. Every piece of data from images must be considered. Your reasoning should be so clear that someone could follow it without seeing the images. When in doubt, show more work rather than less. Connect each step logically to build complete solution narrative. Answer Format Guidelines Determine the nature of your answer: If the problem has definitive, fixed answer (numerical value, specific choice, exact result): Present your complete reasoning and solution process. MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods At the end, clearly state: Therefore, the final answer is <answer>YOUR ANSWER</answer> (with the actual answer substituted for YOUR ANSWER). Examples: <answer>5.2 m/s</answer>, <answer>C</answer>, <answer>2.5 m, 30</answer>. If the problem requires explanation, discussion, or has no single fixed answer: Focus on presenting your points clearly and in structured manner. Provide full analysis and explanation. You may include examples, reasoning steps, or possible conclusions, but single correct answer wrapped with <answer> tags is not required. Problem injection The problem will be inserted as: Problem: {item[question]} Analyze all provided materials carefully, think through the problem step by step, and provide comprehensive solution that demonstrates mastery of both scientific reasoning and visual analysis. Final line constraint The last line of your response must be exactly: \"Therefore, the final answer is <answer>ANSWER</answer>.\" Table 14: Prompt for long chain-of-thought distillation. C.5 Answer Verification"
        },
        {
            "title": "Answer Verification Prompt",
            "content": "You are an expert evaluator. Compare the reference and generated answers only for semantic correctness and factual agreement. Task Determine whether the two answers express the same correct solution. Focus on meaning, correctness, and final results rather than wording or format. Evaluation Guidences Equivalent: same conclusion or final answer, no substantive factual differences. Different: conflicting conclusions, missing required reasoning, or any factual mistake in the generated answer. Input Question {question} Reference Answer {solution} Generated Answer {response} Output Intructions Respond in the following two-line format (no extra text): Analysis: <concise reasoning> Judgment: <Equivalent or Different > Table 15: Prompt for Answer Verification 35 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods"
        },
        {
            "title": "D Filteration Cases",
            "content": "In this section, we provide representative examples for each error type used in our question cleaning stage, including translation, irrelevant content, non-answer question, and low-quality instruction. These cases are drawn from multiple sources such as BMMR, Euclid30K, and VisualSphinx. D.1 Translation Case: Translation (Euclid30k-16648) Question: P-ABCDABCDAD BCAC DB=OPO ABCD BOC=90 OA=1OC=2EPB (2)tan PAC = 2PAPBC Error Type: Translation Corrected Question: The frame model of certain kite is quadrilateral pyramid P-ABCD, where quadrilateral ABCD is an isosceles trapezoid with AD parallel to BC. The diagonals AC and BD intersect at point O, and PO is perpendicular to the plane ABCD. It is given that angle BOC = 90, OA = 1, and OC = 2. Point lies on PB. (2) Research shows that the kite performs best when tan(angle PAC) = 2. Find the sine of the angle between line PA and plane PBC at that time. Filtering Reason: The original question is in Chinese and contains specific reference to the diagram. The translation was needed to ensure the question was clearly understandable in English. The format was adjusted to correct the translation and ensure clarity, maintaining the integrity of the mathematical context. Table 16: Case of translation (Euclid30k-16648). D."
        },
        {
            "title": "Irrelevant content",
            "content": "Case: Irrelevant content (BMMR-84693) Question: 36 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods <image> These triangles are similar. Find the missing length. ![Two 26, base 69101954001041251272727742444710200451977070775401974217746220747051752954015457019440...) hypotenuse 13, 5.](https://latex.artofproblemsolving.com/tex2html/ similar base 12 - height height has hypotenuse The second right and first has triangles. angled $x$. The and Error Type: Irrelevant content Corrected Question: These triangles are similar. Find the missing length. Filtering Reason: The original question contains long Markdown image caption and an external URL that redundantly describe the diagram in text form. This information is not needed to understand or solve the problem, since the visual content is already provided by the image itself. We therefore remove this captionURL block as irrelevant content and keep only the core question text, without changing the mathematical meaning of the task. Table 17: Case of irrelevant content (BMMR-84693). D.3 Non-answer question Case: Non-answer question (BMMR-72354) Question: <image> 1. rectangle has perimeter of 444 centimeters and side of length centimeters. - a. Draw rectangle to represent this situation. Label each side of the rectangle in terms of l. - b. Write an equation for the area of the rectangle in terms of l. - c. Sketch graph of the relationship between the length of side and the area. Be sure to include important features such as the maximum/minimum, y-intercept(s), x-intercept(s), and the line of symmetry. - d. Study your graph, table, and equation for the area of rectangles with fixed perimeter of 444 centimeters. Which representation is the most useful for predicting the maximum area? Explain your choice. Error Type: Non-answer question Response: { \"error type\": [\"non-answer question\"], \"corrected text\": \"\" } 37 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Filtering Reason: This question involves drawing and writing code, which is not an answerable question. It asks the student to graph and sketch diagrams as well as explain reasoning, but it doesnt provide any actual problem to solve or compute. Therefore, this is non-answer question, and the corrected text is left empty. Table 18: Case of non-answer question (BMMR-72354). Case: Non-answer question (VisualSphinx-631) Question: The following diagram is polyhedron composed of 15 white and 5 gray cubes, after its [translation incomplete, seems to be cut off] Options: A) B) C) D) Error Type: Non-answer question Response: { \"error type\": [\"non-answer question\"], \"corrected text\": \"\" } Filtering Reason: The question text is explicitly marked as translation incomplete and is cut off after the phrase after its ..., without specifying what operation or condition is applied to the polyhedron. Only the answer options (A, B, C, D) are listed, but the task to be solved is missing. As result, the problem statement is incomplete and no definite answer can be determined, so this sample is labeled as non-answer question and the corrected text field is left empty. Table 19: Case of non-answer question (VisualSphinx-631). D.4 Low-quality instruction Case: Low-quality instruction (LLaVA-CoT-100K-1761) Question: 38 MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods Who has won the most games in New York Mets franchise history? Answer the question using single word or phrase. Error Type: Low-quality instruction Corrected Question: Who has won the most games in New York Mets franchise history? Please provide clear reasoning process before giving the final answer. Filtering Reason: The original question contains the instruction to answer with single word or phrase, which could limit the reasoning process. This reduces the quality of the question by potentially discouraging more comprehensive reasoning approach. To encourage better reasoning and more thoughtful answer, the corrected question requests clear reasoning process before providing the final answer. Table 20: Case of low-quality instruction (LLaVA-CoT-100K-1761)."
        }
    ],
    "affiliations": [
        "OpenDataLab",
        "Peking University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The University of Hong Kong"
    ]
}