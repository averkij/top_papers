{
    "paper_title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "authors": [
        "Elia Peruzzo",
        "Guillaume Sautière",
        "Amirhossein Habibian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\\mathbf{1.7\\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity."
        },
        {
            "title": "Start",
            "content": "Multi-Scale Local Speculative Decoding for Image Generation"
        },
        {
            "title": "Elia Peruzzo",
            "content": "Guillaume Sautière"
        },
        {
            "title": "Amirhossein Habibian",
            "content": "Qualcomm AI Research {eperuzzo, gsautie, ahabibia}@qti.qualcomm.com 6 2 0 2 8 ] . [ 1 9 4 1 5 0 . 1 0 6 2 : r Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers promising avenue for acceleration, but existing approaches are limited by tokenlevel ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by high-resolution target model. Crucially, we incorporate local rejection and resampling mechanism, enabling eﬀicient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups up to 1.7 outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPGBench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets new state-of-the-art in speculative decoding for image synthesis, bridging the gap between eﬀiciency and fidelity. Project page is available at https://qualcomm-ai-research.github.io/mulosd-webpage. 1. Introduction large language models Recently unified multimodal (MLLMs), merging the generation and understanding of language and vision in unified autoregressive (AR) model, have seen surge in popularity [4, 12, 33, 34, 45, 5154]. Compared to diffusion modQualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Figure 1. Multi-Scale Speculative Decoding extends speculative decoding by using draft model working at lower resolution than the target model, to enable acceleration through coarse-to-fine approach. During verification, we exploit spatial locality in autoregressive models to resample only neighborhood of rejected image tokens, improving efficiency without compromising quality. els [3, 8, 23, 29, 37, 38, 56], unified MLLMs tend to perform better in text-to-image alignment tasks, and more generally in semantic understanding of complex prompts and knowledge-driven generation tasks [35]. Despite their success, fundamental limitation persists: the sequential nature of AR decoding leads to high inference latency, especially for large-scale models and high-resolution outputs. Image and video synthesis with AR models is made harder due to the rapidly exploding sequence size, as the number of tokens grows quadratically with resolution and leads to thousands of tokens even for modest resolution like 1024p. By reformulating the objective from next-token prediction to next-scale prediction, autoregressive image generation can be significantly accelerated by sam1 pling the image in coarse-to-fine manner, i.e. starting from low-resolution samples and progressively refining them [11, 13, 25, 40, 47, 50]. Despite these substantial eﬀiciency gains, the next-scale prediction objective fundamentally differs from the next-token prediction used to train LLMs. This discrepancy hinders the adaptation of next-scale prediction AR models within unified MLLMs. Therefore, accelerating generation under the next-token prediction objective for AR models the focus of this paper remains an important and relatively underexplored problem. Speculative Decoding (SD) [58], originally develintroduces draft-andoped for language models, verify paradigm: lightweight model (drafter) proposes multiple tokens sampled sequentially and the full-size model (target) verifies them in parallel. While this approach has shown impressive speedups in text generation, its application to image synthesis remains underexplored. Recent efforts such as LANTERN [19, 36] have adapted speculative decoding to the visual domain by relaxing acceptance criteria to account for image token ambiguity in the latent space. However, these methods still operate at the token level and ignore the spatial structure and multi-scale nature of images. Additionally, locality-aware decoding strategies like ZipAR [15] and LPD [60] demonstrate that exploiting spatial coherence can further reduce latency by enabling parallel generation across rows or patches. In this work, we present Multi-Scale Local Speculative Decoding (MuLo-SD, see Fig. 1), framework that exploits the structural properties of images to enhance speculative decoding. Our approach introduces two key innovations: 1. Multi-scale drafting: We leverage the natural hierarchy of image resolutions by using low-resolution drafter and learned up-sampler to propose candidate image tokens, which are then verified by high-resolution AR model. 2. Local verification: Inspired by spatial coherence in images, we introduce rejection and re-sampling mechanism that operates over local neighborhoods rather than full raster-scan sequences, improving both eﬀiciency and acceptance rates. We demonstrate that MuLo-SD achieves substantial speedups up to 1.7 outperforming strong speculative decoding baselines such as EAGLE-2 [27] and LANTERN [19] in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval [9], DPG-Bench [17], and FID [16]/HPSv2 [55] on the MSCOCO 2017 5k validation split [32]. 2. Related art Speculative decoding methods aim to accelerate autoregressive generation by relaxing sequential dependencies. For text, Speculative Decoding [22] introduced draft-and-verify scheme wherein lightweight model proposes multiple tokens in sequence, and the target model verifies them in parallel achieving 2 3 speedups. Self-Speculative Decoding [58] reuses internal layers of the target model and hierarchical verification, reaching 3.5 acceleration without additional memory footprint. Medusa [2] employs multihead decoding and tree attention for up to 3.6 speedup. EAGLE [28] drafts by making use of the target models penultimate latent representations, while EAGLE-2 [27] introduces dynamic draft trees based on token confidence, pushing speedups to 4.3. These methods are designed for text generation and do not generalize to the image domain. LANTERN [19, 36] is the first to extend speculative decoding to image synthesis. It addresses token ambiguity in visual models by introducing relaxed acceptance criterion based on latent token interchangeability. This improves acceptance rates while bounding total variation distance to preserve semantic fidelity, achieving 1.751.82 speedups over greedy decoding on LlamaGen [44]. MuLo-SD is closely related to LANTERN in extending speculative decoding to image synthesis. Like LANTERN, it relaxes the verification objective to address token ambiguity in vision models. However, MuLo-SD uniquely leverages the multi-scale prior to further improve decoding eﬀiciency, making it the first speculative decoding method to do so. Multi-scale autoregressive models generate images in coarse-to-fine manner, improving both eﬀiciency and quality. VAR [47] introduced next-scale prediction, conditioning each resolution level on lower ones, and outperformed diffusion models in speed and fidelity. Follow-up works such as M-VAR [40], Switti [50], and others [11, 13, 20] extend this framework. M-VAR decouples intraand inter-scale modeling, combining bidirectional attention with linear-complexity mechanisms like Mamba [10], achieving state-of-the-art FID with fewer parameters. Switti [50] removes explicit cross-scale autoregression and classifier-free guidance at high resolutions, enabling up to 7 faster sampling with competitive quality. These models align well with the hierarchical structure of visual data and demonstrate strong scalability. However, their bespoke sampling schedules hinders integration with next-token prediction frameworks and unified MLLMs, e.g., causing ineﬀicient KV-cache usage and requiring ad-hoc designs [11, 25]. MuLo-SD shares the multi-scale design philosophy of these models but differs in its focus on decoding eﬀiciency through speculative sampling. Unlike prior multi-scale methods, which rely on custom sampling schedules, MuLo-SD integrates well with next-token prediction MLLMs and inject the coarse-to-fine approach in its drafting strategy. Locality-aware autoregressive methods [15, 60] leverage spatial coherence in images to improve generation eﬀiciency. ZipAR [15] is an inference-time trick which reduces the number of forward passes by up to 91% with minimal quality degradation, outperforming prior parallel decoding methods like speculative Jacobi decoding [46]. ZipAR enables inter-row parallel decoding by exploiting spatial adjacency, allowing tokens in the next row to be decoded once suﬀicient context is available. Differently, LPD [60] decouples the two roles tokens typically play, providing context and enabling generation. With separate query and context tokens they allow parallel and arbitrary order sampling of images, albeit requiring full re-training of the AR model. MuLo-SD exploits the locality of AR models exposed by ZipAR [15] and LPD [60] by performing re-sampling within local neighborhoods rather than in raster-scan order. However, it differs by embedding this localityaware strategy within speculative decoding framework and combining it with multi-scale priors, enabling eﬀicient and high-fidelity image synthesis without retraining. ZipAR is parallel decoding method that is orthogonal to our approach and can be combined to potentially enhance performance. However, to isolate the contributions of our method, we do not apply ZipAR during either draft or target model sequential sampling. 3. Method 3.1. Preliminaries Let Mp denote the target autoregressive model, which defines conditional probability distribution p(xtx<t) over the next token xt given prefix x<t. Let Mq denote the draft model, more eﬀicient model, that defines distribution q(xtx<t) for the same task. Speculative Decoding [58] accelerates sampling from Mp by leveraging Mq to propose sequence of draft tokens x0, . . . , xn1 sampled autoregressively from q. These drafts are then verified in parallel by Mp. Each token xi is accepted with probability: (cid:18) min 1, (cid:19) , pi(xi) qi(xi) (1) 3 where pi and qi denote the distributions from Mp and Mq conditioned on the prefix extended by previously accepted tokens. If token is rejected, it is resampled from an adjusted distribution: i(x) = norm (max (0, pi(x) qi(x))) , ensuring that the overall sampling process is exact i.e. the same as sampling from the target distribution p. (2) To address the limitations of Speculative Decoding in domains with high token uncertainty such as visual autoregressive models LANTERN [19] introduces relaxed acceptance criterion based on latent proximity in the VQ-VAE codebook. Let Bk(xi) denote the set of nearest neighbors to xi in latent space. The relaxed acceptance probability becomes: min 1, xBk(xi) pi(x) qi(xi) ! , (3) if its surrounding latent allowing acceptance of xi neighbors collectively have suﬀicient probability mass under Mp. The pooling of neighboring tokens probabilities allows relaxing the acceptance rule and dealing with the typical ambiguity in vision token prediction, wherein the probability distribution over the next token is flatter and less peaked than for text. To control the divergence from the original distribution, LANTERN constrains the Total Variation Distance (TVD) between the relaxed distribution p(k,δ) and the original pi: TVD(p(k,δ) , pi) < δ, (4) where p(k,δ) redistributes mass over the neighborhood Ak,δ(xi) Bk(xi) such that the divergence remains bounded by δ. This relaxation enables higher acceptance rates in domains with ambiguous token distributions, while preserving semantic fidelity and bounding distributional shift. 3.2. Multi-Scale Drafting Multi-scale modeling is strong inductive bias in image synthesis, underpinning key architectures like UNet [41], VQ-VAE-2 [39], and VAR [47]. It follows coarse-to-fine strategy: lower scales capture structure, while higher scales refine texture and detail. Notably, even single-scale models like diffusion implicitly adopt this approach [6], with early denoising steps targeting low-frequency content and later steps focusing on highfrequency details. Motivated by this, we incorporate multi-scale bias into speculative decoding for vision. Figure 2. Overview of our proposed method Multi-Scale Local Speculative Decoding (MuLo-SD). Blue indicates draft tokens, green accepted tokens, purple rejected tokens, blank placeholder tokens. indicates sequential operations, parallel operations, drawing discontinuity due to looping. Recent AR models for image synthesis [12, 33, 44] are commonly released at multiple resolutions, with separate finetuning for each scale. Given desired target resolution sp, we employ draft model Mq at lower resolution sq, with resolution ratio = sp/sq. The drafter is paired with trained up-sampler Ur and down-sampler Dr. The target model Mp operates at higher resolution sp. An overview of the method is shown in Figure 2, and refer to the supplementary material for detailed algorithm and schematic comparison to related approaches. Following Fig. 2, the process begins by sequentially sampling draft tokens from the low-resolution model (y Mq, Step 1 ). These tokens are then upsampled (x = Ur(y)) to expand the sequence length by r2 (Step 2 ). In Step 3 , the target model Mq verifies in parallel. Steps 4 5 , discussed in the next section 3.3, apply an acceptance rule to determine which tokens to keep. Rejected tokens are resampled sequentially using Mp (Step 6 ). Finally, verified tokens are appended to the accepted prefix to form x, which is downsampled to = Dr(x) (Step 7 ). These downsampled tokens serve as the prefix for the next low-res draft sampling. The cycle repeats until = , the target sequence length. Our method has three key differences with standard speculative decoding: (i) unlike speculative decoding, where the draft model typically proposes the next tokens without regard to resolution or image boundaries, our draft model generates full rows to help the up-sampler produce coherent high-resolution patches; (ii) all rejected tokens are re-sampled by the target model, which simplifies the down-samplers role to only processing verified tokens; and (iii) the draft model has the same computational complexity as the target model, so speedup comes from reducing the number of function evaluations (NFE) and exploiting the quadratic gap in sequence size between lowand highresolution representations. While this design simplifies down-sampling, it introduces bottleneck during inference due to sequential sampling within the target model. Consequently, achieving speedups comparable to LANTERN or speculative decoding requires higher acceptance rates. 3.3. Local Veriﬁcation Our initial experiments used the LANTERN rule [19] as described in Eq. (3), which rejects all draft tokens after the first rejected token in raster-scan order. However, because our framework requires re-sampling every rejected token with the target model, this approach resulted in low acceptance rates and negligible speedup. To address this, we adopt relaxed criterion: accept draft token if the pooled probability over its neighborhood exceeds threshold τ (Step 4 in Fig. 2): Accept if xBk(xi) pi(x) τ. (5) Higher τ values yield closer approximation to the target model but slower inference, while lower values trade accuracy for speed. Visual AR models rely on localized attention, where token predictions are strongly influenced by nearby context and weakly by distant regions [15, 60]. To exploit this, we introduce local expansion strategy that re-samples only within small neighborhood 4 512p, and 1024p. These checkpoints share the same LlamaGen 600M [44] architecture and are progressively finetuned to generate longer token sequences at higher resolutions. MuLo-SD Our method is implemented within Tars oﬀicial GitHub repository. We use AR-DTok @ 256 as the autoregressive drafter and pair it with two sets of up/down-samplers to enable 2 (512p) and 4 (1024p) generation. The verifier is represented by ARDTok at the desired output resolution. The up/down-samplers are lightweight convolutional networks composed of residual blocks [14], with re-sampling performed via pixel shuffling [43]. To maintain compatibility with the autoregressive decoding order, all convolutions are masked to be row-causal. Training follows modified VQ-GAN [7] recipe adapted from ImageFolder [26], with an added commitment loss [49] to encourage proximity to the VQ codebook vectors. Each module is trained for 150k steps (under 24 hours on 4 NVIDIA A100 GPUs) using combination of four losses: (i) Distortion losses: Mean squared error (MSE) and LPIPS [59], to balance pixel-level accuracy and perceptual similarity. (ii)Commitment loss: To align outputs with the discrete latent space of the VQ-VAE. (iii) Adversarial loss: PatchGAN discriminator [18] trained with hinge loss [31], LeCam regularization [48], and discriminator augmentation [21] to improve realism and robustness. We first pretrain the 2 up-sampler and subsequently add second stage for 4 up-sampling, finetuning it for an additional 150k steps. Baselines First, we ported the oﬀicial implementation of ZipAR [15] into the Tar codebase and used it as training-free parallel decoding method. Next, we adopted the oﬀicial LANTERN [19] repository, which supports both LANTERN and EAGLE-2 [27]. We trained two drafter models one for 512p and one for 1024p using the provided scripts, adapting them to operate within Tars latent space. We set the LANTERN hyperparameters to = 1000 (defining the codebook search space) and δ = 0.4 (TVD threshold), following the configuration reported in the original paper. For further details, please refer to the supplementary material. Metrics We evaluate all methods along three key dimensions: decoding eﬀiciency, semantic alignment, and perceptual quality. https://github.com/csuhan/Tar https://github.com/thisisbillhe/zipar https://github.com/jadohu/LANTERN Figure 3. Representation of the local expansion rule. Green accepted and purple rejected tokens. (a) Rt, the set of rejected indices under the target mode as in Eq. (5), (b) shows raster-scan rejection as in standard SD, (c) RX , the newly introduced local expansion around rejected tokens Rt with radius = 1 as in Eq. (7). around rejected tokens. This targets areas with high local dependency while preserving distant accepted toIt is illustrated in kens, whose influence is minimal. Step 5 of Fig. 2, and compared to raster-scan rejection in Fig. 3. Our ablation study (Sec. 4.3) confirms that omitting local expansion degrades performance, validating its necessity. This approach improves sampling eﬀiciency without compromising perceptual quality. Let RT = (t0, . . . , tm) be the set of rejected indices under the target model following Eq. (5). For any position RT , we define its local neighborhood of radius as: (t, l) = n (cid:12) (cid:12) (cid:12) iu it l, ju jt l, t0 , (6) where (iu, ju) and (it, jt) are the 2D coordinates of indices and t, and t0 be the index of the first token rejected by the target model. The last condition ensures we do not revisit tokens before the first rejection. We consider the set RX of all locally expanded rejected tokens: [ RX = (t, l), (7) tRT and sequentially re-sample all positions in RX using the target model Mp. This local expansion strategy is illustrated in Fig. 3. 4. Experiments 4.1. Setting We conduct all experiments using Tar-1.5B [12], an MLLM finetuned from QwenVL-1.5B [1]. Tar is equipped with the AR-DTok generative detokenizer, enabling fully autoregressive text-to-image generation in the latent space of discrete VQ-VAE [7]. The model uses single MLLM backbone and supports three resolution-specific AR-DTok checkpoints: 256p, 5 Table 1. Comparison of decoding speedup versus GenEval [9], DPG-Bench [17], and perceptual metrics (FID [16] and HPSv2 [55]), computed on the MS-COCO 5k validation split [32]. We sweep the acceptance threshold τ in MuLo-SD and report the operating point that more closely matches LANTERNs GenEval score. Eﬀiciency Speedup () GenEval () DPG-Bench () FID ()"
        },
        {
            "title": "Perceptual Quality",
            "content": "B 7 2 5 ."
        },
        {
            "title": "Method",
            "content": "Chameleon-7B [45] LWM-7B [34] Lumina-mGPT-7B [33] ILLUME-7B [51] Transfusion-7B [61] Janus-Pro-7B [4] Show-O-1.3B [57] Janus-1.3B [53] D-DiT-2B [30] Emu3 [52] Janus-Pro-1B [4] Harmon-1.5B [54] Tar-1.5B @ 512 + ZipAR-16 [15] + EAGLE-2 [27] + LANTERN [19] + MuLo-SD (2x) Tar-1.5B @ 1024 + ZipAR-16 [15] + EAGLE-2 [27] + LANTERN [19] + MuLo-SD (4x) - - - - - - - - - - - - 1.00 1.88 0.72 1.08 1.22 1.00 3.65 0.78 1.42 1.68 39.0 47.0 56.0 61.0 63.0 80.0 53.0 61.0 65.0 66.0 73.0 76.0 77.7 76.6 (-1.1) 77.7 (-0.0) 75.9 (-1.8) 76.0 (-1.7) 77.1 76.6 (-0.5) 77.1 (-0.0) 75.4 (-1.7) 76.3 (-0.8) - - 79.7 - - 84.2 - 79.8 - 80.6 82.6 82.6 82.8 82.8 (-0.0) 82.8 (-0.0) 82.1 (-0.9) 82.4 (-0.4) 82.3 82.5 (+0.2) 82.3 (-0.0) 82.3 (-0.0) 82.0 (-0.3) HPSv2 () - - - - - - - - - - - - - - - - - - - - - - - - 33.0 33.0 (-0.0) 33.0 (-0.0) 32.7 (-0.3) 33.7 (+0.7) 32.4 32.4 (-0.0) 32.4 (-0.0) 31.1 (-1.3) 32.8 (+0.4) 28.6 28.5 (-0.1) 28.6 (-0.0) 27.7 (-0.8) 27.8 (-0.7) 29.5 29.6 (+0.1) 29.5 (-0.0) 28.5 (-1.0) 28.4 (-1.1) Decoding eﬀiciency is measured with the speedup i.e., the ratio between the latency of the baseline sequential decoding and that of the evaluated method. Values greater than 1 indicate acceleration, while values below 1 reflect slowdown. Latency is measured in seconds using PyTorch CUDA events on an NVIDIA A100 GPU, with batch size of 1 and image resolutions of either 512p or 1024p, depending on the experiment. Semantic alignment between text prompts and generated images is evaluated using GenEval [9] and DPGBench [17], two recent benchmarks designed to measure multimodal consistency and grounding. Perceptual quality is assessed using Fréchet Inception Distance (FID) [16], which quantifies the distributional similarity between generated and real images, and Human Preference Score v2 (HPSv2) [55], learned metric that approximates human judgments of image quality. Datasets The upand down-sampling modules of the drafter models used MuLo-SD, as well as in LANTERN, are trained on the LAION-COCOAesthetic dataset [24], which provides high-quality imagetext pairs with aesthetic filtering. For evaluation, we compute FID and HPSv2 on the MS-COCO 2017 validation split (5k images) [32]. 4.2. Main Results Quantitative Evaluation In Tab. 1, we compare MuLo-SD against several baselines: ZipAR [15], parallel decoding method designed for image generation; EAGLE-2 [27], standard speculative decoding method; and LANTERN [19], speculative approach tailored for images. We also include other representative understanding-and-generation models to contextualize the results within the broader literature. As preliminary observation, we note that Tar [12] provides strong foundation for our work, achieving state-of-the-art performance within the small-scale regime (i.e., models with fewer than 2B parameters). It demonstrates the potential of fully autoregressive models, particularly in tasks involving complex prompts 6 Tar ZipAR-16 LANTERN MuLo-SD Tar ZipAR-16 LANTERN MuLo-SD Figure 4. Visual comparison of 1024p image generations. Each example shows its speedup over the base Tar model (bottom-left). Outputs from EAGLE-2 are omitted since, as an exact decoding method, they match the base model. See the supplementary material for full comparisons and prompts. and semantic alignment. For instance, Tar is surpassed only by Janus-Pro [4] in GenEval score, although the latter operates at substantially larger complexity. We structure our comparison across two resolutions: 512p and 1024p. Standard Speculative Decoding methods such as EAGLE-2 perform poorly on image data, often resulting in negative speedups due to low acceptance rates caused by token ambiguity. LANTERN relaxes the acceptance criterion and achieves latency improvements, at the cost of small degradation in the metrics. We observe lower speedups when applying LANTERN to Tar compared to those reported with LlamaGen in the original paper, likely because Tar is significantly stronger model (e.g., GenEval 77.7% vs. 32% [44]), making its distribution harder to approximate. We discuss this in more detail in the supplementary material. We sweep the acceptance threshold τ in MuLo-SD to match the GenEval scores achieved by LANTERN. Under similar or better scores, MuLo-SD consistently delivers greater speedups, ranking as the second-best method overall. At 1024p, our method incurs slight drop in metrics but achieves nearly 70% faster end-toend generation compared to standard sampling. Finally, while ZipAR [15] achieves the highest speedups and favorable trade-offs, it is not speculative decoding method. As such, it is orthogonal to our approach, and the two techniques could potentially be combined to leverage the strengths of both. Qualitative Evaluation We provide qualitative assessment in Figure 4, using the same operating points as those reported in Table 1. The visual comparisons highlight the perceptual quality of outputs generated by MuLo-SD, LANTERN, and ZipAR under similar GenEval scores. Overall, MuLo-SD achieves image quality comparable to LANTERN while consistently delivering higher speedups. This is particularly evident in complex scenes, such as the calculator in the first row, where our multi-scale formulation proves more effective at maintaining structural coherence. We also observe that our method performs robustly across range of visual patterns, including textures, object boundaries, and semantic layouts and different styles from photorealistic to cartoonish (see supplementary for high-resolution samples and extended comparison). 4.3. Ablation Studies UpDownsampler loss formulation are shown in Fig. 5a. Since Tar operates in the latent space of discrete VQ-VAE, our initial approach employed simple token-level classification loss ( purple ). While this setup provided functional starting point, the resulting images exhibited poor visual fidelity. Next, we removed latent-space supervision and instead applied reconstruction losses directly in pixel space rendering images with the VQ-Decoder. Specifically, we adopted combination of MSE and LPIPS [59] losses, which significantly improved perceptual quality ( green ). To further refine high-frequency details, we incorporate an adversarial component and evaluate two discriminator designs: DINO-based discriminator [42] ( teal ), known for its strong semantic consistency, and lightweight PatchGAN discrimina7 (a) UpDownsampling Losses (b) Probability Pooling (c) Local Verification and Expansion Figure 5. We ablate different components of our method: (a) the contribution of loss functions in the updownsamplers training, (b) the role of probability pooling during the verification process, and (c) comparison between standard rater-scan rejection and our proposed local rejection and expansion. MSD shortened version for multiscale speculative decoding. tor [18] ( pink ). While both improve perceptual quality, PatchGAN offers the best trade-off between visual fidelity and computational eﬀiciency. We adopt this configuration for all subsequent experiments. Probability Pooling as introduced by LANTERN [19] (see Sec. 3 for details) is explored in Fig. 5b. We compare two settings: considering only the drafted token probability ( green ), compared to pooling the probability of the nearest neighbors in the VQ codebook space ( pink ). Incorporating codebook-level proximity information improves acceptance rates and stabilizes performance, particularly beyond the 1.2 speedup regime. However, the gains remain modest compared to the baseline without pooling. This is expected, as the pooling parameter behaves similarly to the acceptance threshold τ , which serves as our primary relaxation mechanism. Local Verification and Expansion is shown in Figure 5c. We compare three configurations: (i) standard raster-scan rejection from speculative decoding ( teal ), which yields speedups at the cost of compromising image quality due to the low acceptance thresholds τ required to achieve high acceptance rates; (ii) naive local verification ( green ), which resamples only the rejected tokens without modifying their local context, resulting in even poorer performance; (iii) local verification with expansion ( pink ), our proposed method, which resamples tokens within radius around each rejected position. Local verification leverages the strong spatial locality inherent in visual autoregressive models, resulting 8 in higher speedups for the same acceptance threshold τ ( teal vs pink ). At the same time, our proposed expansion mechanism plays crucial role in enabling the verifier to correct not only the rejected tokens but also their surrounding context ( green vs pink ). Additional ablations exploring different neighborhood radii are provided in the supplementary. 5. Conclusion In this work we introduced MuLo-SD, multi-scale speculative decoding framework for accelerating autoregressive image generation. By combining lowresolution drafting with learned up/down-sampling modules and locality verification strategy, our method achieves substantial speedups up to 1.7 while maintaining strong semantic alignment and perceptual quality. Through extensive experiments on Tar-1.5B across 512p and 1024p resolutions, we demonstrated that MuLo-SD consistently outperforms speculative decoding baselines such as EAGLE-2 and LANTERN, and approaches the eﬀiciency of parallel decoding methods like ZipAR. Ablation studies further validate the effectiveness of our multi-scale design, probability pooling, and local verification and expansion mechanisms. MuLo-SD integrates seamlessly with next-token prediction objectives and unified MLLMs, making it practical and scalable solution for high-resolution image synthesis. Future work includes exploring hybrid integration with parallel decoding techniques such as ZipAR, and extending our framework to video generation and other multimodal tasks."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. 5, 12 [2] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads, 2024. 2 [3] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-Σ: Weak-toStrong Training of Diffusion Transformer for 4K Textto-Image Generation, 2024. 1 [4] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling, 2025. 1, 6, 7 [5] Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation, 2024. 16 [6] Sander Dieleman. Diffusion is spectral autoregression, 2024. 3 [7] Patrick Esser, Robin Rombach, and Björn Ommer. Taming Transformers for High-Resolution Image Synthesis, 2021. 5, 12 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis, 2024. 1 [9] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment, 2023. 2, 6, 15, 16 [10] Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2024. 2 [11] Hang Guo, Yawei Li, Taolin Zhang, Jiangshan Wang, Tao Dai, Shu-Tao Xia, and Luca Benini. FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning, 2025. 2, [12] Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, and Lu Jiang. Vision as Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations, 2025. 1, 4, 5, 6, 12, 15, 16 [13] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis, 2025. 2 [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770 778. IEEE, 2016. 5 [15] Yefei He, Feng Chen, Yuanyu He, Shaoxuan He, Hong Zhou, Kaipeng Zhang, and Bohan Zhuang. ZipAR: Parallel Auto-regressive Image Generation through Spatial Locality, 2025. 2, 3, 4, 5, 6, 7, 14, 15, 16 [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs Trained by Two Time-Scale Update Rule Converge to Local Nash Equilibrium, 2018. 2, 6, 15 [17] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment, 2024. 2, 6, 15 [18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Image-to-Image Translation with Alexei A. Efros. Conditional Adversarial Networks, 2018. 5, 8, 13 [19] Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, and Eunho Yang. LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding, 2025. 2, 3, 4, 5, 6, 8, 13, 14, 15, 16 [20] Siyu Jiao, Gengwei Zhang, Yinlong Qian, Jiancheng Huang, Yao Zhao, Humphrey Shi, Lin Ma, Yunchao Wei, and Zequn Jie. FlexVAR: Flexible Visual Autoregressive Modeling without Residual Prediction. abs/2502.20313, 2025. 2 [21] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training Generative Adversarial Networks with Limited Data, 2020. 5 [22] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast Inference from Transformers via Speculative Decoding, 2023. 2, 13 [23] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation, 2024. 1 [24] Guangyi Li. The LAION-COCO-Aesthetic Dataset. https://huggingface.co/datasets/guangyil/laion-cocoaesthetic, 2024. Accessed: 2025-11-13. 6, [25] Kunjun Li, Zigeng Chen, Cheng-Yen Yang, and JenqNeng Hwang. Memory-Eﬀicient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression, 2025. 2, 3 [26] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. ImageFolder: Autoregressive Image Generation with Folded Tokens, 2024. 5 [27] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees, 2024. 2, 5, 6, 14, 15 [28] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty, 2025. 2 [29] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao 9 Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-DiT: Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding, 2024. [30] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual Diffusion for Unified Image Generation and Understanding, 2025. 6 [31] Jae Hyun Lim and Jong Chul Ye. Geometric GAN, 2017. 5 [32] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft COCO: Common Objects in Context, 2015. 2, 6, 14, 15, 16 [33] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yi Xin, Xinyue Li, Qi Qin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining, 2025. 1, 4, 6 [34] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World Model on Million-Length Video And Language With Blockwise RingAttention, 2025. 1, [35] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix JuefeiXu, Ji Hou, and Saining Xie. Transfer between Modalities with MetaQueries, 2025. 1 [36] Sihwan Park, Doohyuk Jang, Sungyub Kim, Souvik Kundu, and Eunho Yang. Lantern++: Enhancing relaxed speculative decoding with static tree drafting for visual auto-regressive models, 2025. 2 [37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis, 2023. 1 [38] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-Image Generation, 2021. 1 [39] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating Diverse High-Fidelity Images with VQVAE-2, 2019. 3 [40] Sucheng Ren, Yaodong Yu, Nataniel Ruiz, Feng Wang, Alan Yuille, and Cihang Xie. M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality Image Generation, 2024. [41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation, 2015. 3 [42] Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger. Projected GANs Converge Faster. 34:17480 17492, 2021. 7 [43] Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-Time Single Image and Video Super-Resolution Using an Eﬀicient Sub-Pixel Convolutional Neural Network, 2016. 5 [44] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation, 2024. 2, 4, 5, 7, 12, 15 [45] Chameleon Team. Chameleon: Mixed-Modal EarlyFusion Foundation Models, 2025. 1, 6 [46] Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding, 2025. 3 [47] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction, 2024. 2, 3 [48] Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing Generative Adversarial Networks under Limited Data, 2021. 5 [49] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural Discrete Representation Learning, 2018. 5, 12 [50] Anton Voronov, Denis Kuznedelev, Mikhail and Dmitry Switti: Designing Scale-Wise TransKhoroshikh, Valentin Khrulkov, Baranchuk. formers for Text-to-Image Synthesis, 2025. [51] Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, and Hang Xu. ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance, 2024. 1, 6 [52] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: NextToken Prediction is All You Need. 2024. 6, 16 [53] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation, 2024. 6 [54] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing Visual Representations for Unified Multimodal Understanding and Generation, 2025. 1, 6 [55] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human Preference Score v2: Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis, 2023. 2, 6, 15 [56] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, Bingchen Liu, Daquan Zhou, and Song Han. SANA 1.5: Eﬀicient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer, 2025. 1 [57] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One Single Transformer to Unify Multimodal Understanding and Generation, 2025. 6 [58] Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft-andVerify: Lossless Large Language Model Acceleration via Self-Speculative Decoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), page 1126311282. Association for Computational Linguistics, 2024. 2, 3 [59] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as Perceptual Metric. In CVPR, 2018. 5, 7 [60] Zhuoyang Zhang, Luke J. Huang, Chengyue Wu, Shang Yang, Kelly Peng, Yao Lu, and Song Han. Locality-aware Parallel Decoding for Eﬀicient Autoregressive Image Generation, 2025. 2, 3, 4 [61] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model, 2024. 6 Multi-Scale Local Speculative Decoding for Image Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Primer on Tar We provide concise description of the Tar architecture and the default parameters used to obtain the results reported in this paper. For additional details, we refer the reader to the original publication [12]. Architecture For the purposes of this work, Tar consists of two main components: 1. Multimodal Large Language Model (MLLM) that processes the input prompt and generates conditioning sequence, 2. generative detokenizer that maps the conditioning sequence to VQ-VAE token sequence, which is then decoded to pixel space by the VQ-VAE decoder. The MLLM is fine-tuned from QwenVL [1] and extended to predict visual tokens. It is trained to output sequences of three different lengths: 81, 169, and 729 tokens. Each length corresponds to progressively stronger conditioning signal for the detokenizer. The autoregressive generative detokenizer (ARDTok) is based on the LlamaGen [44] model, fine-tuned to use the output of the MLLM as conditioning. Conditioning is implemented by pre-filling the sequence with one of the desired lengths (e.g., 81, 169, or 729). Importantly, the AR-DTok model operates in the latent space of VQ-VAE [7, 49], which performs 16 downsampling along both spatial dimensions, resulting in sequence lengths of 256, 1024, and 4096 tokens for the resolutions 256p, 512p and 1024p respectively. Sampling We now describe the sampling procedure. For the MLLM, we use the default configuration: topk = 1200, topp = 0.95, the temperature τlogits = 1.0 (different from the relaxed acceptance threshold τ defined in Eq. (5)) and set the sequence length to 729. The absolute latency for generating this conditioning sequence is approximately 17 seconds. Note that this value is not included in our latency analysis. This conditioning sequence is then used to sample from the AR-DTok model. For AR-DTok, we set: topk = 0, topp = 1.0 and the temperature τlogits = 1.0 (i.e. sampling from the full distribution of logits). Additionally, we apply classifier-free guidance with scale of 4.0, we use an empty sequence for the negative prompt. Sampling from AR-Dtok takes on average 5s, 18s and 80s for each resolution respectively, see Table 2 for an overview. Given that sampling the conditionTable 2. Summary of AR-Dtok configurations from Tar [12]."
        },
        {
            "title": "Resolution",
            "content": "Seq. Length Latency 256p 512p 1024p 256 1024 4096 5s 18s 80s ing sequence takes an average of 17s, it reinforces that MuLo-SDs best setting is the 4 case i.e., going from 256p to 1024p. In this scenario, the total latency is largely dominated by the AR-DTok decoding time, and accelerating the visual token generation will lead to substantial speedups. 7. MuLo-SD Method We describe the algorithm of MuLo-SD in Algorithm 1, full description can be found in Sec. 3.2 and Sec. 3.3 of the main paper. The step numbers 1 - 7 are reference to the schematic representation in Fig. 2 of the main paper. We present speculative decoding and LANTERN in the same style as our main method schema in Figure 6. For detailed description of their algorithm, see Sec. 3.1 in the main paper. Implementation Details The drafter model consists of three main components: an autoregressive model, an up-sampler, and down-sampler. The autoregressive model is set as AR-DTok @ 256p and remains fixed throughout all experiments. The upand downsampler are implemented as lightweight convolutional networks with residual blocks, and use pixel-shuffle to perform the correspondent resampling operation. We progressively train the upand downsampler for the 2 and the 4 settings. In the 2 setup, each module contains approximately 20M learnable parameters. These modules are trained on the LAION-COCO-Aesthetic [24] dataset for 150k steps with batch size of 32, using the AdamW optimizer with learning rate of 3e4. We use combination of losses for training: MSE, LPIPS, commitment loss, and discriminator loss. The overall objective is defined as: Ltot = LMSE + LLPIPS + Lcommit + λGAN LGAN. (8) For the first 20k iterations, the upand downsamplers are trained without the discriminator loss; this Algorithm 1: Multi-Scale Local Speculative Decoding Input: The target model Mp at scale sp, the draft model Mq at scale sq, the upand down-sampler Ur and Dr with resampling factor of = sp/sq, the initial sequence x0, . . . , xt, draft sequence length L, the target sequence length , the cardinality of latent neighborhood k, the TVD threshold δ, the probability mass threshold τ and the local neighborhood radius. 1 Initialize: t; 2 while < do 3 4 6 7 8 9 10 12 13 14 15 16 18 7 In parallel, down-sample the tokens to obtain prefix for draft model at scale sp: y1:n/r = Dr(x1:n); for = 1, . . . , L/r do 1 In sequence, sample tokens from draft model yt Mq(x y0, . . . , yn/r, y1, . . . , yt1); 2 In parallel, up-sample the L/r tokens to obtain draft tokens at scale sq: xn:n+L = Ur(yn/r:(n+L)/r); 3 In parallel, compute sets of logits: Mp(x x0, . . . , xn), Mp(x x0, . . . , xn, x1), . . . , Mp(x x0, . . . , xn, x1, . . . , xL); Initialize set of locally expanded rejected tokens RX {}; for = 1, . . . , do Find the neighborhood Ak,δ(xt); if xAk,δ(xt) Mp(x x0, . . . , xn+t1) > τ then 4 Accept: set xn+t xt; else 5 Reject: expand rejection to local neighborhood (t, l) around position with radius l, RX RX (t, l) Sort indices in RX ; for RX do 6 In sequence, sample rejected tokens from target model xn+k Mp(x x0, . . . , xn+k1); Set + Output: xt+1, . . . , xT (a) Speculative Decoding (b) LANTERN Figure 6. Overview of the standard speculative decoding [22] and LANTERN [19] methods. They are drawn in the same style as our main method figure for ease of comparison. Blue indicates draft tokens, green accepted tokens, purple rejected tokens, blank placeholder tokens. due to looping. indicates sequential operations, parallel operations, drawing discontinuity component is introduced afterward. The discriminator follows the standard PatchGAN design [18], consisting of three convolutional layers, and is trained from scratch using AdamW with learning rate of 5e4 with λGAN = 0.25. the 4 case (resulting in approximately 30M parameters for each module). The upand downsampler are warm-started from the 2 checkpoints and trained for another 150k steps. We use the same configurations, except smaller batch size of 8 to fit into memory. Next, we add an additional block of convolutions for During inference, MuLo-SD introduces one primary hyperparameter: the acceptance threshold τ (see Eq. (5)). We perform sweep over various values and ultimately fix τ = 1e 4, unless otherwise specified. Additionally, two other hyperparameters control the probability aggregation from neighboring elements (see Step 4 of Figure 2). These are set to = 1000 and δ = 0.1, and remain constant across all experiments. As discussed in the main paper, (k, δ) and τ play similar role in relaxing the acceptance criterion; therefore, we primarily experiment with τ while keeping the others fixed. Latency Analysis As discussed in the main paper, one of the key characteristics of MuLo-SD is the computational cost associated with the drafter model, which shares the same architecture as the target model. This allows us to estimate the theoretical speedup under different acceptance rates by considering the reduction in the number of function evaluations (NFE) throughout the model. The theoretical speedup ST can be computed as follows. Using the notation from the main paper, let Mp denote the target model and Mq the drafter model, and define Tp and Tq as the sequence lengths for the target and drafter respectively, and let denote the acceptance rate. Then: ST = Tp (1 a) Tp + Tq . (9) We compute the empirical speedup by measuring the time required to generate 500 prompts from MS-COCO 2017 Validaiton Set [32] on single NVIDIA A100 GPU with batch size of 1. We break down the individual cost of each component in Figure 7. First, we observe that the cost of the drafter is fixed, regardless of the acceptance rate, since we always sample the same number of tokens from it. This eventually becomes the bottleneck in the 512p case, reducing the overall utility of our method. Conversely, at higher resolutions, the number of tokens generated by the target model is so large that the drafters cost becomes negligible. This further reinforces the suitability of the 4 setting (256p 1024p) for our model. As shown visually, almost all of the latency budget is spent sequentially sampling from either the target model or the drafter. This leads to two important considerations: (i) our proposed multiscale speculative decoding introduces only negligible overheadabout 5% and 3% for the 512p and 1024p settings, respectively; and (ii) there is still room for improvement by reducing the cost of the drafter and the verifier. Therefore, integrating parallel decoding techniques (e.g., ZipAR [15]) could pave the way for even greater speedups. Figure 7. Breakdown of latency analysis. The figure illustrates the proportion of time spent in each step of our algorithm relative to the total latency. The step number in the legend refers to Fig. 2 in the main paper. 8. Experiments Quantitative Evaluation We extend the quantitative results from the main paper by providing graphical visualization of Table 1 in the main paper in Figure 8. It shows the pareto front of MuLo-SD and contextualizes its performance with competing methods such as ZipAR [15], EAGLE-2 [27] and LANTERN [19]. To create pareto front, we vary the acceptance rate by sweeping different values for the relaxed acceptance threshold τ as defined in Equation 5 in the main paper. We can see that ZipAR dominates all other methods, with mostly unchanged perceptual quality compared to the reference, and only slight degradation to GenEval. Next comes our method MuLo-SD, which across the semantic alignment metrics dominates EAGLE-2 and LANTERN. When it comes to perceptual quality metrics, FID tends to suffer for MuLo-SD compared to other methods, and HPSv2 is sligthly better for MuLoSD. Qualitative Evaluation We supplement the qualitative results with additional visual comparisons. In Figure 10 we show samples from Tar-1.5B 512p and MuLoFigure 8. Quantitative evaluation of ZipAR [15], EAGLE-2 [27], LANTERN [19] and our method MuLo-SD. We report the GenEval [9] and DPG-Bench [17] semantic alignment metrics, along with the FID [16] and HPSv2 [55] perceptual quality metrics as computed on MS-COCO [32] 2017 Val 5k. To obtain curve for MuLo-SD, we sweep the acceptance relaxation parameter τ , as described in Section 3.3 and Equation (5) in the main paper. SD for the 2 case (256p 512p). In Figure 11 and Figure 12 we show additional results from Tar1.5B 1024p and MuLo-SD for the 4 case (256p 1024p). In both the 512p and 1024p cases, the acceleration comes at slight cost in perceptual quality, howevere the semantic alignment is mostly unaltered. Finally, in Figure 13 we showcase the effect of sweeping the relaxed acceptance threshold τ on the output image quality, where the different τ used correspond to the points in Figure 8. The third column τ = 1e 4 corresponds the setting reported in Table 1 in the main paper. Note that it seems like the best tradeoff between speedup and perceptual quality, where the rightmost column (τ = 1e 5) shows the greatest speedup but largest degradation in quality, and the leftmost column (τ = 1e 3) is the closest to the original but ends up slower for more complex prompts. Note that for all qualitative figures, both in the main text and the supplementary, we use prompts sourced from the DPG-Bench [17] benchmark dataset. We report the IDs of the prompts used in Fig. 4 of topthe main paper left-right) and refer to the oﬀicial code for bottom, the actual text: 78.txt, midjourney32.txt, COCOval2014000000580698.txt, stanford34.txt, 5.txt, COCOval2014000000183648.txt, 62.txt, diffusiondb10.txt. (order https://github.com/TencentQQGYLab/ELLA/tree/main/ dpg_bench/prompts. 15 Additional Ablation We extend the ablation presented in Figure 5 (c) in the main paper. We show the effect of the local expansion radius in Figure 9, showcasing = 1 and = 5 in addition to our default value of = 3 shown in the main paper. Similar to the other ablations in the main paper, the study is performed in the 2 case (256p 512p). We can see that = 3 provides the best boost in GenEval performance across the 1 - 1.5 speedup range of interest. It is closely followed by = 1, with = 5 lagging behind. We expect the optimal value for to depend heavily on the resolution, as large resolution will benefit from larger radii, and conversely smaller resolution will suffer from larger radii as it will lead to high rejection rate even for permissive relaxed acceptance thresholds τ . We anyway use = 3 for the 1024p case based on the result of this ablation due to lack of computational resource and time to ablate the parameter on the higher resolution. Discussion on LANTERN As discussed in the main paper, porting LANTERN [19] (and EAGLE-2 [27]) to Tar [12] proved significantly more challenging yielding worse performancethan what was originally reported for LlamaGen [44]. In this section, we detail our training procedure and provide additional justifications for the observed results. We follow the original training script from the LANTERN codebase. The drafter consists of single transformer layer and mate due to its closer alignment with the true data distribution. For instance, Tar achieves significantly higher scores on benchmarks such as GenEval, where LlamaGen reportedly [52] scores 32% compared to 78% for Tar. Furthermore, the original paper notes that the drafter performs worse on slightly stronger models like Anole [5] compared to LlamaGen, reinforcing our hypothesis. Finally, we emphasize that the test sets differ, so direct comparisons are not strictly valid, although they provide context for interpreting our results. 9. Acknowledgments We thank the authors of Tar [12], LANTERN [19] and ZipAR [15] for sharing their models and implementations. Figure 9. We study the effect of the local expansion radius in MuLo-SD on GenEval [9] for the 2case (256p 512p). This expands the ablation in Figure 5 (c) in the main paper. is trained using activations from the last transformer block of the target model (i.e. before the final softmax fully connected layer). The training objective combines two losses: (i) standard cross-entropy loss for next-token prediction, and (ii) an L1 loss to regress the hidden state of the teacher (i.e. the target model). The overall loss is weighted using the configuration reported in LANTERN, with λL1 = 0.1 for the regression term. We train the drafter on subset of the LAION-COCO-Aesthetic dataset [32], using 100k samples for training and reserving 1k samples for evaluation. Since LANTERN does not specify the dataset used to train the drafter, one-to-one comparison is not possible. Nevertheless, in our setting, we measure Top-1 and Top-3 accuracy on the held-out test set as proxies for drafter quality. Higher accuracy correlates with greater inference-time speedups, as more tokens are accepted by the target model. We select the drafter achieving the highest test accuracy as our final model. Our results are as follows: 512p: Top-1 = 0.12, Top-3 = 0.19 1024p: Top-1 = 0.22, Top-3 = 0.33 When compared to LlamaGen results reported in the LANTERN paper (see Fig. 2(b) in [19]), our Top1 accuracy is substantially lower (0.12 vs. 0.38). We attribute this discrepancy to Tar being much stronger model than LLamaGen, making it harder to approxi16 Tar-1.5B @ 512 MuLo-SD (2) Tar-1.5B @ 512 MuLo-SD (2) Figure 10. Visual comparison of 512p resolution, speedup displayed at the bottom-left corner. Prompts from DPG-Bench (top-bottom, left-right): partiprompts175.txt, 55.txt, partiprompts124.txt, partiprompts303.txt, stanford6.txt, 180.txt, partiprompts177.txt, COCOval2014000000231527.txt, stanford36.txt, 189.txt 17 Tar-1.5B @ 1024 MuLo-SD (4) Figure 11. Visual comparison of 1024p image generations. Prompts from DPG-Bench: partiprompts175.txt, 55.txt. 18 Tar-1.5B @ MuLo-SD (4) Figure 12. Visual comparison of 1024p image generations. Prompts from DPG-Bench: 180.txt, partiprompts177.txt. 19 Tar-1.5B@1024 τ = 1e3 τ = 1e τ = 5e5 τ = 1e5 Figure 13. Visual comparison of 1024p image generations. We sweep the value of τ , our relaxed acceptance threshold as defined in Equation 5 in the main paper and show the related results. Prompts from DPG-Bench: drawtext19.txt, partiprompts77, midjourney33, partiprompts177.txt, 74.txt, 73.txt."
        }
    ],
    "affiliations": [
        "Qualcomm AI Research"
    ]
}