{
    "paper_title": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs",
    "authors": [
        "Tiancheng Gu",
        "Kaicheng Yang",
        "Ziyong Feng",
        "Xingjun Wang",
        "Yanzhao Zhang",
        "Dingkun Long",
        "Yingda Chen",
        "Weidong Cai",
        "Jiankang Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Contrastive Language-Image Pre-training (CLIP) framework has become a widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored.In this work, we present UniME (Universal Multimodal Embedding), a novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from a powerful LLM-based teacher model to enhance the embedding capability of the MLLM\\'s language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short and long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 2 3 4 7 1 . 4 0 5 2 : r Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs Tiancheng Gu*, Kaicheng Yang*, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, Jiankang Deng The University of Sydney DeepGlint Tongyi Lab, Alibaba GroupImperial College London tigu8498@uni.sydney.edu.au, kaichengyang@deepglint.com (cid:140) Project Page (cid:135) Code Figure 1: The UniME framework incorporates textual discriminative knowledge distillation and hard negative enhanced instruction tuning stages to learn discriminative representations for diverse downstream tasks. Our framework achieves state-of-the-art performance on both the MMEB benchmark and multiple retrieval tasks. Abstract The Contrastive Language-Image Pre-training (CLIP) framework has become widely used approach for multimodal representation learning, particularly in image-text retrieval and clustering. However, its efficacy is constrained by three key limitations: (1) text token truncation, (2) isolated image-text encoding, and (3) deficient compositionality due to bag-of-words behavior. While recent Multimodal Large Language Models (MLLMs) have demonstrated significant advances in generalized vision-language understanding, their potential for learning transferable multimodal representations remains underexplored. In this work, we present UniME (Universal Multimodal Embedding), novel two-stage framework that leverages MLLMs to learn discriminative representations for diverse downstream tasks. In the first stage, we perform textual discriminative knowledge distillation from powerful LLM-based teacher model to enhance the embedding capability of the MLLMs language component. In the second stage, we introduce hard negative enhanced instruction tuning to further advance discriminative representation learning. Specifically, we initially mitigate false negative contamination and then sample multiple hard negatives per * Equal Contribution Corresponding Author. instance within each batch, forcing the model to focus on challenging samples. This approach not only improves discriminative power but also enhances instruction-following ability in downstream tasks. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including short&long caption retrieval and compositional retrieval. Results demonstrate that UniME achieves consistent performance improvement across all tasks, exhibiting superior discriminative and compositional capabilities. Keywords Vision-Language Model, Multi-Modal, Compositional Understanding"
        },
        {
            "title": "1 Introduction\nModern AI applications increasingly rely on multimodal embed-\ndings to process diverse data types, powering essential tasks\nlike image-text retrieval [3, 49], Retrieval Augmented Genera-\ntion (RAG) [10, 23], and Visual Question Answering (VQA) [9, 13,\n16]. As a seminal model, CLIP [42] demonstrates notable text-image\nretrieval performance via cross-modal contrastive supervision us-\ning large-scale web-collected image-text pairs. However, despite its\nwidespread use, CLIP presents notable limitations. Firstly, it restricts",
            "content": "UniME, Preprint, 2025 Tiancheng Gu, Kaicheng Yang et al. text token length to 77, hindering its ability to process detailed descriptions and limiting its utility in cross-modal retrieval tasks that require extensive contextual information [6, 19, 62]. Moreover, CLIP employs dual-encoder architecture that processes images and text separately, which compromises its effectiveness in complex tasks such as instruction-following multimodal retrieval [22, 34, 56]. Additionally, CLIP exhibits limited advanced language understanding, struggles with compositionality, and tends to display bag-of-words behavior [52, 60]. The success of Large Language Models (LLMs) [2, 17, 50, 51, 59] has motivated researchers to adapt LLMs to understand multimodal inputs. Multimodal Large Language Models (MLLMs) as key component in the construction of general-purpose AI assistants have demonstrated remarkable progress [31, 33]. For example, Qwen2-VL [53] innovates beyond fixed-resolution visual processing, achieving robust performance across diverse image resolutions and aspect ratios. Similarly, LLaVA-OneVision [27] introduces unified modeling approach that enables effective task transfer across scenarios while maintaining architectural simplicity. While these MLLMs show impressive vision-language reasoning capabilities, these MLLMs are inherently constrained by their autoregressive next-token prediction objective, which limits their effectiveness in learning multimodal representations compared to contrastive methods such as CLIP [21, 22]. Recent advances in LLM-based models have demonstrated substantial progress on the MTEB benchmark [37]. Inspired by these developments [4, 26], researchers are now actively investigating MLLMs for unified multimodal representation learning. E5-V [21] proposes an unimodal contrastive learning approach that trains the language component of MLLM on sentence pairs to bridge cross-modal representation disparities. However, this method encounters two primary constraints: (1) constraints arising from the limited scale and diversity of training data [39]; (2) inherent challenges caused by the MLLMs causal attention mechanism, which fundamentally restricts its ability to learn complex contextual representations [36, 57, 58]. These factors collectively constrain the models full embedding potential. VLM2Vec [22] introduces the Massive Multimodal Embedding Benchmark (MMEB), comprising 36 datasets across 4 meta-tasks, and develops contrastive framework that converts state-of-the-art vision-language models into embedding models through MMEB training. Nevertheless, the existence of false negative samples in the batch significantly complicates the discrimination of hard negative pairs when using the standard InfoNCE loss. To overcome these challenges, we present UniME (Universal Multimodal Embedding), novel two-stage framework that empowers multimodal large language models (as shown in Figure 1) to learn universal representations for diverse downstream visionlanguage tasks. In the first textual discriminative knowledge distillation stage, we leverage powerful LLM-based teacher model to enhance the embedding capabilities of MLLMs language component. In the second stage of hard negative enhanced instruction tuning, we first eliminate false negative contamination, then implement hard negative sampling strategy that selects multiple challenging negatives per instance within each batch. This approach forces the model to focus on challenging negative samples, thereby learning more discriminative multimodal representations while also improving instruction-following ability in downstream tasks. We evaluate our approach comprehensively on the MMEB benchmark and multiple retrieval tasks, including both short&long caption retrieval and compositional retrieval. Experimental results demonstrate that UniME achieves significant performance improvement across all tasks, exhibiting both robust discriminative power and superior compositional understanding. The main contributions of this paper are summarized as follows: We present UniME (Universal Multimodal Embedding), novel two-stage framework that empowers Multimodal Large Language Models (MLLMs) to learn universal representations for diverse downstream tasks. We propose textual discriminative knowledge distillation, leveraging powerful LLM-based teacher model to enhance the embedding capability of the MLLMs language component. We introduce hard negative enhanced instruction tuning to further advance discriminative representation learning through false negative filtering and hard negative sampling. We conduct extensive experiments on the MMEB benchmark and multiple retrieval tasks, including both short&long caption retrieval and compositional retrieval. Results show that UniME demonstrates robust discriminative and compositional capabilities, achieving notable performance improvements across all evaluated tasks."
        },
        {
            "title": "2.2 LLMs for Representation Learning\nAs large language models increasingly exhibit remarkable profi-\nciency in natural language processing, recent research has pivoted\ntowards harnessing decoder-only architectures for effective repre-\nsentation learning [4, 12, 26, 35, 45]. Previous work has adapted the\nprompt-based representation method for autoregressive models,\nenabling Large Language Models (LLMs) to perform in-context\nlearning and scale to various model sizes. LLM2Vec [4] transform-\ning pre-trained decoder-only LLMs into versatile text encoders by",
            "content": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs UniME, Preprint, 2025 Figure 2: The framework of the Textual Discriminative Knowledge Distillation stage. We leverage the state-of-the-art LLM-based embedding model to enhance the discriminative capabilities of the MLLMs language component. incorporating three principal advancements: bidirectional attention mechanisms, masked next-token prediction, and unsupervised contrastive alignment. Concurrently, NV-Embed [26] introduces latent attention layer and eliminates the causal attention mask during contrastive training, substantially enhancing the efficiency of embeddings generated from decoder-only LLMs. While these approaches show promising embedding performance, their exclusive focus on text-only inputs fails to meet the growing demands of multimodal applications."
        },
        {
            "title": "3 Method\nThis section first establishes the preliminary definitions, including\ntask formulation and feature extraction (Section 3.1). Then we start",
            "content": "to introduce our proposed novel two-stage framework UniME. We elaborate on the first textual discriminative knowledge distillation stage in Section 3.2. After that, we present the second hard negative enhanced instruction tuning stage in Section 3.3. Finally, we introduce the training recipe in Section 3.4."
        },
        {
            "title": "3.1 Preliminary\n3.1.1 Task Definition. To address the limitations of dual-tower\nencoder structures in acquiring unified multimodal representa-\ntions [42], we employ Multimodal Large Language Models (MLLMs)\nwith robust multimodal understanding to learn universal multi-\nmodal embeddings. Specifically, we feed both the query and can-\ndidate data into the MLLM using customized prompts to extract\ntheir respective embeddings. We then calculate the similarity (Θ)\nbetween the query (𝒒) and candidates (𝒄), followed by ranking and\nselecting the most relevant pairs (𝑷 ). This procedure is formalized\nas follows:",
            "content": "𝑷 = Rank(Θ(𝜙 (𝒒), 𝜙 (𝒄))), (1) where 𝜙 denotes the MLLM employed to extract respective embeddings. Notably, both the query and candidate may be either unimodal (text or image only) or multimodal (interleaved imagetext). Feature Extraction by MLLM.. Unlike the dual-tower struc3.1.2 ture of CLIP, MLLM incorporates three essential components: vision tower, projection layer, and an LLM backbone. This unified structure supports flexible processing of both unimodal (image or text) and multimodal (interleaved image-text) inputs, enabling diverse task execution within single framework. In this work, we present novel two-stage framework that enables MLLMs to learn universal multimodal embedding for diverse downstream tasks. In the first textual discrimination knowledge distillation stage, we follow the previous work [21] and employ the prompt: \"<Text> Summarize above sentences in one word: n\" to guide the LLM to compress textual information into single embedding at the last UniME, Preprint, 2025 Tiancheng Gu, Kaicheng Yang et al. Figure 3: The framework of the Hard Negative Enhanced Instruction Tuning stage. We further improve the discriminative capabilities of the MLLM through false negative filtering and hard negative sampling. token. In the second hard negative enhanced instruction tuning stage, we utilize task-specific prompts from VLM2Vec [22] to adapt UniME fit diverse downstream tasks such as: \"<Image> Represent the given image with the following question: <Text>\" for Visual Question Answering (VQA) tasks and \"<Image> Find caption for the news in the given photo.\" for retrieval tasks. where 𝜏 is the temperature hyper-parameter used to soften distribution representation. By distilling the relationships between different samples within batch, our method demonstrates enhanced efficiency compared to direct contrastive learning under identical data and training conditions and achieves significant performance improvements in downstream tasks."
        },
        {
            "title": "Distillation",
            "content": "3.2.1 Training. Inspired by previous research [21], we enhance the LLM backbone of the MLLM to improve its overall embedding capabilities. The autoregressive decoder architecture of LLMs, constrained by causal masking mechanism, inherently restricts their discriminative capacity and poses challenges in effectively distinguishing between diverse items. To address this limitation, we introduce discriminative textual knowledge distillation (as shown in Figure 2), transferring knowledge from the state-of-the-art LLMbased embedding model NV-Embed V2 [26], which employs multiple diverse datasets and removes the causal attention mask during contrastive training. Specifically, we first decouple the LLM component from the MLLM architecture and process text-only inputs using the embedding prompt: Summarize the above sentences in one word: n. Then we obtain the normalized student text embeddings 𝑒𝑠 R𝑛𝑑 and teacher text embeddings (which is extracted offline) 𝑒𝑡 R𝑛𝑑 from the hidden state of the final token, where 𝑛 is the batch size, 𝑑 is the dimension of the embeddings. Subsequently, we implement discriminative distribution alignment by minimizing the Kullback-Leibler (KL) divergence [46] between the embeddings from the teacher model and student model: L𝐾𝐿 = 𝑛 𝑖=1 KL (cid:169) (cid:173) (cid:173) (cid:173) (cid:173) (cid:171) 𝑒 𝑠𝑖 exp (cid:16) exp (cid:16) 𝑛 (cid:205) 𝑗=1 (cid:17) 𝑒𝑠𝑖 /𝜏 (cid:17) 𝑒𝑠𝑖 /𝜏 𝑒 𝑠 𝑗 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 𝑒 𝑡𝑖 exp (cid:16) exp (cid:16) 𝑛 (cid:205) 𝑗=1 (cid:17) 𝑒𝑡𝑖 /𝜏 (cid:17) 𝑒𝑡𝑖 /𝜏 𝑒 𝑡 𝑗 , (2) (cid:170) (cid:174) (cid:174) (cid:174) (cid:174) (cid:172) Inference. During the training phase, our approach exclu3.2.2 sively utilizes text-only inputs and optimizes solely the language model component within the multimodal language model architecture, while maintaining other parameters frozen. For inference, we restore the original vision encoder and projection layer to enable multimodal processing. For unimodal inputs (text or image), we use modality-specific standardized prompts. For interleaved image-text inputs, we process each modality independently with its corresponding prompt and aggregate the embeddings through element-wise summation to produce the final multimodal representation."
        },
        {
            "title": "3.3 Hard Negative Enhanced Instruction Tuning\nAfter textual discriminative knowledge distillation, UniME devel-\nops preliminary discriminative capabilities but exhibits limited\nvisual sensitivity. This insensitivity results in deviations in image-\ntext alignment and limits the discriminative performance, despite\nthe MLLM’s extensive pretraining on large-scale datasets. More-\nover, the generic instruction prompts used in the first stage hinder\nUniME’s effectiveness in complex retrieval tasks. To address these\nlimitations, we introduce an additional hard negative enhanced\ninstruction tuning stage (as shown in Figure 3), which aims to:\n(1) further enhance discriminative capabilities, (2) improve cross-\nmodal alignment, and (3) strengthen instruction-following ability\nfor downstream tasks.",
            "content": "False Negative Contamination. The presence of false nega3.3.1 tives in training batches hinders effective hard negative differentiation under the standard InfoNCE loss. As evidenced in Table 8, Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs UniME, Preprint, 2025 Table 1: Results on the MMEB benchmark. IND represents the in-distribution dataset, and OOD represents the out-of-distribution dataset. The reported scores are the average Precision@1 over the corresponding datasets. The best results are marked in bold. : UniME with textual discrimination distillation only. : UniME with both textual discrimination distillation and hard negative enhanced instruction tuning. Models #Parameters Per Meta-Task Score Average Score Classification 10 VQA Retrieval Grounding 12 4 IND OOD 16 Overall 36 # of Datasets CLIP(ViT-L) [22] OpenCLIP(ViT-L) [42] Magiclens(ViT-L) [64] SigLIP(So/14) [61] BLIP2(ViT-L) [28] CLIP(ViT-BigG/14) [8] EVA-CLIP [48] E5-V(Phi3.5-V) [21] E5-V(LLaVA-1.6) [21] UniME(Phi3.5-V) UniME(LLaVA-1.6) CLIP(ViT-L) [22] VLM2Vec(Phi3.5-V) [22] VLM2Vec(LLaVA-1.6) [22] UniME(Phi3.5-V) UniME(LLaVA-1.6) 0.4B 0.4B 0.4B 0.9B 1.2B 2.5B 8B 4.2B 7B 4.2B 7B 0.4B 4.2B 7B 4.2B 7B 42.8 41.5 38.8 40.3 27.0 52.3 56.0 39.1 39.7 42.5(+3.4) 43.0(+3.3) 55.2 54.8 56.8 54.8(+0.0) 60.6(+3.8) Zero-shot on MMEB 9.1 6.9 8.3 8.4 4.2 14.0 10.4 9.6 10.8 18.3(+8.7) 17.7(+6.9) 53.0 44.6 35.4 31.6 33.9 50.5 49.2 38.0 39.4 40.5(+2.5) 42.5(+3.1) Fine-tuning on MMEB 51.8 53.5 26.0 59.5 47.0 60.3 58.9 57.6 60.2 59.9(+2.3) 63.2(+3.0) 37.1 32.8 31.0 32.3 25.3 38.9 38.1 33.1 34.2 36.0(+2.9) 37.6(+3.4) 38.7 36.0 23.7 38.0 25.1 45.8 45.6 31.9 33.4 38.3(+6.4) 38.6(+5.2) 39.2 36.6 27.1 35.0 28.0 44.3 43.7 36.1 37.5 40.3(+4.2) 41.6(+4.1) 19.7 54.9 50.4 55.9(+1.0) 52.9(+2.5) 53.2 62.3 63.3 64.5(+2.2) 67.9(+4.6) 62.2 79.5 82.6 81.8(+2.3) 85.1(+2.5) 47.6 66.5 64.9 68.2(+1.7) 68.4(+3.5) 42.8 52.0 53.9 52.7(+0.7) 57.9(+4.0) 47.6 62.9 63.3 64.2(+1.3) 66.6(+3.3) false negatives frequently appear as candidate samples. To mitigate this, we introduce filtering mechanism based on the similarity threshold between the query and positive samples, defined as: 𝛼 = 𝑐𝑜𝑠 (𝑒𝑞, 𝑒+ 𝑐 ) + 𝛽, where 𝛽 is hyper-parameter used to control the threshold margin. During training, we exclude all negative samples whose similarity to the query exceeds 𝛼, effectively eliminating false negatives while preserving challenging hard negatives. rarely), we duplicate existing hard negatives to preserve the fixed size 𝑘. 3.3.3 Training Objective. After obtaining the embedding of the queries (𝑒𝑞), positive candidates (𝑒+ 𝑐 ) and hard negative candidates (𝑒 𝑐 ), we utilize the Noise Contrastive Estimation (InfoNCE) loss [38] over the in-batch sampled hard negatives as follows: 3.3.2 Hard Negative Sampling. Hard negative samples, distinct in label from the positive sample but closely embedded, are poised to offer the greatest utility through the provision of substantial gradient information in the context of contrastive learning. In contrast, easy negatives yield negligible gradients and contribute minimally to the learning process. Drawing inspiration from prior research [24, 25, 44], we propose hard negative sampling strategy to optimize both training efficiency and discriminative performance. The textual knowledge distillation stage equips UniME with preliminary discriminative ability to autonomously identify hard negatives for each query. Building on this capability, we sample 𝑘 corresponding hard negative 𝑒 𝑐 within each training batch as follows: 𝑐 = Rank𝑘 (𝑐𝑜𝑠 (𝑒𝑞, 𝑒𝑐 )), where 𝑒𝑐 (cid:8)𝑒+ 𝑒 𝑐 , 𝑒 𝑐 (cid:9) , (3) 𝑐 and 𝑒+ where 𝑒 𝑐 denote filtered false negative candidates and positive candidates respectively, 𝑒𝑞 is the query embedding, and 𝑒𝑐 represents all candidate embeddings. The function 𝑐𝑜𝑠 () computes pairwise similarity scores, and Rank𝑘 selects the top-𝑘 highestscoring candidates as hard negatives. To maintain batch consistency when fewer than 𝑘 hard negatives are obtained (which occurs = 𝑙𝑜𝑔 𝑒𝑥𝑝 (𝑐𝑜𝑠 (𝑒𝑞, 𝑒+ 𝑒𝑥𝑝 (𝑐𝑜𝑠 (𝑒𝑞, 𝑒+ 𝑐 )/𝜏) + (cid:205)𝑘 𝑐 )/𝜏) 𝑖=1 𝑒𝑥𝑝 (𝑐𝑜𝑠 (𝑒𝑞, 𝑒𝑐 𝑖 )/𝜏) , (4) where 𝑘 denotes the set of all hard negatives, and 𝜏 is temperature hyper-parameter."
        },
        {
            "title": "3.4 Training Recipe\nStage1: Textual Discriminative Knowledge Distillation. We\nemploy QLoRA (Quantized Low-Rank Adaptation) [11] for\nparameter-efficient fine-tuning of the large language model compo-\nnent. This stage exclusively utilizes text-only inputs and introduces\nminimal trainable parameters, typically less than 5% of the total. The\ncomplete training procedures for Phi3.5-V and LLaVA-1.6 require\napproximately 1 hour and 2 hours, respectively.\nStage2: Hard Negative Enhanced Instruction Tuning. To over-\ncome GPU memory limitations in large-batch MLLM training, we\nemploy a dual strategy: (1) Following VLM2Vec [22], we implement\nGradCache [14], a gradient caching technique that decouples back-\npropagation between contrastive loss computation and encoder\nupdates; (2) we employ QLoRA [11] for parameter-efficient fine-\ntuning of all parameters within the MLLM. This combined approach",
            "content": "UniME, Preprint, 2025 Tiancheng Gu, Kaicheng Yang et al. Table 2: Results of zero-shot text-image retrieval on short caption datasets (Flickr30K and MS-COCO), long caption datasets (ShareGPT4V and Urban1K) and compositional benchmark (SugarCrepe). The reported scores are the average Recall@1 over the corresponding datasets. The best results are marked in bold. : UniME with textual discrimination distillation only. : UniME with both textual discrimination distillation and hard negative enhanced instruction tuning. Models #Parameters Flickr30K COCO ShareGPT4V Urban1K SugarCrepe Short Caption Retrieval Long Caption Retrieval Compositional Retrieval OpenCLIP(ViT-L) [42] CLIP(ViT-BigG/14) [8] EVA-CLIP [48] E5-V(Phi3.5-V) [21] E5-V(LLaVA-1.6) [21] UniME(Phi3.5-V) UniME(LLaVA-1.6) VLM2Vec(Phi3.5-V) [22] VLM2Vec(LLaVA-1.6) [22] UniME (Phi3.5-V) UniME (LLaVA-1.6) 0.4B 2.5B 8B 4.2B 7B 4.2B 7B 4.2B 7B 4.2B 7B 𝑞𝑖 𝑐𝑡 𝑞𝑡 𝑐𝑖 𝑞𝑖 𝑐𝑡 𝑞𝑡 𝑐𝑖 𝑞𝑖 𝑐𝑡 𝑞𝑡 𝑐𝑖 𝑞𝑖 𝑐𝑡 𝑞𝑡 𝑐𝑖 Replace Swap Add 67.3 79.5 80.3 72.2 77.3 72.0(-0.2) 77.2(-0.1) 68.7 76.0 77.0(+11.3) 81.9(+5.9) 87.2 92.9 94.5 79.6 85.7 80.6(+1.0) 84.6(-1.1) 83.0 90.6 88.2(+5.2) 93.4(+2.8) 37.0 51.3 52.0 44.7 49.1 44.9(+0.2) 51.0(+1.9) 43.7 46.8 49.8(+6.1) 53.7(+6.1) 58.1 67.3 70.1 53.4 57.6 57.2(+0.8) 56.4(-1.2) 59.8 66.6 66.8(+7.0) 70.1(+3.5) 81.8 90.1 93.1 86.0 85.1 86.8(+3.8) 89.8(+4.7) 90.1 85.8 92.1(+2.0) 93.9(+8.1) 84.0 93.6 91.2 88.5 82.1 92.3(+1.3) 86.9(+4.8) 92.0 90.7 96.4(+4.4) 97.2(+6.5) 47.0 77.8 80.4 83.8 88.9 85.1(+2.3) 91.3(+2.4) 87.9 84.7 92.7(+4.8) 95.2(+10.5) 47.0 80.7 77.8 83.6 83.2 86.9(+3.3) 82.4(-0.8) 86.8 90.8 95.1(+8.3) 95.9(+5.1) 79.5 86.5 85.9 88.2 86.3 90.2(+2.0) 89.5(+3.2) 86.2 85.8 90.1(+3.9) 89.0(+3.2) 62.7 68.9 70.3 66.6 68.7 67.6(+1.0) 64.8(-3.9) 66.7 66.3 70.9(+4.2) 71.5(+5.2) 74.9 88.4 86.7 75.3 66.9 91.2(+15.9) 94.2(+27.3) 84.2 86.5 93.3(+9.1) 94.4(+7.9) facilitates effective training while ensuring manageable memory consumption."
        },
        {
            "title": "4.2 Datasets and Metrics\n4.2.1 Training. Following E5-V [21], we utilize the Natural Lan-\nguage Inference (NLI) dataset [15] which contains around 273k\nsentence pairs for the first textual discriminative knowledge dis-\ntillation stage. For the hard negative enhanced instruction tuning\nstage, similar to the VLM2Vec [22], we employ 20 in-distribution\ndatasets from the MMEB benchmark, which cover four core multi-\nmodal tasks: classification, visual question answering, multimodal\nretrieval, and visual grounding. This comprehensive training cor-\npus, incorporating both unimodal and multimodal input data, totals\n662k carefully curated training pairs, ensuring robust model adap-\ntation across diverse multimodal tasks.",
            "content": "4.2.2 Evaluation. In this study, we evaluate UniME across both in-distribution (20 test sets) and out-of-distribution (16 test sets) benchmarks from MMEB [22] to assess its multimodal embedding capabilities across diverse retrieval tasks. Following standard evaluation protocols [22, 34], we report Precision, which quantifies the proportion of correct matches among the top-ranked candidates for each dataset. To further examine the unimodal embedding performance of UniME, we conduct experiments on multiple crossmodal retrieval tasks, including short-caption image-text retrieval (Flickr30K [41] and COCO2014 [30]), long-caption image-text retrieval (ShareGPT4V [7] and Urban1K [62]), and compositional retrieval (SugarCrepe [18]). Consistent with the MMEB benchmark, Precision serves as the primary evaluation metric across all datasets."
        },
        {
            "title": "4.3 Main Results.\n4.3.1 Multi-Modal Retrieval. In Table 1, we present the perfor-\nmance of our proposed UniME against existing baseline models.\nUnder identical training data and configuration settings, our pro-\nposed UniME consistently achieves significant performance im-\nprovements over E5-V across different foundation models. Specif-\nically, UniME exhibits an average performance improvement of\n4.2% over E5-V using the Phi3.5-V model. With LLaVA-1.6 as the\nbase model, UniME further achieves an average enhancement of\n4.1%. These significant performance improvements are primarily\ndue to our proposed textual discrimination knowledge distillation\nmethod, which more efficiently enhances the discriminative capa-\nbility of embeddings from a powerful teacher model. As depicted\nin Figure 4, we randomly select 50 samples from COCO and visual-\nize the cross-modal cosine similarity matrix. The diagonal clarity\nof the UniME matrix is significantly enhanced compared to that\nof E5-V, indicating that UniME learns representations with supe-\nrior distinctiveness. After the hard negative enhanced instruction\ntuning stage, beneficial from the hard negatives, the discrimina-\ntive capability of the embeddings from UniME is further improved.\nCompared with VLM2Vec [22], our model achieves 1.3% and 10.3%\nperformance improvement when using Phi3.5-V and LLaVA-1.6 as\nthe base model.",
            "content": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs UniME, Preprint, 2025 Figure 4: The discrimination comparison between E5-V and UniME. represents the UniME model only training on the first textual discrimination knowledge distillation stage. Figure 5: The comparison of training loss and pre-clip gradient norms for hard negatives, easy negatives, and random sample negatives. Short&Long Caption Cross-Modal Retrieval. We evaluate our 4.3.2 approach on zero-shot cross-modal retrieval tasks. Firstly, we conduct experiments on the short-caption datasets Flickr30K and MSCOCO. After the textual discrimination knowledge distillation stage, UniME achieves comparable retrieval performance to E5V. Subsequent hard negative enhanced instruction tuning further boosts UniMEs performance, yielding significant improvement of 5.2%11.3% over VLM2Vec. For long-caption retrieval tasks on the ShareGPT4V and Urban1K datasets, UniME demonstrates superior performance across all metrics. Specifically, following the textual discriminative knowledge distillation stage, UniME exhibits performance improvement of 1.3%-3.8% based on the Phi3.5-V model. Subsequent enhancement through hard negative enhanced instruction tuning leads to additional gains, with UniME outperforming VLM2Vec by 2.0%-8.3%. It is noteworthy that, compared to EVA-CLIP(8B), UniME achieves performance improvements of 14.8% and 18.1% in long-caption retrieval on the Urban1K dataset. This significant enhancement primarily stems from the limitation of EVA-CLIP(8B), which is constrained by 77-token length restriction, thereby inhibiting its ability to fully convey the complete semantic information of long captions. 4.3.3 Compositional Cross-Modal Retrieval. We evaluate the capacity of our UniME model to discriminate hard negative samples using the compositional benchmark SugarCrepe. As shown in Table 2, UniME consistently delivers superior performance across all evaluated metrics. Following the textual discriminative knowledge distillation phase, the Phi3.5-V-based UniME outperforms E5-V by 2.0% in relation replacement, 1.0% in object swapping, and 15.9% in attribute addition tasks. After the second hard negative enhanced instruction tuning stage, beneficial from the hard negative, the compositional understanding capabilities of UniME are further enhanced and it achieves 3.9%, 4.2%, and 9.1% performance improvement compared with VLM2Vec. Additionally, UniME exhibits improvements of 4.2%, 0.6%, and 6.6% compared to EVA-CLIP(8B), underscoring its robust capability to discriminate against hard negative samples."
        },
        {
            "title": "5 Analysis\n5.1 Analysis of the Hard Negative\nIn Figure 5, we visualize the training loss and pre-clip gradient\nnorms for three negative types: easy negatives (least similar in\nbatch), random negatives (randomly sampled in batch), and hard",
            "content": "Table 3: Ablation study of different training stages. We report the mean scores on the MMEB benchmark, short and long cross-modal retrieval, as well as compositional cross-modal retrieval. Stage1 Stage2 MMEB 25.3 40.3 63.8 64.2 RShort 44.2 63.7 61.5 70.4 RLong RCompos 62.9 87.8 84.2 94.1 63.1 83.0 77.1 84. negatives (most similar in batch after removing positives and false negatives). Since easy negatives are easily distinguishable, the model struggles to enhance its discriminative power through learning from such data, consequently leading to rapid convergence of the training loss to nearly zero. Using random negatives, the training loss converges more slowly compared to easy negatives, but it eventually nears zero. In contrast, hard negatives pose considerable challenges, sustaining elevated training losses. Correspondingly, gradient norms for easy negatives are minimal, whereas those for hard negatives are substantially higher, differing by orders of magnitude."
        },
        {
            "title": "5.2 Ablation on Training Stages\nWe conduct an ablation study based on Phi3.5-V to evaluate differ-\nent training stages. As shown in Table 3, the initial embeddings\nfrom Phi3.5-V exhibit weak discriminative properties, leading to\nsuboptimal task performance. After the initial stage of textual dis-\ncriminative knowledge distillation, the model registers performance\nimprovements of 15%, 19.5%, 24.9%, and 19.9% on the MMEB bench-\nmark, short and long caption cross-modal retrieval, and composi-\ntional retrieval tasks, respectively. Focusing solely on the second\nstage, which involves hard negative enhanced instruction tuning,\nresults in performance gains of 38.5%, 17.3%, 21.3%, and 14.0% in the\nsame tasks. Notably, the enhancement in MMEB benchmark per-\nformance after the second stage markedly exceeds that of the first,\nprimarily due to improved model capabilities in following complex\ninstructions. By integrating both training stages, our UniME model\nachieves optimal performance across all evaluated downstream\ntasks.",
            "content": "UniME, Preprint, 2025 Tiancheng Gu, Kaicheng Yang et al. Figure 6: Visualization of the top-k next predicted tokens before and after different training stages based on Phi3.5-V. : UniME with textual discrimination distillation only. : UniME with both textual discrimination distillation and hard negative enhanced instruction tuning. Table 4: Ablation study of the false negative filtering threshold 𝛽. FalseNeg(%): proportion of samples which filtered false negatives. Table 5: Ablation study of the number 𝑘 of the hard negatives. HardNeg(%): proportion of hard negative samples within batch. Model 𝛽 FalseNeg(%) Phi3.5-V -0.1 0.0 0.1 0.2 0.3 81.7% 53.2% 22.9% 18.2% 13.1% Average Score OOD Overall 43.4 49.0 52.7 51.9 52.1 55.3 61.1 64.2 63.7 63.9 IND 61.0 66.1 68.2 68.2 68."
        },
        {
            "title": "5.5 Visualization of the Output Distribution\nTo further explore the semantic representations captured by UniME\nembeddings, we utilize the prompt \"<Image> Summary above im-\nage in one word: \\n\" and visualize the prediction probability of the\ntop-k next predicted tokens before and after our proposed different\ntraining stages in Figure 6. We observe that before training, the\npredicted tokens are more abstract, such as “Pastoral” and “Peace-\nful”. After the textual discriminative knowledge distillation, the",
            "content": "Model Top-𝑘 HardNeg(%) Phi3.5-V 4 8 16 32 64 0.4% 0.8% 1.6% 3.2% 6.4% Average Score OOD Overall 52.4 52.7 51.6 51.8 51.1 63.8 64.2 63.5 63.6 63.2 IND 67.8 68.2 68.1 68.4 68. tokens shift towards more concrete semantics, including cow, waterfront, and house, though the probability distribution remains largely concentrated on \"Farm\". After the second stage of hard negative enhanced instruction tuning, the probability distribution becomes more evenly spread across multiple tokens that align with the images semantics, thereby allowing the embeddings to more accurately express the semantic content of the image with enhanced discriminative capability."
        },
        {
            "title": "6 Conclusion\nIn this paper, we introduce UniME (Universal Multimodal\nEmbedding), a novel two-stage framework that enables large mul-\ntimodal language models with the capacity to learn discriminative\nrepresentations applicable to a variety of downstream tasks. In the\nfirst textual discriminative knowledge distillation stage, we leverage\na powerful LLM-based teacher model to enhance the embedding\ncapability of the MLLM’s language component. In the second hard\nnegative enhanced instruction tuning stage, we initially mitigate\nfalse negative contamination and then sample multiple hard neg-\natives per instance within each batch, forcing the model to focus\non challenging samples. This approach not only improves discrim-\ninative power but also enhances instruction-following ability in\ndownstream tasks. We conduct extensive experiments on the MMEB\nbenchmark and multiple retrieval tasks, including short&long cap-\ntion retrieval and compositional retrieval. Results demonstrate that\nUniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capa-\nbilities. We hope that our work provides insights into multimodal\nrepresentation learning.",
            "content": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs UniME, Preprint, 2025 References [1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv:2404.14219 (2024). [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv:2309.16609 (2023). [3] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. 2023. Zero-shot composed image retrieval with textual inversion. In ICCV. 15338 15347. [4] Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. Llm2vec: Large language models are secretly powerful text encoders. COLM (2024). [5] Yi Bin, Wenhao Shi, Yujuan Ding, Zhiqiang Hu, Zheng Wang, Yang Yang, SeeKiong Ng, and Heng Tao Shen. 2024. Gallerygpt: Analyzing paintings with large multimodal models. In ACMMM. 77347743. [6] Anjia Cao, Xing Wei, and Zhiheng Ma. 2024. FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training. arXiv:2411.11927 (2024). [7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2024. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV. [8] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. 2022. Reproducible scaling laws for contrastive language-image learning. arXiv:2212.07143 (2022). [9] Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio De Rezende, Yannis Kalantidis, and Diane Larlus. 2021. Probabilistic embeddings for cross-modal retrieval. In CVPR. [10] Xin Cong, Bowen Yu, Mengcheng Fang, Tingwen Liu, Haiyang Yu, Zhongkai Hu, Fei Huang, Yongbin Li, and Bin Wang. 2023. Universal Information Extraction with Meta-Pretrained Self-Retrieval. In ACL. [11] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. NeurIPS (2023). [12] Aniket Didolkar, Andrii Zadaianchuk, Rabiul Awal, Maximilian Seitzer, Efstratios Gavves, and Aishwarya Agrawal. 2025. CTRL-O: Language-Controllable ObjectCentric Visual Representation Learning. In CVPR. [13] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024. Colpali: Efficient document retrieval with vision language models. In ICLR. [14] Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan. 2021. Scaling deep contrastive learning batch size under memory limited setup. arXiv:2101.06983 (2021). [15] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence embeddings. arXiv:2104.08821 (2021). [16] François Gardères, Maryam Ziaeefard, Baptiste Abeloos, and Freddy Lecue. 2020. Conceptbert: Concept-aware representation for visual question answering. In EMNLP. [17] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv:2407.21783 (2024). [18] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. 2023. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. NeurIPS (2023). [19] Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Xiyang Dai, Dongdong Chen, Chong Luo, et al. 2024. Llm2clip: Powerful language model unlock richer visual representation. arXiv:2411.04997 (2024). [20] Chaoya Jiang, Hongrui Jia, Mengfan Dong, Wei Ye, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang. 2024. Hal-eval: universal and fine-grained hallucination evaluation framework for large vision language models. In ACMMM. 525534. [21] Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. 2024. E5-v: Universal embeddings with multimodal large language models. arXiv:2407.12580 (2024). [22] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. 2025. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. ICLR (2025). [23] Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. In EMNLP. 79697992. [24] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. 2020. Hard negative mixing for contrastive learning. NeurIPS (2020). [25] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering.. In EMNLP. [26] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. ICLR (2024). [27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. 2024. Llava-onevision: Easy visual task transfer. arXiv:2408.03326 (2024). [28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML. [29] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. 2024. Vila: On pre-training for visual language models. In CVPR. [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In ECCV. [31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In CVPR. [32] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. https://llava-vl.github.io/blog/2024-01-30-llava-next/ [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. NeurIPS (2023). [34] Yikun Liu, Pingan Chen, Jiayin Cai, Xiaolong Jiang, Yao Hu, Jiangchao Yao, Yanfeng Wang, and Weidi Xie. 2024. LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant. CVPR (2024). [35] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Finetuning llama for multi-stage text retrieval. In SIGIR. [36] Hieu Man, Nghia Ngo, Franck Dernoncourt, and Thien Nguyen. 2024. Ullme: unified framework for large language model embeddings with generationaugmented learning. In EMNLP. [37] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. MTEB: Massive Text Embedding Benchmark. arXiv:2210.07316 (2022). [38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv:1807.03748 (2018). [39] Yassine Ouali, Adrian Bulat, Alexandros Xenos, Anestis Zaganidis, Ioannis Maniadis Metaxas, Brais Martinez, and Georgios Tzimiropoulos. 2024. Discriminative Fine-tuning of LVLMs. arXiv:2412.04378 (2024). [40] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023. Kosmos-2: Grounding multimodal large language models to the world. arXiv:2306.14824 (2023). [41] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV. [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In ICML. [43] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In SIGKDD. [44] Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. 2020. Contrastive Learning with Hard Negative Samples. In ICLR. [45] Jungkyoo Shin, Bumsoo Kim, and Eunwoo Kim. 2025. Generative Modeling of Class Probability for Multi-Modal Representation Learning. In CVPR. [46] Jonathon Shlens. 2014. Notes on kullback-leibler divergence and likelihood. arXiv:1404.2000 (2014). [47] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. 2023. Eva-clip: Improved training techniques for clip at scale. arXiv:2303.15389 (2023). [48] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. 2023. EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters. arXiv:2402.04252 (2023). [49] Yuanmin Tang, Jing Yu, Keke Gai, Jiamin Zhuang, Gang Xiong, Gaopeng Gou, and Qi Wu. 2025. Missing Target-Relevant Information Prediction with World Model for Accurate Zero-Shot Composed Image Retrieval. In CVPR. [50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv:2302.13971 (2023). [51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288 (2023). [52] Michael Tschannen, Manoj Kumar, Andreas Steiner, Xiaohua Zhai, Neil Houlsby, and Lucas Beyer. 2023. Image captioners are scalable vision learners too. NeurIPS (2023). [53] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. arXiv:2409.12191 (2024). UniME, Preprint, 2025 Tiancheng Gu, Kaicheng Yang et al. [54] Puyi Wang, Wei Sun, Zicheng Zhang, Jun Jia, Yanwei Jiang, Zhichao Zhang, Xiongkuo Min, and Guangtao Zhai. 2024. Large multi-modality model assisted ai-generated image quality assessment. In ACMMM. 78037812. [55] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023. CogVLM: Visual Expert for Pretrained Language Models. arXiv:2311.03079 [cs.CV] [56] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. 2024. Uniir: Training and benchmarking universal multimodal information retrievers. In ECCV. Springer, 387404. [57] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. 2024. Show-o: One single transformer to unify multimodal understanding and generation. arXiv:2408.12528 (2024). [58] Yin Xie, Kaicheng Yang, Ninghua Yang, Weimo Deng, Xiangzi Dai, Tiancheng Gu, Yumeng Wang, Xiang An, Yongle Zhao, Ziyong Feng, et al. 2024. Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension. arXiv:2410.14332 (2024). [59] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv:2412.15115 (2024). [60] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. 2022. When and why vision-language models behave like bags-of-words, and what to do about it? arXiv:2210.01936 (2022). [61] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. In ICCV. [62] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. 2024. Long-clip: Unlocking the long-text capability of clip. In ECCV. [63] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Leizhang, Chunyuan Li, et al. 2024. Llava-grounding: Grounded visual chat with large multimodal models. In ECCV. [64] Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. 2024. Magiclens: Self-supervised image retrieval with open-ended instructions. arXiv:2403.19651 (2024). [65] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv:2304.10592 (2023). Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs UniME, Preprint, This supplementary material provides additional details on our experimental settings, including training configurations and evaluation benchmarks as described in Sec.A. Additionally, it presents expanded results in Sec. B, including an ablation study on the LoRA rank and detailed findings on the MMEB benchmark. Finally, Sec. provides supplementary visualizations that depict the output distribution and examples of negative data. Table 6: Training hyperparameters and computational requirements for UniME(Phi3.5-V) and UniME(LLaVA-1.6). Hyperparameter UniME(Phi3.5-V) UniME(LLaVA-1.6) Stage 1:Textual Discriminative Knowledge Distillation Training samples Batch size Learning rate LoRA rank Epochs GPU configuration Precision Training time 273K 768 5104 32 2 8A100 FP 1 hour 2 hours Stage 2: Hard Negative Enhanced Instruction Tuning Training samples Batch size Learning rate LoRA rank Training steps Optimizer DeepSpeed stage GPU configuration Precision Training time 2105 110 662K 1024 16 1000 AdamW 2 8A100 BF16 26 hours 37 hours Detailed Experiment Settings A.1 Training Details We provide the training configurations of UniME in Table 6. Stage1: Textual Discriminative Knowledge Distillation. We use 8A100 GPUs (80GB each) to train UniME with text-only data. The text embeddings of the teacher model are extracted offline, and the model is trained by utilizing QLoRA [11], training durations are significantly reduced. Specifically, training completes in one hour for the Phi3.5-V [1] backbone and two hours for the LLaVA-1.6 [32] backbone. Stage2: Hard Negative Enhanced Instruction Tuning. Upon integrating the backbone with stage 1 LoRA outputs, we further advance UniME on image-text pairs derived from the MMEB training set. Under identical hardware conditions, this phase requires 26 hours for Phi-3.5V and 37 hours for LLaVA-1.6. A.2 MMEB Benchmarks A.3 Retrieval Task Evaluation Benchmarks We evaluate UniME on diverse retrieval benchmarks, including short-caption, long-caption, and compositional image-text tasks (as Table 7: Summary of the evaluation benchmarks. # Queries represents the number of test queries, and # Candidates denotes the number of test candidates per query."
        },
        {
            "title": "Benchmark",
            "content": "Zero-shot #Queries #Candidates Flickr30K [41] COCO [30] ShareGPT4V [7] Urban1K [62] SugarCrepe [18] 1K 5K 1K 1K 7.5K 5K 25K 1K 1K 2 Table 8: Performance comparison of different LoRA ranks under two training stages (IND: In-Domain, OOD: Out-OfDomain)"
        },
        {
            "title": "LoRA",
            "content": "Average Score IND OOD Overall Stage1 Stage2 4 16 32 64 4 8 32 64 36.0 35.8 35.9 36.0 35. 67.4 68.2 68.2 67.6 67.0 37.9 37. 38.0 38.3 37.8 51.0 51.0 52.7 51.8 51. 40.0 39.9 40.0 40.3 39.9 62.6 63. 64.2 63.2 62.7 shown in Table 7). For each benchmark, we adopt its standard evaluation protocol. In retrieval tasks, we primarily report Recall@1 as the evaluation metric. External Results B.1 Ablation on the LoRA Rank Due to computational limitations and the proven efficacy of LoRA [11], we utilize QLoRA to fine-tune the backbone of UniME. As detailed in Table 8, we assess the impact of various LoRA ranks on the MMEB benchmarks [22] utilizing the Phi-3.5V backbone. Our findings indicate that LoRA rank of 32 delivers optimal performance in Stage 1, whereas rank of 16 is most effective in Stage 2. B.2 Specific results on the MMEB We present comparative results of eight models on the MMEB benchmark in Table 9. The performance metrics for CLIP, SigLIP, BLIP-2, and MagicLens are sourced directly from VLM2Vec [22]. Conversely, results for EVA-CLIP(8B), E5-V, VLM2Vec, and UniME are obtained through reproduction in our experiments. For E5-V, VLM2Vec, and UniME, we only report the metrics for their best-performing variants, all of which employ the LLaVA-1.6 [32] backbone. UniME, Preprint, Tiancheng Gu, Kaicheng Yang et al. Figure 7: Visualization of the top-k next predicted tokens before and after different training stages based on Phi3.5-V. : UniME with textual discrimination distillation only. : UniME with both textual discrimination distillation and hard negative enhanced instruction tuning. Further Analysis C.1 Visualization of the Output Distribution In Figure 7, we further present additional examples that compare the top-k prediction probabilities of subsequent tokens, illustrating the evolution of model behavior across various training stages of our UniME. C.2 Schematic illustration of Negative Data Figure 8 provides additional examples of negative samples encountered during training. Our observations indicate that false negatives are more prevalent in textual data than in visual data, attributable primarily to the presence of synonyms. Figure 8: Schematic illustration of Negative data. Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs UniME, Preprint, 2025 Table 9: The detailed results of the baselines and our UniMEon MMEB, which includes 20 in-distribution datasets and 16 out-of-distribution datasets. The out-of-distribution datasets are highlighted with yellow background in the table. We only introduce the best version of UniME, E5-V and VLM2Vec in the table, which uses LLaVA-1.6 as backone. CLIP SigLIP BLIP2 MagicLens EVA-CLIP E5-V VLM2Vec UniME Classification (10 tasks) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country-211 All Classification VQA (10 tasks) OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA All VQA Retrieval (12 tasks) VisDial CIRR VisualNews_t2i VisualNews_i2t MSCOCO_t2i MSCOCO_i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS All Retrieval Visual Grounding (4 tasks) MSCOCO RefCOCO RefCOCO-matching Visual7W-pointing All Visual Grounding Final Score (36 tasks) All All IND All OOD 55.8 34.7 51.1 50.7 43.4 28.5 25.5 75.6 43.4 19.2 42.8 7.5 3.8 4.0 4.6 1.4 4.0 9.4 8.2 41.3 7.0 9.1 30.7 12.6 78.9 79.6 59.5 57.7 60.4 67.5 11.4 55.0 41.1 81.0 53.0 33.8 56.9 61.3 55.1 51. 39.2 37.1 38.7 45.4 13.9 47.2 64.3 39.6 20.0 42.6 75.0 40.3 14.2 40.3 2.4 1.5 4.2 2.7 3.0 1.2 7.9 2.3 57.5 1.0 8.4 21.5 15.1 51.0 52.4 58.3 55.0 62.9 58.1 20.1 55.1 56.0 23.6 31.6 46.4 70.8 50.8 70.1 59.5 35.0 32.3 38. 10.3 36.0 49.6 52.1 34.5 21.5 3.2 39.7 20.6 2.5 27.0 8.7 3.2 2.6 2.0 0.5 1.3 6.8 4.0 9.7 3.3 4.2 18.0 9.8 48.1 13.5 53.7 20.3 56.5 55.4 9.3 28.7 39.5 54.4 33.9 28.9 47.4 59.5 52.0 47.0 28.0 25.3 25.1 48.0 33.7 49.0 51.6 57.0 31.5 8.0 70.9 31.6 6.2 38. 12.7 2.9 3.0 5.9 0.9 2.5 5.2 1.7 43.5 4.6 8.3 24.8 39.1 50.7 21.1 54.1 40.0 58.1 43.0 11.2 18.7 1.6 62.6 35.4 22.1 22.8 35.6 23.4 26.0 27.1 31.0 23.7 75.0 33.8 49.3 44.3 62.7 38.7 54.8 95.4 67.8 38.7 56.0 9.9 2.8 7.4 6.0 1.5 2.2 14.1 4.3 44.7 10.8 10. 20.4 36.0 82.4 88.2 65.3 67.2 0.2 70.9 16.1 46.7 1.8 95.6 49.2 35.8 59.9 70.0 70.2 58.9 43.7 38.1 45.6 40.5 31.5 49.3 76.7 52.3 32.0 18.2 56.7 34.2 5.9 39.7 15.1 4.7 9.1 8.7 4.2 4.5 9.6 8.6 34.1 9.5 10.8 57.6 41.0 43.9 46.8 68.6 54.8 0.1 33.7 11.2 61.0 0.5 53.8 39. 41.7 62.2 74.9 61.8 60.2 37.5 34.2 33.4 66.5 76.4 60.9 84.0 73.2 42.1 39.9 74.6 34.3 16.1 56.8 66.5 54.9 64.4 34.8 33.1 49.8 37.3 39.9 57.3 65.7 50.4 75.3 51.3 70.7 75.2 69.9 67.7 63.3 83.6 15.2 63.4 49.6 73.7 63.3 77.0 85.9 83.8 83.6 82. 63.3 64.9 53.9 71.3 79.5 64.6 90.4 75.9 45.6 45.5 78.4 36.4 18.7 60.6 68.3 58.7 67.6 37.0 33.4 51.7 40.5 42.7 63.6 65.2 52.9 79.7 52.2 74.8 78.8 74.9 73.8 66.2 89.8 16.5 66.6 55.7 86.2 67.9 76.5 89.3 90.6 84.1 85.1 66.6 68.4 57."
        }
    ],
    "affiliations": [
        "DeepGlint Tongyi Lab, Alibaba Group",
        "Imperial College London",
        "The University of Sydney"
    ]
}