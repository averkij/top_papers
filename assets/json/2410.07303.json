{
    "paper_title": "Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow",
    "authors": [
        "Fu-Yun Wang",
        "Ling Yang",
        "Zhaoyang Huang",
        "Mengdi Wang",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its key components include: 1) using the diffusion form of flow-matching, 2) employing $\\boldsymbol v$-prediction, and 3) performing rectification (a.k.a. reflow). In this paper, we argue that the success of rectification primarily lies in using a pretrained diffusion model to obtain matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs. Based on this, components 1) and 2) are unnecessary. Furthermore, we highlight that straightness is not an essential training target for rectification; rather, it is a specific case of flow-matching models. The more critical training target is to achieve a first-order approximate ODE path, which is inherently curved for models like DDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models. We validate our method on Stable Diffusion v1-5 and Stable Diffusion XL. Our method not only greatly simplifies the training procedure of rectified flow-based previous works (e.g., InstaFlow) but also achieves superior performance with even lower training cost. Our code is available at https://github.com/G-U-N/Rectified-Diffusion."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 2 3 0 3 7 0 . 0 1 4 2 : r Preprint RECTIFIED DIFFUSION: STRAIGHTNESS IS NOT YOUR NEED IN RECTIFIED FLOW Fu-Yun Wang1 Ling Yang2 Zhaoyang Huang1 Mengdi Wang3 Hongsheng Li1 1MMLab, CUHK, Hong Kong SAR 2Peking University, Beijing, China 3Princeton University, New Jersey, USA fywang@link.cuhk.edu.hk drinkingcoder@link.cuhk.edu.hk mengdiw@princeton.edu hsli@ee.cuhk.edu.hk yangling0818@163.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, widely recognized solution, improves generation speed by straightening the ODE path. Its key components include: 1) using the diffusion form of flow-matching, 2) employing v-prediction, and 3) performing rectification (a.k.a. reflow). In this paper, we argue that the success of rectification primarily lies in using pretrained diffusion model to obtain matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs. Based on this, components 1) and 2) are unnecessary. Furthermore, we highlight that straightness is not an essential training target for rectification; rather, it is specific case of flow-matching models. The more critical training target is to achieve first-order approximate ODE path, which is inherently curved for models like DDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models. We validate our method on Stable Diffusion v1-5 and Stable Diffusion XL. Our method not only greatly simplifies the training procedure of rectified flow-based previous works (e.g., InstaFlow) but also achieves superior performance with even lower training cost. Our code is available at https://github.com/G-U-N/Rectified-Diffusion."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have greatly advanced the field of visual generation, enabling the creation of highquality images and vivid videos from text (Ho et al., 2020; Song et al., 2020b; Rombach et al., 2022a; Singer et al., 2022; Podell et al., 2023; Esser et al., 2024; Shi et al., 2024). However, the generation process of diffusion models involves solving an expensive generative ODE numerically, which significantly slows down the generation speed compared to other generative models (e.g., GAN) (Goodfellow et al., 2020; Sauer et al., 2023b;a). widely recognized solution to this issue is rectified flow. The training target of rectified flow, as highlighted in the previous works (Liu et al., 2023; 2022; Yan et al., 2024), is to make the new ODE path straighter, enabling the models to generate high-fidelity images with fewer steps while retaining the flexibility of sampling with more inference steps for further quality enhancement. The key components of rectified flow are threefold: 1) Flow-Matching. Rectified flow proposes to employ the flow-matching based diffusion form (Liu et al., 2022; Lipman et al., 2022). The intermediate noisy state xt is defined as (1 t)x0 + tϵ, 1 Preprint Figure 1: Overview of comparison between Rectified Flow and Our Rectified Diffusion. where x0 is the clean data, ϵ (0, I) is normal noise, and [0, 1] is the timestep. This design is more straightforward compared to the semi-linear form of the original DDPM (Ho et al., 2020). , which is invalid. 2) v-prediction. Rectified flow proposes to adopt v-prediction (Salimans & Ho, 2022; Liu et al., 2022). That is, the model learns to predict = x0 ϵ. This makes the denoising form simple. For example, one can predict x0 based on xt with ˆx0 = xt + tˆvθ, where θ denotes the model parameters and ˆ denotes the predictions. Moreover, it avoids the numerical issue when 1 1t xttˆϵθ with ϵ-prediction. For example, ˆx0 = xttˆϵθ 3) Rectification. Rectification, also known as reflow, is an important technique proposed in rectified flow (Liu et al., 2022). It is progressive retraining technique that greatly improves the generation quality at low-step regime and maintains the flexibility of standard diffusion models. To be specific, it turns an arbitrary coupling of x0 P0 (real data) and ϵ P1 (noise) adopted in standard diffusion training to new deterministic coupling of ˆx0 Pθ 0 (generated data) and ϵ P1 (pre-collected noise). To put it in nutshell, it replaces the xt = (1 t)x0 + tϵ with xt = (1 t)ˆx0 + tˆϵ, where x0 is real data, ˆx0 is data generated by pretrained diffusion models θ, ϵ is the randomly sampled noise, and ˆϵ is the noise used to generate data ˆx0. Previous works emphasize the rectification procedure is only feasible to v-prediction based flow-matching models. That is, they believe the first two points are the foundations to adopt rectification for improving efficiency. And they emphasize the rectification procedure straightens the ODE path. 0 The motivation of this paper is to investigate what is most essential about rectified flow. We argue that the effectiveness of rectified flow stems from using pretrained diffusion model to acquire matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs (i.e., the aforementioned third point). Based on this, the aforementioned first two points (i.e., flow-matching & v-prediction) are unnecessary. This allow us generalize the design space of rectified flow and make it adoptable for different diffusion variants including DDPM (Ho et al., 2020), EDM (Karras et al., 2022), Sub-VP (Song et al., 2020b), and etc. To this end, we propose Rectified Diffusion, as illustrated in Fig. 1, our overall design is straightforward. We keep everything of the pretrained diffusion models unchanged, including noise schedulers, prediction types, networks architectures, and even training and inference code. The only difference is that the noise ϵ and data x0 adopted for training are pre-collected and generated by the pretrained diffusion models instead of independently sampled from Gaussian and real data datasets. Additionally, we highlight that straightness is no more an essential training target when we generalize the design space from solely flow-matching to more general diffusion forms. We analyze and show that the training target of Rectified Diffusion is to obtain first-order approximate ODE path. In simple terms, first-order ODE implies the predictions of models remain consistent along the 2 Preprint Figure 2: Training iterations. 1-step performance of Rectified Diffusion significantly surpasses the 1-step performance of Rectified Flow within only 20,000 iterations with batch size 128 (8% trained images of Rectified Flow) and consistently grows with more training iterations. ODE trajectory and it still maintains at the same ODE trajectory after each denoising step. For models like DDPM (Ho et al., 2020), the first-order ODE path is inherently curved instead of straight. Therefore, straightness is no more suitable for Rectified Diffusion and is just special case when we use the form of flow-matching. To empirically validate our claim, we conduct experiments using Stable Diffusion, comparing our approach with InstaFlow (Liu et al., 2023), key baseline based on rectified flow for text-to-image generation. We adhere the training setting of InstaFlow. The primary distinction is that InstaFlow requires transforming the Stable Diffusion models into v-prediction flow-matching model, while our method leaves everything of original Stable Diffusion unchanged. Our results demonstrate apparently better performance and faster training, likely due to our minimal differences in diffusion configurations. Our one-step performance achieves significantly superior performance with only 8% trained images of InstaFlow as shown in Fig. 2. Besides, we propose to replace the second-stage distillation adopted in InstaFlow with consistency distillation. We observe that the first-order approximate ODE path greatly facilitates consistency distillation, allowing us to achieve better performance at 3% the GPU days than the further distilled model of InstaFlow. Additionally, we introduce Rectified Diffusion (Phased), which divides the ODE path along the time axis into multiple segments and enforces first-order linearity within each segment. While this segmentation increases the minimum number of generation steps to match the number of segments, it substantially reduces both training cost and time. When compared to the previous segment-based rectified flow method, PeRFlow (Yan et al., 2024), our approach demonstrates significantly better performance in experiments conducted on Stable Diffusion v1-5 (Rombach et al., 2022a) and Stable Diffusion XL (Podell et al., 2023). We summarize our main contributions as follows: (i) We conduct an in-depth analysis of the essence of rectification and extend rectified flow to rectified diffusion. (ii) We identify that it is not straightness but first-order property is the essential training target of rectified diffusion with theoretical derivations. (iii) Comprehensive comparisons on rectification, distillation and phased OED segmentation demonstrate our method achieves superior trade-off between generation quality and training efficiency over rectified flow-based models."
        },
        {
            "title": "2 RECTIFIED DIFFUSION: GENERALIZING THE DESIGN SPACE OF RECTIFIED",
            "content": "FLOW INTO GENERAL DIFFUSION MODELS Rectified flow is subset of rectified diffusion. In the following discussion, we apply the diffusion form xt = αtx0 + σtϵ to introduce rectified diffusion. Note that this form of diffusion covers the flow-matching since we can simply set αt = 1 and σt = t. Considering the different prediction types, we apply the epsilon-prediction for the following discussion. But note that different 3 Preprint prediction types can be converted effortlessly through re-parameterization. For x0-prediction, we . For v-prediction utilized in rectified flow, we have = x0 ϵ = xt(αt+σt)ϵ have x0 = xtσtϵ . Hence, we claim that rectified flow is subset of rectified diffusion, and rectified diffusion is generalization of rectified flow. αt αt 2.1 THE NATURE OF RECTIFICATION IS THE RETRAINING WITH PRE-COMPUTED NOISE-SAMPLE PAIR The secret of rectification is using paired noise-sample for training. To illustrate the differences clearly, we visualize the training processes for standard flow matching and rectification (reflow) training, as described in Algorithm 1 and Algorithm 3, respectively. Differences are highlighted in red. key observation is that in standard flow matching training, x0 represents real data randomly sampled from the training set, while the noise ϵ is also randomly sampled from Gaussian. This results in random pairing between noise and sample. In contrast, in rectification training, the noise is pre-sampled from Gaussian, and the images are generated using pre-sampled noise by the model from the previous round of rectification (the pre-trained model), leading to deterministic pairing. Flow-matching training is subset of standard diffusion training. In addition, Algorithm 2 visualizes the training process of more general diffusion model, with differences to Algorithm 1 highlighted in blue and orange. Its important to note that flow matching is specific case of the diffusion forms we discuss. From the algorithms, it is evident that the only distinctions between them lie in the diffusion form and prediction type. Consequently, flow matching training is just special case of general diffusion training under particular diffusion form and prediction type. By comparing Algorithms 2 and 3 with Algorithm 1, it is straightforward to derive Algorithm 4. Essentially, by incorporating the pre-trained model to collect noise-sample pairs and replacing the randomly sampled noise and real samples with these pre-collected pairs in the general diffusion training, we obtain the training algorithm for rectified diffusion. Algorithm 1 Flow Matching v-Prediction Algorithm 2 Diffusion Training ϵ-Prediction Input: Sample x0 from the data distribution Sample time from predefined schedule or uniformly from [0, 1] Sample noise ϵ from normal distribution Compute xt : xt = (1 t) x0 + ϵ Predict velocity ˆv using the model: ˆv = Model(xt, t) Compute loss: = ˆv (x0 ϵ)2 2 Backpropagate and update parameters Input: αt, σt Sample x0 from the data distribution Sample time from predefined schedule or uniformly from [0, 1] Sample noise ϵ from normal distribution Compute xt : xt = αt x0 + σt ϵ Predict noise ˆϵ using the model: Model(xt, t) Compute loss: = ˆϵ ϵ2 2 Backpropagate and update parameters ˆϵ = Algorithm 3 Rectified Flow v-Prediction Algorithm 4 Rectified Diffusion ϵ-Prediction Input: noise-data pair (ϵ, ˆx0) Sample x0 from the data distribution Sample time from predefined schedule or uniformly from [0, 1] Sample noise ϵ from normal distribution Compute xt : xt = (1 t)ˆx0+t ϵ Predict velocity ˆv using the model: ˆv = Model(xt, t) Compute loss: = ˆv(ˆx0ϵ)2 2 Backpropagate and update parameters Input: noise-data pair (ϵ, ˆx0), αt, σt Sample x0 from the data distribution Sample time from predefined schedule or uniformly from [0, 1] Sample noise ϵ from normal distribution Compute xt : xt = αtˆx0+σt ϵ Predict noise ˆϵ using the model: Model(xt, t) Compute loss: = ˆϵ ϵ2 2 Backpropagate and update parameters ˆϵ = 4 Preprint 2.2 UNDERSTANDING THE FIRST-ORDER ODE ( ) For the above discussed general diffusion form xt = αtx0 + σtϵ, there exists an exact ODE solution form (Lu et al., 2022), xt = αt αs xs αt (cid:90) λt λs eλϵθ(xtλ, tλ)dλ , (1) where λt = ln αt xs is pre-defined deterσt ministic scaling. The right term is the exponentially weighted integral of epsilon predictions. The first order ODE means the above integral with arbitrary and is equivalent to , and tλ is the inverse function of λt. The left term αt αs xt = αt αs xs αtϵθ(xs, s) (cid:90) λt λs eλdλ = αt αs xs + αtϵθ(xs, s)( σt αt σs αs ) . (2) We show that the equivalent of Equation 1 and Equation 2 for arbitrary and holds and only holds if the epsilon prediction on the same ODE trajectory is constant in Thereom 1. First-order ODE has the same form of predefined diffusion form. To put it in nutshell, we assume the ODE trajectory is first-order ODE, and there exists solution point x0. Therefore, the epsilon predictions on the ODE trajectory with solution point x0 are constant, which we denote as ϵ. Substitute = 0, x0, αs = 1, σs = 0 and ϵ into Equation 2, we have xt = αtx0 + σtϵ. (3) This has exactly the same form of predefined forward process. Therefore, we have that the first-order ODE is exactly the weighted interpolation of data and noise following predefined forward diffusion form. The only difference is that, the ϵ and x0 in the above equation is deterministic pair on the same ODE trajectory. While, for standard diffusion training, the x0 and ϵ are randomly sampled. That indicates that if we achieve perfect coupling of data x0 and noise ϵ at training, and theres no intersections among different paths (otherwise the epsilon predictions can be the epsilon prediction expectation of different paths), the trained diffusion models in the ideal case (without optimization error) will obtain the first-order ODE. First-order ODE supports consistent generation with arbitrary inference steps. Additionally, note that if the epsilon predictions on the same trajectory are constant, it is easy to show that the x0-predictions are also constant. Therefore, the first-order ODE can flexibly support one-step generation (xT x0) or multi-step generation (xT x0). If perfect first-order ODE is achieved, we will always get indentical generation results with arbitrary inference steps. , we will have yt = αt σt First-order ODE can be inherently curved. For the first-order ODE, though the trajectories of flow-matching based methods are straight, the trajectories of other forms of diffusion models can be inherently curved. But if we define yt = xt x0 + ϵ from the Equation 3. σt We can easily obeserve that the trajectory of yt is straight line from the initial point ϵ towards the direction of x0 (i.e, first-order trajectories can be converted to straight lines). We showcase our findings in Fig. 3.We select x0 = [0, 1] and ϵ = [1, 1] The Fig. 3a and Fig. 3b show the firstorder trajectory of flow-matching and EDM. They are both straight, but EDM has totally different trajectory and magnitude. Fig. 3c and Fig. 3d show the first-order trajectory of DDPM and Sub-VP. Their first-order trajectory are inherently curved. Fig. 3e shows the trajectory of yt = αt x0 + ϵ. It σt shows that all the first-order trajectories can be converted into straight lines with simple timestepdependent scaling. 2.3 RECTIFIED DIFFUSION (PHASED) Completely linearizing the ODE path of pre-trained diffusion model is challenging because the In Fig. 4, we visualize both the original ODE can deviate significantly from first-order form. original diffusion ODE path and the corresponding first-order ODE path. Since its hard to intuitively determine whether curved ODE path satisfies first-order linearity, we represent the first-order ODE path with straight line. significant gap between the two paths is evident. However, enforcing local first-order linearity is more feasible. As shown on the right side of the figure, when the ODE 5 Preprint (a) Flow Matching (b) EDM (c) DDPM (d) Sub-VP (e) Transformed Figure 3: First-order trajectory of different diffusion forms. We show that the first-order ODE has the same form as their predefined forward process, i.e., xt = αtx0 + σtϵ. Though the first-order ODE paths of Flow Matching and EDM are straight, the first-order ODE paths of DDPM and SubVP are inherently curved. First-order ODE paths of all diffusion forms can be converted into straight lines through simple scaling as shown in Fig. 3e. path is divided into two segments along the time axis and each segment is linearized separately, the new ODE path is closer to the original one. This observation motivates the development of our rectification diffusion (phased). We set intermediate time steps as s0 = 0 < s1 < s2 < < sM 1 = tmax along the time axis of ODE, where is the number of phases. The training process begins with sampling x0 from real data, followed by adding random noise at time step sm to obtain xsm. We then use the pretrained diffusion model to perform multi-step numerical solving to obtain xsm1 for the previous intermediate step. However, the phasing idea involves two challenges: 1) determining the noise ϵ corresponding to the first-order path, and 2) determine the sample xt at any time between sm and sm1 on the same first-order ODE, where (sm1, sm). Fortunately, the transition formula between any two points on the first-order ODE is known, as shown in Equation 2. Through simple transformation, we have the noise ϵ corresponding to the ODE path between xsm and xsm1 can be expressed as: xsm1 αsm1 σsm1 αsm1 where represents the change in zt = xt . Once this noise αt ϵ is calculated, it can be directly substituted into Equation 2 to compute xt at any time along the ODE path. , and NSR denotes the change in σt αt xsm αsm σsm αsm NSR ϵ = (4) = , 2.4 RECTIFIED DIFFUSION FACILITATES THE CONSISTENCY DISTILLATION Previous work (Liu et al., 2023) proposes applying naive distillation after rectification to enhance one-step generation ability. This is because, after rectification, the model cannot achieve perfect first-order path due to issues like optimization, model capacity, and ODE path intersections. As result, rectified flow-based methods still do not perform as well as the most advanced distillation methods at low-step regime (e.g., 1-step generation). Following it, we also utilize distillation to further improve the models performance at low-step regime after rectified diffusion. Instead of using naive distillation, we employ the more advanced distillation techniqueconsistency distilla6 Preprint Figure 4: ODE trajectory comparison of diffusion models, rectified diffusion models, and phased consistency models. We apply straight lines for more clear demonstration. The solid line shows the original diffusion ODE path, while the dashed line shows the rectified ODE path. (a) Diffusion models (b) Consistency models (c) Rectified diffusion (Ours) Figure 5: ODE trajectory and prediction comparison of consistency models and reftified diffusion. We apply straight lines for more clear demonstration. The yellow line shows the ODE trajectories, while the blue line shows the predictions. tion (Song et al., 2023), which eliminates the need to regenerate large numbers of samples. Moreover, we found that after rectification, where the ODE path is approximately first-order, consistency distillation leads to significantly faster training and better performance. This is because the training objective of first-order ODE imposes stronger constraint than self-consistency. In Fig. 5, we illustrate the differences between the diffusion model, consistency model, and rectified diffusion. The consistency model only adjusts the direction of the models predictions without altering the ODE path itself, while rectified diffusion enforces change in the ODE path to first-order form. Figure 6: Effectiveness of Classifier-Free Guidance. 7 Preprint"
        },
        {
            "title": "3 EMPIRICAL VALIDATION",
            "content": "3.1 VALIDATION SETUP To thoroughly compare our approach with rectified flow-based methods, we organize the comparison into three levels: 1) Rectification Comparison: InstaFlow (Liu et al., 2023) proposes initializing v-predictionbased flow-matching model using Stable Diffusion v1-5 (Rombach et al., 2022a), followed by further training with their rectified flow method, which we refer to as Rectified Flow. To compare with this, we apply the rectified diffusion method to continue training Stable Diffusion v1-5, referred to as Rectified Diffusion. This comparison aims to demonstrate the faster training speed and superior performance of our proposed rectified diffusion approach. 2) Distillation Comparison: In the InstaFlow paper, the authors suggest using standard distillation technique to improve the models performance in one-step scenario, which we refer to as Rectified Flow (Distill). Similarly, we apply distillation strategy to enhance performance at low-step regimes, specifically using consistency distillation to boost training efficiency. This approach is termed Rectified Diffusion (CD). 3) Phased ODE Segmentation: PeRFlow (Yan et al., 2024) introduces the concept of segmenting the ODE and presents experimental results on both SD and SDXL (Podell et al., 2023), termed PeRFlow and PeRFlow-XL, respectively. We extend this idea by proposing method for phasing the ODE to enforce first-order property within each sub-phase, which we call Rectified Diffusion (Phased) and Rectified Diffusion-XL (Phased). Across all three of these comparative experiments, our methods demonstrate significantly superior performance. Table 1: Performance comparison on validation set of COCO-2017. Method Res. Time () # Steps # Param. FID () CLIP () SDv1-5+DPMSolver (Upper-Bound) (Lu et al., 2022) Rectified Flow (Liu et al., 2023) Rectified Flow (Liu et al., 2023) Rectified Flow (Liu et al., 2023) Rectified Diffusion (Ours) Rectified Diffusion (Ours) Rectified Diffusion (Ours) Rectified Flow (Distill) (Liu et al., 2023) Rectified Flow (Distill) (Liu et al., 2023) Rectified Flow (Distill) (Liu et al., 2023) Rectified Diffusion (CD) (Ours) Rectified Diffusion (CD) (Ours) Rectified Diffusion (CD) (Ours) PeRFlow (Yan et al., 2024) Rectified Diffusion (Phased) (Ours) PeRFlow-SDXL (Yan et al., 2024) Rectified Diffusion-SDXL (Phased) (Ours) 512 512 512 512 512 512 512 512 512 512 512 512 512 512 512 1024 1024 0.88s 0.88s 0.09s 0.13s 0.09s 0.09s 0.13s 0.09s 0.13s 0.21s 0.09s 0.13s 0.21s 0.21s 0.21s 0.71s 0.71s 25 25 1 2 25 1 1 2 4 1 2 4 4 4 4 4 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 3B 3B 20.1 21.65 47.91 31.35 21.28 27.26 22.98 23.72 73.49 103.48 22.83 21.66 21.43 22.97 20. 27.06 25.81 0.318 0.315 0.272 0.296 0.316 0.295 0.309 0.302 0.261 0.245 0.305 0.312 0.314 0.294 0.311 0.335 0. 3.2 COMPARISON Training cost. Following the setup from the InstaFlow paper, we first use Stable Diffusion v1-5 and DPM-Solver (Lu et al., 2022) to generate 1.6 million images. Since InstaFlow does not specify the prompts used, we generate images using randomly sampled set of 1.6 million prompts. During the training of Rectified Diffusion, we used batch size of 128 for total of 200,000 iterations, resulting in total of 128 200, 000 = 25, 600, 000 samples processed. In comparison, InstaFlow processed 6470, 000+102425, 000 = 30, 080, 000 samples. Thus, our total training cost is lower than that of InstaFlow. Additionally, InstaFlows total training time was 75.2 A100 GPU days, whereas our method required approximately 20 A800 GPU days. Typically, the training efficiency of an A800 is about 80% of that of an A100. We attribute this significant reduction in training time to not using the 8 Preprint Table 2: Performance comparison on COCO-2014. Method Res. Time () # Steps # Param. FID () CLIP () DALLE (Ramesh et al., 2021) CogView2 (Ding et al., 2021) Parti-750M (Yu et al., 2022) Parti-3B (Yu et al., 2022) Parti-20B (Yu et al., 2022) Make-A-Scene (Gafni et al., 2022) Muse (Chang et al., 2023) GLIDE (Nichol et al., 2021) DALLE 2 (Ramesh et al., 2022) LDM (Rombach et al., 2022a) Imagen (Saharia et al., 2022) eDiff-I (Balaji et al., 2022) LAFITE (Zhou et al., 2022) StyleGAN-T (Sauer et al., 2023a) GigaGAN (Kang et al., 2023) Autoregressive Models 256 256 256 256 256 256 - - - 6.4s - 25.0s Masked Models 256 1.3 Diffusion Models - - - - - - 12B 6B 750M 3B 20B - 27.5 24.0 10.71 8.10 7.23 11.84 - - - - - - 24 3B 7.88 0. 256 256 256 256 256 15.0s - 3.7s 9.1s 32.0s 250 250+27 250 - 25+10 Generative Adversarial Networks (GANs) 1 1 1 0.02s 0.10s 0.13s 256 512 5B 5.5B 1.45B 3B 9B 75M 1B 1B UFOGen (Xu et al., 2024b) DMD (CFG=3) (Yin et al., 2024a) DMD (CFG=8) (Yin et al., 2024a) SD-Turbo (Sauer et al., 2023c) Stable Diffusion (0.9 B) and its accelerated or distilled versions GANs 512 512 512 512 0.09s 0.09s 0.09s 0.09s 1 1 1 0.9B 0.9B 0.9B 0.9B BOOT (Gu et al., 2023) Guided Distillation (Meng et al., 2023) LCM (Luo et al., 2023) Phased Consistency Model (Wang et al., 2024a) Phased Consistency Model (Wang et al., 2024a) SiD-LSG (κ = 4.5) SiD-LSG (κ = 3) SiD-LSG (κ = 2) SiD-LSG (κ = 1.5) SiD-LSG (κ = 4.5) Distillation 512 512 512 512 512 512 512 512 512 512 0.09s 0.09s 0.09s 0.09s 0.21s 0.09s 0.09s 0.09s 0.09s 0.09s Rectification ( ) SDv1-5+DPMSolver (Upper-Bound) (Lu et al., 2022) Rectified Flow (Liu et al., 2023) Rectified Flow (Liu et al., 2023) Rectified Flow (Liu et al., 2023) Rectified Diffusion (Ours) Rectified Diffusion (Ours) Rectified Diffusion (Ours) Rectified Flow (Distill) (Liu et al., 2023) Rectified Flow (Distill) (Liu et al., 2023) Rectified Diffusion (CD) (Ours) Rectified Diffusion (CD) (Ours) PeRFlow (Yan et al., 2024) Rectified Diffusion (Phased) (Ours) 512 512 512 512 512 512 512 512 512 512 512 512 0.88s 0.88s 0.09s 0.13s 0.88s 0.09s 0.13s 0.09s 0.13s 0.09s 0.13s 0.09s 0.09s 1 1 1 1 4 1 1 1 1 1 25 25 1 2 25 1 2 1 2 1 2 1 1 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B SDXL-Turbo Sauer et al. (2023c) SDXL-Turbo Sauer et al. (2023c) SDXL-Lightning (Lin et al., 2024) SDXL-Lightning (Lin et al., 2024) DMDv2 (Yin et al., 2024b) DMDv2 (Yin et al., 2024b) Stable Diffusion XL (3B) and its accelerated or distilled versions GANs 512 512 1024 1024 1024 0.15s 0.34s 0.35s 0.71s 0.35s 0.71s 1 4 1 4 1 4 3B 3B 3B 3B 3B 3B LCM (Luo et al., 2023) LCM (Luo et al., 2023) Phased Consistency Model (Wang et al., 2024a) Phased Consistency Model (Wang et al., 2024a) PeRFlow-XL (Yan et al., 2024) Rectified Diffusion-XL (Phased) (Ours) Distillation 1024 1024 1024 1024 0.35s 0.71s 0.35s 0.71s Rectification ( ) 1024 1024 0.71s 0.71s 1 4 1 4 4 3B 3B 3B 3B 3B 3B 12.24 10.39 12.63 7.27 6.95 26.94 13.90 9.09 12.78 11.49 14.98 16. 48.20 37.3 37.3 17.91 11.70 16.59 13.21 9.56 8.71 16.59 9.78 11.34 36.68 20.01 10.73 16.88 12.57 13.67 62.81 12.54 11.41 18.59 10.21 24.57 23.19 23.92 24.56 19.01 19. 81.62 22.16 25.31 21.04 20.99 19.71 - - - - - - 0.293 - - - 0.320 0.312 0.26 0.27 0.27 0.296 - 0.317 0.314 0.313 0.302 0. 0.318 0.313 0.272 0.296 0.315 0.293 0.307 0.302 0.261 0.303 0.310 0.264 0.310 0.337 0.334 0.316 0.323 0.336 0.332 0.275 0.317 0.318 0. 0.334 0.340 Results of Stable Diffusion XL-based models are tested with COCO-2014 10k following the evaluation setting of DMDv2 (Yin et al., 2024b). Other results are tested with COCO-2014 30k following the karpathy split. 9 Preprint (a) 1-step comparison (b) 2-step comparison (c) 4-step comparison (d) Phased (4-step) (e) Distillation (1-step) Figure 7: Human preference metrics comparison. LPIPS Loss (Zhang et al., 2018), which generally improves FID but incurs substantial memory and computational costs during the latent diffusion decoding process. For the second-stage distillation, we employ consistency distillation training with batch size of 512 for 10,000 iterations, consuming total of 4.6 A800 GPU days. In contrast, the distillation process described in the InstaFlow paper takes 110 A100 GPU days. Our training cost is approximately 3% of the GPU days of InstaFlows distillation process. Training speed. We monitor the performance of Rectified Diffusion in terms of FID and CLIP score at different stages of training. It was observed from Fig. 2 that our method achieve superior one-step performance compared to Rectified Flow after just 20,000 iterations, with further significant improvements as training continued. At this stage, the number of samples processed was only about 8% of the samples processed by Rectified Flow. This efficiency is largely because Rectified Diffusion does not require converting the original epsilon prediction diffusion model, which follows the DDPM form, into v-prediction flow-matching modela process that incurs significant computational cost. Qualitative comparison. We present comparison of the images generated by Rectified Diffusion and Rectified Flow across various scenarios in Fig. 8 and Fig. 9. First, we can observe that the Rectified Flow model performs poorly at low step counts, producing only very blurry images in fewer than eight steps. Additionally, we notice that the images generated by PeRFlow are blurry and fail to reflect the content of the text. Moreover, the results generated by Rectified Flow (Distill) remain relatively blurry and lack the ability for multi-step refinement, which limits its applicability. Rectified Diffusion shows clearly superiority in these settings. Quantitative comparison. We calculate the FID (Heusel et al., 2017) and CLIP scores (Radford et al., 2021) for different models on the COCO-2017 validation set (Lin et al., 2014) and the 30k subset of the COCO-2014 validation set (Lin et al., 2014), respectively. As shown in Table 1 and Table 2, our model consistently outperforms the methods based on rectified flow across both metrics, different scenarios, and various steps. It also achieves performance comparable to advanced distillation and GAN training methods. Human preference metrics. To more comprehensively evaluate the model performance, we compare the outputs using human preference models. We follow the testing setup of DiffusionDPO (Wallace et al., 2024), generating images with 500 unique prompts from the Pick-apic (Kirstain et al., 2023) validation set for comparison. We used the Laion-Aesthetic Predictor (Schuhmann, 2022), Pickscore (Kirstain et al., 2023), HPSv2 (Wu et al., 2023), and ImageReward (Xu et al., 2024a) to score the generated results from each model individually and calculate the win rate of each model across these metrics. Our results, shown in Fig 7, consistently outperform the results of Rectified Flow-based models. 10 Preprint CFG-influence. We show the performance comparison of FID and CLIP Score between Rectified Flow and Rectified Diffusion under different step counts and CFG values in Fig. 6. We observe that Rectified Diffusion consistently outperforms Rectified Flow, especially in the low-step regime. Additionally, we find that CFG has significant impact on both Rectified Diffusion and Rectified Flow; even in the 1-step generation scenario, using an appropriate CFG value can still significantly enhance performance."
        },
        {
            "title": "4 CONCLUSION",
            "content": "In conclusion, we rethink and investigate the essence of rectified flow. We demonstrate that retraining with pre-collected noise-image pairs is the most important factor. Building on this insight, we propose Rectified Diffusion, extending its scope to general diffusion forms. We identify that it is not straightness but first-order property is the essential training target of Rectified Diffusion. Additionally, by incorporating consistency distillation and introducing Rectified Diffusion (Phased), we further enhance training efficiency and model performance, offering streamlined approach to efficient high-fidelity visual generation. Vast validation demonstrates the advancements of Rectified Diffusion. 11 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Patrick Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), ICML, volume 202 of Proceedings of Machine Learning Research, pp. 40554075. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/chang23b. html. Ricky TQ Chen and Yaron Lipman. Riemannian flow matching on general geometries. arXiv preprint arXiv:2302.03660, 2023. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34:87808794, 2021. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS, 34:1982219835, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-ascene: Scene-based text-to-image generation with human priors. In ECCV, pp. 89106. Springer, 2022. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured Probabilistic Inference {&} Generative Modeling, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. ddpm. NeurIPS, 33:68406851, 2020. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In CVPR, pp. 1012410134, 2023. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. edm. NeurIPS, 35:2656526577, 2022. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. On density estimation with diffusion models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), NeurIPS, 2021. URL https://openreview.net/forum?id=2LdBqxc1Yv. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. Sangyun Lee, Zinan Lin, and Giulia Fanti. Improving the training of rectified flows. arXiv preprint arXiv:2405.20320, 2024. Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. 12 Preprint Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In ECCV, Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. volume 8693, pp. 740755. Springer, 2014. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In ICLR, 2023. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. NeurIPS, 35:57755787, 2022. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In CVPR, pp. 1429714306, 2023. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pp. 4195 4205, October 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICLR, pp. 87488763. PMLR, 2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021. URL https://arxiv.org/ abs/2102.12092. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pp. 1068410695, 2022a. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pp. 1068410695, 2022b. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. URL https://arxiv.org/abs/ 2205.11487. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In ICML, pp. 3010530118. PMLR, 2023a. 13 Preprint Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In ICLR, pp. 3010530118. PMLR, 2023b. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023c. Christoph Schuhmann. Laion-aesthetics. laion-aesthetics/, 2022. Accessed: 2023-11-10. https://laion.ai/blog/ Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pp. 111, 2024. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, pp. 82288238, 2024. Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency model. arXiv preprint arXiv:2405.18407, 2024a. Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Accelerating the animation of personalized diffusion models and adapters with decoupled consistency learning. arXiv preprint arXiv:2402.00769, 2024b. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024a. Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. In CVPR, pp. 81968206, 2024b. Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. arXiv preprint arXiv:2405.07510, 2024. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, pp. 6613 6623, 2024a. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William T. Freeman. Improved distribution matching distillation for fast image synthesis, 2024b. URL https://arxiv.org/abs/2405.14867. 14 Preprint Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, June 2018. Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai Huang. Long and short guidance in score identity distillation for one-step text-to-image generation. arXiv preprint arXiv:2406.01561, 2024a. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In ICML, 2024b. Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In CVPR, pp. 1790717917, 2022. 15 Preprint"
        },
        {
            "title": "APPENDIX",
            "content": "I Related Works II Limitations III Proof for first-order ODE IV More results."
        },
        {
            "title": "I RELATED WORKS",
            "content": "1 1 1 2 Diffusion models. Diffusion models have steadily become the foundational models in image synthesis (Ho et al., 2020; Song et al., 2020b; Karras et al., 2022). Extensive research has been conducted to explore their underlying principles (Lipman et al., 2022; Chen & Lipman, 2023; Song et al., 2020b; Kingma et al., 2021) and to expand or enhance the design space of these models (Song et al., 2020a; Karras et al., 2022; Kingma et al., 2021). Additionally, several works have focused on innovating the model architecture (Dhariwal & Nichol, 2021; Peebles & Xie, 2023), while others have scaled up diffusion models for text-conditioned image synthesis and various real-world applications (Shi et al., 2024; Rombach et al., 2022b; Podell et al., 2023). Moreover, efforts to accelerate sampling have been pursued at both the scheduler level (Karras et al., 2022; Lu et al., 2022; Song et al., 2020a) and the training level (Meng et al., 2023; Song et al., 2023; Zhou et al., 2024b;a). The former typically involves refining the approximation of the PF-ODE (Lu et al., 2022; Song et al., 2020a), while the latter focuses on distillation techniques (Meng et al., 2023; Salimans & Ho, 2022; Song et al., 2023; Wang et al., 2024a;b) or initializing diffusion weights for GAN training (Sauer et al., 2023c; Lin et al., 2024; Xu et al., 2024b). Rectified Flow. Lipman et al. (2022) proposes the flow matching based on continuous normalizing flows, providing different and unified perspective to understand diffusion models. Liu et al. (2022) proposes the method rectified flow, setting up important baseline for diffusion acceleration and providing solid theoretical analysis. It proposes rectification to straighten the ODE path of flow-matching based diffusion models. In the proof, Liu et al. (2022) show that the rectification allows for non-decreasing straightness of ODE. Liu et al. (2023) scale up the idea of rectified flow into large text-to-image generations, achieving one-step generation without introducing GAN. Yan et al. (2024) proposes to split the ODE path into multi-phase following the InstaFlow (Liu et al., 2023). Lee et al. (2024) analysises that one-time rectification is generally enough to achieve pure straightness and proposes better optimization strategy for enhanced performance of rectified flows."
        },
        {
            "title": "II LIMITATIONS",
            "content": "At low-step regime, the performance of methods based on rectification still lags behind state-ofthe-art methods based on distillation (Zhou et al., 2024b) or GAN training (Yin et al., 2024b; Sauer et al., 2023c). Additional distillation steps are needed to improve low-step performance, which is also stated in InstaFlow (Liu et al., 2023). III PROOF FOR FIRST-ORDER ODE Theorem 1 For the general diffusion form xt = αtx0 + σtϵ, there exists an exact ODE solution form as follows: xt = αt αs xs αt (cid:90) λt λs eλϵθ(xtλ, tλ)dλ, (5) 1 Preprint where λt = ln αt σt and tλ is the inverse function of λt. The first-order ODE satisfies xt = αt αs xs αtϵθ(xs, s) (cid:90) λt λs eλdλ = αt αs xs αtϵθ(xs, s)( αs σs αt σt ) . (6) We show the equivalence between Equation 5 and Equation 6 for arbitrary and s, which holds true if and only if ϵθ(xt, t) is constant. Proof 1 If ϵθ(xt, t) is constant, then the Equation 5 and Equation 6 are equivalent. Assumption: Let ϵθ(xs, s) = ϵ0 be constant. Substituting ϵ0 into the Equation 5: Calculating the integral: Substituting the result: xt = αt αs xs αtϵ (cid:90) λt λs eλdλ (cid:90) λt λs eλdλ = eλs eλt = σs αs σt αt xt = αt αs xs αtϵ (cid:19) (cid:18) σs αs σt αt Comparing with the equation: The results match, thus proving equivalence. If Equation 5 and Equation 6 are equivalent, then ϵθ(xt, t) must be constant. Assumption: Assume the two are equivalent: αt (cid:90) λt λs eλϵθ(xtλ, tλ)dλ = αtϵθ(xs, s) (cid:90) λt λs eλdλ Removing the constant factor: (cid:90) λt λs eλϵθ(xtλ, tλ)dλ = ϵθ(xs, s) (cid:90) λt λs eλdλ Differentiating with respect to with Newton-Leibniz theorem: (cid:32)(cid:90) λt λs dt (cid:33) eλϵθ(xtλ, tλ)dλ = eλtϵθ(xtλ, tλ) dλt dt Comparing both sides: eλtϵθ(xtλ, tλ) dλt dt = ϵθ(xs, s)eλt dλt dt Since dλt dt = 0 and eλt > 0, we can cancel terms, leading to: ϵθ(xtλ, tλ) = ϵθ(xs, s), tλ [s, t]. (7) (8) (9) (10) (11) (12) (13) (14) Conclusion: This shows that for any t, ϵθ(xt, t) must be constant, proving the if and only if statement. IV MORE RESULTS. Preprint Figure 8: Qualitative comparison. 3 Preprint Figure 9: Qualitative comparison. Preprint Figure 10: Qualitative comparison."
        }
    ],
    "affiliations": [
        "MMLab, CUHK, Hong Kong SAR",
        "Peking University, Beijing, China",
        "Princeton University, New Jersey, USA"
    ]
}