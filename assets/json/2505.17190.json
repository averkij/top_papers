{
    "paper_title": "Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms",
    "authors": [
        "Baran Hashemi",
        "Kurt Pasque",
        "Chris Teska",
        "Ruriko Yoshida"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, a novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as a third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 0 9 1 7 1 . 5 0 5 2 : r Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms Baran Hashemi Origins Data Science Lab Technical University of Munich Munich, Germany baran.hashemi@tum.de Kurt Pasque Naval Postgraduate School Monterey, California kurt.pasque@nps.edu Chris Teska Naval Postgraduate School Monterey, California christopher.teska@nps.edu Ruriko Yoshida Naval Postgraduate School Monterey, California ryoshida@nps.edu"
        },
        {
            "title": "Abstract",
            "content": "Dynamic programming (DP) algorithms for combinatorial optimization problems work with taking maximization, minimization, and classical addition in their recursion algorithms. The associated value functions correspond to convex polyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning models, however, rely on softmax-normalized dot-product attention where the smooth exponential weighting blurs these sharp polyhedral structures and collapses when evaluated on out-of-distribution (OOD) settings. We introduce Tropical attention, novel attention function that operates natively in the max-plus semiring of tropical geometry. We prove that Tropical attention can approximate tropical circuits of DP-type combinatorial algorithms. We then propose that using Tropical transformers enhances empirical OOD performance in both length generalization and value generalization, on algorithmic reasoning tasks, surpassing softmax baselines while remaining stable under adversarial attacks. We also present adversarial-attack generalization as third axis for Neural Algorithmic Reasoning benchmarking. Our results demonstrate that Tropical attention restores the sharp, scale-invariant reasoning absent from softmax."
        },
        {
            "title": "Introduction",
            "content": "The tropical semiring := (R {}, max, +) (or its max-plus variant) replaces ordinary addition by maximum and multiplication by addition [28]. Polynomials over this semiring evaluate to piecewise-linear, polyhedral functions. These are the main objects of study in tropical geometry, with wide applications across the intersection of matroid theory, combinatorial optimization, Auction theory and algebraic geometry [2, 3, 11, 20, 28], and recently Machine Learning [38, 53, 54, 30, 6, 55]. Because it analyses the entire polyhedral structure of solutions rather than single Euclidean point, tropical geometry is natural mathematical language for algorithms that must reason over families of inputs, particularly those generating such polyhedral structures. Dynamic programming (DP) exemplifies this connection. It is cornerstone for numerous combinatorial optimization problems. The structure of these problems allows DP value functions to be described as piecewise-linear functions, forming polyhedral functions. They are just recursively constructed Preprint. Under review. Figure 1: (top) Tropical attention with sharp attention maps on learning the Quickselect algorithm, showcasing size-invariance and OOD lengths generalization behavior far beyond training (8 1024). In contrast, both (middle) adaptive-softmax and (bottom) vanilla-softmax heads dilute and disperse as sequence length grows, failing to generalize. circuits over the corresponding semirings. Each such circuit computes, in natural way, some polynomial over the underlying semiring. Most known dynamic programming algorithms correspond to circuits over the (max, +) (or (min, +) semirings, or tropical circuits, making the DP update step effectively linear within this algebraic framework [22]. Therefore, DP computations inherently operate tropically, propagating these polyhedral forms [12]. Algorithms like Floyd-Warshall explicitly manifest this, essentially performing tropical matrix operations to explore the facets of polyhedral solution space [21]. This inherent reliance on sharp, tropical operations poses challenge for vanilla Attention operation where it has to execute combinatorial algorithm[48]. Some notable applications are constructing the combinatorial structures of particular type [19], enumeration [16], and dynamic programming [17, 10]. Vanilla Transformers[47] use softmax-normalized dot-product attention, which in Euclidean space yields smooth, quadratic decision boundaries. This smoothness blurs the hard arg max / arg min structure that DPs rely on, that is approximating true maximum. This phenomena where as the length input vector grows, the resulting probability distribution becomes increasingly flat has several names such as dispersion [49] or attention fading [33]. Moreover, the exponential sensitivity of softmax makes logits vulnerable to small ℓ perturbations, harming adversarial robustness. As result, Transformers equipped with softmax attention fail to extrapolate beyond the training regime of input length or magnitude on classical algorithmic benchmarks. For non-algorithmic reasoning tasks, even though injecting positional information [45, 42, 9] can alleviate the length extrapolation issue, we believe that the core of the issue lies within the this softmax function. As result, we propose Tropical Attention, novel attention function that, maps Euclidean input information to the tropical semiring, perform information routing there by tropical geometric operations, then maps the result back to Euclidean space so that subsequent Transformer blocks remain unchanged. Since, the max-plus aggregation is 1-Lipschitz and piecewise-linear, Tropical Attention preserves the polyhedral structure of the underlying DP while inheriting the projective Hilbert metric that captures shortest-path dynamics [20]. As result, we prove that multi-head Tropical attention captures the target class of maxplus piecewise-linear maps that arise in tropical circuits, combinatorial optimization and dynamic programming. Our expressivity theorems build directly on this correspondence, showing that multi-head Tropical Attention simulates such circuits without super-polynomial blow-ups. Our contributions in this paper goes as follows: 1. novel attention mechanism Introducing Tropical Transformer with the Tropical attention mechanism operating in the max-plus arithmetic (max, +), with theoretical guarantee. 2 2. Theoretical study on expressivity Demonstrating that that maxplus dynamic programming lies within the expressive envelope of Tropical attention, and proving that multi-head Tropical attention can simulate any DP-like algorithm, laying theoretical groundwork for future bridges between discrete optimization and reasoning models. 3. Extensive empirical evaluation. combinatorial tasksFLOYDWARSHALL, BALANCEDPARTITION, CONVEXHULL, SUBSETSUMDECISION, 01 and fractional KNAPSACK,STRONGLYCONNECTEDCOMPONENTS, and MINCOINCHANGETropical transformer achieve state-of-the-art (SOTA) out-of-distribution (OOD) generalization in length and value scale and exhibit superior adversarial robustness. canonical 3SUMDECISION, On QUICKSELECT, BINPACKING, eleven The remainder of the paper develops these ideas. Section 2 formally define OOD generalization, tropical algebra, Softmax attention and prior work; Section 3 formalizes our mechanism and theoretical results; Sections 4 5 detail the experimental study; Section 5 discusses the results and implications; and Section 6 concludes and suggests future directions."
        },
        {
            "title": "2.1 Out-of-Distribution Generalization",
            "content": "An important measure of supervised learning models reasoning is its ability to generalize to inputs that differ fundamentally from those encountered during training. This is known as out-of-distribution (OOD) generalization. Following [7] notations formally, let denote the feature space and the label set. model : is learned from training examples drawn independently and identically distributed from training distribution Dtr over Y. Given distinct test distribution Dte on the same space, we define the OOD risk as, RDte(h) := E(x,y)Dte[ℓ(h(x), y)], where ℓ is loss function. Its empirical estimate on finite sample drawn from Dte (i.e., DS te ) is, (cid:98)RS(h) := 1 (x,y)S ℓ(h(x), y). (cid:80) We say that the model OOD-generalizes from Dtr to Dte if its OOD risk RDte (h) remains comparable to its in-distribution risk RDtr(h), indicating minimal performance degradation despite the distributional shift. In the context of neural algorithmic reasoning, three main types of deviation between Dtr and Dte are important in measuring models capabilities: Length Generalization. Both distributions draw their numerical entries from the same range but the test sequences are strictly longer, Dte(X ) (cid:0)R>0(cid:1)nmax with nmax > ntr. Here, good performance indicates that the network has learned parallel or recursive scheme that scales with input size rather than memorizing fixed shallow circuit. Value generalization. The two distributions share the same support with respect to sequence length but supp(cid:0)Dte(X )(cid:1) contains magnitudes never encountered during training, i.e. supp(cid:0)Dte(X )(cid:1) supp(cid:0)Dtr(X )(cid:1) = . For arithmetic or DP-style tasks, value generalization is the clearest evidence that the model has learned the rule rather than the lookup table of seen inputs. Adversarial-attack generalization. Adversarial attacks aim to cause model to make mistakes with perturbations (adversarial samples) or predefined patterns (backdoor triggers). Here Dte is obtained from Dtr by an ℓp-bounded, perturbation map : : xadv = A(x) with xadv xp ε. Robust generalization demands that the risk remains low even under the worst allowed A. This regime probes the stability and smoothness of the learned function of the architecture. The adversarial reliability of Neural Algorithmic Reasoning models are very important for many real-world systems, especially for cryptographic schemes [41]. Length, value, and adversarial generalization stress complementary facets of algorithmic competence[34, 29]. Thus, model as true reasoning circuit[27, 10] that excels simultaneously in all three regimes offers strong evidence of having internalized the underlying combinatorial procedure rather than brittle statistical surrogate."
        },
        {
            "title": "2.2 Softmax Self-Attention Mechanism\nGiven an input sequence X = (cid:2)x1, . . . , xN\nXW⊤\ni and k⊤\nq⊤\nVanilla self-attention computes, for every token i,",
            "content": "K, = , where the parameter matrices satisfy WQ, WK Rddx and WV Rdvdx . Denote by the i-th and j-th rows of and K, respectively, and τ > 0 for temperature parameter. RN dx , let = XW Q, = XW (cid:3) hi = (cid:88) j=1 αij, vj, αij = softmax τ (cid:0)qi, kj(cid:1) := exp(cid:0)qi, kj/τ (cid:1) t=1 exp(cid:0)qi, kt/τ (cid:1) , (cid:80)N = 1, . . . , N, (1) where the softmax is applied independently to each row of the score matrix QK. The temperature τ modulates the sharpness of the resulting probability vector, as τ 0 the weights approach one-hot selection, whereas large τ yields an almost uniform mixture. Equation 1 measures similarity with the Euclidean inner product, which is spherically invariant, meaning that every coordinate contributes equally, regardless of its algorithmic significance. Despite its success in many tasks [47, 8], its geometric and numerical properties are ill-suited to algorithmic reasoning[7, 44]. We summarize the main shortcomings. 1. Inherent blurriness The exponential map assigns non-zero weight to every token; even at low temperatures the second-largest term remains strictly positive. As problem size grows, the gap between the top two logits often decreases (e.g. when costs are drawn from common distribution), so the resulting distribution cannot converge to one-hot vector. In practice this leads to soft rather than decisive selections, hampering tasks that require exact order statistics [49, 33]. Recent diagnostic suites show that large language models fail on simple tasks of finding minima and second-minima even within In Distribution (ID) length tests [31, 37]. The attention kernels inability to sharpen with scale is primary culprit. 2. Sensitivity to small perturbations. Because softmax(z) ez, perturbation of size δ in the largest logit changes the corresponding weight by multiplicative factor eδ. An adversary who can alter single entry of QK/τ by O(log ) may invert the ranking of two tokens, propagating an O(1) error to downstream activations [52, 25]. This ℓ-fragility persists even after common stabilisers such as temperature scaling or normalization layers [52]. 3. Mismatch with polyhedral decision boundaries. In combinatorial optimizations the value function is tropical polynomialpiecewise linear with faces aligned to coordinate hyperplanes [20, 23]. The quadratic forms generated by Euclidean dot products carve the domain into spherical caps [35] rather than polyhedral cones; reproducing DP recurrence therefore demands exponentially many heads or layers unless the desired structure is injected by hand. 4. Temperaturegradient dilemma. Driving the distribution toward hard arg max necessitates lowering the temperature parameter τ . Yet as τ 0 the Jacobian of the softmax grows like τ 1, causing gradient explosion/vanishing [24]. Careful schedule tuning or gradient clipping becomes mandatory [52], adding hyper-parameter overhead."
        },
        {
            "title": "2.3 Tropical Geometry",
            "content": "The most fundamental component of tropical algebraic geometry is the tropical semiring := (cid:0)R {}, , (cid:1). The two operations and , called tropical addition and tropical multiplication respectively, are defined as follows. Definition 2.1. For x, R, their tropical sum is := max{x, y}; their tropical product is := + y; the tropical quotient of over is := y. For any R, we have = 0 = and = . Thus is the tropical additive identity and 0 is the tropical multiplicative identity. Furthermore, these operations satisfy the usual laws of arithmetic, namely associativity, commutativity, and distributivity. The set {} is therefore semiring under the operations and . While it is not ring since it lacks an additive 4 inverse, one may nonetheless generalize many algebraic objects over the tropical semiring, the study of these, in nutshell, constitutes the subject of tropical algebra. In order to have transition from classical arithmetic to tropical arithmetic we need series of transition maps, which is referred to as tropicalization. Definition 2.2. (The valuation map) Let and write Rd the field of real numbers. valuation on is function val : satisfying the following three axioms: 1. val(a) = = 0; 2. val(ab) = val(a) + val(b); 3. val(a + b) max{val(a), val(b)} a, R. One approach to tropical geometry, is to define tropical variety as shadow of an algebraic variety that involves logarithmic limit sets. Classically, the amoeba of variety is its image under taking the coordinatewise logarithm of the absolute value of any point on the variety [12]. The logarithm turns ordinary multiplication into tropical addition: val(xy) = val(x) + val(y), x, R>0, and satisfies the sub-additive inequality val(x + y) max{val(x), val(y)} + log 2. Hence val is non-Archimedean log map up to harmless additive constant. For an input Rd we call Trop(X) := val(X) Td its tropicalization. All subsequent reasoning, including attention weight computations, will take place in this max-plus space. When is smooth manifold, Trop(X) is typically curved domain whose tentacles encode asymptotic directions of X. Passing to the max-plus algebra straightens those curves into polyhedral pieces, providing the piecewise-linear structure on which our Tropical Attention operates. Definition 2.3. (The tropical projective space [28].) We regard Td as semimodule over the tropical semiring by coordinate-wise operations. Introduce 1d+1 := (1, . . . , 1) Rd+1, Td+1 := (R {})d+1 {}d+1. Declare two points x, Td+1 projectively equivalent, written y, if there is scalar λ such that = + λ1d+1. The quotient TPd := Td+1(cid:14) is the tropical projective space. See [28] for more details on tropical geometry. Every class has unique representative with maximal coordinate equal to 0, so TPd identifies with the standard simplex := {w Rd+1 maxi wi = 0}. Attention weights produced by the softmax surrogate live in the Euclidean simplex; Tropical Attention will instead output points of interpreted tropically, guaranteeing sharp arg max behavior. Definition 2.4. (y1, . . . , yd+1) Td+1 put (The tropical Hilbert projective metric.) For := (x1, . . . , xd+1), := dH(x, y) := (cid:0)max (xi yi)(cid:1) (cid:0)min (xi yi)(cid:1) = diam(cid:0)x y(cid:1), where denotes the coordinate-wise tropical quotient (x1 y1, . . . , xd+1 yd+1) and diam its range. The metric descends to TPd and enjoys two key properties: 1. Projective invariance. dH(x + c1d+1, + c1d+1) = dH(x, y) for all R. 2. Non-expansiveness of max-plus-affine maps [46]. Every tropical linear map : Td+1 Tm+1 is 1-Lipschitz: dH (cid:0)Ax, Ay(cid:1) dH(x, y). These facts, due to Nussbaum and further developed by AkianGaubert, furnish tight robustness guarantees, perturbing the inputs by ϵ in Hilbert distance changes the output of any compositional stack of tropical linear layers by at most ϵ [1, 36]."
        },
        {
            "title": "2.4 Neural Algorithmic Reasoning",
            "content": "Bridging symbolic algorithms and differentiable models has become has been established officially under the name of Neural Algorithmic Reasoning (NAR). Neural Algorithmic Reasoning involves developing neural models and learning procedures to facilitate the internalization of algorithms directly in models weights. Starting from some early work [50] that aimed to demonstrate the applicability of Graph Neural Networks (GNNs) to approximate classical algorithm, the community has then developed and expanded further in different directions [26, 43, 13, 14, 4, 51, 44, 5, 18]. fundamental objective of NAR is to achieve robust out-of-distribution (OOD) generalization. Typically, models are trained and validated on small sets/sequences/graphs and tested on larger sets/sequences/graphs. This is inspired by classical algorithms size-invariance, where correctness of the solution is maintained irrespective of the input size. Our work pursues this objective from fresh, tropical-geometric angle and provides universality guarantees for an attention layer within the NAR agenda. For combinatorial tasks in particular, dynamic-programming recurrences over the (max, +) semiring can be seen as shallow tropical circuits; conversely, every tropical circuit induces DP on weighted Directed acyclic graph (DAG). This equivalence was established for mean-payoff games [1, 36], while it was established [23] that depth and size lower bounds for tropical circuits and their DP relations. Our expressivity theorems build directly on this correspondence, showing that multi-head Tropical Attention captures such circuits without super-polynomial blow-ups. Recent work only tried to quantify NAR failures when test sequences are longer [34, 29], numerically larger [7], but not adversarially perturbed scenarios. Adversarial perturbations itself is also important since real-world deployments must withstand the worst-case inputs and noise; robustness in this setting is test for whether model has internalized genuine algorithmic structure rather than superficial statistical cues. Hence, we introduce adversarial-attack generalization as third pillar for NAR benchmarking and show that our Tropical attention demonstrates systematic gains across all three axes. Based on the past related works, one can establish demand for an attention mechanism that (i) respects the underlying algebraic and geometric structure of combinatorial algorithms, (ii) mitigates softmax dispersion that hampers OOD generalization, and (iii) delivers OOD and robustness benefits. Tropical attention positions itself at this intersection, drawing on decade of tropical-geometric insights to advance neural algorithmic reasoning."
        },
        {
            "title": "3 Tropical Attention",
            "content": "Tropical Attention mechanism arises from parallel with classical combinatorial algorithms, that is information exchange is governed less by raw magnitudes than by order statisticsmaxima, minima, and interval widths, all of which live naturally in the max-plus semiring. We therefore posit that the the dot-product kernel design choices hinder scale generalization in methods that incorporate vanilla attention mechanism. In this section, our goal is to replace the dotproduct, Softmax-based kernel of vanilla self-attention with an operation that (i) preserves the piecewise-linear geometry of combinatorial problems, and (ii) inherits the 1Lipschitz robustness of tropical linear maps. We therefore project queries, keys, and values to the maxplus semiring, compute attention weights with the tropical Hilbert metric, aggregate by tropical matrixvector product, and finally map the result back to Euclidean space so that the rest of the Euclidean algorithm (e.g Transformer) modular stack is untouched. We present the framework relating robustness and piecewise-linearity of maps and show how our proposed scheme offers improvements OOD generalizations tasks. Let RN be the embedding of the input via learnable affine feed-forward neural network. We define the tropicalization map by going to the amoeba representation of the input followed by learnable map, Φλ(X) = log(cid:0)ReLU(X)(cid:1) λ, where the learnable vector λ RN . The constant shift enforces maxi ϕλ(x)i = 0, so the output of ϕλ always lies in the tropical simplex, d1 := (cid:8) Rd(cid:12) (cid:12) maxi zi = 0(cid:9), where every vector is projectively equivalent to exactly one point in the tropical simplex. (2) 6 Lemma 3.1. For every embedded coordinate [d], the function vλ(x) := (cid:2)ϕλ(x)(cid:3) = (cid:40) log(x) λ, > 0, , 0, where ϕλ is (projective) valuation map. Hence the shifted map (cid:101)v(x) = vλ(x) + λ = log(ReLU(x)) is non-Archimedean valuation in the classical sense, and Φλ : RN (R {})N is matrix-valued valuation modulo tropical scalars; its image lies in the tropical simplex. After mapping each input token to tropical projective space, = ϕλ(X) TPN d, we compute attention independently across heads. Definition 3.1 (Multi-head Tropical Attention (MHTA)). Let dk = d/H be fixed head dimension. Then, for every head (h [H] one can choose learnable matrices W(h) Rdkd and define the tropical linear projections Q(h) = W(h) , V(h) = W(h) where denotes maxplus matrix multiplication, (A B)ij = maxt{Ait + Btj}. Then, using dH the tropical Hilbert projective metric, defined in 2.4, we will have the tropical attention score , W(h) , W(h) , K(h) = W(h) S(h) ij = dH (cid:0)q(h) , k(h) (cid:1), i, [N ]. Thereafter, the head outputs are aggregated via tropical matrixvector product, C(h) = (cid:77) j=1 ij v(h) S(h) = max (cid:8)S(h) ij + v(h) (cid:9), [N ]. In the end, the contexts per head, will be mapped to the Euclidean domain via smooth inverse map (devaluation ) ψ(z) = exp(z), and concatenated back to the original dimension, = (cid:2)ψ(C(1)) . . . ψ(C(H))(cid:3) RN d. Why Tropical Attention? Every operation inside MHTA, is piecewise linear, hence the entire network computes tropical polygonal map whose cells are polyhedral cut out by hyperplanes. This is aligned with classical DP recurrences, whose value function is itself tropical polynomial. Training neural network with Tropical attention therefore starts from hypothesis space that already mirrors the solution structure of combinatorial algorithm. By contrast, Euclidean soft-max attention inserts an exponential map, blurring the sharp decisions on their input data. Moreover, since every intermediate representation of MHTA lies in the projective simplex d1, going through arg max is therefore well-defined (no equal maxima except on set of measure 0), is stable by global scaling meaning that shifting the entire vector by λ does not alter which index attains the maximum. Whereas classical transformers modulate the entropyversussharpness trade-off through temperature parameter in the softmax; MHTA sharpness is built in and temperature-free. Furthermore, each MHTA head can function as tropical gate in tropical circuit. tropical circuit is finite acyclic digraph whose input vertices store either variable or non-negative real constant, while every internal vertex has in-degree two and outputs the maximum or the sum of its two predecessors. The circuits size is the number of internal gates. Classical pure DP algorithms are nothing more than recursively-defined tropical circuits of this kind; consequently, lower bounds for tropical circuits translate directly into limits for such DP schemes. Tropical circuits compute by alternating max-gates with +-gates (tropical multiplication). An MHTA head can also be interpreted as single tropical gate. single head implements the composite transformation (u, v) (cid:55) maxj{ Sij + vj }, where the score Sij itself is obtained through several applications of max and + gates. The outer maximization provides the -gate, while the summand vj furnishes -gate acting on two variable inputs. Thus every head is compact, differentiable wrapper around the two tropical primitives, and full multi-head layer is simply collection of such gates operating in parallel on shared input tape. Stacking layers composes tropical gates as DP table composes its local recurrences; training therefore amounts to discovering how these gates should be wired together, rather than coaxing Euclidean softmax kernel to emulate maxplus algebra indirectly. As result of developing Multi-Head Tropical Attention, we prove that it is universal approximator of max-plus dynamic programming for combinatorial optimization (Theorem A.3, Corollary A.3.1, and Theorem 3.2). 7 Theorem 3.2 (Simulation of maxplus dynamic programs). Let (S, E) be finite directed acyclic graph with = vertices and edge weights {wuv}(u,v)E T. Fix source vertex v0 and consider the maxplus Bellman recursion dv(t + 1) = (cid:77) (cid:0)wuv du(t)(cid:1), dv(0) = δv,v0, N. u: (u,v)E For every finite horizon there exists MHTA network of depth , using heads per layer and no additional width blow-up, whose token values at layer equal the DP state vector (dv(t))vS for all 0 . In particular, the network outputs the optimal maxplus value function d(T ) after layers. Hence, without any architectural restriction MHTA captures the target class of maxplus piecewise-linear maps that arise in tropical circuits, combinatorial optimization and dynamic programming. The result is tight capacity argument that no super-polynomial blow-up in width or depth is required to embed combinatorial algorithmic reasoning into attention."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate Tropical transformers on eleven canonical combinatorial tasks. For every task we measure both the in-distribution performance and three complementary forms of out-of-distribution (OOD) generalization: Length OOD (longer inputs), Value OOD (unseen magnitudes), and Adversarial Attack (perturbed inputs). procedure to compare between vanilla attention and Tropical attentions is described in Appendix B. All datasets, generation scripts, and OOD protocols are described in Appendix and D. For our experiment we consider three variants, (i) Vanilla: Standard transformer encoder with softmax dot-product attention. (ii) Adaptive: Transformer equipped with adaptive softmax attention from [49]. Tropical: Our proposed transformer in which every attention block is replaced by Multi-Head Tropical Attention (MHTA). To ensure fair comparison, all three variants share identical backbone hyperparameters: depth, width, and number of heads. The only architectural difference is the attention kernel. Crucially, no model sees OOD examples during optimization. We follow uniform procedure in which each model is trained from scratch under the same training regime with task specific fixed input sequence lengths and value ranges. Out-of-Distribution Protocols In order to assess OOD generalization, we construct three stress tests: (1) Length OOD inputs drawn from the same value range but with longer input sequence lengths. (2) Value OOD the input sequence lengths are fixed and the values are sampled from an increasingly large range (for example, if the models trained on inputs sampled from the range [5, 5] an out of distribution evaluation would be inputs sampled from the range [10, 10]). (3) Adversarial OOD the input sequence lengths are fixed and the values are from the same input range, but subset of the input values are perturbed. Combinatorial Tasks Our evaluation suite covers eleven canonical problems: 1. FloydWarshall: predict allpairs shortest-path distances in weighted digraph (regression); 2. QuickSelect: return one-hot mask of the k-th smallest element in an unsorted list (token-wise classification); 3. 3SUM-Decision: decide if any triple of integers sums to target value (binary decision); 4. Subset-Sum-Decision: decide if any subset sums to given target sum (binary decision); 5. Balanced Partition: output minimum absolute difference between two subset sums (regression); 6. 01 Knapsack: compute the maximum value achievable under weight budget with indivisible items (regression); 7. Fractional Knapsack: same objective with items divisible (regression); 8. Convex Hull: label planar points that lie on the convex envelope (pointer classification); 9. Strongly Connected Components (SCC): label each node pair as connected/not-connected in an undirected graph (pairwise classification); 10. Bin Packing: predict number of bins used by first-fit-decreasing for given capacity (regression); 11. Min Coin Change: return the fewest coins required to make target amount with given currency system (regression). Table 1: Out-of-distribution regression performance (MSE) for Vanilla, Adaptive, and Tropical models across three OOD scenarios. Dataset"
        },
        {
            "title": "BalancedPartition\nBinPacking\nFloydWarshall\nFractionalKnapsack\nKnapsack\nMinCoinChange",
            "content": "13.48 136.50 37.57 125 41 0.84 4.19 129.82 38.09 118 37 0.42 0.25 95.36 20.08 117 24 0.19 34.45 0.56 81.44 394 32 1.06 5.35 24.81 58.17 309 45 0.67 5.80 0.20 59.41 168 23 0. 13.28 0.24 48.87 9.12 92 1.36 2.83 10.20 39.86 8.56 55 0.98 0.28 0.12 29.44 5.58 27 0.67 Table 2: Out-of-distribution test Micro-F1 for Vanilla, Adaptive, and Tropical models across three OOD scenarios. Algorithm"
        },
        {
            "title": "Vanilla Adaptive Tropical Vanilla Adaptive Tropical Vanilla Adaptive Tropical",
            "content": "45% ConvexHull 3% Quickselect SCC 40% SubsetSum 18% 81% 3SUM 49% 17% 52% 30% 80% 95% 21% 68% 0% 71% 82% 80% 30% 82% 28% 21% 0% 65% 38% 30% 34% 92% 44% 18% 85% 22% 72% 2% 60% 25% 93% 22% 31% 3% 62% 98% 50% 25% 85% 66%"
        },
        {
            "title": "5 Results and Discussion",
            "content": "Section 2.2 elaborated why and how softmax self-attention - and its descendants - are incapable of generalizing to OOD inputs in combinatorial problems, and Section 3 discussed why Tropical attention can generalize in the combinatorial regime. With our experimentation, we seek to show if and, if so, how Tropical attention generalizes in this domain. To answer if Tropical attention generalizes, we report the numerical results of our experimentation in Tables 1 and 2. The Tropical attention architecture achieves superior OOD performance to both the Vanilla and Adaptive softmax attention. Notably, this out performance can be seen in both regression and classification combinatorial tasks and across OOD protocols, validating our theoretical results from Section 3. The Tropical architectures ability to generalize well across OOD protocols and problem sets, especially the notorious Quickselect, suggests that instead of simply learning the specific data it is trained on, these purpose-built models learn the underlying structure of the combinatorial algorithm. In order to understand how Tropical attention outperforms, we explore the tropical attention maps relative to vanilla and adaptive attention maps for both Quickselect and Knapsack. Figure 2 shows modified attention head for Quickselect, problem that requires sharp argmax/argmin classification. This visualization depicts normalized attentional head for the Quickselect task for batch of 32 sets, over the 8 items with the largest keys by the ℓ2-norm. If the head operates correctly, it must allocate sharp attention to the position of k-th smallest element. We see that the attention on both softmax models quickly dilute/disperse as sequence length grows OOD while the tropical attention maintains focus. Similarly, Figure 3 depicts length OOD on the full attention head for the Knapsack problem, classic dynamic program corresponding to tropical circuits. Each model begins sharp in distribution, but the Tropical attention head maintains the same activation pattern across each input length, strongly suggesting it has learned the structure of the problem vice the specific training data. Limitations Although Tropical attention is out performing in almost all algorithmic tasks, this study was conducted on synthetic combinatorial algorithms and we have not yet demonstrated how Tropical Transformers can scale to perform on other reasoning domains such as natural language or vision. In particular, the computational and memory overhead introduced by max-plus operations and the tropical Hilbert metric could incur nontrivial runtime costs or scaling challenges. Figure 2: Stacked attention head representations for Quickselect under (a) Vanilla, (b) Adaptive, and (c) Tropical models. Each model was trained on length 8 sequences and was evaluated from Left to Right on length 16 to 1024 sequences. Each image was generated by batch of 32 inputs. The columns are the 8 largest keys by ℓ2-norm. Heatmap values are the attention of the row item at the column key. Figure 3: (top) Tropical attention with sharp attention maps on learning the Knapsack algorithm, showcasing size-invariance and OOD lengths generalization behavior far beyond training (8 1024). In contrast, both (middle) adaptive-softmax and (bottom) vanilla-softmax heads dilute and disperse as sequence length grows, failing to generalize."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced Tropical Attention, novel transformer module that replaces the softmax-normalized dot-product with an idempotent max /+ (tropical) normalization. From theoretical standpoint, we proved that Tropical Attention enjoys universal approximation properties on tropical polynomials and simulates maxplus tropical circuits that naturally align with dynamic programming algorithms in combinatorial optimization (Theorem A.3, Corollary A.3.1, and Theorem 3.2). Such an expressiveness result is critical for reasoning engines, where they have to generalize beyond the distribution they are trained on. Empirically, across eleven classic optimization problems, our Tropical transformers achieved SOTA out-of-distribution generalization on both sequence length and input-value scaling, and delivered markedly stronger adversarial robustness than Euclidean counterparts. These findings carry an important message for both neural algorithmic reasoning and Large Language Model communities: we demonstrate that tropical geometric extension beyond softmax not only enrich the algorithmic power of attention mechanisms but also yield tangible gains on reasoning tasks. We believe Tropical attention opens compelling avenues for hybrid semiring architectures and for leveraging tropical geometry to reason over discrete structures within deep learning systems. Future work will explore sparse tropical kernels and applications to graph-theoretic domains, aiming for ever-stronger generalization guarantees in neural algorithm and resoning synthesis."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This research was supported by the Excellence Cluster ORIGINS, funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy EXC2094-390783311. B.H extends his gratitude to the organizers and the wonderful instructors, Marta Panizzut and Margarida Melo, of the 2024 Trieste Algebraic Geometry Summer School (TAGSS) on Tropical Geometry, where the idea of the project was sparked. K.P., C.T. and R.Y. are partially supported by NSF Division of Mathematical Sciences: Statistics Program DMS 2409819."
        },
        {
            "title": "References",
            "content": "[1] MARIANNE AKIAN, STÉPHANE GAUBERT, and ALEXANDER GUTERMAN. Tropical polyhedra are equivalent to mean payoff games. International Journal of Algebra and Computation, 22(01):1250001, February 2012. [2] Federico Ardila and Mike Develin. Tropical hyperplane arrangements and oriented matroids, 2007. [3] Federico Ardila-Mantilla, Christopher Eur, and Raul Penaguiao. The tropical critical points of an affine matroid. SIAM Journal on Discrete Mathematics, 38(2):19301942, 2024. [4] Montgomery Bohde, Meng Liu, Alexandra Saxton, and Shuiwang Ji. On the markov property of neural algorithmic reasoning: Analyses and methods. In The Twelfth International Conference on Learning Representations, 2024. [5] Wilfried Bounsi, Borja Ibarz, Andrew Joseph Dudzik, Jessica Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, and Petar Veliˇckovic. Transformers meet neural algorithmic reasoners, 2025. [6] Marie-Charlotte Brandenburg, Georg Loho, and Guido Montúfar. The real tropical geometry of neural networks. arXiv preprint arXiv:2403.11871, 2024. [7] Artur Back de Luca, George Giapitzakis, Shenghao Yang, Petar Veliˇckovic, and Kimon Fountoulakis. Positional attention: Expressivity and learnability of algorithmic computation, 2025. [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. [9] Shaoxiong Duan, Yining Shi, and Wei Xu. From interpolation to extrapolation: Complete length generalization for arithmetic transformers, 2024. [10] Andrew Joseph Dudzik and Petar Veliˇckovic. Graph neural networks are dynamic programmers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. 11 [11] Alex Fink and Felipe Rincón. Stiefel tropical linear spaces, 2015. [12] Stéphane Gaubert. Tropical considerations in dynamic programming. Presentation at Optimization, Games, and Dynamics, Institut Henri Poincaré, Paris, November 2011. Based on joint work with Akian, Guterman, Allamigeon, Katz, Vigeral, McEneaney, and Qu. [13] Dobrik Georgiev Georgiev, Danilo Numeroso, Davide Bacciu, and Pietro Lio. Neural algorithmic reasoning for combinatorial optimisation. In The Second Learning on Graphs Conference, 2023. [14] Dobrik Georgiev Georgiev, JJ Wilson, Davide Buffelli, and Pietro Lio. Deep equilibrium algorithmic reasoning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [15] Charles R. Harris, K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernández del Río, Mark Wiebe, Pearu Peterson, Pierre Gérard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357362, September 2020. [16] Baran Hashemi, Roderic Guigo Corominas, and Alessandro Giacchetto. Can transformers do enumerative geometry? In The Thirteenth International Conference on Learning Representations, 2025. [17] Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani, Róbert Csordás, Andrew Joseph Dudzik, Matko Bošnjak, Alex Vitvitskyi, Yulia Rubanova, Andreea Deac, Beatrice Bevilacqua, Yaroslav Ganin, Charles Blundell, and Petar Veliˇckovic. generalist neural algorithmic learner. In Bastian Rieck and Razvan Pascanu, editors, Proceedings of the First Learning on Graphs Conference, volume 198 of Proceedings of Machine Learning Research, pages 2:12:23. PMLR, 0912 Dec 2022. [18] Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani, Róbert Csordás, Andrew Joseph Dudzik, Matko Bošnjak, Alex Vitvitskyi, Yulia Rubanova, Andreea Deac, Beatrice Bevilacqua, Yaroslav Ganin, Charles Blundell, and Petar Veliˇckovic. generalist neural algorithmic learner. In The First Learning on Graphs Conference, 2022. [19] Yunhui Jang, Dongwoo Kim, and Sungsoo Ahn. Graph generation with $k^2$-trees. In The Twelfth International Conference on Learning Representations, 2024. [20] Michael Joswig. Essentials of Tropical Convexity. American Mathematical Society, 2021. [21] Michael Joswig and Benjamin Schröter. Parametric shortest-path algorithms via tropical geometry. Mathematics of Operations Research, 47(3):20652081, August 2022. [22] Stasys Jukna. Lower Bounds for Tropical Circuits and Dynamic Programs, volume 57. Springer Science and Business Media LLC, October 2014. [23] Stasys Jukna. Tropical circuit complexity. Limits of Pure Dynamic Programming/by Stasys Jukna.-, 2023. [24] Akhil Kedia, Mohd Abbas Zaidi, Sushil Khyalia, Jungho Jung, Harshith Goka, and Haejun Lee. Transformers get stable: An end-to-end signal propagation theory for language models, 2024. [25] Gihyun Kim, Juyeop Kim, and Jong-Seok Lee. Exploring adversarial robustness of vision transformers in the spectral perspective, 2023. [26] Hefei Li, Chao Peng, Chenyang Xu, and Zhengfeng Yang. Open-book neural algorithmic reasoning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [27] Xihan Li, Xing Li, Lei Chen, Xing Zhang, Mingxuan Yuan, and Jun Wang. Circuit transformer: transformer that preserves logical equivalence, 2025. [28] Diane Maclagan and Bern Sturfels. Introduction to Tropical Geometry. American Mathematical Society, 2015. [29] Sadegh Mahdavi, Kevin Swersky, Thomas Kipf, Milad Hashemi, Christos Thrampoulidis, and Renjie Liao. Towards better out-of-distribution generalization of neural algorithmic reasoning tasks, 2023. [30] Petros Maragos, Vasileios Charisopoulos, and Emmanouil Theodosis. Tropical geometry and machine learning. Proceedings of the IEEE, 109(5):728755, 2021. 12 [31] Larisa Markeeva, Sean McLeish, Borja Ibarz, Wilfried Bounsi, Olga Kozlova, Alex Vitvitskyi, Charles Blundell, Tom Goldstein, Avi Schwarzschild, and Petar Veliˇckovic. The clrs-text algorithmic reasoning language benchmark, 2024. [32] Wes McKinney. Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference, SciPy 10, pages 51 56. SciPy, 2010. [33] Ken M. Nakanishi. Scalable-softmax is superior for attention, 2025. [34] Robert R. Nerem, Samantha Chen, Sanjoy Dasgupta, and Yusu Wang. Graph neural networks extrapolate out-of-distribution for shortest paths, 2025. [35] Stefan K. Nielsen, Laziz U. Abdullaev, Rachel S. Y. Teo, and Tan M. Nguyen. Elliptical attention, 2024. [36] Roger D. Nussbaum. Convexity and log convexity for the spectral radius. Linear Algebra and its Applications, 73:59122, 1986. Department of Mathematics, Rutgers University. [37] Euan Ong and Petar Veliˇckovic. Learnable commutative monoids for graph neural networks, 2022. [38] Kurt Pasque, Christopher Teska, Ruriko Yoshida, Keiji Miura, and Jefferson Huang. Tropical decision boundaries for neural networks are robust against adversarial attacks, 2024. [39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019. [40] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. [41] Pawan Kumar Pradhan, Sayan Rakshit, and Sujoy Datta. Lattice based cryptography : Its applications, areas of interest & future scope. In 2019 3rd International Conference on Computing Methodologies and Communication (ICCMC), pages 988993, 2019. [42] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation, 2022. [43] Gleb Rodionov and Liudmila Prokhorenkova. Neural algorithmic reasoning without intermediate supervision. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [44] Gleb Rodionov and Liudmila Prokhorenkova. Discrete neural algorithmic reasoning, 2025. [45] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. length-extrapolatable transformer, 2022. [46] Roan Talbut, Daniele Tramontano, Yueqi Cao, Mathias Drton, and Anthea Monod. Probability metrics for tropical spaces of different dimensions, 2024. [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. [48] Petar Veliˇckovic and Charles Blundell. Neural algorithmic reasoning. Patterns, 2(7):100273, July 2021. [49] Petar Veliˇckovic, Christos Perivolaropoulos, Federico Barbero, and Razvan Pascanu. softmax is not enough (for sharp out-of-distribution), 2024. [50] Petar Veliˇckovic, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution of graph algorithms. In International Conference on Learning Representations, 2020. [51] Kaijia Xu and Petar Veliˇckovic. Recurrent aggregators in neural algorithmic reasoning. In The Third Learning on Graphs Conference, 2024. [52] Hao Xuan, Bokai Yang, and Xingyu Li. Exploring the impact of temperature scaling in softmax for classification and adversarial robustness, 2025. [53] Ruriko Yoshida, Georgios Aliatimis, and Keiji Miura. Tropical neural networks and its applications to classifying phylogenetic trees. In 2024 International Joint Conference on Neural Networks (IJCNN), pages 19, 2024. 13 [54] Ruriko Yoshida, Leon Zhang, and Xu Zhang. Tropical principal component analysis and its application to phylogenetics, 2017. [55] Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 58245832. PMLR, 1015 Jul 2018."
        },
        {
            "title": "A Proofs",
            "content": "A.1 Proof of lemma 3.1 Proof. If ϕλ is valuation map, hence for all a, R, we have 1. vλ(0) = ; 2. vλ(ab) = vλ(a) + vλ(b) λ; 3. vλ(a + b) max{ vλ(a), vλ(b)}. Property (i) is immediate from the definition. For a, > 0, (ii) follows from log(ab) = log + log b: vλ(ab) = log(ab) λ = (cid:0)log λ(cid:1) + (cid:0)log λ(cid:1) + λ = vλ(a) + vλ(b) λ. If either factor is non-positive, both sides equal . For (iii), note that when a, > 0 we have log(a + b) max{log a, log b} + log 2, so subtracting λ preserves the inequality; if 0 or 0 the claim is trivial. Adding back the constant λ to vλ eliminates the offset in (ii) while leaving (i)(iii) unchanged, yielding the classical valuation (cid:101)v. Collecting the coordinate-wise maps gives the vector-valued projection ϕλ : Rd d1, which is therefore valuation map up to the projective (constant-shift) equivalence native to tropical geometry. The main theorem here establishes that MHTA is an expressive tropically universal approximator of max-plus dynamic programming for combinatorial optimization such that every function that can be computed by finite maxplus circuit admits realization by finite-depth MHTA stack. The proof proceeds in three stages. First we show that single head can act as tropical max gate. Second, we demonstrate that an H-head block can realize tropical map by computing finitely many such maxima in parallel. Finally, we prove by structural induction that stacking finite number of blocks suffices to emulate an arbitrary maxplus circuit. With the first lemma we want to show that single head can realize weighted tropical max gate. Lemma A.1 (Headlevel Weighted gate). Let be finite index set and let {xj}jJ and {wj}jJ T. There exists an attention head [H], querytoken index [N ] {t(j) J}, and distinct seq indices t(j) [N ] such that, after one forward pass, the context returned at is equal to c(h) = (cid:77) (cid:0)xj wj jJ (cid:1) = max (cid:8) xj + wj jJ (cid:9). (3) Proof. For fix and i, for every J, lets select distinct token position t(j). Then one can define the value vectors by v(h) := for all / {t(j)}. To enforce (3) it suffices to make s(h) (cid:0)0 (xj wj)(cid:1) = = (cid:76) maxjJ (xj + wj). = otherwise, because then c(h) t(j) = 0 for and s(h) t(j) := xj wj and v(h) jJ One can write every query / key vector in block form = (u(1), u(2)) Tdk1 T. Fix arbitrary first blocks u(1) and arrange so that dH(cid:0)q(h) , k(h) t(j) q(h) = (cid:0)0, . . . , 0, 0(cid:1), (cid:1) = 0 and hence s(h) k(h) t(j) = (cid:0)0, . . . , 0, 0(cid:1), J, t(j) = 0. For every irrelevant token / {t(j)} set k(h) = (cid:0)0, . . . , 0, Γr (cid:1), Γr 0, so that the last coordinate differs from that of the query by Γr; consequently dH(cid:0)q(h) (cid:1) = Γr and s(h) = Γr. Choosing Γr large enough drives the score to in the semiring, ensuring that irrelevant tokens do not influence the context. Equation (3) follows. , k(h) r 14 Lemma A.2 (Tropical affine layer). Let TM and TM . Embed = (x1, . . . , xN ) TN as the values of tokens t(1), . . . , t(N ) and add one bias token ib whose value is fixed to 0. There exists an MHTA layer with = heads and dk = 2 such that, for each [M ], where im is the query token of head m. c(m) im = (cid:77) (cid:0)Amj xj (cid:1) bm, j=1 Proof. For [M ] and head = m, we can apply Lemma A.1 with = {1, . . . , }, input xj and weights Amj to obtain (cid:76) j(Amj xj). Let the bias relevant to every head by assigning its key identical to the query, whence s(m) . The context becomes the imib maximum of (cid:76) = 0 for all m. Then, we give it value bm in head alone via W(m) j(Amj xj) and bm, completing the proof. Definition A.1 (Tropical circuit [22]). tropical circuit is finite directed acyclic graph whose source nodes are labelled by variables z1, . . . , zn and whose internal nodes are labelled either by the operation tropical addition (u, v) (cid:55) = max{u, v} or by the operation tropical multiplication (u, v) (cid:55) = + v. The circuit computes map : Tn Tm whose outputs are designated sinks. circuit is layered if every edge points from layer ℓ to layer ℓ + 1 for some topological layering {L0, . . . , LL}. We write depth(C) = and size(C) = for the number of internal gates. Because tropical multiplication distributes over tropical addition, every such circuit computes tropical polynomial, namely tropical sum of finitely many monomials, each monomial being tropical product (classical summation) of subset of the indeterminates plus constant. tropical polynomial in variables = (z1, . . . , zn) has an expression of the form (cid:75) (cid:77) (cid:88) (cid:110) (cid:16) (cid:17) (z) = ck k=1 j=1 ekj = max kK ck + ekjzj (cid:111) , j= where ck and ekj N. Thus is already the maximum of finitely many affine forms in z. Lemma A.2 therefore applies directly. Theorem A.3 (Singlelayer universality for tropical polynomials). Let : Tn Tm be vector-valued tropical polynomial map whose coordinates are Pℓ(z) = (cid:76) (Aℓk z) bℓk. There exists single kKℓ MHTA layer with = (cid:80)m ℓ=1 Kℓ heads and dk 2 whose tropical output (the collection of all head contexts before the de-valuation ψ = exp) equals (z). Proof. For each output coordinate ℓ one can allocate Kℓ heads, one per affine term Aℓk bℓk. Lemma A.2 shows that affine map in head (ℓ, k), depositing its value at fresh query token iℓk. Because the score of an irrelevant head is , the contexts written to those tokens are ignored by all other heads. Finally, putting an aggregation head per output ℓ whose query token reads all tokens iℓk with score 0 and returns their , namely maxk(Aℓk bℓk) = Pℓ(z). No de-valuation is applied inside the tropical computation, so the result equals (z) in the maxplus semiring. Corollary A.3.1 (DepthL universality). Let : Tn Tm be the output of layered tropical circuit of depth L. Then, there exists an MHTA stack of successive layers which, on every (R>0)n, produces C(L)(x) = F(cid:0)val(x)(cid:1). Proof. We can apply Theorem A.3 to each (i) in succession, feeding the contexts of layer (still in tropical form) as the inputs to layer + 1. Because no Euclidean de-valuation occurs after all MHTA layers, the tropical composition is preserved. Theorem A.4 (Simulation of maxplus Dynamic Programs). Let (S, E) be finite directed acyclic graph with = nodes and weighted edges {wuv}(u,v)E T. For define dv(t + 1) = (cid:77) (cid:16) wuv du(t) (cid:17) , dv(0) = δv,v0 , u: (u,v)E where v0 is the source node. For every finite horizon there exists MHTA of depth and heads per layer such that the token values at layer equal the vector (cid:0)dv(t)(cid:1) for all . vS Proof. If we label the tokens by the vertices of S, at layer we store dv(t) in the value field of token v. To obtain dv(t + 1) let head = whose query token is v. Then, one can apply Lemma A.1 with index set = { (u, v) E}, input scalars xu = du(t) and weights wuv, thereby producing dv(t + 1) as context at token v. Since every head acts on em disjoint query tokens, all are updated in parallel. Repeating for layers unrolls the dynamic program, hence layer realizes the horizon-T value vector."
        },
        {
            "title": "B Comparison between vanilla attention and Tropical attention",
            "content": "In this section, we compare the algorithmic view between vanilla attention and Tropical attention. Algorithm 1 Comparison between vanilla attention and Tropical attention function TROP_ATTENTION(X : d) Q, K, log(cid:0)ReLU(cid:0)linear(cid:0)X(cid:1)(cid:1)(cid:1)_chunk(3) λ Parameter(cid:0)N(cid:1) λ, λ, λ (cid:0)Qbtj + (Q) Qbtd = max (cid:0)Kbtj + (K) (cid:1) (cid:0)Vbtj + (V ) Kbtd = max Vbtd = max dj dj (cid:1) (cid:1) dj i, : Dbij maxd i, : Cbid maxj exp(C) return linear(cid:0)O(cid:1) end function (cid:0)Qbid Kbjd (cid:1) mind (cid:0)Qbid Kbjd (cid:1) (cid:0)Sbij + Vbjd (cid:1) function ATTENTION(X : d) Q, K, linear(cid:0)X(cid:1).chunk(3) (cid:101)A einsum(cid:0)id, jd ij, Q, K(cid:1) softmax(cid:0) (cid:101)A/ einsum(cid:0)ij, jd id, A, V(cid:1) return linear(cid:0)O(cid:1) d, 1(cid:1) end function"
        },
        {
            "title": "C Dataset Details",
            "content": "FloydWarshall Dataset The FloydWarshall dataset presents the all-pairs shortest-path problem as combinatorial regression task. During training, each example is graph of size with nonnegative integer edge weights. We compute the full distance matrix via the FloydWarshall algorithm, replace unreachable pairs with large finite value, and flatten both weights and distances into inputoutput pairs. Out-of-distribution evaluation uses larger graphs, broader weight intervals, and randomly perturbed input. QuickSelect Dataset The QuickSelect dataset frames the search for the k-th smallest element as combinatorial classification challenge. During training, each example presents an unsorted list of integers alongside the fixed order statistic k. The models task is to predict binary mask marking all positions in the original list that hold the k-th smallest value. To mimic real-world uncertainty. For out-of-distribution evaluation, list lengths are increased, value ranges broadened, and randomly perturbed inputs. ThreeSum Decision Dataset The ThreeSum Decision dataset was constructed as classification task. During training, each example begins with list of integers. We then sort the list and pair each element with the target sum , presenting the model with sequence of input tokens [xi, ]. The label is binary decision: 1 if any three distinct elements sum to , and 0 otherwise. Out-of-distribution evaluation tests models on larger list lengths (larger n), wider value ranges each element can be, and randomly perturbed input data. Balanced-Partition Dataset Each example begins with sorted list of integers drawn from fixed interval; we compute the minimum absolute difference between the sums of two complementary subsets on this clean data. Out-of-distribution evaluation increases list lengths, broadens the integer range, and introduces sparse input perturbations. Convex-Hull Dataset Each example starts with set of two-dimensional points sampled uniformly; we compute the convex hull and label each point as 1 if it lies on the hull and 0 otherwise. Out-of-distribution evaluation uses larger point clouds, wider coordinate ranges, and sparse input perturbations. Subset-Sum-Decision Dataset Each sample is sorted list of integers together with target , and the label indicates whether any subset sums exactly to on the clean data. Out-of-distribution evaluation increases list lengths, broadens both value and target ranges, and adds sparse perturbations. 0/1 Knapsack Dataset Each example consists of sorted sequence of (vi, wi) item pairs and capacity C, from which we compute the maximum achievable value under the capacity constraint on pristine inputs. Out-of-distribution evaluation uses longer item lists, larger value and weight ranges, and sparse perturbations. Fractional-Knapsack Dataset Each case comprises list of (vi, wi) items and capacity C, and we compute the optimal fractional-knapsack value on the unperturbed data via greedy ratio-based algorithm. Out-of-distribution evaluation increases item counts, broadens value and weight intervals, and introduces sparse perturbations. 16 Min-Coin-Change Dataset Each training instance presents sorted list of coin denominations and target amount , and we compute the minimum number of coins (or zero if impossible) on this clean currency system. Out-of-distribution evaluation increases the number of denominations, broadens value and target ranges, and applies sparse perturbations. Bin Packing Dataset Each example begins with sorted list of item sizes and global bin capacity; we label it with the number of bins estimated by First-Fit Decreasing heuristic on the pristine sizes. Out-of-distribution evaluation increases item counts, expands size and capacity ranges, and introduces sparse perturbations. Strongly Connected Components (SCC) Dataset Each sample is random directed graph on nodes, symmetrized to undirected, and labeled by computing connected components on the clean adjacency to produce binary mask for node-pair connectivity. Out-of-distribution evaluation increases graph size, broadens edge-inclusion probabilities, and adds sparse perturbations. Training & Evaluation Protocol This appendix complements the experimental setup outlined in Sec. 4. We focus on the conceptual pipeline. The low-level engineering choices (e.g. logging cadence, file formats) are documented in the public code repository. The primary packages utilized in constructing our experiment is Pytorch [39], Pandas and Scipy [32], SciKitLearn [40], and Numpy [15]. The basic workflow is described below: 1. Dataset generation. For the selected combinatorial task we generate input and output pairs using the hyperparameters in Table 3 2. Model instantiation. shallow Transformer encoderconfigured with 1 layer, 2 attention heads and hidden width 64is equipped with one of three attention mechanisms: Vanilla, Tropical, or Adaptive. 3. Optimization. We train for Nepoch epochs using AdamW (103, constant, no warm-up). We use one NVIDIA Tesla V100 GPU to train each model. Models trained with sufficiently large batch size (500) training over 100k samples, took approximately 2.5 minutes to train. For more memory intensive graph models, our training time was approximately 45 minutes given small batch sizes of 16. The objective is chosen per-task: BCE with logits pooled binary tasks, token-wise BCE, mean-squared error regression tasks. Nepoch = 100 except for BIN PACKING and BALANCEDPARTITION, where Nepoch = 1000 4. Evaluation. After training we reload the final checkpoint, generate new test set, and compute (i) mean loss for regression tasks and (ii) F1 for classification tasks on the generated test set. We evaluate our models on in-distribution data (data generated using the same hyperparameters as during training) and on out-of-distribution (OOD) data using the hyperparameters described in Table 3 using the OOD protocol described in Section 4. For Length OOD, all models were trained on sequence length of 8 and we evaluated them at sequence length of 64, with the exception of the graph problems (FloydWarshall and SCC), which were evaluated on sequence length of 16. For Adversarial OOD, each input was perturbed with probability 0.5 with random integer sampled from the tasks adversarial range. Table 3: Training hyperparameters and data ranges for each combinatorial task. Each task was trained with 100k samples, learning rate of 0.0001, input sequence length of 8, and no adversarial perturbations. The ranges in the table are used to draw random integer values for the given parameter within the data generation portion of the training. Dataset Epochs Target Range Weight Range Value Range OOD Value Range Adversarial Range SubsetSumDecision Knapsack FractionalKnapsack MinCoinChange Quickselect BalancedPartition BinPacking ConvexHull ThreeSumDecision FloydWarshall SCC 1 100 100 100 100 100 1000 1000 100 100 100 (1,10) (10,20) (10,20) (10,20) N/A N/A (10,30) N/A (-75,75) N/A N/A (-5,5) (1,10) (1,10) (1,10) (1,10) (1,10) (1,10) (0,10) (-20,20) (1,15) 0.001 (-20,20) (11,21) (11,21) (11,21) (11,21) (11,100) (11,100) (11,21) (-375,375) (16,30) 0.1 (10,30) (10,30) (1,5) (1,5) (1,5) (10,30) (10,30) (1,5) (40,60) (1,10) N/A N/A (1,10) (1,10) N/A N/A N/A N/A N/A N/A N/A N/A 1SCC uses connectivity probability rather than an integer input value, hence the small decimal for Value Ranges and N/A for adversarial range. For adversarial range the connectivity switches with given perturbation probability."
        }
    ],
    "affiliations": [
        "Naval Postgraduate School, Monterey, California",
        "Origins Data Science Lab, Technical University of Munich, Munich, Germany"
    ]
}