{
    "paper_title": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies",
    "authors": [
        "Renke Wang",
        "Zhenyu Zhang",
        "Ying Tai",
        "Jian Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html"
        },
        {
            "title": "Start",
            "content": "DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies Renke Wang1, Zhenyu Zhang2*, Ying Tai2, Jian Yang1* 1PCA Lab, Nanjing University of Science and Technology, China 2Nanjing University, School of Intelligent Science and Technology 6 2 0 2 5 ] . [ 1 7 6 2 2 0 . 1 0 6 2 : r Figure 1. DiffProxy is trained exclusively on synthetic data and achieves robust generalization to real-world scenarios. Our framework accepts diverse prompts (visual and textual), handles difficult poses, generalizes to challenging environments, and supports partial views with flexible view counts. Three key advantages: (i) Annotation bias-freetraining on synthetic data avoids fitting biases from real datasets; (ii) Flexibleadapts to varying view counts, handles partial observations, and works across diverse capture conditions; (iii) Cross-data generalizationachieves strong performance across unseen real-world datasets without requiring real training pairs."
        },
        {
            "title": "Abstract",
            "content": "Human mesh recovery from multi-view images faces fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) *Corresponding authors. PCA Lab, Key Lab of Intelligent Perception and Systems for HighDimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Sci. & Tech. hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertaintyaware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html 1. Introduction Human mesh recovery (HMR) is fundamental problem in computer vision with broad applications ranging from virtual reality to motion analysis. Existing methods predominantly rely on real-world datasets for training. While these datasets [3, 37, 48, 49, 51] capture diverse real-world scenarios, obtaining perfect SMPL/SMPL-X [33, 42] ground truth remains extremely challenging. Since direct 3D mesh capture is infeasible in most in-the-wild settings, annotations are typically derived from optimization-based fitting procedures [2, 5, 34, 40, 41, 59, 61, 63]. These fitting methods, while effective, are known to be sensitive to initialization, prone to local minima, and dependent on the quality of intermediate cues (e.g., 2D keypoints, silhouettes), inevitably introducing systematic biases into the annotations. Consequently, models trained on such data may inherit these fitting artifacts, potentially limiting their accuracy ceiling. Furthermore, the scarcity of annotated multi-view data exacerbates this issue: despite their geometric advantages, multi-view methods [23, 29, 36, 64] often struggle with cross-dataset generalization due to limited training scale compared to the abundance of single-view data. Synthetic data offers compelling alternative: rendering pipelines provide pixel-perfect correspondences, completely eliminating annotation ambiguity. Recent works [4, 40, 55, 57] have demonstrated that large-scale high-quality synthetic datasets can approach or even match real-data performance. However, synthetic data faces an evident domain gap challenge: synthetic scenes exhibit distributional differences from real images in texture, lighting, background complexity, and photorealism. Prior approaches [4, 40] typically address this gap through extensive domain randomization. Nevertheless, fully bridging the synthetic-to-real divide remains challenging, particularly for regression-based methods that directly predict mesh parameters or vertices. This raises the core question: How can we leverage the precise annotations of synthetic data while effectively overcoming the domain gap to achieve robust generalization to real-world images? We draw inspiration from recent successes in repurposing pre-trained diffusion models for dense prediction tasks. Marigold [26] demonstrated that Stable Diffusion can be finetuned for monocular depth estimation, achieving state-ofthe-art zero-shot performance on real datasets without ever having seen real depth maps. Similarly, GenPercept [52] showed that diffusion priors facilitate cross-domain transfer for multiple dense prediction tasks. These works suggest that pre-trained diffusion models, having learned rich visual priors (appearance, lighting, context) from hundreds of millions of real images, can bridge the gap between synthetic training data and real-world generalization when adapted for structured prediction tasks. Building on this insight, we investigate how such principles can be applied to multi-view human mesh recovery. Different from general-scene depth estimation, this task requires human-specific anatomical priors, geometric consistency across viewpoints, and perception of complex details like hands. To this end, we propose DiffProxy, novel diffusionbased framework for multi-view 3D human mesh recovery of single subjects. Instead of using potentially inconsistent real-world ground truth for training, DiffProxy leverages the advantages of large-scale synthetic datasets by utilizing the diffusion-based generative prior. Benefiting from the precise supervision, DiffProxy predicts pixel-aligned human mesh proxies with fine-grained details. Concretely, our framework first finetunes diffusion model on large-scale synthetic multi-view data (108K multi-view samples, 868K images) to generate dense pixel-to-surface correspondences. The model naturally handles both full-body and partial-body inputs: we adopt coarse-to-fine strategy where hand-region crops are used as additional input views to refine finger-level fidelity. In the second stage, we fit the SMPL-X model to these proxies via reprojection optimization. The stochastic nature of diffusion models also allows us to estimate perpixel uncertainty through multiple sampling, which can be used to weight the optimization when needed. Trained on synthetic data without any real image-mesh paired annotations, DiffProxy generalizes robustly to real-world datasets, obtaining state-of-the-art performance on five benchmarks. In summary, our main contributions are: We introduce novel approach that leverages pre-trained diffusion models to generate multi-view consistent dense correspondences, incorporating epipolar attention mechanisms for geometric consistency and training on largescale synthetic data to achieve robust generalization to real-world scenarios; Our framework incorporates hand refinement module for finger-level details prediction, and an uncertainty-guided test-time scaling mechanism to improve the modeling robustness. These designs ensure that the mesh recovery process effectively leverages the generative advantages of the diffusion-based pipeline. Trained exclusively on synthetic data, our method surpasses current state-of-the-art across five real-world benchmarks, with particularly strong performance on challenging scenarios with occlusions and partial views. 2. Related Work Human mesh recovery. Human mesh recovery (HMR) has been long-standing problem in computer vision. Early optimization-based methods, represented by SMPLify [5] and SPIN [27], fit SMPL [33] or SMPL-X [42] by minimizing weighted sums of 2D keypoint reprojection errors and pose priors, often augmented with collision penalties [41] and silhouette consistency. However, these heterogeneous terms require hand-tuned weights and are sensitive to noisy keypoints. More recent learning-based regression approaches, exemplified by Transformer/ViT architectures, directly predict mesh vertices or SMPL parameters from images [7, 14, 31, 58]. With large-scale datasets [4, 8, 24, 30, 56], these methods achieve generalization but predominantly operate on single-view inputs. Multi-view HMR methods [23, 29, 36, 64], while geometrically advantageous, often suffer from limited training data and poor cross-dataset generalization. Recent work explores diffusion priors for HMR in parameter [9, 12, 47], mesh [11], or video [18, 62] space. In contrast, we exploit multi-view constraints with large-scale synthetic training, generating dense correspondences as an intermediate representation. Dense human correspondence. DensePose [16] established pixel-to-surface dense correspondence for humans, enabling subsequent work to leverage these correspondences for mesh recovery. These methods followed two directions: direct regression methods like DecoMR [60] and MeshPose [28] that generate meshes in feed-forward pass, and iterative fitting methods such as HoloPose [15] and DenseRaC [53] that optimize SMPL parameters using detected correspondences. Our work differs in three aspects: (i) multi-view consistency via epipolar attention versus singleview operation; (ii) diffusion-based generation enabling stochastic sampling and uncertainty quantification versus deterministic CNNs; (iii) large-scale synthetic training (17 DensePose-COCO scale) with pixel-perfect annotations versus noisy manual labels. Multi-view diffusion for dense prediction. Stable Diffusion [46] brought powerful generative priors to visual tasks. Marigold [26] and GenPercept [52] demonstrated that diffusion backbones can be adapted for single-view dense prediction while retaining zero-shot generalization. In parallel, enforcing multi-view consistency in diffusion models has attracted attention [6, 2022, 25, 32, 45, 54] for novel view synthesis and 3D generation. Adapter-based methods [20, 22] introduced plug-and-play modules for multiview generation. SPAD [25] injected cross-view interaction via epipolar-constrained attention. However, these focus on image generation rather than dense correspondence prediction. We are the first to leverage multi-view diffusion for dense correspondence in HMR, introducing pixel-wise uncertainty quantification for reliability-weighted fitting. Synthetic data and zero-shot generalization. Recent works [4, 40, 55, 57] demonstrated that high-fidelity synthetic datasets with precise SMPL/SMPL-X annotations can approach or match real-data performance. AGORA [40] fitted SMPL-X to high-quality scans; SynBody [55] scaled to 10,000+ subjects; BEDLAM [4] validated that synthetic training achieves SOTA on real benchmarks. These results show synthetic data can eliminate annotation bottlenecks while providing noise-free supervision. We follow this training exclusively on synthetic multi-view paradigm, data and demonstrating zero-shot generalization to diverse real-world benchmarks [3, 19, 37, 48, 51]. 3. Overview We cast multi-view human mesh reconstruction as diffusion-based generative problem. Our approach leverages pre-trained diffusion model to synthesize multi-view consistent dense correspondences. Trained exclusively on large-scale synthetic multi-view data with pixel-aligned annotations, our model learns human body priors that transfer to real-world images through generative priors, without requiring real image-mesh paired annotations. Our method consists of two stages: (i) Human Proxy Generationproducing dense pixel-to-surface correspondences (Sec. 4.2); (ii) Human Mesh Recoveryfitting SMPL-X through differentiable optimization (Sec. 4.3). 4. Method 4.1. Synthetic Data Preparation We train our model on large-scale synthetic multi-view dataset with pixel-aligned SMPL-X annotations. Synthetic data provides accurate ground-truth correspondences, eliminating annotation noise inherent in real-world datasets. We construct our dataset by rendering 67,650 subjects from BEDLAM [4] with AMASS [35] motion sequences, and 40,841 subjects from SynBody [55] with MPI-3DHP [37] and MoYo [48] pose annotations, totaling 108,491 clothed SMPL-X subjects. Our rendering pipeline incorporates diverse poses [37, 48], realistic occlusions from 7,953 object meshes in Amazon Berkeley Objects [10], diverse hairstyles from PERM [17], HDR lighting from 863 environment maps in Poly Haven [43], and physically-based clothing simulation [4]. For each subject, we sample 8 cameras with randomized parameters and render 10241024 RGB images with corresponding SMPL-X proxies (segmentation and UV coordinates), yielding 108,491 multi-view samples (867,928 images in total). We evaluate on realworld datasets to assess zero-shot generalization (Sec. 5). 4.2. Human Proxy Generation SMPL-X and proxy definition. SMPL-X [42] is parametric 3D human mesh model with parameters Θ = {β, θ, ψ, T}: shape β R10, pose θ, facial expression ψ, and global translation R3. Each vertex carries 2D uv coordinate [0, 1]2 on predefined texture map partitioned by semantic body parts. , Puv We define SMPL-X proxy as 2D dense representation establishing pixel-to-surface correspondences. For view v, the proxy Pv = (Pseg ) consists of segmentation and UV components. To construct the ground-truth proxies for training, we assign each semantic body part unique RGB color to create Pseg , and directly encode the uv coordinates as RGB values for Puv . For hand fitting, we further subdivide the hands into 12 semantic parts: two palms and ten fingers. We use RGB encoding instead of single-channel 3 Figure 2. Method overview. The figure illustrates our complete pipeline from multi-view images to final mesh recovery, which proceeds as follows: (a) given multi-view images and cameras parameters, the proxy generator produces per-view SMPL-X proxies Pv; (b) hand-focused regions inferred from the body proxies are incorporated as additional views for hand refinement; (c) test-time scaling runs stochastic inference attempts, aggregates predictions through median (UV) and majority voting (segmentation), and computes pixel-wise uncertainty to produce weight map Wv that guides fitting; (d) the body is fitted and then refined with hand-specific proxies to recover the final human mesh. We use three conditioning signals: text embeddings ctxt control output modality; T2I-Adapter [38] features cT2I = ET2I(Iv) enforce pixel-level alignment; DINOv2 [39] tokens cDINO = EDINO(Iv) provide pose and appearance priors. We inject ctxt and cDINO via text cross-attention Atext and image cross-attention Aimg, while cT2I is added as residual features. For multi-modal and multi-view consistency, we introduce trainable cross-modality attention Acm concatenating UV and segmentation tokens, and multi-view epipolar attention Aepi enforcing geometric consistency via epipolarconstrained self-attention [25] with Plucker ray embeddings. We train only the attention modules and T2I-Adapter while keeping the UNet and DINOv2 frozen. We train with standard diffusion objective. Given groundtruth proxy v, we encode it to latent z0 = E(P v), sample timestep and noise ϵ (0, I), and optimize: (cid:3) , (cid:2)ϵ ϵϕ(zt, t, c)2 (2) 2 Ldiff = Ez0,ϵ,t,c αtz0 + where zt = 1 αtϵ and = {ctxt, cT2I, cDINO}. We train with fixed budget of = 4 views per sample, where 24 views are full-body and the remaining slots are filled with hand-region crops (left/right randomly selected) from the same camera viewpoints. This mixed-view training strategy enables the model to handle both body reconstruction and hand refinement without increasing computational cost or modifying the architecture. At inference, we denoise random latent zT (0, I) and decode via VAE decoder to obtain Pv. The model generalizes to different view counts at inference without any fine-tuning. Hand refinement. Hands occupy few pixels and are prone to low-resolution artifacts. We adopt two-pass strategy: first inferring full-body proxies, then using Pseg to localize hand regions and create enlarged crops. In the second pass, we Figure 3. Diffusion-based proxy generator architecture. Our model is built on Stable Diffusion 2.1 with frozen UNet backbone, equipped with three conditioning signals (ctxt, cT2I, cDINO) and four trainable attention modules (Atext, Aimg, Acm, Aepi) for multiview consistent proxy generation. representations because the pre-trained VAE decoder from SD 2.1 is optimized for three-channel outputs, and empirically we find RGB encoding achieves better reconstruction quality. The UV coordinates on mesh faces are computed via barycentric interpolation: for any point on face with vertices a, b, having uv coordinates ua, ub, uc and barycentric weights (λa, λb, λc): u(p) = λa ua + λb ub + λc uc. (1) and Puv . We render the textured mesh with perspective projection to obtain the proxy images Pseg Diffusion-based proxy generator. Our generator Gϕ is built on Stable Diffusion 2.1 [46] with frozen UNet backbone. Given multi-view images {Iv}N v=1 (N 1) and camera parameters {Cv} = {(Kv, Rv, tv)}, the model prev ) R2562563 dicts SMPL-X proxies Pv = (Pseg encoding body part labels and surface coordinates. , Puv treat hand crops as additional views, leveraging cross-view attention to produce refined hand proxies. This coarse-tofine strategy significantly improves finger fidelity without modifying the network architecture. Test-time scaling & uncertainty. Diffusion models are stochastic and produce predictions with varying reliability across regions. Certain areas are more challenging to predict, such as visually ambiguous regions, self-occluded body parts, or fine-grained structures with higher prediction variance. To quantify and mitigate this uncertainty, we leverage the stochastic nature of diffusion: by drawing multiple samples and measuring their disagreement, we identify unreliable regions and down-weight them during optimization. At test time, we draw stochastic samples {Pk,v}K k=1 from the proxy generator per view. For UV aggregation, we compute the pixel-wise median across samples to obtain robust estimate: ˆPuv (x) = mediank=1..K (cid:2) Puv k,v(x) (cid:3), (3) and quantify uncertainty using the channel-wise sample variance averaged over = 3 RGB channels: Uuv (x) = 1 C (cid:88) c=1 Vark=1..K (cid:2) Puv,(c) k,v (x) (cid:3). (4) For segmentation, we first quantize each sample Pseg k,v to the nearest color in predefined palette Pview (restricted to body or hand subsets depending on the view type). We then apply pixel-wise majority voting across samples to obtain the aggregated segmentation ˆPseg . Let nmax(x) denote the maximum vote count at pixel among all labels. We define majority-agreement uncertainty as: uniform dense correspondences. Each foreground pixel is assigned semantic parts and UV coordinates on the SMPL-X surface, turning fitting into single 2D reprojection problem. Given proxies {Pv} from all views, we compute the reprojection loss over all foreground pixels. Let fg(v) denote foreground pixels in view v. For each pixel fg(v), we extract its semantic part label and UV coordinate from the proxy Pv(x). We then locate the corresponding mesh face via the part label and use barycentric interpolation to obtain the 3D point on the SMPL-X surface parameterized by Θ. This 3D point is projected back to the image plane using camera parameters Cv, and the pixel-space L2 distance d(x) between the projected location and the original pixel serves as the reprojection error (see Algorithm 1 in supplementary for full details). The reprojection loss is: Lreproj = (cid:88) (cid:88) d(x)2. xfg(v) (7) Uncertainty weighting. Test-time scaling provides perpixel uncertainty estimates Uuv (x), from which we derive the weight map Wv (Eq. 6). We weight each pixels contribution by its reliability: (x) and Useg Lreproj = (cid:88) (cid:88) Wv(x) d(x)2. (8) xfg(v) This weighting down-weights ambiguous pixels while retaining dense constraints. Optimization. We optimize body pose in VPoser [13] latent space, hand pose in MANO [44] PCA space, and shape β without explicit regularization. We minimize Lreproj (Eq. 8) using L-BFGS with stage-wise parameter optimization. See Sec. 5 for implementation details. Useg (x) = 1, (cid:18) 2 1 (cid:19) nmax(x) nmax(x) 2 , , otherwise. (5) 5. Experiments 5.1. Implementation Details This formulation assigns high uncertainty when no label achieves majority consensus, and decreases linearly as the winning labels vote share increases beyond 50%. The uncertainties modulate the fitting via per-view weight map Wv R256256, where each pixel is assigned reliability weight: Wv(x) = (cid:0)1 Uuv (x)(cid:1) (cid:0)1 Useg (x)(cid:1). (6) This strategy provides computeaccuracy trade-off through without test-time adaptation of network weights. 4.3. Human Mesh Recovery Unlike prior methods relying on heterogeneous multi-modal cues (e.g., 2D/3D keypoints, silhouettes) with hand-tuned loss weights, we use the multi-view SMPL-X proxies as 5 Proxy generator training. We trained with 4 views per sample using random full-body/hand crops and bbox augmentation. From SD-2.1 weights, we optimized the attention modules (Atext, Aimg, Acm, Aepi) and T2I-Adapter ET2I while freezing the UNet backbone and DINOv2 EDINO. Training used batch size 2, Adam optimizer, learning rate 5 105, for 30 epochs on 4 RTX 5090 GPUs (36 hours). VAE decoder refinement. Stable Diffusions pre-trained VAE decoder may introduce quantization artifacts for proxy representations that require high numerical precision. We fine-tuned with learning rate 1 106, batch size 8, for 100K iterations (4 hours on 4 RTX 5090). Inference. We generate proxies Pv for 12 views by default: 4 full-body views plus left/right hand crops for each. Inference involves two passes: first obtaining hand crop locations from full-body proxies (3s), then generating all 12 proxies Table 1. Quantitative comparison on five real-world datasets. indicates the method was trained on that specific dataset. Method SMPLest-X [58] Human3R [7] U-HMR [29] MUC [64] HeatFormer [36] EasyMoCap [1] Ours Method SMPLest-X [58] Human3R [7] U-HMR [29] MUC [64] HeatFormer [36] EasyMoCap [1] Ours rich PA-MPJPE MPJPE PA-MPVPE MPVPE PA-MPJPE MPJPE PA-MPVPE MPVPE PA-MPJPE MPJPE PA-MPVPE MPVPE behave 3dhp 33.7 57.0 69.1 37.9 34.8 47.6 33.6 51.6 106.4 147.8 - 59.8 85.5 42.0 48.8 73.6 81.9 47.9 42.8 59.6 45.0 67.1 129.2 169.9 - 66.4 93.3 51. 26.5 46.2 66.1 33.2 44.9 30.4 23.5 42.8 80.1 140.8 - 88.8 39.2 29.6 33.6 56.3 82.9 40.5 63.1 42.3 27.6 51.7 94.1 168.7 - 106.7 50.0 31.5 29.3 36.6 45.8 25.8 33.8 26.4 22.7 49.5 91.3 118.1 - 67.2 52.9 32.0 4ddress-partial 43.0 50.3 53.1 37.1 47.2 40.1 32.7 65.2 108.0 134.2 - 76.8 63.1 40.3 4ddress PA-MPJPE MPJPE PA-MPVPE MPVPE PA-MPJPE MPJPE PA-MPVPE MPVPE PA-MPJPE MPJPE PA-MPVPE MPVPE moyo 64.0 94.2 110.3 82.5 85.7 44.1 36.2 101.2 149.7 234.5 - 149.5 65.6 29. 77.0 111.0 131.2 73.2 106.8 60.9 51.9 121.1 177.7 274.6 - 171.5 76.5 56.2 35.2 30.5 41.6 28.0 43.8 20.9 17.3 53.8 56.4 77.4 - 69.9 27.8 21.4 52.4 43.6 95.7 39.5 64.5 32.7 24.4 72.0 71.5 53.0 - 88.8 39.0 26. 75.4 42.0 66.7 62.6 140.1 79.6 22.7 106.7 76.0 146.9 - 283.5 447.1 27.2 117.3 58.5 86.8 97.6 174.8 120.7 31.5 147.6 93.0 185.0 - 318.6 466.9 34.2 (10s). With test-time scaling over samples, runtime scales to 10 seconds (default = 5). Mesh fitting. We use stage-wise L-BFGS fitting (learning rate 1 102), optimizing SMPL-X parameters Θ = {β, θ, T} and global scale parameter in stages: global orientation and translation, global scale, body pose, body pose with shape, hand global rotations, and hand articulations. We do not optimize facial expression ψ as our focus is on body and hand reconstruction. We advance to the next stage when relative loss decrease falls below 1%. Fitting converges in 100 iterations over 60 seconds per subject. 5.2. Datasets and Baselines Datasets. We evaluate on five real-world datasets: 3DHP [37], BEHAVE [3], RICH [19], MoYo [48], and 4DDRESS [51], covering studio capture, human-object interaction, outdoor scenes, challenging poses, and loose clothing. We test on 4D-DRESS with random crops (4D-DRESS partial) to evaluate robustness to partial observations. Baselines. We compare against: SMPLest-X [58], singleview model trained on large-scale data; Human3R [7] extending CUT3R for joint human-scene recovery; U-HMR [29] with decoupled camera pose and body estimation; MUC [64] fusing multi-view predictions without calibration; HeatFormer [36] using neural optimization with heatmaps; and EasyMoCap [1], an optimization-based fitting framework. 5.3. Quantitative Results We sample 100 scenes per dataset with 4 full-body views as input (12 views total including hand crops) and report MPJPE, MPVPE, and their Procrustes-aligned variants in millimeters. For single-view baselines, we average errors across views after root alignment. As shown in Table 1, our method achieves the best performance on most metrics across all datasets, demonstrating strong generalization to diverse scenarios including complex poses, partial visibility, varied lighting, and loose clothing. Table 2. Impact of hand refinement. Metrics computed on hand vertices/joints only. Method w/o hand refinement w/ hand refinement (Ours) PA-MPJPE MPJPE PA-MPVPE MPVPE 18.1 17.0 55.8 37.5 17.7 16.6 56.2 34.3 5.4. Qualitative Results Fig. 4 presents qualitative comparisons, demonstrating three key advantages: (i) Free from annotation biasesRealdata trained methods like SMPLest-X, U-HMR, and HeatFormer exhibit similar head tilting artifacts inherited from 3DHP annotations. Synthetic training with pixel-perfect annotations avoids such biases. (ii) Cross-data generalizationAmong synthetic-trained methods, our image-toimage formulation leverages diffusion priors for robust generalization, while direct parameter prediction approaches like Human3R suffer from larger domain gaps. (iii) Flexible multi-view reconstructionOur method handles varying view counts and partial observations robustly, accurately detecting occluded body parts where other methods fail. 5.5. Ablation Studies We systematically analyze key components: hand refinement, input view count, test-time scaling, camera-free inference, and network modules. Hand refinement. Table 2 compares the performance with and without hand refinement on the 4D-DRESS dataset. Hand refinement generates high-resolution crops for left and right hands in addition to full-body views. As shown in Fig. 5, the comparison demonstrates that refinement produces hand proxies that are visually more aligned with the hand regions, with more accurate finger poses, reduced UV coordinate discontinuities, and less part ambiguity. This visual improvement in hand-image alignment translates to better mesh fitting quality, especially for finger articulations. Number of input views. Our method supports flexible view counts without retraining, and performance generally im6 Figure 4. Qualitative comparison with baseline methods. Our method demonstrates: (i) bias-free predictions avoiding real-data annotation artifacts; (ii) strong generalization despite synthetic-only training; (iii) robustness to partial observations. Table 3. More views lead to better performance. #Views 1 view 2 views 4 views 8 views PA-MPJPE MPJPE PA-MPVPE MPVPE 117.8 53.5 36.2 24.5 876.6 59.7 29.1 31.8 152.9 77.9 51.9 38.7 897.0 81.9 56.2 44.1 proves as views increase, benefiting from multi-view geometric constraints and epipolar attention. As shown in Table 3 and Fig. 6, evaluated on the MoYo dataset, singleview inference suffers from depth ambiguity. Two views enable triangulation but may fail on challenging poses. Four views provide sufficient constraints for correct pose recovery, while eight views further refine details. Test-time scaling and uncertainty weighting. Test-time scaling samples multiple proxy candidates to estimate pixel-wise uncertainty maps for computing reliability weight maps Wv (Eq. 6). Fig. 7 illustrates effectiveness: when the proxy incorrectly predicts the left leg as right leg (first row), the uncertainty map Useg assigns high uncertainty to the misclassified region. During fitting, our method Figure 5. Qualitative comparison of hand refinement. Hand refinement improves fitting quality and produces accurate finger details. down-weights these unreliable pixels and relies on confident predictions from other views, successfully recovering the correct configuration. As shown in Table 4 on the BEHAVE 7 Table 6. Contributions of network modules. Each row shows results with one component removed; the last row shows the full model. Configuration w/o DINOv2 w/o T2I-Adapter w/o Atext w/o Aepi w/o Acm w/o uncertainty weighting Full model (Ours) PA-MPJPE MPJPE PA-MPVPE MPVPE 31.1 27.9 26.1 24.6 25.1 23.1 22.7 38.2 41.2 33.0 32.4 31.4 32.4 32.0 46.7 39.9 53.1 37.7 37.6 33.3 32.7 52.7 53.2 56.1 43.7 43.2 40.7 40.3 Figure 6. Our method benefits from increasing view counts, with performance improving from single-view to multi-view. Table 4. Larger benefits reconstruction quality. DINOv2, T2I-Adapter, attention modules (Atext, Aepi, Acm), and uncertainty weighting. Each module contributes to performance, with the full model achieving the best balance. Sampling count PA-MPJPE MPJPE PA-MPVPE MPVPE = 1 = 3 = 5 (default) = 10 41.2 40.3 40.3 40.2 32.7 32.3 32.0 31.9 23.4 22.7 22.7 22.5 34.1 32.5 32.7 32. Figure 7. Test-time scaling with uncertainty weighting improves robustness by down-weighting unreliable predictions and recovering correct poses from erroneous proxy outputs. Table 5. Performance without camera calibration. Configuration w/ ground-truth cameras w/o ground-truth cameras PA-MPJPE PA-MPVPE 22.7 24.7 32.7 36. dataset, increasing improves performance. We use = 5 as default for favorable accuracy-compute trade-off. Inference without camera calibration. While our main results assume calibrated cameras, real-world scenarios often lack ground-truth camera parameters. We test camera-free variant on BEHAVE: we predict camera parameters using VGGT [50], then generate proxies with these predicted cameras. During mesh fitting, we jointly optimize camera parameters alongside body pose and shape to compensate for prediction inaccuracy. As shown in Table 5, our method achieves competitive performance with only moderate degradation, demonstrating practical applicability. We report only Procrustes-aligned metrics, as predicted cameras define coordinate frame differing from ground-truth by an unknown similarity transformation. Network module contributions. Table 6 ablates individual network modules on BEHAVE. We independently remove 8 6. Limitation and Future Works While DiffProxy achieves state-of-the-art performance, several limitations remain. Inference speed: The diffusion generator requires 50 denoising steps and fitting takes around 100 iterations, resulting in inference time of approximately 120 seconds. Future work could explore consistency models or distillation to accelerate generation. Multi-view requirement: Our method requires multiple views for reliable results, as single-view performance suffers from depth ambiguity. Future work could explore extending the framework to single-view scenarios. Single-subject scenarios: Our method focuses on single-subject reconstruction. Extension to multi-person scenarios is straightforward by incorporating per-instance segmentation as an additional modality, with the primary challenge being cross-view identity association. 7. Conclusion We introduced DiffProxy, diffusion-based framework for multi-view human mesh recovery that achieves zero-shot generalization by training on synthetic data. By adapting pretrained Stable Diffusion with epipolar attention and incorporating hand refinement and test-time scaling, our method produces multi-view consistent dense correspondences for accurate mesh fitting. DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating that diffusion models can transfer geometric supervision from synthetic to real-world scenarios. This paradigm opens new possibilities for structured prediction tasks where obtaining real-world annotations is challenging. 8. Acknowledgments This work was supported by the National Science Fund of China under Grant Nos. U24A20330, 62361166670, and 62376121, Basic Research Program of Jiangsu under Grant No. BK20251999, Gusu Innovation Leading Talent Program under Grant No. ZXL2025319, and Jiangsu Provincial Science & Technology Major Project under Grant No. BG2024042."
        },
        {
            "title": "References",
            "content": "[1] Easymocap - make human motion capture easier. Github, 2021. 6 [2] Bharat Bhatnagar, Ilya Petrov, and Xianghui Xie. Rvh mesh registration. https://github.com/bharatb7/RVH_ Mesh_Registration, 2022. GitHub repository. 2 [3] Bharat Lal Bhatnagar, Xianghui Xie, Ilya Petrov, Cristian Sminchisescu, Christian Theobalt, and Gerard Pons-Moll. Behave: Dataset and method for tracking human object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, 2022. 2, 3, 6 [4] Michael Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. Bedlam: synthetic dataset of bodies exhibitIn Proceedings of ing detailed lifelike animated motion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 87268737, 2023. 2, 3 [5] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael Black. Keep it smpl: Automatic estimation of 3d human pose and shape from single image. In European Conference on Computer Vision, pages 561578. Springer, 2016. 2 [6] Zeyu Cai, Ziyang Li, Xiaoben Li, Boqian Li, Zeyu Wang, Zhenyu Zhang, and Yuliang Xiu. Up2you: Fast reconstruction of yourself from unconstrained photo collections. arXiv preprint arXiv:2509.24817, 2025. [7] Yue Chen, Xingyu Chen, Yuxuan Xue, Anpei Chen, Yuliang Xiu, and Gerard Pons-Moll. Human3r: Everyone everywhere all at once. arXiv preprint arXiv:2510.06219, 2025. 2, 6 [8] Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu Chen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, et al. Dna-rendering: diverse neural actor repository for high-fidelity human-centric rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1998219993, 2023. 3 [9] Hanbyel Cho and Junmo Kim. Generative approach for probabilistic human mesh recovery using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41834188, 2023. 3 [10] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2112621136, 2022. 3 [11] Lin Geng Foo, Jia Gong, Hossein Rahmani, and Jun Liu. Distribution-aligned diffusion for human mesh recovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 92219232, 2023. 3 [12] Jing Gao, Ce Zheng, Laszlo Jeni, and Zackory Erickson. Disrt-in-bed: Diffusion-based sim-to-real transfer framework for in-bed human mesh recovery. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1829 1838, 2025. 3 [13] Ghorbani, Bolkart, Osman, Tzionas, Pavlakos, Choutas, Black, Bolkart, and Tzionas. Vposer: Variational human pose prior for body inverse kinematics. arXiv preprint arXiv:1904.05866, 2019. 5 [14] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1478314794, 2023. 2 [15] Riza Alp Guler and Iasonas Kokkinos. Holopose: Holistic 3d human reconstruction in-the-wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1088410894, 2019. 3 [16] Rıza Alp Guler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 72977306, 2018. 3 [17] Chengan He, Xin Sun, Zhixin Shu, Fujun Luan, Soren Pirk, Jorge Alejandro Amador Herrera, Dominik Michels, Tuanfeng Wang, Meng Zhang, Holly Rushmeier, and Yi Zhou. Perm: parametric representation for multi-style 3d hair modeling. In International Conference on Learning Representations, 2025. 3 [18] Jaewoo Heo, Kuan-Chieh Wang, Karen Liu, and Serena Yeung-Levy. Motion diffusion-guided 3d global hmr from dynamic camera. arXiv preprint arXiv:2411.10582, 2024. 3 [19] Chun-Hao P. Huang, Hongwei Yi, Markus Hoschle, Matvey Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel Scharstein, and Michael J. Black. Capturing and inferring In Proceedings of dense full-body human-scene contact. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1327413285, 2022. 3, [20] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. arXiv preprint arXiv:2412.03632, 2024. 3 [21] Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding Liang, Yu Qiao, Bo Dai, et al. Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 97849794, 2024. [22] Yoonwoo Jeong, Jinwoo Lee, Chiheon Kim, Minsu Cho, and Doyup Lee. Nvs-adapter: Plug-and-play novel view synthesis from single image. In European Conference on Computer Vision, pages 449466. Springer, 2024. 3 [23] Kai Jia, Hongwen Zhang, Liang An, and Yebin Liu. Delving deep into pixel alignment feature for accurate multi-view human mesh recovery. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 989997, 2023. 2, 3 [24] Angjoo Kanazawa, Jason Zhang, Panna Felsen, and Jitendra Malik. Learning 3d human dynamics from video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56145623, 2019. 3 [25] Yash Kant, Aliaksandr Siarohin, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, and Igor Gilitschenski. Spad: Spatially aware multi-view diffusers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1002610038, 2024. 3, 4 [26] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. 2, 3 [27] Nikos Kolotouros, Georgios Pavlakos, Michael Black, and Kostas Daniilidis. Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22522261, 2019. 2 [28] Eric-Tuan Lˆe, Antonis Kakolyris, Petros Koutras, Himmy Tam, Efstratios Skordos, George Papandreou, Riza Alp Guler, and Iasonas Kokkinos. Meshpose: Unifying densepose and 3d body mesh reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24052414, 2024. 3 [29] Xiaoben Li, Mancheng Meng, Ziyan Wu, Terrence Chen, Fan Yang, and Dinggang Shen. Human mesh recovery from arbitrary multi-view images. arXiv preprint arXiv:2403.12434, 2024. 2, 3, 6 [30] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: largescale 3d expressive whole-body human motion dataset. In Advances in Neural Information Processing Systems, pages 2526825280, 2023. 3 [31] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human pose and mesh reconstruction with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19541963, 2021. [32] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. In ICLR, 2024. 3 [33] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1248:16, 2015. 2 [34] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J. Black. Learning to dress 3D people in generative clothing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2 [35] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes. In International Conference on Computer Vision, pages 54425451, 2019. 3 [36] Yuto Matsubara and Ko Nishino. Heatformer: neural optimizer for multiview human mesh recovery. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 64156424, 2025. 2, 3, 6 [37] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian Theobalt. Monocular 3d human pose estimation in the wild using imIn 3D Vision (3DV), 2017 Fifth proved cnn supervision. International Conference on. IEEE, 2017. 2, 3, [38] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. 4 [39] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 4 [40] Priyanka Patel, Chun-Hao Huang, Joachim Tesch, David Hoffmann, Shashank Tripathi, and Michael Black. Agora: Avatars in geography optimized for regression analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1346813478, 2021. 2, 3 [41] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, In Proceedings of face, and body from single image. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1097510985, 2019. 2 [42] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, In Proceedings of face, and body from single image. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1097510985, 2019. 2, [43] Poly Haven. Poly haven: The public 3d asset library. https: //polyhaven.com/, 2024. Accessed 2025-11-12. 3 [44] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bodies together. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia), 36(6), 2017. 5 [46] Stability AI. [45] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 3 Stable diffusion v2.1 release. https : //stability.ai/news/stablediffusion21release7-dec-2022, 2022. Accessed 2025-10-21. 3, 4 [47] Anastasis Stathopoulos, Ligong Han, and Dimitris Metaxas. Score-guided diffusion for 3d human recovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 906915, 2024. 3 [48] Shashank Tripathi, Lea Muller, Chun-Hao P. Huang, Taheri Omid, Michael J. Black, and Dimitrios Tzionas. 3D human pose estimation via intuitive physics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47134725, 2023. 2, 3, 6 [49] Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and moving camera. In European Conference on Computer Vision (ECCV), 2018. 2 [50] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: 10 [62] Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, and Chen Chen. Diffmesh: motion-aware diffusion framework for human mesh recovery from videos. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 48914901. IEEE, 2025. 3 [63] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and Yebin Liu. DeepHuman: 3D human reconstruction from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019. 2 [64] Yitao Zhu, Sheng Wang, Mengjie Xu, Zixu Zhuang, Zhixin Wang, Kaidong Wang, Han Zhang, and Qian Wang. Muc: Mixture of uncalibrated cameras for robust 3d human body reconstruction. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1104011048, 2025. 2, 3, 6 Visual geometry grounded transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 8 [51] Wenbo Wang, Hsuan-I Ho, Chen Guo, Boxiang Rong, Artur Grigorev, Jie Song, Juan Jose Zarate, and Otmar Hilliges. 4d-dress: 4d dataset of real-world human clothing with semantic annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 3, [52] Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. What matters when repurposing diffusion models for general dense perception tasks? arXiv preprint arXiv:2403.06090, 2024. 2, 3 [53] Yuanlu Xu, Song-Chun Zhu, and Tony Tung. Denserac: Joint 3d pose and shape estimation by dense render-and-compare. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 77607770, 2019. 3 [54] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. Consistnet: Enforcing 3d consistency for multi-view images diffusion. arXiv, 2023. 3 [55] Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei, Bo Dai, et al. Synbody: Synthetic dataset with layered human models for 3d human perception and modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2028220292, 2023. 2, 3 [56] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael Black. Generating holistic 3d human motion from speech. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 469480, 2023. 3 [57] Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, Lei Yang, and Ziwei Liu. Whac: Worldgrounded humans and cameras. In European Conference on Computer Vision, pages 2037. Springer, 2024. 2, [58] Wanqi Yin, Zhongang Cai, Ruisi Wang, Ailing Zeng, Chen Wei, Qingping Sun, Haiyi Mei, Yanjun Wang, Hui En Pang, Mingyuan Zhang, et al. Smplest-x: Ultimate scaling for expressive human pose and shape estimation. arXiv preprint arXiv:2501.09782, 2025. 2, 6 [59] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. Function4D: Real-time human volumetric capture from very sparse consumer RGBD sensors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 2 [60] Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, and Xiaogang Wang. 3d human mesh regression with dense correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 70547063, 2020. 3 [61] Chao Zhang, Sergi Pujades, Michael J. Black, and Gerard Pons-Moll. Detailed, accurate, human shape estimation from clothed 3D scan sequences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2017."
        }
    ],
    "affiliations": [
        "Nanjing University, School of Intelligent Science and Technology",
        "PCA Lab, Nanjing University of Science and Technology, China"
    ]
}