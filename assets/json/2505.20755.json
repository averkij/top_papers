{
    "paper_title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction",
    "authors": [
        "Yifei Wang",
        "Weimin Bai",
        "Colin Zhang",
        "Debing Zhang",
        "Weijian Luo",
        "He Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we unify more than 10 existing one-step diffusion distillation approaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a theory-driven framework which we name the \\textbf{\\emph{Uni-Instruct}}. Uni-Instruct is motivated by our proposed diffusion expansion theory of the $f$-divergence family. Then we introduce key theories that overcome the intractability issue of the original expanded $f$-divergence, resulting in an equivalent yet tractable loss that effectively trains one-step diffusion models by minimizing the expanded $f$-divergence family. The novel unification introduced by Uni-Instruct not only offers new theoretical contributions that help understand existing approaches from a high-level perspective but also leads to state-of-the-art one-step diffusion generation performances. On the CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet Inception Distance (FID) values of \\textbf{\\emph{1.46}} for unconditional generation and \\textbf{\\emph{1.38}} for conditional generation. On the ImageNet-$64\\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA one-step generation FID of \\textbf{\\emph{1.02}}, which outperforms its 79-step teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35). We also apply Uni-Instruct on broader tasks like text-to-3D generation. For text-to-3D generation, Uni-Instruct gives decent results, which slightly outperforms previous methods, such as SDS and VSD, in terms of both generation quality and diversity. Both the solid theoretical and empirical contributions of Uni-Instruct will potentially help future studies on one-step diffusion distillation and knowledge transferring of diffusion models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 5 7 0 2 . 5 0 5 2 : r Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction Yifei Wang1,5 Weimin Bai1,2,3 Colin Zhang4 Debing Zhang4 Weijian Luo4 He Sun1,2,3 1College of Future Technology, Peking University 2National Biomedical Imaging Center, Peking University 3Academy for Advanced Interdisciplinary Studies, Peking University 4 Xiaohongshu Inc 5 Yuanpei College, Peking University"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we unify more than 10 existing one-step diffusion distillation approaches, such as Diff-Instruct, DMD, SIM, SiD, -distill, etc, inside theorydriven framework which we name the Uni-Instruct. Uni-Instruct is motivated by our proposed diffusion expansion theory of the -divergence family. Then we introduce key theories that overcome the intractability issue of the original expanded -divergence, resulting in an equivalent yet tractable loss that effectively trains one-step diffusion models by minimizing the expanded -divergence family. The novel unification introduced by Uni-Instruct not only offers new theoretical contributions that help understand existing approaches from high-level perspective but also leads to state-of-the-art one-step diffusion generation performances. On the CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet Inception Distance (FID) values of 1.46 for unconditional generation and 1.38 for conditional generation. On the ImageNet-64 64 generation benchmark, UniInstruct achieves new SoTA one-step generation FID of 1.02, which outperforms its 79-step teacher diffusion with significant improvement margin of 1.33 (1.02 vs 2.35). We also apply Uni-Instruct on broader tasks like text-to-3D generation. For text-to-3D generation, Uni-Instruct gives decent results, which slightly outperforms previous methods, such as SDS and VSD, in terms of both generation quality and diversity. Both the solid theoretical and empirical contributions of Uni-Instruct will potentially help future studies on one-step diffusion distillation and knowledge transferring of diffusion models."
        },
        {
            "title": "Introduction",
            "content": "One-step diffusion models, also known as one-step generators [31, 33], have been recognized as stand-alone family of generative models that reach the leading generative performances in wide range of applications, including benchmarking image generation [33, 34, 66, 73, 72, 18, 63], text-toimage generation [32, 35, 65, 66, 34, 71, 17], text-to-video generation [1, 67], image-editing [14], and numerous others [37, 44, 59]. Currently, the mainstream of training the one-step diffusion model is through proper distillation approaches that minimize divergences between distributions of the one-step model and some teacher diffusion models. For instance, Diff-Instruct[33] was probably the first work that introduced one-step diffusion models by minimizing the Kullback-Leibler divergence. DMD [66] improves the DiffInstruct by introducing an additional regression loss. Score-identity Distillation (SiD) [73] studies the one-step diffusion distillation by minimizing the Fisher divergence, but without the proof of gradient Correspondence to luoweijian@xiaohongshu.com. Preprint. Under review. Figure 1: Left: selected FID scores of different models on ImageNet-64 64 conditional generation benchmark. Right: Conception overview of Uni-Instruct. The Uni-Instruct unifies more than 10 existing diffusion distillation methods in wide range of applications. Please check Table 6 for details. equivalence of the loss function. Later, Score Implicit Matching (SIM)[34] introduced complete proof of the gradient equivalence of losses that minimizes the general score-based divergence family, including the Fisher divergence as special case. f-distill [63] and SiDA [72] recently generalized the Diff-Instruct and the SiD to the integral -divergence and auxiliary GAN losses, resulting in performance improvements on image generation benchmarks. Other approaches have also elaborated on the one-step diffusion models on wide range of applications through the lens of divergence minimization [17, 72, 71, 65, 32, 35, 18]. Provided that existing one-step diffusion models have achieved impressive performances, with some of them even outperforming their multi-step teacher diffusions, existing training approaches seem conceptually separated into two lines: (1). Diff-Instruct[33], and its variants like DMD[66], tackle the Integral Kullback-Leibler Divergence, while f-distill[63] unifies the IKL as special case of Integral f-divergence. These KL and - divergence based distillation approaches have the advantage of fast convergence, but suffer from mode-collapse issues and sub-optimal performances; (2). Score Implicit Matching (SIM[34]) proves solid theoretical equivalence of score-based divergence minimization, which unifies the SiD[73] and Fisher divergences as special cases. Though these general score-based divergences minimization has shown surprising generation performance, they may suffer from slow convergence issues and sub-optimal fidelity. Till now, it seems that the KL-based and Score-based divergence minimization approaches are pretty parallel in theory. Therefore, we are strongly motivated to answer an interesting yet important research question: Can we unify KL-based and Score-based approaches in unified theoretical framework? If we can, would the unified approach lead to better one-step diffusion models? In this paper, we provide complete answer to the mentioned question. We successfully built unified theoretical framework based on novel diffusion expansion of the -divergence family. Though the original expanded -divergence family is not tractable to optimize, we introduced new theorems that lead to tractable yet equivalent losses, therefore making Uni-Instruct an executable training method. In this way, we are able to unify more than 10 existing diffusion distillation methods across wide range of applications via our proposed Uni-Instruct. The methods that have been unified by Uni-Instruct include both KL-divergence-based methods (such as Diff-Instruct[33], DMD[66], and - distill[63]) and general score-divergence-based methods (such as Score Implicit Matching (SIM[34]), SiD[73], and SiDA[72]), as is shown in Table 6. Such novel unification of existing one-step diffusion models marks the uniqueness of Uni-Instruct, which brings new perspectives in understanding and connecting different one-step diffusion models. Besides the solid theoretical contributions, UniInstruct also leads to new State-of-the-art one-step image generation performances on competitive image generation benchmarks: it achieved record-breaking FID (Fr√©chet Inception Distance) [15] value of 1.02 on the ImageNet64 64 conditional generation task. This score outperforms its 79-step teacher diffusion with significant improvement margin of 1.33 (1.02 vs 2.35). Uni-Instruct also leads 2 to new state-of-the-art one-step FIDs of 1.46 on CIFAR10 unconditional generation task and 1.36 on CIFAR10 unconditional generation task, significantly outperforming previous one-step models such as f-distill, SiDA, SIM, SiD, DMD, and Diff-Instruct. It also outperforms competitive few-step generative models, including consistency models [12, 52, 28], moment matching distillation models [47], inductive models [70], and many others [61]. Besides the one-step generation benchmark, we are inspired by DreamFusion [44], ProlificDreamer [59], and Diff-Instruct [33]. In Section 5.3, we also successfully apply Uni-Instruct as knowledge transferring approach for text-to-3D generation applications, resulting in robust, diverse, and highfidelity 3D contents which are slightly better than ProlificDreamer in quality and diversity. We summarize the theoretical and practical contributions in this paper as follows: Unified Theoretical Framework: We introduced unified theoretical framework named Uni-Instruct together with novel -divergence expansion theorem. Uni-Instruct is able to unify more than 10 existing one-step diffusion distillation approaches, bringing new perspectives to understanding one-step diffusion models. Tractable and Flexible Training Objective: We introduce novel theoretical tools, such as gradient equivalence theorems, and derived tractable yet equivalent losses for Uni-Instruct. This leads to both flexible training objectives and new tools for one-step diffusion models. New SoTA Practical Performances: Uni-Instruct achieved new state-of-the-art generation performances (measured in FID) on CIFAR10 (a one-step FID of 1.36) and ImageNet6464 (a one-step FID of 1.02) benchmarks. We also successfully applied Uni-Instruct on the text-to-3D generation task, resulting in plausible and diverse 3D generation results."
        },
        {
            "title": "2 Preliminary",
            "content": "2.1 One-step Diffusion Models Diffusion Models. Assume we observe data from the underlying distribution qd(x). The goal of generative modeling is to train models to generate new samples qd(x). The forward diffusion process of DM transforms any initial distribution q0 = qd towards some simple noise distribution, dxt = (xt, t)dt + g(t)dwt, (2.1) where is pre-defined drift function, g(t) is pre-defined scalar-value diffusion coefficient, and wt denotes an independent Wiener process. continuous-indexed score network sœÜ(x, t) is employed to approximate marginal score functions of the forward diffusion process (2.1). The learning of score networks is achieved by minimizing weighted denoising score matching objective [57, 54], LDSM(œÜ) = (cid:90) t=0 Œª(t)Ex0q0,xtx0qt0(xtx0)sœÜ(xt, t) xt log qt(xtx0)2 2dt. (2.2) Here, the weighting function Œª(t) controls the importance of the learning at different time levels, and qt(xtx0) denotes the conditional transition of the forward diffusion (2.1). After training, the score network sœÜ(xt, t) xt log qt(xt) is good approximation of the marginal score function of the diffused data distribution. High-quality samples from DM can be drawn by simulating SDE, which is implemented by the learned score network [54]. However, the simulation of an SDE is significantly slower than that of other models, such as one-step generator models. 2.2 One-step Diffusion Model via KL Divergence Minimization Notations and the Settings of One-step Diffusion Models. We use the traditional settings introduced in Diff-Instruct [33] to present one-step diffusion models. Our basic setting is that we have pre-trained diffusion model specified by the score function sqt(xt) := xt log qt(xt) where qt(xt)s are the underlying distribution diffused at time according to (2.1). We assume that the pre-trained diffusion model provides sufficiently good approximation of the data distribution, and thus will be the only item of consideration for our approach. The one-step diffusion model of our interest is single-step generator network gŒ∏, which can transform an initial random noise pz to obtain sample = gŒ∏(z); this network is parameterized by 3 network parameters Œ∏. Let pŒ∏,0 denote the data distribution of the student model, and pŒ∏,t denote the marginal diffused data distribution of the student model with the same diffusion process (2.1). The student distribution implicitly induces score function spŒ∏,t(xt) := xt log pŒ∏,t(xt), and evaluating it is generally performed by training an alternative score network as elaborated later. Diff-Instruct Diff-Instruct [33] is the first work that trains one-step diffusion models by minimizing the integral of KL divergence between the one-step model and the teacher diffusion model distributions. The integral Kullback-Leibler divergence between one-step model pŒ∏(.) and teacher diffusion model q0(.) is defined as: DIKL(pŒ∏q0) := (cid:82) t=0 w(t)E x0=gŒ∏ (z), zN (0,I) log pŒ∏,t(xt) qt(xt) dt. (cid:27) (cid:26) xtx0qt0(xtx0) Though IKL as training objective is intractable because we do not have direct dependence of Œ∏ and pŒ∏,t(.). [33] proved in theory that tractable yet equivalent objective writes: LDI(Œ∏) := (cid:90) t=0 w(t)E x0=gŒ∏ (z), zN (0,I) xtx0qt0 (xtx0) (cid:26) (cid:27)T SG spSG[Œ∏],t(xt) sqt(xt) xt(Œ∏)dt, (2.3) Where the operator SG() in (2.3) represents the stop-gradient operator. Diff-Instruct proposed to use an online-trained fake diffusion model to approximate the stopped-gradient one-step model score function sœà,t(xt) spSG[Œ∏],t (xt). Such novel use of fake score is kept by following approaches such as DMD, SiD, etc. Two key contributions of Diff-Instruct are (1) first introducing the concept of the one-step distillation via divergence minimization; (2) introducing technical path that derives tractable losses by proving gradient equality w.r.t the intractable divergence. 2.3 One-step Diffusion Model via Score-based Divergence Minimization Score Implicit Matching (SIM). Inspired by Diff-Instruct and the empirical success of SiD [73], recent work, the Score-implicit Matching (SIM) [34], has generalized the KL divergences to general score-based divergence by proving new gradient equivalence theories. The general score-divergence is defined via: D[0,T ](p, q) := (cid:82) the marginal densities of the diffusion process (2.1) at time initialized with and respectively. w(t) is an integral weighting function. d() is distance function. Clearly, we have D[0,T ](p, q) = 0 if and only if all marginal score functions agree, which implies that p0(xt) = q0(xt), a.s. œÄ0. dt, where pt and qt denote d(spt(xt) sqt(xt)) t=0 w(t)ExtœÄt (cid:26) (cid:27) SIM shows that Eq. (2.4) has the same parameter gradient as the intractable score-divergence: LSIM(Œ∏) = (cid:90) t=0 w(t)E zpz ,x0=gŒ∏ (z), xtx0qt(xtx0) (cid:26) d(yt) (cid:27)T (cid:26) spsg[Œ∏],t(xt) xt log qt(xtx0) dt, (2.4) (cid:27) with yt := spsg[Œ∏],t(xt) sqt(xt). Now the objective becomes tractable. In Section 3, we use theoretical tools from Diff-Instruct and SIM to prove the gradient equivalence of tractable Uni-Instruct loss and the intractable expanded -divergence. Furthermore, we are surprisingly to find that the resulting gradient expression recovers novel combination of the DiffInstruct and the SIM parameter gradient. 2.4 Relation Between KL Divergence and Fisher Divergence Inspired by the famous De Bruijn identity [64, 7] that describes entropy evolution along heat diffusion, notable works [49, 40, 36, 53] have built the relationship between KL divergence and Fisher divergence via diffusion expansion: the KL divergence is the integral of the Fisher divergence along diffusion process under mild regularity conditions: DKL(pŒ∏q0) = (cid:90) 0 1 2 g2(t)EpŒ∏ (cid:2)spt(xt) sqt(xt)2 2 (cid:3) dt (2.5) Motivated by the relationship between KL divergence and Fisher divergence, in Section 3, we begin the Uni-Instruct framework by proposing novel diffusion expansion theorem of general KL divergence: the -divergence family."
        },
        {
            "title": "3 Uni-Instruct: Unify One-step Distillation Methods in Theory",
            "content": "In this section, we introduce Uni-Instruct, theory-driven family of approaches for the one-step distillation of score-based diffusion models. Uni-Instruct is able to unify more than 10 existing methods as special cases with proper weighting functions. It also leads to new state-of-the-art one-step generation performances on ImageNet64 64 and CIFAR10 generation benchmarks. Uni-Instruct is built upon novel diffusion expansion theory of the -divergence family. We begin by giving brief introduction to the -divergence family. We then prove novel diffusion expansion theory of -divergences in Section 3.1, which acts as the target objective we would like to optimize. Then in Section 3.2, we provide non-trivial theorem that leads to an equivalent yet tractable loss function that shares the same parameter gradient as the intractable expanded -divergence. 3.1 Diffusion Expansion of -Divergence -divergence. For convex function () on (0, +), where (1) = 0, The -divergence[45] is: Df (qp) = (cid:90) p(x)f (cid:19) (cid:18) q(x) p(x) dx. (3.1) Appropriate choices of the function () lead to many widely-used divergences such as reverse-KL divergence (RKL), forward-KL divergence (FKL), Jeffrey-KL divergence (JKL), Jensen-Shannon divergence (JS), and Chi-Square divergence (œá2). We put more introductions in the appendix B. The Diffusion Expansion Theorem. We use the same notations and settings in Section 2.2. gŒ∏() represents the one-step diffusion model, and qt() represents the distributions of the teacher diffusion model. Our goal is to minimize the -divergence between the output image distribution of the one-step models distribution and the teacher diffusion model distribution Df (q0pŒ∏). However, since -divergences are defined in the image data space, they can not directly incorporate instructions from multiple noise levels of teacher diffusion models. To address this issue, we first introduce diffusion expansion Theorem 3.1 of -divergence along diffusion process. This expansion enables us to construct training objectives by considering all diffusion noise levels. Theorem 3.1 (Diffusion Expansion of -Divergence). Assume p, are distributions that both evolve along Eq. 2.1. We have the following equivalence: Df (q0pŒ∏) = (cid:90) 0 1 2 g2(t)EpŒ∏,t (cid:34)(cid:18) qt pŒ∏,t (cid:19)2 (cid:19) (cid:18) qt pŒ∏,t spŒ∏,t(xt) sqt(xt)2 dt, (3.2) (cid:35) We give complete proof with regularity analysis in Appendix A.1. This fundamental expansion (Eq. 3.2) expands the static -divergence in data space into an integral of divergences along the diffusion process. However, directly optimizing objective (3.2) is not tractable because we do not know the exact expressions of either the density pŒ∏,t or the score function spŒ∏,t() of the diffused one-step models distribution. To step towards tractable objective, we derive the Œ∏ gradient of the expanded -divergence (3.2) in Theorem 3.2. 3.2 Theories to Get Tractable Losses To tackle the intractable issue of the expanded -divergence, we prove novel parameter gradient equivalence theorem 3.2. Theorem 3.2 (Gradient Equality Theorem of the Expanded -divergence). Let qt(x) and pŒ∏,t(x) be probability density functions evolving under the Fokker-Planck dynamics, and : R+ is four-times differentiable convex function. The parameter gradient of the -divergence rate satisfies: (cid:40) g2(t)Œ∏ EpŒ∏,t 1 2 (cid:19)2 (cid:19) (cid:18) qt pŒ∏,t spŒ∏,t (xt) sqt (xt)2 2 (cid:21) (cid:41) (cid:20)(cid:18) qt pŒ∏,t (cid:34) (cid:32) = g2(t) g2(t) 1 1 2 (cid:40) (cid:40) Œ∏ Œ∏ EpŒ∏,t SG C1 (cid:34) (cid:32) EpŒ∏,t SG C2 (cid:18) qt pŒ∏,t (cid:18) qt pŒ∏,t (cid:19) (cid:33) (cid:16) sqt (xt) spsg[Œ∏],t (xt) (cid:17) (cid:16) spsg[Œ∏],t (xt) xt log qt(xt x0) (cid:17) (cid:19) (cid:16) sqt (xt) spŒ∏,t (xt) (cid:17) sqt (xt) spŒ∏,t (xt)2 2 xt (cid:33) (cid:35)(cid:41) (cid:35)(cid:41) (3.3) 5 where SG donates stop gradient operator, and the curvature coupling coefficient C(r) are defined as: C1(r) := r3f (r), C2(r) := 2r2f (r) + 4r3f (r) + r4f (r), := qt(x) pŒ∏,t(x) (3.4) Remark 3.3. It is worth noting that in Theorem 3.2, we derived an equality of the gradient of the intractable expanded -divergence. The right side of the equality is two terms, which are gradients of two tractable functions. With this observation, we can see that minimizing the tractable right-hand side of equality (3.3) using gradient-based optimization algorithms such as Adam [23] is equivalent to minimizing the intractable expanded -divergence, which lies in the left-hand side. We notice that the gradient of the training objective admits composition of the Diff-Instruct[33] gradient and SIM[34] gradient. Therefore, we can formally write down our tractable loss function as: (cid:90) 1 2 LUI(Œ∏) = 0 LSIM = EpŒ∏,t LDI = EpŒ∏,t g2(t) (cid:0)ŒªDI LDI + ŒªSIM LSIM (cid:2)SG (C1 (r)) (cid:0)sqt(xt) spsg[Œ∏],t (xt)(cid:1) (cid:0)spsg[Œ∏],t (xt) xt log qt(xt x0)(cid:1)(cid:3) , (cid:2)SG (cid:0)C2 (r) (cid:0)sqt(xt) spŒ∏,t(xt)(cid:1) sqt(xt) spŒ∏,t(xt)2 (cid:1) xt (cid:3) , 2 (cid:1) dt, (3.5) (3.6) where the weighting coefficients are determined by the -divergence selection, we provide our completed proofs in Appendix A.2. Density Ratio Estimation via an Auxiliary GAN Loss Notice that the tractable loss function (3.5) requires the density ratio between the one-step model and teacher diffusion. For this, we train GAN discriminator along the process, where the discriminator output serves as an estimator. This use of GAN discriminator is also widely applicable in other works like SiDA[72] and -distill [63]. Details on why the GAN discriminator recovers the density ratio can be found in Theorem A.1. Practical Algorithm of the Uni-Instruct We can now present the formal training algorithm of Uni-Instruct. As is shown in Algorithm 1, we maintain the active training status of three models: one-step diffusion model, online fake score network, and discriminator. The training is performed in two steps alternatively: we first optimize the discriminator with real data, and then optimize the online fake score network with score matching loss. After that, we optimize the one with Uni-Instruct loss, which is given by the previous two models. Uni-Instruct loss varies based on the divergence we choose. We provide example divergences in Tab. 5. Note that through choosing proper divergence, we can recover the distillation loss of Diff-Instruct [33], SIM [34], as well as -distill [63]. To be more specific: LSIM vanishes when selecting œá2-divergence, while LDI vanishes if we choose forward-KL, reverse-KL, and Jeffrey-KL divergence. 3.3 How Uni-Instruct can Unify Previous Methods In this section, we show in what cases Uni-Instruct can recover previous methods. As is shown in Tab. 6, Uni-Instruct can effectively unify more than 10 existing distillation methods for one-step diffusion models, such as Diff-Instruct, DMD, -distill, SIM, and SiD. DI, DMD, and -distill are Uni-Instruct with additional time weighting. DI [33] and DMD [66] integrates KL divergence along diffusion process: DIKL(pŒ∏q0) := (cid:82) 0 w(t)DKL(pŒ∏q0)dt. Furthermore, -distill [63] replace KL with general -divergence. Our goal, on the other hand, is to match these two distributions only at the original distributions: Df (q0pŒ∏), which requires no specific weightings œâ(t). Our framework is more theoretically self-consistent for those ad-hoc weightings that may induce mismatches between the optimization target and the true distribution divergence. However, with additional weightings, Uni-Instruct can recover -distill. Corollary 3.4. Suppose (t) = (cid:82) w(t)dt + C, (0) = 0, the expression of Uni-Instruct with an extra weighting (t) is equivalent to -distill: (cid:90) 0 1 2 g2(t)W (t)EpŒ∏,t (cid:20)(cid:18) qt pŒ∏,t (cid:19)2 (cid:19) (cid:18) qt pŒ∏,t spŒ∏,t (xt) sqt (xt)2 (cid:21) dt = (cid:90) 0 w(t)Df (q0pŒ∏,t)dt. (3.7) Complete proof is in Appendix A.4, which leverages integration by parts and Theorem 3.1. 6 2 (cid:2)spŒ∏,t(xt) sqt(xt)2 SIM is Special Case of Uni-Instruct. Suppose d() is l2-norm, SIM in Section 2.3 becomes: (cid:3) dt. It turns out that SIM is special case of Uni-Instruct. We (cid:82) œâ(t)EpŒ∏,t find that the right-hand side of Theorem 3.1 will degenerate to SIM through selecting the divergence (cid:3) dt. As result, 2 g2(t) (cid:82) EpŒ∏,t as reverse-KL divergence: DKL(pŒ∏q0) = 1 SIM is secretly minimizing the KL divergence between the teacher model and the one-step diffusion model, which is special case of our -divergence. Beyond this specific configuration, Uni-Instruct offers enhanced flexibility through its support for alternative divergence metrics, including FKL and JKL, which enable improved mode coverage. This generalized formulation contributes to superior empirical performance, achieving lower FID values. (cid:2)spŒ∏,t(xt) sqt(xt) 2 3.4 Text-to-3D Generation using Uni-Instruct Recent advances in 3D text-to-image synthesis leverage 2D diffusion models as priors. Dreamfusion [44] introduced score distillation sampling (SDS) to align NeRFs with text guidance, while ProlificDreamer [59] improved quality via variational score distillation (VSD). These methods mainly use reverse KL divergence. Uni-Instruct generalizes this framework by allowing flexible divergence choices (e.g., FKL, JKL), enhancing mode coverage and geometric fidelity, and unifying SDS and VSD as special cases. Limitations of Uni-Instruct. One of the major limitations of Uni-Instruct is that it needs an additional discriminator for density ratio estimation, which may bring more computational costs. Moreover, due to the complexity of the gradient formula, Uni-Instruct may result in bad performance with an improper choice of , as its complex gradient formula is not as straightforward as some simpler existing methods like Diff-Instruct. We provide detailed analysis in Appendix F."
        },
        {
            "title": "4 Related Works",
            "content": "Diffusion Distillation Diffusion distillation [31] focuses on reducing generation costs by transferring knowledge from teacher diffusion models to more efficient student models. It primarily includes three categories of methods: (1) Trajectory Distillation: These methods train student models to approximate the generation trajectory of diffusion models using fewer denoising steps. Approaches such as direct distillation [29, 11] and progressive distillation [46, 38] aim to predict cleaner data from noisy inputs. Consistency-based methods [52, 21, 51, 26, 13] instead minimize self-consistency loss across intermediate steps. Most of these methods require access to real data samples for effective training. (2) Divergence Minimization (Distribution Matching): This line of work aims to align the distribution of the student model with that of the teacher. Adversarial training-based methods [60, 62] typically require real data to perform distribution matching. Alternatively, several approaches minimize divergences like the KL divergence (e.g., Diff-Instruct [33, 66]) or Fisher divergence (e.g., Score Identity Distillation [73], Score Implicit Matching [34]), and often do so without requiring real samples. Numerous improvements have been made to these two lines of work: DMD2 [65] and SiDA [72] add real images during training, rapidly surpassing the teachers performance. -distill [63] generalize KL divergence of DIff-Instruct into -divergence and compared the affection of different divergences. Additionally, significant progress has been made toward scaling diffusion distillation for ultra-fast or even one-step text-to-image generation [30, 17, 55, 66, 71, 65]. (3) Other Methods: Several alternative techniques that train the model from scratch have been proposed, including ReFlow [27], Flow Matching Models (FMM) [4], which propose an ODE to model the diffusion process. Indcutive Moment Matching [70] models the self-consistency of stochastic interpolants at different time steps. Consistency models [52, 51, 21, 12] impose consistency constraints on network outputs along the trajectory."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we first demonstrate Uni-Instructs strong capability to generate high-quality samples on benchmark datasets through efficient distillation. Followed by text-to-3D generation, which illustrates the wide application of Uni-Instruct. 7 Figure 2: Generated samples from Uni-Instruct one-step generators that are distilled from pre-trained diffusion models on different datasets. Left: CIFAR10 (unconditional); Mid: CIFAR10 (conditional); Right: ImageNet 64 64 (conditional) . 5.1 Benchmark Datasets Generation Experiment Settings We evaluate Uni-Instruct for both conditional and unconditional generations on CIFAR10 [24] and conditional generations on ImageNet 64 64[9]. We use EDM [20] as teacher models. In each experiment, we implement three types of divergences: Reverse-KL (RKL), ForwardKL (FKL), and Jeffrey-KL (JKL) divergence. We borrow the parameters settings from SiDA [72], which takes the output from the diffusion unet encoder directly as the discriminator. As for evaluation metrics, we use FID, as it simultaneously quantifies both image quality and diversity. Table 1: Comparison image generation on CIFAR-10 (unconditional). The best one/fewstep generator under the FID metric is highlighted with bold. F.S. means from scratch. L.T. means resume and Longer Training. Table 2: Class conditional ImageNet 6464 generation results. Direct generation and Distillation methods require one NFE, while the teacher uses 35 NFE. F.S. means from scratch. L.T. means resume and Longer Training. Family Model NFE FID () Family Model Teacher VP-EDM [20] Diffusion Flow DPM-Solver-3 [69] DDIM [50] DDPM [16] NCSN++ [54] VDM [22] iDDPM [41] Rectified Flow [27] Flow Matching [25] Consistency sCT [28] ECT [12] iCT [51] Few Step PD [46] IMM [70] TRACT [3] KD [29] Diff. ProjGAN [58] PID [56] DFNO [68] iCT-deep [51] Diff-Instruct [33] DMD [66] CTM [21] SiD [73] SiDA [72] SiD2A [72] Uni-Instruct with RKL (F.S.) Uni-Instruct with FKL (F.S.) Uni-Instruct with FKL (L.T.) Uni-Instruct with JKL (F.S.) 48 100 1000 1000 1000 4000 127 142 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1.97 2.65 4.16 3.17 2.38 4.00 2. 2.58 6.35 2.06 2.11 2.46 4.51 1.98 3.32 9.36 2.54 3.92 3.78 2.51 4.53 3.77 1.98 1.92 1.52 1.50 1.52 1.52 1.48 1.46 Teacher VP-EDM [20] RIN [19] DDPM [16] ADM [10] DiT-L/2 [43] DPM-Solver-3 [69] U-ViT [2] BigGAN-deep [5] StyleGAN-XL [48] Diffusion GAN Consistency iCT [51] iCT-deep [51] ECT [12] MMD [47] G-istill [38] PD [46] Diff-Instruct [33] PID [56] iCT-deep [51] EMD-16 [61] DFNO [68] DMD2+longer training [65] CTM [21] SiD [73] SiDA [72] SiD2A [72] -distill [63] Uni-Instruct with RKL(F.S.) Uni-Instruct with JKL(F.S.) Uni-Instruct with FKL(F.S.) Uni-Instruct with FKL(L.T.) Few Step 8 NFE FID () 511 1.36 1000 250 250 250 50 1.23 11.00 2.07 2.91 17.52 4.26 1 1 1 1 1 8 8 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4.06 1.52 4.02 3.25 2. 1.24 2.05 8.95 5.57 9.49 3.25 2.20 7.83 1.28 1.92 1.71 1.35 1.10 1.16 1.35 1.28 1.34 1.02 Performance Evaluations Tab. 1, Tab. 3 and Tab. 2 shows Uni-Instruct performance on both settings of CIFAR10 and ImageNet 64 64. Uni-Instruct achieves new state-of-the-art one-step generation performances on all datasets. Our important findings include: (1) When training from scratch, JKL achieves the lowest FID score. On CIFAR10, JKL trained from scratch has FID score of 1.42, out-perform other baseline methods like DMD [66], SiDA [72], and the teacher model EDM [20]. (2) When resuming trained SiD model (RKL), FKL achieves even better results. As is shown in the Table 1, FKL with longer training achieves new state-of-the-art one-step generation on both datasets. This means two-time training schedule: first trained with RKL until convergence, followed by FKL, enhances the models performance with both mode-seeking behavior from RKL and mode-covering behavior from FKL. 5.2 Ablation Studies Performance Between Different Divergences and the effect of GAN loss. We perform an ablation study on the techniques applied in our experiments. Table 4 ablates different components of our proposed method on CIFAR10, where we use an unconditional generator for all settings. For different divergences, we select three types: JKL, FKL, and RKL are divergences that only contains Grad(SiD), œá2 divergences gradient is only contributed by Grad(DI), Jensen-Shannon (JS) divergence has gradient that contains both: hDI(x)Grad(DI) + hSiD(x)Grad(SiD). Our result shows that JKL achieves the lowest FID value. Due to numerical instability of the weightings, JS yields unsuccessful distillation results. As for the effect of GAN loss, we find that removing it still yields decent result. Our integrated approach also surpasses the performance of using Uni-Instruct loss alone(without adding GAN loss), highlighting the effectiveness of combining expanded -divergence with GAN losses. We also find that using model trained with RKL Uni-Instruct (which recovers the SiD[73] loss) as the initialization leads to better performances for all divergences. Table 3: Label-conditioned image generation results on CIFAR-10. The best one/few-step generator under the FID metric is highlighted with bold. NFE FID () Family Model Teacher VP-EDM [20] Diffusion DDPM [16] iDDPM [41] 35 1000 4000 One Step Diff-Instruct [33] SIM [34] CTM [21] SiD [73] SiDA [72] SiD2A [72] -distill [63] Uni-Instruct w. RKL (from scratch) Uni-Instruct w. JKL (from scratch) Uni-Instruct w. FKL (from scratch) Uni-Instruct w. FKL (longer training) 1 1 1 1 1 1 1 1 1 1 1 1.79 3.17 2.90 4.19 1.96 1.73 1.71 1.44 1.40 1.92 1.44 1.42 1.43 1.38 Table 4: Ablation study on CIFAR10 uncond generation. GAN means using GAN loss. Init means initialize from models. Div. SiD Init. GAN FID 8.21 4.37 5.23 1.46 1.92 1.88 1.52 1.52 1.50 1.48 1.50 None œá2 JS JKL RKL FKL RKL FKL RKL FKL JKL Figure 3: Prompt: refined vase with artistic patterns. Left: ProlificDreamer; Right: UI+forward KL. Our vase demonstrate more diverse shapes as well as realistic patterns. 5.3 Text-to-3D Generation Using 2D Diffusion In this subsection, we apply Uni-Instruct on text-to-3D generation. We re-implement the code base of ProlificDreamer [59] by adding an extra discriminator head to the output of the stable diffusion Unets encoder. We use FKL to distill the model for 400 epochs. Fig. 3 demonstrates the visual results from 9 our 3D experiments. Uni-Instruct archives surprisingly decent 3D generation performances, with improved diversity and fidelity. Due to page limitations, we put detailed experiment settings and quantitative metrics in the Appendix E."
        },
        {
            "title": "6 Conclusions",
            "content": "We present Uni-Instruct, theoretically grounded framework for training one-step diffusion models via distribution matching. Through building upon novel diffusion expansion theory of the - divergence, Uni-Instruct establishes unifying theoretical foundation that generalizes and connects more than 10 existing diffusion distillation methodologies. Uni-Instruct also demonstrates superior performance on benchmark datasets and efficacy in downstream tasks like text-to-3D generation. We hope Uni-Instruct offers useful insights for future studies on efficient generative models."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was supported by the National Natural Science Foundation of China (62371007) and by Xiaohongshu Inc. The authors acknowledge helpful advice from Shanghai AI Lab and Yongqian Peng."
        },
        {
            "title": "References",
            "content": "[1] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79968006, 2024. [2] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. [3] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint arXiv:2303.04248, 2023. [4] Nicholas Boffi, Michael Albergo, and Eric Vanden-Eijnden. Flow map matching. arXiv preprint arXiv:2406.07507, 2024. [5] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. [6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2224622256, 2023. [7] Michael CH Choi, Chihoon Lee, and Jian Song. Entropy flow and de bruijns identity for class of stochastic differential equations driven by fractional brownian motion. Probability in the Engineering and Informational Sciences, 35(3):369380, 2021. [8] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [11] Zhengyang Geng, Ashwini Pokle, and Zico Kolter. One-step diffusion distillation via deep equilibrium models. Advances in Neural Information Processing Systems, 36:4191441931, 2023. [12] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. [13] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured Probabilistic Inference {&} Generative Modeling, 2023. [14] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 23282337, 2023. [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [17] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. arXiv e-prints, pages arXiv2312, 2023. 11 [18] Zemin Huang, Zhengyang Geng, Weijian Luo, and Guo-jun Qi. Flow generator matching. arXiv preprint arXiv:2410.19310, 2024. [19] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. arXiv preprint arXiv:2212.11972, 2022. [20] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. [21] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. [22] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. [23] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [25] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [26] Hongjian Liu, Qingsong Xie, Tianxiang Ye, Zhijie Deng, Chen Chen, Shixiang Tang, Xueyang Fu, Haonan Lu, and Zheng-Jun Zha. Scott: Accelerating diffusion models with stochastic consistency distillation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 54515459, 2025. [27] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [28] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. [29] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. [30] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. [31] Weijian Luo. comprehensive survey on knowledge distillation of diffusion models. arXiv preprint arXiv:2304.04262, 2023. [32] Weijian Luo. Diff-instruct++: Training one-step text-to-image generator model to align with human preferences. arXiv preprint arXiv:2410.18881, 2024. [33] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36:7652576546, 2023. [34] Weijian Luo, Zemin Huang, Zhengyang Geng, Zico Kolter, and Guo-jun Qi. One-step diffusion distillation through score implicit matching. Advances in Neural Information Processing Systems, 37:115377115408, 2025. [35] Weijian Luo, Colin Zhang, Debing Zhang, and Zhengyang Geng. Diff-instruct*: Towards human-preferred one-step text-to-image generative models. arXiv preprint arXiv:2410.20898, 2024. [36] Siwei Lyu. Interpretation and generalization of score matching. arXiv preprint arXiv:1205.2629, 2012. 12 [37] Morteza Mardani, Jiaming Song, Jan Kautz, and Arash Vahdat. variational perspective on solving inverse problems with diffusion models. arXiv preprint arXiv:2305.04391, 2023. [38] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. [39] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [40] Javier Movellan and James McClelland. Learning continuous probability distributions with symmetric diffusion networks. Cognitive Science, 17(4):463496, 1993. [41] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR, 2021. [42] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [43] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [44] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [45] Alfr√©d R√©nyi. On measures of entropy and information. In Proceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1: contributions to the theory of statistics, volume 4, pages 547562. University of California Press, 1961. [46] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [47] Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via moment matching. Advances in Neural Information Processing Systems, 37:3604636070, 2024. [48] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. [49] Jascha Sohl-Dicksteinad, Peter Battaglinobd, and Michael DeWeesebcd. Minimum probability flow learning. arXiv preprint arXiv:0906.4779, 2009. [50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [51] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023. [52] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. [53] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415 1428, 2021. [54] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [55] Yuda Song, Zehao Sun, and Xuanwu Yin. Sdxs: Real-time one-step latent diffusion models with image conditions. arXiv preprint arXiv:2403.16627, 2024. 13 [56] Joshua Tian Jin Tee, Kang Zhang, Hee Suk Yoon, Dhananjaya Nagaraja Gowda, Chanwoo Kim, and Chang Yoo. Physics informed distillation for diffusion models. arXiv preprint arXiv:2411.08378, 2024. [57] Pascal Vincent. connection between score matching and denoising autoencoders. Neural computation, 23(7):16611674, 2011. [58] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022. [59] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. [60] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021. [61] Sirui Xie, Zhisheng Xiao, Diederik Kingma, Tingbo Hou, Ying Nian Wu, Kevin Murphy, Tim Salimans, Ben Poole, and Ruiqi Gao. Em distillation for one-step diffusion models. Advances in Neural Information Processing Systems, 37:4507345104, 2024. [62] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81968206, 2024. [63] Yilun Xu, Weili Nie, and Arash Vahdat. One-step diffusion models with -divergence distribution matching. arXiv preprint arXiv:2502.15681, 2025. [64] Takuya Yamano. de bruijn-type identity for systems with flux. The European Physical Journal B, 86:16, 2013. [65] Tianwei Yin, Micha√´l Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in Neural Information Processing Systems, 37:4745547487, 2024. [66] Tianwei Yin, Micha√´l Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. [67] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. arXiv preprint arXiv:2412.07772, 2, 2024. [68] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. In International conference on machine learning, pages 4239042402. PMLR, 2023. [69] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics. Advances in Neural Information Processing Systems, 36:5550255542, 2023. [70] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive moment matching. arXiv preprint arXiv:2503.07565, 2025. [71] Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai Huang. Long and short guidance in score identity distillation for one-step text-to-image generation. arXiv preprint arXiv:2406.01561, 2024. [72] Mingyuan Zhou, Huangjie Zheng, Yi Gu, Zhendong Wang, and Hai Huang. Adversarial score identity distillation: Rapidly surpassing the teacher in one step. arXiv preprint arXiv:2410.14919, 2024. [73] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024."
        },
        {
            "title": "A Proofs",
            "content": "A.1 Proof of Theorem 3.1 Proof. Let pt and qt be distributions satisfying the Fokker-Planck equations, and decay rapidly at infinity: pŒ∏,t qt = = (cid:20) 1 2 (cid:20) 1 g2(t)pŒ∏,tx log pŒ∏,t (x, t)pŒ∏,t (cid:21) g2(t)qtx log qt (x, t)qt (cid:21) We begin with the definition of -divergence and apply differentiation under the integral sign: dt Df (qtpŒ∏,t) = = (cid:19) (cid:18) qt pŒ∏,t (cid:19) pŒ∏,tf (cid:90) dt (cid:90) pŒ∏,t (cid:18) qt pŒ∏,t dx dx + (cid:90) pŒ∏,t (cid:19) (cid:18) qt pŒ∏,t dx (cid:90) For the second term, apply the chain rule and the quotient rule: (cid:18) qt (cid:19) (cid:18) qt pŒ∏,t pŒ∏,t (cid:19) (cid:18) qt (cid:18) qt pŒ∏,t pŒ∏,tf dx = pŒ∏,t = (cid:19) (cid:90) (cid:90) (cid:18) qt pŒ∏,t qt pŒ∏,t (cid:19) dx (cid:19) dx pŒ∏,t Combining Eq. A.1, Eq. A.2 and Eq. A.3, we obtain: (A.1) (A.2) (A.3) dt Df (qtpŒ∏,t) = + (cid:90) (cid:90) (cid:20) 1 2 (cid:18) qt pŒ∏,t (cid:19) (cid:20) 1 2 g2(t)pŒ∏,tx log pŒ∏,t (x, t)pŒ∏,t (cid:19) dx (cid:21) (cid:18) qt pŒ∏,t (cid:21) g2(t)qtx log qt (x, t)qt dx (cid:90) qt pŒ∏,t (cid:18) qt pŒ∏,t (cid:19) (cid:20) 1 2 g2(t)pŒ∏,tx log pŒ∏,t (x, t)pŒ∏,t (cid:21) dx (A.4) Apply integration by parts to the RHS of Eq. A.4 and with previous assumption that distribution pŒ∏,t and qt decay rapidly at infinity, we have: dt Df (qtpŒ∏,t) = (cid:90) (cid:90) + (cid:90) (cid:20) 1 2 xf (cid:18) qt pŒ∏,t (cid:20) qt pŒ∏,t (cid:19) (cid:20) 1 2 (cid:18) qt pŒ∏,t g2(t)pŒ∏,tx log pŒ∏,t (x, t)pŒ∏,t (cid:21) xf (cid:19) (cid:18) qt pŒ∏,t dx g2(t)qtx log qt (x, t)qt (cid:21) dx (cid:19)(cid:21) (cid:20) 1 g2(t)pŒ∏,tx log pŒ∏,t (x, t)pŒ∏,t (cid:21) dx (A.5) Now we can further expand the gradient terms in Eq. A.5: xf xf (cid:19) (cid:18) qt pŒ∏,t (cid:19) (cid:18) qt pŒ∏,t = (cid:18) qt pŒ∏,t (cid:19) xqtpŒ∏,t xpŒ∏,tqt p2 Œ∏,t = (cid:18) qt pŒ∏,t (cid:19) xqtpŒ∏,t xpŒ∏,tqt p2 Œ∏,t (A.6) (A.7) (cid:20) qt pŒ∏,t (cid:18) qt pŒ∏,t (cid:19)(cid:21) = (cid:18) qt pŒ∏,t (cid:19) xqtpŒ∏,t xpŒ∏,tqt p2 Œ∏,t + qt pŒ∏,t (cid:18) qt pŒ∏,t (cid:19) xqtpŒ∏,t xpŒ∏,tqt p2 Œ∏,t (A.8) 15 Replace the gradient terms in Eq. A.5 with Eq. A.6, Eq. A.7, and Eq. A.8 and after algebraic manipulation, we obtain: dt Df (q0pŒ∏) = (cid:90) g2(t) 1 2 pŒ∏,t (cid:19) (cid:18) qt pŒ∏,t (cid:19) (cid:18) qt pŒ∏,t log qt log pŒ∏,t2dx (A.9) A.2 Proof of Theorem 3.2 Lemma A.1 (Calculate the gradient of pŒ∏,t [63]). Assuming that sampling from pŒ∏,t can be parameterized as = GŒ∏(z) + œÉ(t)œµ, where p(z), œµ (0, I), and GŒ∏, are differentiable mappings. In addition, is constant with respect to Œ∏. Then, (cid:90) Œ∏pŒ∏,t(x)g(x) dx = (cid:90) (cid:90) p(œµ)p(z)xg(x)Œ∏GŒ∏(z) dœµ dz. Proof. As qt and are both continuous functions, we can interchange integration and differentiation: (cid:90) Œ∏pŒ∏,t(x)g(x) dx = Œ∏ (cid:90) pŒ∏,t(x)g(x) dx = = = (cid:90) (cid:90) (cid:90) (cid:90) p(œµ)p(z)Œ∏g(GŒ∏(z) + œÉ(t)œµ) dœµ dz p(œµ)p(z)xg(x)Œ∏GŒ∏(z) dœµ dz (cid:90) pŒ∏,t(x)xg(x) Œ∏ dx, where = GŒ∏(z) + œÉ(t)œµ. Lemma A.2 (Calculate the gradient of the score fuction [34]). If distribution pŒ∏,t satisfies some mild regularity conditions, we have for any score function sqt(), the following equation holds for all parameter Œ∏: Extpsg[Œ∏],t = Œ∏ (cid:20) (cid:0)spŒ∏,t(xt) sqt(xt)(cid:1) spŒ∏,t(xt) Œ∏ (cid:104)(cid:8)(cid:0)ssg[Œ∏],t(xt) sqt(xt)(cid:1)(cid:9)T (cid:8)ssg[Œ∏],t(xt) xt log qt(xtx0)(cid:9)(cid:111) (cid:21) (A.10) (A.11) For completeness, we appreciate the efforts of Luo et al. [34] and provide the proof here. The original version can be refered to Theorem 3.1 from [34]. Proof. Starting with score projection identity [73]: x0pŒ∏ ,0 xtx0qt(xtx0) (cid:8)u(xt, Œ∏)T (spŒ∏,t(xt) xt log qt(xtx0))(cid:9) = 0, Œ∏, u. (A.12) 16 Taking the gradient with respect to Œ∏ on the above identity, we have: 0 = x0pŒ∏ ,0 xtx0qt(xtx0 ) + + = + + x0pŒ∏ ,0 xtx0qt(xtx0 ) x0pŒ∏ ,0 xtx0qt(xtx0 ) x0pŒ∏ ,0 xtx0qt(xtx0 ) x0pŒ∏ ,0 xtx0qt(xtx0 ) x0pŒ∏ ,0 xtx0qt(xtx0 ) (cid:27) xt Œ∏ (cid:26) (cid:27) x0 Œ∏ (cid:0)u(xt, Œ∏)T {xt log qt(xtx0)}(cid:1) (cid:0)u(xt, Œ∏)T {spŒ∏,t(xt) xt log qt(xtx0)}(cid:1) (cid:26) xt (cid:26) x0 u(xt, Œ∏)T Œ∏ (cid:26) u(xt, Œ∏)T Œ∏ (cid:26) (cid:26) (cid:0)u(xt, Œ∏)T {spŒ∏,t(xt) xt log qt(xtx0)}(cid:1) Œ∏ xt u(xt, Œ∏)T sŒ∏(xt) {spŒ∏,t(xt)} {spŒ∏,t(xt)} Œ∏ + (cid:27) (cid:27) (cid:27) (cid:27) xt Œ∏ (A.13) (A.14) (A.15) (A.16) (A.17) (cid:26) x0 (cid:8)u(xt, Œ∏)T {xt log qt(xtx0)}(cid:9) x0 Œ∏ + Œ∏ u(xt, Œ∏)T sŒ∏(xt) (cid:27) = ExtpŒ∏,t (cid:26) u(xt, Œ∏)T Œ∏ (cid:27) {spŒ∏,t(xt)} + Œ∏ x0pŒ∏ ,0 xtx0qt(xtx0) (cid:8)u(xt, Œ∏)T {spŒ∏,t(xt) xt log qt(xtx0)}(cid:9) . (A.18) (A.19) (A.20) Therefore, we obtain the following identity: ExtpŒ∏,t (cid:26) u(xt, Œ∏)T Œ∏ (cid:27) spŒ∏,t(xt) = Œ∏ x0pŒ∏ ,0 xtx0qt(xtx0) (cid:8)u(xt, Œ∏)T (spŒ∏,t(xt) xt log qt(xtx0))(cid:9) . (A.21) Replacing u(xt) with spŒ∏,t(xt) sqt(xt) we can proof the correctness of the original identity. We now complete the proof of Theorem 3.2: Proof. Applying the product rule to the gradient, we can obtain: (cid:40) 1 2 Œ∏ g2(t)EpŒ∏,t (cid:90) g2(t)Œ∏ pŒ∏,t(xt) (cid:90) (cid:90) g2(t) g2(t) Œ∏pŒ∏,t(xt) pŒ∏,t(xt)Œ∏ = = + 1 2 1 2 1 (cid:19)2 (cid:19)2 (cid:19)2 (cid:34)(cid:18) qt pŒ∏,t (cid:18) qt pŒ∏,t (cid:18) qt pŒ∏,t (cid:34)(cid:18) qt pŒ∏,t (cid:19) (cid:19) (cid:19) (cid:18) qt pŒ∏,t (cid:18) qt pŒ∏,t (cid:18) qt pŒ∏,t (cid:18) qt pŒ∏,t (cid:19)2 log pŒ∏,t log qt2 2 (cid:35)(cid:41) log pŒ∏,t log qt2 2dxt log pŒ∏,t log qt 2dxt (cid:19) log pŒ∏,t log qt2 2 dxt, (A.25) (cid:35) (A.22) (A.23) (A.24) 17 (A.29) (A.30) (A.31) which can be further decomposed into the following four terms: Grad = + + + (cid:90) (cid:90) (cid:90) (cid:90) g2(t) g2(t) g2(t) g2(t) 1 2 (cid:124) 1 2 (cid:124) 1 2 (cid:124) 1 2 (cid:124) Œ∏pŒ∏,t(xt) (cid:19)2 (cid:18) qt pŒ∏,t (cid:19) (cid:18) qt pŒ∏,t log pŒ∏,t log qt2 2dxt (A.26) (cid:19)2(cid:35) (cid:34)(cid:18) qt pŒ∏,t (cid:123)(cid:122) f (cid:19) (cid:18) qt pŒ∏,t pŒ∏,t(xt)Œ∏ pŒ∏,t(xt) (cid:18) qt pŒ∏,t (cid:19)2 (cid:20) Œ∏ (cid:123)(cid:122) (cid:18) qt pŒ∏,t (cid:19)(cid:21) (cid:125) log pŒ∏,t log qt2 2dxt (A.27) (cid:125) log pŒ∏,t log qt2 2dxt (A.28) pŒ∏,t(xt) (cid:18) qt pŒ∏,t (cid:19)2 (cid:123)(cid:122) (cid:19) (cid:18) qt pŒ∏,t (cid:123)(cid:122) Œ∏ (cid:2) log pŒ∏,t log qt2 2 (cid:125) (cid:3) dxt (cid:125) We calculate the above four terms separately. = = + + (cid:90) (cid:90) (cid:90) (cid:90) 1 2 1 2 1 2 1 g2(t) g2(t) g2(t) g2(t) Œ∏pŒ∏,t(xt) (cid:18) qt pŒ∏,t pt qt (cid:18) 2 pt qt (cid:18) pt (cid:19)2 (cid:18) qt (cid:18) pt qt (cid:19)2 xt Œ∏ (cid:18) pt qt (cid:19) (cid:18) pt qt qt(xt) qt(xt) qt(xt) (cid:19) (cid:19) (cid:18) qt pŒ∏,t log pŒ∏,t log qt2 2dxt (cid:19) (cid:19) (cid:18) pt qt log pt log qt2 2dxt (cid:19) (cid:19) pt qt xt Œ∏ log pt log qt2 2dxt (A.32) Œ∏ (cid:0) log pt log qt2 2 (cid:1) dxt (A.33) = = (cid:90) (cid:90) g2(t) g2(t) 1 2 1 2 pŒ∏,t(xt)Œ∏ (cid:34)(cid:18) qt pŒ∏,t (cid:19)2(cid:35) (cid:34) pŒ∏,t(xt) 2 (cid:19) (cid:32) (cid:18) qt pŒ∏,t qt p2 Œ∏,t (cid:18) qt pŒ∏,t (cid:33) (cid:19) log pŒ∏,t log qt2 2dxt (A.34) (cid:35) Œ∏pŒ∏,t(xt) (cid:19) (cid:18) qt pŒ∏,t log pŒ∏,t log qt 2dxt = (cid:90) g2(t) 1 2 = 2 Œ∏pŒ∏,t(xt) (cid:19)2(cid:35) (cid:34) 2 (cid:18) qt pŒ∏,t (cid:19) (cid:18) qt pŒ∏,t log pŒ∏,t log qt2 2dxt (A.35) (A.36) (A.37) = (cid:90) g2(t) 1 2 pŒ∏,t(xt) = (cid:90) (cid:90) (cid:90) 1 1 2 1 2 g2(t) g2(t) g2(t) (cid:18) qt pŒ∏,t (cid:32) (cid:19)2 Œ∏ (cid:20) (cid:18) qt pŒ∏,t (cid:19)(cid:21) log pŒ∏,t log qt 2dxt (A.38) (cid:19)2 (cid:18) qt pŒ∏,t qt pŒ∏,t xt Œ∏ (cid:33) (cid:19) (cid:18) qt pŒ∏,t log pŒ∏,t log qt 2dxt (A.39) pŒ∏,t(xt) 3 pŒ∏,t(xt) (cid:18) qt pŒ∏,t (cid:19)3 (cid:18) (cid:19) (cid:18) qt pŒ∏,t qt pŒ∏,t xt Œ∏ (cid:19) log pŒ∏,t log qt2 2dxt pŒ∏,t(xt) (cid:19) (cid:18) qt pŒ∏,t (cid:19) (cid:18) qt pŒ∏,t Œ∏ (cid:0) log pŒ∏,t log qt 2 (cid:1) dxt (A.40) (A.41) 18 = (cid:90) g2(t) 1 2 pŒ∏,t(xt) (cid:19)2 (cid:18) qt pŒ∏,t (cid:19) (cid:18) qt pŒ∏,t Œ∏ (cid:2) log pŒ∏,t log qt2 (cid:3) dxt (A.42) As result: (cid:40) 1 2 Œ∏ g2(t)Eqt (cid:34)(cid:18) pt qt (cid:19)2 (cid:19) (cid:18) pt qt log pt log qt2 2 (cid:35)(cid:41) = + + + = + + = 1 2 g2(t)EpŒ∏,t (cid:34)(cid:18) qt pŒ∏,t (cid:124) (cid:19)3 (cid:18) qt pŒ∏,t (cid:123)(cid:122) weight 1 (cid:19)(cid:35) (cid:125) Œ∏ log pŒ∏,t log qt2 2 + 1 2 g2(t)EpŒ∏,t (...) log pŒ∏,t log qt2 2 (cid:124) (cid:123)(cid:122) (cid:125) weight ( log pŒ∏,t log qt) (A.43) (A.44) (A.45) (A.46) xt Œ∏ where ... stands for 2 (cid:17) (cid:16) qt pŒ∏,t (cid:16) qt pŒ∏,t (cid:17) + 4 (cid:17) (cid:16) qt pŒ∏,t (cid:16) qt pŒ∏,t (cid:17) (cid:16) qt pŒ∏,t + (cid:17)4 (cid:16) qt pŒ∏,t (cid:17) . Applying Lemma A.2, we have: (cid:40) g2(t)Œ∏ EpŒ∏,t 1 2 (cid:19)2 (cid:19) (cid:18) qt pŒ∏,t spŒ∏,t(xt) sqt(xt)2 2 (cid:35) (cid:41) (cid:34)(cid:18) qt pŒ∏,t (cid:34) (cid:32) (cid:40) EpŒ∏,t SG C1 (cid:40) (cid:34) (cid:32) EpŒ∏,t SG C2 = 1 2 1 2 g2(t) g2(t) Œ∏ Œ∏ (cid:19) (cid:33) (cid:0)sqt(xt) spsg[Œ∏],t(xt)(cid:1) (cid:0)spsg[Œ∏],t(xt) xt log qt(xt x0)(cid:1) (cid:35)(cid:41) (cid:19) (cid:0)sqt(xt) spŒ∏,t(xt)(cid:1) sqt(xt) spŒ∏,t(xt)2 (cid:33) (cid:35)(cid:41) xt (cid:18) qt pŒ∏,t (cid:18) qt pŒ∏,t (A.47) where SG donates stop gradient operator, and the curvature coupling coefficient C(r) are defined as: C1(r) := r3f (r), C2(r) := 2r2f (r) + 4r3f (r) + r4f (r), := qt(x) pŒ∏,t(x) (A.48) A.3 Density Ratio Representation Theorem A.1 (Density Ratio Representation). For adversarial discriminator conditioned on the timestep D: [0, ] [0, 1] satisfying: = arg min Exqdata[ log D(x, t)] + Expg [ log(1 D(x, t))], (A.49) The density ratio admits the variational representation: qt(x) pŒ∏,t(x) = D(x, t) 1 D(x, t) . (A.50) Proof of Theorem A.1. Firstly, we calculate the optimal discriminator: 19 Lemma A.3 (Optimal Discriminator Characterization). For measurable functions : [0, ] [0, 1], the minimizer of: (D) = Exqt[ log D(x, t)] + ExpŒ∏,t[ log(1 D(x, t))] (A.51) satisfies the first-order optimality condition: Œ¥J Œ¥D (cid:12) (cid:12) (cid:12) (cid:12)D=D = qt(x) D(x, t) + pŒ∏,t(x) 1 D(x, t) = 0. Solving Lemma A.3s optimality condition yields: D(x, t) = qt(x) qt(x) + pŒ∏,t(x) Through algebraic transformation, we have: qt(x) pŒ∏,t(x) = D(x, t) 1 D(x, t) . (A.52) (A.53) (A.54) A.4 Proof of Corollary 3.4 Proof of Corollary3.4. Using Theorem3.1, assuming some mild assumptions on the growth of log qt and log pt at infinity, we have: Df (q0pŒ∏) = (cid:90) 0 1 2 g2(t)EpŒ∏ (cid:34)(cid:18) qt pŒ∏,t (cid:19) (cid:19) (cid:18) qt pŒ∏,t log pŒ∏,t log qt2 2 dt. (A.55) (cid:35) We also have the differential form of this formula: dt Df (qtpŒ∏,t) = 1 2 g2(t)EpŒ∏,t (cid:34)(cid:18) qt pŒ∏,t (cid:19)2 (cid:19) (cid:18) qt pŒ∏,t log pŒ∏,t log qt2 . (A.56) (cid:35) We can re-weight Eq. A.55 for arbitrary weightings, where (t) is selected in our case. The re-weighted version of the RHS of Eq. A.55 can be written as: 1 2 g2(t)W (t)EpŒ∏,t (cid:34)(cid:18) qt pŒ∏,t (cid:19)2 (cid:19) (cid:18) qt pŒ∏,t log pŒ∏,t log qt2 dt. (A.57) (cid:35) (cid:90) 0 (cid:90) = (t) dt Df (qtpŒ∏,t)dt. = (t)Df (qtpŒ∏,t) (cid:12) (cid:12) (cid:12) (cid:12) 0 + (cid:90) 0 (t)Df (qtpŒ∏,t)dt. = (cid:90) 0 w(t)Df (qtpŒ∏,t)dt. (A.58) (A.59) (A.60)"
        },
        {
            "title": "B Detailed Analysis on f Divergence",
            "content": "In this section, we provide several example divergences derived from our Uni-Instruct framework. Tab. 5 summarizes five types of divergence. 20 Divergence FKL RKL JKL œá2 JS (r) log log (r 1) log (r 1)2 log (r + 1) log (cid:0)r + 1 C1(r) 0 1 0 1 0 4r2 0 (cid:1) r(2r+1) (r+1)2 2r2 - - - C2(r) Mode-Seeking? 2 Table 5: Comparison of different -divergences as function of the likelihood ratio := qt(x) pŒ∏,t(x) (r+1)3 (cid:17) (cid:16) q(x) p(x) Mode Seeking vs. Mode Covering For arbitrary divergence Df (qp) = (cid:82) p(x)f dx, it can be classified into two categories based on its mode seeking behavior. Divergences that are mode-seeking tend to push the generative distribution pŒ∏ toward reproducing only subset of the modes of the data distribution p. This selectivity is problematic for generative modeling because it can cause missing modes and reduce sample diversity. Such mode collapse has been noted for the integral KL loss employed in Diff-Instruct and DMD [33, 66]. convenient way to quantify mode-seeking behavior is to inspect the limit limr (r)/r: the smaller this limit grows, the stronger the mode-seeking tendency. Both reverse KL and JensenShannon (JS) divergences have finite value for this limit. By contrast, forward KL, Jeffrey KL, and œá2 yield an infinite limit, reflecting its well-known mode-covering nature, which tends to recover the entire data distribution q. In practice, we observed that mode covering divergences such as forward-KL and Jeffrey-KL achieves lower FID score. Grad(SIM) vs. Grad(DI) Another way to inspect different divergence is checking the gradient expression. It is worth mentioning that the gradient expression of Uni-Instruct is composed of Grad(SIM) and Grad(DI) (Eq. 3.6). For KL divergence (reverse, forward, Jeffrey), C2(r) = 0 and the gradient is only contributed by Grad(SIM). On contrary, when selecting œá2 divergence, C2(r) = 0 and the gradient is only contributed by Grad(DI). The gradient expression of Jensen-Shannon (JS) is combination of both. Training Stability However, during training we often observe training instability in Jensen-Shannon divergence and œá2 divergence, due to the complex expression of C1(r) and C2(r), which will result in higher FID score (Tab. 4). Tricks such as normalizing the weighting function or implementing the discriminator on the teacher model [63] can be applied to stabilize training. We leave this part to future work."
        },
        {
            "title": "C Unified Distillation Loss",
            "content": "In this section, we discuss how Uni-Instruct unifies previous diffusion distillation methods through recovering previous methods into special case of Uni-Instruct. We summarize the connections in Tab. 6. C.1 One Step Diffusion Model Distillation From Section 3.3 and Corollary 3.4, we have demonstrated that integral KL-based divergence minimization can be treated as Uni-Instruct with special weighting. More surprisingly, we found that if we choose œá2-divergence in Uni-Instruct, the weighting of SIM becomes 0 and the remaining gradient is only contributed by Diff-Instruct, as is shown in Tab. 5 and the third column of Tab. 6. In this way, Uni-Instruct can unify the first line of work: Diff-Instruct [33] is Uni-Instruct with œá2-divergence. DMD [66] added extra regression loss contributed by pre-sampled paired images, while DMD2 [65] added an adversarial loss. SwiftBrush [17] applied the same loss on text-to-image generation. -distill [63] can be seen as Uni-Instruct with manually selected weighting, and has gradient expression of (œá2) divergence in Uni-Instruct. Moreover, in Sec. 3.3, we demonstrate that leveraging the connection between KL divergence and score-based divergence, score matching can be interpreted as minimizing single-step KL divergence. Thus, selecting reverse-KL (RKL) divergence in Uni-Instruct, we can recover score-based divergence, 21 Loss IKL Div. Div. in UI œá2 Method Diff-Instruct (DI) [33] DI++ [32] DI [35] IKL Div. + Reward KL Div. + Reward SDS [44] IKL Div. DDS [14] IKL Div. VSD [59] IKL Div. DMD [66] RedDiff [37] DMD2 [65] Swift Brush [17] SIM [34] SiD [73] SiDA [72] IKL Div. + Reg. IKL Div. + Data Fedility IKL Div. + GAN IKL Div. General KL Div. KL Div. KL Div. + GAN SiD-LSG [71] KL Div. -distill [63] Uni-Instruct (Ours) I-f Div. + GAN Div. + GAN œá2 RKL œá2 œá2 œá2 œá œá2 œá2 œá2 RKL RKL RKL RKL œá2 All Task One-Step Diffusion Human Aligned One-Step Diffusion Human Aligned One-Step Diffusion Text to 3D Image Editing Text to 3D One-Step Diffusion Inverse Problem One-Step Diffusion One-Step Diffusion Loss Function (cid:82) w(t)DKL(pŒ∏,tq0)dt (cid:82) w(t)DKL(pŒ∏,tq0)dt +Lreward DKL(pŒ∏,tq0) +Lreward (cid:82) w(t)DKL(pŒ∏,tq0)dt (cid:82) w(t)DKL(pŒ∏,tq0)dt (cid:82) w(t)DKL(pŒ∏,tq0)dt (cid:82) w(t)DKL(pŒ∏,tq0)dt +LMSE (cid:82) w(t)DKL(pŒ∏,tq0)dt +LMSE (cid:82) w(t)DKL(pŒ∏,tq0)dt +Ladv. (cid:82) w(t)DKL(pŒ∏,tq0)dt Gradient Expression (spŒ∏,t (xt) sqt (xt)) xt Œ∏ Grad(DI) + Œ∏Lreward Grad(SIM) + Œ∏Lreward Grad(DI) Grad(DI) Grad(DI) Grad(DI) + Œ∏MSE Grad(DI) + Œ∏MSE Grad(DI) + Œ∏Ladv. Grad(DI) (cid:16) Œ∏ spsg[Œ∏],t sqt (xt) spŒ∏,t (xt) (xt) log qt(xtx0) (cid:17) (cid:17) One-Step Diffusion One-Step Diffusion One-Step Diffusion One-Step Diffusion One-Step Diffusion All DKL(pŒ∏,tq0) (cid:16) DKL(pŒ∏,tq0) Grad(SIM) DKL(pŒ∏,tq0)+Ladv. Grad(SIM) + Œ∏Ladv. DKL(pŒ∏,tq0) (cid:82) w(t)Df (q0pŒ∏,t)dt +Ladv. Df (q0pŒ∏,t) +Ladv. Grad(SIM) Œªf Grad(DI) + Œ∏Ladv. Œ∏Ladv. + ŒªDI Grad(DI) +ŒªSIM (x)Grad(SIM) Table 6: Distribution matching diffusion distillation loss family. Our method not only extends the distribution matching framework theoretically, but also unifies all previous gradient expressions with specific weightings. as shown in the third column of Tab. 6. In this way, SIM [34] and SiD [73] minimize Uni-Instruct loss with RKL. Additional adversarial loss is added in SiDA[72], while text-to-image distillation is applied in SID-LSG[71], both under the same Uni-Instruct(RKL) setting. Though our experiments on benchmark datasets have already demonstrated the superior performance of Uni-Instruct on distilling one-step diffusion model (Sec. 5). We believe Uni-Instruct can be further applied to large-scale datasets and text-to-image diffusion models. We leave that to future work. C.2 Text-to-3D Generation with Diffusion Distillation DreamFusion [44] and ProlificDreamer [59] propose to leverage text-to-image diffusion models to distill neural radiance fields (NeRF) [39], enabling efficient text-to-3D generation from fixed text prompt. DreamFusion utilizes pretrained text-to-image diffusion model to guide the optimization of NeRF network by performing score-distillation sampling (SDS). This method minimizes KL divergence that aligns the rendered images from NeRF with the guidance from pretrained diffusion model. ProlificDreamer further advances this concept by introducing variational distillation, which involves training an extra student network to stabilize and enhance the distillation process. Specifically, denote pŒ∏(xc, y) as the implicit distribution of the rendered image := g(Œ∏, c) given the camera with the rendering function g(, c), while q0(xyc) as the distribution modeled by the pretrained text-to-image diffusion model with the view-dependent prompt yc. ProlificDreamer approximates the intractable implicit distribution posterior distribution pŒ∏(xc, y) by minimizing the integral KL divergence between the diffusion-guided posterior and the implicit distribution rendered by NeRF: DIKL(pŒ∏(xc, y)q0(xyc)) := (cid:90) 0 w(t)EpŒ∏,t(xtc, y) (cid:20) log pŒ∏,t(xtc, y) qt(xtyc) (cid:21) dt. (C.1) 22 Utilizing Corollary 3.4, we observe that by choosing suitable weighting functions (t), the integral KL divergence used by ProlificDreamer corresponds to the reverse KL (RKL) version of Uni-Instruct: (cid:90) 0 w(t)DKL(pŒ∏,t(xtc, y)qt(xt))dt = (cid:90) 0 1 g2(t)W (t)EpŒ∏,t (cid:2)spŒ∏,t(x) sqt(x)2 2 (cid:3) dt, (C.2) ignoring (t) becomes the RKL loss function we applied in our experiments. Moreover, the gradient expression of DreamFusion and ProlificDreamer can be seamlessly unified under the Uni-Instruct framework, specifically aligning with the œá2 divergence case of Uni-Instruct (third column of Tab. 6). Our experiments indicate that employing Uni-Instruct with KL-based divergence in the text-to-3D setting slightly improves the quality of generated 3D objects (App. E). C.3 Solving Inverse Problems with Diffusion Distillation To solve general noisy inverse problem, which seeks to find from corrupted observation: = h(x) + v, (0, œÉ2 where the forward model is known, we aims to compute the posterior p(xy) to recover underlying signals from its observation y. The intractable posterior p(xy) can be approximated by q(xy) through variational inference, where := (¬µ, œÉ2I) is the variational distribution. Starting from minimizing the KL divergence between these two distributions, we have: (C.3) vI) DKL(pŒ∏(xy)p(xy)) = Eq(xy) [log p(yx)] + DKL (pŒ∏(xy)q(x)) + log p(y), (C.4) where the first term is tractable base on the forward model of the inverse problem and the third term is irrelevant to the optimization problem. RedDiff [37] proposed to estimate the second term with diffusion distillation. Specifically, they expand the KL term with integral KL through manually adding time weighting w(t): DIKL(pŒ∏(xy)q0(x)) := (cid:82) dt. Using Corollary 3.4, choosing (t) = (cid:82) w(t)dt + C, (0) = 0, we can recover the RKL version of Uni-Instruct: t=0 w(t)EpŒ∏,t(xty) log pŒ∏,t(xty) qt(xt) (cid:27) (cid:26) (cid:90) 0 w(t)DKL(pŒ∏,t(xty)qt(xt))dt = (cid:90) 0 1 2 g2(t)W (t)EpŒ∏,t (cid:2)spŒ∏,t(xy) sqt(x)2 2 (cid:3) dt. (C.5) C.4 Human Preference Aligned Diffusion Models Reinforcement learning from human feedback [42, 8] (RLHF) is proposed to incorporate human feedback knowledge to improve model performance. The RLHF method trains the model to maximize the human reward with Kullback-Leibler divergence regularization, which is equivalent to minimizing: L(Œ∏) = ExpŒ∏(x) [r(x)] + Œ≤ DKL (pŒ∏(x)qref (x)) The KL divergence regularization term penalizes the distance between the optimized model and the reference model to prevent it from diverging, while the reward term encourages the model to generate outputs with high human rewards. After the RLHF finetuning process, the model will be aligned with human preferences. (C.6) The KL penalty in Eq. C.6 can be performed with diffusion distillation when aligning the diffusion model with human preference. DI++ [32] propose to penalize the second term with IKL, which minimizes the KL divergence along the diffusion forward process: L(Œ∏) = Ezpz, x0=gŒ∏(z) xtx0p(xtx0) [r(x0)] + Œ≤ (cid:90) 0 w(t) DKL (pŒ∏(xt)qref (xt)) dt (C.7) Alternatively, DI [35] replaces the integral KL divergence with score-based divergence: L(Œ∏) = Ex0pŒ∏(x0) [r(x0)] + Œ≤ g2(t) spŒ∏,t(xt) sqt(xt)2 2 dt (C.8) 0 Leveraging Corollary 3.4, the integral KL divergence in Eq. C.7 is weighted version of KL divergence. Choosing (t) = (cid:82) w(t)dt + C, (0) = 0, we have: (cid:90) 1 (cid:90) 0 w(t)DKL(q0pŒ∏,t)dt = (cid:90) 0 1 g2(t)W (t)EpŒ∏,t (cid:2)spŒ∏,t(xt) sqt(xt)2 2 (cid:3) dt. (C.9) Moreover, the score based divergence in Eq. C.8 is minimizing KL divergence DKL(pŒ∏(x)qref (x)), based on Theorem 3.1, which recovers the RKL version of Uni-Instruct. The gradient of DI++ [32] and DI [35] takes the form of DI [33] and SIM [34], which correspond to œá2 and RKL divergence separately (third column of Tab. 6)."
        },
        {
            "title": "D Practical Algorithms",
            "content": "In this section, we present the detailed algorithms of our experiments. Algorithm 1 shows how to distill one-step diffusion model. Algorithm 2 shows how to distill 3D NeRF model. Algorithm 1: Uni-Instruct Algorithm on Distilling One Step Diffusion Model Input: pre-trained DM sqt, generator gŒ∏, fake score network sœà, discriminator DŒª, divergence , GAN weight wGAN, diffusion timesteps weighting w(t). 1: while not converge do 2: 3: 4: Sample real images and random noises: xreal pdata, œµ (0, I) Generate fake images: xfake = gŒ∏(œµ) Update DŒª with discriminator loss: LD = Exreal[log DŒª(xreal)] Exfake[log(1 DŒª(xfake))] Update sœà with denoising score matching loss: Ldiffusion = (cid:82) Calculate Uni-Instruct loss: LUni = Equation 3.5 Calculate adversarial loss (non-saturating): LGAN = Exfake[log DŒª(xfake)] Update gŒ∏ with total loss: Ltotal = LUni + wGAN LGAN 0 w(t) ExtxfakepŒ∏,t(xtxfake) sœà(xt, t) xt log pt(xt xfake)2 2 dt 5: 6: 7: 8: 9: end while 10: return gŒ∏ Details of 3D Experiments Experiment Settings In this section, we elaborate on the implementation details of Uni-Instruct on text-to-3D generation. We re-implement the code base of ProlificDreamer [59] by adding an extra discriminator head to the output of the stable diffusion Unets encoder. We apply forward-KL and reverse-KL to Uni-Instruct and train the NeRF model. To further demonstrate the visual quality, we transform the NeRF to mesh with the three-stage refinement scheme proposed by ProlificDreamer: (1) Stage one, we use Uni-Instruct guidance to train the NeRF model for 300400 epochs, based on the models performance on different text prompts. (2) Stage 2, we obtain the mesh representation from the NeRF model and use the SDS loss to fine-tune the objects geometry appearance for 150 epochs. (3) Stage 3: We add more vivid texture to the object through further finetuning with Uni-Instruct guidance for an additional 150 epochs. Additionally, we enhance the objects appearance with human-aligned loss provided by reward model. Performance Evaluations Fig. 4 shows the objects produced by the mesh backbone. Uni-Instruct produces more diverse results compared to ProlificDreamer and DreamFusion. Fig. 5 demonstrates more objects trained with the NeRF backbone. Tab. 7 shows the numerical results. Our method slightly outperforms the baseline methods."
        },
        {
            "title": "F Limitaions",
            "content": "Training an additional discriminator to estimate the density ratio brings extra computational costs and may lead to unstable training. For instance, we found that the output of 3D object trained with Uni-Instruct forward KL is more foggy than reverse KL, which doesnt require an extra discriminator. Additionally, Uni-Instruct suffers from slow convergence: Training Uni-Instruct on both 2D distillation and text-to-3D tasks takes twice as long as training DMD and ProlificDreamer on their respective tasks. Moreover, Uni-Instruct may result in bad performance with an improper choice of , as the gradient formula in Eq. 3.3 requires the fourth derivative of function , which will add complexity 24 Algorithm 2: Uni-Instruct for Text-to-3D Generation Input: pre-trained DM sqt, generator gŒ∏, fake score network sœà, discriminator DŒª, divergence , GAN weight wGAN, diffusion timesteps weighting w(t). 1: while not converge do 2: 3: 4: 5: Sample camera view and random noises: œµ (0, I) Render fake images from NeRF: = g(Œ∏, c) Sample real images and random noises: xreal pdata, œµ (0, I) Update DŒª with discriminator loss: LD = Exreal[log DŒª(xreal)] Exfake[log(1 DŒª(xfake))] Compute diffusion guidance: Ldiffusion = (cid:82) 0 w(t) ExtxpŒ∏,t(xtx) Compute Uni-Instruct loss: LUni (Equation 3.5) Update Œ∏ with LUni. (cid:13)spŒ∏,t(xt) sqt(xt)(cid:13) (cid:13) 2 2 dt (cid:13) 6: 7: 8: 9: end while 10: return gŒ∏ Figure 4: Prompt: refined vase with artistic patterns. From top to bottom : ProlificDreamer, Uni-Instruct (Forward-KL), Uni-Instruct (Reverse-KL). Our vase demonstrates more diverse shapes as well as realistic patterns. Figure 5: Results generated from our NeRF backbone. Prompts (From top to buttom): \"A thorny rose.\", \"A high-quality photo of an ice cream sundae.\", \"A sleeping cat.\", \"A baby bunny sitting on top of stack of pancakes.\" 25 Table 7: Comparison of different methods on Mesh and NeRF backbones. The prompt is: \"A refined vase with artistic patterns.\" Method NeRF Mesh 3D-Aes Score 3D-CLIP 3D-Aes Score 3D-CLIP DreamFusion [44] Fantasia3D [6] ProlificDreamer [59] Uni-Instruct (Forward-KL) Uni-Instruct (Reverse-KL) 1.07 - 2.15 2.46 4.45 27.79 - 30.97 31.35 33. - 2.76 4.91 4.83 7.54 - 30.96 31.92 31.74 34.56 to the gradient formula. Therefore, Uni-Instruct is not as straightforward as some simpler existing methods like Diff-Instruct. We hope to develop more stable training techniques in future work."
        },
        {
            "title": "G Additional Results",
            "content": "26 Figure 6: Forward-KL CIFAR10 conditional generation. 27 Figure 7: Jeffrey-KL CIFAR10 conditional generation. 28 Figure 8: Forward-KL ImageNet64 conditional generation. 29 Figure 9: Forward-KL ImageNet64 conditional generation."
        }
    ],
    "affiliations": [
        "Academy for Advanced Interdisciplinary Studies, Peking University",
        "College of Future Technology, Peking University",
        "National Biomedical Imaging Center, Peking University",
        "Xiaohongshu Inc",
        "Yuanpei College, Peking University"
    ]
}