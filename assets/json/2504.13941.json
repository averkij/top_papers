{
    "paper_title": "NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning",
    "authors": [
        "Syeda Nahida Akter",
        "Shrimai Prabhumoye",
        "Matvei Novikov",
        "Seungju Han",
        "Ying Lin",
        "Evelina Bakhturi",
        "Eric Nyberg",
        "Yejin Choi",
        "Mostofa Patwary",
        "Mohammad Shoeybi",
        "Bryan Catanzaro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown strong reasoning capabilities, particularly when enhanced through Reinforcement Learning (RL). While prior work has successfully applied RL to mathematical reasoning -- where rules and correctness are well-defined -- generalizing these methods to broader reasoning domains remains challenging due to limited data, the lack of verifiable reward structures, and diverse task requirements. In this work, we propose NEMOTRON-CROSSTHINK, a framework that systematically incorporates multi-domain corpora, including both synthetic and real-world question-answer pairs, into RL training to improve generalization across diverse reasoning tasks. NEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from varied sources spanning STEM, humanities, social sciences, etc.; (2) applying structured templates (e.g., multiple-choice and open-ended) to control answer-space complexity; (3) filtering for verifiable answers; and (4) optimizing data blending strategies that utilizes data from multiple sources effectively. Our approach enables scalable and verifiable reward modeling beyond mathematics and demonstrates improved accuracies on both math (MATH-500: +30.1%, AMC23:+27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%, GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover, NEMOTRON-CROSSTHINK exhibits significantly improved response efficiency -- using 28% fewer tokens for correct answers -- highlighting more focused and effective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that integrating multi-domain, multi-format data in RL leads to more accurate, efficient, and generalizable LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 1 4 9 3 1 . 4 0 5 2 : r NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning NEMOTRON-CROSSTHINK: Scaling Self-Learning beyond Math Reasoning Syeda Nahida Akter2, Shrimai Prabhumoye1,3, Matvei Novikov1, Seungju Han1, Ying Lin1, Evelina Bakhturi1, Eric Nyberg2, Yejin Choi1, Mostofa Patwary1, Mohammad Shoeybi1, Bryan Catanzaro1 NVIDIA1, Carnegie Mellon University2, Boston University3 sakter@andrew.cmu.edu, sprabhumoye@nvidia.com"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown strong reasoning capabilities, particularly when enhanced through Reinforcement Learning (RL). While prior work has successfully applied RL to mathematical reasoningwhere rules and correctness are well-definedgeneralizing these methods to broader reasoning domains remains challenging due to limited data, the lack of verifiable reward structures, and diverse task requirements. In this work, we propose NEMOTRON-CROSSTHINK, framework that systematically incorporates multi-domain corpora, including both synthetic and realworld question-answer pairs, into RL training to improve generalization across diverse reasoning tasks. NEMOTRON-CROSSTHINK addresses key challenges by (1) incorporating data from varied sources spanning STEM, humanities, social sciences, etc.; (2) applying structured templates (e.g., multiple-choice and open-ended) to control answer-space complexity; (3) filtering for verifiable answers; and (4) optimizing data blending strategies that utilizes data from multiple sources effectively. Our approach enables scalable and verifiable reward modeling beyond mathematics and demonstrates improved accuracies on both math (MATH-500: +30.1%, AMC23: +27.5%) and non-math reasoning benchmarks (MMLU-PRO: +12.8%, GPQADIAMOND: +11.3%, AGIEVAL: +15.1%, SUPERGPQA: +3.8%). Moreover, NEMOTRON-CROSSTHINK exhibits significantly improved response efficiencyusing 28% fewer tokens for correct answershighlighting more focused and effective reasoning. Through NEMOTRON-CROSSTHINK, we demonstrate that integrating multi-domain, multi-format data in RL leads to more accurate, efficient, and generalizable LLMs."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities across wide range of tasks, with Reinforcement Learning (RL) playing crucial role in refining their deep thinking abilities (Hu et al., 2025; Aggarwal & Welleck, 2025; Luo et al., 2025; DeepSeek-AI, 2025; Qin et al., 2024; Huang et al., 2025; Team, 2025b). Recent advances in RL have been particularly successful in mathematical reasoning and coding, where welldefined rules and verifiable correctness criteria enable effective reward modeling. However, extending these techniques to broader reasoning domains presents significant challenges, such asincluding limited training data for RL due to the difficulty of defining verifiable rewards, and ensuring generalization across diverse tasks. Recent work (Hu et al., 2025; Luo et al., 2025; Cui et al., 2025) has shown way to diversify RL training corpora by collecting datasets from multiple sources. However, they do not evaluate the relative importance of each source for downstream tasks, nor do they explore optimal data-blending strategies to maximize performance gains. Furthermore, prior research has largely focused on mathematical reasoning, overlooking the impact Work done during internship at NVIDIA 1 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning of incorporating non-math reasoning domains in RL-based learning for generalization in out-of-distribution domains. major challenge in applying RL to general-purpose reasoning tasks lies in designing verifiable reward model for diverse answer spaces, as unlike mathematical reasoningwhere correctness can be objectively verifiedother reasoning tasks lack deterministic solutions. Moreover, reasoning process varies across domains and question types. For instance, mathematical problem-solving follows rule-based, structured, and symbolic approach (Dehaene, 2011), whereas reasoning in fields such as law, physics, social sciences, and history often relies on narrative structures, contextual knowledge, and heuristic search strategies. Additionally, different question formats require distinct cognitive approaches open-ended questions demand the generation of novel responses from scratch, while multiple-choice (MCQ) questions can often be solved more efficiently by evaluating the given options and selecting the most appropriate answer. Incorporating diverse range of reasoning domains and question types into RL-based self-learning can enhance the broad reasoning capabilities of LLMs by exposing them to varied cognitive strategies and knowledge structures. In this work, we propose NEMOTRON-CROSSTHINK, systematic way to incorporate multidomain corpora for RL training that results in better generalization across wide variety of tasks. As demonstrated in Figure 2, NEMOTRON-CROSSTHINK comprises of phases that(a) curate data from diverse sources, including synthetic data from raw web texts (CommonCrawl) and open-source question-answer pairs, spanning STEM, humanities, law, and social sciences (b) apply templates (MCQ/Open-Ended) to limit the answer-space for synthetically generated data (c) filter out samples that are infeasible for verifiable rewards (d) prepare blending recipes to combine different sources of data efficiently and finally (e) employ self-learning with RL to refine reasoning capabilities in diverse domains. NEMOTRON-CROSSTHINK demonstrates that integrating multi-domain data with different questions formats for RL significantly enhances reasoning ability of LLMs across diverse reasoning tasks. Notably, models trained with NEMOTRONCROSSTHINK not only achieve higher accuracy but also exhibit dynamic response strategiesgenerating concise answers for general-purpose questions and more detailed responses for math problemsthereby reducing inference cost while preserving task-specific rigor. In addition, NEMOTRON-CROSSTHINK addresses the challenge of designing verifiable reward for non-deterministic domains by employing different templates on the curated data to limit the nuances in the answer space diversity. This enables scalable, verifiable reward modeling for general purpose reasoning tasks, ensuring that RL-trained models generalize effectively across diverse benchmarks. Furthermore, NEMOTRON-CROSSTHINK explores simple yet effective filtering approach to rank general purpose reasoning data based on complexity and shows that training with harder samples further amplifies the impact of RL across all domains. Figure 1: Employing self-learning with multidomain data, NEMOTRON-CROSSTHINK outperforms baseline models, including domainspecific training (Only Math) and OpenReasoner-Zero (ORZ-7B), achieving consistent gains across all reasoning tasks. In summary, our key contributions are as follows: We introduce NEMOTRON-CROSSTHINK, novel framework for incorporating multidomain corpora into RL training, enhancing the generalization of LLMs across diverse reasoning tasks with substantial gains across both math (MATH-500: +30.1%, AMC23: +27.5%) and non-math (MMLU-PRO: +12.8%, GPQA-DIAMOND: +11.3%, AGIEVAL: +15.1%, and SUPERGPQA: +3.8%) benchmarks. We demonstrate that applying question/answer templates to constrain output diversity leads to more stable reward modeling. Specifically, using unified open-ended question 2 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning format improves performance by 1.21% on average over mixed-format questions, while short-form answer templates outperform long-form ones by 1.20%. We explore optimal data-blending strategies to balance multi-domain corpora and show that only math data is not enough. Blending multi-domain data boosts average reasoning accuracy by up to 1.61% over math-only training and improves response efficiency by reducing token usage by 28%. We propose simple yet effective model-driven filtering technique that selects harder samples by removing data solvable by smaller models. This leads to an additional 2.15% average accuracy gain for Qwen-2.5-32B, highlighting the scalability of our approach to larger models. In this paper, we evaluate NEMOTRON-CROSSTHINK across three dimensions: (1) the effectiveness of different data blending strategies in self-learning (2) whether the blending impact amplifies by filtering and training with more complex data samples (3) the influence of question and answer templates in the downstream performance. Applying NEMOTRONCROSSTHINK on different data blends yields substantial improvement over base model, ranging from 8.55% to 13.36% improvement on average across seven diverse generalpurpose reasoning and mathematical benchmarks. The most effective blendconstructed using 2:1 ratio of general-purpose reasoning to math dataachieves the highest average accuracy, improving over the baseline by 13.36% (Figure 1). This underscores the effectiveness of conducting self-learning with combination of data from multiple reasoning domains to enable broader generalization. Our filtering experiment with Qwen-2.5-32B shows consistent trend, indicating that larger models can further amplify these gains with more complex samples in the data blend (2.15% average improvement), exceeding the improvements observed in the 7B setting. Additionally, our controlled template studies reveal that data formatting decisions play critical role in model performance. Overall, these findings illustrate that thoughtful choices in data blending, scaling, formatting, and filtering are critical to the success of reinforcement learning with language models. We hope that NEMOTRON-CROSSTHINK serves as practical and extensible framework for leveraging multi-domain data to train more capable, reliable, and generalizable models under the RL paradigm."
        },
        {
            "title": "2 NEMOTRON-CROSSTHINK: Scaling Self-Learning Beyond Math",
            "content": "Figure 2: NEMOTRON-CROSSTHINK. We (a) curate QA pairs from from synthetic (Common Crawl) and open-source datasets, categorized into general-purpose reasoning (Dgpr) and mathematical reasoning (Dmr); (b) apply structured templates to convert data into multiple-choice (MCQ) and open-ended formats, promoting diverse reasoning trajectories; (c) filter out unverifiable or ill-formatted responses; (d) train an RL policy using Group Relative Policy Optimization (GRPO). The final reward is used to update the policy, iteratively improving the models reasoning capabilities across diverse domains. In this work, we investigate reasoning domains beyond mathematics and analyze the impact of RL on LLMs trained with datasets from diverse domains and question formats. core pre-requisite for effective self-learning is access to high-quality, diverse, and rewardcompatible training data (Xie et al., 2025b; Hu et al., 2025). While mathematical reasoning has benefited from clean and verifiable datasets, extending RL to general-purpose reasoning domains remains underexplored due to the lack of structured, high-quality supervision. To address this, we explore methods for leveraging web documents and open-source QA 3 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning benchmarks to collect general-purpose reasoning data. Incorporating mix of structured and unstructured domains introduces wide range of cognitive patterns and task-specific reasoning strategies which will further improve generalization. However, it introduces noise and ambiguityparticularly in open-ended formatsmaking it difficult to apply rule-based reward modeling reliably. To mitigate this, we apply task-specific templates to unify question and answer formats, limiting answer space variability and enabling simple but effective verifiable reward signals. Next, we apply lightweight data filtering strategy to discard examples that are infeasible to verifysuch as open-ended answers exceeding certain length or MCQs with misaligned optionsensuring stable and interpretable RL training. Finally, we explore optimal data blending strategies that combine information across diverse domains and tasks. This allows us to investigate how the inclusion of generalpurpose reasoning data complements mathematical reasoning, ultimately leading to broader and more adaptive generalization in LLMs. Data Curation. We start with carefully curating datasets from multiple sources to ensure diversity in the training data. Our training dataset comprises two sources: = Dsyn Dos Here, Dsyn synthetically generated data from Common Crawl (CC) and Dos publicly available open-source QA datasets. Each sources of data further consists of question answer pairs related to general purpose reasoning and mathematics: Dsyn Dsyn_gpr Dsyn_mr; Dos Dos_gpr Dos_mr General Purpose Reasoning, Dgpr: We collect open source QA datasets (Dos_gpr) Natural Reasoning (Yuan et al., 2025) and MMLU [Train] (Hendrycks et al., 2021a) that span multiple domains, including STEM fields (e.g., Physics, Computer Science), Economics, Social Sciences, and more. To enhance diversity, we further synthesize QA pairs from CC documents using the wide range of domains in MMLU as our seed domain. We denote this dataset as Syn-QA (Dsyn_gpr). Dgpr Dsyn_gpr Dos_gpr Mathematical Reasoning, Dmr: As mathematical questions inherently require Chain-ofThought derivations which emphasizes the LLM to think, we incorporate math reasoning corpus to our training data. We combine open-source mathematical reasoning datasets (Dos_mr), such as MATH (Hendrycks et al., 2021b) and Numina-Math (Beeching et al., 2024). We generate additional math problems applying the similar technique as Ge et al. (2024) and define it as Persona-MATH (Dsyn_mr). Dmr Dsyn_mr Dos_mr Data Source Category Type Samples MMLU [Train] Syn-QA NATURAL REASONING NuminaMath PersonaSkill-MATH Math Applying Templates for Answer Space and Reasoning Diversity. General purpose reasoning benchmarks are often divided into two categories: (a) Multiple Choice Questions (Hendrycks et al., 2021a; Wang et al., 2024) and (b) Open-Ended Questions (Zhong et al., 2023). Recent works have ignored these variations in the answer space for consistent reward design across all tasks which are often predominantly math tasks (Hu et al., 2025; Aggarwal & Welleck, 2025; Luo et al., 2025). We hypothesize that each question type elicits different thinking patterns, leading to diverse reasoning trajectories in the model. Training on different question types will enhance the models ability to generalize by exposing it to diverse answer formats, thereby fostering different reasoning pathways. Therefore, to observe the effect of question type in RL training, we synthesize Dgpr using two templates: TMCQ - Multiple Choice Questions (MCQ), and TOpen - Open-Ended questions. Table 1: Training data distribution by source and type. OE=Open-Ended; GPR =General-Purpose Reasoning; MR =Math Reasoning 99,842 192,930 100,000 87,350 100,000 MCQ MCQ OE OE OE OE GPR GPR GPR MR MR MR 588,645 Total 4 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning We convert the MCQ datasets (MMLU) to open-ended by removing the options from the questions. Dmcq = TMCQ(Dgpr), Dopen = TOpen(Dgpr) Additionally, some MCQ questions are incomplete without options (e.g., Which of the following ways we can file taxes?). We discard such questions to avoid confusion during answer generation. Finally, our general purpose reasoning data, Dgpr, can be represented as: Dgpr = Dmcq Dopen Data Filtering and Formatting. To ensure high-quality training data, we apply series of filtering and formatting steps, H, to remove samples that are infeasible to evaluate with simple rule-based reward function. Specifically, for Dmcq, we check whether the correct answer appears within the question text itself. Given question-answer pair (q, a) with answer choices {a1, a2, . . . , an}, we discard sample if / {a1, a2, . . . , an}. For Dopen, such as those in the Natural Reasoning dataset, we discard samples that are challenging to evaluate with rule-based reward function. Formally, we retain samples where w(a) 10; w(a) represents the number of words in the answer a. Lastly, for the mathematical reasoning corpus, Dmr, we remove entries that lack an associated answer, ensuring that all retained questions have valid response a, i.e., we discard samples where = . = H(D) = (q, a, {a1, . . . , an}) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) {a1, . . . , an} (Dmcq) w(a) 10 = (Dmr) (Dopen) Data Blending. We study the impact of data diversity in three paradigms: Category Blend Name Symbol Blend Description Data Source Natural Distribution More Math More General Purpose Reasoning Question Types More MCQ More Open-Ended Data Usefulness Avg. Score Bnd Bmr Bgpr Bmcq Bopen Bscore Ratio of number of samples in dataset divided by the total number of samples in all the datasets. 2:1 ratio of Dmr and Dgpr 2:1 ratio of Dgpr and Dmr 2:1 ratio of Dmcq and Dopen 2:1 ratio of Dopen and Dmcq Provide weight to each source based on their average benchmark performances Table 2: Overview of Data Blending Strategies. Blends are categorized by data source, question type, and usefulnesseach constructed to assess the impact of domain diversity, format variation, and task relevance on RL-based reasoning. Data Source: We have gathered questions from diverse domains including math (Dmr), STEM, humanities, economics, history, law, social sciences, etc., (Dgpr) and observe the effect of each source on RL training. Question Types: We investigate the impact of question types in downstream tasks. Data Usefulness: We further analyze what is the contribution of each data sources in downstream task performances. We initially run RL using individual data alone and then evaluate them across diverse downstream tasks. Based on their performances, we create new blend. Based on these three categories, we construct six distinct blends, summarized in Table 2, with their corresponding dataset weight distributions detailed in Table 8. Reinforcement Learning with GRPO. We begin with pretrained large language model (LLM) and training blend B, where each sample contains only the input prompt and the final answer which is verifiable. We employ Group Relative Policy Optimization (GRPO) 5 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning (Shao et al., 2024). GRPO does not use separate critic model and instead estimates the baseline from group scores, improving efficiency and reducing memory. For each question q, GRPO samples group of outputs o1, o2, ..., oG from the old policy πθold and then optimizes the policy model πθ by maximizing the following objective: JGRPO(θ) = (cid:104) P(Q), {oi}G (cid:105) (Oq) i=1 πθold (cid:32) (cid:34) 1 i=1 1 oi oi t= min πθ(oi,tq, oi,<t) (oi,tq, oi,<t) πθold (cid:32) ˆAi,t, clip πθ(oi,tq, oi,<t) (oi,tq, oi,<t) πθold (cid:33) (cid:33) , 1 ϵ, 1 + ϵ ˆAi,t βDKL (cid:0)πθπref (cid:1) (cid:35) DKL [πθπref] = πref(oi,tq, oi,<t) πθ(oi,tq, oi,<t) log πref(oi,tq, oi,<t) πθ(oi,tq, oi,<t) 1. (1) where ϵ and β are hyperparameters, and ˆAi,t is the advantage, computed using group of rewards {r1, r2, ..., rG} corresponding to the outputs within each group: ˆAi,t = ri mean({r1, r2, ..., rG}) std({r1, r2, ..., rG}) Rule Based Reward Modeling. To guide the reinforcement learning process, we employ rule-based reward system designed for verifiable evaluation. Similar to (DeepSeek-AI, 2025), we define the total reward function as the logical and of an accuracy reward Racc and format reward Rformat: = Racc Rformat. This implies that the output will get reward only when both the answer and the format are correct. Accuracy Reward: The accuracy reward evaluates correctness based on whether the models response is similar to the ground truth solution to satisfy the correctness criteria: Racc(p, a) = (cid:26)1, if equal(p, a), 0, otherwise. Format Reward: The format reward ensures the response is structured according to predefined tags, where the reasoning will reside in <think></think> tokens and the final answer will be shown inside boxed{}: Rformat(a) = (cid:26)1, if F(a), 0, otherwise. where F(a) returns True if is correctly formatted and False otherwise."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Training Details. We adopt Qwen2.5-7B and Qwen2.5-32B (Team, 2024a) as our baseline models, M, which demonstrate strong generalization capabilities across various natural language reasoning tasks. We directly apply GRPO training on using the veRL framework1, which is an open-source implementation of the HybridFlow RLHF framework (Sheng et al., 2024). We train the base models with key settings including constant learning rate of 1e-6, batch size and PPO mini batch size of 128 and maximum context length of 5000 tokens. Each generation step contains 128 unique prompts sampled from the dataset, and 1https://github.com/volcengine/verl 6 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning performing 8 rollouts with temperature and top-p both set to 1.0. We set KL coefficient to 0.001 in all experiments. During training, the model is directly exposed to mixed types of questions from different domains. Note that we did not conduct extensive hyperparameter tuning, so one can expect further improvements with additional optimization. Evaluation Metrics. To comprehensively evaluate our models reasoning capabilities, we conduct experiments on diverse benchmarks spanning mathematical and general purpose reasoning. We evaluate our models on MATH-500 (Hendrycks et al., 2021b), AMC23, test set of MMLU (Hendrycks et al., 2021a), MMLU-PRO (Wang et al., 2024), AGIEVAL (Zhong et al., 2023), GPQA-DIAMOND (Rein et al., 2024) and SUPERGPQA (Team et al., 2025). Notably, SUPERGPQA is recent and rigorous benchmark designed to test the generalizability of LLMs across 285 graduate-level disciplines, including underrepresented domains like industry, agriculture, and service-related fields. Unlike existing benchmarks that concentrate on wellrepresented domains (e.g., math, law, physics), SUPERGPQA captures long-tail knowledge and includes wide range of real-world professional disciplines, making it reliable and discriminative frontier for evaluating generalizability in LLMs. For both open-ended and MCQ questions, we check the final answer inside the boxed{} format and compare with the ground truth solution. For MCQ benchmarks (e.g., MMLU, GPQA-DIAMOND, etc.), we format the ground truth in the test set to contain both the correct option and the option description to make it consistent with our training data. For each benchmark, we report accuracy averaged over 3 independent inference runs using greedy decoding."
        },
        {
            "title": "4 Experiments and Results",
            "content": "Analyze the effect of Individual Datasets. To prepare an effective blend using diverse sources of data, we begin by understanding impact of individual data sources on the selflearning paradigm so that we can prioritize the useful data sources and provide less weights to others. In this setup, we employ self-learning using M=Qwen-2.5-7B and taking each dataset separately. To make consistent comparison across different data sources, we keep the training recipe constant for all experiments. We run controlled experiments and train each models for fewer steps (250 steps) and evaluate them on the last checkpoint. Data Source MMLU [Train] Syn-QA Natural Reasoning NuminaMath PersonaSkill-Math Math MMLU MMLU-PRO GPQA-DIAMOND AGIEVAL SUPERGPQA MATH-500 AMC23 Avg 74.20 69.76 70.45 68.89 72.94 53.99 63.30 45.00 38.50 52.41 31.33 52.05 28.08 31.64 31.82 32.83 30.81 33.33 33.84 18.69 21. 48.59 47.66 52.10 46.65 54.39 45.69 51.95 25.36 27.69 24.57 22.44 26.97 16.92 18.31 48.30 22.00 54.20 68.60 76.20 77.20 78. 40.00 5.00 35.00 42.50 55.00 50.00 50.00 44.75 34.78 45.65 44.82 53.06 41.51 45.04 Table 3: Results of Self-Learning on Individual Datasets. Each row shows the downstream evaluation results after self-learning on single data source. Results highlight the varying strengths of individual datasets across general-purpose and mathematical benchmarks. Table 2 shows that different datasets have varying impacts on downstream accuracies across reasoning benchmarks. Notably, the NuminaMath yields the highest overall average, outperforming the baseline (M) by over 8.30%. Its strength is especially pronounced on mathematical tasks such as MATH-500 and AMC23 but additionally it achieves superior accuracies on general purpose reasoning tasks showing strong generalization across diverse domains. The Syn-QA dataset demonstrates 1.0% improvement over baseline with stronger accuracy in MMLU-PRO, AGIEVAL and MATH-500 tasks, suggesting that synthetically generated instruction-style data can generalize well when aligned with benchmark distributions. Natural Reasoning, despite modest scores on language-rich benchmarks, delivers surprisingly strong overall average, driven by high scores in MATH-500 and AMC23. This indicates that reasoning-focused datasets, even if less curated, can contribute meaningfully in math-adjacent tasks. On the other hand, Persona-Math, although strong in math, suffers from low generalization across most benchmarks. Finally, the MMLU [Train] dataset underperforms across most tasks, specifically in math reasoning domains, suggest7 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning ing that self-learning with raw MMLU [Train] data alone is insufficient for generalization. However, it obtains the best score for SUPERGPQA, which requires reasoning across wide range of cross-disciplinary domains. This highlights the potential of MMLU [Train] in capturing broad conceptual knowledge and supporting transfer to long-tail domains, making it valuable component when targeting general-purpose reasoning benchmarks. While preparing blends for Data Usefulness, we use the average accuracies of individual sources to obtain Bscore i.e., we provide more weight to datasets like Syn-QA, NuminaMath and less to MMLU [Train]. Analysis across Blends. We observe the effect of NEMOTRON-CROSSTHINK in three different categories using six different blends. To show the distinction between natural distribution and selective weighting of domains, we also prepare Bnd, which represents data sampled in proportion to each datasets original size. Additionally, to analyze the impact of within-domain training versus cross-domain blending, we introduce separate category called Single Source. We prepare two domain-specific blends: Bonly_mr, using only Dmr data, and Bonly_gpr, using only Dgpr data. We further compare NEMOTRONCROSSTHINK with recent math-centric self-learning approach, OPEN-REASONER-ZERO (ORZ) (Hu et al., 2025)which achieved superior accuracy in math benchmarks by training RL on combination of math data. For fair comparison we evaluate the 7B model using our eval setup. Model ORZ I S * Category Blend MMLU MMLU-PRO GPQA-DIAMOND AGIEVAL SUPERGPQA MATHAMC23 Avg 74.20 73.20 73.18 74.85 74.94 74.26 74.46 74. 74.24 72.77 Bnd Bmr Bgpr Bmcq Bopen Bscore Bonly_mr Bonly_gpr 45.00 48.90 54.81 55.51 57.82 55.77 55. 56.16 54.26 52.06 Data Source Question Types Data Usefulness Single Source 31.82 29.30 38.07 40.10 38.58 39.59 43.15 40.10 38.58 37. 48.59 63.49 59.99 61.47 63.71 62.54 61.28 59.80 61.39 56. 25.36 27.60 26.54 26.81 29.16 28.05 26.82 27.37 27.69 27. 48.30 81.40 77.00 77.80 77.60 78.00 78.40 78.00 78.60 72. 40.00 62.50 60.00 67.50 65.00 60.00 62.50 62.50 70.00 55. 44.75 55.20 55.66 57.72 58.12 56.89 57.49 56.95 57.82 53. Table 4: Results of NEMOTRON-CROSSTHINK-7B across Blends. Multi-domain blend Bgpr achieves the highest overall average accuracy, outperforming domain-specific and naturally sampled blendsunderscoring the benefit of self-learning with diverse reasoning data. (*) Due to the space shortage, we use *CROSSTHINK to refer NEMOTRON-CROSSTHINK. As shown in Table 4, each blending strategy consistently outperforms the base model, M, by significant margin. The natural distribution blend, Bnd, yields notable improvement of over 13% on average compared to M, suggesting that simply increasing the amount of training dataeven without rebalancingcan be beneficial. Bgpr from the Data Source category achieves the highest overall average, as well as the strongest results across most reasoning-focused benchmarks (e.g., +12.82% on MMLU-PRO and +15.12% on AGIEVAL). Notably, it performs relatively 5% on average better than ORZ. While Bonly_math performs slightly better on math-specific tasks, such as marginal 1% gain on MATH-500, it lags behind on non-math reasoning benchmarksunderperforming Bgpr by 34% on tasks like AGIEVAL, SUPERGPQA, and MMLU-PRO. The same trend is also seen with ORZ. To better understand these differences, we analyze sub-category accuracies in Appendix and find that Bgpr shows large relative gains in non-math categories while differences in math subcategories are either negligible or even favor Bgpr in some tasks. This highlights that general-purpose reasoning data offers strong cross-domain transfer with minimal compromise on math accuracy, making it more versatile. Both Bmcq and Bopen in Question Types category show consistent gains, with the latter achieving slight edge (0.6% improvement on average). In addition, Bopen yields stronger results on mathematical benchmarks. Mathematical problems are inherently open-ended in structure. As result, highlighting more open-ended domains aligns with the format and reasoning demands of math tasks. This suggests that diversity in question formatsespecially 8 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning open-ended reasoningcan better generalize to both general purpose reasoning and mathfocused downstream tasks. Regarding Data Usefulness, the score-based selection strategy (Bscore) outperforms the base model M, indicating the effectiveness of selective data curation. However, despite focusing more on the better performing datasets in Table 3, Bscore is overall worse than blends like Bmr or Bonly_math. This gap arises because Bscore assigns weights based solely on average dataset scores, without accounting for task-specific strengths. For instance, Math and Persona-Math receive higher weights than Natural Reasoning or MMLU due to their math accuracy, despite the latter performing significantly better on general-purpose reasoning tasks. In contrast, domain-aware blends selectively prioritize datasets based on their utility within specific domains, leading to more effective coverage and stronger scores across both math and general-purpose reasoning tasks. To investigate the impact of single-domain versus mixed-domain training data in RL, we compare the Single Source category with other blending strategies. Notably, Bonly_mr achieves the highest average math score (56.20%) among all blends, ranking as the second-best blend overall in terms of average accuracy. In contrast, while Bonly_gpr outperforms the base model M, it underperforms in mathematical reasoning tasks. Surprisingly, despite being tailored for general-purpose reasoning, Bonly_gpr also lags behind Bonly_mr by 4.2% on average across non-math reasoning benchmarks. This counterintuitive finding suggests that to obtain maximum gain in general purpose reasoning tasks we need to include mathematical problems in the training blend. As discussed earlier, Bgpr gets the best average reasoning accuracy which consists of both math and general purpose reasoning datasets. This confirms that math data alone is transferable to structured reasoning tasks, whereas general-purpose data is less effective when isolated."
        },
        {
            "title": "5 Ablations",
            "content": "NEMOTRON-CROSSTHINK is more token efficient in responses compared to Bonly_mr. To further understand the influence of multi-domain data in response generation, we compare the average token lengths of correct and incorrect responses between models trained on two blends: Bgpr and Bonly_mr. As shown in Figure 3, on general-purpose reasoning (GPR) benchmarks, Bgpr consistently outperforms Bonly_mr and ORZ (Hu et al., 2025), not only in accuracy (as shown in Table 4) but also in response efficiencyproducing correct answers with significantly fewer tokens2. For instance, on MMLU, the average token count for correct responses is 229 for Bgpr, compared to 351 for Bonly_mr. This demonstrates that exposure to multi-domain data enables the model to internalize more efficient reasoning strategy, leading to both improved performance and reduced inference cost. In contrast, on math-specific benchmarks, Bonly_mr and ORZ perform slightly better in accuracy, as expected due to domain alignment. Interestingly, correct responses are generally longer than in reasoning tasks as solving math problems inherently requires detailed, multistep derivations, hypothesis exploration, verification and refinement. Despite this, the Bgpr shows its adaptability by generating longer responses for math tasks and shorter ones for GPR tasksindicating dynamic response strategy learned through multi-domain training. As shown in Table 9, Bgpr has wide dynamic It increases range for generating responses. its average tokens by 62% when generating responses for math tasks (Mean Tokens=622) as opposed to general reasoning tasks (Mean Figure 3: Token efficiency comparison of models trained on Bgpr (multi-domain blend) and two single domain blends (Bonly_math and ORZ). 2Detailed categorization per task is shown in Appendix B. 9 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning Tokens=385). Whereas, Bonly_mr increases it average tokens only by 14% (Mean Tokens=731 for math tasks and Mean Tokens=639 for general reasoning tasks) showing much smaller dynamic range. This trend is also mirrored in ORZ, trained on high-quality blend of math datasets, which shows an even smaller increase (12%) in average token length across domains. This adaptive behavior highlights key strength of multi-domain training: it equips the model with the flexibility to tailor its response style to the nature of the task. By learning from diverse range of domains, Bgpr learns to reason efficientlyacross all tasks, Bgpr uses on average 28% fewer tokens for correct responses than Bonly_mrproducing compact yet accurate answers where appropriate, and detailed ones when necessary. Data Format Study: Question and Answer Templates. To better understand how training data formatting affects model performance, we conduct two controlled studies focused on question and answer template design, as shown in Table 5 and Table 6. In Table 4, we observe that Bopen outperforms Bmcq, suggesting that models trained on more open-ended data generalize better across benchmarks. This motivated us to investigate whether converting all questions into unified open-ended format leads to better performance. In Question Template Study, we use the natural distribution blend (Bnd) and only perturb the question template . To generate the open-ended variant, we remove the answer options from MCQs, prompting the model to produce an answer without selecting from predefined choices. Question Type MMLU MMLU-PRO GPQA-DIAMOND AGIEVAL SUPERGPQA MATHAMC23 Avg MCQ +OPEN-ENDED OPEN-ENDED 73.18 74.61 54.81 54.36 38.07 39.09 59.99 59. 26.54 29.16 77.00 76.60 60.00 65.00 55.66 56.87 Table 5: Impact of Question Format. Converting all questions to open-ended format improves accuracy across benchmarks, reducing reliance on option guessing and encouraging deeper reasoning. Table 5 illustrates that the open-ended-only configuration consistently outperforms the mixed-format setting across nearly all benchmarks, achieving 1.21% higher average score. Notably, it leads to significant improvements on reasoning-intensive and traditionally MCQformatted benchmarks such as MMLU, SUPERGPQA, and GPQA-DIAMOND. This result may be attributed to the inherent structure of MCQ questions, where random guessing can yield an accuracy of approximately 25% in MMLU and GPQA-DIAMOND benchmarks where we have only four options. In contrast, open-ended questions eliminate this guessing advantage, compelling the model to rely more heavily on reasoning to arrive at correct answer. By reducing the likelihood of reward hacking through random option selection, the open-ended format encourages more robust reasoning and leads to improved generalization. In the Answer Template Study, we investigate how the format of output labels influences training effectiveness on MCQ-style datasets. We compare two answer templates: Long - the model is trained to generate both the option label and its corresponding description (e.g., (A) The sky is blue), and Short - the model is trained to output only the option label (e.g., A). For this study, we use the Bonly_gpr blend, which primarily consists of MCQ datasets  (Table 1)  , making it ideal for analyzing the effects of answer formatting in this setting. Answer Type MMLU MMLU-PRO GPQA-DIAMOND AGIEVAL SUPERGPQA MATH-500 AMC23 Avg Long Short 72.77 74. 52.06 54.56 37.06 39.59 56.56 58.01 27.44 28.39 72.20 74.20 55.00 52. 53.30 54.50 Table 6: Impact of Answer Format. Using short-form answers improves accuracy by reducing output ambiguity and avoiding penalization from rigid reward functions in rulebased training. Table Table 6 shows that the short-form answer template consistently outperforms the long-form variant, with 1.20% improvement in average accuracy. This trend holds across 10 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning both reasoning and mathematical benchmarks. These results suggest that reducing the complexity of the output space helps minimize ambiguity and allows the model to better align its predictions with the structure of the question. Furthermore, when training with long-form answers using rule-based reward (e.g., exact string matching), the model is frequently penalized for minor deviations in phrasing, even when the correct option is selected. For instance, if the model outputs the correct option label but paraphrases the description slightly, the strict reward signal treats it as incorrect. This introduces noisy supervision and may hinder learning. While this issue could be mitigated by designing more flexible reward function (e.g., based on semantic similarity or option-label matching), our goal in this work is to keep the approach simple and interpretable. As such, we adopt naive rule-based reward for clarity and reproducibility, and leave more sophisticated reward designs for future investigation. Difficulty Filtering. Training with high-quality data is key factor in self-learning to ensure efficient and stable learning and to obtain correct reward signals. Recent works (Hu et al., 2025; Luo et al., 2025; Cui et al., 2025) have explored various filtering strategies to remove noisy reference answers from datasets, focusing on data that is easily verifiable using simple rule-based rewards. Zeng et al. (2025) further investigate data selection based on question complexity, showing that as the difficulty of the training data increases, the resulting model achieves better downstream accuracy. However, their approach relies on datasets like MATH-500 that come with predefined difficulty scores. In this work, we explore simple approach to estimate question difficulty for general purpose reasoning datasets that do not come with explicit difficulty labels. Specifically, we label questions as difficult if they are answered incorrectly by smaller model (Qwen-2.5-7B) in zero-shot setting and filter out the easy questions. The intuition is that questions easily answered by base model are likely to be knowledge-based or shallow in reasoning depth, whereas those it fails on are likely to require deeper reasoning or broader generalization. We construct two versions of our training dataset Bgpran unfiltered set containing all questions, and filtered set (B (gpr)) that retains only the difficult samplesand use them to train separate instances of larger model = Qwen-2.5-32B. Model Qwen-2.5-32B NEMOTRON-CROSSTHINK-32B Blend MMLU MMLU-PRO GPQA-DIAMOND AGIEVAL SUPERGPQA MATHAMC23 Avg 83.30 83.57 83.60 55.10 68.83 69.43 Bgpr (gpr) 40.40 46.70 49.75 62.77 73.90 75.82 33.16 37.99 38. 60.55 82.40 84.00 45.00 67.50 75.00 54.33 65.84 67. Table 7: Difficulty-Based Filtering. Filtering Bgpr to retain only hard examples (B (gpr)) yields consistent gains across all tasks, highlighting the effectiveness of selective training on challenging data. According to Table 7, this filtering approach results in consistent performance improvements across all evaluated benchmarks. While both filtered and unfiltered models outperform the original baseline Qwen-2.5-32B, the model trained on the filtered datasetdenoted as Bgprfachieves the highest accuracy on every task. The gains are especially prominent in complex benchmarks such as MMLU-PRO, GPQA-DIAMOND, AGIEVAL, and AMC23, where the filtered model improves by up to 28% over its unfiltered counterpart. On average, filtering boosts overall accuracy by 2.15%, notable gain considering that it comes from training on fewer but harder examples. This suggests that selectively training on challenging examples can yield more robust and generalizable models, likely due to stronger gradient signals and focus on harder-to-learn reasoning patterns."
        },
        {
            "title": "6 Related Work",
            "content": "Evolution of Reasoning in LLM. Large Language Models have demonstrated remarkable dominance across numerous Natural Language Processing tasks. To enhance the complex reasoning capabilities of LLMs, (Wei et al., 2022) introduce Chain-of-Thought (CoT), which incorporates multi-step intermediate reasoning before arriving at final conclusions. CoT exhibits significant advantages across multiple domains, including mathematics, science, 11 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning and programming. Subsequently, (OpenAI, 2024) further explore CoT and propose the Long Chainof-Thought framework. In Long CoT, LLMs demonstrate advanced cognitive behaviors such as reflection, verification, correction, and multipath exploration, thereby further enhancing their problem-solving capabilities in complex reasoning tasks. Moreover, Long CoT exhibits excellent test-time scaling properties, where increased computational resources correlate with improved reasoning outcomes. Models like QwQ (Team, 2024b; 2025b), DeepSeek-R1 (DeepSeek-AI, 2025), Kimi k1.5 (Team, 2025a), and InternThinker (Cai et al., 2024) have successfully experimented with Long CoT for enhanced reasoning, combining fine-tuning and Reinforcement Learning to elevate the performance of opensource reasoning models to unprecedented levels. Notably, subsequent models such as Open-Reasoner-Zero (Hu et al., 2025), Open-R1 (Face, 2025), O1-Replication (Qin et al., 2024; Huang et al., 2024; 2025), s1 (Muennighoff et al., 2025) and LIMO (Ye et al., 2025) observes significant benefits from Long CoT even in smaller models through simple distillation. Self-Learning beyond Math. High-quality training data are crucial for scalable ReasonerZero training. Most of the recent works emphasize mathematical benchmark-centric data (AMC, AIME, Math, Olympiads, and AoPS) for reinforcement learning (Hu et al., 2025; Aggarwal & Welleck, 2025; Trung et al., 2024; Ye et al., 2025; Zeng et al., 2025) as designing verifiable rewards is much easier for math tasks. They exclude problems such as multiple choice and proof-oriented problems which reduces the answer space diversity. MCQ type of questions are important for MMLU and other non-reasoning centric tasks. For rulebased reward model, the format of input data and the final answer is crucial and largely underexplored. Furthermore, their additional sources of data synthesis approach has no details making it infeasible to scale for domains other than math. The kind of data and the ratio of each type of data important for the overall improvement of LLMs across multiple benchmarks have yet to be explored. Data Sampling in RL. Recent works have widely explored the idea of combining data from multiple sources during RL training to enhance the diversity of reasoning tasks and improve model generalization (Hu et al., 2025; Luo et al., 2025; Zeng et al., 2025; Wen et al., 2025). These studies primarily concentrate on the mathematical domain, where rule-based correctness allows for straightforward reward modeling. In such setups, data sampling strategies are often driven by factors like question complexity or the ease with which answers can be verified algorithmically. For instance, questions are filtered or prioritized based on whether they are solvable with deterministic programs or satisfy certain symbolic constraints. notable direction is curriculum learning, where Xie et al. (2025b) utilizes synthetically generated puzzle-like data from Xie et al. (2025a) to control the difficulty level and study the progression of learning. However, these works remain narrowly focused on highly structured domains such as logic puzzles or math word problems. Yeo et al. (2025) has shown that including 50% of math and 50% of noisy verifiable data from WebInstruct462k Yue et al. (2024) yields best MMLU-PRO score in RL setupindicating the potential of mixing of domains in the training blend. However, it in unclear how this benefit is attributed to the inclusion non-math reasoning data as 68.36% of WebInstruct-462k is about math. They have performed filtering to obtain data with feasible verifiable reward and this is boost and prioritize the mathematical domain over other domains. Despite this progress, there is lack of systematic investigation into how including non-math reasoning datasuch as legal analysis, social science, commonsense inference, or historical interpretationaffects RL training. NEMOTRON-CROSSTHINK is the first systematic framework to incorporate multi-domain and multi-format data into RL, introducing verifiable reward mechanisms for non-deterministic domains and demonstrating that blending diverse reasoning sources leads to stronger generalization across benchmarks."
        },
        {
            "title": "7 Conclusion",
            "content": "We present NEMOTRON-CROSSTHINK, simple and scalable framework for improving the generalization abilities of LLMs through reinforcement learning with multi-domain corpora. By combining data from diverse reasoning domains and applying lightweight filtering and formatting strategies, NEMOTRON-CROSSTHINK enables consistent gains across both 12 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning general-purpose and mathematical benchmarks. Our best-performing blendconstructed with 2:1 ratio of general-purpose to math dataachieves 13.36% average improvement over strong baselines, with gains further amplified by difficulty-based filtering and thoughtful template design. Importantly, these benefits persist across model scales and task types, demonstrating that data diversity, not just data volume, is key to broader reasoning capabilities. NEMOTRON-CROSSTHINK offers practical recipe for building more generalizable, efficient, and reliable LLMs under the RL paradigmpaving the way for scalable self-learning beyond math."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.04697. Edward Beeching, Shengyi Costa Huang, Albert Jiang, Jia Li, Benjamin Lipkin, Zihan Qina, Kashif Rasul, Ziju Shen, Roman Soletskyi, and Lewis Tunstall. Numinamath 7b cot. https://huggingface.co/AI-MO/NuminaMath-7B-CoT, 2024. Zheng Cai et al. Internlm2 technical report, 2024. URL https://arxiv.org/abs/2403.17297. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Stanislas Dehaene. The number sense: How the mind creates mathematics. OUP USA, 2011. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github.com/huggingface/open-r1. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas, 2024. URL https://arxiv.org/abs/2406.20094. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021b. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-reasoner-zero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025. Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journeypart 2: Surpassing arXiv preprint o1-preview through simple distillation, big progress or bitter lesson? arXiv:2411.16489, 2024. Zhongzhen Huang, Gui Geng, Shengyi Hua, Zhen Huang, Haoyang Zou, Shaoting Zhang, Pengfei Liu, and Xiaofan Zhang. O1 replication journey part 3: Inference-time scaling for medical reasoning. arXiv preprint arXiv:2501.06458, 2025. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5BModel-by-Scaling-RL, 2025. Notion Blog. 13 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: strategic progress report part 1. arXiv preprint arXiv:2410.18982, 2024. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level googleproof q&a benchmark. In First Conference on Language Modeling, 2024. URL https:// openreview.net/forum?id=Ti67584b98. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Kimi Team. Kimi k1.5: Scaling reinforcement learning with llms, 2025a. URL https://arxiv. org/abs/2501.12599. M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Tianyang Pang, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Shanghaoran Quan, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jinyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025. URL https://arxiv.org/abs/2502.14739. Qwen Team. Qwen2.5: party of foundation models, September 2024a. URL https: //qwenlm.github.io/blog/qwen2.5/. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, 2024b. URL https: //qwenlm.github.io/blog/qwq-32b-preview/. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025b. URL https://qwenlm.github.io/blog/qwq-32b/. Luong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. ReFT: Reasoning with reinforced fine-tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 76017614, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.410. URL https://aclanthology.org/2024.acl-long.410/. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. 14 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025. URL https://arxiv.org/abs/2503.10460. Chulin Xie, Yangsibo Huang, Chiyuan Zhang, Da Yu, Xinyun Chen, Bill Yuchen Lin, Bo Li, Badih Ghazi, and Ravi Kumar. On memorization of large language models in logical reasoning, 2025a. URL https://arxiv.org/abs/2410.23123. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning, 2025b. URL https://arxiv.org/abs/2502.14768. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. URL https://arxiv.org/abs/2502.03387. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms, 2025. URL https://arxiv.org/abs/2502.03373. Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason Weston, and Xian Li. Naturalreasoning: Reasoning in the wild with 2.8m challenging questions, 2025. URL https://arxiv.org/abs/ 2502.13124. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems, 2024. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models, 2023."
        },
        {
            "title": "A Data Proportion across Blends",
            "content": "To better understand the data composition used in our reinforcement learning experiments, we report the proportion of each dataset in the six blending strategies introduced in Section 2. These proportions reflect how data is distributed across different sources depending on the specific blending paradigm: data source, question type, and data usefulness. Type Data Name Bnd 0.1696 MMLU Syn-QA 0.3277 NATURAL REASONING OPEN-ENDED 0.1699 OPEN-ENDED 0.1484 NuminaMath OPEN-ENDED 0.1699 Persona-MATH OPEN-ENDED 0.0145 MATH MCQ MCQ Bmr 0.0864 0.1670 0.0866 0.2943 0.3370 0. Bmcq 0.2251 0.4349 0.1149 0.1004 0.1149 0.0098 Bopen 0.1159 0.2241 0.2231 0.1949 0.2231 0.0190 Bgpr 0.1678 0.3242 0.1680 0.1516 0.1736 0.0148 Bscore 0.1296 0.1731 0.1683 0.2020 0.1579 0.1691 Bonly_math Bonly_gpr 0.2542 0.4912 0. 0.4460 0.5105 0.0435 Table 8: Proportion of each dataset in different blends. 15 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning"
        },
        {
            "title": "B Token Efficiency Analysis",
            "content": "Token Efficiency in Correct Responses. Understanding not only whether model answers correctly but also how efficiently it reasons is critical in real-world deployments, especially for reducing inference cost and latency. To this end, we analyze the token lengths of correct responses generated by models trained under different data blending strategies. Table 9 presents the minimum, maximum, and mean number of tokens used in correct answers across two task types: General Purpose Reasoning (GPR) and Math. We compare three models: (1) Bgpr (multi-domain training), (2) Bonly_math (math-only training), and (3) ORZ (a strong math-centric baseline model). Across GPR tasks, Bgpr produces the most concise correct responses, with mean of 385 tokens39.6% fewer than Bonly_math and 65.4% fewer than ORZ. This suggests that training with multi-domain corpora equips the model to reason more efficiently in less structured tasks, avoiding unnecessarily verbose responses. Task Type Model Bgpr Bonly_math ORZ Bgpr Bonly_math ORZ 10130.00 11330.25 12917.00 622.00 730.68 1257.00 2697.80 9594.00 8221. 385.41 638.57 1114.60 170.25 201.75 292.00 83.20 159.60 223.00 Mean Math GPR Max Min On math benchmarks, where detailed stepby-step derivations are essential, all models naturally generate longer outputs. However, Bgpr still demonstrates adaptability, producing appropriately longer responses compared to GPR, while keeping the output concise relative to Bonly_math and ORZ. This behavior underscores the ability of multi-domain trained models to dynamically adjust their reasoning strategy and verbosity based on task requirements. Table 9: Token length statistics (Min, Max, Mean) for correct responses across task types. Interestingly, ORZ exhibits the longest response lengths across both GPR and math tasks. While this aligns with its design as reasoning-heavy model, it also reflects less efficiencypotentially generating unnecessarily long chains of thought, particularly in domains outside its training focus. In summary, the token efficiency analysis reveals that Bgpr achieves favorable trade-off between accuracy and brevity, tailoring its reasoning depth to the complexity of the task. This reinforces the value of diverse, multi-domain training in promoting adaptable and cost-efficient language models. Thinking Long vs Thinking Accurate. Recent studies such as DeepScaler (Luo et al., 2025) have noted that incorrect answers often exhibit longer trajectories, leading to wasted computation and less efficient learning. Echoing this observation, we analyze the average token lengths of correct and incorrect responses for models trained on different blends: Bgpr, Bonly_math, and ORZ. As shown in Figure 4, incorrect responses are consistently and substantially longer than correct onesby 3.6 on average. This pattern holds across both generalpurpose and math reasoning tasks, suggesting that verbose reasoning does not guarantee correctness. In fact, longer responses often reflect the models uncertainty, overthinking, or repetitive CoT traces, rather than productive deduction. Figure 4: Average token lengths of correct and incorrect responses across general-purpose and math reasoning tasks for models trained on Bgpr, Bonly_math, and ORZ. 16 NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning Sub-category Accuracy Analysis To further support our observation that multi-domain training improves general-purpose reasoning while remaining competitive on math tasks, we analyze the number of correct responses across sub-categories in MMLU-PRO and AGIEVAL. Figure 5 and Figure 6 show the count of correct answers produced by Bgpr and Bonly_math across their respective subdomains. Figure 5: Sub-category Accuracy Comparison across MMLU-PRO Domains. The Bgpr blend consistently outperforms Bonly_math in wide range of non-math reasoning categories such as business, law, psychology, and economics. Surprisingly, it also slightly surpasses the math-specialized blend in the MMLU-PRO math category, highlighting the generalizability and versatility of multi-domain training. On MMLU-PRO, Bgpr consistently outperforms Bonly_math across non-math reasoning categories such as business, law, psychology, chemistry, and economics. Notably, it achieves relative improvements of +20.58% in law and +13.26% in business. Surprisingly, Bgpr also performs better in the math category (+7.2%), despite not being trained exclusively on mathematical data. This may be attributed to the nature of MMLU-PROs math problems, which are college-level and benefit from combination of symbolic and heuristic reasoningskills reinforced through exposure to diverse domains. Figure 6: Sub-category Accuracy Comparison across AGIEVAL. While Bonly_math performs marginally better in the math, Bgpr achieves stronger results in non-math domains. the AGIEVAL benchmark In contrast, (shown in Figure 6) features Olympiad-level math questions that are more abstract and complex. Here, Bonly_math has slight edge (+1.8%) in the math category, which aligns with its domain-specific training. However, Bgpr demonstrates stronger performance in symbolic and language-heavy domains, showing +13.06% improvement in Law and +9.88% in English. Averaged across all non-math reasoning categories, Bgpr achieves +8.6% relative gain over Bonly_math, reinforcing its advantage in general-purpose and real-world reasoning tasks. NEMOTRON-CROSSTHINK : Scaling Self-Learning beyond Math Reasoning Figure 7: Sub-category Accuracy Comparison across SUPERGPQA. The Bgpr blend consistently outperforms Bonly_math in wide range of non-math reasoning categories except the science category which consists of fields like mathematics, physics, astronomy, chemistry etc.highlighting the generalizability and versatility of multi-domain training. similar trend is observed in the SUPERGPQA sub-category analysis shown in Figure 7. Bgpr significantly outperforms Bonly_math across nearly all categoriesespecially in engineering, agronomy, economics, education, law, and philosophy. The only exception is the Science category, which includes math-heavy disciplines like physics, chemistry, and astronomy, where both blends perform comparably. This further highlights that multi-domain training with Bgpr enhances reasoning across broad spectrum of fields, achieving strong generalization even in real-world, professional domains that fall outside traditional math tasks."
        }
    ],
    "affiliations": [
        "Boston University",
        "Carnegie Mellon University",
        "NVIDIA"
    ]
}