{
    "paper_title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench",
    "authors": [
        "Lanxiang Hu",
        "Abhilash Shankarampeta",
        "Yixin Huang",
        "Zilin Dai",
        "Haoyang Yu",
        "Yujie Zhao",
        "Haoqiang Kang",
        "Daniel Zhao",
        "Tajana Rosing",
        "Hao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}."
        },
        {
            "title": "Start",
            "content": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench Lanxiang Hu Abhilash Shankarampeta Yixin Huang Zilin Dai Haoyang Yu Yujie Zhao Haoqiang Kang Daniel Zhao Tajana Rosing Hao Zhang University of California, San Diego Physical Commonsense: An inflated balloon is brought close to the flame of lit candle. transparent tube carrying steady stream of water pours out of the tube. Scientific Reasoning: An inflated balloon filled with small amount of water is brought close to the flame of lit candle. transparent tube carrying steady stream of water is taped to the front of speaker playing low-frequency sound. 5 2 0 2 2 ] . [ 1 2 4 9 2 0 . 2 1 5 2 : r Figure 1. VideoScience-Bench Overview. Top: Comparison of physical-commonsense versus scientific-reasoning generations using Sora2. The first scientific reasoning scenario hinges on recognizing that waters high specific heat capacity lets it act as heat sink, thereby preventing immediate balloon rupture. The second example requires reasoning over physical vibration, wave and material properties. Bottom-left: Subcategory frequency of questions From VideoScience-Bench. Note that we are using two-letter code in the pie chart that maps the subcategory provided in the legend. Bottom-right: Expert annotated model performance of seven video models on VideoScienceBench. The scores serve as the ground truth for our quantitative correlational analysis."
        },
        {
            "title": "Abstract",
            "content": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsensebased, offering limited insight into video models scientific 1 reasoning capability. We introduce VideoScience-Bench, benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along four dimensions: Immutability, Correct Dynamism, Spatio-Temporal Continuity, and Phenomenon Congruency. Using VLMas-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: github.com/hao-ai-lab/VideoScience. 1. Introduction The launches of Sora-2 [31] and Veo-3.1 [9] mark transformation in the video-generation landscape: video generation models [5, 24] now deliver cinematic-level visual fidelity and exhibit unprecedented world-modeling capability. As video models evolve, the communitys notion of what constitutes good video generation is shifting beyond distributional quality metrics such as Frechet Video Distance (FVD) [39] and Frechet Video Motion Distance (FVMD) [21] toward evaluations that probe models world-modeling capability [7, 11, 12, 14] and assess whether the generated videos follow commonsense physical regularities [2, 3, 1820, 27]. Recent models have even begun to exhibit zero-shot reasoning abilities, tackling tasks involving scientific, mathematical, and spatial reasoning [38, 40]. Human reasoners trained in science model the world using rich set of scientific principles and anticipate the resulting phenomena, like whether citric acid removes iron stains and what will happen if an LED is powered with or without resistor. This motivates our central question: beyond visual fidelity and physical commonsense, can video models correctly portray scientific phenomena and demonstrate scientific reasoning about the real world? World modeling grounded on scientific principles plays key role in advancing discovery by allowing models to predict outcomes of real-world processes. Unlike commonsense-based world modeling, true scientific world modeling requires reasoning across multiple interacting concepts, for example, Figure 1 shows that integrating specific-heat and heattransfer principles predicts water will absorb the heat and prevent immediate balloon rupture. However, existing video model benchmarks mostly focus on physical commonsense, restricting evaluations to everyday scenarios that only require basic understanding of physical laws [2, 3, 19, 27, 35]. We introduce VideoScience-Bench, benchmark designed to assess the scientific understanding and reasoning of video models. Each instance involves multiple scientific concepts and requires undergraduate-level knowledge, spanning 103 concepts across physics and chemistry. We evaluate whether video models can serve as zero-shot scientific reasoners, capable of understanding complex experimental setups and generating expected phenomena. The benchmark measures performance along five key dimensions: Prompt Consistency, Expected Phenomenon, Dynamism, Immutability, and Coherence. We also introduce VideoScience-Judge, checklistbased, automated VLM-as-a-Judge evaluation framework [10, 23]. VideoScience-Judge first generates promptspecific checklist to guide the evaluation, then selects key frames to identify causally salient moments in each video. These components jointly support the VLM-based scoring process, and the resulting scores are aggregated into single weighted metric reflecting the models overall reasoning quality. For validation, we validate the automated VLM judgments significantly better align with domain-expert annotations in comparison with other physical-commonsensebased and visual fidelity benchmarks. In summary, this paper investigates whether video generation models can reason over multiple interacting scientific concepts. To this end, we make three key contributions: 1. We introduce VideoScience-Bench, high-quality benchmark targeting zero-shot scientific reasoning in video generation models, encompassing real-world phenomena across 103 physics and chemistry concepts. 2. We develop VideoScience-Judge, checklist-based evaluation framework that leverages VLM-as-a-Judge, integrates key-frame selection and computer-vision tools for detecting physical cues (e.g., motion, object interactions), and outputs quantitative scores of scientific validity alongside perceptual quality. 3. We conduct comprehensive empirical study on VideoScience-Bench. While current video models produce high-quality, photorealistic and temporally coherent videos, VideoScience-Bench remains challenging: many systems still lack the capacity to model complex scientific phenomena and often violate basic physical laws. Among the evaluated models, Sora-2 and Veo-3 perform best and show early signs of scientific reasoning. 2 Figure 2. Overview of the data creation pipeline. Each researcher selects two or more scientific concepts and references relevant educational materials or videos to design prompt. Prompts undergo peer and model review, followed by model-based quality checking, before being finalized for dataset inclusion. 2. VideoScience-Bench VideoScience-Bench is designed to evaluate video models capability of generating correct physical and chemical outcomes for complex scientific events that require an understanding of multiple concepts. 2.1. Benchmark Overview Categorization. Leveraging established topic categorizations from undergraduate physics and chemistry, VideoScience-Bench comprises 160 questions covering 14 topics (9 related to physics and 5 related to chemistry) and 103 concepts (with 79 associated with physics and 24 with chemistry). Example topics include physics-related areas like Classical Mechanics and Optics, as well as chemistryrelated topics like Liquid Chemistry and Reaction Kinetics. The selected topics aim to reflect commonly encountered course titles in undergraduate science education, while the concepts provide more detailed categorization corresponding to specific chapters or sections within these courses. The category-frequency distribution for all prompts is illustrated in Figure 1. Distinction with Other Scientific Benchmarks. Current scientific benchmarks for video models primarily assess physical common sense. These benchmarks typically include daily scenes such as mirror reflections, bird flight, falling water, and bouncing balls, which can be understood with basic or high-school level of scientific knowledge [2, 3, 19, 27]. In contrast, each challenge in VideoScience-Bench requires the understanding of at least two undergraduate-level scientific principles. For instance, analyzing the bending of laser beam as it passes through sugar solution with concentration gradient requires the understanding and application of Snells Law in optics alongside principles from diffusion physics (see Figure 5). Data Splits. The VideoScience-Bench consists of two parts: the T2V split, which includes 160 question prompts, and the I2V split, which features set of 40 question prompts that have been paraphrased from the T2V split. The I2V split also includes first-frame images for the initial experimental setups. detailed description of the curation process can be found in Section 2.2. We provide several examples of each split in Appendix B. Metrics. VideoScience-Bench evaluates generated videos across five dimensions that reflect both perceptual quality and physical fidelity. The perceptual aspects include Immutability and Spatio-temporal Coherence, while the physical aspects are assessed through two levels: Phenomenon Congruency and Correct Dynamism. For the assessment to be meaningful, Prompt Consistency is essential, which means that the experimental setup and procedure must align with the prompt. These evaluation criteria are consistent with current practices in video-generation benchmarking, and correlational analysis is presented in Section 4.2. Each dimension is scored on four-point ordinal scale: 1 (absent or contradictory), 2 (weak or partially incorrect), 3 (mostly correct), and 4 (clearly correct). Below, we elaborate on the rating criteria for each of the five dimensions: Prompt Consistency: The experimental setup and procedures align with the descriptions provided in the prompt. Phenomenon Congruency: The observed outcomes are consistent with expected scientific principles. Correct Dynamism: All other fundamental physical laws are followed, including those governing motion and interactions among the objects involved. 3 Immutability: Objects remain unchanged when no transformation is anticipated. Spatio-Temporal Coherence:"
        },
        {
            "title": "Transitions between",
            "content": "frames are smooth, natural, and temporally consistent. 2.2. Benchmark Building In VideoScience-Bench, we adopt three-stage data construction pipeline as shown in Figure 2. Question Making. Following the categorization outlined in Section 2.1, eight domain-specific graduate student experts selected subsets of categories and authored 20 prompts each. Each prompt was designed to require reasoning involving at least two scientific concepts, ensuring sufficient level of difficulty. The prompts were based on public educational experiment examples [1, 29, 30] to guarantee scientific validity and to ensure that each target phenomenon could be feasibly represented in video of no more than 10 seconds. Among these contributors, two graduate students also curated 40 I2V questions, each paired with reference video that depicted the target experiment as the ground truth. The reference videos were sourced from public datasets and online resources [41, 44], with key frame extracted from the initial segment used as the conditional input image to illustrate the complete experimental setup. Panel Review and Quality Check. After drafting the full test suite, all participating graduate students convened as review panel. The panel cross-checked each experts set of 20 prompts to verify specificity, procedural accuracy, and clarity of the expected outcomes. As final quality assurance step, we employed capable video generation models, Sora-2 and Wan-2.5-T2V-Preview [36], to synthesize the opening seconds of each prompt, confirming that the key experimental components and setups can be captured. 3. VideoScience-Judge VLM-as-a-Judge. Recent studies have shown that powcan produce human-aligned judgments erful VLMs In when carefully prompted and calibrated [10, 23]. VideoScience-Bench, we adopt VLM-as-a-Judge framework and examine the performance gap between reasoningoriented VLMs and non-reasoning models such as GPT-4o. We find that directly prompting VLMs, particularly nonreasoning ones, to evaluate video generations often results in under-justified assessments and false-positive ratings (see Appendix E). This observation motivates the design of quantitative auto-eval harness featuring checklist-based grading scheme, where each rating must be supported by salient video frames and an evidence table aggregated from computer-vision analysis of the videos as shown in Figure 3. The full prompt template used in our VLM-as-aJudge framework is provided in Appendix C.2. Figure 3. Overview of our VLM-as-a-Judge pipeline, which combines checklist-based scoring, key-frame extraction, and evidence aggregation to ensure grounded and traceable video evaluations. Checklist Generation and Key Frame Extraction. To improve interpretability and reduce false positives, we incorporate prompt-specific checklists generated by an LLM agent, and the fraction of checklist items satisfied within each evaluation category is used to derive an ordinal rating on four-point scale. Each checklist is scored in reductive, itemized manner, where deductions are only made when supported by concrete evidence. In VideoScienceJudge, we leverage the ground-truth reference phenomenon and reference video (in the I2V split) to guide the VLM in identifying key frames where checklist violations occur, thereby providing evidence-grounded deductions. The prompt template used for checklist generation is provided in Appendix C.1. CV-Augmented Reasoning. VideoScience-Judge further incorporates deterministic CV modules that produce quantitative evidence for the judge. The toolbox includes: Grounding DINO for open-vocabulary-conditioned detection to verify entity presence and attributes in experiment setups [22], ByteTrack for identity association and to quantify spatio-temporal coherence [43], RAFT optical flow to capture and quantify motion direction and magnitude [37], as well as CLIP4Clip for textvideo alignment [26]. When reference video is available, we additionally report LPIPS similarity with VGG backbone [33, 42]. Each module outputs compact JSON records (frame second, type, region, score, attributes). Depending on the evaluation setting, the prompt aggregator optionally attaches checklist, an evidence table, both, or neither to the prompt, and instructs the model to convert the fraction of satisfied items per category into four-point rating. Aluminum-Iodine Reaction Prompt Expected few drops of water are added to small pile of powdered aluminum and iodine crystals in shallow dish. No reaction occurs before water is added. After water is added, the mixture spontaneously ignites with violetblue flash and smoke. Sora-2: Successfully Generated Expected Phenomenon PCS: 4/ PCG: 4/4 CDN: 4/4 IMB: 4/4 STC: 4/4 Hailuo-2.3: Failed to Generate Expected Phenomenon PCS: 2/4 PCG: 1/4 CDN: 4/ IMB: 4/4 STC: 4/4 Rotating Cups with Balls Prompt Expected Two plastic cups are joined mouth-to-mouth, with wooden stick fixed along the outside connecting them. One small ball is placed inside each cup, and the stick is spun rapidly around its center. As the system spins, both balls move outward and press against the sides of their cups. They remain in that outward position while the rotation continues. Sora-2: Failed to Generate Expected Phenomenon PCS: 2/4 PCG: 1/4 CDN: 3/4 IMB: 4/4 STC: 4/4 Veo-3: Failed to Generate Correct Experiment Setup PCS: 1/4 PCG: 2/4 CDN: 1/4 IMB: 2/4 STC: 2/4 Figure 4. Comparison of video generation models on VideoScience. Top: Aluminum-iodine reaction testing chemical dynamics. Sora2 correctly depicts the expected ignition phenomenon, while Hailuo-2.3 fails to generate the reaction. Bottom: Rotating cups with balls testing centrifugal force understanding. Both Sora-2 and Veo-3, the two highest-ranked models in our evaluation, fail to conduct correct experimental procedure and simulate the expected phenomenon accurately. Human Annotation Rating: Prompt Consistency (PCS), Phenomenon Congruency (PCG), Correct Dynamism (CDN), Immutability (IMB), and Spatio-Temporal Coherence (STC). 4. Experiments 4.1. Model Evaluation. Experiment Settings. We evaluate seven state-of-the-art video models on VideoScience-Bench: Sora-2 [31], Veo3 [9], Seedance 1.0 Pro [6, 8], Kling-v2.5-Turbo-Pro [17], Hailuo 2.3 [28], Ray2 [25], and Wan-2.5-T2V-Preview [36]. We generate three videos for each model in all of our experiments. Only one video is generated from Veo-3 due to the prohibitively high API cost. Human Annotation Results. We asked domain experts to rate video outputs of all the models based on the fivedimensional rubrics (defined in section 2.1). Using the human annotations, we calculated both per-criterion and aggregate model scores. This human-grounded evaluation serves as reference for analyzing the scientific reasoning performance of video models and the effectiveness of 5 It allows us to calibrate automatic VideoScience-Judge. VLM-as-a-Judge metrics and correlate judge predictions with expert assessments. The performance of the annotated models is summarized in Figure 1, and the weighted average scores across the five dimensions are presented in the legend. The weights assigned are as follows: Phenomenon Congruency (0.3), Prompt Consistency (0.2), Spatio-temporal Coherence (0.2), Correct Dynamism (0.15), and Immutability (0.15). These weights reflect the relative importance of accurately capturing the expected phenomenon, ensuring proper experimental setup, and maintaining procedural integrity. Model PCS PCG CDN IMB STC Sora-2 Veo-3 Kling-v2.5 Wan-2.5 Seedance-1.0-Pro Hailuo-2.3 Ray 3.32 3.01 2.77 2.87 2.56 2.39 1.65 2.56 2.35 1.91 1.84 1.78 1.67 1.26 3.33 2.83 2.75 2.83 2.52 2.57 2.13 3.73 3.30 3.36 3.36 3.15 3.16 2.44 3.71 3.42 3.60 3.46 3.46 3.46 2.92 Table 1. Raw human annotation scores (mean) per model and score dimension (14 Likert scale). Prompt Consistency (PCS), Phenomenon Congruency (PCG), Dynamism (CDN), Immutability (IMB), and Coherence (STC). The results from human annotations are summarized in Table 1 (raw scores) and Table 9 (min-max normalized scores). Table 1 show that while most models perform well in maintaining Spatio-temporal Coherence and Immutability, qualities commonly emphasized in existing video benchmarks [10, 14, 15], many still struggle to generate physically correct phenomena (Phenomenon Congruency) or adhere to other fundamental physical laws (Correct Dynamism). On Phenomenon Congruency, even the strongest closed-source systems, such as Sora-2 and Veo3, achieve only approximately 64% and 58.7% on the Likert scale, respectively, while the strongest open-source model, Wan-2.5, reaches only around 46%. Achieving high Prompt Consistency also remains challenging: even the best-performing models, such as Sora-2 and Veo-3, with average scores above 3, oftentimes fail to produce accurate experimental setups and procedures, frequently fail to produce accurate experimental setups and procedures, particularly in more complex scenarios involving interactions among multiple key objects. VideoScience-Judge Results. We evaluate different configurations of VideoScience-Judge on the same set of video generations, with results summarized in Table 2. For each method including the baselines, we compute ranking correlations with human annotations using Kendalls τ [16] and Spearmans ρ [34]. Compared to other baselines in the table, VideoScience-Judge achieves the highest alignment with human evaluations, demonstrating its effectiveness in capturing human-judged quality across our test suite. 4.2. Quantitative Analysis 4.2.1. VideoScience-Judge Design Choice the both using evaluate performance non-reasoning Model Choice. We of and VideoScience-Judge reasoning models as the backbone VLM judge. As shown in Table 7, non-reasoning models such as GPT-4o exhibit substantial deviations from human annotations, with strong tendency to assign inflated scores, leading to high false-positive rate. Replacing GPT-4o with reasoning model like GPT-5 pro mitigates the inflation by up to 57% on prompt consistency, yet the discrepancy remains noticeable, resulting in ranking misalignments with human judgments, as shown in the VSci-Judge column from Table 2. Configuration Choice. Table 3 compares evaluation scores from different configurations of VideoScience-Judge against human annotations. With GPT-5 pro, the score inflation is less prononced issue, but notable gaps remain in dimensions such as Dynamism, Coherence, and Immutability, where the VLM still tends to overestimate performance. Incorporating checklist as hint and requiring the model to reason each rating using evidence from key frames leads to noticeable reduction in overestimation across multiple dimensions. Further integrating CV tools as evidence sources, particularly ByteTrackwhich enables reliable identity association across frames and provides concrete evidence for penalizing frame jumps or unexpected object transformations. This substantially narrows the scoring gaps, by 0.07 on Coherence and 0.09 on Immutability respectively, compared with the vanilla VideoScience-Judge without the checklist. 4.2.2. Comparison with Baselines PhyGenEval. We adopt the PhyGenEval framework from PhyGenBench [27] to assess physical commonsense in VideoSciencevideos, employing GPT-4o as our VLM for video understanding and physical commonsense assessment, and CLIP to compute visual-semantic alignment between generated videos and reference prompts, with scores averaged across all prompts within each domain to obtain category-specific  (Table 4)  and total performance metrics  (Table 2)  . However, the significant correlation gap between VSci-Judge (ρ = 0.89) and PhyGenEval (ρ = 0.61) reveals fundamental limitations in the applicability of PhyGenEval to complex real-world video generation tasks. The three-tier hierarchical framework (key phenomena detection, physics order verification, overall naturalness) of PhyGenEval enforces rigid binary judgments at each stage, 6 Model VSci-Judge VSci-Judge (CL) VSci-Judge (CL+CV) PhyGenEval VideoScore2 T2VBench LMArena-T2V Sora-2 Veo-3 Kling-v2.5-Turbo-Pro Wan-2.5-T2V-Preview Seedance 1.0 Pro Hailuo 2.3 Ray2 τ () ρ () 0.76 0.69 0.63 0.66 0.63 0.58 0.54 0.81 0.89 0.75 0.65 0.59 0.60 0.56 0.52 0. 0.90 0.96 0.76 0.65 0.59 0.59 0.54 0.50 0.34 0.90 0.96 0.43 0.47 0.29 0.41 0.33 0.29 0.29 0.52 0.61 0.72 0.78 0.77 0.79 0.77 0.72 0. 0.24 0.29 0.79 0.80 0.78 0.79 0.78 0.77 0.76 0.62 0.75 1319 1361 1226 1128 1191 1253 1062 0.52 0.71 Table 2. We report our VLM-as-a-judge evaluation scores (using GPT-5 pro) for competing video models on VideoScience-Bench, and their ranking correlations to human annotations. VSci-Judge refers to VideoScience-Judge, CL to Checklist and CV to CV-augmented reasoning. For each model, min-max normalization is applied to annotation scores and scores from other benchmarks before averaging across all test cases and runs (except for LMArena-T2V). For each evaluation, the highest score is bolded and the second is underlined. Dimension w/o CL w/ CL w/ (CL +CV) 0.100.10 0.060.08 Prompt Consis. Exp. Phenomenon 0.220.10 0.200.11 +0.320.13 +0.270.10 Dynamism +0.280.08 +0.260.09 Coherence +0.270.12 +0.240.14 Immutability 0.050.09 0.200.11 +0.290.13 +0.210.09 +0.180. Table 3. Gap to human annotations from Table 9 (mean std) for each score dimension and VSci-Judge variants using GPT-5 pro. Larger positive indicates stronger over-estimation relative to annotators. leading to score quantization and reduced discrimination among similar models, while its rule-based approach of checking explicit violations of discrete physical laws (e.g., Does the ball fall downward?, Does water flow correctly?) cannot capture holistic aspects of physical realism such as consistent motion trajectories, causal relationships across time, acceleration patterns, momentum conservation, and nuances of motion smoothness that humans naturally evaluate. These limitations are particularly pronounced for VideoScience-Bench, which differs fundamentally from PhyGenBenchs simplified, single-principle scenarios. VideoScience-Bench involves multiple scientific concepts (e.g., multi-object interactions, fluid dynamics) where outcomes arise from combinations of laws rather than single principles, featuring cascading effects where second-order dynamics matter. VideoScore2. Although VideoScore2 [12] represents comprehensive multi-dimensional evaluation framework covering visual quality, text-to-video alignment and physical consistency, it exhibits limited effectiveness when evaluating complex scientific reasoning in video generation models. As shown in Table 2, VideoScore2s correlations with human annotations (τ =0.24, ρ=0.29) are noticeably lower than the proposed VSci-Judge (τ =0.81, ρ=0.89) and its checklist-augmented variants (τ =0.90, ρ=0.96). This gap can be attributed to the fact that VideoScore2s physical consistency dimension was designed to capture broad common-sense violations, rather than performing deeper scientific reasoning or evaluating physical dynamics. Our dataset, by contrast, emphasizes non-trivial physical processes and scientific constraints. Consequently, explicit physics decomposition through domain-specific checklists yields substantially more reliable rankings than generalpurpose rationales, validating the need for specialized evaluation frameworks when assessing nuanced physical reasoning in video generation. VideoScore2 is therefore not well suited for our benchmark, which requires strict, scientifically grounded physical evaluation, resulting in weaker alignment with human judgments on this dataset. T2V-CompBench. While T2V-CompBench [35] provides comprehensive evaluation of compositional textto-video generation across seven categories (attribute binding, spatial relationships, motion binding, action binding, and object interactions), VSci-Judge focuses specifically on complex physics reasoning with methodologically distinct approach. Using Llava-1.5-7B as the judge for the attribute binding category of T2V-CompBench, it achieves correlations τ = 0.62 and ρ = 0.75. In contrast, VSci-Judge achieves substantially higher human agreement (τ =0.81, ρ=0.89). This critical architectural difference demonstrates that T2V-CompBenchs VLM component struggles to capture nuanced physics violations, whereas VSci-Judges evidence-grounded VLM approach can integrate multiple signals to assess physical plausibility holistically. 4.3. Qualitative Analysis We provide qualitative examples illustrating how different models perform. As shown in the top portion of Figure 4, Sora-2 generates the expected physical phenomenon with strong prompt consistency and accurate motion. The video follows the described setup closely, exhibits coherent temporal evolution, and captures the ignition and color dynamics characteristic of the reaction. In contrast, Hailuo7 2.3, despite maintaining reasonable dynamism, immutability and spatial coherence, fails to respect key elements of the prompt, resulting in static video where the expected phenomenon never emerges. These differences underscore that correct experimental setup and prompt grounding are essential for achieving high scores on our benchmark, even when other dimensions appear satisfactory. In contrast to the successful cases, simpler physical scenarios still pose significant challenges for current models. The bottom portion of Figure 4 reveals that even Sora-2 and Veo-3, the two highest-ranked models in our evaluation, struggle significantly with scenario where text models can readily infer the expected phenomenon. Sora-2 generates setup that partially aligns with the prompt, but contains significant errors and fails to capture the expected phenomenon. Despite this, Sora-2 maintains relatively good dynamism, immutability, and spatio-temporal coherence. Veo-3 depicts the setup poorly, though it momentarily suggests the expected outward movement of the balls. This failure highlights that even seemingly simple physical setups remain challenging under our benchmark. 5. Related Work 5.1. Video Generation Models Recent breakthroughs in T2V generation have been driven by large-scale diffusion and autoregressive architectures capable of synthesizing high-resolution, temporally coherent videos from prompts. State-of-the-art systems such as Sora2 [31] and Veo-3.1 [9], demonstrate impressive visual quality and scene diversity. However, despite the rapid advances, these models often struggle with temporal consistency and physical plausibility in generated videos. To address these challenges, many efforts have focused on establishing comprehensive evaluation suites for video generation. Huang et al. introduce VBench [13], which decomposes video generation quality into multiple hierarchical dimensions containing subject identity consistency, motion smoothness, temporal flicker, and spatial relationships. VBench++ [15] extends this framework by incorporating additional dimensions such as compositionality, physical realism, and instruction following, offering broader coverage of both perceptual and reasoning-oriented aspects of video generation. Other works, such as WorldModelBench [19], VideoPhy [2], and Physion [4], further emphasize the need to measure dynamic coherence, physics adherence, and material interactions. 5.2. VLM-as-a Judge for Video Model Evaluation The use of vision-language models (VLMs) and multimodal large language models as automated judges for video generation has become increasingly prevalent. [11, 12]. Complementary to these judgers, frameworks like T2V8 CompBench [35] propose hybrid evaluation pipeline integrates both VLM-based and computer visionthat based metrics for compositional video understanding. Its VLM-as-a-Judge framework employs Grid-LLaVA to reason over sampled frames combined with detection-based and tracking-based modules (like GroundingDINO, SAM, Depth-Anything, and DOT) to create structured rubric that aligns semantic, spatial and motion dimensions. 5.3. Physical Commonsense Benchmarks for Video"
        },
        {
            "title": "Models",
            "content": "Much of our inspiration for this work comes from past benchmarks designed to evaluate VLMs physical reasoning capabilities. Benchmarks like PhyGenBench [27], VideoPhy [2], VideoPhy-2 [3], Physion [4], and IntPhys [32], all attempt to evaluate basic physical laws of video generation models, like mechanics, object interactions, and gravity. However, there is currently large gap in the space of video reasoning benchmarks that both require synthesis of multiple concepts across multiple physics domains at college undergraduate level and provide an auto-eval framework for evaluation. Our work, VideoScience, fills that need by providing complex examples that require fundamental understanding of the physical world. Our benchmark reveals that advancement is still needed for SOTA video models to truly understand physical phenomena. 6. Conclusion We introduce VideoScience-Bench, the first benchmark designed to evaluate scientific reasoning in video models via the generation of scientific phenomena. Unlike prior benchmarks focused on physical commonsense, VideoScienceBench emphasizes understanding and reasoning across undergraduate-level topics and concepts in physics and chemistry. We ask expert annotators to rate video generations from the seven latest models along five dimensions. The results show that while the latest video models achieve high visual quality and temporal coherence, they still struggle with following instructions and adhering to physical laws. To further support automatic evaluation, we develop VideoScience-Judge, checklist-based VLM-as-a-Judge framework, and requires the VLM judge to reason over evidence extracted from key frames from the video and CV-grounded evidence table. Correlation analysis between our auto-evaluation scores and human annotations shows that VideoScience-Judge achieves the strongest alignment with expert-rated rankings and best captures video models scientific reasoning capability in comparison with existing benchmarks. We hope VideoScience-Bench will foster progress toward video models that not only produce visually compelling results, but also reason faithfully about the underlying scientific principles."
        },
        {
            "title": "References",
            "content": "[1] Experiment Archive. Experiment Fun https : / / www . and easy science experiments. experimentarchive . com/, 2025. Collection of everyday-item experiments in biology, chemistry, physics, earth science and more. 4 archive: [2] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, KaiWei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. 2, 3, 8 [3] Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: challenging action-centric physical commonsense evaluation in video generation. arXiv preprint arXiv:2503.06800, 2025. 2, 3, [4] Daniel M. Bear, Elias Wang, Damian Mrowca, Felix J. Binder, Hsiao-Yu Fish Tung, R. T. Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, Li Fei-Fei, Nancy Kanwisher, Joshua B. Tenenbaum, Daniel L. K. Yamins, and Judith E. Fan. Physion: Evaluating physical prediction from vision in humans and machines. arXiv preprint arXiv:2106.08261, 2021. 8 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [6] ByteDance. Seedance 1.0 pro. Model card, 2025. Text-tovideo and image-to-video model; accessed 2025-11-12. 5 [7] Andong Deng, Taojiannan Yang, Shoubin Yu, Lincoln Spencer, Mohit Bansal, Chen Chen, Serena Yeung-Levy, and Xiaohan Wang. Scivideobench: Benchmarking scientific video reasoning in large multimodal models. arXiv preprint arXiv:2510.08559, 2025. 2 [8] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, Xunsong Li, Yifu Li, Shanchuan Lin, Zhijie Lin, Jiawei Liu, Shu Liu, Xiaonan Nie, Zhiwu Qing, Yuxi Ren, Li Sun, Zhi Tian, Rui Wang, Sen Wang, Guoqiang Wei, Guohong Wu, Jie Wu, Ruiqi Xia, Fei Xiao, Xuefeng Xiao, Jiangqiao Yan, Ceyuan Yang, Jianchao Yang, Runkai Yang, Tao Yang, Yihang Yang, Zilyu Ye, Xuejiao Zeng, Yan Zeng, Heng Zhang, Yang Zhao, Xiaozheng Zheng, Peihao Zhu, Jiaxin Zou, and Feilong Zuo. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 5 [9] DeepMind / Google. Veo 3: video generation system with audio from text or image prompt. https://storage. googleapis . com / deepmind - media / veo / Veo - 3TechReport.pdf, 2025. Latent diffusion audiovideo model. 2, 5, 8 [10] Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Yufan Deng, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, Jie Zhang, Chi Zhang, Li-jia Li, and Yongxin Ni. Video-bench: Human-aligned video generation bench9 mark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18858 18868, 2025. 2, 4, 6 [11] Xuan He, Dongfu Jiang, Ping Nie, Minghao Liu, Zhengxuan Jiang, Mingyi Su, Wentao Ma, Junru Lin, Chun Ye, Yi Lu, Keming Wu, Benjamin Schneider, Quy Duc Do, Zhuofeng Li, Yiming Jia, Yuxuan Zhang, Guo Cheng, Haozhe Wang, Wangchunshu Zhou, Qunshu Lin, Yuanxing Zhang, Zhang Ge, Wenhao Huang, and Wenhu Chen. Videoscore: Evaluating video generation with multi-dimensional criteria. arXiv preprint arXiv:2406.15252, 2024. 2, 8 [12] Xuan He, Dongfu Jiang, Ping Nie, Minghao Liu, Zhengxuan Jiang, Mingyi Su, Wentao Ma, Junru Lin, Chun Ye, Yi Lu, Keming Wu, Benjamin Schneider, Quy Duc Do, Zhuofeng Li, Yiming Jia, Yuxuan Zhang, Guo Cheng, Haozhe Wang, Wangchunshu Zhou, Qunshu Lin, Yuanxing Zhang, Zhang Ge, Wenhao Huang, and Wenhu Chen. Videoscore2: Think arXiv before you score in generative video evaluation. preprint arXiv:2509.22799, 2025. 2, 7, 8 [13] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. arXiv preprint arXiv:2311.17982, 2023. 8 [14] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, [15] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, YingCong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench++: Comprehensive and versatile bencharXiv preprint mark suite for video generative models. arXiv:2411.13503, 2024. 6, 8 [16] Maurice G. Kendall. new measure of rank correlation. Biometrika, 30(1/2):8193, 1938. 6 [17] Kuaishou Technology. Kling 2.5 turbo pro. Model announcement, 2025. Accessed 2025-11-12. 5 [18] Tzu-Wei Lee et al. VHELM: holistic evaluation of vision In NeurIPS Datasets and Benchmarks, language models. 2024. 2 [19] Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph E. Gonzalez, Ion Stoica, Song Han, and Yao Lu. Worldmodelbench: Judging video generation models as world models. arXiv preprint arXiv:2502.20694, 2025. 2, 3, 8 [20] Zhiqiu Lin et al. Genai-bench: Evaluating and improving compositional text-to-visual generation. arXiv preprint arXiv:2406.13743, 2024. 2 [21] Jiahe Liu, Youran Qu, Qi Yan, Xiaohui Zeng, Lele Wang, and Renjie Liao. Frechet video motion distance: metric for evaluating motion consistency in videos. arXiv preprint arXiv:2407.16124, 2024. 2 sive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. 2, 7, 8, [36] Wan AI Team. Wan 2.5 preview: Next-generation textto-video & image-to-video model. https://wan2-1. com/wan25 or https://blog.fal.ai/wan25-preview-is-now-available-on-fal/, 2025. Preview release; supports 1080p, 10-second clips, synchronized audio, text and image input. 4, 5 [37] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In European Conference on transforms for optical flow. Computer Vision (ECCV), pages 402419, 2020. 4 [38] Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, Xinchi Chen, Jun Zhao, Xuanjing Huang, and Xipeng Qiu. Thinking with video: Video generation as promising multimodal reasoning paradigm, 2025. 2 [39] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 2 [40] Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners, 2025. 2 [41] Yicheng Xu, Yue Wu, Jiashuo Yu, Ziang Yan, Tianxiang Jiang, Yinan He, Qingsong Zhao, Kai Chen, Yu Qiao, Limin Wang, Manabu Okumura, and Yi Wang. Expvid: benchmark for experiment video understanding & reasoning. arXiv preprint arXiv:2510.11606, 2025. [42] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In Proceedings of deep features as perceptual metric. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 4 [43] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. In European Conference on Computer Vision (ECCV), pages 121. Springer, 2022. 4 [44] Minghao Zou, Qingtian Zeng, Yongping Miao, Shangkun Liu, Zilong Wang, Hantao Liu, and Wei Zhou. Physlab: benchmark dataset for multi-granularity visual parsing of physics experiments. arXiv preprint arXiv:2506.06631, 2025. 4 [22] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 4 [23] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22139 22149, 2024. Open-access version at CVF Open Access. 2, 4 [24] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: General-purpose video diffusion transformers via mask modeling, 2023. [25] Luma AI. Ray2: Large-scale text-to-video generative model. Product page, 2025. Accessed 2025-11-12. 5 [26] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021. 4 [27] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Quanfeng Lu, Wenqi Shao, Kaipeng Zhang, Yu Cheng, Dianqi Li, and Ping Luo. Towards world simulator: Crafting physical commonsenseIn Proceedings of based benchmark for video generation. the 42nd International Conference on Machine Learning (ICML), pages 4378143806. PMLR, 2025. 2, 3, 6, 8, 17, 18 [28] MiniMax. Hailuo 2.3. Official news post, 2025. Accessed 2025-11-12. [29] Royal Society of Chemistry. Practical videos resource collections. https://edu.rsc.org/resources/ collections / practical - videos, 2025. Highquality chemistry practical videos supporting classroom experiments. 4 [30] The Wonder of Science. The wonder of science: Phenomena resource library. https://thewonderofscience. com/phenomenal, 2025. Open-access list of observable scientific phenomena aligned with NGSS standards. 4 [31] OpenAI. Sora 2: An advanced video generation model from openai. https://openai.com/index/sora2/, 2025. Released September 30 2025. 2, 5, 8 [32] Jean Riochet, Adam Lerer, Jennifer Lerer, Tomaso Poggio, Joshua B. Tenenbaum, Shimon Ullman, and Steven Zucker. Intphys: benchmark for visual intuitive physics reasoning. arXiv preprint arXiv:1803.07616, 2018. 8 [33] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015. 4 [34] Charles Spearman. The proof and measurement of association between two things. The American Journal of Psychology, 15(1):72101, 1904. 6 [35] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehen10 A. Test Suite Categorization A.1. Subcategory-to-Subject Mapping We define set of physics and chemistry subcategories, each with (two-letter code, subject) tuple: SUBCATEGORY_TO_SUBJECT = { \"Classical Mechanics\": (\"CM\", \"Physics\"), \"Optics\": (\"Op\", \"Physics\"), \"Thermodynamics\": (\"Th\", \"Physics\"), \"Fluid Mechanics\": (\"FM\", \"Physics\"), \"Electromagnetism\": (\"El\", \"Physics\"), \"Wave\": (\"Wv\", \"Physics\"), \"Energy\": (\"En\", \"Physics\"), \"Material Mechanics\": (\"MM\", \"Physics\"), \"Modern Physics\": (\"MP\", \"Physics\") \"Redox Reactions\": (\"RR\", \"Chemistry\"), \"Liquid Chemistry\": (\"LC\", \"Chemistry\"), \"Acid-Base\": (\"AB\", \"Chemistry\"), \"Reaction Kinetics\": (\"RK\", \"Chemistry\"), \"Material Chemistry\": (\"MC\", \"Chemistry\") } A.2. Concept-to-Subcategory Mapping Each fine-grained concept is mapped to exactly one subcategory. Here is the complete mapping used: CONCEPTS_TO_SUBCATEGORY = { \"Dispersion\": \"Optics\", \"Reflection\": \"Optics\", \"Refraction\": \"Optics\", \"Diffraction\": \"Optics\", \"Interference\": \"Optics\", \"Polarization\": \"Optics\", \"Total Internal Reflection\": \"Optics\", \"Light Spectrum\": \"Optics\", \"Light\": \"Optics\", \"Gravity\": \"Classical Mechanics\", \"Inertia\": \"Classical Mechanics\", \"Friction\": \"Classical Mechanics\", \"Momentum Conservation\": \"Classical Mechanics\", \"Projectile Motion\": \"Classical Mechanics\", \"Elastic Collision\": \"Classical Mechanics\", \"Inelastic Collision\": \"Classical Mechanics\", \"Pendulum\": \"Classical Mechanics\", \"Simple harmonic motion\": \"Classical Mechanics\", \"Hooke's law\": \"Classical Mechanics\", \"Rotational Dynamics\": \"Classical Mechanics\", \"Angular Momentum\": \"Classical Mechanics\", \"Gyroscopic Precession\": \"Classical Mechanics\", \"Newton's Laws\": \"Classical Mechanics\", \"Kinematics\": \"Classical Mechanics\", 11 \"Impulse\": \"Classical Mechanics\", \"Impact Mechanics\": \"Classical Mechanics\", \"Relative Velocity\": \"Classical Mechanics\", \"Kinetic Energy Loss\": \"Classical Mechanics\", \"Centripetal Force\": \"Classical Mechanics\", \"Centrifugal Force\": \"Classical Mechanics\", \"Buoyancy\": \"Fluid Mechanics\", \"Bernoulli Effect\": \"Fluid Mechanics\", \"Coanda Effect\": \"Fluid Mechanics\", \"Capillary Action\": \"Fluid Mechanics\", \"Fluid Dynamics\": \"Fluid Mechanics\", \"Fluid Flow\": \"Fluid Mechanics\", \"Hydrostatic\": \"Fluid Mechanics\", \"Non-Newtonian Fluid\": \"Fluid Mechanics\", \"Density and Buoyancy\": \"Fluid Mechanics\", \"Air pocket\": \"Fluid Mechanics\", \"Heat\": \"Thermodynamics\", \"Heat Transfer\": \"Thermodynamics\", \"Thermal conduction\": \"Thermodynamics\", \"Thermal convection\": \"Thermodynamics\", \"Heat Capacity\": \"Thermodynamics\", \"Thermodynamics\": \"Thermodynamics\", \"Statistical equilibrium\": \"Thermodynamics\", \"Pressure\": \"Thermodynamics\", \"pressure equilibrium\": \"Thermodynamics\", \"Gas Laws\": \"Thermodynamics\", \"Vacuum\": \"Thermodynamics\", \"Phase transitions\": \"Thermodynamics\", \"Phase Change\": \"Thermodynamics\", \"Condensation\": \"Thermodynamics\", \"Electrostatics\": \"Electromagnetism\", \"Magnetics\": \"Electromagnetism\", \"Magnetism\": \"Electromagnetism\", \"Electromagnetism\": \"Electromagnetism\", \"Simple circuit\": \"Electromagnetism\", \"Simple circuit lighting\": \"Electromagnetism\", \"Eddy Currents\": \"Electromagnetism\", \"Static Electricity\": \"Electromagnetism\", \"Gas Ionization\": \"Electromagnetism\", \"Sound\": \"Wave\", \"Sound Waves\": \"Wave\", \"Acoustics\": \"Wave\", \"Standing waves\": \"Wave\", \"Vibration\": \"Wave\", \"Resonance\": \"Wave\", 12 \"Wave Propagration\": \"Wave\", \"Energy Conversion\": \"Energy\", \"Energy Transfer\": \"Energy\", \"Energy Storage & Release\": \"Energy\", \"Structural Physics\": \"Material Mechanics\", \"stress-concentration\": \"Material Mechanics\", \"Fracture\": \"Material Mechanics\", \"Deformation\": \"Material Mechanics\", \"Stability\": \"Material Mechanics\", \"Equlibrium\": \"Material Mechanics\", \"Quantum Mechanics\": \"Modern Physics\", \"Plasma Physics\": \"Modern Physics\", \"Combustion\": \"Redox Reactions\", \"flammability\": \"Redox Reactions\", \"Redox Reaction\": \"Redox Reactions\", \"Galvanic Cell\": \"Redox Reactions\", \"Electrolysis\": \"Redox Reactions\", \"Electrochemistry\": \"Redox Reactions\", \"Acid-Base Reaction\": \"Acid-Base\", \"Indicator color change\": \"Acid-Base\", \"Solubility\": \"Liquid Chemistry\", \"Emulsion\": \"Liquid Chemistry\", \"Surface Tension\": \"Liquid Chemistry\", \"Surfactants\": \"Liquid Chemistry\", \"Viscosity\": \"Liquid Chemistry\", \"Density\": \"Liquid Chemistry\", \"Tyndall Effect\": \"Liquid Chemistry\", \"Chemical Reaction\": \"Reaction Kinetics\", \"Catalysis\": \"Reaction Kinetics\", \"Exothermic Reaction\": \"Reaction Kinetics\", \"Precipitation reaction\": \"Reaction Kinetics\", \"Polymers\": \"Material Chemistry\", \"Nano Science\": \"Material Chemistry\", \"Fluorescence\": \"Material Chemistry\", \"Phosphorescence\": \"Material Chemistry\", \"Protein Denaturation\": \"Material Chemistry\", } B. Test Suite Examples B.1. T2V Examples We present and discuss few examples of T2V generation from Figure 5 to Figure 8, where prompt describing the experimental setup is provided to the video model. More examples can be found in supplementary materials. Curved Refraction Gradient Prompt Expected transparent water tank is filled with sugar water of layered concentrations, placed in dark room where laser beam enters from the side to make the light path visible. The different densities of the two layers create distinct refractive indices. As the beam passes between them, it bends sharply (Snells law). Because the boundary is diffuse, the beam curves smoothly through the gradient, illustrating how refractive index changes with density. Sora-2: Successfully Generated Expected Phenomenon PCS: 4/4 PCG: 3/4 CDN: 4/4 IMB: 4/4 STC: 3/ Figure 5. Sora-2 generated video of Curved Refraction Gradient, where Sora-2 correctly depicts the setup and the expected phenomenon, despite the visual output being largely static. Human Annotation Rating: Prompt Consistency (PCS), Phenomenon Congruency (PCG), Correct Dynamism (CDN), Immutability (IMB), and Spatio-Temporal Coherence (STC). Water Separation Prompt Expected Add red food coloring to glass of hot water and blue food coloring to glass of cold water. Cover the hot water glass with thin plastic sheet. Carefully invert the glass of hot water, gently position it mouth-to-mouth on top of the glass of cold water and slowly remove the plastic sheet. The red (hot) water remains above the blue (cold) water with minimal mixing, forming stable boundary between the two layers. Sora-2: Failed to Generated Expected Phenomenon PCS: 3/4 PCG: 2/4 CDN: 3/4 IMB: 4/4 STC: 3/4 Figure 6. Sora-2 generated video of Water Separation, where Sora-2 fails to depict the expected phenomenon (the two colored liquids should barely mix). Human Annotation Rating: Prompt Consistency (PCS), Phenomenon Congruency (PCG), Correct Dynamism (CDN), Immutability (IMB), and Spatio-Temporal Coherence (STC). B.2. I2V Examples We present and discuss one example of I2V generation in Figure 9, where both prompt describing the experimental setup, and the first frame image are provided to the video model. More examples can be found in supplementary materials. C. Prompts for VLM-as-a-Judge C.1. Checklist Generation The checklist generation prompt template takes in the question prompt and the ground truth expected phenomenon to generate an itemized checklist for the VLM judge to pay attention to during the scoring process, and the VLM judge will be asked to 14 Prompt Expected Place small chunk of dry ice into tall glass filled with clear warm soapy water. large amount of white, misty bubbles rapidly forms and overflows the rim of the glass. Ray-2: Failed to Generated Expected Phenomenon Dry Ice Bubbles PCS: 1/ PCG: 1/4 CDN: 4/4 IMB: 4/4 STC: 3/4 Figure 7. Ray-2 generated video of water, which is not soapy, and instead of dry ice, some other substance is dropped into the water. Human Annotation Rating: Prompt Consistency (PCS), Phenomenon Congruency (PCG), Correct Dynamism (CDN), Immutability (IMB), and Spatio-Temporal Coherence (STC). Milky Limewater Prompt Expected Two clear cups are shown side by side. The left cup contains fizzing vinegar and baking soda; the right cup contains clear limewater. straw connects the two cups, guiding the gas from the left cup into the limewater in the right cup. The limewater quickly turns milky white, and bubbles rise from the bottom. Wan-2.5: Failed to Generated Expected Phenomenon PCS: 3/4 PCG: 2/4 CDN: 3/4 IMB: 4/ STC: 4/4 Figure 8. In the generated video, the limewater doesnt turn white, and no bubbles appear. In the left glass (which should be transparent), bubbles appear suddenly and randomly. Human Annotation Rating: Prompt Consistency (PCS), Phenomenon Congruency (PCG), Correct Dynamism (CDN), Immutability (IMB), and Spatio-Temporal Coherence (STC). Newtons Cradle Prompt Expected Five identical metal balls are suspended in straight line by thin strings so that they hang just touching each other at rest. One ball on the end is pulled back and released, allowing it to swing and strike the others. When the lifted ball strikes the row, the ball on the opposite end swings outward while the others remain nearly stationary, demonstrating conservation of momentum and energy. Veo-3: Failed to Generate Expected Phenomenon PCS: 4/4 PCG: 1/4 CDN: 1/4 IMB: 4/4 STC: 4/ Figure 9. Veo-3 generated video of Newtons Cradle conditioned on the first-frame image (from Wikipedia), where the video model fails to generate the expected phenomenon after the first strike to demonstrate conservation of energy. 15 reason and find to justify the scoring. Prompt for Checklist Generation From my evaluation of text2video models have generated video using the prompt Prompt: {prompt text} {reference source} Use reference source (ground truth phenomenon and video reference if available) to create the checklist (mandatory). Create comprehensive checklist targeting the following categories (note: not all categories are required for prompt). 1. PHENOMENON CONGRUENCY: Does the video show the correct expected phenomenon? 2. CORRECT DYNAMISM Are the physics dynamics and motion behaviors accurate? 3. SPATIO-TEMPORAL CONTINUITY Are spatial relationships and temporal sequences physically consistent? 4. IMMUTABILITY Do object properties remain physically consistent? 5. INTERACTION REALISM Do object interactions follow physical laws? Guidelines for checklist creation: - only target things which are visually observable in the video - the statements in checklist needs to be assertive statements instead of questions C.2. Scoring The scoring prompt template follows the setup in Figure 3, where the checklist and CV-based evidence table can be selectively enabled. The VLM judge is required to justify each rating using salient video frames, and when available, corresponding checklist items and CV-derived evidence. 16 Prompt for VLM as Judge RUBRIC TEXT You are VLM-Judge evaluating generated science video. Score each rubric from 14 (1=absent/contradictory, 2=weak/partly wrong, 3=mostly correct, 4=clearly correct): a) prompt consistency follows instructions: correct setup and correct experiment execution. b) expected phenomenon expected physical/chemical outcome is present and correct. c) immutability objects remain intact/unchanged unless changes are explicitly expected. d) dynamism other physical laws are obeyed. e) coherence natural transitions across frames; no flicker/teleport/identity swap. Each rating must be supported with clear justification, drawing on salient video frames and, when provided, the corresponding checklist items and CV-based evidence table. OUTPUT SCHEMA FORMAT Return JSON with fields: { scores: { prompt consistency:1-4, expected phenomenon:1-4, immutability:1-4, dynamism:1-4, coherence:1-4 }, explanations: {summary: string, issues: [string]}, evidence: {candidate: [{t:0.0s,observation:}], reference: [{t:0.0s,observation:}]} } Scoring Prompt: Question description question description Ground-truth phenomenon: phenomenon {OPTIONAL CHECKLIST} {OPTIONAL CV BASED EVIDENCE TABLE} {RUBRIC TEXT} {OUTPUT SCHEMA FORMAT} D. Baseline Evaluation Results PhyGenEval. While both VideoScience-Judge and PhyGenEval [27] address the critical challenge of evaluating physical commonsense in text-to-video models, they adopt complementary evaluation philosophies. PhyGenEval employs hierarchical three-tier framework that progressively assesses key physical phenomena detection, physics order verification, and overall naturalness using multiple specialized VLMs with GPT-4o-generated questions. In contrast, VideoScience-Judge grounds its evaluations in explicit, quantifiable evidence by integrating deterministic computer vision modules for entity verification, temporal coherence, motion analysis, and semantic alignment. They produce frame-level JSON records to support checklistbased deductions. This evidence-centric approach addresses the under-justified assessments and false-positive ratings we observed when directly prompting VLMs (Appendix E). Our evaluation of state-of-the-art models on VideoScienceusing the PhyGenEval framework  (Table 4)  reveals that even the best-performing model, Veo-3, achieves only an overall score of 0.47. According to PhyGenEval scores, all the models perform better on physics than on chemistry-related topics. Additionally, we also calculated τ and ρ by giving equal weightage to all five dimensions (See section 2.1) but the τ and ρ changed very insignificantly when compared to Table 2. 17 Model Total Physics Chemistry Ray2 Hailuo 2.3 Seedance 1.0 Pro Veo-3 Kling-v2.5-Turbo-Pro Wan-2.5-T2V-Preview Sora-2 0.29 0.30 0.33 0.47 0.29 0.41 0.43 0.29 0.31 0.34 0.47 0.30 0.42 0.43 0.26 0.22 0.29 0.43 0.21 0.35 0.40 Table 4. Physical commonsense evaluation scores across different models. Higher scores indicate better adherence to physical laws. Evaluated using the PhyGenEval framework [27] on VideoScience. T2VBench. We evaluate our full set of text-to-video models under the Consistent Attribute Binding metric from T2VCompBench. Videos are generated with three independent runs per model, and Llava-1.5-7B is used as the automatic judge. The scores are normalized at the end for correlation analysis. The evaluation reveals normalized scores ranging from 0.75 to 0.8, with Veo-3.1 achieving the highest score and Ray2 achieving the lowest. We find that the automatic metric aligns well with human annotations, achieving moderate-to-strong rank correlation. These results suggest that although current models perform somewhat similarly on controlled attribute-binding tasks, SOTA models such as Veo-3 exhibit more stable objectattribute rendering, whereas models like Ray2 and Hailuo 2.3 struggle with maintaining attribute fidelity across frames for different runs. Refer to Table 5 for more details. Model Veo3 Wan-2.5-T2V-Preview Sora-2 Seedance 1.0 Pro Kling-v2.5-Turbo-Pro Hailuo 2.3 Ray2 Mean Raw Score Mean Normalized 12.1998 12.0894 12.0583 11.9625 11.9450 11.8021 11.6900 0.7999 0.7921 0.7899 0.7830 0.7818 0.7716 0.7636 Table 5. Consistent attribute binding evaluation scores across different models in descending order, with Veo3 achieving the highest score and Ray2 the lowest. Higher scores indicate more consistent object-attribute binding in the videos. Evaluated using the T2VBench framework [35] on ScienceCompass. VideoScore2. We evaluate our full set of text-to-video models using the comprehensive multi-dimensional framework of VideoScore2. We report per-dimension scores and an overall aggregate (simple average across dimensions), min-max normalized to [0,1] for correlation analysis. This evaluation yields normalized aggregate scores ranging from 0.67 to 0.79, with Wan-2.5-T2V-Preview achieving the highest score and Ray2 achieving the lowest. To manage GPU memory constraints, we performed inference for VideoScore2 using half-precision float, standard practice that prioritizes resource efficiency without substantially affecting the models comparative ranking capability. notable disparity is observed between these automated scores and our human annotations. This weak alignment is attributed to the fact that VideoScore2 was primarily designed to assess everyday common-sense plausibility and simple physical rules, capturing surface-level, visually observable anomalies. It does not require deeper scientific reasoning or evaluation of the non-trivial physical dynamics emphasized by our dataset. Refer to Table 6 for the full score breakdown. E. VideoScience-Judge Model Choice In Table 7, we compare the performance of VideoScience-Judge when using non-reasoning model (GPT-4o) versus stateof-the-art reasoning model (GPT-5-Pro), both from OpenAI. The results reveal that GPT-4os ratings are heavily positively skewed, consistently overestimating video quality. Even after minmax normalization, its absolute deviations remain high, as much as 0.384 and 0.417 in dimensions like Coherence and Immutabiilty. Relacing 4o with GPT-5-pro reduces this discrepancy by up to 57%, demonstrating employing strong reasoning model is critical to produce more careful, less inflated judgments. 18 Rank Model Visual Quality TextVideo Alignment Physical Consistency Overall 1 2 3 4 5 6 7 Wan-2.5-T2V-Preview Veo-3 Seedance 1.0 Pro Kling-v2.5-Turbo-Pro Hailuo 2.3 Sora-2 Ray2 0.930.18 0.900.18 0.920.19 0.910.21 0.890.20 0.860.20 0.850.21 0.720.23 0.730.23 0.670.25 0.680.24 0.600.25 0.650.23 0.520. 0.730.27 0.700.28 0.730.27 0.710.28 0.680.28 0.650.28 0.640.28 0.790.17 0.780.17 0.770.18 0.770.19 0.720.18 0.720.18 0.670.18 Table 6. Overall quantitative results of VideoScore2 (mean standard deviation). Score dimension VSci-Judge-4o VSci-Judge-GPT-5-pro +0.372 0.129 Dynamism +0.223 0.120 Prompt consistency Phenomenon congruency +0.125 0.076 +0.417 0.145 Immutability +0.384 0.091 Coherence +0.315 0.131 0.096 0.098 0.218 0.103 +0.273 0.120 +0.278 0.083 Table 7. Gap to human annotations (mean std) for each score dimension and VSci-Judge variants using 4o and GPT-5 pro respectively. F. Details of Expert Annotation Results on VideoScience-Bench We present detailed human annotation rating breakdowns in Table 8 and normalized rating in Table 9, where min-max normalization is applied to each rating instance before averaging them across runs and test instances for each model. Model Prompt Consistency Phenomenon Congruency Dynamism Immutability Coherence Sora-2 Veo-3 Kling-v2.5-Turbo-Pro Wan-2.5-T2V-Preview Seedance 1.0 Pro Hailuo 2.3 Ray2 3.32 0.91 3.01 1.01 2.77 1.06 2.87 1.02 2.56 1.04 2.39 1.10 1.65 0.85 2.56 1.10 2.35 1.09 1.91 1.03 1.84 1.03 1.78 1.03 1.67 0.98 1.26 0.57 3.33 0.96 2.83 1.14 2.75 1.06 2.83 1.07 2.52 1.13 2.57 1.18 2.13 1.06 3.73 0.63 3.30 0.98 3.36 0.94 3.36 0.93 3.15 1.06 3.16 1.11 2.44 1. 3.71 0.71 3.42 0.90 3.60 0.72 3.46 0.89 3.46 0.84 3.46 0.82 2.92 1.13 Table 8. Raw human annotation scores (mean std) per model and score dimension (14 Likert scale). Model Prompt Consistency Phenomenon Congruency Dynamism Immutability Coherence Sora-2 Veo-3 Kling-v2.5-Turbo-Pro Wan-2.5-T2V-Preview Seedance 1.0 Pro Hailuo 2.3 Ray 0.796 0.677 0.567 0.602 0.507 0.425 0.129 0.697 0.586 0.419 0.384 0.354 0.298 0.131 0.775 0.551 0.530 0.558 0.438 0.462 0.268 0.818 0.638 0.661 0.663 0.571 0.602 0.302 0.732 0.601 0.691 0.614 0.616 0.610 0.385 Table 9. Human annotation scores (normalized) per model and per score dimension."
        }
    ],
    "affiliations": [
        "University of California, San Diego"
    ]
}