{
    "paper_title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique",
    "authors": [
        "Wasi Uddin Ahmad",
        "Somshubra Majumdar",
        "Aleksander Ficek",
        "Sean Narenthiran",
        "Mehrzad Samadi",
        "Jocelyn Huang",
        "Siddhartha Jain",
        "Vahid Noroozi",
        "Boris Ginsburg"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in reasoning-based Large Language Models (LLMs), particularly their potential through test-time scaling, have created significant opportunities for distillation in code generation and critique. However, progress in both areas fundamentally depends on large-scale, high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a dataset consists of 2.5M question-solution-critique triples (approx. 35K unique programming questions), making it nearly twice the size of the previous largest publicly available code reasoning dataset. In this work, we employ a two-stage supervised fine-tuning strategy. The first stage focuses on fine-tuning for code generation, while the second stage involves the joint training of models for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct models achieve performance in code generation that either exceeds or equals the best prior open-weight distilled models. Notably, the integration of our code generation and critique models leads to significant improvements in competitive coding performance. Furthermore, we present an extension of the LiveCodeBench benchmark to specifically support the C++ programming language, thereby facilitating more comprehensive LLM evaluation using this benchmark."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 5 7 0 9 0 . 7 0 5 2 : r OPENCODEREASONING-II: Simple Test Time Scaling Approach via Self-Critique Wasi Uddin Ahmad, Somshubra Majumdar, Aleksander Ficek, Sean Narenthiran, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Vahid Noroozi, Boris Ginsburg NVIDIA Santa Clara, CA 95051, USA {wasiuddina, smajumdar, aficek, snarenthiran}@nvidia.com https://huggingface.co/datasets/nvidia/OpenCodeReasoning-"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in reasoning-based Large Language Models (LLMs), particularly their potential through test-time scaling, have created significant opportunities for distillation in code generation and critique. However, progress in both areas fundamentally depends on large-scale, high-quality datasets. In this work, we introduce OPENCODEREASONING-II, dataset consists of 2.5M questionsolution-critique triples ( 35K unique programming questions), making it nearly twice the size of the previous largest publicly available code reasoning dataset. In this work, we employ two-stage supervised fine-tuning strategy. The first stage focuses on fine-tuning for code generation, while the second stage involves the joint training of models for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct models achieve performance in code generation that either exceeds or equals the best prior open-weight distilled models. Notably, the integration of our code generation and critique models leads to significant improvements in competitive coding performance. Furthermore, we present an extension of the LiveCodeBench benchmark to specifically support the C++ programming language, thereby facilitating more comprehensive LLM evaluation using this benchmark."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have undergone rapid advancements in recent years, from chain-ofthought (CoT) prompting (Wei et al., 2022), followed by System-2 reasoning that employs test-time compute scaling approaches (Li et al., 2025b; Zhang et al., 2025b). Test-time scaling has allowed LLMs to dedicate more computational resources during inference to perform logical reasoning. The release of models like DeepSeek-R1 (DeepSeek-AI et al., 2025), which demonstrated impressive reasoning capabilities, has spurred increased interest in distilling these test-time compute capabilities into smaller fine-tuned models. This pursuit is driven by the observation that providing LLMs with more computational power during inference directly translates to tangible improvements in their outputs, particularly on complex tasks like competitive coding. As models have improved on reasoning tasks by leveraging test-time compute, natural research question emerges: how can test-time compute be effectively scaled? Inference time scaling has been found to emerge from applying reinforcement learning on domains with verifiable outcomes such as with math and coding (Setlur et al., 2025; Qu et al., 2025; Yu et al., 2025a). Other works have found benefits by scaling inference compute by using repeated sampling in parallel (Wang et al., 2023a; Brown et al., 2024; Wu et al., 2025). After sampling multiples times the best solution can be selected using variety of methods such as majority voting, reward models or LLM-as-a-judge (Chen Preprint. Figure 1: Demonstrating performance gains on LiveCodeBench, achieved through test-time scaling by generating 10 solutions per problem and employing self-critique for selecting the final output. Self-critique led to the greatest performance boost in our finetuned model, OCR-2-32B. et al., 2024; Liu et al., 2025a; Zeng et al., 2025a; Moshkov et al., 2025). Recent works have found that scaling test-time compute also results in better critique models in the form of reasoning-based judges or Generative Reward Models (GenRMs) (Mahan et al., 2024; Zhang et al., 2025a; Liu et al., 2025b). Additionally, Wang et al. (2025b) introduces Critique Fine-Tuning (CFT), method that uses model-based critiques to more effectively distill reasoning capabilities than standard SupervisedFinetuning (SFT) for math problems. Our work unifies Critique Fine-Tuning (CFT) with reasoning data distillation to effectively scale test-time compute and enhance coding capabilities. Reasoning-based data distillation has proven to be powerful technique for enhancing coding performance, often without requiring reinforcement learning (BespokeLabs, 2025; Penedo et al., 2025b; OpenThoughts, 2025; Li et al., 2025a). Follow-up works have found continual gains from supervised fine-tuning on increasingly larger CoT reasoning datasets (Xu et al., 2025; Ahmad et al., 2025b). Despite the recognized benefits of critique fine-tuning and applying test-time compute to generative reward models, significant gap persists: there are no publicly available coding datasets that specifically include reasoning-based CoT critiques. Existing datasets with critique data feature non-reasoning critiques, execution test pass-rates, or focus on different domains (Ahmad et al., 2025a,b; Zeng et al., 2025a; Zhang et al., 2025a; Wang et al., 2024d). To bridge this gap and further leverage test-time scaling, we present OPENCODEREASONING-II, the largest publicly accessible code reasoning dataset created to date. This dataset comprises 2.5 million question-solution-critique triples originating from roughly 35,000 distinct programming problems. Both solutions and critiques are structured as reasoning CoTs, offering detailed justifications for solution generation and validation. Using this dataset, we trained models with 7B, 14B, and 32B parameters using two-stage finetuning approach. We conduct evaluation under parallel test-time scaling and demonstrated significant improvements compared to open-weight models, as illustrated in Figure 1. Furthermore, we contribute an extension to LiveCodeBench, with specific focus on supporting LLM evaluation using C++. The contributions of this work can be summarized as follows: 1. We introduce OPENCODEREASONING-II, large-scale dataset containing 1.4 million Python and 1.1 million C++ solutions, along with their corresponding critique labels, detailed reasoning traces, and execution pass rates, all derived from 35,000 unique programming questions. 2. With the aim of facilitating more comprehensive LLM evaluation in the C++ programming language, we have extended LiveCodeBench to incorporate C++ support. This enhanced benchmark is now publicly available to promote further research and development. 3. We show the effectiveness of OPENCODEREASONING-II by fine-tuning Qwen2.5-Instruct models in two-stage process. This fine-tuning enabled our models to achieve code generation performance that surpasses or matches the leading prior open-weight distilled models. Furthermore, integrating code generation with critique through simple test-time scaling strategy resulted in substantial gains on the LiveCodeBench benchmark, in both Python and C++. 4. We perform an in-depth analysis to provide insights on the opportunities in self-critique methods under test-time scaling, impact of data scaling, transfer between Python and C++ languages. Figure 2: Overview of the OPENCODEREASONING-II development stages."
        },
        {
            "title": "2 Development of OPENCODEREASONING-II and LiveCodeBench-C++",
            "content": "2.1 Construction of OPENCODEREASONING-II The construction of the OPENCODEREASONING-II dataset involved four-stage approach which is demonstrated in Figure 2. The initial stage consisted of compiling diverse collection of competitive coding problems with unit tests from various origins. Following this, large language model (LLM) equipped with reasoning capabilities was leveraged to produce corresponding solutions. Next, reasoning-capable LLM was employed to generate critique for these solutions. In the final stage, we obtained execution results for portion of the generated solutions. During stage-II and stage-III, the generated solutions and their corresponding critiques underwent post-processing stage aimed at guaranteeing their structural consistency. 2.1.1 Programming Questions Collection To create our dataset, we drew problems from the TACO corpus (Li et al., 2023), the APPS benchmark (Hendrycks et al., 2021), the CodeContests collection (Li et al., 2022), and CodeForces problems through the OpenR1 initiative (Penedo et al., 2025a). Due to the frequent overlap found in public datasets, we applied fuzzy matching-based de-duplication method, leading to final set of 34,799 unique questions of diverse difficulty. The distribution of these questions is detailed in Table 1. Contamination Assessment To ensure the integrity of OPENCODEREASONING-II, we rigorously investigated potential data leakage between our collected programming questions and major code generation evaluation suites (Jain et al., 2025; Li et al., 2022; Chen et al., 2021; Austin et al., 2021). Our methodology mirrored the protocol outlined by Yang et al. (2023), which involved computing the cosine similarity (with cutoff of 0.7) to identify the closest counterpart within the benchmark datasets for every distinct question in OPENCODEREASONING-II. We used Llama-3.3-70B-Instruct (Grattafiori et al., 2024) as judge to assess semantic similarity, and it identified 674 questions that potentially overlap with evaluation benchmarks. Following this decontamination, we proceeded to generate solutions for the remaining 34,125 programming questions. 2.1.2 Solution Generation using DeepSeek-R In this stage, we generated multiple solutions for each question leveraging the DeepSeek-R1 model (DeepSeek-AI et al., 2025). These solutions were generated in Python and C++ and sampled using Nucleus Sampling (Holtzman et al., 2020) with temperature 0.6 and top-p 0.95. We utilized SGLang (Zheng et al., 2024) for this generation process, allowing for maximum output sequence length of 32k tokens. The prompt used to generate solutions is provided in Figure 6. Post-processing and Filtering We post-processed and filtered the generated responses to ensure the required information were present in the output. Initially, we checked if each responses contained reasoning traces enclosed by the <think> and </think> tags. Next, we extracted the solution segments, separating the reasoning traces from the rest of the response. We then confirmed the presence of code blocks delimited by ```python . . . ```or ```cpp . . . ```. Finally, we used Tree Sitter (TreeSitter,"
        },
        {
            "title": "Python",
            "content": "C++ # Question # Sample # Question # Sample 2151 2080 3869 15641 2506 2670 2285 912 1235 71,681 64,468 120,040 834,523 79,771 58,154 73,559 26,106 39,938 29,926 2067 1988 3830 11887 2492 2668 2273 903 1209 775 35,471 62,493 171,882 355,180 155,162 167,610 82,765 43,867 49,699 50,"
        },
        {
            "title": "Total",
            "content": "34,125 1,398,166 30,092 1,174,475 Table 1: Number of questions and corresponding samples in OPENCODEREASONING-II, spanning across ten programming platforms. 2013) to validate the syntactic correctness of these code blocks. Notably, these filtering procedures led to the removal of very few responses. 2.1.3 Critique Generation using QwQ-32B In this stage, we prompted QwQ-32B (Team, 2025b) to to generate critiques for the programming questions and their corresponding code solutions. The specific prompt used to generate critique is detailed in Figure 7. We employed the same generation settings as for solution generation: temperature-based nucleus sampling via SGLang with maximum output length of 24k tokens. Following similar post-processing and filtering approach to solution generation, we verified that each critique contained reasoning traces (within <think> and </think> tags) and final judgment (within <judgment> and </judgment> tags). We retain responses only if their final judgment is binary: either right or wrong. Otherwise, we discard them. The rationale behind this choice stems from our preliminary experiments. When evaluating reasoning-enabled LLMs (R1 and QwQ-32B) with binary, categorical (correct, partially correct, incorrect), and numeric (1-5) judgment options, we observed significant tendency for the models to favor binary responses (such as 1 or 5, or correct/incorrect). Consequently, we adopted binary judgment generation for our critique responses. 2.1.4 Verifying Solutions with Unit Tests In the final stage of OPENCODEREASONING-II construction, we executed the generated code solutions against their corresponding unit tests, which were collected alongside the questions from public benchmarks. To ensure meaningful and manageable execution outputs, we selected subsample of OPENCODEREASONING-II where each question had at least 5 unit tests, with maximum of 50 randomly selected if more were available. This subsample comprised 60% of OPENCODEREASONINGII. Following execution, we calculated the pass rate, which is included in the public release of OPENCODEREASONING-II. We expect these execution outputs to facilitate future research, including the application of offline reinforcement learning for LLM improvement. 2.2 Extending LiveCodeBench for C++ While LiveCodeBench (Jain et al., 2025) aims to provide contamination-free evaluation of LLMs for code, its limitation to Python hindered our ability to assess LLMs on C++, widely used language in competitive coding. To address this, we extended LiveCodeBench to include C++. We selected problems from release_v5 within the date range of 2408 to 2502, resulting in 279 problems (175 from AtCoder and 104 from LeetCode). Notably, AtCoder problems utilize standard input/output for testing, whereas LeetCode problems provide starter code, requiring function invocation for evaluation. We collected the C++ starter code for LeetCode problems and adapted their test cases to enable evaluation in our extended benchmark. The dataset is publicly available at https://huggingface.co/ datasets/nvidia/LiveCodeBench-CPP."
        },
        {
            "title": "3 A Simple Test-time Scaling Approach via Self-Critique",
            "content": "To showcase the potential of OPENCODEREASONING-II, we establish straightforward self-critiquebased test-time scaling approach as simple baseline for future research. This section outlines our fine-tuning methodology and the subsequent inference setup for test-time scaling. Finetuning Setup We fine-tuned the Qwen2.5-Instruct models in two stages. Stage involved fine-tuning for code generation, followed by Stage II where we jointly fine-tuned for both code generation and self-critique. We used the same prompts for fine-tuning as those employed for data generation (illustrated in Figure 6 and Figure 7). The models underwent three epochs of fine-tuning in Stage and one epoch in Stage II. While this work presents our initial approach, we plan to investigate more advanced fine-tuning techniques in the future. Inference Setup for Test-Time Scaling At inference time, we prompt the fine-tuned models to first produce solution to programming question and then to critique their own output (self-critique). This process facilitates parallel scaling (Zeng et al., 2025b), where multiple solutions are generated concurrently, and the best is chosen as the final result. The success of parallel scaling hinges on two factors: (1) coverage, the probability of generating at least one correct solution, and (2) the selection methods accuracy in identifying correct solution if one or more solutions are labeled as correct. In this study, we evaluate coverage using pass@k and selection efficacy using pass@1select@k (which we term as critique@k). limitation of our work is the binary nature of the self-critique judgments, which necessitates strategy for selecting the best solution among multiple (right) generations. Motivated by the findings of Wang et al. (2025a), which found that the longer reasoning traces often correlate with incorrect final solutions, we utilize simple heuristic to choose the right solution from pool of \"right\"-labeled candidates: select the solution with the shortest critique reasoning trace. detailed comparison against randomized selection approach is provided in Appendix A. Recognizing the naivety of this method, we leave the investigation of more sophisticated selection techniques for future work."
        },
        {
            "title": "4 Main Evaluation",
            "content": "Training and Inference Hyper-Parameters By leveraging OPENCODEREASONING-II, we gauged the efficacy of supervised fine-tuning (SFT) through the adaptation of Qwen2.5-Instruct models, spanning parameter counts of 7B, 14B, and 32B. The model training uses AdamW optimizer (Kingma and Ba, 2015) with learning rate 5e 5, batch size of 256, and maximum context length of 32,768 tokens. We used CosineAnnealing learning rate schedule with 5% warmup, and the final checkpoint was used for evaluation. To accelerate training, we utilized sequence packing (Shen et al., 2024), tensor and context parallelism, and BF16 precision. For generating outputs during inference, we employed temperature-based nucleus sampling (Holtzman et al., 2020) via vLLM (Kwon et al., 2023), setting maximum output length of 30,720 tokens. Baselines The following open-weight models were chosen as baselines in our evaluation: DeepSeekR1 and R1-Distill-Qwen models (DeepSeek-AI et al., 2025), QwQ-32B (Team, 2025b), Qwen332B (Team, 2025a), OlympicCoder (Penedo et al., 2025a), OpenThinker2 (OpenThoughts, 2025), DeepCoder-14B-Preview (Luo et al., 2025), and OCR-Qwen (Ahmad et al., 2025b) models. Benchmarks and Metrics For our evaluation, we used the same LiveCodeBench (Jain et al., 2025) split that we utilized for our C++ expansion. This benchmark contains 67 easy, 89 medium, and 279 hard coding questions. We report pass@1 for code generation, pass@1select@k under test-time scaling setup. Furthermore, we report the accuracy of the self-critique capability. An LLMs prediction is considered correct if it accurately judges the correctness of all generated solutions (k unless otherwise mentioend) for given input question. We also evaluate critique accuracy using CodeContests benchmark (Li et al., 2022) and provide details in Appendix B. 4.1 Main Results Tables 2 and 3 summarize the performance of our distilled models against various competing baselines. We consistently observed the following three trends. Model DeepSeek-R1 QwQ-32B Qwen3-32B LiveCodeBench-Python Easy Medium Hard All LiveCodeBench-C++ Easy Medium Hard All 98.5 97.0 98.0 79.8 79.8 79. 37.4 28.5 34.6 65.6 61.3 64.3 95.5 94.0 96.3 Distilled 7B+ Models R1-Distill-Qwen-7B OpenThinker2-7B OlympicCoder-7B OCR-Qwen-7B-Instruct OCR-2-7B 86.6 80.6 82.1 95.4 97. 43.8 16.9 49.4 64.0 71.1 7.0 1.6 12.2 18.0 20.9 Distilled 14B+ Models R1-Distill-Qwen-14B 98.5 DeepCoder-14B-Preview 97.0 97.6 OCR-Qwen-14B-Instruct 97.9 OCR-2-14B 62.9 65.2 74.4 75.4 17.1 19.5 27.6 26. Distilled 32B+ Models R1-Distill-Qwen-32B OpenThinker2-32B OlympicCoder-32B OCR-Qwen-32B-Instruct OCR-2-32B 98.5 97.0 98.5 98.4 97.9 68.5 65.2 71.9 77.2 77.1 28.5 22.8 24.4 30.4 31.8 38.0 25.5 40.9 51.3 55. 51.3 52.7 59.4 59.4 58.1 54.1 57.4 61.7 62.1 26.9 43.3 85.7 13.4 91.4 68.7 61.2 47.8 91.8 80.6 97.0 91.0 65.7 94.7 75.3 68.5 73. 5.6 2.3 46.7 2.3 64.5 39.3 39.3 16.9 68.6 39.3 60.7 62.8 33.7 72.2 29.3 26.0 35.0 59.9 55.9 61.9 1.6 0 10.2 0.8 21. 6.5 8.9 0.8 25.7 11.4 25.2 21.3 4.1 28.0 9.0 11.1 40.0 4.3 51.9 31.9 39.3 17.2 55.3 36.9 53.8 51.4 28.3 58.1 Table 2: Performance comparison of reasoning models on LiveCodeBench. Highlighted rows show our finetuned models performances. Bold indicates the highest performance. Python results are averaged across 64 runs, and C++ results across 16 runs. Model DeepSeek-R1 QwQ-32B Qwen3-32B DeepCoder-14B-Preview OpenThinker2-32B OlympicCoder-32B OCR-2-7B OCR-2-14B OCR-2-32B Pass@1 Pass@10 Python C++ Python C++ Pass@1Select@10 C++ Python 61.3 60.2 63.6 53.0 58.1 55.6 55.2 58.6 61.3 60.1 54.1 61.9 29.4 51.6 52.3 51.8 56.4 59.8 75.3 73.5 77. 65.9 74.2 71.3 67.7 72.0 75.6 72.0 70.3 75.3 54.5 68.8 68.5 69.9 69.5 73.1 63.4 (+2.1) 62.7 (+2.6) 64.2 (+4.0) 56.3 (+2.2) 67.4 (+3.8) 63.8 (+1.9) 57.7 (+4.4) 35.1 (+5.7) 62.4 (+4.3) 53.4 (+1.8) 59.9 (+4.3) 55.6 (+3.3) 60.2 (+5.0) 54.1 (+2.3) 60.6 (+2.0) 58.4 (+2.0) 67.4 (+6.1) 60.6 (+0.8) Table 3: Performance comparison of reasoning models under test-time scaling setup. Highlighted rows show our finetuned models performances. The pass@1 scores are averaged over 10 runs. The performance gains with self-critique are highlighted in blue and bold values indicate the largest gains. Scaling Yields Large Performance Boosts for Smaller Models comparison between our OCR-2 models and the previous OCR-Qwen models in Table 2 demonstrates that scaling the quantity of synthetic solutions particularly benefits smaller models. Increasing the fine-tuning samples from 737K to 2.5M yields more substantial relative performance improvement for smaller models (e.g., 7B parameters) compared to their larger counterparts. This trend is also evident in C++ performance, especially when compared to models like OlympicCoder, which were trained on C++ data. Although the 32B parameter model also improves, the gains suggest it might be approaching performance ceiling achievable through mere scaling of synthetic data quantity for existing problem types. LLMs Show Similar Capabilities in Python and C++ Notably, OPENCODEREASONING-II includes approximately 1.4M Python samples and 1.17M C++ samples as seen in Table 1. Consequently, 6 Model DeepSeek-R1 QwQ-32B Qwen3-32B DeepCoder-14B-Preview 10.4 11.9 OpenThinker2-32B 56.7 OlympicCoder-32B 80.6 OCR-2-7B 88.1 OCR-2-14B 92.5 OCR-2-32B LiveCodeBench-Python Easy Medium Hard All LiveCodeBench-C++ Easy Medium Hard All 91.0 67.2 77.6 47.2 39.3 41.6 1.1 2.2 39.3 47.2 43.8 53.9 8.9 9.8 9. 0 0.8 4.9 14.6 16.3 17.1 40.9 33.0 36.2 2.9 3.9 28.3 40.9 42.3 47.0 82.1 74.6 74.6 4.5 35.8 59.7 70.1 82.1 80.6 40.4 25.8 30. 1.1 1.1 27.0 24.7 36.0 44.9 13.0 9.8 9.8 0 0.8 0.8 8.9 8.9 15.5 38.4 30.5 31.9 1.4 9.3 23.3 28.7 35.1 40.5 Table 4: Accuracy comparison of reasoning models on self-critique. Highlighted rows show our finetuned models performances. Bold indicates the best performance. our models maintain or improve their Python capabilities while demonstrating dramatically enhanced C++ solution quality. Despite being trained on both languages, our models exhibit significantly superior performance on LiveCodeBench-C++ compared to other models of similar size. In particular, the 32B parameter model we train exceeds QwQ-32B and other open-weight reasoning models in both Python and C++. Overall, joint training on substantial, comparably-sized Python and C++ solution sets results in strong performance in both languages, often outperforming models trained on single-language data. We anticipate this positive transfer learning behavior could extend to other programming languages, direction we leave for future work. Test-time Scaling with Self-Critique Yields Significant Improvements Table 3 highlights the benefits of applying self-critique at test-time, capability developed by fine-tuning models on data that includes self-critique labels. For instance, when considering our flagship OCR-2-32B model, using its self-critique ability to select the best solution advances the Pass@1 score by approximately 6 percentage points. This enhancement significantly narrows the performance gap between Pass@1 and Pass@10 for all our model sizes, in both Python and C++, and results in our models outperforming other similarly sized competitors. Therefore, training with self-critique data not only improves baseline Pass@1 scores but also demonstrates that self-critique is an effective test-time strategy for selecting higher accuracy solution from multiple parallel generations."
        },
        {
            "title": "5 Ablation and Analyses",
            "content": "5.1 Test-time Scaling: Opportunities for Enhanced Critique Although self-critique based selection under parallel test-time scaling has proven effective in boosting LLMs performance with limited number of samples, an important question left to address is, what is the gap between models pass@1, pass@1select@k, and pass@k as grows? In this ablation, we aim to answer this question using far larger value of (up to 100) and show the gap that exists between them. It can be seen in Figure 3 that OCR-2-32B quickly attains high pass@k score. For comparison, we plot the pass@1 score for each individual sample, computed independently from the rest of the samples, as well as the pass@1select@k score, as increases to include all past solutions. Two important observations can be drawn from this figure. First, while pass@1select@k is consistently higher than pass@1, this increase does not go beyond certain limit. This observation is consistent with the fact that out of the 279 problems in LiveCodeBench split we evaluate on, nearly 212 problems fall under the Medium/Hard category where self-critique accuracy is insufficient to correctly determine the correctness of the all generated samples, as can be seen in Table 4. On evaluating the accuracy of the critique labels using various models on single solution for each problem, we find that the accuracy of these models for medium difficulty problems tends to be 47% and for hard problems is less than 14% at best when critiquing 10 solutions. This inaccurate final determination of correctness label limits the scope of improvement on the overall benchmark, as the vast majority (75.9%) of samples in LiveCodeBench are inaccurately critiqued. Secondly, the simple heuristic described in section 3, which is to select the solution with shortest critique reasoning trace, may be ineffective in incorporating information from additional solutions and prevents substantial 7 Figure 3: Performance gap between pass@1, pass@1select@k, and pass@k under test-time scaling - large number of samples drawn from OCR-2-32B. improvements. We leave the exploration of more sophisticated heuristics and methods to enhance the accuracy of self-critique-based selection for future research. 5.2 Impact of Temperature on Self Critique We tested how the critic LLMs responded to different decoding temperatures by re-evaluating them at temperature [0.2, 0.4, 0.6, 0.7]. The pass@1select@k and the accuracy of self-critique showed minimal variation (less than 3%) across these temperatures, and t-tests indicated no statistically significant differences. These findings suggest that within the range below 0.7, the decoding temperature does not substantially affect the critics performance. This implies that the models judgments are mainly based on its internal knowledge rather than the randomness of the sampling process. Therefore, we used temperature of 0.6 throughout this work. 5.3 Transfer Learning: Python C++ OPENCODEREASONING-II features solutions in both Python and C++, allowing study to investigate how well models perform when trained on single language and tested on the other. The results of these experiments are detailed in Table 5. First, our findings suggest that crosslanguage transfer does occur, and combining Python and C++ data during training enhances overall performance on both languages. Second, we observe an asymmetry: models trained solely on C++ achieve noticeable scores on Python, whereas models trained only on Python, while performing well on Python, experience significant drop in accuracy on C++. This difference isnt simply due to dataset size, and further research is needed to understand the underlying causes of this performance degradation. Model Dataset Size Python C++ LiveCodeBench C++ Python OCR-2-7B OCR-2-14B OCR-2-32B 1.4M 0 1.1M 1.4M 1.1M 1.4M 0 0 1.1M 1.4M 1.1M 1.4M 0 0 1.1M 1.4M 1.1M 54.9 36.0 55. 56.3 54.0 59.4 60.5 56.6 62.1 1.7 45.3 51.9 9.8 53.6 55.3 16.0 56.4 59.4 Table 5: Pass@1 scores of OCR-2 models trained individually on Python and C++ vs. jointly using OPENCODEREASONING-II. 5.4 Impact of Scaling Up Data on Code Generation We analyze the data scaling study of OpenCodeReasoning (Ahmad et al., 2025b), and substantially increase the dataset size in order to determine whether data scaling shows limits for given model size. As such, we redo the scaling study from 25K samples all the way to 1.4M samples in Python using OPENCODEREASONING-II and plot the trajectory of scores on LiveCodeBench in Figure 4. While the 7B model shows substantial improvements in score with data scaling, such improvement is not observed in the 14B and 32B models. It remains to be seen if access to more novel and complex 8 Figure 4: Impact of scaling up data from 25k to 1.4M samples in OPENCODEREASONING-II. instructions may further improve scores, or if the number of solutions to the existing problems must be scaled by orders of magnitude to improve scores further measurably."
        },
        {
            "title": "6 Related Works",
            "content": "Our work builds upon foundation of research in synthetic data generation for code, the growing reasoning capabilities of LLMs, and model-based critique, particularly within the coding context. The demonstrated effectiveness of LLMs in coding has spurred the creation of numerous impactful synthetic datasets for instruction tuning (Wang et al., 2023b; Wei et al., 2024b; Xu et al., 2024; Wu et al., 2024; Luo et al., 2024; Wei et al., 2024a; Majumdar et al., 2024; Ahmad et al., 2025a). Inspired by the positive impact of extended inference-time computation on code quality, synthetic data generation has also been adapted for training LLMs with reasoning abilities for code. These efforts have shown substantial gains from fine-tuning on as few as 17k reasoning-style CoT examples (Li et al., 2025a) and scaling up to 447k (BespokeLabs, 2025; Penedo et al., 2025b; OpenThoughts, 2025; Li et al., 2025a; Xu et al., 2025). Recent work by Ahmad et al. (2025b) further scaled this data distillation strategy to 737k samples, achieving superior SFT performance in code and reasoning foundation models (Bercovich et al., 2025). In this paper, we aim to push the boundaries of synthetic data scaling for coding and, crucially, explore critique fine-tuning for reasoning data distillation. Training reward models to critique solutions is well-explored research area across various domains. Generalist reward models, trained on preference pairs, have shown strong performance in alignment and reasoning (Nvidia et al., 2024; Liu et al., 2024; Wang et al., 2024c,b). Specialized reward models have also been developed for tasks like math or coding (Wang et al., 2024a; Zeng et al., 2025a; Liu et al., 2025a; Zhang et al., 2025c; Yang et al., 2024). Notably, some of these works have released their training datasets, providing valuable resources for the research community. LLM-based solution critiques, when combined with reasoning, significantly enhance model capabilities in both mathematical and coding tasks. Increased test-time computation consistently improves verifiers like reward models and test case generators (Mahan et al., 2024; Ficek et al., 2025; Liu et al., 2025b; Chen et al., 2025; Moshkov et al., 2025), as demonstrated by Zhang et al. (2025a)s improved math reward model and released CoT data. Moreover, critique fine-tuning positively impacts question-answering (Sun et al., 2024; Yu et al., 2025b). This synergy, further explored in recent work combining LLM critiques with reasoning-driven test-time scaling, offers promising route to advance coding abilities (Wang et al., 2025b; Zhou et al., 2025)."
        },
        {
            "title": "7 Conclusion",
            "content": "This research addresses the critical need for high-quality large-scale data to propel advancements in reasoning-based LLMs for test-time scaling. We introduced OPENCODEREASONING-II, significantly larger and richer dataset of question-solution-critique triples that has enabled us to train powerful distilled models. Our two-stage fine-tuning approach yielded Qwen2.5-Instruct models that demonstrate state-of-the-art or comparable code generation capabilities among open-weight distilled models. More importantly, the synergistic combination of our code generation and critique models led to tangible improvements in competitive coding benchmarks. Additionally, our C++ extension of LiveCodeBench broadens the evaluation landscape for LLMs in code."
        },
        {
            "title": "References",
            "content": "Ahmad, W.U., Ficek, A., Samadi, M., Huang, J., Noroozi, V., Majumdar, S., Ginsburg, B., 2025a. Opencodeinstruct: large-scale instruction tuning dataset for code llms. URL: https://arxiv.org/ abs/2504.04030, arXiv:2504.04030. Ahmad, W.U., Narenthiran, S., Majumdar, S., Ficek, A., Jain, S., Huang, J., Noroozi, V., Ginsburg, B., 2025b. Opencodereasoning: Advancing data distillation for competitive coding. URL: https://arxiv.org/abs/2504.01943, arXiv:2504.01943. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., Sutton, C., 2021. Program synthesis with large language models. URL: https: //arxiv.org/abs/2108.07732, arXiv:2108.07732. Bercovich, A., Levy, I., Golan, I., Dabbah, M., El-Yaniv, R., Puny, O., Galil, I., Moshe, Z., Ronen, T., Nabwani, N., Shahaf, I., Tropp, O., Karpas, E., Zilberstein, R., Zeng, J., Singhal, S., Bukharin, A., Zhang, Y., Konuk, T., Shen, G., Mahabaleshwarkar, A.S., Kartal, B., Suhara, Y., Delalleau, O., Chen, Z., Wang, Z., Mosallanezhad, D., Renduchintala, A., Qian, H., Rekesh, D., Jia, F., Majumdar, S., Noroozi, V., Ahmad, W.U., Narenthiran, S., Ficek, A., Samadi, M., Huang, J., Jain, S., Gitman, I., Moshkov, I., Du, W., Toshniwal, S., Armstrong, G., Kisacanin, B., Novikov, M., Gitman, D., Bakhturina, E., Scowcroft, J.P., Kamalu, J., Su, D., Kong, K., Kliegl, M., Karimi, R., Lin, Y., Satheesh, S., Parmar, J., Gundecha, P., Norick, B., Jennings, J., Prabhumoye, S., Akter, S.N., Patwary, M., Khattar, A., Narayanan, D., Waleffe, R., Zhang, J., Su, B.Y., Huang, G., Kong, T., Chadha, P., Jain, S., Harvey, C., Segal, E., Huang, J., Kashirsky, S., McQueen, R., Putterman, I., Lam, G., Venkatesan, A., Wu, S., Nguyen, V., Kilaru, M., Wang, A., Warno, A., Somasamudramath, A., Bhaskar, S., Dong, M., Assaf, N., Mor, S., Argov, O.U., Junkin, S., Romanenko, O., Larroy, P., Katariya, M., Rovinelli, M., Balas, V., Edelman, N., Bhiwandiwalla, A., Subramaniam, M., Ithape, S., Ramamoorthy, K., Wu, Y., Velury, S.V., Almog, O., Daw, J., Fridman, D., Galinkin, E., Evans, M., Ghosh, S., Luna, K., Derczynski, L., Pope, N., Long, E., Schneider, S., Siman, G., Grzegorzek, T., Ribalta, P., Katariya, M., Alexiuk, C., Conway, J., Saar, T., Guan, A., Pawelec, K., Prayaga, S., Kuchaiev, O., Ginsburg, B., Olabiyi, O., Briski, K., Cohen, J., Catanzaro, B., Alben, J., Geifman, Y., Chung, E., 2025. Llama-nemotron: Efficient reasoning models. URL: https://arxiv.org/abs/2505.00949, arXiv:2505.00949. BespokeLabs, 2025. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation. www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoningdistillation. Accessed: 2025-01-22. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q.V., Ré, C., Mirhoseini, A., 2024. Large language monkeys: Scaling inference compute with repeated sampling. URL: https://arxiv.org/abs/2407. 21787, arXiv:2407.21787. Chen, L., Davis, J.Q., Hanin, B., Bailis, P., Stoica, I., Zaharia, M., Zou, J., 2024. Are more llm calls all you need? towards scaling laws of compound inference systems. URL: https: //arxiv.org/abs/2403.02419, arXiv:2403.02419. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.D.O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al., 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 . Chen, X., Li, G., Wang, Z., Jin, B., Qian, C., Wang, Y., Wang, H., Zhang, Y., Zhang, D., Zhang, T., Tong, H., Ji, H., 2025. Rm-r1: Reward modeling as reasoning. URL: https://arxiv.org/abs/2505. 02387, arXiv:2505.02387. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z.F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J.L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., 10 Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R.J., Jin, R.L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S.S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W.L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X.Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y.K., Wang, Y.Q., Wei, Y.X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y.X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z.Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., Zhang, Z., 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. URL: https://arxiv.org/abs/2501.12948, arXiv:2501.12948. Ficek, A., Majumdar, S., Noroozi, V., Ginsburg, B., 2025. Scoring verifiers: Evaluating synthetic verification in code and reasoning. URL: https://arxiv.org/abs/2502.13820, arXiv:2502.13820. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al., 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 . Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., Steinhardt, J., 2021. Measuring coding challenge competence with apps. NeurIPS . Holtzman, A., Buys, J., Du, L., Forbes, M., Choi, Y., 2020. The curious case of neural text degeneration, in: International Conference on Learning Representations. URL: https://openreview. net/forum?id=rygGQyrFvH. Jain, N., Han, K., Gu, A., Li, W.D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., Stoica, I., 2025. Livecodebench: Holistic and contamination free evaluation of large language models for code, in: The Thirteenth International Conference on Learning Representations. URL: https://openreview.net/forum?id=chfJJYC3iL. Kingma, D.P., Ba, J., 2015. Adam: method for stochastic optimization, in: Bengio, Y., LeCun, Y. (Eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. URL: http://arxiv.org/abs/1412.6980. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.H., Gonzalez, J.E., Zhang, H., Stoica, I., 2023. Efficient memory management for large language model serving with pagedattention, in: Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Li, D., Cao, S., Griggs, T., Liu, S., Mo, X., Tang, E., Hegde, S., Hakhamaneshi, K., Patil, S.G., Zaharia, M., Gonzalez, J.E., Stoica, I., 2025a. Llms can easily learn to reason from demonstrations structure, not content, is what matters! URL: https://arxiv.org/abs/2502.07374, arXiv:2502.07374. Li, R., Fu, J., Zhang, B.W., Huang, T., Sun, Z., Lyu, C., Liu, G., Jin, Z., Li, G., 2023. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852 . Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago, A., Hubert, T., Choy, P., de Masson dAutume, C., Babuschkin, I., Chen, X., Huang, P.S., Welbl, J., Gowal, S., Cherepanov, A., Molloy, J., Mankowitz, D., Sutherland Robson, E., Kohli, P., de Freitas, N., Kavukcuoglu, K., Vinyals, O., 2022. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814 . Li, Z.Z., Zhang, D., Zhang, M.L., Zhang, J., Liu, Z., Yao, Y., Xu, H., Zheng, J., Wang, P.J., Chen, X., Zhang, Y., Yin, F., Dong, J., Li, Z., Bi, B.L., Mei, L.R., Fang, J., Guo, Z., Song, L., Liu, C.L., 2025b. From system 1 to system 2: survey of reasoning large language models. URL: https://arxiv.org/abs/2502.17419, arXiv:2502.17419. Liu, C.Y., Zeng, L., Liu, J., Yan, R., He, J., Wang, C., Yan, S., Liu, Y., Zhou, Y., 2024. Skyworkreward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451 . 11 Liu, Z., Chen, Y., Shoeybi, M., Catanzaro, B., Ping, W., 2025a. Acemath: Advancing frontier math reasoning with post-training and reward modeling. URL: https://arxiv.org/abs/2412.15084, arXiv:2412.15084. Liu, Z., Wang, P., Xu, R., Ma, S., Ruan, C., Li, P., Liu, Y., Wu, Y., 2025b. Inference-time scaling for generalist reward modeling. URL: https://arxiv.org/abs/2504.02495, arXiv:2504.02495. Luo, M., Tan, S., Huang, R., Patel, A., Ariyak, A., Wu, Q., Shi, X., Xin, R., Cai, Deepcoder: https://pretty-radio-b75.notion.site/ C., Weber, M., Zhang, C., Li, L.E., Popa, R.A., Stoica, fully open-source 14b coder at o3-mini level. DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51. Notion Blog. I., 2025. Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., Jiang, D., 2024. Wizardcoder: Empowering code large language models with evol-instruct, in: The Twelfth International Conference on Learning Representations. URL: https://openreview.net/forum?id=UnUwSIgK5W. Mahan, D., Phung, D.V., Rafailov, R., Blagden, C., Lile, N., Castricato, L., Fränken, J.P., Finn, C., Albalak, A., 2024. Generative reward models. URL: https://arxiv.org/abs/2410.12832, arXiv:2410.12832. Majumdar, S., Noroozi, V., Narenthiran, S., Ficek, A., Balam, J., Ginsburg, B., 2024. Genetic instruct: Scaling up synthetic generation of coding instructions for large language models. arXiv preprint arXiv:2407.21077 . Moshkov, I., Hanley, D., Sorokin, I., Toshniwal, S., Henkel, C., Schifferer, B., Du, W., Gitman, I., 2025. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. URL: https://arxiv.org/abs/2504.16891, arXiv:2504.16891. Nvidia, :, Adler, B., Agarwal, N., Aithal, A., Anh, D.H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Cohen, J., Das, S., Dattagupta, A., Delalleau, O., Derczynski, L., Dong, Y., Egert, D., Evans, E., Ficek, A., Fridman, D., Ghosh, S., Ginsburg, B., Gitman, I., Grzegorzek, T., Hero, R., Huang, J., Jawa, V., Jennings, J., Jhunjhunwala, A., Kamalu, J., Khan, S., Kuchaiev, O., LeGresley, P., Li, H., Liu, J., Liu, Z., Long, E., Mahabaleshwarkar, A.S., Majumdar, S., Maki, J., Martinez, M., de Melo, M.R., Moshkov, I., Narayanan, D., Narenthiran, S., Navarro, J., Nguyen, P., Nitski, O., Noroozi, V., Nutheti, G., Parisien, C., Parmar, J., Patwary, M., Pawelec, K., Ping, W., Prabhumoye, S., Roy, R., Saar, T., Sabavat, V.R.N., Satheesh, S., Scowcroft, J.P., Sewall, J., Shamis, P., Shen, G., Shoeybi, M., Sizer, D., Smelyanskiy, M., Soares, F., Sreedhar, M.N., Su, D., Subramanian, S., Sun, S., Toshniwal, S., Wang, H., Wang, Z., You, J., Zeng, J., Zhang, J., Zhang, J., Zhang, V., Zhang, Y., Zhu, C., 2024. Nemotron-4 340b technical report. URL: https://arxiv.org/abs/2406.11704, arXiv:2406.11704. OpenThoughts, 2025. Open Thoughts. https://open-thoughts.ai. Penedo, G., Lozhkov, A., Kydlíˇcek, H., Allal, L.B., Beeching, E., Lajarín, A.P., Gallouédec, Q., Habib, N., Tunstall, L., von Werra, L., 2025a. Codeforces. https://huggingface.co/datasets/open-r1/ codeforces. Penedo, G., Lozhkov, A., Kydlíˇcek, H., Allal, L.B., Beeching, E., Lajarín, A.P., Gallouédec, Q., Habib, N., Tunstall, L., von Werra, L., 2025b. Codeforces cots. https://huggingface.co/datasets/ open-r1/codeforces-cots. Qu, Y., Yang, M.Y.R., Setlur, A., Tunstall, L., Beeching, E.E., Salakhutdinov, R., Kumar, A., 2025. Optimizing test-time compute via meta reinforcement fine-tuning. URL: https://arxiv.org/abs/2503. 07572, arXiv:2503.07572. Setlur, A., Rajaraman, N., Levine, S., Kumar, A., 2025. Scaling test-time compute without verification or rl is suboptimal. URL: https://arxiv.org/abs/2502.12118, arXiv:2502.12118. Shen, G., Wang, Z., Delalleau, O., Zeng, J., Dong, Y., Egert, D., Sun, S., Zhang, J.J., Jain, S., Taghibakhshi, A., Ausin, M.S., Aithal, A., Kuchaiev, O., 2024. Nemo-aligner: Scalable toolkit for efficient model alignment, in: First Conference on Language Modeling. URL: https://openreview. net/forum?id=yK2eGE8QVW. 12 Sun, S., Li, J., Yuan, W., Yuan, R., Li, W., Liu, P., 2024. The critique of critique, in: Ku, L.W., Martins, A., Srikumar, V. (Eds.), Findings of the Association for Computational Linguistics: ACL 2024, Association for Computational Linguistics, Bangkok, Thailand. pp. 90779096. URL: https://aclanthology.org/2024.findings-acl.538/, doi:10.18653/v1/2024.findings-acl.538. Team, Q., 2025a. Qwen3. URL: https://qwenlm.github.io/blog/qwen3/. Team, Q., 2025b. Qwq-32b: Embracing the power of reinforcement learning. URL: https://qwenlm. github.io/blog/qwq-32b/. TreeSitter, 2013. Tree sitter. https://github.com/tree-sitter/tree-sitter. Wang, P., Li, L., Shao, Z., Xu, R.X., Dai, D., Li, Y., Chen, D., Wu, Y., Sui, Z., 2024a. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. URL: https://arxiv.org/abs/2312. 08935, arXiv:2312.08935. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., Zhou, D., 2023a. Self-consistency improves chain of thought reasoning in language models. URL: https://arxiv.org/ abs/2203.11171, arXiv:2203.11171. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.A., Khashabi, D., Hajishirzi, H., 2023b. Selfinstruct: Aligning language models with self-generated instructions, in: Rogers, A., BoydGraber, J., Okazaki, N. (Eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics, Toronto, Canada. pp. 1348413508. URL: https://aclanthology.org/2023.acl-long.754/, doi:10.18653/v1/2023.acl-long.754. Wang, Y., Liu, Q., Xu, J., Liang, T., Chen, X., He, Z., Song, L., Yu, D., Li, J., Zhang, Z., Wang, R., Tu, Z., Mi, H., Yu, D., 2025a. Thoughts are all over the place: On the underthinking of o1-like llms. URL: https://arxiv.org/abs/2501.18585, arXiv:2501.18585. Wang, Y., Yue, X., Chen, W., 2025b. Critique fine-tuning: Learning to critique is more effective than learning to imitate. URL: https://arxiv.org/abs/2501.17703, arXiv:2501.17703. Wang, Z., Bukharin, A., Delalleau, O., Egert, D., Shen, G., Zeng, J., Kuchaiev, O., Dong, Y., 2024b. Helpsteer2-preference: Complementing ratings with preferences. URL: https://arxiv.org/abs/2410. 01257, arXiv:2410.01257. Wang, Z., Dong, Y., Delalleau, O., Zeng, J., Shen, G., Egert, D., Zhang, J.J., Sreedhar, M.N., Kuchaiev, O., 2024c. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv:2406.08673. Wang, Z., Dong, Y., Zeng, J., Adams, V., Sreedhar, M.N., Egert, D., Delalleau, O., Scowcroft, J., Kant, N., Swope, A., Kuchaiev, O., 2024d. HelpSteer: Multi-attribute helpfulness dataset for SteerLM, in: Duh, K., Gomez, H., Bethard, S. (Eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Association for Computational Linguistics, Mexico City, Mexico. pp. 33713384. URL: https://aclanthology.org/2024.naacl-long.185/, doi:10.18653/v1/ 2024.naacl-long.185. Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E.H., Le, Q.V., Zhou, D., 2022. Chain of thought prompting elicits reasoning in large language models, in: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (Eds.), Advances in Neural Information Processing Systems. URL: https://openreview.net/forum?id=_VjQlMeSB_J. Wei, Y., Cassano, F., Liu, J., Ding, Y., Jain, N., Mueller, Z., de Vries, H., Werra, L.V., Guha, A., ZHANG, L., 2024a. Selfcodealign: Self-alignment for code generation, in: The Thirty-eighth Annual Conference on Neural Information Processing Systems. URL: https://openreview.net/ forum?id=xXRnUU7xTL. Wei, Y., Wang, Z., Liu, J., Ding, Y., Zhang, L., 2024b. Magicoder: empowering code generation with oss-instruct, in: Proceedings of the 41st International Conference on Machine Learning, JMLR.org. 13 Wu, Y., Huang, D., Shi, W., Wang, W., Gao, L., Liu, S., Nan, Z., Yuan, K., Zhang, R., Zhang, X., et al., 2024. Inversecoder: Unleashing the power of instruction-tuned code llms with inverse-instruct. arXiv preprint arXiv:2407.05700 . Wu, Y., Sun, Z., Li, S., Welleck, S., Yang, Y., 2025. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. URL: https://arxiv.org/abs/ 2408.00724, arXiv:2408.00724. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., Lin, Q., Jiang, D., 2024. WizardLM: Empowering large pre-trained language models to follow complex instructions, in: The Twelfth International Conference on Learning Representations. URL: https://openreview.net/forum?id= CfXh93NDgH. Xu, Z., Liu, Y., Yin, Y., Zhou, M., Poovendran, R., 2025. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. URL: https://arxiv.org/abs/2503.02951, arXiv:2503.02951. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., Zhang, Z., 2024. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122 . Yang, S., Chiang, W.L., Zheng, L., Gonzalez, J.E., Stoica, I., 2023. Rethinking benchmark and contamination for language models with rephrased samples. arXiv:2311.04850. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Dai, W., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.Y., Zhang, Y.Q., Yan, L., Qiao, M., Wu, Y., Wang, M., 2025a. Dapo: An open-source llm reinforcement learning system at scale. URL: https://arxiv.org/abs/2503.14476, arXiv:2503.14476. Yu, Y., Chen, Z., Zhang, A., Tan, L., Zhu, C., Pang, R.Y., Qian, Y., Wang, X., Gururangan, S., Zhang, C., Kambadur, M., Mahajan, D., Hou, R., 2025b. Self-generated critiques boost reward modeling for language models, in: Chiruzzo, L., Ritter, A., Wang, L. (Eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Association for Computational Linguistics, Albuquerque, New Mexico. pp. 1149911514. URL: https://aclanthology.org/2025.naacl-long.573/. Zeng, H., Jiang, D., Wang, H., Nie, P., Chen, X., Chen, W., 2025a. Acecoder: Acing coder rl via automated test-case synthesis. URL: https://arxiv.org/abs/2502.01718, arXiv:2502.01718. Zeng, Z., Cheng, Q., Yin, Z., Zhou, Y., Qiu, X., 2025b. Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities? URL: https://arxiv.org/abs/2502. 12215, arXiv:2502.12215. Zhang, L., Hosseini, A., Bansal, H., Kazemi, M., Kumar, A., Agarwal, R., 2025a. Generative verifiers: Reward modeling as next-token prediction. URL: https://arxiv.org/abs/2408.15240, arXiv:2408.15240. Zhang, Q., Lyu, F., Sun, Z., Wang, L., Zhang, W., Hua, W., Wu, H., Guo, Z., Wang, Y., Muennighoff, N., King, I., Liu, X., Ma, C., 2025b. survey on test-time scaling in large language models: What, how, where, and how well? URL: https://arxiv.org/abs/2503.24235, arXiv:2503.24235. Zhang, Z., Zheng, C., Wu, Y., Zhang, B., Lin, R., Yu, B., Liu, D., Zhou, J., Lin, J., 2025c. The lessons of developing process reward models in mathematical reasoning. URL: https://arxiv.org/abs/2501. 07301, arXiv:2501.07301. Zheng, L., Yin, L., Xie, Z., Sun, C., Huang, J., Yu, C.H., Cao, S., Kozyrakis, C., Stoica, I., Gonzalez, J.E., Barrett, C., Sheng, Y., 2024. SGLang: Efficient execution of structured language model programs, in: The Thirty-eighth Annual Conference on Neural Information Processing Systems. URL: https://openreview.net/forum?id=VqkAKQibpq. Zhou, C., Zhang, X., Song, D., Chen, X., Gu, W., Ma, H., Tian, Y., Zhang, M., Hu, L., 2025. Refinecoder: Iterative improving of large language models via adaptive critique refinement for code generation. URL: https://arxiv.org/abs/2502.09183, arXiv:2502.09183."
        },
        {
            "title": "A Final Output Selection using Critique",
            "content": "To calculate pass@1, we select the shortest reasoning trace of the critique. straightforward baseline for final solution selection is uniform random selection. Figure 5 contrasts these two selection methods, revealing that our heuristic for choosing the shortest trace yields considerably better scores than randomly picking from the positive critique samples. We hypothesize that the critique models comparatively lower accuracy in identifying correct solutions for medium and hard problems explains the substantially weaker performance of the random selection baseline. Figure 5: Differences in pass@1 scores between randomly selecting the final output vs. choosing right solution with shortest critique thinking, using OCR-2-32B on LiveCodeBench-Python."
        },
        {
            "title": "B Evaluating Critique LLM Accuracy in Judging Code Solutions",
            "content": "This study initially assessed LLMs self-critique ability by having them evaluate their own generated solutions to select the best output in parallel scaling setup. Recognizing that this might not reflect their true critique capabilities due to varying generation accuracy, we further evaluated them as external critics. To do this, we used the CodeContests benchmark (Li et al., 2022), randomly selecting one correct and one incorrect human-written solution for each of its 165 test questions. We limited code solutions to maximum of 4096 tokens (based on the Qwen2.5 tokenizer), resulting in 238 Python and 329 C++ samples. The critique performance results when calculated on single positivenegative sample per problem are shown in Table 6. Notably, while OCR-2-32B excelled in critiquing Python code, OCR-2-14B surprisingly achieved the best performance for C++. Model # Language # Accuracy # Language # Accuracy DeepCoder-14B-Preview OpenThinker2-32B OlympicCoder-32B QwQ-32B Qwen3-32B OCR-2-7B OCR-2-14B OCR-2-32B Python Python Python Python Python Python Python Python 13.4 43.7 46.6 60.1 63.9 60.1 63.9 66. C++ C++ C++ C++ C++ C++ C++ C++ 28.6 45.9 49.5 56.8 65.0 54.7 65.3 65.0 Table 6: Critique accuracy of reasoning-enabled LLMs on human-written solutions provided in the test split of Code-Contests benchmark (Li et al., 2022)."
        },
        {
            "title": "Prompt for solution generation in Python",
            "content": "system: \"\" user: - You are helpful and harmless assistant. You should think step-by-step before responding to the instruction below. Please use python programming language only. You must use ```python for just the final solution code block with the following format: ```python # Your code here ``` {input} Prompt for solution generation in C++ system: \"\" user: - You are helpful and harmless assistant. You should think step-by-step before responding to the instruction below. Please use c++ programming language only. You must use ```cpp for just the final solution code block with the following format: ```cpp # Your code here ``` {input} Figure 6: Prompt template used for solution generation using R1 for OPENCODEREASONING-II. Prompt for critique generation using QwQ-32B system: \"\" user: - You are helpful and harmless assistant. You should think step-by-step before responding to the instruction below. You have solved programming problem. Now, you will critique your solution and conclude with <judgment>right/wrong</judgment>. ## Question {question} ## Solution {solution} Figure 7: Prompt template used for critique data generation for OPENCODEREASONING-II."
        }
    ],
    "affiliations": [
        "NVIDIA Santa Clara, CA 95051, USA"
    ]
}