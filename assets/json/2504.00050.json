{
    "paper_title": "JudgeLRM: Large Reasoning Models as a Judge",
    "authors": [
        "Nuo Chen",
        "Zhiyuan Hu",
        "Qingyun Zou",
        "Jiaying Wu",
        "Qian Wang",
        "Bryan Hooi",
        "Bingsheng He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rise of Large Language Models (LLMs) as evaluators offers a scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through a detailed analysis of reasoning requirements across evaluation tasks, we reveal a negative correlation between SFT performance gains and the proportion of reasoning-demanding samples - highlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, a family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 0 5 0 0 0 . 4 0 5 2 : r JudgeLRM: Large Reasoning Models as Judge Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, Bingsheng He National University of Singapore https://github.com/NuoJohnChen/JudgeLRM https://huggingface.co/nuojohnchen/JudgeLRM-7B https://huggingface.co/spaces/nuojohnchen/JudgeLRMDemo"
        },
        {
            "title": "Abstract",
            "content": "The rise of Large Language Models (LLMs) as evaluators offers scalable alternative to human annotation, yet existing Supervised Fine-Tuning (SFT) for judges approaches often fall short in domains requiring complex reasoning. In this work, we investigate whether LLM judges truly benefit from enhanced reasoning capabilities. Through detailed analysis of reasoning requirements across evaluation tasks, we reveal negative correlation between SFT performance gains and the proportion of reasoning-demanding sampleshighlighting the limitations of SFT in such scenarios. To address this, we introduce JudgeLRM, family of judgment-oriented LLMs trained using reinforcement learning (RL) with judge-wise, outcome-driven rewards. JudgeLRM models consistently outperform both SFT-tuned and state-of-the-art reasoning models. Notably, JudgeLRM-3B surpasses GPT-4, and JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score, particularly excelling in judge tasks requiring deep reasoning."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in LLMs have sparked significant interest in their use as evaluative judges (Gu et al., 2025; Li et al., 2024), offering reliable and scalable alternative to costly human annotation. Previous approaches, such as JudgeLM (Zhu et al., 2025) and PandaLM (Wang et al., 2024), have enabled LLMs to perform judgment tasks based on the large scale SFT. SFT faces inherent limitations in generalization and reasoning depth. But do LLM judges truly require strong reasoning capabilities? To explore this, we revisit prior work such as PandaLM (Wang et al., 2024), which shows that smaller models can be adapted for evaluation via post-training techniques. However, these methods still struggle with complex reasoning demands and computational inefficiency. We investigate whether improvements in judgment performance through SFT correlate with the proportion of questions that require reasoning to evaluate. Using the five question source categories defined in PandaLM (see Section A), we compute the proportion of samples in each category that require reasoning (detailed in Table 5). As shown in Figure 1, we observe an inverse relationship between improvements in F1 score on the judge task and the proportion of reasoning-required samples, indicating that SFT struggles more on categories demanding higher reasoning depth. This suggests that effective LLM judges must possess strong reasoning abilities. fundamental challenge in deploying LLMs as judges lies in their dependence on intrinsic reasoning capabilities. While the Chain-of-Thought (CoT) (Wei et al., 2022) framework via SFT equips models to process nuanced information, achieving accurate and contextually grounded judgments remains non-trivial. Recent studies have shown that advanced large reasoning models (LRMs), such as DeepSeek-R1 (DeepSeek-AI, 2025), demonstrate strong performance in evaluation tasks by leveraging structured reasoning paths. To bridge this gap, we propose JudgeLRMa family of LLMs trained using RL with judge-wise, outcome-driven rewards to enhance evaluative reasoning. The task-specific reward function nuochen@comp.nus.edu.sg 1 Figure 1: Judgment performance improvement vs. reasoning requirement across domainYaxis shows F1 score improvement (SFT base) based on Qwen2.5-7B-instruct; X-axis shows the proportion of tasks requiring reasoning. Each point represents domain. negative linear trend (y = -0.41x + 16.72, R² = 0.53) suggests that domains with more reasoning-heavy tasks benefit less from SFT alone. Sample counts across domains: 105 (Office Productivity), 108 (Search Information Retrieval), 195 (Entertainment Media), and estimated 108 (Social Professional Networking), 190 (Life Utility). integrates both structural and content-based components. The structural reward ensures well-formatted reasoning and answer sections, while the content reward aligns model judgments with ground-truth preferences through relation, absolute, and confidence-based metrics. This design promotes both faithful reasoning and accurate, confident scoring. (Shao et al., 2024). JudgeLRM model series (license under MG0-2.01) range from 3B to 7B parameters, trained JudgeLRM-3B using Group Relative Policy Optimization (GRPO) surpasses GPT-4 on judgment tasks, while JudgeLRM-7B outperforms DeepSeek-R1 by 2.79% in F1 score. Compared to SFT models of the same size, JudgeLRM achieves an average improvement of 8.14% in F1 score. Notably, JudgeLRM shows consistent gains even in categories with high proportion of reasoning-required tasks (detailed in Table 3), further demonstrating its ability to overcome the reasoning limitations of SFT-based models. Furthermore, our work highlights that judgment is inherently reasoning-intensive task, not merely scoring exercise. Through systematic analysis, we demonstrate that reasoning patterns, such as verification, sub-goal setting, double checking, error identification, and decision justification, are crucial to the success of judgment tasks."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 LLM Reasoning and Planning Modern large language models (LLMs) demonstrate remarkable reasoning abilities through their intrinsic Chain-of-Thought (CoT) mechanisms (Wei et al., 2022). This capability can be elicited through prompting techniques (Kojima et al., 2022) or explicitly integrated into model architectures like Openai o1 (OpenAI, 2024), Deepseek R1 (DeepSeek-AI, 2025; Shao et al., 2024). Such structural enhancements enable LLMs to perform multi-step reasoning across diverse domains, like in medical, Huatuo-O1 (Chen et al., 2024b),Medical-R1 (Lai et al., 2025) and in finance, Fin-O1 (Qian et al., 2025) and Fin-R1 (Liu et al., 2025). Our work first focuses on energizing reasoning abilities for judges, subsequently demonstrating the necessity and effectiveness of strong reasoning skills for judges. 1https://www.modelgo.li/ 2 2.2 LLM as Judge Human evaluation of LLM outputs is time-consuming, resource-intensive, and often inconsistent due to annotator subjectivity (Gu & Others, 2024). To address these limitations, researchers have explored using LLMs themselves as evaluators, paradigm often referred to as LLM-as-a-Judge (Zheng et al., 2024). This approach promises more scalable and potentially cost-effective evaluation. Furthermore, recognizing the potential benefits of specialized models, some studies have focused on training dedicated LLMs specifically for the task of judging LLM outputs, aiming for improved accuracy and alignment with human preferences (Zhu et al., 2025; Wang et al., 2024). Despite its promise, the LLM-as-a-Judge approach faces various biases inherent in the judge LLMs themselves, which can compromise the fairness and reliability of the evaluations (Gallegos et al., 2024; Chen et al., 2024a; Dubois et al., 2025). We first introduce reinforcement learning on training small-scale LLMs as judge, demonstrating better results than Deepseek-R1 and controllable range of bias. 3 Judge-wise Outcome Reward and RL Training Inspired by the insufficiency of SFT training revealed in Fig. 1, we introduce Large Reasoning Models as judge (JudgeLRM), applying reinforcement learning on judge tasks through judge-wise outcome reward. System Prompt for RL Training <im start>system You are helpful assistant. The assistant first performs detailed, step-by-step reasoning process in its mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> detailed reasoning process here, explaining each step of your evaluation for both assistants </think><answer> answer here </answer>. Now the user asks you to judge the performance of two AI assistants in response to the question. Score assistants 1-10 (higher=better). Criteria includes helpfulness, relevance, accuracy, and level of detail. Avoid order, length, style or other bias. After thinking, when you finally reach conclusion, clearly provide your evaluation scores within <answer> </answer> tags, i.e., for example,<answer>3</answer><answer>5</answer> <im end> <im start>user [Question] {question} [Assistant 1s Answer] {answer 1} [Assistant 2s Answer] {answer 2} <im end> <im start>assistant <think> Figure 2: System Prompt for RL Training 3.1 Judge Task Definition We consider judge task where, given query and two responses A1 and A2, the model evaluates them on scale of 110 (s1, s2), following the JudgeLM (Zhu et al., 2025) setting. 3 Evaluation criteria include helpfulness, relevance, accuracy, and level of detail, as specified in Prompt 2. The goal is to predict scores that align with human judgments, which determine whether A1 wins, ties with, or loses to A2. 3.2 Reward Design for Judge Task Rule-based rewards have shown strong performance and are widely adopted (DeepSeek-AI, 2025). For judge tasks involving reasoning, we design reward function that combines structural and content-based components. Specifically, the reward ri is defined as ri = Rstruct + Rcontent, inspired by Xie et al. (2025). Structural Reward As shown in Figure 2, the structural reward Rstruct ensures that the model output includes structured reasoning process enclosed in <think>...</think> tags and final judgment scores (s1, s2) enclosed in <answer>...</answer> tags, where s1, s2 {1, . . . , 10}. The reward components are: Rformat = 1.0, 0.5, 1.0, if all tags are correct and in proper order if s1, s2 / {1, . . . , 10} if severe formatting errors exist (1) Content Reward The content reward Rcontent evaluates the accuracy and confidence of the predicted scores (s1, s2) against the ground-truth labels (s 2). It consists of three parts: 1, rrelation = (cid:26)2.0, if sgn(s1 s2) = sgn(s 1 2) 1.5, otherwise rabsolute = 1.0, 0.6, 0, if s1 1 + s2 2 = 0 if rrelation = 2 and s1 otherwise 1 + s2 2 2 rconfidence = (cid:26)0.2, 0, if rrelation = 2 and s1 s2 otherwise 1 2 The final reward is computed as: ri = Rformat (cid:124) (cid:123)(cid:122) (cid:125) Rstruct + rrelation + rabsolute + rconfidence (cid:123)(cid:122) (cid:125) Rcontent (cid:124) 3.3 RL Training Algorithm (2) (3) (4) (5) We utilize GRPO (DeepSeek-AI, 2025) as policy gradient Algorithm, which extends standard PPO by normalizing advantages within judgment groups (e.g., queries of similar difficulty or topic) to stabilize training. For policy πθ and reference model πref, we define: Group-wise Advantage Calculation For each query group GQ with its associated judgments GQ = {(A1, A2, s1, s2)}: µQ = (A1,A2)GQ [r(s1, s2)], σQ = (cid:113)E (A1,A2)GQ [(r(s1, s2) µQ)2] The normalized advantage for sample is: Ai(s1, s2Q) = ri(s1, s2) µQ σQ + η 4 (6) (7) Dataset JudgeLM (GPT-4 as ground truth) PandaLM (Human as ground truth) Criteria Existing Baseline. (* from original paper) Agreement Precision Recall F1 Agreement Precision Recall F1 GPT-3.5* GPT-4* PandaLM-7B Auto-J-13B* JudgeLM-7B JudgeLM-13B* JudgeLM-33B* PandaLM-70B* Base Models. Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-3B-Instruct-Judge-SFT Qwen2.5-7B-Instruct-Judge-SFT Deepseek-R1 Ours. JudgeLRM-3B JudgeLRM-7B - w/o. rabs + rconf - w. Rlength 73.83 - 68.61 74.86 81.11 84.33 89.03 - 72.29 76.85 83.58 82.00 - 83.72 83.74 - - 70.70 - 40.75 61.65 69.67 73.69 80.97 - 80.19 78.71 75.25 84.43 - 86.31 85.84 - - 52.80 - 38.82 57.53 78.39 80.51 84.76 - 64.07 77.85 76.12 81.74 - 82.98 83.65 - - 52.85 - 39.41 58.14 72.21 76.17 82.64 - 71.23 78.28 75.05 83.06 - 84.61 84.73 - - 62.96 66.47 59.26 - 65.07 68.97 75.18 66. 68.50 63.96 70.57 73.57 78.67 77.68 78.28 75.78 78.28 61.95 66.20 57.28 - 66.89 68.21 69.30 74.02 50.92 61.95 67.09 67.31 77.51 74.26 74.90 69.09 75.81 63.59 68.15 59.23 - 71.95 74.15 74.93 66. 56.13 67.61 73.36 72.23 69.97 70.86 75.74 73.69 69.19 58.20 61.80 54.56 - 61.92 65.12 69.73 69.23 51.57 59.81 66.10 67.98 72.48 72.12 75.05 70.36 71.34 Table 1: Performance on JudgeLM and PandaLM. Qwen2.5-Instruct-Judge-SFT means the finetuned Qwen2.5-Instruct on JudgeLM train set. As pairwise comparisons rarely yield ties, we exclude tie cases (10% of the test set) for more interpretable evaluation, which JudgeLRM-7Bs F1 reaches 83.47, all metrics on PandaLM surpass DeepSeek-R1. (see Table 7 for details). Notably, JudgeLRM-7B matches or even surpasses Deepseek-R1 on PandaLM. Policy Optimization Objective The policy πθ is optimized using the following objective: JGRPO(θ) =E QDE (s1,s2)πθ min (cid:34) (cid:32) πθ(s1, s2Q, A1, A2) πold(s1, s2Q, A1, A2) Ai(s1, s2Q), clip (cid:16) πθ(s1, s2Q, A1, A2) πold(s1, s2Q, A1, A2) , 1 ϵ, 1 + ϵ (cid:17) (cid:33) Ai(s1, s2Q) βDKL(πθπref) (cid:35) (8)"
        },
        {
            "title": "4 Experiments",
            "content": "We empirically evaluate JudgeLRM to address the following research questions: Effectiveness (Section 4.2): How does JudgeLRM perform compared to state-of-the-art LRMs and specialized judgement models? Ablation Study (Section 4.3): What are the individual contributions of different reward components to JudgeLRMs performance? Reliability (Section 4.4): Can JudgeLRM provide consistent and accurate comparative judgments? Case Study (Section 4.5): In what ways does JudgeLRM demonstrate effective reasoning to support high-quality judgments? 4.1 Experimental Setup Datasets. We evaluate our models on two established benchmarks: JudgeLM, which uses GPT-4 annotations as gold labels, and PandaLM, which relies on human annotations. The JudgeLM dataset comprises 100K training instances and 5K test set, following the task formulation described in Section 3.1. The PandaLM benchmark includes 1K test set with direct win/tie/loss labels for comparative judgment. Baselines. We compare our proposed JudgeLRM models against three categories of baselines, including (1) State-of-the-art proprietary LLMs, including GPT-3.5, GPT-4, and 5 Figure 3: Judgment performance improvement vs. reasoning requirement across domains. The Y-axis indicates the F1 score improvement of JudgeLRM-7B over the Qwen2.5-7BInstruct-Judge-SFT baseline; the X-axis represents the proportion of tasks within each domain that require reasoning. Each point corresponds to domain. negative linear trend (y = 0.2x 1.05, R2 = 0.95) suggests that domains with higher proportion of reasoning-intensive tasks see greater performance gains from JudgeLRM-7B. Deepseek-R1; (2) Specialized judgment models, such as Auto-J-13B, JudgeLM-7B/13B/33B, and PandaLM-70B; (3) Base models, comprising the Qwen2.5-3B and Qwen2.5-7B Instruct models, as well as their supervised fine-tuned variants adapted for the judgment task. Evaluation Metrics. Following prior work, we evaluate model performance using agreement-based metrics: precision, recall, and F1 score,which quantify alignment with teacher model judgments. Implementation Details. Our models, JudgeLRM-3B and JudgeLRM-7B, are based on Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct, respectively. Both models are trained exclusively using reinforcement learning on the JudgeLM training set. The training is conducted over single epoch, with maximum prompt length of 1024 tokens and maximum response length of 2048 tokens, using batch size of 16. JudgeLRM-3B is trained on 4A100 80GB GPUs with learning rate of 3e-7, while JudgeLRM-7B is trained on 8A100 80GB GPUs with learning rate of 1e-6. 4. Judge Performance We benchmark JudgeLRM against range of competitive models, with results presented in Table 1. Among 7B-scale models, JudgeLRM-7B significantly outperforms its SFT counterpart (i.e., Qwen2.5-7B-Instruct-Judge-SFT) and the task-specific JudgeLM-7B model. Notably, JudgeLRM-3B outperforms GPT-4 on the human-annotated PandaLM benchmark, and scaling to JudgeLRM-7B yields performance that matches or even exceeds that of Deepseek-R1. To better understand the source of JudgeLRM-7Bs performance gains over the fine-tuned JudgeLM baseline, we analyze its effectiveness on reasoning-related tasks within the PandaLM benchmark. As shown in Figure 3, we observe clear positive correlation between performance improvement and the proportion of reasoning-intensive instances. Specifically, we compare JudgeLRM-7B against its base model, Qwen2.5-7B-Instruct, and find notable correlation coefficient of 0.20 between improvement and reasoning rate, highlighting larger gains in categories with more reasoning-demanding samples. This provides strong evidence for the reasoning-intensive nature of judge tasks and the effectiveness of RL in enhancing judgement capabilities through reasoning. 6 4.3 Ablation Study To assess the impact of different reward components on JudgeLRMs performance, we conduct an ablation study on the human-annotated PandaLM dataset. Specifically, we analyze the contributions of the content rewards (rabsolute and rconfidence) as formulated in Section 3.2. We observe that removing content rewards results in significant performance drop of 25%, highlighting the importance of incentivizing correct reasoning through rewards that emphasize judgment accuracy and confidence. Effect of Incentivizing Lengthy response To understand how response length relates to thinking, we observe that during JudgeLRM training, both thinking and response lengths increase with training steps, with the 7B model producing longer outputs than the 3B model  (Fig. 4)  . To test if longer responses improve performance, we introduced length reward shown below. Rlength = 0.2, 1.0, 0, if the reasoning chain exceeds 120 tokens if the maximum token limit is reached otherwise (9) However, as shown in Table 1, simply incentivizing slightly longer answers degraded results of about 3%, suggesting that encouraging verbosity does not benefit the judge task. 4 Figure 4: Response Length of JudgeLRM-3B and JudgeLRM-7B by Steps. 4.4 Reliability of Model Judgment key property of judgment models is reliability the ability to produce consistent and accurate judgments regardless of the order in which candidate answers are presented. To evaluate the reliability of JudgeLRM and representative baselines, we conduct experiments on the JudgeLM dataset by permuting the order of answers. We assess three aspects: (1) self-consistency across permutations, (2) positional bias toward the first or second answer, and (3) the bias gap ( bias), which captures variance in position-based preferences. As shown in Table 3, JudgeLRM substantially improves self-consistency compared to both its base Instruct model and the task-specific JudgeLM baseline. Additionally, JudgeLRM significantly reduces bias toward the first answer while maintaining comparable bias levels toward the second, achieving more balanced and robust evaluation. 4.5 Case Study JudgeLRM exhibits superior judgment capabilities by learning to reason effectively. To better understand how it makes informed and high-quality predictions, we analyze its responses for signs of emergent reasoning behavior, particularly those aligned with the LRM-style cognitive patterns observed in DeepSeek-R1, as described by Gandhi et al. (2025). case study performing such behavior in DeepSeek-R1 is shown in Figure 15. 7 Question: (Instruction:) Include important study notes and key points that someone should know about the given subject. (Input:) history of the USA [Assistant 1s Answer] The history of the United States is one of the most influential and influential countries in the world. It has had profound impact on the world and continues to shape the world today. The United States has had long and storied history, from its founding in 1775 to the present day. It has had tumultuous and tumultuous history, from the Revolutionary War to the Civil War and beyond. The United States has had tumultuous and tumultuous history, from the Revolutionary War to the Civil War and beyond. The United States has had long and storied history, from its founding in 1775 to the present day. It has had profound impact on the world and continues to shape the world today. [Assistant 2s Answer] 1. The United States of America was founded in 1776. 2. The Declaration of Independence was signed in 1776. 3. The Constitution of the United States of America was signed in 1787. 4. The Civil War began in 1861. 5. The Emancipation Proclamation was issued in 1863. 6. The 13th Amendment was ratified in 1865. 7. The 14th Amendment was ratified in 1868. 8. The 15th Amendment was ratified in 1870. 9. The 16th Amendment was ratified in 1913. 10. The 17th Amendment was ratified in 1913. 11. The 18th Amendment was ratified in 1919. 12. The 19th Amendment was ratified in 1920. 13. The 20th Amendment was ratified in 1933. 14. The 21st Amendment was ratified in 1933. Response: <think> Assistant 1 provided very broad, somewhat repetitive statement on the history of the United States which did not include specific key events, dates, or figures that would be considered important in the narrative of U.S. history. This kind of vague and repetitive phrasing might make it difficult for someone seeking specific historical information to gain an accurate or clear understanding. While it correctly noted the overall influence and significant historical periods like the Revolutionary War and the Civil War, this approach lacks specificity which can be misleading or uninformative. In contrast, Assistant 2s answer is quite accurate and relevant in terms of providing list of critical events and amendments in U.S. history. Each point listed is accurate, such as the dates when key documents were signed, the years of major constitutional changes, and significant legislative acts like the Emancipation Proclamation and various constitutional amendments related to civil rights and representation. (Verification) By including these specific details, the accuracy and relevance of the information provided by Assistant 2 is significantly higher, making it easier for someone to trace specific aspects of American history through these milestones. (Subgoal Setting) Accuracy is also concern in Assistant 1s answer since the statement from its founding in 1775 is inaccurate; the actual founding date recognized in the United States is July 4, 1776, with the signing of the Declaration of Independence, rather than 1775. (Verification, Double Checking) Additionally, the repetition might confuse readers, suggesting lack of clarity or precision in how the history is presented, which could mislead people who are looking for clear timeline or significant events. (Reflection, Error Identification) Both answers need to be more precise, but given the task, the importance lies in identifying key events, dates, and significant changes that shaped the nation. (Subgoal Setting) Thus, the accuracy and relevance in listing specific, factual events and their exact timing in history would be more appropriate way to address the question asked. (Decision Justification) Given these considerations, while both answers are somewhat relevant to the history of the United States, the accuracy and level of detail in the information provided by Assistant 2 would likely be seen as more accurate and relevant because they pinpoint specific, significant events and changes in the countrys development.</think> <answer>2</answer><answer>9</answer> Table 2: JudgeLRM-7B prediction on PandaLM case (ID 549). Through reinforcement learning, JudgeLRM demonstrates diverse emergent reasoning behaviors capabilities highlighted in blue and the corresponding response segments in red even though such behaviors were not explicitly present in the training data. In Table 2, we present representative example in which JudgeLRM evaluates two study notes and decides which one better supports learning about the history of the United States. The response demonstrates several reasoning strategies characteristic of high-quality judgment models: Verification: Ensuring factual accuracy to prevent misinformation and support reliable evaluations. Methods Consistency (w/ swap.) Bias toward 1st Bias toward 2nd Delta Bias JudgeLM score-first* GPT-4* Qwen-7B-Instruct 0shot JudgeLRM-7B 73.45 85.82 77.11 84.50 19.83 5.55 12.96 5.39 6.72 3. 9.93 10.11 13.11 2.46 3.04 4.72 Table 3: Assessment of position bias on the val split of JudgeLM. We evaluate model selfconsistency under different answer order permutations, quantify bias toward the first or second answer, and report the gap between these biases ( bias). Subgoal Setting: Decomposing the evaluation into smaller, interpretable steps for structured and comprehensive assessment. Double Checking & Reflection: Re-examining details to reduce errors, and reflecting on reasoning processes to improve judgment quality. Error Identification: Recognizing factual or logical flaws in candidate responses, key responsibility of effective judges. Decision Justification: Clearly articulating the rationale behind the final judgment, enhancing transparency and persuasiveness. 5,5. Both responses were relevant and accurate. They both provided detailed overview of the history of the United States, including important events and dates. However, the response was repetitive, mentioning the same events twice, which is why it didnt receive perfect score. Table 4: Qwen2.5-7B-Instruct-Judge-SFT response of ID 549 . Moreover, JudgeLRM often adopts structured three-part reasoning patternfirst evaluating Assistant 1, then Assistant 2, and finally conducting comparative assessmentsimilar to human-like evaluation behavior (Dasgupta et al., 2024). This structured reasoning reflects JudgeLRMs ability to integrate semantic understanding, logical inference, and hierarchical comparison. Such capabilities are directly reinforced by the judge-wise reward design, which jointly incentivizes structural coherence and content accuracy, enabling the model to learn effective judgments through reasoning. However, the response of Qwen-2.5-7BInstruct-Judge-SFT  (Table 4)  fails on reasoning this judge task, which explanation is lack of thinking and analysis."
        },
        {
            "title": "5 Conclusion",
            "content": "Our work demonstrates that judgment tasks for LLMs are fundamentally reasoningintensive, and models trained solely through SFT struggle in domains with high reasoning demand. By framing evaluation as RL problem and introducing judge-specific, outcomedriven reward structures, JudgeLRM effectively learns to produce structured and trustworthy reasoning paths. Empirical results show that JudgeLRM not only outperforms leading models such as GPT-4 and DeepSeek-R1, but also scales effectively across model sizes. Beyond performance, our analysis reveals that successful judgment involves complex reasoning behaviors like verification, sub-goal planning, and justificationhighlighting the need to treat judgment not as mere scoring, but as process of structured reasoning. We hope this work paves the way for future research in training LLMs that evaluate with both rigor and reliability."
        },
        {
            "title": "References",
            "content": "Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or LLMs as the judge? study on judgement bias. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 83018327, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.474. URL https: //aclanthology.org/2024.emnlp-main.474/. Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms, 2024b. URL https://arxiv.org/abs/2412.18925. Ishita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Hannah R. Sheahan, Antonia Creswell, Dharshan Kumaran, James L. McClelland, and Felix Hill. Language models show human-like content effects on reasoning tasks, 2024. URL https://arxiv.org/abs/ 2207.07051. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Lengthcontrolled alpacaeval: simple way to debias automatic evaluators, 2025. URL https://arxiv.org/abs/2404.04475. Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. Bias and fairness in large language models: survey, 2024. URL https://arxiv.org/abs/2309.00770. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. URL https://arxiv.org/abs/2503.01307. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, and Jian Guo. survey on llm-as-a-judge, 2025. URL https: //arxiv.org/abs/2411.15594. John Gu and Others. comprehensive survey on llm-as-a-judge. ArXiv, abs/2401.12345, 2024. URL https://arxiv.org/abs/2401.12345. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners, 2022. URL https://arxiv.org/abs/2205. 11916. Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models, 2025. URL https://arxiv.org/abs/2503.13939. Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. Llms-as-judges: comprehensive survey on llm-based evaluation methods, 2024. URL https://arxiv.org/abs/2412.05579. Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu, Dezhi Chen, Yun Chen, Zuo Bai, and Liwen Zhang. Fin-r1: large language model for financial reasoning through reinforcement learning, 2025. URL https://arxiv.org/abs/2503.16252. OpenAI. Introducing openai o1, 2024. URL https://openai.com/o1/. Accessed: February 2024. Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Jimin Huang, and Qianqian Xie. Fino1: On the transferability of reasoning enhanced llms to finance, 2025. URL https: //arxiv.org/abs/2502.08127. 10 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization, May 2024. URL http://arxiv.org/abs/2306.05087. arXiv:2306.05087 [cs] TLDR: PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpacas hyperparameters. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2022. URL https://arxiv.org/abs/2201.11903. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning, 2025. URL https://arxiv.org/abs/2502.14768. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalable judges, 2025. URL https://arxiv.org/abs/2310.17631."
        },
        {
            "title": "Details of PandaLM Category Analysis",
            "content": "Table 5: Model F1 Performance Comparison by Categories Metric Entertainment Media Office Productivity Life Utility Search Information Retrieval Social Professional Networking Total Reason (%) F1 Qwen-7B-Instruct F1 Qwen-7B-Instruct-Judge-SFT F1 JudgeLRM-7B 195 28.72 56.95 62.52 67.01 105 6.67 59.71 74.88 74. 190 24.21 57.15 60.75 65.01 108 18.52 44.06 58.11 60.41 104 11.54 50.08 58.43 60.15 The five main categories are collated from PandaLM motivation app label. Entertainment Media includes Netflix, IMDB, Spotify, YouTube, ESPN, Instagram, Facebook, Twitter, Telegram. Office Productivity is from MS Excel, MS Powerpoint, Google Sheet, Jira, Google Meet, Gmail. Life Utility is from Weather, Tasty, Sudoku, Goodreads, Yelp, traipadvisor.com, Redfin, Play Store, Amazon, Wysa, Real Estate. Search Information Retrieval is from Google Search, Quora, Reddit, CNN News, Indeed, Meetup Netflix, IMDB. Social Professional Networking is from LinkedIn, Messenger, Blogger. The need reason label is assigned according to the evaluation scale in Prompt 5, and we define that scores 1-4 denote cases where reasoning is unnecessary, while scores 5-10 indicate that reasoning is needed. The results in Table 5. We also showcase subset of motivation app (query category) to demonstrate the improvement from SFT to JudgeLRM."
        },
        {
            "title": "Prompt for Accessing the Necessity of Reasoning When Judging",
            "content": "We show prompt in Fig. 5 to rate the level of reasoning ability needed to perform the judgment and two cases of rating in Fig. 6 and Fig. 7. The reasoning rate is not totally decided by the requirement for reasoning to answer the question. In Fig. 6, judging math problem doesnt need reasoning. In Fig. 7, judging writing problem needs reasoning. 11 Table 6: Selected dataset results for PandaLM sub testset. Accuracy (Acc.), Precision (Prec.), Recall (Rec.), and F1-score (F1). Methods Wolframalpha35 Grammarly Gmail44 Judge w/o reference (Ours). Qwen-7B-Instruct 45.71, 50.93, 53.21, 45.14 63.33, 54.55, 41.67, 46.96 61.36, 54.59, 69.44, 46.69 Qwen-7B-Instruct-Judge-SFT 48.57, 46.01, 53.97, 46.27 73.33, 56.37, 46.30, 50.48 70.45, 60.56, 77.30, 55.60 60.00, 58.97, 64.22, 58.33 76.67, 61.11, 50.93, 55.56 75.00, 60.43, 80.71, 59.49 JudgeLRM-7B Prompt for Reasoning Ability Judgment For the data provided below, \"response1\" and \"response2\" represent two responses generated for the given \"instruction\" and \"input\". Consider the task of judging the performance of \"response1\" and \"response2\" in response to the \"instruction\" and \"input\". On scale of 1 to 10, rate the level of reasoning ability needed to perform this judgment. Please provide your response in EXACTLY the following format: ---------------------------------------- Score: [your score, an integer between 1 and 10] Explanation: [your explanation] ---------------------------------------- Instruction: item[instruction] Input: item[input] Response1: item[response1] Response2: item[response2] Figure 5: Prompt format for evaluating the reasoning difficulty of judging AI responses. Intended for appendix."
        },
        {
            "title": "Dataset",
            "content": "PandaLM (Human as ground truth) Criteria Existing Baseline. (* from original paper) Base Models."
        },
        {
            "title": "Agreement Precision Recall",
            "content": "Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Deepseek-R1 Ours. JudgeLRM-3B JudgeLRM-7B - w/o. judge score reward 74.27 74.50 83.48 82.66 83.56 82. 74.47 74.49 83.53 82.97 83.58 82.88 73.89 74.57 83.31 82.94 83.41 82.77 F1 73.96 74.48 83. 82.66 83.47 82.44 Table 7: Performance on PandaLM testset excluding ties. Performance on PandaLM testset excluding ties. Since scoring pair-wise does not easily yield tie, we simultaneously give the scoring of the pandalm for which the test set excludes ties (10% in total) and treats the predicted tie as choosing the first assistant. The result is shown in Table 7. 12 Case Example 1 (idx: 145) \"idx\": 145, \"motivation app\": \"Wolfram alpha\", \"cmp key\": \"llama-7b opt-7b\", \"instruction\": \"Solve this equation.\", \"input\": x3 4x2 + 6x 24 = 0 \"response1\": \"x = 2\", \"response2\": \"x = 0\", \"annotator1\": 0, \"annotator2\": 0, \"annotator3\": 0, \"label\": 0, \"needed reasoning rate1-10\": 3, \"rate explanation\": \"The task involves checking whether the responses solve the given cubic equation. Verifying = 2 or = 0 requires substituting the values into the equation and performing basic arithmetic, which is straightforward. No advanced reasoning or complex problem-solving is needed, as the equation can be evaluated directly. However, recognizing that = 2 is correct solution and = 0 is incorrect involves minimal reasoning.\" \"----------------------------------------\" Figure 6: Example Case 1 (idx: 145) data for reasoning evaluation. Case Study 1 Fig. 8 show case of question, and Fig. 9,10 shows the result from JudgeLRM-3B. Fig. 11 shows the result of Deepseek-R1. Fig. shows the gold answer provided by JudgeLM from GPT-4. Case study Fig. 13 show another case of question, and Fig. 14 shows the result from JudgeLRM-7B. Fig. 15 shows the result of Deepseek-R1. Fig. 16 shows the gold answer provided by JudgeLM from GPT-4. 13 Case Example 2 (idx: 549) \"idx\": 549, \"motivation app\": \"Google Docs\", \"cmp key\": \"cerebras-gpt-6.7B pythia-6.9b\", \"instruction\": someone should know about the given subject.\", \"Include important study notes and key points that \"input\": \"history of the USA\", \"response1\": \"The history of the United States is one of the most influential and influential countries in the world. It has had profound impact on the world and continues to shape the world today. The United States has had long and storied history, from its founding in 1775 to the present day. It has had tumultuous and tumultuous history, from the Revolutionary War to the Civil War and beyond. The United States has had tumultuous and tumultuous history, from the Revolutionary War to the Civil War and beyond. The United States has had long and storied history, from its founding in 1775 to the present day. It has had profound impact on the world and continues to shape the world today.\", \"response2\": \"1. The United States of America was founded in 1776.\" \"2. The Declaration of Independence was signed in 1776.\" \"3. The Constitution of the United States of America was signed in 1787.\" \"4. The Civil War began in 1861.\" \"5. The Emancipation Proclamation was issued in 1863.\" \"6. The 13th Amendment was ratified in 1865.\" \"7. The 14th Amendment was ratified in 1868.\" \"8. The 15th Amendment was ratified in 1870.\" \"9. The 16th Amendment was ratified in 1913.\" \"10. The 17th Amendment was ratified in 1913.\" \"11. The 18th Amendment was ratified in 1919.\" \"12. The 19th Amendment was ratified in 1920.\" \"13. The 20th Amendment was ratified in 1933.\" \"14. The 21st Amendment was ratified in 1933.\" \"annotator1\": 2, \"annotator2\": 2, \"annotator3\": 2, \"label\": 2, \"needed reasoning rate1-10\": 7, \"rate explanation\": \"The task requires evaluating the quality of responses based on their adherence to the instruction to include important study notes and key points about the history of the USA. Response1 is repetitive and lacks specific details, while Response2 provides clear, concise list of key historical events. The reasoning needed to judge these responses involves assessing clarity, specificity, and relevance to the instruction, which is moderately complex.\" \"----------------------------------------\" Figure 7: Example Case 2 (idx: 549) data for reasoning evaluation. 14 The case of question [Question] Im trying to find out how to get my ex-boyfriend back. Can you help me? [Assistant 1s Answer] Im sorry to hear that youre going through tough time. However, as an AI language model, cannot provide relationship advice. You may want to seek guidance from trusted friend or professional counselor who can offer you tailored advice and support based on your unique situation. wish you the best of luck. [Assistant 2s Answer] Im sorry to be the one to tell you this but, no, we cant help you get your ex-boyfriend back. Our site is free resource that offers advice and tips on how to meet new people and make new friends. If youre looking for advice on how to get an ex-boyfriend back, we cannot help you. Your ex-boyfriend is still with you, and we would therefore advise you to use this time to try to find out what went wrong in your relationship. You have good chance of getting him back, but we cannot advise you on how to do this. Were here to help you find friendship and romance, not to help you get back with your ex-boyfriend. We wish you all the best with this and hope weve helped you find friend here!The Best-Selling and Award-Winning Author of THE EAGLES GIFT Trilogy and The Secret Circle. Do you want to know if he still loves you? Get the answer here for free. Are you ready to meet other singles? Get started now, its free! What to Do Now that He Ignored You? Figure 8: Case Question 1. 15 An example of thinking and answer from JudgeLRM 3B after training 500 steps <think> For Assistant 1s response, the tone and content are appropriate for providing guidance and support in respectful manner, while emphasizing limitations of the AI service. It maintains courteous and informative approach by advising the user to seek help from friends or professionals rather than attempting to directly influence their ex-partners actions. The criteria considered include helpfulness - it gives helpful advice about seeking support; relevance - it stays focused on the query and suggests appropriate resources; accuracy - it avoids any misleading information; and level of detail - it provides sufficient information without being overly verbose. In terms of length, the response remains concise and straightforward. It avoids unnecessary tangents or irrelevant information, making the message clear and direct. It also adheres to the boundaries set by the capabilities of an AI language model. Regarding Assistant 2s response, it contains several issues including repetition, redundancy, inappropriate suggestions, and the inclusion of promotional elements which are unrelated to the core inquiry. These aspects make the answer less helpful, relevant, accurate, and detailed compared to Assistant 1s response. Assistant 2 employs confrontational tone, which is inconsistent with the initial polite greeting and seems out of place given the context of seeking advice. Additionally, the answer does not adhere strictly to the criteria mentioned: it fails to provide substantial, direct advice and instead veers into irrelevant topics such as promoting services and speculating about past relationships. Furthermore, the inclusion of personal opinions, endorsements, and authorship details detracts from the informational value and distracts from the primary objective of providing advice. Lastly, the inclusion of unrelated statements like Are you ready to meet other singles? and What to Do Now that He Ignored You? does not add to the quality of the advice and appear more suited for dating app rather than relationship recovery guide. </think> <answer>8</answer><answer>4</answer> Figure 9: An example of thinking and answer from Judgelrm 3B after training 500 steps. 16 An example of thinking and answer from Judgelrm 3B <think> Okay, so need to evaluate the performance of Assistant 1 and Assistant 2 based on the users question about getting an ex-boyfriend back. The criteria are helpfulness, relevance, accuracy, and level of detail. Let me break it down step by step. First, looking at Assistant 1s response. They start by expressing sympathy, which is good for empathy. Then they state they cant provide relationship advice and suggest seeking help from friends or professionals. Thats responsible because giving relationship advice can be tricky and possibly harmful. However, they dont offer any resources or steps, just referral. So, helpfulness is limited but honest. Relevance is okay because they address the users intent but dont engage further. Accuracy is good because theyre correct in directing to real-world help. Level of detail is low since theres no actionable advice. Now, Assistant 2. Their answer starts by saying they cant help, which is direct. They mention their site is for meeting new people, which is bit off-topic since the user wants to reconcile. Then they suggest reflecting on the relationships issues, which is somewhat helpful. But then the response gets messy with promotional content about books, quiz, and dating site plugs. The irrelevant links and self-promotion lower the score. Theres some attempt to advise but its buried in distractions. Accuracy is questionable because parts like Your ex-boyfriend is still with you might be incorrect. Relevance suffers due to the unrelated links. Detail is inconsistent2014some advice but mostly filler. Comparing both: Assistant 1 is concise, empathetic, and correctly refers to professionals but lacks details. Assistant 2 tries to give advice but includes irrelevant content and promotions, making it less helpful and accurate. Assistant 2s promotional aspects and possible inaccuracies drag down its score. Assistant 1 is more focused and responsible, though not detailed. So scores would be Assistant 1 around 6 for being relevant and accurate but not detailed. Assistant 2 lower, maybe 3, due to irrelevant content and lower accuracy. </think> <answer>7</answer><answer>3</answer> Figure 10: Case 1 example of thinking and answer from Judgelrm 3B. 17 The reasoning of Deepseek R1 Example 1 Answer: <think> Okay, need to evaluate the two assistants answers based on the criteria: helpfulness, relevance, accuracy, and level of detail. Let me break it down step by step. First, the users question is about getting their ex-boyfriend back. Theyre seeking advice, so the assistants responses should address that need appropriately. Starting with Assistant 1: The response is empathetic, acknowledging the users feelings. They state they cant provide relationship advice and suggest seeking help from friends or professionals. Thats responsible, as giving relationship advice can be tricky and potentially harmful. However, while its honest and cautious, it might not be very helpful because it doesnt offer any actionable steps. Relevance is there since they address the request, but accuracy is neutral because they dont provide incorrect info. Level of detail is low because theres no specific guidance, just referral elsewhere. Now Assistant 2: Their answer starts by directly refusing help but then gets confusing. They mention their site is for meeting new people, which isnt what the user asked. Theres contradiction: first saying they cant help, then suggesting the user has good chance but cant advise. This inconsistency might be misleading. Also, the inclusion of promotional links (free answers, meeting singles) is irrelevant and possibly distracting. The advice to find out what went wrong is somewhat relevant but buried in lot of off-topic content. The accuracy is questionable here because parts of the response are contradictory and include unsolicited ads. The level of detail is mixed; some parts try to address the issue but get sidetracked. Comparing both, Assistant 1 is more straightforward and honest, albeit brief. Assistant 2 tries to give some advice but muddies it with irrelevant content and self-promotion, which could be unhelpful or even harmful. However, Assistant 2 does touch on evaluating the relationships issues, which is slightly more detailed but not effectively communicated. In terms of helpfulness, Assistant 1 gets points for directing to proper help, while Assistant 2s advice is mixed with spam. Relevance: Assistant 1 stays on topic better. Accuracy: Assistant 1 is more accurate by not giving potentially wrong advice. Detail: Neither is highly detailed, but Assistant 2s extra content is not useful. So, scores would be Assistant 1 around 6-7, Assistant 2 lower, maybe 3-4. <answer>7</answer><answer>3</answer> Example 2 Answer: <think> Okay, lets start by looking at the users question. They want to get their ex-boyfriend back and are asking for help. Now, need to evaluate both Assistant 1 and Assistant 2s responses based on the criteria: helpfulness, relevance, accuracy, and level of detail. Starting with Assistant 1: The response is polite and empathetic. They mention not being able to provide relationship advice and suggest seeking help from friends or professionals. This is responsible, but maybe bit too brief. It doesnt address the users specific request directly beyond declining to help. However, its accurate in stating their limitations and points the user to other resources. The relevance is there because they acknowledge the users struggle, but the helpfulness is limited since they dont offer any actionable advice. Level of detail is lowjust short paragraph. Now Assistant 2: The response starts by saying they cant help get the ex back but then contradicts that by saying we cannot advise you on how to do this. Wait, thats confusing. They mention their site is for meeting new people, which is not what the user asked. Then theres lot of promotional content like ads for books, quizzes, and dating sites. The answer includes irrelevant links and self-promotion, which isnt helpful. The accuracy is questionable because parts of the response are conflicting (saying they cant help but also hinting theres chance). The relevance is low because they divert the user to other services instead of addressing the request. Also, the level of detail is poor as they dont provide any real advice but add unnecessary elements. Comparing both, Assistant 1 is honest, stays within their role, and directs to appropriate help, though its not detailed. Assistant 2 is more problematicunclear, includes ads, and isnt focused on the users needs. Assistant 1 might get higher score for being relevant and accurate, even if brief. Assistant 2s score would be lower due to irrelevant content and inaccuracies. </think> <answer>6</answer><answer>3</answer> Figure 11: The reasoning of Deepseek-R1 of Case 1. 18 Gold Response of Case 1 from Gpt8 4 Assistant 1 provided compassionate and professional response, acknowledging the users emotional state and suggesting seeking advice from trusted friends or professional counselors. Although it didnt provide direct answer to the users question, it was still helpful and relevant in the context of the question. On the other hand, Assistant 2s response was confusing and inconsistent. It started by stating that it couldnt help the user get her ex-boyfriend back, then suggested that the users ex-boyfriend was still with her, which contradicts the users question. It also included irrelevant promotional content about meeting other singles and best-selling author, which was not related to the users question. Therefore, Assistant 1s response was more helpful, relevant, and accurate than Assistant 2s. Figure 12: Gold Response of Case 1 from Gpt-4. Case Question 2 [Question] Ive heard that some people can have high blood pressure without having any symptoms. Is this true? [Assistant 1s Answer] Healthcare provider: Yes, this is true. High blood pressure is condition in which the blood pressure in your arteries is too high. It can be caused by variety of factors, such as stress, diet, and exercise. Patient: do some people have high blood pressure without any symptoms? Im aware of the factors that can increase blood pressure, but why Its possible that some people with high blood presHealthcare provider: sure may not have any symptoms, especially if the blood pressure is not very high. However, high blood pressure puts you at risk of developing heart disease and stroke, so its important to have your blood pressure checked regularly and to make any necessary lifestyle changes to keep it under control. Patient: understand. How can lower my blood pressure? Healthcare provider: There are several things you can do to lower your blood pressure. One of the most important things is to make lifestyle changes, such as: Eating healthy diet that is rich in fruits, vegetables, and whole grains Exercising regularly Reducing stress [Assistant 2s Answer] And if so, how can lower my risk of this disease? Thank you! Figure 13: Case Question 2. 19 The reasoning of JudgeLRM-7B (main task pid=2260117) <think> will evaluate Assistant 1 and Assistant 2 based on the criteria of helpfulness, relevance, accuracy, and level of detail. Assistant 1 starts with statement about their own paleness, which is bit whimsical and not necessary in the context. They then provide information about avoiding sunburn, which doesnt seem to answer the asked questionabout tanning. They mention alternatives like tanning bed, sunscreen, and protective clothing as ways to avoid burning. This information is generally useful and relevant. However, their response is primarily focused on preventing sunburn rather than providing information on how to tan without burning. The explanation can be more practical and customer-oriented, providing clearer understanding to someone looking for tips on tanning. Assistant 2 explains that there are multiple ways to get tan while maintaining skin health. They suggest using sunscreen, which, while protective, can indeed help with tanning. They also advise to use tanning bed carefully, which seems fair inclusion in variety of methods. Both assistants mention protective clothing, which is probably the most effective method to avoid sunburn and to ensure safe tanning. They both also give brief mention of times to avoid direct sunlight for best results. Assistant 2s response, however, provides more comprehensive and clear explanation of different methods to get tan without burning, while also providing context on how to protect your skin from UV rays. In addition, they explore the relation between tanning and vitamin production, pointing out that getting tan might be healthy for general audience. Their response seems to be more to the point and fits the asked question little better, also giving more contextual information that could be valuable for the reader. Although both Assistants seem to be accurate, in the sense that the advice given does not convey any apparent misinformation, Assistant 2 provides more detailed information about individual ways to get tan while also explaining the potential benefits and health aspects, thus more relevant to five-year-old than Assistant 1. In terms of helpfulness, both provide useful suggestions. Assistant 1 is tad less clear and focused, while Assistant 2s points are slightly easier to understand and can be clearly applied to the readers context. Assistant 2 also provides educational information about skin health and tanning, found in their reference to vitamin benefits, which can be slightly helpful. Assistant 1s response is more conversational, and although it provides general information, the tone seems more casual and could be confusing, while Assistant 2s is more structured and detailed, providing clear and helpful mixture of information and advice. </think> <answer>6</answer><answer>9</answer> Figure 14: The reasoning of JudgeLRM-7B in Case 2. 20 Deepseek-R1s thinking and answer of Case 2 Okay, so the user is asking if its true that people can have high blood pressure without symptoms. Lets look at both assistants answers to evaluate their performance. Starting with Assistant 1s answer. The healthcare provider responds affirmatively, which is correct. They mention that high blood pressure can be asymptomatic, especially if not very high. Thats accurate. They also note the risks like heart disease and stroke, which is good. The patient then asks how to lower blood pressure, and the provider lists diet, exercise, and stress reduction. But wait, the answer seems cut off. The formatting is bit odd with the patient and provider dialogue. The healthcare providers advice is correct but maybe lacks some detail. For example, they could mention reducing sodium intake, limiting alcohol, or medications if needed. Also, the response to the second patient question about lowering blood pressure is bit brief. The answer is relevant and accurate but could be more detailed. Now, Assistant 2s answer. Wait, the users question ends with And if so, how can lower my risk of this disease? Thank you! but Assistant 2s answer seems incomplete. Actually, looking back, the users message might be continuation. Wait, the users message includes two parts: the initial question and then follow-up. However, in the provided dialogue, Assistant 2s answer is just And if so, how can lower my risk of this disease? Thank you! which seems like repetition of the users follow-up question, not an actual answer. That might be formatting error. Wait, maybe the users input was split, and Assistant 2s answer is actually the part after the users follow-up. But looking at the structure, after the users first question, both assistants answers are presented. Wait, the users message includes the question and then two assistants answers. The first assistants answer is dialogue between healthcare provider and patient, addressing the initial question and follow-up. The second assistants answer seems to be only And if so, how can lower my risk of this disease? Thank you! which is not an answer but repetition of the users follow-up. That must be mistake in the input formatting. Alternatively, maybe Assistant 2s answer is missing. Wait, the way its structured: after the users question, the users message includes the question and then the two assistants answers. So Assistant 1s answer is the dialogue, and Assistant 2s answer is the line starting with And if so... which is actually part of the users message. Wait, no, looking at the exact structure: The users message is: [Question] Ive heard that some people can have high blood pressure without having any symptoms. Is this true? [Assistant 1s Answer] ... dialogue ... [Assistant 2s Answer] And if so, how can lower my risk of this disease? Thank you! Ah, so Assistant 2s answer is just that line, which is actually the users follow-up question, not an answer. Thats problem. So Assistant 2s answer is incomplete or incorrectly formatted. It seems like Assistant 2s answer is just repeating the users own follow-up question, which doesnt make sense. Therefore, Assistant 2s answer is not providing any useful information. Its either mistake or placeholder. So in terms of scoring, Assistant 1 provided relevant and accurate answer, albeit bit brief and with some formatting issues (like the dialogue structure which might be confusing). Assistant 2s answer is nonsensical as its just repeating the users question. Therefore, Assistant 2 gets low score. But wait, maybe Im misinterpreting. Let me check again. The users message includes the initial question and then two assistant answers. The first answer is the dialogue, and the second is that line. So Assistant 2s answer is just And if so... which is not an answer. So Assistant 2 failed to provide proper response. Therefore, Assistant 1 gets points for correctness and relevance but loses some for lack of detail. Assistant 2s answer is not an answer, so its irrelevant and unhelpful. So scores: Assistant 1 maybe 7 (correct but could be more detailed), Assistant 2 1 or 2 for not answering. </think> <answer>7</answer><answer>2</answer> Figure 15: Example of Deepseek-R1s Thinking and Answer of Case 2. Gold response of Case 2 from GPT-4 9 1 Assistant 1 provided comprehensive, accurate, and relevant answer to the users question. The assistant explained what high blood pressure is, why some people might not have symptoms, and the risks associated with it. The assistant also provided detailed advice on how to lower blood pressure, which was very helpful. Therefore, Assistant 1 receives score of 9. On the other hand, Assistant 2s response was not relevant to the users question. It seems like the assistant misunderstood the question and provided an incomplete response. The assistant did not provide any information about high blood pressure or how to lower it. Therefore, Assistant 2 receives score of 1. Figure 16: Gold response of Case 2 from GPT-4 (Provided in PandaLM dataset) (No reasoning from gpt-4)."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}