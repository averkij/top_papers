{
    "paper_title": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1",
    "authors": [
        "Qianli Ma",
        "Siyu Wang",
        "Yilin Chen",
        "Yinhao Tang",
        "Yixiang Yang",
        "Chang Guo",
        "Bingjie Gao",
        "Zhening Xing",
        "Yanan Sun",
        "Zhipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in a single command, but in a collaborative, hierarchical process. We introduce $\\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated \"Checker\" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the author's vision, transforming the system from a mere tool into a powerful collaborative assistant. To rigorously validate our approach, we also construct $\\textbf{PageBench}$, the first benchmark for this new task. Experiments show AutoPage not only generates high-quality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than \\$0.1. Code and dataset will be released at $\\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$."
        },
        {
            "title": "Start",
            "content": "Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1 Qianli Ma1* Siyu Wang1* Yilin Chen1* Yinhao Tang2* Yixiang Yang1 Chang Guo1 Bingjie Gao1 Zhening Xing2 Yanan Sun2 Zhipeng Zhang1 1AutoLab, SAI, Shanghai Jiao Tong University 2Shanghai AI Laboratory {mqlqianli,zhipengzhang}@sjtu.edu.cn Project Page1: https://AutoPage.github.io 5 2 0 2 2 ] . [ 1 0 0 6 9 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In the quest for scientific progress, communicating research is as vital as the discovery itself. Yet, researchers are often sidetracked by the manual, repetitive chore of building project webpages to make their dense papers accessible. While automation has tackled static slides and posters, the dynamic, interactive nature of webpages has remained an unaddressed challenge. To bridge this gap, we reframe the problem, arguing that the solution lies not in single command, but in collaborative, hierarchical process. We introduce AutoPage, novel multiagent system that embodies this philosophy. AutoPage deconstructs paper-to-page creation into coarse-to-fine pipeline from narrative planning to multimodal content generation and interactive rendering. To combat AI hallucination, dedicated \"Checker\" agents verify each step against the source paper, while optional human checkpoints ensure the final product aligns perfectly with the authors vision, transforming the system from mere tool into powerful collaborative assistant. To rigorously validate our approach, we also construct PageBench, the first benchmark for this new task. Experiments show AutoPage not only generates highquality, visually appealing pages but does so with remarkable efficiency in under 15 minutes for less than $0.1. Code and dataset will be released at Webpage1."
        },
        {
            "title": "Introduction",
            "content": "Efficient communication is as crucial to scientific advancement as the generation of new knowledge. Although academic papers are the primary medium for research dissemination, their density can hinder accessibility. To address this, researchers often create project pages to distill their work into accessible summaries, highlight key contributions, and *Equal Contribution. Corresponding Author. 1This page is generated by AutoPage. Refresh to see new, randomly selected version created by AutoPage. 1 Figure 1: Overview of our work. (a) End-to-end LLMs directly convert papers into project pages, resulting in unreasonable layouts and lacking human feedback. (b) Our proposed AutoPage integrates human-agent collaboration into automated page generation with higher content and visual quality. The figure also illustrates the process for constructing the PageBench benchmark. showcase demos. However, this is manual, repetitive process, typically involving the adaptation of existing templates, which consumes valuable research time and also results in inconsistent quality. This motivates our central question that Can we automate the generation of high-quality project pages directly from academic papers, thereby freeing researchers to focus on core research tasks? While our work is the first to tackle project webpages, the broader goal of automating research communication has been explored. Prior efforts have focused on converting papers into static visual formats, including slides (Zheng et al., 2025), posters (Sun et al., 2025; Pang et al., 2025), and videos (Ge et al., 2025; Zhu et al., 2025; Liu et al., 2025), with recent agent-based systems demonstrating impressive results. These solutions, however, are inherently tailored to fixed-size layouts and non-interactive content. Project webpages are fundamentally different as they demand flexible, scrollable structure and must integrate interactive elements such as expandable sections and dynamic visualizations. Moreover, the framework must adapt to varied paper structures and user preferences. This mismatch in format and functionality means existing methods are ill-suited for the task, thereby highlighting distinct gap which our work seeks to address by generating high-quality, interactive webpages for academic papers. We experimentally found that addressing this gap demands fundamental shift away from monolithic, end-to-end pipelines, such as an LLM like GPT-4o (see Sec. 5.2 for more details), shown in Fig. 1a. Instead, we conceptualize it as hierarchical, coarse-to-fine generation process augmented by iterative human-agent collaboration, as illustrated in Fig. 1b. This core principle enables us to manage the tasks inherent complexity by first establishing global narrative structure before progressively refining multimodal details. Crucially, by integrating human feedback at key stages, our approach ensures authorial control and alignment, reframing the system from simple autonomous generator into powerful collaborative assistant. Guided by this philosophy, we introduce AutoPage, multi-agent system that instantiates our coarse-to-fine, collaborative framework. AutoPage decomposes the complex task of webpage creation into structured pipeline encompassing three core phases, including narrative planning, multimodal content generation, and interactive page rendering. To ensure factual accuracy and mitigate the risk of LLM hallucination, each phase concludes with verification step. Here, dedicated LLM/VLMbased \"checkers\" act like quality inspectors on an assembly line, validating the generated content against the source paper before it proceeds to the next stage. Furthermore, the system is designed for flexible collaboration. While AutoPage can operate fully autonomously from start to finish, it also provides optional checkpoints for human intervention. This allows authors to steer the narrative, adjust visual elements, or make fine-grained edits if they choose, ensuring the final output is not only automated but also perfectly aligned with their vision. To rigorously evaluate AutoPage and spur future research, we further construct PageBench, the first benchmark dataset tailored for automated paper-topage generation. PageBench comprises diverse collection of over 1,500 academic papers paired with their corresponding human-created project pages, along with proposed comprehensive evaluation protocol that assesses content accuracy, narrative coherence, and visual design. Our evaluation reveals that AutoPage exhibits model-agnostic adaptability, functioning with various end-to-end models (e.g., GPT-4o, Gemini, and Qwen) without any adjustments to prompts or configurations. It provides substantial performance uplift in the task of academic page generation. Critically, the system demonstrates exceptional costeffectiveness and speed, with single page generated in under 15 minutes for less than $0.1 using Gemini-2.5-Flash. Our contributions are fourfold: We introduce the novel task of automated webpage generation for an academic paper. We propose an innovative coarse-to-fine, collaborative, and user-friendly framework, implemented as multi-agent pipeline that integrates LLM/VLM-based Checker agents for quality control and supports optional human oversight for author alignment. We introduce PageBench, the first benchmark for this task, featuring suite of novel metrics for holistically evaluating webpages on factual consistency, aesthetics, and authorial alignment. We demonstrate through extensive evaluations that AutoPage effectively generates factually accurate, visually appealing, and high-quality project pages."
        },
        {
            "title": "2 Related Works",
            "content": "LLM Agents. The story of Large Language Models (LLMs) is no longer one of solitary genius. They have broken free from their shells as standalone systems, stepping into the role of intelligent agents within dynamic, collaborative frameworks (Wang et al., 2024b; Xi et al., 2025; Xie et al., 2024; Zhao et al., 2023). This evolution equips them with the autonomy to tackle complex, multi-step tasks once thought to be the exclusive domain of human intellect. Works like ReAct (Yao et al., 2023) have shown how these agents can now autonomously plan strategies (Huang et al., 2024; Sun et al., 2023), wield digital tools (Qu et al., 2025; Shi et al., 2025), and reason through intricate problems (Fu et al., 2023). At their core, these agents operate in sophisticated loop. Specifically, they deconstruct abstract goals into actionable plans, enrich their understanding by retrieving external knowledge (Li et al., 2025b,a), and critically improve their own work through selfreflection (Shinn et al., 2023) or even by collaborating with other agents (Tran et al., 2025). This powerful paradigm is already reshaping the landscape of scientific discovery. Weve seen them act as tireless research assistants, automating literature surveys (Wang et al., 2024c), aiding in scholarly writing (Weng et al., 2025), and ensuring experimental reproducibility (Seo et al., 2025) by trans2 lating unstructured material into coherent outputs. Yet, the story doesnt end when the research is complete. compelling new chapter is now unfolding, focusing on the crucial post-research phase of communication and dissemination. There is growing recognition that these same agentic systems, supported by mature development frameworks like LangChain (Chase, 2022), AutoGen (Wu et al., 2024), and Voyager (Wang et al., 2023), can be harnessed to present and share research findings. This emerging trend promises to dramatically boost researcher productivity and amplify the impact of their work, moving beyond discovery to ensure it is effectively heard and understood. Automated Presentation Generation. As aforementioned, crucial aspect of the productivity enhancement lies in streamlining the creation of visual artifacts for research dissemination. Early, rule-based pipelines (Hu and Wan, 2013; Paramita and Khodra, 2016) represent the first attempt, but their rigid, template-driven nature means they are often brittle and struggle to weave cohesive narrative from complex scholarly text. new chapter begins with the rise of agentic systems powered by Vision-Language Models (VLMs), bringing fresh vitality to this field. This evolution is best witnessed in the journey of automated poster generation. Initial efforts like PosterBot (Xu and Wan, 2022) show promise with neural summarization but are constrained by simple layouts and lowresolution visuals. The true breakthrough comes with sophisticated multi-agent systems like those in Paper2Poster (Pang et al., 2025) and P2P (Sun et al., 2025). These advanced agents can autonomously plan layouts, write rendering code, and even use VLM feedback to self-correct visual errors. Similar agent-driven innovation has also reshaped slide generation, with tools like PPTAgent (Zheng et al., 2025) and SlideSpawn (Kumar and Chowdary, 2024) demonstrating modular designs that create high-fidelity slides. While slides and posters have received attention, the modern project webpage, arguably todays most vital format for online dissemination, remains surprisingly unexplored territory. To bridge this gap, we propose an agent-driven, automated, and interactive paper-to-page generation system to further enhance researcher productivity."
        },
        {
            "title": "3 AutoPage",
            "content": "This section details the workflow of our AutoPage, including Narrative Planning (Sec. 3.1), Multimodal Content Generation (Sec. 3.2), and Interactive Page Rendering (Sec. 3.3), as illustrated in Fig. 2. Critically, our design incorporates verification mechanism at the end of each phase to ensure factual grounding, and optional human-in-the-loop checkpoints for flexible collaboration. 3.1 Narrative Planning and Structuring The initial step in AutoPage is to transform the source paper in PDF format into structured narrative blueprint for the webpage. This process is orchestrated by two collaborating agents that first deconstruct the papers content and then architect new, web-centric narrative. The process begins with the Paper Content Parser, which ingests the source document and systematically deconstructs it. Using tools like MinerU (Wang et al., 2024a) and Docling (Team, 2024), it first converts the document into raw Markdown format, which is then refined by an LLM into clean, json-like structure. The result is an asset library that neatly organizes the papers core components, which contains: (i) textbased representations, mapping section headings to paragraph-level summaries, and (ii) visual-related representations, linking figures and tables to their corresponding captions and image files. Building upon this semantically rich asset library, the Page Content Planner then architects the webpages high-level structure. Rather than performing simple one-to-one mapping of the papers original sections, the Planner devises compelling narrative flow optimized for webpage presentation. It proposes logical outline, which mirrors how human would first establish layout before filling in the details. The output of this stage is foundational blueprint, which undergoes verification step to ensure its completeness and logical soundness before proceeding to content generation."
        },
        {
            "title": "3.2 Multimodal Content Generation",
            "content": "Once the narrative blueprint is finalized, the system proceeds to populate this structure with rich, multimodal content. This task is orchestrated by the Page Content Generator, which operates on deliberate \"text-first\" principle. This principle dictates that the narrative prose is generated first, serving as the anchor for the subsequent selection and placement of visual elements, thus ensuring tight semantic alignment between them. The process begins with Text Content Generator sub-component. For each section defined in the 3 Figure 2: Overview of AutoPage. AutoPage conducts multi-agent pipeline for transforming papers into interactive webpages: (1) Narrative Planning and Structuring parses PDFs into Markdown and generates section-level outlines; (2) Multimodal Content Generation produces coherent textvisual sections; (3) Interactive Page Rendering matches templates, compiles full HTML pages, and performs final layout checks. Throughout all phases, AutoPage integrates verification mechanisms and optional human-in-the-loop checkpoints for reliable and flexible generation. blueprint, this component synthesizes the key information from the parsers asset library, transforming it into polished, human-readable paragraphs. Its role is not merely to extract, but to craft clear and compelling narrative tailored for the web, forming the textual backbone of the page. With this textual backbone in place, the Visual Content Generator is activated. It analyzes the finalized prose of each section to select and render the most relevant figures or tables from the asset library. This text-driven approach guarantees that each visual element directly supports the accompanying narrative, rather than appearing as disconnected object, resulting in coherent module of information. To ensure the fidelity and quality of the generated content, two-stage verification and refinement process is employed. First, an automated Content Checker verifies the consistency between the generated text and its paired visuals. Then, the system offers crucial checkpoint for Human-inthe-Loop Refinement. At this stage, authors can provide language feedback (e.g., \"delete this section\", \"reorder the sections\") to iteratively refine the content until it perfectly aligns with their intent. The outcome is collection of author-approved content modules, ready for final rendering. 3."
        },
        {
            "title": "Interactive Page Rendering",
            "content": "With the author-approved content modules finalized, the system begins the process of rendering them into polished and interactive webpage. The process is driven by the Page Template Matcher, which operates on curated library of templates, each annotated with descriptive tags characterizing its layout and aesthetic properties (e.g., background_color, has_navigation). Instead of the system making an autonomous choice, the user can specify their stylistic preferences by selecting combination of these tags. The agent then filters the library to present only those templates that match the chosen attributes, allowing the user to select the final design. Once template is chosen, the system integrates the content modules into its structure and incorporates interactive features. This complete package is then passed to the HTML Generator, which renders the final web artifacts including the HTML, CSS, and JavaScript files. The process also concludes with crucial verification and customization stage. First, an automated HTML Checker inspects the rendered page for layout and visual integrity, flagging potential issues like oversized images or color clashes. Following this, final Human-in-the-Loop mechanism is enabled. Authors can provide direct language 4 commands (e.g., \"add the navigation bar\", \"adjust the table colors to match the theme\") to fine-tune the webpage styles, ensuring the webpages visual presentation is polished and precise. It is worth noting that all the aforementioned human-in-the-loop interactions are optional, as the system can operate in fully autonomous mode. For instance, template can be arbitrarily specified or randomly selected by the system without requiring any author intervention. We provide this interactive functionality primarily to enhance user control and flexibility, acknowledging that no agentbased system can be infallible. This optional oversight allows authors to make final corrections and ensure the output perfectly aligns with their vision."
        },
        {
            "title": "4 PageBench",
            "content": "4.1 Dataset Curation Data Source. The dataset of PageBench is curated from project pages associated with papers from three top-tier AI conferences, including NeurIPS, ICML, and ICLR, spanning the years 2023 to 2025. Our curation process involved collecting over 1,500 project pages, followed by meticulous manual filtering to ensure each entry was valid project homepage. The resulting benchmark is curated corpus rich with multimodal content, including text, figures, and interactive demos, intended to facilitate the development and evaluation of automated project page generation systems. Test Set and Template Library Construction. To ensure diversity and representativeness, we employed two-stage sampling strategy to construct test set and template library. First, to build the test set, we extracted structural and stylistic features from our entire corpus, then applied dimensionality reduction and clustering to group similar pages. By sampling from these clusters, we selected 100 pages that represent broad range of observed page archetypes, forming our primary test set. Second, to create the template library, we deduplicated this test set using multi-stage algorithm. This process yielded final, curated Template Library of 87 stylistically distinct pages. The detailed procedure of the sampling strategy for the test set and template library is described in Appendix D."
        },
        {
            "title": "4.2 Evaluation Metrics",
            "content": "To comprehensively evaluate the quality of the generated webpages, we designed suite of metrics that assess two primary dimensions: Content Quality and Visual Quality. These metrics collectively measure models proficiency in both accurately conveying information and delivering visually coherent and pleasing presentation. 4.2.1 Content Quality Readability. We assess the linguistic fluency and coherence of the generated text by computing its Perplexity (PPL) across all textual content on the webpage. PPL is standard metric that quantifies how well model predicts given text sequence, where lower score indicates that the text is more natural and predictable, thus signifying higher readability. See Appendix B.1 for more details. Semantic Fidelity. To ensure the generated content accurately reflects the source material, we evaluate Semantic Fidelity. This metric measures the semantic correspondence between each webpage section and its original paragraph from the source document. The score is derived by first aligning generated-to-source text pairs and then computing the cosine similarity of their vector embeddings. higher score indicates that the generated content faithfully preserves the meaning of the original text. See Appendix B.2 for more details. Compression-Aware Information Accuracy. Inspired by Pang et al. (2025), we introduce Compression-Aware Information Accuracy to evaluate factual preservation under content compression. This metric is evaluated using questionanswering (QA) pipeline where we first generate questions from the source paper and then use the generated webpages text to answer them. The final score synthesizes two dimensions: the accuracy of the answers (factual correctness) and the text compression ratio (conciseness). This approach rewards models that produce content that is not only factually accurate but also efficiently summarized. See Appendix B.3 for more details."
        },
        {
            "title": "4.2.2 Visual Quality",
            "content": "To evaluate the overall visual quality of the generated pages, we employ unified VLM-as-Judge framework. For each dimension, the VLM is prompted to act as specialized, strict reviewer, assigning score on five-point scale. The detailed prompts and scoring rubrics used for these evaluations are provided in Appendix H. Visual Content Accuracy. This metric assesses the correct presentation of critical visual elements on the page. The evaluation focuses strictly on two 5 Table 1: Main evaluation results across our full suite of PageBench. The best performance among all methods for each metric is in bold, and the second best is underlined. For ease of comparison, AutoPage and its corresponding proprietary base models are highlighted in matching colors. AutoPage improves both content and visual quality over different base models, validating its effectiveness in producing accurate, coherent, and visually refined webpages. Method System Type Open-Source GPT-OSS-120B llama-3.1-70B Grok4-fast GLM-4.5-Air E2E E2E E2E E2E Qwen3-235B-A22B AutoPage-Qwen GPT4o-mini AutoPage-GPT4o-mini E2E Multi-Agent E2E Multi-Agent Gemini2.5-Flash AutoPage-Gemini2.5-Flash Multi-Agent E2E Content Quality Visual Quality Readability Semantic Fidelity Comp.-Aware Info. Acc. Visual Content Accuracy Layout and Cohesion Aesthetic Score 9.665 15.967 14.101 10.568 11.590 10.425 10.047 10. 11.343 10.992 0.608 0.442 0.603 0.587 0.571 0.663 0.554 0.621 0.684 0.742 1.719 1.270 1.808 1. 1.890 1.837 1.786 1.941 1.276 1.591 2.74 2.52 2.68 2.98 2.52 3.01 2.96 3. 2.82 3.13 1.82 1.78 2.01 2.03 1.93 2.28 2.08 2.38 2.00 2.15 2.65 2.41 2.67 2. 2.46 2.72 2.71 2.95 2.48 2.69 objective criteria: the correct rendering of mathematical formulas and the contextual relevance of images to their surrounding text. Layout and Cohesion. This metric evaluates the structural integrity and visual balance of the page. The assessment focuses on identifying flaws that disrupt the visual flow and professional appearance, such as disproportionately scaled images and the presence of excessive or poorly distributed white space, aiming to reward designs that offer coherent and comfortable reading experience. Aesthetic Score. This dimension quantifies the pages holistic visual appeal by assessing its overall aesthetic feel. The evaluation focuses on design harmony, including the coherence of the color scheme, style consistency, and clarity of the visual hierarchy. Distinct from content-focused metrics, this score reflects the pages overall visual impression and professional polish."
        },
        {
            "title": "5.1 Experiment Setup",
            "content": "We assess the efficacy of the proposed AutoPage by benchmarking it against diverse set of advanced LLMs, and then ablating the influence of each component. It is important to note that for the purpose of scalable evaluation, all subsequent experiments were conducted using AutoPage in an exclusively automated fashion, without human intervention. The human-in-the-loop configuration, while potentially beneficial, is infeasible for batch testing. Therefore, the performance metrics reported here should be interpreted as conservative lower bound of our systems full potential. Baselines. The compared baseline models can be categorized into: Closed-Source Models in- (Achiam et al., 2023), cluding GPT-4o-mini Grok-4-fast (xGr), and GLM-4-Air (GLM et al., 2024). Open-Source Models including Qwen3235B (Yang et al., 2025), GPT-OSS-120B (OpenAI, 2025), and Llama-3.1-70B (Meta AI, 2024). Avoiding Information Leakage in Evaluation. To accurately evaluate each models ability to automatically generate project webpages, we designed an evaluation protocol to prevent key form of information leakage. The potential issue is that model might copy content directly from the provided webpage template, rather than synthesizing it from the source papers content. Such behavior would not reflect true generative understanding. Therefore, to ensure fair assessment, we decouple the papers content from its original webpage layout. In our setup, each model must generate webpage for given paper using template derived from completely different papers project website. This cross-pairing strategy forces the model to rely on the source document to generate content, providing more rigorous test of its capabilities."
        },
        {
            "title": "5.2 Main Results",
            "content": "AutoPage enhances end-to-end methods. key finding from our experiments is that AutoPage acts as powerful enhancer for existing end-toend methods, significantly elevating both their content and visual generation quality. As detailed in Tab. 1, we observed consistent and substantial performance gains compared to their respective endto-end baselines. For instance, when paired with GPT4o-mini, AutoPage-GPT4o-mini surpasses the baseline GPT4o-mini across all evaluated metrics. Notably, it boosts the Aesthetic Score from 2.71 to 2.95 and improves Layout and Cohesion from 2.08 to 2.38, demonstrating its superior ca6 pability in generating visually appealing and wellstructured webpages. similar trend is observed with AutoPage-Gemini-2.5-Flash, which achieves higher Semantic Fidelity (0.742 v.s. 0.684) and Visual Content Accuracy (3.13 v.s. 2.82) compared to the baseline Gemini-2.5-Flash. Furthermore, it shows dramatic improvement in CompressionAware Information Accuracy, jumping from 1.276 to 1.941, which is the highest score achieved for this metric across all tested methods. This pattern also holds for the open-source model, where AutoPage-Qwen shows marked improvements over the end-to-end Qwen baseline, particularly in all three Visual Quality metrics. These results collectively validate that AutoPage is an effective and versatile framework that can augment various large models, systematically enhancing their ability to produce higher-quality webpages. AutoPage Narrows the Performance Gap Across Backbones An interesting finding is AutoPages ability to narrow the performance gap between backbone models of varying capabilities. As shown in Tab. 1, significant performance gap initially exists between the high-performing Gemini-2.5-Flash and the weaker open-source Qwen, e.g., in Visual Content Accuracy (2.82 v.s. 2.52). However, AutoPage acts as great equalizer. While it enhances both models, its impact is far more transformative for the weaker backbone. Specifically, AutoPage boosts Qwens Visual Content Accuracy score by 0.49 (from 2.52 to 3.01), gain substantially larger than the 0.31 seen for Gemini-2.5-Flash. This disproportionate improvement slashes the initial performance gap by more than half (0.30 to 0.12). This demonstrates that AutoPage is not just an incremental add-on for strong models, but transformative component that elevates weaker backbones to competitive state Beyond Content: AutoPage as Visual Architect. Beyond ensuring high content fidelity, AutoPage demonstrates exceptional strength as \"visual architect,\" skillfully enhancing the aesthetic and structural quality of the generated pages. This layered capability is evident on the Qwen model. The framework secures respectable 16% gain in Semantic Fidelity (from 0.571 to 0.663), and in addition to this, delivers powerful leap in visual metrics. Specifically, Visual Content Accuracy soars by nearly 20% (from 2.52 to 3.01). This mastery of the visual dimension is not just quantitative improvement but translates directly into superior Figure 3: Human preference study. The bar chart shows that AutoPage attains the highest user preference score, surpassing all baselines with more informative, coherent, and visually engaging webpages. user experience. This conclusion is decisively supported by our user study (Sec. 5.3), as shown in Fig. 3, our method achieved the highest average score (7.16 out of 10), confirming that its superior visual architecture results in product that is tangibly more appealing and usable to humans."
        },
        {
            "title": "5.3 User Study",
            "content": "To evaluate the human-perceived quality of the generated webpages, we conducted user study with 20 participants. For each source paper, participants were shown group of 8 webpages generated by the different models. key aspect of our methodology was forced-choice rating mechanism. To elicit fine-grained distinctions, participants were required to assign unique score from 1 (Completely Unusable) to 10 (Perfect) to each of the 8 pages within group. This constraint effectively compelled them to create relative ranking, preventing score clustering and providing clearer signal of preference. Further details on the study protocol and the complete scoring rubric are available in Appendix E. Our user study demonstrates that webpages generated by AutoPage are preferred by human evaluators. As shown in Fig. 3, AutoPage achieved the highest average score of 7.16, establishing clear performance hierarchy. It not only leads but also maintains discernible advantage over other strong models like Grok4-fast (6.93) and Gemini2.5-Flash (6.79). Furthermore, the substantial gap between our model and the lowerscoring models(e.g. GPT4o-mini (3.97)) underscores the difficulty of the task and validates AutoPages superior ability to produce webpages that better align with human expectations. Notably, this study did not incorporate human-in-the-loop feedback. We believe introducing this could further im7 Figure 4: Qualitative comparison illustrating AutoPages superior generation quality over baselines. The figure highlights four common scenarios where AutoPage demonstrates superior performance: (a) Formula Presentation; (b) Image Layout; (c) Table Presentation; (d) Content Planning. This qualitative comparison demonstrates AutoPages ability not just to fill page with content, but to thoughtfully design it. prove PageAgents scores by more directly aligning the generated webpages with human preferences."
        },
        {
            "title": "5.4 Qualitive Study",
            "content": "As shown in Fig. 4, we present several cases that highlight the qualitative leap AutoPage provides over standard end-to-end methods. For instance, baseline end-to-end methods frequently fail to correctly render mathematical formulas and resort to inefficient, vertically stacked layouts for image galleries, as shown in Fig. 4a and Fig. 4b. Beyond ensuring the correct presentation of intricate elements like formulas, AutoPage also applies keen design sense. It organizes images into structured galleries and styles components like tables to match the pages theme, shown in Fig. 4c, avoiding the discordant look produced by the baseline. Furthermore, AutoPage demonstrates crucial strategic capability absent in baselines: content planning, shown in Fig. 4d. It analyzes the documents substance and enriches it by generating relevant visualizations, transforming dense text into an engaging and easy-to-digest format."
        },
        {
            "title": "6 Discussion",
            "content": "Efficiency and cost-effectiveness analysis. critical aspect of AutoPage is its exceptional efficiency in time and cost. Our experiments show that generating complete, high-quality page costs between just $0.06 and $0.20, with turnaround times ranging from 4 to 20 minutes depending on the chosen model. This demonstrates clear and valuable trade-off, allowing users to balance their specific needs for speed versus budget. Rethinking the readability scores. While some baselines appear to excel in readability scores, our analysis of Tab. 1 reveals that this widely used metric can sometimes be misleading. We observe the primary failure modes: sacrificing accuracy for fluency, as seen with GPT-4o-mini, which pairs low PPL (10.047) with poor Semantic Fidelity (0.554), just better than llama-3.1-80B (0.442). We argue that lower PPL, when coupled with inferior performance in semantic fidelity and accuracy metrics, is not an indicator of more sophisticated, truly greater generation process, underscoring the need for holistic evaluation beyond just readability."
        },
        {
            "title": "7 Conclusion",
            "content": "We presented AutoPage, multi-agent framework that transforms academic papers into interactive project webpages. Together with PageBench, the first benchmark for this task, we enable principled evaluation across fidelity, compression, and aes8 thetics. Experiments show that AutoPage produces coherent, dynamic, and accessible webpages, lowering the cost of scientific communication. We hope this work provides foundation for future systems that further expand the accessibility and impact of research. References Grok 4 xAI x.ai. https://x.ai/news/grok-4. [Accessed 15-10-2025]. Mistral Small 3.1 Mistral AI mistral.ai. https: [Ac- //mistral.ai/news/mistral-small-3-1. cessed 15-10-2025]. Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, and 1 others. 2024. Phi-4 technical report. arXiv preprint arXiv:2412.08905. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Harrison Chase. 2022. LangChain. Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023. Improving language model negotiation with self-play and in-context learning from ai feedback. arXiv preprint arXiv:2305.10142. Jiaxin Ge, Zora Zhiruo Wang, Xuhui Zhou, Yi-Hao Peng, Sanjay Subramanian, Qinyue Tan, Maarten Sap, Alane Suhr, Daniel Fried, Graham Neubig, and 1 others. 2025. Autopresent: Designing structured visuals from scratch. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 29022911. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, and 37 others. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. Preprint, arXiv:2406.12793. Yue Hu and Xiaojun Wan. 2013. Ppsgen: Learning to generate presentation slides for academic papers. In IJCAI, pages 20992105. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024. Understanding the planning of llm agents: survey. arXiv preprint arXiv:2402.02716. Keshav Kumar and Ravindranath Chowdary. 2024. Slidespawn: An automatic slides generation sysarXiv preprint tem for research publications. arXiv:2411.17719. Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, and Minjoon Seo. 2024. Prometheusvision: Vision-language model as judge for finegrained evaluation. Preprint, arXiv:2401.06591. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025a. Search-o1: Agentic searchenhanced large reasoning models. arXiv preprint arXiv:2501.05366. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. 2025b. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023a. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916. Jingwei Liu, Ling Yang, Hao Luo, Fan Wang Hongyan Preacher: arXiv preprint Li, Paper-to-video agentic system. arXiv:2508.09632. and Mengdi Wang. 2025. Meta AI. 2024. The llama 3.1 herd of models. Preprint, https://huggingface.co/ arXiv:2407.19033. meta-llama. OpenAI. 2025. gpt-oss-120b & gpt-oss-20b model card. Preprint, arXiv:2508.10925. Wei Pang, Kevin Qinghong Lin, Xiangru Jian, Xi He, and Philip Torr. 2025. Paper2poster: Towards multimodal poster automation from scientific papers. arXiv preprint arXiv:2505.21497. Kanya Paramita and Masayu Leylia Khodra. 2016. Tailored summary for automatic poster generator. In 2016 International Conference On Advanced Informatics: Concepts, Theory And Application (ICAICTA), pages 16. IEEE. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. 2025. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8):198343. Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. 2025. Paper2code: Automating code generation from scientific papers in machine learning. arXiv preprint arXiv:2504.17192. Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng, Xiuyi Chen, Zhumin Chen, Dawei Yin, Suzan Verberne, and Zhaochun Ren. 2025. Tool learning in the wild: Empowering language models as automatic tool agents. In Proceedings of the ACM on Web Conference 2025, pages 22222237. 9 Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. 2023. Adaplanner: Adaptive planning from feedback with language models. Advances in neural information processing systems, 36:58202 58245. Tao Sun, Enhao Pan, Zhengkai Yang, Kaixin Sui, Jiajun Shi, Xianfu Cheng, Tongliang Li, Wenhao Huang, Ge Zhang, Jian Yang, and 1 others. 2025. P2p: Automated paper-to-poster generation and fine-grained benchmark. arXiv preprint arXiv:2505.17104. Deep Search Team. 2024. Docling technical report. Technical report. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry OSullivan, and Hoang Nguyen. 2025. Multi-agent collaboration mecharXiv preprint anisms: survey of arXiv:2501.06322. llms. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, and Conghui He. 2024a. Mineru: An open-source solution for precise document content extraction. Preprint, arXiv:2409.18839. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, and 1 others. 2024. Autogen: Enabling next-gen llm applications via multiIn First Conference on Lanagent conversations. guage Modeling. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, and 1 others. 2025. The rise and potential of large language model based agents: survey. Science China Information Sciences, 68(2):121101. Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. 2024. Large multimodal agents: survey. arXiv preprint arXiv:2402.15116. Sheng Xu and Xiaojun Wan. 2022. Posterbot: system for generating posters of scientific papers with neural In Proceedings of the AAAI Conference models. on Artificial Intelligence, volume 36, pages 13233 13235. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Kaizhong Zhang and Dennis Shasha. 1989. Simple fast algorithms for the editing distance between trees and related problems. SIAM J. Comput., 18:12451262. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, and 1 others. 2023. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2). Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, and 1 others. 2024b. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345. Hao Zheng, Xinyan Guan, Hao Kong, Jia Zheng, Weixiang Zhou, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun. 2025. Pptagent: Generating and evaluating presentations beyond text-to-slides. arXiv preprint arXiv:2501.03936. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, and 1 others. 2024c. Autosurvey: Large language models can automatically write surveys. Advances in neural information processing systems, 37:115119115145. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. 2025. Cycleresearcher: Improving automated research via automated review. In The Thirteenth International Conference on Learning Representations. 10 Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Fine-tuned large language arXiv preprint 2023. Judgelm: models are scalable judges. arXiv:2310.17631. Zeyu Zhu, Kevin Qinghong Lin, and Mike Zheng Shou. 2025. Paper2video: Automatic video genarXiv preprint eration from scientific papers. arXiv:2510.05096."
        },
        {
            "title": "A Ablation on Verifiers",
            "content": "We evaluate the performance of AutoPages selfcorrection mechanism by conducting an ablation study on its verifiers: the full content checker and the HTML checker. The full content checker automatically verifies the consistency and relevance between the generated text and its accompanying visuals. Subsequently, the HTML checker inspects the final page for layout and visual integrity, flagging issues like oversized images or tables and colors that clash with the templates theme. In our ablation experiment, we systematically disable these components to create three distinct variants: (1) AutoPage w/o full content checker, (2) AutoPage w/o HTML checker, and (3) AutoPage w/o all checkers. The results, presented in Tab. 2, show notable performance degradation across all ablation settings, with the most significant drop observed when both verifiers are removed. For instance, when both verifiers are removed, the Visual Content Accuracy drops from 3.13 to 2.75, and the Aesthetic Score plummets from 2.69 to 1.90. Disabling only the HTML checker causes the Layout and Cohesion score to fall sharply from 2.15 to 1.65. Meanwhile, removing the full content checker leads to degradation in Semantic Fidelity from 0.739 to 0.695. This result confirms the indispensable value of each of our verifiers and establishes that the entire verifierdriven, multi-turn refinement process is the key to producing high-quality webpages. Table 2: Ablation study on different verifiers of AutoPage. w/o Verifier Metric AutoPage Full Content HTML All Content Quality Readability Semantic Fidelity Comp.-Aware Info. Acc. Visual Quality Visual Content Acc. Layout and Cohesion Aesthetic Score 10.090 0.739 1.744 3.13 2.15 2.69 10.745 0.695 1. 10.795 0.708 1.583 10.745 0.695 1.533 3.05 1.95 2.25 2.90 1.65 2.20 2.75 1.60 1."
        },
        {
            "title": "B Details of PageBench",
            "content": "B.1 Readability Perplexity (PPL) is standard metric in natural language processing used to evaluate the quality of probabilistic language models. In our work, it serves as proxy for the readability and linguistic naturalness of the generated webpage text. It is formally defined as the exponentiation of the cross-entropy loss. Given token sequence = (w1, w2, . . . , wN ), the Perplexity is calculated as: (cid:32) PPL(W ) = exp (cid:33) log p(wiw<i) (1) 1 (cid:88) i= where is the total number of tokens in the text sequence and p(wiw<i) is the conditional probability of the token wi given the preceding tokens w1, . . . , wi1. This probability is estimated by pre-trained language model, where our implementation uses the 01-ai/Yi-6B2 model to compute PPL. To compute perplexity for texts exceeding the pre-trained language models maximum context length, we employ sliding window approach. This method processes the text in overlapping segments, but crucially, calculates the loss only on the new, non-overlapping tokens in each step. This ensures that the conditional probability for each token across the entire document is evaluated exactly once, yielding single, coherent PPL score for texts of any length. Conceptually, PPL measures the average \"surprise\" or uncertainty of language model when processing given text. lower PPL score signifies that the model is less \"perplexed\" by the text, meaning the sequence of words is highly probable and predictable. This high predictability strongly correlates with text that is fluent, coherent, and natural-sounding to human reader. B.2 Semantic Fidelity The Semantic Fidelity score quantifies how well the meaning of the generated webpage content is preserved from the original source document. The calculation is multi-stage process designed to be robust and accurate. The first step is to accurately pair each generated section of the webpage with its corresponding paragraph in the source document. Given that the model may merge, split, or rephrase content, direct one-to-one mapping is not always possible. We employ rapid alignment model based on sentence embeddings to find the most semantically similar source paragraph for each generated section. This produces set of (generated section, source paragraph) pairs. For each aligned pair, we use pre-trained sentencetransformer model from huggingface3 to encode both the generated text and the source text into 2https://huggingface.co/01-ai/Yi-6B 3https://huggingface.co/sentence-transformers/allroberta-large-v 11 Table 3: Performance Comparison across Different Methods. The best performance among all methods for each metric is in bold, and the second best is underlined. For ease of comparison, AutoPage and its corresponding proprietary base models are highlighted in matching colors. The compression rate is also listed in the table. Raw-ACC Compression-Aware ACC Method System Type Open-Source GPT-OSS-120B llama-3.1-70B Grok-4-fast GLM-4.5-Air E2E E2E E2E E2E Qwen3-235B-A22B AutoPage-Qwen GPT-4o-mini AutoPage-GPT-4o-mini E2E Multi-Agent E2E Multi-Agent Gemini-2.5-flash AutoPage-Gemini-2.5-flash Multi-Agent E2E Detail question Understanding question OpenSource CloseSource D-Avg OpenSource CloseSource U-Avg 0.662 0.422 0.725 0.688 0.692 0. 0.635 0.635 0.735 0.715 0.617 0.218 0.711 0.619 0.547 0.635 0.374 0.398 0.723 0. 0.640 0.320 0.718 0.653 0.620 0.668 0.505 0.517 0.729 0.701 0.869 0.617 0.880 0.905 0.859 0. 0.838 0.899 0.869 0.882 0.844 0.380 0.890 0.850 0.794 0.833 0.606 0.662 0.901 0. 0.856 0.499 0.885 0.877 0.826 0.874 0.722 0.781 0.885 0.876 Compression Overall D-Avg U-Avg Overall 0.748 0.409 0.801 0.765 0.723 0.771 0.613 0.649 0.807 0.788 10.833 29.594 10.347 11. 15.931 11.592 20.045 23.528 5.302 8.419 1.469 0.992 1.617 1.526 1.659 1.593 1.472 1. 1.150 1.411 1.970 1.548 1.999 2.049 2.121 2.081 2.099 2.389 1.402 1.770 1.719 1.270 1.808 1. 1.890 1.837 1.786 1.941 1.276 1.591 high-dimensional vectors, denoted as Vg and Vs respectively. These dense vectors capture the semantic meaning of the text. We then compute the cosine similarity between the two vectors Vg and Vs. This value, ranging from -1 to 1, measures the cosine of the angle between them. value close to 1 indicates that the vectors point in almost the same direction, signifying high degree of semantic similarity. The formula is: Semantic Similarity(Vg, Vs) = Vg Vs VgVs (2) The final Semantic Fidelity score for the entire webpage is the average of the cosine similarity scores across all aligned section pairs. This provides single, comprehensive measure of how well the webpage preserves the semantics of the source document as whole. B.3 Compression-Aware Information"
        },
        {
            "title": "Accuracy",
            "content": "The Compression-Aware Information Accuracy metric is designed to jointly evaluate the factual accuracy and conciseness of the generated content. The calculation involves the following four steps: QA-Based Accuracy Measurement. We first automatically generate set of 100 question-answer pairs from the source document using powerful large language model GPT-o3. Similar to Paper2Poster (Pang et al., 2025), we select six powerful large language models to answer the questions above, including GPT-4o-mini (Achiam et al., 2023), gemini-2.5-flash (Team et al., 2023), grok-4fast (xGr), Phi4-14B (Abdin et al., 2024), Mistralsmall-3.1-24B (mis) and Qwen3-14B (Yang et al., 2025). Then, an answering model is tasked to answer these questions based solely on the textual content extracted from the generated webpage. The QA Accuracy, denoted as A, is the fraction of correctly answered questions. Text Compression Ratio. The Text Compression Ratio, C, measures how much shorter the generated text is compared to the original source text. It is defined as the ratio of the token counts: ="
        },
        {
            "title": "Tokensori\nTokensgen",
            "content": "(3) value of > 1 indicates compression. Final Score Calculation. To combine accuracy and compression into single score, Sfinal, we multiply the accuracy by the natural logarithm of the compression ratio. Using the logarithm rewards conciseness while dampening the effect of extreme compression. The formula is: Sfinal = ln(C) (4) This final, normalized score effectively and comparably rewards models that produce concise yet factually accurate content. B.4 VLM-as-Judge for Visual Quality"
        },
        {
            "title": "Evaluation",
            "content": "To quantitatively assess the visual quality of the generated pages, we employ Vision Language Model (VLM) as an automated judger. This approach, termed \"VLM-as-Judge\" (Pang et al., 2025; Sun et al., 2025; Liu et al., 2023a,b; Zhu et al., 2023; Lee et al., 2024), allows for consistent and scalable evaluation of the key visual elements presented in the documents, focusing specifically on their correctness and relevance rather than on subjective aesthetic appeal. The VLM is guided by carefully designed system prompt, instructing it to act as an extremely strict visual elements reviewer. Detailed prompt templates for visual quality evaluation as listed in Appendix H."
        },
        {
            "title": "C Detailed Analysis of",
            "content": "Compression-Aware QA Performance comprehensive breakdown of the Question Answering (QA) results is provided in Tab. 3. We report performance using two primary metrics: Raw Accuracy and Compression-Aware Accuracy, the latter of which is modulated by the compression ratio, as described in Appendix B.3. The results are further disaggregated by question type. Our analysis yields several key observations: (i) AutoPage variants significantly enhance their base models performance. For instance, AutoPageGPT-4o-mini elevates its base models score from 1.786 to 1.941, while AutoPage-Gemini-2.5-flash improves its score from 1.276 to 1.591, validating our structured compression method. (ii) While leading end-to-end models excel in raw accuracy, AutoPage demonstrates distinct advantage by achieving much higher compression rates while maintaining comparable accuracy. This superiority is reflected in the Compression-Aware ACC metric, where AutoPage-GPT-4o-mini attains the highest overall score of 1.941, underscoring the efficacy of our multi-agent approach in efficient information distillation. In summary, AutoPage proves its value by achieving significantly higher compression rates than E2E models while delivering comparable raw accuracy. This effective balance sets new benchmark in compression-aware evaluations."
        },
        {
            "title": "Library Construction",
            "content": "To construct diverse and representative test set, we moved beyond random sampling. We implemented two-stage diversity sampling strategy, designed to produce two distinct assets: diverse Test Set and stylistically unique Template Library. First, to create the test set, we performed feature extraction on the entire corpus to capture their structural and stylistic properties. We then applied dimensionality reduction and clustering techniques to group pages with similar layouts. By sampling from the resulting clusters, we selected approximately 100 pages that represent broad range of the page archetypes found in the wild. This collection serves as our primary test set for evaluation. Second, to build library of unique design patterns for template matching tasks, we further refined this test set. We applied multi-stage deduplication algorithm to the 100 candidate pages. This algorithm combines rapid SimHash-based filtering with preFigure 5: Comprehensive Evaluation of Model Performance with and without AutoPage. The radar plot shows that integrating AutoPage consistently boosts both content and visual quality, thereby demonstrating its strong advantage in generating more accurate, coherent, and visually appealing webpages. cise Zhang-Shasha (Zhang and Shasha, 1989) tree edit distance computation on standardized DOM structures to identify and remove pages originating from the same template. The most structurally complex page from each identified group was chosen as the representative. This filtering step yielded final, curated collection of 87 stylistically distinct pages, which constitutes our Template Library."
        },
        {
            "title": "E User Study Protocol and Materials",
            "content": "This section provides the detailed protocol, recruitment materials, and scoring rubric used in our user study, as referenced in the main paper. Participants and Recruitment. We recruited 20 undergraduate and graduate students to act as expert evaluators. Participants were informed that the task would take approximately 2 hours and would be compensated upon successful completion. Each participant was compensated accordingly for their contribution. Task Procedure and Instructions. Participants were given detailed guide explaining their task. For each set, they were instructed to evaluate 8 webpages generated by different AI models and our system from the same source paper. The instructions guided them to first quickly browse all 8 pages to form general impression and preliminary mental ranking. Following this, they were to begin the scoring process by assigning high score 13 (e.g., 10 or 9) to the best page and low score (e.g., 1 or 2) to the worst, establishing anchors for their judgment. Finally, they would assign the remaining pages unique, unused scores based on their quality relative to these anchors and to each other. Once all 8 pages in group were scored, the results were submitted, and the system would present the next group. Forced-Choice Scoring Rubric. The core of our study was the forced-choice rating system. Participants were required to use unique integer score from 1 to 10 for each page within single group of 8. The detailed rubric provided to them was as follows: 10 (Perfect): Professional, flawless design and content presentation. \"gold standard\" page. 9 (Excellent): Near-perfect, with only minuscule flaws discoverable upon close inspection. 8 (Good): High-quality page with complete functionality, but perhaps lacking in design flair. 7 (Decent): Generally usable but with minor, noticeable issues like misalignment or incorrect rendering of non-critical content. 6 (Fair/Average): Inconsistencies or minor clutter in layout and content, but does not impede usability. 5 (Marginally Usable): Noticeable design issues (e.g., slight element overlap) that begin to affect the reading experience. 4 (Poor): Chaotic layout with significant content rendering failures, severely hindering comprehension of core content. 3 (Very Poor): The layout is mostly broken (akin to failed CSS), making most content unreadable. 2 (Broken): The page is fundamentally broken, with only scattered, isolated pieces of content being recognizable. 1 (Completely Unusable): blank page, server/browser error (e.g., 404), or complete gibberish. The page has zero value."
        },
        {
            "title": "F Additional Comparision Resuls",
            "content": "In this section, we present additional visual results, shown in Fig. 6, Fig. 7, and Fig. 8, to further demonstrate the superiority of AutoPage over the baseline method. As shown in Fig. 6, the baseline method struggles with complex page elements, failing to render mathematical formulas and disrupting the layout of images and tables. In contrast, AutoPage accurately renders all components, preserving the pages intended structure. Fig. 7 highlights the difference in visual fidelity. The baselines improper image scaling leads to catastrophic visual presentation. AutoPage, however, not only resizes images appropriately but also adapts table styles to the pages theme, creating cohesive and aesthetically pleasing result. Finally, Fig. 8 reveals more fundamental failure of the baseline: content loss. The baselinegenerated page fails to display content within entire sections, rendering it incomplete. AutoPage, conversely, ensures content integrity by correctly displaying all textual and visual elements. Collectively, these examples underscore AutoPages robustness in handling diverse web content, consistently producing high-fidelity results where the baseline fails. Effectiveness of Human-in-the-loop"
        },
        {
            "title": "Feeback",
            "content": "In this section, we demonstrate the effectiveness of human feedback in AutoPage, shown in Fig. 9, Fig. 10, Fig. 11, Fig. 12. As demonstrated in Fig. 9 and Fig. 10, the initial automated generation can produce suboptimal layouts, such as those with disproportionately large images that disrupt the page structure. With guidance from human feedback, AutoPage effectively corrects these scaling issues to restore balanced and visually appropriate layout. Fig. 11 highlights different type of layout refinement, where human intervention resolves excessive vertical whitespace between content modules, leading to more compact and visually coherent page. Beyond layout adjustments, Fig. 12 illustrates how the feedback mechanism can rectify content-level errors by identifying and removing erroneous assets, such as an incorrect logo in the header. Collectively, these examples confirm the versatility and precision of the human-in-the-loop process, enabling fine-grained corrections that range from 14 Figure 6: Visual comparison of baseline and AutoPage. The webpage generated by the baseline (left) exhibits rendering failures, including an inability to display the formula and distorted layout for images and tables. Conversely, the page generated by AutoPage (right) renders the formula correctly and preserves the intended layout of all visual elements. layout spacing and image scaling to content validation, thereby significantly enhancing the final output quality."
        },
        {
            "title": "H Prompt Templates",
            "content": "In this section, we present the prompt templates we used for each component in AutoPage. 15 Figure 7: Additional Visual comparison of baseline and AutoPage. The baseline-generated page (left) suffers from improper image scaling, leading to catastrophic visual presentation. In contrast, AutoPage (right) styles the tables to match the pages theme and displays the image at an optimal size. Figure 8: Additional Visual comparison of baseline and AutoPage. The webpage generated by the baseline method (left) fails to render the contents within their respective sections. Conversely, the page generated by AutoPage (right) successfully displays both the textual and visual elements in their correct layout. 16 Figure 9: Correcting Page Layout with Human Feedback. The baseline generation (left) results in poor layout with an oversized image. By integrating human feedback, AutoPage (right) produces corrected layout with properly-sized image. Figure 10: Impact of Human Feedback on Visual Layout. The initial page generated without human feedback (left) suffers from flawed layout, featuring disproportionately large image. In contrast, after incorporating human feedback, AutoPage (right) corrects the layout and renders the image at an appropriate size. 17 Figure 11: Impact of Human Feedback on Vertical Layout Spacing. The initial page generated by AutoPage without human feedback (left) exhibits excessive vertical whitespace between content modules, resulting in sparse and poorly structured layout. In contrast, after incorporating human feedback, the system (right) corrects the spacing to produce more compact and visually coherent page. Figure 12: Human-in-the-Loop Correction for Page Assets. This figure demonstrates how human feedback is used to refine UI components. The initial output (left) features an incorrect or broken logo image in the header. After processing the feedback, AutoPage generates corrected version (right) where the erroneous asset has been removed. 18 ı Prompt: Filter Figures System Prompt You are helpful academic expert. You need to determine which section of the paper each image and table in the figures belongs to from given research papers contents and figures. Template Description Below is the figures with descriptions, paths, width and height in the paper: <figures> {{figures}} </figures> have already generated the text-based project page content as follows: <project_page_content> {{project_page_content}} </project_page_content> The paper content is as follows: <paper_content> {{paper_content}} </paper_content>"
        },
        {
            "title": "Tasks",
            "content": "1. Determine which section of the article each image and table in the figures belongs to, and then add field called original_section to every figure in the original figures, filling it with the determined section. If figure does not appear in the paper content, then original_section should be set to null. Your output should be json format. 2. Extract figure and table tags from figure or table captions. Key of these tags is tag. 3. Remove the extracted tag from caption of each figure. Output Format ```json <Your output> ``` ı Prompt: Section Generation System Prompt You are an expert content planner specializing in creating engaging project pages for research papers. Your role is to analyze research content and plan an effective structure that communicates the research clearly and professionally. You will be given: 1. Research Paper in markdown format. 2. List of images extracted from the paper. 3. List of tables extracted from the paper. Your goal is to create section plan that organizes the research into an effective project page structure. Template Description Please analyze the paper content and identify the key sections that should be included in the project page. For each section, provide concise description of what should be included. First, determine the 19 paper type: For methodology research papers: Focus on method description, experimental results, and research methodology. For benchmark papers: Highlight task definitions, dataset construction, and evaluation outcomes. For survey/review papers: Emphasize field significance, key developmental milestones, critical theories/techniques, current challenges, and emerging trends. Note that the specific section names should be derived from the papers content. Related sections can be combined to avoid fragmentation. Limit the total number of sections to maintain clarity. You must include some section that describe the methodology and experiments. Do not include acknowledgements or references sections. Do not include related work and experiment setting sections. The number of sections you design must not exceed five. Return the result as flat JSON object with section names as keys and descriptions as values, without nested structures. You MUST use Markdown code block syntax with the json language specifier. Paper Content: {{paper_content}} Output Format ```json <Your generated section json content> ``` ı Prompt: Text Content Generation System Prompt You are helpful academic expert and web developer, who is specialized in generating text-based paper project page, from given contents. Template Description You will be given the research papers paper markdown content, figures, and section plan that describe what content should be included in each section of the project page. Your task is to fill in the actual content for each section based on the requirements outlined in the section plan and the content of the research paper. In the project page, you should introduce it from the authors perspective rather than from third-party viewpoint. This content will ultimately be displayed on the project page. The content you generated must include all key components of the paper. Below is the image and table figures with descriptions and paths in the paper: <figures> {{figures}} </figures> Below is the content of the paper: <paper_content> {{paper_content}} </paper_content> Section Plan: 20 {{format_instructions}} If figures can effectively convey the poster content, simplify the related text to avoid redundancy. Include essential mathematical formulas where they enhance understanding. Dont leave any important content in the research paper. If the paper content has conclusion section, this section should not contain any figures. Do not include any tag of figure or table in the text Your output must be in JSON format, and the section names in your output must exactly match those in the section plan. Output Format ```json <Your output> Ensure all sections are precise, concise. ı Prompt: Full Content Generation System Prompt You are helpful academic expert and web developer, who is specialized in generating paper project page, from given research papers contents and figures. Template Description Below is the figures with descriptions, paths, width and height in the paper: <figures> {{figures}} </figures> have already generated the text-based project page content as follows: <project_page_content> {{project_page_content}} </project_page_content> The paper content is as follows: <paper_content> {{paper_content}} </paper_content> Task Requirements Your task is inserting figures into the project page content using figure index notation as: ![figure_description][figure_path][width=figure_width, height=figure_height] (figure_index) For example: ![Overview][\"assets/paper-picture-8.png\"][width=1068, height=128](8)"
        },
        {
            "title": "Specific Requirements",
            "content": "1. The figure_index MUST be an integer starting from 1, and no other text should be used in the figure_index position. 2. Each figure should be used at most once, with precise and accurate placement. 3. Prioritize pictures and tables based on their relevance and importance to the content. 4. The teaser figure that appears early in the paper must be included in the content. 5. Dont leave any important figure in the research paper. 6. If chapter has multiple tables, only the one most relevant to the chapter should be included. 7. Your output must be in JSON format, and the section names in your output must exactly match those in the project_page_content. 8. Please ensure that the images you insert are closely related to the context and align well with the content of the section. Output Format ```json <Your output> ``` ı Prompt: HTML Generation System Prompt You are an expert web developer specializing in creating professional project pages for research papers. You have extensive experience in HTML5, CSS3, responsive design, and academic content presentation. Your goal is to create complete, production-ready HTML project page that is visually appealing and professional. Template Description Instructions: Generate complete, production-ready HTML project page based on the provided project page content and html template. Project Page Content: {{ generated_content }} HTML Template: {{ html_template }}"
        },
        {
            "title": "Requirements",
            "content": "1. The HTML files you generate should follow the format and style of the reference HTML template as closely as possible, but you can replace the content in the reference HTML template. 2. The content of your project page should be filled completely with the project page content, without any omissions. 3. All content sections in Project Page Content should be properly formatted in your html file. 4. Images and tables in Project Page Content should be integrated into your html using the correct image path or table path. 5. You should make sure that paths of css files included in the html file should not be changed. 6. If there is formula in the generated content, please add the relevant js code such as: <script> window.MathJax = { tex: { inlineMath: [['$', '$'], ['(', ')']], displayMath: [['$$', '$$'], ['[', ']']] } }; </script> <script src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"> </script> 7. For images and tables, only the caption needs to be retained and the tag before the caption needs to be deleted (e.g. Figure 1., Table 2.). 8. Do not place the table in small container. If the table is large, place it in larger container. 9. The image (not table) should be placed in the container to limit its size. 10. Formulas should not be placed in separate paragraphs. 11. Be careful not to let the image overflow the screen, the screen width is generally 1280. You can add container that is the same width as the screen to limit. Layout Specification For example, if there are two images in two columns whose aspect ratios are 1.2 and 2 respectively, the flex grow of two columns should be 1.2 and 2 respectively, to make the columns have the same height. Calculate the size of each image based on column width and aspect ratios. Add comment <! width = display_width, height = display_height > before each image. Rearrange the structure and order of sections, texts and images to make the height of each column in the same group approximately the same. For example, if there are too many images in one section that make the height of the column too large, group the images into columns. The display width of each image should not be too large or too small compared to its original width. In each section, if an image or table is placed in single column layout, it should be horizontally centered within that column or content block (not the full page). Use CSS techniques such as display: block; margin: auto; to achieve proper visual centering. There should be certain amount of spacing between adjacent images. All sections on the entire project page must be in single column and span the full width of the page. Formulas do not need to be opened in separate paragraph such as section-text block (such as <p class=\"section-text\" style=\"text-align: center;\">). Output Format ```html <html_content> ``` 23 ı Prompt: Full Content Review System Prompt You are an expert reviewer for scientific project pages. Your task is to carefully review the generated content by comparing it with the original paper content and figures. You will be given three inputs: 1. paper_content: the original scientific content of the paper. 2. figures: the list of figures (including captions, tag, intended placement, and meaning). 3. generated_content: the project page content automatically generated by another agent. You cannot violate these basic rules below for the project page when generating suggestions. 1. Number of tables in the whole page must be less than or equal to 3. 2. Any figure or table can just appear once in the content. 3. Include at least one table in experiment and ablation section if these two are included in generated sections. 4. Include at least one image in visualization section if it is included in generated sections. Remember that you should just restrict number of tables under 4, rather than restrict the total number of visual elements in the whole content. You can know if visual element is table by its tag. You should first get the number of tables and number of figures respectively in the content and then tell if the number of tables is more than 3. Your review must focus on the following dimensions: 1. Figure Placement and Usage Verify whether figures are inserted in the correct sections according to their meaning in the paper. For each section, you should check whether the text content and captions of figures and tables it includes is tightly related. Check if two figures convey similar idea. If it is, you should remain the more important figure. 2. Relation between figures and text Check if the core idea that figure shows is mentioned in text of its section. If the correlation between them is weak, please suggest to remove the figure or move it to other section. 3. Number of tables You should tell whether the number of tables is more than 2. If it is, you should choose 2 most important table to remain. Do not restrict the number of figures. Below is the figures with captions, paths, width and height in the paper: <figures> {{figures}} </figures> 24 Below is the tables with captions, paths, width and height in the paper: <tables> {{tables}} </tables> The paper content is as follows: <paper_content> {{paper_content}} </paper_content> The generated project page content is as follows: <project_page_content> {{generated_content}} </project_page_content> Requirements 1. Do not suggest adding or deleting entire sections. 2. The generated project page content should present the more important parts of the paper content in concise manner, so your review should not require including too many unimportant details. 3. Remember that the original section of image is not necessary to be same as the section it belongs to in the page. Do not correlate the two sections together. 4. Do not give suggestions of including figure Captions, because they will be included during the generation of html, not full content. 5. Do not give suggestion to change any text content in any section, you can just suggest to add or delete or move figures and tables. 6. Tables and Figures from Ablation section in the paper content should belong to Experiment section in the generated content if Ablation is not included in generated sections. Output Format You must return your review in strict JSON format with the following fields: Output Format You must return your review in strict JSON format with the following fields: { \"weakness\": [ \"weakness_1\", \"weakness_2\" ], \"strength\": [ \"strength_1\", \"strength_2\" ], \"suggestion\": [ \"suggestion_1\", \"suggestion_2\" ] } 25 ı Prompt: Full Content Revise System Prompt Please revise the previously generated project page content according to the review below: <review_content> {{review_content}} </review_content> Instructions 1. Carefully read the weakness, strength, and suggestion fields in the review JSON. 2. Improve the previously generated content by: Fixing weaknesses Preserving strengths Applying suggestions directly and concretely 3. Ensure the revised content is: Accurate (aligned with the original intent of the paper and figures). Clear and fluent (scientifically precise, grammatically correct, and concise). Well-structured (logical flow, correct figure placement). 4. Please do not add or remove any sections. 5. Do not change the name of any section in the page content. 6. Do not include two identical figures in the page content. 7. Do not change any text content. Output Format ```json <Your output> ``` ı Prompt: HTML Review System Prompt You are professional reviewer specializing in images and figures on research project pages. Your task is to inspect all images individually and meticulously, considering only width, vertical spacing, and internal text size. Return summary report that aggregates feedback across all images. Evaluation Criteria 1. Width of images Judge strictly whether each image is too wide, too narrow, or within the ideal visual range (approximately 7090% of the main text block). Mark as weakness if: The image exceeds the main text block width. The image is clearly smaller than 65% of the text block width. Allowed adjustment: specify percentage of the original image width (e.g., shrink to 90% of original width). 26 Special rule for containers: Only if an image clearly overflows outside the viewport/screen (not just wider than text block), recommend adding container restriction. In all other cases, simply suggest proportional resizing by percentage. 2. Vertical spacing Check if the spacing above and below each image is visually balanced. Mark as weakness if one side is noticeably larger than the other. Allowed adjustment: specify exact pixel values (e.g., set both top and bottom margin to 24px). Spacing must be consistent with surrounding blocks to maintain overall page rhythm. 3. Text inside images Judge whether text inside the image is disproportionately larger than body text (1214px). Font inside images cannot be changed directly. Allowed adjustment: suggest resizing the whole image by specific percentage of its original width (e.g., shrink to 80% of original width) to bring internal text visually closer to body text size. 4. Strictness Rules Only consider the three aspects above: width, vertical spacing, internal text. Skip tables when checking internal text size. Provide precise, actionable suggestions using percentages for width adjustments and pixels for margins. Be conservative: report all issues, even if minor, with clear reference to the affected image. Container restriction is suggested only if the image overflows outside the viewport. Formulas should not be placed in separate paragraphs. Output Format Return single JSON object with three arrays that summarize all images: \"weaknesses\": list all weaknesses across all images, each item must indicate the image it comes from and the specific problem. \"strengths\": list all positive aspects across all images, each item must indicate the image it comes from. \"suggestions\": list all actionable suggestions for all images, using specific percentages for width adjustments and pixels for vertical spacing. Only include add container restriction if the image actually overflows the viewport. 27 ```json { \"weaknesses\": [ \"weakness_1\", \"weakness_2\" ], \"strengths\": [ \"strength_1\", \"strength_2\" ], \"suggestions\": [ \"suggestion_1\", \"suggestion_2\" ] } ı Prompt: HTML Revise System Prompt You are an expert web developer specializing in creating professional project pages for research papers. You have extensive experience in HTML5, CSS3, responsive design, and academic content presentation. Your goal is to produce complete, production-ready HTML project page that is visually appealing, professional, and adheres to specified constraints. Template Description Instructions: Generate refined, production-ready HTML project page based on the existing HTML and the provided suggestions. Existing HTML: {{ existing_html }} Suggestions: {{ suggestions }}"
        },
        {
            "title": "Requirements",
            "content": "1. Apply all the suggestions carefully to the existing HTML without omitting any improvements. 2. Preserve the original formatting, style, and constraints from the existing HTML, unless explicitly adjusted by the suggestions. 3. All content sections must remain properly formatted and intact; do not remove or lose any original content. 4. Images and tables should retain correct paths and aspect ratios; apply size adjustments or centering only if suggested. 5. Maintain responsive design, single-column full-width layout, and professional visual presentation. 6. Ensure proper spacing, alignment, symmetry, and column height balance as specified in the previous layout rules. 7. Comment <! width = display_width, height = display_height > before each image whose size is adjusted according to column width and aspect ratio. 28 8. Center images or tables horizontally within their column or content block where applicable. 9. All other previous layout constraints and formatting rules must be respected. 10. Modify the image size according to the suggestions, making sure it is centered and there should be certain amount of space between the images. Output Format ```html <html_content> ``` ı Prompt: Aesthetics Quality Judge System Prompt: You are an extremely strict visual aesthetics reviewer. Focus solely on the overall aesthetic feel of the page or project presentation, including color scheme, style consistency, visual hierarchy, text-image coordination, and overall visual impression. Do not consider the accuracy of formulas or specific content of text or images, only evaluate aesthetics and visual feel. Do not easily give high scores unless the overall aesthetics reach an extremely high level. Template: Scoring Description: Five-point scale: 1 point: The overall color scheme of the page or content is chaotic or conflicting, resulting in very poor visual experience. The style is inconsistent, with no sense of hierarchy among elements, and the overall impression is cluttered. Poor coordination between images, text, or charts, causing visual fatigue. 2 points The color scheme or style shows obvious inconsistencies, with some areas having weak aesthetic appeal. The hierarchy of elements is slightly chaotic, with unclear visual guidance. Text-image coordination is average, and the overall impression is slightly cluttered. 3 points The color scheme is generally reasonable but has minor conflicts or imbalances. The style is relatively unified, but some elements are slightly uncoordinated. Text-image coordination is good, but there is still room for improvement in overall aesthetics. 4 points The color scheme is comfortable, with consistent style and good overall visual aesthetics. The hierarchy of elements is clear, with reasonable visual guidance and high text-image coordination. The overall impression is comfortable, with strong visual appeal. 29 5 points Rarely used; reserved for publication-level visual aesthetics. Color, style, hierarchy, and layout are perfectly unified, with harmonious and beautiful overall visuals. Text-image coordination is exceptional, with natural visual rhythm, an excellent impression, and no flaws. - Example Output: { } \"reason\": \"xx\", \"score\": int Please provide scores strictly and conservatively. ı Prompt: Element Quality Judge System Prompt: You are an extremely strict visual elements reviewer. Focus solely on the presentation of formulas and images, as well as the relevance of images to paragraph content. Do not consider webpage layout, paragraph formatting, or overall design. Do not easily give high scores unless the visual elements fully meet the highest standards. Template: Scoring Description: Five-point scale: 1 point: Formulas are incompletely displayed or entirely missing. Images are almost irrelevant to paragraph content or entirely missing. Colors are hard to distinguish, affecting comprehension. 2 points Some formulas or images are displayed correctly, but others are missing or incorrect. Image labels or captions are unclear, with weak relevance to text. Colors or clarity have some issues, making comprehension difficult. 3 points Most areas have reasonable layout, but there are occasional issues with uneven white space or slightly oversized/undersized images. The page is generally visually balanced but has slight inconsistencies. Minor impact on reading or comprehension, but overall acceptable. 4 points The page layout is good, with well-proportioned visual elements. White space is reasonable, and image sizes are appropriate. 30 The overall visual experience is comfortable, with smooth information delivery. 5 points Rarely used; reserved for publication-level page layout. White space and image sizes are perfectly balanced, with natural visual rhythm. The page is visually harmonious, offering an excellent reading experience with no distractions. - Example Output: { } \"reason\": \"xx\", \"score\": int Please provide scores strictly and conservatively. ı Prompt: Layout Quality Judge System Prompt: You are an extremely strict webpage layout reviewer. Focus solely on the visual layout and typesetting experience of the page, without considering the clarity of formulas, images, or their relevance to text. Pay attention to issues such as white space, image size, and visual balance, particularly noting whether large areas of white space or oversized images disrupt the visual effect. Do not easily give high scores unless the layout is highly reasonable. Template: Scoring Description: Five-point scale: 1 point: The page layout is extremely chaotic or cluttered. Large areas of white space or oversized images disrupt the overall visual effect. The page is visually unbalanced, resulting in poor reading experience. 2 points Some areas have reasonable layout, but there are still noticeable large areas of white space or oversized images. The page lacks visual balance, with mediocre overall impression. Some element arrangements may hinder information acquisition. 3 points Most formulas and images are displayed correctly, but there are noticeable issues with clarity, style, or annotations. Some images have average relevance to paragraph content. Minor issues with labels or colors, but they do not significantly affect comprehension. 4 points Formulas are fully displayed, and images are clear and highly relevant to paragraph content. 31 Labels, legends, and colors are reasonable and aid comprehension. Style is relatively consistent, with overall good visual presentation. 5 points Rarely used; reserved for publication-level visual presentation. Formulas are perfectly displayed, and images are clear and highly relevant to text. Labels and legends are flawless, with unified colors and style, and impeccable visual presentation. - Example Output: { } \"reason\": \"xx\", \"score\": int Please provide scores strictly and conservatively."
        }
    ],
    "affiliations": [
        "AutoLab, SAI, Shanghai Jiao Tong University",
        "Shanghai AI Laboratory"
    ]
}