{
    "paper_title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence",
    "authors": [
        "Xiaopeng Lin",
        "Shijie Lian",
        "Bin Yu",
        "Ruoqi Yang",
        "Changti Wu",
        "Yuzhuo Miao",
        "Yurun Jin",
        "Yukun Shi",
        "Cong Huang",
        "Bojun Cheng",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 3 9 7 6 1 . 2 1 5 2 : r PhysBrain: Human Egocentric Data as Bridge from Vision Language Models to Physical Intelligence Xiaopeng Lin1,3,, Shijie Lian2,6,, Bin Yu2,5,, Ruoqi Yang3, Changti Wu2, Yuzhuo Miao2,5, Yurun Jin3, Yukun Shi3, Cong Huang3, Bojun Cheng1, Kai Chen2,3,4 1The Hong Kong University of Science and Technology (Guangzhou) 2Zhongguancun Academy 3Zhongguancun Institute of Artificial Intelligence 4DeepCybo 5Harbin Institute of Technology 6Huazhong University of Science and Technology Equal contribution, Corresponding author Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multilevel, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentricaware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9%), demonstrating effective transfer from human egocentric supervision to downstream robot control. Date: December 19, 2025 Code: https://zgc-embodyai.github.io/PhysBrain/ Correspondence: bocheng@hkust-gz.edu.cn, kaichen@zgci.ac.cn"
        },
        {
            "title": "1 Introduction",
            "content": "Vision-Language-Action (VLA) systems rely on reliable embodied brain that integrates scenario understanding and action generation. Recent multimodal systems (Hurst et al., 2024a; Bai et al., 2025a) show rapid gains in visual perception, spatial and video reasoning, and long context understanding. These advances provide rich open vocabulary recognition and semantic inference capabilities that can be transferred to action prediction, thereby enabling modern VLAs (Zitkovich et al., 2023; Kim et al., 2024; Bjorck et al., 2025; Black et al., 2024, 2025) to achieve strong performance across diverse manipulation tasks. These developments highlight that strong VLA performance is driven by an embodied brain that grounds executable planning and interaction decisions in the agents own perceptual stream. For future humanoid robots, this perceptual stream is expected to be predominantly first-person, since perception, planning, and action feasibility are fundamentally grounded in the agents own body and workspace (Grauman et al., 2022). This places stringent demands on multimodal models operating under egocentric settings. However, empirical results on egocentric benchmarks (Lin et al., 2022; Pramanick et al., 2023; Chen et al., 2024; Patel et al., 2025; Li et al., 2025a) indicate that current multimodal models still struggle with long-horizon understanding, planning, and reliability under egocentric videos. These deficits stem from challenges intrinsic to egocentric perception, including rapid viewpoint changes, frequent handobject occlusions, the absence of the actors full body, and the need for cross-frame inference of contact and object Figure 1 Human egocentric supervision improves first-person embodied brains and transfers to control. Left: EgoThink radar plot comparing egocentric VLM performance across six dimensions (Activity, Forecast, Localization, Object, Planning, Reasoning) for representative baselines. Right Top: \"Phys\" means that the VLM was supervised fine-tuning on our annotated first-person (egocentric) data (described in Sec. 3.1), both VST-7B and Qwen2.5-VL-7B achieve significantly better EgoThink performance, with the most pronounced gains on Planning. Right Bottom: when used as the VLM backbone in standard VLA fine-tuning pipeline, the same Phys-enhanced backbones yield substantially higher SimplerEnv success rates, indicating that better egocentric planning and interaction reasoning translate to improved downstream manipulation. state (He et al., 2025). Consequently, current performance bottlenecks are more likely due to insufficient egocentric embodied cognition, state tracking, and planning supervision, rather than limitations in model scale or single-frame recognition. These limitations raise fundamental scalability question: whether advancing VLA in head-mounted egocentric settings necessarily depends on extensive robot data, including robot egocentric supervision. Acquiring largescale and diverse robot manipulation data is widely acknowledged to be costly and difficult to scale, due to substantial hardware, labor, and safety constraints (Khazatsky et al., 2024). Even imitation learning relies on expensive human demonstrations, while existing large-scale robot data pipelines often require long collection cycles or sustained multi-institution collaboration (Brohan et al., 2022; Zitkovich et al., 2023; ONeill et al., 2024). As result, learning and aligning embodied brains primarily through such robot data fundamentally constrains the scalability and coverage of egocentric VLA systems. In contrast to costly and hard-to-scale robot data, human first-person videos provide naturally scalable source of egocentric supervision, covering diverse everyday behaviors and environments. This data modality offers observations closely aligned with real interaction distributions for learning embodied brains. Large-scale datasets such as Ego4D (Grauman et al., 2022), BuildAI (BuildAI, 2025), and EgoDex (Hoque et al., 2025) demonstrate that egocentric videos can capture long-horizon activities, humanobject interactions, and fine-grained manipulation dynamics at scale. An open question is how to leverage the latent planning structure and handobject interaction regularities in human egocentric videos as supervision to strengthen egocentric embodied brains without robot data, thereby improving the sample efficiency and generalization of VLA systems. Motivated by this observation, we develop scalable annotation and instruction pipeline that transforms 2 human egocentric videos into structured, multi-level first-person VQA supervision for embodied brain learning. Each VQA instance encodes complementary information across multiple levels, including planning decompositions, key states, interaction constraints, and temporal relations, providing supervision beyond static visual recognition. To directly assess the effectiveness of this supervision, we conduct controlled evaluation using EgoDex-derived VQA data alone, as shown in Fig.1. Embodied brains trained on top of different VLM backbones consistently outperform their corresponding base models when evaluated as embodied brains. Under this setting, the resulting models enable efficient few-shot adaptation on first-person VLA tasks and achieve performance comparable to, or exceeding, VLA systems trained with large-scale robot data, despite the absence of any robot-data pretraining. Building on this evidence, we train PhysBrain by scaling the supervision to mixture of Ego4D, BuildAI, and EgoDex, together with general-purpose visionlanguage data, to further strengthen egocentric planning and interaction reasoning while preserving general visionlanguage capability. This direction is complementary rather than replacement for robot data: robot egocentric supervision remains critical for physical grounding and can further raise the performance ceiling when combined with our approach. In summary, our contributions are as follows: We introduce scalable annotation and instruction pipeline, called Egocentric2Embodiment Translation Pipeline, which converts large-scale human egocentric videos from multiple scenarios into multi-level embodied supervision. We provide well-structured and validated egocentric VQA dataset E2E-3M that can effectively improve models first-person vision performance and generalization capability on VLA tasks. Extensive experiments have demonstrated that human egocentric videos provide effective supervision for learning embodied brains in egocentric settings, leading to improved generalization in VLA tasks. We find that human egocentric data is complementary to robot data and is significantly more scalable, offering promising basis for studying future scaling laws in first-person VLA."
        },
        {
            "title": "2.1 First Person Vision Language Model",
            "content": "Vison Language Models (VLMs) that excel on third-person content often degrade when the input shifts to egocentric imagery and video. Multiple lines of evidence point to persistent viewpoint domain gap and to missing egocentric cues such as hand manipulation, egomotion, and partial observability (He et al., 2025). EgoVLP (Lin et al., 2022) were among the first to document that third-person pretraining transfers poorly and that explicitly egocentric objectives are needed for first-person retrieval, recognition, and temporal grounding. EgoVLPv2 (Pramanick et al., 2023) further reports that fusing first-person video and language during pretraining is important for egocentric tasks. Beyond these early works, recent evaluations arrive at the same conclusion. EgoPlan-Bench (Chen et al., 2024) shows that mainstream multimodal models struggle with egocentric planning even when the scenes are household and the instructions are simple, and it analyzes typical failure modes such as viewpoint confusion and missing contact reasoning. Studies on QaEgo4D (Barmann and Waibel, 2022) and QaEgo4Dv2 (Patel et al., 2025) find that both proprietary and open source VLMs lag on long-horizon egocentric reasoning. EgoM2P (Li et al., 2025a) also emphasizes the structural gap between third-person and first-person streams and argues for egocentric priors during pretraining."
        },
        {
            "title": "2.2 Vision Language Action",
            "content": "Vision-Language-Action (VLA) models (Brohan et al., 2023; Zitkovich et al., 2023; Team et al., 2024) represent recent paradigm shift in robotic manipulation by unifying language understanding, visual perception, and motor control within single end-to-end framework. Building upon large-scale vision-language models, VLAs directly map high-dimensional visual observations and natural language instructions to low-level robot actions, enabling intuitive human-robot interaction and task execution. Early works such as RT-1 (Brohan et al., 2023) and RT-2 (Zitkovich et al., 2023) demonstrate that scaling robot data and leveraging pretrained 3 vision-language representations significantly improve manipulation performance across diverse tasks. Building upon these foundations, OpenVLA (Kim et al., 2024), π0 (Black et al., 2024; Pertsch et al., 2025; Black et al., 2025), and GR00T-N1 (Bjorck et al., 2025) further advance VLA capabilities through large-scale cross-embodiment and multi-task pretraining, demonstrating superior generalization and action prediction performance. Several works (Zhou et al., 2025; Yang et al., 2025c; Fang et al., 2025; Mazzaglia et al., 2025) attempt to address the catastrophic forgetting of language capabilities during VLA training, while others (Zawalski et al., 2025; Sun et al., 2024; Lin et al., 2025; Huang et al., 2025; Lee et al., 2025; Yuan et al., 2025) explore incorporating chain-of-thought reasoning into the VLA inference process. To pursue better generalization, several works (Shen et al., 2025; Cen et al., 2025; Liang et al., 2025; Jia et al., 2025) attempt to incorporate video generation models or world models into VLA action prediction, while others (Li et al., 2025b; Yu et al., 2025; Chen et al., 2025a,b) explore applying reinforcement learning to train VLA models. However, the aforementioned works primarily rely on robot-specific data for VLA training. Due to the high cost of robot data collection in practical scenarios, high-quality robot demonstration data remains extremely scarce. In contrast to these approaches, our work explores leveraging human egocentric data for model training, with the hypothesis that large-scale human demonstration data can effectively elicit generalization capabilities in VLA models."
        },
        {
            "title": "2.3 Learning VLAs from Human demonstration",
            "content": "Robot data acquisition is hard to scale due to the stringent robotoperator configuration and reliance on expert tele-operation. Egocentric VLA trained on the egocentric human demonstrations offers more scalable path, with strong potential to advance perceptionaction learning and real-world executability. EgoVLA (Yang et al., 2025b) utilizes scaled egocentric videos plus unified humanrobot action space with light robot finetuning, enabling efficient skill transfer and strong gains. Being-H0 (Luo et al., 2025) leverages physical-instruction tuning with discrete hand-motion codes (mm-level) and physics-aligned cross-view space supports finegrained VLA training from human videos. H-RDT (Bi et al., 2025) sets large bimanual pretraining with 3D hand pose and two-stage, 2B-parameter diffusion policy delivers substantial improvements. GR-3 (Cheang et al., 2025) utilizes multi-source training (web, VR, robot trajectories) yields strong generalization, rapid few-shot adaptation, and robust long-horizon bimanual and mobile control. RynnVLA-001 (Jiang et al., 2025) pretrains on large-scale human video demonstrations with video generation objectives and compresses actions into continuous latent space via ActionVAE to align video prediction with downstream robot fine-tuning. VITRA (Li et al., 2025c) treats the human hand as proxy end-effector, converts in-the-wild egocentric hand videos into robot-aligned formats, and combines VLMs with diffusion-based action experts for policy learning. These approaches rely on explicit alignment of human demonstrations to robot action spaces, which is inherently constrained by embodiment gaps between humans and robots. In contrast, our work targets more upstream objective by transforming egocentric human data into embodiment supervision signals for an embodied brain, providing scalable foundation that complements robot-data-based pipelines."
        },
        {
            "title": "3 Egocentric Embodied Supervision",
            "content": "In this section, we introduce the egocentric data annotation pipeline and the E2E-3M dataset."
        },
        {
            "title": "3.1 Egocentric2Embodiment Translation Pipeline",
            "content": "Human egocentric videos encode rich embodied experience, including action progression, handobject interaction, and task-level structure. However, this experience is not directly usable for training embodied brains. Raw videos lack explicit structure, free-form language annotations are unstable, and unconstrained generation often introduces temporal ambiguity or hallucinated interactions. Our key idea is to translate egocentric human data into structured and verifiable supervision that captures the hierarchical structure of embodied behavior, spanning action semantics, temporal organization, interaction dynamics, and task-level reasoning. To this end, we design schema-driven, rule-validated egocentric VQA data engine as shown in Fig.2 that systematically converts raw egocentric human videos into multi-level supervision aligned with embodied planning and interaction reasoning. 4 Figure 2 Illustration of the Egocentric2Embodiment Translation Pipeline. 3.1.1 Data Intake and Pre-processing To define the basic supervision units, the engine chunks each episode into short temporal clips, with episodelevel metadata serving as contextual priors. Given the large variation in egocentric action amplitude and frequency across scenarios, we adopt scenario-aware temporal segmentation, including fixed-interval, eventdriven, and kinematic-aware strategies. All clips are associated with explicit temporal spans and exposed through unified interface for downstream annotation. Episode-level metadata is used as contextual conditioning to limit the semantic space of subsequent question answering. The resulting representations are temporally localized and preserve short-range state transitions relevant to embodied manipulation and interaction. 3.1.2 Annotation Scheme Definition and Execution To produce supervision that reflects embodied cognition rather than generic video description, we define finite, schema-driven annotation space. Each clip is labeled with one of seven complementary VQA modes, including temporal, spatial, attribute, mechanics, reasoning, summary, and trajectory. Each mode is paired with template set that standardizes wording and controls the information granularity. The engine samples mode and template, then generates customized question and detailed sentence answer for each clip. VQA generation is performed by set of VLM annotation engines. The schema constrains both the question form and the required semantic content, which keeps supervision targets consistent across different generators. Answers must be natural-language and grounded in the visual evidence. The engine enforces egocentric conventions such as left/right hand references and manipulation-specific phrasing such as contact verbs. This stage yields multi-level annotations that capture complementary aspects of planning and interaction reasoning. 3.1.3 Quality Assurance and Validation Logic Open-ended generation easily produces errors that are harmful for training supervision. Common failures include references to non-visible hands, incorrect temporal ordering, and under-specified placeholders. We therefore introduce deterministic rule checker as validation gate. Samples that fail validation are rejected and sent back for regeneration with structured error messages that indicate the violated constraint. The checker applies three types of constraints. Evidence grounding requires that all mentioned actions, hands, and contact states are supported by the clip frames. Egocentric consistency enforces the correct hand references and prohibits mentions of unseen limbs or contradictory assignments. Mode-specific temporal logic requires explicit temporal connectors for temporal-sensitive modes and verifies that the described order matches the clip timeline. The generationvalidation loop repeats until all constraints are satisfied, producing supervision that is consistent, temporally coherent, and suitable for embodied learning. 5 Figure 3 Overview and Data Distribution Statistics of E2E-3M dataset. 3.1.4 Structured Egocentric Supervision Output Samples that satisfy all validation constraints are retained and compiled into the egocentric VQA supervision dataset. Each entry records the sampled frames, the selected VQA mode and template, the generated questionanswer pair, and the validation outcome. This design ensures traceability and reproducibility. The dataset produced by the proposed data engine offers structured and logic-verified supervision that encodes action organization and handobject interaction, completing the translation of egocentric video data into reliable training signals for egocentric planning and interaction reasoning."
        },
        {
            "title": "3.2 Egocentric2Embodiment Dataset (E2E-3M)",
            "content": "3.2.1 Data Sources and Domain Coverage The proposed Egocentric2Embodiment Translation Pipeline is applied to large-scale human egocentric video corpora collected across three complementary domains: household, factory, and laboratory environments, as shown in Fig.3(a). Collectively, these corpora comprise thousands of hours of egocentric video and capture substantial variation in environmental context, object composition, and interaction patterns. Specifically, Ego4D represents open-world household activities and provides extensive geographic and contextual diversity. BuildAI captures real industrial workflows, emphasizing procedural regularity and dense hand visibility in factory environments. EgoDex focuses on laboratory settings and offers high-resolution egocentric manipulation sequences with fine-grained interaction cues. These sources differ systematically in spatial layout, object distribution, and task structure. The aggregation yields the Egocentric2Embodiment dataset with complementary coverage across the space of egocentric embodied experience. 3.2.2 Diversity Analysis To evaluate whether the dataset provides sufficiently rich supervision for embodied planning and interaction, we analyze diversity along two interpretable axes: object coverage and action (verb) coverage in Fig.3(b). These dimensions correspond to what entities are involved in interactions and how those interactions are performed. Object coverage measures how many distinct objects appear in the dataset annotations. It reflects the breadth of perceptual and interactional contexts captured. For each domain s, the object coverage is calculated as: ObjectDiv(s) = noun noun 1000, (1) is the number of unique noun lemmas and noun where noun is the total noun token count in domain s. ObjectDiv values are grouped into four descriptive ranges: low (< 200), medium (200300), high (300350), and very high ( 350). 6 Figure 4 VLA architecture built on PhysBrain. Given an egocentric observation sequence and language instruction, PhysBrain encodes multimodal context for action generation. (a) PhysGR00T conditions flow-matching diffusion action expert on the last-layer hidden states of PhysBrain. (b) PhysPI more tightly couples PhysBrain and the action expert by injecting multiple VLM layers via layer-wise cross-attention. As shown in Fig.3(b), Household data falls into the high to very high range, showing broad object diversity typical of open environments. Lab data falls in the medium range, consistent with more limited set of experimental objects. Factory data shows low to medium coverage, reflecting repeated use of domain-specific parts and tools. These domain differences confirm that object coverage is complementary across sources rather than uniform. Action coverage quantifies the diversity of interaction verbs and reflects the richness of manipulation semantics. We evaluate verb diversity per VQA mode, since different modes are designed to emphasize distinct aspects of embodied behavior. Measuring coverage within functional subsets follows standard practice in lexical diversity analysis and enables mode-aware comparison. The verb diversity is calculated as: verb Nm where verb denotes the number of unique verb lemmas in mode m, and Nm denotes the number of QA pairs for that mode. The score is reported as the number of distinct verbs per 1,000 VQA pairs. erbDiv values are summarized into four descriptive ranges: low (< 80), medium (80-120), high (120-160), and very high ( 160). VerbDiv(m) = 1000, (2) Measured by VerbDiv, action-centric modes including Reasoning, Mechanics, Temporal, and Summary are predominantly very high across domains. Spatial, Trajectory, and Attribute are mostly medium. This separation is consistent across domains and aligns with the intended role of each mode. This supports that verb coverage is mode-specific and controlled, rather than uniformly distributed across annotations. The E2E-3M dataset bridges human egocentric video and embodied brain learning by providing structured supervision with broad scene coverage and rich action diversity. We expect that releasing this dataset will support future research on egocentric VLA and physical intelligence."
        },
        {
            "title": "4 Methodology",
            "content": "Using the data annotation pipeline proposed in the previous section, we translate embodied experience from egocentric videos into structured supervision suitable for learning an embodied brain. This process yields E2E-3M, dataset with roughly 3 million VQA cropus. To preserve general-purpose visionlanguage capability during SFT, we additionally mix an equal-sized subset sampled from FineVision, large-scale curated vision language corpus. We then perform supervised fine-tuning (SFT) on base VLMs (e.g., Qwen2.5-VL-7B) using 7 this mixture, resulting in an egocentric-centered VLM backbone (PhysBrain) with improved first-person understanding, reasoning, and planning capabilities. Quantitative results are reported in Sec. 5 (Tab. 1). With PhysBrain in hand, we study how these egocentric gains transfer to downstream control under standard VLA instantiations. Our goal in this section is not to propose new VLA architecture, but to evaluate transferability while minimizing confounding factors from additional heuristics or hand-crafted priors. We follow two widely adopted community paradigms, GR00T-style and Pi-style, and keep the action expert lightweight and consistent across both. We denote an observation (a short egocentric image sequence) as ot, the language instruction as x, and the VLM parameters as ϕ. The VLM produces token-level hidden states Hℓ = VLMϕ(ot, x)[ℓ] RN d, ℓ = 1, . . . , L, (3) where is the number of layers in the VLM, is the token length, and is the hidden dimension. The action policy predicts future action chunk at:t+K RKda . PhysGR00T (A GR00T-Style VLA). We introduce PhysGR00T, which follow the dual-system design in GR00T N1.5 (Bjorck et al., 2025): the VLM plays the role of System 2 to produce high-level multimodal representations, while Flow-Matching (FM) action expert (Liu, 2022) serves as System 1 to generate continuous actions. Concretely, PhysGR00T uses the last-layer VLM hidden states Zt = HL The FM expert is implemented as diffusion transformer (DiT) (Peebles and Xie, 2023) that denoises an action trajectory by cross-attending to Zt (VLM features are keys/values, action tokens are queries). Under the rectified-flow parameterization, we sample Gaussian noise ϵ (0, I) and time scalar τ (0, 1], then linearly interpolate between noise and the target action chunk to obtain the noised trajectory a: as the conditioning signal. = (1 τ ) ϵ + τ a, = ϵ. (4) Here is the target (time-independent) velocity that transports the noise trajectory to the data trajectory under this parameterization. The action expert predicts this velocity field conditioned on VLM features (and optional proprioceptive state st): (5) and is trained with simple regression objective ˆv = fθ(a, τ ; Zt, st), LFM = E(cid:2)ˆv v2 2 (cid:3). (6) At inference, we start from noise and apply small number of FM denoising steps (we use steps = 8) to obtain the action chunk at:t+K with K=16. This design provides controlled setting to examine how informative the egocentric VLM representation Zt is for action prediction. PhysPI (A Pi-Style VLA). We also instantiate Pi-style VLA, called PhysPI, in the spirit of π0 (Black et al., 2024), where the VLM backbone is more tightly coupled with the action expert. Instead of only using the last VLM layerlike PhysGR00T, PhysPI conditions the DiT blocks with multiple VLM layers. Let be the number of transformer blocks in the action DiT; we take the last VLM hidden states and inject them layer-wise into the DiT through cross-attention: Zt = (cid:8)HLM +1 , . . . , HL (cid:9), u(i+1) = DiTBlocki (cid:0)Q = u(i), KV = HLM +i (cid:1), = 1, . . . , M, (7) (8) where u(0) is the embedded (noised) action token sequence. The FM training objective remains identical, LFM = E(cid:2)fθ(a, τ ; Zt, st) v2 (cid:3). (9) 2 The layer-wise conditioning in PhysPI provides stronger coupling between intermediate VLM representations and the action expert. This allows us to test whether egocentric improvements distributed across VLM layers can be more effectively utilized for control."
        },
        {
            "title": "5 Experiment",
            "content": "This section details the experimental setup, benchmarks, and results. We report results from two primary evaluation tracks: (i) evaluating VLM performance in egocentric settings; and (ii) evaluating performance in robotic simulation environment following VLA fine-tuning."
        },
        {
            "title": "5.1 VLM Egocentric Evaluation",
            "content": "5.1.1 Egocentric Understanding Evaluation To validate egocentric understanding under fair and leakage-free setting, we evaluate on EgoThink (Cheng et al., 2024), widely used benchmark for egocentric reasoning built on Ego4D. Since Ego4D is included in the E2E dataset, our training protocol excludes the Ego4D portion when preparing PhysBrain for EgoThink evaluation. PhysBrain is trained only on the non-Ego4D subsets including EgoDex (Lab) and BuildAI (Factory), and is mixed with an equal-scale sample of general-purpose instruction data to preserve general visionlanguage capability. Baselines. We primarily compare our method against two categories of baselines: (i) General VLM, which include closed-source models such as GPT-4 and widely-used open-source models (MiniGPT-4-7B, LLaVA1.5-7B, LLaMA-3.2-11B, and Qwen2.5-VL-7B); and (ii) Embodied Brain, which include VST-RL-7B (Yang et al., 2025a) and RoboBrain2.0-7B (Team et al., 2025) for comprehensive evaluation. Evaluation. The comparison methods are evaluated through using the released weight for direct inference. Evaluation conditions are standardized across models. All models use the same prompt template and the generation outputs are scored with single GPT-4o (Hurst et al., 2024b) judging protocol across all EgoThink subtasks. These controls ensure that performance differences reflect model capability rather than data leakage, prompt variation, or inconsistent scoring. Table 1 summarizes performance on the six EgoThink dimensions (Activity, Forecast, Localization, Object, Planning, Reasoning). GPT-4 achieves the highest average performance, while our PhysBrain achieves sub-optimal performance and consistently outperforms strong open and competitive baselines. The most pronounced improvement is observed on Planning, where PhysBrain substantially exceeds all baselines and also outperforms GPT-4, indicating clear advantage in translating egocentric observations into executable plans. Importantly, this improvement is achieved without degrading egocentric perception, under strict Ego4D exclusion during training. Table 1 Results of evaluating the Egocentric Understanding of VLM models with the EgoThink benchmark. We highlight the best results in bold and the second-best results with underline Method Activity Forecast Localization Object Planning Reasoning Average GPT-4 (Achiam et al., 2023) MiniGPT-4-7B (Zhu et al., 2023) LLaVA-1.5-7B (Liu et al., 2024) LLaMA-3.2-11B (Dubey et al., 2024) Qwen-2.5-VL-7B (Bai et al., 2025c) VST-RL-7B (Yang et al., 2025a) RoboBrain2.0-7B (Team et al., 2025) PhysBrain (ours) 70.5 50 39.5 33.5 56.5 53 36 70 General VLM 61.5 15.5 50 50 54 88.5 59 74 59 71. Embodied Brain 56 49.5 53.5 70.5 78 77 79 48 62 64 64. 67.7 61.3 65.3 35.5 13 25.5 41 32 17 37 64.5 65.3 32 51 48.7 63.7 52.7 58 67.4 36.8 51.2 50.4 57.3 56.2 53.1 64.3 5.1.2 Complementary Evaluation on E2E Dataset To further validate the effectiveness and complementary of the proposed E2E dataset, we evaluate Spatial Aptitude Training (SAT) by performing supervised fine-tuning (SFT) on VST using only E2E data, without introducing any SAT-specific training samples. VST serves as the base model, as it is pre-trained on large-scale, high-quality spatial intelligence datasets and thus provides strong priors for static and object-centric spatial 9 reasoning. This setting allows us to assess whether E2E supervision offers complementary benefits, particularly for egocentric and dynamic spatial reasoning, beyond existing spatial intelligence training. Prior to fine-tuning, VST attains an overall accuracy of 45.33, with particularly low performance on Egocentric Movement (26.09), indicating limited sensitivity to egocentric motion and viewpoint changes. After fine-tuning on E2E dataset, overall accuracy increases to 59.33, while Egocentric Movement improves markedly to 91.30. Moderate gains are also observed on Action Consequence (54.05 64.86) and Perspective (39.39 48.48), whereas Object Movement remains comparable (39.13 34.78) and Goal Aim is unchanged (58.82). These results indicate that E2E supervision yields targeted improvements in egocentric and dynamic spatial reasoning, complementing the static spatial priors of VST and generalizing without task-specific training data."
        },
        {
            "title": "5.2 VLA Simulation Evaluation",
            "content": "To validate the efficacy of our model when deployed as the VLA for robotic control, we adopt PhysBrain as the VLM backbone and fine-tune it within the VLA paradigm using downstream robotics data. We then evaluate on the SimplerEnv (Li et al., 2024c) simulation benchmark with the WidowX robot. 5.2.1 Experiment Settings Architecture. We instantiate the VLA model using the PhysGR00T and PhysPI architecture as described in Sec. 4. The VLM component is initialized with weights from PhysBrain, whereas the Action Expert is initialized with random weights. Training. To adapt the VLM to the VLA architecture and the target robotic platform, we follow the training configuration of the starVLA (starVLA Community, 2025) framework and fine-tune VLA on two subsets of the Open X-Embodiment (OXE) (ONeill et al., 2024) dataset: Bridge (Walke et al., 2023) and Fractal (Brohan et al., 2023). Each training run requires approximately 22 hours on 8NVIDIA H100 GPUs. Detailed training hyperparameters are provided in Appendix A. Evaluation. The benchmark consists of four manipulation tasks: \"put spoon on towel\", \"put carrot on plate\", \"stack green block on yellow block\", \"put eggplant in the yellow basket\". For each task, we evaluate our VLA policy using the official evaluation script provided by the SimplerEnv repository (Li et al., 2024c). To mitigate randomness, we run five independent trials and report the mean performance. Baselines. We primarily compare our method against two categories of baselines: (i) VLA baselines, which include several widely used VLA models (RT-1-X, Octo, OpenVLA, RoboVLM, TraceVLA, SpatialVLA, CogACT, VideoVLA and π0); and (ii) VLM baselines, where we fine-tune several commonly used VLMs (RoboBrain2.0, VST-RL and Spatial-SSRL) under the VLA paradigm and evaluate them using the same training configuration as our method. 5.2.2 Experiment Results Table 2 summarizes the SimplerEnv evaluation results, comparing our PhysBrain model, fine-tuned under the VLA paradigm following the PhysGR00T architecture, against all baseline methods. More evaluation results under the PhysPI architecture are presented in Appendix B. (i) Comparison with VLA Baselines. Despite being fine-tuned on only two subsets of the OXE dataset (Bridge and Fractal), our VLA model achieves an average success rate of 53.9%, outperforming VLA baselines trained on substantially larger robot datasets (e.g., the full OXE dataset comprising 55 subsets). This improvement demonstrates that egocentric human data, when properly annotated and leveraged during pretraining, can effectively compensate for the robot-specific data. (ii) Comparison with VLM Baselines. Under the same training paradigm, we fine-tune several commonly used open-source VLMs into VLA models for comparison. As demonstrated in Table 2, our model consistently outperforms all VLM baselines across all tasks, achieving an average improvement of 8.8% over the second-best performing model and substantial 16.1% gain over RoboBrain (Team et al., 2025), which is specifically designed for embodied intelligence tasks. These results provide evidence that VLMs pretrained on large-scale human egocentric data yield more effective initialization for downstream VLA fine-tuning. Notably, the domainagnostic generalization capabilities induced by egocentric human data enable successful VLA training with 10 Table 2 Results of evaluating the VLA models with the WidowX robot in the SimplerEnv simulation environment, where the VLM backbone is fine-tuned under the VLA paradigm following the PhysGR00T architecture. We highlight the best results in bold and the second-best results with underline. Method Put Spoon on Towel Put Carrot on Plate Stack Green Block on Yellow Block Put Eggplant in Yellow Basket Average RT-1-X (ONeill et al., 2024) Octo-Base (Team et al., 2024) Octo-Small (Team et al., 2024) OpenVLA (Kim et al., 2024) OpenVLA-OFT (Kim et al., 2025) RoboVLM (Li et al., 2024b) TraceVLA (Zheng et al., 2025) SpatialVLA (Qu et al., 2025) CogACT (Li et al., 2024a) VideoVLA (Shen et al., 2025) π0 (Black et al., 2024) π0-FAST (Pertsch et al., 2025) Qwen2.5-VL-7B (Bai et al., 2025b) RoboBrain2.0-7B (Team et al., 2025) VST-RL-7B (Yang et al., 2025a) Spatial-SSRL-7B (Liu et al., 2025) PhysBrain (ours) 0.0 15.8 41.7 4.2 12.5 50.0 12.5 20.8 71.7 75.0 29.1 29.1 59.2 30.8 57.7 56.3 65.6 VLA Baselines 4.2 12.5 8.2 0.0 4.2 37.5 16.6 20.8 50.8 20.8 0.0 21. VLM Baselines 30.8 24.7 41.7 44.8 37.5 0.0 0.0 0.0 0.0 4.2 0.0 16.6 25.0 15.0 45.8 16.6 10.8 3.3 2.5 16.7 6.2 33. 0.0 41.7 56.7 12.5 72.5 83.3 65.0 70.8 67.5 70.8 62.5 66.6 44.2 93.3 50.0 72.9 79.2 1.1 17.5 26.7 4.2 23.4 42.7 27.7 34.4 51.3 53.1 27.1 48.3 34.4 37.8 41.3 45.1 53. only limited amount of robot-specific data, highlighting the transferability of human behavioral priors to robotic manipulation."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we address the fundamental challenge of exploiting human egocentric videos to bridge visionlanguage models with physical intelligence for robotic generalization. We introduce an Egocentric2Embodiment translation pipeline that systematically converts raw human egocentric videos into multi-level, schema-driven VQA supervision with deterministic rule validation, producing the E2E-3M dataset with approximately three million verified instances across household, factory, and laboratory domains. By supervised fine-tuning on this dataset without requiring any robot-collected data for VLM pretraining, we develop PhysBrain, which substantially improves egocentric capability (particularly in planning) as demonstrated by the EgoThink Benchmark, and achieves high success rate on SimplerEnv when used as the VLM backbone in standard VLA fine-tuning. Our results validate that scalable human egocentric supervision can serve as practical and effective bridge from vision-language understanding to physical intelligence, opening promising directions for expanding egocentric data diversity, developing more sophisticated translation mechanisms, and exploring efficient policy learning from human demonstrations."
        },
        {
            "title": "Limitations and Future Work",
            "content": "While our work demonstrates that VLMs pretrained on human egocentric data yield effective pretrained checkpoints for VLA training, several limitations warrant further investigation. First, our experimental evaluation primarily focuses on the PhysGR00T architecture with limited exploration of the PhysPI variant. more comprehensive analysis encompassing diverse architectural configurations, refined experimental protocols, and systematic ablation studies remains to be conducted. Second, further investigations into the complementarity between human egocentric data and robot demonstration data are ongoing. We plan to 11 progressively release these additional experimental results and extended analyses in subsequent versions of this work."
        },
        {
            "title": "Acknowledgements",
            "content": "This work is supported by the Zhongguancun Institute of Artificial Intelligence (Grant No.A20250501). Additionally, we would like to extend our special gratitude to DeepCybo for their assistance with data annotation and other processes."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-VL technical report. arXiv preprint arXiv:2511.21631, 2025a. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025c. Leonard Barmann and Alex Waibel. Where did leave my keys?episodic-memory-based question answering on egocentric videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 15591567, 2022. Hongzhe Bi, Lingxuan Wu, Tianwei Lin, Hengkai Tan, Zhizhong Su, Hang Su, and Jun Zhu. H-rdt: Human manipulation enhanced bimanual robotic manipulation, 2025. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. GR00T-N1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Robert Equi, Chelsea Finn, Niccolo Fusai, Manuel Galliker, et al. π0.5: vision-language-action model with open-world generalization. In Annual Conference on Robot Learning (CoRL), 2025. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, 12 Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2023. BuildAI. Egocentric-10k, 2025. https://huggingface.co/datasets/builddotai/Egocentric-10K. Jun Cen, Siteng Huang, Yuqian Yuan, Kehan Li, Hangjie Yuan, Chaohui Yu, Yuming Jiang, Jiayan Guo, Xin Li, Hao Luo, Fan Wang, Deli Zhao, and Hao Chen. Rynnvla-002: unified vision-language-action and world model, 2025. Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, and Yichu Yang. Gr-3 technical report, 2025. Kang Chen, Zhihao Liu, Tonghe Zhang, Zhen Guo, Si Xu, Hao Lin, Hongzhi Zang, Xiang Li, Quanlu Zhang, Zhaofei Yu, Guoliang Fan, Tiejun Huang, Yu Wang, and Chao Yu. πrl: Online rl fine-tuning for flow-based vision-language-action models, 2025a. Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. EgoPlanBench: Benchmarking multimodal large language models for human-level planning. arXiv preprint arXiv:2312.06722, 2024. Zengjue Chen, Runliang Niu, He Kong, Qi Wang, Qianli Xing, and Zipei Fan. Tgrpo: Fine-tuning vision-language-action model via trajectory-wise group relative policy optimization, 2025b. Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, and Yang Liu. Egothink: Evaluating first-person perspective thinking capability of vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429114302, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. Zhen Fang, Zhuoyang Liu, Jiaming Liu, Hao Chen, Yu Zeng, Shiting Huang, Zehui Chen, Lin Chen, Shanghang Zhang, and Feng Zhao. Dualvla: Building generalizable embodied agent via partial decoupling of reasoning and action, 2025. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4D: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1899519012, 2022. Yuping He, Yifei Huang, Guo Chen, Lidong Lu, Baoqi Pei, Jilan Xu, Tong Lu, and Yoichi Sato. Bridging perspectives: survey on cross-view collaborative intelligence with egocentric-exocentric vision. arXiv preprint arXiv:2506.06253, 2025. Ryan Hoque, Peide Huang, David Yoon, Mouli Sivapurapu, and Jian Zhang. EgoDex: Learning dexterous manipulation from large-scale egocentric video. arXiv preprint arXiv:2505.11709, 2025. Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. Thinkact: Vision-languageaction reasoning via reinforced visual latent planning, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024a. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024b. Yueru Jia, Jiaming Liu, Shengbang Liu, Rui Zhou, Wanhe Yu, Yuyang Yan, Xiaowei Chi, Yandong Guo, Boxin Shi, and Shanghang Zhang. Video2act: dual-system video diffusion policy with robotic spatio-motional modeling, 2025. Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, et al. Rynnvla-001: Using human demonstrations to improve robot manipulation. arXiv preprint arXiv:2509.15212, 2025. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. 13 Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, et al. OpenVLA: An open-source vision-language-action model. In Annual Conference on Robot Learning (CoRL), 2024. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success, 2025. Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, and Ranjay Krishna. Molmoact: Action reasoning models that can reason in space, 2025. Gen Li, Yutong Chen, Yiqian Wu, Kaifeng Zhao, Marc Pollefeys, and Siyu Tang. EgoM2P: Egocentric multimodal multitask pretraining. arXiv preprint arXiv:2506.07886, 2025a. Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, and Ning Ding. Simplevla-rl: Scaling vla training via reinforcement learning, 2025b. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, and Baining Guo. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation, 2024a. Qixiu Li, Yu Deng, Yaobo Liang, Lin Luo, Lei Zhou, Chengtang Yao, Lingqi Zeng, Zhiyuan Feng, Huizhi Liang, Sicheng Xu, Yizhong Zhang, Xi Chen, Hao Chen, Lily Sun, Dong Chen, Jiaolong Yang, and Baining Guo. Scalable vision-language-action model pretraining for robotic manipulation with real-life human activity videos, 2025c. Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-language-action models, 2024b. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. In Annual Conference on Robot Learning (CoRL), 2024c. Haotian Liang, Xinyi Chen, Bin Wang, Mingkang Chen, Yitian Liu, Yuhao Zhang, Zanxin Chen, Tianshuo Yang, Yilun Chen, Jiangmiao Pang, Dong Liu, Xiaokang Yang, Yao Mu, Wenqi Shao, and Ping Luo. Mm-act: Learn from multimodal parallel generation to act, 2025. Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, and Yang Gao. Onetwovla: unified visionlanguage-action model with adaptive reasoning, 2025. Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Z. XU, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, Chengfei Cai, WANG HongFa, Dima Damen, Bernard Ghanem, Wei Liu, and Mike Zheng Shou. Egocentric video-language pretraining. In Advances in neural information processing systems (NeurIPS), volume 35, pages 75757586, 2022. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2629626306, 2024. Qiang Liu. Rectified flow: marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577, 2022. Yuhong Liu, Beichen Zhang, Yuhang Zang, Yuhang Cao, Long Xing, Xiaoyi Dong, Haodong Duan, Dahua Lin, and Jiaqi Wang. Spatial-ssrl: Enhancing spatial understanding via self-supervised reinforcement learning, 2025. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. Hao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, and Zongqing Lu. Being-h0: Vision-language-action pretraining from large-scale human videos, 2025. Pietro Mazzaglia, Cansu Sancaktar, Markus Peschl, and Daniel Dijkman. Hybrid training for vision-language-action models, 2025. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. Alkesh Patel, Vibhav Chitalia, and Yinfei Yang. Advancing egocentric video question answering with multimodal large language models. arXiv preprint arXiv:2504.04550, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 41954205, 2023. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models, 2025. Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou, Rama Chellappa, and Pengchuan Zhang. EgoVLPv2: Egocentric video-language pre-training with fusion in the backbone. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 52855297, 2023. Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and Xuelong Li. Spatialvla: Exploring spatial representations for visual-language-action model, 2025. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training In Proceedings of the International Conference for High Performance Computing, trillion parameter models. Networking, Storage and Analysis, SC 20. IEEE Press, 2020. ISBN 9781728199986. Yichao Shen, Fangyun Wei, Zhiying Du, Yaobo Liang, Yan Lu, Jiaolong Yang, Nanning Zheng, and Baining Guo. Videovla: Video generators can be generalizable robot manipulators, 2025. starVLA Community. Starvla: lego-like codebase for vision-language-action model developing, 2025. https: //github.com/starVLA/starVLA. Qi Sun, Pengfei Hong, Tej Deep Pala, Vernon Toh, U-Xuan Tan, Deepanway Ghosal, and Soujanya Poria. Emma-x: An embodied multimodal action model with grounded chain of thought and look-ahead spatial reasoning, 2024. BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, Yingbo Tang, Xiangqi Xu, Wei Guo, Yaoxu Lyu, Yijie Xu, Jiayu Shi, Mengfei Du, Cheng Chi, Mengdi Zhao, Xiaoshuai Hao, Junkai Zhao, Xiaojie Zhang, Shanyu Rong, Huaihai Lyu, Zhengliang Cai, Yankai Fu, Ning Chen, Bolun Zhang, Lingfeng Zhang, Shuyi Zhang, Dong Liu, Xi Feng, Songjing Wang, Xiaodan Liu, Yance Jiao, Mengsi Lyu, Zhuo Chen, Chenrui He, Yulong Ao, Xue Sun, Zheqi He, Jingshu Zheng, Xi Yang, Donghai Shi, Kunchang Xie, Bochao Zhang, Shaokai Nie, Chunlei Men, Yonghua Lin, Zhongyuan Wang, Tiejun Huang, and Shanghang Zhang. Robobrain 2.0 technical report, 2025. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy, 2024. Homer Rich Walke, Kevin Black, Tony Z. Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, Abraham Lee, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: dataset for robot learning at scale. In Annual Conference on Robot Learning (CoRL), 2023. Rui Yang, Ziyu Zhu, Yanwei Li, Jingjia Huang, Shen Yan, Siyuan Zhou, Zhe Liu, Xiangtai Li, Shuangye Li, Wenqian Wang, Yi Lin, and Hengshuang Zhao. Visual spatial tuning, 2025a. Ruihan Yang, Qinxi Yu, Yecheng Wu, Rui Yan, Borui Li, An-Chieh Cheng, Xueyan Zou, Yunhao Fang, Xuxin Cheng, Ri-Zhao Qiu, Hongxu Yin, Sifei Liu, Song Han, Yao Lu, and Xiaolong Wang. Egovla: Learning vision-language-action models from egocentric human videos, 2025b. Shuai Yang, Hao Li, Yilun Chen, Bin Wang, Yang Tian, Tai Wang, Hanqing Wang, Feng Zhao, Yiyi Liao, and Jiangmiao Pang. Instructvla: Vision-language-action instruction tuning from understanding to manipulation, 2025c. Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, and Yu Wang. Rlinf: Flexible and efficient large-scale reinforcement learning via macro-to-micro flow transformation, 2025. Yifu Yuan, Haiqin Cui, Yaoting Huang, Yibin Chen, Fei Ni, Zibin Dong, Pengyi Li, Yan Zheng, and Jianye Hao. Embodied-r1: Reinforced embodied reasoning for general robotic manipulation, 2025. Michał Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning, 2025. 15 Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daumé III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies, 2025. Zhongyi Zhou, Yichen Zhu, Junjie Wen, Chaomin Shen, and Yi Xu. Chatvla-2: Vision-language-action model with open-world embodied reasoning from pretrained knowledge, 2025. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. RT-2: Vision-language-action models transfer web knowledge to robotic control. In Annual Conference on Robot Learning (CoRL), pages 21652183, 2023."
        },
        {
            "title": "A VLA Training Hyperparameters",
            "content": "We initialize the language model weights in the VLA architecture using PhysBrain and VLM baselines. During VLA fine-tuning, we employ distributed training across 8 GPUs with per-device batch size of 16. The model is trained for maximum of 104K steps using the AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate of 4e-5 and cosine learning rate scheduling. We set gradient accumulation steps to 1 and apply gradient clipping with maximum norm of 1.0. Training is accelerated using DeepSpeed (Rajbhandari et al., 2020) with the ZeRO2 optimization level."
        },
        {
            "title": "B PhysPI Architecture Experiment",
            "content": "Table 3 presents the SimplerEnv benchmark results obtained by fine-tuning PhysBrain and other VLM baselines under the PhysPI architecture within the VLA paradigm. Table 3 Results of evaluating the VLA models with the WidowX robot in the SimplerEnv simulation environment, where the VLM backbone is fine-tuned under the VLA paradigm following the PhysPI architecture. Method Put Spoon on Towel Put Carrot on Plate Stack Green Block on Yellow Block Put Eggplant in Yellow Basket Average Qwen2.5-VL-7B (Bai et al., 2025b) VST-RL-7B (Yang et al., 2025a) Spatial-SSRL-7B (Liu et al., 2025) PhysBrain (ours) VLM Baselines 8.3 20.9 16.7 22. 13.8 29.2 19.4 30.6 0.0 4.2 2.1 6.3 12.5 89.6 90.3 87. 8.65 35.9 32.1 36."
        }
    ],
    "affiliations": [
        "DeepCybo",
        "Harbin Institute of Technology",
        "Huazhong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Zhongguancun Academy",
        "Zhongguancun Institute of Artificial Intelligence"
    ]
}