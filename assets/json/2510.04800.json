{
    "paper_title": "Hybrid Architectures for Language Models: Systematic Analysis and Design Insights",
    "authors": [
        "Sangmin Bae",
        "Bilge Acun",
        "Haroun Habeeb",
        "Seungyeon Kim",
        "Chien-Yu Lin",
        "Liang Luo",
        "Junjie Wang",
        "Carole-Jean Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in large language models demonstrates that hybrid architectures--combining self-attention mechanisms with structured state space models like Mamba--can achieve a compelling balance between modeling quality and computational efficiency, particularly for long-context tasks. While these hybrid models show promising performance, systematic comparisons of hybridization strategies and analyses on the key factors behind their effectiveness have not been clearly shared to the community. In this work, we present a holistic evaluation of hybrid architectures based on inter-layer (sequential) or intra-layer (parallel) fusion. We evaluate these designs from a variety of perspectives: language modeling performance, long-context capabilities, scaling analysis, and training and inference efficiency. By investigating the core characteristics of their computational primitive, we identify the most critical elements for each hybridization strategy and further propose optimal design recipes for both hybrid models. Our comprehensive analysis provides practical guidance and valuable insights for developing hybrid language models, facilitating the optimization of architectural configurations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 0 0 8 4 0 . 0 1 5 2 : r Hybrid Architectures for Language Models: Systematic Analysis and Design Insights Sangmin Bae1,3,, Bilge Acun1, Haroun Habeeb2, Seungyeon Kim2, Chien-Yu Lin1, Liang Luo2, Junjie Wang2, Carole-Jean Wu1 1FAIR at Meta, 2Meta, 3KAIST AI Work done at Meta Recent progress in large language models demonstrates that hybrid architecturescombining selfattention mechanisms with structured state space models like Mambacan achieve compelling balance between modeling quality and computational efficiency, particularly for long-context tasks. While these hybrid models show promising performance, systematic comparisons of hybridization strategies and analyses on the key factors behind their effectiveness have not been clearly shared to the community. In this work, we present holistic evaluation of hybrid architectures based on inter-layer (sequential) or intra-layer (parallel) fusion. We evaluate these designs from variety of perspectives: language modeling performance, long-context capabilities, scaling analysis, and training and inference efficiency. By investigating the core characteristics of their computational primitive, we identify the most critical elements for each hybridization strategy and further propose optimal design recipes for both hybrid models. Our comprehensive analysis provides practical guidance and valuable insights for developing hybrid language models, facilitating the optimization of architectural configurations. Date: October 7, 2025 Correspondence: bsmn0223@kaist.ac.kr, carolejeanwu@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Recent language models (Llama Team, 2024; Microsoft Research, 2024; DeepSeek-AI, 2024; OpenaAI et al., 2024; Qwen Team, 2025; Gemini Team, 2025) have demonstrated strong scalability and human-like performance across wide range of tasks. Most of these models are based on the Transformer architecture (Vaswani et al., 2017), which alternates self-attention and feed-forward layers. However, the self-attention mechanism exhibits quadratic complexity with respect to input sequence length, leading to slow inference and substantial memory footprint. To address these limitations, new class of computational primitivesstate space models (SSMs)has emerged, inspired by signal processing (Gu et al., 2020, 2021a,b, 2022; Fu et al., 2022; Gu and Dao, 2023; Dao and Gu, 2024; Yang et al., 2024). These architectures scale more efficiently to long sequences by compressing prior context into finite-dimensional state. Among these, the Mamba model (Gu and Dao, 2023; Dao and Gu, 2024) stands out for being competitive with Transformer on language modeling, while also accelerating training speed through work-efficient parallel scan algorithms (Blelloch, 1990; Smith et al., 2022). These new primitives expand the architecture design space, opening up new possibilities for hybrid models that leverage the strengths of different architectural choices (Glorioso et al., 2024b; Ren et al., 2024; Lieber et al., 2024; Wang et al., 2024a; Poli et al., 2024; Dong et al., 2024; Ren et al., 2025; Basant et al., 2025). Notably, mixing Transformer and Mamba blocks often outperforms homogeneous architectures, while maintaining high efficiency. Most prior work on hybrid models has focused on the sequential interleaving of standard Transformer and Mamba blocksan inter -layer hybrid approach (Glorioso et al., 2024b; Ren et al., 2024; Jamba Team et al., 2024; Hunyuan Team et al., 2025). This practical strategy allows for balance between model quality and throughput by adjusting the ratio of quadratic (Transformer) to linear (Mamba) attention blocks. In addition, few intra-layer hybrid models have been proposed, which fuse the two primitives in parallel within individual layers. These approaches use either head-wise (Dong et al., 2024; Xiao et al., 2025) or sequence-wise splits (Zhang et al., 2024; Li et al., 2025b) to further combine the benefits of both architectures at finer granularity. Figure 1a summarizes which attention primitives each hybridization type can use. 1 (a) Overview of hybrid architecture (b) Pareto frontier of throughput Figure 1 (a) Two hybridization strategies construct attention using either Transformer (or intra-hybrid) or Mamba blocks. Varying ratios of these blocks controls the degree of hybridization. In intra-hybrid blocks, all heads are split into two halves, which are then processed by half-sized Transformer and Mamba blocks, respectively. (b) The hybrid architectures achieve superior quality-throughput trade-offs compared to homogeneous architectures. Negative log likelihood (i.e., loss) is measured on the DCLM validation set, and inference throughput is averaged across total lengths of 2K, 4K, 8K, 16K, and 32K, with the prompt length fixed at 512. All models have 1B parameters and are trained with the same FLOPs budget of 4.5e20 and 8K context length. For hybrids, we connect results for different block ratios (1:0, 1:1, 1:3, 1:5, 1:12, 0:1)where each ratio denotes (Transformer / Intra-hhybrid : Mamba blocks)with dashed lines. In the sliding window attention (SWA) model, global attention and SWA are interleaved at 1:5 ratio, with window size of 512 and an attention sink size of 64 (Gemma Team et al., 2025; Cohere et al., 2025; OpenAI et al., 2025). Despite the emergence of various hybrid architectures, the current literature lacks openly shared, in-depth analysis of hybridization strategies, making it difficult to understand their relative strengths and design trade-offs (Sun et al., 2025). In particular, most prior works focus on introducing specific hybrid architectures rather than providing detailed, systematic comparisons across possible hybridization approaches. As result, the key intuitions driving final design choices, as well as the relative merits of different strategies from multiple perspectives, remain open questions for the community. In this work, we address these research questions by conducting holistic evaluation of hybrid architectures via comprehensive, multi-faceted comparisons. Specifically, for model architecture choices, we compare inter-layer and intra-layer hybridization strategies with Transformer (Llama Team, 2024), Mamba (Dao and Gu, 2024), and striped models of global and sliding window attention (Beltagy et al., 2020; Gemma Team et al., 2025) with attention sink (Xiao et al., 2023). From quality perspectives, we identify optimal design choices for hybrid models by providing key insights into critical architectural considerations, based on language modeling perplexity (4.1) and extensive ablation studies (4.5 and 4.6). Additionally, we analyze long-context retrieval performance (Rae et al., 2019a; Kamradt, 2023) with the characteristics of each computational primitive (4.3). Our exploration of Mixture-of-Experts (Fedus et al., 2022; DeepSeek-AI, 2024) and compute-optimal scaling (Kaplan et al., 2020; Hoffmann et al., 2022) further offers practical guidance for scaling up hybrid models (4.4). On the efficiency front, we conduct an in-depth comparison of both training and inference, measuring training time, memory usage, inference throughput, and cache size (4.2). Based on our comprehensive experiments, we summarize the principal insights regarding hybrid architectures as follows: [Quality] Both hybrid model types outperform homogeneous architectures up to 2.9% accuracy and even surpass widely adopted SWA models in quality. Intra-layer hybridization shows best pareto-frontier of model quality and efficiency (see Figure 1b and Table 2). [Quality] While baselines show poor in-context retrieval and length generalization, all hybrid architectures achieve robust and superior long-context retrieval (see Figures 3c and 4). [Quality] Hybrid models are fully compatible with MoE structures and achieve an optimal compute-scaling slope between those of Transformer and Mamba (see Table 3). 2 Compute Memory Models"
        },
        {
            "title": "Llama",
            "content": "12dmodelLctx(Lctx+1)/"
        },
        {
            "title": "Sliding\nWindow",
            "content": "12dmodelLswa ((Lswa+1)/2+(LctxLswa))"
        },
        {
            "title": "Mamba",
            "content": "3Lctx(9dssmdstate+2dssm)"
        },
        {
            "title": "Parameter Counts",
            "content": "2dmodeldmodel +2dmodeldheadNkv 2dmodeldmodel +2dmodeldheadNkv"
        },
        {
            "title": "Cache Size",
            "content": "2dheadNkvLctx +2dheadNkvLctx 2dheadNkvLswa +2dheadNkvLswa dmodel(2dssm+2dstate+Nhead) +dstate(Nconv+dmodel)+2Nhead 2dssmdstate +2Nconv(2dstate + dssm) Table 1 Overall comparison of per-block compute and memory costs across different primitives. For simplicity, we exclude the additional term of 6 times the block parameter count from FLOPs. Here, denotes number, is dimension, and is sequence length. Lswa is the sum of sliding window size and attention sink sizes, and dssm is typically 2dmodel. For Transformers, we assume grouped-query attention, and cache sizes are calculated with bfloat16 precision. [Efficiency] By fully leveraging Mambas efficiency strengths (i.e., linear complexity with respect to sequences), hybrid models achieve fast end-to-end training time (see Figure 2) and high inference throughput with lower cache sizes (see Figures 1b, 3a, and 3b). [Design] Insights about optimal block ratios, ordering of computational primitives for hybrid architectural configurations are thoroughly explored (see Tables 4 and 5)."
        },
        {
            "title": "2 Background",
            "content": "Transformer architecture. The Transformer architecture (Vaswani et al., 2017) consists of two main components: the multi-head attention (MHA) module and feed-forward network (FFN). The MHA mechanism leverages multiple attention heads to capture diverse dependencies within the input sequence. For each head, attention is computed as follows: Attention(Q, K, V) = softmax (cid:19) (cid:18) QKT dk V, where Q, K, and are derived from learned linear projections of the input using WQ , , and WV , WK ℓ ℓ ℓ to restore the original model respectively. The outputs of all heads are concatenated and transformed by WO ℓ dimension. To reduce key-value cache size, recent models divide query heads into groups, with each group sharing single key and value heada structure known as grouped-query or multi-query attention (Ainslie et al., 2023). On the other hand, sliding window attention (SWA) (Beltagy et al., 2020; Jiang et al., 2023) reduces the context length over which queries attend to key and value states. For the FFN component, recent models adopt gating structure based on the SiGLU mechanism, which is variant of SwiGLU (Shazeer, 2020; Chowdhery et al., 2023) but uses the SiLU activation instead of Swish. This FFN can be replaced with Mixture-of-Experts layer (Shazeer et al., 2017; Fedus et al., 2022), where multiple FFNs (experts) are adaptively routed for each token. Mamba architecture. The Mamba architecture (Gu and Dao, 2023) is recent advancement in sequence modeling, building on structured state space models like S4 (Gu et al., 2021a), H3 (Fu et al., 2022), and S4D (Gu et al., 2022). Mamba replaces the attention mechanism with state space model (SSM) layer, enabling efficient and expressive modeling of long-range dependencies: ht = Aht1 + Bxt, yt = Cht + Dxt Here, ht represents the latent state at time t, xt is the input, and yt is the output. In practice, is discretized using zero-order hold (ZOH), and is discretized via the Euler method. key innovation is the inputdependent modulation of the state space parameters B, C, and the discretization step , allowing the model to flexibly control context retention and input integration for each token. This selectivity enables Mamba to match 3 the modeling quality of Transformers. Additionally, inspired by gated attention units (Hua et al., 2022), Mamba incorporates SiLU-based gating mechanism. Mamba 2 (Dao and Gu, 2024) advances this design by introducing the state space duality (SSD) layer, which reformulates SSM computations as matrix multiplications highly optimized for modern hardware (e.g., GPUs, TPUs). This results in significantly faster training and improved practical efficiency. Throughout this paper, we primarily utilize the Mamba 2 architecture as the computational primitive, with each block always followed by FFN layer, which can optionally be replaced with MoE layer. Computate and memory costs comparison. When analyzing costs in hybrid architectures (also for SWA models combining global and local attention), we reference the per-block costs in Table 1 (see Appendix for details). For example, in 1B model with 8K context, Transformer uses about 18% more FLOPs per sample than Mamba, due to its quadratic scaling versus Mambas linear scaling. From memory perspective, Mamba uses 2.5 more parameters per block (25M vs. 10M), but its cache size is 95% smaller than that of Transformer (256 MiB vs. 13.4 MiB)."
        },
        {
            "title": "3 Hybrid Architecture",
            "content": "Hybrid architectures can be categorized by their integration approach: inter-layer and intra-layer hybridization. We define each strategy and outline related research questions below."
        },
        {
            "title": "3.1 Inter-layer Hybrid Model\nDefinition. The inter-layer hybrid model alternates softmax attention layers with linear sequence modeling\nlayers at specific intervals (see Figure 1a). A key design choice in this approach is determining the order\nand proportion in which Transformer and Mamba primitives are arranged and interpolated. Its practical\neffectiveness and straightforward implementation have made the inter-layer approach a dominant strategy\nin hybrid architecture development.",
            "content": "Related works. Most recent hybrid models combine Mamba (Gu and Dao, 2023; Dao and Gu, 2024) with various attention mechanisms to enhance quality and efficiency. Notable examplesZamba (Glorioso et al., 2024b,a), Jamba (Jamba Team et al., 2024; Lieber et al., 2024), Samba (Ren et al., 2024, 2025), HunyuanTurboS (Hunyuan Team et al., 2025), Nemotron nano 2 (Basant et al., 2025), and IBM Granite 4.0share similar designs that leverage Mambas efficiency with global or local attention, scaling to over 1M context length. Some post-training approachessuch as MambaInLLaMA (Wang et al., 2024a), MOHAWK (Bick et al., 2024), Zebra-Llama (Yang et al., 2025), and Jet-Nemotron (Gu et al., 2025)use knowledge distillation to convert parts of pretrained Transformer into linear modules, while STAR (Thomas et al., 2024) and Composer (Acun et al., 2025) perform architecture search from scratch. Other hybridsRecurrentGemma (Botev et al., 2024), Griffin (De et al., 2024), Titans (Behrouz et al., 2024), RWKV-X (Hou et al., 2025), MiniMax-01 (Li et al., 2025a), and Qwen3-Nextcombine diverse self-attention variants (Beltagy et al., 2020; Qin et al., 2024; Lu et al., 2025; Qiu et al., 2025) with different linear modules (Qin et al., 2022; De et al., 2024; Yang et al., 2024; Peng et al., 2025). Block ratios in inter-layer hybrid. One of the key research questions in designing inter-layer hybrids is determining the optimal ratio of Transformer blocks to Mamba blocks when stacking layers. This ratio determines the degree of interpolation between two computational primitives, impacting not only model quality but also efficiency metrics like throughput and cache size. Prior work (Jamba Team et al., 2024; Lieber et al., 2024; Basant et al., 2025) has generally favored configurations with high proportion of Mamba layersfor example, 1:7 ratio. We revisit this design choice and offer deeper insights by comparing various ratios from both quality and efficiency perspectives. Positioning of computational primitives. Another open question is whether the position of interleaved blocks affects model quality, and how best to arrange them for optimal performance. There is currently no clear consensus on optimal positioning, as it may depend on factors such as block ratio and model scale. Although prior work (Lieber et al., 2024; Ren et al., 2025; Basant et al., 2025) often places Transformer blocks in the middle layers, comprehensive studies on this topic are still lacking. To fill this gap, we conduct extensive ablation experiments, varying the positions of Transformer blocks (e.g., early, middle, and late layers) across different ratios and model sizes."
        },
        {
            "title": "3.2 Intra-layer Hybrid Model",
            "content": "Intra-layer hybrid models achieve fine-grained fusion within individual layers by blending softmax Definition. attention and linear attention in parallel. common approach (Dong et al., 2024; Xiao et al., 2025) is head-wise splitting, where some heads use Transformer attention while others use Mamba (see intra-hybrid block in Figure 1a). Here, we use intra-hybrid and Mamba blocks as primitive candidates, so that this includes an interleaving mechanism akin to inter-layer hybrid models. Intra-layer hybrid models can be classified by how each token interacts with different modules. Related works. In the head-wise splitting approach (e.g., Hymba (Dong et al., 2024), WuNeng (Xiao et al., 2025)), attention heads are divided into groups, with each group assigned to distinct module. Alternatively, works like Liger (Lan et al., 2025) process each token through all modules in parallel and fuse the outputs. For sequencewise splitting, different modules are applied to context tokens based on their positions when computing attention scores. This splitting can be determined by an absolute point in the sequence, as in TransMamba (Li et al., 2025b), or by relative position, as in LoLCATs (Zhang et al., 2024). While not hybrid, differential architectures such as Diff-Transformer (Ye et al., 2024) and Diff-Mamba (Schneider et al., 2025) also use head-wise splitting, but use the same primitive type and subtract their attention scores. Architectural variants for intra-layer hybrid. We adapt head-wise splitting approach, assigning different primitives to two groups of attention heads. Especially, query and key states in Transformer are projected to reduced dimensions, while the value state is expanded back to the original size (Ye et al., 2024; Schneider et al., 2025). For Mamba, the hidden dimension of SSMs is similarly reduced based on configuration. Various fusion designs are explored, including normalization strategieswith or without group normalization (Wu and He, 2018) as in Hymba (Dong et al., 2024)); learnable scalarsuch as scaling (Dong et al., 2024; Ye et al., 2024; Schneider et al., 2025) or gating parameters (Lan et al., 2025; Xiao et al., 2025); fusion operationsincluding addition (Dong et al., 2024; Lan et al., 2025; Xiao et al., 2025), subtraction (Ye et al., 2024; Schneider et al., 2025), or concatenation (Xiao et al., 2025); and the number of output projection. We thoroughly evaluate these variants at both 350M and 1B scales, comparing their performance to existing intra-hybrid architectures. Dimension ratios in intra-hybrid block. The ratio of parameter sizes between Transformer and Mamba modules within intra-hybrid blockscontrolled by their hidden dimension allocationsis one of key architectural considerations. By observing how model quality changes as this ratio varies, we can infer the relative importance of each primitive. Furthermore, assuming expert parallelism (Rajbhandari et al., 2022) enables parallel execution of the modules, overall efficiency will be bounded by the slower Transformer component. Thus, we investigate how much we can reduce the dimension assigned to the Transformer, aiming to enhance efficiency without compromising overall quality. In our intra-layer hybrid, each block is either an intra-hybrid block Block ratios and positioning of primitives. or Mamba block. By varying the block ratio to control the interpolation degree, we analyze how different configurations affect both quality and efficiency, aiming to understand the trade-offs in intra-hybridization. We further examine how the placement of these intra-hybrid blocks at different depths influences model quality. Since both Transformer and Mamba are included, unlike using homogeneous primitives, we explore how its behavior differs from that of inter-layer hybrid models."
        },
        {
            "title": "4 Experiments",
            "content": "We build hybrid models with computational primitives following the configurations of Llama 3.2 (Llama Team, 2024) and Mamba 2 (Dao and Gu, 2024) architectures. We use the TorchTitan framework (Liang et al., 2024) for large-scale LLM training with H200 GPUs. See Appendix for further details."
        },
        {
            "title": "4.1 Main Results on Quality\nHybrid architectures significantly outperform homogeneous models. Table 2 compares our two hybridization\nstrategies—optimized along positioning and design choice axes (see §4.5 and §4.6)—with conventional base-\nlines (Llama Team, 2024; Dao and Gu, 2024; Gemma Team et al., 2025). Our controlled experiments show that\nhybrid models, including SWA, consistently outperform homogeneous models in negative log-likelihood (NLL)",
            "content": "5 Base Config Compute NLL () Few-shot Accuracy () Models N-emb Nattn Nswa Nssm Nmix Tok FLOPs Cache DCLM PG19 LD HS PQ ARC OB Avg 0.97B Llama 0.97B SWA Mamba 0.99B Inter-H 0.96B Intra-H 0.98B 0.97B SWA Mamba 0.97B Inter-H 0.96B Intra-H 0.98B 16 3 - 2 - 3 - 2 - - 13 - - - 13 - - - - - 13 11 11 - 13 11 11 - - - - 2 - - - 2 60B 4.5 e20 60B 3.8 e20 60B 3.7 e20 60B 3.7 e20 60B 3.7 71B 4.5 e20 73B 4.5 e20 73B 4.5 e20 72B 4.5 e20 256 63 13 43 38 63 13 43 38 2.750 2.741 2.758 2.735 2.728 2.724 2.736 2.716 2.709 43.2 56.1 55.2 72.8 56.0 55.9 72.6 44.2 53.7 55.1 73.8 43.9 56.4 55.4 73. 2.875 32.7 52.0 2.867 34.8 52.7 2.891 35.2 52.3 44.8 36.6 53.3 2.861 2.853 58.7 57.4 73.3 47.9 36.4 54.7 57.0 56.9 73.1 55.2 57.1 73.8 56.7 57.6 73.3 36.0 53.8 2.845 36.0 53.5 2.865 35.8 54.0 2.842 2.831 58.4 57.7 74.4 46.9 37.0 54.9 46.0 45.3 46.5 Table 2 Hybrid models achieve better quality than baselines under equal data and compute constraints. Nmix denotes the number of intra-hybrid block primitive. We calculate total training compute FLOPs and cache sizes for sequence length of 8K tokens. We use an approximate block ratio of 1:5 for SWA and hybrid architectures, which mix two different types of block. (a) FLOPs per sample () (b) End-to-end step time () (c) Memory need () Figure 2 Hybrid models have lower FLOPs, which directly leads to reduced actual training time. We measure metrics for 1B model using 8 H200 GPUs with FSDP, torch.compile, 8K lengths, and local batch size of 4, without activation checkpointing. Both SWA and hybrid models use 1:5 block ratio. indicates theoretical time achievable with parallelism (Rajbhandari et al., 2022). and few-shot accuracy under the same data budget. This demonstrates that effectively combining complementary inductive biases from different primitives is key to improving model quality through hybridization. Notably, we provide meaningful comparison between interand intra-hybrid architectures under matched budgets, which has not been previously explored or shared (Ren et al., 2024; Jamba Team et al., 2024; Dong et al., 2024; Basant et al., 2025). Under the same FLOP budget, hybrids achieve even more substantial quality gains (e.g., 2.9% increase in accuracy and 0.04 reduction in NLL). We find that these advantages are consistent across different block ratios and model scales, demonstrating the robustness of the hybrid approach (see Figure 1b)."
        },
        {
            "title": "4.2 Main Results on Efficiency\nFLOPs reduction in hybrid models translate into faster actual end-to-end training time. Mamba’s linear complexity\nleads to lower FLOPs than Transformer (18% lower for 1B scale at 8K lengths; see Table 1 for details). As shown\nin Figure 2, hybrid models leverage these advantage for superior performance in compute-bound scenarios.\nEmpirically, lower FLOPs almost directly yield faster training, aided by parallel scan algorithms (Blelloch, 1990;\nSmith et al., 2022). For intra-hybrid models, the expert parallelism (Rajbhandari et al., 2022) can theoretically\noptimize training speed further. Although Mamba and hybrid models may use slightly more memory during\ntraining, adjusting the block ratio provides flexibility to balance between efficiency and model quality.",
            "content": "Hybrid models achieve superior Pareto frontier of inference throughput and quality. Figures 3a and 3b demonstrate that hybrid architectures, by leveraging Mambas linear complexity and constant cache size, sustain 6 (a) Throughput () (b) Cache sizes () (c) NLL on PG19 () Figure 3 (a, b) Hybrid architectures show sub-quadratic scaling of inference throughput and memory as sequence increases. SWA and hybrid 1B models use block ratio of 1:5. For throughput, we set the prompt length to 512 and the batch size to 4. (c) Mamba enables length generalization in terms of perplexity, allowing hybrids to maintain strong performance. We use 1B models trained with compute budget of 4.5e20 and 8K lengths. Loss is averaged every 1K positions over 30 samples. (a) Llama (b) Sliding window (c) Mamba (d) Inter-hybrid (e) Intra-hybrid Figure 4 Hybrid models overcome the limitations of both foundational primitives, achieving superior in-context retrieval performance. We insert needle (random 7-digit number associated with random city name (Gemini Team et al., 2024)) across the 0100% depth range (y-axis) for context lengths up to 14K (x-axis). Over 100 trials, green indicates 100% accuracy, while red denotes 0% accuracy. We use 1B model checkpoints trained with 8K length and FLOPs budget of 4.5e20. SWA and hybrid models use 1:5 block ratio. high throughput and sub-quadratic memory growth across context lengths. This enables faster generation compared to baselines, while still maintaining high output quality (see Figure 1b). Although SWA uses 512 + 64 (window + sink) attention map, it remains slower than Mambas linear attention, making hybrids overall faster. Interestingly, intra-layer hybrids, which utilize half-sized Transformer, further outperform inter-layer hybrids even when executed sequentially."
        },
        {
            "title": "4.3 Long-context Capability Evaluation\nPosition-wise loss on PG19 shows hybrid-Mamba models extrapolates well to longer contexts. Figure 3c compares\nposition-wise loss on the PG19 corpus (Rae et al., 2019a) to examine the long-context capabilities of various\narchitectures. All models show decreasing NLL up to the 8K-token pretraining context, as later tokens benefit\nfrom more context. Beyond 8K, SSMs can effectively extrapolate (Gu and Dao, 2023; Dao and Gu, 2024;\nYang et al., 2024), while Transformer struggles due to its positional encoding methods (Press et al., 2021;\nKazemnejad et al., 2023; Zhou et al., 2024). Therefore, both hybrid architectures with a 1:5 block ratio benefit\nmore from Mamba, demonstrating stronger length generalization. This characteristic will vary depending\non the interpolation degree determined by the block ratio.",
            "content": "Hybrid models overcome weakness of Transformer and Mamba in in-context retrieval. Figure 4 presents in-context retrieval performance on the Needle-In-A-Haystack benchmark (Kamradt, 2023; Kuratov et al., 2024). needle is inserted at specific depth in the context, and models are evaluated on their ability to retrieve (generate) it. Transformer accuracy drops to near zero beyond 8K context length, as expected. On the other 7 Base Config MoE Performance ( / / ) - - - - - - Llama Mamba 2.750 2. 0.99B 0.99B - 0.99B 3.28B - 0.97B 0.97B 16 0.97B 3.79B 16 Models Act Total Nattn Nssm Nmix FLOPs Num Act DCLM PG19 Acc 4.5 e20 52.0 4.5 e20 1+8 1+1 2.656 2.775 55.8 52.3 3.7 e20 3.7 e20 1+8 1+1 2.673 2.803 55.0 3.7 e20 53.3 3.7 e20 1+8 1+1 2.653 2.780 56.0 54.7 3.7 e20 3.7 e20 1+8 1+1 2.648 2.775 56.9 0.96B 0.96B 2 0.96B 3.25B 2 0.98B 0.98B - 0.98B 3.27B - 2.758 2. 2.735 2.861 2.728 2.853 Intra-H Inter-H 11 11 13 11 11 2 2 - - - - - - - - - - Table 3 (Left) All architectures consistently achieve significant quality gains from MoE integration. All models are trained on 60B tokens, with hybrid models using 1:5 block ratio. We use one shared expert and the selected top-1 expert among eight. Additionally, token-choice router with loss-free balancing algorithm is utilized (Wang et al., 2024b; DeepSeek-AI, 2024). (Right) Hybrid architectures demonstrates compute-optimal scaling line that lies between those of Transformer and Mamba. We train models at four different scales100M, 350M, 1B, and 3Bacross five compute budgets. Each marker indicates the optimal model size for given compute budget. For hybrid models, we use 1:5 block ratio. hand, SWA and Mamba also struggle with retrieval outside their local window and sink token regions, or in long-range tokens, despite strong perplexity scores in Figure 3c; this is likely due to their focus on local information (Ben-Kish et al., 2024). In contrast, both interand intra-hybrid models surprisingly maintain strong retrieval performance up to about 1.5x the pretraining length, overcoming the limitations of the base primitives rather than simply inheriting them. While retrieval accuracy in the middle of extrapolated contexts does decline (Liu et al., 2023), hybrid models consistently demonstrate improved retrieval capabilities."
        },
        {
            "title": "4.4 Scaling Analysis\nMixture-of-Experts are fully compatible with hybrid architectures. A few recent inter-layer hybrid models (Lieber\net al., 2024; Jamba Team et al., 2024) have incorporated Mixture-of-Experts (MoE) (Shazeer et al., 2017; Fedus\net al., 2022) to improve performance at fixed compute. In Table 3, we re-examine MoE in inter-layer hybrids\nand also evaluate intra-layer hybridization, comparing both to baseline models. Across all architectures, MoE\nyields a substantial reduction in NLL (by 0.08) and 4 point increase in few-shot accuracy. Since hybridization\nis applied to the attention component, integrating MoE into the FFN layer remains compatible with all\nhybrid models. The data-hungry nature of MoE also enables more efficient scaling of hybrid models for a\nfixed number of activated parameters.",
            "content": "Hybrid models are efficient and scalable architectures. The figure to the right of Table 3 analyzes the scalability of different model architectures and identifies compute-optimal scaling strategies (Kaplan et al., 2020; Hoffmann et al., 2022). The compute-optimal line illustrates the best achievable quality and parameter sizes for each architecture under given computational budget. Mamba performs best with larger models and less data feeding, while Transformers favor higher token-to-parameter ratio of around 20 (Hoffmann et al., 2022). Hybrid models show intermediate scaling behavior, with intra-layer hybrids being little slightly more data-hungry. These results provide clear guidance on how to optimally scale up hybrid architectures."
        },
        {
            "title": "4.5 Ablation Studies for Inter-layer Hybridization",
            "content": "In Table 4, we compare To ensure quality, aim for high 1:1 ratio, but to balance with efficiency, use about 1:5 ratio. inter-hybrid models with six block ratios at two scales (1:0 = Transformer, 0:1 = Mamba). Hybrid architectures consistently outperform homogeneous ones, with the balanced 1:1 ratio yielding the best quality. However, due to the Transformers quadratic complexity, inference throughput gains become limited. To balance both efficiency and quality, ratio of around 1:5 appears to be optimal, which aligns with the lower ratios (e.g., 1:5, 1:7) adopted by large hybrid language models (Lieber et al., 2024; Wang et al., 2025; Basant et al., 2025). 8 Sizes 1B 350M Base Config N-emb Nattn Nssm 16 0.97B - 0.99B - 13 0.96B 0.94B 0.96B 0.97B 0.35B 0.35B 0.35B 0.34B 0.35B 0.36B 7 3 2 1 14 - 6 3 2 1 7 10 11 12 - 11 6 8 9 10 Performance ( / / ) DCLM PG 2.750 2.758 2.725 2.732 2.735 2.741 2.882 2.880 2.850 2.858 2.860 2.864 2.875 2.891 2.847 2.857 2.861 2. 3.015 3.024 2.985 2.994 2.999 3.003 Acc 52.0 52.3 54.0 53.8 53.3 53.1 48.7 48. 49.3 50.2 49.4 49.3 Ratio 1 : 0 0 : 1 1 : 1 1 : 3 1 : 5 1 : 12 1 : 0 0 : 1 1 : 1 1 : 3 1 : 5 1 : Table 4 (Left) Inter-layer hybrid achieves the best quality at 1:1 block ratio, but the 1:5 ratio is preferable to balance efficiency and quality. Transformer blocks are evenly distributed in the middle. All models are trained on 60B tokens. (Right) Interleaving Transformer blocks at intermediate depths is key for optimal performance. The upper figure shows results of ablating the position of single Transformer block in 1B (13 layers) and 350M (11 layers) models with 1:12 ratio. In the lower figure, after distributing Transformer blocks in the middle of 1B model, we move the first block to the first layer (Front) or the last block to the last layer (End). All models are trained on 60B tokens. Never place Transformer blocks at the front. Deciding the order of Transformer and Mamba blocks is key design choice. Our ablation study (see figures next to Table 4) shows that, for 1:12 ratio, placing the Transformer block in the early layers leads to significant performance drop, while positioning it in the middle yields the best results. Similar experiments with higher block ratios (bottom-right figure) also confirm that putting Transformer blocks at the front consistently leads to worse performance than homogeneous models, regardless of ratio. These finding support prior work favoring middle placement of Transformer (Jamba Team et al., 2024; Dong et al., 2024). In the future, more efficient methods like layer-wise sensitivity analysis (Yang et al., 2025) or architecture search (Thomas et al., 2024) may help optimize block positions."
        },
        {
            "title": "4.6 Ablation Studies for Intra-layer Hybridization",
            "content": "In designing intra-hybrid blocks, we explore four Novel outperforming architectural variant for intra-hybrid block. axes: normalization layer, learnable scalar, fusion operation, and output projection. In Table 5, our experiments reveal that normalization is crucial due to the scale differences between modules (Dong et al., 2024), which makes additional scaling factors unnecessary. For output fusion, either subtracting the outputs to mitigate attention noise or simply concatenating them yield the best quality. The resulting architecture surpasses previous intrahybrid models (Dong et al., 2024), and hybridizing Transformer and Mamba proves superior in both quality and efficiency over differential architectures that use same type of primitives (Ye et al., 2024; Schneider et al., 2025). Enlarging Transformer dimension improves quality, despite reduced efficiency. The upper-right figure presents ablation results for dimension allocation ratios within the intra-hybrid block, defined by the proportion of query / key dimension in Transformer versus pre-expansion dimension in Mamba (with 1:0 and 0:1 representing pure Transformer and Mamba, respectively). The results indicate that balanced allocation is important for quality, with larger Transformer dimensions leading to greater performance gainssuggesting that the Transformer component even plays more critical role than Mamba. However, since throughput is limited by the Transformer under parallel execution, 1:1 dimension ratio offers practical and effective balance. Evenly scattering intra-hybrid blocks across depths yields the best quality. We further investigate block ratio and ordering strategies for intra-hybrid models (see lower figure next to Table 5). Increasing the proportion of Transformer-containing intra-hybrid blocks consistently improves quality, in line with previous findings on primitive importance from dimension ratio ablations. However, using more Mamba blocks remains practical choice for efficiency. For block positioning, motivated by lessons from 4.5, we keep intra-hybrid blocks in the middle positions for ablation studies. Notably, evenly distributing these intra-hybrid blocks across depths yields the best results, while placing them at the ends (Sandwich strategy) leads to huge performance drops. 9 Models Llama Mamba Diff-T Diff-M Hymba Variants Performance ( / / ) Module Norm Scalar Fusion Out DCLM PG19 Acc Design Choices - - - - - - / / / Group - - Diff-T Diff Diff-M Diff Add Scale / - / Group / Group / Group / Group / Group / Group Scale - Gate - - - - Add Add Add Diff Conc Add Diff 1 1 1 2 1 1 1 1 1 2 2 2.750 2.758 2.727 2.759 2.726 2.740 2.721 2.751 2.721 2.715 2. 2.712 2.875 2.891 2.848 2.892 2.846 2.865 2.840 2.876 2.842 2.833 2. 52.0 52.3 53.6 53.0 52.4 53.9 54.5 53.1 54.0 54.8 54.5 2. 54.9 Table 5 (Left) We identify more optimal architecture than previous designs through ablation studies on intra-hybrid block. We train 1B model on 60B tokens, replacing all layers with intra-hybrid blocks (i.e., 1:0 block ratio). All heads are split into two, with each linked to half-sized primitive. (Right) Keeping larger Transformer dimension within intra-hybrid block improves quality (despite reduced efficiency), and placing the intra-hybrid block in the middle yields the best results. All 1B models with the optimal architecture from left, are trained on 60B tokens. In the figure above, we use block ratio of 1:0, where all layers become intra-hybrid blocks. For lower figure, we place intra-hybrid blocks in the middle of 1B model by: placing consecutively (Cluster), distributing evenly (Scatter), or placing at the beginning and end as well (Sandwich)."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this work, we present holistic analysis for inter-layer and intra-layer hybrid architectures. Our comprehensive evaluation demonstrates that hybrid models consistently outperform homogeneous architectures across multiple quality metrics, with intra-layer hybrids showing the strongest results. Notably, these hybrid approaches also deliver superior efficiency, achieving faster training and inference, much more than widely adopted sliding window attention models. Our findings shed new insights and practical guidance for designing high-quality, efficient hybrid architectures. Validation at scale. Our study is limited to 1B models, pretrained on 60B tokens. Since standalone Mamba models often show diminishing returns at scale (Waleffe et al., 2024), crucial next step is to validate whether the performance advantages of our hybrid model persist with longer training and at larger scales. However, we are optimistic they will, based on our scaling analysis up to the 3B parameter size and prior work suggesting that hybrid models scale more effectively (Waleffe et al., 2024; Jamba Team et al., 2024; Basant et al., 2025). Compatibility of advanced primitives. Our work combined base Transformer and Mamba blocks, whereas recent models incorporate more advanced variants. For instance, some models (De et al. (2024), Ren et al. (2025), Qwen3-Next) employ self-attention variants like local (Beltagy et al., 2020), differential (Ye et al., 2024), and gated attention (Qiu et al., 2025), while others (Hou et al. (2025), Qwen3-Next) use linear attention mechanisms such as RWKV-7(Peng et al., 2025) or Gated DeltaNet (Yang et al., 2024). key question is whether our design insights still apply to these newer hybrids and if the advantages of each component are preserved when combined. Modality extension. To achieve superintelligence, models must move beyond language to internalize the laws of physics that govern our world through video modality. This shift to multimodal learning (e.g., video, audio) intensifies the need for architectures that can support tokenization-free processing (Yu et al., 2023; Pagnoni et al., 2024; Assran et al., 2025) and overcome long-context bottlenecks. Consequently, hybrid architectures incorporating SSMs are rapidly gaining traction, making their extension beyond the language domain critical next step."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Prasoon Sinha, Meghana Madhyastha, Manzil Zaheer, Nellie Wu, Nicholas Monath for helpful conversations. We also thank Tianyu Liu for the support in utilizing the TorchTitan pretraining framework."
        },
        {
            "title": "References",
            "content": "Bilge Acun, Prasoon Sinha, Newsha Ardalani, Sangmin Bae, Alicia Golden, Chien-Yu Lin, Meghana Madhyastha, Fei Sun, Neeraja Yadwadkar, and Carole-Jean Wu. Composer: search framework for hybrid neural architecture design. arXiv preprint arXiv:2510.00379, 2025. Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pages 929947, 2024. Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adi Renduchintala, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, et al. Nvidia nemotron nano 2: An accurate and efficient hybrid mamba-transformer reasoning model. arXiv preprint arXiv:2508.14444, 2025. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba. arXiv preprint arXiv:2406.14528, 2024. Aviv Bick, Kevin Li, Eric Xing, Zico Kolter, and Albert Gu. Transformers to ssms: Distilling quadratic knowledge to subquadratic models. Advances in Neural Information Processing Systems, 37:3178831812, 2024. Guy Blelloch. Prefix sums and their applications. 1990. Aleksandar Botev, Soham De, Samuel Smith, Anushan Fernando, George-Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, et al. Recurrentgemma: Moving past transformers for efficient open language models. arXiv preprint arXiv:2404.07839, 2024. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Team Cohere, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Milad Alizadeh, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, et al. Command a: An enterprise-ready large language model. arXiv preprint arXiv:2504.00698, 2025. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. DeepSeek-AI. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. Hymba: hybrid-head architecture for small language models. arXiv preprint arXiv:2411.13676, 2024. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. 11 Daniel Fu, Tri Dao, Khaled Saab, Armin Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. https://zenodo.org/records/12608602. Google Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Google Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Google Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Paolo Glorioso, Quentin Anthony, Yury Tokpanov, Anna Golubeva, Vasudev Shyam, James Whittington, Jonathan Pilault, and Beren Millidge. The zamba2 suite: Technical report. arXiv preprint arXiv:2411.15242, 2024a. Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge. Zamba: compact 7b ssm hybrid model. arXiv preprint arXiv:2405.16712, 2024b. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33:14741487, 2020. Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021a. Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572585, 2021b. Albert Gu, Karan Goel, Ankit Gupta, and Christopher Ré. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:3597135983, 2022. Yuxian Gu, Qinghao Hu, Shang Yang, Haocheng Xi, Junyu Chen, Song Han, and Han Cai. Jet-nemotron: Efficient language model with post neural architecture search. arXiv preprint arXiv:2508.15884, 2025. Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, and Se-Young Yun. Block transformer: Global-to-local language modeling for fast inference. Advances in Neural Information Processing Systems, 37:4874048783, 2024. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Haowen Hou, Zhiyi Huang, Kaifeng Tan, Rongchang Lu, and Fei Richard Yu. Rwkv-x: linear complexity hybrid language model. arXiv preprint arXiv:2504.21463, 2025. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International conference on machine learning, pages 90999117. PMLR, 2022. Tencent Hunyuan Team, Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, et al. Hunyuan-turbos: Advancing large language models through mambatransformer synergy and adaptive chain-of-thought. arXiv preprint arXiv:2505.15431, 2025. Ai2 Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. Jamba-1.5: Hybrid transformer-mamba models at scale. arXiv preprint arXiv:2408.12570, 2024. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, 12 Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. https://arxiv.org/abs/2310.06825. Gregory Kamradt. NeedleInAHaystack: repository for testing LLMs. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack/blob/main/README.md, 2023. Accessed: 2023-10-31. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36:2489224928, 2023. Yury Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. Babilong: Testing the limits of llms with long context reasoning-in-a-haystack. Advances in Neural Information Processing Systems, 37:106519106554, 2024. Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, and Yu Cheng. Liger: Linearizing large language models to gated recurrent structures. arXiv preprint arXiv:2503.01496, 2025. Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025a. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. Yixing Li, Ruobing Xie, Zhen Yang, Xingwu Sun, Shuaipeng Li, Weidong Han, Zhanhui Kang, Yu Cheng, Chengzhong arXiv preprint Xu, Di Wang, et al. Transmamba: Flexibly switching between transformer and mamba. arXiv:2503.24067, 2025b. Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Zhang, Wei Feng, Howard Huang, Junjie Wang, et al. Torchtitan: One-stop pytorch native solution for production ready llm pre-training. arXiv preprint arXiv:2410.06511, 2024. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023. Meta Llama Team. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. Microsoft Research. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. OpenaAI, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. OpenAI, Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, et al. Byte latent transformer: Patches scale better than tokens. arXiv preprint arXiv:2412.09871, 2024. Bo Peng, Ruichong Zhang, Daniel Goldstein, Eric Alcaide, Xingjian Du, Haowen Hou, Jiaju Lin, Jiaxing Liu, Janna Lu, William Merrill, et al. Rwkv-7\" goose\" with expressive dynamic state evolution. arXiv preprint arXiv:2503.14456, 2025. Michael Poli, Armin Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. arXiv preprint arXiv:2210.10340, 2022. Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024. Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, et al. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free. arXiv preprint arXiv:2505.06708, 2025. Qwen Team. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Jack Rae, Anna Potapenko, Siddhant Jayakumar, Chloe Hillier, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019a. https://arxiv.org/abs/1911.05507. Jack Rae, Anna Potapenko, Siddhant Jayakumar, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019b. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. In International conference on machine learning, pages 1833218346. PMLR, 2022. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint arXiv:2406.07522, 2024. Liliang Ren, Congcong Chen, Haoran Xu, Young Jin Kim, Adam Atkinson, Zheng Zhan, Jiankai Sun, Baolin Peng, Liyuan Liu, Shuohang Wang, et al. Decoder-hybrid-decoder architecture for efficient reasoning with long generation. arXiv preprint arXiv:2507.06607, 2025. Nadav Schneider, Itamar Zimerman, and Eliya Nachmani. Differential mamba. arXiv preprint arXiv:2507.06204, 2025. Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. https://arxiv.org/abs/2002.05202. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, et al. Speed always wins: survey on efficient architectures for large language models. arXiv preprint arXiv:2508.09834, 2025. Armin Thomas, Rom Parnichkun, Alexander Amini, Stefano Massaroli, and Michael Poli. Star: Synthesis of tailored architectures. arXiv preprint arXiv:2411.17800, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-based language models. arXiv preprint arXiv:2406.07887, 2024. Dustin Wang, Rui-Jie Zhu, Steven Abreu, Yong Shan, Taylor Kergan, Yuqi Pan, Yuhong Chou, Zheng Li, Ge Zhang, Wenhao Huang, et al. systematic analysis of hybrid linear attention. arXiv preprint arXiv:2507.06457, 2025. Junxiong Wang, Daniele Paliotta, Avner May, Alexander Rush, and Tri Dao. The mamba in the llama: Distilling and accelerating hybrid models. Advances in Neural Information Processing Systems, 37:6243262457, 2024a. Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024b. 14 Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 319, 2018. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Liu Xiao, Li Zhiyuan, and Lin Yueyu. Wuneng: Hybrid state with attention. arXiv preprint arXiv:2504.19191, 2025. Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio. walk with sgd. arXiv preprint arXiv:1802.08770, 2018. Mingyu Yang, Mehdi Rezagholizadeh, Guihong Li, Vikram Appia, and Emad Barsoum. Zebra-llama: Towards extremely efficient hybrid models. arXiv preprint arXiv:2505.17272, 2025. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464, 2024. Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei. Differential transformer. arXiv preprint arXiv:2410.05258, 2024. Lili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers. Advances in Neural Information Processing Systems, 36: 7880878823, 2023. Michael Zhang, Simran Arora, Rahul Chalamala, Alan Wu, Benjamin Spector, Aaryan Singhal, Krithik Ramesh, and Christopher Ré. Lolcats: On low-rank linearizing of large language models. arXiv preprint arXiv:2410.10254, 2024. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou. Transformers can achieve length generalization but not robustly. arXiv preprint arXiv:2402.09371, 2024."
        },
        {
            "title": "A Detailed Experimental Results",
            "content": "We will release the detailed settings and experimental results for each experiment soon. The code is also planned to be made publicly available."
        },
        {
            "title": "B The Use of Large Language Models",
            "content": "We used large language models based on Llama 3.2 (Llama Team, 2024) and Llama 4 to polish the overall writing after drafting the paper ourselves. Additionally, we utilized the same models for vibe coding when fixing bugs or when making the initial drafts of the figures."
        },
        {
            "title": "C Details for Computational and Memory Costs Comparison",
            "content": "FLOPs per sample. Most parameters are linear weight matrices, so total FLOPs can be estimated by multiplying the parameter count by 6Lctx (accounting for addition, multiplication, and forward / backward passes, with the backward pass being roughly twice as expensive as the forward pass). For Transformer, attention further adds FLOPs from query-key dot products and value scaling, totaling 12NLdmodelLctx(Lctx + 1)/2, considering only causal attention and excluding Flash-Attention overhead (Dao et al., 2022). In SWA, FLOPs depend on the number of activated tokens based on window and sink sizes. Mamba, on the other hand, introduces additional FLOPs from its work-efficient parallel scan algorithm (Blelloch, 1990; Smith et al., 2022), calculated as 3Lctx(9dssmdstate + 2dssm). Thus, FLOPs per sample scale quadratically with sequence length (Lctx) for Transformer, while Mamba scales linearly. For 1B model with 8K context, Mamba uses about 18% fewer FLOPs per sample than Transformer. Parameter counts. The attention in Transformer block uses four projection weightsquery, key, value, and output. Variants like GQA reduce parameter count by decreasing the number of key and value heads, shrinking the corresponding projection matrices. In contrast, Mamba featurizes the input for state space parameters using several projection weights and 1D convolution layers, typically projecting the hidden dimension to twice its original size (i.e., dssm = dmodel). It also includes time-invariant parameters and D, as well as gating and output projection weights. As result, when comparing only the attention components (excluding FFN), Mambas parameter count is approximately 2.5 times higher than that of Transformer block in 1B model (e.g., 25M vs. 10M parameters per block). Cache size. Cache size is major inference bottleneck due to the GPU memory hierarchy, increasing HBMSRAM communication (Dao et al., 2022). In Transformer, the cache consists of key and value states. The cache size is 4NLdheadNkvLctx, where 4 accounts for key, value, and 2 bytes per bfloat16 element. Meanwhile, Mamba maintains two types of caches: hidden states for convolution layer and memory states compressed into finite space. Its total cache size is 2NL(dssmdstate + Nconv(2dstate + dssm)), with bfloat16 precision. Notably, Mambas cache size is independent of sequence length, so its efficiency benefits increase with longer contexts. For 1B model with 8K context, Mambas cache footprint is about 95% smaller than Transformers (e.g., 256 MiB vs. 13.4 MiB)."
        },
        {
            "title": "D Detailed Experimental Setup",
            "content": "Model architecture details. Each model is constructed based on the Llama-based Transformer (Llama Team, 2024) and the Mamba architectures (Dao and Gu, 2024). We primarily follow to the configurations of Llama 3.2 and Mamba 2 across various model scales. Table 6 provides summary of the detailed architectures for both the Transformer and Mamba models, which serve as the foundational computational primitives for our hybrid architecture variants. Base Configuration Self-Attention SSM Models Sizes N-emb Emb Vocab NL dmodel dffn Nhead Nkv dhead dssm dhead dstate Nconv Llama Mamba 100M 0.10B 0.13B 128K 8 350M 0.35B 0.20B 128K 14 0.97B 0.26B 128K 16 2.78B 0.39B 128K 28 1B 3B 100M 0.10B 0.13B 128K 6 350M 0.37B 0.20B 128K 11 0.98B 0.26B 128K 13 2.80B 0.39B 128K 21 1B 3B 1024 1536 2048 1024 1536 2048 3072 3072 4096 8192 8192 3072 4096 8192 8192 16 24 32 32 16 24 32 32 4 8 8 - - - - 64 64 64 96 - - - - - - - - 2048 3072 4096 6144 - - - - 128 128 128 192 - - - 128 128 128 256 - - - 4 4 4 4 Table 6 Architectural configurations for Transformer and Mamba models across different scales. For clarity, we separately detail the specific configurations for Self-Attention and SSM components. When constructing the interand intra-layer hybrid architectures, we refer to these configurations as the base computational primitives. The sliding window attention models use window size of 512 and an attention sink size of 64 under the same Llama configuration. Training settings. We conduct experiments using TorchTitan (Liang et al., 2024), PyTorch-native platform designed for large-scale training of LLMs. We pretrain different hybrid models variants from scratch on DCLMBaseline pretraining dataset (Li et al., 2024), which contains 4T tokens from 3B documents. We pretrain on randomly sampled 60B tokens using 8 H200 GPUs by default. We pack the corpus using the Llama 3.2 tokenizer (Llama Team, 2024), which has vocabulary size of 128K. bos token is prepended to every sample before packing them into context length of 8K tokens. We enable the model to attend to all previous tokens, not just those within the same document, while the eos token is used to distinguish document boundaries. We employ only FSDP (Zhao et al., 2023) with degree equal to the number of GPUs, and activation checkpointing to reduce memory footprint. We also utilize torch.compile (Ansel et al., 2024) to accelerate training. We use batch size of 2M tokens and utilize trapezoid learning rate scheduler (Xing et al., 2018) consisting of warmup (about 25%), stable, and cool down (about 20%) phases. The AdamW optimizer (Loshchilov and Hutter, 2017) and gradient clipping are used for all experiments. For the different model scales100M, 350M, 1B, and 3B parametersthe learning rates are set to 6e-3, 3e-3, 6e-4, and 3e-4, respectively. Evaluation settings. To assess model quality, we evaluate language modeling performance on the validation set of DCLM-Baseline (Li et al., 2024) and the PG19 datasets (Rae et al., 2019b). Additionally, following the settings of prior work (Gemini Team et al., 2024; Ho et al., 2024), we conduct evaluations on the Needle-In-aHaystack long-context benchmark (Kamradt, 2023; Kuratov et al., 2024). We further measure few-shot accuracy on five benchmarks using the Language Model Evaluation Harness (Gao et al., 2024): LAMBADA (LD), HellaSwag (HS), PIQA (PQ), ARC (Easy and Challenge), and OpenBookQA (OB). For all the few-shot datasets except LAMBADA, accuracy is normalized by the byte length of the target string. We adhere to the standard number of shots for each dataset. All evaluation experiments are performed on single H200 GPU."
        }
    ],
    "affiliations": [
        "FAIR at Meta",
        "KAIST",
        "Meta"
    ]
}