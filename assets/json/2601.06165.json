{
    "paper_title": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models",
    "authors": [
        "Dasol Choi",
        "Guijin Son",
        "Hanwool Lee",
        "Minhyuk Kim",
        "Hyunwoo Ko",
        "Teabin Lim",
        "Ahn Eungyeol",
        "Jungwhan Kim",
        "Seunghyeok Hong",
        "Youngsook Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment."
        },
        {
            "title": "Start",
            "content": "What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models Dasol Choi1,2* Guijin Son3* Hanwool Lee1* Minhyuk Kim4 Hyunwoo Ko3 Teabin Lim5 Eungyeol Ahn5 Jungwhan Kim6 Seunghyeok Hong7 Youngsook Song8 1AIM Intelligence 5Doodlin Corp. 6NAVER Cloud 2Yonsei University 3OneLineAI 7Hankuk University of Foreign Studies 4Korea University 8Lablup Inc. GitHub dasolchoi@yonsei.ac.kr,"
        },
        {
            "title": "Leaderboard",
            "content": "spthsrbwls123@yonsei.ac.kr 6 2 0 2 7 ] . [ 1 5 6 1 6 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, benchmark of 653 realworld visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 45 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, underspecified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting critical gap between benchmark evaluation and real-world deployment."
        },
        {
            "title": "Introduction",
            "content": "When users ask visual questions, they rarely provide complete, well-structured queries. Instead, they write informally, omit context, and rely on images to convey what they leave unsaid. user might ask How do do this? alongside an image, expecting the responder to identify the problem, infer the relevant domain, and provide step-by-step solution. This natural tendency toward under-specification poses fundamental challenge for vision-language models (VLMs) (Li et al., 2025), yet current benchmarks predominantly feature clean, explicit prompts failing to capture this phenomenon (Kim and Jung, 2025; Ju et al., 2024). *Equal contribution. Corresponding authors. 1 We introduce HAERAE-Vision, benchmark constructed from authentic user queries in Korean online communities. Starting from 86,052 question-image pairs across nine platforms, we apply six-stage filtering pipeline to yield 653 rigorously validated items (0.76% survival rate). The resulting questions are ambiguous, informal, and under-specified, mirroring the noisy nature of authentic multimodal interactions. To isolate the effect of query under-specification, we additionally construct HAERAE-Vision-Explicit, parallel dataset where each question is systematically rewritten to state the missing information explicitly. Our experiments reveal that query explicitation alone yields up to 22 point improvements across models, with smaller models benefiting most dramatically. Even state-of-the-art models achieve under 50% on original queries but surpass 55% with explicitation (GPT-5: 48.0%57.6%, Gemini 2.5 Pro: 48.5%56.7%). Furthermore, we demonstrate that even with web search enabled, under-specified queries still underperform explicit queries without search. This reveals that current retrieval systems cannot compensate for what users leave unsaid, as models must first understand user intent before search becomes effective. These findings challenge common assumption in VLM evaluation: that benchmark difficulty reflects model capability limitations. We show that substantial portion of difficulty stems instead from the natural under-specification of user queries, highlighting critical gap between benchmark evaluation and real-world deployment. Our contributions are: Real-world query benchmark: HAERAEVision, comprising 653 user-generated visual questions, filtered from 86K candidates (0.76% survival), spanning 13 domains. Paired explicit rewrites: parallel dataset Figure 1: Representative examples from HAERAE-Vision across six of the 13 domains. Each example shows an underspecified Korean question with English translation, the corresponding image, and evaluation checklist criteria. Note the informal, context-dependent nature of the original queries. of clarified queries enabling controlled measurement of under-specification effects. Quantifying under-specification: Empirical evidence that explicitation yields up to 22% improvements, with smaller models benefiting most. This demonstrates that query ambiguity accounts for substantial VLM difficulty."
        },
        {
            "title": "2 HAERAE-Vision Benchmark",
            "content": "We present HAERAE-Vision, benchmark constructed from authentic user queries, designed to capture the under-specified, informal nature of realworld visual questions. Our six-stage pipeline transforms large-scale, noisy community data into highquality evaluation problems while preserving the natural characteristics of user queries."
        },
        {
            "title": "2.1 Dataset Construction Pipeline",
            "content": "Starting from 86,052 raw question-image pairs from nine Korean platforms spanning general Q&A, gaming, science, and coding forums (see Appendix A.1 for detailed platform descriptions), we obtain 653 high-quality problems (0.76% survival rate). Figure 2 illustrates the filtering process. Stage 1: Data Collection. We collect (question, image, answer) triplets, prioritizing those with an accepted answer rewarded by the asker or with high online engagement (views, likes, comments), targeting questions the community finds valuable. Stage 2: Appropriateness Filtering. Each triplets are screened along three axes: (i) content safety (political/religious material, discrimination, adult content), (ii) objectivity (overly subjective or unverifiable prompts), and (iii) time-sensitiveness. GPT-4o is used for the automated filtering, flagging problematic items while allowing borderline cases to proceed to human validation. This removes 49.6% of raw data (see Appendix B.1). Stage 3: Difficulty Calibration. Following prior benchmarks (Zellers et al., 2019; Hendrycks et al., 2021), we remove questions that strong models solve trivially. Three models (GPT4o, Gemini-1.5-Flash, Claude-3.5) are evaluated against community-provided human answers using semantic-overlap scoring. Items with an average score above 0.6 are removed, eliminating 87.6% of the remaining items. 2 Figure 2: Filtering pipeline showing data reduction at each stage. Numbers indicate pipeline stages described in Section 2.1. The 0.76% survival rate reflects rigorous quality control. Each validated question is paired with an explicitated rewrite, yielding 1,306 query variants. Stage 4: Image Dependency Verification. To confirm that each question requires visual reasoning, we generate two responses per item using Gemini 2.0 Flash: one with the image and one without. Both responses are evaluated against the human reference, and items where the quality gap is below 1 point (on 0-10 scale) are discarded as imageindependent (see Appendix B.2). Stage 5: Checklist Generation. Each answer is converted into structured checklist with 1 to 5 criteria using o4-mini. The model is instructed to define the minimal necessary elements for response to be deemed correct, focusing on correctness, explanation quality, and reasoning steps rather than exhaustive coverage. This design enables partialcredit scoring and ensures reproducible, automated evaluation across models (see Appendix B.3). Stage 6: Human Validation. Seven native Korean annotators conduct three-phase validation: (1) filtering based on image appropriateness, question clarity, and checklist validity, removing any item flagged by at least one annotator; (2) refinement of questions and LLM-generated checklists, where annotators rewrite unclear criteria and remove items not grounded in the original questionimage pair; (3) final audit for category consolidation and consistency. This removes 37.2% of remaining items, yielding 653 problems (see Appendix C.1)."
        },
        {
            "title": "2.2 Dataset Statistics",
            "content": "Our final benchmark contains 653 problems with an average of 3.3 checklist items and 1.3 images per question. Table 1 presents the distribution across 13 categories, where Natural Objects and Gaming are the most represented. The survival rate per platform varies significantly (0.2% to 14.4%), showing distinct community characteristics (see Appendix A.2 for detailed breakdown)."
        },
        {
            "title": "Metric",
            "content": "Q length (char) Images per Checklist items"
        },
        {
            "title": "Category",
            "content": "Gaming Entertainment/Arts"
        },
        {
            "title": "Natural Objects\nScience\nMathematics",
            "content": "IT/Computer Coding/Development Machinery Daily Life Business/Economics Transportation Shopping/Consumer Health/Medical"
        },
        {
            "title": "Range",
            "content": "94.4 1.3 3.3 # Items 91 50 92 81 26 75 45 22 51 37 35 27 62,030 16 15 % 13.9 7.7 14.1 12.4 4.0 11.5 6.9 3.4 7.8 5.7 5.4 4.1 3."
        },
        {
            "title": "Total",
            "content": "653 100.0 Table 1: Overview of HAERAE-Vision. Statistics of question length, number of images, and checklist items, highlighting the diversity and multimodal nature of HAERAE-Vision."
        },
        {
            "title": "2.3 HAERAE-Vision-Explicit",
            "content": "To isolate the effect of query under-specification, we construct parallel dataset where each question is rewritten explicitly state the missing information while preserving the original intent. Figure 3 illustrates the transformation from under-specified to explicit queries across different domains. We use GPT-5.1 with web search to rewrite each question following strict guidelines (Appendix B.4): (1) preserve the original intent and scope without broadening or narrowing, (2) make implicit context explicit by specifying domains, entities, and concrete references, (3) replace vague references such as this, that, or here, (4) incorporate visual information from the image into the question, and (5) use web search only to verify proper nouns (e.g., game titles, product names) implied by the original question. Each rewritten question then undergoes human validation. Three annotators reviewed all 653 explicitated questions against their corresponding images,"
        },
        {
            "title": "Explicitated",
            "content": "이거는 어떻게 빼는걸까요? 저 고리를 빼고나니 저렇 게 남았는데 저부분은 어떻게 빼야하나요? (How do remove this? After removing the hook, this part remainshow do take it out?) 천장에 설치된 흰색 고리형 행거를 제거한 후 남은 금속 부 속품을 완전히 분리하려면 어떻게 해야 하나요? (How do completely remove the metal fitting left after detaching the white ceiling hook hanger?) 어린용 저 3마리 말고 더 있나요? (Are there more besides those 3 baby dragons?) 게임 원신에서 파카틴 NPC가 의뢰하는 임무 중 등장하는 이 어린 용 세 마리 외에 추가로 찾아야 하는 용이 더 있나요? (In Genshin Impact, are there additional dragons to find beyond the three baby dragons in Parkatins quest?) 한글 머리말 경계선 없애는 법. 동그라미 친 부분 없앨 수 있나요? (How to remove header border in Hangul. Can remove the circled part?) 한글 문서에서 머리말 구역 상단에 표시되는 여백 경계선을 제거하려면 어떻게 해야 하나요? (How do remove the margin border line shown at the top of the header area in Hangul word processor?) Figure 3: Examples of query explicitation across three domains (Daily Life, Gaming, IT/Software). Original queries contain vague references that depend on images. Explicitated versions include background information to clarify the user request. verifying factual accuracy, correcting hallucinated details through additional search, and adjusting specificity by removing overly specific terms or adding missing context where necessary. This process yields 653 explicitated questions paired with the original under-specified versions."
        },
        {
            "title": "2.4 Korean Cultural Grounding",
            "content": "We consider an item culturally grounded if it requires knowledge of Korean institutions, services, policies, local brands or products, or Koreanlanguage UI and text conventions; items solvable through globally shared knowledge are marked non-cultural. Under this criterion, 23.7% of items require distinctively Korean cultural knowledge, including local interfaces (Seoul Metro signage, Naver SmartPlace), region-specific objects (winter road sandbags), or Korean media (drama actors, traditional calligraphy). These items are rarely represented in English-centric training corpora. Figure 4 shows representative examples."
        },
        {
            "title": "3.1 Checklist-based Assessment",
            "content": "To mitigate the subjectivity of single-label scoring and the noise inherent in raw web text, our methodology centers on detailed checklists that decompose complex answers into specific criteria. Supported by recent findings that instance-specific rubrics align better with human judgments (Kim et al., 2024), each problem includes 15 evaluation points assessing different reasoning aspects. This checklist approach provides several advantages over traditional methods: (1) Fine-grained assessment of partial understanding, (2) Reduced subjectivity through explicit criteria, (3) Diagnostic capability for pinpointing model weaknesses, and (4) Scalability for automated evaluation."
        },
        {
            "title": "3.2 LLM Judge Protocol",
            "content": "GPT-5-Mini is instructed to act as the primary judge, following structured prompt that enforces consistent scoring across all problems (Appendix D). Each checklist item is scored on threelevel scale: met (1.0), partially met (0.5), or not met (0.0), based solely on explicit evidence found in the models response. Each score is accompanied by supporting evidence and justification, where the evidence is single line directly extracted from the response and the justification is short rationale clarifying the decision. The model outputs structured report containing evidence blocks and fractional totals (e.g., 3.5/5 when one item is partially and three are fully satisfied out of five). The overall score is computed as the average of instance-level means, where each instance has mi checklist items with item scores rij {0, 0.5, 1}: Sfinal ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 1 mi mi(cid:88) j=1 rij , ensuring comparability across problems with differing checklist lengths."
        },
        {
            "title": "4.1 Model Selection",
            "content": "We evaluate 45 VLMs covering broad range of families and scale. Proprietary models. This group includes OpenAIs GPT-5 series (GPT-5, GPT-5-Mini, GPT-5-Nano) (OpenAI, 2025a), Googles Gemini (2.5-Pro, 2.5-Flash, 2.5-FlashLite) (Google DeepMind, 2025), and proprietary 4 Figure 4: Examples highlighting the cultural specificity of HAERAE-Vision: (a) Seoul subway interface, (b) traditional painting with calligraphy, (c) Korean drama scene requiring celebrity recognition, (d) TV channel settings, (e) historical family registry. Such culturally grounded items require knowledge rarely represented in English-centric datasets. systems such as Perplexity-Sonar-Pro (Perplexity AI, 2025), xAI-Grok-4 (xAI, 2025), Mistral (Medium-3.1, Small-24B) and Pixtral (Large, 12B) (Mistral AI, 2024; Agrawal et al., 2024). Open-source models. We evaluate Gemma-3 (27B, 12B, 4B) (Gemma Team, Google DeepMind, 2025), Qwen2.5-VL (72B, 7B, 3B) (Bai et al., 2025), Qwen3-VL (235B-A22B, 32B, 30B-A3B, 8B, 4B, 2B; each in Instruct and Thinking variants) (Yang et al., 2025), Skywork-R1V3-38B (Shen et al., 2025), InternVL3.5 (38B1B) (Wang et al., 2025), and AIDC-AI-Ovis2 (34B1B) (Lu et al., 2025). Korean models. Finally, we include Koreanincluding VARCO-VISION-2.0 specific models, (14B, 1.7B) (NCSOFT AI Center, 2025) and HyperCLOVA-3B (Yoo et al., 2024). 4."
        },
        {
            "title": "Implementation Details",
            "content": "We used temperature=0.6 (1.0 for GPT-5 due to provider constraints), top_p=0.95, and max_tokens=4096 across all models. Each instance was evaluated three times and averaged."
        },
        {
            "title": "5.1 Overall Performance",
            "content": "Table 2 summarizes the performance of 18 VLMs across four categories (full results are provided in Appendix E.1). Even the best-performing modelsGemini 2.5 Pro (48.5%) and GPT-5 (48.0%)fall short of 50% accuracy, highlighting that authentic, culturally grounded multimodal queries remain far from solved. Proprietary systems consistently outperform open-weight counterparts, with the strongest open-weight models (Skywork-R1V3-38B: 27.8%, Qwen2.5-VL72B: 25.3%) reaching roughly half the accuracy of top proprietary models. Neither searchaugmented models (Perplexity Sonar-Pro: 44.3%) nor reasoning-specialized models (Skywork-R1V3) achieve notable gains, suggesting that solving would require capabilities beyond current retrievalaugmented or reasoning-optimized paradigms. Korean-specialized models also struggled to achieve competitive results (VARCO-VISION 2.0 14B: 15.6%, HyperCLOVA X-SEED-3B: 12.7%), indicating that dedicated local models have yet to demonstrate clear advantages on this benchmark. See Appendix for domain-level analysis."
        },
        {
            "title": "5.2 Effect of Query Explicitation",
            "content": "Figure 5 shows the effect of query explicitation on model performance. Across all six models, explicitation yields substantial improvements of 7.8 to 21.7 points. Smaller models benefit most from explicitation: GPT-5-Nano improves by 21.7 points (21.2 43.0), more than doubling its performance, while larger models like GPT-5 and Gemini 2.5 Pro show gains of 9.6 and 8.1 points respectively. This pattern suggests that under-specified queries disproportionately disadvantage smaller models, which may lack the capacity to infer implicit context from images alone. Even with explicitation, the bestperforming model (GPT-5) achieves only 57.6%, indicating that query under-specification accounts for substantial portion, but not all, of the difficulty in HAERAE-Vision. Our error analysis (Section 6) reveals that the remaining challenges stem primarily from cultural knowledge gaps."
        },
        {
            "title": "5.3 Effect of Web Search",
            "content": "To isolate the contributions of query explicitation and retrieval augmentation, we evaluated GPT-5 and GPT-5-Mini across all four conditions: original and explicitated queries, each with and without web search. We use the official OpenAI search API (OpenAI, 2025b). As shown in Table 3, web search yields moderate improvements for original queries (GPT-5: +7.57; GPT-5-Mini: +5.87), but these gains are smaller than those obtained through explicitation alone (+9.56 and +7.83, respectively). Notably,"
        },
        {
            "title": "Proprietary Models",
            "content": "Gemini 2.5 Pro GPT-5 GPT-5 Mini Perplexity Sonar-Pro Gemini 2.5 Flash Grok-4 Gemini 2.5 Flash-Lite GPT-5 Nano"
        },
        {
            "title": "Open Source Models",
            "content": "Skywork-R1V3-38B Mistral Medium 3.1 Gemma-3 27B Qwen2.5-VL-72B Pixtral Large InternVL3.5-38B Ovis2-34B Mistral Small 24B 40.520.61 33.070.87 27.380.81 32.840.76 29.311.09 26.880.67 18.390.59 11.640.53 15.030.73 13.740.80 11.590.58 10.890.66 11.430.82 8.810.46 9.520.47 6.460.29 51.440.40 48.140.96 50.620.93 47.980.59 45.040.98 31.030.64 38.171.47 20.101.24 35.310.88 30.770.86 25.800.61 26.711.49 21.790.50 23.250.61 21.880.55 10.180.45 53.890.79 55.710.84 51.880.74 47.171.23 44.050.53 44.180.80 32.740.84 27.151. 30.220.49 28.870.67 22.281.04 21.600.53 21.770.38 17.920.73 21.000.51 13.300.66 52.640.93 55.980.75 51.311.32 49.640.64 48.721.38 39.670.55 35.470.92 29.680.54 33.750.72 28.781.01 30.850.61 25.610.52 25.650.91 23.360.78 24.820.58 16.200.66 48.540.11 48.010.19 45.210.70 44.280.48 41.050.79 36.080.30 30.290.24 21.220.26 27.760.34 24.860.56 22.530.16 20.580.46 20.100.24 18.010.22 18.500.02 11.200.01 Korean-specialized Models VARCO-VISION 2.0 (14B) HyperCLOVA X-SEED-3B 7.870.80 6.250.25 16.560.65 14.870.51 16.880.57 11.990.50 22.130.88 17.930.73 15.550.29 12.660.10 Table 2: Performance of 18 models averaged by category. For model families with multiple sizes, only the largest variant is shown. Full results across all model sizes and detailed 13-category breakdowns are in Appendix 11. All scores are reported as meanSE, where SE is the standard error over 3 independent runs (n=3). The highest-scoring model is highlighted in bold."
        },
        {
            "title": "Model",
            "content": "Orig Orig+S"
        },
        {
            "title": "Expl",
            "content": "Expl+S GPT-5 GPT-5-Mini 48.01 45.21 55.58 51.08 57.57 53.04 59.72 56. Δ from Original (no search) GPT-5 GPT-5-Mini +7.57 +5.87 +9.56 +7.83 +11.71 +11.48 Figure 5: Effect of query explicitation on model performance. Models are sorted by improvement magnitude. Smaller models benefit most from explicitation, with GPT-5 Nano showing +21.7 points improvement. All results averaged over 3 runs. original queries augmented with search still underperform explicit queries without search (GPT-5: 55.58 vs. 57.57; GPT-5-Mini: 51.08 vs. 53.04). This indicates that retrieval cannot compensate for under-specified queries; models must first infer user intent for search to be effective. We observe recurring failure mode in which models rely on textual cues during search while failing to ground visual features, suggesting that current web search integration operates at largely surface level and is not deeply leveraged by GPT-5. The highest performance is achieved when explicitation and search are combined (GPT-5: 59.72; GPT-5-Mini: 56.69), demonstrating additive benefits. However, the marginal improvement from adding search to explicit queries (+2.15 and +3.65) is smaller than when added to original queries, implying that ex6 Table 3: Effect of web search and query explicitation. Scores reported as mean over 3 runs. Original+Search still underperforms Explicit alone, indicating retrieval cannot compensate for under-specification. plicitation already supplies much of the contextual information that search would otherwise retrieve."
        },
        {
            "title": "6 Additional Analysis on Explicitation",
            "content": "To understand why explicitation improves performance, we analyzed error patterns across original and explicitated conditions. We collected 3,164 (original) and 2,834 (explicitated) error cases where models scored below 1.0, spanning six models (GPT-5, GPT-5-Mini, GPT-5-Nano, Gemini 2.5 Pro, Gemini 2.5 Flash, Gemini 2.5 Flash-Lite). Each error was annotated by an LLM judge (Claude 3.5 Sonnet) along two dimensions: (1) failure categoryhow the error manifests (lack of explicitness, procedural reasoning, object recognition, cultural concept mismatch, visual-text grounding, spatial reasoning, See Table 4); and (2) root causewhy the error occurs (general reasoning, cultural knowledge, language). The full annotation prompt and category definitions are provided in Appendix F."
        },
        {
            "title": "Description",
            "content": "Lack of explicitness Procedural reasoning Object recognition Cultural mismatch Visual-text grounding Wrong image region referenced Spatial reasoning Missing checklist-required facts Failed multi-step execution Misidentified visual entities Misunderstood Korean conventions"
        },
        {
            "title": "General reasoning\nCultural knowledge\nLanguage",
            "content": "Logic/inference failure Missing Korean-specific knowledge Korean language misunderstanding Table 4: Error annotation taxonomy (abbreviated)."
        },
        {
            "title": "Expl",
            "content": "Δ Lack of explicitness Procedural reasoning Object recognition Cultural concept mismatch Visual-text grounding 84.3% 69.7% -14.6 -2.3 66.6% 64.3% 20.6% 18.5% -2.1 13.1% 22.5% +9.4 5.2% 16.6% +11.4 Table 5: Failure category shifts from original to explicitated queries."
        },
        {
            "title": "6.1 What Explicitation Fixes",
            "content": "Table 5 shows the key shifts. The most striking change is the reduction in lack of explicitness failures, which drop from 84.3% to 69.7% (-14.6pp), directly confirming that explicitation addresses surface-level ambiguity. Smaller models show the largest reductions in error cases after explicitation (GPT-5-Nano: -83 cases, +12.7pp perfect rate) compared to larger models (GPT-5-Mini: -40 cases, +6.1pp), confirming that under-specification disproportionately impacts smaller models. Category-level analysis (Figure 6) reveals that explicitation yields the largest gains in Mathematics, Science, Coding, and Shoppingcategories where failures primarily stemmed sfrom underspecified problem descriptions. In contrast, Natural Objects and Entertainment remain challenging even after clarification (all-models-pass rate: 0% in both conditions), with failures shifting toward visual-text grounding and cultural knowledge gaps."
        },
        {
            "title": "6.2 Why Retrieval Alone Is Insufficient",
            "content": "Earlier, our results have shown that original queries with search (55.6) underperform explicit queries without search (57.6). This reveals fundamental limitation: retrieval cannot compensate for query under-specification. Under-specified queries like 이거 어떻게 해요? (How do do this?) contain no searchable keywords. Since the critical context is embedded solely within the visual modality, current text-based search engines fail to bridge the 7 Figure 6: Category-level explicitation effects. Categories like Mathematics and Coding show large gains, while Entertainment and Natural Objects remain difficult even after clarification, with failures shifting toward cultural knowledge and visual grounding. modality gap without explicit textual grounding. Even when models attempt searches, they lack the specific terms (product names, game titles, error codes) needed to retrieve useful results. In contrast, explicitated queries contain concrete references (e.g., 천장에 설치된 흰색 고리형 행거 (white ring-shaped hanger installed on the ceiling)) that enable targeted retrieval. The best performance is achieved when both are combined (59.7), but the key finding is that search on under-specified queries cannot match explicitation alone; models must first understand what to search for."
        },
        {
            "title": "6.3 Cultural Knowledge Gaps",
            "content": "After explicitation, what errors remain? Analyzing root causes reveals shift toward cultural knowledge gaps  (Table 6)  . The increase in cultural knowledge attribution (+6.4pp) suggests that once query ambiguity is resolved, the dominant remaining challenge is Korea-specific knowledge. For example, when shown orange bags along rural road, models identified them as road safety markers or wasp traps, missing that these are winter snow preparation sandbags, something all native Korean drivers would have known. Similarly, all SOTA models misidentified Korean folder phone (SKY IM-100) as global brands like Sony or Nokia. Finally, the negligible language error rate (<1.5%) confirms that Korean proficiency is no longer hurdle for global models, but cultural contents are."
        },
        {
            "title": "7 Reliability of LLM-as-a-Judge",
            "content": "It is widely known that LLM-Judges may be prone to biases (Son et al., 2024). Accordingly, to ensure the credibility of our evaluation, we assess the interjudge agreement among four LLM judges (GPT-5,"
        },
        {
            "title": "Expl",
            "content": "Δ"
        },
        {
            "title": "General reasoning\nCultural knowledge\nLanguage",
            "content": "86.6% 79.8% -6.9 12.7% 19.0% +6.4 1.2% +0.5 0.7% Table 6: Root cause distribution. After explicitation, cultural knowledge becomes more prominent as surface-level ambiguity is resolved. GPT-5-mini GPT-5 Gem-2.5-Pro Gem-2.5-Flash GPT-5-mini GPT-5 Gem-2.5-Pro Gem-2.5-Flash 0.87 0.90 0.90 0.87 0.90 0. 0.90 0.90 0.89 0.90 0.86 0.89 Krippendorffs α = 0.867 Table 7: Pairwise Pearson correlations among four LLM judges. Spearman correlations range 0.870.90. Krippendorffs α = 0.867 indicates substantial agreement. GPT-5-mini, Gemini-2.5-Pro, Gemini-2.5-Flash). stratified random sample of 250 model responses (50 per 0.2-score interval) was re-evaluated under identical protocols. Table 7 shows consistently high correlations, with Pearson ranging from 0.863 to 0.903 and Spearman from 0.866 to 0.901. Krippendorffs α = 0.867 exceeds the conventional 0.80 threshold, indicating substantial agreement across models with different architectures. Furthermore, to assess alignment with human judgments, the same 250-sample set was evaluated by four independent human annotators, who rated the appropriateness of GPT-5-Mini judgments on 5-point scale. Agreement was high (Pearson = 0.820, Spearman ρ = 0.810, < 0.001), demonstrating that our judge provides stable and human-aligned evaluation signal. Detailed analyses of low-agreement cases suggest that most discrepancies stem from superficial keyword matching or excessive leniency (examples in Appendix C.2)."
        },
        {
            "title": "8 Related Work",
            "content": "Evaluating VLMs. As VLMs become more general-purposed, evaluation has shifted toward diagnostic suites that aim to separate recognition, OCR, and knowledge from higher-level reasoning and instruction following (Liu et al., 2024; Li et al., 2024; Yu et al., 2024). To better probe reasoning, several benchmarks target domain knowledge grounded with visual inputs (Yue et al., 2024, 2025; Lu et al., 2023). This was rapidly followed by the Korean community, first by text benchmarks that measure Korean knowledge (Son et al., 2023, 2025; Hong et al., 2025), then by multiIn addition, modal benchmarks: KRETA, KViscuit, and KOFFVQA (Hwang et al., 2025; Park et al., 2024; Kim and Jung, 2025). localized evaluation tools such as KMMB, KSEED, and KDTCBench have been released alongside Korean VLM development efforts (Ju et al., 2024). However, these benchmarks have already been saturated by older-generation models such as GPT4o (e.g., KRETA (Hwang et al., 2025): 84.6; KVISCUIT (Park et al., 2024): 89.5; K-MMB: 81.01; K-SEED: 76.98; K-DTCBench: 85.80 (Ju et al., 2024)), motivating the creation of more challenging benchmark. Query Underspecification. Underspecified or ambiguous queries are pervasive in conversational settings (Rahmani et al., 2023), forcing systems to choose between answering, hedging, or asking for missing constraints. Prior efforts to evaluate LLMs in ambiguity handling include AmbigQA (Min et al., 2020), and clarification-focused resources such as ClariQ (Aliannejadi et al., 2021) and the ConvAI3 shared task (Aliannejadi et al., 2020), which measure how effectively system reduces uncertainty through clarification. More recently, QuestBench tests minimal question asking as information acquisition for underspecified reasoning (Li et al., 2025). In the multimodal setting, ClearVQA evaluates whether models can ask image grounded clarification questions to resolve ambiguous visual queries (Jian et al., 2025). Overall, however, multimodal resources for query underspecification remain scarce. To bridge this gap, we introduce HAERAE-Vision, which further targets niche and underexplored setting by focusing on underspecification in Korean language interactions with culturally grounded content and assumptions."
        },
        {
            "title": "9 Conclusion",
            "content": "We introduce HAERAE-Vision, benchmark of 653 authentic Korean questions from real-life users, each paired with explicit rewrites. Our experiments show that query underspecification accounts for an 822 point drop in VLM performance. Retrievalaugmented prompting does not close this gap: search-augmented underspecified queries still underperform explicitated queries without search. We further find that many remaining failures reflect missing cultural knowledge rather than surfacelevel ambiguity. Together, these findings highlight challenges that sanitized, clean-query benchmarks fail to capture."
        },
        {
            "title": "Limitations",
            "content": "Although this work focuses on constructing Korean multimodal dataset for studying query underspecification, the same pipeline can be adapted to other languages to support broader multilingual investigation, which we leave for future work. Guided by quality over quantity principle, our filtering procedure yields 0.76% survival rate. This aggressive filtering may exclude some informative edge cases; however, it should be noted that our goal is not to provide comprehensive evaluation of Korean knowledge. Rather, we aim to study how LLM behavior changes under different levels of information density in user prompts. Furthermore, our web search augmentation analysis is also limited in scope, as it evaluates only OpenAIs web search, and results may differ with more advanced retrieval systems. However, based on our observations, the primary bottleneck appears to be less about the search API itself and more about the models ability to extract and formulate meaningful questions grounded in the image and accompanying text. Finally, our error annotation relies on an LLM judge, which may introduce systematic biases despite the high inter-judge agreement we observe."
        },
        {
            "title": "Ethics and Data Governance",
            "content": "This study received ethical approval from the Institutional Review Board of Hankuk University of Foreign Studies. All data were collected from publicly available Korean community platforms. We implemented rigorous filtering process to exclude sensitive content, and all personally identifiable information (PII) has been systematically removed. We release balanced 25% development subset covering 12 categories; the Health/Medical category is withheld to mitigate potential privacy risks. The full test set is hosted on rate-limited, anonymous evaluation server to prevent data contamination and ensure fair model comparison."
        },
        {
            "title": "References",
            "content": "Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, and 1 others. 2024. Pixtral 12b. arXiv. Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, and Mikhail Burtsev. 2020. Convai3: Generating clarifying questions for opendomain dialogue systems (clariq). arXiv preprint arXiv:2009.11352. Mohammad Aliannejadi, Julia Kiseleva, Aleksandr Chuklin, Jeff Dalton, and Mikhail Burtsev. 2021. Building and evaluating open-domain dialogue corpora with clarifying questions. In EMNLP. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025. Qwen2.5-vl technical report. Preprint, arXiv:2502.13923. Gemma Team, Google DeepMind. 2025. Gemma 3 technical report. arXiv. Google DeepMind. 2025. Gemini 2.5 pro: Model https://storage.googleapis.com/ card. model-cards/documents/gemini-2.5-pro.pdf. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Seokhee Hong, Sunkyoung Kim, Guijin Son, Soyeon Kim, Yeonjung Hong, and Jinsik Lee. 2025. From kmmlu-redux to kmmlu-pro: professional korean benchmark suite for llm evaluation. arXiv preprint arXiv:2507.08924. Taebaek Hwang, Minseo Kim, Gisang Lee, Seonuk Kim, and Hyunjun Eun. 2025. Kreta: benchmark for korean reading and reasoning in text-rich vqa attuned to diverse visual contexts. arXiv preprint arXiv:2508.19944. Pu Jian, Donglei Yu, Wen Yang, Shuo Ren, and Jiajun Zhang. 2025. Teaching vision-language models to ask: Resolving ambiguity in visual questions. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 36193638, Vienna, Austria. Association for Computational Linguistics. Jeongho Ju, Daeyoung Kim, SunYoung Park, and Youngjune Kim. 2024. Varco-vision: Expanding frontiers in korean vision-language models. arXiv preprint arXiv:2411.19103. Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, and 1 others. 2024. The biggen bench: principled benchmark for fine-grained evaluation of language models with language models. arXiv preprint arXiv:2406.05761. Yoonshik Kim and Jaeyoon Jung. 2025. Koffvqa: An objectively evaluated free-form vqa benchmark for large vision-language models in the korean language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 575 585. 9 Belinda Li, Been Kim, and Zi Wang. 2025. Questbench: Can llms ask the right question to acquire arXiv preprint information in reasoning tasks? arXiv:2503.22674. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. 2024. Seedbench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, and 1 others. 2024. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255. Shiyin Lu, Yang Li, Yu Xia, Yuwei Hu, Shanshan Zhao, Yanqing Ma, Zhichao Wei, Yinglun Li, Lunhao Duan, Jianshan Zhao, Yuxuan Han, Haijun Li, Wanying Chen, Junke Tang, Chengkun Hou, Zhixing Du, Tianli Zhou, Wenjie Zhang, Huping Ding, and 23 others. 2025. Ovis2.5 technical report. Preprint, arXiv:2508.11737. Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. Ambigqa: Answering ambiguous open-domain questions. arXiv preprint arXiv:2004.10645. Mistral AI. 2024. Pixtral-large-instruct-2411: Model https://huggingface.co/mistralai/ card. Pixtral-Large-Instruct-2411. NCSOFT AI Center. 2025. Varco-vision-2.0 technical report. arXiv. OpenAI. 2025a. Gpt-5 system card. https: //openai.com/index/gpt-5-system-card/. PDF: Updated gpt-5-system-card-aug7.pdf. https://cdn.openai.com/ OpenAI. 2025b. Web search openai api reference. ChaeHun Park, Yujin Baek, Jaeseok Kim, Yu-Jung Heo, Du-Seong Chang, and Jaegul Choo. 2024. Evaluating visual and cultural interpretation: The k-viscuit benchmark with human-vlm collaboration. arXiv preprint arXiv:2406.16469. Perplexity AI. 2025. overview. getting-started/models/models/sonar-pro. Sonar pro: Model https://docs.perplexity.ai/ Hossein Rahmani, Xi Wang, Yue Feng, Qiang Zhang, Emine Yilmaz, and Aldo Lipani. 2023. survey on asking clarification questions datasets in conversational systems. arXiv preprint arXiv:2305.15933. 10 W. Shen and 1 others. 2025. Skywork-r1v3 technical report. arXiv. Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, and Seunghyeok Hong. 2024. Llm-as-a-judge & reward model: What they can and cannot do. arXiv preprint arXiv:2409.11239. Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. 2025. Kmmlu: Measuring massive multitask language understanding in korean. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 40764104. Guijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jaecheol Lee, Je Won Yeom, Jihyu Jung, Jung Woo Kim, and Songseong Kim. 2023. Hae-rae bench: Evaluation of korean knowledge in language models. arXiv preprint arXiv:2309.02706. Weiyun Wang and 1 others. 2025. Internvl 3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv. xAI. 2025. Grok 4: Model card. https://data.x.ai/ 2025-08-20-grok-4-model-card.pdf. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Kyung-Min Yoo and 1 others. 2024. Hyperclova technical report. arXiv. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2024. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International conference on machine learning. PMLR. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, and 1 others. 2024. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9556 9567. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, and 1 others. 2025. Mmmupro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15134 15186. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830. 11 13 13 13 13 14 14 15 16 16 18 18 21 21 21 21 21 22"
        },
        {
            "title": "A Dataset Construction Details",
            "content": "A.1 Detailed Platform Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Platform-wise Filtering Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B Pipeline Prompts",
            "content": "B.1 Stage 2 (Safety, Objectivity, Temporal) . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Stage 4 Prompt Excerpt (Image Dependency Rubric) . . . . . . . . . . . . . . . . . . . B.3 Stage 5 (Checklist Generation) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Query Explicitation Prompt ."
        },
        {
            "title": "C Human Annotation",
            "content": "C.1 Annotation Guidelines . C.2 LLM Judge Failure Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . LLM-as-Judge Prompt Additional Results & Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1 Full Results . E.2 Performance by Model Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Performance by Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Investigating Failure Modes . . . Error Annotation Methodology . . F.1 Annotation Setup . F.2 Annotation Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Dataset Construction Details",
            "content": "A.1 Detailed Platform Descriptions We collected data from nine Korean online platforms representing diverse user communities and domain expertise. Table 9 provides detailed information about each platform."
        },
        {
            "title": "Description",
            "content": "Naver KnowledgeIn General Q&A"
        },
        {
            "title": "Hardware Community",
            "content": "i-Boss"
        },
        {
            "title": "Coding Education",
            "content": "Koreas largest general Q&A platform covering everyday queries, academic subjects, and technical issues Specialized community for biological research and biotechnology with scientific discussions and professional knowledge sharing Major gaming community covering video games, hardware reviews, game mechanics, and technical gaming issues Fitness and bodybuilding community discussing workout routines, nutrition, supplements, and exercise techniques Hardware enthusiast community focused on computer components, electronics, PC building, and technology reviews Business and entrepreneurship platform for startup strategies, operations, marketing, and professional development Online learning platform with community features for programming questions and coding experiences Coding education platform with forums for programming discussions and technical support Developer Community Developer community platform for programming discussions, career advice, and technical problem-solving Table 8: Korean online platforms used for data collection These platforms were selected to ensure comprehensive coverage of different user demographics, expertise levels, and domain-specific knowledge, reflecting the diversity of real-world multimodal questions Korean users encounter online. A.2 Platform-wise Filtering Statistics Table 9 provides detailed breakdown of data collection and filtering across all platforms."
        },
        {
            "title": "Platform",
            "content": "Raw Data Appropri. Difficulty Image Dep. Human Val."
        },
        {
            "title": "Survival",
            "content": "KnowledgeIn BRIC Ruliweb Coding MonsterZym Quasarzone i-Boss"
        },
        {
            "title": "Total",
            "content": "31,484 291 305 27,896 3,090 2,986 20,000 86,052 10,495 291 240 8,369 3,090 896 20,000 43,381 1,404 163 54 837 2,234 90 578 5, 648 60 42 198 8 22 62 1,040 441 42 32 135 6 15 42 713 441 42 32 135 6 15 42 1.4% 14.4% 10.5% 0.5% 0.2% 0.5% 0.2% 0.76% Table 9: Detailed data collection and filtering statistics by platform (Stages 16). Coding platforms include Inflearn, Codeit, and Okky combined."
        },
        {
            "title": "B Pipeline Prompts",
            "content": "B.1 Stage 2 (Safety, Objectivity, Temporal) We used three LLM-based filters in Stage 2: content safety, objectivity, and temporal dependency. Below we excerpt only the core exclusion criteria from the prompts (full wording omitted). 13 B.1.1 Content Safety Mark as inappropriate if the questionimage pair includes: Political content (politicians, parties, elections, political opinions) Religious advocacy/criticism or conflicts Hate/discrimination Suicide or self-harm; sensitive mental-health topics Sexual/adult content, nudity, explicit innuendo B.1.2 Objectivity Mark as inappropriate if the pair is subjective or ambiguous, e.g.: Preference/aesthetic judgments (pretty/ugly, which outfit is nicer?) Suitability/personal advice without criteria Moral/intentionality speculation (who is wrong?, good person?) Multiple valid interpretations or unverifiable answers B.1.3 Temporal Dependency Mark as inappropriate if the pair requires time-specific information, e.g.: today/now weather, traffic, store hours, last train Current events or status queries (is it open now?, stock price today?) Questions that become invalid/meaningless as time passes B.2 Stage 4 Prompt Excerpt (Image Dependency Rubric) Input: (Q), model answer with image, model answer without image, optional gold answer snippet. Task: Compare the two answers and decide image dependency. Decision labels IMAGE_REQUIRED: with-image answer is substantially more accurate/specific; text-only answer is vague, incorrect, or explicitly requests the image. NO_IMAGE_NEEDED: both answers are comparable in correctness and specificity without relying on visual cues. UNCERTAIN: evidence is inconclusive (e.g., partial improvements or conflicting signals). Scoring (15 quality gap) 1: negligible difference; 3: clear but moderate gain; 5: decisive gain (critical visual details). Output (natural language) Judgment: IMAGE_REQUIRED / NO_IMAGE_NEEDED / UNCERTAIN Reason: brief comparison citing concrete differences QualityGap: integer in {1,2,3,4,5} B.3 Stage 5 (Checklist Generation) This appendix provides the instruction prompt used for checklist generation along with illustrative examples of the resulting decompositions. We used GPT-4-mini to derive structured criteria directly from reference answers that users found satisfactory. These checklists therefore represent strict, human-aligned evaluation standards: model must satisfy all listed criteria to be considered correct. 14 Game (Stardew Valley) What is the circled item in the screenshot? Identify circled item as sap tap (수액채취기) Economics/Management Cost allocation: is S2 missing 100,000? Provide correct S1/S2 values Mention install only on fully grown trees Reset self-allocation entries to zero Explain how to obtain/craft it Note sap can be collected after time Daily Life Is this ceiling tile asbestos? Identify material as gypsum, not asbestos Explain gypsum board contains no asbestos Explicitly name 석고텍스 Assure user it is safe Derive allocation ratios (0.5F, 0.4M) Science Why does neutron mass ratio decrease? Explain neutron beta decay Clarify neutrons inside He nucleus Relate x-axis to cosmic cooling Interpret H:He ratio 3:1 Figure 7: Examples of checklist decomposition across domains, generated in Stage 5. For brevity, the checklists shown here are abbreviated; full checklists typically contain 15 criteria per item. B.4 Query Explicitation Prompt The following prompt was used with GPT-5.1 (web search enabled) to generate explicitated versions of under-specified queries. You rewrite incomplete, ambiguous, or context-dependent questions into clear, fully self-contained questions. Your goal is to produce rewritten question that can be understood and answered on its own, without requiring prior conversation or hidden context. Preserve the original intent, scope, and tone of the question. Do NOT answer the question. Rules: 1. Intent and scope preservation Preserve what the original question is asking and its level of specificity. Do not broaden or narrow the scope. Do not generalize away concrete entities or situations implied by the original question or answer. 2. Essential context inclusion Explicitly include essential context if it is implied or required to understand the question, such as: the relevant domain or subject; the specific scenario, task, or situation involved; named entities (e.g., people, organizations, characters, locations); concrete objects, items, or targets referenced. Avoid vague references such as this, that, here, the scene, or the above. 3. Search usage You may use search ONLY to identify widely accepted proper nouns (e.g., titles, names, commonly used labels) that are strongly implied by the original question or associated answer. Do NOT use search to introduce new mechanics, steps, conditions, quantities, or interpretations. Do NOT resolve ambiguity by inventing details. 4. Handling missing or ambiguous information If critical context cannot be inferred with high confidence, include brief clarifying placeholder inside the question, such as: [SPECIFY: missing detail]. Do not attempt to guess or fix the question beyond what the inputs support. 5. Image usage (if an image is provided) You may incorporate information visible in the image to clarify the question. The rewritten question must remain answerable without viewing the image. Do not exhaustively describe the image or convert all visual details into text. Include only visual information that is essential to understanding the question. 6. Language and style Maintain tone consistent with the original question. Do not unnecessarily formalize or casualize the language. Remove slang, conversational fillers, and vague references that reduce clarity. Output requirements: Output ONLY the rewritten question text. No explanations, no bullet points, no headers. Do not include meta-instructions or commentary."
        },
        {
            "title": "C Human Annotation",
            "content": "C.1 Annotation Guidelines Seven Korean-speaking annotators conducted human validation in three phases using custom web-based tools. C.1.1 Phase 1: Conservative Filtering Using the annotation interface shown in Figure 8, annotators independently reviewed each item along five dimensions, removing any item flagged by at least one annotator: Image-Question Relevance: Assess whether images provide essential visual information required to answer the question. Question-Answer Quality: Evaluate question clarity, answerability, and reference answer accuracy. Checklist Validation: Review each LLM-generated checklist item for necessity, clarity, and completeness. Category Appropriateness: Verify correct classification into one of 13 domain categories. Overall Assessment: Flag items with fundamental issues such as inappropriate content or unsolvable questions. C.1.2 Phase 2: Refinement Three annotators refined surviving items through separate annotation interface: Question Rewriting: Rewrite unclear or ambiguous questions while preserving original intent and scope. Checklist Revision: Evaluate each LLM-generated checklist item for appropriateness, revising unclear criteria or removing items not grounded in the original questionimage pair. Category Re-assignment: Re-assign categories where the original classification was incorrect, with option to propose new categories. C.1.3 Phase 3: Final Audit One senior annotator consolidated categories across the dataset and verified cross-item consistency. 16 Figure 8: Screenshot of our Phase 1 annotation tool. The interface (shown in Korean) allowed annotators to assess image relevance, question/answer appropriateness, checklist accuracy, and category assignment. 17 C.2 LLM Judge Failure Cases Table 10 presents representative examples of human annotator feedback for inappropriate judge evaluations, revealing systematic failure patterns."
        },
        {
            "title": "Rating",
            "content": "Human Reasoning (translated)"
        },
        {
            "title": "Very Inappropriate",
            "content": "\"Judge awarded points based on superficial word matching rather than actual checklist compliance\""
        },
        {
            "title": "Inappropriate",
            "content": "\"Judge gave 1 point despite response not addressing checklist criteria, incorrectly interpreting explicit mention as meeting requirements\" \"Checklists 1,2,4 satisfied. Item 3 not clearly inappropriate but ambiguous and open to interpretation\""
        },
        {
            "title": "Inappropriate",
            "content": "\"Even if intent aligns with checklist, response lacks clarity and remains ambiguous\""
        },
        {
            "title": "Inappropriate",
            "content": "\"Judge overlooked insufficient explanations that clearly failed checklist requirements\" Table 10: Representative human feedback explaining inappropriate judge ratings. Analysis reveals judge failures primarily stem from: (1) superficial keyword matching without semantic understanding, (2) excessive leniency toward incomplete responses, and (3) difficulty distinguishing between implicit intent and explicit satisfaction of requirements. LLM-as-Judge Prompt This appendix provides the full prompt used for the checklist-based evaluation by the GPT-5-Mini judge. The prompt enforces explicitness, evidence grounding, and consistent scoring across items. For reproducibility, we include the full decision rules, evidence policy, and output format constraints. [GOAL] Given Question, Response, and natural-language Checklist, decide for each checklist item whether the Response explicitly satisfies it: met = 1, partially met = 0.5, not met = 0. Final score = (# met) / (total checklist items). [INPUT] [Question] QUESTION [Response] RESPONSE [Checklist] CHECKLIST Treat each string as one criterion. Remove numbering such as \"1.\" or \"2)\". [DECISION RULES] Use only the Response text. No outside knowledge or assumptions. If uncertain 0. Explicitness: direct fulfillment = 1, implicit or suggestive = 0.5, otherwise = 0. Completeness (\"all / every / complete\"): explicit = 1, implied = 0.5, absent = 0. Method requirements: actionable steps = 1, vague = 0.5, absent = 0. \"Various / multiple types\": 2 specific types = 1, vague or 1 type = 0.5, none = 0. Synonyms: unambiguous = 1, ambiguous = 0.5, different meaning = 0. [EVIDENCE POLICY] For 1 or 0.5: include 1060 character direct quote. For 0: provide brief explanation. Each item must include: evidence explanation met. [OUTPUT FORMAT STRICT] <evidence> Item 1: evidence: \". . . direct quote from Response. . . \" 18 explanation: Brief justification referencing criteria met: 0 0.5 1 Item 2: evidence: \". . . \" explanation: . . . met: 0 0.5 1 ... (repeat) </evidence> <score> K/N </score> [NOTES] Output only the two blocks above. No code fences or additional prose. 19 e C u s T h p S i c c c a h e H"
        },
        {
            "title": "T\nI",
            "content": "l M 8 1 . 0 4 5 . 8 4 8 3 . 1 5 0 . 1 4 2 4 . 0 9 2 . 0 3 2 3 . 0 1 0 . 8 4 1 2 . 1 1 2 . 5 6 4 . 0 2 2 . 1 2 3 5 . 0 8 0 . 6 3 2 9 . 0 1 9 . 0 5 5 2 . 2 6 2 . 9 3 6 6 . 2 3 6 . 2 3 4 2 . 1 7 0 . 5 3 5 . 0 2 0 . 7 5 0 8 . 4 4 8 . 2 3 0 2 . 0 2 0 . 9 3 7 5 . 0 5 4 . 3 5 9 9 . 0 7 3 . 4 4 2 3 . 0 6 1 . 7 3 2 . 1 2 4 . 3 5 3 7 . 2 7 1 . 1 4 0 6 . 1 7 2 . 5 2 8 2 . 1 9 2 . 0 3 1 8 . 2 9 6 . 7 5 7 3 . 2 2 1 . 8 8 7 . 3 0 1 . 4 3 3 4 . 2 7 9 . 4 5 1 1 . 3 2 2 . 9 4 3 6 . 1 7 1 . 6 2 0 9 . 4 3 4 . 0 5 8 1 . 1 7 3 . 4 3 6 . 3 2 9 . 1 3 8 6 . 0 2 8 . 8 1 0 2 . 2 0 8 . 3 3 0 2 . 2 4 5 . 5 2 7 6 . 0 1 8 . 2 1 8 7 . 1 7 7 . 4 6 3 . 1 4 9 . 0 6 7 8 . 3 4 1 . 1 5 9 4 . 7 2 6 . 5 4 4 5 . 4 0 7 . 4 5 4 9 . 3 9 1 . 8 5 2 1 . 6 6 5 . 3 6 8 . 2 9 0 . 2 2 5 1 . 5 0 0 . 6 4 9 1 . 5 3 1 . 6 4 8 3 . 2 2 8 . 8 2 4 1 . 3 6 9 . 5 5 6 9 . 4 3 3 . 4 6 2 . 3 2 4 . 6 2 5 6 . 1 3 7 . 6 3 7 5 . 2 2 3 . 1 5 0 1 . 2 9 5 . 4 4 8 8 . 0 7 6 . 0 3 3 8 . 2 3 9 . 6 5 7 . 4 8 2 . 0 5 1 4 . 1 9 4 . 1 2 0 6 . 1 7 5 . 0 4 2 3 . 4 2 2 . 6 5 0 7 . 1 6 8 . 5 4 0 1 . 3 0 3 . 8 2 7 . 4 5 8 . 2 5 7 4 . 1 8 6 . 1 5 5 4 . 7 9 5 . 6 2 6 8 . 1 3 6 . 7 4 3 4 . 1 3 9 . 9 3 4 1 . 3 2 6 . 9 7 4 . 1 3 7 . 1 4 0 6 . 1 1 3 . 6 3 4 4 . 0 9 4 . 2 5 1 2 . 1 7 4 . 1 1 3 1 . 1 0 7 . 0 4 2 7 . 1 9 0 . 1 4 6 . 6 5 0 . 8 4 2 8 . 3 4 8 . 8 3 3 6 . 1 1 4 . 8 5 3 5 . 5 9 1 . 0 5 5 6 . 0 1 8 . 4 2 9 7 . 2 4 4 . 4 5 7 . 1 7 6 . 6 3 4 0 . 1 0 7 . 6 2 2 9 . 1 7 9 . 7 1 8 0 . 2 4 3 . 2 3 1 7 . 1 2 2 . 9 2 1 7 . 1 6 4 . 0 9 4 . 1 0 0 . 9 2 8 9 . 0 6 8 . 4 2 1 4 . 0 0 1 . 0 2 1 4 . 0 3 4 . 4 1 2 0 . 0 0 2 . 1 1 5 3 . 2 9 0 . 1 7 6 . 0 4 6 . 2 2 6 4 . 2 6 3 . 6 1 0 8 . 2 4 9 . 2 1 2 6 . 2 2 6 . 9 1 2 6 . 0 8 0 . 8 1 7 7 . 1 1 6 . 0 6 4 . 0 0 6 . 9 5 6 . 2 6 4 . 5 2 4 3 . 1 3 9 . 1 2 9 4 . 3 4 8 . 3 1 9 3 . 2 6 0 . 5 1 8 2 . 0 3 5 . 2 3 6 . 0 6 7 . 8 1 8 7 . 0 7 4 . 5 1 5 1 . 2 1 8 . 0 2 3 1 . 0 1 4 . 3 2 8 8 . 0 5 4 . 3 1 0 4 . 0 1 6 . 8 7 4 . 1 3 4 . 3 1 8 0 . 1 8 6 . 4 1 2 6 . 2 6 6 . 0 2 5 6 . 1 1 0 . 1 2 2 1 . 2 6 5 . 9 1 3 0 . 0 0 5 . 8 0 5 . 0 8 1 . 7 1 7 3 . 0 6 4 . 4 1 1 1 . 0 8 1 . 2 1 2 2 . 0 4 5 . 9 5 2 . 0 2 5 . 6 6 2 . 0 8 7 . 6 2 9 . 1 2 1 . 6 1 0 8 . 1 7 7 . 4 1 0 0 . 1 1 3 . 8 8 8 . 3 8 9 . 8 3 1 . 1 3 4 . 4 7 3 . 1 7 7 . 8 3 6 . 0 7 4 . 6 1 5 3 . 0 7 1 . 3 1 8 5 . 0 6 2 . 4 1 6 5 . 0 3 7 . 9 8 3 . 1 0 8 . 6 9 3 . 0 1 0 . 8 7 3 . 0 4 0 . 6 1 2 8 . 0 6 1 . 3 1 8 2 . 0 9 0 . 4 1 9 4 . 0 8 4 . 9 3 1 . 0 3 4 . 5 2 1 . 2 6 7 . 4 3 1 . 3 2 7 . 9 1 2 2 . 1 1 0 . 3 1 5 2 . 2 0 9 . 4 1 8 2 . 2 7 0 . 9 3 4 . 0 4 6 . 5 8 9 . 1 1 4 . 6 7 0 . 1 5 0 . 2 1 5 2 . 0 7 5 . 2 1 7 3 . 1 8 7 . 1 1 1 5 . 0 3 0 . 7 4 7 . 0 3 9 . 2 9 8 . 2 6 8 . 1 4 8 . 0 1 2 . 0 2 8 0 . 3 2 4 . 7 1 1 0 . 3 5 6 . 7 1 9 1 . 2 3 3 . 1 1 5 5 . 2 0 1 . 8 4 8 . 0 0 1 . 1 8 5 . 1 6 7 . 5 1 3 7 . 1 8 6 . 2 1 6 3 . 1 3 8 . 3 1 4 1 . 3 4 1 . 0 1 5 0 . 1 1 3 . 7 0 8 . 0 8 5 . 0 6 8 . 0 5 1 . 3 1 6 3 . 0 0 2 . 8 9 5 . 1 1 3 . 5 2 6 8 . 2 8 2 . 3 1 4 8 . 0 4 5 . 6 9 7 . 0 6 3 . 9 4 7 . 0 5 3 . 2 1 8 3 . 0 5 8 . 7 2 6 . 2 2 0 . 1 2 6 6 . 1 6 0 . 4 1 6 4 . 3 1 3 . 0 1 6 7 . 0 1 4 . 8 5 7 . 0 7 4 . 5 3 3 7 . 0 8 0 . 6 3 4 7 . 0 9 4 . 5 3 4 7 . 0 1 4 . 5 3 0 7 . 0 2 9 . 0 3 7 6 . 0 1 0 . 8 4 6 . 0 1 5 . 4 2 5 6 . 0 8 1 . 6 2 6 5 . 0 5 0 . 8 1 7 4 . 0 7 8 . 3 1 3 4 . 0 5 1 . 1 1 3 2 . 3 0 1 . 0 6 1 . 3 7 8 . 4 3 6 0 . 3 5 2 . 2 3 4 0 . 3 9 5 . 7 3 4 0 . 3 5 9 . 9 2 2 9 . 2 5 6 . 9 2 0 8 . 2 0 2 . 4 7 7 . 2 7 0 . 5 2 4 5 . 2 0 6 . 2 2 3 7 . 2 3 8 . 0 2 8 8 . 1 6 3 . 2 1 2 1 . 2 4 0 . 4 1 4 7 . 1 4 4 . 7 0 7 . 1 4 0 . 3 3 6 6 . 1 3 4 . 4 3 2 7 . 1 4 0 . 5 3 6 6 . 1 6 4 . 5 3 9 4 . 1 7 1 . 5 2 9 5 . 1 5 7 . 8 1 3 . 1 0 4 . 0 2 9 3 . 1 1 3 . 5 2 0 1 . 1 3 2 . 1 1 6 0 . 1 1 8 . 3 1 6 6 . 0 8 8 . 5 2 9 . 2 3 3 . 6 0 8 . 2 2 1 . 4 3 5 1 . 3 2 0 . 6 3 4 1 . 3 7 5 . 5 3 3 6 . 2 0 4 . 7 3 4 5 . 2 2 2 . 0 3 0 3 . 2 0 9 . 4 0 6 . 2 1 4 . 4 2 2 4 . 2 5 8 . 3 2 7 9 . 1 0 4 . 8 1 2 2 . 2 1 6 . 7 1 6 9 . 1 3 8 . 2 1 1 4 . 1 8 1 . 7 5 . 1 2 7 . 8 3 5 . 0 4 0 . 8 0 5 . 0 7 1 . 7 4 6 . 0 8 2 . 7 8 9 . 0 5 0 . 5 7 1 . 0 3 5 . 3 5 . 0 4 7 . 7 9 7 . 0 9 2 . 5 6 9 . 0 8 4 . 6 6 2 . 1 3 6 . 5 7 5 . 0 4 2 . 3 5 4 . 0 7 9 . 8 2 . 0 1 7 . 6 8 1 . 0 8 8 . 4 2 4 . 2 5 7 . 2 2 2 3 . 2 9 2 . 0 2 7 8 . 1 7 6 . 9 1 5 1 . 2 9 1 . 9 3 1 . 2 1 8 . 7 1 6 6 . 1 2 1 . 5 1 0 7 . 1 0 1 . 4 1 3 4 . 1 6 6 . 0 1 9 8 . 1 2 9 . 4 1 6 2 . 1 9 6 . 8 3 . 1 3 0 . 9 0 0 . 1 3 4 . 6 6 4 . 2 1 9 . 2 2 1 3 . 1 2 3 . 4 2 9 2 . 5 1 8 . 9 1 4 8 . 0 6 1 . 3 2 3 . 1 7 5 . 6 0 4 . 2 7 5 . 6 7 4 . 2 4 5 . 4 2 3 6 . 1 5 4 . 6 2 8 0 . 2 6 9 . 2 2 4 6 . 2 2 7 . 5 1 7 . 1 8 1 . 4 1 3 5 . 1 2 2 . 0 1 1 3 . 3 7 2 . 0 2 0 0 . 3 3 8 . 4 1 3 3 . 1 1 5 . 3 1 9 8 . 2 8 6 . 0 1 0 . 2 7 2 . 1 1 7 9 . 1 1 1 . 8 8 5 . 0 3 4 . 9 1 2 5 . 3 0 2 . 1 2 3 8 . 1 7 3 . 8 1 1 5 . 1 3 4 . 6 4 1 . 1 9 9 . 1 1 8 9 . 0 3 0 . 8 9 1 . 0 0 4 . 8 1 9 0 . 3 0 7 . 7 1 2 9 . 2 9 9 . 1 1 3 0 . 2 8 9 . 4 0 6 . 1 9 6 . 7 0 3 . 1 9 7 . 7 0 9 . 1 5 2 . 9 1 8 9 . 1 7 6 . 4 1 6 5 . 1 7 2 . 1 1 2 5 . 1 1 1 . 5 4 5 . 1 4 9 . 8 2 0 . 1 9 2 . 3 7 4 . 1 6 4 . 9 2 0 5 . 3 4 9 . 3 2 8 9 . 1 2 0 . 1 2 5 1 . 3 5 0 . 7 3 4 . 4 4 6 . 3 1 2 1 . 1 5 9 . 5 4 4 . 4 0 9 . 0 2 5 9 . 3 1 4 . 7 1 5 3 . 3 6 1 . 6 1 2 4 . 0 4 8 . 8 7 8 . 2 9 6 . 2 1 8 5 . 0 4 7 . 5 2 5 . 1 3 9 . 4 3 9 3 . 1 9 7 . 8 2 2 1 . 0 0 5 . 2 2 6 5 . 0 7 9 . 3 0 8 . 0 8 6 . 4 2 5 8 . 1 5 4 . 9 1 7 1 . 1 6 1 . 6 1 0 7 . 1 4 2 . 2 1 1 7 . 0 2 5 . 7 7 2 . 0 9 7 . 8 8 1 . 1 4 6 . 4 2 9 8 . 1 4 1 . 0 2 4 2 . 1 0 2 . 3 2 7 8 . 0 5 0 . 6 1 2 3 . 0 2 1 . 9 5 4 . 0 4 4 . 9 7 2 . 1 8 5 . 1 2 8 2 . 3 8 1 . 9 1 6 7 . 2 9 8 . 5 1 4 3 . 2 4 7 . 3 1 7 0 . 1 7 0 . 8 2 5 . 1 5 8 . 4 6 9 . 0 2 7 . 0 2 7 8 . 1 4 4 . 0 2 0 6 . 1 1 7 . 9 1 6 8 . 1 0 5 . 5 1 7 0 . 0 2 3 . 0 1 7 7 . 0 7 8 . 8 1 . 0 8 0 . 8 0 3 . 0 7 0 . 6 7 2 . 0 0 0 . 6 3 8 . 0 0 3 . 5 1 3 . 0 4 7 . 4 7 5 . 1 9 0 . 2 1 . 1 6 2 . 8 7 5 . 0 4 1 . 7 2 5 . 0 2 7 . 7 4 3 . 0 4 2 . 5 9 0 . 0 9 3 . 3 2 2 . 7 2 3 . 2 7 0 . 4 6 2 . 3 1 8 9 . 0 4 1 . 0 1 4 1 . 3 6 8 . 5 2 0 0 . 4 0 0 . 7 1 7 4 . 1 9 8 . 6 8 3 . 0 2 7 . 9 2 9 . 0 0 7 . 3 1 6 1 . 0 2 2 . 9 2 4 . 1 5 5 . 3 2 9 7 . 3 0 5 . 8 1 1 5 . 2 6 7 . 3 1 2 6 . 2 6 4 . 8 9 8 . 3 9 4 . 0 2 0 7 . 1 4 5 . 1 1 6 0 . 1 5 5 . 5 2 5 0 . 1 6 9 . 8 1 7 1 . 1 6 7 . 2 1 8 0 . 4 1 3 . 4 5 9 . 3 1 5 . 6 5 7 7 . 3 2 9 . 0 6 5 6 . 3 7 5 . 4 6 1 7 . 3 9 6 . 8 6 4 9 . 3 8 3 . 1 5 1 9 . 3 3 8 . 7 1 4 . 3 4 9 . 5 3 1 2 . 4 9 6 . 6 4 6 5 . 3 0 0 . 5 3 7 2 . 3 7 9 . 6 2 3 3 . 2 7 7 . 2 1 3 0 . 4 8 9 . 0 8 6 . 3 8 1 . 4 3 2 7 . 3 8 3 . 3 4 6 8 . 3 6 1 . 8 3 7 8 . 3 9 9 . 7 3 0 8 . 3 3 5 . 6 3 7 8 . 3 3 5 . 2 7 4 . 3 8 9 . 7 2 1 3 . 3 9 2 . 1 3 5 8 . 2 5 3 . 1 2 9 8 . 2 9 0 . 6 1 2 6 . 2 2 7 . 4 1 5 6 . 2 4 1 . 9 9 4 . 2 2 0 . 0 3 7 4 . 2 8 2 . 4 3 0 4 . 2 8 3 . 9 2 5 6 . 2 4 3 . 2 3 5 7 . 2 4 9 . 1 3 7 3 . 2 0 7 . 6 9 4 . 2 7 2 . 5 2 5 3 . 2 9 8 . 4 2 1 9 . 1 7 5 . 8 1 7 4 . 1 8 5 . 3 1 3 3 . 1 7 1 . 9 0 0 . 4 2 4 . 7 7 9 . 3 7 3 . 8 3 6 7 . 3 3 7 . 1 4 3 5 . 3 6 6 . 5 3 1 8 . 3 8 4 . 8 3 3 5 . 3 8 1 . 4 3 9 6 . 3 3 7 . 9 5 9 . 2 6 4 . 7 2 9 3 . 3 2 7 . 9 2 7 0 . 3 2 6 . 1 2 8 1 . 2 2 3 . 3 1 5 9 . 1 1 2 . 2 1 6 7 . 1 1 5 . 1 1 9 . 1 9 1 . 9 4 6 8 . 1 9 3 . 1 5 5 7 . 1 4 9 . 1 5 7 8 . 1 2 9 . 9 4 6 6 . 1 0 4 . 0 4 8 7 . 1 7 0 . 2 0 7 . 1 5 8 . 4 3 5 7 . 1 3 5 . 8 3 5 3 . 1 2 0 . 3 2 9 3 . 1 3 5 . 7 1 7 2 . 1 3 4 . 7 1 5 4 . 3 6 1 . 3 2 1 . 3 0 3 . 7 4 8 1 . 3 9 2 . 1 4 6 9 . 2 1 2 . 8 3 1 2 . 3 4 4 . 8 3 9 2 . 3 2 0 . 2 4 0 7 . 2 2 9 . 3 9 9 . 2 7 9 . 9 2 6 8 . 2 6 6 . 0 3 2 0 . 3 4 0 . 1 3 4 2 . 2 7 6 . 9 1 2 2 . 2 2 8 . 9 1 4 2 . 2 0 8 . 1 6 2 . 1 5 9 . 5 4 4 . 0 2 8 . 2 3 9 . 1 8 2 . 3 2 7 8 . 1 7 9 . 3 2 2 6 . 1 3 1 . 8 1 2 7 . 1 6 7 . 9 3 7 . 1 6 0 . 8 1 5 6 . 1 6 8 . 8 1 7 1 . 1 1 2 . 1 1 2 5 . 1 1 6 . 3 1 5 3 . 1 3 8 . 2 1 2 3 . 1 4 9 . 7 9 . 0 3 4 . 5 6 8 . 0 8 2 . 5 2 3 . 2 6 4 . 1 1 0 5 . 2 4 5 . 1 1 9 0 . 1 6 7 . 7 5 3 . 0 7 1 . 1 4 . 3 9 9 . 8 3 0 1 . 1 9 8 . 9 1 1 8 . 1 7 5 . 8 1 7 2 . 2 3 8 . 6 1 5 . 2 2 2 . 5 2 6 6 . 4 1 0 . 9 8 6 . 3 4 3 . 5 1 4 3 . 1 4 4 . 1 1 6 0 . 1 1 4 . 4 2 9 0 . 1 6 1 . 4 2 7 6 . 2 7 0 . 3 1 0 4 . 2 6 6 . 1 0 2 . 1 4 1 . 4 3 1 2 . 1 1 4 . 3 2 1 0 . 2 3 5 . 1 2 5 6 . 2 6 4 . 6 1 3 2 . 1 0 7 . 3 3 9 2 . 2 0 4 . 7 6 4 . 1 7 4 . 0 2 1 1 . 0 2 1 . 4 1 8 4 . 2 8 4 . 8 2 6 3 . 3 2 3 . 4 2 4 8 . 1 9 2 . 2 2 3 5 . 0 9 4 . 7 9 4 . 1 1 0 . 6 1 3 3 . 1 3 3 . 1 1 5 3 . 1 0 0 . 7 4 9 . 0 4 7 . 6 0 3 . 1 3 4 . 9 2 1 . 1 3 2 . 0 1 . 1 4 8 . 6 4 7 . 2 5 8 . 3 2 9 8 . 2 5 2 . 7 1 3 6 . 2 4 5 . 3 1 1 0 . 2 7 0 . 4 2 7 4 . 1 0 4 . 2 1 2 . 1 1 2 . 8 1 2 1 . 1 2 7 . 6 2 7 2 . 1 0 2 . 9 1 7 8 . 0 9 5 . 5 1 8 2 . 6 6 3 . 7 2 0 6 . 3 4 4 . 7 3 3 . 1 5 2 . 1 2 1 2 . 3 1 7 . 1 3 0 3 . 1 1 9 . 7 2 7 3 . 4 7 6 . 9 1 5 5 . 1 5 7 . 3 1 4 4 . 1 2 5 . 0 6 9 . 0 1 9 . 8 9 4 . 1 0 9 . 0 4 2 3 . 1 0 6 . 6 3 8 0 . 1 3 2 . 4 3 8 1 . 1 1 3 . 0 2 9 6 . 0 5 1 . 5 3 6 . 1 3 4 . 2 1 3 3 . 2 7 1 . 2 6 5 9 . 3 0 1 . 6 5 8 4 . 4 4 5 . 3 4 9 5 . 2 1 6 . 2 6 0 4 . 2 5 4 . 0 2 0 . 1 8 9 . 5 4 6 1 . 1 6 9 . 6 3 3 6 . 1 3 7 . 0 5 4 3 . 0 8 9 . 2 4 2 8 . 1 2 9 . 5 2 1 0 . 2 5 9 . 9 4 7 . 3 9 5 . 9 4 4 6 . 2 9 9 . 2 2 9 8 . 1 4 6 . 9 3 6 9 . 5 1 0 . 7 3 3 3 . 2 9 0 . 5 3 5 2 . 4 7 0 . 5 8 5 . 3 9 1 . 4 2 6 7 . 2 7 7 . 4 2 2 8 . 1 9 0 . 9 1 3 9 . 1 8 3 . 5 1 7 7 . 0 6 7 . 8 6 1 . 2 5 1 . 0 5 7 . 0 8 9 . 8 3 4 5 . 1 2 6 . 3 3 3 9 . 3 6 6 . 3 2 1 0 . 1 0 1 . 6 1 8 5 . 2 2 6 . 2 1 2 8 . 4 5 9 . 0 6 4 . 4 1 8 . 6 2 2 1 . 3 1 1 . 3 2 0 3 . 0 3 3 . 3 2 3 1 . 4 6 8 . 0 2 9 9 . 2 4 9 . 7 8 3 . 1 0 3 . 1 1 5 . 4 4 0 . 1 2 0 9 . 3 9 4 . 8 1 6 9 . 3 4 4 . 4 5 1 0 . 4 2 1 . 2 5 8 7 . 3 0 3 . 6 5 8 2 . 4 9 3 . 2 0 5 . 3 8 3 . 6 5 6 4 . 3 1 7 . 4 5 1 6 . 3 5 5 . 9 4 8 8 . 3 5 6 . 5 4 1 7 . 3 7 0 . 5 4 4 8 . 2 0 0 . 1 3 1 . 3 8 5 . 4 2 7 0 . 3 1 7 . 9 1 5 3 . 1 0 9 . 5 1 7 6 . 1 0 2 . 1 1 0 3 . 1 0 8 . 9 5 7 . 1 6 7 . 2 2 . 0 4 1 . 6 1 9 . 0 3 8 . 4 3 6 . 0 4 9 . 4 1 5 0 . 2 0 5 . 5 1 8 0 . 1 2 2 . 0 1 5 1 . 1 0 7 . 5 2 . 0 2 3 . 5 3 4 . 0 1 2 . 3 6 3 . 1 3 5 . 6 1 0 7 . 0 3 3 . 0 1 5 1 . 2 8 0 . 6 9 2 . 2 5 7 . 7 8 3 . 2 9 1 . 4 3 7 1 . 2 4 7 . 6 3 6 3 . 2 2 9 . 3 3 3 2 . 2 9 1 . 6 3 6 2 . 2 3 1 . 1 3 2 1 . 2 7 2 . 8 7 0 . 2 1 3 . 5 2 8 0 . 2 3 2 . 4 2 1 9 . 1 3 2 . 0 2 3 3 . 1 1 8 . 1 1 3 5 . 1 3 1 . 1 1 i s 5 . 2 l 5 . 2 5 . 2 m n G m G"
        },
        {
            "title": "5\nT\nP\nG",
            "content": "s o a r P M 5 o 5 4 G i F t / t s o r - O 1 . 3 d e L t l x 4 2 m r i 2 1 t y a m l G 7 2 2 1 3 4 3 e m m y m 2 O - A 4 3 - 2 O 6 1 - 2 O 8 - 2 O 4 - 2 O 2 - 2 O 1 - 2 O m V - 5 . 2 Q i F - 3 Q 2 7 5 7 5 3 5 . . . 2 Q 2 Q 2 Q 8 3 4 8 4 2 1 5 . 3 r I 5 . 3 r I 5 . 3 r I 5 . 3 r I 5 . 3 r I 5 . 3 r I - k T - 2 2 5 3 2 - - 3 Q u n - - 2 2 5 3 2 - - 3 Q - k T - 3 0 3 - - 3 Q r I - - 3 0 3 - - 3 Q k T - 2 3 - - 3 Q r I - 2 3 - - 3 Q k T - 8 - - 3 Q u n - 8 - - 3 Q k T - 4 - - 3 Q k T - 2 - - 3 Q r I - 2 - - 3 Q u n - 4 - - 3 Q m 5 . 3 r I V p 8 5 . 0 6 7 . 7 2 6 2 . 0 2 4 . 6 2 6 2 . 0 8 3 . 7 2 3 7 . 2 0 1 . 0 3 6 9 . 1 6 7 . 4 1 3 5 . 4 1 7 . 1 5 9 . 1 7 2 . 8 2 3 6 . 2 3 4 . 6 2 0 8 . 1 5 2 . 7 3 9 6 . 0 4 8 . 6 3 4 4 . 2 7 3 . 2 3 3 6 . 1 0 3 . 5 2 9 . 2 4 9 . 7 4 4 7 . 0 2 1 . 7 2 u - O t 8 3 - 3 1 - w 0 5 . 0 5 5 . 5 1 8 1 . 0 6 6 . 2 6 4 . 0 7 8 . 1 1 1 3 . 1 1 3 . 3 1 6 8 . 1 3 5 . 9 7 5 . 3 6 4 . 0 1 7 5 . 0 7 9 . 4 1 3 9 . 0 1 2 . 6 2 7 . 0 3 1 . 2 1 5 7 . 1 4 8 . 8 1 5 7 . 1 0 2 . 4 1 1 0 . 1 1 8 . 2 1 4 6 . 2 0 8 . 7 0 7 . 0 6 1 . 8 6 . 0 1 1 . 8 0 9 . 1 8 6 . 2 1 4 4 . 2 6 8 . 9 5 3 . 5 4 5 . 2 1 0 8 . 2 5 0 . 4 1 3 8 . 3 3 4 . 3 8 0 . 1 8 8 . 2 1 9 0 . 1 9 8 . 1 2 7 6 . 0 8 3 . 3 1 2 3 . 0 0 7 . 2 1 6 1 . 3 6 4 . 3 2 4 1 . 2 0 8 . 5 6 1 . 1 2 2 . 6 1 1 7 . 2 3 0 . 2 2 1 4 . 0 4 5 . 8 1 3 6 . 0 9 7 . 7 1 0 3 . 2 3 8 . 7 1 0 4 . 1 7 1 . 5 6 9 . 0 7 0 . 6 1 5 8 . 0 4 9 . 7 9 4 . 0 3 3 . 6 8 3 . 2 5 9 . 5 8 7 . 4 6 7 . 4 3 8 9 . 2 4 7 . 9 0 5 . 1 4 3 . 1 2 9 7 . 0 0 9 . 1 1 8 9 . 0 2 4 . 8 1 2 . 1 9 0 . 8 d e a p - r 4 1 - 0 . 2 - S C V - 3 - L p 7 . 1 - 0 . 2 - S C - 3 o r r a h i"
        },
        {
            "title": "E\nS",
            "content": "e w ,"
        },
        {
            "title": "E\nS\nn\na\ne\nm\ns\na",
            "content": "d o e e s . ) % e s ( d e l l o i e c 3 1 s c n o p l C : 1 b . ) 3 = ( r d e (a) Performance scaling with model size. Accuracy rises up to 10B parameters but improves more slowly thereafter. (b) Domain-level results. Health/Medical yields the highest accuracy, whereas Entertainment/Gaming remains the most challenging. Figure 9: Scaling and domain-level performance on HAERAE-Vision. Additional Results & Analysis E.1 Full Results Table 11 reports the full category-wise results for the 45 evaluated models; we will continuously update the leaderboard with newly released models. E.2 Performance by Model Scale Grouping models by size tiers (Small 4B, Medium 814B, Large 30B) reveals clear scaling trend: performance improves with size. Large models reach mean score of 0.3009 (95% CI [0.2974, 0.3046]), more than double Medium (0.1460) and triple Small (0.0854). All pairwise differences are significant (permutation 0.001) with large effect sizes (LargeSmall = +0.2155, 0.78), confirming that scaling reliably enhances multimodal reasoning. However, gains become less pronounced beyond about 10B parameters. Accuracy still rises but with smaller marginal improvements (Figure 9a), indicating that scale alone cannot close the gap. Further progress likely depends on advances in reasoning and cultural grounding. At the family level, commercial systems (Gemini, GPT, Sonar) consistently outperform open-weight models (e.g., InternVL3), with effect sizes around = 0.71.2 (e.g., Gemini-2.5-Pro vs InternVL3 0.49, 1.21). Thus, both scaling and architectural or cultural factors jointly drive performance. E.3 Performance by Domain Performance varies widely across the 13 domains (global mean = 0.1987, range 0.11790.332). Health/Medical achieves the highest checklist satisfaction (0.332), followed by Science (0.250), while Entertainment/Arts (0.118) and Gaming (0.119) remain the most challenging. Within all domains, large models (30B) consistently outperform small models (4B) (permutation < 0.05), with the largest gains in Health/Medical ( = +0.189) and Mathematics ( = +0.163). Even in Gaming and Entertainment, scale effects remain positive though absolute performance stays low (Figure 9b). E."
        },
        {
            "title": "Investigating Failure Modes",
            "content": "In Table 11, we observe that VARCO-VISION and HyperCLOVA Xtwo Korean-focused VLMsunderperform multilingual counterparts of similar scale. While the precise reasons remain unclear due to the closed nature of these models and limited information about their training, we propose two possible explanations: (A) Training Data Coverage. Current benchmarks that capture progress on culturally grounded, 21 Root Cause (select one) language cultural_knowledge general_reasoning Failure Category (select 13) Misunderstood Korean grammar, negation, particles, or expressions Lacked Korean-specific cultural/institutional knowledge Understood language and context but failed at reasoning"
        },
        {
            "title": "Fails to identify key objects in the image\nMisinterprets spatial relations",
            "content": "object_recognition spatial_reasoning cultural_concept_mismatch Misunderstands Korean-specific concepts or conventions Refers to the wrong region/entity relative to the question visual_text_grounding Fails to execute multi-step procedures procedural_reasoning Misses explicit facts demanded by the checklist lack_of_explicitness None of the above fit other"
        },
        {
            "title": "Severity",
            "content": "minor moderate severe Almost correct; small missing detail Mixed correctness; partially useful Largely incorrect or misleading Table 12: Error annotation taxonomy. information-deficient queries are scarce. Model developers may not have explicitly emphasized such aspects in their training data, leading to weaker performance on this type of evaluation. (B) Pretraining Scale and Robustness. Robustness to imperfect or fragmented user queries may emerge from exposure to large-scale, diverse pretraining corpora. Larger multilingual models are more likely to encounter noisy, colloquial, or partially specified inputs, thereby preparing them better for benchmarks of this kind."
        },
        {
            "title": "F Error Annotation Methodology",
            "content": "F.1 Annotation Setup We used Claude 3.5 Sonnet as the LLM judge for error annotation, accessed via OpenRouter API with temperature=0.0 and max_tokens=2048. For each error case (model response with score < 1.0), the judge was provided with the original question, gold answer, checklist items, model response, and metadata (source, category, model name, score). F.2 Annotation Prompt System prompt: You are an impartial error analysis assistant for Korean multimodal QA benchmark. Your job is to carefully inspect each example and classify the models failure according to predefined taxonomy. Follow the provided schema exactly. Think step by step, but ONLY return the final JSON object in your response. Do NOT include explanations outside the JSON. Be strict and consistent with the taxonomy definitions. User prompt: You are given one question-answering example from Korean multimodal benchmark, together with models answer and detailed checklist used for scoring. Your goal is to analyze WHY the model failed or was only partially correct. Based on the question, gold answer, checklist, and model answer: 1. Decide the SINGLE most important root cause of failure: language, cultural_knowledge, or general_reasoning 2. Choose 13 failure_categories describing HOW the error manifests 3. Choose severity: minor, moderate, or severe 4. Provide analysis_comment: 23 sentences in Korean explaining why the answer is wrong or incomplete [Output format] Return ONLY single JSON object: {root_cause: ..., failure_categories: [...], severity: ..., analysis_comment: ...} [Metadata] - source: {source} - category: {category} - question_idx: {question_idx} - model_name: {model} - model_score: {score} [Question] {question} [Gold answer] {answer} [Checklist items] {checklist} [Model answer] {model_response}"
        }
    ],
    "affiliations": [
        "AIM Intelligence",
        "Doodlin Corp.",
        "Hankuk University of Foreign Studies",
        "Korea University",
        "Lablup Inc.",
        "NAVER Cloud",
        "OneLineAI",
        "Yonsei University"
    ]
}