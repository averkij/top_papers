{
    "paper_title": "VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation",
    "authors": [
        "Wenhao Wang",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal ($0.29\\%$) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over $1.09$ million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify $1,291$ user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about $1.09$ million video clips. Our experiments reveal that (1) current $16$ text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset is publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO under the CC BY 4.0 License."
        },
        {
            "title": "Start",
            "content": "VideoUFO: Million-Scale User-Focused Dataset for Text-to-Video Generation Wenhao Wang University of Technology Sydney wangwenhao0716@gmail.com Yi Yang* Zhejiang University yangyics@zju.edu.cn 5 2 0 2 ] . [ 1 9 3 7 1 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTubes official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both brief and detailed caption (description). Specifically, through clustering, we first identify 1, 291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset is publicly available here under the CC BY 4.0 License. 1. Introduction Text-to-video generation aims to convert textual descriptions into dynamic visual content. Its applications are extensive and transformative, covering areas from creative media [4] and entertainment [43] to practical domains such as education [6], advertising [46], and assistance [14]. Despite their popularity and usefulness, current text-tovideo models often fail to meet users expectations in real- *Corresponding author. Figure 1. The glowing firefly: (a) generated by Sora [2] and (b) captured in real video. The generated firefly is noticeably different from its real-life counterpart and thus unsatisfying. We attribute this primarily to lack of exposure to such topics. world applications. For example, when we ask Sora [2] to generate video using the prompt firefly is glowing on grasss leaf on serene summer night, it fails to capture the concept of glowing firefly while successfully generating grass and summer night, as shown in Fig. 1 (a). From the data perspective, we infer this is mainly because Sora [2] has not been trained on firefly-related topics, while it has been trained on grass and night. Furthermore, if Sora [2] has seen the video shown in Fig. 1 (b), it will understand what glowing firefly should look like. To this end, this paper presents VideoUFO, the first Video dataset curated specifically based on real-world Users FOcus in text-to-video generation. Such dataset can help improve the alignment between text-to-video models and actual user needs. Specifically, we focus on: (1) curating VideoUFO by analyzing real users interests and scraping relevant videos; (2) comparing the VideoUFO with recent video datasets to highlight differences; and (3) demonstrating how our VideoUFO benefits video generation. The first dataset that aligns with real-world users focus in text-to-video generation. Our key idea is to analyze user-focused topics from user-provided prompts and then search for videos related to these topics. As shown in Fig. 2, the resulted VideoUFO comprises more than 1.09 million video clips spanning 1, 291 user-focused topics. Specifically, we initiate by analyzing user-focused topics, which involves embedding all 1.67 million user-provided prompts from VidProM [35], clustering these embeddings using Kmeans, and generating topic for each cluster with GPT-4o [21], followed by combining similar topics. After obtaining 1 Figure 2. VideoUFO is the first dataset curated in alignment with real-world users focused topics for text-to-video generation. Specifically, the dataset comprises over 1.09 million video clips spanning 1, 291 topics. Here, we select the top 20 most popular topics for illustration. Researchers can use our VideoUFO to train or fine-tune their text-to-video generative models to better meet users needs. these topics, we (1) search for these topics on YouTube, (2) segment the fetched videos into multiple semantically consistent short clips, (3) generate both brief and detailed captions for each clip, (4) filter out clips that do not contain the specific topics, and (5) assign each clip video quality scores that align with human perception. Note that although our current VideoUFO comprises about one million videos, it can be easily scaled up to ten million or more by sourcing additional videos for each topic. The VideoUFO can also be easily extended to the image-to-video domain by using the text and image prompts in TIP-I2V [34]. Differences between VideoUFO and other recent video datasets. Recently, several video datasets have been released, including OpenVid-1M [20], HD-VILA-100M [41], InternVid [38], Koala-36M [33], LVD-2M [40], MiraData [9], Panda-70M [5], VidGen-1M [29], and WebVid-10M [3]. While inheriting their excellent attributes such as large scale, accurate captions, and high resolution our (1) Guided VideoUFO explores some novel directions: by real user focus. Specifically, whereas recent datasets are typically gathered from open-domain sources, we concentrate on topics that are focused by text-to-video users. This feature enables text-to-video models trained on our dataset to better cater to users, while avoiding unneces- (2) sary expansion of the dataset and wasting resources. Introducing new data. Although these recent papers claim to contribute new datasets, most of them primarily introduce new data pipeline that is, they reprocess HDVILA-100M [41] and are subsets of it. While their contributions are useful and meaningful, in theory, generative model that has already been fully fitted on HD-VILA100M [41] would not gain new video knowledge from them. In contrast, we collect new data from YouTube, with only 0.29% of the videos overlapping with existing datasets. (3) Data compliance. When curating VideoUFO, we retrieve videos using YouTubes official API and select only those with Creative Commons license. In contrast, most recent datasets do not explicitly address the regulatory compliance of their data collection process. This feature grants researchers greater flexibility in using our data. 2 Benchmarking current text-to-video models on userfocused topics and demonstrating the effectiveness of our VideoUFO. We present new benchmark to quantify the performance of text-to-video models on user-focused (1) selecting topics. Specifically, our process involves: 10 user-provided prompts per topic; (2) generating video for each prompt; (3) using multimodal large language model to describe each video; and (4) calculating the similarity between the generated descriptions and the original prompts. For each topic, we calculate the average similarity between the 10 prompts and their corresponding descriptions. By sorting these averages, we can identify the worstperforming and best-performing topics for each model. Using this benchmark, we evaluate the performance of current text-to-video models and newly trained text-to-video model on our VideoUFO. Experimental results indicate that (1) current 16 text-to-video models have some poorperforming topics, and (2) our model achieves the highest similarities on worst-performing topics while maintaining performance on the best-performing ones. In conclusion, our key contributions are as follows: 1. We present VideoUFO, the first video dataset curated based on the focus of real text-to-video users. This dataset comprises over 1.09 million video clips spanning 1, 291 user-focused topics. 2. We compare VideoUFO with recent video datasets, highlighting their differences in both fundamental attributes and topics coverage, thereby emphasizing the necessity of our dataset. We also follow best practices in their curation processes to ensure the quality of our dataset. 3. We evaluate current text-to-video models on userfocused topics and observe that simple model trained on our VideoUFO outperforms competing models on worst-performing topics. 2. Related Works Text-to-Video Generation. The introduction of Sora [2] has sparked significant research interest in text-to-video generation. Commercial models such as Movie Gen [23], Veo [27], Kling [1], PixelDance [13], and Seaweed [13] demonstrate strong performance and are increasingly being integrated into various industries. Meanwhile, opensource models like HunyuanVideo [12], LTX-Video [15], CogVideoX [42], Mochi-1 [30], and Pyramidal [8] empower researchers to experiment, customize, and enhance existing frameworks. Although they are powerful, their training data determines the upper limit of the video quality they can generate. In this paper, we propose dataset, VideoUFO, which has the potential to help these models better cater to users preferences in real-world applications. Text-Video Datasets. text-video dataset consists of video clips paired with corresponding textual descriptions or captions. Many text-video datasets, such as OpenVidFigure 3. The semantic distribution of users focused topics. It is visualized by WIZMAP [39]. Please (cid:252) zoom in to see the details. 1M [20], HD-VILA-100M [41], InternVid [38], Koala-36M [33], LVD-2M [40], MiraData [9], Panda-70M [5], VidGen1M [29], and WebVid-10M [3], have been proposed to advance multimodal understanding and generation. However, these datasets are not designed to align with the preferences of text-to-video users, creating gap between training data topics and real-world applications. In contrast, our dataset VideoUFO is collected from user-focus perspective. To better cater users, future researchers can fine-tune text-tovideo models with the VideoUFO or integrate it with existing datasets to develop new models. Preference Alignment in Diffusion Models. While diffusion models have demonstrated impressive capabilities, there is an increasing demand to ensure their outputs align with human preferences. Direct Preference Optimization (DPO) [25], originally proposed to align large language models (LLMs) with human preferences, can also be effectively applied to diffusion models. For example, DiffusionDPO [31] fine-tunes text-to-image models using human comparison data to improve image generation quality. Additionally, VideoDPO [17] introduces comprehensive preference scoring system that evaluates both visual quality and semantic alignment to enhance text-to-video generation. Unlike other approaches that align text-to-image or text-to-video models with attributes such as aesthetics, motion, consistency, and visual appeal, our goal is to improve the performance of text-to-video models on realworld users focused topics. 3. Curating VideoUFO 4 illustrates data point from our million-scale Fig. VideoUFO, which comprises: (1) video clip along with its ID; (2) the topic of the clip; (3) the start and end times 3 Figure 4. Each data point in our VideoUFO includes video clip, an ID, topic, start and end times, brief caption, and detailed caption. Beyond that, we evaluate each clip with six different video quality scores from VBench [7]. within its original video; (4) the corresponding brief and detailed captions; and (5) the quality of the clip. The following provides an explanation of how we curate the VideoUFO. Analyzing text-to-video users focused topics. We aim to analyze the topics that real users focus on or prefer when generating videos from texts. Here, we use VidProM [35] as it is the only publicly available, million-scale text-to-video prompt dataset written by real users. First, we embed all 1.67 million prompts from VidProM into 384-dimensional vectors using SentenceTransformers [26]. Next, we cluster these vectors with K-means. Note that here we preset the number of clusters to relatively large value, i.e., 2, 000, and merge similar clusters in the next step. Finally, for each cluster, we ask GPT-4o [21] to conclude topic (1 2 words). The prompt is shown in the Supplementary (Section 8). After merging singular and plural forms, restoring verbs to their base forms, removing duplicates, and manually verifying the topics removing overly broad ones such as animation, scene, movement, and film we finally obtained 1, 291 topics. The semantic distribution of these topics is shown in Fig. 3, and the full list is available in the Supplementary (Section 12). Collecting videos from YouTube. For each topic obtained, we use the official YouTube API to search for videos. Specifically, for each topic, we search for approximately 500 videos by relevance, with each video meeting the following criteria: (1) it is shorter than 4 minutes, (2) it has resolution of 720p or higher, and (3) it is licensed under Creative Commons. These requirements ensure that the collected videos are suitable and freely usable for training video generation models. In the end, we obtain 586, 490 videos from YouTube. We compare the YouTube IDs of these videos with those in existing datasets, including OpenVid-1M [20], HD-VILA-100M [41], InternVid [38], Koala-36M [33], LVD-2M [40], MiraData [9], Panda-70M [5], VidGen-1M [29], and WebVid-10M [3], and find that only 1, 675 IDs (0.29%) are already present in these datasets. This suggests that our VideoUFO introduces novel information or knowledge, which can serve to expand the range of existing training sources. Splitting videos and generating captions. After obtaining the videos, we segment them into multiple semanticsconsistent short clips following the steps in curating PandaThe process includes shot boundary detec70M [5]. tion, stitching, and video splitting, producing total of 3, 181, 873 clips. To facilitate the training of text-to-video models on our VideoUFO, we generate both brief and detailed captions for each video clip. For the brief captions, we utilize the video captioning model provided by Panda-70M [5]. For the detailed captions, we adopt the pipeline used in Open-Sora-Plan v1.3.0 [16], which employs QWen2-VL-7B [32] for video annotation. In Fig. 5 (a) and (b), we present the statistical distribution of the lengths of brief and detailed captions, respectively. Verifying clips. We notice that, although the videos are searched by topics, not every clip contains our intended topics. straightforward solution is to use GPT-4o [21] to verify whether given clip corresponds to specific topic. However, while effective, this approach would be prohibitively expensive for verifying more than 3 million video clips. As an alternative, we use the detailed caption (instead of the original clip) and feed it into GPT-4o mini (instead of GPT-4o) to verify whether it belongs to specific topic. Since (1) GPT-4o mini and GPT-4o have similar capabilities in basic language understanding, and (2) the video understanding model provides detailed descriptions of the videos, we effectively complete the final verification step at significantly lower cost. After verifying, there are 1, 091, 712 remaining clips. In Fig. 5 (c), we present the statistical distribution of clips duration. Video quality assessment. To further support research in text-to-video generation, we evaluate the quality of each clip in VideoUFO. Specifically, we adopt six different video quality assessment metrics from VBench [7], which automatically assess videos and align well with human perception. The evaluation dimensions are: (1) subject consistency: assesses whether the main subjects identity and appearance remain consistent throughout the video; (2) background consistency: evaluates the temporal stability and uniformity of the videos background; (3) motion smoothness: measures the continuity and fluidity of movements within the video; (4) dynamic degree: assesses the level of activity and variation in motion present in the video; (5) aesthetic quality: judges the visual appeal and attractiveness of 4 Figure 5. The statistical information of captions and video clips in the proposed VideoUFO dataset. The average word count for brief and detailed captions is 13.8 and 155.5, respectively, while the average clip duration is 12.6 seconds. the video; and (6) imaging quality: examines the clarity, brightness, and color accuracy of the video. The statistical distributions of scores assessed by these six metrics are shown in the Supplementary  (Fig. 9)  . Extension. Future researchers can easily extend our VideoUFO in three aspects: (1) Scaling up. Although our VideoUFO has already reached million-scale level, future researchers may still want to scale it up to 10 million or even 100 million. They can easily achieve this by leveraging our extracted topics to search more videos on platforms such as YouTube and TikTok. (2) New focusing. When we curate VideoUFO, the only available text-to-video prompt dataset is VidProM [35], which is used to analyze user focus and preferences. In the future, user focus may change, and other text-to-video prompt datasets may emerge for analyzing it. Future researchers can easily use our topic ex- (3) Imagetraction pipeline to study these new focuses. to-video. Our current focus is text-to-video, which is the most common approach in the video generation community. Meanwhile, we also notice that image-to-video is gaining popularity. Future researchers can use text and image prompts, such as those in TIP-I2V [34], to analyze the focus of image-to-video users and collect corresponding datasets. 4. Comparison with Other Video Datasets This section compares the proposed VideoUFO with other recent video datasets in terms of fundamental attributes and topics coverage. These differences underscore the necessity of introducing VideoUFO for text-to-video generation. 4.1. Fundamental Attributes From the Table 1, we draw three conclusions: VideoUFO is collected in line with real text-to-video users focus or preferences. In contrast to VideoUFO, other datasets collect videos from the open domain, which may not cover the topics users focus on, and text-to-video models trained on them may fail to meet users needs. VideoUFO is released under more flexible license and introduces new data. Specifically, we search for videos on YouTube by ourselves and only select those with In contrast, most recent Creative Commons license. datasets (including Panda-70M [5], LVD-2M [40], Koala36M [33], VidGen-1M [29], and OpenVid-1M [20]) directly source data from HD-VILA-100M [41]. As result: (1) they do not contribute new data but rather introduce new data processing pipeline; (2) they must adhere to the same license as HD-VILA-100M, i.e., the Research Use of Data Agreement (R-UDA), which restricts commercial use. In addition, one of the most widely used datasets, WebVid-10M [3], has been retracted due to potential copyright infringement. VideoUFO inherits best practices from other curated datasets. We observe that recent datasets (1) feature detailed captions generated by multimodal large language models (MLLMs), (2) contain millions of video clips, and (3) are high-resolution (i.e., 720p). Therefore, to provide large-scale and high-quality resource, our VideoUFO inherits these features. 4.2. Topics Coverage In this section, we analyze the differences in topics coverage between the proposed VideoUFO and existing datasets. Calculation process. We calculate the number of userfocused topics covered by the existing datasets as follows: Extract topics of recent video datasets. We repeat VidProMs topic extraction process (Step 1 in Section 3) on the video datasets listed in Table 1. The number of topics extracted for each dataset is shown in Table 1 (#Topics). Match the extracted topics with users focused ones. We observe that the same or similar topic may be described using different words. For example, both cathedral and church refer to similar topic. Therefore, when comparing user-focused topics with those from each existing dataset, we choose semantic matching rather than word-to-word approach. Specifically, (1) we first use SentenceTransformers [26] to embed the two lists of topics; and (2) then for each user-focused topic, if there exists topic in the existing dataset with cosine similarity greater than 0.6, we consider that user-focused topic to be covered by the existing dataset. Note that the threshold of 0.6 is an empirical value, as we observe that most similar topics exhibit cosine similarity greater than 0.6. 5 Table 1. The comparison between the proposed VideoUFO with other recent video datasets based on their fundamental attributes. Unlike previous collections, our VideoUFO is derived directly from real user-focused topics and also offers more flexible license, while remaining comparable to these datasets in other key aspects. # and are abbreviations for numbers and average, respectively. Dataset Venue Caption #Vid. Len. Words Resolution Domain #Topic License WebVid-10M [3] HD-VILA-100M [41] InternVid [5] Panda-70M [5] LVD-2M [40] MiraData [9] Koala-36M [33] VidGen-1M [29] OpenVid-1M [20] ICCV21 CVPR22 ICLR24 CVPR24 NeurIPS24 NeurIPS24 Arxiv24 Arxiv24 ICLR25 Download Recognition Generated Generated Generated Generated Generated Generated Generated 18.0s 10M 103M 13.4s 234M 11.7s 8.5s 70M 20.2s 2M 0.33M 72.1s 17.2s 36M 10.6s 1M 7.2s 1M"
        },
        {
            "title": "VideoUFO",
            "content": "Ours Generated 1M 12.6s 14.2 32.5 17.6 13.2 88.7 318.0 202.1 89.3 127.3 155. <360p 720p 720p 720p Diverse 720p 720p 720p Diverse 720p Open Open Open Open Open Open Open Open Open 1,000 648 1,051 719 814 639 724 835 671 Retracted R-UDA Apache 2.0 R-UDA R-UDA GPL 3.0 R-UDA R-UDA R-UDA"
        },
        {
            "title": "Users",
            "content": "1,"
        },
        {
            "title": "CC BY",
            "content": "dataset. This is because downloading and re-captioning all these videos is prohibitively expensive in terms of network bandwidth and computation. The quality of these captions/descriptions may affect the number of extracted topics, as some may not be informative. To address this limitation, in the next section, we demonstrate that text-to-video models trained on these datasets fail to generate satisfactory videos for some topics. 5. VideoUFO Benefits Video Generation 5.1. Benchmark This section details the proposed benchmark, BenchUFO, for evaluating text-to-video models performance on userfocused topics. We first introduce the design of the benchmark and then explain why it is reasonable. Construction. As shown in Fig. 7, the calculation process of our BenchUFO includes: (1) Selecting prompts. First, we select 791 concrete nouns from 1, 291 user-focused topics. (Please refer to the next section for the rationale behind selecting concrete nouns.) Then, for each chosen topic, we randomly select 10 text prompts from VidProM [35]. All these prompts constitute the prompt set for our benchmark. (2) Generating videos. For each prompt, we use textto-video model to generate corresponding video. (3) Describing videos. For each video, we use multimodal large language model (here, we choose QWen2-VL-7B [32]) to understand and describe it. The instruction prompt is provided in the Supplementary (Section 10). (4) Calculating similarity. We use sentence embedding model (in this case, SentenceTransformers [26]) to encode both the input prompt and the output description, and then compute the cosine similarity between them. We calculate the average similarity across 10 prompts for each topic. higher similarity score indicates better performance on that specific topic. Our benchmark computes and considers the 10/50 worst-performing and 10/50 best-performing topics. Justification. This section provides justification for the proposed BenchUFO from three perspectives: Figure 6. The number of user-focused topics covered by recent video datasets. None successfully includes all user-focused topics. Observations. The experimental results in Fig. 6 shows: None of the existing datasets cover all user-focused topics in VideoUFO. Specifically, the most comprehensive dataset, InternVid [38], covers 821 topics, accounting for 63.6% of all user-focused topics. Furthermore, if we combine all existing datasets in Table 1 into the largest one, the coverage of user-focused topics will increase to 79.5%. However, this will lead to higher computational burden when training text-to-video models. Interestingly, dataset may cover more user-focused topics than the actual number of topics it contains. For instance, OpenVid-1M [20] contains 671 topics but covers 703 user-focused ones. This is reasonable because two topics in our dataset, VideoUFO, may be similar and thus mapped to single topic in OpenVid-1M [20]. Nevertheless, this does not compromise the quality of our VideoUFO since it only leads to some relative duplicated topics, while the videos in VideoUFO still accurately reflect real users focus and interests. Limitation. The above topic analysis is based directly on the captions/descriptions provided by each recent video 6 Figure 7. The calculation process of BenchUFO. It is designed to evaluate whether text-to-video model can effectively generate videos that contain user-focused topics. It comprises 791 concrete noun topics, each paired with 10 real-world user-provided prompts. Table 2. The performance of both publicly available text-to-video models and our trained models on the proposed BenchUFO. The publicly available models are trained on various datasets, including both public and private ones. MVDiT [20] is trained on VidGen [29], OpenVid [20], and VideoUFO, respectively. Low/Top denotes the average score of the worst/best-performing topics."
        },
        {
            "title": "Models",
            "content": "Low 10 Low 50 Top 50 Top 10 Mira [10] Show-1 [44] LTX-Video [15] Open-Sora-Plan [16] TF-T2V [36] Mochi-1 [30] HiGen [24] Open-Sora [45] Pika [22] RepVideo [28] T2V-Zero [11] CogVideoX [42] Latte-1 [18] HunyuanVideo [12] LaVie [37] Pyramidal [8] 0.236 0.266 0.268 0.314 0.316 0.323 0.352 0.363 0.365 0.368 0.375 0.383 0.384 0.388 0.399 0. 0.282 0.303 0.310 0.361 0.359 0.367 0.394 0.409 0.404 0.402 0.410 0.419 0.421 0.427 0.426 0.433 0.508 0.524 0.532 0.559 0.560 0.580 0.589 0.601 0.583 0.589 0.586 0.601 0.592 0.612 0.595 0.606 0.550 0.564 0.574 0.598 0.595 0.616 0.625 0.639 0.619 0.619 0.616 0.629 0.636 0.645 0.632 0.647 MVDiT-VidGen [29] MVDiT-OpenVid [20] MVDiT-VideoUFO 0.382 0.401 0.442 0.426 0.437 0. 0.594 0.609 0.619 0.626 0.645 0.651 Why choose concrete nouns? This is due to the inconsistency between the input prompt and the output description when using abstract nouns. For instance, for the abstract noun topic freedom and the prompt In freedom world, ideal and love both exist, model may generate videos containing dove, star, or heart, and the video understanding model will summarize them as dove is playing..., star is shining..., and heart is beating.... However, the embeddings of these descriptions are not expected to be close to those of the given prompt. This might create false impression that the model cannot effectively generate content for that topic. Why use video understanding model reasonable? This is because the video understanding model has advanced significantly with the advent of large language models, and it is reasonable to assume that these models have been exposed to images/videos containing user-focused topics. Here, we choose QWen2-VL-7B [32] as the video understanding model. Trained on 800 billion tokens of visualrelated data and 600 billion tokens of text data, it is expected to accurately comprehend these user-focused topics and faithfully summarize video. Why not use established benchmarks? This is because the existing benchmarks fail to reflect real-world scenarios.. Specifically, we note that there exists well-established benchmark, VBench [7], whose object class and multiple objects dimensions are similar to our BenchUFO. However, the prompts in VBench have two main draw- (1) They cover only limited number of topbacks: ics/objects specifically, just 79. (2) They focus exclusively on common topics/objects (e.g., bicycle, car, and airplane), while many topics that users care about, such as forest, sunset, and beach, are missing. 5.2. Observations In this section, we evaluate current text-to-video models on this new benchmark and show that, with the help of VideoUFO, we achieve state-of-the-art performance. The quantitative and qualitative results are shown in Table 2 and Fig. 8, respectively. We observe that: Current text-to-video models do not consistently perform well across all user-focused topics. Specifically, there is score difference ranging from 0.233 to 0.314 between the top-10 and low-10 topics. These models may not effectively understand topics such as giant squid, animal cell, Van Gogh, and ancient Egyptian due to insufficient training on such videos. Current text-to-video models show certain degree of consistency in their best-performing topics. We discover that most text-to-video models excel at generating videos on animal-related topics, such as seagull, panda, dolphin, camel, and owl. We infer that this is partly due to bias towards animals in current video datasets. The proposed VideoUFO helps reduce the gap between the worst-performing and best-performing topics. To demonstrate the effectiveness of the proposed VideoUFO, we train an MVDiT [20] solely on VideoUFO. We find 7 Figure 8. Visual comparisons between our approach (MVDiT-VideoUFO) and other text-to-video models. The model trained on VideoUFO outperforms the alternatives in generating user-focused topics. In the prompts, concepts in green indicate those successfully generated solely by our method, whereas those in yellow denote concepts successfully generated by both our model and the competing models. Here we represent each video with single frame, and all generated videos are provided in the supplementary materials. that the trained model achieves the highest low-10 scores (a +4.2% improvement compared to the current stateof-the-art) while maintaining performance on the top-10 topics. Visually, the trained model successfully generates videos on topics that other models previously could not. The proposed VideoUFO outperforms other similar-scale datasets, such as OpenVid [20] and VidGen [29]. To demonstrate that the improvement originates from our VideoUFO rather than from the MVDiT architecture [20], we replace our VideoUFO with OpenVid [20] and VidGen [29]. We find that when using the popular OpenVid [20], the low-10 scores are similar to previous stateof-the-art models (0.401 vs. 0.400). Furthermore, when training on VidGen [29], the performance is even lower. 5.3. Future works Due to limited computational resources, when validating the effectiveness of our dataset, we currently follow the video generation method from the academic community introduced by [20], which already requires 32 A100 GPUs. However, the quality of videos generated by this baseline remains limited. In the future, industry researchers could incorporate our dataset into their large-scale foundational model training pipelines to further explore its potential. 6. Ablation Studies on Curation of VideoUFO This section performs ablation studies to investigate key components in curating VideoUFO, i.e., examining (1) the impact of the number of clusters in user-focused topic analysis and (2) the methods for clip verification. For more details, please refer to Supplementary (Section 11). 7. Conclusion This paper presents newly collected million-scale dataset for text-to-video generation with focus on user needs. Beyond this, our dataset exhibits minimal overlap with existing datasets and is released under more permissive license (i.e., CC BY). We first describe the curation process, which involves analyzing user-focused topics, searching for these topics, segmenting the resulting videos, and applying various post-processing techniques. Then, we highlight the differences between our dataset and existing ones in terms of fundamental attributes and topics coverage. Finally, we build new benchmark to evaluate text-to-video models on user-focused topics and demonstrate that our dataset helps improve model performance in this regard. We encourage both the research community and industry to use our dataset to further advance the field of text-to-video generation."
        },
        {
            "title": "Acknowledgment",
            "content": "We sincerely thank OpenAI for their support through the Researcher Access Program. Without their generous contribution, this work would not have been possible."
        },
        {
            "title": "References",
            "content": "[1] Kling - kuaishou - image-to-video. https://kling. kuaishou.com/en. 3 [2] Sora: Ai text-to-video model. https://openai.com/ index/sora/, 2024. 1, 3 [3] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. IEEE/CVF International Conference on Computer Vision, 2021. 2, 3, 4, 5, 6 [4] Stuart Marshall Bender. Coexistence and creativity: Screen media education in the age of artificial intelligence content generators. Media Practice and Education, 24(4):351366, 2023. [5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple IEEE/CVF Conference on Comcross-modality teachers. puter Vision and Pattern Recognition, 2024. 2, 3, 4, 5, 6 [6] Marta Garcia-Sampedro, Susana Agudo Prado, and Antonio Torralba-Burrial. Pre-service teachers skills development through educational video generation. European Journal of Teacher Education, pages 119, 2024. 1 [7] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 4, 7 [8] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. International Conference on Learning Representations, 2025. 3, 7 [9] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. Thirty-eighth Conference on Neural Information Processing Systems, 2024. 2, 3, 4, 6 [10] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. 7 [11] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-toimage diffusion models are zero-shot video generators. the IEEE/CVF International Conference on Computer Vision, 2023. [12] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3, 7 [13] ByteDance AI Lab. Pixeldance and seaweed: Advanced ai video generation models. https://pixeldance.io/ zh, 2024. 3 [14] Chenxin Li, Hengyu Liu, Yifan Liu, Brandon Feng, Wuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, and Yixuan Yuan. Endora: Video generation models as endoscopy simulators. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 230240. Springer, 2024. 1 [15] Lightricks. Ltx-video: Open-source video editing tools, 2024. 3, 7 [16] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 4, [17] Runtao Liu, Haoyu Wu, Zheng Ziqiang, Chen Wei, Yingqing Videodpo: OmniHe, Renjie Pi, and Qifeng Chen. preference alignment for video diffusion generation. arXiv preprint arXiv:2412.14167, 2024. 3 [18] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 7 [19] Leland McInnes, John Healy, Steve Astels, et al. hdbscan: Hierarchical density based clustering. J. Open Source Softw., 2(11):205, 2017. 1 [20] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for textto-video generation. International Conference on Learning Representations, 2025. 2, 3, 4, 5, 6, 7, 8 [21] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. 1, 4 [22] Pika Labs. Pika labs: Ai text-to-video generation. https: //pika.art/. 7 [23] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh 9 Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [24] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang. Hierarchical spatio-temporal decoupling for text-to-video generthe IEEE/CVF Conference on Computer Vision and ation. Pattern Recognition, 2024. 7 [25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2023. 3 [26] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. Conference on Empirical Methods in Natural Language Processing, 2019. 4, 5, 6 [27] Abhishek Sharma, Adams Yu, Ali Razavi, Andeep Toor, Andrew Pierson, Ankush Gupta, Austin Waters, Aaron van den Oord, Daniel Tanis, Dumitru Erhan, Eric Lau, Eleni Shaw, Gabe Barth-Maron, Greg Shaw, Han Zhang, Henna Nandwani, Hernan Moraldo, Hyunjik Kim, Irina Blok, Jakob Bauer, Jeff Donahue, Junyoung Chung, Kory Mathewson, Kurtis David, Lasse Espeholt, Marc van Zee, Matt McGill, Medhini Narasimhan, Miaosen Wang, Mikołaj Binkowski, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Nando de Freitas, Nick Pezzotti, Pieter-Jan Kindermans, Poorva Rane, Rachel Hornung, Robert Riachi, Ruben Villegas, Rui Qian, Sander Dieleman, Serena Zhang, Serkan Cabi, Shixin Luo, Shlomi Fruchter, Signe Nørly, Srivatsan Srinivasan, Tobias Pfaff, Tom Hume, Vikas Verma, Weizhe Hua, William Zhu, Xinchen Yan, Xinyu Wang, Yelin Kim, Yuqing Du, and Yutian Chen. Veo. 2024. 3 [28] Chenyang Si, Weichen Fan, Zhengyao Lv, Ziqi Huang, Yu Qiao, and Ziwei Liu. Repvideo: Rethinking crosslayer representation for video generation. arXiv preprint arXiv:2501.08994, 2025. 7 [29] Zhiyu Tan, Xiaomeng Yang, Luozheng Qin, and Hao Li. Vidgen-1m: large-scale dataset for text-to-video generation. arXiv preprint arXiv:2408.02629, 2024. 2, 3, 4, 5, 6, 7, [30] Genmo Team. Mochi 1. https://github.com/ genmoai/models, 2024. 3, 7 [31] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3 [32] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 4, 6, 7 [33] Qiuheng Wang, Yukai Shi, Jiarong Ou, Rui Chen, Ke Lin, Jiahao Wang, Boyuan Jiang, Haotian Yang, Mingwu Zheng, Xin Tao, et al. Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content. arXiv preprint arXiv:2410.08260, 2024. 2, 3, 4, 5, [34] Wenhao Wang and Yi Yang. Tip-i2v: million-scale real text and image prompt dataset for image-to-video generation. arXiv preprint arXiv:2411.04709, 2024. 2, 5 [35] Wenhao Wang and Yi Yang. Vidprom: million-scale real prompt-gallery dataset for text-to-video diffusion models. Thirty-eighth Conference on Neural Information Processing Systems, 2024. 1, 4, 5, 6 [36] Xiang Wang, Shiwei Zhang, Hangjie Yuan, Zhiwu Qing, Biao Gong, Yingya Zhang, Yujun Shen, Changxin Gao, and Nong Sang. recipe for scaling up text-to-video generation the IEEE/CVF Conference on Comwith text-free videos. puter Vision and Pattern Recognition, 2024. 7 [37] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, 2024. 7 [38] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. The Twelfth International Conference on Learning Representations, 2024. 2, 3, 4, 6 [39] Zijie Wang, Fred Hohman, and Duen Horng Chau. Wizmap: Scalable interactive visualization for exploring large machine learning embeddings. Annual Meeting Of The Association For Computational Linguistics, 2023. [40] Tianwei Xiong, Yuqing Wang, Daquan Zhou, Zhijie Lin, Jiashi Feng, and Xihui Liu. Lvd-2m: long-take video arXiv preprint dataset with temporally dense captions. arXiv:2410.10816, 2024. 2, 3, 4, 5, 6 [41] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2, 3, 4, 5, 6 [42] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. The Thirteenth International Conference on Learning Representations, 2025. 3, 7 [43] Tao Yu, Wei Yang, Junping Xu, and Younghwan Pan. Barriers to industry adoption of ai video generation tools: study based on the perspectives of video production professionals in china. Applied Sciences, 14(13):5770, 2024. 1 [44] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng 10 Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. International Journal of Computer Vision, 2024. [45] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 7 [46] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 1 11 VideoUFO: Million-Scale User-Focused Dataset for Text-to-Video Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 9. The statistical distributions of scores assessed by these six video quality metrics, which have been aligned with human perception. Researchers can use these scores to filter videos according to their specific training needs. 8. The Prompt for Concluding Topics Prompt: Could you describe the topic in the following short sentences using only 1-2 words? Please return only the topic (1-2 word) as singular noun or in the base form of the verb. 9. The Distributions of Video Quality Score Fig. 9 shows the statistical distributions of scores evaluated in terms of subject consistency, background consistency, motion smoothness, dynamic degree, aesthetic quality, and imaging quality. 10. The Prompt for Describing Videos Prompt: Please describe the content of this video in as much detail as possible, including the objects, scenery, animals, characters, and camera movements within the video. Do not include in your response. Start the description with the video content directly. Describe the content of the video and the changes that occur, in chronological order. 11. Ablation Studies on Curation of VideoUFO This section performs ablation studies to investigate key components in curating VideoUFO, i.e., examining (1) the impact of the number of clusters used in user-focused topic analysis and (2) the methods used for clip verification. The number of clusters. As shown in Fig. 10, we vary the number of pre-set clusters from 100 to 3, 000 when analyzing the focused topics of text-to-video users. We observe that initially, as the number of pre-set clusters increases, the number of resulting topics also increases; however, after reaching approximately 2, 000 clusters, this increase stops and begins to fluctuate. Therefore, we finally choose 2, 000 as the number of pre-set clusters. This is reasonable because the number of user-focused topics is limited, and an excessive number of pre-set clusters would eventually merge together. Beyond this, we also experiment with DBSCAN and HDBSCAN [19] to automatically determine the number of clusters; however, we do not obtain reasonable results. The method for clip verification. We randomly select 1, 000 video clips to conduct ablation studies on the methods used to verify whether clip contains specific topic. Feeding the entire video into GPT-4o (denoted as Direct GPT-4o) is considered the most accurate approach, and we compare the overlap of other cost-effective methods with it. As shown in Table 3, we observe that: (1) Feeding videos directly into GPT-4o is very expensive processing 1, 000 clips costs $162.23. (2) Using detailed captions instead of video clips reduces the API cost by approximately 5, 400, while maintaining about 80% prediction overlap. (3) Although using brief captions is even cheaper, the overlap 1 vehicle celebrity combat emotion wildlife mystery motorcycle wolf batman travel egypt astronaut destruction frog advertise butterfly hair pirate vampire halloween shark read luxury mouse spacecraft indian culture squirrel action iron man hinduism artificial intelligence whisker princess crime duck neon shop wed cow ninja goddess jump disney volcano storytelling monk barbie marine life ice cream firework sculpture party camera movement cycle din door escape arab culture scenery underwater ocean meditation transportation light royalty sleep futurism panda train horse design smile mar fox box festival censorship couple soldier river snake elon musk atmosphere loneliness nightlife spirituality timelapse imagery fall football city insect flag study turtle jesus witch dreamscape literature clown tranquility firearm filmmaking expression skateboard military school neuroscience the matrix gold meteor space battle spider-man skyscraper face cemetery dinosaur sunrise ritual demon cyborg fairy tree family comedy lake drone child samurai religion sky cosmos aviation steampunk medicine love exercise fruit kitten santa claus basketball man ant noir meme chicken decoration pig sadness automobile nostalgia waterfall viking concert interior owl news nighttime cannabis pikachu spiderman swordsmanship piano model television creature experiment bible octopus eagle crash launch food combination room humor history apocalypse bear warfare monster tiger mermaid tourism water interaction soccer chinese culture sing winter wind growth guitar mushroom spaceship eye martial arts urbanization castle temple storm office star wars metropolis survival performance santa paranormal arte accident tema supernatural character journey rap boat psychedelics medieval dj ship rainforest market surf infant vlogging time travel park whale autumn finance tea fight ai Figure 10. The relationship between pre-set number of clusters for K-means and final number of user-focused topics. Table 3. The methods for clip verification. We use 1, 000 clips for this ablation study. Direct GPT-4o refers to processing videos directly with GPT-4o, and Overlap measures the extent to which the predictions of other methods match those of Direct GPT-4o."
        },
        {
            "title": "Method",
            "content": "# Verified"
        },
        {
            "title": "API Cost",
            "content": "Direct GPT-4o GPT-4o GPT-4o-mini GPT-4o GPT-4o-mini"
        },
        {
            "title": "Detail",
            "content": "507 174 276 299 393 0.621 0.657 0.770 0. $162.23 $0.04 $0.01 $0.46 $0.03 drops by 14.9%, which is reasonable since brief caption (4) Interestcannot capture all the information in clip. ingly, we find that GPT-4o-mini achieves slightly better performance than GPT-4o. In conclusion, for verifying clips, we choose detailed captions with GPT-4o-mini. 12. The List of Full Topics dance music cyberpunk drive dog mythology eat moon dragon rabbit christmas paint animal fish car coffee snow swim ufo woman flower elephant horror fashion portrait nature celebration art zombie robot cityscape race bird girl play war architecture mountain logo friendship smoke cartoon conversation psychedelia alien walk flight rain romance explosion run space lion desert cook visualization fire jungle boy weather wave haunt garden chase ghost superheroes cat forest battle sunset beach conflict landscape village robotics night culture beauty magic game astronomy warrior monkey cloud gothic cinematography angel future 2 time crow nightclub night sky pizza parody black hole death dragonfly cake politics childhood new york dystopia routine money chaos snowfall cosmology street dream prayer island parent traffic gangster advertisement symbolism deer mutation religious imagery china construction ancient greece gallop robot combat fitness iceberg footage sun mysticism bar relationship lionel messi toilet education mansion sail marvel skyline clean sci-fi yoga mirror rave abandonment minecraft sitcom police conversion phone sport earth banana heart haunted mansion giant bunny cowboy penguin reflection cricket classroom climb laughter clothe skeleton glass ancient rome book virtual reality teddy bear bicycle violence bollywood pixar wizardry crustacean portal chess gameplay greet sound attire costume paris drum galaxy simulation trade farm employment slavic mythology homelessness bakery ceremony gorilla humanoid write egg cuisine hack turkey puppetry render reunion spider wealth botany trump cryptocurrency rotation dolphin superman russia countryside origami drift perseverance horseback riding sheep witchcraft fractal treasure king fear color meet sea creature investigation farmer commercial youth aesthetic victory library cafe elf hell bounce toy videogame heaven werewolf satan blink civilization archaeology graffiti protest youtube shadow vegetable unicorn africa godzilla chocolate tropical dark determination bridge goat rainbow bedroom bloom doll apple balloon crystal environment planet knight krishna statue gather act star jellyfish rise deity evolution flying car pyramid candy agriculture disaster tokyo food jewelry encounter teenager solar neighborhood puppy crossover parrot pollution ambiance ballet beer islam hospital cat behavior rock mask divinity dentistry assassination wine resilience gamble speed visual effects clock realism thriller bodybuilding thunderstorm elderly appearance be house 3 hulk hunt background lightning swimming pool gift seaside imprisonment punk record truck compassion italy wrestle crocodile giraffe sand textile victoriana blonde heroism picnic leaf bee marine tornado cherry blossoms aurora tattoo goku digital art emergency behavior waste pokemon mexico sneaker woodworking coin korea einstein superpower message pixel art anatomy primate mentorship burn makeup rebellion eiffel tower urban map wisdom science fiction genetics foot dynasty anger communication field tennis energy business program kitchen hand floral aerial digital retro fog restaurant cristiano ronaldo introspection raccoon spaghetti living room moonlight bikini motivation spongebob astrology hug prehistory music production brazil flood science ganesha stargaze physic shoe coral reefs tesla palace burger diplomacy campfire bottle collaboration jazz rescue donkey beverage cabin milk splash surprise buddhism mecha tunnel procrastination mercedes-benz diamond cell taylor swift happiness skull candle disco bus alcohol gesture nutrition life draw ski computer tribalism lego speech character design joy church skydive satire safety telephone occult teach display health hip-hop breakdancing suspense student joker theater delivery holography machinery dive anthropomorphism blue calendar presentation grandmother interview body terrorism brand dog eating wonderland archery synthesizer kiss mathematics nuclear warfare lo-fi print indonesia secrecy philosophy hybridization meadow dinner intelligence relaxation illness disapproval cry social media soviet shiva bubble cinema hallway cybersecurity camp peacock cuteness manufacture fatherhood helicopter extraterrestrial hotel nuclear photoshoot asteroid robbery darkness darth vader geometry circus cheese quantum folklore card villain prophet surveillance muppets antique flame bedtime drug electricity sea monster skate lizard event connectivity pumpkin contemplation bully chemistry buddha farewell fable duel economics bmw golf camel corgi cliff dubai diversity climate geology cybernetics shrek android tsunami confusion mosque soul maze gender techno seascape repair shepherd injury potato silhouette companionship pakistan bullfighting oil empire grief real estate jugando slavery sunshine laptop carnival community law noahs ark heist tank eminem chair deception fast food campus hotdog charge laser riot school bus monroe lgbtq+ donut panther submarine metalworking hope gratitude crowd bat fluid dynamics selfie afterlife microbiology public speaking intimacy pond leadership aztec baseball surgery mickey mouse espionage security vehicle parade migration season firefly treehouse skincare kungfu perfume watermelon earthquake himalaya putin afrofuturism cottage businessman diwali orange awaken gladiator anxiety tolkien charity pope noodle pandemic sunlight supercar thailand strawberry abstraction colonialism breakfast exhibition morocco empowerment amusement park head movement stock market sibling firefighting iran lightsaber kingdom perspective mario conspiracy levitation alphabet style cooky furniture wake bath terminator argument melt heal university entertainment discovery soda kaiju hong kong street photography napoleon mortal kombat blood hamster stadium fungus craft hygiene courtroom titanic childrens art temperature luck snail subterranean confrontation mahabharata type ethereal ceo collision goblin zoo sushi superhero atlantis iphone lip-sync biblical weightlift bioluminescence violin signage bag worm malfunction imran khan purple maya pregnancy mcdonalds hydration sphere muslim mental health muscle creativity grasshopper tarot vaporwave metal illusion trailer super mario psychology data antarctica phoenix wall redhead hanuman tyrannosaurus glitch hedgehog wash skin justice spain bigfoot mammoth cinderella biomechanics pug hamburger rooster naval warfare polar bear swamp doraemon predation mongol ink spartan web development minimalism e-commerce vend cryptids beard 4 expressionism typography lava yacht celtic nebula lemon radio cup honey baldness lifestyle content sandwich biology dog smiling minion graduation cleopatra thor cub childbirth businesswoman watch square anticipation rama cannibalism sloth pineapple impact alligator indoor snack laundry existentialism scorpion decay cosplay llama knit nail mughals augmented reality luffy snoop dogg swan abraham drought nino positivity drink confidence soup blade runner sew fry retrofuturism blacksmithing landmark stress particle prince indoors ottoman empire virus examination transformer arrest knife metamorphosis slime emoji crucifixion emergence ballistics kidnap consultation plague kangaroo saturn spice umbrella hallucination search plumb engine logistics pottery decision-making long deadpool cheetah sikhism squid massage adam driver interracial gundam patriotism notification pool party commute contrast monastery interrogation taj mahal teletubbies ladybug salad cuba cancer consciousness astral projection lecture pastry lily ramayana airport capybara caterpillar aquarium fireplace cube infidelity unity influencer gas station entrepreneurship suit shower priest taco inspiration hill prophecy dracula sonic burnout dunk automation navigation breathe fairytale seagull blindness mindfulness greed rollercoaster game show alexander imagination schwarzenegger freedom tupac chameleon nursery rhyme handshake profession morality shawn mendes observation digestion vote comet caminar sofa homecoming remote work stormtrooper addiction sri lanka peach harley quinn inquisitor genshin impact dessert lotus nurse mujer warhammer currency courage connection proposal taxi gandhi raven spacewomen nativity language alley preach fan motherhood peru juice aspiration puzzle mandala wish mine australia backroom customer hockey transition volleyball cocktail alchemy geography fatigue hippy dust curse sniper workspace popcorn independence mutant group seahorse therapy jupiter olympics vase circulation venom gingerbread philippine forage gravity rugby backflip zelda chaplin curtain persona kurt cobain division billie eilish biblical-figures half-life tower seinfeld urban decay asmr throw teleportation rapunzel bass sleep paralysis mia khalifa dieselpunk deepika padukone oracle tea ceremony elvis marathon theory hatsune miku space shuttle horizon dua lipa chuck norris puppeteering garvey toast hypnosis odyssey temptation destiny yin-yang garfield unreal engine eggplant limitation"
        }
    ],
    "affiliations": [
        "University of Technology Sydney",
        "Zhejiang University"
    ]
}