{
    "paper_title": "COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning",
    "authors": [
        "Dmitriy Shopkhoev",
        "Denis Makhov",
        "Magauiya Zhussip",
        "Ammar Ali",
        "Stamatios Lefkimmiatis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-training compression of large language models (LLMs) largely relies on low-rank weight approximation, which represents each column of a weight matrix in a shared low-dimensional subspace. While this is a computationally efficient strategy, the imposed structural constraint is rigid and can lead to a noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression via Sparse Dictionary Learning), a novel training-free compression framework that replaces low-rank decomposition with a more flexible structured sparse factorization in which each weight matrix is represented with a dense dictionary and a column-sparse coefficient matrix. This formulation enables a union-of-subspaces representation: different columns of the original weight matrix are approximated in distinct subspaces spanned by adaptively selected dictionary atoms, offering greater expressiveness than a single invariant basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the factorization such that the output activations of compressed projection layers closely match those of the original ones, thereby minimizing functional reconstruction error rather than mere weight approximation. This data-aware strategy preserves better model fidelity without any fine-tuning under reasonable compression ratios. Moreover, the resulting structured sparsity allows efficient sparse-dense matrix multiplication and is compatible with post-training quantization for further memory and latency gains. We evaluate CoSpaDi across multiple Llama and Qwen models under per-layer and per-group settings at 20-50\\% compression ratios, demonstrating consistent superiority over state-of-the-art data-aware low-rank methods both in accuracy and perplexity. Our results establish structured sparse dictionary learning as a powerful alternative to conventional low-rank approaches for efficient LLM deployment."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 5 7 0 2 2 . 9 0 5 2 : r Preprint. Under review. COSPADI: COMPRESSING LLMS VIA CALIBRATIONGUIDED SPARSE DICTIONARY LEARNING Dmitriy Shopkhoev1, Denis Makhov1, Magauiya Zhussip1, Ammar Ali2, Stamatios Lefkimmiatis1 1MTS AI, 2ITMO d.shophoev@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Post-training compression of large language models (LLMs) largely relies on lowrank weight approximation, which represents each column of weight matrix in shared low-dimensional subspace. While this is computationally efficient strategy, the imposed structural constraint is rigid and can lead to noticeable In this work, we propose CoSpaDi (Compression via model accuracy drop. Sparse Dictionary Learning), novel training-free compression framework that replaces low-rank decomposition with more flexible structured sparse factorization in which each weight matrix is represented with dense dictionary and column-sparse coefficient matrix. This formulation enables union-of-subspaces representation: different columns of the original weight matrix are approximated in distinct subspaces spanned by adaptively selected dictionary atoms, offering greater expressiveness than single invariant basis. Crucially, CoSpaDi leverages small calibration dataset to optimize the factorization such that the output activations of compressed projection layers closely match those of the original ones, thereby minimizing functional reconstruction error rather than mere weight approximation. This data-aware strategy preserves better model fidelity without any fine-tuning under reasonable compression ratios. Moreover, the resulting structured sparsity allows efficient sparsedense matrix multiplication and is compatible with post-training quantization for further memory and latency gains. We evaluate CoSpaDi across multiple Llama and Qwen models under per-layer and per-group settings at 20 50% compression ratios, demonstrating consistent superiority over state-of-the-art data-aware low-rank methods both in accuracy and perplexity. Our results establish structured sparse dictionary learning as powerful alternative to conventional low-rank approaches for efficient LLM deployment."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have demonstrated remarkable performance across wide range of applications, from conversational agents Brown et al. (2020); OpenAI (2023) to general-purpose reasoning systems Touvron et al. (2023a); Anil et al. (2023), owing to their ability to capture longrange dependencies through attention mechanisms Vaswani et al. (2017); Devlin et al. (2019). However, this success entails substantial memory and computational demands during both training and inference, which severely limits their deployment in resource-constrained environments. Various compression and acceleration techniques have been proposed over recent years to mitigate these challenges, including pruning Frankle & Carbin (2019); Sanh et al. (2020), quantization Dettmers et al. (2022); Xiao et al. (2023); Yao et al. (2022); Dettmers et al. (2023), and weight decomposition/factorization Denton et al. (2014); Hu et al. (2022); Liu et al. (2024); Zhang et al. (2023); Ma et al. (2019); Chen et al. (2018); Hsu et al. (2022); Yu & Wu (2023); Wang et al. (2025a). Recently, there has been growing interest in effective compression of large models without retraining or fine-tuning, since this is often computationally prohibitive. prominent line of work leverages low-rank approximations of weight matrices via Singular Value Decomposition (SVD). Early approaches such as DRONE Chen et al. (2021), FWSVD Hsu et al. (2022) and ASVD Yuan et al. (2023) introduced activation-aware truncation of singular values, while SVD-LLM Wang et al. 1 Preprint. Under review. (2025a) and its variant SVD-LLMv2 Wang et al. (2025b) proposed truncation-aware and optimized singular value selection strategies. In parallel, Basis Sharing Wang et al. (2024) explored crosslayer sharing of common basis to further enhance compression efficiency by exploiting inter-layer redundancy. Despite their effectiveness, these methods are inherently constrained by the use of single shared low-dimensional subspace, which may limit representational flexibility and prevent full exploitation of redundancies in LLM parameters. In this work, we argue that moving beyond the low-rank weight decomposition is promising new direction. Specifically, we propose to adopt dictionary learning, well-established paradigm in signal and image processing Aharon et al. (2006); Elad (2010), and apply it to LLM compression. Unlike low-rank approximations that confine all columns of weight matrix to shared linear subspace, dictionary learning enables richer union-of-subspaces representation: each column is approximated using only sparse subset of atoms from learned dictionary, allowing different columns to reside in distinct subspaces spanned by different atom combinations. Such representation better accommodates heterogeneous features and introduces additional flexibility to reduce the overall approximation error. To this end, we introduce CoSpaDi (Compression via Sparse Dictionary Learning), training-free framework that applies dictionary learning to jointly estimate dictionaries and sparse coefficients used for compressing LLM weight matrices. Our contributions are as follows: We introduce dictionary learning with sparse coding as new paradigm for LLM compression, addressing the limitation of using single invariant basis for the weight approximation which is inherent in SVD-based methods. We demonstrate that CoSpaDi is effectively integrated with data-aware optimization, yielding state-of-the-art compression performance for wide range of compression ratios, while being compatible with post-training quantization for further memory and latency gains. Through extensive experiments, we show that our approach is effective in both per-layer and crosslayer (shared dictionary) compression scenarios, consistently outperforming regular SVD, ASVD Yuan et al. (2023), SVD-LLM Wang et al. (2025a), and Basis Sharing Wang et al. (2024) strategies as well as state-of-the-art structure-pruning methods Ma et al. (2023); Shopkhoev et al. (2025)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Research on Transformer compression covers pruning, quantization, distillation, and matrix factorization. Below we provide brief overview of the recent progress in these directions. Early work on pruning showed substantial redundancy in deep nets Han et al. (2015); Frankle & Carbin (2019); Sanh et al. (2020). For LLMs, SparseGPT performs one-shot post-training pruning and achieves high sparsity with small quality drop Frantar & Alistarh (2023). Wanda reduces overhead further with simple activation-based rules Sun et al. (2023). LLM-Pruner removes blocks using gradient-guided criteria and recovers accuracy with brief adapter tuning Ma et al. (2023), while ReplaceMe substitutes multiple transformer blocks with single linear transformation Shopkhoev et al. (2025). Quantization reduces precision to reduce memory consumption and accelerate inference. For weight-only post-training quantization, LLM.int8 enables INT8 inference via vector-wise quantization with path for outliers Dettmers et al. (2022); GPTQ/OPTQ minimize output error using Hessian to reach 34 bit weights Frantar et al. (2022; 2023); AWQ derives scales from activations and protects small set of salient weights Lin et al. (2024), and SpQR stores outliers at higher precision to maintain quality at 34 bits Dettmers et al. (2024). For quantizing both weights and activations, SmoothQuant rebalances channel ranges to achieve accurate W8A8 across matrix multiplications Xiao et al. (2023), while QuaRot applies orthogonal rotations to suppress outliers, enabling end-to-end 4-bit inference including the KV cache Ashkboos et al. (2024). The general framework on knowledge distillation by Hinton et al. and sequence-level distillation by Kim and Rush established strong baselines Hinton et al. (2015); Kim & Rush (2016). DistilBERT demonstrates task-agnostic compression for Transformers Sanh et al. (2019); TinyBERT uses twostage distillation procedure Jiao et al. (2020); MobileBERT introduces teacher-guided compact architecture Sun et al. (2020); Patient-KD leverages multi-layer hints Sun et al. (2019); MiniLM and MiniLMv2 distill self-attention relations Wang et al. (2020; 2021). BERT-of-Theseus compresses 2 Preprint. Under review. Figure 1: Left side: weight factorization methods using low-rank decomposition. Low-rank approximation decomposes matrix into two dense matrices of lower rank. Right side: proposed CoSpaDi. dictionary of atoms and column-sparse coefficient matrix are employed. No restrictions on size of (undercomplete : < d1, complete: = d1 or overcomplete : > d1 dictionaries are possible), while sparsity is defined by non-zero elements per column of the coefficient matrix. models via progressive module replacement Xu et al. (2020). Orca and Orca 2 show that richer teacher signals such as explanations can improve reasoning in smaller students Mukherjee et al. (2023); Mitra et al. (2023). Classic low-rank approximations reduce parameters and FLOPs with limited loss Denton et al. (2014). DRONE considers an objective related to the output activation for the weight approximation, Chen et al. (2021). FWSVD introduces Fisher-weighted reconstruction to emphasize important directions Hsu et al. (2022), while ASVD adapts truncation using activation-aware transforms and layer sensitivity Yuan et al. (2023). SVD-LLM utilizes truncation-aware whitening , while SVDLLM v2 optimizes budget allocation across layers Wang et al. (2025a;b). Basis Sharing reuses one part of low-rank factorization across layers and learns layer-specific coefficients to exploit inter-layer redundancy Wang et al. (2024). Dictionary learning and sparse coding. Dictionary learning factorizes original weight into dictionary and sparse codes, yielding union-of-subspaces model with strong results in vision and image compression Engan et al. (1999); Aharon et al. (2006); Mairal et al. (2010); Gregor & LeCun (2010); Elad (2010). In NLP, GroupReduce explores block-wise low-rank/dictionary-style compression Chen et al. (2018), and tensorized Transformers factorize attention and embeddings Ma et al. (2019). Recent work also targets the KV-cache by learning universal dictionaries and decoding with OMP during inference Kim et al. (2024). Complementary to these directions, recent work proposes cross-layer weight sharing via matrix-based dictionary learning formulation for Transformer attention, directly exploiting inter-layer redundancy Zhussip et al. (2025). Cross-layer/shared-basis schemes in attention are thus closely related Wang et al. (2024). comprehensive, training-free approach to weight-space dictionary learning for LLMs at both per-layer and cross-layer levels remains underexplored and is the focus of this work."
        },
        {
            "title": "3 PROPOSED METHOD",
            "content": "In this section, we first provide conceptual link between low-rank weight approximation and dictionary learning. Then, we introduce the proposed Sparse Dictionary Learning (SDL) strategy for LLM compression covering all practical aspects including data-aware SDL, dictionary-sharing among layers, compression ratios and inference complexity. 3.1 LOW-RANK WEIGHT APPROXIMATION widely adopted strategy to reduce the parameter count in LLMs is to approximate weight matrices Rd1d2 representing the parameters of network layers with matrices of reduced rank 3 Preprint. Under review. < min(d1, d2). Such low-rank approximation can be derived as follows: = arg min rank( W)=r (cid:13) (cid:13)W (cid:13) (cid:13) (cid:13) (cid:13)F . (1) According to the EckartYoungMirsky theorem Eckart & Young (1936), the orthogonal projection to the space of r-rank matrices admits an analytical solution. Specifically, if admits the singular value decomposition = UΣVT, with Rd1k, Σ Rkk and Rd2k with = min (d1, d2), then the minimizer of Eq. (1) can be expressed as: = UrΣrVT , where Ur Rd1r contains the first left singular vectors, Σr Rrr holds the top-r singular values, and Vr Rd2r contains the first right singular vectors of W, respectively. There is deep connection between this problem and the principal component analysis (PCA) Bishop & Nasrabadi (2006). Specifically, consider the weight matrix as collection . . . , wd2], where wj Rd1 , with = 1, . . . , d2. We seek to of d1-dimensional vectors = [w1, approximate each vector wj as linear combination of basis vectors spanning lower-dimensional subspace of Rd1. The optimal basis and coefficients can be found by minimizing the total approximation error: = d2(cid:88) j=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) wj (cid:88) i=1 ci,jbi (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 BC2 , (2) subject to the orthogonality constraint BTB = I, where = [b1, . . . , br] Rd1r is the basis matrix, Rrd2 is the coefficient matrix with entries ci,j, and Rrr is the identity matrix. The optimal pair (B, C) is obtained by solving: B, = arg min s.t BTB = I. B,C (3) The solution is given by = Ur and = ΣrVT (we refer to Appendix A.1 for the proof). This result highlights that in the low-rank factorization BC, the matrix corresponds to the basis of the shared r-dimensional subspace in which the columns of approximately lie, and contains their coordinate vectors (coefficients) with respect to that basis. 3.2 SPARSE DICTIONARY LEARNING Motivated by this interpretation, we propose an alternative strategy for compressing the weights W, based on the sparse dictionary learning methodology. Specifically, rather than modeling each column of the weight matrix as linear combination of basis vectors bi, we aim to learn dictionary Rd1k and approximate each column wj as linear combination of dictionary atoms di Rd1 with = 1, . . . , k. Although the two strategies appear similar, they differ in two key respects. First, in dictionary learning, unlike low-rank approximation, only subset of atoms is used to represent each weight vector wj. This principle, known in the literature as sparse coding Lee et al. (2006), is motivated by the observation that learned dictionaries typically yield sparse representations: each signal (column) activates only few atoms. In other words, instead of approximating the weight matrix as the product of two dense matrices, the orthogonal basis and the coefficient matrix C, we approximate it as the product of dense dictionary matrix and column-wise sparse coefficient matrix S, where each column sj indicates which atoms contribute to the representation of wj, and with what weights. Second, and of significant practical consequence, we do not impose orthogonality constraints on the dictionary atoms. This grants greater representational flexibility, allowing the model to adapt more effectively to the intrinsic structure of the weight matrix. Finally, under the proposed strategy, rather than enforcing shared low-dimensional subspace for all columns, each weight vector wj is represented within its own low-dimensional subspace spanned by the atoms it activates. Consequently, the entire matrix is modeled as union of low-dimensional subspaces, introducing additional representational capacity and enabling lower approximation error compared to single shared subspace. 4 Preprint. Under review. Formally, the dictionary learning problem can be expressed as: D, = arg min D,S DS2 s.t. sj0 = 1, . . . , d2, (4) where sj denotes the j-th column of the sparse coefficient matrix S, and 0 denotes the ℓ0 pseudonorm, which counts the number of nonzero entries in vector. The optimization problem in Eq. (4) is NP-hard in its original form and admits no closed-form solution. Nevertheless, it has been extensively studied, and several efficient algorithms have been proposed to obtain high-quality approximate solutions. Among these, K-SVD Aharon et al. (2006) is well-established algorithm widely adopted across diverse applications. K-SVD alternates between two main steps: (a) Sparse coding: Each signal wj is approximated as sparse linear combination of dictionary atoms (typically via orthogonal matching pursuit or LASSO). (b) Dictionary update: Each atom di is refined to better fit the data while preserving the sparsity pattern in S. This is performed sequentially per atom: the algorithm identifies the subset of signals that activate di, computes the residual error over those signals, and applies rank-1 SVD to jointly update the atom and its associated coefficients. 3.3 COSPADI: ACTIVATION-AWARE SDL In several recent works, including Chen et al. (2021), it has been observed that LLM weight matrices generally do not exhibit intrinsic low-rank structure. Consequently, direct low-rank approximation of the weights often leads to significant performance degradation. However, under the hypothesis that the input latent features to given layer reside in low-dimensional subspace, it remains feasible and often effective to approximate the corresponding layer weights as low-rank. This approach is well-motivated: the goal is not to preserve the weights in isolation, but rather to maintain the fidelity of the activations they produce when applied to low-dimensional inputs. Within this framework, an effective approximation of can be obtained by minimizing the output error: = arg min (cid:13) (cid:13)XW (cid:13) (cid:13) (cid:13) (cid:13)F s.t. rank(X W) = r, (5) where RN d1 is matrix of calibration input vectors (cid:8)xi Rd1(cid:9)N admits the analytical solution: = L1UrΣrVT transformation matrix derived from the calibration data (see Appendix A.2 for the derivation). i=1. This minimization = BC, where Rd1d1 is non-singular Motivated by this data-aware perspective, we propose adapting our sparse dictionary learning framework accordingly. Rather than directly approximating as = DS, we instead minimize the reconstruction error of the output activations = XW: D, = arg min D,S XW XDS2 s.t. sj0 = 1, . . . , d2. (6) To simplify this optimization, let = XL1 RN d1 be column-orthogonal matrix, i.e., YTY = Id1, obtained by applying linear transformation Rd1d1 to X. We assume d1 and that has full column rank conditions typically satisfied with sufficient calibration data. The matrix can be computed via QR decomposition, SVD of X, or Cholesky/eigen-decomposition of XTX. Introducing the auxiliary variables WL = LW and DL = LD, and noting that the Frobenius norm is invariant under left-multiplication by column-orthogonal matrix, we can reformulate Eq. (6) as: L, = arg min DL,S WL DLS2 s.t. sj0 = 1, . . . , d2. (7) Since is invertible, the final structured approximation of the original weights is given by: = DaS, where Da = L1DL is the activation-aware learned dictionary. Pseudocode for the full procedure is provided in Appendix A.3. (8) 5 Preprint. Under review. Cross-Layer SDL Although most compression techniques focus on intra-layer optimizations, the repetitive layered structure of LLMs suggests the presence of significant inter-layer redundancy. To exploit this, we propose sharing single dictionary across weight matrices of the same type from multiple layers. Specifically, let denote group of layer indices (e.g., all attention projection layers or all FFN layers). We form grouped weight matrix by horizontally concatenating the correspond- . . . , WℓL] Rd1(d2L), where each Wℓ Rd1d2, ing layer weights: WG = [Wℓ1, Wℓ2, and = is the number of layers in the group. We then apply the same activation-aware dictionary learning procedure as before, but now to WG. To ensure the approximation remains dataaware across all layers in the group, we construct grouped calibration matrix by vertically stack- . . . ; XℓL] R(N L)d1 where each ing the corresponding input batches: XG = [Xℓ1 ; Xℓ2 ; Xℓ RN d1 is the calibration input for layer ℓ. The transformation matrix Rd1d1 is then computed from XG (e.g., via QR or Cholesky decomposition), ensuring that YG = XGL1 is column-orthogonal. The optimization proceeds by solving Eq. (7) with WL = LWG, yielding shared dictionary Da and sparse coefficient matrix SG Rk(d2L). This approach, inspired by Wang et al. (2024), reduces memory overhead by amortizing the dictionary cost across multiple layers, while preserving activation fidelity through data-aware calibration. Each layers compressed weights are then recovered by slicing the corresponding block from WG = DaSG. Compression ratio. For the low-rank setting, the compression ratio is γLR := 1 r(d1+d2) , where is the retained rank. For CoSpaDi, we store an overcomplete dictionary and sparse codes plus binary mask (details are provided in section A.4), giving d1d2 γSD := 1 dict. (bf16) (cid:122)(cid:125)(cid:124)(cid:123) d1k + codes (bf16) (cid:122)(cid:125)(cid:124)(cid:123) sd2 + d1d mask (1 bit/entry) (cid:122) (cid:123) (cid:125)(cid:124) (kd2)/16 . (9) Unlike the low-rank case, γSD depends on two knobs the number of atoms and sparsity allowing us to trade off model capacity and storage at fixed budget. We parameterize this trade-off by the ratio ρ := k/s, which uniquely determines (k, s) at target γSD: = (1 γSD) d1d2 d1 + d2 ρ + 16 , = ρ . (10) Inference Complexity In terms of computational efficiency, low-rank and dictionary learning exhibit distinct inference-time complexity profiles: the former has cost of r(d1 + d2), whereas the latter when exploiting sparsity and reusing inner products achieves d1Kactive + sd2 (see section A.5), potentially yielding superior efficiency under favorable sparsity patterns. Although both methods share identical theoretical complexity under matched compression ratios, practical inference latency varies significantly due to factors such as the number of active atoms, indexing overhead, and hardware-specific kernel efficiency. Further details regarding complexity derivations are provided in Appendix A.5."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we present our experimental setup. We first compare CoSpaDi with low-rank baselines in the per-layer setting, where each linear weight matrix is compressed independently. We then investigate cross-layer sharing, analyze the impact of coefficient matrix quantization, and conduct ablation studies on the ks-ratio. 4.1 EXPERIMENTAL SETUP We evaluate our method in both per-layer and cross-layer settings. For per-layer evaluations, we consider LLaMA-3.2 1B, Qwen-3 0.6B, LLaMA-3 8B, and Qwen-3 8B. For the cross-layer study, we follow the Basis Sharing Wang et al. (2024) setup and conduct experiments on LLaMA-2 7B. All models are evaluated in zero-shot setting on PIQA Bisk et al. (2019), HellaSwag Zellers et al. (2019), OpenAI LAMBADA Paperno et al. (2016), ARC-easy and ARC-challenge Clark et al. 6 Preprint. Under review. (2018), SciQ Welbl et al. (2017), Race Lai et al. (2017), and MMLU Hendrycks et al. (2021), while perplexity is additionally reported on WikiText Merity et al. (2016) and LAMBADA-OpenAI. Compression is applied with target ratios of 0.2-0.5 with 0.1 step. For methods requiring calibration data, we randomly sample 256 sequences from the RefinedWeb dataset Penedo et al. (2023), each consisting of 1024 tokens. Unless specified otherwise, we compress all dense linear projections in self-attention (Q/K/V/O) and feed-forward network (gate/up/down). Embeddings and lm head are left intact. For low-rank methods the rank is uniquely defined with γLR and we floored it to the nearest integer. For CoSpaDi we fixed ρ = 2 (k/s ratio) for all experiments, so and were obtained according to Eq. (10) and then were floored to the nearest integers. In CoSpaDi we employ K-SVD using power iterations instead of full-svd for the dictionary updates. Specifically, we use 60 K-SVD iterations and 8 power iterations to ensure stable convergence. Further implementation details and convergence analysis are provided in Appendix A.8."
        },
        {
            "title": "4.2 ABLATION STUDY",
            "content": "In the proposed CoSpaDi we can redistribute capacity across both and Influence of k/s-ratio. while preserving predefined compression ratio by varying the ρ value (k/s). We verified its influence on the Llama 3.2 1B and report the metrics in Fig. 2. CR Bitwidth Avg. Acc. PPL 0.1686 0.1843 0.2001 0.2726 0.2864 0. 0.3765 0.3883 0.4002 0.4804 0.4902 0.5001 bFP16 bFP15 bFP14 bFP16 bFP15 bFP14 bFP16 bFP15 bFP14 bFP16 bFP15 bFP 0.6198 0.6195 0.6176 0.5408 0.5394 0.5373 0.4096 0.4086 0.4069 0.2846 0.2838 0.2837 1.94E+01 1.95E+01 1.97E+01 4.34E+01 4.37E+01 4.46E+ 1.80E+02 1.82E+02 1.84E+02 8.23E+02 8.15E+02 8.10E+02 Table 1: Results on truncation of bfloat16 mantissa bits of coefficient matrix with reported average accuracy (Avg. Acc.) and Wiki Text word perplexity (PPL) for Llama3-8B resulting in different compression ratios (CR). Figure 2: Dual-axis plot showing average accuracy solid lines, left axis) and perplexity (- - - dashed ( lines, right axis, logarithmic scale with inverted direction) as functions of ρ for Llama3.2-1B under three compression levels: 0.2, 0.3 and 0.4. Perplexity decreases upward due to axis inversion. From the provided plots we can observe that depending on the compression ratio the optimal ks-ratio differs, thus, for simplicity in further experiments we select in all cases ρ = 2. Data-Free and Data-Aware Scenarios We argue that our proposed CoSpaDi allows for more flexible representation compared to low-rank approximation, regardless of whether data-free or dataaware scenarios are considered. To prove this claim we performed an ablation study on Llama3-1B for different compression levels. We selected SVD for the data-free scenario and SVD-LLM for the data-aware case, while varying the compression ratio from 0.2 to 0.5. In Table 2 we report the results only for 0.2-0.5 compression. These results clearly indicate that in both data-free and data-aware settings the SDL-based methods outperform low-rank baselines by wide margin, while CoSpaDi outperforms all methods. Quantization of Sparse Coefficient Matrix We investigated the quantization of the coefficient matrix in the post-training regime to leverage the memory add-on due to the requirement of storing the indices of the nonzero values. We utilized naive mantissa bit truncation of the original bfloat16 coefficient values. In Table 1 we report how truncation affects the performance at different compression levels. According to the obtained results, truncation of 2 bits leads to negligible drop. Thus, in 7 Preprint. Under review. Table 2: SDL-based methods comparison vs low-rank counterparts in data-free and data-aware scenarios on Llama3.2-1B at different compression ratios (CR). We denote CoSpaDi as the proposed method without using calibration data. Best results are provided in bold. Method Data-Aware CR Llama3.2 1B SVD CoSpaDi SVD-LLM CoSpaDi SVD CoSpaDi SVD-LLM CoSpaDi SVD CoSpaDi SVD-LLM CoSpaDi SVD CoSpaDi SVD-LLM CoSpaDi v v v v 0.2 0.3 0. 0.5 PIQA Hella Swag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Wiki Text LAMBADA Accuracy Perplexity 0.7453 0.5201 0.5174 0.6213 0.6605 0.5234 0.5049 0.5565 0.5691 0.5277 0.5103 0.5180 0.5348 0.5392 0.5120 0.5109 0. 0.6366 0.2566 0.2635 0.3643 0.4288 0.2564 0.2626 0.3006 0.3241 0.2590 0.2626 0.2725 0.2824 0.2566 0.2607 0.2663 0.2703 0. 0.0003 0.0002 0.2443 0.3839 0.0002 0.0002 0.0910 0.1822 0.0001 0.0001 0.0126 0.0380 0.0001 0.0001 0.0004 0.0031 0.6047 0. 0.8830 0.3779 0.2471 0.2538 0.3603 0.3994 0.2416 0.2449 0.3047 0.3194 0.2386 0.2546 0.2685 0.2778 0.2441 0.2572 0.2609 0. 0.2210 0.2534 0.2509 0.2602 0.2346 0.2611 0.2150 0.2210 0.2133 0.2688 0.2287 0.2295 0.2039 0.2654 0.2594 0.2398 0.2000 0.2100 0.6490 0.7160 0.1950 0.2180 0.4590 0. 0.1990 0.2130 0.3230 0.3690 0.2040 0.2280 0.2610 0.2950 0.2144 0.2182 0.2900 0.3167 0.2191 0.2153 0.2583 0.2804 0.2220 0.2115 0.2440 0.2402 0.2077 0.2211 0.2392 0. 0.3700 0.2537 0.2413 0.2298 0.2483 0.2698 0.2552 0.2323 0.2308 0.2693 0.2549 0.2297 0.2310 0.2690 0.2704 0.2303 0.2326 0. 0.2391 0.2447 0.3762 0.4267 0.2425 0.2453 0.3022 0.3368 0.2411 0.2470 0.2621 0.2753 0.2406 0.2519 0.2536 0.2578 11.57 2.93E+06 3.33E+05 1.69E+02 6.37E+ 1.09E+06 2.06E+05 5.90E+02 2.89E+02 1.23E+06 3.05E+06 1.58E+03 7.96E+02 1.22E+06 6.09E+05 3.13E+03 1.80E+03 5.73 4.56E+06 2.16E+06 1.70E+02 3.51E+01 3.92E+06 4.34E+06 2.47E+03 6.59E+ 4.17E+06 3.67E+07 3.30E+04 9.23E+03 1.88E+07 1.08E+07 1.03E+05 7.26E+04 all our experiments CoSpaDi consists of bfloat16 dictionary and coefficient matrices but is evaluated with the truncated version of the sparse coefficient matrix using 14-bits. 4.3 MAIN RESULTS Per-Layer Scenario: In this section we apply CoSpaDi for each projection layer independently and compare it against current sota training-free low-rank method SVD-LLM Wang et al. (2025a). (a) Accuracy & Perplexity for LLaMA3.2-1B (b) Accuracy & Perplexity for Qwen3-0.6B Figure 3: Average benchmark accuracy and WikiText perplexity for (a) LLaMA-3.2-1B and (b) Qwen-3 0.6B using SVD-LLM and CoSpaDiwith respect to compression ratio. From the provided plots CoSpaDi significantly outperforms SVD-LLM in both avg. accuracy and perplexity for small Llama3 and Qwen3 models. We further investigate how our method scales to larger models of the same families with 8B parameters. The related results are provided in Table 3. We have also performed comparisons on the same models and compression levels against other training-free compression strategies. In particular we compare CoSpaDi against ASVD Yuan et al. (2023), ReplaceMe Shopkhoev et al. (2025) and LLMPruner Ma et al. (2023), with the last two being sota pruning methods. These results are provided in Appendix A.6 Cross-Layer Scenario In this section, we validate CoSpaDi on the cross-layer compression scenario, where common dictionary is shared across different layers. To ensure fair comparison, we adopt the identical layer selection and grouping procedure introduced in Basis Sharing Wang et al. (2024) (for detailed description we refer to Appendix A.7) and compare the performance of the two methods on the Llama2-7B model Touvron et al. (2023b). As shown in Table 4, CoSpaDi consistently outperforms Basis Sharing by large margin across all benchmarks and compression Preprint. Under review. Table 3: Performance comparison of CoSpaDi vs sota SVD-LLM on Llama3-8B and Qwen3-8B at different compression levels on different benchmarks. Best results are highlighted with bold. Method CR Llama3 8B SVD-LLM CoSpaDi SVD-LLM CoSpaDi SVD-LLM CoSpaDi SVD-LLM CoSpaDi Qwen3 8B SVD-LLM CoSpaDi SVD-LLM CoSpaDi SVD-LLM CoSpaDi SVD-LLM CoSpaDi 0.2 0.3 0.4 0. 0.2 0.3 0.4 0.5 PIQA Hella Swag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Wiki Text LAMBADA Accuracy Perplexity 0.8069 0.7106 0. 0.6578 0.7051 0.6028 0.6371 0.5381 0.5642 0.7769 0.7383 0.7650 0.7035 0. 0.6627 0.6888 0.6023 0.6159 0.7913 0.5837 0.6649 0.4640 0.5620 0.3450 0. 0.2790 0.3102 0.7494 0.6391 0.6804 0.5522 0.6050 0.4458 0.4898 0.3543 0. 0.7557 0.5925 0.7380 0.3806 0.6128 0.1143 0.3029 0.0041 0.0390 0. 0.6218 0.6559 0.5377 0.6255 0.3790 0.4991 0.1956 0.2791 0.7769 0. 0.9390 0.4029 0.5551 0.6654 0.4188 0.5421 0.3237 0.3914 0.2731 0. 0.3396 0.4155 0.2765 0.3353 0.2449 0.2662 0.2389 0.2218 0.8640 0.8950 0.7000 0. 0.4420 0.6850 0.2660 0.3770 0.3550 0.3818 0.3177 0.3617 0.2574 0.3053 0.2230 0. 0.8068 0.5674 0.9570 0.4086 0.6869 0.7218 0.5926 0. 0.4503 0.4941 0.3363 0.3565 0.4573 0.4889 0.3712 0.4121 0.2807 0.2986 0.2278 0. 0.9010 0.9320 0.8720 0.8840 0.7730 0.8200 0.6670 0.6860 0.4048 0.4067 0.3837 0. 0.3531 0.3675 0.2909 0.3167 0.6215 0.3262 0.4282 0.2715 0.3224 0.2305 0. 0.2297 0.2301 0.7295 0.5473 0.6075 0.4482 0.5127 0.2901 0.3660 0.2305 0. 0.7036 7.26E+00 3.09E+00 0.5408 0.6176 0.4358 0.5373 0.3201 0. 0.2565 0.2837 4.07E+01 1.97E+01 1.47E+02 4.46E+01 5.49E+02 1.84E+02 1.37E+03 8.10E+02 1.09E+01 4.27E+ 6.09E+01 9.18E+00 1.30E+03 1.19E+02 2.80E+04 7.64E+03 0.7045 1.22E+01 4.58E+ 0.6246 0.6573 0.5576 0.5997 0.4544 0.5030 0.3631 0.3868 2.06E+01 1.81E+01 2.74E+01 2.29E+ 4.30E+01 3.59E+01 8.72E+01 6.82E+01 6.40E+00 4.88E+00 1.07E+01 6.29E+00 3.59E+01 1.47E+01 2.69E+02 1.10E+ rates (0.20.5). This performance gap suggests that CoSpaDis sparsity-aware, dictionary-based decomposition better preserves task-critical features under aggressive compression. We believe that more sophisticated layer grouping strategy, based on feature map similarity or endto-end differentiable search, could work better with CoSpaDi and further widen this performance margin, but we leave this as future research direction. We provide details for (dictionary size) and (sparsity) parameters in the Appendix A.7. Table 4: Performance comparison of CoSpaDi vs Basis Sharing Wang et al. (2024) on Llama2-7B under different compression levels on various benchmarks. Best results are highlighted with bold. Method CR Llama2 7B Basis Sharing CoSpaDi Basis Sharing CoSpaDi Basis Sharing CoSpaDi Basis Sharing CoSpaDi 0.2 0. 0.4 0.5 Accuracy Perplexity PIQA Hella Swag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Wiki Text LAMBADA 0.7797 0.7008 0.7459 0.6556 0.7084 0.6061 0. 0.5631 0.5751 0.5709 0.4337 0.6627 0.3757 0.5850 0.3310 0.4801 0.2975 0. 0.7367 0.6297 0.7132 0.5354 0.6447 0.4114 0.5180 0.2377 0.3264 0. 0.4326 0.9390 0.3923 0.6646 0.6738 0.5880 0.6191 0.4945 0. 0.3729 0.4171 0.3370 0.3933 0.2730 0.3430 0.2355 0.2986 0.2048 0.2491 0.9100 0. 0.8680 0.8760 0.8300 0.8150 0.7450 0.7470 0.3493 0.3828 0.3263 0.3589 0.2967 0. 0.2536 0.2632 0.4087 0.2476 0.2720 0.2317 0.2404 0.2305 0.2300 0.2282 0. 0.6276 0.5341 0.5915 0.4817 0.5469 0.4295 0.4800 0.3628 0.3965 8. 15.18 11.72 22.23 15.42 39.51 25.24 98.56 57.31 3.40 7.24 4. 13.57 6.31 37.67 14.58 225.09 72.61 Limitations and Future Work In this work, we employ k-SVD as representative instantiation of our framework, though other dictionary learning algorithms could be similarly applied. key limitation of k-SVD lies in its reliance on Orthogonal Matching Pursuit (OMP) for sparse coding and its sequential atom updates, which can be computationally slow. However, more efficient variants do exist that accelerate convergence while maintaining the same performance and we plan to explore them in the future. As an additional direction for future work, we plan to investigate adaptive capacity allocation across the model by dynamically distributing sparsity and dictionary size according to layer-specific demands, while leveraging cross-layer dictionary sharing in structured and scalable manner. 5 CONCLUSIONS In summary, we introduced CoSpaDi, novel training-free compression framework for LLMs that challenges the dominance of low-rank approximation in post-training model compression. By introducing dictionary learning with sparse coding as more expressive alternative, we shift from the rigid constraint of shared linear subspace to flexible union-of-subspaces representation, where each column of weight matrix is approximated using sparse combination of learned 9 Preprint. Under review. dictionary atoms. This work demonstrates that moving beyond SVD-driven paradigms, long considered the default for matrix compression in LLMs, can yield significant gains in model fidelity under aggressive compression. We hope CoSpaDi serves as conceptual stepping stone toward richer, data-aware factorizations inspired by classical signal processing, yet tailored for the complexity of modern language models."
        },
        {
            "title": "REFERENCES",
            "content": "Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54(11): 43114322, 2006. doi: 10.1109/TSP.2006.881199. Rohan Anil, Sebastian Borgeaud, Jiecao Chen, Aakanksha Chowdhery, Jonathan Clark, et al. Palm 2 technical report. In arXiv preprint arXiv:2305.10403, 2023. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. In Advances in Neural Information Processing Systems, 2024. Christopher Bishop and Nasser Nasrabadi. Pattern recognition and machine learning, volume 4. Springer, 2006. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Patrick Chen, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. Drone: Data-aware low-rank compression for large nlp models. Advances in neural information processing systems, 34:29321 29334, 2021. Patrick H. Chen, Si Si, Yang Li, Ciprian Chelba, and Cho-Jui Hsieh. Groupreduce: Block-wise low-rank approximation for neural language model shrinking. In Advances in Neural Information Processing Systems, pp. 1101111021, 2018. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In Advances in Neural Information Processing Systems (NeurIPS), 2014. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: sparse-quantized representation for near-lossless llm weight compression. In International Conference on Learning Representations, 2024. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep In Conference of the North American bidirectional transformers for language understanding. Chapter of the Association for Computational Linguistics (NAACL), 2019. 10 Preprint. Under review. Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211218, 1936. Michael Elad. Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing. Springer, 2010. Kjersti Engan, Sven Ole Aase, and J. Hakon Husø y. Method of optimal directions for frame design. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 5, pp. 24432446, 1999. doi: 10.1109/ICASSP.1999.760624. Ky Fan. On theorem of Weyl concerning eigenvalues of linear transformations I. Proceedings of the National Academy of Sciences of the United States of America, 35(11):652655, 1949. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations (ICLR), 2019. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1032310337. PMLR, 2023. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Optq: Accurate post-training In International Conference on Learning quantization for generative pre-trained transformers. Representations, 2023. Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In Proceedings of the 27th International Conference on Machine Learning (ICML), pp. 399406, 2010. Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2015. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language model In International Conference on Learning compression with weighted low-rank factorization. Representations (Workshop Track), 2022. ICLR 2022 Workshop. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 41634174, 2020. doi: 10.18653/v1/2020. findings-emnlp.372. Junhyuck Kim, Jongho Park, Jaewoong Cho, and Dimitris Papailiopoulos. Lexico: Extreme kv cache compression via sparse coding over universal dictionaries, 2024. Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1317 1327, 2016. doi: 10.18653/v1/D16-1139. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations, 2017. Honglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng. Efficient sparse coding algorithms. Advances in neural information processing systems, 19, 2006. 11 Preprint. Under review. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. In Proceedings of Machine Learning and Systems (MLSys), volume 6, 2024. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, KwangTing Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In International Conference on Machine Learning, 2024. Oral. Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Dawei Song, and Ming Zhou. tensorized transformer for language modeling. In Advances in Neural Information Processing Systems, 2019. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On the structural pruning of large language models. In Advances in Neural Information Processing Systems, 2023. Julien Mairal, Francis R. Bach, Jean Ponce, and Guillermo Sapiro. Online learning for matrix factorization and sparse coding. Journal of Machine Learning Research, 11:1960, 2010. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, and Subhabrata Mukherjee. Orca 2: Teaching small language models how to reason. arXiv preprint arXiv:2311.11045, 2023. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring broad discourse context, 2016. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert: distilled version of bert: Smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019. Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by fine-tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, and Sergey Zagoruyko. Replaceme: Network simplification via depth pruning and transformer block linearization, 2025. Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for BERT model compression. In Proceedings of EMNLP-IJCNLP 2019, pp. 43234332, 2019. doi: 10.18653/v1/ D19-1441. Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: compact task-agnostic bert for resource-limited devices. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 21582170, 2020. doi: 10.18653/v1/2020.acl-main.195. Preprint. Under review. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. In International Conference on Learning Representations (ICLR), 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, In Advances in Neural InforŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. mation Processing Systems (NeurIPS), 2017. Jingcun Wang, Yu-Guang Chen, Ing-Chao Lin, Bing Li, and Grace Li Zhang. Basis sharing: Crosslayer parameter sharing for large language model compression. arXiv preprint arXiv:2410.03765, 2024. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. In Advances in Neural Information Processing Systems 33 (NeurIPS), 2020. Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, and Furu Wei. Minilmv2: Multi-head In Findings of the self-attention relation distillation for compressing pretrained transformers. Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 21402151, 2021. doi: 10. 18653/v1/2021.findings-acl.188. Xin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. Svd-llm: Truncation-aware singular value decomposition for large language model compression. In International Conference on Learning Representations, 2025a. Xin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. Svd-llm v2: Optimizing singular value truncation for large language model compression. In Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2025b. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions, 2017. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: In International Accurate and efficient post-training quantization for large language models. Conference on Machine Learning (ICML), 2023. Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. Bert-of-theseus: Compressing bert by progressive module replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 78597869, 2020. Zhewei Yao, Jiarui Zhao, Shiyang Zhang, Boxiao Wang, Ye Zhang, George Biros, Dan Alistarh, Kurt Keutzer, Michael W. Mahoney, and Joseph E. Gonzalez. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Hao Yu and Jianxin Wu. Compressing transformers: Features are low-rank, but weights are not! In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1100711015, 2023. Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activation-aware singular value decomposition for compressing large language models. arXiv preprint arXiv:2312.05821, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In International Conference on Learning Representations, 2023. 13 Preprint. Under review. Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, and Stamatios Lefkimmiatis. Share your attention: Transformer weight sharing via matrix-based dictionary learning. arXiv preprint arXiv:2508.04581, 2025. 14 Preprint. Under review."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DERIVATION OF THE OPTIMAL PAIR OF BASIS AND COEFFICIENT MATRICES We are seeking the optimal pair (B, C) that minimizes the constrained problem in Eq. (3). First, we rewrite the objective in its equivalent form: = tr (cid:0)WTW(cid:1) 2 tr (cid:0)WTBC(cid:1) + tr (cid:0)CTBTBC(cid:1) . (11) Next, we consider the basis matrix as fixed and compute the gradient of the objective w.r.t the matrix coefficient as CJ = 2BTBC 2BTW. (12) Setting the gradient to zero and taking into account the constraint BTB = I, we can recover the optimum matrix coefficient as: = BTW. Now, putting back in Eq. (11) we get: = tr (cid:0)WTW(cid:1) 2 tr (cid:0)WTBBTW(cid:1) + tr (cid:0)WTBBTBBTW(cid:1) = tr (cid:0)WTW(cid:1) tr (cid:0)WTBBTW(cid:1) (from the constraint BTB = I) = tr (cid:0)WTW(cid:1) tr (cid:0)BTWWTB(cid:1) . (13) Based on this we can recover the optimum basis matrix as the maximizer of the constrained problem: = arg max tr (cid:0)BTWWTB(cid:1) s.t BTB = I. (14) The above maximization problem enjoys closed-form solution Fan (1949), which is fully defined by the eigenvalues of the matrix = WWT. Specifically, the matrix Rd1d1, which is symmetric and positive semi-definite, admits the eigenvalue decomposition = UΛUT, with Rd1d1 holding the eigenvectors of in its columns. Then, the maximizer of Eq. (14) is recovered as = Ur where Ur Rd1r is reduced version of formed with the eigenvectors corresponding to the largest eigenvalues of P. useful observation is that the eigenvectors of exactly match the left-singular vectors of Rd1d2 . Indeed, if admits the singular value decomposition = UΣVT, then we have that: = WWT = UΣ2UT UΛUT, with Λ = Σ2. Therefore, instead of performing the eigenvalue decomposition on we can recover and consequently Ur by computing the SVD of W. Finally, we can compute the optimum coefficient matrix as: = (B)T = UT (cid:122) (cid:123) (cid:125)(cid:124) [Ur Udr] = UT = (cid:2)Irr Or(dr) (cid:122) (cid:20) Σr O(dr)r (cid:20) ΣrV ΣdrVT (cid:3) dr Σ (cid:125)(cid:124) (cid:123) (cid:21) VT (cid:122) (cid:125)(cid:124) (cid:123) (cid:21) (cid:20) VT VT dr Or(dr) Σdr (cid:21) = ΣrV , (15) where Vr Rd2r is reduced version of formed with the right singular vectors of that correspond to its top-r singular values, which are kept in the diagonal matrix Σr Rrr. A.2 DATA-AWARE LOW-RANK WEIGHT APPROXIMATION While the low-rank approximation of the weights has been extensively used for compression tasks, in practice is not well suited to LLMs and it can lead to severe drop of their performance . Several recent works have suggested that instead of approximating the weights with low-rank matrix, more efficient strategy is to model the weight activations, = XW, as low-rank. Here, . . . xN ]T RN holds in its rows the d-dimensional input vectors xn the matrix = [x 15 Preprint. Under review. with = 1 . . . , , which play the role of calibration data. Under this modeling framework, we can approximate the matrix weights as the minimizer of the following problem: = arg min (cid:13) (cid:13)XW (cid:13) (cid:13) (cid:13) (cid:13)F s.t. rank (cid:16) (cid:17) = r. (16) Let us now consider = XL1 RN d1 to be semi-orthogonal matrix (column-orthogonal matrix), that is YTY = Id1, which is obtained by linearly transforming the matrix using nonsingular matrix Rd1d1. Here we assume that and the matrix is of full rank. We note that there are different ways we can achieve this column-orthogonalization of X. Among them we can employ the QR/SVD decomposition on and the Cholesky/Eigen-value decomposition on XTX to compute proper linear transformation L. By using the representation = YL we can rewrite the problem of Eq. (16) as: = arg min (cid:13) (cid:13)YLW YL (cid:13) (cid:13) (cid:13) (cid:13)F s.t. rank (cid:16) YL (cid:17) = r. (17) To solve the above minimization problem we first note that due to the orthonormal columns of Y, it can be expressed in the equivalent form: (cid:13) (cid:13) (cid:13) = arg min s.t. rank = r, (18) (cid:16) (cid:17) (cid:13) (cid:13) (cid:13)F where = LW. Next, we introduce the auxiliary matrix = and the problem in Eq. (18) becomes: = arg min (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F s.t. rank (cid:17) (cid:16) = r, (19) which is the orthogonal projection of to the space of r-rank matrices. Given that = and is invertible, we can now recover = L1 . To conclude, if admits the singular value decomposition = UΣVT, then the optimal r-rank approximation of that minimizes the loss in Eq. (16) can be written in the form: = BC = L1Ur (cid:124) (cid:123)(cid:122) (cid:125) . ΣrVT (cid:124) (cid:123)(cid:122) (cid:125) (20) We note that in this case, unlike the direct weight low-rank approximation, the matrix = L1Ur does not correspond to basis of subspace of Rd1, since its columns are no longer orthonormal, that is BTB = UT (cid:0)LLT(cid:1)1 Ur = . A.3 PSEUDO ALGORITHM OF THE PROPOSED METHOD Goal. Given weight matrix Rd1d2 and small calibration set RN d1, compute an activation-aware sparsedictionary factorization (cid:102)W = DS under target compression ratio. The procedure consists of whitening the activation objective, alternating sparse coding and dictionary updates on the whitened weights, and final de-whitening step. (1) Calibration and whitening. Compute an invertible transform Rd1d1 (e.g., via QR/SVD of or Cholesky of XX) such that = XL1 has orthonormal columns (YY = I). Leftmultiply to obtain the whitened weights WL = LW. Whitening converts the data-aware loss XW (cid:99)W2 that is amenable to dictionary learning. into standard Frobenius objective WL DLS Rd1k (e.g., random permutation of (2) Initialization. columns of W) and set S(0) = 0. The pair (k, s) is set from the target compression ratio via Eq. 10 optionally using the fixed ratio ρ = k/s. Initialize the whitened dictionary D(0) 16 Preprint. Under review. (3) Alternating minimization. Repeat for = 1, . . . , : (a) Sparse coding. For each column j, solve s(t) arg min s0s (cid:13) (cid:13)(WL):,j DL (t1)s(cid:13) 2 2, (cid:13) using OMP (greedy selection with orthogonal residual updates) to enforce exactly nonzeros per column. (b) Dictionary update. For each atom i, collect its support Ωi = {j : s(t) i,j = 0} and form the residual on those columns: Ri = WL[:, Ωi] (cid:88) DL,ℓ (t1) s(t) ℓ,Ωi . ℓ=i (t), s(t) i,Ωi ) by the best rank-1 approximation Ri σ (set /mD(t) Update (DL,i L,i u, s(t) σv). This preserves the current sparsity pattern while reducing the residual. Iterate atoms i,Ωi sequentially. Stop when the maximum iteration is reached or when the relative improvement falls below tolerance. (4) De-whitening and packing. Map the dictionary back to activation space via Da = L1D(T) and set (cid:102)W = DaS(T ). For storage, keep Da and the sd2 nonzero entries of in bf16 along with packed binary mask {0, 1}kd2 for locations (one bit per entry; kd2 16 words). This yields the compression ratio in Appendix A.4 and Eq. 9. (5) Inference. At runtime, apply (cid:102)W as matmul(x, DaS) with sparsedense kernels. Reuse inner products x, Da,:,i across columns to achieve the complexity in Appendix A.5; the number of active atoms controls the practical speedup. (6) Cross-layer variant. For group of layers G, concatenate weights horizontally WG = [Wℓ1 WℓL ] and stack calibration batches vertically to form XG. Compute from XG, run the same alternating procedure on WL,G = LWG to obtain single shared Da, and slice the corresponding blocks of SG back to per-layer coefficients. A.4 DERIVATION OF THE COSPADI COMPRESSION RATIO We derive the expression for the compression ratio γSD of our sparsedictionary (SD) parameterization. Let Rd1d2 be factorized as DS, Rd1k, Rkd2, where each column of has exactly nonzeros (column-s-sparse). Throughout, we store real values in bfloat16 (16 bits) as is common in modern LLMs. Dense baseline. dense requires d1d2 bf16 values. Dictionary term. The dictionary stores d1k bf16 values. Sparse codes. Naively, would need kd2 values. Since is column-s-sparse, only sd2 values are stored. For locations, one option is COO: per nonzero we keep row index and (redundantly) the column index. Because sparsity is fixed per column, column indices can be omitted; keeping only row indices yields sd2 indices. With 16-bit indices, the total becomes sd2 values + sd2 indices = 2sd2 16-bit words. For typical ρ := k/s = 2, this equals kd2 wordsoffering no savings over dense storage. Instead, we use bit mask {0, 1}kd2 to mark nonzero positions. This requires kd2 bits, i.e., kd2/16 16-bit words after packing. We then store sd2 bf16 values for the nonzeros and the packed mask for their positions. Total and ratio. The SD parameterization thus stores d1k (cid:124)(cid:123)(cid:122)(cid:125) dictionary + sd2 (cid:124)(cid:123)(cid:122)(cid:125) values + kd2 16 (cid:124)(cid:123)(cid:122)(cid:125) mask 17 Preprint. Under review. Algorithm 1: Pseudo algorithm of the proposed CoSpaDi which consists of two steps: (a) sparse coding to compute the coefficients and (b) sequential dictionary update step. Input : Rd1d2: weight matrix to compress RN d1: calibration input data (N samples) k: dictionary size (number of atoms, s) s: sparsity level (max nonzeros per column in S) : number of K-SVD iterations Output: Da Rd1k: activation-aware dictionary Rkd2: sparse coefficient matrix = DaS: compressed weight matrix Compute Rd1d1 such that = XL1 satisfies YTY = Id1; % e.g., via QR: = QR = % e.g., via Cholesky: XTX = CTC = WL LW; Initialize D(0) Initialize S(0) Rkd2 as zero matrix; for = 1 to do Rd1k with random Gaussian or SVD-based atoms; for = 1 to d2 do (cid:13) (cid:13)WL,j D(t1) s(t) (cid:13) arg mins0s % Solve via OMP, LASSO, or thresholding (cid:13) 2 (cid:13) (cid:13) 2 ; end for = 1 to do s(t) Ωi if Ωi = then (cid:110) i,j = 0 (cid:111) ; Ri WL[:, Ωi] (cid:80) l=i d(t1) L,l [u, σ, v] rank-1 SVD of Ri; d(t) s(t) i,Ωi σ vT; L,i u; s(t) l,Ωi ; end end end Da L1D(T ) ; DaS(T ); return Da, S(T ), W; 16-bit words. Relative to the dense baseline d1d2, the resulting compression ratio is γSD := 1 dict. (bf16) (cid:122)(cid:125)(cid:124)(cid:123) d1k + codes (bf16) (cid:122)(cid:125)(cid:124)(cid:123) sd2 + d1d2 mask (1 bit/entry) (cid:122) (cid:123) (cid:125)(cid:124) (kd2)/16 . This matches the expression used in the main text and makes explicit the dependence on the two design knobs (k, s). A.5 LOW-RANK AND COSPADI INFERENCE COMPLEXITY Here we derive the multiplication complexity for the original weight, SVD-compressed weight, and dictionary-learning (k-SVD) compression. We count multiplications only (additions are of the same order). Let Rd1d2 be projection matrix in some layer and RN d1 be an input feature map; then dense product = XW costs Obaseline = d1d2. (21) 18 Preprint. Under review. For low-rank, in particular, SVD compression with rank the projection matrix is approximated with two matrices UV with Rd1r and Rrd2 , resulting in the following complexity: OLR = d1r + rd2 = r(d1 + d2). (22) Sparse dictionary (SD) learning similarly represents DS with dictionary Rd1k of atoms and sparse coefficient matrix Rkd2. Omitting sparsity of will result in: Taking into account that each column sj of is s-sparse, the (i, j) element of = XDS is OSD,dense = d1k + kd2 = k(d1 + d2). yi,j = (cid:88) k= Sk,jXi,:, D:,k = (cid:88) kSj Sk,jXi,:, D:,k, (23) (24) where Sj = supp(sj) and Sj = s. The overall sparse complexity depends on whether the inner products Xi,:, D:,k are reused across columns. With the most efficient way with reuse letting = (cid:83)d2 j=1 Sj and Kactive = we have: OSD,sparsereuse = d1Kactive + sd2, Kactive min(K, sd2). (25) With proposed truncation of 2-bits in mantissa we can omit term for storing indices of nonzero elements in resulting in corrected compression ratio: ˆγSD = 1 d1k + sd2 d1d2 The rank for the low-rank decomposition could be estimated from the compression ratio with the following equation: = (1 γLR)d1d2 d1 + d2 For sparse dictionary based method we defined ρ = k/s and, thus we can estimate both and in the following way: = (1 ˆγSD) d1d2 d1 + d2 ρ , = ρ . Under the same compression ratio for both SVD and CoSpaDi we obtain exactly the same complexity: OLR = r(d1 + d2) = (1 γLR)d1d OSD = d1k+N sd2 = (d1k+d2 ρ ) = k(d1+ d2 ρ ) = (1 ˆγSD) d1d2 d1 + d2 ρ (d1+ d2 ρ ) = (1ˆγSD) d1d2 OLR = OSD = (1 γ)d1d2. While, theoretical complexity is the same, in practice inference time depends on the sparsity structure (Kactive), indexing overhead, and kernel efficiency, as can be observed from Figs. 4, 5 and 6. (26) A.6 COMPARISON WITH OTHER TRAINING-FREE METHODS We also evaluate the performance of the proposed CoSpaDi relative to other training free methods, particularly structural pruning ones and ASVD Yuan et al. (2023) which is another data-aware SVDbased method. The results are provided in Table 5. Across LLaMA-3 8B, our method consistently outperforms structural pruning baselines at moderate budgets. At CR 0.2, CoSpaDi attains the best average accuracy (0.618 vs. 0.580 for ReplaceMe and 0.551 for LLM-Pruner) while also delivering the lowest perplexities, indicating balanced generative and discriminative quality. At CR 0.3, CoSpaDi widens the margin in average accuracy (0.537 vs. 0.469/0.405) and avoids the severe perplexity blow-ups observed for pruning methods. Even under aggressive compression (CR 0.40.5), CoSpaDi remains competitive or superior on most tasks and preserves far more stable perplexity profiles, whereas ReplaceMe and LLM-Pruner exhibit task collapse (near-zero LAMBADA and extreme PPL spikes). Overall, the results support dictionarysparsity as more robust data-free alternative to structural pruning at matched or tighter budgets, especially when both accuracy and perplexity must be maintained. 19 Preprint. Under review. Figure 4: Inference time for different projection layers of Llama3.2 1B for different compression ratios and k/s ratios on A100 Figure 5: Inference time for different projection layers of Llama3 8B for different compression ratios and k/s ratios on A.7 CROSS-LAYER SCENARIO In Table 6, we report the dictionary size (k), sparsity level (s), and input/output weight dimensions for each linear transform under fixed k/s ratio. This includes the query, key, value, and output projections within the attention block, as well as the up, down, and gated projections in the gated MLP block. The dictionary and sparsity dimensions are adjusted according to the target compression rate. 20 Preprint. Under review. Figure 6: Inference time for different projection layers of Qwen3 0.6B for different compression ratios and k/s ratios on A100 A.8 K-SVD AND POWER ITERATION ANALYSIS We conducted ablation studies to assess the effect of the number of K-SVD iterations and power iterations on performance using Llama3.2-1B with fixed ρ = 2. The left plot in Fig. 7 shows that average accuracy stabilizes after roughly 50 K-SVD iterations, while perplexity continues to decrease slightly before flattening out. The right plot of Fig. 7 indicates that very few power iterations are sufficient for stable convergence: performance improves sharply up to around 5 iterations, after which additional iterations yield minimal benefit. Based on these results, we fixed the number of K-SVD iterations to 60 and power iterations to 8 in our final experiments, which provides good balance between accuracy, perplexity, and computational efficiency. A.9 USE OF LARGE LANGUAGE MODELS The authors acknowledge the use of large language model (LLM) solely for language editing and grammatical refinement of the current manuscript. All scientific content, analysis, and interpretations presented herein are the sole responsibility of the authors. 21 Preprint. Under review. Table 5: Comparison of the proposed CoSpaDi method with other training-free methods including ASVD Yuan et al. (2023) and state-of-the-art structured pruning methods ReplaceMe Shopkhoev et al. (2025) and LLM-Pruner Ma et al. (2023) on Llama3 8B under different compression ratios. We report accuracy on different benchmarks as well as its average and perplexity. Best results are highlighted with bold Method CR Llama3 8B 0.2 0.22 0.2 0.3 0. 0.3 0.4 0.41 0.4 0.5 ASVD ReplaceMe LLM-Pruner CoSpaDi ASVD ReplaceMe LLM-Pruner CoSpaDi ASVD ReplaceMe LLM-Pruner CoSpaDi ASVD ReplaceMe LLM-Pruner CoSpaDi PIQA Hella Swag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Wiki Text LAMBADA Accuracy Perplexity 0.8069 0.5577 0.7307 0.7546 0.7519 0.5359 0.6659 0.6725 0.7051 0.5365 0.6170 0.5033 0. 0.5196 0.5724 0.5038 0.5642 0.7913 0.2666 0.6572 0.6746 0.6649 0.2595 0.5382 0.4509 0.5620 0.2575 0.4430 0.2580 0.4138 0.2547 0.3635 0.2623 0. 0.7557 0.0070 0.4211 0.5096 0.7380 0.0000 0.2401 0.2090 0.6128 0.0000 0.0976 0.0151 0.3029 0.0000 0.0367 0.0043 0.0390 0. 0.5350 0.9390 0.4029 0.3237 0.6587 0.6208 0.6654 0.2694 0.5067 0.4541 0.5421 0.2504 0.3742 0.2643 0. 0.2601 0.2976 0.2630 0.2988 0.2150 0.4369 0.3660 0.4155 0.2142 0.3788 0.2875 0.3353 0.2287 0.2747 0.2577 0.2662 0.2261 0.2543 0.2637 0.2218 0.3050 0.8640 0.8780 0. 0.2190 0.7730 0.6340 0.8570 0.2080 0.6040 0.2810 0.6850 0.1980 0.3810 0.2560 0.3770 0.2182 0.3541 0.3512 0.3818 0.2115 0.3397 0.3014 0.3617 0.2201 0.3158 0.2182 0. 0.2134 0.2785 0.2172 0.2287 0.6215 0.2347 0.5167 0.2495 0.4282 0.2349 0.3058 0.2292 0.3224 0.2431 0.2638 0.2324 0.2538 0.2554 0.2300 0.2371 0. 0.7036 7.26E+00 3.09E+00 0.2660 0.5799 0.5505 0.6176 0.2431 0.4685 0.4048 0.5373 0.2430 0.3738 0.2537 0. 0.2409 0.3017 0.2509 0.2837 9.38E+04 3.37E+01 1.60E+01 1.97E+01 3.43E+05 6.67E+01 3.79E+01 4.46E+01 2.05E+05 2.30E+02 1.84E+02 1.72E+06 6.99E+02 1.24E+03 8.10E+ 9.89E+05 2.01E+01 1.05E+01 4.27E+00 1.35E+07 1.33E+02 2.21E+02 9.18E+00 2.57E+07 1.76E+03 5.67E+05 1.19E+02 1.72E+07 5.03E+04 1.23E+06 7.64E+03 Figure 7: Average benchmark accuracy and WikiText perplexity with respect to the number of KSVD iterations (left) and the number of power iterations (right) for Llama3.2-1B with ρ = 2 Preprint. Under review. Table 6: Compression configurations for Llama2 7B weight matrices across varying compression rates (20%50%). Each row specifies sparsity parameters (k, s, k/s ratio) and weight dimensions (d1, d2) for different weight types (Query, Key, Value, Up, Gate, Down, Out), grouped by compression rate and group size (1 or 2). d1 4096 4096 4096 4096 4096 11008 4096 4096 4096 4096 4096 4096 11008 4096 4096 4096 4096 4096 11008 4096 4096 4096 4096 4096 4096 11008 4096 d2 8192 8192 8192 22016 22016 4096 8192 8192 8192 22016 22016 4096 4096 8192 8192 8192 22016 22016 4096 4096 8192 8192 8192 22016 22016 4096 Weight Type Compression Rate Group size Dictionary, Sparsity, k/s ratio Query Key Value Up Gate Down Out Query Key Value Up Gate Down Out Query Key Value Up Gate Down Out Query Key Value Up Gate Down Out 20% 30% 40% 50% 1638 1638 1638 2388 2388 1381 1092 1433 1433 1433 2089 1208 955 1228 1228 1228 1791 1791 1036 819 1024 1024 1024 1492 1492 863 682 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 2 1 3276 3276 3276 4776 4776 2762 2184 2866 2866 2866 4178 2416 1910 2456 2456 2456 3582 3582 2072 1638 2048 2048 2048 2984 2984 1726"
        }
    ],
    "affiliations": [
        "ITMO",
        "MTS AI"
    ]
}