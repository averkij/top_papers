{
    "paper_title": "An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging",
    "authors": [
        "Kunat Pipatanakul",
        "Pittawat Taveekitworachai",
        "Potsawee Manakul",
        "Kasima Tharnpipitchai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with a particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing language-specific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and a computational budget of $120, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 6 5 0 9 0 . 2 0 5 2 : r AN OPEN RECIPE: ADAPTING LANGUAGE-SPECIFIC LLMS TO REASONING MODEL IN ONE DAY VIA MODEL MERGING Kunat Pipatanakul, Pittawat Taveekitworachai, Potsawee Manakul, and Kasima Tharnpipitchai SCB 10X R&D SCBX Group Bangkok, Thailand {kunat,pittawat,potsawee,kasima}@scb10x.com"
        },
        {
            "title": "ABSTRACT",
            "content": "This paper investigates data selection and model merging methodologies aimed at incorporating advanced reasoning capabilities such as those of DeepSeek R1 into language-specific large language models (LLMs), with particular focus on the Thai LLM. Our goal is to enhance the reasoning capabilities of language-specific LLMs while maintaining their target language abilities. DeepSeek R1 excels in reasoning but primarily benefits high-resource languages such as English and Chinese. However, low-resource languages remain underserved due to the dominance of English-centric training data and model optimizations, which limit performance in these languages. This limitation results in unreliable code-switching and diminished effectiveness on tasks in low-resource languages. Meanwhile, local and regional LLM initiatives have attempted to bridge this gap by developing languagespecific LLMs that focus on improving local linguistic fidelity. We demonstrate that, with only publicly available datasets and computational budget of $1201, it is possible to enhance the reasoning capabilities of language-specific LLMs to match the level of DeepSeek R1, without compromising their performance on target language tasks. This work releases the data, merge configurations, and model weights to promote the advancement of language-specific LLM initiatives."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly through innovations in scaling at test time and specialized training paradigms (DeepSeek-AI et al., 2025). Notable breakthroughs by models such as OpenAI o1 and DeepSeek R1 (DeepSeek-AI et al., 2025) have established new standards in tackling reasoning challenges that were previously difficult for LLMs. However, these achievements primarily focus on high-resource languages, particularly English and Chinese, creating significant gap in capabilities for low-resource languages. The underlying foundation models, such as Llama (Grattafiori et al., 2024) and Qwen (Qwen et al., 2025), predominantly rely on training data in English and Chinese, leading to limitations when applied to low-resource languages. While these models may achieve impressive scores on certain low-resource language benchmarks, they frequently exhibit issues such as incorrect character usage and code-switching in practical applications, see example in Appendix A.1. These problems become more pronounced during reasoning-focused fine-tuning and reinforcement learning (Team, 2024), where both the query language and solution paths are optimized in high-resource languages. Several local and regional LLM initiatives, including Typhoon (Pipatanakul et al., 2024), Sailor (Dou et al., 2024), EuroLLM (Martins et al., 2024), Aya ( Ustun et al., 2024), Sea-lion (Singapore, 14 H100 GPUs 15 hours $2 per hour = $120 2Links to weights and data will be shown after the review process to comply with anonymity. 1 2024), and SeaLLM (Nguyen et al., 2024), have attempted to address these limitations through continuous pretraining and post-training approaches tailored to specific target languages. However, data-centric approach to adapting reasoning capabilities for low-resource languages through knowledge distillation presents two main challenges: (1) most reasoning models do not disclose their data recipes, creating uncertainties about the impact of source prompts and reasoning trace quality on model performance, an underexplored area; (2) scaling data-centric approaches for teaching reasoning capabilities requires substantial computational resources. For example, adapting DeepSeek R1 70B involved 600K examples for distillation and 200K for general SFT, totaling 800K examples, far exceeding the 17K used in Sky-T1 (Team, 2025), Bespoke-Stratos (Labs, 2025), and similar academic efforts, which cost several thousand dollars per run. In parallel to the aforementioned efforts, model merging has shown promise in combining the weights of multiple models with different specializations, resulting in new models that improve performance across multiple tasks without requiring additional training (Siriwardhana et al., 2024; Lu et al., 2024; Akiba et al., 2025). In practice, we can frame the problem of combining two modelsone with reasoning capability and one specialized in languageas merging task between models with different specializations. This involves treating the reasoning model as one specialized component and the language-focused model as another, then integrating them via merging. In this work, we examine techniques for data selection, augmentation using only publicly available datasets, and optimize model merging ratio to integrate DeepSeek R1s reasoning capabilities into 70B Thai language model. Our goal is to enhance the reasoning capabilities of language-specific models while maintaining strong performance in the target language. We demonstrate that by leveraging publicly available datasets and an affordable computational budget, it is possible to significantly improve language-specific models to match DeepSeek R1s reasoning capabilities without compromising general language tasks performance."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "Figure 1: Overview of our Typhoon2 R1 70B recipe In this section, we first explain the motivation and process behind our approach. First, we select two specialized LLMsone proficient in target lowresource language (e.g., Thai) and the other 2 specialized in long-thought reasoning (Section 2.1). After selecting the models, we employ twostage procedure: 1. Representation Alignment via Supervised Fine-Tuning (SFT), described in Section 2.2) 2. Ability-Aware Model Merging, described in Section 2.3)"
        },
        {
            "title": "2.1 MODEL SELECTION",
            "content": "To integrate advanced reasoning with strong language capabilities in low-resource setting, we begin by selecting two LLMs that share the same foundational architecture. This architectural compatibility facilitates effective alignment and merging of learned parameters. Specifically, we choose one language-specific model specialized in Thai and another long-thought reasoning model. For our approach, we choose Typhoon2 70B Instruct (Pipatanakul et al., 2024), Thai-specialized model derived from Llama 3.1 70B. This model has undergone continuous pretraining (CPT) and alignment using Thai-focused dataset to enhance its performance in Thai. Additionally, we incorporate DeepSeek R1 70B Distill (DeepSeek-AI et al., 2025), reasoning-focused model fine-tuned from Llama 3.3 70B and SFT from an 600K-instance distilled reasoning trace dataset generated by DeepSeek R1 + 200K general SFT dataset. Both models belong to the Llama 3.1 70B family (Grattafiori et al., 2024), sharing common pretrained backbone. This shared foundation ensures well-aligned internal representations, improving the effectiveness of subsequent merging steps."
        },
        {
            "title": "2.2 REPRESENTATION ALIGNMENT VIA SUPERVISED FINE-TUNING (SFT)",
            "content": "To facilitate effective model merging, we first align the internal representations of the languagespecific and reasoning-focused LLMs through supervised fine-tuning. The goal of this step is to ensure that the two models develop similar representation space, enabling smoother integration in the subsequent merging phase. For this purpose, we construct training dataset that promotes alignment between language and reasoning capabilities. Our foundation is Bespoke-Stratos (Labs, 2025), carefully curated reasoning distillation dataset comprising 17,000 examples generated by DeepSeek R1. These examples have demonstrated state-of-the-art (SOTA) performance in reasoning tasks when fine-tuned (SFT) directly on Qwen2.5-32B-Instruct (Qwen et al., 2025)3. However, since Bespoke-Stratos is primarily in English, we adapt it for our low-resource setting by translating the question and solution components into Thai while retaining the original English reasoning traces. Intuitively, this bilingual setup aims to teach model to align Thai question-solution pairs to the original English long-form reasoning traces. This approach is inspired by work on multilingual CoT (Shi et al., 2022). In addition to this translated dataset, we explore additional datasets to assess the impact of each data type on the final merged models performance, including (i) Distillation of DeepSeek R1 longthought reasoning examples on general Thai prompts; (ii) General Thai instruction-tuning dataset."
        },
        {
            "title": "2.3 ABILITY-AWARE MODEL MERGING",
            "content": "After fine-tuning the language-specific model, we merge its parameters with those of the longthought reasoning model using merge operation, specifically, we adopt the method proposed by Yu et al. (2024). Additionally, we optimize the merge ratio based on the hypothesis that different layers contribute uniquely to distinct capabilities, as observed in early empirical experiments. Intuitively: Early to middle layers are assigned higher weight from the long-thought reasoning model, as these layers primarily handle comprehension and abstract thinking. Later layers are more influenced by the language-specific model, as they play crucial role in fluent generation in the output. The objective is to create merged model that excels in both reasoning and Thai capabilities. 3https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation"
        },
        {
            "title": "3.1 BASE MODELS AND TRAINING CONFIGURATION",
            "content": "We select Typhoon2 70B Instruct and DeepSeek R1 70B Distill as the base models for all experiments. SFT is applied to Typhoon2 70B, and we merge DeepSeek R1 70B with Typhoon2+SFT. All models are trained using LoRA with rank of 32 and α of 16, employing cosine learning rate schedule with peak learning rate of 4e-4 over single training epoch. To enhance computational efficiency, we utilize sequence packing with maximum length of 16,384, along with Liger kernels (Hsu et al., 2024), FlashAttention-2 (Dao, 2023), and DeepSpeed ZeRO-3. (Rajbhandari et al., 2020) Each experiment is conducted on 4H100 GPUs for maximum of 15 hours. Training is performed using axolotl4. Model merging is performed using Mergekit (Goddard et al., 2025)."
        },
        {
            "title": "3.2 EVALUATION",
            "content": "Evaluation is conducted on two aspects: (i) reasoning capability and (ii) performance on language tasks. First, the reasoning capability is assessed using standard benchmarks commonly employed for evaluating reasoning models, such as AIME 2024 and MATH-500 for the mathematics domain, and LiveCodeBench for the coding domain based on evaluation in Sky T1 (Team, 2025). We translate the math and code evaluation query to Thai using GPT-4o to assess the performance in Thai on reasoning tasks. Second, for languages tasks, we focus on assessing general instruction-following performance and the usability of LLMs in Thai. Which consist of IFEval, MT-Bench and Language accuracy. We also add Think accuracy, which represents how well the model can correctly output its thoughts before responding. All evaluation datasets are as follows: IFEval: We use IFEval (Zhou et al., 2023) to evaluate instruction-following capabilities based on verifiable instructions and predefined test cases, measuring accuracy as the adherence metric. Alongside the standard (English) IFEval, we employ the Thai translated version scb10x/ifeval-th5. MT-Bench: We adopt MT-Bench, an LLM-as-a-judge framework, to assess responses based on correctness, fluency, and adherence to instructions. For Thai, we use Thai MT-Bench, proposed by ThaiLLMLeaderboard (10X et al., 2024; VISTEC, 2024), while the English version follows LMSYS (Zheng et al., 2023). Language Accuracy: We use code-switching metrics based on Pipatanakul et al. (2024), inspired by IFEval (Zhou et al., 2023). Lang-acc measures the model ability to respond in the query language while adhering to natural linguistic conventions. Specifically, it must comply with two criteria: (1) Valid responses should contain only Thai and English, excluding Chinese or Russian, as these represent the languages commonly used by Thai speakers in daily life. (2) English usage must follow native conventions, with the total number of English characters being fewer than Thai characters. Evaluations are conducted using 500 sampled instructions from airesearch/WangchanThaiInstruct (Vistec, 2024), tested at = 0.7 to ensure consistency in non-greedy setting. More details are provided in A.2. Think Accuracy: Inspired by R1 think patterns, think accuracy quantifies instances where the LLM generates structured thinking trace responses across the evaluation dataset. We measure think accuracy by assessing all responses obtained from other evaluations in this setup and aggregating their tendency to think before answering by checking the presence of the < /think> token and analyzing the content between think tokens. More details are provided in Appendix A.3. MATH500: We use MATH-500 (Lightman et al., 2023) subset of the MATH benchmark Hendrycks et al. (2021), dataset consisting of 500 problems to evaluate math reasoning ability. Additionally, we include translated set, bringing the total to 1,000 problems. AIME 2024: Also to evaluate math reasoning ability, we use AIME 20246, which is derived from 2024 USA Math Olympiad (AIME). The evaluation consists of 30 challenging problems. Additionally, we translate the dataset into Thai, resulting in total of 60 problems. 4https://github.com/axolotl-ai-cloud/axolotl/ 5https://huggingface.co/datasets/scb10x/ifeval-th 6https://huggingface.co/datasets/HuggingFaceH4/aime_2024 4 LiveCodeBench: We evaluate code reasoning ability using LiveCodeBench (Jain et al., 2024), which collects coding problems from three competitive programming platforms: LeetCode, AtCoder, and CodeForces. We use the release v2 version, which contains 511 problems, along with an equally translated Thai version. We also report Average columns, which represent the average results from all evaluations. To ensure equal weighting, we multiply the MT-Bench score by 10 and use simple arithmetic mean."
        },
        {
            "title": "4.1 THE ENGLISH-THAI PERFORMANCE GAP",
            "content": "First, we examine the performance of our two base models on language and reasoning tasks to understand the performance gap between them. As shown in 1, while DeepSeek R1 70B Distill performs well in reasoning tasks such as AIME and MATH500, its performance in IFEval is slightly worse but still acceptable. However, in MTBench-TH and language accuracy tasks, it falls significantly behind language-specific models such as Typhoon2 70B Instruct. On the other hand, Typhoon2 70B Instruct struggles with most reasoning tasks, achieving only 10% accuracy in AIME and trailing DeepSeek R1 70B Distill by more than 20% in MATH500. Table 1: Performance comparison between Typhoon2 70B Instruct and DeepSeek R1 70B Distill, Showing that Typhoon2 have stronger language task performance, while DeepSeek has stronger reasoning performance. However, neither model compensates for its weakness. Experiment IFEval MT-Bench Response Acc AIME MATH500 LCB EN TH EN TH Lang Think EN TH EN TH EN TH Avg. Typhoon2 70B 88.7 81.4 8.856 7.362 98.8 Deepseek R1 70B 85.7 74.3 8.939 6.329 19.0 0.0 84.2 10.0 3.3 66.2 60.9 39.9 36.4 54.0 63.3 40.0 88.4 78.7 64.7 62.8 67."
        },
        {
            "title": "4.2 MERGE RATIO",
            "content": "Based on the performance gap shown in Section 4.1, our goal is to combine the strengths of both models. To achieve this, we investigate model merging. In this section, We design two experiments to explore the merge ratio based on the intuition described in Section 2.3. We employ merging strategy based on the dare-linear method (Yu et al., 2024) and constrain the search space by optimizing only the mixing ratios of two models: Typhoon+SFT-v1 and DeepSeek R1 70B Distill. To conduct this experiment, we fine-tuned the SFT-v1 model, which includes Bespoke-Stratos (English) and 2K Thai translation of Bespoke-Stratos. The details of the dataset used for SFT-v1 are provided in Table 6. In this case, we try to confirm our hypothesis in Section 2.3 with this two questions"
        },
        {
            "title": "4.2.1 WHICH MODEL SHOULD BE ASSIGNED A HIGHER RATIO IN THE FINAL MERGE TO BEST",
            "content": "PRESERVE ITS REASONING CAPABILITIES? In this experiment, we maintained fixed ratio of merged to single constraints across all model layers. Specifically, we applied 25%75% merge ratio to each model combination. This experiment is represented by configurations M1 (More Typhoon) and M2 (More DeepSeek). As shown in Table 3, our findings indicate that high ratio of DeepSeek R1 70B Distill in M2 improves performance across all evaluation metrics including reasoning tasks. Even within 3%7 of original DeepSeek R1 70B Distill. However, there is still degradation in the Language Accuracy task. This outcome aligns with expectations, as DeepSeek R1 70B Distill struggles to generate Thai reliably. 7An average score of 64.90% vs. 66.32% on AIME, MATH500 and LiveCodeBench. 5 Table 2: Merge config for question 4.2.1, where DS-R @ represents the DeepSeek ratio (DS-R) at layer Merge Config DS-R @ Layer 0 - Layer 80 M1 (More Typhoon) M2 (More DeepSeek) 25% 75% Table 3: Comparison between the merged models: M1 (More Typhoon) and M2 (More DeepSeek), showing that M2 performs better overall but still exhibits degradation in language accuracy. Experiment M1 IFEval EN TH MT-Bench EN Response Acc TH Lang Think AIME MATH500 LCB EN TH EN TH EN TH Avg. 57.4 58.2 7.728 6.412 86.4 86.9 76.0 8.606 6.950 59.8 96.6 100.0 26.6 26.6 82.4 78.5 43.8 44.6 61.9 46.6 50.0 89.8 83.7 58.3 61.0 72.3 Deepseek R1 70B 85.7 74.3 8.939 6.329 19.0 84. 63.3 40.0 88.4 78.7 64.7 62.8 67."
        },
        {
            "title": "4.2.2 HOW DOES INCREASING TYPHOON’S CONTRIBUTION IN THE LATER LAYERS",
            "content": "ENHANCE LANGUAGE-SPECIFIC PERFORMANCE? After finding that M2 performs better on reasoning tasks and many language tasks but has lower language response accuracy, which may reduce its usefulness for end users, we explore ways to improve language response accuracy while preserving reasoning capability. To achieve this, we allocate higher ratio to Typhoon in the later layers. This experiment is represented by M2 and M3. Table 4: Merge config for question 4.2.2, where DS-R @ represents the DeepSeek ratio (DS-R) at layer Merge Config DS-R @ Layer 0-53 DS-R @ Layer 53M2 (Constraint ratio) M3 (More Typhoon in later layer) 75% 75% 75% 75% linearly decrease to 12.5% As shown in Table 5, reducing the contribution of DS-R @ layer 80 to 12.5% and increasing Typhoons contribution to 87.5% increase language accuracy to 87.6%, representing an improvement of over 25%. Additionally, this adjustment increases MT-Bench TH scores, indicating improved Thai language performance. Overall, the language benchmark performance gap decreases from 12.2%8 to 6.8%9. Meanwhile, performance in reasoning tasks remains comparable to the M2(Constraint ratio) configuration. Based on these findings, we ultimately select M3 as our final merge configuration. Table 5: Performance comparison between the merged model with M2(Constraint ratio) and M3(More Typhoon in the later layer), showing that M3 improves language accuracy and enhances overall performance. Experiment IFEval EN TH MT-Bench EN Response Acc TH Lang Think AIME MATH500 EN TH EN TH EN TH LCB Avg. M2 M3 86.9 76.0 8.606 6.950 82.9 75.7 8.390 7.164 59.8 87.6 100.0 100. 46.6 50.0 89.8 83.7 58.3 61.0 72.3 46.6 40.0 90.0 81.9 55.9 58.5 72.9 Typhoon2 70B 88.7 81.4 8.856 7.362 98.8 0.0 10.0 3. 66.2 60.9 39.9 36.4 54.0 The merge configuration for Mergekit Goddard et al. (2025) is provided in Appendix A.4. 8An average score of 75.65% vs. 86.21% on IFEval, MT-Bench, and Language accuracy. 9An average score of 80.35% vs. 86.21% on IFEval, MT-Bench, and Language accuracy."
        },
        {
            "title": "4.3 SUPERVISED FINE TUNING (SFT): DATA MIXTURE",
            "content": "After identifying merge configuration that effectively combines the abilities of two models in Section 4.2, we focus on optimizing the data mixture for the SFT model to enhance alignment before merging, ultimately improving end-to-end performance. In this section, we explore the impact of the SFT dataset on overall model performance by addressing the following key dataset considerations 1. Does increasing the data mixture of Thai to 30% improve performance compared to 10%? - We investigate the impact of Thai-English data proportions, we add an additional 4.5k Thai translation examples based on translation of Bespoke-Stratos as in Section 2.2, which increase the Thai language ratio from 10% to 30%. 2. Does adding distilled reasoning traces on general Thai queries improve performance? - We hypothesize that Bespoke-Stratos primarily covers math, code, and puzzle domains, lacking diversity in instruction-following tasks. Does adding general-domain distillation with long-form reasoning improve performance? To test this hypothesis, we sample 1,000 prompts from the Thai general instruction dataset Suraponn/thai instruction sft10, distill responses using DeepSeek R1, and apply rejection sampling to exclude non-Thai solutions, retaining approximately 50% of the samples. The final dataset consists of 500 examples. 3. Does adding general instruction dataset improve performance? - We hypothesize that adding general instruction dataset might improve dataset diversity and help prevent catastrophic forgetting. To investigate this, we incorporate 10,000 general instruction examples. For English, we use Capybara11, and for Thai, we use Suraponn/thai instruction sft, following its usage in Typhoon 2 (Pipatanakul et al., 2024). Each dataset is subsampled to 10,000 examples to maintain balance. We construct dataset based on the above question which can is summarized in Table 6. The SFT model is later merged using the M3 configuration before evaluation, as we try to optimize for endto-end performance. Table 6: summary of the SFT data configurations used in our SFT: data mixture experiment. Dataset Language #Examples SFT-V1 SFT-V2 SFT-V3 SFT-V4 Bespoke-Stratos (Original) EN Bespoke-Stratos TH Translate (Small) TH TH Bespoke-Stratos TH Translate (Large) Deepseek R1 Distill thai instruction sft TH EN Capybara (Original) thai instruction sft (Original) TH 17K 2K 6.5K 0.5K 10K 10K Results: As shown in Table 7, we began with SFT-v1 + M3 as our baseline. First, we add 4.5k Thai translations from the Bespoke-Stratos dataset, in SFT-v2. This resulted in slight improvement, primarily in general performance for both English and Thai. Next, we incorporated 500 distilled Thai responses from DeepSeek R1, which mainly enhanced language-accuracy performance. We hypothesize that the model benefited from greater generalization on general task due to increased diversity in prompts. Further, we experimented with adding general instruction-domain data. The results were mixed, with performance remaining comparable to SFT-v3. We suspect this may be due to the instruction dataset quality. Additionally, the general instruction data spans multiple dimensions, suggesting that further investigation is needed to understand its effects comprehensively. Based on these findings, we use SFT-v3 as our final dataset mixture."
        },
        {
            "title": "4.4 DOES DIRECTLY MERGING THE ORIGINAL MODEL WORK?",
            "content": "Based on both merge configuration and SFT dataset mixture in Section 4.2 and 4.3 we also validate that whether merging alone, without any SFT, is sufficient for the model to function properly. In this 10https://huggingface.co/datasets/Suraponn/thai_instruction_sft 11https://huggingface.co/datasets/LDJnr/Capybara 7 Table 7: Performance comparison of each SFT mixture. Result in Section 4.3 Experiment IFEval MT-Bench Response Acc AIME MATH500 EN TH EN TH Lang Think EN TH EN TH TH EN LCB Avg. 82.9 75.7 8.390 7.164 87.6 SFT-v1 + M3 83.5 78.6 8.725 7.082 89.4 +Add 4.5k TH translation (SFT-v2) +Distil 500 TH general thought (SFT-v3) 85.1 75.9 8.843 7.181 96.0 77.8 77.8 8.806 6.939 93.2 +General Instruction (SFT-v4) 100.0 46.6 40.0 90.0 81.9 55.9 58.5 72.9 60.0 50.0 91.6 82.1 59.6 61.4 76.1 99.9 63.3 46.6 90.4 83.5 60.0 57.3 76.5 99.9 43.3 46.6 89.8 85.7 53.8 56.1 73.4 99.7 experiment, we compare our best model (Typhoon2+SFT-v3+M3) with directly merged version (Typhoon2+M3), skipping SFT entirely. Our results in Table 8 suggest that direct merging may not be effective, as it results in lower performance across all benchmarks. Table 8: Performance comparison between our best model(Typhoon2+SFT-v3+M3) and direct merging(Typhoon2+M3). Experiment IFEval EN TH MT-Bench EN Response Acc TH Lang Think AIME MATH LCB EN TH EN TH EN TH Avg. Typhoon2+M3 77.0 58.6 8.581 5.835 90.8 85.1 75.9 8.843 7.181 96.0 Best Model 65.0 99.9 46.6 20.0 88.2 67.9 61.0 47.3 63.9 63.3 46.6 90.4 83.5 60.0 57.3 76."
        },
        {
            "title": "4.5 DOES SFT ONLY MODEL WORK?",
            "content": "After verifying that the merged-only model does not work in Section 4.4, we also evaluate whether SFT alone, is effective. We set up the experiment in the same way as in Section 4.4. Specifically, we compare the best model (Typhoon2+SFT-v3+M3) with directly fine-tuned version (Typhoon2+SFT-v3) without merging. Although there are examples of SFT improving performance in high-resource languages (Team, 2025; Labs, 2025), our results in Table 8 suggest that direct SFT alone is not effective. It results in lower performance across all benchmarks, which may be due to the limitations of language-specific LLM capabilities, the use of LoRA in our setup, or other factorsan aspect left for future work. Table 9: Performance comparison between our best model(Typhoon2+SFT-v3+M3) and direct SFT(Typhoon2+SFT-v3). Experiment IFEval EN TH MT-Bench EN Response Acc TH Lang Think AIME MATH500 LCB EN TH EN TH EN TH Avg. Typhoon2+SFT-v3 70.3 60.9 7.868 6.412 98.6 85.1 75.9 8.843 7.181 96.0 Best Model 97.7 99.9 10.0 16.6 72.8 67.9 35.8 34.6 59.0 63.3 46.6 90.4 83.5 60.0 57.3 76."
        },
        {
            "title": "4.6 FINAL MODEL",
            "content": "Based on the combination of all experiments in this work, we found that our best model, which we call Typhoon2-R1-70B, demonstrates the feasibility of leveraging model merging to combine the reasoning ability of DeepSeek R1 70B Distill with the Thai language proficiency of Typhoon2 70B Instruct. The results, presented in Table 10, suggest that Typhoon2-R1-70B achieves performance within approximately 4%12 of Typhoon2 70B Instruct on language tasks and comparable13 on reasoning tasks. Additionally, it boosts average across all tasks performance by 41.6% over Typhoon2 70B Instruct and by 12.8% over DeepSeek R1 70B Distill. We also show additional sample responses from the model in Appendix A.5. 12An average score of 83.44% vs. 86.21% on IFEval, MT-Bench, and Language Accuracy. 13An average score of 66.85% vs 66.31% on AIME, MATH500, and LiveCodeBench. 8 Table 10: Performance comparison of Typhoon2 70B Instruct, Typhoon2 R1 70B (Best Model), and DeepSeek R1 70B Distill shows that we can combine the performance of two models into one using SFT and model merging. Experiment IFEval MT-Bench Response Acc AIME MATH500 EN TH EN TH Lang Think EN TH EN TH EN TH LCB Avg. 88.7 81.4 8.856 7.362 98.8 Typhoon2 70B Instruct Typhoon2-R1-70B(Best Model) 85.1 75.9 8.843 7.181 96.0 Deepseek R1 70B 85.7 74.3 8.939 6.329 19.0 0.0 99.9 84.2 10.0 3.3 66.2 60.9 39.9 36.4 54.0 63.3 46.6 90.4 83.5 60.0 57.3 76.5 63.3 40.0 88.4 78.7 64.7 62.8 67."
        },
        {
            "title": "4.7 ADDITIONAL MODEL & ADDITIONAL LANGUAGE",
            "content": "Based on our final model configuration (Section 4.6), we investigate whether our method can be transferred to another model. To validate this, we design an experiment applying our approach to South-east Asia (SEA) language-specific model that supports Thai: Sealion v3 70B Instruct (Singapore, 2024)14. We apply our final recipe to Sealion 70B to ensure that the method is transferable between languagespecific LLMs. As shown in 11, we find that our method successfully transfers the reasoning capability of DeepSeek R1 70B to the Sealion model despite differences in the CPT and SFT recipes, similar to its effect on Typhoon. Additionally, it preserves comparable language performance. In theory, our approach relies solely on translating the English reasoning dataset and target-language prompts. As result, it should be adaptable to any language for which language-specific model of the same reasoning model size and pretraining architecture is available. However, verifying this across additional languages is left for future work. Table 11: Performance comparison of Sealion 70B Instruct, Sealion 70B Instruct+SFT-v3+M3 (Best recipe), and DeepSeek R1 70B Distill demonstrates that this recipe can be transferred between different CPT/SFT recipes of language-specific LLMs. Experiment IFEval MT-Bench Response Acc AIME MATH500 EN TH EN TH Lang Think EN TH EN TH EN TH LCB Avg. 89.5 78.2 9.056 6.972 90.0 Sealion 70B Instruct Sealion 70B+SFT-v3+M3 83.3 78.0 8.653 7.104 90.4 85.7 74.3 8.939 6.329 19.0 Deepseek R1 70B 0.0 20.0 6.66 69.8 58.9 35.4 25.2 52.8 100.0 50.0 43.3 89.4 83.5 59.4 60.0 74.6 63.3 40.0 88.4 78.7 64.7 62.8 67.8 84."
        },
        {
            "title": "5 CONCLUSION & LIMITATION",
            "content": "In this work, we propose method to enhance reasoning in language-specific models by combining two specialize models: one language-specific and another with long-thought reasoning capability. We showed that SFT & merging can be practical resources alternative for teaching model reasoning capability, however due to combination of merging and SFT technique, it has certain limitations. Experimentally, we focus solely on merging DARE (Yu et al., 2024) with simple two-model setup and evaluate it on only one model family. Additionally, we do not optimize the instruction tuning subset, despite the availability of high-quality open-source instruction datasets such as Tulu3 (Lambert et al., 2025). At higher level, several challenges remain in the realm of multilingual reasoning and model merging. These include the absence of culturally aware reasoning traces, performance disparities between low-resource and high-resource languages, and limited understanding of the internal representations of reasoning in LLMs. Nonetheless, our goal is to advance LLMs in underrepresented languages, ensuring they remain competitive within the broader AI community. 14https://huggingface.co/aisingapore/llama3.1-70b-cpt-sea-lionv3-instruct"
        },
        {
            "title": "REFERENCES",
            "content": "SCB 10X, VISTEC, and SEACrowd. Thai LLM Leaderboard, 2024. URL https:// huggingface.co/spaces/ThaiLLM-Leaderboard/leaderboard. Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary Optimization of Model Merging Recipes, 2025. URL https://arxiv.org/abs/2403.13187. Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, 2023. URL https://arxiv.org/abs/2307.08691. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025. URL https://arxiv.org/abs/2501.12948. Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, and Min Lin. Sailor: Open Language Models for South-East Asia, 2024. URL https://arxiv.org/abs/2404. 03608. Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcees MergeKit: Toolkit for Merging Large Language Models, 2025. URL https://arxiv.org/abs/2403.13257. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan 11 Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The Llama 3 Herd of Models, 2024. URL https://arxiv.org/abs/2407.21783. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset, 2021. URL https://arxiv.org/abs/2103.03874. Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger Kernel: Efficient Triton Kernels for LLM Training. arXiv preprint arXiv:2410.10989, 2024. URL https://arxiv.org/abs/2410. 10989. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code. arXiv preprint arXiv:2403.07974, 2024. Bespoke Labs. Bespoke-Stratos: reasoning dishttps://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-"
        },
        {
            "title": "The unreasonable effectiveness of",
            "content": "tillation. reasoning-distillation, 2025. Accessed: 2025-01-22. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing Frontiers in Open Language Model Post-Training, 2025. URL https://arxiv.org/abs/2411.15124. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets Verify Step by Step, 2023. URL https://arxiv.org/abs/2305.20050. Wei Lu, Rachel K. Luu, and Markus J. Buehler. Fine-tuning large language models for domain adaptation: Exploration of training strategies, scaling, model merging and synergistic capabilities, 2024. URL https://arxiv.org/abs/2409.03444. Pedro Henrique Martins, Patrick Fernandes, Joao Alves, Nuno M. Guerreiro, Ricardo Rei, Duarte M. Alves, Jose Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, Pierre Colombo, Barry Haddow, Jose G. C. de Souza, Alexandra Birch, and Andre F. T. Martins. EuroLLM: Multilingual Language Models for Europe, 2024. URL https://arxiv.org/abs/2409. 16235. Xuan-Phi Nguyen, Wenxuan Zhang, Xin Li, Mahani Aljunied, Zhiqiang Hu, Chenhui Shen, Yew Ken Chia, Xingxuan Li, Jianyu Wang, Qingyu Tan, Liying Cheng, Guanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, and Lidong Bing. SeaLLMs Large Language Models for Southeast Asia, 2024. URL https://arxiv.org/abs/2312.00738. Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai NaThalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, and Kasima Tharnpipitchai. Typhoon 2: Family of Open Text and Multimodal Thai Large Language Models, 2024. URL https://arxiv.org/abs/2412.13702. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 Technical Report, 2025. URL https://arxiv.org/abs/2412.15115. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, 2020. URL https://arxiv.org/abs/ 1910.02054. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language Models are Multilingual Chain-of-Thought Reasoners, 2022. URL https://arxiv. org/abs/2210.03057. AI Singapore. SEA-LION (Southeast Asian Languages In One Network): Family of Large Language Models for Southeast Asia. https://github.com/aisingapore/sealion, 2024. Shamane Siriwardhana, Mark McQuade, Thomas Gauthier, Lucas Atkins, Fernando Fernandes Neto, Luke Meyers, Anneketh Vij, Tyler Odenthal, Charles Goddard, Mary MacCarthy, and Jacob Solawetz. Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and Model Merging: Comprehensive Evaluation, 2024. URL https://arxiv.org/abs/ 2406.14971. NovaSky Team. Sky-T1: Train your own O1 preview model within $450. https://novaskyai.github.io/posts/sky-t1, 2025. Accessed: 2025-01-09. Qwen Team. QwQ: Reflect Deeply on the Boundaries of the Unknown, November 2024. URL https://qwenlm.github.io/blog/qwq-32b-preview/. VISTEC. MT-Bench Thai, 2024. ThaiLLM-Leaderboard/mt-bench-thai. URL https://huggingface.co/datasets/ Vistec. airesearch/WangchanThaiInstruct, 2024. datasets/airesearch/WangchanThaiInstruct. URL https://huggingface.co/ Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language Models are Super Mario: Absorbing Abilities from Homologous Models as Free Lunch, 2024. URL https://arxiv. org/abs/2311.03099. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https: //openreview.net/forum?id=uccHPGDlao. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-Following Evaluation for Large Language Models, 2023. URL https: //arxiv.org/abs/2311.07911."
        },
        {
            "title": "Ahmet",
            "content": "Ustun, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model, 2024. URL https://arxiv.org/abs/2402.07827."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 EXAMPLE OF CODE-SWITCHING & LANGUAGE ACCURACY PROBLEM In Figure 2 and Figure 3, we demonstrate the problem more concretely. We show how codeswitching manifests in real-world situations. First, there is code-switching, where the LLMs incorporate incorrect language words into the response. Second, the model ignores the given language order and responds in its familiar language. Figure 2: Example demonstrate code-switching / language accuracy problem in DeepSeek R1 70B Distill. - The question is Which came first, the chicken or the egg? - The model generated final response, but it was unsatisfactory as it contained unnatural code-switching that not in Thai. A.2 LANGUAGE ACCURACY EVALUATION In order to evaluate language accuracy, such as the example in Appendix A.1 we focus on creating verifiable rule that has two sub-rules: 1. Valid responses should contain only Thai and English characters, excluding Chinese, Russian, or Vietnamese, as these represent languages commonly used by Thai speakers in daily life. 2. English usage must follow native conventions, with the total number of English characters being fewer than Thai characters. The verifiable rule pseudo-code is shown in A.2. the ensure To the airesearch/WangchanThaiInstruct (Vistec, 2024) (Vistec, 2024) test set, due to its authenticity and the fact that it is the only Thai instruction dataset created by humans in Thai, making it representative of real prompts that Thai person would write. To prevent unclear validation prompts works, based use we on 14 Figure 3: Example demonstrate code-switching / language accuracy problem in DeepSeek R1 70B Distill. - The question is Convert the point (0, 3) in rectangular coordinates to polar coordinates. Enter your answer in the form (r, θ), where final response, but it was entirely in Chinese, which is not the usual language in Thai. 0 θ < 2π. - The model generated > 0, instructions, we explicitly instruct the LLM to generate responses based on the prompt language.We validate accuracy based only on rule adherence, not on the correctness of the answer itselfsimilar to IFEval (Zhou et al., 2023). Additionally, we performed non-greedy sampling at temperature of 0.7 to simulate user scenarios where LLM responses is not static. Listing 1: Pseudo code for language accuracy validation function function is_mainly_thai(response: str): define thai_character_ranges define allowed_symbols (mathematical symbols, Latin with diacritics, quotes, punctuation, whitespace, emoji, etc.) initialize counters for thai_count, english_count, and other_chars set (chinese, russian, vietnamese, etc.) for each char in response: if char is Thai: increment thai_count elif char is in allowed_symbols: if char is English or Latin with diacritics: increment english_count else: add char to other_chars if other_chars is not empty: return False # contains non-Thai usage characters if english_count > thai_count: return False # content is not in Thai, but English return True 15 A.3 THINK ACCURACY EVALUATION To evaluate think accuracythe rate at which an LLM correctly utilizes the ability to think before generating responsewe define the problem as verifiable rule for both the format and content of the thought process. Specifically, DeepSeek R1 uses the think and /think tokens to separate its reasoning from the final solution. Our evaluation focuses on verifying whether the LLM-generated response meets the following criteria: 1. Does it follow the format? Does the response correctly include the <think> and < /think> tokens? 2. Does it actually think? Our initial investigation revealed that DeepSeek R1 70B Distill, even when correctly formatting its response with <think> and < /think>, sometimes generates an empty thought, such as <think>nn< /think> To apply this evaluation across various use cases, we enforce these verifiable rules on all responses generated across multiple benchmark datasets, including MT-Bench, IFEval, language-accuracy, AIME, MATH500, and LiveCodeBench. We then compute the accuracy based on the models tendency to both format its thoughts correctly and generate non-empty reasoning. The pseudocode for the verifiable rule implementation for think accuracy is provided in A.3. Listing 2: Pseudo code for think accuracy validation function function is_think(response: str): if <think> or </think> in response: think_content = extract_content_between <think> and </think> if len(think_content.strip()) >= 0: return True return False A.4 MERGE CONFIG To enhance understanding and transparency of our recipe, we provide our merge configuration for Mergekit below (Goddard et al., 2025). Listing 3: Merge config: M1 models: - model: meta-llama/Llama-3.1-70B - model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B parameters: density: [0.3, 0.3, 0.3, 0.3] weight: [0.2, 0.2, 0.2, 0.2] - model: SFT-v1 parameters: density: [1.0, 1.0, 1.0, 1.0] weight: [0.6, 0.6, 0.6, 0.6] merge_method: dare_linear base_model: meta-llama/Llama-3.1-70B parameters: normalize: true dtype: bfloat16 tokenizer: source: deepseek-ai/DeepSeek-R1-Distill-Llama-70B 16 Listing 4: Merge config: models: - model: meta-llama/Llama-3.1-70B - model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B parameters: density: [1.0, 1.0, 1.0, 1.0] weight: [0.6, 0.6, 0.6, 0.6] - model: SFT-v1 parameters: density: [0.3, 0.3, 0.3, 0.3] weight: [0.2, 0.2, 0.2, 0.2] merge_method: dare_linear base_model: meta-llama/Llama-3.1-70B parameters: normalize: true dtype: bfloat16 tokenizer: source: deepseek-ai/DeepSeek-R1-Distill-Llama-70B Listing 5: Merge config: M3 models: - model: meta-llama/Llama-3.1-70B - model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B parameters: density: [1.0, 1.0, 1.0, 0.3] weight: [0.6, 0.6, 0.6, 0.1] - model: SFT-v1 parameters: density: [0.3, 0.3, 0.3, 1.0] weight: [0.2, 0.2, 0.2, 0.7] merge_method: dare_linear base_model: meta-llama/Llama-3.1-70B parameters: normalize: true dtype: bfloat16 tokenizer: source: deepseek-ai/DeepSeek-R1-Distill-Llama-70B A.5 AN EXAMPLE RESPONSE FROM OUR MODEL Figure 4: Example from our model: The question is, Which came first, the chicken or the egg? - The model successfully responds fully in Thai while reasoning through its thought process on general question. 17 Figure 5: Example demonstrate code-switching / language accuracy problem in DeepSeek R1 70B Distill. - The question is Convert the point (0, 3) in rectangular coordinates to polar coordinates. Enter your answer in the form (r, θ), where responds fully in Thai while reasoning through its thought process on math question. 0 θ < 2π. - The model successfully > 0,"
        }
    ],
    "affiliations": [
        "SCB 10X R&D SCBX Group Bangkok, Thailand"
    ]
}