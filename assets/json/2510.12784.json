{
    "paper_title": "SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models",
    "authors": [
        "Weiyang Jin",
        "Yuwei Niu",
        "Jiaqi Liao",
        "Chengqi Duan",
        "Aoxue Li",
        "Shenghua Gao",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within a single framework. However, a significant gap exists where a model's strong visual understanding often fails to transfer to its visual generation. A model might correctly understand an image based on user instructions, yet be unable to generate a faithful image from text prompts. This phenomenon directly raises a compelling question: Can a model achieve self-improvement by using its understanding module to reward its generation module? To bridge this gap and achieve self-improvement, we introduce SRUM, a self-rewarding post-training framework that can be directly applied to existing UMMs of various designs. SRUM creates a feedback loop where the model's own understanding module acts as an internal ``evaluator'', providing corrective signals to improve its generation module, without requiring additional human-labeled data. To ensure this feedback is comprehensive, we designed a global-local dual reward system. To tackle the inherent structural complexity of images, this system offers multi-scale guidance: a \\textbf{global reward} ensures the correctness of the overall visual semantics and layout, while a \\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads to powerful capabilities and shows strong generalization, boosting performance on T2I-CompBench from 82.18 to \\textbf{88.37} and on T2I-ReasonBench from 43.82 to \\textbf{46.75}. Overall, our work establishes a powerful new paradigm for enabling a UMMs' understanding module to guide and enhance its own generation via self-rewarding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 4 8 7 2 1 . 0 1 5 2 : r SRUM: FINE-GRAINED SELF-REWARDING FOR UNIFIED MULTIMODAL MODELS Weiyang Jin1,2 Chengqi Duan1,2 Yuwei Niu2 Aoxue Li3 Jiaqi Liao2 Shenghua Gao2 Xihui Liu1,"
        },
        {
            "title": "HKU MMLab",
            "content": ""
        },
        {
            "title": "The University of Hong Kong",
            "content": "3 Noahs Ark Lab, Huawei"
        },
        {
            "title": "ABSTRACT",
            "content": "Recently, remarkable progress has been made in Unified Multimodal Models (UMMs), which integrate vision-language generation and understanding capabilities within single framework. However, significant gap exists where models strong visual understanding often fails to transfer to its visual generation. model might correctly understand an image based on user instructions, yet be unable to generate faithful image from text prompts. This phenomenon directly raises compelling question: Can model achieve self-improvement by using its understanding module to reward its generation module? To bridge this gap and achieve self-improvement, we introduce SRUM, self-rewarding post-training framework that can be directly applied to existing UMMs of various designs. SRUM creates feedback loop where the models own understanding module acts as an internal evaluator, providing corrective signals to improve its generation module, without requiring additional human-labeled data. To ensure this feedback is comprehensive, we designed global-local dual reward system. To tackle the inherent structural complexity of images, this system offers multi-scale guidance: global reward ensures the correctness of the overall visual semantics and layout, while local reward refines fine-grained, object-level fidelity. SRUM leads to powerful capabilities and shows strong generalization, boosting performance on T2ICompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82 to 46.75. Overall, our work establishes powerful new paradigm for enabling UMMs understanding module to guide and enhance its own generation via self-rewarding."
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-to-Image (T2I) models have achieved remarkable progress in generating high-quality and diverse images from given prompts (Ramesh et al., 2021; Saharia et al., 2022; Podell et al., 2024). However, they often fail to accurately interpret instructions involving world knowledge, complex spatial relationships, detailed attribute binding, or compositional reasoning (Huang et al., 2023). These limitations point to fundamental lack of deep semantic understanding in standard T2I models. To address this challenge, researchers have developed Unified Multimodal Models (UMMs), which integrate both understanding and generation capabilities within single framework (Wu et al., 2024b;a; Dong et al., 2024; Xie et al., 2024). By sharing common backbone, UMMs possess the inherent potential for synergy, offering promising path to resolve the comprehension challenges that plague traditional T2I models. Despite their advanced architecture, fundamental paradox plagues current UMMs: their capacity to generate falls far behind their ability to understand (Tong et al., 2024a; Chen et al., 2025b; Pan et al., 2025; Xie et al., 2025b; Wang et al., 2024d). For instance, model can often correctly judge the alignment between detailed prompt and complex image, yet be incapable of generating faithful image from that same prompt (Figure 1). This persistent gap between understanding and generation suggests that the key to unlocking better generation lies within the model itself. Equal contribution. Corresponding Author. Project page: https://waynejin0918.github.io/srum_web/ 1 To address this challenge, we propose bridging this module gap through self-rewarding. We introduce Self-Rewarding for Unified Multimodal Models (SRUM), novel post-training framework designed to create synergistic feedback loop within the model itself. Our core insight is that the solution lies within the UMMs own architecture. By treating the generation module as student and the more capable understanding module as an internal teacher or evaluator, we establish self-contained system for improvement, obviating the need for external supervision (like reward models and human labels) or additional image data during its training phase. Furthermore, to effectively guide the generation of complex scenes, reward signal should provide multi-scale feedback. As our ablation studies confirm, single, holistic score is insufficient because it fails to provide the fine-grained corrective signals needed for detailed improvement. Therefore, we propose global-local dual reward framework. The global reward evaluates high-level compositional coherence to ensure overall scene plausibility. Concurrently, the local reward targets object-level details, optimizing attribute binding and spatial arrangements. This synergistic design enables SRUM to enhance the performance of the base model on complex generation tasks. Figure 1: The example on the left suggests that the current UMMs understanding module has exceeded the capability of its generation module: the generation module is prone to producing incorrect candidate images based on given prompt in relevant scenarios, situation which the understanding module can reasonably identify. This not only highlights gap between understanding and generation but also reveals the potential for understanding to guide generation. Inspired by this insight, we propose SRUM to bridge this gap, particularly in complex generation domains. Through extensive experiments, we demonstrate that our approach significantly improves the composition, reasoning, and visual fidelity of UMMs, showing strong generalization across indomain and out-of-domain settings. SRUM achieves SOTA results on T2I-CompBench and T2IReasonBench, improving the overall score of strong baseline model from 82.18 to 88.37 in composition and from 43.82 to 46.75 in reasoning. Our key contributions can be summarized as follows: We are the first to propose comprehensive self-rewarding framework for UMMs at the posttraining stage, successfully bridging the gap between their understanding and generation. We introduce novel dual reward design that combines global compositional assessment with local object-level feedback, providing solid and multi-scale guidance during model training. We achieve better performance on complex compositional generation and demonstrate strong generalization. Ultimately, SRUM establishes powerful paradigm for UMMs understanding module to guide its own generation module to achieve self-improvement."
        },
        {
            "title": "2.1 UNIFIED MULTIMODAL MODELS",
            "content": "Unified Multimodal Models (UMMs) have emerged as prominent direction in multimodal learning, aiming to integrate diverse tasks, such as visual understanding and generation, within single end-to-end trained architecture. By consolidating multiple capabilities into one model, UMMs seek to promote synergy across modalities and reduce systemic complexity. Recent works can be broadly categorized into several architectural paradigms: Autoregressive (AR) Models. Several UMMs, including Chameleon (Team, 2024), Janus (Wu et al., 2024a), and Emu3 (Wang et al., 2024d), employ autoregressive generation, tokenizing visual inputs and generating outputs sequentially. Show-O (Xie et al., 2024) extends this by integrating discrete-diffusion schedule to refine token prediction. AR with Diffusion Head. Another line of work combines autoregressive modeling with diffusion-based decoders, such as Transfusion. Some methods keep pre-trained MLLM frozen for reasoning and route its features via learnable queries to an external image generator (Tong et al., 2024a; Shi et al., 2024; Lin et al., 2025), facilitating complex multimodal interactions. third approach, Integrated Transformers (Zhao et al., 2024; Chen et al., 2024a), unifies both paradigms within single transformer backbone to eliminate bottlenecks. Notably, to improve the scalability of these architectures, the Mixture-of-Transformers (MoT) (Liang et al., 2025; Deng et al., 2025) paradigm has been introduced, which employs sparse and modular design by Bagel. SRUM inherits the basic framework and demonstrates the versatility of the method on UMMs. 2.2 POST-TRAINING STAGE IN UMMS In addition to architectural innovations, considerable research has focused on post-training strategies to enhance the generative abilities of UMMs. Methods such as Chain-of-Thought (CoT) and testtime verification introduce explicit reasoning steps or iterative output validation (Guo et al., 2025b; Fang et al., 2025; Duan et al., 2025). However, these often depend on external models and do not fundamentally improve the native generative capacity of the UMMs. Reinforcement learning techniquesincluding Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO) which leverage human or automated feedback to refine generation policies. While effective, these require carefully curated paired data and delicate advantage function tuning (Rafailov et al., 2023; Guo et al., 2025a). Reconstruction Alignment (RecA) introduces post-training method based on reconstruction loss, yielding improved semantic understanding (Xie et al., 2025a). Some work has also attempted to use rule-level rewards for guidance, but this is not universal and needs to be designed for different tasks (Hong et al., 2025; Mao et al., 2025; Han et al., 2025). In contrast, SRUM operates without additional data generation. It leverages the models inherent understanding to score self-generated samples and incorporates them into training, thereby enhancing performance. 2.3 SELF-REWARDING IN UNDERSTANDING MODELS Self-rewarding mechanisms have emerged as significant paradigm for enhancing the understanding and reasoning capabilities of MLLMs. These approaches aim to reduce reliance on external preference data by enabling models to generate their own reward signals, facilitating continuous selfimprovement. For instance, CSR (Zhou et al., 2024) achieves zero-cost self-enhancement through iterative online DPO with visual constraint rewards. SRPO (Choi et al., 2024) introduces two-stage reflective reward mechanism, significantly improving the quality of reflection and answer accuracy in complex reasoning tasks. R1-Reward leverages process consistency rewards and stable reinforcement learning algorithms to enhance long-range reasoning stability (Guo et al., 2025a). Collectively, these works signal paradigm shift from external rewards to self-criticism and optimization. However, they tend to focus on single dimension of feedback, such as visual grounding, reflective critique, or reasoning consistency. Building on this momentum, our SRUM framework proposes more holistic approach. It distinguishes itself by incorporating global-local dual reward system designed to provide more comprehensive training signal. The global reward assesses the overall quality of the final output, while the local reward offers fine-grained feedback on the accuracy of intermediate steps and multimodal grounding. Furthermore, rather than appending an external reward function, SRUM strategically leverages its own internal modules to facilitate this dual-level critique, enabling more cohesive and efficient self-improvement cycle within the UMMs framework."
        },
        {
            "title": "3 SRUM: SELF-REWARDING FOR UNIFIED MULTIMODAL MODELS",
            "content": "Figure 2: Showcase of the SRUM pipeline. It consists of two main stages: Self-Rewarding Data Generation and Reward-Weighted Training. The first stage generates high-quality data and scores it to produce reward signal for the next training stage for self-improvement. To drive self-improvement where the models understanding capabilities guide its generation abilities, we established multi-stage self-rewarding process step by step. First, the Unified Multimodal Models (UMMs) generate high-quality candidate images with corresponding bounding boxes (as detailed in Section 3.1). Next, these candidates undergo meticulous evaluation using global-local judgment framework that assesses both the overall composition and fine-grained details, ensuring holistic judgment for rewarding (Section 3.2). Finally, all of rewards directly inform rewardweighted training process, which enables targeted, region-specific optimization and effectively prevents reward hacking (Section 3.3). 3.1 IMAGE CANDIDATES AND BOUNDING BOX GENERATION We developed self-data generation pipeline that enables our model to create its own training data, removing the need for external image resources. This data, consisting of image-text pairs with their bounding boxes, is formatted to be rewarded by the models own understanding module. The pipeline initiates with the Unified Multimodal Models (UMMs) using its think mode (a form of CoT) to generate images of high semantic quality (Deng et al., 2025; Wang et al., 2025). While an external model initially proposes bounding boxes for grounding by understanding module (Kirillov et al., 2023), the next step is the subsequent verification: the UMMs own understanding module assesses and filters these boxes against the original prompts. This validation ensures that the resulting dataset is precisely grounded and perfectly suited for the downstream self-rewarding task. 3.2 REWARDING PROCESS Self-Judgment for Reliable Rewarding. cornerstone of self-improvement is enabling the models internal understanding module to serve as stable and reliable evaluator. To ensure the scores it generates are consistently trustworthy, we designed comprehensive self-judgment mechanism to meticulously assess image quality and prompt alignment (Xu et al., 2023; Zhang et al., 2023b; Lin et al., 2024; Ghosh et al., 2023). This dual-level Judgment is key to guaranteeing the assessment is thorough. First, local judgment evaluates object-level fidelity and artifacts on strict [1.0, 1.0] scoring scale. mandatory Reason field elicits an interpretable rationale for the score, akin to chain-of-thought prompting (Guo et al., 2025b; Fang et al., 2025), which further bolsters the reliability of the process. We enforce semantic grounding by verifying that identified objects correspond to prompt keywords, and non-linear penalty maps severe distortions to high-penalty negative range (e.g., -0.9 to -0.5) to better reflect human visual sensitivity. Subsequently, global judgment evaluates the holistic composition and spatial alignment with the prompts intent. Crucially, for prompts lacking specific compositional directives (e.g., picture of tree), neutral score range (e.g., -0.4 to 0.4) is applied, which ensures fair and solid assessment. Rewards Generation for Training. To serve as the core learning signal for self-improvement, the reliable scores from the self-judgment phase are converted into dense reward map. This critical step 4 translates abstract textual evaluations into tangible, spatially-aware feedback. The process begins by leveraging the UMMs grounding capabilities to generate two types of rewards: fine-grained local reward scores for all prompt-relevant image regions (both foreground and background) and single global reward score for the entire image. To ensure it functions as valid quality weight, the global score is normalized to the [0, 1] range; this prevents issues such as two negative values creating spurious positive signal (detailed in Appendix Section C)."
        },
        {
            "title": "3.3 REWARD-WEIGHTED TRAINING",
            "content": "The reward-weighted training stage is where the model achieves self-improvement through training with rewards. The core objective is to translate the capabilities of the understanding module directly into the functionality of the generation module. By using fine-grained local rewards and layoutaware global rewards to weight the training objective, we guide the generator to learn more detailed and accurate patterns from the original data. This process is the key to bridging the gap between the models understanding and generation components, enabling the generator to benefit from the insights of the evaluator. The mechanism for this goal is reward-weighted training objective, centered on the loss term Lr. This term operates on the models velocity prediction vθ, standard practice in flow-based frameworks (Liu et al., 2023b; Lipman et al., 2023). The loss is modulated by two feedback signals from the understanding module: regional reward map [1, 1] for localized refinement and global scalar α for overall compositional quality. The product of these signals, α R, weights the squared error between the predicted velocity vθ and the target velocity derived from the original latent xgt 0 . This allows for fine-grained control, encouraging preservation where feedback is positive (α > 0) and promoting change where it is negative (α < 0): Lr = (cid:104) α (cid:0)vθ (ϵ xgt 0 )(cid:1)2(cid:105) (1) Second, to ensure the models output conforms to the desired overall structure and to prevent reward hacking, we introduce reference constraint term, Lref. This term acts as regularizer, penalizing the squared ℓ2 distance to the target velocity of the artifact-free latent xgt 0 : Lref = (cid:104)(cid:13) (cid:13)vθ (ϵ xgt 0 )(cid:13) (cid:13) 2(cid:105) (2) The final training objective is weighted sum of these two losses, balanced by hyperparameter λc. This composite design enables targeted local refinement while maintaining global coherence, effectively translating the understanding modules assessments into generative improvements without distorting the overall output distribution from base model: LTotal = Lr + λc Lref (3)"
        },
        {
            "title": "4 ANALYSIS OF SELF-REWARDING: GENERALIZATION AND PRINCIPLES",
            "content": "We validate our Self-Rewarding for Unified Multimodal Models (SRUM) method across various unified multimodal models (UMMs) and evaluation benchmarks. In particular, we investigate the following aspects: Generality and Performance: SRUM achieves better performance on compositional generation and delivers consistent performance gains across different frameworks.  (Table 1)  Component Efficacy: Ablation studies confirm that each component of the SRUM framework makes critical contribution to the overall performance. (Figure 3) Generalization: SRUM demonstrates in-domain and out-of-domain generalization, indicating its improvements in generation translating from understanding. (Table 3, Figure 6, Table 4) 4.1 EXPERIMENTAL SETUP Model Architectures. We evaluate SRUM on two powerful open-source UMMs. All experiments are conducted as post-training phase, starting from the official pre-trained weights. Bagel (Deng 5 et al., 2025) is versatile UMM that serves as our primary model for comprehensive analysis, including main results, ablation studies, and generalization tests. We evaluate both its standard and Chain-of-Thought (CoT) inference modes. Blip3o (Chen et al., 2025a) is another one of current SOTA UMMs used to validate the generality and effectiveness of our proposed SRUM with frozen MLLM training. Notably, our discussion is confined to AR with Diffusion head and MoT-type models, which represent the current SOTA in UMMs. AR-type models, such as the Show-O or janus, may exhibit biases stemming from under-leveraged generation and understanding capabilities. Datasets and Benchmarks. Our experiments leverage several specialized datasets for training and evaluation to ensure thorough and multi-faceted analysis. For consistent and objective scoring across all generation benchmarks, we employ QwenVL-2.5-32B/QwenVL-2.5-72B (Bai et al., 2025) as the designated multimodal evaluator. Our experiment begins with instruction data sourced from the T2I-CompBench training set (Huang et al., 2023). For our primary evaluation, we use the standard split of the same benchmark to compare SRUM-enhanced models against leading T2I and UMMs baselines. To assess generalization, we evaluate the models in-domain transferability on GenEval (Ghosh et al., 2023) which includes similar compositional challenges and WISE (Niu et al., 2025) with knowledge-informed generation evaluation. Furthermore, we evaluate in broader, outof-domain reasoning-informed capabilities on T2I-ReasonBench (Sun et al., 2025), benchmark containing complex prompts that require knowledge beyond the training distribution. 4.2 MAIN RESULTS Model 3d Spatial Color Complex Nonspatial Numeracy Shape Spatial Texture Overall FLUX.1-dev FLUX.1-schnell SD-3-medium SD-xl-base-1 Janus-Pro Show-O2 OmniGen2 BLIP3o +SRUM Bagel +SRUM Bagel(CoT) +SRUM 76.39 79.38 77.83 72. 76.17 88.61 82.21 81.73 83.78 77.98 83.10 84.66 88.60 90.63 84.53 91.63 77.75 84.25 87.73 92. 89.92 90.22 89.30 92.90 88.85 92.90 T2I Models 83.51 81.96 84.73 75.00 87.47 85.55 86.12 85. 75.30 72.82 72.80 57.14 80.20 82.20 83.72 72.18 84.23 85.49 88.20 77.08 Unified Multimodal Models 80.28 87.88 86.87 85.55 86. 83.32 88.69 86.10 91.31 80.47 85.91 88.51 84.78 85.10 85.03 88.47 85.64 90. 56.43 69.74 72.00 71.67 74.52 70.40 78.52 75.36 80.12 65.14 73.99 83.95 83.75 85. 81.94 84.23 84.33 84.47 79.67 86.60 90.07 92.47 93.88 81.52 86.92 82.71 89. 87.07 86.38 89.03 78.38 69.67 82.17 90.88 87.45 86.52 87.93 89.57 88.07 89.15 83.10 82.29 84.26 74. 74.01 82.83 85.84 84.66 85.75 82.18 86.55 84.46 88.37 Table 1: Comprehensive T2I-CompBench Results. This table includes T2I (Labs, 2024; Esser et al., 2024; Podell et al., 2024) and UMMs (Chen et al., 2025b; Xie et al., 2025b). Models incorporating the SRUM are denoted with +SRUM. Bold values indicate the highest score in each respective column under. Green values indicate the improvements. As shown in Table 1, our proposed method, SRUM, achieves consistent and substantial performance gains across various compositional generation tasks. Specifically, when evaluating with CoT mode, Bagel+SRUM attains an overall score of 88.37, ranking first among current UMMs baselines. This marks significant improvement of +3.91 points over the baseline Bagel with CoT, demonstrating the efficacy of our approach. The advantages of SRUM are particularly pronounced in categories demanding spatial and complex reasoning as well as numeracy problems. For instance, our method sets new SOTA scores in Spatial (93.88), 3D Spatial (88.60) and Complex (91.31) reasoning including 3D and action parts. Although we noticed slight drop in performance for texture and 6 color categories in some cases, the overall trend remains very positive, which might be because our algorithm does not overly focus on low-level information of certain objects."
        },
        {
            "title": "4.3 EMPIRICAL STUDY",
            "content": "We primarily employed three basic models for Bagel analysis: Base Model, Bagels open-source weights are used directly for inference. SFT Model, Bagel generates images based on training instructions, then directly trains the model itself to create self-training SFT model. SRUM Model, we use the SRUM framework on Bagel training to obtain the final evaluation model. Ablation Results. To further verify the effectiveness of our proposed reward configuration, we perform an ablation study on the results of Bagel on T2I-CompBench by systematically modifying the reward scheme. As shown in Figure 3 (Left), our full SRUM model achieves the highest overall accuracy, with the ablation results confirming the critical role of each component. The omission of the global reward led to notable decrease in performance, underscoring its importance for capturing the overarching coherence and compositional structure of the generated images. Removing the KL constraint resulted in less severe but still significant drop, proving its value in ensuring training stability. This aligns with conclusions from post-training methods like Direct Preference Optimization (DPO) (Rafailov et al., 2023), where such constraint is essential to prevent the model from significant policy deviation due to reward hacking. Furthermore, using simple sparse reward led to significant performance degradation, reinforcing the necessity of continuous, dense reward signal for providing richer gradient information. This is particularly evident as sparse reward schemes, such as Dance-GRPO (Xue et al., 2025), are ill-suited for providing granular regional feedback, which highlights the value of our dense reward design. Overall, this ablation confirms the efficacy of our framework stems from the synergistic contributions of each component. Figure 3: Left: Module Evaluation. We report the accuracy drop ( Acc. %) from our SRUM. Specifically, 0-1 Reward represents the sparse reward. Right: Hyperparameters Evaluation on T2ICompBench. We report the accuracy in different λ under two modes: CoT and without CoT. In the Figure 3 Right, we analyze the effect of different constraint ratios on the experimental outcomes. Across both Bagel with CoT and without CoT configurations, the results consistently indicate that λc = 0.5 is the most effective choice. Consequently, we set this hyperparameter as fixed one in our subsequent experiments for more significant evaluation results. Further Analysis. For more granular investigation, we leverage the same powerful MLLM like QwenVL-2.5-72B from our primary evaluation to conduct deeper analysis of our method and the baseline. Specifically, we employ the MLLM to perform step-by-step scoring of the inference process. The evaluation is divided into two metrics: (1) layout, which assesses the concordance of the overall structure and quality, and (2) detail, which measures the fidelity of the generated finegrained details. Our ablation study, visualized in Figure 4, systematically isolates the effects of each component. We observe that the think mode primarily bolsters the initial layout generation by improving the high-level reasoning process. The global reward component of SRUM then further refines this layout during the early stages of inference. In contrast, baseline using only this global reward (labeled sample reward) yields negligible improvements in detail fidelity. This highlights crucial finding: the fine-grained, local rewards are essential for the subsequent optimization of details, with their benefits becoming most apparent in the later inference steps. Collectively, these (a) Detail score per step during inference (b) Layout score per step during inference Figure 4: Score per step during inference in Bagel with its ablation models. results demonstrate that our dual global-local reward mechanism provides multi-stage optimization path: first establishing coherent layout and then progressively refining the details. This synergistic approach allows SRUM to significantly outperform standard SFT on same self-generated data. MME-P MME-C MMBench MM-Vet MMMU MathVista MMVP Base 1687 701 85.0 67.2 55.3 73.1 69.3 SFT SRUM 1682 683 84.6 66.5 55.0 72.8 68.7 1673 677 84.8 67.0 55.2 73.0 70.0 Figure 5: Functional cluster activation patterns of the different models (Bagel, SFT and SRUM) on understanding and generation tasks. The average activation strength of Understanding and Generation clusters is shown. Table 2: Comparison with the results of different models (Bagel, SFT and SRUM) on understanding benchmarks. MMEP and MME-C represents the perception and the cognition part respectively. Impact on Understanding Module. As shown in Table 2, our method has minimal impact on the models core understanding capabilities. On prevalent benchmarks such as MME (Fu et al., 2023), MM-Vet (Yu et al., 2024b), MMBench (Liu et al., 2024b), MMMU (Yue et al., 2024), and MathVista (Lu et al., 2023), the results exhibit only marginal fluctuations compared to the base version. Notably, performance on MMVP (Tong et al., 2024b) even improves which consistent with prior works (Tong et al., 2024a; Wang et al., 2024c;a). This indicates that our method holds significant potential for further iterative enhancement. In Figure 5, by tracking the activation of Understanding and Generation functional clusters, we found that SFT specializes by suppressing irrelevant clusters (a narrowing effect). In contrast, SRUM enhances the primary task-relevant cluster while maintaining supportive activation in secondary ones (an enhancing and orchestrating effect). This promotes more robust and generalizable function. Details can be seen in Appendix Section B. In-Domain Generalization. We then investigate the in-domain generalization capability of our model. We posit that the compositional abilities learned from the T2I-CompBench training set should be transferable to other benchmarks with similar evaluation perspectives. To test this hypoth8 esis, we evaluate SRUM on the GenEval benchmark without any further fine-tuning. The comparative results are summarized in Table 3. Model Single obj. Two obj. Counting Colors Position Color attr. Bagel Bagel+SFT Bagel+SRUM 0.99 0.96 0.98 0.94 0.94 0.94 0.81 0.79 0. 0.88 0.92 0.90 0.64 0.59 0.64 0.82 0.78 0.83 Table 3: Results on key visual attributes at GenEval. For brevity, some model names have been shortened: The meaning of the abbreviation can be found at the beginning of the Section 4.3. Bold values are the best in each column. As shown in the table, our evaluation on GenEval further validates the strengths of SRUM, particularly in the challenging domain of object counting. SRUM attains the highest score of 0.83 in Counting, surpassing both the base model and the SFT baseline. Crucially, this superior performance in numerical generation aligns perfectly with our previous results on T2I-CompBench. This consistency across benchmarks underscores our methods reliable improvement in processing quantitative information. By excelling at complex task like counting while retaining proficiency in simpler ones, the model demonstrates strong in-domain generalization. This confirms that the targeted enhancements by SRUM are transferable improvement. In-Domain Knowledge-based Generalization. Following this, we explore whether our method holds distinct advantage for the task of reasoning generation, current area of focus in the community. Consequently, we designed an experiment wherein we train the model on one category of prompts from the WISE Benchmark and perform in-domain evaluations on the remaining two categories. This method allows us to construct three distinct evaluation sets for thorough analysis of the models generalization capabilities. Figure 6: The results of Bagel on WISE. We use one of the three tasks in WISE and evaluate on the other two, which shows the knowledge in-domain generalization and translation for SRUM. As illustrated in Figure 6, selecting any single group for training generally enhances the image generation performance of the other two groups. This improvement is consistent across both standard and CoT reasoning paradigms. It shows that the SRUM can promote generalization in the knowledge field, so that the generation can better fit the instruction semantics at the knowledge level. Out-of-Domain Knowledge-based Generalization. To further evaluate the generalization capability of our model on unseen domains, we utilize T2I-ReasonBench, large-scale and wellregarded benchmark for analyzing the reasoning quality of generated images. In this experiment, we take the model trained with T2I-CompBench prompts and directly evaluate its performance on this benchmark. This setup is designed to demonstrate the models ability to generalize to advanced, reasoning-informed image generation tasks. We primarily focus on the accuracy scores, which measure the models high-level semantic alignment with the given prompts. To prevent self-rewrite from directly parsing hidden high-level semantics (e.g., in the Idiom category, phrase like piece of cake might be literally interpreted as easy, which would obscure the models ability to transfer understanding and could interfere with the evaluation), we use bagel without CoT during evaluation. 9 Model Entity Idiom Scientific Textual Overall Bagel Bagel+SFT Bagel+SRUM 49.70 50.53 52.85 34.46 39.43 40.51 47.52 47.45 47.83 43.59 44.08 45. 43.82 45.37 46.75 Table 4: Performance comparison of Bagel models across four categories and their overrvall scores. Bold values indicate the best performance in each column. Scores are normalized between 0-100. As illustrated in the Table 4, our SRUM method achieves superior understanding of the given prompts compared to both the SFT and Base models. While SFT also yields noticeable improvement, the enhanced performance of SRUM demonstrates that our approach effectively improves generalization on complex problems from both data and an algorithmic perspective. Furthermore, in the evaluation of image-based prompts, SRUM provides consistent improvements, in stark contrast to the volatility exhibited by the SFT model. This further substantiates that our algorithmic design is more adaptable, taking into account more nuanced factors."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This paper introduces SRUM, fine-grained post-training framework that enables models understanding module to reward its generation module. Additionally, SRUM decomposes the reward into local and global components, facilitating multi-scale alignment and refinement. Extensive experiments validate SRUMs effectiveness, setting new state-of-the-art results on complex compositional and reasoning benchmarks such as T2I-CompBench and T2I-ReasonBench. The framework demonstrates robust in-domain and out-of-domain generalization, and our empirical analysis confirms the efficacy of the fine-grained reward design. These findings illuminate the synergistic development of understanding and generation capabilities within single model and establish the principle of self-reward as promising direction for future research. SRUM is just preliminary exploration of Unified Multimodal Models (UMMs). We found that there is still room for improvement in the prompts for the understanding part during the scoring phase, and we hope to scale this method to larger datasets. This article also utilizes some external prompts to improve performance for illustrative purposes. In fact, it is entirely possible to allow the understanding part to self-play questions and answers to build more closed-loop training system."
        },
        {
            "title": "6 ACKNOWLEDGEMENT",
            "content": "We are grateful to Ning Gao and Ji Xie for the excellent visualization template. We also extend our sincere thanks to Shengbang Tong, Xichen Pan for their insightful discussions and suggestions, which greatly inspire the SRUM project."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Jinbin Bai, Wei Chow, Ling Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, and Shuicheng Yan. Humanedit: high-quality human-rewarded dataset for instruction-based image editing. arXiv preprint arXiv:2412.04280, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 10 James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. OpenAI blog, 2023. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent SitzIn NeurIPS, mann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. 2024a. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Xiaokang Chen, Chengyue Wu, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. SCIS, 2024b. Eugene Choi, Arash Ahmadian, Matthieu Geist, Oilvier Pietquin, and Mohammad Gheshlaghi Azar. Self-improving robust preference optimization. arXiv preprint arXiv:2406.01660, 2024. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. In ICLR, 2024. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Chengqi Duan, Rongyao Fang, Yuqing Wang, Kun Wang, Linjiang Huang, Xingyu Zeng, Hongsheng Li, and Xihui Liu. Got-r1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning. arXiv preprint arXiv:2505.17022, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, et al. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2023. Goyal. Accurate, large minibatch sg d: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. 11 Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Rui Huang, Haoquan Zhang, Manyuan Zhang, Jiaming Liu, Shanghang Zhang, Peng Gao, et al. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025b. Yujin Han, Hao Chen, Andi Han, Zhiheng Wang, Xinyu Lin, Yingya Zhang, Shiwei Zhang, and Difan Zou. Self-contradiction as self-improvement: Mitigating the generation-understanding gap in mllms. arXiv preprint arXiv:2507.16663, 2025. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models. arXiv preprint arxiv:2203.15556, 2022. Jixiang Hong, Yiran Zhang, Guanzhong Wang, Yi Liu, Ji-Rong Wen, and Rui Yan. Reinforcing multimodal understanding and generation with dual self-rewards. arXiv preprint arXiv:2506.07963, 2025. Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies. In COLM, 2024. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. In ICML, 2020. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Black Forest Labs. Flux, 2024. URL https://github.com/black-forest-labs/flux. Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, et al. Omnicorpus: unified multimodal corpus of 10 billion-level images interleaved with text. arXiv preprint arXiv:2406.08418, 2024a. Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, et al. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024b. Weixin Liang, LILI YU, Liang Luo, Srini Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen tau Yih, Luke Zettlemoyer, and Xi Victoria Lin. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models. TMLR, 2025. Jiaqi Liao, Yuwei Niu, Fanqing Meng, Hao Li, Changyao Tian, Yinuo Du, Yuwen Xiong, Dianqi Li, Xizhou Zhu, Li Yuan, et al. Langbridge: Interpreting image as combination of language embeddings. arXiv preprint arXiv:2503.19404, 2025. 12 Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pp. 366384. Springer, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36:3489234916, 2023a. Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. In ECCV, 2024a. Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024b. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In NeurIPS Workshop on Mathematical Reasoning and AI, 2023. Weijia Mao, Zhenheng Yang, and Mike Zheng Shou. Unirl: Self-improving unified multimodal models via supervised and reinforcement learning. arXiv preprint arXiv:2505.23380, 2025. Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, et al. theory on adam instability in largescale machine learning. arXiv preprint arXiv:2304.09871, 2023. Yuwei Niu, Munan Ning, Mengren Zheng, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, pp. 1068410695, 2022. 13 Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, pp. 22562265. PMLR, 2015. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Kaiyue Sun, Rongyao Fang, Chengqi Duan, Xian Liu, and Xihui Liu. T2i-reasonbench: Benchmarking reasoning-informed text-to-image generation. arXiv preprint arXiv:2508.17472, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arxiv:2312.11805, 2023. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024a. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, pp. 95689578, 2024b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. arXiv preprint arXiv:2410.09575, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Wenxuan Wang, Quan Sun, Fan Zhang, Yepeng Tang, Jing Liu, and Xinlong Wang. Diffusion feedback helps clip see better. arXiv preprint arXiv:2407.20171, 2024c. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arxiv:2409.18869, 2024d. Yi Wang, Mushui Liu, Wanggui He, Longxiang Zhang, Ziwei Huang, Guanghao Zhang, Fangxun Shu, Zhong Tao, Dong She, Zhelun Yu, et al. Mint: Multi-modal chain of thought in unified generative models for enhanced image generation. arXiv preprint arXiv:2503.01298, 2025. 14 Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In ICLR, 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024a. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, and Yao Lu. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024b. URL https://arxiv.org/abs/2409.04429. Ji Xie, Trevor Darrell, Luke Zettlemoyer, and XuDong Wang. Reconstruction alignment improves unified multimodal models, 2025a. URL https://arxiv.org/abs/2509.07295. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arxiv:2408.12528, 2024. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025b. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024a. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In ICML, 2024b. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. In NeurIPS, 2023a. Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang Yang, and Kede Ma. Blind image quality assessment via vision-language correspondence: multitask learning perspective. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1407114081, 2023b. Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao, and Jingdong Wang. Monoformer: One transformer for both diffusion and autoregression. arXiv preprint arXiv:2409.16280, 2024. URL https://arxiv.org/abs/2409.16280. Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. Calibrated self-rewarding vision language models. Advances in Neural Information Processing Systems, 37:5150351531, 2024."
        },
        {
            "title": "A DETAIL SETTINGS",
            "content": "Following the configuration of stage 4 from the Bagel (Deng et al., 2025) framework during our post-training phase, we employed the AdamW optimizer (Loshchilov, 2017), configured with momentum parameters β1 = 0.9 and β2 = 0.95. Drawing inspiration from (Molybog et al., 2023), we set the epsilon value to 1.0 1015 to mitigate loss spikes. When we increase the resolution during generation, we also adjust the diffusion timestep from 1.0 to 4.0, which helps maintain stable noise-level distribution. We chose constant learning rate, as this approach, as suggested by (Hu et al., 2024), simplifies the scaling of training data without needing to restart the training process. These empirical observations, along with established practices for large-scale model training (Goyal, 2017; Hoffmann et al., 2022; Kaplan et al., 2020; Liao et al., 2025), informed our final training protocol. Our model architecture builds upon the standard Transformer (Vaswani et al., 2017) and ViT (Dosovitskiy et al., 2021) paradigms, incorporating modern enhancements for stability and efficiency, such as RMS Layer Normalization (Zhang & Sennrich, 2019), GLU variants for activation functions (Shazeer, 2020), RoPE (Su et al., 2024), and GQA (Ainslie et al., 2023). The generative process is fundamentally based on principles from diffusion process (Ho et al., 2020; Sohl-Dickstein et al., 2015; Song et al., 2021), and utilizes classifier-free guidance (Ho & Salimans, 2022) within latent space (Rombach et al., 2022) for high-resolution synthesis. The complete training recipe is summarized in Table 5. Hyperparameters Learning rate LR scheduler Weight decay Gradient norm clip Optimizer Warm-up steps Max context window Gen resolution (min short side, max long side) Diffusion timestep shift Post-training 2.5 105 Constant 0.0 1.0 AdamW (β1 = 0.9, β2 = 0.95, ϵ = 1.0 1015) 500 40k (512, 1024) 4.0 Table 5: Training recipe of SRUM. In Section 3.1, we explain how to generate detection boxes in all cases. Here, we note that Bagel uses an external model (SAM), while BLIP3o relies on its own native capabilities. We suggest that the rationale for this choice can be based on the models performance on grounding benchmarks (such as RefCOCO)."
        },
        {
            "title": "B DEFINITION AND CALCULATION OF AVERAGE ACTIVATION STRENGTH",
            "content": "To investigate the internal functional mechanisms of different training methods, we introduce the metric of Average Activation Strength. This metric is designed to quantify the overall activity level of predefined functional neural cluster when the model is performing specific type of task. This appendix provides detailed definition, mathematical formulation, and the statistical implementation procedure. The Average Activation Strength is defined as the mean activation value of all neurons within specific functional cluster, averaged over an entire dataset for given task. The calculation involves two-level averaging process: 1. Intra-Cluster Average: For single input sample, we compute the mean of the activation values of all neurons belonging to the target cluster. 2. Dataset-Wide Average: We then average these single-sample cluster means across all samples in the entire task dataset. This metric reflects the degree of engagement of functional cluster (e.g., the Understanding Cluster) while processing certain category of tasks (e.g., Generation Tasks). higher value indicates that the cluster is more strongly and broadly activated for that task. 16 To formalize this definition, we first introduce the following notation: : specific neural network model (e.g., Base, SFT, or SRUM). Ck: functional neural cluster (e.g., Cunderstand or Cgenerate), which is set of specific neuron indices. Ck: The number of neurons in cluster Ck. DT : The dataset for specific task type (e.g., Dunderstanding or Dgeneration). DT : The number of samples in the dataset DT . x: An individual input sample from the dataset, where DT . ai(x): The activation value of neuron in model given the input x, where Ck. This typically refers to the output of neuron after its activation function (e.g., ReLU) has been applied. For single input sample x, the average activation strength of cluster Ck, denoted as Ssample, is calculated as: Ssample(M, Ck, x) = ai(x) (4) 1 Ck (cid:88) iCk The final Average Activation Strength of cluster Ck for model over the entire dataset DT , denoted as Sfinal, is the expected value of Ssample over all samples. In practice, this is estimated by averaging across the dataset: Sfinal(M, Ck, DT ) = 1 DT (cid:88) xDT Ssample(M, Ck, x) = 1 DT Ck (cid:88) (cid:88) xDT iCk ai(x) (5) This Sfinal value corresponds to the height of each bar in the activation figures. Algorithm details can be seen in Algorithm 1. Algorithm 1 Calculation of Average Activation Strength (Concise) Require: Models Mset, datasets Dset, pre-computed clusters Cund, Cgen Ensure: Activation strengths Sfinal for each model-task pair 1: for each model Mset do 2: 3: 4: 5: 6: Initialize lists Aund [ ], Agen [ ] for each sample do for each dataset Dset do a(x) ForwardPass(M, x) (cid:80) Sund 1 (cid:80) Sgen iCund Cund iCgen Cgen ai(x); Append to Aund ai(x); Append to Agen end for Sfinal(M, D) (mean(Aund), mean(Agen)) 7: 8: 9: 10: 11: end for end for Record activations Store final scores"
        },
        {
            "title": "C DATA CURATION",
            "content": "We leverage the training instructions from T2I-CompBench (Huang et al., 2023) to guide our image generation process. Specifically, we utilize the generation capabilities of UMs (Wu et al., 2024b;a; Xie et al., 2024; Dong et al., 2024), which are representative of the state-of-the-art in text-to-image synthesis (Betker et al., 2023; Saharia et al., 2022; Esser et al., 2024; Labs, 2024; Wu et al., 2025), to synthesize corresponding images based on these instructions. Subsequently, the understanding end of UMs, which possesses powerful vision-language comprehension abilities akin to models like LLaVA, InternVL, and Gemini (Liu et al., 2024a; Chen et al., 2024b; Wang et al., 2024b; Team et al., 2023), is employed to evaluate and score the generated images. The capabilities of these models are built upon massive web-scale datasets (Schuhmann et al., 2022; Li et al., 2024a) and canonical vision datasets (Lin et al., 2014), which are often enhanced with 17 high-quality captioning and instruction-following data (Sharma et al., 2018; Li et al., 2024b; Liu et al., 2023a). Our prompting strategy for eliciting rewards is inspired by the methodologies used in instruction-based image editing (Brooks et al., 2023; Wei et al., 2024; Zhang et al., 2023a; Yu et al., 2024a; Hui et al., 2024; Bai et al., 2024). The detailed data used in this evaluation are as follows: Generated Prompt Content: # TASK: Global Layout and Composition Analysis You are an expert image analyst. Your task is to score the overall composition of an image based on users prompt. Focus solely on how the arrangement of elements and scene structure align with the prompts spatial intent. **Original Prompt:** \"{original_prompt}\" --- ## YOUR TASK & OUTPUT FORMAT Provide single score from **-1.0 to 1.0** and brief reason. * **Scoring Guide:** * **1.0:** Perfect alignment with the prompts spatial intent. * **0.5 to 0.9:** Mostly correct layout with minor flaws. * **-0.4 to 0.4:** Neutral. No specific spatial info in prompt, or generic layout. * **-0.9 to -0.5:** Incorrect layout or contradictory to the prompt. * **-1.0:** Fundamentally contradicts the prompts spatial intent. * **Output Lines:** Score: [A single number between -1.0 and 1.0] Reason: [Your justification] --- ## DIVERSE EXAMPLES ### Example 1 (Perfect Alignment) Score: 0.95 Reason: The wide shot of sunset over the ocean perfectly matches the prompts implied composition. ### Example 2 (Contradictory Layout) Score: -0.7 Reason: The cat is on the right of the dog, but the prompt asked for the cat on the left. --- Begin your analysis now. Table 6: Documentation for create global layout reward prompt. 18 Generated Prompt Content: # TASK: Integrated Region Analysis and Scoring You are an expert AI image analyst. Your task is to analyze unlabeled regions in an image based on users prompt. For each region, you will perform two-stage analysis. **Original Prompt:** \"{original_prompt}\" --- **UNLABELED REGIONS FOR YOUR ANALYSIS:** {regions_text} --- ## YOUR TWO-STAGE TASK & OUTPUT FORMAT For **every Region ID** listed above, you must perform the following steps. ### STAGE 1: Identify Object First, identify the primary object within the bounding box. * **Output Line:** Identified Object: [Your description of the object] ### STAGE 2: Score and Justify Provide single, overall score from **-1.0 to 1.0** that considers BOTH the objects **relevance** to the prompt and its **visual quality**. You must provide clear reason for your score. Be as strict as possible and only give full marks when the image quality is beyond doubt. * **Scoring Guide:** * **1.0:** Perfect. The object is exactly what the prompt asks for and is technically flawless and perfect. * **0.5 to 0.9:** Very good. highly relevant object with minor flaws, or well-executed secondary element. * **-0.4 to 0.4:** Neutral/Acceptable. moderately relevant object, an object with mixed qualities, or an irrelevant but harmless background element. score of 0.0 is perfectly neutral. * **-0.9 to -0.5:** Bad. The object is irrelevant and distracting, or it is relevant object with severe visual artifacts/flaws. * **-1.0:** Very Bad. The object actively undermines the image and directly contradicts the prompts intent. * **Output Lines:** Score: [A single number between -1.0 and 1.0] --- ## EXAMPLE OUTPUT STRUCTURE **Region ID: 1** Identified Object: running golden retriever. Score: 0.95 --- **Region ID: 2** Identified Object: tall green tree in the background. Score: 0.2 --- Begin your analysis now. Table 7: Documentation for create hybrid evaluation prompt. 19 Object Bounding Box (bbox) Score Reason global layout reward [0, 0, 1024, 1024] 1.00 image perfectly The aligns with the prompts spatial intent by depicting horse positioned in front of microwave, effectively hiding it from view. The composition is well-executed, with the horses body and legs obscuring the microwave, and the plain background ensuring focus on the interaction between the two elements. brown horse with white blaze and white socks. brown horse with white blaze and white socks. brown horse with white blaze and white socks. microwave. The floor. The floor. brown horse with white blaze and white socks. brown horse with white blaze and white socks. brown horse with white blaze and white socks. brown horse with white blaze and white socks. [164, 97, 957, 990] [0, 0, 1023, 831] [349, 28, 920, 880] [349, 28, 920, 389] [0, 681, 1023, 1023] [0, 838, 1023, 1023] [422, 94, 748, 292] [429, 589, 856, 795] [430, 121, 848, 793] [430, 607, 755, 780] 0.95 0.95 0.95 0.50 0.00 0.00 0. 0.95 0.95 0.95 Table 8: VLM Rewards for Prompt: microwave hidden by horse"
        }
    ],
    "affiliations": [
        "Noahs Ark Lab, Huawei"
    ]
}