{
    "paper_title": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation",
    "authors": [
        "Leon Sick",
        "Lukas Hoyer",
        "Dominik Engel",
        "Pedro Hermosilla",
        "Timo Ropinski"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks."
        },
        {
            "title": "Start",
            "content": "S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation"
        },
        {
            "title": "Timo Ropinski\nUlm University",
            "content": "5 2 0 2 6 1 ] . [ 1 0 4 4 4 1 . 2 1 5 2 : r Figure 1. Learning Unsupervised Video Instance Segmentation from Real Videos. Given single-frame unsupervised instance masks, we first discover temporally-coherent, high-quality keymasks. This sparse labelset is then propagated using Sparse-To-Dense Distillation, aided by Temporal DropLoss. Finally, we train our model on the resulting dense labelset and demonstrate state-of-the-art results."
        },
        {
            "title": "Abstract",
            "content": "In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train segmentation model for implicit mask propagation, for which we propose Sparse-To-Dense Distillation approach aided by Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks. Project Page: leonsick.github.io/s2d 1. Introduction Video instance segmentation is key task in perceptive computer vision, empowering range of applications, such as autonomous driving, AR/VR applications, and video editing. Compared to image instance segmentation, video instance segmentation though is fundamentally more challenging. To segment videos, models must learn temporal dynamics and maintain identity-preserving masks, while instances may appear, disappear and undergo occlusions. In recent years, range of supervised Video Instance Segmentation models have advanced rapidly [4, 34, 36, 41, 43], steadily increasing segmentation accuracy. However, many of these models rely on per-frame human annotations, which require immense annotation efforts and thus result 1 in high costs. For example, the largest video segmentation dataset yet, SA-V [23], contains 190K manual annotations, for which the authors employed large group of crowdworkers. Such costly efforts have sparked the field of Unsupervised Video Instance Segmentation. This research area follows the same core motivation as unsupervised image segmentation: Enabling segmentation without task-specific or any other human annotations, therefore removing entirely the need for human labeling anywhere in the pipeline [1, 10, 11, 25, 26, 32, 33]. Wang et al. [33] have proposed VideoCutLER, video segmenter trained on synthesized sequences of shifting instances through object-centric images from ImageNet [7]. Critically, their training data models only single-object translational movement examples, that lack the complexity of multiple dynamic objects in real-world videos. To overcome this limitation, we enable our model to learn complex temporal dynamics, involving multi-instance motion, from real-world videos without requiring human annotations. Specifically, we propose S2D, an approach for training video instance segmentation model to make dense predictions by training on sparse set of temporally-coherent, high-quality keymasks, predicted by an unsupervised image instance segmenter. We achieve this relying on three key insights. First, we can extract high-confidence masks for certain frames with existing image unsupervised instance segmentation models. Second, we can identify which frames contain reliable masks by inspecting their temporal coherence. Lastly, we can train video instance segmentation model on sparse image annotations via Sparse-To-Dense distillation to achieve mask propagation. Our proposed pipeline is illustrated in Figure 1. In the first step, our Keymask Discovery algorithm identifies the quality of frame-wise image masks without human supervision by leveraging point tracks as deep motion prior. The resulting masks are identified as keymasks of the video, i.e., temporally-coherent instance masks. We predict single-frame masks using an off-the-shelf unsupervised image instance segmentation model. By tracking each single-frame mask over the entire video sequence, we identify when appearances and disappearances occur. Performing this analysis for all single-frame masks yields visibility groupings, which contain instance masks whose proxypropagations, i.e., point tracks, appear and disappear together. Following, we determine which of the simultaneously visible masks correspond to each other by applying our temporal correspondence matching. Using the overlap of proxy-propagations and per-frame instance masks, we subdivide the visibility groups into clusters that share temporal and spatial correspondence. This process yields sparse video annotations, i.e., the video instances have annotations for some frames where high-quality masks could be identified. However, our method aims to obtain mask annotations for video instances in every frame where they are visible, i.e., dense annotation set. To achieve this, we demonstrate that VideoMask2Former [4] can be trained as an implicit mask propagation & object discovery model. We propose novel approach for training this model on sparse annotations to complete the missing video instance masks. First, we waive the loss penalty for missing instance annotations by proposing Temporal DropLoss. Second, we propose Sparse-To-Dense Distillation, where we train student model to learn dense predictions, guided by teacher model. Sparse-To-Dense Distillation results in dense video annotation set, which we further refine in another round of training. Since our process to generate pseudolabelsets is applicable to any video dataset, we experiment with scaling the training video data beyond the in-domain data and find that this yields strong zero-shot performance. In summary, we propose the following contributions: 1. We propose Keymask Discovery algorithm to construct sparse pseudo-labelset of high-quality temporally-coherent instance masks. 2. We introduce Sparse-To-Dense Distillation with Temporal DropLoss which achieves implicit mask propagation & object discovery. This allows our method to convert the sparse keymasks to dense video labelset. 3. We present video instance segmentation model, trained task-specific labels and exclusively on real without videos, which demonstrates state-of-the-art in-domain and zero-shot performance. 2. Related Work Unsupervised Image Instance Segmentation. Early methods for unsupervised instance segmentation have been focused on leveraging features from self-supervised learning models to extract masks. Taking bottom-up approach, MaskDistill [28] leverages pixel grouping prior from MoCo features to extract masks, which are then used to train standard detection network like Mask R-CNN [13]. In contrast, FreeSOLO [31] leverages DenseCL features to extract attention maps based on key-query mechanism. The resulting maps are used as pseudo-masks to train SOLO model [30]. Another significant direction extends Normalized Cut-based segmentation techniques. Wang et al. propose CutLER [32] by generalizing the use of Normalized Cut on DINO [3] features for mask extraction, first introduced by TokenCut [35], to the multi-instance case. After extracting pseudo-masks with MaskCut, detection network is trained for multiple rounds with self-training. CuVLER, by Arica et al. [1], enhances the pseudo-mask generation by using six-DINO-model ensemble and soft target loss. Finally, CutS3D [26] leverages 3D information to separate instances from semantics. LocalCut is used on 3D point cloud to cut the instances, and the detectors 2 training is augmented with Spatial Confidence components. The resulting model achieves improved instance separation compared to previous state-of-the-art methods for zero-shot and in-domain evaluations. We utilize CutS3D in our work to predict single-frame instance masks. Unsupervised Video Instance Segmentation. Unsupervised video instance segmentation is challenging task that requires not only the separation of foreground objects from the background but also the differentiation and tracking of multiple distinct instances over time, all without any human annotations [44]. Previous research on unsupervised video segmentation largely proposed approaches for unsupervised video object segmentation [16, 24, 37, 38]. VOS aims to detect all moving objects as single foreground, without the necessity of distinguishing between individual object instances. common practice in prior studies on this task [29, 3739] is the dependence on optical flow networks. Notably, work by Karazija et al. [16] has leveraged modelpredicted point tracking and optical flow as supervision for their segmentation model. MotionGroup [39] train transformer to extract motion-grouped segmentations using only optical flow and no RGB frames. Furthermore, OCLR [37] propose an object-centric approach, also leveraging modelpredicted optical flow. Another line of work tackles unsupervised label propagation throughout video sequence, using human annotations only for the first frame [3, 14, 27] or by incorporating supervised learning with extensive external labeled data [18]. For unsupervised video instance segmentation, VideoCutLER [33] has proposed copy-paste shifting ImageNet [7] masks to generate synthetic videos. The scale of their synthetic dataset allows the method to scale to zero-shot model. 3. Method An overview of our method is presented in Fig. 1: Starting from noisy single-frame masks, we perform Keymask Discovery to obtain sparse pseudo-annotation set. Next, we propagate the extracted temporally-coherent masks using Sparse-To-Dense Distillation, resulting into dense annotation set. Finally, we train the final model on the dense pseudo-labels. Specifically, we use CutS3D [26] to first extract frame-wise instance masks for all videos in the dataset. We then filter those predictions to identify sparse, temporally-coherent keymasks (see Figure 3 and Sec. 3.1), which are then used to train our implicit mask propagation module using our proposed Temporal DropLoss (Sec. 3.2). To train this model, we employ Sparse-To-Dense Distillation setup, which allows the model to smooth out ambiguities remaining in the pseudo-annotations in dual-stage setup (Figure 4 and Sec. 3.3). The resulting model predicts high-quality video instance masks, as is shown in Fig. 2. Figure 2. Qualitative Results. Our model is able to segment more fine-grained structures in videos compared to VideoCutLER [33]. Furthermore, we find our predictions are less noisy. 3.1. Temporal Grouping for Keymask Discovery We start with unsupervised single-frame predictions from CutS3D [26] as the basis for video pseudo annotation. However, in video context, the CutS3D predictions lack any classification or temporal coherence, i.e., they do not encode which mask in frame 1 belongs to which mask in frame 2. Furthermore, there might be inconsistencies in video across single-frame predictions. To establish temporal coherence and filter out noisy masks, we propose novel unsupervised keymask discovery method. Obtaining Mask-Point Trajectories. Using an off-theshelf point tracker [15], we initialize sparse point grid for each single frame mask and track it forward/backward through the video. Practically, we initialize set of Ni in1, . . . , pt stance points Pt } for given instance mask Ni mt of instance at time t. We track all points throughout the video for all time steps {1, . . . , }, i.e. forward & backward, and record their coordinates and visibilities. This results in set of trajectories Ti = {τ1, . . . , τNi} for each instance i, where each trajectory τj = {(xt j, vt j) = R2 and visi1, . . . , } encapsulates the coordinates xt bility vt {0, 1} of point over the video duration. With trajectory and visibility data present for each single-frame mask in the video, we employ it as proxy to validate our fundamental temporal coherence concepts. = {pt 3 Figure 3. Keymask Discovery. Given single-frame unsupervised instance masks, we discover temporally-coherent keymasks by first generating instance tracks, then matching image instance masks based on temporal coherence concepts. Visibility Grouping: Instance Tracks of Single-Frame Masks belonging to the same video instance have similar visibility windows. Proxy Propagate-And-Match: In Visibility Group, frame masks can be temporally associated if their instance tracks match other masks. (cid:80)Ni j=1 vt j, exceeds threshold γthr. Visibility Grouping. In the next step, we validate that single-frame mask tracks of the same temporal instance appear and disappear at the same time, and we verify this concept in the following section. For each single-frame instance mask mt i, we determine if its proxy-propagation, given by the instance trajectories Ti, is visible at any given timestep in the video. This is the case if the proportion of tracked points that are visible at given frame, γt = 1 In practice, Ni we consider an instance as occluded if the ratio of visible points is below γthr = 0.3. We collect these binary visibility tracks, displayed in Fig. 3, for each single-frame mask across the video and cluster them with DBSCAN [9]. The input to the clustering algorithm is the set of binary visibility vectors = {v1, . . . , vM }, where each vector vi = {1(γt > γthr) = 1, . . . , } represents the sequence of visibility states for single mask mt across the video duration and 1() is the indicator function that is 1 if the condition is true and 0 otherwise. The resulting clusters contain all single-frame masks whose proxy-propagations appear and disappear together, i.e., the instance tracks are visible at the same time. We find this provides useful initial grouping to find masks of temporally-coherent instances. In the example in Fig. 3, the human on the left appears later, hence it is different video instance. In addition, the clustering returns outlier detections for masks that cannot be grouped by visibility. These are often noisy masks which do not belong to any instance. Proxy Propagate-And-Match. For cases where each video instance has separate visibility windows, our Visibility Grouping identifies video instances correctly. However, there are many cases where multiple instances have the same visibility, e.g., both are visible throughout the entire video, as in the case for the van and the person on the right in Fig. 3. Hence, we further subdivide the visibility groups using our proxy propagate-and-match approach. We validate our second temporal coherence concept by measuring the spatial consistency between instance masks across frames. Specifically, the point tracks associated with single-frame instance mask are used as proxy to propagate that instance forward and backward to different frame. This proxy-propagation must exhibit high overlap with the single-frame mask in that target frame. We restrict this search to masks within the same Visibility Group, which serves as an effective initial grouping. To compute this overlap, we again leverage the instance trajectories Ti for each single-frame mask within visibility cluster C. For reference instance and its propagated point set Xt } at the frame of timestep t, we calculate the degree of overlap with every instance mask mt in this frame. Specifically, we propose to compute the pointmask intersection between the point set Xt and the instance masks mt using the Point-Mask Jaccard Index (i, k, t), which measures the overlap of the propagated points of instance and the mask of image instance of the frame at time t. Given that Xt is is sparse set of Ni points and mt 1, . . . , xt Ni = {xt 4 dense set of pixels, the Point-Mask Jaccard Index is formally expressed as: (i, k, t) = (cid:80)Ni j=1 1(xt Ni mt k) , Xt xt (1) If (i, k, t) > λJ , we consider the point grid Xt with the mask mt k. In practice, we set λJ = 0.5. matched We compute the matches for all instance point-tracks to all instance masks within visibility cluster C. The matching process yields binary matching matrix {0, 1}NmasksNmasks(T ), where the match is defined by the following indicator function: Mt i,k = 1(J (i, k, t) > λJ ) (2) We collect the set of all matching tracks where each element represents the temporal sequence of matches for point track . We then cluster the resulting matching tracks again with DBSCAN [9], which provides matched subgroups within visibility cluster. In the case of the van-person visibility group in Fig. 3, this leads to both instances being separated successfully. With this, we arrive at the final instance mask assignments over time, which meet the defined temporal coherence concepts. Since all masks which do not meet these criteria are filtered out, this pseudoannotation set is inherently sparse. For cases where no temporal coherence can be established from the image instance masks, the video is discarded from the annotation set. Note that other unsupervised video segmentation methods have also relied on model-predicted motion cues [16, 18, 3739]. However, contrary to existing approaches, our method does not use point tracks or optical flow as supervision to the model. 3.2. Temporal DropLoss For Mask Propagation is to obtain model Our goal that makes temporally dense predictions by learning only from sparse annotations, thereby achieving implicit mask propagation. From the Keymask Discovery process, we obtain sparsely annotated pseudo-label set, i.e., not every frame might contain segIn mentation mask, even though the instance is present. order to compensate for this training data sparsity, we make several modifications to our training pipeline. First, we modify the dataloading process: Instead of sampling frames randomly throughout the entire video, we prioritize frames with at least one temporally dense annotated instance mask, thereby constructing pseudo-dense training samples. Training on this data gives the model some samples with annotated temporal consistency. However, these sampled video snippets do not only contain dense mask annotations, but also sparsely labeled instances. For example, second instance in the video snippet might only have mask for 1 of 2 frames. Computing standard video instance segmentation loss on this sample would assign penalty for predicting the second, un-annotated mask. To address this, we propose Temporal DropLoss for video instance segmentation: For instances with sparse annotations, we limit the loss computation to the frame(s) where given instance is annotated, and drop the loss for frames without an annotated mask. We find this encourages mask propagation to sparsely labeled instances since the model has learned to propagate temporally-coherent masks from the densely annotated instance samples in the data. In practice, we use VideoMask2Former [4] as our video instance segmentation model, which employs bipartite matching between pseudo ground-truth masks and its predictions, to then compute the mask losses only for these matches. Consequently, we formulate our Temporal DropLoss for the matched predictions on the mask loss of the model. Given set of sparse instance mask annotations At and matched model predictions ˆMt, we drop the segmentation loss Lmask for given instance at frame if the sum of its pseudo-annotated binary mask mt is zero, i.e. no matching mask is present at t. For matched instance prediction, the loss is formally defined as: LTempDrop(i) = (cid:88) t=1 1(mt > 0)Lmask( ˆmt i, mt i) (3) where mt denotes the total number of pixels in the pseudo ground-truth mask. Lmask is the standard Mask2Former [5] mask loss which combines binary crossentropy loss with dice loss [20], i.e. Lmask = λceLce + λdiceLdice (4) Our approach can be understood as the temporal extension of Image DropLoss [32], which is popular technique in unsupervised image instance segmentation to counteract the penalty of predicting masks that are completely disjoint from the pseudo-annotations. 3.3. Dual-Stage Distillation Training To train our final video instance segmentation model, we use dual-stage training setup: In the first stage, we train VideoMask2Former on our sparse keymask annotations to achieve implicit mask propagation throughout the training dataset. In the second stage, we train the final video instance segmentation model on the predicted dense annotations. In the first step, we propose Sparse-To-Dense distillation approach for training video instance segmentation model on temporally sparse annotations. We set up student-teacher distillation with both models being VideoMask2Former. The student receives two learning signals. We train the student model on the sparse keymask annotations with the Temporal DropLoss, where the keymasks serve as an anchoring labelset. The loss already encourages does not exist. Hence, we train and evaluate only the stage 1 Sparse-To-Dense model on the dataset to demonstrate its ability to implicitly propagate instance masks. The previous state-of-the-art model, VideoCutLER [33], trains on 1 million synthetic videos from ImageNet. The dataset scale of their approach allows the model to generalize well, showing strong zero-shot performance. Hence, we investigate how our approach scales with large video dataset, comprising video datasets outside of the domain of the video instance segmentation evaluation. For this, we combine the training splits of VIPSeg [19], MOSEv1 [8], and all horizontal videos from SA-V [23]. After the Keymask Discovery has filtered out unsuitable videos, the resulting dataset has combined size of 13,000 videos, far surpassing YouTubeVIS which contains only 2,900 training videos. By training on this dataset with Sparse-To-Dense Distillation, we obtain zero-shot model. Experiment Setup. We first predict single-frame masks with CutS3D [26] on the respective training datasets. Next, we extract temporally-coherent keymasks and train our stage 1 Sparse-to-Dense distillation model on the sparse annotations. As mentioned for the DAVIS dataset, we only perform this stage to convert the sparse annotations to dense predictions. For the YouTube-VIS datasets, we perform both rounds of distillation training, as described in Sec. 3.3. We apply the same weight initialization as Wang et al. [33] and use VideoMask2Former [4] with ResNet50 [12] backbone, trained with video snippets comprised of 2 frames, as is standard for this model. After training, we keep the teacher model for evaluation. Evaluation Datasets & Protocols. We evaluate our indomain models for unsupervised video instance segmentation on the validation splits of YTVIS 2019 & 2021 [40, 42], as well as DAVIS-all [21]. Furthermore, we evaluate semisupervised and fully-supervised video instance segmentation. Here, the unsupervised model serves as pre-trained weight initialization. We finetune the model on different percentages of labeled videos for semi-supervised learning, and the full dataset for supervised learning. Again, all models are evaluated on the YTVIS 2021 validation split. In addition to YTVIS 2019 & 2021, we evaluate our zero-shot model on YTVIS 2022 validation, OVIS [22] & Cityscapes [6]. For the latter, we extract instance annotations from the video panoptic segmentation CityscapesVPS dataset [17]. For OVIS, we evaluate on the training split. We provide further details in the supplementary. Figure 4. Sparse-To-Dense Distillation. We supervise the student model with our Temporal DropLoss on the sparse anchor labelset. Additionally, the teacher model provides dense supervision on-the-fly, which improves as training progresses. the student to learn temporally-consistent implicit mask propagation. We update the teacher model weights with an exponential moving average (EMA), controlled by the update rate µ. This way, the teacher becomes better at predicting temporally-dense video instance segmentations as the training progresses. Harvesting this signal, we task the student model to match the dense teacher predictions. This enables the student to refine its temporal coherence capabilities by learning temporally-consistent predictions from the teacher on-the-fly. In addition, maintaining supervision from the sparse keymasks prevents the model from deviating too far from the pseudo ground-truth. With this, the full loss can be formulated as LFull = LTempDrop + LDistill mask (5) We illustrate our Sparse-To-Dense Distillation in Fig. 4. In the second stage, we aim to further refine the models predictions. We employ the same student-teacher setup again. Using the final Sparse-To-Dense model from stage 1, we predict new set of now dense pseudo-annotations to serve as anchoring labelset. The new student model learns from these temporally dense pseudo-annotations and is additionally tasked to match the teachers predictions. For the second round of distillation, the model learns refined instance segmentations by self-distilling its own predictions. We also observe that keeping the dense anchoring annotation set as supervision helps the model to converge. 4. Experiments 4.1. Unsupervised Video Instance Segmentation Training Datasets. We perform in-domain experiments on YouTube-VIS (YTVIS) 2019 [40] & 2021 [42] as well as DAVIS-all [21]. For the YouTube-VIS datasets, we train on the training split with our Dual-Stage Distillation Training. For DAVIS-all, traditional training-validation split In-Domain Evaluation. After training our model indomain, we report our results in Tab. 1. Across all datasets, we compare our approach against baselines from VideoCutLER [33]. This includes the image-trained CutLER [32] model, and two approaches utilizing model-predicted op6 Table 1. Unsupervised Video Instance Segmentation. When trained on in-domain data, our model is able to outperform the previous state-of-the-art across YouTubeVIS-2019, YouTubeVIS-2021 and DAVIS-all. * Results obtained using official checkpoint. YouTube-VIS 2019 YouTube-VIS 2021 DAVIS-all Method AP50 AP75 MotionGroup [39] OCLR [37] CutLER [32] VideoCutLER* [33] S2D (Ours) 0.5 5.5 36.4 48.2 51.2 0.0 0.3 13.5 22.8 25.2 AP 0.1 1.6 16.0 24.5 26.2 APS APM APL AP50 AP75 0.0 0.1 3.5 7.0 6. 0.4 1.6 13.9 17.5 21.2 0.1 6.1 26.0 36.1 39.4 0.3 4.3 28.5 37.0 42.5 0.0 0.1 9.3 16.1 17.6 AP 0.1 0.9 12.0 18.0 20. 0.0 0.1 2.7 4.9 5.1 0.2 1.0 12.7 17.0 21.7 0.1 4.9 27.2 36.8 41.0 - 36.9 44.7 44.9 50.6 vs. SOTA +3. +2.4 +1.7 -1.0 +3.7 +3.3 +5. +1.5 +2.1 +0.2 +3.1 +4.2 +5. - 36.3 44.6 44.8 50.5 +5.6 - 37.6 44.9 45.0 50.8 +5.8 APS APM APL J&F J-Mean -Mean Table 2. Zero-Shot Unsupervised Video Instance Segmentation. By scaling up to 13K training videos, we show our method can scale to powerful zero-shot model: Training on 13K real videos outperforms the model using 1.3 million synthetic videos. Dataset Method Training Data VideoCutLER [33] S2D (Ours) 1.3M Syn. Videos 13K Real Videos vs. SOTA YouTube-VIS 2019 YouTube-VIS 2021 YouTube-VIS 2022 OVIS Cityscapes AP 48.2 51.4 +3.2 AP 24.5 25.6 +1.1 AP 37.0 41.8 +4.8 AP 18.0 19.8 +1.8 AP 31.7 35.3 +3.6 AP 15.0 16.4 +1.4 AP 4.2 5.3 AP 1.1 1.7 AP50 1.7 3.9 AP 0.9 1.4 +1.1 +0.6 +2.2 +0.5 tical flow estimations, OCLR [37] and MotionGroup [39]. For both YTVIS datasets, our model outperforms the bestperforming baseline by up to +2.1 AP. We show qualitative results on YTVIS-2021 in Fig. 2. When evaluating the models predictions on DAVIS-all, we find it outperforms VideoCutLER by +5.7 &F. This improvement is despite only performing Sparse-To-Dense Distillation on the keymasks from DAVIS to obtain full dense prediction set. Zero-Shot Evaluation. Beyond training and evaluating our model on in-domain data, we experiment with training zero-shot model by learning exclusively from real videos. We start by constructing large and diverse video dataset. We combine videos from VIPSeg [19], MOSEv1 [8] and SA-V [23] and extract keymasks on this mixture-of-dataset. After this filtering, we obtain dataset of 13,000 training videos with sparse pseudo-annotations. We perform SparseTo-Dense Distillation on this dataset and find that one round is sufficient. After evaluating the general video instance segmentation capabilities of our model, we report zero-shot results in Tab. 2. Across all evaluated datasets, our approach outperforms VideoCutLER by up to +4.8 AP50. This clearly emphasizes the value real-world data brings to the learning process. For Cityscapes, we find both models struggle. We assume their training data is to far out of domain for the driving scenarios. 4.2. Semi-Supervised & Fully-Supervised Video Instance Segmentation We experiment with semi-supervised & supervised finetuning, using our models as pre-trained weight initialization. Evaluation Protocol. We re-use the weights of our model as unsupervised weight initialization, pre-trained on YTVIS-2021. For the semi-supervised evaluation, we select varying amounts of annotated videos as labeled subset to finetune the model. For the fully-supervised evaluation, we train the model on the entire training dataset. We provide further training details in the supplementary. Results. We report our evaluation results on the YouTubeVIS 2021 validation split in Tab. 3. For low-data regimes, such as using only the 10% of labeled training data, our model outperforms VideoCutLER [33] by +2.7 AP50. For fully-supervised training, our model is competitive with the baseline. This is despite VideoCutLER being pre-trained on over million synthetic videos, while we pre-train our model only on 13,000 real videos. Table 3. Semi-Supervised & Fully-Supervised Video Instance Segmentation. We show that in low-data regimes, our model trained on real videos outperforms VideoCutLER with ImageNet training across various data splits on YouTubeVIS-2021 [42]. Pct. 1% 5% 10% 25% 100% AP AP50 AP AP50 AP AP50 AP AP50 AP AP50 YouTube-VIS DINO*[3] VideoCutLER*[33] S2D (Ours) 11.1 20.0 9.4 18.9 11.5 21. 12.3 20.9 26.3 43.3 14.5 23.9 14.3 22.8 16.2 25.7 20.9 32. 32.4 51.0 15.2 25.1 15.4 25.4 17.4 28.4 21.1 33.1 33.0 53. 7 5. Ablations We present ablations for the various steps of our method, starting with the components of our Keymask Discovery (Tab. 4), then ablating the components of our model trainings (Tab. 5) and finally comparing our implicit mask propagation to using separate propagation algorithm (Tab. 6). Further ablations are presented in the supplement. Keymask Discovery Components. In Tab. 4, we ablate all components of the Keymask Discovery. As baseline, we train an Image Mask2Former [5] on the singleframe masks. We then compare training on video annotations from just visibility grouping to the sparse keymasks with proxy propagate-and-match. Finally, we compare to the model performance on the final dense labels. We report results on YouTube-VIS 2021, by training our video instance segmentation model on the resulting annotations in the dual-stage setup. While we find that training model on single-frame masks results in performance competitive with VideoCutLER [33], the model significantly improves with temporally-coherent sparse annotations from our keymask discovery. Furthermore, training on the final dense annotations, obtained through Sparse-To-Dense distillation, further improves the results. Model Training. We ablate the modifications to the VideoMask2Former. As baseline, we train the model on the pseudo-annotations without any modifications. We experiment with just adding pseudo-dense data loading, then we add the Temporal DropLoss. Finally, we compare against training the final model with dual-stage training. We again evaluate on YouTube-VIS 2021 and report the performance of our final dual-stage model training. We see improvements of our method from implicit mask propagation with our Temporal DropLoss. When employing Sparse-ToDense distillation in addition, our final model further improves to 20.1 AP. DINO Propagation vs. Sparse-To-Dense Distillation. The purpose of Sparse-To-Dense Distillation and the Temporal DropLoss is to enable implicit mask propagation using Video Instance Segmentation model. However, alternatives exist, such as explicit propagation algorithms [3, 14], i.e., propagating masks throughout video by connecting semantic encodings of individual frames. Hence, we compare using an off-the-shelf propagation algorithm based on DINO [3] semantics to our Sparse-To-dense Distillation on DAVIS-all [21]. Since these algorithms are designed to propagate instance masks from starting frame, we perform this propagation for all single-frame masks in the video, and select the best result per instance to report our results in Tab. 6. We find that our approach outperforms this propagation alternative. Table 4. Keymask Discovery Components. Metrics on YTVIS2021. Single-Frame Masks trained with an Image Mask2Former, the weights are compatible with the VideoMask2Former."
        },
        {
            "title": "Components",
            "content": "APmask"
        },
        {
            "title": "APmask",
            "content": "Single-Frame Masks + Visibility Grouping + Temporal Matching + Dense Labelset 36.5 38.9 40.9 42.5 17.8 18.9 19.5 20.1 Table 5. Model Design Choices. We ablate the effect of modifications to our training pipeline on YTVIS-2021."
        },
        {
            "title": "Components",
            "content": "VideoMask2Former + Pseudo-Dense Dataloading + Temporal DropLoss + Sparse-To-Dense Distillation APmask"
        },
        {
            "title": "APmask",
            "content": "31.2 39.9 41.9 42.5 14.0 19.3 19.8 20.1 Table 6. Sparse-To-Dense Distillation vs. DINO Propagation. On DAVIS, we propagate every mask, then match with the groundtruth to calculate the J&F. Method J&F J-Mean -Mean Propagation with DINO [2] Sparse-To-Dense Distillation 42.2 50.6 43.4 50.5 41.0 50.8 6. Discussion In our experiments, we find that our approach struggles with segmenting smaller objects, as does the previous stateof-the-art. potential solution could be implementing copy-paste augmentation for videos and scale down copied objects. Furthermore, our zero-shot model struggles for driving-domain data. We believe adding such domainspecific data to our dataset-mix can help the model improve. While our method relies on deep motion cues to establish temporal coherence, future methods could improve this aspect by using deep temporal features. 7. Summary In this paper, we have introduced S2D, novel approach to learn unsupervised video instance segmentation from real video data. We have proposed novel Keymask Discovery algorithm for identifying keymasks with temporal coherence from unsupervised image instance masks. To perform mask propagation for these sparse annotations, we have introduced Sparse-To-Dense Distillation with Temporal DropLoss. Finally, we have trained self-distilled video instance segmentation model on the resulting dense annotations. Our final model outperforms the current state-of-theart for unsupervised video instance segmentation, for both in-domain and zero-shot settings."
        },
        {
            "title": "References",
            "content": "[1] Shahaf Arica, Or Rubin, Sapir Gershov, and Shlomi Laufer. Cuvler: Enhanced unsupervised object discoveries through exhaustive self-supervised transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2310523114, 2024. 2 [2] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 8 [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 2, 3, 7, 8 [4] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexander Kirillov, Rohit Girdhar, and Alexander G. Schwing. Mask2former for video instance segmentation, 2021. 1, 2, 5, 6 [5] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. 5, 8 [6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. Licensed under the Cityscapes Dataset License (non-commercial use only; no redistribution). [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 2, 3 [8] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. Mose: new dataset for video object segmentation in complex scenes. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2022420234, 2023. Data licensed under CC BY-NC-SA 4.0. 6, 7 [9] Martin Ester, Hans-Peter Kriegel, Jorg Sander, and Xiaowei Xu. density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, page 226231. AAAI Press, 1996. 4, 5 [10] Oliver Hahn, Christoph Reich, Nikita Araslanov, Daniel Cremers, Christian Rupprecht, and Stefan Roth. Scene-centric In Proceedings of unsupervised panoptic segmentation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2448524495, 2025. 2 [11] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William Freeman. Unsupervised semantic segmentation by distilling feature correspondences. In International Conference on Learning Representations. 2 [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 6 [13] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. [14] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as contrastive random walk. Advances in neural information processing systems, 33:1954519560, 2020. 3, 8 [15] Nikita Karaev, Yuri Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudolabelling real videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 60136022, 2025. 3 [16] Laurynas Karazija, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Learning segmentation from point trajectories. Advances in Neural Information Processing Systems, 37:112573112597, 2024. 3, 5 [17] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So In Proceedings of Kweon. Video panoptic segmentation. the IEEE/CVF conference on computer vision and pattern recognition, pages 98599868, 2020. 6 [18] Jonathon Luiten, Idil Esen Zulfikar, and Bastian Leibe. Unovost: Unsupervised offline video object segmentation and In Proceedings of the IEEE/CVF winter confertracking. ence on applications of computer vision, pages 20002009, 2020. 3, 5 [19] Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, and Yi Yang. Large-scale video panoptic segIn Proceedings of mentation in the wild: benchmark. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2103321043, 2022. The data is released for non-commercial research purpose only. 6, [20] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric In 2016 fourth international medical image segmentation. conference on 3D vision (3DV), pages 565571. Ieee, 2016. 5 [21] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. Data licensed under CC BY 4.0. 6, 8 [22] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and Song Bai. Occluded video instance segmentation: benchInternational Journal of Computer Vision, 130(8): mark. 20222039, 2022. Data licensed under CC BY-NC-SA 4.0. 6 [23] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. 9 arXiv preprint arXiv:2408.00714, 2024. Data licensed under CC BY 4.0. 2, 6, transactions on pattern analysis and machine intelligence, 2023. 2 [36] Junfeng Wu, Yi Jiang, Song Bai, Wenqing Zhang, and Xiang Bai. Seqformer: Sequential transformer for video instance segmentation. In European Conference on Computer Vision, pages 553569. Springer, 2022. 1 [37] Junyu Xie, Weidi Xie, and Andrew Zisserman. Segmenting moving objects via an object-centric layered representation. Advances in neural information processing systems, 35: 2802328036, 2022. 3, 5, 7 [38] Junyu Xie, Weidi Xie, and Andrew Zisserman. Appearancebased refinement for object-centric motion segmentation. In European Conference on Computer Vision, pages 238256. Springer, 2024. 3 [39] Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, and Weidi Xie. Self-supervised video object segmentation by motion grouping. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 71777188, 2021. 3, 5, 7 [40] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In ICCV, 2019. Data licensed under CC BY 4.0. [41] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segIn Proceedings of the IEEE/CVF international mentation. conference on computer vision, pages 51885197, 2019. 1 [42] Linjie Yang, Yuchen Fan, Yang Fu, and Ning Xu. The 3rd large-scale video object segmentation challenge - video instance segmentation track, 2021. Data licensed under CC BY 4.0. 6, 7 [43] Tao Zhang, Xingye Tian, Yu Wu, Shunping Ji, Xuebo Wang, Yuan Zhang, and Pengfei Wan. Dvis: Decoupled video In Proceedings of the instance segmentation framework. IEEE/CVF International Conference on Computer Vision, pages 12821291, 2023. 1 [44] Tianfei Zhou, Fatih Porikli, David Crandall, Luc Van Gool, and Wenguan Wang. survey on deep learning technique for video segmentation. IEEE transactions on pattern analysis and machine intelligence, 45(6):70997122, 2022. 3 [24] Sadra Safadoust and Fatma Guney. Multi-object discovery In Proceedings of the by low-dimensional object motion. IEEE/CVF International Conference on Computer Vision, pages 734744, 2023. [25] Leon Sick, Dominik Engel, Pedro Hermosilla, and Timo Ropinski. Unsupervised semantic segmentation through depth-guided feature correlation and sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36373646, 2024. 2 [26] Leon Sick, Dominik Engel, Sebastian Hartwig, Pedro Hermosilla, and Timo Ropinski. Cuts3d: Cutting semantics in 3d for 2d unsupervised instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2126521275, 2025. 2, 3, 6 [27] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. Advances in Neural Information Processing Systems, 36:13631389, 2023. 3 [28] Wouter Van Gansbeke, Simon Vandenhende, and Luc Van Gool. Discovering object masks with transformers arXiv preprint for unsupervised semantic segmentation. arXiv:2206.06363, 2022. 2 [29] Carles Ventura, Miriam Bellver, Andreu Girbau, Amaia Salvador, Ferran Marques, and Xavier Giro-i Nieto. Rvos: Endto-end recurrent network for video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 52775286, 2019. 3 [30] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Solo: simple framework for instance segmentation. IEEE transactions on pattern analysis and machine intelligence, 44(11):85878601, 2021. 2 [31] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz, Anima Anandkumar, Chunhua Shen, and Jose Alvarez. Freesolo: Learning to segment objects without annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1417614186, 2022. [32] Xudong Wang, Rohit Girdhar, Stella Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3124 3134, 2023. 2, 5, 6, 7 [33] Xudong Wang, Ishan Misra, Ziyun Zeng, Rohit Girdhar, and Trevor Darrell. Videocutler: Surprisingly simple unsupervised video instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2275522764, 2024. 2, 3, 6, 7, 8 [34] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 87418750, 2021. 1 [35] Yangtao Wang, Xi Shen, Yuan Yuan, Yuming Du, Maomao Li, Shell Xu Hu, James Crowley, and Dominique Vaufreydaz. Tokencut: Segmenting objects in images and videos with self-supervised transformer and normalized cut. IEEE"
        }
    ],
    "affiliations": []
}