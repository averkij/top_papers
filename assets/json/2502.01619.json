{
    "paper_title": "Learning to Generate Unit Tests for Automated Debugging",
    "authors": [
        "Archiki Prasad",
        "Elias Stengel-Eskin",
        "Justin Chih-Yao Chen",
        "Zaid Khan",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to a large language model (LLM) as it iteratively debugs faulty code, motivating automated test generation. However, we uncover a trade-off between generating unit test inputs that reveal errors when given a faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGen, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions and candidate code. We integrate UTGen into UTDebug, a robust debugging pipeline that uses generated tests to help LLMs debug effectively. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), UTDebug (i) scales UTGen via test-time compute to improve UT output prediction, and (ii) validates and back-tracks edits based on multiple generated UTs to avoid overfitting. We show that UTGen outperforms UT generation baselines by 7.59% based on a metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDebug, we find that feedback from UTGen's unit tests improves pass@1 accuracy of Qwen-2.5 7B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3% and 12.35% (respectively) over other LLM-based UT generation baselines."
        },
        {
            "title": "Start",
            "content": "Archiki Prasad * 1 Elias Stengel-Eskin * 1 Justin Chih-Yao Chen 1 Zaid Khan 1 Mohit Bansal"
        },
        {
            "title": "Abstract",
            "content": "Unit tests (UTs) play an instrumental role in assessing code correctness as well as providing feedback to large language model (LLM) as it iteratively debugs faulty code, motivating automated test generation. However, we uncover trade-off between generating unit test inputs that reveal errors when given faulty code and correctly predicting the unit test output without access to the gold solution. To address this trade-off, we propose UTGEN, which teaches LLMs to generate unit test inputs that reveal errors along with their correct expected outputs based on task descriptions and candidate code. We integrate UTGEN into UTDEBUG, robust debugging pipeline that uses generated tests to help LLMs debug effectively. Since model-generated tests can provide noisy signals (e.g., from incorrectly predicted outputs), UTDEBUG (i) scales UTGEN via test-time compute to improve UT output prediction, and (ii) validates and back-tracks edits based on multiple generated UTs to avoid overfitting. We show that UTGEN outperforms UT generation baselines by 7.59% based on metric measuring the presence of both error-revealing UT inputs and correct UT outputs. When used with UTDEBUG, we find that feedback from UTGENs unit tests improves pass@1 accuracy of Qwen-2.5 7B on HumanEvalFix and our own harder debugging split of MBPP+ by over 3% and 12.35% (respectively) over other LLM-based UT generation baselines. 5 2 0 2 3 ] . [ 1 9 1 6 1 0 . 2 0 5 2 : r 1. Introduction With rapid advancements in training large language models (LLMs; Achiam et al., 2023; Anthropic, 2024; Gemini et al., 2023; Dubey et al., 2024), enhancing their coding abilities has garnered significant attention (Chen et al., 2021; Li et al., 2023; Roziere et al., 2023; Guo et al., 2024b; Hui et al., 2024, inter alia). However, these models are far from perfect and much like human-written code model-written *Equal contribution 1Department of Computer Science, UNC Chapel Hill. Correspondence to: Archiki Prasad <archiki@cs.unc.edu>. Link to datasets and code: https: //github.com/archiki/UTGenDebug 1 Figure 1. Overview: Self-evaluating code correctness using LLMs can be unreliable and collecting human-written unit tests can be laborious; we propose UTGEN, which automatically generates failing unit tests for faulty code (triggering errors) without access to the gold solution. The generated unit tests can in turn be used for LLM debugging based on unit test feedback via UTDEBUG, leading to improved downstream code accuracy. code contains errors. Human developers often improve their code through test-driven development with iterative debugging, i.e., identifying failure cases by providing example inputs and their expected outputs, reasoning over causes of failure, and modifying the code to address the issues. Unit tests (UTs) are one form of such pairings: code inputs and expected outputs that test individual functionalities in piece of code. failing unit test provides targeted feedback, not only identifying that piece of code is not operating as expected but also helping to localize the error to one part of the code (Maximilien & Williams, 2003; Nagappan et al., 2008; Ficco et al., 2011). Models have similarly been shown to benefit from iterative debugging based on explicit or implicit feedback stemming from failing unit tests (Chen et al., 2023b; Moon et al., 2023). However, the ability to provide feedback and debug incorrect code is often bottlenecked by the availability of (failing) unit tests for given problem. While several coding benchmarks come with human-written UTs for evaluation purposes, these suites of UTs are typically small due to the laborious annotation process (Liu et al., 2024). Unit test collection is challenging for two reasons: first, it requires sampling inputs that are likely to trigger an error. For example in Figure 1, the unit test: Learning to Generate Unit Tests for Automated Debugging next smallest pld(120) == 121 would not trigger any error (despite the fact that the code is incorrect since the error happens to not be triggered for multiples of 10), while another unit test: next smallest pld(123) == 131 would lead to an incorrect output, revealing bug in the function. Secondly, UTs require expected outputs, i.e., the desired behavior of the code being tested must be known. For example, in Figure 1, the expected output 131 must be given in advance. Due to these challenges, prior work employs LLMs to generate unit test inputs at random (i.e., without conditioning on faulty code to generate failing inputs) and often uses the gold (correct) code solution for generating outputs (Chen et al., 2023a; Chae et al., 2024), which is generally not available at test time. Therefore, these approaches do not scale to automated debugging, which requires an online and automated method for dynamically generating UTs as problems are iteratively debugged by the model. To address this gap, we pose the following research questions: RQ1: What are desirable properties for UT generators, and how do we measure them? RQ2: How well do LLMs perform zero-shot UT generation, and how can we improve their abilities? RQ3: How do we best use an automated but potentially noisy UT generator to improve code debugging? To address RQ1, we characterize two desirable properties of unit test generators (in Section 3.1): 1) high attack rate, i.e., given faulty code, the unit test generator should generate UT inputs that are likely to trigger errors;1 2) high output accuracy, ensuring that the corresponding output of the unit test is consistent with the task description (and that of correct solution). For instance, in Figure 1, next smallest pld(120) would lead to lower attack rate, as it does not trigger any errors, while the generated UT next smallest pld(123) == 131 does trigger the error. While one test is attacking and another is not, both have high output accuracy, since in both cases the output is correct. By design, we can measure these properties via three intrinsic metrics: measuring attack rate and output accuracy independently and then, crucially, how often UT generator can generate both failing inputs along with correct UT outputs based only on the problem description and faulty code, i.e., without access to the gold solution. We benchmark the ability of various LLMs to act as zero-shot unit test generators using attack rate and output accuracy (i.e., intrinsic metrics for UT generation) and show that coding LLMs generally struggle with this task (cf. Section 5.1). Moreover, addressing RQ2, we find that, without any training, zero-shot models often exhibit strong trade-off between attack rate and output accuracy, since the UTs that 1Here, an error means either syntax error, runtime error, or an assertion error where the result of the target code does not match the expected output of the UT. We expand on this in Section 3. 2 are most likely to trigger errors (i.e., higher attack-rate UTs) are generally more challenging edge cases, making it harder for the model to predict their output (i.e., lower output accuracy). To address this trade-off, we propose training models; due to lack of dedicated datasets for unit test generation in prior work, we introduce UTGEN, data creation and training recipe (Section 3.2) that bootstraps training data for UT generation from existing code generation benchmarks by perturbing code to simulate errors, generating failing unit test and augmenting it with chain-of-thought-style rationales (Wei et al., 2022). We show that training via UTGEN better balances this trade-off and yields higher number of unit tests that have both an input that triggers an error and also predicts the correct output by 7.59% more over merely prompting models to generate failing UTs (in Section 5.1). Building on these intrinsic evaluations, we examine RQ3, testing our generated UTs on downstream, i.e., extrinsic task: code debugging. Here, automated UT generation methods face additional challenges: unlike human-generated gold UTs which have correct outputs but require human involvement generated UTs provide noisy feedback, as the UT might fail to reveal the buggy codes errors or have an incorrectly predicted output. To mitigate this issue, we propose UTDEBUG, an improved multi-turn debugging method that addresses these challenges in two major ways (cf. Section 3.3). First, we improve the output accuracy of generated UTs by scaling test-time compute via self-consistency (Wang et al., 2022), taking vote over multiple output samples to obtain better results. Moreover, to prevent overfitting to noisy feedback from inaccurate UTs, we regularize the debugging process by generating multiple UTs and accepting code edits based on their effect on validation suite of UTs. Specifically, if proposed change results in fewer generated tests passing (i.e., hurts the code quality) we allow the debugger to backtrack to previous stage and re-attempt debugging. We then plug UTs generated from multiple LLM-based methods including UTGEN into UTDEBUG to extrinsically to measure the utility of generated UTs as feedback (Chen et al., 2023b; Zhong et al., 2024), as illustrated by the final output of debugging in Figure 1. Our results show that on our most challenging subset of MBPP+ problems, UTDEBUG with UTs generated by UTGEN improves pass@1 accuracy of Qwen2.5 by 12.9% (absolute) compared to debugging without unit tests, and by 5.3% over zero-shot baseline. Our key contributions are: We bring attention to the trade-off between attack rate and output accuracy, releasing challenging dataset with subtle errors that require models to balance these properties. We find that this data is challenging even for models trained extensively on code. We introduce UTGEN for training UT generators that Learning to Generate Unit Tests for Automated Debugging alleviates the trade-off between attack rate and output accuracy through training, building UT generators with improved performance on both. Building on the scalability of UTGEN, we introduce UTDEBUG, debugging method that uses test-time scaling and back-tracking validation strategy enabled by suite of generated unit tests to improve debugging performance and avoid overfitting. 2. Related Work Automatic Unit Test Generation. While unit tests play pivotal role in assessing code correctness, manually writing suite of unit tests is laborious and often infeasible (Chen et al., 2023a; Liu et al., 2024). Therefore, prior work in software testing has often focused on automatic unit test generation (King, 1976; Cadar et al., 2008; Holler et al., 2012; Cha et al., 2015, inter alia). With the application of LLMs to program synthesis, recent work has explored using LLMs for automatic unit test generation (Chen et al., 2023a; Schafer et al., 2023; Liu et al., 2024). Notably, Schafer et al. (2023) and Liu et al. (2024) focus solely on unit test input generation, relying on the gold (correctly implemented) function being tested to generate the corresponding output of the unit test. Both prompt strong frontier models such as GPT-4, followed by additional input mutations (Liu et al., 2024) or iteratively prompting the LLM (Schafer et al., 2023). In contrast, our open-source models trained using UTGEN automatically generate both input-output pairs of unit tests, i.e., at inference time, UTGEN generates candidate input and the corresponding output as per the task description without access to the gold solution. While Chen et al. (2023a) also generate both input-output unit test pairs, they only rely on standard LLM prompting to produce unit tests as means of improving code-generation performance. As result, their method does not evaluate or optimize for the validity of generated UTs, especially output accuracy; on the other hand, in UTGEN, we explicitly model output accuracy and train the models to reason about the output of the unit test correctly. LLM Debugging. Using LLMs for debugging faulty code (also known as program repair) has been subject of prior work. Debugging approaches can be categorized into two groups: those training models to debug (Moon et al., 2023; Ni et al., 2024; Chae et al., 2024) and those providing external feedback to pretrained models (Chen et al., 2023b; Olausson et al., 2023; Zhong et al., 2024). Both groups share reliance on (gold) unit tests to train models and/or provide feedback. Thus, UTGEN is complementary to both kinds of debugging methods and can be easily integrated into either by using unit tests generated by UTGEN when human-written unit tests are scarce or unavailable. To this end, in Section 5 we show that UTGENs generated unit tests can be used to provide feedback to LLMs via rubberduck debugging proposed in Chen et al. (2023b). Additionally, in Section 3.3, we propose UTDEBUG, an improvised debugging pipeline that handles noisy feedback from inaccurately generated UTs via test-time scaling and backtracking. Orthogonal to our work, another direction of prior work involves curating benchmarks to measure LLMs debugging abilities (Muennighoff et al., 2024; Jain et al., 2024; Tian et al., 2024) which we also use and build upon in our experiments to showcase the effectiveness of UTGEN. 3. UTGEN and UTDEBUG: Unit Test"
        },
        {
            "title": "Generation and Automated Debugging",
            "content": "3.1. Background and Task Setup Given natural language task description for coding task, we focus on generating unit tests (UTs) in the format of input-output pairs that are consistent with the task description d. Our setup is consistent with Chen et al. (2023a) and Jain et al. (2024) who also consider unit tests in the form of input-output pairs generated without utilizing the correct implementation of the function. More generally, our setting is akin to parameterized unit testing (Tillmann et al., 2010) and uses the notion of functional correctness, i.e., measuring correctness by simulating an exhaustive set of scenarios (UT inputs) and ensuring that the function performs as expected (correct output as per the problem description d).2 Notation and Desiderata. Let denote the natural language description of the function to be implemented (top yellow box in Figure 1). We assume that this description specifies the input space and output space Y. Furthermore, let the set of all functions that correctly solve the task be Fd. Then, valid unit test (x, y) for task is: , i.e., the input of the unit test is valid input as per the task description d. fr(x) = y, fr Fd, i.e., is the expected output as per the task description d, and therefore, is the result one gets by executing any correct implementation. For example, in Figure 1, 120 is valid input, as it is number, whereas \"apple\" is not. Similarly, 121 is the expected output of the function (given 120 as input), while 122 would be an invalid output. Thus, (120, 121) is valid unit test for the task. Unit Test Generator. Addressing RQ1, we define the desirable properties of an automatic unit test generator Tθ, parameterized by θ. Ideally, Tθ should generate valid unit 2We note that the correctness of code or function can also be determined via formal verification, i.e., by mathematically proving it functions correctly in all situations. However, we do not consider this paradigm in our work, as we focus on empirical methods and applications, e.g., code debugging. 3 Learning to Generate Unit Tests for Automated Debugging Figure 2. UTGEN Training Pipeline: Starting with training data for code generation (problem description and gold code), we create training data for UT generation in three stages: (I) perturbing gold code to generate faulty codes, (II) generating UT inputs and filtering for failing UTs, and (III) generating and relabeling chain-of-thought rationales conditioned on the gold codes outputs. tests from task description d, i.e., Tθ(d) (cid:55) (x, y) without any manual intervention. However, to account for downstream utility of unit tests, we denote potentially buggy code under testing as ˆfb. If ˆfb / Fd is faulty or implemented incorrectly, it should fail one or more valid unit tests generated by Tθ. Therefore, given buggy piece of code ˆfb for the problem d, desirable unit test generator should be able to efficiently generate failing unit tests: Tθ(d, ˆfb) (cid:55) (x, y), such that ˆfb(x) = y, i.e., valid unit test input that uncovers the errors in ˆfb. Moreover, Tθ must also predict the correct UT output y, i.e., the output of the correct code. We test both generators of the form Tθ(d) that only consider the description, as well as debugging-style generators of the form Tθ(d, ˆfb) that start with buggy code solution ˆfb and take it into account in generating unit tests for it. Empirically, we find that generators of the form Tθ(d) lack sufficient context to generate tests that help reveal errors, limiting their utility for debugging. We therefore focus on training functions of the form Tθ(d, ˆfb). 3.2. UTGEN: Training LLMs for Unit Test Generation While several lines of prior work focus on curating training data for improving code generation (Guo et al., 2024a; Muennighoff et al., 2023), there is general lack of dedicated datasets for training the desired UT generator outlined above. Therefore, to improve along the lines of RQ2, we design training recipe that bootstraps this data from training datasets for code generation i.e., collection of problem descriptions and their corresponding code implementations. After collecting these examples, we perturb their gold solutions to generate faulty code for which we will be generating unit tests. For each example, we sample multiple unit tests and select the ones that are adversarial w.r.t. to the buggy code (i.e., inputs that cause it to fail), which we use to construct our training data. Below we describe each step: Problem Descriptions and Target Codes. We start with collection of coding problems with problem descriptions and gold codes. Specifically, we use publicly available data from Tulu-3 (Lambert et al., 2024) due to its large scale and the improvements noted by Lambert et al. (2024) when finetuning off-the-shelf LLMs on coding tasks. We filter it to focus on Python code with functional abstractions (further details in Appendix B). This yields total of 48.3K unique code problems. Corresponding to each problem description (prompt), the dataset contains correct target code solution intended for finetuning, which we use as the reference (gold) code solution (fr) as illustrated in Figure 2 (I).3 However, in order to train an adversarial unit test generator we would need access to incorrect or faulty code solutions that can be debugged, as unit tests must have some error to attack. We obtain these by using the LLM to perturb the reference code solution in realistic ways (prompt and details given in appendix Appendix E). Annotating Unit Tests. As mentioned in Section 3.1, one of the goals of the unit test generator is to be able to not only generate valid unit tests, but failing unit tests (that trigger errors). To facilitate this, given problem description, reference code fr, and buggy candidate code ˆfb, we sample different unit test inputs (via the prompts in Appendix E). unit test (x, fr(x)) is failing if fr(x) = ˆfb(x), i.e., if the output of the candidate fails to match the reference output (cf. Figure 2 (II)). Note that, while the LLM can be used to generate the output of unit test, it can often be inaccurate (Jain et al., 2024; Gu et al., 2024); so to ensure output accuracy, we use the output of the reference code during training. We revisit the discussion of how accurate LLMs are at output prediction during inference in Section 5. 3The code solutions in the Tulu-3 SFT mix have either been written by humans or by frontier models and are thus highly likely to be correct (Lambert et al., 2024). 4 Learning to Generate Unit Tests for Automated Debugging Figure 3. Left: We highlight potential issues with debugging faulty code using generated UTs: (I) non-failing UTs misclassify faulty code as correct; (II) UTs with incorrect outputs produce incorrect feedback and consequently, unsuccessful debugging. Right: To handle these unique challenges, we introduce UTDEBUG which (a) uses inference-time scaling to select better UT outputs based on majority vote, and (b) generates multiple UTs for validation and backtracking, discarding edits when overall pass rate decreases (as in round 1) and accepting edits when overall pass rate improves (as in round 2). Data Curation for Supervised Finetuning. When training LLMs according to UTGEN, the input to the LLM is the same prompt used for sampling unit tests (listed in Appendix E) and the output is an adversarial unit test. The goal here is to improve both output accuracy and attack rate jointly via supervised finetuning using the negative loglikelihood loss. Since LLMs have been known to struggle at the task of output prediction (Jain et al., 2024), we train the model to generate chain-of-thought (CoT; Wei et al., 2022) reasoning before predicting the UT output. To this end, we employ the post-hoc rationalization procedure outlined in Zelikman et al. (2022) given the entire UT (x, fr(x)), we ask the LLM to generate rationales supporting why fr(x) is the output corresponding to the input x. Then, to create the supervision data, we add these rationales as CoTs prior to the output prediction (see Figure 2 (III)). 3.3. UTDEBUG: Debugging with Generated Unit Tests crucial difference between using generated unit tests (as opposed to human-generated or gold unit tests) is the degree of noise in unit test feedback. Despite training models via UTGEN or otherwise generated unit test (ˆx, ˆy) may not be 100% accurate. This manifests in two ways: 1) the generated unit test input is not failing for given code under debugging ˆfb, i.e., fr(ˆx) = ˆfb(ˆx); 2) the generated unit test output is inaccurate, i.e., not consistent with what gold solution to the task description would yield (ˆy = fr(ˆx)). Both these types of errors can negatively impact downstream utility of unit tests (in our context, ability to debug) in different ways as illustrated in Figure 3 (left). First, lack of adversarial input could result in faulty code (albeit with subtle errors) being misclassified as correct, and thus, erroneously taken out of the debugging process. Next, incorrect unit test outputs could also lead to false positive, with subsequent edits introducing errors to an otherwise correct code. Moreover, even if the candidate code contains bugs, an incorrect unit test output can result in incorrect feedback given to the model during debugging, which in turn could be wasteful or detrimental to downstream performance. These considerations motivate our RQ3 on how best to incorporate potentially noisy feedback. Below we propose UTDEBUG, which contains two effective ways of mitigating noisy feedback from automatically generated unit tests (with additional details in Appendix C): Boosting Output Accuracy via Test-Time Scaling. Building on past work that has shown the benefits of scaling up inference-time compute (Wang et al., 2022; Lightman et al., 2023; Snell et al., 2024) for LLMs, we first improve output accuracy by allocating additional computation to the problem. Specifically, we use self-consistency (SC; Wang et al., 2022), whereby, for given UT input, we sample = 8 output completions (including CoT rationales) and take the most common final UT output (majority vote) as the final answer, as shown in Figure 3 (top-right). To further boost output accuracy, we upsample UT inputs and only retain those where the final answer gets over 50% of the votes (i.e., 4 votes), discarding the unit test otherwise. Our results in Section 5.3 show that this boosts accuracy by up to 11.4%. Back-Tracking and Cross-Validation. In addition to enabling test-time scaling to improve output accuracy, in settings with noisy feedback, it is important to know when to abstain or backtrack, e.g., discarding edits made to candidate code. This can be beneficial when the generated 5 Learning to Generate Unit Tests for Automated Debugging unit test has an incorrect output, and therefore, the feedback is incorrect. It can also come into play when the feedback is sound but is not correctly incorporated by the model. Inspired by test-driven development with human programmers debugging failing unit test, we accept changes only if the revised code passes the previously failing UT. Moreover, in practice, developers typically use suite of multiple UTs; when changes are made to codebase, all UTs are run and errors are logged. Such suite can inform developers when change or fix in one part of the code has caused errors in another part. In the context of debugging, this process can help prevent overfitting, whereby the debugging agent modifies code to make single unit test pass, but thereby causes other tests to fail, leading to one unit test passing, but making the code unusable generally. Therefore, when debugging via UTGEN, in each round we generate UTs. We use one to provide feedback to the debugging LLM, and accept the debuggers edits only if the pass rate on the entire set of unit tests improves; if it does not, we backtrack. This is illustrated in Figure 3 (b) as the edits in round 1 are discarded and backtracked whereas the edits in round 2 improve the entire test suite and are accepted. 4. Experimental Setup Models. We demonstrate the effectiveness of UTGEN on three 7-8B scale LLMs across different model families that are adept at coding, namely, Llama-3 8B Instruct (AI@Meta, 2024), Llama-3.1 8B Instruct (Dubey et al., 2024), Qwen 2.5 Coder 7B (Hui et al., 2024). Dataset. We use debugging datasets based on popular LLM coding benchmarks, HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) along with their extended evaluation suites with more unit tests as proposed by (Liu et al., 2024). We describe their construction below, with further details in Appendix A. HE+Fix. We start with the HumanEvalFix dataset containing human-introduced errors to gold solutions proposed by Muennighoff et al. (2024). However, this dataset only uses minimal unit tests to evaluate code correctness (from HumanEval) which has shown to be unreliable (Li et al., 2023), as it can miss errors due to low coverage. Therefore, we filter for problems that overlap with EvalPlus (Liu et al., 2024), which contains over 80 more unit tests. This yields 158 problems that each have faulty solution and an expanded suite of private UTs i.e., UTs not used for debugging for evaluation. MBPP+Fix. We construct debugging dataset based on the MBPP+ benchmark (Austin et al., 2021; Liu et al., 2024; Ni et al., 2024). To obtain faulty codes, we sample 16 solutions for each problem from different 7-8B scale models and filter for incorrect solutions based on the private unit tests (described in Appendix A). Then, we select one faulty solution at random corresponding to each problem. This results in total of 325 problems with incorrect solutions representing realistic errors made by LLM coders. We find such errors are generally more challenging for models to debug (cf. Section 5). MBPP+Fix (Hard). To make the debugging task more challenging, we create different split from MBPP+ with more subtle errors that are harder to debug. We identify these subtle errors by following similar code generation setup described above, but only retain faulty code that passes between 50% - 95% of unit tests, as these partially correct solutions contain less obvious logical flaws and often require handling difficult corner cases. This results in total of 170 problems with faulty codes. For additional details on these datasets including the number of test cases per problem and pass rate of faulty codes, we refer interested readers to Appendix A. Evaluation Metrics. Below we describe three intrinsic evaluation metrics for unit test generation: attack rate, output accuracy, and accuracy attack; along with pass@1 accuracy as the extrinsic metric to measure LLMs debugging abilities using UT-feedback. Attack Rate. This metric measures UT generator Tθs attacking ability, i.e., its ability to generate failing unit test input ˆx for given buggy solution ˆfb. We measure this by matching if the output of gold reference solution fr for input ˆx differs from that of the buggy code ˆfb. Note that this does not take into account the accuracy of the unit test output which we measure separately below. Mathematically, for any dataset of coding problems, attack rate is defined as: AttackRate = 100 (cid:88) 1 fr(ˆx)= ˆfb(ˆx); dD where(ˆx, ˆy) Tθ(d, ˆfb) Output Accuracy. This metric measures how often the output of generated unit test ˆy is consistent with the problem description, i.e., generates the same output as the reference gold code fr. Output accuracy does not require the generated unit test to fail. In other words, OutputAcc = 100 (cid:88) dD 1ˆy=fr(ˆx); (ˆx, ˆy) Tθ(d, ˆfb) Accuracy Attack. This metric combines both attack rate and output accuracy and represents how often unit test generator Tθ generates useful (i.e., failing) unit test 6 Learning to Generate Unit Tests for Automated Debugging for given target code ˆfb while also predicting the output correctly. We calculate this as, Acc. Attack = 100 (cid:88) 1 dD fr(ˆx)=ˆ(fb)(ˆx),ˆy=fr(ˆx); where (ˆx, ˆy) Tθ(d, ˆfb) Code Accuracy. In order to evaluate the utility of generated unit tests via code debugging, we follow prior work (Chen et al., 2023b; Chae et al., 2024) in reporting pass@1 code accuracy, i.e., the percentage of codes passing all unit tests after 3 rounds of debugging. Note that while we debug with model-generated UTs, we evaluate code accuracy using private human-annotated UTs. As the downstream application of unit-test generation is independent of the inherent task, we rely on intrinsic metrics during development on validation split from MBPP+ data, e.g., for checkpoint selection and prompt design. Then we measure the utility and effectiveness of UTGEN by comparing against baselines on code debugging tasks. Baselines. We compare UTDEBUG with feedback from UTGEN against the following (prompts in Appendix E): No UT feedback: In this baseline, we use self-generated or self-critique feedback, following prior work (Madaan et al., 2024; Shinn et al., 2024; Chen et al., 2023b). Specifically, we prompt the model to generate an explanation about the bugs in the target code in addition to determining if the target code is correct, i.e., does not have any more bugs (consistent with Chen et al. (2023b)). Randomly-sampled UTs: Here, we relax the requirement for the LLM to generate failing unit tests, and prompt the model to jointly generate valid unit tests (inputs and outputs) based on the task description, irrespective of the target code, i.e., we sample (ˆx, ˆy) Tθ(d). Prompted Failing UTs: This baseline uses the same prompts as UTGEN without any training and prompts an LLM to generate failing unit test (with correct output) given the problem description and the target code. Here, we sample (ˆx, ˆy) Tθ(d, ˆfb). 5. Results and Analysis Table 1. Evaluation on intrinsic UT generation metrics of different UT generation baselines and UTGEN across different model families on MBPP+Fix (Hard) over 3 runs. Higher is better for all three intrinsic metrics. Model Method Attack Rate Out Acc. Acc. Attack Llama3 8B Llama3.1 8B Qwen2.5 7B Random Prompted UTGEN Random Prompted UTGEN Random Prompted UTGEN 30.98 38.04 41.18 21.76 40.59 41. 26.27 39.80 41.96 46.86 37.10 48.24 39.80 26.86 47.75 57.25 40.78 58.04 10.24 11.51 16.57 9.02 9.61 16. 13.53 14.71 20.20 Trade-offs in Attack Rate and Output Accuracy. In Table 1, we benchmark the zero-shot abilities of different models on UT generation, as well as our improved training method (corresponding to RQ2). Here, we observe clear tradeoff: prompted and random unit test generation each optimizes output accuracy or attack rate at the cost of the other metric. While randomly sampled UTs have relatively higher output accuracy (i.e., the output of the unit test is correct according to the problem description), the random baseline often lacks the ability to generate UTs with inputs that trigger errors (failing UTs). This can be explained by the fact that it is not conditioned on the faulty code ˆfb. On the other hand, when models are prompted to generate UT that breaks the faulty code in the prompted UT baseline, they generally achieve higher attack rate; however, they lag in terms of output accuracy. For instance, in the case of Qwen2.5, switching from randomly sampled to prompting failing UTs improves the attack rate by 13.53% but decreases the output accuracy by 16.47%, hence leading to comparable Acc. Attack scores (13.53% vs 14.71%). Taken together, this suggests the presence of competing desirable traits of UT generators (cf. Section 3.1; RQ1): without training, one baseline often generates trivial unit tests for which the output can easily be predicted (high output accuracy, low attack rate), while the other generates challenging unit tests for which it cannot produce the expected answer (low output accuracy, high attack rate). It also indicates that while models can generate challenging unit test inputs that result in code failure, models struggle with reasoning over correct outputs of failing UT inputs. 5.1. Intrinsic Evaluation of Unit Test Generation In order to study the inherent UT generation abilities of different models and baselines, we use the intrinsic metrics defined by RQ1 and outlined in Section 4 in Table 1 on the most challenging debugging task MBPP+Fix (Hard) averaged over 3 runs. UTGEN is Most Effective at Unit Test Generation. We propose to break through this trade-off (thereby addressing RQ2, How can we improve LLMs UT generation abilities?) by training models with UTGEN, where we directly supervise models to have both high output accuracy and high attack rate. Ideally, UTGEN should lead to models that Learning to Generate Unit Tests for Automated Debugging Table 2. Evaluating pass@1 accuracies after debugging with UTDEBUG, using UTs generated by UTGEN and other baselines for 3 rounds, on HumanEval+Fix (HE+Fix), MPBB+Fix, and MBPP+Fix (Hard). Model UT Method HE+Fix MBPP+Fix (Hard) Llama-3 8B Llama-3.1 8B Qwen-2.5 7B No UT Random Prompted UTGEN No UT Random Prompted UTGEN No UT Random Prompted UTGEN 27.22 51.90 51.90 53.80 31.65 62.03 56.33 67. 52.53 79.75 75.32 82.91 16.31 30.46 28.92 37.54 10.15 33.54 28.00 36.92 23.08 34.77 32.92 37.54 11.76 17.06 22.94 28.82 11.18 13.53 24.71 28. 16.47 17.06 24.12 29.41 generate unit tests in the sweet spot of difficulty, where they are hard enough to trigger errors but not so hard that their output cannot be predicted. Table 1 shows that models trained with UTGEN consistently rank highest at jointly generating failing UT inputs with correct outputs (as measured via Acc. Attack in Table 1). For instance, on the strongest coding model in our setting, Qwen2.5, UTGEN obtains the highest attack rate, and output accuracy as well as improves Acc. Attack score by 5.49% (absolute) over failing UTs generated by prompting. Similarly, Llama3.1 improves (in terms of attack rate) over randomly-sampled UTs by 19.61% and the output accuracy of prompted failing UTs by 20.89%, ultimately improving the joint Acc. Attack score by up to 7.65%. We report the intrinsic metrics for HE+Fix and MBPP+Fix in Appendix D. 5.2. Impact of Generated Unit Tests on Debugging UTs from UTGEN are Best for UTDEBUG. Addressing RQ3, we measure how effective each type of UT generator is in downstream debugging evaluation. From Table 2, we observe that across different LLM families, multi-turn debugging with UTDEBUG is more effective when using generated UTs from UTGEN than debugging with UTs generated by the baselines, as well as debugging without feedback. For instance, with Qwen2.5, after 3 rounds of debugging on MBPP+Fix, UTGEN improves over the randomly-sampled UT baseline by 3% (absolute), debugging by failing UTs by 7.59% and without UT feedback by 21.23% (absolute). Moreover, on the more challenging MBPP+Fix (Hard) split, UTGEN improves over randomly-sampled UTs by 12.35% and the baseline without UT feedback by 12.94%. Given that we use the same underlying LLM and similar feedback templates, the results in Table 2 show that UTs generated by UTGEN provide the most useful and effective feedback. Table 3. Ablating components of UTDEBUGs pipeline (cf. Section 3.3) for two different unit test generation methods (including UTGEN) on MBPP+Fix using Qwen2.5. UT Method Randomly-sampled (Qwen2.5) - Output Test-time Scaling - Backtracking UTGEN (Qwen2.5) - Output Test-time Scaling - Backtracking Acc. 34.77 30.77 32.61 37.54 26.15 34. - 4.0 - 2.2 - 11.4 - 3.2 Lastly, we find that LLM debugging without any UT feedback is least effective across model families, thus, establishing that despite noise and issues discussed in Section 3.3 and Figure 3, UTDEBUG with UT-based feedback is more effective than self-generated feedback. This aligns with past work indicating that LLMs are poor at self-critique (Huang et al., 2024; Stechly et al., 2024; Chen et al., 2024). Debugging Difficulty Varies Across Datasets. Comparing the post-debugging accuracy across the three datasets in Table 2 suggests different difficulty levels when it comes to LLM debugging. The relatively high accuracies on HE+Fix (especially with Qwen-2.5) suggest that human-introduced errors in that dataset are obvious and thus too easy for the models to discover and debug. This is also corroborated by the fact that starting codes in this dataset have pass rate of 0%, i.e., fail on all unit tests, suggesting high-level coarse-grained errors, e.g., syntax errors that prevent execution, or incorrect function references. On the other hand, MBPP+Fix (Hard) (with an initial pass rate of 75%) appears to be the hardest to debug, with the lowest overall post-debugging accuracy across models up to difference of 53.5% in final accuracy for UTGEN based on Qwen-2.5. This in turn suggests that LLMs still struggle with identifying and fixing subtle flaws in generated code, especially in scenarios involving corner cases wherein the model passes some unit tests and fails at the rest. 5.3. Effectiveness of the UTDEBUG Pipeline In Section 3.3, we highlighted the challenges of debugging with imperfect model-generated unit tests and suggested remedies to make our debugging pipeline, UTDEBUG, robust to noisy feedback from such UTs. We study the effectiveness of these measures, i.e., test-time scaling of UT outputs, and backtracking, on debugging with = 3 generated UTs for 3 rounds using Qwen2.5 on the MBPP+Fix dataset in Table 3. Test-time Scaling and Backtracking are Crucial for LLM Debugging with Generated UTs. From Table 3, we observe that irrespective of the underlying method for UT 8 Learning to Generate Unit Tests for Automated Debugging Table 4. Evaluation of closed-source models on intrinsic UT generation metrics on MBPP+Fix (Hard) over 3 runs. Model Method Attack Rate Out Acc. Acc. Attack Qwen2.5 7B GPT-4o DeepSeek3 Random Prompted UTGEN Random Prompted Random Prompted 26.27 39.80 41.96 23.92 54.51 24.31 54.31 57.25 40.78 58.04 57.25 60. 63.53 58.04 13.53 14.71 20.20 17.25 33.33 17.84 33.33 validation on the entire generate test suite. We analyze the impact of increasing the number of generated unit tests on downstream accuracy of Qwen2.5 after 3 rounds of debugging in Figure 4. Our findings are as follows: First, Figure 4 shows that despite increasing the number of generated unit tests, UTGEN consistently outperforms randomly sampled UTs that may not be failing. This highlights the benefits of generating targeted unit tests conditioned on buggy code in order to trigger errors and generate appropriate feedback for debugging. In settings with constrained resources, i.e., sampling 3 UTs, UTGEN is more effective at identifying errors by up to 3% on MBPP+Fix and 12% on MBPP+Fix (Hard). In the case of MBPP+Fix (Hard), which contains codes with less obvious errors and are harder to debug, we find that despite scaling to up to 15 generated UTs, the performance gap between UTGEN and randomly-sampled UTs remains at 10% (absolute). 5.5. Comparison to Frontier LLMs In Table 1 we evaluated open-source LLMs that we trained using UTGEN, and in Section 3.3 we applied these models in our UTDEBUG framework for debugging, finding the trained models to be effective. Table 4 compares stronger frontier models GPT-4o (Achiam et al., 2023) and DeepSeek-V3 (Guo et al., 2024b) to the best open-source 7-8B model tested (Qwen2.5 7B). Here, we see that, while they outperform Qwen, even these stronger and much larger frontier models struggle at the task of generating failing unit tests. First, we find that when prompting models with faulty codes, the attack rate of the generated UTs is slightly over 50% on MBPP+Fix (Hard) dataset, showcasing the inherently challenging nature of isolating corner cases and subtle errors in partially correct code. Furthermore, both GPT-4o, DeepSeek-V3 attain comparable output accuracies to that of our trained UTGEN model on top of Qwen2.5, while being larger and more costly to run (although their Acc. Attack scores are higher). Overall, we find that using UT-based feedback even with stronger LLMs only triggers Figure 4. Impact of increasing number of unit tests across MBPP+Fix and MBPP+Fix (Hard) datasets using UTs generated by UTGEN and randomly-sampling with Qwen2.5 model. generation (either randomly sampled or from UTGEN), removing either backtracking or test-time scaling hurts downstream performance. First, we find that removing test-time scaling for predicting the output of UTs decreases the performance of randomly-sampled UTs by 4% and that of UTGEN by 11.4%. Note that larger drop in UTGENs performance when removing test-time scaling of UT output prediction is consistent with the findings in Section 5.1 that models struggle with reasoning over correct outputs for failing UT inputs (more often generated by UTGEN). Therefore, test-time scaling in output prediction provides greater boost for reasoning over challenging inputs (Wang et al., 2022), and consequently, removing it yields larger drops for UTGEN. Additionally, Table 3 demonstrates that, without backtracking and validation on other generated unit tests, LLMs tend to overfit on the unit test contained in the feedback, resulting in up to 3.2% drop in performance of UTGEN without backtracking. 5.4. Scaling with Number of Generated UTs Thus far, all our experiments use = 3 generated UTs across baselines and models. However, as we described in Section 3.3, having multiple UTs can be advantageous because: (i) there is higher likelihood of generating failing UT and getting reliable signal for when the code is correct, (ii) more robust signal for when to backtrack using 9 Learning to Generate Unit Tests for Automated Debugging and identifies errors 1 in 3 times, leaving tremendous room for growth in the ability of models to identify and provide feedback on partially correct code. Note that cost is major factor in automated debugging; we show that using UTGEN for debugging in UTDEBUG is successful, but involves generating multiple UTs with selfconsistency across several rounds of debugging for each problem (cf. Algorithms 1 and 2 in Appendix C). Such calls will quickly become costly on frontier models like GPT-4o and DeepSeek-3, making 7-8B models like the ones we train with UTGEN attractive options. Moreover, since these frontier models are not trainable, there is less room for improvement when scaling training data. 6. Discussion and Conclusion Our work on UTGEN contributes to the broader landscape of verification and feedback generation for LLM-generated code. While recent work has focused on training verifiers to provide feedback (Mahan et al., 2024; Zhang et al., 2024), key challenge remains in obtaining high-quality feedback signals for debugging. UTGEN addresses this by directly generating unit tests that can identify problems in code, complementing existing work on how to effectively incorporate and present feedback for debugging (Chen et al., 2023b; Zhong et al., 2024) along with test-time scaling and backtracking incorporated in UTDEBUG. Our results demonstrate that without quality signal to determine code correctness and/or how faulty code is failing (in the form of unit tests), using LLMs to generate feedback and debug still proves to be challenging. This is one of the first efforts in this direction, and we hope to spark more interest in future work toward LLM-generated unit tests (both input and outputs) that reveal the models coding errors. Our approach connects to and complements recent work on handling real-world programming issues. While approaches designed for SWEBench (Jimenez et al., 2024) focus on fixing known issues from GitHub by understanding and implementing fixes for bug reports, UTGEN addresses the upstream challenge of automatically discovering potential issues in new code through test generation. Both tasks share core challenge: determining the expected behavior of code without access to correct implementations. This connects to the fundamental concept of simulatability from computability theory (Rice, 1953), where we ask whether program can predict the behavior of another program. Recent work such as Jain et al. (2024) shows that while LLMs can often simulate existing code by tracing execution steps, they struggle more with predicting correct outputs from specifications alone. Our results align with these findings while UTGEN can generate test inputs that trigger errors (high attack rate), predicting correct expected outputs remains challenging (lower output accuracy). This suggests that improving LLMs ability to reason about intended program behavior from specifications remains crucial direction for future work. Nevertheless, we find that the modifications made to debugging in UTDEBUG help boost UTGENs accuracy and account for noise, leading to downstream gains. Conclusion. We first identified key trade-off between attack rate and output prediction accuracy when predicting unit tests with models. In light of this trade-off, we introduced UTGEN, new method for creating training data and teaching models to produce unit tests. This allows us to train models to produce better unit tests, as measured by intrinsic metrics like attack rate and output accuracy. Moreover, finding that existing data contains large numbers of easy errors, we introduce new subset of data with challenging and hardto-diagnose errors. To enable debugging with automated unit tests, we propose UTDEBUG, wherein we augment our predictors accuracy with test-time scaling and regularize it using cross-validation and back-tracking procedure that prevents it from overfitting to narrow or incorrect unit test. This, combined with our improved predictor, results in consistent increases in debugging performance across models, gap that persists as we scale the number of unit tests."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Swarnadeep Saha for insightful discussions in the early phases of this work. This work was supported by NSF-CAREER Award 1846185, DARPA ECOLE Program No. HR00112390060, and NSF-AI Engage Institute DRL-2112635. Any opinions, findings, and conclusions or recommendations in this work are those of the author(s) and do not necessarily reflect the views of the sponsors."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. AI@Meta. Llama 3 model card. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. 2024. Anthropic. Opus, claude haiku. The sonnet, 3 model ily: 2024. https://www-cdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf. famURL Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. 10 Learning to Generate Unit Tests for Automated Debugging Cadar, C., Dunbar, D., Engler, D. R., et al. Klee: unassisted and automatic generation of high-coverage tests for complex systems programs. In OSDI, volume 8, pp. 209224, 2008. Y., and Liang, W. Deepseek-coder: When the large language model meets programming the rise of code intelligence, 2024a. URL https://arxiv.org/abs/ 2401.14196. Cha, S. K., Woo, M., and Brumley, D. Program-adaptive mutational fuzzing. In 2015 IEEE Symposium on Security and Privacy, pp. 725741. IEEE, 2015. Chae, H., Kwon, T., Moon, S., Song, Y., Kang, D., Ong, K. T.-i., Kwak, B.-w., Bae, S., Hwang, S.-w., and Yeo, J. Coffee-gym: An environment for evaluating and improving natural language feedback on erroneous code. arXiv preprint arXiv:2409.19715, 2024. Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J.-G., and Chen, W. Codet: Code generation with generated tests. In The Eleventh International Conference on Learning Representations, 2023a. Chen, J. C.-Y., Prasad, A., Saha, S., Stengel-Eskin, E., and Bansal, M. Magicore: Multi-agent, iterative, coarse-to-fine refinement for reasoning. arXiv preprint arXiv:2409.12147, 2024. URL https://arxiv. org/abs/2409.12147. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Chen, X., Lin, M., Scharli, N., and Zhou, D. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023b. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ficco, M., Pietrantuono, R., and Russo, S. Bug localization in test-driven development. Advances in Software Engineering, 2011(1):492757, 2011. Gemini, T., Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gu, A., Rozi`ere, B., Leather, H., Solar-Lezama, A., Synnaeve, G., and Wang, S. I. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024. Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y., Luo, F., Xiong, Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y., et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024b. Holler, C., Herzig, K., and Zeller, A. Fuzzing with code fragments. In 21st USENIX Security Symposium (USENIX Security 12), pp. 445458, 2012. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. Large language modIn The Twelfth els cannot self-correct reasoning yet. International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=IkmD3fKBPQ. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. R. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=VTF8yNQM66. King, J. C. Symbolic execution and program testing. Communications of the ACM, 19(7):385394, 1976. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tulu 3: Pushing frontiers in open language model post-training. 2024. Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. 11 Learning to Generate Unit Tests for Automated Debugging Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with selffeedback. Advances in Neural Information Processing Systems, 36, 2024. Mahan, D., Van Phung, D., Rafailov, R., Blagden, C., Lile, N., Castricato, L., Franken, J.-P., Finn, C., and Albalak, A. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. Maximilien, E. and Williams, L. Assessing test-driven development at ibm. In 25th International Conference on Software Engineering, 2003. Proceedings., pp. 564569, 2003. doi: 10.1109/ICSE.2003.1201238. Moon, S., Chae, H., Song, Y., Kwon, T., Kang, D., Ong, K. T.-i., Hwang, S.-w., and Yeo, J. Coffee: Boost your code llms by fixing bugs with feedback. arXiv preprint arXiv:2311.07215, 2023. Muennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B., Zhuo, T. Y., Singh, S., Tang, X., von Werra, L., and Longpre, S. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023. Muennighoff, N., Liu, Q., Zebaze, A. R., Zheng, Q., Hui, B., Zhuo, T. Y., Singh, S., Tang, X., Werra, L. V., and Longpre, S. Octopack: Instruction tuning code large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=mw1PWNSWZP. Nagappan, N., Maximilien, E. M., Bhat, T., and Williams, L. A. Realizing quality improvement through test driven development: results and experiences of four industrial teams. Empir. Softw. Eng., 13(3):289302, 2008. doi: 10. 1007/S10664-008-9062-Z. URL https://doi.org/ 10.1007/s10664-008-9062-z. Ni, A., Allamanis, M., Cohan, A., Deng, Y., Shi, K., Sutton, C., and Yin, P. Next: Teaching large language models to reason about code execution. In Forty-first International Conference on Machine Learning, 2024. Olausson, T. X., Inala, J. P., Wang, C., Gao, J., and SolarLezama, A. Is self-repair silver bullet for code genIn The Twelfth International Conference on eration? Learning Representations, 2023. Rice, H. G. Classes of recursively enumerable sets and their decision problems. Transactions of the American Mathematical Society, 74(2):358366, 1953. Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Schafer, M., Nadi, S., Eghbali, A., and Tip, F. An empirical evaluation of using large language models for automated unit test generation. IEEE Transactions on Software Engineering, 2023. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time compute optimally can be more effecarXiv preprint tive than scaling model parameters. arXiv:2408.03314, 2024. URL https://arxiv. org/abs/2408.03314. Stechly, K., Valmeekam, K., and Kambhampati, S. On the self-verification limitations of large language models on reasoning and planning tasks. arXiv preprint arXiv:2402.08115, 2024. URL https://arxiv. org/abs/2402.08115. Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi`ere, M., Kale, M. S., Love, J., et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Tian, R., Ye, Y., Qin, Y., Cong, X., Lin, Y., Pan, Y., Wu, Y., Hui, H., Liu, W., Liu, Z., et al. Debugbench: Evaluating debugging capability of large language models. arXiv preprint arXiv:2401.04621, 2024. Tillmann, N., de Halleux, J., and Xie, T. Parameterized unit testing: theory and practice. In 2010 ACM/IEEE 32nd International Conference on Software Engineering, volume 2, pp. 483484, 2010. doi: 10.1145/1810295. 1810441. Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Selfconsistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 12 Learning to Generate Unit Tests for Automated Debugging Zelikman, E., Wu, Y., Mu, J., and Goodman, N. STaR: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Zhang, L., Hosseini, A., Bansal, H., Kazemi, M., Kumar, A., and Agarwal, R. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. Zhong, L., Wang, Z., and Shang, J. Ldb: large language model debugger via verifying runtime execution step-bystep. arXiv preprint arXiv:2402.16906, 2024. A. Debugging Datasets HE+Fix. This dataset contains total of 158 problems each with one incorrect human-written code and has an initial pass rate (prior to debugging) of 0%, i.e., all private unit tests are failing. As mentioned in Section 4, we use the dataset provided by Muennighoff et al. (2024)4 but replace the test set for each problem from the original test suite in HumanEval (Chen et al., 2021) to that in the EvalPlus evaluation suite (Liu et al., 2024). This increases the average unit tests per problem from 8.17 to 775.18 gold unit tests. Note that we have an automatic UT extraction script for the test code in EvalPlus, and we only retain problems for which this extraction is successful (158 out of 164). MBPP+Fix and MBPP+Fix (Hard). We begin with 378 problems in the MBPP+ dataset (Liu et al., 2024)5 and follow the same gold UT extraction step described above, discarding problems for which the extraction fails. This leaves us with 375 problems, for which we sample 16 solutions per problem across multiple LLMs: Gemma-7B-IT (Team et al., 2024), Llama3 8B Instruct (AI@Meta, 2024), Llama3.1 8B Instruct (Dubey et al., 2024), Tulu-3 8B SFT (Lambert et al., 2024), DeepSeek 7B coder (Guo et al., 2024b), Qwen-2.5 Coder (Hui et al., 2024). To generate MBPP+Fix, we filter for incorrect solutions (i.e., with at least one failing UT) and then randomly sample one incorrect code per problem. This yields 325 problems in total, each with one faulty code. This dataset has an initial pass rate of 24.21% and an average of 107.45 gold unit tests per problem. In order to construct, MBPP+Fix (Hard), we follow similar process but select only incorrect solutions which pass 50 - 95% of unit tests. The intuition here is that solutions that are partially correct are often harder to debug than those that are fully incorrect. We then randomly sample one such incorrect solution per problem, yielding dataset of 170 problems with an initial pass rate of 74.83% and an average of 107.49 gold unit tests. 4https://huggingface.co/datasets/bigcode/ humanevalpack 5https://huggingface.co/datasets/ evalplus/mbppplus 13 B. UTGEN Training Preprocessing Tulu Data. We use the Tulu-3 SFT mixture dataset released by Lambert et al. (2024) which contains total of 939.3K instances.6 However, it contains mixture of prompts for instruction-following, math reasoning, and coding. Therefore, we filter for prompts involving Python coding by regex search for keywords python and def which suit our task setup described in Section 3.1. Furthermore, we filter out instances with more than 2K tokens in the prompt and ensure the prompt is valid unicode string. This results in total of 48.3K instances for which we use the prompt as the problem description and extract code from the response of the last turn when multi-turn interactions are present provided that the extracted code executes without any errors or issues. Finally, we prompt the LLM to be trained with UTGEN to generate 2 corrupted versions of this code to serve as the target code, and for each target code, we make 5 attempts to generate failing unit test inputs and filter out instances that do not have any such UTs. This is followed by the rationalization step (with the same underlying LLM to be trained) described in Section 3.2 and in Figure 2, which results in roughly 30K instances for each model trained with UTGEN (Qwen2.5, Llama-3, -3.1 8B). Training Hyperparameters. We train each model for 10 epochs with batch size of 16 and learning rate of 5e-6 with cosine learning rate scheduler. Moreover, we only compute negative log-likelihood loss over the completions. We use LoRA (Hu et al., 2021) with rank of 16, α = 32 with dropout of 0.05. We perform checkpoints selection based on the intrinsic Acc. Attack metric. C. Overall Pipeline for UTDEBUG In Algorithm 1 we describe the process of generating UTs for candidate buggy code ˆfb using any UT generation method and perform test time scaling for UT output prediction. This is then used within UTDEBUG as shown in Algorithm 2, which identifies failing UTs, debugs the target code ˆfb based on feedback from failing UTs over multiple rounds, and returns the debugged (edited code). As illustrated in Figure 3 (b), edits are accepted only if the newly generated code achieves higher pass rate than the code before debugging, otherwise the edits are discarded. Inference Hyperparameters. When sampling multiple UTs and for generating LLMs response to UT feedback we use temperature sampling with temp = 0.7 and top = 0.9. 6https://huggingface.co/datasets/allenai/ tulu-3-sft-mixture Learning to Generate Unit Tests for Automated Debugging Table 5. Evaluation on intrinsic metrics on HE+Fix and MBPP+Fix for different UT generation methods across multiple models. Model UT Method HE+Fix MBPP+Fix Attack Rate Out Acc. Acc. Attack Attack Rate Out Acc. Acc. Attack Llama-3 8B Llama-3.1 8B Qwen-2.5 7B Random Prompted UTGEN Random Prompted UTGEN Random Prompted UTGEN 89.63 95.73 96.34 76.02 92.68 96.54 90.85 93.29 96.54 72.97 39.59 53.27 63.52 34.53 56. 87.36 54.55 72.90 72.97 38.78 52.04 63.52 33.89 55.76 86.47 53.91 72.48 62.56 62.67 67.18 47.28 59.28 62. 55.28 62.97 65.54 41.85 29.64 42.67 36.0 19.38 43.59 52.31 35.29 48.62 24.28 16.41 26.87 21.33 11.08 25. 30.38 22.60 32.82 Algorithm 1 BuildUT: Build Generated Unit Test Suite Input: d, ˆfb // problem description and buggy code Params: Number of UTs n, Number of SC samples k, Unit Test Generator Tθ Output: Set of Generated UTs // Initialization, i.e., no generated UTs for UT index [1, , 3 n] do // Generates up to distinct UTs from the UT generator Tθ (ˆxi, ˆyi) Tθ(d, ˆfb) // Sample UTs from UT generator νi // Initialize vote lookup for self-consistency for Output index [1, , k] do Tθ(d, ˆfbxi) // Sample UT output ri yj extractAns(ri νi[yj] νi[yj] + 1 // Append vote tally j) // Extract UT output ˆyi argmax(νi) // Use majority vote for UT output if νi[ˆyi] 0.5k then // Answer gets over 50% of the vote + (ˆxi, ˆyi) // Add to generated UT set if then return return D. Additional Intrinsic Evaluation Similar to Table 1, we report intrinsic evaluation metrics (cf. Section 4) for HE+Fix and MBPP+Fix datasets in Table 5. Consistent with the findings in Section 5.1, without training, we observe trade-off between attack rate and output accuracy, with randomly sampled UTs showing higher output accuracy whereas the prompted baseline exhibits higher attack rates and vice-versa. Once again UTGEN training best balances this trade-off yielding both high output accuracy and attack rate. However, due to the relative ease of attacking faulty codes in HE+Fix (initial pass rate of 0%) almost any generated UT fails and thus can be used for debugging. This, coupled with the higher output accuracy, results in the random baseline having the highest score on Acc. Attack. Note that we mitigate this impact on downstream performance by using test-time scaling for output prediction in UTDEBUG, which especially boosts the accuracies of targeted UTs generated based on the buggy code. On Algorithm 2 UTDebug: Debugging with generated UTs Input: d, ˆfb // problem description and buggy code Params: Number of UTs n, Number of SC samples k, Unit Test Generator Tθ, Number of debugging rounds Output: Debugged code ˆfe // Initializing number of rounds left ˆfe ˆfb // Initializing edited code with code to debug acceptEdit rue // Accept edits to start debugging while > 0 do 1 // One round of debugging if acceptEdit = rue then BuildUT(d, ˆfe) // Obtain generated UTs (xd, yd) // Initialize UT for debugging feedback for (x, y) do if ˆfe(x) = then (xd, yd) (x, y) // Failing UT to debug prePass EvalCode( ˆfe, U) // Get pass rate if xd = then // No need to debug return ˆfe else // Prompt LLM to debug code LLM( ˆfeFeedback(xd, yd, ˆfe)) // Prompt LLM to debug code with UT feedback postPass EvalCode(f , U) if postPass > prePass then ˆfe // Based on validation on the generated UTs, accept the edits, otherwise discard, i.e., backtrack return ˆfe MBPP+Fix, UTGEN consistently yields the highest Acc. Attack score, followed by the random baseline. E. Prompts In the following pages, we list all the prompts we use. Learning to Generate Unit Tests for Automated Debugging Prompted and UTGen Prompt for UT generation You are given Python function {signature} to solve the following task: ## Task: {description} ## Code Solution: {code} The code solution have provided to you is **incorrect**. Your job is to give feedback by generating unit test that 1. Is **valid** input based on the task description, i.e., an acceptable input consistent with task description that correct program should be able to execute. 2. The output enclosed in . and is **faithful** to the task description, i.e., the output of the unit test is consistent with what correct program would return. 3. **Breaks** the given code, i.e., does **not** execute to the **correct** output and brings out its mistakes and vulnerabilities. Provide reasoning for your answer and identify general hypothesis or rationale identifying the cause of error. Then provide input and output of the unit test consistent with the pattern (hypotheis) you have identified. Note: - that you MUST directly write ALL input arguments of the function in the correct order. Skip writing any names of arguments. - you MUST enclose the unit test inputs and outputs in. Respond in the format below: ## Hypothesis <step-by-step reasoning > Error Pattern: <an identified pattern of inputs that yields erroneous or incorrect outputs > ## Unit Test ### Input Arguments <step-by-step reasoning for constructing unit test that fits the error pattern identified above and is valid as per the task description >Arguments: {entry point}(<all arguments >) ### Output <step-by-step reasoning for what **correct** {entry point} would execute to based on the task description and your input above. Make sure your data type of the final answer matches the expected output type of the function. > Output: <your final answer > No UT Feedback Prompt # Your Task ## Task {prompt} ## Code: {code} Based on given task and code, generate feedback that decides whether the code is correct or wrong in the format Feedback: <your feedback >. Always end your feedback with the line The above code is correct. if it is correct, otherwise the feedback should end with The above code is wrong, please fix it. Learning to Generate Unit Tests for Automated Debugging UT Generation Prompt for Randomly-sampled UTs Given Python function {signature} to solve the following task: {description} Code Solution: {code} The code solution have provided to you is incorrect. Your job is to give feedback by generating unit test input that 1. Valid input based on the task description, i.e., an acceptable input consistent with task description that correct program should be able to execute. 2. Given code will NOT be able to solve and brings out its mistakes and vulnerabilities. Provide reasoning for your answer and present your response in the format below: <reasoning > Arguments: {entry point}(<all arguments >) Note that you MUST directly write ALL input arguments of the function in the correct order. Skip writing any names of arguments. UT Feedback Prompt The above code is incorrect and does not pass the testcase. Input: {wrong testcase input} Output: {wrong testcase output} Expected: {wrong testcase expected} Training UT Gen Code Corruption Prompt You are given Python function {signature} to solve the following task: ## Task {description} ## Correct Code Solution: {code} Assume you are TA for programming course. Your task is to corrupt the correct code or implementation of function {entry point} to introduce realistic errors that can be made by your programming students. Note that you should write code that fails one or more unit tests that this correct would succeed in. Also, give all your reasoning for why your generated code is incorrect outside the code block, i.e., **do not** leave comments in the code block that reveals the code is incorrect. Give your output in the format: <reasoning of error introduced > ## Incorrect Code Solution <your generated incorrect code for {entry point} > 16 Learning to Generate Unit Tests for Automated Debugging Output Rationalization Prompt ## Example Given Python function check(string) to solve the following task: Write python function to accept the strings which contains all vowels. user provides an the following input to function. The teacher lets you know that correct function generates the following output. Input: BCDEFG Output: False Now **without** coding up the the function check(string), provide step-by-step reasoning for why function check when given input of BCDFG generates or returns False. ### Reasoning Lets think step by step. - According to the problem description, given string the check should return accepted if it contains all the vowels and not accepted otherwise. - The vowels (in lower case) that should be present in the string are: a, e, i, and u. - The given input is BCDEFG which contains characters: b, c, d, e, f, g. - While the input string BCDEFG contains only vowel and is missing vowels: a, i, o, u. - Therefore, the output of the function is not accepted. ## Test Problem Given Python function {signature} to solve the following task: {description} user provides an the following input to function {signature}. The teacher lets you know that correct function generates the following output. Input: {unit input} Output: {unit output} Now **without** coding up the the function {signature}, provide step-by-step reasoning for why function {entry point} when given input of {unit input} generates or returns {unit output}. Write your output under header ### Reasoning."
        }
    ],
    "affiliations": [
        "Department of Computer Science, UNC Chapel Hill"
    ]
}