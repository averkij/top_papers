{
    "paper_title": "Intuitive physics understanding emerges from self-supervised pretraining on natural videos",
    "authors": [
        "Quentin Garrido",
        "Nicolas Ballas",
        "Mahmoud Assran",
        "Adrien Bardes",
        "Laurent Najman",
        "Michael Rabbat",
        "Emmanuel Dupoux",
        "Yann LeCun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 1 3 8 1 1 . 2 0 5 2 : r Intuitive physics understanding emerges from self-supervised pretraining on natural videos Quentin Garrido1,2, Nicolas Ballas1, Mahmoud Assran1, Adrien Bardes1, Laurent Najman1, Michael Rabbat1, Emmanuel Dupoux1,3, Yann LeCun1 1FAIR at Meta, 2Univ Gustave Eiffel, 3EHESS We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge set of innate systems to help understand the world needs to be hardwired to develop an understanding of intuitive physics. Date: February 18, 2025 Correspondence: Quentin Garrido at garridoq@meta.com Code and data: https://github.com/facebookresearch/jepa-intuitive-physics An intuitive understanding of physics is fundamental to human cognition: we expect objects to behave predictably, i.e., not to appear or disappear abruptly, move through obstacles, or change shape or color arbitrarily. This basic grasp of the physical world has been documented not only in human infants (Piaget, 1954; Baillargeon and DeVos, 1991; Baillargeon et al., 1992; Baillargeon and Hanko-Summers, 1990; Spelke et al., 1995), but also in primates (Cacchione and Krist, 2004; Mendes et al., 2007), marine mammals (Singer and Henderson, 2015; Herman, 2010), corvids (Bird and Emery, 2009; Taylor et al., 2012), and chicks (Vallortigara, 2012; Wood, 2013). This has been taken as evidence for the core knowledge (or core systems) hypothesis, according to which humans are equipped with set of innate or early developing evolutionary, ancient computational systems specialized to represent and reason about basic properties of the world: objects, space, numbers, geometry, agents, etc. (Baillargeon, 2008; Spelke and Kinzler, 2007; Spelke, 2000; Carey, 2000). In the pursuit of building machines with advanced human-level intelligence, rapid progress has produced artificial intelligence (AI) systems that often surpass human performance on high-level cognitive tasks like language, coding or mathematics (OpenAI, 2024), but paradoxically struggle in common sense physical understanding (Riochet et al., 2022; Weihs et al., 2022; Jassim et al., 2024; Bisk et al., 2020; Benchekroun et al., 2023; Bansal et al., 2024; Bear et al., 2021), illustrating Moravecs paradox (Moravec, 1988), namely, that tasks trivial for biological organisms can be remarkably difficult for artificial systems, and vice versa. Previous work developing AI models with the aim of improving intuitive physics understanding can be sorted into two classes: structured models and pixel-based generative models. Structured models leverage hand-coded abstract representations of objects and their relationships in an Euclidean 3D space (Battaglia et al., 2013; Watters et al., 2017), yielding powerful mental game engine\" able to capture humans physical intuitions (Ullman et al., 2017). This class of models can be seen as possible computational implementation of the core knowledge hypothesis (Spelke and Kinzler, 2007; Spelke, 2000).1 Pixel-based generative models take radically opposite view and deny the need for any hard-coded abstraction. Instead, they propose general purpose learning mechanism consisting of reconstructing future sensory inputs (e.g., images) based on past ones (Lerer et al., 2016; Goyal et al., 2017a; Finn et al., 2016). 1Weaker versions of structured models use object masks and depth cues instead of full 3D reconstruction, e.g., (Riochet et al., 2020). 1 Figure 1 Video prediction in representation space (V-JEPA) achieves an understanding of intuitive physics. (A) Video models are evaluated on three intuitive physics datasets using the Violation of Expectation paradigm (IntPhys, GRASP, and InfLevel). V-JEPA is significantly more surprised by implausible videos. Random initializations of V-JEPA (untrained networks) show near-chance performance, and state-of-the-art video models based on text or pixel prediction are much closer to chance. Confidence intervals at 95% are obtained via bootstrapping, except for untrained networks (n = 20) which use normal distribution assumption. (B) V-JEPA is trained to inpaint natural videos in learned representation space. Starting from video and corrupted version, representations are first extracted. The goal is then to predict the representation of the original video from the representation of the corrupted ones. (C) From trained V-JEPA, we compute surprise metric by predicting representations of future frames based on past ones and comparing the predictions to the representations of observed events. The surprise metric is then used to decide which of the two videos contains physical violation. Here, we explore third class of models that occupies middle ground between these opposing views, integrating features from both: Joint Embedding Predictive Architectures (JEPAs) (LeCun, 2022; Bardes et al., 2024). As structured models, JEPAs posit that prediction of future world states should be done in the models learned abstract, internal representation, and not in terms of low-level, pixel-based prediction or generation.. However, unlike structured models, JEPAs leave it to the algorithm to learn its own representation rather than hand-coding it. The mechanism consisting of predicting in representation space is congruent with the predictive coding hypothesis of cognitive neuroscience (Hohwy, 2013; Rao and Ballard, 1999; Clark, 2013). Here we study video version of this architecture, V-JEPA (Bardes et al., 2024), which learns to represent video frames by reconstructing masked portions of the video in representation space. We rely on the violation-of-expectation framework to probe for intuitive physics understanding without requiring any task-specific training or adaptation (Smith et al., 2019; Riochet et al., 2022; Piloto et al., 2022; Riochet et al., 2020). By prompting the model to imagine the (representation of the) future of video and comparing its predictions with the actual observed future of the video, we obtain quantitative measure of surprise that can be used to detect violations of intuitive physics concepts. We find that V-JEPA accurately and consistently distinguishes between videos that follow the laws of physics and those that violate them. Specifically, when tasked with classifying the physical plausibility of video pairs, where one video is plausible and the other is not, V-JEPA model trained on natural videos achieves 98% zero-shot accuracy on the IntPhys benchmark (Riochet et al., 2022) and 62% zero-shot accuracy on the InfLevel benchmark (Weihs et al., 2022).2 Surprisingly, we find that multimodal large-language models (Wang et al., 2024; Reid et al., 2024) and comparable video prediction methods making predictions in pixel-space (Wang 2Here zero-shot refers both to the fact that the V-JEPA models were not trained specifically for the task of distinguishing et al., 2023) perform around chance. To better understand which design choices lead to the emergence of intuitive physics understanding in V-JEPA, we ablate the effect of the training data, the pretraining prediction objective (what to predict from what), and the model size. While we observe that varying each of these components influences performance, all V-JEPA models achieve performance significantly above chance, including small 115 million parameter model, or model trained on only one week of unique video, thereby suggesting that video prediction in learned representation space is robust objective for acquiring intuitive physics understanding."
        },
        {
            "title": "Measuring intutive physics understanding",
            "content": "Violation of Expectation. The violation-of-expectation paradigm has its roots in developmental psychology (Margoni et al., 2024; Baillargeon et al., 1985). Subjects, typically infants, are presented with two similar visual scenes, one of which contains physical impossibility. surprise reaction to each scene is then obtained through various physiological measures, such as relative gaze time (Spelke, 1985), and is used to determine whether concept violation has occurred in the subject (Baillargeon and DeVos, 1991; Spelke, 1985; Margoni et al., 2024).3 This paradigm has been extended to evaluate the physical understanding of AI systems (Riochet et al., 2022; Smith et al., 2019; Riochet et al., 2020), where, similarly to infant trials, pairs of scenes are presented to model with all aspects (properties of objects, number of objects, occluders, etc.) kept identical across the two scenes, apart from single aspect or event that violates specific intuitive physics concept. For example, ball may roll behind an occluder but never reappear in one of the paired videos, thereby testing for the concept of object permanence. higher surprise response attributed by the model to the impossible scenario reflects correct understanding of the violated concept. Video Prediction for intuitive physics understanding. The V-JEPA architecture (LeCun, 2022) has been primarily developed to improve the capacity of model to adapt to high-level downstream tasks, such as activity recognition (Kay et al., 2017) and action classification (Goyal et al., 2017b), directly from the input without hard-wiring cascade of intermediate representations like object contours or pose estimation (Bardes et al., 2024). Here, we test the hypothesis that the reason this architecture is successful at high-level tasks is that it has learned representation that implicitly captures the structure and dynamics of objects in the world without the need to represent them directly. As illustrated in Figure 1.B, V-JEPA is instantiated with an encoder (a neural network) that extracts representations from video, and predictor (also neural network) that predicts the representation of an artificially masked part of the video, such as randomly masked spatiotemporal block, random pixels, or future frames. This joint training of the encoder and predictor enables the encoder to learn abstract representations that encode predictable information and discard low-level (typically less semantic) features. Refer to Section A.1 in the supplementary material for more details on architecture and training. After self-supervised training, we can use the encoder and predictor networks, without any additional adaptation, to probe the models understanding of the world. Specifically, iterating through stream of video, the model encodes the observed pixels and subsequently predicts the representation of the following frames in the video, as illustrated in Figure 1.C. By recording the prediction error the distance between the predicted video representations and the actual encoded video representations at each time-step, we obtain temporally aligned quantitative measure of the models surprise throughout the video. Varying how many past video frames (context) model can use to predict the future allows us to control for memory, while varying the frame rate of the video allows us to control for the fineness of motions. Refer to Section A.7 in the supplementary material for more details. between physically plausible and implausible videos, and that the model was not trained on data from any of the benchmarks. 3Non-conceptual interpretations of gaze-times, e.g., based on low-level processes such as perceptual preferences, are typically mitigated to some degree in these experiments by conducting series of habituation trials prior to the violation-of-expectation trials."
        },
        {
            "title": "Representation prediction learns to detect violations of intuitive physics",
            "content": "We evaluate intuitive physics understanding on three datasets: the dev set of IntPhys (Riochet et al., 2022), GRASP (Jassim et al., 2024) and InfLevel-lab (Weihs et al., 2022). This mix of benchmarks provides diversity in the visual quality (synthetic/photorealistic), in the diversity of scenes considered, as well as in the intuitive physics properties that are probed. Specifically, the combination of these datasets allows us to probe the understanding of object permanence (Baillargeon and DeVos, 1991), continuity (Spelke et al., 1992), shape and color constancy (Wilcox, 1999; Wilcox and Chapa, 2004), gravity (Kim and Spelke, 1992), support (Baillargeon and Hanko-Summers, 1990; Baillargeon et al., 1992), solidity (Spelke et al., 1992), inertia (Spelke et al., 1992), and collision (Baillargeon, 1995). See Section A.5 in the supplementary material for exact definitions. We compare V-JEPA to other video models to investigate how important to intuitive physics understanding is the video prediction objective, as well as the representation space where prediction is performed. We consider two other classes of models: video prediction models that predict directly in pixel space, and Multimodal Large Language Models (MLLMs). The former set of pre-training methods have similar prediction objective as V-JEPA, but often learn representation spaces with poor semanticity (Wang et al., 2023; Bardes et al., 2024); they are useful once fine-tuned for specific task. As representative method, we evaluate VideoMAEv2 (Wang et al., 2023). While different prediction objectives and pretraining data are used, this allows comparison to V-JEPA in terms of prediction space. Given its predictive nature, VideoMAEv2 can be evaluated in the same way as V-JEPA, by predicting the future and measuring surprise via prediction error. The latter class of models, MLLMs, are trained to predict text and are only interleaved with video posteriori, making them devoid of video prediction objective. As exemplar methods, we study Qwen2-VL-7B (Wang et al., 2024), state-of-the-art, open-weights, video-language model, and Gemini 1.5 pro (Reid et al., 2024), closed commercial model. These models are both significantly larger than V-JEPA in terms of parameter count and the amount of data they were trained on, and they learn primarily from text data. Multimodal LLMs take videos and potentially text prompt as input and learn to generate corresponding textual output. Due to their text-only output, those models cannot use the same evaluation protocol based on quantitative measure of surprise. Instead, we give the model pair of videos, asking which one of the two is impossible. Section A.7 in the supplementary material describes the detailed protocol. For every method considered, we evaluate the flagship models proposed in the original works. We further compare all models with untrained neural networks, testing the learnability of intuitive physics understanding. For each property and model, the context size is chosen as the one maximizing performance, allowing the models to adapt to the different evaluation setups. This process is done for all methods, and leads to results illustrating the best performance achievable by the model. We expand on this choice in section in the supplementary material. We summarize the performance of methods across datasets on pairwise classification (i.e., detecting the impossible video in pair) in Figure 1.A. Refer to Section in the supplementary material for detailed results, and Section A.8 for detailed parameters used. We find that V-JEPA is the only method that achieves significantly higher performance than untrained networks across all datasets, achieving average accuracies of 98% (95% CI [95%,99%]), 66% (95% CI [64%,68%]) , 62% (95% CI [60%,63%]) respectively on IntPhys, GRASP, and InfLevel-lab. These results show that prediction in learned representation space is sufficient to develop an understanding of intuitive physics. This is done without any predefined abstractions, and without knowledge of the benchmarks during pretraining or development of the method. By comparison, we find that VideoMAEv2, Qwen2-VL-7B, and Gemini 1.5 pro achieve performance that is only marginally above that of randomly-initialized models. The low performance of pixel prediction and multimodal LLMs corroborates previous findings (Riochet et al., 2022; Jassim et al., 2024). These comparisons further highlight the benefit of V-JEPA over the existing VideoMAEv2, Gemini 1.5 pro, and Qwen2-VL-72B models. These results, however, do not mean that LLMs or pixel prediction models cannot achieve intuitive physics understanding, but merely that this seemingly simple task remains difficult even for frontier models (Jassim et al., 2024; Kang et al., 2024; Bansal et al., 2024). 4 Figure 2 V-JEPA accuracy increase relative to randomly-initialized models and humans across different physical properties and benchmarks. (A) Because some benchmarks contain low-level biases, we test the model performance against set of randomly initialized networks (n = 20). V-JEPA models (n = 5) have higher relative classification accuracy on intuitive physics benchmarks for most, but not all concepts. (B) V-JEPA relative (left) and absolute (right) accuracy on the IntPhys test set across different conditions compared to naive human performance, showing high correlation between human and machine errors. The V-JEPA score uses the maximum surprise from each video, which generalizes better for single-video classification. Human data are taken from (Riochet et al., 2022). Per property analysis of V-JEPA We now take closer look at the per-property performance of V-JEPA on the previously used datasets in order to obtain more precise understanding of its intuitive physics understanding. Here, the V-JEPA encoder and predictor are based on the Vision Transformer-Large (ViT-L, instead of ViT-H for the flagship model) (Dosovitskiy et al., 2021; Bardes et al., 2024) architecture and are trained on the HowTo100M dataset (Miech et al., 2019). We perform two-sample one-tailed Welchs t-test to assess whether V-JEPA (n=5) provides increased performance over randomly-initialized, untrained models (n=20). The results are summarized in Figure 2. On IntPhys, we find V-JEPA to significantly outperform untrained networks on multiple intuitive physics properties: Object Permanence: M=85.7, SD=7.6 vs. M=51.4, SD=1.0 (t(4.0) = -8.9, = 4.19 104), with an effect size = 9.0 (95% CI [6.3,11.7]); Continuity: M=86.3, SD=6.2 vs. M=51.2, SD=1.2 (t(4.1) = -11.3, = 1.61 104), with an effect size = 11.0 (95% CI [7.8,14.2]); Shape Constancy: M=83.7, SD=7.8 vs. M=51.7, SD=1.2 (t(4.0) = -8.1, = 5.96 104), with an effect size = 8.1 (95% CI [5.7,10.6]). On GRASP, we find significantly higher accuracies for V-JEPA on: Object Permanence: M=70.7, SD=7.8 vs. M=54.1, SD=5.9 (t(5.0) = -4.0, = 5.10 103), with an effect size = 2.4 (95% CI [1.2,3.6]); Continuity: M=65.0, SD=6.1 vs. M=55.0, SD=5.0 (t(5.2) = -3.0, = 1.36 102), with an effect size = 1.8 (95% CI [0.7,2.9]); Support: M=98.1, SD=3.0 vs. M=58.4, SD=10.5 (t(21.4) = -14.0, = 1.48 1012), with an effect size = 3.9 (95% CI [2.4,5.3]); Gravity: M=74.9, SD=2.4 vs. M=55.3, SD=4.3 (t(10.3) = -12.6, = 6.83 108), with an effect size = 4.5 (95% CI [2.9,6.1]); Inertia: M=62.0, SD=2.4 vs. M=54.3, SD=4.2 (t(10.1) = -5.1, = 2.36 104), with an effect size = 1.8 (95% CI [0.7,2.9]). However, we do not find significant gain on: Color Constancy, Solidity, or Collision (p > 0.05). On InfLevel, we find significantly higher accuracies for V-JEPA on: Object Permanence: M=72.1, SD=2.9 vs. M=52.5, SD=3.5 (t(6.8) = -11.9, = 4.46 106), with an effect size = 5.4 (95% CI [3.6,7.1]). However, we do not find significant gain on: 5 Figure 3 Influence of type of mask, type and amount of training data, and model size on V-JEPA IntPhys scores. (A) When pretrained on VM2M, V-JEPA exhibits an understanding of intuitive physics with every masking strategy. (B) Of the three training datasets, two give high accuracies when trained separately (K710 and Howto100M). High scores are found with only 1289 hours of Howto100M (the largest dataset), and even 128h gives better than chance performance. (C) While larger encoders improve performance, we find that the performance remains non-trivial across sizes when pretraining on HowTo100M. Confidence intervals obtained via bootstrapping. Gravity or Solidity (p > 0.05). V-JEPA excels at properties related to the scenes content (e.g., object permanence), but struggles with categories that require knowledge of contextualizing event (gravity and solidity in InfLevel-lab) or the modeling of precise object interactions such as collisions. We hypothesize that these limitations come mainly from the models framerate constraints. Nevertheless, V-JEPA demonstrates an understanding of intuitive physics while learning the required abstractions from the raw perceptual signal and without relying on strong prior information. In contrast to previous work (Smith et al., 2019; Riochet et al., 2022), this suggests that core knowledge is not necessary for deep learning systems to understand intuitive physics concepts. We further compare V-JEPA to human performance using the private test set from IntPhys (Riochet et al., 2022). The human data is taken from (Riochet et al., 2022, 2020), where it was obtained through Amazon Mechanical Turk. For this experiment, we focus on the flagship V-JEPA architecture, using ViT-Huge (Dosovitskiy et al., 2021; Bardes et al., 2024) with pretraining on VideoMix2M (Bardes et al., 2024). We find that V-JEPA achieves equal or higher performance for all intuitive physics properties, as illustrated in Figure 2.B. We find that using the maximum surprise in video, rather than the average, leads to better performance on single videos. We discuss further this distinction in Section A.7 in the supplementary material. In general, we observe lower performance in both V-JEPA and humans for videos where the physics-breaking event happens behind an occluder. Additionally, performance is well-correlated between humans and V-JEPA for the occluded settings."
        },
        {
            "title": "Keys to intuitive physics understanding",
            "content": "We now ablate V-JEPA design choices to better understand the conditions for intuitive physics understanding to emerge. We focus on three components that play crucial role in the models capabilities. First, we examine the impact of the training data. The choice of data defines the learning environment of the model, with different video sources providing variations in semantic diversity, movement patterns, and quantity. Second, we consider the effect of the model size. While conventional wisdom states that larger models perform better, we also ponder the minimum size required to achieve non-trivial performance. Third, we study the influence 6 of the pretraining prediction task. Does selecting what to predict from what observed context (pretraining masking strategy) affect the models understanding of intuitive physics? Importance of the pretraining task. Recall that V-JEPA models are trained to predict representations of randomly masked portions of video, but always perform causal prediction at inference time, where the context includes frames up to some time and the model should predict representations of frames at times greater than t. Although we compute V-JEPAs surprise using causal prediction and have observed above that this is effective for intuitive physics understanding, V-JEPA is never trained using causal prediction task. Rather, the pre-training task is referred to as Block Masking (Bardes et al., 2024), where large spatial block is masked for the full duration of the video. V-JEPAs performance on action and activity recognition tasks has previously been observed to vary drastically depending on the exact strategy used (Bardes et al., 2024). To understand the extent to which V-JEPA intuitive physics understanding emerges specifically from the Block Masking training task, we study the effect of changing this training task, and consider two possible alternatives. Causal Block Masking is similar to Block Masking, but also fully masks the last 25% of the video, thereby incorporating future prediction into the training procedure, and Random Masking which masks random pixels in the video. Contrary to classical video tasks (Bardes et al., 2024), we find that the prediction task is not as important for intuitive physics understanding (see figure 3.B). Whereas Random Masking leads to drop of 20 points on average on video classification tasks (Bardes et al., 2024), the drop on IntPhys is only around 5 points on average. Interestingly, Causal Block Masking seems to perform worse than its non-causal counterpart, despite being more closely aligned to the models prediction setup at test time. The effective performance of Random Masking, perhaps the simplest strategy, suggests that the understanding of intuitive physics does not require tailored objective, but that predicting in an abstract representation space is the key aspect. Importance of pre-training data. Data is key ingredient of deep learning models and video models are no exception (Bardes et al., 2024). Video datasets can be described along several axes, such as the number of distinct videos, the (average) duration of videos, whether videos are captured from egocentric or exocentric views, whether that camera is static or moving, and so on. We thus investigate in more detail the influence of pretraining data on intuitive physics performance. V-JEPA has previously been trained on mixture of three popular video datasets, referred to as VideoMix2M (Bardes et al., 2024): Kinetics 710 (K710 (Kay et al., 2017)), Something-Something-v2 (SSv2 (Goyal et al., 2017b)) and HowTo100M (HowTo (Miech et al., 2019)). Each of these datasets focuses on different slice of the distribution of natural videos, namely, activities in K710 (e.g., playing basketball), fine-grained motion in SSv2 (e.g., throwing something), and tutorials in HowTo100M (e.g., cooking). To study the influence of training data on learning intuitive physics, we re-train V-JEPA-L models separately using only one of the three component datasets. Unsurprisingly, we find strong impact of data sources on performance. Training only with videos based on motion understanding (SSv2) leads to almost chance-level performance. While more action-focused data (K710) leads to an above-chance understanding of intuitive physics, we find that tutorial videos (HowTo) yield the best performance among individual component datasets. However, HowTo is also larger than SSv2 and K710 (15 years vs. 3 months combined). We thus further examine the evolution of performance with smaller datasets coming from the same distribution by subsampling HowTo100M. We hold the compute budget fixed across these experiments such that model training always processes the equivalent of 30 years of video (by revisiting videos from the training dataset multiple times) even when only using 0.1% of HowTo100M, which represents only 128 hours of unique video in total. We find in Figure 3.C that the size of the dataset does not meaningfully impact performance, and that the model can adequately distinguish violations of intuitive physics concepts even with 128h of unique videos, maintaining pairwise accuracy of over 70% on all considered properties. Importance of the encoder size. Common wisdom in the deep learning literature is that larger models perform better (Kaplan et al., 2020). Here, we are also interested in the minimal size at which we observe evidence of non-trivial intuitive physics understanding. We thus investigate what happens in both directions of the scaling, using smaller and larger encoders. In Figure 3.C, we find that larger models tend to perform better. However, 115 million parameters model still achieves an accuracy of over 85%, demonstrating robust understanding of intuitive physics."
        },
        {
            "title": "Discussion",
            "content": "In this work, we studied the emergence of intuitive physics understanding in state-of-the-art deep learning models. By pretraining on natural videos with simple prediction task in learned representation space, V-JEPA exhibits an understanding of intuitive physics on both synthetic and real videos without any taskspecific adaptation. Our results show that intuitive physics understanding can be acquired using general learning principle, and thus does not require hardwired core knowledge. Although we find that the size of the model, the choice of pretraining data, and the exact pretraining task influence this understanding, its emergence can be attributed to the general framework of representation space prediction rather than precise design choice of V-JEPA. When studying other methods such as multimodal LLMs and pixel prediction methods, we find that current models perform around chance level. Higher-capacity generative video models could potentially benefit from certain understanding of intuitive physics (Brooks et al., 2024) in order to produce realistic videos. Yet, current evidence points to an incomplete understanding of physics in existing video generative models (Motamed et al., 2025; Bansal et al., 2024)4. Nonetheless, the demonstrated understanding of V-JEPA is not without limitations. Indeed, V-JEPA is not uniformly accurate under all conditions. Figure 2 shows that although the accuracies are high for physical violations that imply properties intrinsic to objects (except for the color property), violations implicating interactions between objects, like solidity or collision, are close to chance. This may be due to the fact that object interactions are not very frequent in the model training data, and are not learned as well as more frequent ones. Furthermore, current JEPA models have limited memory, and consequently process very short video clips at time (typically 34 seconds). V-JEPA also lacks the ability to condition its predictions on additional context, such as an action taking place, and thus predicts the future only as an observer. Although this lends itself well to the tested properties, more complex interactions are out of reach at the moment. Indeed, it could be that interactions between objects require higher-order representations, and that more powerful hierarchical version of JEPA is needed to capture these interactions. Finally, it is also possible that an agent has to be able to interact with objects themselves in order to learn about interactions, suggesting the need to add action channels to the learning system. From data standpoint, it would also be interesting to study models trained on videos that mimic what infants see (Sullivan et al., 2021; Long et al., 2024), and whether an understanding of intuitive physics also emerges in models trained on such data. Nonetheless, through the results reported here, we believe that the latent prediction framework is path forward toward building neural networks that understand the physical world. 4Most state-of-the-art models (Brooks et al., 2024) being proprietary complicates rigorous assessment of their physics understanding due to their lack of openness."
        },
        {
            "title": "References",
            "content": "Renee Baillargeon. Physical reasoning in infancy. The cognitive neurosciences, pages 181204, 1995. Renee Baillargeon and Julie DeVos. Object Permanence in Young Infants: Further Evidence. Child Development, 62 (6):1227, December 1991. ISSN 00093920. doi: 10.2307/1130803. Renee Baillargeon, Elizabeth Spelke, and Stanley Wasserman. Object permanence in five-month-old infants. Cognition, 20(3):191208, 1985. Renée Baillargeon. Innate Ideas Revisited: For Principle of Persistence in Infants Physical Reasoning. Perspectives on Psychological Science, 3(1):213, January 2008. ISSN 1745-6916, 1745-6924. doi: 10.1111/j.1745-6916.2008.00056.x. Renée Baillargeon and Stephanie Hanko-Summers. Is the top object adequately supported by the bottom object? young infants understanding of support relations. Cognitive Development, 5(1):2953, January 1990. ISSN 08852014. doi: 10.1016/0885-2014(90)90011-H. Renée Baillargeon, Amy Needham, and Julie Devos. The development of young infants intuitions about support. Early Development and Parenting, 1(2):6978, January 1992. ISSN 1057-3593, 1099-0917. doi: 10.1002/edp.2430010203. Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv:2406.03520, 2024. Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. Featured Certification. Peter W. Battaglia, Jessica B. Hamrick, and Joshua B. Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):1832718332, November 2013. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.1306572110. Daniel Bear, Elias Wang, Damian Mrowca, Felix Binder, Hsiao-Yu Fish Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. arXiv:2106.08261, 2021. Youssef Benchekroun, Megi Dervishi, Mark Ibrahim, Jean-Baptiste Gaya, Xavier Martinet, Grégoire Mialon, Thomas Scialom, Emmanuel Dupoux, Dieuwke Hupkes, and Pascal Vincent. Worldsense: synthetic benchmark for grounded reasoning in large language models. arXiv:2311.15930, 2023. Christopher David Bird and Nathan John Emery. Rooks use stones to raise the water level to reach floating worm. Current Biology, 19(16):14101414, 2009. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. Proceedings of the AAAI conference on artificial intelligence, 34:74327439, 2020. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators, 2024. Trix Cacchione and Horst Krist. Recognizing impossible object relations: intuitions about support in chimpanzees (pan troglodytes). Journal of Comparative Psychology, 118(2):140, 2004. Susan Carey. The origin of concepts. Journal of Cognition and Development, 1(1):3741, 2000. Andy Clark. Whatever next? predictive brains, situated agents, and the future of cognitive science. Behavioral and brain sciences, 36(3):181204, 2013. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. International Conference on Learning Representations, 2021. Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. Advances in neural information processing systems, 29, 2016. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video 9 database for learning and evaluating visual common sense. Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017a. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017b. Louis Herman. What laboratory research has told us about dolphin cognition. International Journal of Comparative Psychology, 23(3), 2010. Jakob Hohwy. The predictive mind. OUP Oxford, 2013. Serwan Jassim, Mario Holubar, Annika Richter, Cornelius Wolff, Xenia Ohmer, and Elia Bruni. Grasp: novel benchmark for evaluating language grounding and situated physics understanding in multimodal language models. In Kate Larson, editor, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24, pages 62976305, 8 2024. doi: 10.24963/ijcai.2024/696. Main Track. Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv:2411.02385, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv:2001.08361, 2020. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv:1705.06950, 2017. In Kyeong Kim and Elizabeth Spelke. Infants sensitivity to effects of gravity on visible object motion. Journal of Experimental Psychology: Human Perception and Performance, 18(2):385, 1992. Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62(1):162, 2022. Adam Lerer, Sam Gross, and Rob Fergus. Learning physical intuition of block towers by example. International conference on machine learning, pages 430438, 2016. Bria Long, Violet Xiang, Stefan Stojanov, Robert Z. Sparks, Zi Yin, Grace E. Keene, Alvin W. M. Tan, Steven Y. Feng, Chengxu Zhuang, Virginia A. Marchman, Daniel L. K. Yamins, and Michael C. Frank. The BabyView dataset: High-resolution egocentric videos of infants and young childrens everyday experiences. arXiv:2406.10447, June 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. International Conference on Learning Representations, 2019. Francesco Margoni, Luca Surian, and Renée Baillargeon. The violation-of-expectation paradigm: conceptual overview. Psychological Review, 131(3):716748, April 2024. ISSN 1939-1471, 0033-295X. doi: 10.1037/rev0000450. Natacha Mendes, Daniel Hanus, and Josep Call. Raising the level: orangutans use water as tool. Biology letters, 3 (5):453455, 2007. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. Hans Moravec. Mind children: The future of robot and human intelligence. Harvard UP, 1988. Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video models learn physical principles from watching videos? arXiv:2501.09038, 2025. OpenAI. Gpt-4 technical report, 2024. Jean Piaget. The Construction of Reality in the Child. Basic Books, 1954. Luis S. Piloto, Ari Weinstein, Peter Battaglia, and Matthew Botvinick. Intuitive physics learning in deep-learning model inspired by developmental psychology. Nature Human Behaviour, 6(9):12571267, July 2022. ISSN 2397-3374. doi: 10.1038/s41562-022-01394-8. Rajesh PN Rao and Dana Ballard. Predictive coding in the visual cortex: functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):7987, 1999. 10 Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530, 2024. Ronan Riochet, Josef Sivic, Ivan Laptev, and Emmanuel Dupoux. Occlusion resistant learning of intuitive physics from videos. arXiv:2005.00069, 2020. Ronan Riochet, Mario Ynocente Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Véronique Izard, and Emmanuel Dupoux. IntPhys 2019: Benchmark for Visual Intuitive Physics Understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):50165025, September 2022. ISSN 1939-3539. doi: 10.1109/TPAMI.2021. 3083839. Rebecca Singer and Elizabeth Henderson. Object permanence in marine mammals using the violation of expectation procedure. Behavioural Processes, 112:108113, 2015. Kevin Smith, Lingjie Mei, Shunyu Yao, Jiajun Wu, Elizabeth Spelke, Josh Tenenbaum, and Tomer Ullman. Modeling expectation violation in intuitive physics with coarse probabilistic object representations. Advances in neural information processing systems, 32, 2019. Elizabeth Spelke. Preferential-looking methods as tools for the study of cognition in infancy. In Gilbert Gottlieb and Norman A. Krasnegor, editors, Measurement of audition and vision in the first year of postnatal life: methodological overview, pages 323363. Ablex Publishing, 1985. Elizabeth S. Spelke. Core knowledge. American Psychologist, 55(11):12331243, 2000. 10.1037/0003-066X.55.11.1233. Place: US Publisher: American Psychological Association. ISSN 1935-990X. doi: Elizabeth S. Spelke and Katherine D. Kinzler. Core knowledge. Developmental Science, 10(1):8996, January 2007. ISSN 1363-755X, 1467-7687. doi: 10.1111/j.1467-7687.2007.00569.x. Elizabeth S. Spelke, Karen Breinlinger, Janet Macomber, and Kristen Jacobson. Origins of knowledge. Psychological Review, 99(4):605632, 1992. ISSN 1939-1471, 0033-295X. doi: 10.1037/0033-295X.99.4.605. Elizabeth S. Spelke, Roberta Kestenbaum, Daniel J. Simons, and Debra Wein. Spatiotemporal continuity, smoothness of motion and object identity in infancy. British Journal of Developmental Psychology, 13(2):113142, June 1995. ISSN 0261-510X, 2044-835X. doi: 10.1111/j.2044-835X.1995.tb00669.x. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. corr abs/2104.09864 (2021). arXiv:2104.09864, 2021. Jessica Sullivan, Michelle Mei, Andrew Perfors, Erica Wojcik, and Michael Frank. Saycam: large, longitudinal audiovisual dataset recorded from the infants perspective. Open mind, 5:2029, 2021. Alex Taylor, Rachael Miller, and Russell Gray. New caledonian crows reason about hidden causal agents. Proceedings of the National Academy of Sciences, 109(40):1638916391, 2012. Tomer Ullman, Elizabeth Spelke, Peter Battaglia, and Joshua Tenenbaum. Mind games: Game engines as an architecture for intuitive physics. Trends in cognitive sciences, 21(9):649665, 2017. Giorgio Vallortigara. Core knowledge of object, number, and geometry: comparative and neural approach. Cognitive neuropsychology, 29(1-2):213236, 2012. Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1454914560, June 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv:2409.1219, 2024. Nicholas Watters, Andrea Tacchetti, Theophane Weber, Razvan Pascanu, Peter Battaglia, and Daniel Zoran. Visual Interaction Networks. arXiv:1706.01433, June 2017. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXiv:2201.11903, 2023. Luca Weihs, Amanda Rose Yuile, Renée Baillargeon, Cynthia Fisher, Gary Marcus, Roozbeh Mottaghi, and Aniruddha Kembhavi. Benchmarking progress to infant-level physical reasoning in ai. TMLR, 2022. Teresa Wilcox. Object individuation: Infants use of shape, size, pattern, and color. Cognition, 72(2):125166, 1999. Teresa Wilcox and Catherine Chapa. Priming infants to attend to color and pattern information in an individuation task. Cognition, 90(3):265302, 2004. Justin Wood. Newborn chickens generate invariant object representations at the onset of visual object experience. Proceedings of the National Academy of Sciences, 110(34):1400014005, 2013."
        },
        {
            "title": "A Materials and Methods",
            "content": "A.1 Unsupervised pretraining of V-JEPA V-JEPA (Bardes et al., 2024) is composed of multiple components. First, context encoder fθ whose goal is to output abstract representations of corrupted video. target encoder fθEM is used to encode the full video and produce targets for the predictor. The weights of the target encoder θEM are an exponential moving average of the weights of the context encoder θ. For an exponential moving average parameter α [0, 1] and at iteration during training, we get the update rule of θEM . Finally, the predictor pϕ is used to predict the uncorrupted representations from the corrupted ones. During training, we start from video , which is corrupted into VC, by masking random blocks in the video. The target thus becomes the complementary VC. The training objective is thus to predict the representations of VC from VC by minimizing the following objective: t+1 = (1 α)θt + αθEM pϕ (fθ (VC)) fθEM ( VC)1. (S1) While at training time the corruption used is the removal of spatio-temporal blocks, we can see that if we instead use the first frames to predict the rest of the video, this objective turns into measure of error for the prediction of the future. A.2 Pretraining Data For the pretraining of V-JEPA we rely on multiple data sources. The original data mix that was used is VideoMix2M (Bardes et al., 2024), which is the concatenation of three datasets: Kinetics710 (Kay et al., 2017), SomethingSomething-v2 (Goyal et al., 2017b) and HowTo100M (Miech et al., 2019). Kinetics710 consists of around 650k videos spanning 710 action classes (e.g. kayaking, ironing, etc.), each lasting around 10 seconds. SomethingSomething-v2 is focused more on motions, where we find classes such as \"Uncovering something\" or \"Throwing something\". It consists of around 200k clips that last few seconds on average. HowTo100M is much larger dataset, containing around 1.2M videos lasting 6m30s on average, for total of around 15 years of unique video data. Here, the individuals are not curated as precisely as Kinetics or SomethingSomething, yielding more \"in the wild\" data source. As discussed in the main text, most of our experiments are done with only HowTo100m, which exhibits the highest performance and demonstrates how V-JEPA can leverage uncurated data sources. A.3 V-JEPA pretraining hyperparameters Throughout our experiments, we use the same set of hyperparameters across models, only varying the ablated component. We provide summary of the hyperparameters in Table S1. We rely on the same training protocol as in the original V-JEPA paper (Bardes et al., 2024), but use RoPE (Su et al., 2021) to encode positional information instead of absolute positional embeddings. To use RoPE on 3D data (heightwidthtime) we split the feature dimensions in 3 and use each third for spatiotemporal-dimension. Here, we expand on three core elements: architecture, optimization and masking. Architecture. We use the Vision Transformer (ViT) (Dosovitskiy et al., 2021) for the context encoder and target encoder. All encoders are trained to take in clip of maximum 3 seconds over 16 frames (5.33 fps), at resolution of 224 224. The video clips are flattened in sequence of non-overlapping patches of shape 16 16 2. The predictor also uses ViT inspired architecture, consisting of 12 blocks with smaller embedding dimension of 384. Optimization. We rely on the AdamW (Loshchilov and Hutter, 2019) optimizer to train the context encoder and predictor. For all experiments, we use batch size of 3072, where we train the models for 90000 iterations. This corresponds to total of around 26 years of (non-unique) video. The learning rate is increased linearly from 2 104 to 6.25 104 over the first 12000 iterations. The learning rate is then decayed to 1 106 13 Table S1 Pretraining hyper-parameters for V-JEPA. Table structure and values identical to the original V-JEPA paper (Bardes et al., 2024), apart from positional embedding where we rely on RoPE (Su et al., 2021). Hyper-parameter positional embeddings Type theta data resolution num_frames framerate horizontal_flip random_resize_scale random_resize_aspect_ratio masking block_aspect_ratio shortrange_mask_num_blocks shortrange_mask_spatial_scale longrange_mask_num_blocks longrange_mask_spatial_scale optimization optimizer batch_size total_number_of_iterations scheduler warmup_iterations learning_rate start_lr final_lr start_momentum final_momentum start_weight_decay final_weight_decay scheduler_scale_factor architecture patch_size tubelet_size pred_depth pred_embed_dim hardware dtype accelerator ViT-B/ ViT-L/16 ViT-H/16 RoPE 10000 224 16 5.33 fps true (0.3, 1.0) (0.75, 1.35) (0.75, 1.5) 8 0.15 2 0.7 RoPE 224 16 5.33 fps true (0.3, 1.0) (0.75, 1.35) (0.75, 1.5) 8 0.15 2 0.7 RoPE 10000 224 16 5.33 fps true (0.3, 1.0) (0.75, 1.35) (0.75,1.5) 8 0.15 2 0.7 AdamW 3072 AdamW 3072 90000 AdamW 3072 90000 Linear + Cosine Linear + Cosine Linear + Cosine 12000 6.25104 2104 1106 0.998 1.0 0.04 0.4 1.25 12000 6.25104 2104 1106 0.998 1.0 0.04 0.4 1.25 12000 6.25104 2104 1106 0.998 1.0 0.04 0.4 1.25 16 2 12 384 16 2 12 16 2 12 384 bfloat16 A100 80G bfloat16 A100 80G bfloat16 A100 80G 14 following cosine schedule during the remaining iterations. We stretch the schedule by factor of 1.25, meaning that the schedule lasts for 112500 iterations of which we only perform 90000. This avoids decay of performance at the end of training when the learning rate becomes too small, which would lead the context and target encoder to collapse partially. Masking. In our experiments, we rely on few masking strategies that we describe precisely here. Block masking. We mask the union of 8 blocks with spatial scale of 0.15, as well as 2 blocks with scale 0.7. For all blocks, we use an aspect ratio sampled uniformly between 0.75 and 1.5. This strategy is used unless specified otherwise. Causal Block masking. This strategy is identical to Block masking, where we additionally fully mask the last 4 frames of the video clips. Random masking. This strategy randomly masks 90% of all patches in the video clips, following uniform distribution. A.4 Evaluation Data Table S2 Summary of datasets used for evaluation. IntPhys, GRASP and InfLevel-lab provide qualitatively different data sources to perform more holistic evaluation of models. Dataset IntPhys GRASP InfLevel-lab Realistic Diverse scenes No No Yes Yes No No Size 360 4000 4000 Number of Properties 3 10 3 To provide more general assessment of the models considered, we focus on multiple data sources for evaluation, for which we summarize the main characteristics in Table S2. IntPhys (Riochet et al., 2022) is the most carefully curated data source, with pairs of videos being aligned at the pixel level thanks to the use of simulator and each frame being individually stored (avoiding compression artifacts). Due to its formulation as challenge with private test set, we rely on the smaller \"dev\" subset of the data, which has publicly available labels. Nevertheless, for each pair of videos, the number of objects, occluders, and texture/shape/color of objects are randomized, which ensures that the model performs well in diverse environments. GRASP (Jassim et al., 2024) is similar to IntPhys in the sense that it also uses simulated data, but covers wider range of properties (10 compared to 3) with more total videos. caveat to its use for our study is that it was originally designed to evaluate models on single video rather than pair. As such, even if the videos are paired in practice, we found some issues regarding the performance of untrained networks where they could latch on spurious features and achieve high accuracy. Some videos in GRASP can be attributed to multiple properties, so in practice, we consider that they belong to all properties when presenting the results (e.g., video belonging to gravity and support will be counted for both gravity and support separately). InfLevel-lab (Weihs et al., 2022) gives us source of natural videos where the manipulations are remarkably paired. The only visual differences between videos inside given pair are in the lighting of the scene, to which models should be robust. Here, three properties are tested; however, for two of them (solidity and gravity), the model needs to first see contextualizing event where the objects used during the manipulation are shown. Without seeing this pretext video, the task becomes impossible. Inflevel-lab thus requires more memory and adaptability than the tested models have. We further rename the continuity property as object permanence for consistency with other datasets. The difference between the two is subtle, but the experimental setup in InfLevel-lab is closer to the one of object permanence in IntPhys and GRASP. We emphasize that none of these datasets are seen during training and are only used for evaluation purposes where the networks are frozen, making all of these datasets out of distribution. A.5 Properties We provide brief descriptions of the intuitive physical properties considered in our work: 15 Object permanence (Baillargeon and DeVos, 1991). Objects do not spontaneously materialize or vanish out of thin air. Objects also keep existing when occluded. Continuity (Spelke et al., 1992). Objects follow continuous path and do not teleport in space or time. This concept is closely linked to object permanence, but leads to more subtle differences in experimental setups. Shape and color constancy (Wilcox, 1999; Wilcox and Chapa, 2004). Objects do not spontaneously change color or shape. Gravity (Kim and Spelke, 1992). Objects fall down without support under them. Support (Baillargeon and Hanko-Summers, 1990; Baillargeon et al., 1992). Objects are stable when positioned on platform, but become unstable/fall when unsupported. This is closely related to gravity, where the main difference lies in the precise experimental setup. For example, an object can be pushed off of platform to test support, and an object can just be dropped in the air to test gravity. Solidity (Spelke et al., 1992). Objects cannot overlap or pass through each other. When tested behind an occluder, this property shares similarities with continuity, where an object should also not teleport across another object. Inertia (Spelke et al., 1992). Inanimate objects do not spontaneously alter their motion, such as change in direction. Collision (Baillargeon, 1995). Objects do not stay still when hit by another similar moving object. For exact experimental setups, we refer the reader to the original datasets (Riochet et al., 2022; Jassim et al., 2024; Weihs et al., 2022), A.6 Baselines Pixel prediction. For our pixel prediction baseline, we use VideoMAEv2 (Wang et al., 2023) which is trained similarly to V-JEPA. However, instead of predicting missing parts of the video in latent space, this objective is solved in normalized pixel space. Each patch of 16x16x2 pixels is first normalized before being used as target. This makes VideoMAEv2 good comparison with V-JEPA due to the similarity in implementation details while being fundamentally different framework. Multimodal Large Language Models. for this baseline, we choose to use Qwen2-VL (Wang et al., 2024), one of the best open source Multimodal LLM that can handle video inputs at the time of writing, as well as Gemini 1.5 pro, proprietary Multimodal LLM which shines at video understanding. Using Qwen2-VL allows us to have full control over how the video is processed (e.g., some proprietary models (Reid et al., 2024) downsample the video to 1 fps) and provide easily reproducible results. A.7 Evaluation Protocol Prediction based methods. Both pixel and latent prediction methods can be evaluated in the same way, with the only difference being how the target of the prediction is encoded. For V-JEPA, we use abstract representations of the future obtained by encoding the video and then only keeping the future frames, while for VideoMAEv2 the target is simply the normalized future of the video. Considering video with frames 1, ..., , context encoder fθ handling frames, target encoder gψ producing the groundtruth future frames from the video, and predictor predicting frames in the future, we can measure surprise at time as This surprise can then be computed over full video to obtain global surprise score: St = pϕ (fθ (Vt:t+C)) gψ (Vt:t+C+M ) 1. AvgSurprise = 1 (cid:88) t{1,1+s,...,T (C+M )} St or MaxSurprise = max t{1,1+s,...,T (C+M )} St. (S2) (S3) Where is stride parameter which helps reduce the amount of compute used. In practice, we use = 2, which corresponds to predicting starting from frames 1, 3, 5, etc. 16 Figure S1 Different surprise measures are better suited for different tasks. Focusing on IntPhys, we find that looking at the average surprise over video leads to better performance when comparing pairs of videos. one-sample t-test was performed to see if the relative surprises are greater than zero (left). However, when looking at individual videos surprise, choosing the maximum surprise over video leads to better separation between possible and impossible videos. two-sample t-test was performed to see if impossible videos have higher surprise than possible ones. (rigt). For each property, we select the value of (C + being fixed) which maximizes performance to account for different constraints on memory for various tasks. On IntPhys, we are able to get rid of this sweep on context lengths by computing the minimal surprise over all context lengths for each starting frame t. By comparing the surprise score between possible and impossible videos, we can thus measure whether or not the model has understood the intrinsic physical property. Using an average surprise score (Smith et al., 2019; Piloto et al., 2022) is ideal for comparing similar videos, but maximum surprise score can be used on unique videos by eliminating the surprise contribution coming from the complexity of the scene. Relative surprise (Piloto et al., 2022; Smith et al., 2019; Riochet et al., 2022), which looks at the difference between surprise on the impossible and possible videos, is commonly used as it allows to precisely measure the effect of the physics breaking event. Focusing on IntPhys, we performed single-tail one sample t-test to assess whether the model exhibits relative surprise greater than zero. Using the average surprise of video, we find that for all properties, V-JEPA produces relative surprise over zero: Object Permanence: M=7.8e-03, SD=6.3e-03 (t(59.0) = 9.7, = 4.64 1014);Shape Constancy: M=7.1e-03, SD=4.5e-03 (t(59.0) = 12.2, = 5.29 1018);Continuity: M=8.5e-03, SD=5.7e-03 (t(59.0) = 11.5, = 6.03 1017). These results can be visualized in the left column in Figure S1. Using the maximum surprise, we find that for all properties, V-JEPA produces relative surprise over zero: Object Permanence: M=8.5e-03, SD=5.7e-03 (t(59.0) = 7.9, = 4.54 1011);Shape Constancy: M=8.5e-03, SD=5.7e-03 (t(59.0) = 8.9, = 7.51 1013);Continuity: M=8.5e-03, SD=5.7e-03 (t(59.0) = 6.8, = 3.19 109). For this pairwise classification task, we find that using the average surprise over video performs better, however both strategies provide high performance. harder task, but closer to reality, is to look at the surprises of possible and impossible videos individually rather than in pairs. Being able to separate possible and impossible videos without pairs is significantly harder (Riochet et al., 2022, 2020) and requires deeper understanding of the tested properties. Focusing on IntPhys, we perform one-tailed two-sample Welchs t-test to assess whether the impossible videos have higher average surprise than possible ones. Using the average surprise over video, we find that for all properties, impossible videos had higher surprise than possible ones on average: Object Permanence: M=5.7e-01, SD=1.2e-02 vs M=5.60e-01, SD=1.57e-02 (t(108.5) = 3.1, = 1.23 103);Shape Constancy: M=5.7e-01, SD=1.3e-02 vs M=5.61e-01, SD=1.62e-02 (t(111.2) = 2.7, = 4.46 103);Continuity: M=5.7e-01, SD=1.5e-02 vs M=5.66e-01, SD=1.72e-02 (t(116.3) 17 Figure S2 Normalized probabilities output by Qwen2-VL-72B. When presented with pair of videos, we find that the model outputs similar probabilities for possible and impossible videos. = 2.9, = 2.52 103). Using the maximum surprise over video, we find that for all properties, impossible videos had higher surprise than possible ones on average: Object Permanence: M=6.1e-01, SD=9.5e-03 vs M=5.77e-01, SD=2.43e-02 (t(76.7) = 8.4, = 1.03 1012);Shape Constancy: M=6.0e-01, SD=9.9e-03 vs M=5.78e-01, SD=2.26e-02 (t(80.6) = 8.1, = 2.061012);Continuity: M=6.1e-01, SD=2.0e-02 vs M=5.93e-01, SD=3.41e-02 (t(95.3) = 4.3, = 2.01 105). These results can be visualized in the right column of Figure S1. For this task, using the maximum surprise of video rather than the average is more desirable as it should lead to measures that only focus on the most surprising event, and are thus agnostic to other properties of the video. Multimodal LLM. Due to the output of the models being text only, the most direct approach is simply to ask the model which video is impossible in pair. We use the following prompt, inspired by the ones used in GRASP (Jassim et al., 2024) for single video classification: \"Video 1: <video_1>, Video 2: <video_2>. You are seeing pair of videos, Video 1 and Video 2. They were both generated in simulator, so ignore the quality of the videos. Exactly one of the two videos has an event which breaks the laws of physics. Given how objects behave on Earth, which one is it ? End your answer with the video name.\" Where <video_1> and <video_2> are replaced by the pair of video. We shuffle the order of the videos to avoid any bias where the model may prefer the first/second video. Asking the model to end its answer by the video name allows us to convert the output to predicted videos easily. We experimented with different other strategies such as 0-shot chain of thought (Wei et al., 2023), or using more detailed prompts, but did not find any qualitative or quantitative difference in behavior. For all models we set the sampling temperature to 0. When model refuses to answer the question, e.g. answering \"Both videos are plausible\" we count this as an error. Qwen2-VL never fell in this cases but Gemini 1.5 pro did around 10-15% of the time. Since we have access to the model for Qwen2-VL, and its output is probability distribution over possible tokens, we can also look at the probabilities assigned to the choice of video, i.e. \"1\" or \"2\" at the end of the answer. We thus computed normalized probabilities for each video as = (1) (1) + (2) or (2) (1) + (2) (S4) This gives us more granular surprise measure, although we find that the probability is often around 0.5, meaning that the model predicts almost as if it were coin toss. This can be seen in Figure S2 for Qwen2-VL-72B. A.8 Evaluation hyperparameters For every method, we use the following hyperparameters per dataset: IntPhys: Frame skip in [2,5,10]; Window size (C +M ) in [16,32]; Context lengths [2,4,6,8,10](C +M )/16 GRASP: Frame skip in [2,5,10]; Window size (C + ) in [16,32]; Context lengths [2,4,6,8,10](C + )/16 18 InfLevel-lab: Frame skip in [5,10,20] for V-JEPA and VideoMAEv2, [5,10,20,30] for Qwen and Gemini; Window size (C + ) in [16,32]; Context lengths [2,4,6,8,10](C + )/16; Table S3 Evaluation hyperparameters. Dataset IntPhys GRASP Method V-JEPA VideoMAEv2 Qwen-2-VL-72b Gemini-1.5-pro V-JEPA VideoMAEv2 Qwen-2-VL-72b Gemini-1.5-pro InfLevel-lab V-JEPA VideoMAEv2 Qwen-2-VL-72b Gemini-1.5-pro Frame skip FPS Window size Window Stride 2 2 5 2 10 10 10 10 5 10 20 30 7.5 7.5 3 7.5 5 5 5 5 6 3 1.5 1 16 16 All All 16 16 All All 32 16 All All 2 2 N/A N/A 2 2 N/A N/A 2 2 N/A N/A For all properties, we choose the context size which gives us the best performance. This means that on given dataset, different properties may use different optimal context sizes. For IntPhys, we find that using the minimal surprise over all windows for each start frame can be used as it removes one hyperparameter to optimize and helps filter surprise spikes coming from possible events. For example, an object entering the scene is hard to predict and leads to surprise spike, but this filtering enables us to remove it. Choice of prediction hyperparameters and influence on 0-shot performance As described in materials and methods, choice of prediction-related hyperparameters must be made to evaluate prediction-based models, such as V-JEPA or VideoMAEv2. Due to the 0-shot nature of the evaluation, the models have no priori calibration on the task they are solving. We thus need to find way to select hyperparameters, especially the context length for the prediction. This directly dictates how far back in the past models can look, and how far in the future they are predicting. The fact that most datasets such as GRASP or InfLevel-lab do not come with ready-made validation and test split further complicates classical approaches. Even in their presence, the similarities of the scenes among given property mean that validation set would be too similar to the test set and cause information leakage. It is worth noting that the results obtained on the dev set of IntPhys correlate with the results on its private test set, as shown in Figures 1 and 2. We thus opted to showcase the maximal attainable performance by optimizing the context size per property, as is commonly done in the self-supervised learning literature (Bardes et al., 2024). This means that we are evaluating whether the model has the necessary information or capabilities to solve the task at hand. We now study this choice in more depth and its impact on performance. First, we study what happens with fixed context size across all properties. Just as infants are not told which property is being tested in classical experiments (Baillargeon and DeVos, 1991), can the studied models perform well in this setting? Second, we study the distribution of performance when the context size is varied for each property. As illustrated in Figure S3, fixed context size across properties and dataset can be used, with minor impact on performance for V-JEPA. The small context size of 2 frames allows the model to do longer term prediction of 14 frames. We hypothesize that this leads to better performance than the opposite, i.e. 14 frame context, as it may be easier to predict some properties of the scene over long horizon rather than remembering them from the past. To illustrate, if at current frame red ball is in frame, the long-term prediction may still include information about it. However, if it was shown at the beginning of the context and then hidden, the model may struggle to predict that the red ball is visible again. Further experiments would be necessary to better understand how models leverage their context. 19 Figure S3 Models perform suboptimally with fixed context size. Due to limitations in how long of video models can process, we find drops in performance when using single context size across all properties and datasets. Performance remains non-trivial for V-JEPA in this scenario. Figure S4 Variation of performance when changing context size for predictions. While models tend to perform better with smaller context sizes, we find the optimal context size to be dependent on the property and dataset. GRASP exhibits the most variation whereas IntPhys and InfLevel-lab are less sensitive in general. Figure S5 Influence of motion and scene diversity. By pretraining V-JEPA-L on subsets of HowTo100M, we investigate how the diversity in motion and scenes affects performance on IntPhys.(left) By subsampling videos, we reduce the diversity in scenes, where we find that the model can still reach good performance with 128h of unique videos. (right) By subsampling frames in videos, we reduce the diversity of motions in each scene. Here we find lower performance than when subsampling videos, but the model still achieves good performance with 2% of the frames (2579h). Looking at Figure S4, we obtain better understanding of how context influences performance for specific properties and dataset. Performance on IntPhys and InfLevel-lab is stable for different contexts for both V-JEPA and VideoMAEv2. However, GRASP leads to the largest variations, which is also observed with untrained networks. We find that for the majority of properties, having shorter context, and thus longer predictions, improves performance. This may be explained by biases in the design of GRASP, as well as biases in the models predictions. The variation in performance using different context sizes is limitation of existing methods, which future models should address by handling longer video sequences."
        },
        {
            "title": "C Influence of semantic and motion diversity in videos",
            "content": "While we investigated the impact of the size of the pretraining dataset on performance, there are multiple ways to do so. Sampling only part of the videos forming dataset is one way, but we can also keep all videos and subsample the frames inside the each. One way to see the difference between the two approaches is that, at fixed total size, the former reduces the diversity of videos in terms of scenes, while the latter reduces the diversity in motion for given scene. Thus, there is no reason why both should be equivalent. To investigate frame subsampling, we use the following protocol: take the middle X% of video. Here, choosing the middle frames is not innocuous. The beginning and end of videos, especially tutorial videos (Miech et al., 2019), often consist of an intro, respectively an outro. These sections are not as related to the content of the videos and contain fewer movements and actions. We thus choose the middle frames as way to get more meaningful data. For video subsampling, the protocol is even simpler: sample uniformly X% of the videos. In both cases, we ensure that every smaller set of videos in included in the bigger ones. If < the set of X% of videos/frames is included in the set of Y% of videos/frames. In Figure S5 we can see that while both subsampling strategies lead to non-trivial performance, subsampling videos tend to produce higher performance. This further reinforces that all pretraining distributions are not equal and that certain ones lend themselves better to learn an understanding of intuitive physics."
        },
        {
            "title": "D Results on the IntPhys challenge",
            "content": "IntPhys was originally introduced as challenge, with private test set. While this makes analysis harder to provide for every experiment on this test set, we provide an analysis of V-JEPA on it. As can be seen in Tables S4 and S5, the high accuracy obtained by V-JEPA on the development set of IntPhys is also present 21 Table S4 Pairwise error rates on IntPhys test set. For pairs of videos, taking either the maximum or average surprise from video leads to high performance, surpassing the human results reported in (Riochet et al., 2022). Method Surprise Object permanence Shape constancy V-JEPA-H V-JEPA-H V-JEPA-L V-JEPA-L Riochet et al. (2020) Human Max Avg Max Avg Visible Occluded 0.6% 0.0% 5.2% 0.9% 5.0% 10.0% 8.2% 0.56% 35.4% 1.8% 19.0% 15.0% Visible Occluded All 4.4% 0.8% 0.28% 0.0% 0.20% 8.8% 1.4% 2.5% 12.0% 11.0% 12.5% 13.0% 8.1% 0.0% 35.0% 3.5% 31.0% 16.0% All 4.4% 0.19% 0.0% 0.0% 21.9% 5.9% 3.1% 0.7% 21.0% 26.0% 14.5% 20.0% 25.6% 0.19% 41.5% 3.3% 47.0% 40.0% All 12.87% 0.09% 23.8% 2.0% 41.0% 30.0% Continuity Visible Occluded Table S5 Single video classification error rates (1-AUROC) on IntPhys test set. For single videos, we see that the maximum surprise of video leads to the highest performance, surpassing the human baselines reported in (Riochet et al., 2022). Here, the average surprise of video is not good metric, possibly due to values being too dependent on the experimental setup. We report the metric as percentages for legibility. Method Surprise Object permanence Shape constancy V-JEPA-H Max V-JEPA-H Avg V-JEPA-L Max V-JEPA-L Avg Human Visible Occluded 8.0% 27.8% 25.5% 33.4% 18.0% 28.1% 38.9% 47.8% 41.7% 30.0% All Visible Occluded All 19.2% 11.9% 38.3% 31.2% 40.0% 29.9% 41.5% 37.0% 24.0% 22.0% 29.7% 39.3% 47.8% 42.5% 30.0% 21.9% 7.8% 39.2% 28.4% 41.8% 26.0% 42.7% 34.4% 26.0% 28.0% 43.9% 31.3% 49.0% 38.8% 47.0% All 29.67% 37.05% 41.6% 41.5% 38.0% Continuity Visible Occluded when looking at the private test set. It consists of 3600 videos per property, compared to the 120 of the development set. These results reinforce the conclusions previously drawn, confirming the robustness of the learned intuitive physics understanding. Similar to what was visible in Figure 2.B, V-JEPA is able to achieve performance similar or higher when compared to human baselines. We further find that V-JEPA surpasses the performance of previously published methods (Riochet et al., 2020) which leverage pre-defined abstractions such as segmentation masks. Focusing on the single video classification setting, we find notable difference of performance between V-JEPA-H and V-JEPA-L. Where V-JEPA-H matches human peformance, V-JEPA-L remains far from it. This highlights the fact that scale can be beneficial when the task is harder, whereas it does not bring significant improvements in pairwise setting."
        },
        {
            "title": "E Importance of contextualization events on InfLevel",
            "content": "As discussed in the main text, we find that models struggle on InfLevel-lab due to the importance of contextualization event. Without knowledge and memory of it, the task becomes unsolvable as the occluders modifications are invisible during the main experiment. We thus propose to relabel the data, assuming that objects are unmodified: cup always has bottom, and cylinder always has an uncut back. This provides testing ground that evaluates the understanding of the world of models where objects would be unmodified. There are some limitations, however. For gravity, for example, objects are seen going through cup or not. Assuming that the cup always has bottom, for the possible video nothing happens once the object is dropped, but for the impossible video, the model also needs to predict the trajectory of the object, bouncing on table. This means that the impossible video is by default much harder than the possible one, which can skew performance upwards. In Figure S6, we find significant increase in performance for both V-JEPA and VideoMAEv2 but also for untrained networks. The increase for the latter models would suggest that the task can be solved with some heuristics, where the assumption that both videos are matched apart from physics breaking event breaks. As such, while this increase in performance is encouraging, it has to be taken with grain of salt, and the results on continuity remain the most important due to the more controlled nature of the setup. 22 Figure S6 Relabeling InfLevel to remove contextualization events. Gravity and solidity both require to remember the properties about the containers shown in video before the actual experiment. By relabeling the videos such that the prefix video is not necessary, we find significant increase in performance for both V-JEPA and VideoMAE. However, this relabeling breaks the assumption that the possible and impossible videos have the same difficulty. Per-property performance of methods Figure S7 Complete results for V-JEPA-L. The models (n = 5) achieve accuracies higher than untrained networks on most properties. Black dots represent the performance of 5 seeds. Figure S8 Complete results for V-JEPA-H. The model achieves accuracies higher than untrained networks on most properties. Gray dots represent the performance of the 20 untrained networks. Confidence intervals obtained via bootstrapping. 23 Figure S9 Complete results for VideoMAEv2. The model achieves performance on par or slightly higher than untrained networks across properties, apart from solidity and collision. Gray dots represent the performance of the 20 untrained networks. Confidence intervals obtained via bootstrapping. Figure S10 Complete results for Qwen2-VL-72B. The model achieves performance on par or slightly higher than untrained networks across properties, except for color constancy and support. Gray dots represent the performance of the 20 untrained networks. Confidence intervals obtained via bootstrapping. Figure S11 Complete results for Gemini 1.5 pro. The model achieves performance on par or slightly higher than untrained networks across properties. Gray dots represent the performance of the 20 untrained networks. Confidence intervals obtained via bootstrapping."
        }
    ],
    "affiliations": [
        "EHESS",
        "FAIR at Meta",
        "Univ Gustave Eiffel"
    ]
}