{
    "paper_title": "Discriminative Fine-tuning of LVLMs",
    "authors": [
        "Yassine Ouali",
        "Adrian Bulat",
        "Alexandros Xenos",
        "Anestis Zaganidis",
        "Ioannis Maniadis Metaxas",
        "Georgios Tzimiropoulos",
        "Brais Martinez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a \"bag of words\" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks. In this work, we propose to combine \"the best of both worlds\": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding. Our contributions include: (1) A carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework's components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality."
        },
        {
            "title": "Start",
            "content": "Discriminative Fine-tuning of LVLMs Yassine Ouali*1 Adrian Bulat*1,2 Alexandros Xenos1,3 Anestis Zaganidis1 Ioannis Maniadis Metaxas1 Brais Martinez1 Georgios Tzimiropoulos1,3 1Samsung AI Cambridge 2Technical University of Iasi 3Queen Mary University of London 4 2 0 2 ] . [ 1 8 7 3 4 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting bag of words behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks. In this work, we propose to combine the best of both worlds: new training approach for discriminative finetuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts generative LVLM into discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding. Our contributions include (1) carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our frameworks components. (2) parameter-efficient adaptation method using combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality. 1. Introduction Contrastively-trained Vision Language Models (VLMs) (e.g. CLIP [36]) have become the predominant direction for vision-language representation learning, exhibiting remarkable zero-shot abilities [21, 28, 31, 36, 48]. However, the great success of these models in many vision-language and vision tasks, even in zero-shot manner, sweeps under the rug some of their important limitations. Specifically, * Equal contribution. Listing order is random. such models struggle to exhibit advanced language understanding capabilities, suffer from limited understanding of compositionality, and manifest bag of words behavior [25, 47]. For example, even with bag of words behavior, VLMs have shown remarkable zero-shot retrieval accuracy on the Flickr [46] and COCO [33] datasets. Still, they perform poorly on simple word order permutation task on the same datasets [47]. Unfortunately, these issues persist even when the model and the dataset size increase [19]. Concomitantly, inspired by the success of LLMs [5, 42] in acting as generalist assistants [15], series of works combine pretrained vision encoders and LLMs [26, 27, 50] to construct Large Vision-Language Models (LVLMs) capable of performing interactive multi-modal conversations between the model and the user. Among others, these models have been shown capable of exhibiting strong reasoning and vision-language understanding capabilities, offering finegrained and detailed responses [9, 11, 26, 27]. However, they are trained with next-token prediction loss in an autoregressive manner, which appears less suitable for direct utilization in discriminative image-text tasks (e.g. imagetext retrieval). To our knowledge, the very recent (concurrent) work [23] is the first one to show that, with appropriate prompting, LVLMs can serve as zero-shot discriminative models. Importantly, [23] advocates for text-text optimization approach, stating that contrastive image-text fine-tuning has detrimental effect on the models performance. In contrast to [23], we propose new training framework for discriminative image-text fine-tuning of LVLMs, aiming to convert the original generative LVLM into discriminative one, thereby significantly enhancing its capability for image-text discrimination while preserving the compositional strengths of the original model. In our approach, following the (independent) two-towers paradigm, the vision embeddings are produced by passing the image through the entire LVLM, and the text embeddings are produced by passing the text through the LLM of the LVLM. Intuitively, for vision embedding, the LLM acts as an information extractor/processor that refines the visual information extracted by the data while simultane1 ously aligning the visual representations with the textual ones. We coin our approach VladVA: Vision-Language Adaptation for Discriminative Visual Assistant. Our main contributions are: We devise carefully designed optimization framework that utilizes image-text pairs of variable length and granularity for model training (i.e. both short and long captions). Using this data, the model is trained with both contrastive and next-token prediction losses, which are both shown to be necessary for unlocking strong discrimination and compositionality capabilities. Our design choices are accompanied by ablation studies which justify the necessity of our frameworks components. To facilitate efficient training, we show how the model can be fine-tuned using parameter-efficient adaptation method based on combination of soft prompting [30] and LoRA adapters [20]. We show the positive impact of both components. We report significant improvements over state-of-the-art two-tower models (e.g. CLIP-like models) of similar size on standard image-text retrieval benchmarks (+4.7-7.0% gains in absolute terms). Moreover, we report notable gains on several vision-language understanding and compositionality benchmarks (up to +15%). 2. Related work 2.1. Large Vision Language Models (LVLMs) Inspired by breakthrough research in language modeling [5, 22, 40, 42], series of methods seek to combine pretrained LLMs and vision encoders to construct Large Vision Language Models (LVLMs) capable of processing image-text data jointly [3, 11, 32, 34, 35, 44, 45, 50]. The prevalent strategy consists in aligning the features produced by pretrained vision encoder to the textual space assumed by pretrained LLM using projection module, e.g. LLaVA [35], following two-stage alignment procedure. Follow-up works expand this to interleaved image-text data [1, 27] and multiple input crops [1] while seeking to improve the models efficiency [10]. Despite their strong generative and comprehension abilities [34], current LVLMs are primarily restricted to generative tasks. Only very recently, Jiang et al. [23], inspired by the recent progress in NLP [4, 24] adapted LLaVA-NeXT [26] model to discriminative tasks with the help of contrastive-like loss, using text data only. We note that unlike [23], we introduce training framework that learns from multi-turn image-text pairs (as opposed to text only) using novel formulation that jointly combines contrastive loss with next-token prediction, reflecting the data characteristics and inducing gradual representation buildup. Moreover, we compare our approach with E5V [23], significantly improving upon their results despite using smaller/lighter models. 2.2. Discriminative Vision-Language Models The prevalent approach for training Discriminative VLMs follows the two-tower contrastive approach pioneered by CLIP [36], whereby an image and text encoder are trained on web-collected image-text pairs to learn joint multimodal ( i.e., vision and language) space. Subsequent works build upon CLIP by scaling the data [37, 48], improving the architecture using late/early interactions [29] or improving the training loss [7, 48]. Despite their remarkable zeroshot and representation learning abilities [36] such models were shown to have significant shortcomings related to limited language understanding capabilities, including: lack of compositionality understanding [25], manifesting bag of words behavior [47], struggling with spatial relations [25], being susceptible to typographical attacks [17], etc. Recent works aim to address these shortcomings by constructing synthetic hard negatives [47] or performing cross-modality attention [29]. However, the former does not inherently change the models behaviors and has been shown to potentially learn series of shortcuts/artifacts [19]. Meanwhile, the latter is impractical for deployment at scale, as, due to the interactions between the encoders, each new query incurs an additional inference for every image within the set. To alleviate these shortcomings and improve the overall capabilities of such models, we depart from the prevalent approach of training VLMs using contrastive loss and, instead, propose new approach that seeks to convert generative LVLMs into discriminative models by adapting them using newly proposed framework that combines generative and discriminative objectives. 3. Method The following sections detail our approach which we call VladVA: Vision-Language Adaptation for Discriminative Visual Assistant. 3.1. LVLMs as zero-shot discriminative models LVLMs consist of an LLM Φt, vision encoder Φv, and module that projects the vision features into the LLMs textual space. Once fine-tuned, such models can produce textual answer xa = Φt(g(Φv(xv)), xq) when presented with an input image xv and text query (or prompt) xq. Despite being solely trained with an autoregressive nexttoken prediction loss on limited amounts of data (< 5M), such models can act as multi-modal discriminative models in zero-shot manner [23]. To elicit this capability, the image embedding fv = Φt(g(Φv(xv), xp)[eos] is obtained by passing the image alongside handcrafted prompt xp through the LVLM and taking the last token, prior to any generation, as the reference feature. Analogously, the text 2 Figure 1. Overall VladVA framework: generative LVLM is adapted into discriminative model. At test time, the vision features (fv) are produced by passing the image xv alongside the handcrafted and/or soft prompt xv to the LVLM. The last token of this sequence will contain the summarized representation. Analogously, the textual features (ft) are produced by passing the prompt corresponding to the language modality (xt ). During training, the LVLM will also take as input long, detailed caption xlong p) through the LLM of the LVLM (i.e. the weights of the LLM are fully shared) alongside the short caption (xshort . embedding ft = Φt(xp, xq)[eos] is produced by passing the handcrafted prompt xp and input xq query through the LLM of the LVLM and taking again the features of the last token, prior to any autoregressive step. We will refer to these particular tokens as summary tokens. Note that, typically, the respective handcrafted prompts for the image and text modalities are different. Finally, the similarity between an image and given text can be computed by taking the cosine similarity between the two: = cos sim(fv, ft). Figure 2. Entropy of the output probability distribution at the next-to-be-predicted token location using LLaVA-1.5-7B for set of 50 prompts for both images and captions. Figure 3. Cumulative variance of the image and text embedding matrices over set of 50 prompts on Flickr30k. Embeddings that capture more information about the input translate into cumulative variance that requires more principal components to be explained, i.e. higher rank embedding matrix. 3 What makes good prompt? Zero-shot adaptation by prompting already provides decent results despite the task changing from generation to discrimination. To shed some light, herein, we study (a) what makes good prompt and (b) how we can identify it. To answer these questions, we construct testbed consisting of 1,000 image-caption pairs from Flickr30k [46], which we then use to evaluate the quality of various prompts. The prompts (50 image-text pairs in total) are constructed using ChatGPT. Each prompt pair is fed, alongside an image and its respective caption, through the LLaVA1.5-7B model. For each image-prompt pair and, respectively, caption-prompt pair, we take its token embedding at the output position and its output probability distribution over the vocabulary and use them to compute two measures for each prompt: the average entropy of its output distributions and the cumulative variance of its embeddings. As Figs. 2 and 4 show, when the model is prompted with sentences consisting of certain keywords, such as in few words or in one word, the model is pushed to condense the information of the image or text in the next token, resulting in an output distribution with high entropy. More importantly, when investigating the generated embeddings, we observe that higher entropy prompts result in embeddings with more spread-out cumulative variance, i.e. requiring more principal components to capture the same amount of variance, indicating an embedding matrix with high rank (see Fig. 3). This translates into discriminative embeddings that can capture more information about the inputs, making them suitable for embedding tasks. The benefit of this behavior is illustrated in Fig. 5, which shows positive correlation between prompts with high entropy scores and the models zero-shot retrieval performance. Hence, our approach should seek to produce embeddings with a) spreadout variance and b) probability distributions over the vocabulary with increased entropy. Figure 4. Top-k next-to-be-predicted tokens before and after the proposed fine-tuning approach. On the right, we show the output probability distribution for each case. We observe that, when using the optimal prompt, the representations of the next token can encode diverse and more discriminative information, making potentially better-quality embeddings. This behavior is further improved by the proposed fine-tuning method. Figure 5. Image and text retrieval score on Flickr30k over set of 50 image-text prompts ordered by their entropy scores  (Fig. 2)  . We can observe that prompts with high average entropy scores correlate positively with the zero-shot retrieval performance. 3.2. Discriminative fine-tuning of LVLMs: from generation to discrimination Despite exhibiting surprising innate zero-shot abilities, LVLMs direct discriminative performance lags behind that of state-of-the-art contrastively trained VLMs. Hence, carefully designed frameworks are needed to unlock the full potential of such models. This is the very goal of our work: to introduce well-grounded image-text adaptation/training framework that surfaces the discriminative vision-language capabilities of generative LVLM. Notably, our findings contradict those of the very recent (concurrent) work of [23], which found that contrastive image-text fine-tuning is detrimental and limits training to text-text contrastive learning alone. This highlights the importance of our proposed approach, which overcomes such impediments and significantly boosts the discriminative performance of the model. Having established the architecture in the previous section, the two other pillars are the data and training strategy. Data strategy: We argue for the importance of data diversity in terms of granularity and group captions according to their length: short captions (< 30 tokens) and long captions (30 500 tokens). The short captions capture coarse details and summarize image content, which teaches the model to discriminate with regard to the high-level information of an image. Longer captions capture finer details in images and promote better understanding of language concepts such as spatial relationships and compositionality. For strong discriminative model, both are necessary. Therefore, for images missing either caption type, we use BLIP2 [29] captioner to generate short captions and ShareGPT-4V [8] to generate long captions. This allows us to leverage both supervisory signals for training. Training strategy: As we demonstrate in this work, the variable length of the training data poses its own challenges: unlike the case of short captions, where direct training using the well-studied contrastive loss performs well, it collapses for longer captions. This brings us to the proposed training strategy, whereby, to address this challenge, we propose well-motivated hybrid training approach that combines contrastive loss (see Sec. 3.2.1) and next-token prediction loss for discriminative adaptation (see Sec. 3.2.2). Finally, as full model fine-tuning is computationally expensive, in Sec. 3.3, we detail fine-tuning strategy that combines adapters with soft prompting. 3.2.1. Image-text contrastive alignment Under multi-modal contrastive formulation, the image and text representations, fv and ft respectively, must be close if they are semantically similar and far apart otherwise, under specified distance metric. At train time, this is enforced using symmetric image-text and text-image contrastive loss, which, for given mini-batch containing randomly selected samples, can be described as: Lc = 1 (cid:88) ( log k=1 exp(sk,k ) exp(sk,j ) (cid:80) log exp(sk,k ) exp(sj,k ) (cid:80) ), = cos sim(f (1) , where sk,j ) denotes the cosine similarity between the k-th image and the j-th caption (image-totext), and similarity, sk,j the text-to-image similarity. During training, the contrastive loss is applied to the very same tokens used for the zero-shot evaluation, as they represent the optimal starting point for further fine-tuning. We note that the contrastive loss is mostly suitable for train4 ing using short captions (< 30 tokens), like the ones typically used for CLIP pre-training. We found that training the model using contrastive loss on longer captions proves challenging. Hence, to address this, in the following section, we study and propose new formulation that enables discriminative training on variable-length data. 3.2.2. Autoregressive training for learning discriminative"
        },
        {
            "title": "LVLM representations",
            "content": "Until now, the modality-specific embeddings are obtained by taking the last token, prior to any generation, while the training is largely focused on short (i.e. < 30 tokens) captions, mimicking the CLIP-style data used for contrastive training. This contrasts with the LLaVA-style autoregressive training, where long and highly descriptive captions (typically 200500 tokens) are used to help the LVLM learn strong links between the vision and text domains, pay attention to fine-grained details, and develop strong reasoning and compositionality capabilities. As noted earlier, directly using the long captions with the contrastive loss is ineffective, as, due to the high specificity of the long captions, the task is easy and nearly trivial to solve, with the loss going to 0 in just few hundred iterations. To address this, we propose to instead apply the next-token prediction loss over the long captions: LCE = (cid:88) i=1 log pθ(uixv, xp, xq,<i), (2) where is the length of the long caption, xq, xv the input image and xp the prompt, and pθ as the next-token probability distribution produced by the model at input token ui. Intuitively, this formulation possesses multiple advantages: (1) It allows the model to learn from long captions, as predicting each and every token correctly is challenging task (as opposed to applying the contrastive loss to long captions). (2) The decoding process encourages the condensation of information into the starting token used as feature embedding. (3) It offers an avenue for retaining the generative capabilities of the model while strengthening its discriminative abilities. 3.2.3. Overall training loss Template for the image modality: USER: [Image Prompt] <image> ASSISTANT: <out token> USER: Describe the image in detail. ASSISTANT: <long caption> <out token> Template for the text modality: USER: [Text Prompt] <short caption> ASSISTANT: <out token> the applied between contrastive the loss with <out token> belonging to the image modality on one side, and the text modality on the other. Concomitantly, the next-token prediction cross-entropy loss is applied over the tokens belonging to the <long caption>. Note that the short caption must be sufficiently different from the long caption to prevent shortcuts during training, property that does not naturally emerge in our case due to the difference in length and annotation procedure. 3.3. Parameter Efficient Adaptation As direct fine-tuning of the LVLM is costly, especially while attempting to maintain reasonably large batch size for contrastive learning, herein we explore and adopt softprompting combined with LoRA adapters, both trained under the same loss formulation of Sec. 3.2. Soft prompting was recently proposed as an efficient taskadaptation approach for both LLM [30] and CLIP [6, 49] models, representing direct departure from the prompt hand-crafting solution. Specifically, for given input i.e. image and text, we define set of modality, modality(m)-specific learnable vectors [vm 2 , , vm ], RC with denoting the models vocabulary embedvm ding size. These vectors can be inserted across the input In practice, we sequence to adjust the models behavior. opt to replace the tokens belonging to the hard prompts (i.e. xp; see Sec. 3.1) with the learnable vectors, initializing their values with the embeddings of the handcrafted ones. Adapter fine-tuning: While efficient, the representation power of the soft prompts is somewhat limited. Hence, following best practices, we also attach LoRA [20] adapters to the linear layers located inside Φt. Such adapters offer multifold advantage: lower memory requirements, reduced potential of overfitting during training, and no additional compute requirements during inference. 1 , vm The model is fine-tuned using these components. Importantly, both have positive impact on overall accuracy. 3.4. How does the models behavior change? We apply the next-token prediction loss over the long captions and the contrastive loss over the short ones in unified manner. During training, the templates presented to the LVLM for the image and, respectively, text modality take the following form: Building upon the analysis from Sec. 3.1, we show that our training approach elicits the following behavioral changes: (1) The summary token - vision tokens attention increases in density. (2) Both the entropy of the output distribution of the summary token and the spread of the cumulative vari5 Figure 6. Attention map between the summary and vision tokens shown for set of heads. Notice that post-training, the attention maps densify. This behavioral change can be interpreted as follows: For generative tasks, at every step in the generation process, the model has the chance to look back at the vision tokens, selectively attending to the regions of interest at the current step. In contrast, in discriminative setting, the model must compress all information present in the image within the summary token. Table 1. Zero-shot text-image retrieval accuracy on Flickr30K, COCO and nocaps. image retrieval text retrieval"
        },
        {
            "title": "Method",
            "content": "Flickr30K"
        },
        {
            "title": "COCO",
            "content": "nocaps Flickr30K"
        },
        {
            "title": "COCO",
            "content": "nocaps R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@ CLIP (ViT-B) [36] CLIP (ViT-L) [36] BLIP (ViT-L) [28] BLIP2 (ViT-L) [29] OpenCLIP (ViT-G/14) [37] OpenCLIP (ViT-BigG/14) [37] EVA-02-CLIP (ViT-E/14+) [38] EVA-CLIP (8B) [39] EVA-CLIP (18B) [39] LLaVA-1.5-7B [34] E5-V (LLaVA-Next-8B) [23] E5-V (LLaVA-1.5-7B) [23] VladVA (Ours) (LLaVA-1.5-7B) 58.8 67.3 70.0 74.5 77.8 79.5 78.8 80.3 83.3 59.6 79.5 76.7 85.0 89.8 93.3 95.2 97.2 96.9 97.1 96.8 97.2 97.9 89.3 97.6 96.9 98. 30.5 37.0 48.4 50.0 48.8 51.3 51.1 52.0 55.6 34.4 52.0 48.2 59.0 66.8 71.5 83.2 86.1 81.5 83.0 82.7 82.9 85.2 69.6 84.7 82.1 88.6 46.8 48.6 62.3 63.0 63.7 65.1 64.5 65.3 69.3 46.9 65.9 62.0 72. 85.1 85.7 93.4 93.8 93.2 93.5 92.9 93.2 94.8 83.3 94.3 93.0 96.5 77.8 87.2 75.5 86.1 91.5 92.9 93.9 94.5 95.3 65.6 88.2 86.6 94.3 98.2 99.4 97.7 99.4 99.6 97.1 99.8 99.7 99.8 92.3 99.4 99.0 99. 51.0 58.1 63.5 63.0 66.3 67.3 68.8 70.1 72.8 35.6 62.0 57.4 72.9 83.5 87.8 92.5 93.1 91.8 92.6 92.8 93.1 94.2 70.5 89.7 88.4 94.4 67.3 70.0 72.1 74.4 81.0 82.3 83.0 83.5 85.6 52.1 74.9 71.9 85. 95.6 96.2 97.7 98.3 98.7 98.8 98.9 98.6 99.2 88.1 98.3 97.0 99.5 ance of the embeddings increase. The attention map densification, as exemplified in Fig. 6, shows that, for discriminative tasks, the model gathers evidence from all parts of the image in order to correctly encode the information therein. This is not needed for generation, as at every generation step, the model can peak back at the vision tokens and select the required information. Entropy and cumulative variance: As shown in Fig. 3, our approach results in models where the cumulative variance of the image and text embeddings is significantly more spread out, which translates into richer and better-aligned embeddings, capable of more accurately capturing finegrained details. Additionally, the model maintains the diversity of output distribution at the summary token, i.e. high entropy, as illustrated in Fig. 4. 4. Experiments We compare our approach with the current state-of-the-art on two tasks of interest in zero-shot manner: image-text retrieval and compositionality/language understanding. Models compared: We compare with state-of-the-art models based on the two-towers (independent) approach, which is practical for retrieval purposes and also followed by our method. We cover wide variety of settings, including different models and model sizes, training data, training losses, etc.: CLIP (ViT-B and ViT-L) [36] the original CLIP trained with contrastive loss on 400M image-text pairs; BLIP (ViT-L) [28] trained on over 120M samples using contrastive, captioning and image-text matching losses; BLIP2 (T5-XXL) improved and scaled-up version of BLIP that makes use of qformer and LLM; OpenCLIP (ViT-G/14) [37] scaled-up version of [36] trained on 2B samples; OpenCLIP (ViT-BigG/14) [37], EVA02-CLIP (ViT-E/14+) [38], EVA-CLIP (8B) [39] and EVA-CLIP (18B) [39] large contrastively trained models, with up to 18B parameters, fine-tuned from vision encoders trained with Masked Image Modeling (MIM); E5V (LLaVA-Next-8B) and E5-V (LLaVA-1.5-7B) LVLMs finetuned using text-text contrastive loss. Depending on the task, we also include additional specialized baselines (e.g. NegCLIP [47] for compositionality). Table 2. Comparison with state-of-the-art on the SugarCrepe compositionality benchmark."
        },
        {
            "title": "Human",
            "content": "CLIP (ViT-B) [36] CLIP (ViT-L) [36] BLIP (ViT-L) [28] BLIP2 (ViT-L) [29] OpenCLIP (ViT-G/14) [37] OpenCLIP (ViT-BigG/14) [37] EVA-02-CLIP (ViT-E/14+) [38] EVA-CLIP (8B) [39] EVA-CLIP ((18B) [39] NegCLIP [47] LLaVA-1.5-7B [34] E5-V (LLaVA-Next-8B) [23] E5-V (LLaVA-1.5-7B) [23] VladVA (Ours) (LLaVA-1.5-7B)"
        },
        {
            "title": "Add",
            "content": "(B) 0.15 0.43 0.23 1.17 1.37 2.54 5.04 8.22 18.3 0.15 7.06 8.36 7.06 7."
        },
        {
            "title": "Object Attribute Relation Object Attribute Object Attribute",
            "content": "100 90.9 94.1 96.5 97.6 95.8 96.6 97.1 96.4 97.5 92.7 88.0 96.7 95.8 98.1 99 80.1 79.2 81.7 81.7 85.0 87.9 88.5 86.6 88. 85.9 81.6 89.5 86.6 92.1 97 69.2 65.2 69.1 77.8 72.4 74.9 74.2 74.8 76.1 76.5 76.1 85.3 81.6 86. 99 61.4 60.2 66.6 62.1 63.0 62.5 67.3 66.1 65.3 75.2 60.9 75.0 62.9 79.0 100 64.0 62.3 76.8 65.5 71.2 75.2 74.1 74.6 76. 75.4 58.8 70.1 64.0 82.9 99 77.2 78.3 92.0 92.4 91.5 92.2 91.8 91.3 92.4 88.8 67.0 89.2 93.5 95. 99 68.8 71.5 85.1 87.4 82.1 84.5 83.9 82.0 85.0 82.8 62.4 83.5 88.0 95.8 4.1. Zero-shot image-text retrieval We test our approach on the standard Flickr30k [46], MSCOCO [33] and nocaps [2] datasets, containing 1,000, 5,000 and 15,100 test samples respectively. For the latter, we simply average the results on the three partitions. Across all three datasets, our approach significantly surpasses the current state-of-the-art models, including models of similar size. It even outperforms the much bigger EVA-CLIP (18B) model (85.0% vs. 83.3%) on Flickr30k, (59.0% vs. 55.6%) on MS-COCO and (72.3% vs. 69.3%) on nocaps in terms of @R1 for image retrieval. Similarly, we also outperform the LVLM-based E5-V model by 5.5% on Flickr30k, 6% on MS-COCO, and 6.2% on nocaps. 4.2. Image-text compositionality Compositionality benchmarks construct hard negatives that elicit various types of compositional failures. Herein, we focus our comparison on the currently most challenging test sets, SugarCrepe [19] and SugarCrepe++ [16] (For Winoground [41] please see supp. material). For SugarCrepe++, we are primarily interested in the Image-to-Text (ITT) setting since the Text-to-Text (TOT) setting evaluates the language component of the methods only. As Tables 2 and 3 show, our approach is the best in both SugarCrepe and SugarCrepe++ (ITT). On SugarCrepe, we outperform the 18B EVA-CLIP model on all categories, with particularly large gains on relation replacement (76.1 vs. 86.8), attribution adding (85.0 vs. 95.8), and object swapping (65.3 vs. 85.8). The last case is particularly interesting as it directly measures the bag-of-words behavior, showcasing the significant improvements our method achieves in this direction. Additionally, we outperform the E5-V variant based on the same LLaVA-1.5-7B model that we used, and the one based on the heavier LLaVA-Next8B. similar trend is observed on SugarCreppe++ where we outperform EVA-CLIP (18B) by up to 10.9% (on object swapping) and E5-V (ITT) in all but relation replacement. Note that, thanks to its text-text training, E5-V surpasses our method for the TOT setting, but we note that their loss can be readily incorporated into our framework, leaving this for future work. 5. Ablation studies 5.1. Impact of methods components We quantify the impact of the proposed methods components by training on smaller 1M subset, reporting results on SugarCrepe and on Flickr30k. Impact of adaptation components: We start by measuring the impact of the efficient adaptation strategy based on soft prompting and adapter-finetuning. For simplicity, we ablate this by training using only the contrastive loss. As the results from Tab. 4 show, both components, individually and jointly, provide notable gains on top of the original LLaVA-1.5-7B model (i.e. the case of no adaptation). While LoRA fine-tuning performs better than softprompting (due to its bigger capacity), the latter alone performs surprisingly well. To understand why, we analyze the changes the soft prompts undergo by finding the closest embedding in the LLMs vocabulary. This results in the </s> <Summarize following decoded sentences: the provided image in one word:/ $[ and, ωaSummarize the provided text in one word:. The two sentences remain unchanged semantically, with the only characters changed being the ones at the start and end of the prompt. Intuitively, this allows the 7 Table 3. Comparison with state-of-the-art on the SugarCrepe++ compositionality benchmark. Method Human CLIP (ViT-B) [36] CLIP (ViT-L) [36] BLIP (ViT-L) [28] BLIP2 (ViT-L) [29] OpenCLIP (ViT-G/14) [37] OpenCLIP (ViT-BigG/14) [37] EVA-02-CLIP (ViT-E/14+) [38] EVA-CLIP (8B) [39] EVA-CLIP (18B) [39] NegCLIP [47] CLIP-SVLC [14] BLIP-SGVL [18] LLaVA-1.5-7B [34] E5-V (LLaVA-Next-8B) [23] E5-V (LLaVA-1.5-7B) [23] VladVA (Ours) (LLaVA-1.5-7B) Params Swap Object Swap Attribute Replace Object Replace Attribute Replace Relation (B) 0.15 0.43 0.23 1.17 1.37 2.54 5.04 8.22 18.3 0.15 0.15 0.15 7.06 8.36 7.06 7.06 ITT TOT ITT TOT ITT TOT ITT TOT ITT 100.00 45.2 46.0 46.8 37.9 40.7 48.8 48.4 43.6 45.2 55.3 43.0 13.2 23.8 50.8 39.5 56.1 96.7 19.7 14.5 29.8 39.5 27.4 28.2 28.2 25.4 25. 34.7 18.9 30.7 48.4 42.3 36.7 96.7 45.2 44.5 60.1 51.9 54.2 57.7 56.3 55.2 55.5 58.0 48.4 38.8 28.0 49.7 40.7 63. 93.3 33.0 28.7 52.5 55.4 49.6 52.4 49.4 46.9 47.6 56.5 34.6 29.5 56.9 48.5 62.5 100.00 97. 100.00 86.8 92.0 92.6 94.8 93.1 94.2 94.5 93.7 94.1 89.5 80.9 53.8 58.1 93.1 89.7 95.0 83.7 81.3 89.1 96.9 89.4 90.5 88.9 85.8 85.1 94.5 91.6 63.0 97.6 94.6 93.0 65.6 68.8 71.7 73.2 72.5 76.4 76.3 73.4 77.0 69.4 57.0 34.4 46.8 76.1 71.7 78.2 98.3 59.1 56.3 75.0 86.5 73.1 72.6 70.6 67.9 69. 76.3 66.9 58.1 87.1 86.4 82.3 100.00 56.3 53.4 56.8 65.1 57.6 59.4 59.4 59.7 60.4 52.3 47.3 30.7 52.3 74.7 72.0 71. TOT 96.7 38.62 39.1 57.7 69.6 51.4 53.6 49.4 49.2 47.8 51.6 51.3 63.4 84.4 81.5 66.3 Table 4. Impact of adaptation components and AR loss. All models are trained on 1M samples. 7. Results for additional model sizes and architectures Method AR SugarCrepe Flickr30k loss Replace Swap Add T2I I2T LLaVA-1.5-7B + soft-prom. + adapter-ft. + adapter-ft. + soft-prom. + adapter-ft. + soft-prom. 81.9 86.4 87.0 87.1 89.5 59.8 66.9 69.8 72.0 75. 64.7 59.6 65.6 89.3 88.8 88.6 89.5 76.7 79.1 79.6 80.6 91.7 91.4 92.9 91. model to mark/specialize the token that should gather the visual or textual evidence for discriminative tasks. Impact of AR loss: We measure the impact of the proposed autoregressive loss on long captions from Sec. 3.2.2. As Tab. 4 shows, the AR loss adds notable performance boost across all datasets tested. Finally, we note that using the long captions in isolation, without the proposed training strategy and loss, does not result in measurable gains. To further showcase the generalizability of our approach, herein we report results on two additional models: LLaVA1.5-13B [34] and Qwen2-VL-2B [43]. The 1st is scaledup version of the LLaVA-1.5-7B [34] used in the main manuscript and tests the scalability of our approach with size. The second follows different architecture and training procedure and has only 2B parameters, testing both generalizations to different architectures and finetuning in lower-parameters regime. As the results from Tables 5, 6 7 and 8 show, on all 6 datasets (i.e. Flickr, coco, nocaps, SugarCrepe, SugarCrepe++ and Winoground) for both retrieval and compositionality, in all cases we significantly improve upon the original zero-shot model performance, showing good scalability with size in both directions, i.e. for smaller and bigger models. 6. Conclusions We introduced new framework for adapting generative LVLM into discriminative model, unlocking its innate capability for powerful image-text discrimination and enhanced language understanding. Our framework uses both short and long captions for training the LVLM with contrastive and next-token prediction losses respectively. We also presented parameter-efficient adaptation method using combination of soft prompting and LoRA adapters. Finally, we showed that our approach results in significant improvements over state-of-the-art models of similar size for image-text retrieval and compositionality benchmarks. 8. Compositionality evaluation on Winoground In addition to the results from the main paper, herein, we report results on Winoground [41], curated dataset consisting of 400 images with difficult/unusual scenarios that go beyond compositionality and largely act as natural adversarial set [13, 47]. As the results from Table 8 show, our approach matches and outperforms prior models, including the large 18B EVA-CLIP model (17.5 vs. 15.0, 40.5 vs. 35.8 and 12.8 vs. 10.5, for image, text and respectively group set). 8 9. Zero-shot image recognition on ImageNet From an evaluation point of view, the main focus of this work is on improved zero-shot retrieval and, more generally, improved vision-language compositional ability. We focus on these tasks, as they require stronger (vision-)language understanding abilities, which we show an LVLM can offer under appropriate training regimes. As study case, herein, for completeness, we also measure the zero-shot ability of the model for image recognition on ImageNet [12]. As the results from Table 9 show, our approach significantly improves upon the zero-shot LVLM we start from (54.7 vs 70.6%). In comparison, E5-V approach only offers modest performance gains (45.8 vs 48.2%) and has notably lower performance than our approach (48.2 vs 70.6%) despite using bigger model. While significantly improving upon the model we start from, the low data regime we train our model in (only 8.1M samples) limits its overall performance, with contrastive models trained on billion samples performing better. This is expected as the image recognition ability of model, especially on the highly specific categories of ImageNet, will depend on how often (if at all) they are seen in the training set. This is especially significant given that many of the datasets used for contrastive learning are filtered based on the ImageNet classes [36]. In lower data regimes, comparable with ours, we can observe that our approach produces notably better results (e.g. 51.1% for FFF [7], trained on 15M samples vs 70.6% for ours). Finally, when comparing it with other models focusing on retrieval (i.e. BLIP and BLIP2) our approach outperforms either of them by more than 15% in absolute terms despite the fact that these models were trained on 129M samples. All in all, we outperform all models trained in comparable settings, showing promising initial results in this direction too. Table 5. Zero-shot text-image retrieval accuracy on Flickr30K, COCO and nocaps. image retrieval text retrieval"
        },
        {
            "title": "Method",
            "content": "Flickr30K"
        },
        {
            "title": "COCO",
            "content": "nocaps Flickr30K"
        },
        {
            "title": "COCO",
            "content": "nocaps R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@10 Qwen2-VL-2B [43] VladVA (Ours) (Qwen2-VL-2B) LLaVA-1.5-7B [34] VladVA (Ours) (LLaVA-1.5-7B) LLaVA-1.5-13B [34] VladVA (Ours) (LLaVA-1.5-13B) 54.1 80.4 59.6 85.0 61.7 85.6 86.0 97. 89.3 98.5 90.4 98.6 32.4 52.5 34.4 59.0 37.9 58.2 68.2 84. 69.6 88.6 74.1 88.4 41.2 68.3 46.9 72.3 48.4 74.0 80.1 94. 83.3 96.5 85.0 96.6 59.6 93.7 65.6 94.3 66.9 94.5 89.2 99. 92.3 99.9 93.6 99.8 35.3 71.9 35.6 72.9 35.3 75.0 71.8 93. 70.5 94.4 71.0 95.6 54.0 86.0 52.1 85.7 48.0 85.4 90.3 99. 88.1 99.5 87.9 99.6 Table 6. Zero-shot results on SugarCrepe compositionality benchmark."
        },
        {
            "title": "Method",
            "content": "Qwen2-VL-2B [43] VladVA (Ours) (Qwen2-VL-2B) LLaVA-1.5-7B [34] VladVA (Ours) (LLaVA-1.5-7B) (B) 2.21 2.21 7.06 7.06 LLaVA-1.5-13B [34] VladVA (Ours) (LLaVA-1.5-13B) 13.35 13."
        },
        {
            "title": "Object Attribute Relation Object Attribute Object Attribute",
            "content": "89.9 97.9 88.0 98.1 90.0 98.1 72.0 89.7 81.6 92.1 80.6 93. 75.0 81.5 76.1 86.8 76.3 89.8 56.1 76.5 60.9 79.0 71.8 81. 56.1 82.6 58.8 82.9 61.9 86.0 73.2 93.6 67.0 95.2 69.3 95. 70.1 95.4 62.4 95.8 59.1 97.0 Table 7. Zero-shot results on the SugarCrepe++ compositionality benchmark. Method Qwen2-VL-2B [43] VladVA (Ours) (Qwen2-VL-2B) LLaVA-1.5-7B [34] VladVA (Ours) (LLaVA-1.5-7B) (B) 2.21 2.21 7.06 7.06 LLaVA-1.5-13B [34] VladVA (Ours) (LLaVA-1.5-13B) 13.35 13. Params Swap Object Swap Attribute Replace Object Replace Attribute Replace Relation ITT TOT ITT TOT 32.7 50.8 23.8 56. 35.5 55.2 27.8 33.5 30.7 36.7 32.3 38.3 25.3 48.2 29.5 62. 32.4 60.6 30.5 60.4 28.0 63.0 30.2 65.6 9 ITT 73.6 93.7 58.1 95.0 68.7 94.5 TOT 65.9 93.8 63.0 93. 66.8 92.5 ITT 46.8 74.8 46.8 78.2 44.8 80.7 TOT 43.0 77.5 58.1 82.3 43.1 81.1 ITT 57.6 63.6 52.3 71. 52.3 73.2 TOT 58.3 57.4 63.4 66.3 55.6 66.4 ples post-training, so we can see the direct effect the training has on the model. As the results from Fig.7 show, generally, our approach better retains the generative capabilities of the model post-training, producing fine-grained captions, simIn contrast, E5-V appears to ilar with the original ones. predominantly produce only very-shot, not-descriptive outputs. Table 8. Comparison with state-of-the-art on the Winoground compositionality benchmark."
        },
        {
            "title": "Text Group",
            "content": "CLIP (ViT-B) [36] CLIP (ViT-L) [36] BLIP (ViT-L) [28] BLIP2 (ViT-L) [29] OpenCLIP (ViT-G/14) [37] OpenCLIP (ViT-BigG/14) [37] EVA-02-CLIP (ViT-E/14+) [38] EVA-CLIP (8B) [39] EVA-CLIP (18B) [39] NegCLIP [47] LLaVA-1.5-7B [34] E5-V (LLaVA-Next-8B) [23] E5-V (LLaVA-1.5-7B) [23] VladVA (Ours) (LLaVA-1.5-7B) 10.5 12.3 10.0 10.5 12.8 15.5 14.0 14.8 15.0 10.5 11.3 14.8 17.4 17. 25.0 27.5 30.5 29.5 32.0 35.5 33.8 36.5 35.8 29.5 18.5 32.3 31.3 40.5 7.3 8.3 7.8 8.5 9.3 12.0 10.8 10.3 10.5 8.0 6.5 11.3 10.5 12. Table 9. Zero-shot image recognition results on ImageNet dataset in terms of Top-1 and Top-5 (%) accuracy."
        },
        {
            "title": "Model",
            "content": "Data. size Top-1 Top-5 CLIP (ViT-B) [36] CLIP (ViT-L) [36] EVA-CLIP (18B) [39] CLIP (ViT-B) [36] HiDeCLIP (ViT-B) [36] FFF (ViT-B) [7] BLIP (ViT-L) [28] BLIP2 (ViT-L) [29] LLaVA-Next-8B [26] E5-V [23] (LLaVA-Next-8B) LLaVA-1.5-7B [34] VladVA (Ours) (LLaVA-1.5-7B) Qwen2-VL-2B [43] VladVA (Ours) (Qwen2-VL-2B) 400M 400M 2.7B 15M 15M 15M 129M 129M 0M 0M 0M 8.1M 0M 8.1M 68.4 74.0 83.5 32.8 45.9 51.1 54.2 46.7 45.8 48.2 42.0 63.7 54.7 70. 91.9 94.0 97.2 - - - 81.5 74.2 74.6 76.6 74.6 88.3 79.4 91.1 10. Qualitative text generation examples post discriminative adaptation Our main objective is to convert generative LVLMs into discriminative ones, hence the proposed approach is designed from the perspective of maximizing the discriminative abilities of the model. Still, it may be interesting to qualitatively see how our model, and the closest relevant approach E5-V behave. We note, that in principle both our approach and E5-V use LoRAs adapters, hence it is easy to switch between the discriminative and the generative mode without compromising either, by enabling or disabling the adapters. That being said, herein we present some qualitative exam10 Figure 7. Qualitative comparison on image captioning of the base LLaVA-1.5-7B model and its fine-tuned versions using both E5V [23] and our proposed method. We show that with our method, the LLaVA-1.5-7B better retains its captioning capabilities, while E5-V fine-tuning appears to result in less informative captions."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 2 [2] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 89488957, 2019. 7 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2 [4] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schmidt. Visit-bench: benchmark for visionlanguage instruction following inspired by real-world use. arXiv preprint arXiv:2308.06595, 2023. 2 [5] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 1, 2 [6] Adrian Bulat and Georgios Tzimiropoulos. LASP: Text-totext optimization for language-aware soft prompting of vision & language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2323223241, 2023. 5 [7] Adrian Bulat, Yassine Ouali, and Georgios Tzimiropoulos. FFF: Fixing flawed foundations in contrastive pre-training results in very strong vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1417214182, 2024. 2, 9, 10 [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 4 [9] Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, and Ajay Divakaran. Measuring and improving chain-ofthought reasoning in vision-language models. arXiv preprint arXiv:2309.04461, 2023. [10] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, et al. Mobilevlm v2: Faster and stronger baseline for vision language model. arXiv preprint arXiv:2402.03766, 2024. 2 [11] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 1, 2 [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 9 failures in visuolinguistic compositionality. arXiv preprint arXiv:2211.00768, 2022. 8 [14] Sivan Doveh, Assaf Arbelle, Sivan Harary, Eli Schwartz, Roei Herzig, Raja Giryes, Rogerio Feris, Rameswar Panda, Shimon Ullman, and Leonid Karlinsky. Teaching structured vision & language concepts to vision & language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 26572668, 2023. 8 [15] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [16] Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Sastry, Evangelos Milios, Sageev Oore, and Hassan Sajjad. Sugarcrepe++ dataset: Vision-language model sensiarXiv preprint tivity to semantic and lexical alterations. arXiv:2406.11171, 2024. 7 [17] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. Distill, 6(3):e30, 2021. 2 [18] Roei Herzig, Alon Mendelson, Leonid Karlinsky, Assaf Arbelle, Rogerio Feris, Trevor Darrell, and Amir Globerson. Incorporating structured representations into pretrained vision & language models using scene graphs. arXiv preprint arXiv:2305.06343, 2023. 8 [19] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. Advances in neural information processing systems, 36, 2024. 1, 2, 7 [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 5 [21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. conference on machine learning, pages 49044916. PMLR, 2021. [22] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume LamarXiv preprint ple, Lucile Saulnier, et al. Mistral 7b. arXiv:2310.06825, 2023. 2 [23] Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, E5-v: Universal embeddings and Fuzhen Zhuang. arXiv preprint with multimodal large language models. arXiv:2407.12580, 2024. 1, 2, 4, 6, 7, 8, 10, 11 [24] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nvembed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428, 2024. 2 [13] Anuj Diwan, Layne Berry, Eunsol Choi, David Harwath, and Kyle Mahowald. Why is winoground hard? investigating [25] Martha Lewis, Nihal Nayak, Peilin Yu, Qinan Yu, Jack Merullo, Stephen Bach, and Ellie Pavlick. Does clip bind concepts? probing compositionality in large image models. arXiv preprint arXiv:2212.10537, 2022. 1, 2 [26] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. 1, 2, 10 [27] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 1, 2 [28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 1, 6, 7, 8, 10 [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 2, 4, 6, 7, 8, 10 [30] Xiang Lisa Li and Percy Liang. Prefix-tuning: OptimizarXiv preprint ing continuous prompts for generation. arXiv:2101.00190, 2021. 2, [31] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie Yan. Supervision exists everywhere: data efficient contrastive arXiv preprint language-image pre-training paradigm. arXiv:2110.05208, 2021. 1 [32] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 2 [33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 1, 7 [34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2, 6, 7, 8, 9, 10 [35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 2, 6, 7, 8, 9, [37] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 2, 6, 7, 8, 10 [38] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 6, 7, 8, 10 [39] Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. Eva-clip18b: Scaling clip to 18 billion parameters. arXiv preprint arXiv:2402.04252, 2024. 6, 7, 8, 10 [40] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. 2 [41] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visiolinguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. 7, [42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 2 [43] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 8, 9, 10 [44] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 2 [45] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 2 [46] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. 1, 3, 7 [47] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it? arXiv preprint arXiv:2210.01936, 2022. 1, 2, 6, 7, 8, [48] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. 1, 2 [49] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei InLiu. Learning to prompt for vision-language models. ternational Journal of Computer Vision, 130(9):23372348, 2022. 5 13 [50] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1,"
        }
    ],
    "affiliations": [
        "Queen Mary University of London",
        "Samsung AI Cambridge",
        "Technical University of Iasi"
    ]
}