{
    "paper_title": "VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format",
    "authors": [
        "Yueqian Wang",
        "Xiaojun Meng",
        "Yuxuan Wang",
        "Jianxin Liang",
        "Jiansheng Wei",
        "Huishuai Zhang",
        "Dongyan Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent researches on video large language models (VideoLLM) predominantly focus on model architectures and training datasets, leaving the interaction format between the user and the model under-explored. In existing works, users often interact with VideoLLMs by using the entire video and a query as input, after which the model generates a response. This interaction format constrains the application of VideoLLMs in scenarios such as live-streaming comprehension where videos do not end and responses are required in a real-time manner, and also results in unsatisfactory performance on time-sensitive tasks that requires localizing video segments. In this paper, we focus on a video-text duet interaction format. This interaction format is characterized by the continuous playback of the video, and both the user and the model can insert their text messages at any position during the video playback. When a text message ends, the video continues to play, akin to the alternative of two performers in a duet. We construct MMDuetIT, a video-text training dataset designed to adapt VideoLLMs to video-text duet interaction format. We also introduce the Multi-Answer Grounded Video Question Answering (MAGQA) task to benchmark the real-time response ability of VideoLLMs. Trained on MMDuetIT, MMDuet demonstrates that adopting the video-text duet interaction format enables the model to achieve significant improvements in various time-sensitive tasks (76% CIDEr on YouCook2 dense video captioning, 90\\% mAP on QVHighlights highlight detection and 25% R@0.5 on Charades-STA temporal video grounding) with minimal training efforts, and also enable VideoLLMs to reply in a real-time manner as the video plays. Code, data and demo are available at: https://github.com/yellow-binary-tree/MMDuet."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 2 ] . [ 1 1 9 9 7 1 . 1 1 4 2 : r VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format Yueqian Wang1 Xiaojun Meng2 Yuxuan Wang3 Jianxin Liang1 Jiansheng Wei2 Huishuai Zhang1* Dongyan Zhao1,4 1Wangxuan Institute of Computer Technology, Peking University 2Huawei Noahs Ark Lab 3Beijing Institute for General Artificial Intelligence 4National Key Laboratory of General Artificial Intelligence {wangyueqian,liangjx,zhanghuishuai,zhaodongyan}@pku.edu.cn {xiaojun.meng,weijiansheng}@huawei.com wangyuxuan1@bigai.ai"
        },
        {
            "title": "Abstract",
            "content": "Recent researches on video large language models (VideoLLM) predominantly focus on model architectures and training datasets, leaving the interaction format between the user and the model under-explored. In existing works, users often interact with VideoLLMs by using the entire video and query as input, after which the model generates response. This interaction format constrains the application of VideoLLMs in scenarios such as live-streaming comprehension where videos do not end and responses are required in real-time manner, and also results in unsatisfactory performance on time-sensitive tasks that requires localizing video segments. In this paper, we focus on videotext duet interaction format. This interaction format is characterized by the continuous playback of the video, and both the user and the model can insert their text messages at any position during the video playback. When text message ends, the video continues to play, akin to the alternative of two performers in duet. We construct MMDuetIT, videotext training dataset designed to adapt VideoLLMs to videotext duet interaction format. We also introduce the MultiAnswer Grounded Video Question Answering (MAGQA) task to benchmark the real-time response ability of VideoLLMs. Trained on MMDuetIT, MMDuet demonstrates that adopting the video-text duet interaction format enables the model to achieve significant improvements in various timesensitive tasks (76% CIDEr on YouCook2 dense video captioning, 90% mAP on QVHighlights highlight detection and 25% R@0.5 on Charades-STA temporal video grounding) *Corresponding Author with minimal training efforts, and also enable VideoLLMs to reply in real-time manner as the video plays. Code, data and demo are available at: https://github.com/yellowbinary-tree/MMDuet. 1. Introduction Videos are becoming an increasingly important medium to acquire information on daily basis. Powered by recent advancements in large language models (LLMs) [2, 11, 25, 28, 37] and vision encoders [21, 22, 26, 30, 39], several video large language models (VideoLLM) [1518, 32, 40] have already demonstrated strong abilities for holding conversations and answering questions about videos. common feature of these models is using visual encoders to encode all frames sampled from the entire video at first, and integrate them into text input by concatenating them to input embeddings or using cross attention. Recent research on VideoLLMs has primarily concentrated on model architectures and training datasets, with limited exploration of the interaction format between the user and the model. In this paper, the interaction format of VideoLLMs comprises the following two aspects: (1) chat template used to convert input sources, e.g., video, user text query, and model response, into sequence of tokens; (2) turn-taking rule organizing inputs of different sources to finalize an interaction format. For example, for most existing VideoLLMs, the interaction format is: (1) for the chat template, the model uses (frames sampled from) the full video and text query as input, and then outputs response; (2) for the turn-taking rule, usually the model is 1 Figure 1. An example of the common Whole Video Interaction Format and our Video-Text Duet Interaction Format. permitted to take its turn to generate response when both the whole video content and user query have ended, e.g., when an <eos> token is explicitly provided. We refer to this traditional interaction method as whole video in the rest of this paper. However, this all-along used whole video interaction has the following three defects, which hinder the performance and real-world usage scenarios of VideoLLMs: Firstly, it does not admit timely interactions. As the video is often input as whole, this limits its usage in more scenarios like live broadcasts or surveillance videos, in which the video does not end at specific time. Even if we can segment the video into multiple fixed-length clips for input, the model still cannot generate responses in real-time manner when necessary. Instead, it must wait until the end of the clip before starting to reply, which negatively impacts the timeliness of the interaction. Secondly, it performs unfavorably In this paon time-sensitive video comprehension tasks. per we use time-sensitive tasks to refer to tasks in which the model is required to provide responses that include specific times in the video, such as temporal video grounding [3, 8, 13], video highlight detection [14], dense video captioning [13, 41], grounded video question answering [34], etc. Existing approaches refer to the video timeline by generating text that contains time information, such as second numbers like 3.5 7.1 seconds or timeline percentages like from 30 to 50. However, the performance of these models has been consistently unsatisfactory, possibly due to LLMs limited ability to handle numbers [24]. Thirdly, it suffers from sub-optimal performance of describing long videos. When generating responses relevant to specific segment in lengthy video, the model needs to first implicitly retrieve this relevant segment from the entire video that is already encoded in the context, which is challenging for most VideoLLMs [33]. In this work, we formalize the Video-Text Duet Interaction Format, an interaction method that aims to enhance VideoLLMs by addressing the aforementioned issues. An illustration of the whole video interaction format and the video-text duet interaction format is shown in Fig. 1. With our video-text duet interaction format, the video is continuously played and input to the model frame-by-frame. Both the user and model can insert their text messages right after any frame during the video play. When text message ends, the video continues to play, akin to the show of two performers in duet. This is implemented by also defining the video stream as participant in the dialogue, with its messages only comprising video frames. When dialogue turn from either the user or the model ends, the video stream can have the floor and input video frames to the model until another turn is started by either the user or the model. This improves the timeliness of interaction and better suits realworld applications such as live-streaming or surveillance video comprehension. Moreover, by inserting responses to the video where is most relevant, the model can learn to generate responses by referencing smaller but fine-grained fraction of the video before this position. In this manner, it facilitates information retrieval to describe lengthy videos, as well as enables response to be grounded at the targeted position of the video. We believe this design contributes to addressing the above discussed three issues of existing VideoLLMs. To prove the effectiveness of the video-text duet interaction format, we construct MMDuetIT, dataset to facilitate the training of versatile VideoLLM following the videotext duet interaction format. MMDuetIT is constructed by reformatting existing dense video captioning and temporal video grounding datasets by inserting each turn of the conversation at proper position within the video. We also propose Multi-Answer Video Grounded QA (MAGQA), novel task that requires the model to generate answers at appropriate positions within the video in real-time manner, to align with potential applications of live-streaming video comprehension. We also train MMDuet, VideoLLM that 2 implements our proposed video-text duet interaction format. Initialized with LLaVA-OneVision [15] and trained with MMDuetIT at low cost, MMDuet achieves significant performance improvement in various time-sensitive tasks, and is able to generate responses in real-time as the video plays. 2. Related Works 2.1. Video Large Language Models The advancement of large language models (LLMs) and visual encoders has led to numerous efforts on their integration, aiming to utilize the powerful understanding and generation abilities of existing LLMs for video-related tasks [1518, 32, 36]. These models exhibit decent ability of video understanding such as captioning or summarizing [36]. However, their performance on time-sensitive tasks is still unsatisfactory, primarily due to two reasons: First, many models are trained on datasets in which video contents are relatively static with annotations not dense enough [40]. It thus constrains the model to discern the temporal differences across various parts of videos. Second, differing from image models that can easily capture word patch alignment [5], most VideoLLMs are not explicitly trained to distinguish differences between different clips in the same video. 2.2. Localizing Video Segments with VideoLLMs Recent works attempt to empower VideoLLMs with the ability to localize and represent segments in videos, and thus achieve better performance on tasks like temporal video grounding or dense video captioning. These works explore new ways on how to easily represent video clips with texts, such as second numbers of timestamp (TimeChat [23]), timeline percentage (VTimeLLM [10]) or using special textual tokens (VTG-LLM [6], Grounded-VideoLLM [29]). However, their performance has not been satisfactory yet, possibly due to LLMs limited ability to accurately count and generate numbers [24] to localize each video frame. To alleviate this issue, HawkEye [31] uses coarsegrained method by referring to larger fraction of the video, but it requires multiple rounds of recursive grounding to precisely locate segment and may not express multiple segments at time. The work most similar to our motivation is VideoLLMOnline [1], which proposes framework named LIVE for training VideoLLMs to interrupt video streams and insert responses. However, they only finetune model on Ego4D [4] and COIN [27] to demonstrate the LIVE training and inference, and do not explore on how the model capabilities vary with this new type of interaction, especially the zeroshot performance on time-sensitive tasks. Our work differs from VideoLLM-Online at: Firstly, providing more general description of the video-text dual interaction format, including wider variety of criteria for determining whether response should be generated, and its application on new tasks such as temporal video grounding and grounded question answering; Secondly, introducing new dataset MMDuetIT and the method on building such datasets; Thirdly, proposing new task MAGQA; Lastly, proposing more powerful model MMDuet that has stateof-the-art performance on various time-sensitive tasks and zero-shot generalization ability. 3. The Video-Text Duet Interaction Format In Sec. 1, we have defined the concept of interaction format with two aspects (i.e., chat template & turn-taking rule), as well as the drawbacks of the commonly-used whole video interaction format. Now we re-emphasize and formalize our video-text duet interaction format, which is completely different from previous to implement VideoLLMs. (1) For the chat template, inspired by but different from the LIVE framework which is used to implement VideoLLM-Online [1], we consider the video stream as conversation participant just like the role of user/assistant, and the input sequence consists of alternating turns among these three roles. Take the LLM Qwen2 [37] as an example, the token sequence input to the model should be as follows: <im start>systemn[system-prompt]<im end> <im start>streamn<frame> <frame><im end> <im start>usern[user-message]<im end> <im start>streamn<frame> <frame><im end> <im start>assistantn[assistant-message]<im end> <im start>streamn<frame> (2) For the turn-taking rule, when the turn of the user or assistant ends, the video stream can take the floor and start its turn to input video frames. When each single frame is consumed, both the user and the assistant role can interrupt the video stream at any time, and start its own turn to query or generate response, as totally decided by the user or the assistant, respectively. 4. MMDuet: Our Proposed VideoLLM 4.1. Model Structure We propose MMDuet, model trained following the videotext duet interaction format, which can thus autonomously decide at what position in the video to generate what response. Like almost all existing VideoLLMs, MMDuet consists of three components: 1) visual encoder that encodes sampled frames from the video to visual feature, 2) projector that transforms the encoded visual feature to list of visual tokens that is aligned into the LLM textual embedding space, and 3) transformer-decoder-based LLM that 3 takes both textual and visual tokens as input and uses its language modeling head to predict the next token. Note that we simply use linear layer instead of video-level or sliding-window Qformers [17, 23] as the projector, as in the video-text duet interaction format each frame is added to LLM input sequence independently. The only difference in model structure between our MMDuet and existing VideoLLMs is that we add two more heads in addition to the language modeling head (LM Head) of the LLM, namely the informative head and the relevance head, for determining whether to start response after each frame. Each head is linear layer and has weight with shape 2, where is the hidden size of the used LLM. Each head takes the final layer hidden state of the last visual token of each frame as input, and performs binary classification. To be specific, 1) the informative head is designed to predict how much new information is acquired upon viewing the current frame. If the model can obtain significant amount of new information upon viewing new frame (which we will further discuss in Sec. 5.1), it should classify this frame as TRUE category; otherwise, it should classify it as FALSE. 2) The relevance head is designed to predict whether the current frame is related to the user query. Similarly, TRUE category means to be related, while FALSE means not. We denote the probability of TRUE category of informative head and relevance head as informative score and relevance score for each sampled video frame. These two scores will be used to decide whether the model (i.e., assistant role) should interrupt the video and start its own turn. Compared with VideoLLMOnline [1] that makes this decision by predicting special token using the LM Head, our design of using two additional heads has the following merits: (1) By combining two scores we can flexibly set different criteria for response generation, rather than only relying on the logits of the special tokens; (2) The relevance head can be used to precisely perform temporal video grounding and highlight detection tasks, expanding the application scenarios of MMDuet. 4.2. Inference Procedure python-style pseudocode of the inference process is shown in Listing 1. When consuming every single sampled frame of the video, we first check if there is user query happening at this time. If yes, we first input this user turn to the model. Then the sampled frame is input to the model, after which the informative score and relevance score are calculated. In addition, we use function need response to estimate whether the model should generate an assistant response according to the informative scores and relevance scores for this frame along with previous frames. If yes, the generate function of the LLM outputs response. Different need response functions can be designed depending on the specific task, which is introduced in the ex- # Input: # # # # # Output: # system_prompt video: list of frames fps: frames per second to sample from video user_turns: list of (time, text) sorted by time model_turns: generated list of (time, text) model_turns = [] v_inf_list, v_rel_list = [], [] kv_cache = model(system_prompt) time = 0 for frame in video: if len(user_turns) and time>=user_turns[0].time: kv_cache = model(kv_cache, user_turns[0].text) user_turns = user_turns[1:] kv_cache, v_inf, v_rel = model(kv_cache, frame) v_inf_list.append(v_inf) v_rel_list.append(v_rel) if need_response(v_inf_list, v_rel_list): # informative score # relevance score kv_cache, response = model.generate(kv_cache) model_turns.append((time, response)) time += 1 / fps Listing 1. Inference Process of MMDuet periment section (Sec. 6). This process can be efficiently implemented by updating the KV Cache each time when frame or text is input or generated. 5. MMDuetIT: Dataset for Training MMDuet We build MMDuetIT, dataset for training the MMDuet model to learn to calculate the informative and relevance scores, and autonomously output replies at any necessary time in the play of the video. MMDuetIT is composed of three different types of tasks that benefit our model training: dense captioning, multi-answer grounded video question answering, and temporal video grounding. An example of the input format for each task is listed in the appendix. 5.1. Dense Captioning We use Shot2Story [7], video-text dataset with segmentlevel captions, as our dense captioning training data. Specifically, we use the 43k human-annotated subset due to its high-quality and detailed annotations. We preprocess the data to serve our purposes, and an illustration of reformatting the video segment and caption annotations to video-text duet interaction format is in Fig. 2. Choices of insertion We randomly sample position from 50% to 75% time duration for the corresponding video segment, and insert the caption at that position as model response. Here we introduce some randomness in the insertion position to prevent the model from developing bias or shortcut such as responses can only be generated at some specific positions. The earliest and latest time for inserting live broadcast of basketball game and want to track the actions of particular player in the game. This exemplifies MAGQA task: the question is What does this particular player do in the video?. When this player performs an action, the model should respond with description of this action (i.e., multiple answers) in real time manner. We believe this newly proposed MAGQA task can be widely used in real-world scenarios when users interact with livestreaming video. We construct training data for this task using GPT4o2024-08-06 [20]. Given the captions of all segments from the video as input, GPT4o is prompted to generate question related to one or more captions. For each of the segment captions, if it is related to the question, then GPT4o should also generate an answer that can be inferred from this caption. Otherwise, GPT4o should reply with Not Mentioned., and this answer is not added to the training data. We use the same insertion method of dense captioning task as described in Sec. 5.1, to insert the answers into the video stream and construct informative head labels, and the question is inserted at random place before the first answer. We also use the same insertion method to convert the human-annotated Shot2Story test set and randomly sampled 2000 examples as the test set of our MAGQA benchmark in Sec. 6.3. Therefore, this dataset contains 36834 examples in the train set and 2000 examples in the test set. We name it as Shot2Story-MAGQA-39k and show its statistics in Tab. 1. Data Quality Assessment We sample 100 examples (with 290 answers) from our test set for manual quality assessment. Among the sampled examples, we find 1 example with question unanswerable from the video, 5 examples have 6 answers (2.1%) that contradict the video content, and 5 examples have 7 answers (2.4%) unrelated to the question. Overall, manual quality assessment shows that above 95% data of our test set belongs to the high quality, which confirms the potential value of using Shot2Story-MAGQA-39k to benchmark models. The reason for the high quality is when the video captions are provided, generating questions and answers based on these text captions is very simple task for advanced LLMs like GPT4o. However, we also find that in 21 examples, the video contains additional information that is not covered in the answers. This is because some questions are very general, like What scene is the video displaying?, and describing scenes in videos elaborately has been long-lasting challenge for annotating video datasets. Figure 2. Example of reformatting the annotation of video segment to video-text duet interaction format in MMDuetIT. Information from the original annotation is emphasized with underlines. responses, i.e., at the 50% and 75% place of segment duration, are empirically chosen, as it works well in our preliminary study. We avoid inserting responses too early like in the first half of duration, because it is unfeasible to generate responses related to this video segment at very starting point. It is reasonable that some further observations are required to gain more comprehensive understanding of it. We also avoid inserting responses too late like in the last one-fourth duration, as we hope the model to output response as soon as it has sufficient understanding of the segment, rather than wait until the disappearance of the segment. It thereby improves the timeliness of the whole interaction between users and videos, especially when the user can still watch the segment as well as perceive the content of the model response talking about it. Creating informative labels We also create labels for the informative head in dense captioning tasks. According to the previous paragraph, the model can not have comprehensive understanding of this video segment until it has viewed sufficient portion of the segment (50% in this case). Meanwhile, once the caption has been generated as model response, we assume that the remaining frames in this video segment no longer provide new information that is not covered in the caption. Therefore, we set the informative heads label to TRUE for frames between 50% of this segment and the insertion point of the response, and set labels to FALSE for the other frames, as shown in Fig 2. To adapt to long video input, we also select videos with 2 to 4 minutes in length from COIN [27] as dense captioning task to MMDuetIT. The annotations in COIN are reformatted using the same method as Shot2Story. 5.2. Multi-Answer Grounded Video QA 5.3. Temporal Video Grounding An important application scenario for the video-text duet interaction format is multi-answer grounded video questionanswering (MAGQA). Consider when we are watching We also add DiDeMo [8], HiRESTgrounding [38] and QuerYD [19], three temporal video grounding tasks in MMDuetIT. Note that these data are used only for training the num examples 36834 2000 answers per video 2.96 3.04 words per ques./ans. 7.75/12.17 7.77/12.17 video seg. len (sec) 4.22 4.28 Train Test Table 1. Dataset statistics of Shot2story-MAGQA-39k. relevance head, which is designed for performing temporal video grounding tasks and judging the relevance between the question and the video for QA tasks. The query is first added at the beginning of the input sequence. For frames that are annotated as relevant to the query, we set the relevance heads label to TRUE; otherwise, we set it to FALSE. 5.4. Dataset Statistics The data distribution of MMDuetIT is shown in Fig. 3. Note that this dataset only contains 109k examples, which is relatively small compared to modern post-training datasets like [15, 17, 31]. The reason is that due to computational resource constraints, we plan to demonstrate the feasibility of our proposed video-text duet interaction format by finetuning state-of-the-art VideoLLM. We assume that the used backbone model already possesses enough video comprehension capabilities. By using small dataset, we aim to train this model to efficiently adopt this new interaction with minimum catastrophic forgetting of its existing abilities. 6. Experiments Implementations MMDuet is initialized with LLaVAOneVision [15]. We train the model on MMDuetIT for one epoch. The training takes about one day on node with 8 Tesla V100 GPUs, and the inference runs on 1 Tesla V100 GPU. Implementation details are listed in the appendix. Baselines As MMDuet mainly focuses on time-sensitive video tasks, we use the following baselines that are able to represent time spans in videos by different representation formats: TimeChat [23], VTimeLLM (7B) [10], HawkEye [31] and VTG-LLM [6]. Since the initialization of MMDuet is stronger than that of the baselines, for fair comparison we also conduct controlled experiment in which the only difference is the interaction format. Specifically, we use the same initialization model (LLaVA-OneVision), training data (MMDuetIT) and training schedule, but reformat the data to the respective interaction formats and video segment representation formats used by TimeChat and VTimeLLM to train two baseline models. We refer to these models as LLaVA-OV-TC and LLaVA-OV-VT. 6.1. Highlight Detection and Temporal Video"
        },
        {
            "title": "Grounding",
            "content": "We use highlight detection and temporal video grounding to evaluate the performance of the relevance head of MMDuet. Baseline models are required to generate list of float numbers to represent the relevance score for each clip in QVHighlights [14], and start and end time for the relevant video span in Charades-STA. However, for LLaVA-OV-TC and LLaVA-OV-VT, despite using different prompts as input, we were still unable to instruct the model to output sequence of scores as in [23]. Therefore, we follow the method of Charades-STA to instruct the model to output related span, and assign the score to 1 for clips within this span and 0 otherwise. MMDuet uses the relevance score min-max normalized to [0, 1] as the score in QVHighlights, and to classify whether this frame is relevant and calculate frame-level IoU in Charades-STA. Since the relevance head provides relevance score immediately after each frame, its prediction cannot leverage the context from subsequent video frames. To mitigate this limitation, we smooth the relevance score sequence. Specifically, we set each frames smoothed relevance score as the mean value of its original score, the relevance scores of the preceding frames and the following frames, where is the window size. We set = 2 for QVHighlights and = 6 for Charades-STA. Results are shown in Tab. 2. We observe that, compared to the baselines, MMDuet exhibits significantly greater improvement in performance on QVHighlights. This indicates that traditional VideoLLMs struggle with generating long sequence of relevance scores using text-based form or identifying multiple related video segments in its text-based responses, whereas MMDuets approach of directly assigning relevance scores to each frame circumvents this issue. Parameter sensitivity of Though the is empirically set for the results in Tab. 2, in Fig. 4 we show that within fairly large range of w, MMDuet consistently outperforms all baseline models. It confirms that MMDuet is robust to these tasks with respect to the window size. 6.2. Dense Video Captioning We test dense video captioning performance on YouCook2 [41], challenging task that requires models to output the caption, start point and end point for about 8 steps in minutes-long cooking video. Baseline models output the start time, end time and caption for each step in the textbased form. For MMDuet, since this task requires the model to continuously identify important actions from the video and output periodically, we employ heuristic method to determine whether model response should be output after each frame (need response function in Sec. 4.2). We TimeChat VTimeLLM HawkEye VTG-LLM LLaVA-OV-TC LLaVA-OV-VT MMDuet (Ours) QVHighlights mAP/HIT@1 14.5/23.9 - - 16.5/33.5 17.6/32.9 19.0/40.0 31.3/49.6 Charades-STA R@0.5/0.7 32.2/13.4 31.2/11.4 31.4/14.5 33.8/15.7 33.1/12.4 36.5/12.3 42.4/18.0 Figure 3. Data Distribution of MMDuetIT. Table 2. Zero-shot performance on highlight detection and temporal video grounding. TimeChat VTG-LLM LLaVA-OV-TC LLaVA-OV-VT MMDuet (Ours) + rm. prev. resp. YouCook2 SODAc/CIDEr/F1 1.2/3.4/12.6 1.5/5.0/17.5 1.9/3.3/21.8 2.5/6.7/14.0 2.4/5.7/19.2 2.9/8.8/21.7 Table 3. Zero-shot performance on dense video captioning. Figure 4. Performance on temporal video grounding and highlight detection with different w. sum up the informative score for each frame as the video plays. When the sum reaches threshold (we set = 2), the model generates response right after this frame as the caption for that step, and then we reset the sum to 0 to start new round of sum. However, MMDuet cannot directly predict when step starts or ends just by this video-text duet interaction format, as the model is unable to determine whether frame is the beginning of step without observing enough subsequent content. To get the start and end time for each step as required by this task, we adopt simple workaround: we use the time of the previous response and the current response as the start time and end time for step. If two adjacent steps have the same caption, we merge them into one step. It has been long-lasting problem that LLMs tend to repeat previously-generated content [35], and we find that this problem is especially severe in dense video captioning. It indicates that VideoLLMs are probably generating captions relying on text shortcuts rather than the video content. We have attempted common solutions such as repetition penalty [12], which though is still sub-optimal. Since the responses from MMDuet are separated across multiple turns, we find that simply removing previously generated turns from the context (rm. prev. resp. for short) by not appending their Figure 5. Performance on dense video captioning with different s. attention keys and values to the KV Cache alleviates this issue, leading to significant improvement in performance. As shown in Tab. 3, MMDuet does not show significant improvements on F1 metric, likely due to the simple solution we use to derive the start and end time based on responses. Even so, the CIDEr and CODA metric (inaccurate predicted time spans can have negative effects on these metrics) of MMDuet is still higher than all baselines, indicating that MMDuet outperforms baselines in terms of text quality, possibly due to its facilitation to information retrieval discussed in Sec. 1. Parameter sensitivity of We report the performance with threshold from 1 to 3 in Fig. 5. Results show that the performance is quite robust across wide range of s, and we simply choose = 2 in Tab. 3. Note that we can use different to suit various downstream tasks especially in such zero-shot setting. 6.3. Multi-Answer Grounded Video QA To align closely with the widely-used streaming video comprehension scenario, we propose MAGQA that requires model to generate answers at multiple necessary positions of video. Different from conventional Video QA in which one question corresponds to only one answer, In MAGQA, question corresponds to multiple turns of answers, and these 7 Model RealTime? In-Span Score LLaMA/GPT # turns (w/o. / w/. dedup) time per example Baselines LLaVA-OV-TC LLaVA-OV-VT MMDuet (Ours) = 0.6 = 0.5 = 0.4 = 0.3 2.92/2.79 2.94/2. 2.46/2.33 2.77/2.61 3.00/2.81 3.13/2.93 3.4/1.9 5.4/2.2 13.7/4.0 18.4/5.3 23.0/6.6 27.0/7.6 0.76 1.00 1.80 2.23 2.59 2.73 Table 4. Results on the test set of Shot2Story-MAGQA-39k with the rm. ass. turns method used. For the time per example column, the time used by LLaVA-OV-VT is set to 1, and the times for other rows are set as multiples of the time used by LLaVAOV-VT. turns are derived from different video segments. Therefore, this task requires the response to be accurate and in time. We introduce an in-span score metric, which takes both the text content and reply time of the predicted answer into account to evaluate the models performance on this task. Suppose the model prediction has answers, each answer has prediction time timep and prediction text predp, = 1, 2, . . . , . The ground truth has answers, each answer has ground truth start time startq, ground truth end time endq, and ground truth text goldq, = 1, 2, . . . , Q. First, we use an LLM to calculate relevance score from 1 to 5 between each answer in prediction predp and ground truth goldq: = {sp,q} RP Q. For each ground truth answer q, we select the predicted answers with predicted time in ground truth time span: Pq = {p timep [startq, endq]}, and use the average score between the ground truth answer and the selected predicted answers as the score for this ground truth answer: scoreq = 1 if Pq > 0. If Pq = 0 (no Pq predicted answer falls in this ground truth span), scoreq is set to 1. Finally, we calculate the average score of all ground truth answers as the final in-span score of this example: in span score = 1 q=1 scoreq. GPT-4o-2024-08-06 [20] is used as the scorer LLM for in-span score. To prevent reproducibility issues due to potential changes of OpenAI API, we also report the in-span score obtained using LLaMA 3.1 70B Instruct [2]. (cid:80)Q sp,q pPq (cid:80) As MAGQA requires the answers to be both informative and related to the question, we set the need response as: if the sum of informative score and relevance score of frame is larger than threshold t, then the model needs to generate response right after this frame. We also use the rm. prev. resp. method in dense video captioning task introduced in Sec. 6.2. As baseline models are not capable of generating responses at specific positions in the video, we employ an output format the same as dense video cap8 Model MMDuet w/o rand. resp. pos. w/o multi informative YouCook2 2.9/8.8/21.7 2.1/7.3/19.0 2.9/8.0/16.5 Table 5. Ablation study on training methods. tioning, i.e., output the start time, end time, and predicted text for each turn after watching the entire video in both training and testing, and use the average of the start and end time as the response time. For the cases that baseline models directly reply with an answer without corresponding time, we pair this predicted answer with all ground truth answers. Note that this is simplified requirement than that of MMDuet, as the MAGQA task simulates streaming video comprehension application scenario, which requires the model to respond as soon as the video plays to segments relevant to the question, which ensures that users can see the responses timely, rather than waiting until the entire video concludes before generating replies. Results are shown Tab. 4. We provide results for different as it represents trade-off between inference time and performance: as decreases from 0.6 to 0.3, the performance of MMDuets real-time replies continuously rise and even outperform baselines with simplified setting of providing non-real-time replies after watching the entire video. However, this is achieved at cost of generating lots of duplicate replies with around 2.5 inference time. 6.4. Ablation Studies We conduct ablation studies on YouCook2 dense video captioning to assess two empirical yet important findings for effectively training the informative head in data construction: randomly inserting the response at position from 50% to 75% of the corresponding video segment (rand. resp. pos.), and setting informative heads label to TRUE for all frames between 50% of the segment and the response time (multi resp. pos. is disabled, the informative). When rand. response is always inserted at the end of the corresponding segment. When multi informative is disabled, only the informative label of the frame right before the response is set as TRUE. As illustrated in Tab. 5, disabling either method negatively impact MMDuets performance, which shows the importance of carefully handling the response time and informative labels. 7. Limitations and Future Works As preliminary and pioneering effort of video-text duet interaction format, we acknowledge that there is much room for improvement which should be addressed in future research: the need response criterion) are required during inference. (1) Some hyperparameters (e.g., Though the parameter sensitivity test shows that this criterion is quite robust across different thresholds, there still exists unknowns for applying MMDuet on new tasks. (2) Information from subsequent frames is not incorporated when generating in-time responses for the current frame, especially for the live-streaming video that indeed has unpredictable future frames. It can be crucial in some scenarios, such as determining the start of an action. (3) Slow inference speed. better inference process is needed to improve the parallelism of video inputs and avoid generating duplicate responses. (4) Real-time response datasets with longer live-streaming videos are required to be collected to better fit the real-world application scenarios. 8. Conclusion In this paper, we formalize the video-text duet interaction format, which addresses the shortcomings of existing interaction formats in terms of time-sensitive task performance, more application scenarios and in-time information retrieval for long videos. We collect MMDuetIT, dataset for training models to follow the video-text duet interaction format. Based on MMDuetIT we train MMDuet, model with significant improvements on various time-sensitive tasks, and is able to generate responses in real time. We believe such improvements can be substantial step towards building powerful and useful video comprehension systems."
        },
        {
            "title": "References",
            "content": "[1] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1840718418, 2024. 3, 4 [2] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab A. AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guanglong Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, 9 Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Laurens Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline C. Muzzi, Mahesh Babu Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melissa Hall Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Chandra Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Ben Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, ShangWen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank J. Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory G. Sizov, Guangyi Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Andrei Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xia Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models. ArXiv, abs/2407.21783, 2024. 1, [3] J. Gao, Chen Sun, Zhenheng Yang, and Ramakant Nevatia. Tall: Temporal activity localization via language query. 2017 IEEE International Conference on Computer Vision (ICCV), pages 52775285, 2017. 2, 1 [4] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh K. Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Z. Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Kahsay Gebreselasie, Cristina Gonzalez, James M. Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran K. Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David J. Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard A. Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1897318990, 2021. 3 [5] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, Chunjing XU, and Hang Xu. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. In Advances in Neural Information Processing Systems, pages 2641826431. Curran Associates, Inc., 2022. 3 [6] Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Xi Chen, and Bo Zhao. Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding. ArXiv, abs/2405.13382, 2024. 3, 6 [7] Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng Wang. Shot2story20k: new benchmark for comprehensive understanding of multi-shot videos. ArXiv, abs/2312.10300, 2023. 4, 1 [8] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan C. Russell. Localizing moments in video with natural language. 2017 IEEE International Conference on Computer Vision (ICCV), pages 5804 5813, 2017. 2, 5, 1 [9] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 1 [10] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1427114280, 2023. 3, 6 [11] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023. 1 [12] Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. Ctrl: conditional transformer language model for controllable generation. ArXiv, abs/1909.05858, 2019. 7 [13] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. 2017 IEEE International Conference on Computer Vision (ICCV), pages 706715, 2017. [14] Jie Lei, Tamara L. Berg, and Mohit Bansal. Qvhighlights: Detecting moments and highlights in videos via natural language queries. ArXiv, abs/2107.09609, 2021. 2, 6, 1 [15] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. ArXiv, abs/2408.03326, 2024. 1, 3, 6 [16] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. ArXiv, abs/2407.07895, 2024. [17] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2219522206, 2023. 4, 6 [18] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. ArXiv, abs/2404.00308, 2024. 1, 3 [19] Andreea-Maria Oncescu, Joao F. Henriques, Yang Liu, Andrew Zisserman, and Samuel Albanie. Queryd: video dataset with high-quality text and audio narrations. ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 22652269, 2021. 5, 1 [20] OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt4o/, 2024. Accessed: 2024-11-13. 5, 8 [21] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ Howes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. ArXiv, abs/2304.07193, 2023. 1 [22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 1 [23] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1431314323, 2023. 3, 4, 6 [24] Eli Schwartz, Leshem Choshen, Joseph Shtok, Sivan Doveh, Leonid Karlinsky, and Assaf Arbelle. Numerologic: Number encoding for enhanced llms numerical reasoning. ArXiv, abs/2404.00459, 2024. 2, [25] Zhihong Shao, Damai Dai, Daya Guo, Bo Liu (Benjamin Liu), Zihan Wang, and Huajian Xin. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. ArXiv, abs/2405.04434, 2024. 1 [26] Quan Sun, Yuxin Fang, Ledell Yu Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. ArXiv, abs/2303.15389, 2023. 1 [27] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: large-scale dataset for comprehensive instructional video analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 3, 5, 1 [28] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. 1 [29] Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu [40] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. 2024. 1, 3 [41] Luowei Zhou, Chenliang Xu, and Jason J. Corso. Towards automatic learning of procedures from web instructional videos. In AAAI Conference on Artificial Intelligence, 2017. 2, 6, 1 Huang. Grounded-videollm: Sharpening fine-grained temporal grounding in video large language models. 2024. 3 [30] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, Yansong Shi, Tianxiang Jiang, Songze Li, Hongjie Zhang, InYifei Huang, Yu Qiao, Yali Wang, and Limin Wang. ternvideo2: Scaling video foundation models for multimodal video understanding. ArXiv, abs/2403.15377, 2024. 1 [31] Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video-text llms for grounding text in videos. ArXiv, abs/2403.10228, 2024. 3, 6 [32] Yuxuan Wang, Yueqian Wang, Pengfei Wu, Jianxin Liang, Dongyan Zhao, and Zilong Zheng. Efficient temporal extrapolation of multimodal large language models with temporal grounding bridge. 2024. 1, 3 [33] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng. Videollamb: Long-context video understanding with recurrent memory bridges. ArXiv, abs/2409.01071, 2024. 2 [34] Junbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question answering. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1320413214, 2023. 2 [35] Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. ArXiv, abs/2206.02369, 2022. [36] Zenan Xu, Xiaojun Meng, Yasheng Wang, Qinliang Su, Zexuan Qiu, Xin Jiang, and Qun Liu. Learning summaryworthy visual representation for abstractive summarization in video. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23, pages 52425250. International Joint Conferences on Artificial Intelligence Organization, 2023. Main Track. 3 [37] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-Yang Chen, Kexin Yang, Mei Li, Min Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yunyang Wan, Yunfei Chu, Zeyu Cui, Zhenru Zhang, and Zhi-Wei Fan. Qwen2 technical report. ArXiv, abs/2407.10671, 2024. 1, 3 [38] Abhaysinh Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Ouguz, Yasher Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2305623065, 2023. 5, 1 [39] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1194111952, 2023. 1 12 VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Details of Training MMDuet A.1. Training Hyperparameters. LLaVA-OneVision uses SigLIP-Large [39] as the vision encoder, and converts an image with 384 384 into 24 24 = 576 tokens. In the official settings of LLaVA-OneVision [15], when encoding videos, the visual tokens corresponding to each frame are spatially downsampled to 12 12 = 144 tokens using pooling operation with size of 2. However, this number of tokens is also too large when training and inference with long videos. To address this, we further modified the pooling size to 4, resulting in 77 = 49 tokens per frame. We set the maximum number of frames sampled from each video to 120 in the training process, which is constrained by the memory of our GPUs. The sampling frame rates are set to different numbers for different video sources to ensure that for the vast majority (>90%) of videos, video length (in seconds) sampled frame per second (fps) 120. For the videos that are too long, we only keep the first 120 frames (and the conversation turns that are inserted within the first 120 frames), and discard the subsequent contents. Specifically, the sampled frame per second (fps) is set as: 2 for videos from Shot2Story [7] and DiDeMo [8], 0.5 for COIN [27] and QueryD [19], and 0.33 for HiRESTgrounding [38]. The projector, the relevance head, the informative head and LoRA [9] weights of the LLM (add to all attention proj. layers and FFN layers) are trained, while other parameters of the model are frozen. More training hyperparameters are listed in Tab. 6. A.2. Inference Settings Videos from different sources are also sampled with different fps during inference. Specifically, we set the maximum number of frames sampled from each video to 400, and fps to 2 for videos from Shot2Story [7] and Charades-STA [3], 1 for videos from QVHighlights [14], and 0.5 for videos from YouCook2 [41]. For few videos in YouCook2 that are even longer than 400(frames) 0.5(fps) = 800 seconds, we uniformly sample 400 frames from this video to ensure that information from the latter part of the video is not truncated. This inference setting is consistent across MMDuet, LLaVA-OV-TC, and LLaVA-OV-VT. 1 Hyper-parameter batch size gradient acc steps learning rate warmup ratio lora lora alpha attn implementation value 1 8 2e-5 0.05 16 32 sdpa Table 6. Hyper-parameters used for training MMDuet. B. Example Inputs for Each Task in MM-"
        },
        {
            "title": "DuetIT",
            "content": "Example inputs for each task for training and inference are listed in Tab. 7. The dense video captioning user input is selected from one of the following sentences: Please concisely narrate the video in real time. Help me to illustrate my view in short. Please simply describe what do you see. Continuously answer what you observed with simple text. Do concise real-time narration. Hey assistant, do you know the current video content? Reply me concisely. Simply interpret the scene for me. What can you tell me about? Be concise. Use simple text to explain what is shown in front of me. What is the action now? Please response in short. The temporal video grounding user input is selected from one of the following sentences (where %s denotes the caption to localize): %s What segment of the video addresses the topic %s? At what timestamp can find information about %s in the video? Can you highlight the section of the video that pertains to %s? Which moments in the video discuss %s in detail? Identify the parts that mention %s. Where in the video is %s demonstrated or explained? What parts are relevant to the concept of %s? Which clips in the video relate to the query %s? Can you point out the video segments that cover %s? What are the key timestamps in the video for the topic %s? <im start>system multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant. <im end> <im start>user (A Dense Video Captioning Query)<im end> <im start>stream <frame><frame><frame> ...<im end> <im start>assistant person pulls knife from black bag.<im end> <im start>stream <frame><frame><frame> ...<im end> <im start>assistant man in hat and red clothes speaks with dagger, and tree behind him.<im end> <im start>stream <frame><frame><frame> ...<im end> . . . (More stream and assistant turns) <im start>system multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant. <im end> <im start>stream <frame><frame><frame> ...<im end> <im start>user What happens during the basketball game?<im end> <im start>stream <frame><frame><frame> ...<im end> <im start>assistant Several players in white jerseys are celebrating by high-fiving each other.<im end> <im start>stream <frame><frame><frame> ...<im end> <im start>assistant player in white jersey makes successful shot.<im end> <im start>stream <frame><frame><frame> ...<im end> . . . (More stream and assistant turns) <im start>system multimodal AI assistant is helping users with some activities. Below is their conversation, interleaved with the list of video frames received by the assistant. <im end> <im start>user (A Temporal Video Grounding Query)<im end> <im start>stream <frame><frame><frame> ...<im end> Table 7. Input examples of different tasks during the training and evaluation phase of MMDuet."
        },
        {
            "title": "Temporal\nVideo\nGrounding",
            "content": "C. Qualitative Study We list some examples of dense video captioning on videos with several minutes in length and contains many actions in Figs. 6 to 8, and examples of multi-answer grounding video question answering (MAGQA) in Figs. 9 to 11. For LLaVAOV-TC and LLaVA-OV-VT, we directly list their generated outputs. For MMDuet, we list the numerical order (in round brackets), time (in square brackets) and content (in the second line) for each turn. If line contains multiple numerical orders and times, this indicates that these turns have the same content, which is shown in the following line. To help readers to identify the position of these turns within the video, we also annotate the numerical order of the turns at the corresponding timestamps in the video stream. When handling long videos for dense video captioning, baseline models often recall only part of the video or gener2 ate repeated content, failing to provide complete description of all steps in the video. In contrast, MMDuet, due to its ability to focus only on small portion of the video content preceding each generation step and using the rm. prev. turns trick to avoid interference from previous turns, can provide more accurate and detailed video descriptions. For the MAGQA task, due to the relatively short video length, baseline models can also locate video segments and answer questions effectively. The advantage of MMDuet in this task is its ability to provide answers in real-time manner. 3 Figure 6. An example of dense video captioning with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT. 4 Figure 7. An example of dense video captioning with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT. 5 Figure 8. An example of dense video captioning with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT. 6 Figure 9. An example of multi-answer grounded video question answering with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT. 7 Figure 10. An example of multi-answer grounded video question answering with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT. 8 Figure 11. An example of multi-answer grounded video question answering with MMDuet, LLaVA-OV-TC and LLaVA-OV-VT."
        }
    ],
    "affiliations": [
        "Beijing Institute for General Artificial Intelligence",
        "Huawei Noahs Ark Lab",
        "National Key Laboratory of General Artificial Intelligence",
        "Wangxuan Institute of Computer Technology, Peking University"
    ]
}