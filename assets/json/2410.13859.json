{
    "paper_title": "$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models",
    "authors": [
        "Yaxin Luo",
        "Gen Luo",
        "Jiayi Ji",
        "Yiyi Zhou",
        "Xiaoshuai Sun",
        "Zhiqiang Shen",
        "Rongrong Ji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains a barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of ``activated tokens''. Our key insight is that if most tokens are redundant for the layer computation, then can be skipped directly via the MoD layer. However, directly converting the dense layers of MLLMs to MoD layers leads to substantial performance degradation. To address this issue, we propose an innovative MoD adaptation strategy for existing MLLMs called $\\gamma$-MoD. In $\\gamma$-MoD, a novel metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of attention maps (ARank). Through ARank, we can effectively identify which layer is redundant and should be replaced with the MoD layer. Based on ARank, we further propose two novel designs to maximize the computational sparsity of MLLM while maintaining its performance, namely shared vision-language router and masked routing learning. With these designs, more than 90% dense layers of the MLLM can be effectively converted to the MoD ones. To validate our method, we apply it to three popular MLLMs, and conduct extensive experiments on 9 benchmark datasets. Experimental results not only validate the significant efficiency benefit of $\\gamma$-MoD to existing MLLMs but also confirm its generalization ability on various MLLMs. For example, with a minor performance drop, i.e., -1.5%, $\\gamma$-MoD can reduce the training and inference time of LLaVA-HR by 31.0% and 53.2%, respectively."
        },
        {
            "title": "Start",
            "content": "γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models γMOD: EXPLORING MIXTURE-OF-DEPTH ADAPTATION FOR MULTIMODAL LARGE LANGUAGE MODELS Yaxin Luo1, Gen Luo2, Jiayi Ji3,4, Yiyi Zhou3, Xiaoshuai Sun3, Zhiqiang Shen5, Rongrong Ji3 1Technical University Of Denmark 2OpenGVLab, Shanghai AI Laboratory 3Xiamen University 4National University of Singapore 5MBZUAI 4 2 0 2 7 1 ] . [ 1 9 5 8 3 1 . 0 1 4 2 : r Project Page: Gamma-MOD"
        },
        {
            "title": "ABSTRACT",
            "content": "Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of activated tokens. Our key insight is that if most tokens are redundant for the layer computation, then can be skipped directly via the MoD layer. However, directly converting the dense layers of MLLMs to MoD layers leads to substantial performance degradation. To address this issue, we propose an innovative MoD adaptation strategy for existing MLLMs called γ-MoD. In γ-MoD, novel metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of attention maps (ARank). Through ARank, we can effectively identify which layer is redundant and should be replaced with the MoD layer. Based on ARank, we further propose two novel designs to maximize the computational sparsity of MLLM while maintaining its performance, namely shared vision-language router and masked routing learning. With these designs, more than 90% dense layers of the MLLM can be effectively converted to the MoD ones. To validate our method, we apply it to three popular MLLMs, and conduct extensive experiments on 9 benchmark datasets. Experimental results not only validate the significant efficiency benefit of γ-MoD to existing MLLMs but also confirm its generalization ability on various MLLMs. For example, with minor performance drop, i.e., -1.5%, γ-MoD can reduce the training and inference time of LLaVA-HR by 31.0% and 53.2%, respectively."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent years have witnessed the great success of large language models (LLMs) in natural language processing (NLP) (Achiam et al., 2023; Touvron et al., 2023; Cai et al., 2024b), which attracts increasing attentions in extending LLMs to vision-language (VL) tasks. Despite the progress, recent multimodal large language models (MLLMs) (Liu et al., 2024d;c; Chen et al., 2024a; Alayrac et al., 2022) are often criticized by their expensive computational costs. For example, the inference speed of existing MLLMs like LLaVA-HR (Luo et al., 2024b) is still far from practical requirements, e.g., 4.7 samples per second. Driven by the progress of NLP, recent advances have employed the mixture-of-experts (MoEs) (Lin et al., 2024a; Jiang et al., 2024) to MLLMs to reduce the activated parameters, thus achieving trade-off between efficiency and performance. Orthogonal to MoEs, we aim to tackle the efficiency bottleneck of MLLMs from the perspective of activated tokens. As shown in Fig. 1 (a), large number of tokens are less important in the computation, such as visual background and prepositional words. However, existing MoEs still allocate the same experts to all input tokens, leading to redundant computational costs. promising solution to this issue is the recently proposed mixture-of-depths (MoDs) in NLP (Raposo et al., 2024), which equips each token with router to determine whether module should be computed. Corresponding author. 1 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Figure 1: Visualization of attention maps in the MLLM and comparison of MoE with MoD. (a) Lower-rank layers often exhibit redundancy in their attention computation. (b) Different from MoE, MoD achieves the computational sparsity from the perspective of activated token, where the computational budget is dynamically allocated to each token. However, recent MoDs (Raposo et al., 2024) typically require pre-training LLMs from scratch, and their employment on MLLMs still remains under-explored. In this paper, we focus on the efficient adaptation of MoDs to existing MLLMs. In particular, our goal is to maximize the computational sparsity of MLLMs while maintaining competitive performance. However, directly converting all dense layers of MLLMs to MoD layers leads to significant performance degradation, e.g., -33.3% of LLaVA-HR (Luo et al., 2024b) on TextVQA (Singh et al., 2019). In practice, we observe that such issue is mainly caused by two aspects. Firstly, the deployment of MoDs lacks practical guidance to measure the layer redundancy, thus undermining the necessary dense layers. As illustrated in Fig. 1 (a), attention patterns vary significantly across layers, and some layers exhibit less redundancy. Additionally, the setting of MLLMs, e.g., input modality, differs substantially from that of LLMs, making the direct adaptation of MoDs suboptimal. To overcome these limitations, we first propose novel metric to guide the deployment of MoDs in MLLMs, called the rank of attention maps (ARank). Our key insight is that lower-rank attention maps indicate that fewer tokens are necessary for computation. As shown in Fig. 1 (a), most of tokens of Layer-4 are assigned small attention weights, contributing minimally to the final output. This provides valuable hint for us to replace the redundant layer with the MoD one under the guidance of ARank. In practice, the calculation of ARank is both efficient and flexible. Empirically, we find that the average ARank always keeps the similar despite the change of samples. Therefore, randomly sampling small amount of data can already accurately estimate the ARanks. Based on the ARank, we propose an innovative MoD adaptation strategy for existing MLLMs, called γ-MoD. Specifically, γ-MoD is plug-and-play approach that can be seamlessly integrated into current MLLMs via instruction tuning. In γ-MoD, two novel designs are adopted to maximize its benefits to MLLMs, namely shared vision-language router and masked routing learning. The shared vision-language router performs routing on the entire multimodal sequence and uses weight-sharing strategy to facilitate optimization. Then, masked routing learning is introduced to prevent critical tokens from being skipped during training, i.e., instruction tokens. With these designs, over 90% of dense layers can be converted to MoD layers with minimal performance sacrifice, resulting in even larger computational sparsity than the native MoD-based LLM (Raposo et al., 2024). To validate γ-MoD, we apply it to two popular MLLMs and conduct extensive experiments on 9 vision-language benchmarks. Experimental results show that γ-MoD significantly improves the training and inference efficiency of existing MLLMs while keeping their performance competitive. For example, γ-MoD reduces 51.6% Flops, 31% training time and 53.2% inference time for LLaVAHR (Luo et al., 2024b), but its average performance decline is only -1.5%. More importantly, the great generalization ability of γ-MoD is also witnessed on different MLLM structures and parameter sizes. Overall, the contribution of the paper can be summarized in three folds: γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models We present novel mixture-of-depth (MoD) framework for the sparse computation of existing MLLMs, namely γ-MoD, which can seamlessly convert most dense layers in MLLMs to the sparse MoD layers. We propose an innovative metric to measure the layer redundancy, namely rank of attention maps (ARank). With ARank, we can best determine that which dense layer should be convert to the MoD one. We carefully explore the design of γ-MoD in existing MLLMs, including the shared visionlanguage router and the masked routing learning, which can achieve up to 51.6% computational sparsity with minor performance sacrifice. Extensive experiments also confirm the generalization ability of γ-MoD."
        },
        {
            "title": "2.1 MULTIMODAL LARGE LANGUAGE MODELS",
            "content": "Large language models (LLMs) (Achiam et al., 2023; Touvron et al., 2023; Jiang et al., 2024; Almazrouei et al., 2023; Cai et al., 2024b; Abdin et al., 2024; Shen et al., 2023) have proven their strong capabilities in various natural language processing tasks (Paperno et al., 2016; Fyodorov et al., 2000; Reddy et al., 2019; Ziegler et al., 2019). Motivated by this, numerous efforts (Liu et al., 2024d; Bai et al., 2023a; Ye et al., 2023; Dai et al., 2023; Chen et al., 2024b; Li et al., 2024b; Tong et al., 2024; Rasheed et al., 2024; Dong et al., 2023; Xie et al., 2024; Zhou et al., 2024; Chen et al., 2023; Alayrac et al., 2022; Sun et al., 2024) have been devoted into extending LLMs to multimodal large language models (MLLMs). Among them, the most representative work is LLaVA (Liu et al., 2024d), which uses lightweight project to connect visual encoder and an LLM. This simple framework has now become the de-facto paradigm in the community, empowering set of MLLMs like Mono-InternVL (Luo et al., 2024a), Mini-Gemini (Li et al., 2024b) and InternVL (Chen et al., 2024b). Recently, researchers have shifted their attentions to high-resolution MLLMs. For example, LLaVA-NexT (Liu et al., 2024c) and InternVL-1.5 (Chen et al., 2024a) adopt the dynamic image slicing strategy for high-resolution adaptation. LLaVA-HR (Luo et al., 2024b) further propose dual-branch structure to reduce the cost of high-resolution MLLMs. Despite the effectiveness, existing high-resolution MLLMs (Liu et al., 2024c; Li et al., 2024a) will produce much longer input tokens, resulting in prohibitively expensive computational costs. In this paper, the proposed γ-MoD can greatly overcome the efficiency bottleneck of existing MLLMs, which is significant for their practical applications. 2.2 SPARSE COMPUTATION FOR LLMS Existing LLMs has grown rapidly in their parameter scale, which results in ever-increasing computational costs (Dubey et al., 2024; Liu et al., 2024f; Yang et al., 2024; Pal et al., 2024; Adler et al., 2024). Therefore, an influx of attentions have been focused on the sparse computation of LLMs. Specifically, the mixture of experts (MoEs) are the most popular technology in the community (McKinzie et al., 2024; Cai et al., 2024a; Xue et al., 2024), which dynamically activates part of expert networks for each token, thereby achieving trade-offs between capability and efficiency. For example, Mixtra-87B (Jiang et al., 2024) and DeepSeekMoE (Dai et al., 2024) replace the feed-forward(FFN) layer of transformer block by an MoE Layer and the input tokens will be dynamically processed by top-K experts via the router. Orthogonal to MoE, Raposo et al. (2024) proposed the mixture of depths (MoDs) to dynamically allocate computations for each token. Compared to MoE, the main principle of MoD is to reduce the activated tokens instead of the activated parameters. This paradigm has shown great potentials for the sparse computation of LLMs, but its potential on MLLM is still under exploration. In the existing literature, most existing works aim to adapt MoEs to MLLMs. For instance, MoE-LLaVA (Lin et al., 2024a) proposed novel approach to convert dense MLLM to mixture-of-expert structure. However, these methods often require additional training costs to realize the adaptation. Orthogonal to these works, we are the first to explore MoDs on MLLMs, which can seamlessly realize sparse computations for exiting MLLMs. 3 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Figure 2: Illustration of our γ-MoD adaptation on LLaVA-HR. γ-MoD is plug-and-play approach that can be directly applied in existing MLLMs. After vision-language alignment, γ-MoD can replace most redundant layers with MoD ones via the rank-based redundancy estimation."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "We first recap the mechanism of Mixture of Experts (MoEs) and Mixture of Depths (MoDs). Mixture of experts. In particular, the main principle of MoE is to reduce the activated parameters in dense models. Existing MoE-based LLMs (Dai et al., 2024; Liu et al., 2024a; Lin et al., 2024a; Jiang et al., 2024) and MLLMs (Luo et al., 2024b; Chen et al., 2024a; Liu et al., 2024d) often contain multiple FFN modules in their layers, also termed experts. During training and inference, only few experts are activated to participate in computations, thus retaining the trade-offs between performance and efficiency. Given input features Rld, MoE mechanism can be defined by = + (cid:88) j=1 Dj(x)Rj(x). (1) Here, D() denotes the expert layer, i.e., FFN. is the number of activated experts, and Rj() is the corresponding routing function. In practice, top-k experts are selected according to their routing scores, where is much smaller than the total number of experts K. Mixture of depths. Different from MoEs, MoDs aim to improve the model efficiency via the reduction of activated tokens. Compared to MoEs, the routing mechanism of MoDs performs on input tokens, and most tokens will directly skip the dense layer in MLLMs. Thus, MoDs can be written as xj = (cid:26)xj + D(xj)R(xj) xj if R(xj) δs, if R(xj) < δs, (2) where xj Rd denotes the token vector in x, and δs is routing threshold. As defined in Eq. 2, inactive tokens will directly skip the layer D() to save the computational cost. Discussion. In existing MLLMs (Lin et al., 2024a), MoE is typically used to efficiently scale up the model size, while its computations are not directly reduced. In contrast, MoD can perform as plug-and-play module to save the cost of common dense layer, which is more significant to the efficient scenario. Unfortunately, the adaptation of MoD to existing MLLMs is still under-explored, and its practical use in LLMs also requires expensive pretraining. 4 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Figure 3: Visualization of ARank based on different tasks (left) and sample sizes (right). The horizontal axis represents the layer index of LLaVA-HR. The darker color indicates the larger ARank."
        },
        {
            "title": "4.1 OVERVIEW",
            "content": "In this paper, we propose novel method to efficiently deploy MoDs to existing MLLMs, namely γ-MoD. The core principle of γ-MoD is to identify redundant MLLM layers via novel metric called rank of attention maps (ARank) and replace them with the proposed MoD layer. Therefore, the deployment of γ-MoD in the given MLLM, i.e., FMLLM(), can be formulated by FMLLM = G0 G1 G2... Gn, if τ (Di) δτ , if τ (Di) < δτ . (cid:26)Di Si where Gi = (3) Here, G() denotes the layer of the MLLM, where S() and D() indicate the dense layer and its MoD alternative, respectively. τ () is function to estimate the redundancy of the given dense layer Di, and δτ is threshold. Given the architecture in Eq. 3, γ-MoD aims to maximize the sparsity while maintaining the performance. Thus, the optimization objective of γ-MoD can be written as: arg min θ,θr Lobj(FMLLM(x0; θ)) + s.t. (cid:88) (cid:88) i= j=1 R(xi )<δs = α. (cid:88) i= Laug(R(xi; θr)), (4) Here, Lobj and Laug denote the auto-regressive loss and the routing loss for the router R(), respectively. xi is the input tokens of i-th layer, and α is the pre-defined sparse target. )<δs {0, 1} is the indicator function, which is equal to 1 when R(xi R(xi j) < δs. 4.2 RANK-BASED REDUNDANCY ESTIMATION The key challenge of γ-MoD is how to identify the dense layer that should be converted to the MoD one. In practice, directly replacing all layers with MoD ones will lead to significant performance degeneration. The original MoD-based LLM (Raposo et al., 2024) overcomes this issue by the hand-craft attempt, which is still sub-optimal and time-consuming. However, in existing MLLMs, the LLM is already pre-trained on large scale of corpus, which can intuitively provide sufficient knowledge to achieve the process automatically. Motivated by this, we propose an innovative metric to estimate the token-wise redundancy of layer in MLLM, namely rank of attention maps (ARank). In particular, given tokens xi Rld of i-th layer, ARank is defined by the average rank of attention maps: τ (xi, Di) = 1 nh nh(cid:88) h= rank(cid:0)Ah (cid:1), (5) where Ah = (xiW Q)(xiW K)T . Here, rank() denotes the rank calculation. nh is the number of attention heads. Ah Rll is the and attention map in h-th head, and are the corresponding weights. Rd Rd 5 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Theoretical analysis of ARank. In Eq. 5, attention map Ah can well reflect the contribution of different tokens. Thus, Ah with low rank suggests that most tokens are less informative. To validate this, we conduct SVD (G.H.Goulb & C.Reinsch, 1971) analysis for Ah, which is written as Ah = (cid:88) i=1 σiuivT = (cid:88) i=1 σiuivT + (cid:88) i=r+ σiuivT , (6) where is the rank of Ah and is constant value. σi, ui and vi denote the i-th single value, left single vector and right single vector of Ah, respectively. As shown in Eq. 6, Ah can be deposed to matrix of rank and additional information, i.e., (cid:80)r . Therefore, lower-rank attention map suggests higher redundancy, which implies that MoD can be deployed to skip most tokens. i=r+1 σiuivT Practical calculation of ARank. As defined in Eq. 5, the calculation of ARank is highly dependent on input samples. Therefore, it is still challenging to accurately calculate the ARank due to the variance of individual samples. Inspired by HRank (Lin et al., 2020), we estimate ARank using its expectation on batch of samples, which is practically robust. As shown in Fig. 3, we visualize average ARank values of LLaVA-HR (Luo et al., 2024b) based on different samples. From these results, we empirically find that the expected ARank remains largely consistent across different tasks. Therefore, small batch of samples is sufficient to accurately calculate ARank. In our experiments, we set the sample size to 50, ensuring computational efficiency. 4.3 MIXTURE-OF-DEPTH ADAPTATION To maximize the effectiveness of MoDs to existing MLLMs, we carefully investigate the micro design of MoDs, including the shared vision-language router and the masked routing learning. Shared vision-language router. Conventional MoDs (Raposo et al., 2024) are designed for LLMs, so their routing is only performed on textual tokens. In MLLMs, such strategy is sub-optimal due to the large redundancy of visual tokens (Jin et al., 2024; Kim et al., 2024; Chen et al., 2024a; Li et al., 2024a). Therefore, the router of γ-MoD, i.e., R(), aims to skip both visual and textual tokens, which is defined by R(x) = softmax(xWR + bR), (7) where = {q, a, t} denotes the vision-language tokens, which consist of question tokens Rlqd, image tokens Rlad and textual response tokens Rltd. WR Rl2 and bR R2 are the weights and bias, respectively. Notably, we use binary softmax function to produce the routing probability, where R(x)0 denotes the probability of skipping. Based on Eq. 7, we further share the router parameters for all MoD layers, which is significant for the stable optimization. To explain, the shared router receives more gradients from different layers, greatly facilitating its convergence at the beginning of training. Masked routing learning. During VL training, not all tokens contribute equally to the optimizing process. In particular, the skip of key tokens in the question, e.g., subject, will greatly hurt the generative training as the answer relies on these conditional elements. Therefore, we introduce masked routing learning strategy to prevent these tokens from being dropped during training. In this case, the objective of the routing learning can be defined by Laug(x) = (cid:0)R(x)1 Mq (cid:1) log( ˆR) + (cid:0)1 R(x)0 Mq (cid:1) log(1 ˆR). (8) Here, Mq Rl1 denotes the binary mask, where the question tokens are assigned to 0. ˆR is the one-hot vector, where the position with top-k routing scores are assigned to 1. The training scheme. γ-MoD is plug-and-play approach for existing MLLMs, and the training scheme of existing MLLMs does not necessarily need to be carefully adjusted. In particular, the training of existing MLLMs can be roughly divided into two stages: vision-language alignment and instruction tuning. After VL alignment, γ-MoD estimates the redundancy of layer using ARank, and directly replaces the redundant one with our MoD layer. During instruction tuning, the routing parameters are jointly optimized with the routing and task objectives via Eq. 4. Notably, other training configurations can simply remain the same as the original setting of MLLM. 6 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Table 1: Comparison of different γ-MoD configurations on LLaVA-HR. The default setting used in the table is colored in gray. and refer to question and answer tokens. Methods GQA SQA MMMU TextVQA Average Acc. Skip Acc. Skip Acc. Skip Acc. Skip Acc. TFlops Skip LLaVA-HR (Luo et al., 2024b) 64.2 0% 67. 0% 34.6 0% 67.1 0% 58.5 19.2 0% MoD layer: All layers 1 MoD per 2 layers 2 MoDs per 3 layers ARank-based deployment Masked token: None + Shared router: Not Share Share Routing ratio: 17% 34% 51% 68%"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "45.9 38.2% 42.6 33.7% 25.9 32.8% 33.8 34.1% 37.1 57.8 19.1% 52.3 16.5% 26.9 16.6% 54.0 17.9% 47.8 38.1 26.8% 46.5 24.6% 24.3 24.4% 42.1 24.9% 37.8 63.7 40.7% 68.5 35.9% 35.6 36.8% 65.3 38.2% 58.3 63.2 52.0% 66.8 46.9% 33.9 47.0% 64.7 49.8% 57.2 63.7 40.7% 68.5 35.9% 35.6 36.8% 65.3 38.2% 58.3 62.8 38.8% 68.6 30.5% 34.7 35.4% 62.0 37.2% 57.0 12.3 16.1 15.9 12.6 10.7 12.6 13.0 34.7% 17.5% 25.2% 37.9% 48.9% 37.9% 35.5% 60.6 55.8% 64.5 48.2% 32.1 48.9% 58.4 52.9% 53.9 63.1 60.3% 67.9 56.9% 34.7 56.6% 64.9 57.1% 57.6 10.3 9.3 51.5% 57.7% 63.6 18.9% 68.9 15.5% 34.7 14.7% 66.1 16.5% 58.3 63.7 40.7% 68.5 35.9% 35.6 36.8% 65.3 38.2% 58.3 63.1 60.3% 67.9 56.9% 34.7 56.6% 64.9 57.1% 57.6 59.1 77.8% 70.1 73.5% 33.7 71.8% 58.4 74.1% 55.3 16.3 12.6 9.3 6.5 16.4% 37.9% 57.7% 74.3% In this section, we provide extensive ablation studies to analyze the key designs that contribute to the effectiveness of our proposed γ-MoD framework. We also evaluate γ-MoD on multiple benchmarks and variant settings with existing dense and sparse MLLMs. 5.1 DATASETS AND METRICS We evaluate our γ-MoD on five multimodal benchmarks for MLLMs, which includes POPE (Li et al., 2023), MME (Fu et al., 2024), MMB (Liu et al., 2024e), MMMU (Yue et al., 2024) and MM-Vet (Yu et al., 2023). Specifically, POPE and MM-Vet aim to evaluate the visual hallucinations of MLLMs. MME measures both perception and cognition abilities on total of 14 subtasks e.g., numerical calculation, text translation, and code reasoning. MMBench is structured objective benchmark designed for comprehensive and robust evaluation of vision-language models. MMMU is designed to measure the perception, knowledge, and reasoning of MLLMs abilities. We report all the results in their default settings. For MME, we report the perception score. We also evaluate γ-MoD on six image question answering benchmarks,VQAv2 (Goyal et al., 2017), VizWiz (Gurari et al., 2018), TextVQA (Singh et al., 2019), SQA (Lu et al., 2022), GQA (Hudson & Manning, 2019) and SEED (Ge et al., 2023). In particular, SQA (Lu et al., 2022) and VizWiz (Gurari et al., 2018) are two zero-shot tasks, and none of their samples are present in our training data. We report the overall accuracy of SEED, the test set of VizWiz and we organize samples of these tasks in instruction formats of LLaVA-1.5 (Liu et al., 2024b). 5.2 IMPLEMENTATION DETAILS For all models, pre-training is conducted on LCS-558K dataset (Liu et al., 2024b), which includes high-quality 558k image-text pairs. For instruction tuning, we follow LLaVA-1.5 (Liu et al., 2024b) to use 665k vision-language instruction data. To deploy γ-MoD to MLLMs, ARank is calculated to identify redundant layers after the pre-training stage. For all models, the fourth largest ARank value is used as the threshold for converting dense layers to MoD ones. During instruction tuning, the coefficient for the routing loss is set to 0.01. The remaining settings are kept the same with LLaVA-HR (Luo et al., 2024b) and LLaVA (Liu et al., 2024b), including learning rate, training epochs, optimizer and datasets, etc. 7 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Table 2: Ablation study of γ-MoD on LLaVA-HR. Param, Acc. and Skip indicate the parameter, accuracy, and skip ratio, respectively. Methods Param GQA SQA MMMU TextVQA Average Acc. Skip Acc. Skip Acc. Skip Acc. Skip Acc. TFlops Skip LLaVA-HR (Luo et al., 2024b) 7.4B 64.2 0% 67.9 0% 34.6 0% 67.1 0% 58. + Default MoD (Raposo et al., 2024) 7.4B 45.9 38.2% 42.6 33.7% 25.9 32.8% 33.8 34.1% 37.1 7.4B 63.2 52.0% 66.8 46.9% 33.9 47.0% 64.7 49.8% 57.2 + ARank-based deployment (ours) 7.4B 63.1 60.3% 67.9 56.9% 34.7 56.6% 64.9 57.1% 57.6 + Masked routing learning (ours) 19.2 12.3 10.7 9.3 0% 34.7% 48.9% 57.7% Table 3: Results of γ-MoD on different MLLM architectures and model scales. γ-MoD-0.3 and γ-MoD-0.5 denote the routing ratio of 30% and 50%, respectively. Methods Param GQA SQA MMMU TextVQA Average Acc. Skip Acc. Skip Acc. Skip Acc. Skip Acc. TFlops Skip MLLM architecture: LLaVA +γ-MoD-0.3 +γ-MoD-0.5 LLaVA-HR +γ-MoD-0.3 +γ-MoD-0.5 Mini-Gemini-HD +γ-MoD-0.3 +γ-MoD-0.5 Model scales: LLaVA-HR +γ-MoD-0.3 +γ-MoD-0.5 LLaVA-HR +γ-MoD-0.3 +γ-MoD-0. 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 7B 13B 13B 13B 62.0 61.1 41. 64.2 63.7 63.1 62.9 62.1 62.2 64.2 63.7 63.1 64.8 64.5 64.8 0% 66.8 34.1% 64.7 60.9% 62. 0% 67.9 40.7% 68.5 60.3% 67.9 0% 69.6 37.1% 69.0 59.2% 70.4 0% 34.3 29.4% 35.4 54.8% 31. 0% 34.6 35.9% 35.6 56.9% 34.7 0% 36.8 34.6% 34.1 56.8% 33.9 0% 58.2 29.8% 56.3 53.6% 42. 0% 67.1 36.8% 65.3 56.6% 64.9 0% 66.5 36.4% 66.4 58.6% 67.0 0% 55.3 30.7% 54.4 56.2% 44. 0% 58.5 38.2% 58.3 57.1% 57.6 0% 59.0 36.6% 57.9 57.7% 58.4 0% 67.9 40.7% 68.5 60.3% 67. 0% 68.1 38.1% 70.5 58.8% 69.5 0% 34.6 35.9% 35.6 56.9% 34.7 0% 36.7 33.1% 37.8 52.2% 35. 0% 67.1 36.8% 65.3 56.6% 64.9 0% 68.1 32.5% 67.0 53.8% 66.8 0% 58.5 38.2% 58.3 57.1% 57. 0% 59.4 36.0% 60.0 55.4% 59.2 10.7 7.7 5.3 19.2 12.6 9.3 60.2 39.4 27.8 19.2 12.6 9. 37.1 25.1 18.4 0% 31.0% 56.4% 0% 37.9% 57.7% 0% 36.2% 58.1% 0% 37.9% 57.7% 0% 34.9% 55.1% 5.3 EXPERIMENTAL RESULTS 5.3.1 QUANTITATIVE ANALYSIS Comparison with different MoD configurations. In Tab. 1, we first compare different settings of MoD on LLaVA-HR (Luo et al., 2024b). From this table, the first observation is that directly converting all layers to MoD ones leads to worse results, e.g., 33.8% on TextVQA. Besides, although the hand-craft strategy performs much better, its performance declines are still obvious, e.g., -10.7% of 1 MoD per 2 layers on average. These results confirm the challenges of adopting MoDs to MLLMs. However, after employing our ARank-based strategy, the efficiency of LLaVA-HR is greatly increased while the performance is well maintained. Such comparison greatly confirm the effectiveness of our ARank-based strategy against these baselines. In Tab. 1, we also validate different micro-designs for deploying MoD on MLLM, including the masked routing learning, the shared router and the routing ratio. From these comparisons, we first see that the masked learning strategy is much beneficial to the optimization of γ-MoD, providing up to +1.7% gains on SQA. Without this strategy, question tokens will be dropped in MoD layers, easily resulting in the semantic ambiguity for answering. In addition, we also find that the router sharing strategy plays significant role in γ-MoD. After removing this strategy, model performance will obviously drop on TextVQA by -6.5%. Finally, we validate the impact of different routing ratio on LLaVA-HR. From results we can see that model performance can be retained under relatively small routing ratios, i.e., 17% and 34%. When routing ratio is increased to 51%, model performance drops slightly from 58.3% to 57.6% on average. However, the benefit of efficiency is still notable, i.e., -51.5% Flops. Overall, these comparisons greatly validate our motivations and the design of γ-MoD. Ablation studies. To validate contributions of each design in γ-MoD, we conduct ablation study in Tab. 2. From this table, we can see that the default MoD will cause obvious performance degeneration, γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Table 4: Training and inference efficiency of γ-MoD on LLaVA-HR. The inference efficiency is tested on an NVIDIA A100 GPU, which is the average value of GQA, SQA, MMMU, and TextVQA. Methods LLaVA-HR +γ-MoD-0.3 +γ-MoD-0.5 Gains Training Time 20.7 15.4 14.3 -31.0% Inference Throughput 4.7 samples/s 5.9 samples/s 7.2 samples/s +53.2% Inference Memory 19 15 14 -26.3% Inference TFlops 19.2 12.6 9.3 -51.6% Avg. Acc. 58.5 58.3 57.6 -1.5% Table 5: Comparison with existing dense and sparse MLLMs on 9 benchmarks. Speed is the average samples per second of GQA, SQA, MMMU, and TextVQA. Methods Param. Image Question Answering Benchmark Toolkit TextVQA VQAv2 GQA SQAI POPE MME MMB MMMU MM-Vet Speed Dense Model: I-80B (Laurencon et al., 2024) InstructBLIP (Dai et al., 2023) VILA (Lin et al., 2024b) Qwen-VL (Bai et al., 2023b) LLaVA-1.5 (Liu et al., 2024b) LLaVA-HR (Luo et al., 2024b) LLaVA-HR (Luo et al., 2024b) Sparse Model: MoE-LLaVA (Lin et al., 2024a) MoE-LLaVA (Lin et al., 2024a) γ-MoD-LLaVA(ours) γ-MoD-LLaVA-HR(ours) γ-MoD-LLaVA-HR(ours) 65B 14B 7B 10B 7B 7B 13B 3B 5B 7B 7B 13B - 50.7 64.4 63.8 58.2 67.1 68.1 50.1 51.4 56.3 64.9 66.8 60.0 - 79.9 78.8 78.5 81.9 82.3 76.7 77.6 77.6 80.6 82. 45.2 49.5 62.3 59.3 62.0 64.2 64.8 60.3 61.4 61.1 63.1 64.8 - 63.1 68.2 67.1 66.8 67.9 68.1 62.6 68.5 64.7 67.9 69.5 - - 54.5 - - 78.9 1212.8 85.5 1533.0 68.9 1487.6 38.2 85.9 1510.7 64.3 87.6 1554.9 66.8 87.8 1540.9 64.5 85.7 1318.2 60.2 86.3 1423.0 65.2 86.0 1342.1 59.4 87.3 1516.0 63.4 86.7 1515.4 65.2 - - - - 34.3 35.2 36.3 - - 35.4 34.7 35. - 25.6 34.9 - 30.5 31.2 34.8 26.9 34.3 29.8 31.5 34.0 - - - 4.6 8.1 4.7 3.1 8.5 5.6 10.3 7.2 4.8 resulting up to -25.3% on SQA. In stark contrast, with our ARank-based deployment, the average performance of LLaVA-HR is improved from 37.1% to 57.6%, and the computational sparsity also boosts from 34.7% to 48.9%. Such comparison confirms that not all layers can be converted to MoD layers, and ARank is critical to identify the redundant ones. In addition, the use of masked routing learning can further benefit the model training, providing +0.8% on MMMU and +0.2% on TextVQA, respectively. It worth noticing that the masked routing learning also increases the efficiency of γ-MoD, where the average computational costs are further reduced from 10.7 TFlops to 9.3 TFlops. These results further confirm the effectiveness of γ-MoD. Generalizations of γ-MoD on different MLLMs. In Tab. 3, we also evaluate the generalization capability of γ-MoD across different MLLM architectures and model scales. In particular, γ-MoD with 30% routing ratio demonstrates great trade-off between performance and efficiency on LLaVA. When the routing ratio increases to 51%, the performance of LLaVA decreases significantly, suggesting its relatively low tolerance to high routing ratio. For LLaVA-HR, the γ-MoD-0.3 configuration maintains high accuracy 63.7% on GQA and 65.3% on TextVQA while reducing TFlops by 34% and skipping 37.9% of tokens. When the routing ratio increases to 51%, the token skip rate improves to 57.7%, though slight drop in accuracy is observed e.g., -0.6% on GQA. These comparisons also reflect that high-resolution MLLMs often have higher token redundancy than low-resolution ones. When scaling to larger models, such as the LLaVA-HR-13B, our method continues to perform strongly. The γ-MoD-0.3 configuration yields 38.1% skip rate and 25.1 TFlops with minimal accuracy loss, suggesting that larger models are better suited to handle higher skip rates while maintaining performance. Even increasing the routing ratio to 51% the competitive accuracy is still maintained, e.g., 64.8% on GQA and 66.8% on TextVQA. Efficiency analysis. In Tab. 4, we compare the training and inference efficiency of γ-MoD on LLaVA-HR. From these results, we observe comprehensive advantages of γ-MoD in terms of training and inference inference. In particular, γ-MoD-0.3 already achieves an obvious improvement in efficiency, i.e., -26% training time and -35% TFlops. However, the performance drops of γ-MoD-0.3 can be almost ignorable, i.e., -0.2% average accuracy. When increasing the routing ratio to 50% tokens, the inference throughput of γ-MoD-0.5 further improves by up to +53.2%. Despite the 9 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Figure 4: Visualization of routing results for different MoD layers. Q, and denote the question, image and response, respectively. The skipped tokens in sub-figure (b) are colored in gray. significant efficiency gains, the performance drop of γ-MoD is still acceptable, i.e., -1.5% average accuracy. These results well validate the obvious benefits of γ-MoD in efficiency. 5.3.2 COMPARISON WITH EXISTING MLLMS In Tab. 5, we compare MLLMs deployed by γ-MoD with both dense and sparse models on 9 benchmarks. From it we can see γ-MoD can maintain the competitive performance on all benchmarks, while achieving significant efficiency gains on LLaVA and LLaVA-HR. Specifically, γ-MoD-LLaVAHR (13B) can reach similar inference speed as LLaVA-HR (7B) while outperforming the latter on multiple benchmarks, e.g., +3.0% on MMVet. Compared to other dense MLLMs, similar merits of γ-MoD-LLaVA-HR can still be witnessed. For example, γ-MoD-LLaVA-HR-7B not only obviously outperforms Qwen-VL on GQA and VQAv2, but also demonstrates superior inference efficiency, i.e., 1.5 speedup. In addition, compared to existing sparse models, i.e., MoE-LLaVA (Lin et al., 2024a), our approaches also achieve better trade-off between performance and efficiency. In particular, γ-MoD-LLaVA-HR (7B) outperforms MoE-LLaVA (5B) on 5 of 8 benchmarks, e.g., + 93 scores on MME, while still maintaining better efficiency, i.e., +28% gains on inference speed. It is worth noting that although the parameter scale of MoE-LLaVA is smaller, its routing calculation often leads to higher latency. More importantly, MoE-LLaVA requires additional training stages for its MoE adaptation, which also consumes much more training data than our methods, i.e., 2.2M vs. 1.1M. Overall, these comparisons further confirm the effectiveness and efficiency of γ-MoD. 5.3.3 QUALITATIVE ANALYSIS In Fig. 4,we visualize the routing ratio and the skipped content in both images and the corresponding conversations. The first observation from Fig. 4.(a) is that question, image, and response tokens are routed in consistent pattern: question tokens are mostly kept, while image tokens are the most redundant, and thus routed the most. In Fig. 4.(b), we visualize the skipped content on images and texts. The gray portions of the images represent tokens that are skipped by the router, indicating that many regions in the images, such as background pixels, are redundant and do not provide critical information for understanding. Routing out these tokens allows the model to focus more on the white portions, which highlight the image regions or text parts that the model pays closer attention to. For example, in the middle of the first row with the IQ test example, the model can concentrate 10 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models and spending more computations on the arithmetic and geometric aspects of the image, leading to reasonable answer."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we aim to overcome the efficiency problem in multimodal large language models (MLLMs) from the perspective of activated token. In particular, we present γ-MoD, novel mixture-of-depth adaptation strategy for computationally efficient MLLM. In γ-MoD, an innovative metric is introduced to identify the redundant layers for MoD deployment, namely rank of attention maps (ARank). Moreover, γ-MoD also maximizes its benefit to MLLMs via two designs called shared vision-language router and masked routing learning. With these novel designs, γ-MoD can obviously reduce computational costs of existing MLLMs while maintaining their performance. Extensive experiments on 9 multimodal benchmarks validate the efficiency and effectiveness. Besides, the great generalization ability of γ-MoD is also validated across different MLLMs. Acknowledgements: This work was supported by the National Natural Science Foundation of China (No. 623B2088)."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Bo Adler, Niket Agarwal, Ashwath Aithal, Dong Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 2022. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023a. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023b. Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. survey on mixture of experts. arXiv preprint arXiv:2407.06204, 2024a. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024b. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian 11 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024b. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. Yaroslav Fyodorov, Yoad Winter, and Nissim Francez. natural logic inference system. Proceedings of the 2nd workshop on inference in computational semantics (ICoS-2), 2000. In Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting seed of vision in large language model, 2023. G.H.Goulb and C.Reinsch. Singular value decomposition and least squares solutions. In Handbook for Automatic Computation: Volume II: Linear Algebra, pp. 134151. Springer, 1971. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering, 2017. Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people, 2018. Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering, 2019. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin Tan, Zhenye Gan, et al. Efficient multimodal large language models: survey. arXiv preprint arXiv:2405.10739, 2024. Minchul Kim, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, and Hongxia Jin. Token fusion: Bridging the gap between token pruning and token merging. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 13831392, 2024. Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36, 2024. 12 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024b. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023. Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning, and Li Yuan. Moe-llava: Mixture of experts for large vision-language models, 2024a. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2668926699, 2024b. Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling Shao. Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 15291538, 2020. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-ofexperts language model. arXiv preprint arXiv:2405.04434, 2024a. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024b. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024c. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024d. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024e. Zhengzhong Liu, Bowen Tan, Hongyi Wang, Willie Neiswanger, Tianhua Tao, Haonan Li, Fajri Koto, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Liqun Ma, Liping Tang, Nikhil Ranjan, Yonghao Zhuang, Guowei He, Renxi Wang, Mingkai Deng, Robin Algayres, Yuanzhi Li, Zhiqiang Shen, Preslav Nakov, and Eric Xing. Llm360 k2-65b: Scaling up fully transparent open-source llms. 2024f. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering, 2022. Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. Monointernvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. arXiv preprint arXiv:2410.08202, 2024a. Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024b. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. 13 γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad S. Khan. Llava++: Extending visual capabilities with llama-3 and phi-3, 2024. Siva Reddy, Danqi Chen, and Christopher Manning. Coqa: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. Zhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva, Daria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations for llm training. arXiv preprint arXiv:2309.10818, 2023. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: An early effort on open mixture-of-experts language models. arXiv preprint arXiv:2402.01739, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024. γMoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        }
    ],
    "affiliations": [
        "MBZUAI",
        "National University of Singapore",
        "OpenGVLab, Shanghai AI Laboratory",
        "Technical University Of Denmark",
        "Xiamen University"
    ]
}