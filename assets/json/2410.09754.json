{
    "paper_title": "SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning",
    "authors": [
        "Hojoon Lee",
        "Dongyoon Hwang",
        "Donghu Kim",
        "Hyunseung Kim",
        "Jun Jet Tai",
        "Kaushik Subramanian",
        "Peter R. Wurman",
        "Jaegul Choo",
        "Peter Stone",
        "Takuma Seno"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce a simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms-including off-policy, on-policy, and unsupervised methods-is consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments."
        },
        {
            "title": "Start",
            "content": "SIMBA: SIMPLICITY BIAS FOR SCALING UP PARAMETERS IN DEEP REINFORCEMENT LEARNING Hojoon Lee1,2 Dongyoon Hwang1 Donghu Kim1 Hyunseung Kim1 Jun Jet Tai2,3 Kaushik Subramanian2 Peter R.Wurman2 Jaegul Choo1 Peter Stone2,4 Takuma Seno2 1KAIST {joonleesky, godnpeter}@kaist.ac.kr 3Coventry University 4UT Austin 2Sony AI"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting. These large networks avoid overfitting by integrating components that induce simplicity bias, guiding models toward simple and generalizable solutions. However, in deep RL, designing and scaling up networks have been less explored. Motivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) residual feedforward block to provide linear pathway from the input to output, and (iii) layer normalization to control feature magnitudes. By scaling up parameters with SimBa, the sample efficiency of various deep RL algorithmsincluding off-policy, onpolicy, and unsupervised methodsis consistently improved. Moreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-theart deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench. These results demonstrate SimBas broad applicability and effectiveness across diverse RL algorithms and environments. Explore codes and videos at https://sonyresearch.github.io/simba 4 2 0 2 3 1 ] . [ 1 4 5 7 9 0 . 0 1 4 2 : r Figure 1: Benchmark Summary. (a) Sample Efficiency: SimBa improves sample efficiency across various RL algorithms, including off-policy (SAC, TD-MPC2), on-policy (PPO), and unsupervised RL (METRA). (b) Compute Efficiency: When applying SimBa with SAC, it matches or surpasses state-of-the-art off-policy RL methods across 51 continuous control tasks, by only modifying the network architecture and scaling up the number of network parameters. Equal Contribution Work done during internship at Sony AI"
        },
        {
            "title": "INTRODUCTION",
            "content": "Scaling up neural network sizes has been key driver of recent advancements in computer vision (CV) (Dehghani et al., 2023) and natural language processing (NLP) (Google et al., 2023; Achiam et al., 2023). By increasing the number of parameters, neural networks gain enhanced expressivity, enabling them to cover diverse functions and discover effective solutions that smaller networks might miss. However, this increased capacity also heightens the risk of overfitting, as larger networks can fit intricate patterns in the training data that do not generalize well to unseen data. Despite this risk of overfitting, empirical evidence shows that neural networks tend to converge toward simpler functions that generalize effectively (Kaplan et al., 2020; Nakkiran et al., 2021). This phenomenon is attributed to simplicity bias inherent in neural networks, where standard optimization algorithms and architectural components guide highly expressive models toward solutions representing simple, generalizable functions (Shah et al., 2020; Berchenko, 2024). For instance, gradient noise in stochastic gradient descent prevents models from converging on sharp local minima, helping them avoid overfitting (Chizat & Bach, 2020; Gunasekar et al., 2018; Pesme et al., 2021). Architectural components such as ReLU activations (Hermann et al., 2024), layer normalization (Ba et al., 2016), and residual connections (He et al., 2020) are also known to amplify simplicity bias. These components influence the types of functions that neural networks represent at initialization, where networks that represent simpler function at initialization are more likely to converge to simple functions (Valle-Perez et al., 2018; Mingard et al., 2019; Teney et al., 2024). While scaling up network parameters and leveraging simplicity bias have been successfully applied in CV and NLP, these principles have been underexplored in deep reinforcement learning (RL), where the focus has primarily been on algorithmic advancements (Hessel et al., 2018; Hafner et al., 2023; Fujimoto et al., 2023; Hansen et al., 2023). Motivated by this opportunity, we introduce the SimBa network, novel architecture that explicitly embeds simplicity bias to effectively scale up parameters in deep RL. SimBa comprises of three key components: (i) an observation normalization layer that standardizes inputs by tracking the mean and variance of each dimension, reducing overfitting to high-variance features (Andrychowicz et al., 2020); (ii) pre-layer normalization residual feedforward block (Xiong et al., 2020), which maintains direct linear information pathway from input to output and applies non-linearity only when necessary; and (iii) post-layer normalization before the output layer to stabilize activations, ensuring more reliable policy and value predictions. To verify whether SimBa amplifies simplicity bias, we compared it against the standard MLP architecture often employed in deep RL. Following Teney et al. (2024), we measured simplicity bias by (i) sampling random inputs from uniform distribution; (ii) generating network outputs; and (iii) performing Fourier decomposition on these outputs. smaller sum of the Fourier coefficients indicates that the neural network represents low-frequency function, signifying greater simplicity. We define the simplicity score as the inverse of this sum, meaning higher score corresponds to stronger simplicity bias. As illustrated in Figure 2.(a), our analysis revealed that SimBa has higher simplicity score than the MLP (Further details are provided in Section 2). To evaluate the benefit of leveraging simplicity bias on network scaling, we compared the performance of Soft Actor-Critic (Haarnoja et al., 2018, SAC) using both MLP and our SimBa architecture across 3 humanoid tasks from DMC benchmark (Tassa et al., 2018). We increased the width of both actor and critic networks, scaling from 0.1 to 17 million parameters. As shown in Figure 2.(b), SAC with MLP experiences performance degradation as the number of parameters increases. In contrast, SAC with the SimBa network consistently improves its performance as the number of parameter increases, highlighting the value of embedding simplicity bias when scaling deep RL networks. Figure 2: (a) SimBa exhibits higher simplicity bias than MLP. (b) SAC with SimBa improves its performance with increased parameters, whereas SAC with MLP degrades it. Each standard deviation is 95% CI. To further evaluate SimBas versatility, we applied it to various RL algorithms by only changing the network architecture and scaling up parameters. The algorithms included off-policy (SAC (Haarnoja et al., 2018), TD-MPC2 (Hansen et al., 2023)), on-policy model-free (PPO (Schulman et al., 2017)), 2 and unsupervised RL (METRA (Park et al., 2023)). As illustrated in Figure 1.(a), SimBa consistently enhances the sample efficiency of these algorithms. Furthermore, as shown in Figure 1.(b), when SimBa is integrated into SAC, it matches or surpasses state-of-the-art off-policy methods across 51 tasks in DMC, MyoSuite (Caggiano et al., 2022), and HumanoidBench (Sferrazza et al., 2024). Despite the increased number of parameters, SAC with SimBa remains computationally efficient because it does not employ any computationally intensive components such as self-supervised objectives (Fujimoto et al., 2023), planning (Hansen et al., 2023), or replay ratio scaling (Nauman et al., 2024), which state-of-the-art methods rely on to achieve high performance."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "Simplicity bias refers to the tendency of neural networks to prioritize learning simpler patterns over capturing intricate details (Shah et al., 2020; Berchenko, 2024). In this section, we introduce metrics to quantify simplicity bias; in-depth definitions are provided in Appendix A."
        },
        {
            "title": "2.1 MEASURING FUNCTION COMPLEXITY",
            "content": "We begin by defining complexity measure : [0, ) to quantify the complexity of functions in = {f : Y} where Rn denote the input space and Rm the output space. Traditional complexity measures, such as the VapnikChervonenkis dimension (Blumer et al., 1989) and Rademacher complexity (Bartlett & Mendelson, 2002), are well-established but often intractable for deep neural networks. Therefore, we follow Teney et al. (2024) and adopt Fourier analysis as our primary complexity measure. Given (x) := (2π)d/2 (cid:82) (k) eikxdk where (k) := (cid:82) (x) eikxdx is the Fourier transform, we perform discrete Fourier transform by uniformly discretizing the frequency domain, {0, 1, . . . , K}. The value of is chosen by the Nyquist-Shannon limit (Shannon, 1949) to ensure accurate function representation. Our complexity measure c(f ) is then computed as the frequency-weighted average of the Fourier coefficients: (1) Intuitively, larger c(f ) indicates higher complexity due to dominance of high-frequency components such as rapid amplitude changes or intricate details. Conversely, lower c(f ) implies larger contribution from low-frequency components, indicating lower complexity function. k=0 k=0 (k) / ΣK (k). c(f ) = ΣK 2.2 MEASURING SIMPLICITY BIAS In theory, simplicity bias can be measured by evaluating the complexity of the function to which the network converges after training. However, directly comparing simplicity bias across different architectures after convergence is challenging due to the randomness of the non-stationary optimization process, especially in RL, where the data distribution changes continuously. Empirical studies suggest that the initial complexity of network strongly correlates with the complexity of the functions it converges to during training (Valle-Perez et al., 2018; De Palma et al., 2019; Mingard et al., 2019; Teney et al., 2024). Therefore, for given network architecture with an initial parameter distribution Θ0, we define the simplicity bias score s(f ) : (0, ) as: s(f ) EθΘ (cid:21) (cid:20) 1 c(fθ) (2) where fθ denotes the network architecture parameterized by θ. This measure indicates that networks with lower complexity at initialization are more likely to exhibit higher simplicity bias, thereby converging to simpler functions during training."
        },
        {
            "title": "3 RELATED WORK",
            "content": "3.1 SIMPLICITY BIAS Initially, simplicity bias was mainly attributed to the implicit regularization effects of the stochastic gradient descent (SGD) optimizer (Soudry et al., 2018; Gunasekar et al., 2018; Chizat & Bach, 2020; 3 Figure 3: SimBa architecture. The network integrates Running Statistics Normalization (RSNorm), residual feedforward blocks, and post-layer normalization to embed simplicity bias into deep RL. Pesme et al., 2021). During training, SGD introduces noise which prevents the model from converging to sharp minima, guiding it toward flatter regions of the loss landscape (Wu et al., 2022). Such flatter minima are associated with functions of lower complexity, thereby improving generalization. However, recent studies suggest that simplicity bias is also inherent in the network architecture itself (Valle-Perez et al., 2018; Mingard et al., 2019). Architectural components such as normalization layers, ReLU activations (Hermann et al., 2024), and residual connections (He et al., 2020) promote simplicity bias by encouraging smoother, less complex functions. Fourier analysis has shown that these components help models prioritize learning low-frequency patterns, guiding optimization toward flatter regions that generalize better (Teney et al., 2024). Consequently, architectural design plays crucial role in favoring simpler solutions, enabling the use of overparameterized networks. 3.2 DEEP REINFORCEMENT LEARNING For years, deep RL has largely focused on algorithmic improvements to enhance sample efficiency and generalization. Techniques like Double Q-learning (Van Hasselt et al., 2016; Fujimoto et al., 2018), and Distributional RL (Dabney et al., 2018) have improved the stability of value estimation by reducing overestimation bias. Regularization strategiesincluding periodic reinitialization (Nikishin et al., 2022; Lee et al., 2024), Layer Normalization (Lee et al., 2023; Gallici et al., 2024), Batch Normalization (Bhatt et al., 2024), and Spectral Normalization (Gogianu et al., 2021)have been employed to prevent overfitting and enhance generalization. Incorporating self-supervised objectives with model-based learning (Fujimoto et al., 2023; Hafner et al., 2023; Hansen et al., 2023) has further improved representation learning and sample efficiency. Despite these advances, scaling up network architectures in deep RL remains underexplored. While larger models improved performance in supervised learning by leveraging simplicity bias, this principle has not been fully explored in RL. Several recent studies have attempted to scale up network sizesthrough ensembling (Chen et al., 2021; Obando-Ceron et al., 2024), widening (Schwarzer et al., 2023), and deepening networks (Bjorck et al., 2021; Nauman et al., 2024). However, they often rely on computationally intensive layers like spectral normalization (Bjorck et al., 2021) or require sophisticated training protocols (Nauman et al., 2024), limiting their applicability. In this work, we aim to design an architecture that amplifies simplicity bias, enabling us to effectively scale up parameters in RL, independent of using any other sophisticated training protocol."
        },
        {
            "title": "4 SIMBA",
            "content": "This section introduces the SimBa network, an architecture designed to embed simplicity bias into deep RL. The architecture comprises Running Statistics Normalization, Residual Feedforward Blocks, and Post-Layer Normalization. By amplifying the simplicity bias, SimBa allows the model to avoid overfitting for highly overparameterized configurations. Running Statistics Normalization (RSNorm). First, RSNorm standardizes input observations by tracking the running mean and variance of each input dimension during training, preventing features with disproportionately large values from dominating the learning process. Given an input observation ot Rdo at timestep t, we update the running observation mean µt Rdo and variance σ2 Rdo as follows: µt = µt1 + 1 δt, σ2 = 1 (σ2 t1 + 1 δ2 ) (3) where δt = ot µt1 and do denotes the dimension of the observation. 4 Once µt and σ2 are computed, each input observation ot is normalized as: ot = RSNorm(ot) = ot µt (cid:112)σ2 + ϵ (4) where ot Rdo is the normalized output, and ϵ is small constant for numerical stability. While alternative observation normalization methods exist, RSNorm consistently demonstrates superior performance. comprehensive comparison is provided in Section 7.1. Residual Feedforward Block. The normalized observation oi is first embedded into dhdimensional vector using linear layer: xl = Linear(ot). (5) At each block, {1, ..., L}, the input xl passes through pre-layer normalization residual feedforward block, introducing simplicity bias by allowing direct linear pathway from input to output. This direct linear pathway enables the network to pass the input unchanged unless non-linear transformations are necessary. Each block is defined as: (6) Following Vaswani (2017), the MLP is structured with an inverted bottleneck, where the hidden dimension is expanded to 4 dh and ReLU activation is applied between the two linear layers. + MLP(LayerNorm(xl xl+1 = xl t)). Post-Layer Normalization. To ensure that activations remain on consistent scale before predicting the policy or value function, we apply layer normalization after the final residual block: zt = LayerNorm(xL (7) The normalized output zt is then processed through linear layer, generating predictions for the actors policy or the critics value function. )."
        },
        {
            "title": "5 ANALYZING SIMBA",
            "content": "In this section, we analyze whether each component of SimBa amplifies simplicity bias and allows scaling up parameters in deep RL. We conducted experiments on challenging environments in DMC involving Humanoid and Dog, collectively referred to as DMC-Hard. Throughout this section, we used Soft Actor Critic (Haarnoja et al., 2018, SAC) as the base algorithm. 5.1 ARCHITECTURAL COMPONENTS To quantify simplicity bias in SimBa, we use Fourier analysis as described in Section 2. For each network , we estimate the simplicity bias score s(f ) by averaging over 100 random initializations θ Θ0. Following Teney et al. (2024), the input space = [100, 100]2 R2 is divided into grid of 90,000 points, and the network outputs scalar value for each input which are represented as grayscale image. By applying the discrete Fourier transform to these outputs, we compute the simplicity score s(f ) defined in Equation 2 (see Appendix for further details). Figure 4.(a) shows that SimBas key componentssuch as residual connections and layer normalizationincrease the simplicity bias score s(f ), biasing the architecture toward simpler functional representations at initialization. When combined, these components induce stronger preference for low-complexity functions (i.e., high simplicity score) than when used individually. Figure 4.(b) reveals clear relationship between simplicity bias and performance: architectures with higher simplicity bias scores lead to enhanced perSpecifically, compared to the MLP, formance. adding residual connections increases the average return by 50 points, adding layer normalization adds 150 points, and combining all components results in substantial improvement of 550 points. Figure 4: Component Analysis. (a) Simplicity bias scores estimated via Fourier analysis. Mean and 95% CI are computed over 100 random initializations. (b) Average return in DMC-Hard at 1M steps. Mean and 95% CI over 10 seeds, using the SAC algorithm. Stronger simplicity bias correlates with higher returns for overparameterized networks."
        },
        {
            "title": "5.2 COMPARISON WITH OTHER ARCHITECTURES",
            "content": "To assess SimBas scalability, we compared it to BroNet (Nauman et al., 2024), SpectralNet (Bjorck et al., 2021), and MLP. Detailed descriptions of each architecture are provided in Appendix B. The key differences between SimBa and other architectures lie in the placement of components that promote simplicity bias. SimBa maintains direct linear residual pathway from input to output, applying non-linearity exclusively through residual connections. In contrast, other architectures introduce non-linearities within the input-to-output pathway, increasing functional complexity. Additionally, SimBa employs post-layer normalization to ensure consistent activation scales across all hidden dimensions, reducing variance in policy and value predictions. Conversely, other architectures omit normalization before the output layer, which can lead to high variance in their predictions. To ensure that performance differences were attributable to architectural design choices rather than varying input normalization strategies, we uniformly applied RSNorm as the input normalization method across all models. Additionally, our investigation primarily focused on scaling the critic networks hidden dimension, as scaling the actor network showed limited benefits (see Section7.2). As illustrated in Figure 5.(a), SimBa achieves the highest simplicity bias score compared to the experimented architectures, demonstrating its strong preference for simpler solutions. In Figure 5.(b), while the MLP failed to scale with increasing network size, BroNet, SpectralNet, and SimBa all showed performance improvements as their networks scaled up. Importantly, the scalability of each architecture was strongly correlated with its simplicity bias score, where higher simplicity bias led to better scalability. SimBa demonstrated the best scalability, supporting the hypothesis that simplicity bias is key factor in scaling deep RL models. Figure 5: Architecture Comparison. (a) SimBa consistently exhibits higher simplicity bias score. (b) SimBa demonstrates superior scaling performance in terms of average return for DMC-Hard compared to the other architectures. The results are from 5 random seeds."
        },
        {
            "title": "6 EXPERIMENTS",
            "content": "This section evaluates SimBas applicability across various deep RL algorithms and environments. For each baseline, we either use the authors reported results or run experiments using their recommended hyperparameters. Detailed descriptions of the environments used in our evaluations are provided in Appendix F. In addition, we also include learning curves and final performance for each task in Appendix and I, along with hyperparameters used in our experiments in Appendix G. 6.1 OFF-POLICY RL Experimental Setup. We evaluate the algorithms on 51 tasks across three benchmarks: DMC (Tassa et al., 2018), MyoSuite (Caggiano et al., 2022), and HumanoidBench (Sferrazza et al., 2024). The Figure 6: Environment Visualizations. SimBa is evaluated across four diverse benchmark environments: DMC, MyoSuite, and HumanoidBench, which feature complex locomotion and manipulation tasks, and Craftax, which introduces open-ended tasks with varying complexity. 6 Figure 8: Off-policy RL Benchmark. Average episode return for DMC and HumanoidBench and average success rate for MyoSuite across 51 continuous control tasks. SimBa (with SAC) achieves high computational efficiency by only changing the network architecture. DMC tasks are further categorized into DMC-Easy&Medium and DMC-Hard based on complexity. We vary the number of training steps per benchmark according to the complexity of the environment: 500K for DMC-Easy&Medium, 1M for DMC-Hard and MyoSuite, and 2M for HumanoidBench. Baselines. We compare SimBa against state-of-the-art off-policy RL algorithms. (i) SAC (Haarnoja et al., 2018), an actor-critic algorithm based on maximum entropy RL; (ii) DDPG (Lillicrap, 2015), deterministic policy gradient algorithm with deep neural networks; (iii) TD7 (Fujimoto et al., 2023), an enhanced version of TD3 incorporating state-action representation learning; (iv) BRO (Nauman et al., 2024), which scales the critic network of SAC while integrating distributional Q-learning, optimistic exploration, and periodic resets. For fair comparison, we use BRO-Fast, which is the most computationally efficient version; (v) TD-MPC2 (Hansen et al., 2023), which combines trajectory planning with long-return estimates using learned world model; and (vi) DreamerV3 (Hafner et al., 2023), which learns generative world model and optimizes policy via simulated rollouts. Integrating SimBa into Off-Policy RL. The SimBa architecture is algorithm-agnostic and can be applied to any deep RL method. To demonstrate its applicability, we integrate SimBa into two model-free (SAC, DDPG) and one model-based algorithm (TD-MPC2), evaluating their performance on the DMC-Hard benchmark. For SAC and DDPG, we replace the standard MLP-based actor-critic networks with SimBa. For TD-MPC2, we substitute the shared encoder from MLP to SimBa while matching the number of parameters as the original implementation. Figure 7 shows consistent benefits across all algorithms: integrating SimBa increased the average return by 570, 480, and, 170 points for SAC, DDPG, and TD-MPC2 respectively. These results demonstrate that scaling up parameters with strong simplicity bias significantly enhances performance in deep RL. Figure 7: Off-Policy RL with SimBa. Replacing MLP with SimBa leads to substantial performance improvements across various off-policy RL methods. Mean and 95% CI are averaged over 10 seeds for SAC and DDPG, and 3 seeds for TD-MPC2 in DMC-Hard. Comparisons with State-of-the-Art Methods. Here, we compare SAC + SimBa against state-ofthe-art off-policy deep RL algorithms, which demonstrated the most promising results in previous experiments. Throughout this section, we refer to SAC + SimBa as SimBa. Figure 8 presents the results, where the x-axis represents computation time using an RTX 3070 GPU, ) indicate higher compute and the y-axis denotes performance. Points in the upper-left corner ( efficiency. Here, SimBa consistently outperforms most baselines across various environments while requiring less computational time. The only exception is TD-MPC2 on HumanoidBench; however, TD-MPC2 requires 2.5 times the computational resources of SimBa to achieve better performance, making SimBa the most efficient choice overall. The key takeaway is that SimBa achieves these remarkable results without the bells and whisIt is easy to implement, which only retles often found in state-of-the-art deep RL algorithms. quires modifying the network architecture into SimBa without additional changes to the loss functions (Schwarzer et al., 2020; Hafner et al., 2023) or using domain-specific regularizers (Fujimoto et al., 2023; Nauman et al., 2024). Its effectiveness stems from an architectural design that leverages simplicity bias and scaling up the number of parameters, thereby maximizing performance. 7 Figure 9: Impact of Input Dimension. Average episode return for DMC tasks plotted against increasing state dimensions. Results show that the benefits of using SimBa increase with higher input dimensions, effectively alleviating the curse of dimensionality. Figure 10: On-policy RL with SimBa. Average return of achievements and task success rate for three different tasks comparing PPO + SimBa and PPO on Craftax. Integrating SimBa enables effective learning of complex behaviors. Impact of Input Dimension. To further identify in which cases SimBa offers significant benefits, we analyze its performance across DMC environments with varying input dimensions. As shown in Figure 9, the advantage of SimBa becomes more pronounced as input dimensionality increases. We hypothesize that higher-dimensional inputs exacerbate the curse of dimensionality, and the simplicity bias introduced by SimBa effectively mitigates overfitting in these high-dimensional settings. 6.2 ON-POLICY RL Experimental Setup. We conduct our on-policy RL experiments in Craftax (Matthews et al., 2024), an open-ended environment inspired by Crafter (Hafner, 2022) and NetHack (Kuttler et al., 2020). Craftax poses unique challenge with its open-ended structure and compositional tasks. Following Matthews et al. (2024), we use Proximal Policy Optimization (Schulman et al., 2017, PPO) as the baseline algorithm. When integrating SimBa with PPO, we replace the standard MLP-based actorcritic networks with SimBa and train both PPO and PPO + SimBa on 1024 parallel environments for total of 1 billion environment steps. Results. As illustrated in Figure 10, integrating SimBa into PPO significantly enhances performance across multiple complex tasks. With SimBa, the agent learns to craft iron swords and pickaxes more rapidly, enabling the early acquisition of advanced tools. Notably, by using these advanced tools, the SimBa-based agent successfully defeats challenging adversaries like the Orc Mage using significantly fewer time steps than an MLP-based agent. These improvements arise solely from the architectural change, demonstrating the effectiveness of the SimBa approach for on-policy RL. 6.3 UNSUPERVISED RL Experimental Setup. In our unsupervised RL study, we incorporate SimBa for online skill discovery, aiming to identify diverse behaviors without relying on task-specific rewards. We focus our experiments primarily on METRA (Park et al., 2023), which serves as the state-of-the-art algorithm in this domain. We evaluate METRA and METRA with SimBa on the Humanoid task from DMC, running 10M environment steps. For quantitative comparison, we adopt state coverage as our main metric. Coverage is measured by discretizing the and axes into grid and counting the number of grid cells covered by the learned behaviors at each evaluation epoch, following prior literature (Park et al., 2023; Kim et al., 2024a;b). Results. As illustrated in Figure 1.(a) and 11, integrating SimBa into METRA significantly enhances state coverage on the Humanoid task. The highdimensional input space makes it challenging for METRA to learn diverse skills. By injecting simplicity bias to manage high input dimensions and scaling up parameters to facilitate effective diverse skill acquisition, SimBa effectively leads to improved exploration and broader state coverage."
        },
        {
            "title": "7 ABLATIONS",
            "content": "Figure 11: URL with Simba. Integrating SimBa to METRA enhances state coverage. We conducted ablations on DMC-Hard with SAC + SimBa, running 5 seeds for each experiment."
        },
        {
            "title": "7.1 OBSERVATION NORMALIZATION",
            "content": "A key factor in SimBas success is using RSNorm for observation normalization. To validate its effectiveness, we compare 5 alternative methods: (i) Layer Normalization (Ba et al., 2016); (ii) Batch Normalization (Ioffe & Szegedy, 2015); (iii) Env Wrapper RSNorm, which tracks running statistics for each dimension, normalizes observations upon receiving from the environment, and stores them in the replay buffer; (iv) Initial Steps: fixed statistics derived from initially collected transition samples, where we used = 5, 000; and (v) Oracle Statistics, which rely on pre-computed statistics from previously collected expert data. As illustrated in Figure 12, layer normalization and batch normalization offer little to no performance gains. While the env-wrapper RSNorm is somewhat effective, it falls short of RSNorms performance. Although widely used in deep RL frameworks (Dhariwal et al., 2017; Hoffman et al., 2020; Raffin et al., 2021), the env-wrapper introduces inconsistencies in off-policy settings by normalizing samples with different statistics based on their collection time. This causes identical observations to be stored with varying values in the replay buffer, reducing learning consistency. Fixed initial statistics also show slightly worse performance than RSNorm, potentially due to their inability to adapt to the evolving dynamics during training. Overall, only RSNorm matched the performance of oracle statistics, making it the most practical observation normalization choice for deep RL. 7.2 SCALING THE NUMBER OF PARAMETERS Figure 12: Obs Normalization. RSNorm consistently outperforms alternative normalization methods. Mean and 95% CI over 5 seeds. Here, we investigate scaling parameters in the actor-critic architecture with SimBa by focusing on two aspects: (i) scaling the actor versus the critic network, and (ii) scaling network width versus depth. For width scaling, the actor and critic depths are set to 1 and 2 blocks, respectively. For depth scaling, the actor and critic widths are fixed at 128 and 512, respectively, following our default setup. As illustrated in Figure 13, scaling up the width or depth of the critic ( ) generally improves performance, while scaling up the actors width or depth ( ) tends to reduce performance. This contrast suggests that the target complexity of the actor may be lower than that of the critic, where scaling up the actors parameters might be ineffective. These findings align with previous study (Nauman et al., 2024), which highlights the benefits of scaling up the critic while showing limited advantages in scaling up the actor. Figure 13: Scaling Actor and Critic Network. Performance of SAC with SimBa by varying width and depth for the actor and critic network. 9 Furthermore, for the critic network, scaling up the width is generally more effective than scaling up the depth. While both approaches can enhance the networks expressivity, scaling up the depth can decrease the simplicity bias as it adds more non-linear components within the network. Based on our findings, we recommend width scaling as the default strategy."
        },
        {
            "title": "7.3 SCALING REPLAY RATIO",
            "content": "According to scaling laws (Kaplan et al., 2020), performance can be enhanced not only by scaling up the number of parameters but also by scaling up the computation time, which can be done by scaling up the number of gradient updates per collected sample (i.e., replay ratio) in deep RL. However, in deep RL, scaling up the replay ratio has been shown to decrease performance due to the risk of overfitting the network to the initially collected samples (Nikishin et al., 2022; DOro et al., 2022; Lee et al., 2023). To address this issue, recent research has proposed periodically reinitializing the network to prevent overfitting, demonstrating that performance can be scaled with respect to the replay ratio. In this section, we aim to assess whether the simplicity bias induced by SimBa can mitigate overfitting under increased computation time (i.e., higher replay ratios). To evaluate this, we trained SAC with SimBA using 2, 4, 8, and 16 replay ratios, both with and without periodic resets. For SAC with resets, we reinitialized the entire network and optimizer every 500,000 gradient steps. We also compared our results with BRO (Nauman et al., 2024), which incorporates resets. Surprisingly, as illustrated in Figure 14, SimBas performance consistently improves as the replay ratio increases, even without periodic resets. We have excluded results for BRO without resets, as it fails to learn meaningful behavior for all replay ratios (achieves lower than 300 points). Notably, when resets are included for SimBa, the performance gains become even more pronounced, with replay ratio of 8 outperforming the most computationally intensive BRO algorithm (RR10)."
        },
        {
            "title": "8 LESSONS AND OPPORTUNITIES",
            "content": "Figure 14: Scaling Replay Ratio. Performance of SimBa with and without periodic resets for various replay ratios. Lessons. Historically, deep RL has suffered from overfitting, necessitating sophisticated training protocols and implementation tricks to mitigate these issues (Hessel et al., 2018; Fujimoto et al., 2023). These complexities serve as significant barriers for practitioners with limited resources, hindering the effective usage of deep RL methods. In this paper, we improved performance by solely modifying the network architecture while keeping the underlying algorithms unchanged. This approach simplifies the implementation of SimBa, making it easy to adopt. By incorporating simplicity bias through architectural design and scaling up the number of parameters, the network converges to simpler, generalizable functions, matching or surpassing state-of-the-art deep RL methods. We also believe that SimBa follows the insights from Richard Suttons Bitter Lesson (Sutton, 2019): although task-specific designs can yield immediate improvements, an approach that scales effectively with increased computation may provide more sustainable benefits over the long term. Opportunities. Our exploration of simplicity bias has primarily concentrated on the network architecture; however, optimization techniques are equally vital. Strategies such as dropout (Hiraoka et al., 2021), data augmentation (Kostrikov et al., 2020), and diverse optimization algorithms (Foret et al., 2020) can further enhance convergence to simpler functions during training. Integrating these techniques with continued parameter scaling presents promising avenue for future research in deep RL. Furthermore, while our focus has been on the basic model-free algorithm SAC, there exists considerable empirical success with model-based algorithms (Schwarzer et al., 2020; Hansen et al., 2023; Hafner et al., 2023). Preliminary investigations into applying SimBa to model-based RL, specifically TD-MPC2 (Hansen et al., 2023), have shown promise. We encourage the research community to collaboratively pursue these avenues to advance deep RL architectures and facilitate their successful application in real-world scenarios."
        },
        {
            "title": "REPRODUCIBILITY",
            "content": "To ensure the reproducibility of our experiments, we provide the complete source code to run SAC with SimBa. Moreover, we have included Dockerfile to simplify the testing and replication of our results. The raw scores of each experiment including all baselines are also included. The code is available at https://github.com/SonyResearch/simba."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We would like to express our gratitude to Craig Sherstan, Bram Grooten, Hanseul Cho, Kyungmin Lee, and Hawon Jeong for their helpful discussions and insightful feedback on this paper."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. (Cited on page 2) Marcin Andrychowicz, Anton Raichuk, Piotr Stanczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What arXiv preprint matters in on-policy reinforcement learning? arXiv:2006.05990, 2020. (Cited on page 2) large-scale empirical study. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. (Cited on page 2, 9) Peter Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):463482, 2002. (Cited on page 3) Yakir Berchenko. Simplicity bias in overparameterized machine learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1105211060, 2024. (Cited on page 2, 3) Aditya Bhatt, Daniel Palenicek, Boris Belousov, Max Argus, Artemij Amiranashvili, Thomas Brox, and Jan Peters. Crossq: Batch normalization in deep reinforcement learning for greater sample efficiency and simplicity. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=PczQtTsTIX. (Cited on page 4) Nils Bjorck, Carla Gomes, and Kilian Weinberger. Towards deeper deep reinforcement learning with spectral normalization. Advances in neural information processing systems, 34:82428255, 2021. (Cited on page 4, 6) Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred Warmuth. Learnability and the vapnik-chervonenkis dimension. Journal of the ACM (JACM), 36(4):929965, 1989. (Cited on page 3) Vittorio Caggiano, Huawei Wang, Guillaume Durandau, Massimo Sartori, and Vikash Kumar. arXiv preprint Myosuitea contact-rich simulation suite for musculoskeletal motor control. arXiv:2205.13600, 2022. (Cited on page 3, 6, 21) Xinyue Chen, Che Wang, Zijian Zhou, and Keith Ross. Randomized ensembled double q-learning: Learning fast without model. arXiv preprint arXiv:2101.05982, 2021. (Cited on page 4) Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In Conference on learning theory, pp. 13051338. PMLR, 2020. (Cited on page 2, 3) Will Dabney, Mark Rowland, Marc Bellemare, and Remi Munos. Distributional reinforcement learning with quantile regression. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. (Cited on page 4) 11 Giacomo De Palma, Bobak Kiani, and Seth Lloyd. Random deep neural networks are biased towards simple functions. Advances in Neural Information Processing Systems, 32, 2019. (Cited on page 3, 17) Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pp. 74807512. PMLR, 2023. (Cited on page 2) Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https: //github.com/openai/baselines, 2017. (Cited on page 9) Pierluca DOro, Max Schwarzer, Evgenii Nikishin, Pierre-Luc Bacon, Marc Bellemare, and Aaron Courville. Sample-efficient reinforcement learning by breaking the replay ratio barrier. In Deep Reinforcement Learning Workshop NeurIPS 2022, 2022. (Cited on page 10) Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020. (Cited on page 10) Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In International conference on machine learning, pp. 15871596. PMLR, 2018. (Cited on page 4) Scott Fujimoto, Wei-Di Chang, Edward Smith, Shixiang Shane Gu, Doina Precup, and David Meger. For sale: State-action representation learning for deep reinforcement learning. arXiv preprint arXiv:2306.02451, 2023. (Cited on page 2, 3, 4, 7, 10) Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja, Jakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning. arXiv preprint arXiv:2407.04811, 2024. (Cited on page 4) Florin Gogianu, Tudor Berariu, Mihaela Rosca, Claudia Clopath, Lucian Busoniu, and Razvan Pascanu. Spectral normalisation for deep reinforcement learning: an optimisation perspective. In International Conference on Machine Learning, pp. 37343744. PMLR, 2021. (Cited on page 4) Gemini Google, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. (Cited on page 2) Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear convolutional networks. Advances in neural information processing systems, 31, 2018. (Cited on page 2, 3) Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. PMLR, 2018. (Cited on page 2, 5, 7) Danijar Hafner. Benchmarking the spectrum of agent capabilities. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= 1W0z96MFEoH. (Cited on page 8, 21) Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. (Cited on page 2, 4, 7, 10) Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous control. arXiv preprint arXiv:2310.16828, 2023. (Cited on page 2, 3, 4, 7, 10, 21, 24, 25) Fengxiang He, Tongliang Liu, and Dacheng Tao. Why resnet works? residuals generalize. IEEE transactions on neural networks and learning systems, 31(12):53495362, 2020. (Cited on page 2, 4) Katherine Hermann, Hossein Mobahi, Thomas Fel, and Michael Mozer. On the foundations of shortcut learning. Proc. the International Conference on Learning Representations (ICLR), 2024. (Cited on page 2, 4) Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. (Cited on page 2, 10) Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa TsuarXiv preprint ruoka. Dropout q-functions for doubly efficient reinforcement learning. arXiv:2110.02034, 2021. (Cited on page 10) Matthew W. Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev, Danila Sinopalnikov, Piotr Stanczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, Leonard Hussenot, Robert Dadashi, Gabriel Dulac-Arnold, Manu Orsini, Alexis Jacq, Johan Ferret, Nino Vieillard, Seyed Kamyar Seyed Ghasemipour, Sertan Girgin, Olivier Pietquin, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Abe Friesen, Ruba Haroun, Alex Novikov, Sergio Gomez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020. URL https://arxiv.org/abs/2006.00979. (Cited on page 9) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448456. pmlr, 2015. (Cited on page 9) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. (Cited on page 2, 10) Hyunseung Kim, Byung Kun Lee, Hojoon Lee, Dongyoon Hwang, Sejik Park, Kyushik Min, and Jaegul Choo. Learning to discover skills through guidance. Advances in Neural Information Processing Systems, 36, 2024a. (Cited on page 8) Hyunseung Kim, Byungkun Lee, Hojoon Lee, Dongyoon Hwang, Donghu Kim, and Jaegul arXiv preprint Choo. Dos and donts: Learning desirable skills with instruction videos. arXiv:2406.00324, 2024b. (Cited on page 8) Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020. (Cited on page 10) Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron Courville, George Tucker, and Sergey Levine. DR3: Value-based deep reinforcement learning requires explicit regularization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=POvMvLi91f. (Cited on page 19) Heinrich Kuttler, Nantas Nardelli, Alexander H. Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktaschel. The NetHack Learning Environment. In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS), 2020. (Cited on page 8, 21) Hojoon Lee, Hanseul Cho, Hyunseung Kim, Daehoon Gwak, Joonkee Kim, Jaegul Choo, Se-Young Yun, and Chulhee Yun. Plastic: Improving input and label plasticity for sample efficient reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. (Cited on page 4, 10) Hojoon Lee, Hyeonseo Cho, Hyunseung Kim, Donghu Kim, Dugki Min, Jaegul Choo, and Clare Lyle. Slow and steady wins the race: Maintaining plasticity with hare and tortoise networks. arXiv preprint arXiv:2406.02596, 2024. (Cited on page 4, 19) 13 TP Lillicrap."
        },
        {
            "title": "Continuous control with deep reinforcement",
            "content": "learning. arXiv preprint arXiv:1509.02971, 2015. (Cited on page 7) Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will Dabney. Understanding plasticity in neural networks. Proc. the International Conference on Machine Learning (ICML), 2023. (Cited on page 19) Michael Matthews, Michael Beukman, Benjamin Ellis, Mikayel Samvelyan, Matthew Thomas Jackson, Samuel Coward, and Jakob Nicolaus Foerster. Craftax: lightning-fast benchmark for open-ended reinforcement learning. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=hg4wXlrQCV. (Cited on page 8, 21, 25) Chris Mingard, Joar Skalse, Guillermo Valle-Perez, David Martınez-Rubio, Vladimir Mikulik, and Ard Louis. Neural networks are priori biased towards boolean functions with low entropy. arXiv preprint arXiv:1909.11522, 2019. (Cited on page 2, 3, 4, 17) Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021, 2021. (Cited on page 2) Michal Nauman, Mateusz Ostaszewski, Krzysztof Jankowski, Piotr Miłos, and Marek Cygan. Bigger, regularized, optimistic: scaling for compute and sample-efficient continuous control. arXiv preprint arXiv:2405.16158, 2024. (Cited on page 3, 4, 6, 7, 9, 10) Evgenii Nikishin, Max Schwarzer, Pierluca DOro, Pierre-Luc Bacon, and Aaron Courville. The primacy bias in deep reinforcement learning. Proc. the International Conference on Machine Learning (ICML), 2022. (Cited on page 4, 10, 19) Johan Obando-Ceron, Ghada Sokar, Timon Willi, Clare Lyle, Jesse Farebrother, Jakob Foerster, Gintare Karolina Dziugaite, Doina Precup, and Pablo Samuel Castro. Mixtures of experts unlock parameter scaling for deep rl. arXiv preprint arXiv:2402.08609, 2024. (Cited on page 4) Seohong Park, Oleh Rybkin, and Sergey Levine. Metra: Scalable unsupervised rl with metric-aware abstraction. arXiv preprint arXiv:2310.08887, 2023. (Cited on page 3, 8, 21, 25) Scott Pesme, Loucas Pillaud-Vivien, and Nicolas Flammarion. Implicit bias of sgd for diagonal linear networks: provable benefit of stochasticity. Advances in Neural Information Processing Systems, 34:2921829230, 2021. (Cited on page 2, 4) Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Journal of Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Machine Learning Research, 22(268):18, 2021. URL http://jmlr.org/papers/v22/ 20-1364.html. (Cited on page 9) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. (Cited on page 2, 8, 25) Max Schwarzer, Ankesh Anand, Rishab Goel, Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. arXiv preprint arXiv:2007.05929, 2020. (Cited on page 7, 10) Max Schwarzer, Johan Samir Obando Ceron, Aaron Courville, Marc Bellemare, Rishabh Agarwal, and Pablo Samuel Castro. Bigger, better, faster: Human-level atari with human-level efIn International Conference on Machine Learning, pp. 3036530380. PMLR, 2023. ficiency. (Cited on page 4) Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, and Pieter Abbeel. Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation. arXiv preprint arXiv:2403.10506, 2024. (Cited on page 3, 6, 21) Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. Advances in Neural Information Processing Systems, 33:95739585, 2020. (Cited on page 2, 3) 14 Claude Elwood Shannon. Communication in the presence of noise. Proceedings of the IRE, 37(1): 1021, 1949. (Cited on page 3, 17) Ghada Sokar, Rishabh Agarwal, Pablo Samuel Castro, and Utku Evci. The dormant neuron phenomenon in deep reinforcement learning. arXiv preprint arXiv:2302.12902, 2023. (Cited on page 19) Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(70): 157, 2018. (Cited on page 3) Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. (Cited on page 10) Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. (Cited on page 2, 6, 21) Damien Teney, Armand Mihai Nicolicioiu, Valentin Hartmann, and Ehsan Abbasnejad. Neural redshift: Random networks are not random functions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 47864796, 2024. (Cited on page 2, 3, 4, 5, 17, 18) Guillermo Valle-Perez, Chico Camargo, and Ard Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. arXiv preprint arXiv:1805.08522, 2018. (Cited on page 2, 3, 4, 17) Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qIn Proceedings of the AAAI conference on artificial intelligence, volume 30, 2016. learning. (Cited on page 4) Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. (Cited on page 5) Lei Wu, Mingze Wang, and Weijie Su. The alignment property of sgd noise and how it helps select flat minima: stability analysis. Advances in Neural Information Processing Systems, 35:4680 4693, 2022. (Cited on page 4) Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 1052410533. PMLR, 2020. (Cited on page 2)"
        },
        {
            "title": "APPENDIX",
            "content": "A Definition of Simplicity Bias A.1 Complexity Measure . A.2 Simplicity Bias Score . . . Measuring Simplicity Bias Plasticity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Comparison to Existing Architectures Computational Resources Environment Details F.1 DeepMind Control Suite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 MyoSuite . . . . . F.3 HumanoidBench . F.4 Craftax . . . . . . Hyperparameters G.1 Off-Policy . G.2 On-Policy . . . . . . . . . G.3 Unsupervised RL . . . . . . . . . . . . . . . . . . . Learning Curve Full Result 17 17 18 19 20 20 21 21 21 21 24 24 25 26"
        },
        {
            "title": "A DEFINITION OF SIMPLICITY BIAS",
            "content": "This section provides more in-depth definition of simplicity bias for clarity. A.1 COMPLEXITY MEASURE Let Rn denote the input space and Rm the output space. Consider the function space = {f : Y}. Given training dataset = {(xi, yi)}N i=1 and tolerance ϵ > 0, define the set of functions achieving training loss below ϵ: (cid:40) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Ltrain(f ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:41) ℓ(f (xi), yi) < ϵ , (8) where ℓ : [0, ) is loss function such as mean squared error or cross-entropy. To quantify the complexity of functions within F, we adopt Fourier-based complexity measure as described in Section 2. Specifically, for function with Fourier series representation: (x) = (2π)d/2 (cid:90) (k) eikx dk, (9) where (k) = (cid:82) (x) eikx dx is the Fourier transform of . Given Fourier series representation , we perform discrete Fourier transform by uniformly discretizing the frequency domain. Specifically, we consider frequencies {0, 1, . . . , K}, where is selected based on the Nyquist-Shannon sampling theorem (Shannon, 1949) to ensure an accurate representation of . This discretization transforms the continuous frequency domain into finite set of frequencies suitable for computational purposes. Then, the complexity measure : [0, ) is defined as the weighted average of the Fourier coefficients from discretization: c(f ) = . (10) (cid:80)K (k) (k) k=0 (cid:80)K k=0 Equation 10 captures the intuition that higher complexity arises from the dominance of highfrequency components, which correspond to rapid amplitude changes or intricate details in the function . Conversely, lower complexity measure indicates greater contribution from low-frequency components, reflecting simpler, smoother functions. A.2 SIMPLICITY BIAS SCORE Let be neural network, and let alg denote the optimization algorithm used for training, such as stochastic gradient descent. We quantify the simplicity bias score : (0, ) as: s(f, alg) = Ef Pf,alg (cid:21) (cid:20) 1 c(f ) (11) where Pf,alg denotes the distribution over induced by training with alg. higher simplicity bias score indicates stronger tendency to converge toward functions with lower complexity. Then, from equation 11, we can compute the simplicity bias score with respect to the architecture by marginalizing over the distribution of the algorithm as: s(f ) = EalgA (cid:20) Ef Pf,alg (cid:20) 1 c(f ) (cid:21)(cid:21) . (12) Directly measuring this bias by analyzing the complexity of the converged function is challenging due to the non-stationary nature of training dynamics, especially in reinforcement learning where data distributions evolve over time. Empirical studies (Valle-Perez et al., 2018; De Palma et al., 2019; Mingard et al., 2019; Teney et al., 2024) suggest that the initial complexity of network is strongly correlated with the complexity of the functions it learns after training. 17 Therefore, as practical proxy for simplicity bias, we define the approximate of the simplicity bias score s(f ) for network architecture with an initial parameter distribution Θ0 as: s(f ) EθΘ0 (cid:20) 1 c(fθ) (cid:21) . (13) where fθ denotes the network parameterized by θ. higher simplicity bias score indicates that, on average, the network initialized from Θ0 represents simpler functions prior to training, suggesting predisposition to converge to simpler solutions during optimization. This measure aligns with the notion that networks with lower initial complexity are more likely to exhibit higher simplicity bias."
        },
        {
            "title": "B MEASURING SIMPLICITY BIAS",
            "content": "To quantify the simplicity bias defined in Equation 13, we adopt methodology inspired by the Neural Redshift framework (Teney et al., 2024). We utilize the original codebase provided by the authors1 to assess the complexity of random neural networks. This approach evaluates the inherent complexity of the architecture before any optimization, thereby isolating the architectural effects from those introduced by training algorithms such as stochastic gradient descent. Following the methodology outlined in Teney et al. (2024), we perform the following steps to measure the simplicity bias score of neural network fθ(): Sampling Initialization. For each network architecture , we generate random initializations θ Θ0. This ensemble of initial parameters captures the variability in complexity introduced by different random seeds. In this work, we used = 100. Sampling Input. We define the input space = [100, 100]2 R2 and uniformly divide it into grid of 90,000 points, achieving 300 divisions along each dimension. This dense sampling ensures comprehensive coverage of the input domain. Evaluating Function. For each sampled input point , we compute the corresponding output fθ(x). The collection of these output values forms 2-D grid of scalars, effectively representing the networks function as grayscale image. Discrete Fourier Transform. We apply discrete Fourier transform (DFT) to the grayscale image obtained from the function evaluations. This transformation decomposes fθ() into sum of basis functions of varying frequencies. Measuring Function Complexity. Utilizing the Fourier coefficients obtained from the DFT, we compute the complexity measure c(fθ) as defined in Equation 10. Specifically, we calculate the frequency-weighted average of the Fourier coefficients: (cid:80)K fθ(k) fθ(k) where fθ(k) denotes the Fourier coefficient at frequency k. Estimating Simplicity Bias Score. For each network , we estimate the simplicity bias score s(f ) by averaging the inverse of the complexity measures over all random initializations: c(fθ) = k=0 (cid:80)K (14) k= , s(f ) 1 (cid:88) i=1 1 c(fθi) , (15) where θi represents the i-th random initialization. 1https://github.com/ArmandNM/neural-redshift"
        },
        {
            "title": "C PLASTICITY ANALYSIS",
            "content": "Recent studies have identified the loss of plasticity as significant challenge in non-stationary training scenarios, where neural networks gradually lose their ability to adapt to new data over time (Nikishin et al., 2022; Lyle et al., 2023; Lee et al., 2024). This phenomenon can severely impact the performance and adaptability of models in dynamic learning environments such as RL. To quantify and understand this issue, several metrics have been proposed, including stable-rank (Kumar et al., 2022), dormant ratio (Sokar et al., 2023), and L2-feature norm (Kumar et al., 2022). The stable-rank assesses the rank of the feature matrix, reflecting the diversity and richness of the representations learned by the network. This is achieved by performing an eigen decomposition on the covariance matrix of the feature matrix and counting the number of singular values σj that exceed predefined threshold τ : s-Rank = (cid:88) j=1 I(σj > τ ) (16) where Rdm is the feature matrix with samples and features, σj are the singular values, and I() is the indicator function. The dormant ratio measures the proportion of neurons with negligible activation. It is calculated as the number of neurons with activation norms below small threshold ϵ divided by the total number of neurons D: where ai represents the activation of neuron i. Dormant Ratio = {i ai < ϵ} (17) The L2-feature norm represents the average L2 norm of the feature vectors across all samples: Feature Norm = 1 (cid:88) i= Fi2 (18) where Fi is the feature vector for the i-th sample. Large feature norms can signal overactive neurons, potentially leading to numerical instability. To assess how different architectures impact plasticity, we conducted experiments in DMC-Hard. We compared Soft Actor-Critic (SAC) implemented with standard MLP architecture against SAC with the SimBa architecture. Each configuration was evaluated across 5 random seeds. Figure 15: Plasticity Metrics Comparison. Average episode return for the DMC-Hard environment. Metrics compared include dormant ratio, s-rank, and feature norm. Higher dormant ratio and feature norm, along with lower s-rank, indicate greater loss of plasticity. Figure 15 shows that SAC with the MLP architecture exhibits high dormant ratio, low s-rank, and large feature norms. These metrics indicate significant loss of plasticity. In contrast, the SimBa architecture maintains lower dormant ratios, higher s-rank values, and balanced feature norms. This ensures more active neurons, diverse and rich feature representations, and stable activations. These results demonstrate that SimBa effectively preserves plasticity, avoiding the degenerative trends seen with the MLP-based SAC. It will be interesting for future work to understand how simplicity bias and architectural choices influence network plasticity."
        },
        {
            "title": "D COMPARISON TO EXISTING ARCHITECTURES",
            "content": "Here we provide more in-depth discussion related to the architectural difference between SimBa, BroNet, and SpectralNet. As shown in Figure 16, SimBa differs from BroNet in three key aspects: (i) the inclusion of RSNorm, (ii) the implementation of pre layer-normalization, (iii) the utilization of linear residual pathway, (iv) and the inclusion of post-layer normalization layer. Similarly, compared to SpectralNet, SimBa incorporates RSNorm, employs linear residual pathway, and leverages spectral normalization differently. Figure 16: Architecture Comparison. Illustration of SimBa, BroNet, and SpectralNet."
        },
        {
            "title": "E COMPUTATIONAL RESOURCES",
            "content": "The training was performed using NVIDIA RTX 3070, A100, or H100 GPUs for neural network computations and either 16-core Intel i7-11800H or 32-core AMD EPYC 7502 CPU for running simulators. Our software environment included Python 3.10, CUDA 12.2, and Jax 4.26. When benchmarking computation time, experiments were conducted on same hardware equipped with an NVIDIA RTX 3070 GPU and 16-core Intel i7-11800H CPU. 20 Table 1: Environment details. We list the episode length, action repeat for each domain, total environment steps, and performance metrics used for benchmarking SimBa. DMC MyoSuite HumanoidBench Craftax Episode length Action repeat Effective length Total env. steps Performance metric Average Return Average Success 1, 000 2 500 500K-1M 100 2 50 1M 500 - 1, 000 2 250 - 500 2M Average Return 1 1B Average Score"
        },
        {
            "title": "F ENVIRONMENT DETAILS",
            "content": "This section details the benchmark environments used in our evaluation. We list all tasks from each benchmark along with their observation and action dimensions. Visualizations of each environment are provided in Figure 6, and detailed environment information is available in Table 1. F.1 DEEPMIND CONTROL SUITE DeepMind Control Suite (Tassa et al., 2018, DMC) is standard benchmark for continuous control, featuring range of locomotion and manipulation tasks with varying complexities, from simple low-dimensional tasks (s R3) to highly complex ones (s R223). We evaluate 27 DMC tasks, categorized into two groups: DMC-Easy&Medium and DMC-Hard. All Humanoid and Dog tasks are classified as DMC-Hard, while the remaining tasks are grouped under DMC-Easy&Medium. The complete lists of DMC-Easy&Medium and DMC-Hard are provided in Tables 2 and 3, respectively. For our URL experiments, we adhere to the protocol in Park et al. (2023), using an episode length of 400 steps and augmenting the agents observations with its x, y, and coordinates. For the representation function ϕ (i.e., reward network) in METRA, we only used x, y, and positions as input. This approach effectively replaces the colored floors used in the pixel-based humanoid setting of METRA with state-based inputs. F.2 MYOSUITE MyoSuite (Caggiano et al., 2022) simulates musculoskeletal movements with high-dimensional state and action spaces, focusing on physiologically accurate motor control. It includes benchmarks for complex object manipulation using dexterous hand. We evaluate 10 MyoSuite tasks, categorized as easy when the goal is fixed and hard when the goal is randomized, following Hansen et al. (2023). The full list of MyoSuite tasks is presented in Table 4. F.3 HUMANOIDBENCH HumanoidBench (Sferrazza et al., 2024) is high-dimensional simulation benchmark designed to advance humanoid robotics research. It features the Unitree H1 humanoid robot with dexterous hands, performing variety of challenging locomotion and manipulation tasks. These tasks range from basic locomotion to complex activities requiring precise manipulation. The benchmark facilitates algorithm development and testing without the need for costly hardware by providing simulated platform. In our experiments, we focus on 14 locomotion tasks, simplifying the setup by excluding the dexterous hands. This reduces the complexity associated with high degrees of freedom and intricate dynamics. All HumanoidBench scores are normalized based on each tasks target success score as provided by the authors. complete list of tasks is available in Table 5. F.4 CRAFTAX Craftax (Matthews et al., 2024) is an open-ended RL environment that combines elements from Crafter (Hafner, 2022) and NetHack (Kuttler et al., 2020). It presents challenging scenario requiring the sequential completion of numerous tasks. key feature of Craftax is its support for vectorized parallel environments and full end-to-end GPU learning pipeline, enabling large number 21 of environment steps at low computational cost. The original baseline performance in Craftax is reported as percentage of the maximum score (226). In our experiments, we report agents raw average scores instead. Table 2: DMC Easy & Medium. We consider total of 20 continuous control tasks for the DMC Easy & Medium benchmark. We list all considered tasks below and baseline performance for each task is reported at 500K environment steps. Task Observation dim Action dim"
        },
        {
            "title": "Acrobot Swingup\nCartpole Balance\nCartpole Balance Sparse\nCartpole Swingup\nCartpole Swingup Sparse\nCheetah Run\nFinger Spin\nFinger Turn Easy\nFinger Turn Hard\nFish Swim\nHopper Hop\nHopper Stand\nPendulum Swingup\nQuadruped Run\nQuadruped Walk\nReacher Easy\nReacher Hard\nWalker Run\nWalker Stand\nWalker Walk",
            "content": "6 5 5 5 5 17 9 12 12 24 15 15 3 78 78 6 6 24 24 24 1 1 1 1 1 6 2 2 2 5 4 4 1 12 12 2 2 6 6 6 Table 3: DMC Hard. We consider total of 7 continuous control tasks for the DMC Hard benchmark.We list all considered tasks below and baseline performance for each task is reported at 1M environment steps. Task Observation dim Action dim Dog Run Dog Trot Dog Stand Dog Walk Humanoid Run Humanoid Stand Humanoid Walk 223 223 223 223 67 67 67 38 38 38 38 24 24 24 22 Table 4: MyoSuite. We consider total of 10 continuous control tasks for the MyoSuite benchmark, which encompasses both fixed-goal (Easy) and randomized-goal (Hard) settings. We list all considered tasks below and baseline performance for each task is reported at 1M environment steps. Task Observation dim Action dim"
        },
        {
            "title": "Key Turn Easy\nKey Turn Hard\nObject Hold Easy\nObject Hold Hard\nPen Twirl Easy\nPen Twirl Hard\nPose Easy\nPose Hard\nReach Easy\nReach Hard",
            "content": "93 93 91 91 83 83 108 108 115 115 39 39 39 39 39 39 39 39 39 39 Table 5: HumanoidBench. We consider total of 14 continuous control locomotion tasks for the UniTree H1 humanoid robot from the HumanoidBench domain. We list all considered tasks below and baseline performance for each task is reported at 2M environment steps. Task Observation dim Action dim Balance Hard Balance Simple Crawl Hurdle Maze Pole Reach Run Sit Simple Sit Hard Slide Stair Stand Walk 77 64 51 51 51 51 57 51 51 64 51 51 51 51 19 19 19 19 19 19 19 19 19 19 19 19 19 19 Table 6: Craftax. We consider the symbolic version of Craftax. Baseline performance is reported at 1B environment steps. Task Observation dim Action dim Craftax-Symbolic-v 8268"
        },
        {
            "title": "G HYPERPARAMETERS",
            "content": "G.1 OFF-POLICY Table 7: SAC+SimBa hyperparameters. The hyperparameters listed below are used consistently across all tasks when integrating SimBa with SAC. For the discount factor, we set it automatically using heuristics used by TD-MPC2 (Hansen et al., 2023). Hyperparameter Critic block type Critic num blocks Critic hidden dim Critic learning rate Target critic momentum (τ ) Actor block type Actor num blocks Actor hidden dim Actor learning rate Initial temperature (α0) Temperature learning rate Target entropy (H) Batch size Optimizer Optimizer momentum (β1, β2) Weight decay (λ) Discount (γ) Replay ratio Clipped Double Value SimBa Residual 2 512 1e-4 5e-3 SimBa Residual 1 128 1e-4 1e-2 1e-4 A/2 256 AdamW (0.9, 0.999) 1e-2 Heuristic 2 HumanoidBench: True Other Envs: False Table 8: DDPG+SimBa hyperparameters. The hyperparameters listed below are used consistently across all tasks when integrating SimBa with DDPG. For the discount factor, we set it automatically using heuristics used by TD-MPC2 (Hansen et al., 2023). Hyperparameter Critic block type Critic num blocks Critic hidden dim Critic learning rate Target critic momentum (τ ) Actor block type Actor num blocks Actor hidden dim Actor learning rate Exploration noise Batch size Optimizer Optimizer momentum (β1, β2) Weight decay (λ) Discount (γ) Replay ratio Value SimBa Residual 2 512 1e-4 5e-3 SimBa Residual 1 128 1e-4 (0, 0.12) 256 AdamW (0.9, 0.999) 1e-2 Heuristic 2 Table 9: TDMPC2+SimBa hyperparameters. We provide detailed list of the hyperparameters used for the shared encoder module in TD-MPC2 (Hansen et al., 2023). Aside from these, we follow the hyperparameters specified in the original TD-MPC2 paper. The listed hyperparameters are applied uniformly across all tasks when integrating SimBa with TD-MPC2. Hyperparameter Value"
        },
        {
            "title": "Encoder block type\nEncoder num blocks\nEncoder hidden dim\nEncoder learning rate\nEncoder weight decay",
            "content": "SimBa Residual 2 256 1e-4 1e-1 G.2 ON-POLICY Table 10: PPO+SimBa hyperparameters. We provide detailed list of the hyperparameters when integrating SimBa with PPO (Schulman et al., 2017). Aside from these, we follow the hyperparameters specified in the Craftax paper (Matthews et al., 2024). Hyperparameter Critic block type Critic num blocks Critic hidden dim Critic learning rate Actor block type Actor num blocks Actor hidden dim Actor learning rate Optimizer Optimizer momentum (β1, β2) Optimizer eps Weight decay (λ) Value SimBa Residual 2 512 1e-4 SimBa Residual 1 256 1e-4 AdamW (0.9, 0.999) 1e-8 1e-2 G.3 UNSUPERVISED RL Table 11: METRA+SimBa hyperparameters. We provide detailed list of the hyperparameters when integrating SimBa with METRA (Park et al., 2023). Aside from these, we follow the hyperparameters specified in the original METRA paper. Hyperparameter Critic block type Critic num blocks Critic hidden dim Actor block type Actor num blocks Actor hidden dim Value SimBa Residual 2 512 SimBa Residual 1"
        },
        {
            "title": "H LEARNING CURVE",
            "content": "Figure 17: Per task learning curve for DMC Easy&Medium. Mean and 95% CI over 10 seeds for SimBa and BRO, 5 seeds for TD7 and SAC, 3 seeds for TD-MPC2 and DreamerV3. Figure 18: Per task learning curve for DMC Hard. Mean and 95% CI over 10 seeds for SimBa and BRO, 5 seeds for TD7 and SAC, 3 seeds for TD-MPC2 and DreamerV3. 26 Figure 19: Per task learning curve for MyoSuite. Mean and 95% CI over 10 seeds for SimBa and BRO, 5 seeds for TD7 and SAC, 3 seeds for TD-MPC2 and DreamerV3. Figure 20: Per task learning curve for HumanoidBench. Mean and 95% CI over 10 seeds for SimBa, 5 seeds for BRO, TD7, and SAC, 3 seeds for TD-MPC2 and DreamerV3. We no Figure 21: Per task learning curve for Craftax. We visualize the success rate learning curve for 66 tasks in Craftax. Mean and 95% CI over 5 seeds for PPO + SimBa and PPO."
        },
        {
            "title": "I FULL RESULT",
            "content": "Table 12: Per task results for DMC Easy&Medium. Results for SimBa and BRO are averaged over 10 seeds, for TD7 and SAC over 5 seeds, and for TD-MPC2 and DreamerV3 over 3 seeds."
        },
        {
            "title": "BRO",
            "content": "TD"
        },
        {
            "title": "SAC",
            "content": "TD-MPC2 DreamerV"
        },
        {
            "title": "Acrobot Swingup\nCartpole Balance\nCartpole Balance Sparse\nCartpole Swingup\nCartpole Swingup Sparse\nCheetah Run\nFinger Spin\nFinger Turn Easy\nFinger Turn Hard\nFish Swim\nHopper Hop\nHopper Stand\nPendulum Swingup\nQuadruped Run\nQuadruped Walk\nReacher Easy\nReacher Hard\nWalker Run\nWalker Stand\nWalker Walk",
            "content": "IQM Median Mean OG 331.57 999.05 940.53 866.53 823.97 814.97 778.83 881.33 860.22 786.73 326.69 811.75 824.53 883.68 952.96 972.23 965.96 687.16 983.02 970.73 885.70 816.78 823.12 0.1769 390.78 998.66 954.21 878.69 833.18 739.67 987.45 905.85 905.30 680.34 315.04 910.88 816.20 818.62 936.06 933.77 956.52 754.43 986.58 973.41 888.02 836.62 833.78 0.1662 39.83 998.79 988.58 878.09 480.74 897.76 592.74 485.66 596.32 108.84 110.27 445.50 461.40 515.13 910.55 920.38 549.50 782.32 984.63 976. 740.19 623.18 636.18 0.3638 57.56 998.79 1000.00 863.24 779.99 716.43 814.69 903.07 775.21 462.67 159.43 845.89 476.58 116.91 147.83 951.80 959.59 629.44 972.59 956.67 799.75 676.59 679.42 0.3206 361.07 993.93 1000.00 876.07 844.77 757.60 984.63 854.67 876.27 610.23 303.27 936.47 841.70 939.63 957.17 919.43 913.73 820.40 957.17 978.70 895.47 836.93 836.34 0.1637 360.46 994.95 800.25 863.90 262.69 686.25 434.06 851.11 769.86 603.34 192.70 722.42 825.17 471.25 472.31 888.36 935.25 620.13 963.28 925. 748.58 684.18 682.16 0.3178 Table 13: Per task results for DMC Hard. Results for SimBa and BRO are averaged over 10 seeds, for TD7 and SAC over 5 seeds, and for TD-MPC2 and DreamerV3 over 3 seeds. Method SimBa BRO TD SAC TD-MPC2 DreamerV3 Dog Run Dog Stand Dog Trot Dog Walk Humanoid Run Humanoid Stand Humanoid Walk IQM Median Mean OG 544.86 960.38 824.69 916.80 181.57 846.11 668.48 773.28 706.39 706.13 0. 374.63 966.97 783.12 931.46 204.96 920.11 672.55 771.50 694.20 693.40 0.3066 127.48 753.23 126.00 280.87 79.32 389.80 252.72 216.04 272.62 287.06 0.7129 36.86 102.04 29.36 38.14 116.97 352.72 273.67 69.03 159.36 135.68 0. 169.87 798.93 500.03 493.93 184.57 663.73 628.23 527.11 528.26 491.33 0.5087 15.72 55.87 10.19 23.36 0.91 5.12 1.33 9.63 17.13 16.07 0.9839 29 Table 14: Per task results for MyoSuite. Results for SimBa and BRO are averaged over 10 seeds, for TD7 and SAC over 5 seeds, and for TD-MPC2 and DreamerV3 over 3 seeds."
        },
        {
            "title": "BRO",
            "content": "TD"
        },
        {
            "title": "SAC",
            "content": "TD-MPC2 DreamerV"
        },
        {
            "title": "Key Turn Easy\nKey Turn Hard\nObject Hold Easy\nObject Hold Hard\nPen Twirl Easy\nPen Twirl Hard\nPose Easy\nPose Hard\nReach Easy\nReach Hard",
            "content": "IQM Median Mean OG 100.00 7.00 90.00 96.00 80.00 77.00 100.00 0.00 100.00 93.00 95.20 77.00 74.30 99.93 100.00 42.00 90.00 42.00 90.00 76.00 100.00 0.00 100.00 74.00 87.60 72.00 71.40 99.93 100.00 0.00 20.00 10.00 100.00 12.00 0.00 0.00 100.00 14. 22.31 34.00 35.60 99.96 100.00 10.00 90.00 96.00 50.00 55.00 90.00 0.00 100.00 16.00 71.40 62.50 60.70 99.94 100.00 0.00 100.00 56.67 70.00 40.00 100.00 0.00 100.00 83.33 77.50 65.00 65.00 99.94 88.89 0.00 33.33 9.44 96.67 53.33 100.00 0.00 100.00 0. 46.56 47.00 48.17 99.95 Table 15: Per task results for HumanoidBench. Results for SimBa are averaged over 10 seeds, for BRO, TD7 and SAC over 5 seeds, and for TD-MPC2 and DreamerV3 over 3 seeds. Method SimBa BRO TD SAC TD-MPC2 DreamerV3 Balance Hard Balance Simple Crawl Hurdle Maze Pole Reach Run Sit Hard Sit Simple Slide Stair Stand Walk IQM Median Mean OG 137.20 816.38 1370.51 340.60 283.58 1036.70 523.10 741.16 783.95 1059.73 577.87 527.49 906.94 1202.91 747.43 733.09 736.30 0. 145.95 246.57 1373.83 128.60 259.03 915.89 317.99 429.74 989.07 1151.84 653.17 249.86 780.52 1080.63 558.03 632.93 623.05 0.4345 79.90 132.82 868.63 200.28 179.23 830.72 159.37 196.85 293.96 1183.26 197.07 77.19 1005.54 143.86 256.77 385.41 396.33 0.6196 69.02 113.38 830.56 31.89 254.38 760.78 347.92 168.25 345.65 994.58 208.46 65.53 1029.78 412.21 311.46 399.93 402.31 0. 64.56 50.69 1384.40 1000.12 198.65 1269.57 505.61 1130.94 1027.47 1074.96 780.82 398.21 1020.59 1165.42 885.67 796.18 787.64 0.2904 16.07 14.09 906.66 18.78 114.90 289.18 341.62 85.93 9.95 40.53 24.43 49.04 280.99 125.59 72.30 160.49 165.55 0."
        }
    ],
    "affiliations": [
        "Coventry University",
        "KAIST",
        "Sony AI",
        "UT Austin"
    ]
}