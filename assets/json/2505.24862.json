{
    "paper_title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization",
    "authors": [
        "Cailin Zhuang",
        "Ailin Huang",
        "Wei Cheng",
        "Jingwei Wu",
        "Yaoqi Hu",
        "Jiaqi Liao",
        "Zhewei Huang",
        "Hongyuan Wang",
        "Xinyao Liao",
        "Weiwei Cai",
        "Hengyuan Xu",
        "Xuanyang Zhang",
        "Xianfang Zeng",
        "Gang Yu",
        "Chi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Story visualization, which aims to generate a sequence of visually coherent images aligning with a given narrative and reference images, has seen significant progress with recent advancements in generative models. To further enhance the performance of story visualization frameworks in real-world scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We collect a diverse dataset encompassing various story types and artistic styles, ensuring models are evaluated across multiple dimensions such as different plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D renderings). ViStoryBench is carefully curated to balance narrative structures and visual elements, featuring stories with single and multiple protagonists to test models' ability to maintain character consistency. Additionally, it includes complex plots and intricate world-building to challenge models in generating accurate visuals. To ensure comprehensive comparisons, our benchmark incorporates a wide range of evaluation metrics assessing critical aspects. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, fostering targeted improvements."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 2 6 8 4 2 . 5 0 5 2 : r ViStoryBench: Comprehensive Benchmark Suite for Story Visualization Cailin Zhuang1,2,3, Ailin Huang2,, Wei Cheng2, Jingwei Wu2 Yaoqi Hu3 Jiaqi Liao2,4 Hongyuan Wang2 Xinyao Liao2 Weiwei Cai2 Hengyuan Xu2 Xuanyang Zhang2 Xianfang Zeng2 Zhewei Huang2, Gang Yu2, Chi Zhang4, Equal contribution Project lead Corresponding authors 1 ShanghaiTech University 2 StepFun 3 AIGC Research 4 AGI Lab, Westlake University Dataset Code Project Page Story Explorer"
        },
        {
            "title": "Abstract",
            "content": "Story visualization, which aims to generate sequence of visually coherent images aligning with given narrative and reference images, has seen significant progress with recent advancements in generative models. To further enhance the performance of story visualization frameworks in real-world scenarios, we introduce comprehensive evaluation benchmark, ViStoryBench. We collect diverse dataset encompassing various story types and artistic styles, ensuring models are evaluated across multiple dimensions such as different plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D renderings). ViStoryBench is carefully curated to balance narrative structures and visual elements, featuring stories with single and multiple protagonists to test models ability to maintain character consistency. Additionally, it includes complex plots and intricate world-building to challenge models in generating accurate visuals. To ensure comprehensive comparisons, our benchmark incorporates wide range of evaluation metrics assessing critical aspects. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, fostering targeted improvements."
        },
        {
            "title": "Introduction",
            "content": "In recent years, story visualization [77, 6, 89] has become an increasingly fascinating and rapidly evolving domain. The primary objective of story visualization is to produce sequence of visually consistent images that faithfully represent given narrative and align with provided reference images, ultimately delivering an engaging and immersive storytelling experience. This capability holds immense potential across various domains, including entertainment and education. Recent progress in generative models [75, 40, 48] have propelled the AI-driven creation of images and videos into practical, open-domain applications. Compared to general image or video generation quality evaluation [24, 23, 64, 21, 62], evaluating story visualization involves more dimensions. In addition to considering the diversity and aesthetics of the generated images, story visualization also needs to focus on character portrayal. Recent works [71, 77, 74] often only observe limited number of indicators for experiments, and there is no uniformity across different literature. Moreover, evaluating models on restricted topics and scenarios does not fully reflect their capabilities in real-world situations. To comprehensively advance the Figure 1: Overview of scripts of ViStoryBench. Dataset Construction: we create story construction pipeline empowered by LLMs. Casts: the character reference images are collected manually with the same style. Dataset Statistics: Ssamples and distribution of realistic and unrealistic story are shown upper, and story category and casts statistics are illustrated below. performance of the story visualization framework and various methods in in-the-wild scenarios, there is an urgent need to establish comprehensive evaluation benchmark. In this paper, we introduce ViStoryBench, comprehensive benchmark for story visualization. Our benchmark is characterized by the following key features: Multifaceted Dataset Creation: We focus on creating diverse dataset that encompasses various story types and artistic styles. This diversity ensures that models can be evaluated across multiple dimensions, including different plots (such as comedy and horror) and visual aesthetics (like anime and 3D renderings). We carefully curate 80 story segments with 344 roles to balance narrative structures and visual elements. ViStoryBench includes stories with both single and multiple protagonists, testing the models ability to maintain character consistency. Additionally, it features complex plots and intricate world-building, challenging models to generate accurate visual content. Comprehensive Evaluation Metrics: Beyond the general evaluation metrics for image generation methods, such as image quality, diversity, and general prompt adherence, we have also quantified several attributes that are particularly important in story visualization. These include the stylistic consistency of the entire generated sequence, the alignment of characters actions and interactions with the given descriptions, the liveliness of the generated characters rather than simply copying and pasting reference images, and the correctness of the character sets in each generated scene. Our benchmark incorporates wide range of evaluation metrics. Our test involves 12 automated evaluation metrics. This structured and multifaceted framework enables researchers to thoroughly identify both the strengths and weaknesses of different models, thus fostering targeted improvements. Extensive Model Evaluation: We have conducted extensive evaluation tests on over twenty methods (18 principal methods and their variants). We have analyzed the consistency between user studies and automated metrics, providing insights into model characteristics. We release the entire benchmark, details of the prompts used in the data construction pipeline, automatic and manual evaluation results for each model, and the code necessary to reproduce the automatic evaluation results. Our goal is to drive future advancements in the field of story visualization."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Story Visualization Recent advancements in diffusion and auto-regressive models have driven significant progress in multimodal long-story generation and visualization, particularly in addressing visual consistency and cross-modal sequence coherence. 2 For story image generation [84, 57, 52, 53, 33, 3, 43, 22, 61, 66, 58, 32, 41, 85, 30, 14], methods focus on enhancing cross-frame alignment. UNO [72] combines progressive cross-modal alignment with rotary positional encoding for high-consistency multi-subject image synthesis. SeedStory [78] leverages Multimodal Large Languages Models (MLLM) and SDXL [54] for story continuation, while StoryGen [37] adopts auto-regressive conditioning on historical images/text. Training-free approaches like StoryDiffusion [89] (consistent self-attention) and Story-Adapter [44] (iterative refinement) improve long-range coherence. TheaterGen [6] uses LLMs for character state tracking. Earlier frameworks StoryGAN [34] employs dual discriminators to enforce sentence-image pair consistency. Comic generation [5, 70, 4] is also derivative task of story visualization. In story video generation [31, 65, 25, 82, 87, 19, 26], MovieAgent [69] employs multi-agent reasoning for synchronized narrative, audio, and subtitles. AnimDirector [35] expands prompts into story sequences via LLMs, generated image filtered by VLMs, while Vlogger [92] directly maps scripts to visual prompts. MMStoryAgent [76] further integrates character libraries for multi-scene videos. DreamRunner [68] creates long-form, multi-action, multi-scene story videos through retrievalaugmented motion adaptation. Our benchmark primarily focuses on image-level evaluation. For video generation methods, we will extract video frames for evaluation in our benchmark. Furthermore, Story3D-Agent [28] and SIMS [67] extend story visualization into the 3D domain. Closed-source MLLMs (e.g., GPT-4o [29], Gemini [10]) and commercial platforms (e.g., Morphic Studio [49], MOKI [47], Doubao [18]) have begun to accelerate the transition of story visualization from research to real-world applications. Current research continues to grapple with challenges such as multi-image coherence maintenance, long-range dependency modeling, fine-grained controllability, and alignment with complex textual prompts. 2.2 Datasets and Benchmark We list the datasets [27, 20, 34, 79, 80] related to the story visualization task in the Appendix. Story visualization datasets exhibit significant growth and evolution trends in terms of scale, resolution, automated generation, and stylistic diversity, reflecting technological advancements and the expansion of research interests. In the realms of video generation and visual storytelling, several benchmarks have emerged. StoryBench [1] offers multi-task framework evaluating text-to-video models across action execution, story continuation, and story generation. DreamSim [15] introduces holistic perceptual similarity metric surpassing traditional pixel-level comparisons. The Short Film Dataset (SFD) [17] focuses on long-term, story-oriented video tasks with diverse genres. StoryEval assesses story completion capabilities in text-to-video models using advanced vision-language models. MovieBench [73] provides movie-length videos with rich narratives for long video generation analysis. RISEBench [88] is the first benchmark for reasoning-informed visual editing, covering temporal, causal, spatial, and logical reasoning. DropletVideo [86] explores spatio-temporal consistency in video generation with large dataset of dynamic camera motions. MovieAgent [74] automates movie generation via multi-agent planning, reducing human effort. VinaBench [16] enhances visual narrative generation by annotating commonsense and discourse constraints, improving faithfulness and cohesion. These benchmarks collectively present new challenges and opportunities for advancing related research."
        },
        {
            "title": "3 ViStoryBench",
            "content": "3.1 Problem Definition Based on some prior work [34, 36, 79], we design story generation task with relatively comprehensive given conditions. Given story script, we first provide the appearance descriptions T1, T2, . . . , Tn and corresponding images S1, S2, . . . , Sn for characters C1, C2, . . . , Cn, where Ti and Si are consistent with each other, and Ci = (Ti, Si). Next, we provide storyboard shot descriptions: Shot1, Shot2, . . . , Shotm. Each Shoti includes text description that contains the following content: Setting Description: The description of the setting of current scene. Plot Correspondence: The story segment from the original narrative corresponding to this shot. Onstage Characters: list of characters present in the current shot. 3 Figure 2: Illustration of generation mismatches in story visualization. Static Shot Description: description of the static actions or positions of characters and objects in the frame, representing fixed visual state. Shot Perspective Design: Photography information, including descriptions of shot scale, shot type, and camera angle. The objective of the story visualization task is to generate sequence of images I1...Im that faithfully represent the described storyboard shots, adhering to the provided prompts and character information. This involves accurately depicting the characters, their actions, the scene settings, and the specified camera perspectives. The specific quantitative evaluation method will be described later. 3.2 Source Data Story and Script. We strive for diversity in the stories involved in the benchmark. We manually collect 80 story segments from various sources such as film and television scripts, literary masterpieces, legends from around the world, novels, and picture books. For stories that are too long, we will have LLM assist humans in summarizing them, resulting in each story having hundreds of words. Then, we convert the story into script that includes character descriptions and storyboards, with the assistance of an LLM. There is an example in Figure 1(a). Character Reference Image. We manually collect images from the Internet that align with the character descriptions for each character. We ensure consistent character image styles within the same story, as shown in Figure 1(b). Reference images for small portion of the characters were generated using SDXL [54]. Some basic statistics are presented in Figure 1(c). In total, there are 344 characters and 509 reference images for the characters. More details on source data collection and statistics are described in the Appendix. 3.3 Evaluation Metrics Overview. We present comprehensive suite of evaluation metrics to gauge story visualization model performance across various dimensions. We showcase some typical issues in story visualization results in Figure 2 to facilitate an intuitive understanding of the evaluation metric development. Automated evaluation metrics include the following aspects: 4 Figure 3: CIDS calculation pipeline. Cross-Similarity and Self-Similarity are computed. Cross-Similarity and Self-Similarity: Assessing the resemblance between generated images and reference images, as well as the consistency within the generated images themselves. Prompt Adherence: Measuring how well the generated images align with the storyboard descriptions provided in the prompts. We primarily use GPT-4.1[50] for automated evaluation and also specifically calculate the accuracy of the number of characters in each generated image. Aesthetic and Copy-Paste: Additionally, we evaluate the aesthetic metric, generation quality and diversity of the generated results. We specifically design Copy-Paste Detection to check if the model excessively references the character reference images. Preliminary. We briefly introduce the models and tools used. Grounding DINO [83, 39] is an openset object detector. Grounding DINO [39] can detect objects in images based on text descriptions. We can utilize Grounding DINO [39] to crop out the bounding box of character that match specific description from either character reference images or generated images. For character reference images, Grounding DINO [39] merely crops some edges of the pictures. However, when dealing with generated images, Grounding DINO [39] might fail to locate matching characters, in which case it will return an empty result. For cropped character image, if the content is realistic character, we use ArcFace [11] for feature extraction; otherwise, we use CLIP [55] for feature extraction. Both feature extraction methods generate 512-dimensional feature vector for each character. When extracting the style features of the entire image, we use the model provided by CSD [59], CLIP [55] model fine-tuned on large dataset of style images. In addition, we utilize the Inception Score (IS) [56] with Inception V3 model and Aesthetic Predictor V2.5 [12] to evaluate diversity and aesthetic quality, respectively. IS evaluates batch of generated images based on clarity and diversity. Aesthetic Predictor V2.5 [12] is SigLIP-based [81] predictor that assesses the aesthetics of an image on scale from 1 to 10. It tends to assign lower scores to images that are blurry, noisy, or are perceived as less visually appealing. Images with scores of 5.5 or higher are considered to have excellent quality. Next, we briefly introduce how to specifically calculate each metric, with some details expanded further in the Appendix. We convert the cosine similarity and the original rating scale of 0 to 4 into hundred-point scale for presentation. Character Identification Similarity (CIDS). As depicted in Figure 3, we first utilize Grounding DINO [39] along with character description prompts to first crop out specific characters from the reference image. We then extract features using CLIP [55]. Subsequently, we again employ Grounding DINO [39] and character description prompts to crop potential human subjects from the generated image. After extracting CLIP [55] features, we compute similarity with the features from the reference image and perform matching. This process allows us to obtain the characters matched to each human subject in the outputs, as well as the successfully matched human CLIP [55] features and bounding boxes. Then we calculate the cosine similarity between the CLIP [55] features of the matched characters in all output scenes and their corresponding reference features. Finally, we take the average. 5 Figure 4: Prompt adherence evaluation. GPT-4 is used to evaluate prompt adherence scores on four aspects. Scoring samples are also illustrated. Style Similarity. Our style similarity metric, adapted from CSD [59], quantifies both self-similarity (within generated images) and cross-similarity (between generated and reference images) through CSD-CLIP [59] feature analysis. The process involves three key steps: (1) Encoding each image into visual features using CLIP [55] vision encoder pre-trained on large-scale style datasets; (2) Disentangling content and style features via CSD [59] layers, where only the style features are retained; (3) Computing pairwise cosine similarity between style feature embeddings. The final similarity score is obtained by averaging all valid image pairs in the comparison set. Prompt Adherence. Regarding the similarity between the generated images and the provided storyboard descriptions, we categorize them as follows: Character Interaction Actions: Consistency between the overall character interactions in the generated image and the Static Shot Description in the storyboard description. Shooting Method: Consistency between the camera shooting method in the generated image and the Shot Perspective Design in the storyboard description. Static Shot Description: Consistency between the Static Shot Description in the generated image and the Static Shot Description of storyboard description. Individual Actions: Consistency between the actions/expressions of individual characters after segmentation and actions/expressions in the Static Shot Description of storyboard description. As shown in Figure 4, we have established Likert scale questionnaires to evaluate the consistency of each generated image on scale from 0 to 4. Onstage Character Count Matching. We observe that various models struggle to generate the correct set of onstage characters, including both superfluous characters (unexpected additions) and omissive ones (failure to render specified roles), so we specifically calculate the Onstage Character Count Matching (OCCM) score, which is calculated as: OCCM = 100 exp (cid:18) (cid:19) E ϵ + (1) where denotes the detected number of Onstage Characters, represents the expected character count from storyboard specifications, and ϵ = e6 is smoothing factor to prevent division by zero. 6 Copy Paste Detection. prevalent shortcut employed by generative models in story visualization task is the direct \"copy-pasting\" of character reference images. To gauge the extent of this phenomenon, we devise \"copy-paste degree\" metric, where higher value signifies greater propensity of the model to replicate characters directly from reference images. Specifically, for methods limited to single-image input, we compute the disparity in similarity between the output characters and both the input image and an alternative reference image. This disparity is then averaged across all output images and character samples, and subsequently across all stories equipped with alternative reference images. In the case of models supporting multi-image input, this metric calculation is omitted. Image Quality. For each generation method, we calculate the aesthetic quality score of all its generated results and average them. The same applies to the calculation of IS."
        },
        {
            "title": "4 Diagnosis of Evaluation Result",
            "content": "4.1 Experimental Setup Our benchmark comes in two versions: the complete ViStoryBench and ViStoryBench-lite, with the latter being quarter subset of the former. Through manual curation, our aim is to ensure that the distribution of text styles and character reference image styles in the lite version closely mirrors that of the original version. Specifically, the lite version features 20 stories, with character references comprising 36 animated characters, 41 real people, and 4 non-human entities. The proportion of various types of characters is similar to that of the entire collection. In our main experiments (shown in Table 1), we evaluate diverse set of methods for both image and video generation. We develop simple Copy-Paste Baseline. We automatically paste the reference images of the Onstage Characters from each shot onto 1080p canvas using program. For image generation, we assess various approaches including StoryDiffusion [89], Story-Adapter [45], StoryGen [37], UNO [71], TheaterGen [6] and SEED-Story [79]. For video generation, we test methods including Vlogger [92], MovieAgent [74], Anim-Director [35], and MM-StoryAgent [77]. In the field of story generation, many closed-source commercial software can provide quite good results. However, due to limited resources and other reasons, we can only report their results on the ViStoryBench-lite benchmark. We also include results from some open-source methods as baselines. The results we obtain are from the May 2025 versions of these software, and future results may vary. Due to variations in problem definitions among different methods or software, we discuss the detailed adaptation process in the Appendix. For video generation methods that do not produce images as intermediate results, we select the first frame from each shot-related video. We require most methods or software to produce results in 1080p resolution, although there are exceptions, such as Gemini, where the image size is not completely controllable. Additionally, we provide continuously maintained leaderboard webpage to encourage robust competition within the community. The final ranking in this table is determined by averaging the ranking indices of each metric, ensuring balanced consideration of all metrics. The results of some commercial software tests are marked with asterisks. Due to content policy restrictions, certain stories could not generate results normally. Therefore, we only considered the average on the data that successfully generated results, which introduced some discrepancies. User Study. To evaluate the consistency and aesthetic quality of the generated images, we conduct user study involving participants who assessed the results across three dimensions: Environment Consistency, Character Identification Consistency and Subjective Aesthetics. Environment Consistency focuses on whether the scenes within the same environment description appeared visually cohesive. For Character Identification Consistency, participants rate how consistently the main characters are identifiable and coherent throughout the story. Subjective Aesthetics assesses the overall artistic appeal, detail richness, and storytelling effectiveness of the visualizations. Detailed scoring criteria for each dimension are provided in the Appendix. The detailed scoring results will also be open-sourced. We list the top four methods in each category based on user ratings: Environment Consistency: UNO (82.0), GPT-4o (81.2), Doubao (80.4), Story-Adapter (79.6) : image-ref, scale=5 Character Identification Consistency: Doubao (92.6), AIbrm (88.4), UNO (84.0), GPT-4o (81.6) 7 Table 1: Automated test results of different methods on ViStoryBench and ViStoryBench-Lite. We use gray backgrounds to mark some results that are excluded from the sorting. Because the SEED-Story is only trained on three animations and does not pursue generalization ability. The Copy-Paste Baseline simply pastes the character reference image as the generated result. For some methods, we test multiple inference configurations and report the results. CSD (Style) CIDS (Character) Alignment Score Self Self Cross OCCM Inception Aesthetics Copy-Paste Score Degree Score Score Method Copy-Paste Baseline - Story Visualizing Method StoryGen [37] (mixed) (auto-regressive) (multi-image-condition) StoryDiffusion [89] (text-only) StoryDiffusion [89] (image-ref) Story-Adapter [44] () (text-only, scale=0) (image-ref, scale=0) (text-only, scale=5) UNO [72] (many2many) TheaterGen [6] SEED-Story [78] - Video Generation Method Vlogger [92] (text-only) Vlogger [92] (image-ref) MovieAgent [69] (ROICtrl) MovieAgent [69] (SD3) Anim-Director [35] (SD3) MM-StoryAgent [76] Story-Adapter [44] () UNO [72] (many2many) MM-StoryAgent [76] - Commercial Software MOKI [47] Morphic Studio [49] AIbrm [42] ShenBi [46] Typemovie [63] Doubao [2] - Multimodal Large Language Model GPT-4o [29] Gemini-2.0 [9] Cross 72.8 28.8 38.0 37.1 28.6 35.1 33.3 29.0 46.2 32.6 40.8 19.9 23. 21.4 27.1 20.0 30.9 29.8 25.3 38.2 44.6 28.0 22.9 59.0 42.6 29.2 34.2 38.6 48.7 38.1 The following results are obtained on ViStoryBench 99.7 71.5 94. 26.2 98.7 58.4 53.4 52.6 63.5 55.4 73.7 46.1 55.4 73.5 61.0 40.4 74.9 40.7 45.2 49.3 48.3 50.7 66.8 34.1 36.0 35.2 30.5 33.8 27.7 29.6 35.3 27.5 39.4 27.1 21.7 26.9 28.6 27.3 31.6 33.9 31. 66.7 64.1 64.0 64.8 66.3 63.2 60.0 66.5 63.5 67.1 60.5 52.9 56.6 59.9 61.4 60.7 65.3 64.6 35.9 36.1 37.1 59.7 61.2 58.9 62.7 58.5 62.2 70.4 37.9 29.5 53.8 55.1 36.5 76.1 76.0 52.1 86.2 85.8 86.2 85.3 86.8 84.9 85.7 85.8 85.7 89.8 84.4 85.9 84.5 84.5 86.7 87.5 88.4 85. The following results are obtained on ViStoryBench-Lite 75.9 65.6 66.2 70.0 63.8 73.0 58.2 65.2 69.8 68.5 58.6 29.5 42.3 32.1 29.2 50.0 55.9 34.7 39.2 39. 53.2 31.5 55.0 69.2 56.6 62.6 66.4 76.2 61.1 54.6 71.4 73.1 53.7 56.1 72.1 50.4 38.8 67.5 65.2 75.8 62.4 84. 89.3 76.1 81.5 89.1 82.2 82.0 82.9 89.4 85.6 84.1 87.5 93.4 86.9 6.72 7.31 8.72 8.89 15.71 10.06 13.73 16.34 12.98 13.13 12.40 14.88 6. 10.48 10.03 11.63 15.02 12.04 9.09 12.03 10.50 8.09 10.36 9.00 9.55 11.60 11.16 9.88 9.02 10.12 4.48 3.84 4.02 4.02 5.76 5.13 4.89 5.18 5.00 4.90 5.23 4.90 3. 4.33 4.34 4.65 5.33 5.60 5.88 4.80 5.13 5.91 5.81 4.96 5.73 5.07 5.32 5.62 5.52 4.91 24.0 1.11 2.17 1.54 0.02 0.11 -0.49 0.11 0.29 -0.16 - 0.59 0. 0.82 -0.07 0.33 -0.40 0.15 -0.25 -0.15 - 0.82 0.67 - -0.52 0.66 -2.20 0.78 0.97 -0.38 : These six indicators are normalized to hundred-point scale. : An excessively high value (marked with an underline) may indicate degradation issue. Some methods that support multi-image input do not support this metric test and are marked as \"-\". : image-ref, scale=5 Table 2: Correlation analysis. Correlations between human evaluations and automated metrics are reported. Metrics τ ρ σ Self CSD (Style) Self CIDS (Character) Aesthetics 0.4165 0.5648 0.6042 0.4978 0.6759 0.7956 0.2565 0.3966 0. Subjective Aesthetics: GPT-4o (85.6), Doubao (85.0), AIbrm (83.0), UNO (80.4) We establish one-to-one correspondences between three human evaluation metrics and automated metrics, similar with Theatergen [6]: Character Identification Consistency corresponds to CIDS, Environment Consistency to Style Similarity, and Subjective Aesthetics to Aesthetic Quality. Subsequently, we evaluate the correlation coefficients between automated metrics and human evaluation results, including Kendalls τ , Spearmans ρ, and Pearsons σ, as shown in Table 2.The results demonstrate that automated metrics can effectively reflect human preferences. In this analysis, we exclude results from StoryGen [37], as its limited generation quality caused human evaluators to penalize character consistency scores, This is due to the generated character deviating significantly from the distribution of typical characters, leading to mismatched human expectations. 4.2 Insights and Discussions Based on test results, we briefly outline some findings: In story visualization tasks, comprehensive evaluation metrics are extremely important. For instance, the simple Copy-Paste Baseline achieves optimal results across numerous metrics. However, its alignment score is notably low. Although IS can generally measure the quality and diversity of image generation, it is quite challenging to compare different models by examining the IS metric alone. When using only text as input, StoryDiffusion [89] and Story-Adapter [44] achieve excellent IS and aesthetic quality. However, relying solely on text input clearly cannot produce results that resemble the features and styles of the character reference images. The recently released UNO [72] has achieved comprehensive leading position in the opensource approach. It achieves good user favorability in all three categories of ratings. It has achieved relatively leading metrics in every quantitative indicator. Commercial software has excellent comprehensive capabilities. Among them, Doubao and GPT4o [29] stand out in terms of prompt adherence ablity (alignment score). This may be attributed to their integration of LLMs with massive parameter counts. Moreover, GPT-4o [29] exhibits the highest accuracy in selecting the set of characters to appear. AIbrm [42] is highly proficient in utilizing the character reference images to incorporate the depicted character features into its generated results. In this regard, the top open-source project is UNO [72], but AIbrm [42] significantly outperforms it. Using automated metrics, it is easy to understand the improvement space of model or find good/poor examples. For example, early work StoryGen faces issues with the diversity and quality of the generated images. The introduction of the image prior by Diffusion has elevated subsequent methods in terms of IS and aesthetic quality. AIbrm [42] is an application tailored for story visualization scenarios, offering more accurate character feature generation compared to general applications like GPT-4o [29]. However, AIbrm [42]s ability to understand textual instructions still needs improvement. With automated metrics, we can easily identify some good/poor generation results, as shown in Figure 4. Our quantitative metrics demonstrate alignment with qualitative observations. For StoryAdapter [44], the scoring consistency between automated metrics and human evaluation is particularly evident: (1) In text-only mode (its native setting), the overall quality score (scale=5) systematically surpasses the baseline (scale=0), as theoretically expected; (2) When using image references, scale=0 achieves higher cross-similarity but lower self-similarity compared to scale=5 in both CIDS and CSD."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce ViStoryBench, comprehensive benchmark suite designed to advance the field of story visualization. By addressing the critical need for diverse and multifaceted evaluation framework, ViStoryBench enables researchers to rigorously assess and compare various story visualization models across wide range of dimensions."
        },
        {
            "title": "Appendix",
            "content": "1 Introduction 2 Related Work 2.1 Story Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Datasets and Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 ViStoryBench 3.1 Problem Definition . . 3.2 Source Data . . . 3.3 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Diagnosis of Evaluation Result 4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Insights and Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2 2 3 3 3 4 4 7 8 5 Conclusion Limitation and Societal Impact Details of Source Data Collection and Statistics B.1 Character Reference Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details of Style Similarity Calculation Qualitative Results Complete Evaluation Result on the ViStoryBench-Lite Benchmark Adapting Methods to ViStoryBench F.1 Copy-Paste Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Story Visualizing Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Video Generation Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 Commercial Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.5 MLLM . . . . . . . . . Impact of Reference Image Selection on Cross-CIDS Metric Details of User Study H.1 Prompts for Dataset Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . Details of Prompt Adherence Evaluation I.1 Character Interaction Actions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.2 Shooting Method . . . . I.3 I.4 Static Shot Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Individual Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details of Computing Resource"
        },
        {
            "title": "A Limitation and Societal Impact",
            "content": "9 10 11 11 11 12 12 14 15 17 18 19 20 21 24 25 25 26 27 Some images in our dataset are sourced from popular movies, TV shows, animations, etc. Consequently, there is possibility that certain metrics may be overfitted to these specific data, potentially leading to the manipulation or \"hacking\" of those metrics. Our dataset includes both Chinese and English versions. Although we choose the appropriate language for each method or software, the quality of the generated results may vary due to differences in the language of the instructions. We do not take into account the differences in generation quality caused by language differences. Our work is unable to evaluate the outcomes of comic generation/manga generation tasks that involve generating multiple panels within single image, as the proposed methodology lacks an accurate panel segmentation approach. Our work does not include discussion of the inference speed of each method. For the story video generation method, there are still some video-related issues such as frame consistency or quality that need attention. Our work does not specifically design tests for this aspect. We hope that story visualization models can become more powerful educational and creative tools, as well as aid in the preservation and promotion of cultural heritage. In collecting stories, we make efforts to select narratives from various cultures and regions. Despite these efforts, generative models still face challenges in overcoming stereotypes and data biases. Crucially, generative models should not be used as tools for creating false content, which requires collaboration between policymakers and technology experts. Table 3: Comparison of multi-modal story generation datasets. Datasets VIST [27] Flintstones [20] Pororo [34] StorySalon [38] StoryStream [79] Generated Generated OpenStory [80] Caption Manual Manual Manual ASR # Images 146k 123k 74k 160k 258k 107M Resolution - 128 128 128 128 432 803 480 854 720p&1080p # Shots 5 5 5 14 30 28 Style Realistic Anime Anime Anime Anime Realistic"
        },
        {
            "title": "B Details of Source Data Collection and Statistics",
            "content": "Story visualization datasets show significant growth and evolution trends in terms of scale, resolution, automated generation, and stylistic diversity, reflecting technological advancements and the expansion of research interests, as shown in Table 3. The primary distinction between ViStoryBench and these datasets lies in our approach: we aim to generate shot descriptions starting from textual story, rather than extracting captions from key frames to form narrative. When creating benchmarks, we also pay close attention to the diversity of styles and themes. Among all these stories we collect, there are 13 folktales, 10 love stories, 4 suspense crime stories, 3 horror stories, 6 historical stories, 10 fantasy stories, 7 science fiction stories, 3 war stories, 10 stories about social life, 3 adventure survival stories, and 11 fairy tales. We then adapt these segments into shot scripts using LLM, Step-1V [60]. The entire dataset comprises 1317 Shots, with each story containing between 4 and 30 shots, averaging 16.5 shots per story. To evaluate broader range of methods, all test-related texts in the dataset are provided in both English and Chinese versions. For methods that only support Chinese or significantly perform better with Chinese input, we use Chinese as input, while English is used as input for other methods. Each individual shot includes the following descriptions: Setting Description, Plot Correspondence, Onstage Characters, Static Shot Description, and Shot Perspective Design. B.1 Character Reference Image For most well-known stories, character reference images are sourced from relevant visual works. For the remaining stories, we either retrieve screenshots from films or TV shows (for 16 stories) with similar settings or have SDXL [54] generate animation character images (for 7 stories). The dataset contains total of 344 characters, including 190 real humans, 135 virtual humans, and 19 non-humans. Among them, there are 210 males, 108 females, and 26 characters with no gender or non-binary gender. Each character has between 1 and up to 10 images, with 89 characters having more than one image. The entire dataset consists of 509 reference images, part of which is shown in Figure 5. We categorize all 80 stories into two types based on the image category of the main characters: realistic stories and unrealistic stories. Among them, there are 39 realistic stories and 41 unrealistic stories, and realistic casts are labeled with ethic definition followed by [7, 8, 51]. This classification is used to subsequently evaluate differences in results between different works."
        },
        {
            "title": "C Details of Style Similarity Calculation",
            "content": "We provide figure to illustrate the calculation process of the Style Similarity metric, as shown in Figure 6. Our style similarity metric, adapted from CSD [59, 91], quantifies both self-similarity (within generated images) and cross-similarity (between generated and reference images) through CSD-CLIP [59] feature analysis. The process involves three key steps: (1) Encoding each image into visual features using CLIP [55] vision encoder pre-trained on large-scale style datasets; (2) Disentangling content and style features via CSD [59] layers, where only the style features are retained; (3) Computing pairwise cosine similarity between style feature embeddings. 11 Figure 5: Random character reference samples from the dataset. The reference images include full-body shots, half-body shots, or portraits. Figure 6: Style Similarity Calculation."
        },
        {
            "title": "D Qualitative Results",
            "content": "We provide the visualization generation results of the methods tested on story 09, as shown in figure 7. More results can be found on the website: https://huggingface.co/datasets/ ViStoryBench/ViStoryBenchResult Complete Evaluation Result on the ViStoryBench-Lite Benchmark We present the results of several selected methods on the ViStoryBench-Lite benchmark in main paper. To facilitate more comprehensive comparison, we provide the complete evaluation results for all the methods on the lite set, as shown in Table 4. These results are intended to offer thorough reference for assessing the performance of different approaches."
        },
        {
            "title": "F Adapting Methods to ViStoryBench",
            "content": "In this section, we report how we adapt each method to the ViStoryBench test. In general, we strive to implement reasonable inputs for reference character images and shot script prompts on each work as much as possible. We make efforts to standardize inputs, such as adjusting output resolution to 16:9 aspect ratio whenever feasible. To guarantee reproducibility, we fix the random seed. Additionally, 12 13 Figure 7: Qualitative result on story 09. From left to right are shot1 to shot5. Reference images of each shots on-stage characters is shown in Copy-Paste baseline results. we implement mechanisms for inputting reference images and adapting lengthy shot script prompts. These adaptations enable methods to generate continuous image results of stories. F.1 Copy-Paste Baseline The Copy-Paste Baseline aims to verify the correctness of certain metrics by constructing the simplest possible generation method, which involves directly copying and pasting characters into images. 14 Table 4: Automated test results of different methods on ViStoryBench-Lite. We use gray backgrounds to mark some results that are excluded from the sorting: SEED-Story is only trained on three animations and does not pursue generalization ability, the Copy-Paste Baseline simply pastes the character reference image as the generated result. For some methods, we test multiple inference configurations and report the results. CSD (Style) CIDS (Character) Alignment Score Self Self Cross OCCM Inception Aesthetics Copy-Paste Score Degree Score Score Method Copy-Paste Baseline - Story Visualizing Method StoryGen [37] (mixed) (auto-regressive) (multi-image-condition) StoryDiffusion [89] (text-only) StoryDiffusion [89] (image-ref) Story-Adapter [44] () (text-only, scale=0) (image-ref, scale=0) (text-only, scale=5) UNO [72] (many2many) TheaterGen [6] SEED-Story [78] - Video Generation Method Vlogger [92] (text-only) Vlogger [92] (image-ref) MovieAgent [69] (ROICtrl) MovieAgent [69] (SD3) Anim-Director [35] (SD3) MM-StoryAgent [76] - Commercial Software MOKI [47] Morphic Studio [49] AIbrm [42] ShenBi [46] Typemovie [63] Doubao [2] - MLLM GPT-4o [29] Gemini-2.0 [9] Cross 73.6 31.5 40.7 39.9 31.5 42.3 38.2 35.9 52.7 36.4 44.6 23.7 26.4 25.7 31.5 24.8 36.1 32.1 28.0 22.9 59.0 42.6 29.2 34.2 38.6 48.7 38. The following results are obtained on ViStoryBench-Lite 77.3 60.9 55.8 54.9 68.7 61.8 75.9 51.5 61.6 75.4 65.6 42.1 76.2 46.2 49.8 55.7 54.3 55.5 66.2 70.0 63.8 73.0 58.2 65.2 69.8 68.5 58. 93.2 36.4 37.6 37.0 30.7 33.1 29.5 31.4 34.5 29.6 42.3 26.5 21.9 28.7 30.0 27.7 35.7 38.4 32.1 29.2 50.0 55.9 34.7 39.2 39.4 53.2 31.5 99. 68.8 58.8 64.2 57.8 61.0 55.0 54.2 59.7 59.3 69.2 54.6 48.6 55.4 54.0 57.7 60.5 65.5 56.6 62.6 66.4 76.2 61.1 54.6 71.4 73.1 53.7 27.0 35.8 35.3 36.9 59.3 60.7 56.1 62.1 60.4 56.5 72.2 40.5 29. 55.8 56.2 38.7 76.3 76.7 50.4 38.8 67.5 65.2 75.8 62.4 84.2 89.3 76.1 100.0 5.5 83.3 83.6 83.1 81.9 83.2 81.5 82.7 84.5 82.6 89.1 81.3 83. 83.4 82.0 84.3 86.9 87.7 82.2 82.0 82.9 89.4 85.6 84.1 87.5 93.4 86.9 6.25 7.15 7.67 12.99 8.18 12.03 12.72 11.49 10.59 10.50 13.60 4.93 8.44 9.05 10.07 12.05 10.00 8.09 10.36 9.00 9.55 11.60 11.16 9. 9.02 10.12 4.40 3.86 4.09 4.09 5.83 5.21 4.80 5.12 4.89 4.85 5.13 4.94 3.82 4.30 4.30 4.70 5.33 5.61 5.91 5.81 4.96 5.73 5.07 5.32 5.62 5.5 4. 24.5 2.02 1.86 1.93 -0.40 -0.42 -0.15 -0.04 0.63 -0.72 - 1.08 0.14 0.78 0.06 1.66 -0.61 0.86 0.82 0.67 - -0.52 0.66 -2.20 0.78 0.97 -0.38 : These six indicators are normalized to hundred-point scale. : An excessively high value (marked with an underline) may indicate degradation issue. Some methods that support multi-image input do not support this metric test and are marked as \"-\". : image-ref, scale= For example, this baseline demonstrates that the metrics in ViStoryBench can perform an accurate calculation in areas such as CIDS, CSD, Copy-paste degree, and Matched Character Count calculation. We obtain the image for the current shot by simply stitching together the images of the on-stage characters in each shot, resulting in the image outcome for the current shot. sample is shown in Figure 8. Figure 8: Sample results of copy-paste baseline. F.2 Story Visualizing Method F.2.1 StoryDiffusion The original StoryDiffusion [89] is primarily designed for image generation using multiple reference images of different characters and multi-shot prompts, where each prompt includes character name and description. We implement the following adaptations for ViStoryBench evaluation: Since the native implementation can only handle short texts that cannot exceed the models maximum sequence length limit (77 tokens), similarly to StoryGen adaption, we employ the grouped encoder sd_embed [90] to address this issue. 15 Given StoryDiffusion does not support inputting reference images of multiple characters while generating one shot, we sort characters based on their appearance frequency in the full shot script, from highest to lowest, and only introduced the highest-priority characters in one shot. StoryDiffusion predefines various style templates, each containing detailed style descriptions, including quality words. We insert the character prompt into the specified position within the style description, i.e., between the style word and the quality word, to achieve richer style expression. During testing, we obtain two types of results: those based on image reference (img ref) and those based solely on text (text only). We report the metrics for both types of results separately. F.2.2 Story-Adapter The original Story-adapter [44] generates multiple images through multiple prompts and does not inherently support input image references. We implement the following adaptations for ViStoryBench evaluation: Incorporating Reference Images: Story-adapter performs multiple rounds of iterative inference. In the first round, it generates all the storyboard images, which are then used as image references for regeneration in subsequent rounds. This multi-round process enhances both prompt alignment and style consistency of the images. We leverage this inherent image-referencing capability of Story-adapter by inputting the image of the current on-stage character into the pipeline during the first round of generation, thereby achieving image generation with reference images. We test the results of different iteration rounds (scale 0 and scale 5) under two modes: text-only and image-Ref, and reported the results of all four methods. F.2.3 UNO UNO [72]s original implementation supports various reasoning methods such as one2one, one2many, many2one, and many2many. Among these, the many2many approach involves inputting multiple images along with prompts that provide simple descriptions of the images. We exclusively evaluate the many2many method, which is capable of completing the ViStoryBench task. For each shot, we generate the corresponding image result by inputting the shots prompt and the images of the on-stage characters. F.2.4 StoryGen The original StoryGen [37] supports the integration of previously generated image-text pairs as context to construct image sequences that align with the input shot scripts. We implement the following adaptations for ViStoryBench evaluation: Mix Inference Method: The mix method behaves similarly to the multi-image-condition method for the first frame, while subsequent frame generation follows the auto-regressive method. When image references are absent (e.g., no on-stage characters), the first shot uses aggregated story-wide character descriptions and images for consistent initial representation. For subsequent shots, dynamic sliding window blends historical generation results as input, maintaining temporal coherence and mitigating quality degradation from long-range dependencies. Grouped Encoder for Ultra Long Prompts: To handle ultra-long prompts and prevent information loss due to truncation in CLIP models, we employ grouped encoder sd_embed [90] to address this issue. Resolution Adjustment: We modify the resolution settings to approximate 9:16 aspect ratio, specifically 512 912, to align with the generation resolutions of other methods. Deterministic Random Seed Strategy: To enhance test controllability, we adopt deterministic random seed strategy. F.2.5 TheaterGen The original TheaterGen [6] only supports text input, where an LLM is used to parse the description of each character and the overall image of the text. Subsequently, each characters independent image is generated through the IP-Adapter, and then these images are placed in their corresponding positions 16 using detection and segmentation models. Finally, the final image is generated under the guidance of the character images. Since the method does not open-source the code for invoking the LLM part, we supplement the corresponding code to primarily obtain the bounding box coordinates of each IP image required for model input. Afterward, following TheaterGens setup, we place the character reference images in our dataset into the corresponding positions to generate final results. F.2.6 SEED-Story The original SEED-Story [78] focuses on generating long multi-modal stories and their visualization through an auto-regressive approach. SEED-Story requires the user to provide an initial image and text description, then proceeds with the story generation, using each output as the next input. Due to the story-continuation nature, it is different from other approaches (SEED-Story was only trained on the StoryStream dataset [78], which contains data from only three cartoon series, the model does not have generalization). Input Handling: We only adapt our data in the visualization stage, and there is no need for the previous story generation. For the first input image (000start), select the first reference image of the first character. In the pre-visualization input data, add the prompt words of this role to the beginning of the prompt list to ensure the first shots prompts are not compromised. F.3 Video Generation Method F.3.1 MovieAgent Similar to Vlogger, MovieAgent [69] also receives story and utilizes an LLM to generate series of fine-grained descriptions. Additionally, ROIctrl includes the bounding boxes of characters. We directly map shot prompts onto Vloggers fine-grained descriptions for generation. We only perform the step from prompt to image, without further generating video, and use this image as the final result. F.3.2 Anim-Director Anim-Director [35] does not support input reference images. It utilizes LLM to supplement character and scene descriptions based on simple prompts provided by users, refining them into complete story sequences by scenes. Visual images are generated through the Stable Diffusion 3 [13] model, filtered by VLM, and subsequently used to generate videos. We adopt the following strategies to complete the test on ViStoryBench: 1. We directly input the prompt of each shot from ViStoryBench as the story sequence prompt to generate images. After filtering by VLM, we obtain the results without proceeding with the subsequent video generation steps. 2. To accommodate ultra-long prompts, we employ grouped encoder sd_embed [90] to address this issue. 3. We modify the original resolution of 1024 1024 to resolution close to 16:9, specifically 768 1344. 4. We fix the random seed to ensure reproducibility. F.3.3 MM-StoryAgent MM-StoryAgent [76] does not support the input of reference images. We only utilize the Image Agent mentioned in the code to generate images from prompts, without further generating video. As MM-StoryAgent is based on StoryDiffusion, we also employ grouped encoder sd_embed [90] to adapt to long prompts. F.3.4 Vlogger The original Vlogger [92] takes short story and utilizes an LLM to generate series of finegrained descriptions. These descriptions encompass character and object descriptions, video script descriptions (in both Chinese and English), characters and objects present, duration settings, etc. 17 We directly map shot prompts onto Vloggers fine-grained descriptions, selecting the first frame of each shot from the generated video as the result. Similar to StoryDiffusion, vlogger does not support inputting reference images of multiple characters while generating one shot. We adopt similar solution to that of StoryDiffusion. F.4 Commercial Software Given the absence of API or similar call methods for the following commercial softwares and their intricate interaction processes, we employ full-time annotators within the company to generate image results for all the commercial software mentioned below, receiving wage of 4.33 U.S. dollars per hour. The local minimum wage being 1.93 U.S. dollars per hour. All the following methods were tested between May 1 and May 7, 2025. F.4.1 MOKI When generating shots on MOKI [47], it is necessary to select one of the provided painting style options. To best replicate the effects of real human usage, we instruct the annotators to choose the option that most closely aligns with the style of the character reference images for each script. Given that MOKI restricts the maximum number of reference characters in story to three, we sort the characters based on their appearance frequency among all shots. The three characters with the highest frequency were then added as reference characters to maximize the performance of the model. We generate images of MOKI using Chinese version dataset. MOKI has limit on the length of each shots prompt, capped at 60 Chinese characters. Consequently, we make certain omissions in the prompt for each shot. For instance, we only used the Static Shot Description and Plot Correspondence sections as input. If the character count still exceeded the limit, we employ an LLM to abbreviate the text while preserving key information. For each shot, the platform generated four images at time. We select the first image as the final result. F.4.2 Morphic Studio Morphic Studio [49] restricts each shot to include only one reference character. Consequently, we rank the frequency of character appearances in the script and use this order as priority to select the current on-stage reference character for each shot. Morphic Studio allows uploading 2 to 10 images for the same reference character. For characters with only one reference image, we upload an additional identical image to meet the minimum requirement of two images. For characters with multiple reference images, we upload all available reference images. Therefore, we do not provide the Copy-Paste Degree for Morphic Studio because all reference images were uploaded and used for generation, making it incalculable. For each shot, the platform generates four images at time. We select the first image as the final result. F.4.3 AIbrm AIbrm [42] restricts each shot to maximum of two reference characters. We employ the same sorting methodology as utilized in Morphic Studio. The character creation process in AIbrm involves uploading real human image, selecting style from the provided options, and inputting character prompt to generate the character. For the real human category in our dataset, we upload images and chose the \"realistic\" style. For virtual human and non-human categories, since image uploads were not feasible, we select the closest available style. Subsequently, we input the character prompts from our dataset across all styles to complete the character creation. F.4.4 ShenBi Firstly, when creating project, ShenBi [46] requires selecting generation style from list. We choose the style that is closest to the reference images in the dataset. 18 When creating characters, we upload the character images along with their corresponding prompts to generate new character images, which are subsequently utilized within the method to produce shot images. ShenBi restricts each shot to maximum of three reference characters. We employ the same sorting methodology as utilized in Morphic Studio. The output of ShenBi is in the form of videos. We extract the first frame of each scene video as the final result. F.4.5 Typemovie Firstly, when creating project, Typemovie [63] requires selecting generation style from list. We choose the style that is closest to the reference images in the dataset. The character creation process in Typemovie involves uploading real human image, selecting style from the provided options, and inputting character prompt to generate the character. For the real human category in our dataset, we simply upload reference images. For virtual human and non-human categories, since image uploads were not feasible, we select the closest available style. Subsequently, we input the character prompts from our dataset across all styles to complete the character creation. Typemovie restricts each shot to include only one reference character. We employ the same sorting methodology as utilized in Morphic Studio. F.4.6 Doubao We conduct our tests using the grayscale test version of the \"Image Generation\" model on the Doubao homepage [2], dated April 27, 2025. Since this image generation model only supports uploading single image, we employ sorting method similar to that used in Morphic Studio to prioritize characters and selected the highest-priority character to upload as single reference image. The prompt used during generation was (translated to English): \"This is an image of the protagonist <character_name>. Next, please generate storyboard scenes based on the protagonists image and the script provide. The script for the first scene is as follows: <shot_prompt>.\" We perform multiple rounds of generation in one session to obtain the desired consistent image results for one story. F.5 MLLM F.5.1 GPT-4o and Gemini-2.0 GPT-4o [29], developed by OpenAI, is an advanced multi-modal model demonstrating strong capabilities in understanding and generating content across text and image modalities. Its architecture is inherently suited for tasks requiring nuanced interpretation of combined textual and visual inputs, and for producing contextually relevant visual outputs. Googles Gemini [10] is powerful multi-modal system designed for sophisticated reasoning and generation across various data types, including text and images. It supports image generation conditioned on both textual prompts and visual references. To integrate GPT-4o and Gemini into the ViStoryBench evaluation framework, we adopt the following key adaptation strategies: Atomic Shot Processing: Each shot defined within ViStoryBench was treated as an independent generation request to the model, ensuring focused processing for individual narrative segments. Comprehensive Prompt Engineering: For every shot, structured textual prompt was meticulously crafted. This prompt amalgamated all critical textual information provided by ViStoryBench, including plot details, scene descriptions, character portrayals, camera perspective guidelines, and the desired aspect ratio for the output image. Direct Visual Referencing: Character reference images, after undergoing standardization pre-processing pipeline (e.g., resizing, color space conversion), were directly incorporated as 19 visual inputs for each shots generation request. This aimed to guide the model in rendering characters consistent with their specified appearances. Conversational Context Continuation: To foster narrative coherence across sequential shots, the models inherent capability to process conversational history was leveraged. The most recent interaction cycles, encompassing the prompt for the preceding shot and the models response (which includes the generated image), served as contextual information for the subsequent shots generation task. Impact of Reference Image Selection on Cross-CIDS Metric In certain methods, the reference images/features of characters used for image generation are not from the ones provided in our dataset but are instead additionally generated images. For instance, one scenario is that the method utilizes the real-person reference images from our dataset to regenerates an anime character, which is not very similar to the original person. When calculating metrics, the selection of character reference images directly impacts the results of the Cross-CIDS metric. In other tables, we report the calculation results using the character reference images within the dataset. To provide more insight, we also offer the Cross-CIDS metric results of these methods on newly generated character reference images for comparative reference, as shown in Table 5. Table 5: Cross-CIDS metric with different reference image. \"Within dataset\" refers to results calculated with reference images in ViStoryBench dataset, \"Newly generated\" refers to results calculated with newly generated reference images of methods. All results below are obtained on ViStoryBench-Lite. Method MOKI [47] AIbrm [42] ShenBi [46] Within dataset Newly generated 0.292 0.559 0. 0.338 0.683 0."
        },
        {
            "title": "H Details of User Study",
            "content": "To assess the consistency and aesthetic quality of the generated images, we conduct user study involving participants who evaluated the results across three dimensions. We employ three annotators who scored all the generated stories, and the final result was determined by averaging their scores, as shown in Table 6. The likert scale questionnaires utilized during the user study are as follows: Character Identification Consistency: Based on the provided story visualization results, please assess the character id consistency of characters throughout the story and provide score. Scoring Criteria: 0: There is lack of fundamental ID consistency, with nearly every image featuring different characters, indicating an almost complete absence of images with matching characters. 1: In smaller subset of images (about 10-30%), the main characters demonstrate mutual consistency. 2: Within moderate number of images (around 30-60%), the main characters can be recognized as having mutual consistency. 3: In substantial subset of images (approximately 60-80%), the main characters exhibit mutual consistency. However, minor portion of images still shows inconsistencies in character representation. 4: The main characters are consistently identifiable across the vast majority of images. Environment Consistency: Based on the provided story visualization results, please assess the environment consistency throughout the story and provide score. Scoring Criteria: 0: There is lack of fundamental environmental consistency; under the same environmental description, the generated scenes exhibit neither consistent style nor content. 1: At glance, the scenes appear to have some level of consistency, such as similar styles. However, upon closer inspection, the content is entirely different and lacks any coherence. 2: There is certain level of consistency in the style and semantic information of the image scenes, such as the presence of similarly styled beds, windows, and desk lamps. However, inconsistencies 20 Table 6: Results of user study. : image-ref, scale=5. Method - Story Visualizing Method StoryGen [37] (mixed) (auto-regressive) (multi-image-condition) StoryDiffusion [89] (text-only) StoryDiffusion [89] (image-ref) Story-Adapter [44] () (text-only, scale=0) (image-ref, scale=0) (text-only, scale=5) UNO [72] (many2many) TheaterGen [6] SEED-Story [78] - Video Generation Method Vlogger [92] (text-only) Vlogger [92] (image-ref) MovieAgent [69] (ROICtrl) MovieAgent [69] (SD3) Anim-Director [35] (SD3) MM-StoryAgent [76] - Commercial Software MOKI [47] Morphic Studio [49] AIbrm [42] ShenBi [46] Typemovie [63] Doubao [2] - Multi-modal Large Language Model GPT-4o [29] Gemini-2.0 [9] Character Identification Consistency Environment Subjective Consistency Aesthetics 0.37 0.10 0.18 2.72 2.62 2.90 2.33 2.55 3.10 3.20 0.35 2.05 0.87 1.30 1.90 2.45 2.52 2. 1.73 2.60 3.42 2.74 2.25 3.63 3.08 2.84 0.22 0.12 0.27 2.73 2.33 2.98 2.23 2.62 2.67 3.10 0.55 2.11 1.07 1.33 1.95 2.47 2.28 2.78 2.20 2.53 2.97 2.89 2.35 3.02 3.06 2. 0.10 0.05 0.13 2.45 2.30 2.68 2.50 2.80 2.68 3.02 0.30 1.05 0.67 1.08 1.55 1.83 1.77 2.55 2.55 2.39 3.15 2.48 2.00 3.25 3.28 2.26 exist in either the style or specific content, for instance, while tables and desk lamps are present in both, the desk lamps themselves are not similar. 3: The majority of image scenes have consistent semantic information, with their style and specific content being largely uniform. 4: Nearly all image scenes exhibit strong consistency in both style and specific content, akin to the effect of video recording within the same scene over continuous timeframe. Subjective Aesthetics: Based on the provided story visualization results, please assess the aesthetics of the story and provide score. Scoring Criteria: 0: Most characters have very obvious generation problems, such as distorted faces, extra/missing limbs, or the painting style is very uncomfortable for humans to watch. Or the image quality is extremely poor. 1: Characters have obvious generation problems, such as extra/missing limbs, distortion, etc., but there is no discomforting content. Or the image quality is poor. 2: Over 80% of the characters have no obvious physical problems, and there is no obvious content that causes physical discomfort, but the visual experience is poor. Almost all the content of the images, such as character poses, is completely the same, lacking variation. 3: Over 80% of the characters have no obvious physical problems. Mediocre picture books with ordinary visual experience, lacking variation and storytelling in images. 4: Over 80% of the characters have no obvious physical problems. Excellent and beautiful picture books that can be commercialized, with rich content, beautiful details, diversity, and interest, and obvious storytelling. H.1 Prompts for Dataset Construction We employ an LLM to transform narratives into corresponding shots. Below, we present the specific system prompt utilized for this purpose. 21 You are seasoned film script artist skilled at transforming descriptive text from novel scripts into visual content descriptions. You also adapt scripts into static shot scripts. Your designs must incorporate wide variety of compositions to perfectly capture the scripts content through imagery, ensuring the storyline is effectively conveyed in the visual descriptions. In addition, you also need to provide comprehensive introduction to the characters that appear in the shot scripts, mainly describing their appearance and clothing. Task Description You are required to write shot scripts based on the users input story, totaling <num_of_shots> shots. Each shot can feature 0 to 3 characters, meaning it can be scene shot, single-character shot, twocharacter shot, or three-character shot. For each shot in the shot script, you need to output <Plot Correspondence>, <Setting Description>, <Shot Perspective Design>, <On-stage characters>, <Static Shot Description>. The <Plot Correspondence>section requires dividing the original plot into <num_of_shots> scenes, presenting the plot of each scene in the form of narration and dialogue. Note that when dividing the input plot into different scenes, the rationality of the plot needs to be considered. Concept Explanation of Various Fields in Shot Scripts Setting Description: The story will be divided into <num_of_shots> scenes. The setting refers to the environmental setup of each scene. It should not include any characters. You need to describe all elements in the environment in detail in this field, so that the scene where the story takes place can be vividly recreated. Standard writing format: time, location, atmosphere description, other elements in the environment, lighting effects. Shot Perspective Design: Shot Perspective Design refers to information from several dimensions: shot distance, camera angle, and camera type. On-stage characters: Please select the characters appearing in this scene from the character list. The number of characters should be controlled between 0-3. If no characters appear, leave it blank. Static Shot Description: This part describes the static actions or positions of characters and items in the scene, ensuring that it describes fixed state. Writing format: <character position>, <character expression>, <character action>, <position of elements in the scene>. Requirements for Creating Setting Description in Shot Scripts You need to design shot script for the users input story. Break the story into <num_of_shots> main scenes and write scene descriptions for each of these <num_of_shots> scenes. Note that character descriptions should not appear in Setting Descriptions. Only describe the scene itself, ensuring that the scene is consistent with the original story. Do not include backgrounds, items, or other elements that are not present in the story. Think from the perspective of the visuals, using the visuals to drive the content of the shots. Ensure that all plot elements can be directly depicted through the visuals. Avoid thinking from the perspective of screenwriters script and refrain from using abstract or metaphorical expressions. Pay attention to the consistency between the characters locations and the Setting Descriptions. When writing the background content, do not directly use the expressions from the origin story. Use clear and concise sentences to describe in detail all the elements included in the background visuals and their relationships. If there are characters in the visuals, clearly express their facial expressions, demeanor, and actions. Pay attention to the visual narrative continuity between adjacent shot panels. Requirements for Creating Shot Perspective Design in Shot Scripts Continuity in Shot Composition: Adjacent shots should maintain coherence in shot composition and camera angles. By employing diverse yet consistent combination of shot compositions and camera angles, create viewing experience that is both spatially immersive and visually engaging. Shot Distance Selection: Choose the most appropriate shot distance from \"wide shot, full shot, medium long shot, medium shot, medium close up, close up\" based on the emotional atmosphere of the current scene. smaller subject size results in more relaxed emotional tone, while larger subject size creates tenser atmosphere. Consider the shot distance design of preceding and following shots as well. Guidelines for Shot Composition Combinations: Smooth Narrative Transitions: The way shot compositions are connected impacts narrative fluidity. Effective transitions involve gradual tightening or loosening. Moving from wider to tighter shots is called \"tightening,\" while moving from tighter to wider shots is called \"loosening.\" Avoid abrupt shifts from Wide Shot to extreme close-ups or vice versa. Avoid Repetitive Compositions: Ensure that adjacent shots have different compositions to prevent visual monotony. 22 Emotional Resonance: Since shot composition affects emotional tone, match shot compositions with the emotional intensity of the plot. For instance, intense emotional scenes are better suited for tighter shots. Rhythmic Pacing: The combination of shot compositions influences the visual rhythm of the scene. Use an appropriate mix of shot compositions within each episode to convey the narratives pace effectively. Camera Angle Selection: Opt for the camera angle\"front view, side view, back view\"that best suits the current scenes visual requirements. Camera Type Selection: Choose the camera type\"Eye level shot, low-angle shot, high-angle shot, birds eye view shot, dutch angle shot, foreshortening, inverted shot\"that aligns with the scenes content and emotional tone. Consider the camera types used in preceding and following shots for continuity. Camera Type Combinations: Select camera types that complement the scenes content and emotional context. Pay attention to how different camera types interact to enhance the visual storytelling. Please refer to the provided materials to choose the most fitting camera type for the current scenes content and emotional tone, ensuring that the combination of camera types supports the overall narrative effectively. Reference Materials for Camera Design For Shot Distance selection: Wide Shot: Displays the relationship between characters and their environment, commonly used to showcase scenes and background settings. Full shot: Shows the entire body of character, often used to present full actions or the overall view of scene. Medium long shot: Captures from above the characters knees. Medium shot: Captures from above the characters waist. Medium close up: Captures from above the characters chest. Close up: Focuses on close-up of the characters head or face, with the background and environment typically blurred or entirely out of view. For determining the relationship between Camera Type and content: Eye level shot: The camera is positioned at the same level as the eyeline. Low-angle shot: The camera shoots upward from below the eyeline, enhancing the subjects authority or size, often conveying power or intimidation. Suitable for emphasizing an individuals dominance or creating visual pressure, such as highlighting hero or villain. High-angle shot: The camera looks down from above the eyeline, showing the breadth of scene or diminishing the visual importance of the subject. Effectively reduces the visual scale of characters or objects, used to depict sense of isolation or helplessness in characters, or to present vast landscapes. Birds eye view shot: The camera shoots downward from high altitude, providing top-down perspective, usually covering extensive geographical areas. Highly effective when global view of events or environments is needed. Dutch angle shot: The camera is deliberately tilted during filming, often used to create sense of imbalance or tension. Particularly effective in portraying scenes of chaos, tension, or psychological instability. Foreshortening: Emphasizes depth of field through perspective techniques, making the relationship between foreground and background more prominent. Suitable for highlighting spatial relationships and depth, commonly used to enhance visual guidance and sense of depth. Inverted shot: The image is filmed upside down, challenging the audiences visual habits, often used to represent confusion or an unstable mental state. Notes Each Static Shot Description in the shot scripts should correspond sequentially to each segment of the story, providing detailed descriptions of characters expressions, actions, and states. Do not include any characters in the Setting Description. Ignore the characters in the story and only describe the environmental setting. Do not introduce items or backgrounds in the Static Shot Description that are not mentioned in the story. Ensure that the location of actions aligns with the story. Each sentence in the plot has context. Use this context to determine the best shot design. Ensure that the transitions between different shots in the shot scripts follow creative requirements. Approach the creation of shot content from visual perspective. Consider the best way to present the script visually. 23 Only provide content that meets the output format requirements; do not include any explanations. When dividing the original plot content, rewrite it into format suitable for presentation. Strictly follow the format and requirements of the output example when writing. Sample output { \"Shot 1\": { (cid:44) (cid:44) \"Plot Correspondence\": \"xxx\", \"Setting Description\": \"Twilight, stone street leading to the bell tower, bell tower with arches, town streets, distant bell tower with arches, (cid:44) romantic atmosphere, mysterious atmosphere, glowing arrow on the ground, fallen leaves, pigeons perched on rooftops, soft golden light\", (cid:44) \"Shot Perspective Design\": \"Medium shot, eye level shot\", \"On-stage characters\": [\"Fern\"], \"Static Shot Description\": \"Fern is pressed against the bell tower pillar, with an expectant and serious expression. Her right index and middle (cid:44) fingers are together, suspended in mid-air, and fine golden particles are pouring out from her fingertips.\" (cid:44) (cid:44) }, ... \"Characters\": { \"Fern\": \"A beautiful girl with long purple hair and purple eyes, wearing silver butterfly hair accessory, black coat and black boots, and (cid:44) white dress under the coat. She holds wooden staff in her hand.\", (cid:44) \"Frieren\": \"A young white-haired female elf with twin ponytails, (cid:44) blue-green eyes, pointed elf ears, pair of red earrings, white wizard robe with gold trim and brown boots, holding staff with round ruby on the top.\", (cid:44) \"Himmel\": \"A young boy with short blue hair and blue eyes, wearing blue knight uniform and white cape, holding long sword in his hand, (cid:44) with gentle yet firm expression.\" (cid:44) (cid:44) } }"
        },
        {
            "title": "I Details of Prompt Adherence Evaluation",
            "content": "Table 7: Detailed alignment scores. SEED-Story is only trained on three animations and does not pursue generalization ability. : image-ref, scale=5. Method Copy-Paste Baseline - Story Visualizing Method StoryGen [37] (mixed) (auto-regressive) (multi-image-condition) StoryDiffusion [89] (text-only) StoryDiffusion [89] (image-ref) Story-Adapter [44] () (text-only, scale=0) (image-ref, scale=0) (text-only, scale=5) UNO [72] (many2many) TheaterGen [6] SEED-Story [78] - Video Generation Method Vlogger [92] (text-only) Vlogger [92] (image-ref) MovieAgent [69] (ROICtrl) MovieAgent [69] (SD3) Anim-Director [35] (SD3) MM-StoryAgent [76] Scene score Camera Global character score action score Single character action score 0.65 1.24 1.24 1.30 1.83 2.41 2.36 2.52 2.49 2.45 2.62 0.87 0.59 2.19 2.20 1.22 3.23 3.24 1.69 0.49 0.90 1.04 1.12 3.11 1.88 1.84 2.05 1.95 1.83 3.47 2.67 1. 1.58 1.60 1.21 3.46 3.61 2.83 2.06 2.12 2.13 2.12 2.94 3.27 3.17 3.30 3.26 3.13 3.06 1.65 1.64 2.85 2.95 2.31 2.98 2.89 2.46 24 0. 1.48 1.37 1.39 1.67 2.23 2.05 2.17 2.25 1.94 2.12 0.88 0.56 1.98 2.07 1.11 2.50 2.44 1.36 In the prompt adherence evaluation, we utilize GPT-4.1 [50] to assess the consistency between the generated images and the specific content of each shot. This is achieved by prompting the LLM to provide Likert scale rating based on the given image and prompt, resulting in score ranging from 0 to 4. After averaging the numerical values of each subtask, we obtain the Alignment Score in Table 4. The detailed scores for each subtask are shown in Table 7. The detailed system prompts for each subtask are as follows: I.1 Character Interaction Actions Task Definition You will be provided with an image and text prompt describing the main characters action. As an experienced evaluator, your task is to evaluate the semantic consistency between the image and the text prompt, according to the scoring criteria. This evaluation focuses specifically on whether the action of the main character in the image aligns with the action described in the text. Scoring Criteria When evaluating the semantic consistency between an image and its corresponding text prompt, the following aspects are crucial: Relevance: Does the image show the main character performing the action or behavior mentioned in the text? The action in the image should match the core description provided in the text. Accuracy: Does the image depict the action correctly according to the text prompt? Any specific details related to the action, such as gestures, posture, or environment, should align with the description. Completeness: Does the image show the main character completing the entire action as described in the text? The image should not omit important parts of the action or behavior. Scoring Range Based on these criteria, you will assign score from 0 to 4 that reflects the degree of semantic consistency between the image and the text prompt: Very Poor (0): No correlation. The image does not reflect any aspect of the action described in the text prompt. Poor (1): Weak correlation. The image addresses the text in very general way but misses most details and accuracy of the action. Fair (2): Moderate correlation. The image depicts the action to some extent, but there are several inaccuracies or missing details. Good (3): Strong correlation. The image accurately portrays most elements of the action with minor inaccuracies or omissions. Excellent (4): Near-perfect correlation. The image closely aligns with the text prompt and portrays the main characters action with high accuracy and precision. Input format Every time you will receive text prompt and an image. Please carefully review the image and text prompt. Before giving score, please provide brief analysis of the above evaluation criteria, which should be very concise and accurate. Output Format Analysis: <Your analysis> Score: <Your Score> I.2 Shooting Method Task Definition You will be provided with an image and text prompt describing the shot type of the image. As an experienced evaluator, your task is to assess whether the generated image meets the specified shot requirements based on the evaluation criteria. Additional Material 25 Instruction: You are professor evaluator. Below is information about different shot types and shot distances. Please evaluate whether the generated image meets the requested shot type. Shot Distance Descriptions Long Shot: Shows the relationship between characters and their environment, typically used to display the scene or environment. Full Shot: Shows the full body of character, commonly used to display movement or the full scene. Medium Long Shot: Starts from above the characters knees, capturing part of the environment. Medium Shot: Captures the character from the waist up. Close-Up: Captures the character from the chest up. Extreme Close-Up: Focuses on the characters head or face, with the background and environment typically blurred or not visible. Angle Descriptions Eye Level Shot: The camera is positioned at the subjects eye level. Low Angle Shot: The camera is positioned below eye level, shooting upward, emphasizing the characters power or size. High Angle Shot: The camera is positioned above eye level, shooting downward, often minimizing the subjects significance. Birds Eye View: Camera shot taken from directly above, providing an overview of the scene. Tilted Shot: The camera is intentionally tilted to create sense of imbalance or tension. Perspective Compression: technique that emphasizes depth and the relationship between foreground and background through perspective. Scoring Range score between 0 and 4 will be assigned based on how well the shot type aligns with the content described in the prompt: Very Poor (0): The image does not meet any shot or angle requirements. Poor (1): The image meets some but not most of the shot or angle requirements. Fair (2): The image partially meets the shot or angle requirements, but some elements are off. Good (3): The image meets most of the shot or angle requirements. Excellent (4): The image fully meets all of the shot and angle requirements. Input Format You will receive text prompt and an image. Please carefully review the image and text prompt. Provide an analysis followed by score. Output Format Analysis: <Your analysis >Score: <Your score> I.3 Static Shot Description Task Definition You will be provided with an image and text prompt that describes the background, objects, and mood of the scene (excluding characters). Your task is to evaluate the consistency between the background and objects described in the prompt and what is visually represented in the image. Evaluation Criteria When assessing the semantic consistency between the image and the text prompt, focus on how well the background and non-character elements in the image match the description provided in the text. The evaluation should be based on the following aspects: Relevance: The image should clearly relate to the primary background elements and objects It should reflect the main setting and environment described, without described in the text. introducing irrelevant or unrelated features. Accuracy: Check if the specific details mentioned in the text are correctly represented in the image. This includes any mentioned objects, scenery, environmental conditions (e.g., weather, lighting), and relevant background elements. 26 Completeness: Evaluate whether the image accurately includes all critical background elements described in the text. The image should reflect the key details and setting, not leaving out essential aspects of the described background or scene. Context: The image should maintain the context of the description. If the text describes specific environment or atmosphere, the image must capture that context appropriately, considering the described mood and setting elements. Scoring Criteria Based on these factors, the image will be assigned score from 0 to 4, indicating the degree of consistency between the image and the description in the text: Very Poor (0): No correlation. The image completely fails to reflect the background or objects described in the text. Poor (1): Weak correlation. The image touches on the background or objects in very general sense but misses most of the important details or has significant inaccuracies. Fair (2): Moderate correlation. The image contains some relevant background and objects, but several important details are missing or inaccurately represented. Good (3): Strong correlation. The image accurately represents most of the described background and objects with minor omissions or inaccuracies. Excellent (4): Near-perfect correlation. The image perfectly captures the background and objects as described in the text, leaving no significant details missing or inaccurate. Input Format You will receive text prompt and an image. Please carefully review the image and text prompt. Provide an analysis followed by score. Output Format Analysis: <Your analysis> Score: <Your Score> I.4 Individual Action We first crop the person from the image using bounding box, which is then utilized for metric calculation: Task Definition For each evaluation, you will receive text prompt, an image, and character name. Your task is to first extract the individual action or behavior of the specified character from the text prompt, then determine whether the image accurately reflects this description for that character, and finally assign score based on the criteria. Evaluation Process Extract Action Information: Carefully extract the specific action or behavior described for the given character (character name) from the text prompt. Image Comparison: Examine the image to determine whether the specified characters action matches the extracted description. Analyze and Score: Analyze the match according to the scoring criteria and assign score. Scoring Criteria Focus on the following aspects when evaluating: Relevance: Does the image show the specified character performing the action or behavior described in the text? Accuracy: Are the details of the characters action in the image (such as posture, gestures, environment) consistent with the text description? Completeness: Does the image fully depict the character completing the entire action as described, without omitting important parts? Assign score from 0 to 4 based on the degree of semantic consistency: 0 (Very Poor): No correlation. The image does not reflect any aspect of the described action. 1 (Poor): Weak correlation. The image only generally addresses the text, missing most details and accuracy. 2 (Fair): Moderate correlation. The image depicts the action to some extent but with several inaccuracies or missing details. 27 3 (Good): Strong correlation. The image accurately portrays most elements of the action, with only minor inaccuracies or omissions. 4 (Excellent): Near-perfect correlation. The image closely aligns with the text prompt and depicts the characters action with high accuracy and completeness. Output Format Analysis: <Your analysis>Score: <Your Score>"
        },
        {
            "title": "J Details of Computing Resource",
            "content": "To ensure the reproducibility of computational experiments, we provide detailed specifications of the hardware resources here. For each method, we document the type of GPU accelerators (e.g., NVIDIA H800 80GB) and the computational efficiency of each metric. Specifically, we report the average runtime per evaluation metric in Table 8. Table 8: Computational efficiency of evaluation metrics. Metric Scope Time Value Unit Notes Aesthetics Score Style Similarity Character Similarity Inception Score Prompt Alignment* Single image Pair images Pair images Total data Single image 26 46 450 8.057 25.173 ms ms ms per generated image cross or self cross or self full dataset per generated image * Longest computation due to LLM inference constraints"
        },
        {
            "title": "References",
            "content": "[1] Emanuele Bugliarello, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, and Paul Voigtlaender. Storybench: multifaceted benchmark for continuous story visualization. Advances in Neural Information Processing Systems, 36:7809578125, 2023. [2] ByteDance Inc. Doubao ai assistant. https://www.doubao.com/, 2024. Accessed: 2025-04-16. [3] Hong Chen, Rujun Han, Te-Lin Wu, Hideki Nakayama, and Nanyun Peng. Character-centric story visualization via visual planning and token alignment. arXiv preprint arXiv:2210.08465, 2022. [4] Siyu Chen, Dengjie Li, Zenghao Bao, Yao Zhou, Lingfeng Tan, Yujie Zhong, and Zheng Zhao. Manga generation via layout-controllable diffusion. arXiv preprint arXiv:2412.19303, 2024. [5] Junhao Cheng, Xi Lu, Hanhui Li, Khun Loun Zai, Baiqiao Yin, Yuhao Cheng, Yiqiang Yan, and Xiaodan Liang. Autostudio: Crafting consistent subjects in multi-turn interactive image generation. arXiv preprint arXiv:2406.01388, 2024. [6] Junhao Cheng, Baiqiao Yin, Kaixin Cai, Minbin Huang, Hanhui Li, Yuxin He, Xi Lu, Yue Li, Yifei Li, Yuhao Cheng, et al. Theatergen: Character management with llm for consistent multi-turn image generation. arXiv preprint arXiv:2404.18919, 2024. [7] Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu Chen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, et al. Dna-rendering: diverse neural actor repository for high-fidelity human-centric rendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1998219993, 2023. [8] Wei Cheng, Su Xu, Jingtan Piao, Chen Qian, Wayne Wu, Kwan-Yee Lin, and Hongsheng Li. Generalizable neural performer: Learning robust radiance fields for human novel view synthesis. arXiv preprint arXiv:2204.11798, 2022. [9] Google DeepMind. Gemini 2.0 flash: Native image generation in google ai studio. https://developers. googleblog.com/en/experiment-with-gemini-20-flash-native-image-generation/, 2025. Accessed: 2025-04-16. [10] Google DeepMind. Google gemini2. experiment with gemini 2.0 flash native image generation, 2025. [11] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2019. [12] discus0434. Aesthetic predictor v2.5. https://github.com/discus0434/ aesthetic-predictor-v2-5, 2024. Accessed: 2025-05-10. [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [14] Zhangyin Feng, Yuchen Ren, Xinmiao Yu, Xiaocheng Feng, Duyu Tang, Shuming Shi, and Bing Qin. Improved visual story generation with adaptive context modeling. arXiv preprint arXiv:2305.16811, 2023. [15] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. [16] Silin Gao, Sheryl Mathew, Li Mi, Sepideh Mamooler, Mengjie Zhao, Hiromi Wakaki, Yuki Mitsufuji, Syrielle Montariol, and Antoine Bosselut. Vinabench: Benchmark for faithful and consistent visual narratives. arXiv preprint arXiv:2503.20871, 2025. [17] Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, and Ivan Laptev. Short film dataset (sfd): benchmark for story-level video understanding. arXiv preprint arXiv:2406.10221, 2024. [18] Lixue Gong, Xiaoxia Hou, Fanshi Li, Liang Li, Xiaochen Lian, Fei Liu, Liyang Liu, Wei Liu, Wei Lu, Yichun Shi, et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. [19] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. arXiv preprint arXiv:2503.10589, 2025. [20] Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha Kembhavi. Imagine this! scripts to compositions to videos. In Proceedings of the European conference on computer vision (ECCV), pages 598613, 2018. [21] Sebastian Hartwig, Dominik Engel, Leon Sick, Hannah Kniesel, Tristan Payer, Poonam Poonam, Michael Glöckler, Alex Bäuerle, and Timo Ropinski. survey on quality metrics for text-to-image generation. arXiv preprint arXiv:2403.11821, 2024. 29 [22] Huiguo He, Huan Yang, Zixi Tuo, Yuan Zhou, Qiuyue Wang, Yuhang Zhang, Zeyu Liu, Wenhao Huang, Hongyang Chao, and Jian Yin. Dreamstory: Open-domain story visualization by llm-guided multi-subject consistent diffusion. arXiv preprint arXiv:2407.12899, 2024. [23] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [25] Panwen Hu, Jin Jiang, Jianqi Chen, Mingfei Han, Shengcai Liao, Xiaojun Chang, and Xiaodan Liang. Storyagent: Customized storytelling video generation via multi-agent collaboration. arXiv preprint arXiv:2411.04925, 2024. [26] Haoyang Huang, Guoqing Ma, Nan Duan, Xing Chen, Changyi Wan, Ranchen Ming, Tianyu Wang, Bo Wang, Zhiying Lu, Aojie Li, et al. Step-video-ti2v technical report: state-of-the-art text-driven image-to-video generation model. arXiv preprint arXiv:2503.11251, 2025. [27] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies, pages 12331239, 2016. [28] Yuzhou Huang, Yiran Qin, Shunlin Lu, Xintao Wang, Rui Huang, Ying Shan, and Ruimao Zhang. Story3d-agent: Exploring 3d storytelling visualization with large language models. arXiv preprint arXiv:2408.11801, 2024. [29] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [30] Hyeonho Jeong, Gihyun Kwon, and Jong Chul Ye. Zero-shot generation of coherent storybook from plain text story using diffusion models. arXiv preprint arXiv:2302.03900, 2023. [31] Taewon Kang, Divya Kothandaraman, and Ming C. Lin. Text2story: Advancing video storytelling with text guidance. arXiv preprint arXiv:2503.06310, 2025. [32] Seungkwon Kim, GyuTae Park, Sangyeon Kim, and Seung-Hun Nam. Visagent: Narrative-preserving story visualization framework. arXiv preprint arXiv:2503.02399, 2024. [33] Bowen Li and Thomas Lukasiewicz. Word-level fine-grained story visualization. arXiv preprint arXiv:2208.02341, 2022. [34] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, and Jianfeng Gao. Storygan: sequential conditional gan for story visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63296338, 2019. [35] Yunxin Li, Haoyuan Shi, Baotian Hu, Longyue Wang, Jiashun Zhu, Jinyi Xu, Zhen Zhao, and Min Zhang. Anim-director: large multimodal model powered agent for controllable animation video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [36] Zuzeng Lin, Ailin Huang, and Zhewei Huang. Collaborative neural rendering using anime character sheets. arXiv preprint arXiv:2207.05378, 2022. [37] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimm - open-ended visual storytelling via latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 61906200, June 2024. [38] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimmopen-ended visual storytelling via latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61906200, 2024. [39] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [40] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [41] Xiangyang Luo, Junhao Cheng, Yifan Xie, Xin Zhang, Tao Feng, Zhou Liu, Fei Ma, and Fei Yu. Object isolated attention for consistent story visualization. arXiv preprint arXiv:2503.23353, 2024. [42] MagicLight AI. Brmgo: Ai-powered tool for story script generation. https://brmgo.cn/, 2025. Accessed: 2025-04-16. 30 [43] Adyasha Maharana and Mohit Bansal. Integrating visuospatial, linguistic and commonsense structure into story visualization. arXiv preprint arXiv:2110.10834, 2021. [44] Jiawei Mao, Xiaoke Huang, Yunfei Xie, Yuanqi Chang, Mude Hui, Bingjie Xu, and Yuyin Zhou. StoryAdapter: Training-free Iterative Framework for Long Story Visualization, 2024. [45] Jiawei Mao, Xiaoke Huang, Yunfei Xie, Yuanqi Chang, Mude Hui, Bingjie Xu, and Yuyin Zhou. Storyadapter: training-free iterative framework for long story visualization. arXiv preprint arXiv:2410.06244, 2024. [46] Maoyan Entertainment. Shenbi: Ai-powered scriptwriting tool by maoyan. https://shenbi.maoyan. com/, 2025. Accessed: 2025-04-16. [47] Meitu Inc. Moki: Ai short film creation tool. https://www.moki.cn, 2024. [48] Ruibo Ming, Zhewei Huang, Zhuoxuan Ju, Jianming Hu, Lihui Peng, and Shuchang Zhou. survey on future frame synthesis: Bridging deterministic and generative approaches. arXiv preprint arXiv:2401.14718, 2024. [49] Morphic, Inc. Introducing morphic studio. https://www.morphic.com/, July 2024. Accessed: 202504-16. [50] OpenAI. Gpt-4.1: Advanced large language model for natural language understanding and generation. https://openai.com/research/gpt-4-1, 2025. Accessed: 2025-04-16. [51] Dongwei Pan, Long Zhuo, Jingtan Piao, Huiwen Luo, Wei Cheng, Yuxin Wang, Siming Fan, Shengqi Liu, Lei Yang, Bo Dai, et al. Renderme-360: large digital asset library and benchmarks towards high-fidelity head avatars. Advances in Neural Information Processing Systems, 36:79938005, 2023. [52] Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, and Wenhu Chen. Synthesizing coherent story with autoregressive latent diffusion models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 29202930, 2024. [53] Christos Papadimitriou, Giorgos Filandrianos, Maria Lymperaiou, and Giorgos Stamou. Masked generative story transformer with character guidance and caption augmentation, 2024. [54] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [56] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in neural information processing systems, pages 22342242, 2016. [57] Fei Shen, Hu Ye, Sibo Liu, Jun Zhang, Cong Wang, Xiao Han, and Wei Yang. Boosting consistency in story visualization with rich-contextual conditional diffusion models. arXiv preprint arXiv:2407.02482, 2024. [58] Jaskirat Singh, Junshen Kevin Chen, Jonas Kohler, and Michael Cohen. Storybooth: Training-free multi-subject consistency for improved visual storytelling. arXiv preprint arXiv:2504.05800, 2024. [59] Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. arXiv preprint arXiv:2404.01292, 2024. [60] StepFun. Step-1v: hundred billion parameter multimodal large model, 2024. [61] Ming Tao, Bing-Kun Bao, Hao Tang, Yaowei Wang, and Changsheng Xu. Storyimager: unified and efficient framework for coherent story visualization and completion. arXiv preprint arXiv:2404.05979, 2024. [62] Chongjun Tu, Lin Zhang, Pengtao Chen, Peng Ye, Xianfang Zeng, Wei Cheng, Gang Yu, and Tao Chen. Favor-bench: comprehensive benchmark for fine-grained video motion understanding. arXiv preprint arXiv:2503.14935, 2025. [63] TypeMovie Team. Typemovie: Text-to-video storytelling with style and rhythm. https://typemovie. art, 2024. Accessed: 2025-04-16. [64] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, 2023. [65] Jiuniu Wang, Zehua Du, Yuyuan Zhao, Bo Yuan, Kexiang Wang, Jian Liang, Yaxi Zhao, Yihen Lu, Gengliang Li, Junlong Gao, Xin Tu, and Zhenyu Guo. Aesopagent: Agent-driven evolutionary system on story-to-video production. arXiv preprint arXiv:2403.07952, 2024. 31 [66] Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, and Chunhua Shen. Autostory: Generating diverse storytelling images with minimal human effort. Int. J. Computer Vision, 2024. [67] Wenjia Wang, Liang Pan, Zhiyang Dou, Jidong Mei, Zhouyingcheng Liao, Yuke Lou, Yifan Wu, Lei Yang, Jingbo Wang, and Taku Komura. Sims: Simulating stylized human-scene interactions with retrievalaugmented script generation. arXiv preprint arXiv:2411.19921, 2024. [68] Zun Wang, Jialu Li, Han Lin, Jaehong Yoon, and Mohit Bansal. Dreamrunner: Fine-grained compositional story-to-video generation with retrieval-augmented motion adaptation. arXiv preprint arXiv:2411.16657, 2024. [69] Mike Zheng Shou Weijia Wu, Zeyu Zhu. Automated movie generation via multi-agent cot planning, 2025. [70] Jianzong Wu, Chao Tang, Jingbo Wang, Yanhong Zeng, Xiangtai Li, and Yunhai Tong. Diffsensei: Bridging multi-modal llms and diffusion models for customized manga generation. arXiv preprint arXiv:2412.07589, 2024. [71] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. [72] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. [73] Weijia Wu, Mingyu Liu, Zeyu Zhu, Xi Xia, Haoen Feng, Wen Wang, Kevin Qinghong Lin, Chunhua Shen, and Mike Zheng Shou. Moviebench: hierarchical movie level dataset for long video generation. arXiv preprint arXiv:2411.15262, 2024. [74] Weijia Wu, Zeyu Zhu, and Mike Zheng Shou. Automated movie generation via multi-agent cot planning. arXiv preprint arXiv:2503.07314, 2025. [75] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 2023. [76] Xuenan Xu, Jiahao Mei, Chenliang Li, Yuning Wu, Ming Yan, Shaopeng Lai, Ji Zhang, and Mengyue Wu. Mm-storyagent: Immersive narrated storybook video generation with multi-agent paradigm across text, image and audio. arXiv preprint arXiv:2503.05242, 2024. [77] Xuenan Xu, Jiahao Mei, Chenliang Li, Yuning Wu, Ming Yan, Shaopeng Lai, Ji Zhang, and Mengyue Wu. Mm-storyagent: Immersive narrated storybook video generation with multi-agent paradigm across text, image and audio. arXiv preprint arXiv:2503.05242, 2025. [78] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683, 2024. [79] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683, 2024. [80] Zilyu Ye, Jinxiu Liu, JinJin Cao, Zhiyang Chen, Ziwei Xuan, Mingyuan Zhou, Qi Liu, and Guo-Jun Qi. Openstory: large-scale open-domain dataset for subject-driven visual storytelling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79537962, 2024. [81] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [82] Chi Zhang, Yuanzhi Liang, Xi Qiu, Fangqiu Yi, and Xuelong Li. Vast 1.0: unified framework for controllable and consistent video generation. arXiv preprint arXiv:2412.16677, 2024. [83] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection, 2022. [84] Jinlu Zhang, Jiji Tang, Rongsheng Zhang, Tangjie Lv, and Xiaoshuai Sun. Storyweaver: unified world model for knowledge-enhanced story character customization. Proceedings of the AAAI Conference on Artificial Intelligence, 39(9):99519959, Apr. 2025. [85] Min Zhang, Zilin Wang, Liyan Chen, Kunhong Liu, and Juncong Lin. Dialogue director: Bridging the gap in dialogue visualization for multimodal storytelling. arXiv preprint arXiv:2412.20725, 2024. [86] Runze Zhang, Guoguang Du, Xiaochuan Li, Qi Jia, Liang Jin, Lu Liu, Jingjing Wang, Cong Xu, Zhenhua Guo, Yaqian Zhao, et al. Dropletvideo: dataset and approach to explore integral spatio-temporal consistent video generation. arXiv preprint arXiv:2503.06053, 2025. [87] Canyu Zhao, Mingyu Liu, Wen Wang, Jianlong Yuan, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence, 2024. 32 [88] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, Zicheng Zhang, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, and Haodong Duan. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. [89] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. Advances in Neural Information Processing Systems, 37:110315110340, 2024. [90] Shudong Zhu(Andrew Zhu). Long prompt weighted stable diffusion embedding. https://github.com/ xhinker/sd_embed, 2024. [91] Cailin Zhuang, Yaoqi Hu, Xuanyang Zhang, Wei Cheng, Jiacheng Bao, Shengqi Liu, Yiying Yang, Xianfang Zeng, Gang Yu, and Ming Li. Styleme3d: Stylization with disentangled priors by multiple encoders on 3d gaussians. arXiv preprint arXiv:2504.15281, 2025. [92] Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, and Yali Wang. Vlogger: Make your dream vlog. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88068817, 2024."
        }
    ],
    "affiliations": [
        "AGI Lab, Westlake University",
        "AIGC Research",
        "ShanghaiTech University",
        "StepFun"
    ]
}