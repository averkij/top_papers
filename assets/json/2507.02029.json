{
    "paper_title": "RoboBrain 2.0 Technical Report",
    "authors": [
        "BAAI RoboBrain Team",
        "Mingyu Cao",
        "Huajie Tan",
        "Yuheng Ji",
        "Minglan Lin",
        "Zhiyu Li",
        "Zhou Cao",
        "Pengwei Wang",
        "Enshen Zhou",
        "Yi Han",
        "Yingbo Tang",
        "Xiangqi Xu",
        "Wei Guo",
        "Yaoxu Lyu",
        "Yijie Xu",
        "Jiayu Shi",
        "Mengfei Du",
        "Cheng Chi",
        "Mengdi Zhao",
        "Xiaoshuai Hao",
        "Junkai Zhao",
        "Xiaojie Zhang",
        "Sh/anyu Rong",
        "Huaihai Lyu",
        "Zhengliang Cai",
        "Yankai Fu",
        "Ning Chen",
        "Bolun Zhang",
        "Lingfeng Zhang",
        "Shuyi Zhang",
        "Xi Feng",
        "Songjing Wang",
        "Xiaodan Liu",
        "Yance Jiao",
        "Mengsi Lyu",
        "Zhuo Chen",
        "Chenrui He",
        "Yulong Ao",
        "Xue Sun",
        "Zheqi He",
        "Jingshu Zheng",
        "Xi Yang",
        "Donghai Shi",
        "Kunchang Xie",
        "Bochao Zhang",
        "Shaokai Nie",
        "Chunlei Men",
        "Yonghua Lin",
        "Zhongyuan Wang",
        "Tiejun Huang",
        "Shanghang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: a lightweight 7B model and a full-scale 32B model, featuring a heterogeneous architecture with a vision encoder and a language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across a wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io."
        },
        {
            "title": "Start",
            "content": "RoboBrain 2.0 Technical Report"
        },
        {
            "title": "BAAI RoboBrain Team",
            "content": "Please see Contributions and Author List for more author details."
        },
        {
            "title": "Abstract",
            "content": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: lightweight 7B model and full-scale 32B model, featuring heterogeneous architecture with vision encoder and language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent longhorizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io. 5 2 0 2 5 ] . [ 2 9 2 0 2 0 . 7 0 5 2 : r Figure 1 Benchmark comparison across spatial and temporal reasoning. RoboBrain2.0-32B achieves best performance on both spatial and temporal reasoning benchmarks across BLINK-Spatial, RoboSpatial, RefSpatial-Bench, Where2Place, EgoPlan2 and Multi-Robot-Plan, outperforming prior open-source models and proprietary models."
        },
        {
            "title": "Contents",
            "content": "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Input Modalities and Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Vision Encoder and Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 LLM Decoder and Output Representations 3 Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 General MLLM VQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Spatial Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Temporal Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Training Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 Stage 1: Foundational Spatiotemporal Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Stage 2: Embodied Spatiotemporal Enhancement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Stage 3: Chain-of-Thought Reasoning in Embodied Contexts 5 Infrastructures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Large-Scale Training Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Multi-Dimensional Hybrid Parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Pre-Allocate Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.3 Data Pre-Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.4 Distributed Data Loading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.5 Fault Tolerance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Reinforcement Fine-Tuning Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inference Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5. 6 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1 Spatial Reasoning Capability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Temporal Reasoning Capability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Conclusion and Future Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Contributions and Author List . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Qualitative examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Examples for Pointing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Examples for Affordance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Examples for Trajectory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Examples for EgoPlan2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Examples for Close-Loop Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.6 Examples for Multi-Robot Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.7 Examples for Synthetic Benchmarks Prompts Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Spatial Understanding: Coordinates Pointing . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Spatial Understanding: Coordinates Trajectory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Spatial Understanding: Bounding Box Affordance B.4 Spatial Understanding: Freeform Q&A General Spatial Analysis . . . . . . . . . . . . . . . B.5 Temporal Understanding: Long-horizon Planning . . . . . . . . . . . . . . . . . . . . . . . . . B.6 Temporal Understanding: Closed Loop Conversation . . . . . . . . . . . . . . . . . . . . . . . B.7 Temporal Understanding: Multi-Robot Planning . . . . . . . . . . . . . . . . . . . . . . . . . 4 5 5 6 6 6 7 8 9 9 10 10 11 11 11 11 11 12 12 12 12 13 13 15 22 23 23 40 42 44 47 51 52 54 54 54 54 55 55 55 55"
        },
        {
            "title": "Introduction",
            "content": "In recent years, large language models (LLMs) and vision-language models (VLMs) have emerged as key driving forces in the advancement of general artificial intelligence (AGI). Within digital environments, these models have demonstrated remarkable capabilities in perception [5, 16, 83], understanding [22, 73], and reasoning [2, 17, 18, 45, 65], and have been widely applied in tasks such as multimodal question answering [35, 60], image generation and editing [24, 57], GUI control [37, 71], and video understanding [7, 63, 72]. They have also seen early adoption in practical domains such as education, healthcare, search, and intelligent assistants [11, 21, 82]. However, bridging the gap between digital intelligence and physical intelligenceenabling models to perceive their surroundings, understand embodied tasks, and interact with the real worldremains critical challenge on the path toward AGI. Embodied foundation models [4, 64, 74] represent promising research direction toward physical intelligence. Several recent efforts have extended the capabilities of LLMs and VLMs to embodied scenarios, advancing multimodal fusion, perception, and action execution. While these models have achieved encouraging progress, they still face three fundamental capability bottlenecks when deployed in complex and open-ended real-world environments: (1) Limited spatial understanding: Current models struggle to accurately model relative and absolute spatial relationships and identify affordances in physical environments, which hinders real-world applicability; (2) Weak temporal modeling: The lack of understanding of multi-stage, cross-agent temporal dependencies and feedback mechanisms limits long-horizon planning and closed-loop control; (3) Insufficient reasoning chains: Existing models are often incapable of extracting causal logic from complex human instructions and aligning it with dynamic environmental states, restricting their generalization to open-ended embodied tasks. To address these challenges, we present RoboBrain 2.0, our latest generation of embodied vision-language foundation models, tailored to bridge perception, reasoning, and planning in physically environments. RoboBrain 2.0 processes visual observations and language instructions in unified architecture, enabling holistic understanding of the environment, goal-directed reasoning, and long-horizon planning. We release two variants of the model: the lightweight RoboBrain 2.07B and the full-scale RoboBrain 2.032B, designed to meet different deployment needs under varying resource constraints. On both spatial reasoning and temporal reasoning benchmarks, the 32B variant mostly achieves state-of-the-art performance, outperforming prior open-source and proprietary models, as shown in Figure 1. Model capabilities are summarized in Figure 2. This report provides systematic overview of the design principles, core components and key innovations. In particular, we highlight the extensive data contributions that support spatial understanding, temporal reasoning, and causal inference, which form the foundation of RoboBrain 2.0s capabilities. To address the scarcity of spatial data, we develop spatial data synthesis pipeline that constructs large-scale, high-quality datasets spanning tasks such as pointing, affordance prediction, and trajectory generation. To improve temporal reasoning and feedback modeling, we design multi-robot coordination templates across common scenarios via RoboOS [61], generate cross-agent long-horizon planning trajectories using external models [31], and simulate randomized failure events to collect closed-loop feedback data that enhances model robustness. To further enrich reasoning data, we extract step-by-step thought traces from powerful reasoning VLMs [22], conditioned on spatiotemporal task contexts. These traces serve as supervision signals for learning causal chains across vision, language, and action. RoboBrain 2.0 adopts high-efficiency heterogeneous architecture and progressive multi-stage training strategy to support spatial understanding, temporal modeling, and long-chain causal reasoning in embodied settings. The model comprises lightweight vision encoder with approximately 689M parameters and decoder-only language model with 7B/32B parameters. It is trained using three-stage curriculumcovering foundational spatiotemporal learning, embodied spatiotemporal enhancement, and chain-of-thought reasoningon large-scale multimodal and embodied datasets. Training is conducted using our open-source framework FlagScale, which integrates hybrid parallelism, pre-allocated memory optimization, high-throughput I/O pipelines, and robust fault tolerance. These infrastructure innovations significantly reduce training and deployment costs while ensuring scalability for large-scale multimodal models. We evaluate RoboBrain 2.0 on over 12 public benchmarks covering spatial understanding, temporal modeling and multimodal reasoning, achieving state-of-the-art results on 6 of them despite its compact size. We release code, checkpoints, and benchmarks as open-source resources to benefit the research community. These materials facilitate reproducible 3 Figure 2 The overview of RoboBrain 2.0s Capabilities. RoboBrain 2.0 supports interactive reasoning with long-horizon planning and closed-loop feedback, spatial perception for precise point and bounding box prediction from complex instructions, temporal perception for future trajectory estimation, and scene reasoning through real-time scene graph construction and updating. research, accelerate embodied AI development, and enable practical deployment in robotic systems. To provide comprehensive view of RoboBrain 2.0s architecture, training methodology, and capabilities, this report is organized as follows: Section 2 introduces the overall model design, including the coordination between the vision encoder and language model, as well as image and video input strategies. Section 3 describes the data curation and construction process, covering three major categories: general multimodal understanding, spatial reasoning, and temporal modeling. Section 4 presents our multi-stage training strategies, including foundational spatiotemporal learning, embodied enhancement, and chain-of-thought reasoning. Section 5 outlines the infrastructure stack supporting scalable training and inference, including hybrid parallelization, memory optimization, data loading, and failure recovery. Section 6 reports extensive evaluation results on public benchmarks, highlights RoboBrain 2.0s capabilities in spatial reasoning, temporal feedback, and embodied planning. Finally, Section 7 discusses current limitations, and outlines future research directions."
        },
        {
            "title": "2 Architecture",
            "content": "RoboBrain 2.0 employs modular encoder-decoder architecture that unifies perception, reasoning, and planning for complex embodied tasks. As shown in Figure 3, it processes multi-view visual observations and natural language instructions through four core components: (1) tokenizer for textual/structured inputs, (2) vision encoder, (3) an MLP projector mapping visual features to the language models token space, and (4) language model backbone initialized from Qwen2.5-VL [5]. Unlike conventional VLMs [2, 22] focused on general static VQA, RoboBrain 2.0 maintains strong general VQA capabilities while specializing in embodied reasoning tasks like spatial perception, temporal modeling, and long-chain causal reasoning. The architecture encodes high-resolution images, multi-view inputs, video frames, language instructions, and scene graphs into unified multimodal token sequence for comprehensive processing. 4 Figure 3 The Architecture of RoboBrain 2.0. The model supports multi-image, long video, and high-resolution visual inputs, along with complex task instructions and structured scene graphs on the language side. Visual inputs are processed via vision encoder and an MLP projector, while textual inputs are tokenized into unified token stream. All inputs are fed into an LLM decoder that performs long-chain-of-thought reasoning and generates variety of outputs depending on the task, including structured plans, spatial relations, or relative and absolute coordinates. 2."
        },
        {
            "title": "Input Modalities and Tokenization",
            "content": "RoboBrain 2.0 supports diverse set of input modalities tailored for embodied AI tasks: Language instructions: Natural language commands describing high-level goals or low-level actions. RoboBrain 2.0 processes natural language commands spanning different abstraction levels: from high-level, spatially grounded instructions (e.g., Carry the apple to the nearest table, aligned with the leftmost cup) to low-level motor commands (e.g., Navigate to the nearest table, Grasp the apple, Detect position aligned with the leftmost cup, Place the apple into the box). Scene graph: structured JSON representation of the explored environment, containing information about discovered objects, their categories, spatial locations, and embodiment configuration (e.g., name: KitchenTable1, type: table, object: [basket, knife], robot: RealMan-single-arm). Multi-view static images: Images captured from multiple viewpoints, such as head-mounted cameras, wristmounted cameras, or multi-view projections from 3D environment. These are processed independently by the vision encoder and concatenated into unified token sequence. Video frames: Video sequences (e.g., egocentric views from the agent), optionally annotated with timestamp tokens [5] to facilitate temporal grounding and reasoning. Language instructions and scene graphs are tokenized using the language tokenizer. Visual inputsincluding multi-view images and video framesare processed by the vision encoder into dense visual embeddings, which are then projected into the LLMs token space through an MLP projector, enabling unified multi-modal reasoning within the decoder."
        },
        {
            "title": "2.2 Vision Encoder and Projection",
            "content": "RoboBrain 2.0 vision encoder supports dynamic-resolution image and video inputs through adaptive positional encoding and windowed attention mechanisms [5]. This design choice enables efficient processing of highresolution and multi-view visual observations common in embodied tasks. To accommodate the long-horizon and temporally grounded nature of such tasks, we adopt frame-wise visual tokenization with multi-dimensional RoPE [5] for spatiotemporal encoding. Each visual embedding is projected via lightweight MLP into the token space of the language model. For multi-view scenarios, visual tokens from different camera perspectives are serialized and augmented with view-specific positional identifiers before being fused with other input modalities."
        },
        {
            "title": "2.3 LLM Decoder and Output Representations",
            "content": "RoboBrain 2.0 employs decoder-only language model designed to unify high-level reasoning and spatially grounded output generation. Unlike conventional VLMs that primarily return short-form answers to static prompts, RoboBrain 2.0 flexibly supports both concise responses and multi-step chain-of-thought reasoning. This capability enables deeper understanding of complex instructions and physical scenes. To enable the decoder to handle embodied tasks, the decoder is trained to produce diverse range of outputs, including semantically grounded expressions (e.g., referring to objects or actions), spatial coordinates (e.g., absolute positions or bounding boxes), and intermediate reasoning traces. Rotary positional encodings and temporally conditioned tokens allow the model to maintain coherence across multi-round perception-action loops, which are essential for long-horizon planning in dynamic environments. Output formats supported by RoboBrain 2.0 include: (1) Free-form text: Used for task decomposition, scene graph updates, agent invocation, and human-agent dialogue. (2) Spatial coordinates: Used to represent point locations, bounding boxes, or trajectories in the image space for downstream controllers. (3) Reasoning traces (Optional): Long-chain-of-thought explanations to support deep problem solving and decision transparency. This unified decoding formulation allows RoboBrain 2.0 to effectively handle wide range of embodied tasks, from spatial grounding and visual understanding to long-horizon multi-agent planning and causal reasoning."
        },
        {
            "title": "3 Training Data",
            "content": "As shown in Figure 4, RoboBrain 2.0 is trained on diverse and extensive dataset designed to enhance its capabilities in spatial understanding, temporal modeling and long-chain causal reasoning in embodied settings. The training data encompasses wide range of modalities, including high-resolution images, multi-view inputs, video sequences, scene graph and natural language instructions. This comprehensive dataset is meticulously categorized into three primary types: general multimodal understanding, spatial perception, and temporal modeling, ensuring the model can effectively perceive, reason, and plan in complex physical environments. Figure 4 Training Data Distribution for RoboBrain 2.0. This figure illustrates the distribution of training data supporting RoboBrain 2.0s capabilities, including interactive reasoning with long-horizon planning and closed-loop feedback, spatial perception for precise point and bounding box prediction from complex instructions, and multi-agent collaboration tasks, which is meticulously categorized into three primary types: general multimodal understanding, spatial perception, and temporal modeling."
        },
        {
            "title": "3.1 General MLLM VQA\nHigh Quality Data. The general training dataset for RoboBrain 2.0 includes 873K high-quality samples,\nprimarily derived from LLaVA-665K [33] and LRV-400K [32], spanning standard Visual Question Answer-\ning(VQA), region-level queries, OCR-based VQA, and visual dialogues. (1) LLaVA-665K serves as the primary",
            "content": "6 source and contains diverse VQA-style data, including standard VQA datasets, OCR-based questions, regionlevel queries, visual conversations, and language-only dialogues. To improve training efficiency, multiple question-answer(QA) pairs from the same image merge into single conversations; invalid ShareGPT [10] entries are filtered out, and overly long conversations (>2048 tokens) are truncated (resulting in 40K valid samples). Specifically, A-OKVQA [54] samples are augmented by duplicating choices to balance multiple-choice formats, OCR-VQA [41] contributes 80K sampled conversations focused on scene text understanding, Visual Genome(VG) [27] provides dense object-level annotations limited to 10 entries per image with additional captions, and RefCOCO [76] dialogues are split into short multi-turn segments (<10 exchanges). Languageonly conversations, which are generally longer than visual ones, are sampled in single-modality batches to improve throughput by 25% without performance degradation. After removing bounding-box-dependent QA pairs, 531K high-quality samples are retained from this source. (2) LRV-400K is synthetically generated using GPT-4 [44] under few-shot instruction-following setting. It produces 400K image-conditioned instructions across 16 vision-language tasks with textual answers. Unlike prior works that rely on sparse image captions, this dataset leverages the dense annotations in VG (e.g., bounding boxes, dimensions, and 21 object regions per image). GPT-4 generates both declarative and interrogative prompts for each image, with 10 tasks randomly sampled per instance. After filtering out bounding-box-related QA pairs, 342K samples are selected for training."
        },
        {
            "title": "3.2 Spatial Data\nVisual Grounding. The visual grounding dataset is constructed to enhance multimodal understanding through\nprecise object-level localization, leveraging the extensive annotations from LVIS [19]. We carefully curate\n152K high-resolution images from LVIS, ensuring broad coverage of diverse object categories and complex\nvisual scenes. Each object annotation is converted into standardized bounding box coordinates (x1, y1, x2, y2)\nrepresenting the top-left and bottom-right corners, enabling consistent spatial referencing. To facilitate rich\nvisual dialogue, we generated 86K conversational sequences, each containing multiple rounds of QA pairs that\nprogressively explore visual relationships, attribute reasoning, and contextual understanding. The dataset\nmaintains a balanced distribution across object categories while preserving challenging cases of occlusion,\nviewpoint variation, and rare instances to support robust visual grounding.\nObject Pointing. The object pointing dataset is constructed to enable RoboBrain 2.0 to identify the locations of\nspecified objects through pointing within an image. We leverage the Pixmo-Points [13] dataset, which includes\n2.3M point annotations across 223K images as our data source. However, direct utilization of Pixmo-Points\ndata for RoboBrain 2.0 training presents challenges due to densely repeated object instances (e.g., books on\na shelf). To address this, we implement a two-step filtering process: (1) we discard annotations with more\nthan ten labeled points to simplify training, and (2) we use GPT-4o [22] as a scene analyzer to select only\nindoor-relevant objects, such as kitchenware, furniture, and decorations, excluding irrelevant or outdoor scenes.\nThis process yields 190K QA pairs for 64K images with reduced clutter, making the data more suitable for\nembodied contexts. To construct QA pairs for pointing tasks, we construct 28 human-designed templates,\nsuch as “Point out all instances of {label} in the image.” or “Help me find {label} in the image by pointing\nto them.” Here, {label} refers to object categories from the annotations. Templates are randomly selected\nto ensure linguistic diversity and improve the model’s generalization ability in referencing tasks. For object\nreference pointing, we incorporate object reference data sourced from RoboPoint [77], which includes 347K\nQA annotations across 288K images. To address the potential issue of excessive points hindering training\nconvergence, we randomly sample up to ten points per question. Additionally, the normalized coordinates are\nconverted into absolute values to better support RoboBrain 2.0 training.\nAffordance. The affordance dataset focuses on understanding object functionality and spatial vacant areas\nfor placement. For object affordance recognition, we utilize part-level annotations from PACO-LVIS [51],\ncovering 75 object categories and 200 part categories across 46K images. Bounding boxes and segmentation\nmasks are extracted for both whole objects and their functional parts. These annotations are transformed\ninto bounding box coordinates (x1, y1, x2, y2), serving as ground truth labels for affordance prediction tasks.\nQuestions are constructed using GPT-4o [22] to query object functionality and part usage, e.g., “Which\npart of a handbag can be grasped to carry it? ” for the handle of a handbag. For whole-object affordances,\nquestions avoid naming the object directly, such as “What device can be moved to control the cursor on a\nscreen? ” for a mouse (computer equipment). This automatic process results in 561K QA pairs. For spatial",
            "content": "7 affordance learning, we include region reference data from RoboPoint [77]. This dataset consists of 270K images with 320K QA pairs and 14 spatial relationship labels. Each annotation is converted into set of absolute coordinates [(x1, y1), (x2, y2), ...], and ground truth points are resampled to maximum of ten points per answer for optimization. This dataset enables RoboBrain 2.0 to reason about spatial affordances for object placement in real-world settings. Spatial Understanding. To enhance RoboBrain 2.0s 3D spatial reasoning, we present the Spatial Understanding Dataset, comprising 826K samples. This dataset emphasizes object-centric spatial attributes (e.g., position, orientation) and inter-object relations (e.g., distance, direction), covering both qualitative and quantitative aspects. It covers 31 distinct spatial concepts, substantially surpassing the 15 typically found in previous datasets. We partially adopt the RefSpatial [81] pipeline to construct 2D web image and 3D video datasets via automated templateand LLM-based generation: (1) 2D web images aim to provide core spatial concepts and depth perception across diverse indoor and outdoor scenes. To bridge scale and category gaps between these domains, we utilize the large-scale OpenImage [28] dataset. Since direct 3D reasoning from 2D images is challenging, we convert them into pseudo-3D scene graphs. Specifically, after filtering 1.7M images to 466K, we first use RAM [79] for object category prediction and GroundingDINO [34] for 2D boxes Detection. Then we enhance using Qwen2.5-VL [50] and heuristic method to generate hierarchical captions given the 2D bounding box, ranging from coarse (e.g., cup) to fine-grained (e.g., the third cup from the left). This enables unambiguous spatial referring in cluttered environments and captures both coarse and fine-grained spatial references. Next, we use UniDepth V2 [48] and WildeCamera [84] for depth and camera intrinsics to enable 3D point cloud reconstruction. Finally, combining this with object boxes from GroundingDINO [34] and masks from SAM 2.1 [52], each scene graph includes object labels, 2D boxes, instance masks, and object-level point clouds, yielding axis-aligned 3D boxes. Object captions serve as nodes, and spatial relations form the edges. QA pairs are generated via templates and LLMs (e.g., QwQ [66]), including object-location questions derived from the hierarchical captions. (2) 3D scene-based videos integrates multimodal 3D scene understanding data from five original datasets: MMScan [38], 3RScan [69], ScanQA [3], SQA3D [39], and SpaceR [46]. We conduct template-based question filtering through rigorous data processing to ensure task relevance, perform multi-stage quality screening (e.g., consistency checks, outlier removal), and standardize all formats into unified representation. This curation enables fine-grained environmental perception with enhanced reliability, supporting tasks ranging from object localization to complex spatial reasoning in 3D scenes. (3) 3D embodied videos focus on fine-grained spatial understanding in indoor environments. We leverage the CA-1M [29] dataset, filtering 2M frames to 100K high-quality ones. Compared to 2D, the availability of accurate 3D bounding boxes allows us to construct richer scene graphs with more diverse spatial relations, thereby generating more quantitative QA pairs (e.g., size, distances). Spatial Referring. After enhancing foundational 3D spatial understanding, we extend these capabilities to physical-world interactions by introducing the Spatial Referring Dataset [81], consisting of 802K samples. Unlike prior datasets in visual grounding or object pointing, which often deal with ambiguous or multiple referents, this dataset targets single unambiguous target, aligning with robotic applications such as precise pick-and-place that demand accurate object identification and localization. Following the RefSpatial [81] construction pipeline, for location data, we sample caption-point pairs from scene graphs built on 2D web images (OpenImage [28]) and 3D embodied videos (CA-1M [29]), using hierarchical captions. For placement data, we leverage fully annotated 3D datasets to generate top-down occupancy maps encoding object positions, orientations, and metric spatial relations (e.g., 10cm right of the chair), facilitating accurate spatial referring."
        },
        {
            "title": "3.3 Temporal Data",
            "content": "Ego-View Planning. We construct Ego-View Planning dataset by partially processing the EgoPlan-IT [9] dataset, which contains 50K automatically generated samples. For each selected task instance, we extract multiple frames from prior actions to represent task progress, and one frame to capture the current viewpoint. To enhance linguistic variety, we use multiple prompt templates that describe the task goal, video context, and current observation. Each question includes the correct next action along with up to three distractor actions randomly sampled from negative examples. This setup supports multimodal instruction tuning with diverse visual and textual input, aimed at improving egocentric task planning performance. ShareRobot Planning. The ShareRobot dataset [23] is large-scale, fine-grained resource for robotic manipu8 lation, offering multi-dimensional annotations tailored for task planning. Its planning component provides detailed low-level instructions aligned with individual video frames, effectively transforming high-level task descriptions into structured and executable sub-tasks. Each data instance includes precise planning annotations to support accurate and consistent task execution. The dataset comprises 1M QA pairs from 51K instances, spanning 102 diverse scenes across 12 robot embodiments and 107 atomic tasks filtered according to the Open-X-Embodiment taxonomy [47]. All planning data were meticulously annotated by human experts following the RoboVQA [55] format, enabling models to learn robust multi-step planning strategies grounded in diverse real-world scenarios. The scale, quality, and diversity of ShareRobot help improve the models ability to perform fine-grained reasoning and task decomposition in complex embodied environments. Agitbot Planning. The AgiBot Planning dataset is large-scale robotics task planning dataset built upon the AgiBot-World [6] dataset, comprising 9,148 QA pairs across 19 manipulation tasks with 109,378 firstperson perspective images. Each sample contains 4-17 consecutive frames documenting task progression with multimodal conversational format. AgiBot-Planning provides step-by-step planning instructions that transform high-level goals into executable sub-tasks. Each data point includes current objectives, historical steps, and required subsequent actions. The dataset covers diverse scenarios from household refrigerator operations to supermarket shopping tasks across different environments. The meticulously crafted annotations use standardized conversational formats, enabling models to learn from varied real-world contexts. Through continuous visual sequences and fine-grained action plans, AgiBot-Planning enhances RoboBrain 2.0s ability to perform long-horizon task planning and spatial reasoning in complex embodied scenarios. Multi-Robot Planning. The Multi-Robot Planning dataset is constructed by simulating collaborative task scenarios across three environmentshousehold, supermarket, and restaurantbased on RoboOS [61]. Each sample is generated using structured templates that specify detailed scene graph, robot specifications, and associated tool lists. For every scenario, we design high-level, long-horizon collaborative task goals that require coordination among multiple robots present in the scene, and generate corresponding workflow graphs that decompose the tasks into subtasks with detailed reasoning explanations. Based on these decompositions, we further generate agent-specific robotic tool plans that translate high-level task goals into precise low-level Observation-Action pairs for each subtask. Specifically, we define 1,659 types of multi-robot collaboration tasks across the three environments and produce 44,142 samples using DeepSeek-V3 [31]. Close-Loop Interaction. The Close-Loop Interaction dataset is designed to facilitate advanced embodied reasoning [80], featuring large-scale collection of synthesized Observation-Thought-Action (OTA) trajectories that combine first-person visual observations with structured thought tokens. It spans 120 diverse indoor environmentsincluding kitchens, bathrooms, bedrooms, and living roomscontaining over 4,000 interactive objects and receptacles. The dataset is constructed within the AI2Thor [25] simulator through rigorous multi-stage pipeline based on Embodied-Reasoner [78], which includes: (1) crafting task instructions from constrained templates to ensure scene-appropriate validity; (2) deriving key action sequences from an objectaffiliation graph encoding functional relationships; and (3) strategically incorporating search actions to emulate realistic exploration. To enrich the depth of reasoning, GPT-4o generates detailed thought processescovering situational analysis, spatial reasoning, self-reflection, task planning, and verificationwhich are seamlessly integrated between observations and actions, forming coherent reasoning chains that guide models through complex, long-horizon interactive tasks."
        },
        {
            "title": "4 Training Strategy",
            "content": "RoboBrain 2.0 achieves embodied capabilities (spatial understanding, temporal modeling, and chain-of-thought reasoning) through progressive three-phase training strategy, as shown in Table 1. Starting from robust vision-language foundation, we introduce escalating complexity in embodied supervision, enabling the model to evolve from static perception to dynamic reasoning and actionable planning in real-world environments."
        },
        {
            "title": "4.1 Stage 1: Foundational Spatiotemporal Learning",
            "content": "The first stage focuses on building general capabilities in spatial perception and temporal understanding. We fine-tune the model on large-scale multimodal datasets covering dense captioning, object localization, interleaved image-text documents, and basic video QA, along with referring expression comprehension. These 9 Table 1 Detailed configuration for each training stage of the RoboBrain 2.0. D o i r Dataset #Samples Stage-1 SFT Foundation 4.8M Stage-2 SFT Embodied 224K Stage-3 COT-SFT RFT (RLVR) Embodied (Phase 1) 195K Embodied (Phase 2) 45K Trainable Part #Tunable Parameters Full Model 8.29B or 33.45B Full Model 8.29B or 33.45B Full Model 8.29B or 33.45B Full Model 8.29B or 33.45B Per-device Batch Size Gradient Accumulation LR: {ψViT , ϕLLM } Epoch Optimizer Deepspeed Weight Decay Warmup Ratio LR Schedule Max Seq. Length Max Compl. Length Num. of Compl. GPU Nums 2 2 1104 1 AdamW 0.1 0.01 Cosine 16384 16/64 2 2 1 105 1 AdamW 0.1 0.01 Cosine 16384 16/64 8 4 2 1 105 1 AdamW Zero3 0.1 0.03 Cosine 32768 4 8 1 2 1 106 3 AdamW Zero3 0.0 0.00 Cosine 32768 1024 8 4 8 datasets span common physical scenes and interaction patterns, helping the model develop fundamental grounding for objects, spatial relations, and motion events. This stage lays the groundwork for understanding egocentric video streams and spatially anchored instructions."
        },
        {
            "title": "4.2 Stage 2: Embodied Spatiotemporal Enhancement",
            "content": "To better align the model with embodied tasks, we introduce carefully curated collection of high-resolution, multi-view, and egocentric video datasets, along with instruction-augmented navigation and interaction data. Tasks include viewpoint-aware referring expressions, 3D affordance estimation, and object-centric scene graph construction. This stage of training emphasizes the modeling of long-horizon temporal dependencies, enabling the model to reason over extended sequences of actions and observations. Additionally, it incorporates multi-agent coordination scenarios, where the model learns to interpret and predict the behaviors of other agents in shared environments. To support these capabilities, we employ extended sequence lengths and multi-camera input encoding, allowing the model to process and fuse visual information from multiple viewpoints simultaneously. Through this training stage, the model can integrate historical visual cues with current instructions, fostering more coherent long-horizon planning, robust scene understanding, and adaptive decision-making in dynamic, interactive settings."
        },
        {
            "title": "4.3 Stage 3: Chain-of-Thought Reasoning in Embodied Contexts",
            "content": "In the third stage, we augment the models high-level reasoning capabilities using Chain-of-Thought (CoT) methodology, following the two-phase framework of Reason-RFT [62]: CoT-based Supervised Fine-Tuning (CoT-SFT) and Reinforcement Fine-Tuning (RFT). We leverage multi-turn reasoning examples from both synthetic and real-world embodied scenarios, encompassing long-horizon task planning, manipulation prediction, closed-loop interaction, spatiotemporal understanding, and multi-robot collaboration, sourced from Section 3. Specifically, (1) CoT-SFT Phase: We annotate 10% of the constructed training data with CoT rationales annotated by GPT-4o [22] with custom prompts, then perform supervised fine-tuning for initial model from Stage 2. (2) RFT Phase: An additional 10% of the constructed training data is sampled to collect models responses, with incorrect answers curated into reformatted training set (e.g., multiple-choice questions or LaTeX/numerical answers). Optimization employs Group Relative Policy Optimization (GRPO) [17], guided by composite reward function evaluating both answer accuracy and format correctness."
        },
        {
            "title": "5.1 Large-Scale Training Infrastructure",
            "content": "To improve the efficiency and stability of multimodal model training, we have developed and integrated series of key optimization techniques, including hybrid parallelism strategies, memory pre-allocation, distributed data loading, kernel fusion, and fine-grained compute-communication overlapping. These optimizations significantly enhance both resource utilization and training throughput. For data preprocessing, we build upon the MegatronEnergon framework [30] and incorporate custom optimization strategies. Our system supports dynamic mixing of multiple datasets containing diverse modalities, including plain text, single image, multiple images, and video, while also allowing for strict sample order preservation within each dataset. custom WebDataset-based format [1] enables compatibility with various data modalities and greatly reduces preprocessing time while improving flexibility and scalability in data handling. 5.1.1 Multi-Dimensional Hybrid Parallelism Multimodal models differ significantly from conventional LLMs in both architecture and data characteristics [33]. On the architectural side, multimodal models are inherently heterogeneous: the vision module (e.g., ViT with Adaptor) is typically small-scale encoder-only component, while the language module is much larger decoder-only transformer. On the data side, training samples include plain text, single images, multi-image sequences, and videos. The number of image tokens, text tokens, and the length of the fused token sequence can vary dramatically between samples. These heterogeneities pose substantial challenges to distributed training frameworks. To address this, we implemented several targeted strategies in our custom framework, FlagScale [12]: Non-uniform Pipeline Parallelism [43]: Since the ViT module appears early in the model and has relatively low computational cost, we reduce the number of LLM layers in the first pipeline stage, thereby improving training throughput without increasing memory overhead. Separate Recompute Strategy: During the annealing stage, the vision input may contain up to 20,00030,000 tokens, frequently causing an Out-of-Memory (OOM) error in the ViT module. To mitigate this, we enable recompute [8, 26] only in the ViT module to reduce memory usage of intermediate activations, while disabling recompute in the LLM module to preserve computational efficiency. 5.1.2 Pre-Allocate Memory In the supervised fine-tuning training process of RoboBrain 2.0, input lengths vary significantly across samples. PyTorchs default caching memory allocator [49] can lead to memory fragmentation under such dynamic input conditions, frequently resulting in OOM errors. common but inefficient workaround is to call torch.cuda.empty_cache() before every forward pass, which severely degrades performance.Instead, we take more efficient approach by analyzing PyTorchs memory allocation mechanism. Fragmentation often results from the lack of sufficiently large and contiguous cached memory block for new tensors, prompting new allocations and worsening fragmentation. To address this, we introduce memory pre-allocation strategy: we compute the maximum sequence length across the entire dataset before training, and pad all samples to this maximum length in the first step. This ensures that tensors can reuse pre-allocated memory blocks, reducing fragmentation and maintaining throughput. 5.1.3 Data Pre-Processing We adopt native Megatron-Energon [30] for unified data loading, eliminating the need for external training frameworks. Additionally, we optimized the preprocessing pipeline to reduce time consumption by up to 90%. We evaluated and compared two preprocessing strategies: Preprocessing Both JSON and Images. Using the default Megatron-Energon data pipeline, both JSON metadata and images are compressed into binary files for WebDataset. However, this approach suffers from two major issues: (1) Low efficiency: Preprocessing 320,000 samples can take over 2 hours. (2) Inconsistent image readers: Megatron-Energon uses cv2, while models such as RoboBrain 2.0 use PIL, introducing subtle differences that may affect training performance. 11 Preprocessing JSON Only (Recommended). In our optimized pipeline, only JSON files are preprocessed, and images are kept in their original form. Image preprocessing is deferred to the TaskEncoder module using the same preprocessor as Qwen2.5-VL. (1) High efficiency: Preprocessing 320,000 samples takes less than 10 minutes. (2) Alignment with model input: Ensures image handling is fully aligned between preprocessing and training, eliminating inconsistency and improving model performance. 5.1.4 Distributed Data Loading To minimize the I/O burden on compute nodes, we reduce redundant data loading in large-scale distributed training. Unlike single-node setups, GPUs in distributed training systems play different roles depending on the chosen parallel strategy. Data loading typically occurs along the data parallel (DP) dimension, where each DP rank handles unique data shard. However, in multi-dimensional hybrid parallelism (e.g., DP-PP-TP), only subset of GPU processes actually need to load data: (1) In each Pipeline Parallel (PP) [42] group, only the first and last stages need to perform data loading. (2) Within Tensor Parallel (TP) [58] groups, only one GPU per group is required to load data, with others receiving data via broadcast. This design significantly reduces redundant I/O operations and improves overall data throughput. 5.1.5 Fault Tolerance To handle both hardware and software failures during training, we co-designed fault-tolerant mechanisms between our FlagScale [12] training framework and the system platform. Common errors, such as LostCard, KubeNodeNotReady, are automatically detected and trigger automatic job recovery and restart, ensuring minimal disruption. Furthermore, our custom DataLoader module based on Megatron-Energon supports full data state recovery, allowing seamless resumption from the most recent checkpoint with complete consistency in data loading and sample shuffling states."
        },
        {
            "title": "5.2 Reinforcement Fine-Tuning Infrastructure",
            "content": "We employ Reinforcement Learning with Verifiable Rewards (RLVR) to enhance RoboBrain 2.0 using VeRL [68], an open-source RL framework specifically designed for post-training LLMs and VLMs. Based on the HybridFlow architecture [56], VeRL features hybrid-controller model that integrates both global controller for inter-RL-role dataflow coordination and distributed controllers for intra-RL-role parallel processing. This architecture enables efficient execution of complex post-training workflows while ensuring scalability. VeRLs support for multiple RL algorithms (e.g., GRPO) and seamless LLM integration makes it particularly suitable for RoboBrain 2.0s reinforcement fine-tuning (RFT) requirements. The framework enables high-performance model tuning with minimal overhead through its optimized dataflow management and parallel processing capabilities. Its efficient handling of large-scale training tasks and rigorous reward verification establishes VeRL as an ideal platform for advancing RoboBrain 2.0s capabilities via RLVR."
        },
        {
            "title": "5.3 Inference Infrastructure",
            "content": "To improve the efficiency of model inference, we adopt FlagScale [12], also multi-backend inference framework, which can automatically search for the optimal inference engine and configuration parameters based on the performance characteristics of different models on heterogeneous hardware accelerators, thereby effectively reducing inference latency. Given the high sensitivity of embodied AI models to accuracy, we further introduce mixed-bit quantization strategy [40, 70]. This strategy enhances inference efficiency and resource utilization while maintaining model performance. Specifically, the vision encoder retains full-precision floating-point computation to ensure the accuracy of key feature extraction. In contrast, during the language module, weights are quantized to 8-bit integers, while activations are preserved in 16-bit floating-point format. This mixed-precision approach significantly reduces computational overhead and memory usage with negligible impact on model accuracy. Moreover, the quantization process is minimally invasive to existing inference pipelines and can be flexibly integrated into current systems. In end-to-end embodied tasks, weight-only quantization alone achieves approximately 30% reduction in inference latency, demonstrating the effectiveness and practicality of the proposed method in real-world deployment scenarios."
        },
        {
            "title": "6 Evaluation Results",
            "content": "We conducted comprehensive evaluation of RoboBrain-2.0, focusing on its performance across spatial and temporal reasoning capabilities on embodiment. To ensure consistency and rigor in evaluation, we adopted FlagEvalMM [20], our flexible framework for systematic multimodal model assessment. Evaluations on spatial reasoning benchmarks (e.g., CV-bench [67], Blink [15], Where2Place [77], ShareRobot-Bench [23]), presented in Section 6.1, underscore the models strengths in embodied spatial reasoning. An in-depth analysis of multi-robot collaboration [61] and long-horizon planning (e.g., EgoPlan2 [9], RoboBench) capabilities is provided in Section 6.2, highlighting the models advancements in temporal reasoning tasks. Qualitative examples and prompt details are provided in Appendix and Appendix B, respectively."
        },
        {
            "title": "6.1 Spatial Reasoning Capability",
            "content": "RoboBrain-32B-2.0 and RoboBrain-7B-2.0 demonstrate exceptional performance across nine spatial reasoning benchmarks: BLINK, CV-Bench, EmbSpatial, RoboSpatial, and RefSpatial-Bench  (Table 2)  , as well as SAT, VSI-Bench, Where2Place, and ShareRobot-Bench  (Table 3)  . Below is detailed analysis highlighting their state-of-the-art (SOTA) achievements and near-SOTA competitive results. Table 2 Performance across five spatial reasoning benchmarks. The best results among different models are highlighted in bold, while the second-best results are underlined. Models / Metrics General Baselines Gemini-2.5-Pro-preview-05-06 [16] Gemini-2.5-Flash-preview-04-17 [16] GPT-o4-mini-2025-05-16 [45] GPT-4o-2024-11-20 [22] Claude-Sonnet-4-2025-05-14 [2] Qwen2.5-VL-32B-Instruct [50] Qwen2.5-VL-72B-Instruct [50] Embodied Baselines Cosmos-Reason1-7B [4] VeBrain-8B [36] Magma-8B [74] RoboBrain-7B-1.0 [23] RoboBrain-7B-2.0 RoboBrain-32B-2.0 BLINK CV-Bench EmbSpatial RoboSpatial RefSpatial-Bench Dep. Spa. All All All All Loc. Pla. All 79.03 77.42 79.03 72.58 75.81 77.42 74.19 63.71 78.23 65.32 75.81 84.68 79.84 84.62 79.02 88.11 83.22 80.42 85.31 78. 73.43 81.12 66.43 78.32 83.22 87.41 81.83 78.22 83.57 77.90 78.12 81.37 76.26 68.57 79.68 65.88 77.07 83.95 83.63 84.59 84.03 85.21 78.63 78.43 81.59 82.68 74.71 78.57 60.98 76. 85.75 83.92 78.74 74.75 78.29 71.92 64.26 74.45 73.30 65.22 70.52 64.59 68.13 76.32 78.57 59.87 54.10 51.25 44.42 51.26 52.16 48.33 38.81 42.48 33.71 51.53 54.23 72. 44.58 37.50 15.00 8.00 5.00 16.83 23.50 9.84 0.03 1.00 14.43 36.00 31.73 23.00 19.58 9.55 10.37 10.60 15.83 1.04 0.57 8.00 5.41 29.00 38.16 30.25 17.29 8.78 7.69 13.72 19.67 5.44 0.30 4.50 9.92 32. 54.00 54.00 54.00 BLINK. In the BLINK [15] benchmark, models are evaluated on depth perception (Dep.) and spatial relation understanding (Spa.). RoboBrain-7B-2.0 achieves SOTA average score of 83.95 (Dep.: 84.68, Spa.: 83.22), outperforming all general baselines, including GPT-o4-mini-2025-05-16 (83.57), Gemini2.5-Pro-preview-05-06 (81.83), Qwen2.5-VL-32B-Instruct (81.37), Claude-Sonnet-4-2025-05-14 (78.12), GPT-4o-2024-11-20 (77.90), and Qwen2.5-VL-72B-Instruct (76.26), as well as embodied baselines like VeBrain-8B (79.68) and Cosmos-Reason1-7B (68.57). RoboBrain-32B-2.0 follows closely with an average of 83.63 (Dep.: 79.84, Spa.: 87.41), surpassing all general and embodied baselines except RoboBrain-7B-2.0, demonstrating strong spatial reasoning capabilities. CV-Bench. The CV-Bench [67] benchmark assesses models accuracy in 2D/3D spatial understanding and visual processing. RoboBrain-7B-2.0 secures SOTA accuracy of 85.75, slightly ahead of RoboBrain-32B-2.0 (83.92), both outperforming all general baselines, including GPT-o4-mini-2025-05-16 (85.21), Gemini-2.5Pro-preview-05-06 (84.59), Qwen2.5-VL-72B-Instruct (82.68), Qwen2.5-VL-32B-Instruct (81.59), GPT-4o2024-11-20 (78.63), and Claude-Sonnet-4-2025-05-14 (78.43), as well as embodied baselines like VeBrain-8B (78.57) and Cosmos-Reason1-7B (74.71). EmbSpatial. The EmbSpatial [14] benchmark evaluates models on embodied spatial tasks. RoboBrain-32B2.0 achieves near SOTA accuracy of 78.57, slightly less than Gemini-2.5-Pro-preview-05-06 (78.74) and surpassing all other general baselines, including GPT-o4-mini-2025-05-16 (78.29), Qwen2.5-VL-32B-Instruct 13 Table 3 Performance across four spatial reasoning benchmarks. The best results among different models are highlighted in bold, while the second-best results are underlined. Models / Metrics General Baselines Gemini-2.5-Pro-preview-05-06 [16] Gemini-2.5-Flash-preview-04-17 [16] GPT-o4-mini-2025-05-16 [45] GPT-4o-2024-11-20 [22] Claude-Sonnet-4-2025-05-14 [2] Qwen2.5-VL-32B-Instruct [50] Qwen2.5-VL-72B-Instruct [50] Embodied Baselines Cosmos-Reason1-7B [4] VeBrain-8B [36] Magma-8B [74] RoboBrain-7B-1.0 [23] RoboBrain-7B-2.0 RoboBrain-32B-2.0 SAT All 79.33 74.00 82.00 66.67 75.33 80.00 58.67 60.67 58.00 71.33 59.33 75.33 86.67 VSI-Bench Where2Place ShareRobot-Bench All Seen Unseen All Afford. Traj. (DFD ) 47.81 48.83 41.96 43.60 47.02 36.07 35.51 25.64 26.30 12.65 31.12 36.10 42.69 42.92 31.54 26.63 20.28 21.56 18.22 35.74 5.07 12.27 9.93 54.58 64.33 73. 41.13 21.73 26.49 20.71 35.11 32.55 49.65 6.53 9.17 13.14 49.45 61.88 72.74 42.38 28.60 26.59 20.41 25.63 22.52 39.92 5.51 11.34 10.89 53.04 63.59 73. 10.26 2.50 8.27 6.00 8.00 11.97 23.80 9.98 3.66 10.20 28.05 35.28 0.7666 0.9087 0.5726 0.6850 0.7591 0.9222 0.5034 0.8524 1.1659 0.7478 0.6248 0.5512 0. (74.45), Qwen2.5-VL-72B-Instruct (73.30), GPT-4o-2024-11-20 (71.92), and Claude-Sonnet-4-2025-05-14 (64.26). RoboBrain-7B-2.0 follows with competitive score of 76.32, outperforming most general baselines and all embodied baselines, indicating strong embodied spatial reasoning. RoboSpatial. The RoboSpatial [59] benchmark measures spatial reasoning in robot environments, such as object localization and manipulation. RoboBrain-32B-2.0 achieves clear SOTA score of 72.43, substantially ahead of general baselines like Gemini-2.5-Pro-preview-05-06 (59.87), Qwen2.5-VL-72B-Instruct (48.33), GPT-o4-mini-2025-05-16 (51.25), and Claude-Sonnet-4-2025-05-14 (51.26). RoboBrain-7B-2.0 scores 54.23, outperforming all general baselines except RoboBrain-32B-2.0, demonstrating significant improvements in spatial reasoning for robotic tasks. RefSpatial-Bench. The RefSpatial-Bench [81] benchmark evaluates models on spatial referring expressions, requiring precise point predictions under spatial constraints, with metrics for Location (Loc.) and Placement (Pla.) accuracy. RoboBrain-32B-2.0 achieves SOTA scores of 54.00 (Loc.) and 54.00 (Pla.), significantly outperforming all general baselines, including Gemini-2.5-Pro-preview-05-06 (44.58, 31.73), Qwen2.5-VL72B-Instruct (23.50, 15.83), Qwen2.5-VL-32B-Instruct (16.83, 10.60), GPT-o4-mini-2025-05-16 (15.00, 19.58), GPT-4o-2024-11-20 (8.00, 9.55), and Claude-Sonnet-4-2025-05-14 (5.00, 10.37). RoboBrain-7B-2.0 scores 36.00 (Loc.) and 29.00 (Pla.), outperforming all general baselines except RoboBrain-32B-2.0, showing competitive precision in complex spatial referring tasks. SAT. The SAT [53] benchmark measures general spatial reasoning abilities across various scenes and tasks. RoboBrain-32B-2.0 achieves clear SOTA score of 86.67, significantly outperforming all general baselines, including GPT-o4-mini-2025-05-16 (82.00), Gemini-2.5-Pro-preview-05-06 (79.33), Qwen2.5VL-72B-Instruct (58.67), and Claude-Sonnet-4-2025-05-14 (75.33). RoboBrain-7B-2.0 achieves 75.33, surpassing most general and embodied baselines, showcasing its strong spatial reasoning capability. VSI-Bench. The VSI-Bench [75] evaluates visual-spatial integration capabilities. Gemini-2.5-Flash-preview04-17 achieves the best performance with 48.83. RoboBrain-32B-2.0 achieves 42.69, outperforming most general and embodied baselines, including GPT-o4-mini-2025-05-16 (41.96) and Qwen2.5-VL-72B-Instruct (35.51). RoboBrain-7B-2.0 reaches 36.10, indicating solid visual-spatial integration skills. Where2Place. The Where2Place [77] benchmark measures models ability to predict object placements in both seen and unseen scenarios under spatial constraints. RoboBrain-32B-2.0 achieves SOTA average of 73.59 (Seen: 73.95, Unseen: 72.74), substantially surpassing all general and embodied baselines, including Qwen2.5-VL-72B-Instruct (39.92), Gemini-2.5-Pro-preview-05-06 (42.38), Claude-Sonnet-4-2025-05-14 (25.63), and VeBrain-8B (11.34). RoboBrain-7B-2.0 also performs strongly with an average of 63.59 (Seen: Upon inspection, we found that the test set included several incorrect cases, which were manually screened and excluded. 14 64.33, Unseen: 61.88), outperforming all baselines except RoboBrain-32B-2.0. ShareRobot-Bench-Affordance. The ShareRobot Affordance task [23] evaluates models on object functionality and interaction understanding. RoboBrain-32B-2.0 secures SOTA performance with an accuracy of 35.28, ahead of all general baselines, including Qwen2.5-VL-72B-Instruct (23.80), Qwen2.5-VL-32B-Instruct (11.97), GPT-4o-2024-11-20 (6.00), and Claude-Sonnet-4-2025-05-14 (8.00). RoboBrain-7B-2.0 achieves 28.05, outperforming all general and embodied baselines except RoboBrain-32B-2.0. ShareRobot-Bench-Trajectory. The ShareRobot Trajectory task [23] assesses navigation and motion prediction, using Dynamic Fréchet Distance (DFD), where lower values denote better performance. RoboBrain-32B-2.0 achieves SOTA DFD of 0.2368, outperforming all general and embodied baselines, including Qwen2.5-VL-72B-Instruct (0.5034), GPT-o4-mini-2025-05-16 (0.5726), and Gemini-2.5-Propreview-05-06 (0.7666). RoboBrain-7B-2.0 follows with competitive DFD of 0.5512, demonstrating strong path-planning capabilities."
        },
        {
            "title": "6.2 Temporal Reasoning Capability",
            "content": "RoboBrain-32B-2.0 and RoboBrain-7B-2.0 exhibit outstanding performance across three critical measures of temporal reasoning benchmarks: Multi-Robot Planning, Ego-Plan2, and RoboBench, as shown in Table 4. Below is detailed analysis highlighting their state-of-the-art (SOTA) achievements and near-SOTA results. Table 4 Performance across three temporal reasoning benchmarks. The best results among different models are highlighted in bold, while the second-best results are underlined. Models / Metrics Multi-Robot Planning Ego-Plan2 RoboBench Super. Rest. House. All Daily. Hobbies. Rec. Work. All Plan. General Baselines Gemini-2.5-Pro-preview-05-06 [16] Gemini-2.5-Flash-preview-04-17 [16] GPT-o4-mini-2025-05-16 [45] GPT-4o-2024-11-20 [22] Claude-Sonnet-4-2025-05-14 [2] Qwen2.5-VL-32B-Instruct [50] Qwen2.5-VL-72B-Instruct [50] Embodied Baselines Cosmos-Reason1-7B [4] VeBrain-8B [36] Magma-8B [74] RoboBrain-7B-1.0 [23] RoboBrain-7B-2.0 RoboBrain-32B-2.0 63.51 59.44 63.32 77.89 73.08 67.84 77.39 35.17 41.70 4.52 83.92 84. 54.77 55.78 55.28 67.34 61.81 61.81 68.34 25.62 35.67 7.04 77.39 72.36 78.39 76.88 78.89 79.40 80.40 75.38 79.40 40.70 39.69 5.03 84.42 85. 65.39 63.86 65.50 74.50 71.30 68.00 74.67 33.66 38.83 5.50 81.50 80.33 44.19 38.72 47.61 47.38 43.51 64.46 60.36 30.75 31.79 4.56 39.41 64. 43.05 35.59 35.93 40.00 41.02 51.53 48.14 27.12 35.31 3.39 32.20 53.22 46.45 43.72 42.62 44.81 42.62 57.92 63.39 31.69 31.19 6.56 33.88 57. 39.60 33.42 37.13 35.64 38.87 50.00 46.29 20.30 34.43 2.97 26.98 52.48 42.85 37.09 41.11 41.79 41.26 56.25 53.75 26.87 27.30 4.09 33.23 57. 63.49 69.33 70.01 68.60 70.21 45.92 66.94 53.17 46.77 38.93 72.16 68.33 Multi-Robot Planning. In the Multi-Robot Planning task [61], models are evaluated on their ability to coordinate multiple robots across different scenarios: Super (Supermarket), Rest (Restaurant), and House (Household). RoboBrain-32B-2.0 achieves SOTA average score of 80.33 (Super: 84.42, Rest: 72.36, House: 85.43), significantly outperforming all general baselines, including GPT-4o-2024-11-20 (74.50), Qwen2.5-VL-72B-Instruct (74.67), Claude-Sonnet-4-2025-05-14 (71.30), Gemini-2.5-Pro-preview-05-06 (65.39), and Qwen2.5-VL-32B-Instruct (68.00). It also surpasses the embodied baseline RoboBrain-7B-2.0 (81.50). RoboBrain-7B-2.0 follows closely with an average of 81.50 (Super: 83.92, Rest: 77.39, House: 84.42), outperforming all general baselines and matching the performance of RoboBrain-7B-1.5-OS in Rest and House scenarios. Ego-Plan2. The Ego-Plan2 [9] benchmark assesses models capability to plan daily activities across four categories: Daily (Daily Routines), Hobbies, Rec (Recreation), and Work. RoboBrain-32B-2.0 secures SOTA average score of 57.23 (Daily: 64.01, Hobbies: 53.22, Rec: 57.92, Work: 52.48), significantly outperforming all general and embodied baselines, including Qwen2.5-VL-32B-Instruct (56.25), Qwen2.5VL-72B-Instruct (53.75), Gemini-2.5-Pro-preview-05-06 (42.85), GPT-4o-2024-11-20 (41.79), ClaudeSonnet-4-2025-05-14 (41.26), GPT-o4-mini-2025-05-16 (41.11), VeBrain-8B (27.30), and Cosmos-Reason17B (26.87). In contrast, RoboBrain-7B-2.0 achieves an average of 33.23 (Daily: 39.41, Hobbies: 32.20, Rec: 33.88, Work: 26.98), which is lower than general baselines like Qwen2.5-VL-32B-Instruct and Qwen2.5-VL-72B-Instruct but surpasses embodied baselines such as VeBrain-8B and Cosmos-Reason1-7B. RoboBench. The RoboBench Benchmark (Planning part) evaluates models ability to plan robotic mobile manipulation tasks according to their pre-defined skills across three categories: cross-embodiment, crossobject, and cross-view. On this benchmark, RoboBrain-7B-2.0 achieves state-of-the-art (SOTA) score of 72.16, surpassing all general and embodied baselines, including Claude-Sonnet-4-2025-05-14 (70.21), GPTo4-mini-2025-05-16 (70.01). The performance of RoboBrain-32B-2.0, with score of 68.33, outperforming several general baselines like GPT-4o-2024-11-20 (68.60) and Qwen2.5-VL-72B-Instruct (66.94), as well as other embodied baselines such as Cosmos-Reason1-7B (53.17) and VeBrain-8B (46.77)."
        },
        {
            "title": "7 Conclusion and Future Works",
            "content": "In this report, we introduced RoboBrain 2.0, our latest generation of embodied vision-language foundation models, developed to support unified perception, reasoning, and planning in complex physical environments. Built on modular architecture with dedicated vision encoder and decoder-only language model, RoboBrain 2.0 enables high-resolution image and video comprehension, as well as spatial and temporal reasoning. Through progressive three-stage training strategyencompassing foundational spatiotemporal learning, embodied enhancement, and chain-of-thought reasoningthe model demonstrates strong generalization across wide variety of challenging embodied tasks. Despite its compact size, RoboBrain 2.0 achieves state-of-the-art results on most of public embodied spatial and temporal reasoning benchmarks, outperforming both open-source and proprietary models in spatial understanding, closed-loop interaction, and long-horizon planning. Its capabilities span broad spectrum of embodied scenarios, including affordance prediction, spatial referring, trajectory forecasting, multi-agent coordination, and scene graph construction and updating. We regard RoboBrain 2.0 as solid foundation toward developing more general embodied AI, emphasizing the importance of tightly integrated perception, reasoning, and planning. Moving forward, we plan to expand RoboBrain 2.0 along two key directions: Embodied VLM-powered VLA: We aim to integrate cutting-edge embodied VLMs into the Vision-LanguageAction (VLA) framework. By harnessing the powerful spatiotemporal perception and high-level reasoning capabilities of VLMs, this direction seeks to substantially enhance the generality and robustness of action generation. The resulting system will support more nuanced understanding and precise execution of complex, open-ended instructions in real-world scenarios. System-Level Integration: To improve RoboBrain 2.0s practical utility, we will pursue tight integration with advanced robotics platforms and operating systems. This will enable serverless deployment, adaptation-free skill registration, and low-latency real-time control. In parallel, we envision building collaborative embodied AI ecosysteman intelligence app storethat supports plug-and-play components for perception, reasoning, and control in real-world robotic systems. We release RoboBrain 2.0 at https://superrobobrain.github.io, including model checkpoints, training recipes, and evaluation tools, to support broader research and downstream applications in embodied AI. We hope this work bridges the gap between vision-language intelligence and real-world physical interaction."
        },
        {
            "title": "References",
            "content": "[1] Thomas Breuel Alex Aizman, Gavin Maltby. Webdataset: High-performance data loading for deep learning, 2020. URL https://webdataset.github.io/webdataset/. [2] Anthropic. Claude sonnet 4. 2025. [3] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In CVPR, pages 1912919139, 2022. [4] Alisson Azzolini, Hannah Brandon, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, Francesco Ferroni, Rama Govindaraju, et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [7] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024. [8] Tianqi et al. Chen. Gradient checkpointing in pytorch, 2018. URL https://pytorch.org/docs/stable/ checkpoint.html. [9] Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplan-bench: Benchmarking multimodal large language models for human-level planning, 2024. URL https://arxiv.org/abs/2312.06722. [10] Wei-Lin Chiang, Zhuohan Xu, Hao Zhao, Shuyang Zhuang, Zi Lin Li, Yonghao Lin, Isaac Safo, Eric Singh, Rishi Taori, Noah Shinn, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. URL https://arxiv.org/abs/2306.05685. [11] Zhendong Chu, Shen Wang, Jian Xie, Tinghui Zhu, Yibo Yan, Jinheng Ye, Aoxiao Zhong, Xuming Hu, Jing Liang, Philip Yu, et al. Llm agents for education: Advances and applications. arXiv preprint arXiv:2503.11733, 2025. [12] FlagScale Contributors. Flagscale: unified meta-framework enabling adaptive heterogeneous computing for the llm ecosystem. https://github.com/FlagOpen/FlagScale, 2024. Accessed: 2025-06-26. [13] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [14] Mengfei Du, Binhao Wu, Zejun Li, Xuan-Jing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. In ACL, 2024. [15] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In ECCV, 2024. [16] Google. Gemini 2.5 pro preview: even better coding performance. https://developers.googleblog.com/en/ gemini-2-5-pro-io-improved-coding-performance/, 2025. Accessed: 2025-05-06. [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [18] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [19] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. [20] Zheqi He, Yesheng Liu, Jing shu Zheng, Xuejing Li, Jin-Ge Yao, Bowen Qin, Richeng Xuan, and Xi Yang. Flagevalmm: flexible framework for comprehensive multimodal model evaluation. 2025. URL https://arxiv. org/abs/2506.09081. [21] Yining Huang, Keke Tang, Meilian Chen, and Boyuan Wang. comprehensive survey on evaluating large language model applications in the medical industry. arXiv preprint arXiv:2404.15777, 2024. [22] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [23] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17241734, 2025. [24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017, 2023. [25] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. [26] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models, 2022. URL https://arxiv. org/abs/2205.05198. [27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. [28] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. [29] Justin Lazarow, David Griffiths, Gefen Kohavi, Francisco Crespo, and Afshin Dehghan. Cubify anything: Scaling indoor 3d object detection. arXiv preprint arXiv:2412.04458, 2024. [30] Xuechen Li, Yifan Mai, Percy Liang, and Matei Zaharia. Energon: Scaling megatron-lm training with data and expert parallelism, 2023. URL https://github.com/HazyResearch/megatron-energon. [31] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [32] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. [33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [34] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. [35] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [36] Gen Luo, Ganlin Yang, Ziyang Gong, Guanzhou Chen, Haonan Duan, Erfei Cui, Ronglei Tong, Zhi Hou, Tianyi Zhang, Zhe Chen, et al. Visual embodied brain: Let multimodal large language models see, think, and control in spaces. arXiv preprint arXiv:2506.00123, 2025. [37] Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. [38] Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, and Jiangmiao Pang. Mmscan: multi-modal 3d scene dataset with hierarchical grounded language annotations. arXiv preprint arXiv:2406.09401, 2024. [39] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. In ICLR, 2023. URL https://openreview.net/forum?id=IDJx97BC38. [40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training, 2018. URL https://arxiv.org/abs/1710.03740. [41] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947952. IEEE, 2019. [42] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the international conference for high performance computing, networking, storage and analysis, pages 115, 2021. [43] NVIDIA. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2021. URL https://github.com/NVIDIA/Megatron-LM. [44] OpenAI. Gpt-4 technical report, 2023. URL https://doi.org/10.48550/arXiv.2303.08774. [45] OpenAI. Gpt-4v(ision) system card. https://openai.com/index/introducing-o3-and-o4-mini/, 2025. Accessed: 2025-04-16. [46] Kun Ouyang. Spatial-r1: Enhancing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025. [47] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. [48] Luigi Piccinelli, Christos Sakaridis, Yung-Hsu Yang, Mattia Segu, Siyuan Li, Wim Abbeloos, and Luc Van Gool. Unidepthv2: Universal monocular metric depth estimation made simpler. arXiv, 2025. [49] PyTorch Developers. Cuda memory management, 2023. URL https://pytorch.org/docs/stable/notes/cuda. html#cuda-memory-management. [50] Qwen Team. Qwen2.5-vl: Multimodal llms from alibaba, 2025. URL https://github.com/QwenLM/Qwen2.5-VL. [51] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71417151, 2023. [52] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. ICLR, 2025. [53] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. [54] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European conference on computer vision, pages 146162. Springer, 2022. [55] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for robotics. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 645652. IEEE, 2024. [56] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 12791297. ACM, March 2025. doi: 10.1145/3689031.3696075. URL http://dx.doi.org/10.1145/3689031.3696075. [57] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. [58] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. arXiv preprint Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv:1909.08053, 2019. [59] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1576815780, 2025. [60] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. Multimodalqa: Complex question answering over text, tables and images. arXiv preprint arXiv:2104.06039, 2021. [61] Huajie Tan, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Yaoxu Lyu, Mingyu Cao, Zhongyuan Wang, and Shanghang Zhang. Roboos: hierarchical embodied framework for cross-embodiment and multi-agent collaboration. arXiv preprint arXiv:2505.03673, 2025. [62] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [63] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. IEEE Transactions on Circuits and Systems for Video Technology, 2025. [64] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. [65] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [66] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm. github.io/blog/qwq-32b/. [67] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. NeurIPS, 2024. [68] volcengine. verl: Volcano engine reinforcement learning for llms, 2024. URL https://github.com/volcengine/ verl. [69] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Nießner. Rio: 3d object instance re-localization in changing indoor environments. In ICCV, pages 76587667, 2019. [70] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quantization with mixed precision, 2019. URL https://arxiv.org/abs/1811.08886. [71] Shuai Wang, Weiwen Liu, Jingxuan Chen, Yuqi Zhou, Weinan Gan, Xingshan Zeng, Yuhan Che, Shuai Yu, Xinlong Hao, Kun Shao, et al. Gui agents with foundation models: comprehensive survey. arXiv preprint arXiv:2411.04890, 2024. [72] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In European Conference on Computer Vision, pages 396416. Springer, 2024. [73] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 20 [74] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1420314214, 2025. [75] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. [76] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. [77] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics, 2024. URL https://arxiv.org/abs/2406.10721. [78] Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, et al. Embodied-reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks. arXiv preprint arXiv:2503.21696, 2025. [79] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong image tagging model. In CVPR, 2024. [80] Enshen Zhou, Qi Su, Cheng Chi, Zhizheng Zhang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, and He Wang. Code-as-monitor: Constraint-aware visual programming for reactive and proactive robotic failure detection. arXiv preprint arXiv:2412.04455, 2024. [81] Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025. [82] Hao Zhou, Chengming Hu, Ye Yuan, Yufei Cui, Yili Jin, Can Chen, Haolun Wu, Dun Yuan, Li Jiang, Di Wu, et al. Large language model (llm) for telecommunications: comprehensive survey on principles, key techniques, and opportunities. IEEE Communications Surveys & Tutorials, 2024. [83] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [84] Shengjie Zhu, Abhinav Kumar, Masa Hu, and Xiaoming Liu. Tame wild camera: in-the-wild monocular camera calibration. NIPS, 2023."
        },
        {
            "title": "8 Contributions and Author List",
            "content": "Core Contributors Model Training Mingyu Cao Huajie Tan Yuheng Ji Minglan Lin Zhiyu Li Zhou Cao Pengwei Wang Data & Evaluation Enshen Zhou Yi Han Yingbo Tang Xiangqi Xu Wei Guo Yaoxu Lyu Yijie Xu Jiayu Shi Mengfei Du Cheng Chi Mengdi Zhao Xiaoshuai Hao Research Leads Yonghua Lin Zhongyuan Wang(cid:66) Tiejun Huang Shanghang Zhang(cid:66) Contributors Real-Robot Experiments Junkai Zhao Xiaojie Zhang Shanyu Rong Huaihai Lyu Zhengliang Cai Yankai Fu Ning Chen Bolun Zhang Lingfeng Zhang Shuyi Zhang Product & Operations Xi Feng Songjing Wang Xiaodan Liu Yance Jiao Infrastructure Mengsi Lyu Zhuo Chen Chenrui He Yupu Feng Yulong Ao Evaluation Xue Sun Zheqi He Jingshu Zheng Xi Yang System Management Donghai Shi Kunchang Xie Bochao Zhang Shaokai Nie Chunlei Men Equal Contribution (Co-first Authors). Project Leaders. (cid:66) Corresponding Author. Team Email: robobrain@baai.ac.cn"
        },
        {
            "title": "A Qualitative examples",
            "content": "This section provides comprehensive set of qualitative examples that illustrate the capabilities of RoboBrain 2.0 in various embodied AI tasks. These examples demonstrate the models proficiency in spatial reasoning, temporal planning, and interactive reasoning, showcasing its potential for real-world applications. A.1 Examples for Pointing In the pointing task, RoboBrain 2.0 is required to identify and point to specific objects within an image based on complex spatial instructions. For instance, given the instruction Please point out the orange box, the model accurately identifies the orange box in the image. Similarly, for more complex instructions such as Please point out the brown box on the shelf, RoboBrain 2.0 demonstrates its ability to understand spatial relationships and accurately points to the correct object. The models proficiency in this task is further exemplified by its performance on variety of pointing examples, as shown in Figure 5-Figure 20. These examples highlight the models robust spatial reasoning capabilities, enabling it to handle wide range of pointing tasks with high precision. Whether the instructions involve simple object identification or more intricate spatial relationships, RoboBrain 2.0 consistently demonstrates its ability to accurately locate and point to the specified objects. This capability is crucial for applications in robotics and automation, where precise object localization is essential for effective interaction with the physical environment. Figure 5 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 23 Figure 6 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 24 Figure 7 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 25 Figure 8 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 26 Figure 9 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 27 Figure 10 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 28 Figure 11 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 29 Figure 12 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 30 Figure 13 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 31 Figure 14 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 32 Figure 15 Pointing Examples of RoboBrain 2.0. The blue point represents the models spatial referring prediction. 33 Figure 16 Pointing Examples of RoboBrain 2.0. The objects or their parts are pointed according to their affordances queried in the instruction. 34 Figure 17 Pointing Examples of RoboBrain 2.0. The objects referred by spatial relations or object attributes are pointed out. 35 Figure 18 Pointing Examples of RoboBrain 2.0. The objects referred by spatial relations or object attributes are pointed out. 36 Figure 19 Pointing Examples of RoboBrain 2.0. The free space indicated by spatial relations and the referenced objects are pointed out. 37 Figure 20 Pointing Examples of RoboBrain 2.0. The free space indicated by spatial relations and the referenced objects are pointed out. 38 Figure 21 Pointing Examples of RoboBrain 2.0. The free space indicated by spatial relations and the referenced objects are pointed out. 39 A.2 Examples for Affordance The affordance task assesses RoboBrain 2.0s understanding of object functionalities and interaction possibilities. For example, when asked What part of mug holds the liquid for drinking? the model correctly identifies the interior of the mug as the part that holds the liquid. In another example, the instruction Which part of handbag can be grasped to carry it? is accurately answered by identifying the handle of the handbag. These examples showcase the models ability to reason about object affordances, making it capable of understanding how objects can be interacted with in the real world. As shown in Figure 22-Figure 23, the model demonstrates its proficiency in identifying functional parts of objects and their potential uses. Figure 22 Affordance Examples of RoboBrain 2.0. The purple bounding boxes denote the actionable affordance areas for specific tasks. 40 Figure 23 Affordance Examples of RoboBrain 2.0. The purple bounding boxes denote the actionable affordance areas for specific tasks. 41 A.3 Examples for Trajectory The trajectory task evaluates the models ability to predict and navigate paths based on given instructions. For instance, given the instruction Please provide the trajectory to move the robot arm to grasp the apple, RoboBrain 2.0 generates smooth and efficient path for the robot arm to follow. The models trajectory predictions are accurate and take into account the spatial constraints and obstacles in the environment, demonstrating its proficiency in spatial and temporal reasoning for navigation tasks. As shown in Figure 24Figure 25, the model effectively plans and executes trajectories that are both optimal and collision-free. Figure 24 Trajectory Examples of RoboBrain 2.0. The blue trajectories, composed of key trajectory points, represent the model-predicted paths for task completion. 42 Figure 25 Trajectory Examples of RoboBrain 2.0. The blue trajectories, composed of key trajectory points, represent the model-predicted paths for task completion. 43 A.4 Examples for EgoPlan2 The EgoPlan2 task focuses on planning daily activities from an egocentric perspective. For instance, given the instruction Plan the steps to prepare cup of coffee, RoboBrain 2.0 outlines detailed sequence of actions, including locating the coffee machine, fetching the coffee beans, and following the steps to brew the coffee. The models ability to break down complex tasks into actionable steps demonstrates its proficiency in task decomposition and sequential planning. As shown in Figure 26-Figure 28, the model effectively plans and executes multi-step tasks, showcasing its capabilities in long-horizon planning and task execution. Figure 26 EgoPlan Examples of RoboBrain 2.0. The key reasoning process and final answer related to specific tasks are depicted. 44 Figure 27 EgoPlan Examples of RoboBrain 2.0. The key reasoning process and final answer related to specific tasks are depicted. 45 Figure 28 EgoPlan Examples of RoboBrain 2.0. The key reasoning process and final answer related to specific tasks are depicted. 46 A.5 Examples for Close-Loop Interaction Close-loop interaction examples showcase RoboBrain 2.0s ability to engage in interactive reasoning with feedback. For example, in scenario where the model is asked to Find muff cup and pour coffee into it, it not only needs to navigate and search for the mug multiple times within the task environment but also must operate the coffee machine based on feedback to complete the pouring process. This iterative process highlights the models capability to refine its actions based on real-time feedback, ensuring more accurate and reliable performance in interactive tasks. As shown in Figure 29-Figure 32, the model demonstrates its ability to adapt and improve its responses through iterative feedback loops. Figure 29 Close-loop planning Examples of RoboBrain 2.0. The key planning steps related to specific tasks are depicted. 47 Figure 30 Close-loop planning Examples of RoboBrain 2.0. The key planning steps related to specific tasks are depicted. 48 Figure 31 Close-loop planning Examples of RoboBrain 2.0. The key planning steps related to specific tasks are depicted. 49 Figure 32 Close-loop planning Examples of RoboBrain 2.0. The key planning steps related to specific tasks are depicted. 50 A.6 Examples for Multi-Robot Planning In multi-robot planning scenarios, RoboBrain 2.0 coordinates the actions of multiple robots to achieve common goal. For example, in supermarket scenario, the model plans the movements of multiple robots to efficiently restock shelves. The planning involves assigning specific tasks to each robot, coordinating their movements to avoid collisions, and ensuring that the overall goal is achieved in timely manner. These examples highlight the models advanced capabilities in multi-agent coordination and long-horizon planning. As shown in Figure 33, the model demonstrates its ability to orchestrate complex multi-robot activities with high precision and efficiency. In the restaurant setting (Figure 33(a)), Unitree G1 humanoid and Agilex dual-arm robot collaborate on burger preparation and delivery for the command Im hungry and order normal burger, with RoboBrain 2.0 performing scene-aware task decomposition. The household scenario (Figure 33(b)) features Realman single-arm and Agilex dual-arm robot executing commands like Give me an orange and knife. In the supermarket (Figure 33(c)), RoboBrain 2.0 assists customers with gift selection by analyzing dimensions and bag compatibility, coordinating the Realman robot for gift placement and the Agilex executing VLA-cerebellum skills like open the gift bag. Please refer to RoboOS [61] for more details. Figure 33 We showcase multi-robot collaboration in three scenarios: (a) Restaurant: Unitree G1 and Agilex robots prepare burgers. (b) Household: Realman and Agilex robots fetch items. (c) Supermarket: Robots coordinate gift selection and packaging. A.7 Examples for Synthetic Benchmarks Synthetic benchmarks are used to evaluate RoboBrain 2.0s performance on variety of spatial and temporal reasoning tasks. For instance, in the BLINK benchmark, which assesses depth perception and spatial relation understanding, the model achieves high accuracy in identifying the relative positions and distances of objects. In the CV-Bench benchmark, which evaluates 3D spatial understanding, RoboBrain 2.0 demonstrates its ability to accurately process and reason about 3D scenes. These synthetic benchmarks provide comprehensive evaluation of the models capabilities across different reasoning dimensions. As shown in Figure 34-Figure 35, the model consistently performs well across various synthetic benchmarks, showcasing its robust abilities. Figure 34 CVbench Benchmark Examples of RoboBrain 2.0. The solid circle in the diagram represents the selected point. 52 Figure 35 BLINK Benchmark Examples of RoboBrain 2.0. The solid circle in the diagram represents the selected object."
        },
        {
            "title": "B Prompts Details",
            "content": "This section outlines the system prompts for various spatial understanding and planning tasks assigned to robot with advanced visual and analytical capabilities. Each task requires simulating visual-spatial reasoning, leveraging visual inputs as if directly perceiving the scene, and generating step-by-step reasoning processes within <think></think> tags, with answers in <answer></answer> tags. Reasoning is kept concise (200500 words) and follows five-stage process tailored to each task. Instructions emphasize direct visual-spatial language, avoiding abstract references to input data (e.g., based on the description) and maintaining the robots role. B.1 Spatial Understanding: Coordinates -- Pointing The robot is tasked with identifying specific points within an image based on visual criteria, such as locating points in vacant area on delineated plane. The reasoning process includes: Object Analysis Observe the objects shape, size, and spatial relationships (e.g., red rectangular border delineating plane with vacant patches). Capability Assessment Relate visual processing capabilities to identifying vacant regions and pinpointing coordinates. Contextual Relevance Focus on the task requirement to select points within the vacant area. Verification Ensure selected coordinates lie within boundaries and are distinct. Point Conclusion Output coordinates as list of tuples, justified by visual analysis. Example Prompt: Locate points within vacant area on plane outlined by red border. Your answer should be formatted as list of tuples, i.e. [(x1, y1), (x2, y2), . . . ], where each tuple contains the and coordinates of point satisfying the conditions above. The coordinates should indicate the normalized pixel locations of the points in the image. Example Output w/ Thinking: <think> . . . </think><answer>[(296, 282), (321, 256), . . . ]</answer>. B.2 Spatial Understanding: Coordinates -- Trajectory The robot predicts sequence of key trajectory points to achieve goal, such as reaching an object. The reasoning process includes: Object Analysis Identify the target objects properties and spatial relationships (e.g., banana on plate with potential obstacles nearby). Capability Assessment Use joint control to plan smooth end-effector paths, avoiding obstacles. Contextual Relevance Ensure the trajectory aligns with the goal (e.g., reaching the banana). Verification Confirm the path avoids obstacles and reaches the target. Trajectory Conclusion Output trajectory points as [[x1, y1], [x2, y2], . . . ], justified by visual and kinodynamic analysis. Example Prompt: You are robot using the joint control. The task is Reach for banana on plate. Please predict up to 10 key trajectory points to complete the task. Your answer should be formatted as list of tuples, i.e. [[x1, y1], [x2, y2], . . . ], where each tuple contains the and coordinates of point.\" Example Output w/ Thinking: <think> . . . </think><answer>[[116, 114], [153, 97], . . . ].</answer>. B.3 Spatial Understanding: Bounding Box -- Affordance The robot identifies an affordance area for interaction with an object, such as grasping handle. The reasoning process includes: Object Analysis Describe the objects shape, size, and material properties (e.g., blue coffee mug with handle, inferred as ceramic from sheen). Capability Assessment Assess gripper compatibility with the objects features (e.g., handle size vs. gripper opening). Contextual Relevance Align with the task goal (e.g., preparing coffee by grasping the mug). Verification Confirm the affordance area suits the interaction and is within reach. Affordance Conclusion Output the bounding box as [xmin, ymin, xmax, ymax], justified by visual compatibility. 54 Example Prompt: You are robot using the joint control. The task is hold coffee mug. Please predict possible affordance area of the end effector. Example Output w/ Thinking: <think> . . . </think><answer>[915, 408, 1109, 533].</answer>. B.4 Spatial Understanding: Freeform Q&A -- General Spatial Analysis The robot answers questions about spatial relationships or action outcomes based on one or more images. The reasoning process includes: Scene Perception Detail prominent features and their spatial arrangement (e.g., metallic gripper above green book on shelf). Task Objective Interpretation Clarify the questions focus (e.g., predicting the outcome of grippers trajectory). Focused Visual Analysis Examine relevant scene elements or project actions (e.g., yellow trajectory toward lower shelf). Relational Reasoning Synthesize observations to form hypothesis, evaluating provided options. Conclusion Derivation Output the answer, justified by visual evidence and logical reasoning. Example Prompt: Predict the outcome of gripper following yellow trajectory. Options: (A) place book on lower shelf; (B) place book on upper shelf. Example Output w/ Thinking: <think> . . . </think><answer>(A)</answer>. B.5 Temporal Understanding: Long-horizon Planning The robot determines the next action in task (e.g., cooking) based on sequence of images and the current view. The reasoning process includes: Task Progress Analysis Interpret completed actions from the sequence (e.g., onions peeled and sliced on cutting board). Current Scene Analysis Describe the current views objects and state (e.g., frying pan on hob, oil container nearby). Contextual Relevance Align with the task goal (e.g., cook onions by preparing the pan). Action Option Evaluation Assess options for suitability (e.g., pour oil vs. peel onion, considering onions are already prepared). Next Action Conclusion Output the next action, justified by visual evidence and task flow. Example Prompt: Prepare and cook onions; choose the next action (options: pour oil, turn up hob, etc.). Example Output w/ Thinking: <think> . . . </think><answer>Pour oil.</answer>. B.6 Temporal Understanding: Closed Loop Conversation The robot answers question within conversation history, leveraging prior visual inputs and responses. The reasoning process includes: Task Progress Recall Recap previous actions and their outcomes (e.g., opened the fridge to access ingredients). Initial Analysis Focus on current visual input relevant to the question (e.g., coffee machine on the countertop). Contextual Relevance Align with the current task goal (e.g., flipping the coffee machine switch). Action Option Evaluation Assess options for logical progression based on history and current state. Next Action Conclusion Output the action, justified by visual evidence and conversation context. Example Prompt: The task is Flip the coffee machine switch after opening the fridge. After you have finished <action> , you can see <image>, and the feedback of final action is xxx. What is your next action? Example Output w/ Thinking: <think> . . . </think><answer>Toggle on Coffee Machine.</answer>. B.7 Temporal Understanding: Multi-Robot Planning The robot coordinates actions with other robots to achieve common goal, devided into global task decomposition and agent-based tool-calling. Example Prompt for Global Task Decomposition: Please Refer to Figure 36. Example Prompt for Agent-based Tool-calling: Please Refer to Figure 37. Example Output w/ Thinking: <think> . . . </think><answer>Graph of TaskFlow</answer>. Figure 36 Prompt for global task decomposition. 56 Figure 37 Prompt for agent-based tool calling."
        }
    ],
    "affiliations": []
}