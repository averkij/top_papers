{
    "paper_title": "D$^2$iT: Dynamic Diffusion Transformer for Accurate Image Generation",
    "authors": [
        "Weinan Jia",
        "Mengqi Huang",
        "Nan Chen",
        "Lei Zhang",
        "Zhendong Mao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the naturally varying information densities present in these regions. However, large compression leads to limited local realism, while small compression increases computational complexity and compromises global consistency, ultimately impacting the quality of generated images. To address these limitations, we propose dynamically compressing different image regions by recognizing the importance of different regions, and introduce a novel two-stage framework designed to enhance the effectiveness and efficiency of image generation: (1) Dynamic VAE (DVAE) at first stage employs a hierarchical encoder to encode different image regions at different downsampling rates, tailored to their specific information densities, thereby providing more accurate and natural latent codes for the diffusion process. (2) Dynamic Diffusion Transformer (D$^2$iT) at second stage generates images by predicting multi-grained noise, consisting of coarse-grained (less latent code in smooth regions) and fine-grained (more latent codes in detailed regions), through an novel combination of the Dynamic Grain Transformer and the Dynamic Content Transformer. The strategy of combining rough prediction of noise with detailed regions correction achieves a unification of global consistency and local realism. Comprehensive experiments on various generation tasks validate the effectiveness of our approach. Code will be released at https://github.com/jiawn-creator/Dynamic-DiT."
        },
        {
            "title": "Start",
            "content": "D2iT: Dynamic Diffusion Transformer for Accurate Image Generation Weinan Jia1, Mengqi Huang1, Nan Chen1, Lei Zhang1, Zhendong Mao1 2* 1University of Science and Technology of China, Hefei, China; 2Institute of Artificial intelligence, Hefei Comprehensive National Science Center, Hefei, China {jiawn, huangmq, chen nan}@mail.ustc.edu.cn, {leizh23, zdmao}@ustc.edu.cn 5 2 0 2 3 1 ] . [ 1 4 5 4 9 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models are widely recognized for their ability to generate high-fidelity images. Despite the excellent performance and scalability of the Diffusion Transformer (DiT) architecture, it applies fixed compression across different image regions during the diffusion process, disregarding the naturally varying information densities present in these regions. However, large compression leads to limited local realism, while small compression increases computational complexity and compromises global consistency, ultimately impacting the quality of generated images. To address these limitations, we propose dynamically compressing different image regions by recognizing the importance of different regions, and introduce novel two-stage framework designed to enhance the effectiveness and efficiency of image generation: (1) Dynamic VAE (DVAE) at first stage employs hierarchical encoder to encode different image regions at different downsampling rates, tailored to their specific information densities, thereby providing more accurate and natural latent codes for the diffusion process. (2) Dynamic Diffusion Transformer (D2iT) at second stage generates images by predicting multi-grained noise, consisting of coarse-grained (less latent code in smooth regions) and fine-grained (more latent codes in detailed regions), through an novel combination of the Dynamic Grain Transformer and the Dynamic Content Transformer. The strategy of combining rough prediction of noise with detailed regions correction achieves unification of global consistency and local realism. Comprehensive experiments on various generation tasks validate the effectiveness of our approach. Code will be released at https://github. com/jiawn-creator/Dynamic-DiT. Figure 1. Illustration of our motivation. Compression here refers to the VAE + Patchify operation. (a) Existing fixed-compression diffusion transformer (DiT) ignore information density. Fixed large compression leads to limited local realism due to the limited representation of few tokens preventing accurate recovery of rich information, whereas fixed small compression leads to limited global consistency and high computational complexity due to the burden of global modeling across patched latents. Samples in (a) are obtained from [38]. (b) Our Dynamic Diffusion Transformer (D2iT) adopts dynamic compression strategy and adds multi-grained noise based on information density, achieving unified global consistency and local realism. 1. Introduction In recent years, vision generative models have advanced significantly, raising the realism and fidelity of visual gen- *Zhendong Mao is the corresponding author. eration to new heights. Among them, the Diffusion Transformer (DiT) has attracted considerable attention and become the de facto choice for many modern image and video generative models such as Stable Diffusion 3 [10], Flux [24] and CogvideoX [56], primarily because it combines the best of both worlds, i.e., the scalability of transformer architecture and the powerful modeling of diffusion process. Due to the high computational cost in pixel space and challenges of capturing high-level semantic features, existing DiT-based generation models [2, 11, 27, 38, 59] generally follow two-stage paradigm, i.e., (1) in the first stage, Variational Autoencoder (VAE) [21] is utilized to spatially compress the image into low-dimensional latent space representation; (2) in the second stage, this latent representation is further spatially patched to form more compressed one, which is then modeled by the diffusion process within transformer architecture. The key to the success of DiT lies in its spatial compression, which significantly reduces the image sequence length. This reduction is essential for the transformers self-attention mechanism to model global structures, while not markedly compromising local details. Recent improvements to DiT primarily focus on accelerating convergence and broadening its applicability to various downstream tasks. Approaches such as MDT [11] and Mask DiT [59] employ masking strategies to speed up training convergence and enhance the models capacity for representative learning. FiT [33] utilizes rotational position encoding to enable variable-resolution capabilities, allowing the model to generate images of different sizes. Furthermore, industrial-scale models such as Pixart-α [2] and Hunyuan-DiT [27] incorporate cross-attention mechanisms and advanced text encoders like CLIP [40] and T5 [41], achieving high-quality, text-guided image generation using large datasets and effective training strategies. Though great progress has been made, the commonality among existing DiT-based methods is that they all leave the key compression principle untouched, i.e., using fixed downsampling ratio for all image regions equally. Specifically, fixed pixel region is compressed into latent token, which is then diffused and denoised for visual generative modeling without considering the information density of different regions. In this study, we argue that the fixed compression employed by existing DiT-based models overlooks the natural variation in spatial complexity across different image regions. As result, these models are constrained in integrating consistent global visual structures with realistic local visual details, and suffer from slow training convergence. The root cause of this limitation is that the image diffusion transformer model, by its very nature, learns to progressively recover each regions image information from pure Gaussian noisy patched latent through the built-in self-attention mechanism. On the one hand, while large fixed compression with short image sequence is effective for the self-attention mechanism to capture dependencies across patched latents, it fails to accurately recover all the rich information for detailed regions due to their high spatial complexity. As shown in Figure 1 (a) (i), the large compressed tokens are overwhelmed to model the details and do not guarantee the realism of the lions face. On the other hand, the small fixed compression results in much longer image sequence, which can better recover the local details within each region but significantly increases the computational difficulty and burden for consistent global modeling across patched latents. As shown in Figure 1 (a) (ii), despite the realistic lions face in the generated image, its body structure has obvious defects due to inaccurate global modeling of the long image sequence. To address the above challenges, we propose Dynamic Diffusion Transformer (D2iT) for accurate image representation modeling both across and within patched latents, achieving the integration of consistent global structures and realistic local details with faster training convergence and smaller computational burden. As shown in Figure 1 (b), the key idea of D2iT is to adaptively compress image regions into different grained representations with various numbers of latents. Specifically, we introduce novel twostage framework, i.e., (1) Dynamic VAE (DVAE) is used in the first stage to encode image regions into different downsampling continuous latent representations according to their different spatial complexity; (2) D2iT is proposed to complete multi-grained noise prediction task. Considering that the natural image regions inherently contain spatial density information and image content information, multigrained noise prediction can be decomposed into two parts: spatial information density modeling and content information modeling. Therefore, we designed Dynamic Grain Transformer for modeling spatial information density and Dynamic Content Transformer for modeling noise content. The Dynamic Grain Transformer learns the true spatial density distribution from DVAE and predicts grain map. Then, the Dynamic Content Transformer applies different compression levels of noise to different regions according to their assigned granularities. In order to maintain global consistency and local details, we adopt global noise rough prediction and fine-grained noise correction strategy in Dynamic Content Transformer. In summary, our main contributions are as follows: Conceptual Contributions. We identified that existing diffusion processes overlook the natural variation in spatial complexity across different image regions, leading to limitations in integrating consistent global visual structure and realistic local visual details. We propose more naturally information-density-based diffusion process and dynamic information compression. Technical Contributions. We propose novel twostage generation architecture. The proposed D2iT generates image by predicting multi-grained noise. The novel rough prediction and fine-grained correction strategy make the diffusion process more efficient. Experimental Contributions. We used similar number of parameters and only 57.1% of the training resources to achieve 23.8% quality improvement compared to DiT on the ImageNet Dataset, i.e., our 1.73 FID vs. DiTs 2.27 FID. 2. Related Works 2.1. Variational Autoencoder for Generation Model Variational Autoencoder (VAE) [9] is compression coding model that can represent information such as images and videos more compactly in latent space. Most current mainstream image and video generation models adopt two-stage paradigm. In the first stage, VAE compresses In and encodes the image or video into the latent space. the second stage, the image or video distribution is remodeled within this low-dimensional latent space. Thanks to the compact information representation, this two-stage generation approach has become prevalent and is utilized by many milestone models, such as DALL-E [42], latent diffusion [44], Sora [37], etc. However, this fixed-length coding does not consider information density. Regions rich in detail have the same number of coding representations as background areas, leading to insufficient and redundant coding. To address this problem, [15] proposed dynamic coding VAE and achieved success in the autoregressive model. Diffusion models using fixed-length coding also overlook information density. Our work refines this approach and is the first to transfer the concept of dynamic coding to diffusion models in continuous space. 2.2. Diffusion Model Diffusion models [14, 47, 49] are effective generative models. The noise prediction network gradually generates denoised samples of the input data by learning the denoising process. They have demonstrated significant potential in generating high-quality images [19, 36, 4345] and have advanced progress in various fields [3, 4, 16, 22, 26, 35, 54, 55]. Recent improvements in sampling methods [18, 32, 48] and the classifier-free guidance [13] have further enhanced their capabilities. Latent Diffusion Models (LDMs) [38, 39, 44] adopts two-stage generative architecture that efficiently encodes images using pre-trained autoencoders [21] and performs the diffusion process in lowdimensional latent space. This approach addresses the challenges associated with generating images directly in pixel space. In this study, we develop dynamic grained denoising network and verify its applicability in DiT framework. 3. Method The Diffusion Transformer (DiT) [38] employs two-stage generation framework that compresses fixed pixel regions into tokens, allowing the transformer framework to model diffusion noise. However, the uniform treatment of all regions makes it difficult to distinguish between detailed regions and smooth background, hindering accurate modeling of global consistency of entire image and details in highinformation regions. This leads to suboptimal results. Considering that the natural images possess varying perceptually important regions and diverse spatial distributions, we design two-stage framework shown in Figure 2 to learn the dynamic priors of the natural images. The first stage DVAE (Section 3.1) aims to encode the image more accurately by identifying the information density and using different downsampling rates for different regions, and the second stage D2iT (Section 3.2) learns the spatial density information and content information of the multi-grained codes to generate the image more naturally. 3.1. Dynamic VAE (DVAE) Different from existing VAE-based works [9, 25, 57] that use fixed downsampling factor to represent image regions with fixed encoding length, our Dynamic VAE (DVAE) first defines set of staged downsampling factors {f1, f2, , fk}, where f1 < f2 < < fk. As shown in Stage 1 of Figure 2, an image RH0W03 is first encoded into grid features = {Z1, Z2, , Zk} through the hierarchical encoder Eh, where Zi RHiWinz and the shape (Hi, Wi) is defined as: (Hi, Wi) = (H0/fi, W0/fi), {1, 2, , k}. (1) Using the maximum downsampling factor fk, the original image is segmented into regions with the size S2 = 2 , i.e., total of Np = H0/S W0/S regions. Subsequently, the Dynamic Grained Coding module allocates the most suitable granularity to each region based on the local information entropy, resulting in multi-grained latent representation. The Dynamic Grained Coding module employs Gaussian kernel density estimation to analyze pixel intensity distributions within each region and uses Shannon entropy to quantify the complexity of each region. To handle the irregular latent code with different grained regions, we further propose simple and effective neighbor copying method. Specifically, the latent code for each region is copied to the finest granularity of codes if the finest granularity is not assigned for it. Dynamic Grained Coding. Inspired by the discrete version of DQVAE1[15], the Dynamic Grained Coding module begins by converting the original image into singlechannel image denoted as RH0W01. Then, the single-channel image is divided into non-overlapping regions, each of size S. To assess the local information content of each region, the Dynamic Grained Coding module employs Gaussian kernel density estimation to compute the probability density function (PDF) ˆpk() of pixel intensities within the k-th region: ˆpk(bj) ="
        },
        {
            "title": "1\nS2",
            "content": "S2 (cid:88) i=1 (cid:32) exp (cid:18) xk,i bj σ 1 2 (cid:19)2(cid:33) , (2) 1Code of discrete version is released at https://github.com/ CrossmodalGroup/DynamicVectorQuantization. Figure 2. The overview of our proposed two-stage framework. (1) Stage 1: DVAE dynamically assigns different grained codes to each image region through the Herarchical Encoder and Dynamic Grained Coding (DGC) module. (2) Stage 2: D2iT consists Dynamic Grain Transformer and Dynamic Content Transformer, which respectively model the spatial granularity information and content information. We present the network with two granularities. The grain map uses 1 to denote coarse-grained regions and 2 for fine-grained regions. where xk,i denotes the i-th pixel value in the k-th region, σ = 0.01 is the smoothing parameter, and {bj}P j=1s represents set of histograms uniformly distributed, where = SS denotes the total number of pixels in each region. Subsequently, the entropy Ek of each region is calculated using Shannons entropy formula [28]: Ek = (cid:88) j=1 ˆpk(bj) log ˆpk(bj). (3) These entropy values are then assembled into an entropy map RH0/SW0/S. To determine the appropriate processing granularity for each region, we pre-calculate the entropy distribution of natural images in the ImageNet dataset [6]. This allows us to establish entropy thresholds corresponding to specific percentiles of information content. By specifying set of desired grained ratios = {r1, r2, , rk}, we select corresponding entropy thresholds = {T1, T2, , Tk} such that the proportion of image regions with entropy values exceeding Ti matches the grained ratio ri. This ensures that ratio ri of the regions are assigned to granularity fi, where regions with higher entropy undergo finer-grained processing, and those with lower entropy receive coarser-grained treatment. 3.2. Dynamic Diffusion Transformer (D2iT) 3.2.1. Overview of Multi-grained Diffusion. The multi-grained diffusion process can be conceptualized as two consecutive steps. First, we predict the spatial distribution of information complexity (grain map) across the image using the Dynamic Grain Transformer. Then, we perform the multi-grained diffusion process within this naturally informed spatial distribution using the Dynamic Content Transformer, allowing for better representation of the inherent characteristics of natural images. 3.2.2. Dynamic Grain Transformer The first goal of D2iT is to model the spatial granularity distribution and predict the grain map R(H0/S)(W0/S) for the entire image. To achieve this, as shown in Stage 2 (a) of Figure 2, we employ Dynamic Grain Transformer, which generates the grain map by sampling from random noise, thereby effectively capturing the global granularity distribution throughout the image. The ground truth grain map used for training originates from the outputs of the Dynamic Grained Coding module within the DVAE. Specifically, for the patch region in row and column j, the granularity θi,j is determined by the downsampling factor utilized during the reconstruction phase of the DVAE, reflecting the appropriate granularity for that specific region: fine-grained regions, which further corrects the predicted finer-grained noise ϵ2 using small patch size PS: ϵ 2 = RefineNet(ϵ2, znoised, PS, c), (8) where ϵ 2 denotes the fine-grained noise corrected by RefineNet. The different grained noises are then merged to create comprehensive multi-grained noise: θi,j = arg max (gi,j,l) {1, 2, ..., k}, (4) where gi,j,l is the predicted probability of granularity for each region, 1 < < H0/S, 1 < < W0/S , and {1, 2, . . . , k}. The cross-entropy loss function for Dynamic Grain Transformer training is: Lgrain = Ei,j (cid:88) l=1 yi,j,l log(gi,j,l), (5) where yi,j,l is the ground truth granularity distribution at region (i, j). By learning real spatial grain map, the Dynamic Grain Transformer can effectively model the spatial distribution of information complexity. This grain map is then utilized to guide the multi-grained diffusion process. 3.2.3. Dynamic Content Transformer After obtaining the spatial information of the image, the next step is to model the content information. We propose the Dynamic Content Transformer for the task of multigrained noise prediction. In order to align with the patchify operation of existing SOTA methods [38],which generally adopt patch size of 2, we present dual-grained network (coarse with patch size of 2 and fine with patch size of 1). As depicted in Stage 2 (b) of Figure 2, the Dynamic Content Transformer consists of Multi-grained Net and Fine-grained Net, which realizes multi-grained noise rough prediction and fine-grained noise correction. First, multigrained Net tokenizes znoised using large patch size PL: = Patchify(znoised, PL, c), (6) where are the multi-grained tokens, denotes the conditional information, i.e., class label y, grain map and diffusion timestep t. These multi-grained tokens are processed by the standard DiT blocks. Following this, the routing module utilizes the grain map to differentiate between coarse and fine tokens and restores them to latent code: ϵθ = ϵ1 ϵ 2, (9) where denotes the combination operation for the complementary noises, and ϵθ is the final predicted noise of D2iT. Fine-grained Efficient RefineNet. The patchify operation with smaller patch size PS leads to more tokens and quadratic computational complexity due to self-attention. Inspired by [30, 50, 58], we propose using mini-windows to divide long tokens into several short tokens. Specifically, as illustrated in the fine patchify part of Figure 2 (b), we preprocess the input fine-grained features by dividing them into multiple non-overlapping windows. Considering that global consistency is effectively modeled by the Multigrained Net, multi-head self-attention in RefineNet is performed independently within each window, thereby effectively reducing the computational complexity. Specifically, the transformer blocks of Efficient RefineNet require effective modeling relative positional relationships of fine-grained tokens. position-aware selfattention structure is proposed to enhance this capability. Specifically, local learnable relative position bias is introduced when calculating the self-attention score in each block, enabling the model to better capture the relationship between tokens of the same granularity. The self-attention computation with the added bias is defined as: Attention(Q, K, ) = Softmax (cid:18) QK dk (cid:19) + Br V, (10) where Br is an matrix representing the relative positional deviation between given location and others. It is set of learnable parameters updated during each training iteration, where denotes the number of patches in the latent codes. Q, K, and belong to RN and represent the query, key, and value in the self-attention module. Multi-Grained Noise Loss. The proposed D2iT contains two granularities. To accommodate dynamic assignment, we design multi-grained noise loss function Ldyn: (ϵ1, ϵ2) = Router(T M, ), (7) Ldyn = EiEz0,ϵi,c,t [αi ϵi ϵθi (z0, c, t)2 2], (11) where ϵ1, ϵ2 denote the predicted coarse noise and fine noise, respectively. However, large patch size PL is not enough to handle the detailed regions. The Fine-grained Net is specifically designed to enhance noise correction in where z0 is the latent representation of the original image. αi is the loss weight for different granularities, given by αi = 1/(2ki 2ki), where = 2 represents the number of granularities. denotes the conditional input that pling. Given RGB image of shape 256 256 3, it encodes dual-grained mixture representation, i.e. 32324 and 16 16 4, where the coarse grained codes are copied and filled to the corresponding position to combine the two grains. In the second stage, the Dynamic Grain Transformer of D2iT only needs to predict simple spatial grain distribution, so the settings remain the same as DiT-S (small model) with 33M parameters. Dynamic Content Transformer of D2iT is trained with three different settings, i.e., D2iT-B (base model), D2iT-L (large model), D2iT-XL (extra-large model ) with 136M, 467M and 687M parameters, respectively. The Multi-grained Net and Fine-grained Net use patch sizes of 2 and 1, respectively. The maximum time step of diffusion process is set to 1000, and linear variance schedule of diffusion noise ranging from 104 to 2 102 is used. More detailed information of DVAE and D2iT is presented in the supplementary Material. Training details. To ensure fair comparison with SOTA models, we adhered to the setups used in prior works: (1) The D2iT model is trained using the AdamW optimizer [31] with batch size of 256 and learning rate of 1104. (2) We maintain an exponential moving average (EMA) of the D2iT weights with decay rate of 0.9999 during training. All models are trained using eight A800 GPUs. Benchmarks. Following previous work, we use two standard benchmarks, i.e., the unconditional FFHQ [17] and class-conditional ImageNet [6] at resolution of 256 256. Metrics. We use common metrics to evaluate the model. The standard Frechet Inception Distance (FID) [12] is used It to evaluate the generation and reconstruction quality. measures the diversity and accuracy of the generated images. Inception Score (IS) [46], Precision and Recall [23] are also used to measure class-conditional generation on ImageNet. In order to align with previous works, we report FID-10K on FFHQ and FID-50K on ImageNet. 4.2. Comparison Results We compared D2iT with the state-of-the-art diffusion models on the unconditional FFHQ and class-conditional ImageNet datasets. The main results of D2iT are reported using dual-grained design of = {8, 16} with the ratio rf =8 = 0.5. The following results were obtained after training for 800 epochs on the respective datasets. Unconditional generation. As shown in Table 1, we compare our D2iT-L results with existing fundamental models, achieving 28.6% improvement in quality over DiT-L along with an FID-10K score 4.47. Qualitative results for unconditional generation of D2iT-L are shown in Figure 3. We use single grain map to generate several images, and the information density distribution of the generated images is highly consistent with the grain map, demonstrating the effectiveness of our Dynamic Content Transformer. Class-conditional generation. In Table 2, we comFigure 3. Qualitative results of our unconditional generation on FFHQ. In the grain map, red blocks represent fine-grained regions, while blue blocks indicate coarse-grained regions. Model Type Method Param(M) FID-10K GAN GAN VAE Diffusion Diffusion Diffusion Diffusion Diffusion VQGAN [9] ViT-VQGAN [57] VDVAE [5] ImageBART [8] UDM [20] LDM-4 [44] DiT-L [38] D2iT-L(ours) 307 738 115 713 - 274 458 467 11.4 13.06 28.5 9.57 5.54 4.98 6. 4.47 Table 1. Comparison of unconditional generation on FFHQ. guides the diffusion process, i.e., class label and grain map. ϵi (0, I) represents the random noise at the i-th granularity. ϵθi is the predicted noise at the corresponding granularity. By weighting the loss according to the specified granularity and combining it with granularity selection matrix, the multi-grained noise loss function effectively updates different noise control networks. 4. Experiments 4.1. Implementation We present the implementation details of DVAE and D2iT, including model architecture, training details, benchmarks and evaluation metrics. Model architecture. The existing SOTA DiTbased image generation models generally use 8 VAE downsampling and 2 DiT patchify operation. Therefore, in order to align with the previous work, We use two grains in pipeline. Specifically, in the first stage, the hierarchical encoder in DVAE is downsampled by factors of = {8, 16} to achieve reasonable compression, i.e., detail regions with 8 downsampling and smooth regions with 16 downsamFigure 4. Qualitative results of D2iT-XL on ImageNet. The grain maps are generated by the Dynamic Grain Transformer based on class labels, and the images are generated by the Dynamic Content Transformer based on class labels and grain maps. Method Param(M) FID-50K IS Prec. Rec. VQGAN [9] MaskGIT [1] DQ-Transformer [15] LlamaGen [51] DiGIT [60] Open-MAGVIT2-XL [34] VAR [53] ADM [7] LDM-4 [44] DiT-XL [38] MDT[11] D2iT-XL(ours) ADM-G [7] ADM-G-U [7] LDM-4-G [44] DiT-XL-G [38] RDM-G [52] DiMR [29] MDT-G[11] MDTv2-G[11] D2iT-XL-G(ours) 397 227 655 775 732 1500 600 554 400 675 676 687 554 554 400 675 848 505 676 676 687 15.78 7.32 5.11 2.62 3.39 2.33 2.57 10.94 10.56 9.62 6.23 5. 4.59 3.94 3.60 2.27 1.99 1.70 1.79 1.58 1.73 - 0.78 - 0.80 - 78.3 156.0 178.2 244.1 205.96 271.77 0.84 0.83 302.6 100.98 0.69 103.49 0.71 121.50 0.67 143.02 0.71 156.29 0.72 186.70 0.82 215.84 0.83 247.67 0.87 278.24 0.83 260.45 0.81 289.0 0.79 283.01 0.81 314.73 0.79 307.89 0.87 - 0.50 - 0.57 - 0.54 0. 0.63 0.62 0.67 0.66 0.66 0.52 0.53 0.48 0.57 0.58 0.63 0.61 0.65 0.56 Table 2. Comparison of class-conditional generation on ImageNet 256 256. -G indicates the results with classifier-free guidance. pare the class-conditional image generation performance of D2iT with existing methods. We observe that D2iT outperforms DiT and other models in the table, achieving an FID score of 1.73 and 23.8% improvement over DiT-XL in quality with only 57.1% of the training steps. This demonstrates the effectiveness and scalability of our model. Qualitative results of class-conditional generation are shown in Figure 4. 4.3. Ablation Study and Analysis In this section, we conduct ablation studies to validate the design of D2iT. We report the results of the D2iT-B model on the FFHQ dataset, using FID-10K as the evaluation metric unless otherwise stated. All D2iT-B models in the ablation study were trained for 50 epochs. Dynamic Granularity Strategy. We first demonstrate Figure 5. The curves of different grain ratios of reconstruction quality (rFID) to generation quality (FID) on FFHQ. that our dynamic granularity noise diffusion has better generation ability compared to the existing fixed noise diffusion. We use VAE with = 16 and = 8 as baselines, and DVAE uses = {8, 16} dual granularity. Figure 5 shows the line graph of the reconstruction ability of DVAE and the generation ability of D2iT when the proportions of different granularities vary. We could conclude that: (1) As the proportion of fine granularity increases, the reconstruction quality of DVAE gradually improves because more codes can better represent the image. (2) At the appropriate fine grain ratio, D2iT shows better image generation ability (FID is 22.11 when rf =8 = 0.5) compared to the fixed level noise(FID is 29.15 when rf =8 = 1 and FID is 51.33 when rf =8 = 0). The reason is that important regions require more coding, i.e., more noise representation, while less important regions suffice with less noise due to their lesser information. (3) When the fine grain ratio increases from 0.7 to 1.0, DVAE only gets 0.16 improvement (from 1.83 to 1.67) in rFID, but the performance of D2iT declines from 23.64 to 29.15, indicating that the last 30% of less important regions contribute little effective information to the image, and most of it is redundant. Consequently, using too much code to represent coarse-grained regions hinders the models performance. The experimental results strongly support the motivation of dynamic diffusion to eliminate Configuration DiT-B with Predicted Grain Map + DVAE & Routing Module + Dynamic grain loss + RefineNet & Learnable Pos-embed (ours D2iT) FID-10K 34.67 29.10 27.62 22.11 Table 3. Ablation study of D2iT-B on FFHQ. All additional designs use grain map predicted by Dynamic Grain Transformer. Total Layers DiT RefineNet 12 12 12 14 16 12 10 8 12 12 12 0 2 4 0 2 4 FID-10K 25.15 22.11 26. 25.15 20.99 19.96 Table 4. Effect of numbers of RefineNet Blocks in D2iT-B. Experiments with fixed total layers increasing Refinenet layers and fixed DiT layers increasing Refinenet layers. Grain Map Setting Random Ground Truth Dynamic Grain Transformer FID-10K 15.93 4.35 4.47 Table 5. Effect of Dynamic Grain Transformer with D2iT-L. settings after 800 training epochs on the FFHQ dataset in Table 5. The grain map generated by the Dynamic Grain Transformer yields results comparable to the Ground Truth grain map of the datasets, significantly outperforming the random grain map. This demonstrates that the lightweight Dynamic Grain Transformer is sufficient to accurately model the spatial distribution of real images. 5. Conclusion & Future Direction In this study, we point out that existing Diffusion Transformer (DiT) models apply fixed denoising to uniformly sized image regions, disregarding the naturally varying information densities across different regions. This oversight results in insufficient denoising in crucial regions and redundant processing in less significant ones, compromising both local detail authenticity and global structural consistency. To address these limitations, novel two-stage framework is designed to enhance the effectiveness of image generation. The effectiveness of our method is demonstrated in various comprehensive generative experiments. Future Direction. Our study confirms the effectiveness of the dynamic granularity strategy in diffusion process and uses two granularities to align with existing In future work, more granularities within the methods. dynamic diffusion transformer architecture can be explored. Figure 6. Training convergence comparison of DiT and our D2iT with different parameters on ImageNet. FID-50K is evaluated. both insufficiency and redundancy. In addition, Figure 6 shows the training convergence between our method and DiT with different parameters on the ImageNet dataset. D2iT shows faster convergence than DiT in models with similar parameters. Analysis on the enhanced design. We first verify the design of various detailed components in Table 3. Initially, DVAE and the Routing Module are introduced based on the standard VAE & DiT architecture to implement simple dynamic grain diffusion. It was observed that the FID score improved from 34.67 to 29.10. Subsequently, by distinguishing different losses in coarse-grained and fine-grained regions, we further improved the FID score to 27.62. Next, we replaced the last two standard DiT backbone layers with RefineNet layers that have smaller patch size and learnable position embeddings, ultimately achieving an FID score of 22.11, compared to DiTs 34.67. Analysis on the effectiveness of the RefineNet. Table 4 presents two sets of control experiments to verify the effectiveness of Efficient RefineNet. (1) We adjusted the number of DiT Blocks and RefineNet Blocks, keeping the model size and Gflops2 relatively constant, and discovered that processing the detail regions can further enhance generation capability. This illustrates that the RefineNet with small patch size can capture richer details. In addition, we cannot blindly increase the number of RefineNet layers under certain computing resources. Too few layers in DiT backbone will damage global consistency and the generation of coarse-grained regions. Setting the appropriate ratio of layers, i.e. five DiT layers to one RefineNet layer, proves beneficial for generation under fixed computing resources. (2) We examined the impact of increasing the number of RefineNet blocks while maintaining constant number of DiT blocks, demonstrating RefineNets scaling capabilities. Analysis on Dynamic Grain Transformer. We present the results of D2iT-L experiments with various grain map 2Gflops (Giga Floating Point Operations per Second) is metric to describe the computational complexity of model."
        },
        {
            "title": "References",
            "content": "[1] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 7 [2] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2 [3] Nan Chen, Mengqi Huang, Zhuowei Chen, Yang Zheng, Lei Zhang, and Zhendong Mao. Customcontrast: multilevel contrastive perspective for subject-driven text-to-image customization. arXiv preprint arXiv:2409.05606, 2024. 3 [4] Zhuowei Chen, Shancheng Fang, Wei Liu, Qian He, Mengqi Huang, and Zhendong Mao. Dreamidentity: enhanced editability for efficient face-identity preserved image generaIn Proceedings of the AAAI Conference on Artificial tion. Intelligence, pages 12811289, 2024. 3 [5] Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. arXiv preprint arXiv:2011.10650, 2020. 6 [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255. Ieee, 2009. 4, 6 [7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:87808794, 2021. [8] Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. Advances in Neural Information Processing Systems, 34:3518 3532, 2021. 6 [9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1287312883, 2021. 3, 6, 7 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1 [11] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2316423173, 2023. 2, 7 [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017. 6 [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 3 [15] Mengqi Huang, Zhendong Mao, Zhuowei Chen, and Yongdong Zhang. Towards accurate image coding: Improved autoregressive image generation with dynamic vector quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2259622605, 2023. 3, 7 [16] Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom: narrowing real text word for real-time open-domain text-to-image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74767485, 2024. 3 [17] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44014410, 2019. 6 [18] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:2656526577, 2022. 3 [19] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024. 3 [20] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: universal training technique of score-based diffusion model for high precision score estimation. arXiv preprint arXiv:2106.05527, 2021. 6 [21] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3 [22] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020. 3 [23] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32, 2019. 6 [24] Black Forest Labs. Flux.1. https://github.com/black-forestlabs/flux, 2024. 1 [25] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. 3 [26] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:43284343, 2022. [27] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 2 [28] Jianhua Lin. Divergence measures based on the shannon entropy. IEEE Transactions on Information theory, 37(1):145 151, 1991. 4 [29] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. arXiv preprint arXiv:2406.09416, 2024. 7 [30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: In Hierarchical vision transformer using shifted windows. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1001210022, 2021. 5 [31] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [32] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 3 [33] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, XiFit: Flexible arXiv preprint hui Liu, Wanli Ouyang, and Lei Bai. vision transformer for diffusion model. arXiv:2402.12376, 2024. 2 [34] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 7 [35] Zhendong Mao, Mengqi Huang, Fei Ding, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom++: Representing images as real-word for real-time customization. arXiv preprint arXiv:2408.09744, 2024. 3 [36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [37] OpenAI. Sora: Creating video from text. https://openai.com/sora, 2024. [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 2, 3, 5, 6, 7 [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. 2 [41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. 2 [42] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 88218831. PMLR, 2021. 3 [43] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. 3, 6, 7 [45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 3 [46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in Neural Information Processing Systems, 29, 2016. 6 [47] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International Confernonequilibrium thermodynamics. ence on Machine Learning, pages 22562265. PMLR, 2015. 3 [48] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. and Stefano Ermon. arXiv preprint [49] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. 3 [50] George Stein, Jesse Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Ross, Valentin Villecroze, Zhaoyan Liu, Anthony Caterini, Eric Taylor, and Gabriel Loaiza-Ganem. Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 5 [51] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 7 [52] Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350, 2023. 7 [53] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in Neural Information Processing Systems, 37:8483984865, 2024. 7 [54] Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis, et al. Lion: Latent point diffusion models for 3d shape generation. Advances in Neural Information Processing Systems, 35:1002110039, 2022. [55] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: geometric diffusion model arXiv preprint for molecular conformation generation. arXiv:2203.02923, 2022. 3 [56] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1 [57] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 3, 6 [58] Bowen Zhang, Shuyang Gu, Bo Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, and Baining Guo. Styleswin: Transformer-based gan for high-resolution image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1130411314, 2022. 5 [59] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. 2 [60] Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, and Lidong Bing. Stabilize the latent space for image autoregressive modeling: unified perspective. arXiv preprint arXiv:2410.12490, 2024."
        }
    ],
    "affiliations": [
        "Institute of Artificial intelligence, Hefei Comprehensive National Science Center, Hefei, China",
        "University of Science and Technology of China, Hefei, China"
    ]
}