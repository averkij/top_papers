{
    "paper_title": "Unified Latents (UL): How to train your latents",
    "authors": [
        "Jonathan Heek",
        "Emiel Hoogeboom",
        "Thomas Mensink",
        "Tim Salimans"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 1 ] . [ 1 0 7 2 7 1 . 2 0 6 2 : r Unified Latents (UL): How to train your latents Jonathan Heek1, Emiel Hoogeboom1, Thomas Mensink1 and Tim Salimans1 1Google DeepMind Amsterdam We present Unified Latents (UL), framework for learning latent representations that are jointly regularized by diffusion prior and decoded by diffusion model. By linking the encoders output noise to the priors minimum noise level, we obtain simple training objective that provides tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set new state-of-the-art FVD of 1.3. 1. Introduction Diffusion models have become remarkably successful for image, video, and audio generation. An important factor in this success has been latent representations, compact encodings that allow diffusion models to scale to higher resolutions more efficiently. However, it remains unclear how best to learn such latents. The original Latent Diffusion Model (Rombach et al., 2022) uses VAE-style KL penalty between the latent distribution and standard Gaussian. Since the decoder lacks likelihood-based loss, the weight of the KL term must be set manually, making it difficult to reason about the information content of the latents. Recently, works have focused on getting semantic representations from either pretrained networks (e.g. from DINO) or heavily regularized autoencoders. These latents are usually easier to learn due to their lower information density and obtain impressive FIDs. However, high frequency information typically gets lost, which can be seen by worse PSNRs or heavy reconstruction artifacts. Simply put, there is trade-off between the information content of the latent, and the reconstruction quality of the output. If the structure of the latent is easier to learn, this generally leads to better generation performance. The easier to learn latent is while retaining its information density, the better the resulting generation quality. In theory, even single unregularized latent channel could encode an arbitrary amount of information. In practice, the actual information is limited by machine precision and encoder smoothness. The number of latent channels therefore determines the information capacity: fewer channels yield easier-to-model latents at the cost of reconstruction quality, while more channels enable near-perfect reconstruction but require greater modeling capacity. This paper shows how to navigate this trade-off systematically. The key question we address is: How should latents be regularized when they will subsequently be modeled by diffusion model? Our answer: by co-training diffusion prior on them. This approach, which we call Unified Latents, rests on three key ideas: Encode latents with fixed amount of Gaussian noise. Align the prior diffusion model with the minimum noise level. As consequence the KL term reduces to simple weighted MSE over noise levels. Use reweighted elbo loss (sigmoid weighting) for the decoder. These components work together to train latents that are simultaneously encoded, regularized, and Corresponding author(s): {jheek, emielh, mensink, salimans}@google.com 2026 Google DeepMind. All rights reserved Unified Latents (UL): How to train your latents modeled using diffusion. This provides an interpretable bound on the bits in the latents, and simple hyper-parameters to control the reconstruction-modelling tradeoff. 2. Background Variational AutoEncoders Variational inference provides principled approach to learning latent representations. Given images ğ’™ that we wish to model, we can derive the Evidence Lower Bound (ELBO) on the log-likelihood when using latent variable ğ’›0: + KL(cid:104) log ğ‘ğœƒ(ğ’™) ğ”¼ğ’›0ğ‘ğœƒ (ğ’›0 ğ’™) = ğ¿(ğ’™), (1) (cid:105) (cid:105) (cid:104) log ğ‘ğœƒ(ğ’™ğ’›0) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) decoder (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) ğ‘ğœƒ(ğ’›0ğ’™) (cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) (cid:124) encoder (cid:12) (cid:12) ğ‘ğœƒ(ğ’›0) (cid:32) (cid:32) (cid:125) (cid:123)(cid:122) (cid:124) prior In this work both the decoder ğ‘ğœƒ(ğ’™ğ’›0) and the prior ğ‘ğœƒ(ğ’›0) will be learned with diffusion model. Diffusion Models Diffusion models can be used to model arbitrary continuous distributions. diffusion model learns to revert gradual destruction process which enables compression, likelihood estimation, and sampling from the distribution of interest. Consider data distribution ğ‘(ğ‘¥) and destruction process ğ’™ğ‘¡ = ğ›¼(ğ‘¡)ğ‘¥ + ğœ(ğ‘¡)ğœ– with ğœ– (0, 1). The level of destruction is defined by the logsnr schedule ğœ† (ğ‘¡) = log(ğ›¼2 ğ‘¡ = 1 for convenience. learned model predicts the clean data Ë†ğ’™(ğ’›ğ‘¡, ğœƒ). One can show (Ho et al., 2020; Kingma et al., 2021) that the information required to encode sample ğ‘(ğ’™0ğ’™) using the diffusion model ğ‘(ğ’™0) is ğ‘¡ ). Additionally, we use ğ›¼2 ğ‘¡ +ğœ2 ğ‘¡ /ğœ2 KL(cid:104) ğ‘(ğ’™0ğ’™)(cid:12) (cid:12)ğ‘(ğ’™0) (cid:105) KL(cid:104) ğ‘(ğ’™0, . . . , ğ’™1ğ’™)(cid:12) = ğ”¼ğ‘¡U (0,1) (cid:104) dğœ† (ğ‘¡) dğ‘¡ (cid:105) (cid:12)ğ‘ğœƒ(ğ’™0, . . . , ğ’™1) exp ğœ† (ğ‘¡) 2 ğ‘¤(ğœ†ğ‘¡)ğ’™ Ë†ğ’™(ğ’™ğ‘¡, ğœƒ)2(cid:105) + KL(cid:2) ğ‘(ğ’™1ğ’™)(cid:12) (cid:12)ğ‘(ğ’™1)(cid:3), (2) where ğ‘¤(ğœ†ğ‘¡) = 1 is required for the bound to hold. However, in standard diffusion models more image-quality friendly weighting is chosen such as ğ‘¤(ğœ†ğ‘¡) = sigmoid(ğœ†ğ‘¡ ğ‘). This re-weighted ELBO formulation has the added benefit that the weighting is invariant to the choice of schedule ğœ† (ğ‘¡) (Kingma & Gao, 2023). Note that although the destruction process is applied to clean data point ğ’™, it only models up to the minimal noise level ğ’™0. This will be important later, as our prior model will output slightly noisy latents ğ’›0. If the VAE encoder does not predict single encoding but distribution ğ‘(ğ‘§ğ‘¥) the above ELBO is insufficient. Prior work has generalized the KL for the more complex case of arbitrary encoder distribution (Vahdat et al., 2021). Weighting diffusion ELBOs The above mentioned weighting means that diffusion models offer unique perspective on log-likelihood optimization. Their loss is decomposed over noise levels. This can be used for example to down-weight the loss contributions of imperceptible high frequency details. For latent prior however, the encoder could abuse this by encoding information at the most discounted noise levels. Therefore, VAE with diffusion prior should use unweighted ELBO loss ğ‘¤(ğœ† ğ‘§ (ğ‘¡)) = 1. 2 Unified Latents (UL): How to train your latents Figure 1 Schematic overview of our model, include the Encoder (ğ¸ğœƒ), the prior latent diffusion model (ğ‘ƒğœƒ), and the diffusion decoder model (ğ·ğœƒ). 3. Unified Latent Diffusion This section describes how to train Unified Latents. The first section covers the latent encoding which is regularized by diffusion prior using Eq. 1. Secondly, we describe how to design diffusion decoder which models the reconstruction term log ğ‘ğœƒ(ğ’™ğ’›0). Lastly, we describe the second stage of training where the encoder and decoder are frozen and new model is trained on the latents. An overview of inputs and outputs during training is visualized in Figure 1. Algorithm 1 Training Unified Latents Sample ğ’™ ğ‘data Encode the data ğ‘§clean = ğ¸(ğ’™, ğœƒ) Sample ğ‘¡ (0, 1), ğ (0, I) ğ’›ğ‘¡ = ğ›¼ğ‘§ (ğ‘¡)ğ’›clean + ğœğ‘§ (ğ‘¡)ğ Compute prior loss Lğ‘§ (ğœƒ) = Sample ğ‘¡ (0, 1), ğ (0, I), ğğ‘§ (0, I) ğ’›0 = ğ›¼ğ‘§ (0)ğ’›clean + ğœğ‘§ (0)ğğ‘§ ğ’™ğ‘¡ = ğ›¼ğ‘¥ (ğ‘¡)ğ‘¥ + ğœğ‘¥ (ğ‘¡)ğ Compute decoder loss Lğ‘¥ (ğœƒ) = Optimize (ğœƒ) = Lğ‘§ (ğœƒ) + Lğ‘¥ (ğœƒ) exp ğœ† ğ‘§ (ğ‘¡) 2 dğœ† ğ‘¥ (ğ‘¡) dğ‘¡ dğœ† ğ‘§ (ğ‘¡) dğ‘¡ exp ğœ† ğ‘¥ (ğ‘¡) ğ’›clean Ë†ğ’›(ğ’›ğ‘¡, ğœƒ)2 + KL(cid:2) ğ‘(ğ’›1ğ’™)(cid:12) (cid:12)ğ‘(ğ’›1)(cid:3) 2 ğ‘¤(ğœ† ğ‘¥ (ğ‘¡))ğ’™ Ë†ğ’™(ğ’™ğ‘¡, ğ’›0, ğœƒ)2 Algorithm 2 Sampling Unified Latents Sample ğ’›1 (0, I) Sample ğ’›0 ğ‘ğœƒ(ğ’›0ğ’›1) from diffusion base model Sample ğ’™1 (0, I) Sample ğ’™ ğ‘ğœƒ(ğ’™ğ’›0, ğ’™1) from diffusion decoder model 3.1. Encoding and Prior: Linking encoding noise and diffusion precision key design decision is how much precision to use when encoding the latent. In principle, continuous variable can encode infinite bits, and floating-point representations typically support 1632 bits 3 Unified Latents (UL): How to train your latents Figure 2 Unified Latents overview. An image ğ’™ is encoded to ğ’›clean. diffusion prior models the path from pure noise ğ’›1 to slightly noisy latent ğ’›0. This ğ’›0 is then used by diffusion decoder to reconstruct the image. The prior thus measures and regularizes the information content of ğ’›0. though encoder and decoder smoothness make it practically impossible to utilize this capacity in full. In standard VAEs, encoder noise limits information content; for example, Stable Diffusion learns this noise level. We take different approach: we explicitly link the encoder noise to the maximum precision of the prior diffusion model. Let ğ’›clean = ğ¸(ğ’™, ğœƒ) denote the deterministic latent encoding. The encoder should approximate the posterior ğ‘(ğ‘§ğ‘¥), which is typically parameterized by flexible distribution. However, following Vahdat et al. (2021), we find that learning flexible encoder distribution causes instability. We propose simpler approach: the encoder predicts single deterministic latent ğ‘§clean, which is then forward-noised to time ğ‘¡ = 0. We use final log-SNR of ğœ† (0) = 5, defining ğ‘(ğ’›0ğ‘§clean) = (ğ›¼0ğ‘§clean, ğœ0). For variance-preserving noise schedule, this corresponds to ğ›¼0 = sigmoid(+5) 1.0 and ğœ0 = sigmoid(5) 0.08. The KL term for the VAE loss is then: KL(cid:104) ğ‘(ğ’›0ğ’™)(cid:12) (cid:12)ğ‘ğœƒ(ğ’›0) (cid:105) KL(cid:104) (cid:104) = ğ”¼ğ‘¡ ğ‘(ğ’›0, . . . , ğ’›1ğ’™)(cid:12) exp ğœ† ğ‘§ (ğ‘¡) dğœ† ğ‘§ (ğ‘¡) 2 dğ‘¡ (cid:12)ğ‘ğœƒ(ğ’›0, . . . , ğ’›1) (cid:105) ğ‘¤(ğœ† ğ‘§ (ğ‘¡))ğ’›clean Ë†ğ’›(ğ’›ğ‘¡, ğœƒ)2(cid:105) + KL(cid:2) ğ‘(ğ’›1ğ’™)(cid:12) (cid:12)N (0, I)(cid:3) . (3) Thus, the latent ğ‘§0 is sampled using learned mean and fixed diagonal noise. 3.2. Decoding: diffusion decoder The decoder is also diffusion model, but operating in the image space with ğ’™ğ‘¡ = ğ›¼ğ‘¡ ğ’™ + ğœğ‘¡ğ. The reconstruction loss can be written as: log ğ‘ğœƒ(ğ’™ğ’›0) KL(cid:104) (cid:12)ğ‘ğœƒ(ğ’™0, . . . , ğ’™1ğ’›0) (cid:105) ğ‘(ğ’™0, . . . , ğ’™1ğ’™)(cid:12) (cid:104) dğœ† ğ‘¥ (ğ‘¡) dğ‘¡ exp ğœ† ğ‘¥ (ğ‘¡) 2 = ğ”¼ğ‘¡U (0,1) ğ‘¤ğ‘¥ (ğœ† ğ‘¥ (ğ‘¡))ğ’™ Ë†ğ’™(ğ’™ğ‘¡, ğ’›0, ğœƒ)2(cid:105) (4) The key distinction is that the decoder network ğ·ğœƒ = Ë†ğ’™(ğ’™ğ‘¡, ğ’›0, ğœƒ) conditions on both the noisy data ğ’™ğ‘¡ and the latent ğ’›0. Since the decoder does not affect ğ’™, the prior term KL[ ğ‘(ğ’™1ğ‘¥)N (0, I)] can be ignored from the loss. In contrast with the prior, the decoder loss can be re-weighted Decoder weighting and loss factor ELBO. By discounting low noise levels, high frequency features will always be modelled by the decoder because the cost per bit of information is lower. For example, in many experiments we use the sigmoid loss (Hoogeboom et al., 2024; Kingma & Gao, 2023), ğ‘¤(ğœ† (ğ‘¡)) = sigmoid(ğœ† (ğ‘¡) ğ‘). 4 Unified Latents (UL): How to train your latents Figure 3 Decoder weighting on ğ-mse, ğ‘¤ğ(ğœ†ğ‘¡) = ğ‘lf sigmoid(ğ‘ ğœ†ğ‘¡), showing which noise levels are penalized (via loss factor ğ‘lf = 1.6 in this case) and which noise levels are discounted. In theory, for weightings above 1 the latent model is preferred and for weightings below 1 the decoder is preferred. In practise, the decoder will model information even if the weighting is slightly above 1. Even with equal weighting, literature has shown that it is difficult to use the latent space in VAEs when the decoder is powerful, phenomenon referred to as posterior collapse (Razavi et al., 2019). For that reason, we up-weigh the decoder loss with loss factor (which is equivalent to down-weighting the KL-term). See Figure 3 for combined view of weighting and the loss factor. We find that we only need small loss factor (1.3 to 1.7 in most experiments). Thus, in our experiments we set the decoder ELBO weighting using the loss factor (ğ‘lf) and sigmoid bias ğ‘. These 2 hyper-parameters effectively control the amount of information in the latents. higher information latent naturally leads to better reconstruction quality. However, by using more informative latent we defer more of the modelling complexity to the base model. 3.3. Base model: Stage 2 training In principle we could use the prior diffusion model as described above to generate ğ’›0 and subsequently sample ğ‘(ğ’™ğ’›0) using the diffusion decoder. However we find that prior trained using an ELBO loss does not produce good samples (see App. B). Because the prior can only be trained on an ELBO weighting in stage 1, it places equal weight on low-frequency and high-frequency content in the latent. Therefore, we find that performance can be improved considerably by retraining the prior model as base model with sigmoid weighting. Because only frozen encoder is required during this stage, the base model size and batch size can be much larger than in stage 1. The training of the base model largely follows the same procedure as existing Latent Diffusion Models (Rombach et al., 2022). The only difference is that Unified Latents have fixed amount of noise so there is fixed logsnr max ğœ† (0) for the base model which is the same as the one used in the prior. This deviates from the standard approach where the final logsnr is hyper-parameter and we use the final prediction Ë†ğ’› instead of ğ’›0 as the sampled latent. There are alternative design choices that allow for single stage training. In this case the prior model will already achieve better performance. This requires different weightings of decoder and prior losses, and are discussed in Appendix B. 5 Unified Latents (UL): How to train your latents 4. Related Work Our work combines diffusion-based decoding with diffusion-based priors to learn latent representations optimized for generation. We review the most relevant prior work below. Diffusion Decoders Several works have explored using diffusion models as decoders in VAE-like frameworks. DiffuseVAE (Pandey et al., 2022) trains conventional MSE autoencoder first, then finetunes diffusion decoder using the original decoders output as conditioning. SWYCC (Birodkar et al., 2024) and ğœ–-VAE (Zhao et al., 2025) train latents with diffusion decoder, but still rely on channel bottleneck for regularization rather than learned prior. DiVAE (Shi et al., 2022) combines diffusion decoder with discrete VQ-VAE tokens. In contrast, our approach uses continuous latents regularized by diffusion prior, providing interpretable control over the bitrate. Diffusion Priors LSGM (Vahdat et al., 2021) jointly trains diffusion prior in VAE framework, but requires separate encoder entropy term ğ”¼ğ‘(ğ’›0 ğ’™) log ğ‘(ğ’›0ğ’™) that introduces training instability. Our approach sidesteps this by using deterministic encoder with fixed noise, absorbing the encoder distribution into the diffusion forward process. This yields simpler two-term objective (decoder loss + prior loss) while maintaining tight bound on latent information. Diffusion Decoder and Prior DiffAE (Preechakul et al., 2022) uses diffusion for both encoding and decoding, but its latent comes from pre-trained semantically meaningful encoder rather than being optimized for generation quality. Our work differs by jointly training the encoder, prior, and decoder, with the explicit goal of maximizing generation efficiency. Latent Diffusion and Efficient Autoencoders The original Latent Diffusion Model (Rombach et al., 2022) uses GAN-trained autoencoder with channel-bottlenecked latents and small KL penalty, but provides no principled way to control latent information. Recent work on efficient autoencoders (Chen et al., 2024) achieves high compression ratios but does not address the interplay between autoencoder design and downstream diffusion modeling. Token-based approaches like TiTok (Yu et al., 2024) compress images to discrete tokens, trading reconstruction quality for faster sampling. Lastly, pretrained semi-supervised encoders like DINO (Caron et al., 2021) can be used to focus on semantically meaningful representations (Shi et al., 2025; Zheng et al., 2025) and obtain impressive generation quality metrics. downside of these approaches is that PSNR scores are low ( 20) causing reconstructions to appear different from their original in particular on high-frequency details. Latents from Self-Supervised Representation number of recent works have replaced the AutoEncoder all-together and model semi-supervised representation like SigLip or Dino instead (Shi et al., 2025; Zheng et al., 2025). 5. Experiments We evaluate Unified Latents on their ability to improve pre-training efficiencythe relationship between training compute and generation quality. We conduct experiments on ImageNet-512 and Kinetics-600 for direct comparison with prior work, and include scaling studies on large-scale text-to-image and text-to-video datasets. We focus on pre-training efficiency and avoid fine-tuning stages (such as Unified Latents (UL): How to train your latents aesthetics fine-tuning) or MS-COCO evaluations, as these introduce confounding factors unrelated to the quality of the learned latents. 5.1. Model Architecture Our encoder and decoder models use 2x2 patching to save compute. The encoder is Resnet model with [128, 256, 512, 512] channels and 2 residual blocks for downsampling stage and 3 blocks in the final stage. The prior model is single level ViT with 8 blocks and 1024 channels. In the base model we use 2 stage ViT with [512, 1024] channels [6, 16] blocks. The base model is regularized with dropout rate of 0.1 in both stages. The decoder is UVit model (Hoogeboom et al., 2024) with channel counts [128, 256, 512] in the convolutional down-sampling and up-sampling stages. The transformer in the middle has 8 blocks and 1024 channels. We use dropout rate of 0.1 for regularization. 5.2. Evaluation Metrics To assess the quality of samples and autoencoder reconstructions we use FID and FVD for images and videos, respectively. When sampling from base model we denote the FID as gFID. For reconstruction we use the term rFID and use the same samples from the dataset to compute reconstructions and the FID references. This breaks the convention of standard FID where the reference statistics are computed on the entire train (or sometimes eval) dataset. The same approach and the rFID and gFID convention is used by the majority of existing literature. We also use PSNR (Peak Signal-to-Noise Ratio) to measure how closely reconstructions match their originals. This complements FID, since reconstruction can be in-distribution (low FID) while still differing substantially from the original image. Additionally, because our models provide an upper bound on latent information, we report the estimated bits per dimension (bpd) in the latent space. For computational cost, we count FLOPs for all linear projections and attention operations. For training cost, we multiply by 3 to approximate the cost of computing gradients. 5.3. Image Generation In this section we test Image Generation performance. The autoencoder operates on resolution of 512 512 and downsamples 16 16 to produce 32 32 latents. For each experiment, the optimal latent bitrate is chosen so that gFID is highest (for details see Sec. 5.4). ImageNet First we show scaling performance of Unified Latents in training flops vs generation FID (Figure 4) on ImageNet5121. There are several important things to note. Firstly, UL outperforms other approaches in literature in training cost vs generation performance trade-off, which means it is the most efficient pre-training approach on this dataset. Secondly, for fair comparison we train the exact architecture (a 2-level ViT) on Stable Diffusion latents (baselines small SD and medium SD). Here we see that UL is outperforming the baselines to greater extend. We find that patching is detrimental to the performance of the base model. The UNet (SD) baseline is small model that uses an additional convolution stack instead of patching the SD latents. 1The original time of writing was March 2025, in the meantime other (often complimentary) approaches may have discovered. 7 Unified Latents (UL): How to train your latents small (SD) medium (SD) DiT-XL/2 (interval) Previous Baselines (SD) Unified latents (ours) 5 3 UNet (SD) 1.5 UL, small (tti AE) SiD2, small EDM2-S (interval) UL, small UL, medium (tti AE) UL, medium RAE EDM2-XXL (interval) SiD2, flop 1 0. 0.5 1.0 1.5 2.0 2.5 3. 3.5 Training cost (zettaflops per model) Figure 4 FID vs. training cost on ImageNet-512. UL outperforms all other approaches on base training compute versus generation equality We assume that one training iteration is three times as expensive as evaluating the model (i.e., forward pass, backprop to inputs, backprop to weights). Note that auto-encoder training cost is not included. AutoEncoder transfer Previous work like Stable Diffusion uses auto-encoder that is trained on another dataset than ImageNet. To test the effect of using an out-of-distribution autoencoder we also train base model on Unified Latents trained on an internal text-to-image dataset (tti AE). We did not observe significant difference in training efficiency. In-distribution autoencoders seem slightly better when training small base models with low information latent. Figure 5 selection of samples from text-to-image trained with Unified Latents Text-To-Image In order to test our methods at scale we trained multiple AutoEncoders on internal Text-To-Image datasets sweeping over loss factor (1.25-1.7). For each AutoEncoder we train base models add various sizes (100, 300, and 970 GFlops). To evaluate these models we take 30k samples without guidance and compute clip and FID scores against the training set. Figure 5 shows some hand-picked samples from one of the large models. See Figure 10 for additional and non cherry picked samples. Figure. 6 shows how AutoEncoders with low latent bitrate lead to better image quality as measure 8 Unified Latents (UL): How to train your latents latents gFID@30K clip UL (LF=1.5) Pixel (no latents) StableDiffusion 4.1 5.0 6.8 27.1 27.0 27.0 Table 1 Generation quality and text alignment for text-to-image models trained with Unified Latents, pixel diffusion (no latents), and StableDiffusion latents. Figure 6 Image generation quality (left) and text alignment (right) against AutoEncoder Loss Factor for various base model sizes. by gFID. This effect is more pronounced for smaller models. The text alignment (clip) on the other hand suffers slightly from very low loss factors even for smaller models. Indicating that perhaps the decoder would also benefit from text conditioning. However, we also note that text-alignment can be easily improved by applying guidance. In Table 1 we compare the text-to-image models trained with Unified Latents to models trained with pixel diffusion (Hoogeboom et al., 2024), and Stable Diffusion latents (Rombach et al., 2022). We add additional convolution blocks to deal with the higher resolution but do not compensate the UL model for the additional flops used by the other models. The UL el significantly outperform these baselines on perceptual quality and has slightly better text-alignment. 5.4. Latent bitrate tuning Recall that there is reconstruction FID vs generation FID trade-off. The goal of the combined auto-encoder and base model stack is to achieve the highest gFID possible. On the other hand, it is trivial to obtain very good rFID by allowing more and more bits to flow through the latents. This is problem, because high bitrate latents will be more difficult to model. One way to control the amount of bits in the latent is by changing the loss factor (see Table 2 and Fig. 7). Note that for smaller models, typically lower bitrates are optimal: even though rFID (and thus decoding) is somewhat worse, the smaller capacity models can only fit low bitrate latents properly. On the contrary, larger models are less sensitive to latent bitrates, and can achieve even better performance on higher bitrates. An alternative method to tune latent bitrates is via the bias in the sigmoid loss, which is entangled with the loss factor. For lower biases, one typically requires higher loss factors. In Figure 8 we show sweep over decoder bias and loss factors, which demonstrates that several settings give roughly equal performance / latent bitrate curves. Unified Latents (UL): How to train your latents Table 2 Increasing the loss factor leads to improved reconstruction metrics (rFID, PSNR) at the cost of increased bitrate in the latent encoding. For small models, the loss factor (and bits in the latent) matter lot. For larger base models the loss factor is less sensitive. LF bits/pixel rFID@50k PSNR gFID (small) gFID (medium) 1.3 1.5 1.7 1.9 2.1 0.035 0.059 0.083 0.101 0.116 0.79 0.47 0.36 0.31 0.27 25.7 27.6 28.9 29.6 30.1 1.42 1.54 1.77 2.02 2.38 1.37 1.31 1.38 1.45 1. Figure 7 Reconstruction quality vs loss factor. Fine details like small text are lost for low bitrate latents. 5.5. Latent shape For Latent Diffusion Models the downsampling factor and latent channels are the main factors determining the information bottleneck (Rombach et al., 2022). In this experiment we use fixed spatial downsampling to 32 32, and vary the number of latent channels (from 4 to 64). The results are in Table 3. From the results we conclude that Unified Latents are mostly insensitive to the number of latent channels. Only for very low latent channel count the encoder is unable to pass enough information to enable good reconstructions (4 and 8). In the next experiment, we vary the spatial downsampling (8ğ‘¥ to 32ğ‘¥), while using fixed number of latent channels (32). The results are in Table 4. First, we observe that 32 channels work well for any of the spatial dimensions of the latents. Second, we see that the rFID results are similar for 16ğ‘¥ and 8ğ‘¥ spatial downsampling (to 32 32 and to 64 64), while it seems that the former is easier to model for the decoder, resulting in lower gFID numbers. 5.6. L2 Regularization It can be cumbersome to train 2 diffusion models simultaneously. Here we find that for slight decrease in performance it is also possible to first train the encoder using diffusion prior while placing l2 regularization on the decoder and different loss weightings (see Table 5). This experiment may raise another question: How about simpler version where the latents are regularized by an l2 loss / normal prior, as is typical for VAEs? We find that training with VAE with normal prior requires higher bitrate latents to reach good reconstruction quality. This then results in more difficult to learn latents and worse gFID. 10 Unified Latents (UL): How to train your latents Figure 8 Image quality for various latent bitrates (FID vs bits/pixel) for small model variant. Left: generation (gFID). Middle: reconstruction (rFID). Right: reconstruction PSNR. We sweep over sigmoid shift and loss factor. For each shift, we sweep over loss factors [1.5, 1.75, 2., 3., 4.]. # chan rFID gFID@50K 4 8 16 32 7.19 1.53 0.54 0.42 0.48 - - 1.76 1.60 1.77 Table 3 FID metrics on ImageNet512 are insensitive to latent channel count. The AE is unable to obtain good reconstruction quality (rFID) with to few channels ( 8). 5.7. Video Generation Kinetics600 In this experiment we show that our method outperforms literature on k600 on training cost vs FVD tradeoff. Here we use 4 8 8 downsampling for 16 frames of 128 128 kinetics videos. Following Video Diffusion, we condition on 5 frames and generate 11 frames. For MAGVIT and W.A.L.T. due to tokenization choices the models operate on 17 frames, temporal latent dimensions of 5, and FVD is measured on 5-12 generations. To make comparison more fair, we discard the extra token of processing in the FLOP computation of these baselines. Here again, UL outperforms other approaches on training cost vs FVD performance (see Figure 9). Note that the small model already achieves 1.7 FVD, whereas the medium model achieves 1.3 FVD which is currently SOTA. 5.8. Ablations In this section we aim to ablate our approach by removing the key innovations. We also consider the classic VAE setup where the encoder is allowed to predict mean and variance. The results are listed in Table 6. Firstly (A), we want to make sure the prior improves and regularizes the latents. To test this we added stop-gradient to the prior input so we still get bitrate estimate but the encoder no longer receives gradient with respect to the prior. Instead, we regularize the latent with strongly discounted KL to (0, ğ¼) like prior works (Rombach et al., 2022). To get reasonable bitrate and gFID we must reduce the latent channels. The best result reported here uses 8 latent channels vs. 32 in the baseline. Secondly (B), we ablate the noisy latents by using ğœ† ğ‘§ (0) = 10 which corresponds to adding very small amount of noise (ğœ 0.007). At this precision the prior fails to accurately model the bitrate 11 Unified Latents (UL): How to train your latents latent shape (â„ ğ‘¤ ğ‘) rFID@50K, gFID@50K 64 64 32 32 32 32 16 16 32 0.40 0.41 1.41 2.12 1.63 1. Table 4 FID metrics on ImageNet512 for AutoEncoders with spatial downsampling factors between 8x and 32x. Prior Reconstruction loss Latent bpd Latent bpd with prior with base model rFID@50K gFID@50K Diffusion Diffusion Diffusion MSE Normal Diffusion 0.079 0.072 0.39 0.079 0.072 0.26 0.86 1.1 0.83 1.4 2.4 2.5 Table 5 Ablations on the auto-encoder training. of the latent and the loss is reduced by simply modelling most information in the decoder. The reconstructions (rFID) are too low quality to train useful base model. Thirdly (C), we test what happens if we train on text-to-image dataset rather than ImageNet. rFID is strongly affected while generation still works well. Other work that trains autoencoders directly on ImageNet data has also reported very low rFID scores (Chen et al., 2024). We hypothesize that this is mostly caused by minor differences in high-frequency statistics that FID seems overly sensitive to compared to human perception. Lastly (D), we consider more traditional VAE setup with an encoder that predicts mean and variance. Prior work (Vahdat et al., 2021) shows that the KL term can be generalized to arbitrary distribution. The generalization adds two entropy terms subtracting the encoder entropy from the entropy. For an encoder distribution ğ‘(ğ’›cleanğ‘¥) = (ğœ‡ğ‘§, diag(ğœ2 ğ‘§ )) the extra terms reduce to Lğ‘’ = 1 log (cid:2)ğœ2 ğ‘§ ğ‘’ğœ† ğ‘§ (0) + 1(cid:3) . (5) For the noisy latent setting ğœ† ğ‘§ (0) = 5 we find that the learned noise quickly drops to 0 and the model becomes unstable. For high precision latents ğœ† ğ‘§ (0) = 10 the encoder does learn to inject additional noise into the latent. The estimate of the KL term is high variance as reported before (Vahdat et al., 2021). The gFID is worse than the baseline. Thus, we conclude that the fixed encoder variance is useful simplification that increases both stability and performance. 6. Discussion Larger base models benefit from more informative latents. natural direction for future work is to establish scaling laws for Unified Latents that predict the optimal bitrate given training budget. Such scaling laws would depend on implementation details including dataset, evaluation metrics, and model architecture, and would be best studied in the context of production-scale foundation models. While this work focuses primarily on images with some extension to video, the Unified Latent framework appears broadly applicable. With discrete (diffusion) decoder discrete data like text could in theory be compressed with latents as well. 12 Unified Latents (UL): How to train your latents Table 6 Ablations study on Unified Latents components. bits/pixel rFID@50k gFID@50k UL baseline (LF=1.5) A. prior model B. noisy latents C. ImageNet data D. learned variance 0.059 0.121 0.008 0.034 0.060 0.47 1.81 28.27 1.37 0.69 1.54 7.80 - 1.63 1.81 Previous Unified Latents (ours) Video Diffusion RIN MAGVIT-v2 W.A.L.T. UL (small) UL (medium) FVD 10 5 3 2 0.0 0.5 1.0 Training cost (zettaflops, base model only), Kinetics-600 Figure 9 FVD vs. training cost on Kinetics-600. Plotted until convergence. 6.1. Limitations Existing literature and this work show trend towards less-informative latents (measured by bitrate or reconstruction PSNR) being easier to model. To what extend are weaker latents moving part of the modelling problem toward the decoder? This work uses U-Net diffusion models, while most prior work uses GAN based decoder with discriminator loss but without noise input (Rombach et al., 2022). diffusion decoder is strictly more powerful than such GANs because it predicts distribution rather than single image. However, the mode-collapsing nature of GAN training might help this class of models producing better looking images with better rFID scores. Comparison between latent diffusion models is further complicated by differences in AutoEncoder training data. The original Stable Diffusion autoencoder was trained on large-scale web dataset (Rombach et al., 2022), whereas most of our experiments use only ImageNet. Semi-supervised approaches (Caron et al., 2021; Shi et al., 2025; Zheng et al., 2025) introduce encoders trained on large external datasets, making direct comparison even more challenging. Finally, diffusion decoders are an order of magnitude more expensive to sample from than GAN based decoders. Without an additional distillation step for the decoder, the computational cost of using Unified Latents is significantly higher than standard LDM. 13 Unified Latents (UL): How to train your latents 6.2. Conclusion In summary, we have demonstrated that latent representations can be effectively learned by jointly training an encoder, diffusion prior, and diffusion decoder. This approach outperforms existing methods in both training efficiency and generation quality. Unified Latents provide stable, interpretable control over latent information through simple hyper-parameters, making the reconstructionmodeling trade-off explicit. We believe this principled approach to latent design will prove valuable as latent diffusion models continue to scale. 14 Unified Latents (UL): How to train your latents"
        },
        {
            "title": "References",
            "content": "Birodkar, V., Barcik, G., Lyon, J., Ioffe, S., Minnen, D., and Dillon, J. V. Sample what you cant compress. Technical report, arXiv, 2024. URL https://arxiv.org/abs/2409.02529. Caron, M., Touvron, H., Misra, I., JÃ©gou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pp. 96309640. IEEE, 2021. Chen, J., Cai, H., Chen, J., Xie, E., Yang, S., Tang, H., Li, M., Lu, Y., and Han, S. Deep compression autoencoder for efficient high-resolution diffusion models. CoRR, abs/2410.10733, 2024. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS, 2020. Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., and Salimans, T. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23:47:147:33, 2022. Hoogeboom, E., Mensink, T., Heek, J., Lamerigts, K., Gao, R., and Salimans, T. Simpler diffusion (sid2): 1.5 FID on imagenet512 with pixel-space diffusion. CoRR, abs/2410.19324, 2024. Kingma, D. P. and Gao, R. Understanding the diffusion objective as weighted integral of elbos. CoRR, abs/2303.00848, 2023. Kingma, D. P., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. CoRR, abs/2107.00630, 2021. Pandey, K., Mukherjee, A., Rai, P., and Kumar, A. Diffusevae: Efficient, controllable and highfidelity generation from low-dimensional latents. Technical report, arXiv, 2022. URL https: //arxiv.org/abs/2201.00308. Preechakul, K., Chatthee, N., Wizadwongsa, S., and Suwajanakorn, S. Diffusion autoencoders: Toward meaningful and decodable representation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Razavi, A., van den Oord, A., Poole, B., and Vinyals, O. Preventing posterior collapse with delta-vaes. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 1067410685. IEEE, 2022. Shi, J., Wu, C., Liang, J., Liu, X., and Duan, N. Divae: Photorealistic images synthesis with denoising diffusion decoder, 2022. URL https://arxiv.org/abs/2206.00386. Shi, M., Wang, H., Zheng, W., Yuan, Z., Wu, X., Wang, X., Wan, P., Zhou, J., and Lu, J. Latent diffusion model without variational autoencoder, 2025. URL https://arxiv.org/abs/2510.15301. Vahdat, A., Kreis, K., and Kautz, J. Score-based generative modeling in latent space. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS, pp. 1128711302, 2021. 15 Unified Latents (UL): How to train your latents Yu, Q., Weber, M., Deng, X., Shen, X., Cremers, D., and Chen, L. An image is worth 32 tokens for reconstruction and generation. CoRR, abs/2406.07550, 2024. Zhao, L., Woo, S., Wan, Z., Li, Y., Zhang, H., Gong, B., Adam, H., Jia, X., and Liu, T. Epsilon-vae: Denoising as visual decoding, 2025. URL https://arxiv.org/abs/2410.04081. Zheng, B., Ma, N., Tong, S., and Xie, S. Diffusion transformers with representation autoencoders. CoRR, abs/2510.11690, 2025. 16 Unified Latents (UL): How to train your latents A. Additional samples Figure 10 Generations from the text-to-image model. Guidance is set to 2. Images are not cherrypicked. Used prompts: (1) couple gets caught in the rain, oil on canvas, (2) lone traveller walks in misty forest, (3) walking figure made out of water, (4) In the swamp, crocodile stealthily surfaces, revealing only its eyes and the tip of its nose as it moves forward, (5) fox dressed in suit dancing in park, (6) Pouring chocolate sauce over vanilla ice cream in cone, studio lighting, (7) An astronaut riding horse, (8) Aurora Borealis Green Loop Winter Mountain Ridges Northern Lights, (9) Sailboat sailing on sunny day in mountain lake, (10) dog driving car on suburban street wearing funny sunglasses. B. End-to-end latent training In addition to the 2-stage training approach described in this paper, we also tried training the encoder, decoder, and base diffusion model end-to-end in single stage. This can be done in two ways. In our first attempt we shifted the loss of the decoder diffusion model towards more noisy data, following Hoogeboom et al. (2024), combined with the standard ELBO loss on the base model. Both models can then be trained jointly in stable way, but we did not get FID below 2 using this approach. In second attempt we trained the base model with weighted ELBO loss that is equivalent to training this model with unweighted ELBO loss on data with additional added noise (Kingma & Gao, 2023). This means it is possible to train the decoder and base model jointly, using differently weighted ELBO losses on the base model and decoder, by randomizing the maximum log signal-to-noise ratio of the base model according to particular truncated logistic distribution. The decoder diffusion model is then modified to condition on the log-SNR of the latents, similar to the conditioning augmentation of Ho et al. (2022). Using the exact settings used in the 2-stage approach, but training in single stage, we achieved an FID of about 4 in 400k training steps."
        }
    ],
    "affiliations": [
        "Google DeepMind Amsterdam"
    ]
}