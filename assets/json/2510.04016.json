{
    "paper_title": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents",
    "authors": [
        "Thanapol Popit",
        "Natthapath Rungseesiripak",
        "Monthol Charattrakool",
        "Saksorn Ruangtanusak"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fluid voice-to-voice interaction requires reliable and low-latency detection of when a user has finished speaking. Traditional audio-silence end-pointers add hundreds of milliseconds of delay and fail under hesitations or language-specific phenomena. We present, to our knowledge, the first systematic study of Thai text-only end-of-turn (EOT) detection for real-time agents. We compare zero-shot and few-shot prompting of compact LLMs to supervised fine-tuning of lightweight transformers. Using transcribed subtitles from the YODAS corpus and Thai-specific linguistic cues (e.g., sentence-final particles), we formulate EOT as a binary decision over token boundaries. We report a clear accuracy-latency tradeoff and provide a public-ready implementation plan. This work establishes a Thai baseline and demonstrates that small, fine-tuned models can deliver near-instant EOT decisions suitable for on-device agents."
        },
        {
            "title": "Start",
            "content": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents Thanapol Popit Department of Computer Engineering KMUTT Bangkok, Thailand thanapol.popi@kmutt.ac.th Natthapath Rungseesiripak Innovation Lab SCBX Bangkok, Thailand natthapath.r@scbx.com Monthol Charattrakool Innovation Lab SCBX Bangkok, Thailand monthol.c@scbx.com Saksorn Ruangtanusak R&D SCBX Bangkok, Thailand saksorn.r@scbx.com AbstractFluid voice-to-voice interaction requires reliable and low-latency detection of when user has finished speaking. Traditional audio silence end-pointers add hundreds of milliseconds of delay and fail under hesitations or languagespecific phenomena. We present, to our knowledge, the first systematic study of Thai text-only end-of-turn (EOT) detection for real-time agents. We compare zero-shot and few-shot prompting of compact LLMs to supervised fine-tuning of lightweight transformers. Using transcribed subtitles from the YODAS corpus and Thai-specific linguistic cues (e.g., sentencefinal particles), we formulate EOT as binary decision over token boundaries. We report clear accuracylatency tradeoff and provide public-ready implementation plan. This work establishes Thai baseline and demonstrates that small, finetuned models can deliver near-instant EOT decisions suitable for on-device agents. Index TermsEnd-of-utterance detection, turn-taking, Thai, transformers, speech interfaces, real-time agents I. INTRODUCTION Semantic End-of-Turn Detection (EOT) is the task of predicting whether speaker has finished their conversational turn using purely the linguistic content of their utterance. Unlike traditional methods that rely heavily on acoustic cues like silence duration (pauses), such as the widely used Silero Voice Activity Detector (VAD) [1], this approach analyzes the transcribed text to understand if the sentence or thought is semantically complete. The primary importance of this task is to reduce latency and improve the naturalness of human-computer interactions. By accurately predicting the end of users turn before they fall silent, conversational agent can begin processing the request and formulating response immediately, leading to faster, more fluid, and less awkward conversations [2][4]. While research in Thai NLP has explored tasks such as sentence segmentation in written text, the specific problem of predicting conversational turn completion in spoken dialogue remains largely unaddressed. To the best of current knowledge, no prior work has explicitly tackled semantic end-of-turn detection for Thai. This highlights substantial research gap and presents an opportunity to extend dialogue modeling efforts to linguistically and culturally distinct context. Work conducted as part of the SCBX Summer Internship program. 1 0.9 0.8 1 Specialist (Thai) Generalist (Multilingual) Typhoon2-1B Qwen3-0.6B mDeBERTa-v3-base WangchanBERTa 0.7 0 0. 0.3 0.2 Latency (s) 0.4 0.5 Fig. 1: Accuracylatency trade-off for Thai EOT detection models (all Fine-tuned). Purple markers denote specialist (Thai-specific) models; blue markers denote generalist (multilingual) models. We study text-only EOT: given speech-to-text transcription, decide if the user intends to finish their conversational turn. Text-only EOT avoids tuning acoustic thresholds, reduces perceived delay, and aligns with sentence boundary detection. Our contributions: First Thai EOT benchmark. formal task and dataset recipe for Thai EOT using public subtitles, with Thaispecific analysis. Fine-tune vs zero-/few-shot comparison. We assess compact LLMs in zero/few-shot mode against fine-tuned encoders/decoders under the same evaluation protocol, emphasizing CPU latency (see Fig. 1). II. RELATED WORK A. End-of-Turn Detection Prior work often models acoustic-prosodic cues with auxiliary intent prediction [5]. Industrial EOT detectors typically gate on silence durations [1], [11]. Recent opensource systems learn semantic turn cues directly from audio (Smart Turn v2) or from conversation context (TEN Turn Detection) [8][10]. Turnsense fine-tunes 135Mparameter transformer for End-of-Utterance (EOU) detection [7]. TurnGPT [13] demonstrated that Transformer-based LMs trained on dialogue corpora can predict likely turn shifts, outperforming RNNs. Similarly, Masumura et al. (2018) [19] proposed dual LSTM models for Japanese dialogues, showing context from both speakers improves EOT detection. Aldeneh et al. (2018) further showed that predicting speaker intentions as an auxiliary task boosts EOT performance [5]. These works confirm that contextual language modeling is beneficial for turn-taking tasks. B. Decoder-Only Based EOT Detection Transformer architectures underpin modern LLMs and specialized EOT detectors. Within this paradigm, decoderonly models can be leveraged in two primary ways: zeroshot/few-shot prompting and fine-tuning. For zero-shot EOT detection, compact models like Qwen3-0.6B [23] and Llama3.2-Typhoon2-1B [24] serve as efficient generative backbones. While their larger, instruction-tuned counterparts (Qwen3-8B, Llama3.1-Typhoon2-8B) generally provide superior performance, they do so at the cost of greater latency. Alternatively, fine-tuning these models for EOT marks notable conceptual shift. Rather than framing the task as discrete binary classification problem, this approach models turn completion as continuous, probabilistic prediction. [13], [14] Such formulation aligns naturally with the incremental, real-time dynamics of spoken dialogue. In this view, the system is not explicitly designed as an EOT detector; instead, generative language model implicitly learns the statistical likelihood of turn-shift at any point in the conversation. The underlying principle is to treat the end of conversational turn as simply another predictable event within sequence of language. [12] This is operationalized by introducing special token into the models vocabulary. During fine-tuning on dialogue corpus, this token is inserted at speaker-change boundaries. With standard autoregressive next-token prediction objective, the model learns to assign high probabilities to the EOT token when turn is semantically complete. [13] In real-time interaction, the likelihood assigned to this special token at each step is interpreted as the probability that the speakers turn has concluded. This fine-tuning approach also presents trade-off between performance and efficiency. Smaller variants, such as fine-tuned Qwen3-0.6B, are ideal for on-device applications where low latency is critical. In contrast, mid-sized models like fine-tuned Llama3.1-Typhoon2-8B can strike more effective balance between predictive accuracy and resource consumption. C. Encoder-Only Based EOT Detection Encoder-based models excel in token-level sequence classification. WangchanBERTa (106M parameters) [26], pretrained on large Thai corpora, captures Thai-specific linguisEnd-of-Turn Detection Model ùë•1:ùë° Finished AI Speaking Unfinished AI Listening Fig. 2: Turn detection in full-duplex dialogue: given streaming transcript ùë•1:ùë° , the model decides whether the boundary at position ùë° is an EOT (end) or Not-EOT (notend), which then dictates if the AI starts speaking or continues listening. tic phenomena like particles and implicit sentence boundaries. mDeBERTa-v3-base (276M parameters) [25] introduces disentangled attention mechanisms and multilingual coverage, supporting robust performance across Thai and English. These encoders are optimized for fine-tuning and efficient CPU inference, making them strong baselines for real-time EOT detection. An encoder-based paradigm approaches semantic end-ofturn detection as single, holistic classification task applied to complete unit of speech. [16], [17] The process generally unfolds as follows: segment of dialoguetypically full utterance captured after pauseis prepared as the models input sequence. [18] special classification token is prepended to the sequence, which is then processed by an encoder model such as BERT [28]. The encoder operates bidirectionally, producing contextualized representations for all tokens, with the final hidden state of the [CLS] token serving as an aggregated semantic embedding of the entire input. This representation is subsequently passed through lightweight classification head, often implemented as linear projection followed by sigmoid or softmax activation. The output is binary decision indicating whether the input segment constitutes an end-of-turn (EOT) or not-end-ofturn (non-EOT) [15]. By treating the problem in this way, encoder-based models rely on complete context windows, offering global judgment on turn completion rather than the incremental, probabilistic assessments characteristic of decoder-based approaches. III. TASK AND DATA A. Problem definition Given streaming transcript ùë•1:ùë° , decide whether boundary at position ùë° is an EOT (end) or not-end as shown in Figure 2. We treat subtitle line ends (and sentence-final punctuation when present) as positive labels. B. Dataset construction We derive data from the YODAS corpus [6], filtering to retain only Thai utterances. Raw subtitles include substantial noise: songs, advertisements, and non-dialogue content. We therefore applied several preprocessing steps: Regex filtering: Retain utterances containing Thai characters and within reasonable length bounds. Noise filtering with LLM: large instruction-tuned model (Typhoon-v2.1-12B-Instruct [24]) was prompted to remove lines judged to be songs, advertisements, or otherwise irrelevant to conversational turn-taking. Sentence segmentation: The same LLM was further used to split subtitle lines into sentence-like units, improving alignment with potential turn boundaries. C. Labeling strategy For decoder models, each sentence is simply terminated with the models existing EOT token, enabling direct likelihood estimation at run time. For encoder models, we convert each sentence into positive and negative examples: if the text length is sufficient, we cut it at the middle and label the first segment as not-end and the full sentence as end. This produces balanced classification dataset without requiring explicit punctuation heuristics. D. Splits and statistics After preprocessing, the dataset contains approximately 59k Thai sentences. We split the data into 80% training, 10% validation and 10% testing. IV. MODELS We evaluate four classes of models for text-only Thai Endof-Turn (EOT) detection as shown in Figure 3: (i) zero-shot and few-shot prompting with decoder-only models, which serve as baseline to measure intrinsic model capabilities; (ii) zero-shot thresholding which determines the end of conversational turn based on the probability of the native stop token, using fixed threshold value; (iii) fine-tuned encoder-only models trained for explicit classification; and thresholding with fine-tuned decoder-only (iv) zero-shot models adapted through generative training. Across these paradigms, we compare the performance of Thai-specific specialist models against powerful multilingual generalist ones. A. Model Selection For the encoder paradigm, we selected two models to create direct comparison. WangchanBERTa-base [26] is pre-trained exclusively on massive Thai corpus. It was selected to test the hypothesis that specialized, in-domain knowledge is essential for capturing the nuanced linguistic features of EOT cues in Thai. mDeBERTa-v3-base [25] is our multilingual baseline. It allows us to determine if more advanced generalist architecture can overcome lack of language-specific pre-training to match or exceed the performance of the specialist model. For the decoder paradigm, we selected models from two distinct scalesa compact and mid-size version of each familyto assess how parameter count impacts performance and efficiency. Llama3.2-Typhoon2-1B and Llama3.1-Typhoon28B [24] are our Thai-specialist models. Based on modern Llama architectures, they have undergone extensive pre-training and instruction tuning on large corpora of Thai text, allowing us to assess the impact of model scale on this language-specific task. Qwen3-0.6B and Qwen3-8B [23] are chosen as their multilingual counterparts. These models are recognized for strong cross-lingual performance and architectural efficiency. Including both compact (0.6B) and mid-size (8B) version allows us to evaluate the tradeoff between low-latency deployment and the higher accuracy offered by larger parameter count. B. Decoder Zero-shot and Few-shot Prompting To evaluate zero-shot and few-shot performance of our selected decoder models, we frame Thai text-only EOT detection as an instruction-based binary classification task. In the zero-shot setting, the model is prompted to classify text sequence as completed or incomplete turn [20] template is shown in Figure 5). To (the full zero-shot assess the impact of in-context learning, the few-shot setting extends this with small number of demonstrations [21]; the complete few-shot prompt is illustrated in Figure 6, covering common Thai conversational patterns. C. Encoder Model Training: Binary Classification The encoder models were fine-tuned on binary classification task designed to explicitly identify sentence completion. We formulated the training data by labeling each complete instruction from the training set as Finished. Concurrently, we generated one negative sample labeled Unfinished by truncating each instruction at its midpoint. This data augmentation strategy was designed to ensure the models learned the difference between partial and complete utterances. Both models were trained using the AdamW opti5. WangchanBERTa mizer [29] with learning rate of 2 was trained for 5 epochs with batch size of 256, while the larger mDeBERTa-v3-base was trained for 2 epochs with batch size of 64. The best-performing checkpoint was selected based on the highest weighted F1-score on the heldout validation set. 10 D. Decoder Model Training: Supervised Fine-Tuning In contrast, the decoder models were adapted using Supervised Fine-Tuning (SFT) with causal language modeling objective. We fine-tuned the models exclusively on the complete, Finished utterances from the YODAS Thai dataset [6], using the field text and maximum sequence length of 512 tokens. Training ran for single epoch with perdevice batch sizes of 16 for both training and evaluation (no gradient accumulation). We used the AdamW optimizer with Decoder LLM Zero/Few-shot Prompting Zero-shot Thresholding Fine-tuned Thresholding Encoder LM Fine-tuned Specialist (Thai) Generalist (Multilingual) Typhoon2 Qwen3 WangchanBERTa mDeBERTa-v3 Fig. 3: Model taxonomy: zero-shot, few-shot prompting with decoder LLMs, zero-shot thresholding with decoder LLMs, fine-tuned decoder LLMs, and fine-tuned encoder LMscrossed with specialist (Thai-specific) and generalist (multilingual) models. 10 5, weight 8-bit parameterization, learning rate of 2 decay 0.01, and cosine learning rate scheduler with 3% warmup; mixed precision was enabled with bfloat16. We evaluated and checkpointed every 100 steps. At inference turn completion was estimated by monitoring the time, probability assigned to the models native stop token at each boundary and applying threshold to this probability. E. Decoder Zero-Shot Thresholding In addition to supervised fine-tuning, we also evaluated decoder models in zero-shot setting. In this mode, the pretrained model weights are left unchanged, and turn completion is estimated solely from the probability mass assigned to the models native stop token at each boundary. decision threshold is then applied to this probability. Rather than fixing the threshold arbitrarily, we select the operating point by analyzing the ROC curve on the validation set and choosing the threshold that maximizes Youdens ùêΩ statistic (ùêΩ = TPR 1) [22]. This approach provides fair, model-agnostic comparison and ensures balanced trade-off between true positive and false positive detections without requiring additional training. TNR + V. EXPERIMENTS A. Experimental Setup We evaluated all models on the test split of our preprocessed YODAS dataset [6], which was held out from training and validation. Our primary metric is the F1-score of the positive (end) class, which provides balanced measure of precision and recall. We also report overall accuracy, precision, and recall for comprehensive performance. B. Evaluation Methods We assessed model performance across three distinct paradigms: Fine-tuning: Models were explicitly trained on our Thai EOT dataset. Encoders were trained as binary classifiers, while decoders were trained with causal language modeling objective to predict stop token. Instruction Prompting (Zero-shot and Few-Shot): Pretrained decoder models were evaluated using an instruction prompt to classify turns without any weight updates. This was performed in both zero-shot setting (no examples) and few-shot setting (five examples). The full prompts are shown in Figures 5 and 6. Zero-Shot Thresholding: The raw probability of the pretrained decoders native stop token was used as score. An optimal decision threshold was then applied to classify the turn. C. Operating Points and Calibration For methods that output probability score (fine-tuning and zero-shot thresholding), we report results at two operating points: Uncalibrated (0.5): fixed 0.5 threshold, showing outof-the-box performance. Calibrated (val-optimized): An optimal threshold ùúè found by maximizing Youdens J-statistic on the validation sets ROC curve. This simulates lightweight calibration step. D. Latency Measurement To assess real-time viability, average per-sample inference latency was measured on an Intel Xeon Platinum 8480+ CPU with batch size of one. E. Results and Analysis Our experimental results are summarized in Tables I, II, and III, with ROC curves presented in Figure 4. The findings reveal clear performance hierarchy across the different methods and model types. a) Effectiveness of Supervised Fine-Tuning: The primary finding is that supervised fine-tuning yields the highest performance. As shown in Table I, the fine-tuned Llama3.2Typhoon2-1B [24] achieves the top F1-score of 0.881, followed closely by the fine-tuned Qwen3-0.6B [23] (0.866) and the encoder-based mDeBERTa-v3-base [25] (0.861). These scores significantly outperform all zero-shot methods, demonstrating that while pretrained models have an intrinsic understanding of sentence structure, task-specific training is TABLE I: Main results at validation-optimized thresholds (Youdens ùêΩ) with CPU latency (batch=1) and parameter size. Latency measured on an Intel Xeon Platinum 8480+ CPU, averaged over 100 samples. Metrics are computed on the test set. Model F1 Acc Prec Rec Time (s) Size Zero-shot Thresholding Qwen3-0.6B Qwen3-8B Typhoon2-1B Typhoon2-8B Fine-tuned Qwen3-0.6B Typhoon2-1B WangchanBERTa mDeBERTa-v3-base 0.622 0.662 0.820 0. 0.866 0.881 0.784 0.861 0.570 0.651 0.823 0.820 0.861 0.874 0.792 0.855 0.571 0.664 0.864 0.833 0.861 0.860 0.844 0.853 0.684 0.659 0.780 0. 0.872 0.902 0.733 0.870 0.09 0.71 0.11 0.62 0.09 0.11 0.14 0.29 0.6B 8B 1B 8B 0.6B 1B 106M 276M TABLE II: Threshold sensitivity on the test set: Calibrated (val-optimized via ROC/Youdens ùêΩ) vs. Uncalibrated (fixed 0.5). ùúè is the validation-optimized threshold. Model ùùâ Calibrated (val-optimized) Uncalibrated (0.5) F1 Acc Prec Rec F1 Acc Prec Rec Zero-shot Qwen3-0.6B Qwen3-8B Typhoon2-1B Typhoon2-8B Fine-tuned Qwen3-0.6B Typhoon2-1B WangchanBERTa mDeBERTa-v3base 3.47 2.60 1.74 2.58 8 8 6 10 10 10 10 0.066 0.064 0.570 0.521 0.622 0.662 0.820 0.824 0.866 0.881 0.784 0. 0.570 0.651 0.823 0.820 0.861 0.874 0.792 0.855 0.571 0.664 0.864 0.833 0.861 0.860 0.844 0.853 0.684 0.659 0.780 0.816 0.872 0.902 0.733 0. 0.000 0.000 0.000 0.004 0.512 0.586 0.791 0.861 0.482 0.482 0.482 0.483 0.658 0.693 0.791 0.854 0.000 0.000 0.000 1.000 0.978 0.975 0.821 0. 0.000 0.000 0.000 0.002 0.346 0.418 0.763 0.875 TABLE III: Zero-/few-shot prompting results on the test set. Latency measured on an Intel Xeon Platinum 8480+ CPU, batch=1, averaged over 100 samples. Model Shots Acc Prec Rec Time (s) Zero-shot Prompting Qwen3-0.6B Qwen3-8B Typhoon2-1B Typhoon2-8B Few-shot Prompting Qwen3-0.6B Qwen3-8B Typhoon2-1B Typhoon2-8B 0 0 0 0 5 5 5 5 0.316 0.706 0.320 0.436 0.362 0.695 0.412 0.577 0.483 0.715 0.472 0.536 0.500 0.707 0.532 0. 0.671 0.757 0.380 0.690 0.638 0.758 0.632 0.725 0.483 0.715 0.472 0.536 0.500 0.707 0.532 0.619 0.179 1.568 0.379 2.356 0.259 2.497 0.414 2. essential for reaching state-of-the-art performance in EOT detection. b) Limitations of Zero-shot and Few-Shot Prompting: Our evaluation of instruction-based prompting (Table III) shows this method is impractical for real-time EOT, regardless of the number of examples provided. The best F1-score achieved is 0.706 (Qwen3-8B, zero-shot). While adding five examples (few-shot) improves performance, the gains are inconsistent and failed to lift any model to competitive level. More critically, the inference latency is prohibitively high (1.52.6 seconds) and increases with fewshot examples due to the longer input context. This makes instruction prompting, whether zero-shot or few-shot, orders of magnitude too slow for real-time conversational agent. c) The Importance of Threshold Calibration: The zeroshot into thresholding method reveals critical decoder models. As seen in Table II, their out-of-the-box performance with default 0.5 threshold is effectively zero (F1 0.000). This is because models raw probability for stop token is naturally very low. However, after calibrating the threshold on validation set, performance becomes respectable, with Llama3.1-Typhoon2-8B reaching insight R i o r 1 0.8 0.6 0. 0.2 0 0 0.2 0.4 0. 0.8 1 False Positive Rate Qwen3-0.6B Zero-shot (AUC=0.5698) Qwen3-8B Zero-shot (AUC=0.7063) Qwen3-0.6B Fine-tuned (AUC=0.9375) Typhoon2-1B Zero-shot (AUC=0.8987) Typhoon2-8B Zero-shot (AUC=0.8979) Typhoon2-1B Fine-tuned (AUC=0.9461) mDeBERTa-v3-base Fine-tuned (AUC=0.9344) WangchanBERTa Fine-tuned (AUC=0.8724) Pattern key: Zero-shot = dashed Pattern key: Fine-tuned = solid Fig. 4: ROC curves for Thai EOT detection models with thresholding on the test set. Color encodes model family and size (darker = larger model); line pattern encodes training regime (dashed = zero-shot, solid = fine-tuned). an F1-score of 0.824. This highlights that while zero-shot thresholding is feasible, it is not plug-and-play solution and mandates data-driven calibration step. In contrast, the fine-tuned encoder models are naturally well-calibrated for classification and perform robustly without this step. d) Comparing Specialist and Generalist Models: The comparison between specialist (Thai) and generalist (multilingual) models produced nuanced results. In zero-shot scenarios, the Thai-specialist Typhoon models clearly outperform the generalist Qwen3 models, suggesting that languagespecific pretraining is key for understanding turn-taking cues without fine-tuning. However, after fine-tuning, the powerful architecture of the generalist mDeBERTa-v3-base (0.861 F1) surpassed the Thai-specialist WangchanBERTa [26] (0.784 F1). This indicates that for this classification task, more modern and capable model architecture can be more important than language-specific pretraining, provided sufficient fine-tuning data is available. VI. DISCUSSION Our experimental analysis provides clear, actionable insights for developing real-time Thai EOT detection system. The results point to an optimal balance of accuracy and latency. a) Identifying the Optimal Accuracy-Latency Trade-off: For production systems, the central challenge is balancing performance with efficiency. Our results (Table and Figure 1) show that the fine-tuned Llama3.2-Typhoon2-1B [24] strikes an optimal balance. It delivers the highest accuracy (0.881 F1) while maintaining low CPU latency of 110ms, making it well-suited for real-time voice agents. For applications on highly constrained devices, the fine-tuned Qwen3-0.6B [23] is an excellent alternative, offering nearly comparable accuracy (0.866 F1) at an even faster 90ms. b) Deployment Considerations: Encoders vs. Decoders: The choice between an encoder and decoder model involves trade-off between deployment simplicity and maximum performance. For Peak Performance: fine-tuned decoder like Typhoon2-1B is preferable. It achieves the highest F1score but requires calibration step on validation set to determine the optimal decision threshold. For Simplicity and Robustness: fine-tuned encoder like mDeBERTa-v3-base [25] offers compelling alternative. It provides strong, reliable performance that is not sensitive to the decision threshold, making it drop-in solution that works well out-of-the-box. c) The Role of Thai-Specific Pretraining: Our findings indicate that Thai-specific pretraining is most critical when task-specific fine-tuning is not possible. In such zero-shot scenarios, model like Typhoon2 is the only viable option. However, if dataset for fine-tuning is available, the underlying power of the model architecture becomes more dominant factor, allowing strong generalist model to excel. d) Advancing Beyond Silence-Based Endpointing: This work demonstrates that lightweight, fine-tuned transformer can provide fast and accurate semantic EOT signal. This approach avoids the arbitrary delays of silence-based endpointing and correctly interprets linguistically complete utterances even when they contain pauses. By integrating such model, conversational AI systems can reduce response latency and achieve more natural, fluid turn-taking, significantly enhancing the user experience. VII. LIMITATIONS AND ETHICS Our labels inherit subtitle biases and timing drift; true conversational EOT may diverge from subtitle line breaks. We do not model acoustic cues (prosody, overlap), which matter in multi-party settings. Banking dialogs may include sensitive content; dataset handling must comply with privacy policies. VIII. CONCLUSION We introduce Thai-focused, text-only EOT formulation and compare fine-tuned small transformers against zero- /few-shot LLMs for real-time agents. This establishes practical baseline and complements audio-native turn detectors in open-source stacks. Future work will integrate lightweight prosodic features and extend to multi-party overlap. [18] G. Skantze, Turn-taking in Conversational Systems and HumanRobot Interaction: Review, Computer Speech & Language, vol. 67, p. 101178, 2021. doi: 10.1016/j.csl.2020.101178. [19] R. Masumura, T. Tanaka, A. Ando, R. Ishii, R. Higashinaka, and Y. Aono, Neural Dialogue Context Online End-of-Turn Detection, in Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue, Melbourne, Australia: Association for Computational Linguistics, 2018, pp. 224228. [Online]. Available: https://aclanthology. org/W18-5024/ [20] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with unified text-to-text transformer, Journal of Machine Learning Research, vol. 21, no. 140, pp. 167, Jan. 2020. [21] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, Language Models are Few-Shot Learners, arXiv preprint arXiv:2005.14165, 2020. https: //arxiv.org/abs/2005.14165. [22] W. J. Youden, Index for rating diagnostic tests, Cancer, vol. 3, pp. 3235, 1950. doi: 10.1002/1097-0142(1950)3:1<32::AIDCNCR2820030106>3.0.CO;2-3. [23] Qwen Team, Qwen3 Technical Report, in arXiv:2505.09388, 2025. [24] K. Pipatanakul, P. Manakul, N. Nitarach, W. Sirichotedumrong, S. Nonesung, T. Jaknamon, P. Pengpun, P. Taveekitworachai, A. NaThalang, S. Sripaisarnmongkol, K. Jirayoot, and K. Tharnpipitchai, Typhoon 2: Family of Open Text and Multimodal Thai Large Language Models, in arXiv:2412.13702, 2024. [25] P. He, J. Gao, and W. Chen, DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing, in arXiv:2111.09543, 2021. [26] L. Lowphansirikul, C. Polpanumas, et al., Pretraining transformerbased Thai Language Models, arXiv:2101.09635, 2021. [27] A. Grattafiori et al., The Llama 3 Herd of Models, in arXiv:2407.21783, 2024. [28] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding, in arXiv:1810.04805, 2019. [29] D. P. Kingma and J. Ba, Adam: Method for Stochastic Optimization, in arXiv:1412.6980, 2017. [30] T. Supnithi, K. Kosawat, M. Boriboon, and V. Sornlertlamvanich, Language Sense and Ambiguity in Thai, in PRICAI, 2004. [31] W. Aroonmanakun, Thoughts on Word and Sentence Segmentation in Thai, SNLP, 2007. [32] LiveKit Engineering, turn detection with transformers, engineering blog, 2024. https://blog.livekit.io/ using-a-transformer-to-improve-end-of-turn-detection. Improving voice AIs ACKNOWLEDGMENTS The authors would like to express their sincere gratitude to Weerin Chantaroje and Tutanon Sinthuprasith for their guidance and continuous support in providing the necessary resources for this work. We also wish to thank our colleagues, Peerawat Rojratchadakorn and Nut Chukamphaeng, for their helpful feedback and for sharing their experience relevant to this project. Additional thanks go to our colleagues in the SCBX R&D and Innovation Lab teams for their feedback, particularly on Thai conversational examples. Computing resources were generously provided by Hatari, which greatly facilitated our experiments and analyses. REFERENCES [1] Silero Team, Silero VAD: pre-trained enterprise-grade Voice Activity Detector (VAD), Number Detector and Language Classifier, in GitHub repository, GitHub, 2024. [Online]. Available: https://github. com/snakers4/silero-vad [2] O. Zink, Y. Higuchi, C. Mullov, A. Waibel, and T. Kobayashi, Predictive Speech Recognition and End-of-Utterance Detection Towards Spoken Dialog Systems, in arXiv:2409.19990, 2024. [3] G. Skantze and B. Irfan, Applying General Turn-taking Models to Conversational Human-Robot Interaction, in arXiv:2501.08946, 2025. [4] K. Mori, S. Kawano, A. F. G. Contreras, and K. Yoshino, Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model, in arXiv:2508.04403, 2025. [5] Z. Aldeneh, D. Dimitriadis, and E. M. Provost, Improving end-ofturn detection in spoken dialogues by detecting speaker intentions as secondary task, in arXiv:1805.06511, 2018. [6] X. Li, S. Takamichi, T. Saeki, W. Chen, S. Shiota, and S. Watanabe, YODAS: YouTube-Oriented Dataset for Audio and Speech, arXiv:2406.00899, 2024. [7] L. B. Hendra, Turnsense: Lightweight End-of-Utterance Detection Model, GitHub repository, 2025. https://github.com/latishab/ turnsense. [8] TEN Framework, TEN Turn Detection, GitHub repository, 2025. https://github.com/ten-framework/ten-turn-detection. [9] Pipecat AI, Smart Turn v2, model card (Hugging Face) and repository, 2025. https://huggingface.co/pipecat-ai/smart-turn-v2. [10] Pipecat AI, Smart Turn Overview, documentation, 2025. https:// docs.pipecat.ai/server/utilities/smart-turn/smart-turn-overview. [11] Speechmatics, End of Turn Detection, product docs, accessed 2025. https://docs.speechmatics.com/speech-to-text/realtime/end-of-turn. [12] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, Improving language understanding by generative pre-training, Technical Report, OpenAI, 2018. https://cdn.openai.com/research-covers/ language-unsupervised/language_understanding_paper.pdf [13] E. Ekstedt and G. Skantze, TurnGPT: Transformer-based Language Model for Predicting Turn-taking in Spoken Dialog, in Findings of the Association for Computational Linguistics: EMNLP 2020, Online: Association for Computational Linguistics, 2020, pp. 29812990. [Online]. Available: https://aclanthology.org/2020. findings-emnlp.268 [14] B. Jiang, E. Ekstedt, and G. Skantze, Response-conditioned Turntaking Prediction, in Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, pp. 1224112248, Jul. 2023. https://aclanthology.org/2023.findings-acl.776. [15] S. Singh, Building Smarter End-Of-Turn Detection for Conversational AI Using Transformer-Based Semantic Models, International Journal of Scientific Research in Engineering and Management, vol. 9, pp. 19, May 2025. doi: 10.55041/IJSREM47748. [16] A. Maier, J. Hough, and D. Schlangen, Towards Deep End-of-Turn Prediction for Situated Spoken Dialogue Systems, in Interspeech 2017, pp. 16761680, 2017. doi: 10.21437/Interspeech.2017-1593 [17] R. Meena, G. Skantze, and J. Gustafson, Data-driven models for timing feedback responses in Map Task dialogue system, Computer Speech & Language, vol. 28, no. 4, pp. 903922, 2014. doi: 10.1016/j.csl.2014.02.002. APPENDIX In this appendix, we provide the full text of the prompts used in our experiments. We include both zero-shot and few-shot variants. a) Zero-Shot Prompt.: The zero-shot prompt (Figure 5) provides only task instructions and output constraints. It emphasizes clarity and single-token outputs, serving as baseline for detecting complete vs. incomplete turns. Zero-Shot Prompting You are an expert in Thai conversational analysis. user's turn is considered 'Complete' if they finish sentence, ask question (e.g., ‡πÉ‡∏ä‡πÑ‡∏´‡∏°), or use concluding particle like ‡∏Ñ‡∏£‡∏ö, ‡∏Ñ‡∏∞, or ‡∏ô‡∏∞. turn is 'Incomplete' if it ends abruptly mid-thought or trails off. Analyze the transcript and respond with Complete or Incomplete. Your answer must be single token. Text: { {TRANSCRIPT_PREFIX} } Label: Fig. 5: The full zero-shot prompt b) Few-Shot Prompt.: The few-shot prompt (Figure 6) extends the zero-shot version with positive and negative examples. These cover common conversational patterns and help reduce ambiguity in borderline cases. Few-Shot Prompting You are an expert in Thai conversational analysis. user's turn is considered 'Complete' if they finish sentence, ask question (e.g., ‡πÉ‡∏ä‡πÑ‡∏´‡∏°), or use concluding particle like ‡∏Ñ‡∏£‡∏ö, ‡∏Ñ‡∏∞, or ‡∏ô‡∏∞. turn is 'Incomplete' if it ends abruptly mid-thought or trails off. Analyze the transcript and respond with Complete or Incomplete. Your answer must be single token. Examples: Text: ‡∏ä‡∏ß‡∏¢‡πÄ‡∏õ‡∏î‡∏ö‡∏ç‡∏ä‡πÉ‡∏´‡∏´‡∏ô‡∏≠‡∏¢‡∏Ñ‡∏£‡∏ö Label: Complete Text: ‡∏ß‡∏ô‡∏ô ‡πÇ‡∏°‡∏á‡∏Ñ‡∏£‡∏ö Label: Complete Text: ‡∏Ç‡∏≠‡πÇ‡∏≠‡∏ô‡πÄ‡∏á‡∏ô‡πÑ‡∏õ‡∏ó Text: ‡πÄ‡∏î Text: ‡∏á ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏ö Label: Complete Text: { {TRANSCRIPT_PREFIX} } Label: ‡∏¢‡∏ß‡∏Ç‡∏≠‡πÄ‡∏ä‡∏Ñ‡∏¢‡∏≠‡∏î‡∏Å‡∏≠‡∏ô‡∏ô‡∏∞ ‡πÅ‡∏•‡∏ß Label: Incomplete ‡∏ô‡∏ï‡∏î‡∏ö‡∏ï‡∏£‡πÄ‡∏Ñ‡∏£‡∏î‡∏ï‡πÉ‡∏ö‡∏ô ‡∏ö‡∏ç‡∏ä‡∏Ñ‡∏ì‡πÅ‡∏°‡πÅ‡∏•‡∏ß‡∏Å Label: Incomplete ‡πÇ‡∏≠‡∏ô‡πÄ‡∏á‡∏ô‡πÑ‡∏î‡∏ñ‡∏á‡∏Å Fig. 6: The full few-shot prompt"
        }
    ],
    "affiliations": [
        "Department of Computer Engineering KMUTT Bangkok, Thailand",
        "Innovation Lab SCBX Bangkok, Thailand",
        "R&D SCBX Bangkok, Thailand"
    ]
}