{
    "paper_title": "IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations",
    "authors": [
        "Zhibing Li",
        "Tong Wu",
        "Jing Tan",
        "Mengchen Zhang",
        "Jiaqi Wang",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Capturing geometric and material information from images remains a fundamental challenge in computer vision and graphics. Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view inputs, while still struggling with inherent ambiguities between lighting and material. On the other hand, learning-based approaches leverage rich material priors from existing 3D object datasets but face challenges with maintaining multi-view consistency. In this paper, we introduce IDArb, a diffusion-based model designed to perform intrinsic decomposition on an arbitrary number of images under varying illuminations. Our method achieves accurate and multi-view consistent estimation on surface normals and material properties. This is made possible through a novel cross-view, cross-domain attention module and an illumination-augmented, view-adaptive training strategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides large-scale multi-view intrinsic data and renderings under diverse lighting conditions, supporting robust training. Extensive experiments demonstrate that IDArb outperforms state-of-the-art methods both qualitatively and quantitatively. Moreover, our approach facilitates a range of downstream tasks, including single-image relighting, photometric stereo, and 3D reconstruction, highlighting its broad applications in realistic 3D content creation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 1 3 8 0 2 1 . 2 1 4 2 : r IDARB: INTRINSIC DECOMPOSITION FOR ARBITRARY NUMBER OF INPUT VIEWS AND ILLUMINATIONS Zhibing Li1 Tong Wu1 1 The Chinese University of Hong Kong Jing Tan1 Mengchen Zhang2,3 2 Zhejiang University Jiaqi Wang3 Dahua Lin1,3 3 Shanghai AI Laboratory https://lizb6626.github.io/IDArb/"
        },
        {
            "title": "ABSTRACT",
            "content": "Capturing geometric and material information from images remains fundamental challenge in computer vision and graphics. Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view inputs, while still struggling with inherent ambiguities between lighting and material. On the other hand, learning-based approaches leverage rich material priors from existing 3D object datasets but face challenges with maintaining multi-view consistency. In this paper, we introduce IDArb, diffusion-based model designed to perform intrinsic decomposition on an arbitrary number of images under varying illuminations. Our method achieves accurate and multi-view consistent estimation on surface normals and material properties. This is made possible through novel cross-view, cross-domain attention module and an illumination-augmented, view-adaptive training strategy. Additionally, we introduce ARB-Objaverse, new dataset that provides large-scale multi-view intrinsic data and renderings under diverse lighting conditions, supporting robust training. Extensive experiments demonstrate that IDArb outperforms state-of-the-art methods both qualitatively and quantitatively. Moreover, our approach facilitates range of downstream tasks, including single-image relighting, photometric stereo, and 3D reconstruction, highlighting its broad applications in realistic 3D content creation."
        },
        {
            "title": "INTRODUCTION",
            "content": "The color we perceive from objects results from complex interaction between the incident light, the material properties, and the surface geometry of those objects. Recovering these intrinsic properties from captured images is fundamental challenge in computer vision, enabling variety of downstream applications, such as relighting (Wimbauer et al., 2022) and photo-realistic 3D content generation (Zhang et al., 2024; Siddiqui et al., 2024). This decomposition process, commonly referred to as inverse rendering, is inherently ambiguous and severely under-constrained, particularly when only one or limited number of observation views are available. For instance, black pixel could indicate black base color or is the result of lacking incident light. Existing inverse rendering research can be broadly categorized into two approaches: optimizationbased methods and learning-based methods. The former category (e.g. NeRFactor (Zhang et al., 2021b), NVDiffRecMC (Hasselgren et al., 2022), TensoIR (Jin et al., 2023)) typically requires hundreds of multi-view images as input and focuses on optimizing intrinsic properties for each case independently. This approach involves time-consuming iterative optimization, often requiring several hours. Moreover, without incorporating strong priors on material distribution or addressing the inherent ambiguity between lighting and texture, these optimization-based methods frequently converge to sub-optimal solutions. This can lead to unrealistic decompositions, such as embedding lighting effects into intrinsic components, as shown in Fig. 1(b). To address these limitations, learning-based methods aim to extract useful priors from large-scale training datasets and perform fast inference in feed-forward manner. While many of these approaches focus on single-image decomposition, they tend to produce inconsistent intrinsic properties when applied across multiple views, as demonstrated in Fig. 1(a). Additionally, single-image models struggle to leverage complementary information from multiple views, making it difficult to resolve material ambiguities, which results in less accurate outcomes in more complex cases. Corresponding authors 1 Figure 1: IDArb tackles intrinsic decomposition for an arbitrary number of views under unconstrained illumination. Our approach (a) achieves multi-view consistency compared to learningbased methods and (b) better disentangles intrinsic components from lighting effects via learnt priors compared to optimization-based methods. Our method could enhance wide range of applications such as image relighting and material editing, photometric stereo, and 3D reconstruction. To mitigate these challenges, we propose IDArb, model capable of taking an arbitrary number of images captured under unconstrained, varying lighting conditions and predicting corresponding intrinsic components, including albedo, normal, metallic and roughness. Our key contributions are three-fold. First, we adopt the cross-view, cross-component attention module from Wonder3D (Long et al., 2023) to fuse information across different views and intrinsic components. This module facilitates holistic understanding of the multi-view correspondence and joint distribution of intrinsic components, enabling consistency across viewpoints and reducing decomposition uncertainty. Despite being trained on fixed number of input views, our model shows the flexibility to decompose an arbitrary number of input images without requiring camera poses. Second, to improve performance under complex lighting conditions, we create custom dataset based on Objaverse (Deitke et al., 2022), namely ARB-Objaverse, which contains 5.7M multi-view RGB images and intrinsic components with varying illumination scenarios for effective training. Lastly, we devise novel and effective illumination-augmented and view-adapted training strategy to achieve robust performance under varying lighting conditions and leverage both multi-view cues and general object material prior for better multi-view and single-view inverse rendering. We evaluate our model extensively on both synthetic and real data. Our approach significantly outperforms existing learning-based methods (Kocsis et al., 2024; Zeng et al., 2024; Chen et al., 2024) by large margin, both qualitatively and quantitatively, achieving state-of-the-art results in intrinsic decomposition. Our model offers practical benefits for range of downstream tasks, including material editing, relighting, and photometric stereo, and it can also serve as strong prior to improve optimization-based methods by better disentangling lighting effects from intrinsic appearance. We believe that IDArb provides unified solution across different input regimes in inverse rendering, advancing our ability to understand and model the physical world."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 OPTIMIZATION-BASED INVERSE RENDERING Optimization-based inverse rendering methods aim to jointly reconstruct shape, materials, and lighting from multi-view images. Volumetric representation methods (Boss et al., 2021a; Kuang et al., 2022; Boss et al., 2021b; Zhang et al., 2021b) extend NeRF (Mildenhall et al., 2020) to model in2 trinsic appearance and lighting conditions, rendering images using volume rendering techniques. Surface-based representation methods (Zhang et al., 2021a; 2022a;b; Sun et al., 2023) extract surfaces as signed distance functions (SDFs) (Wang et al., 2021) or differentiable meshes (Munkberg et al., 2022; Hasselgren et al., 2022), apply explicit material models such as Bidirectional Reflectance Distribution Functions (BRDFs) (Nicodemus, 1965), and render images through physicsbased procedures. Recent works explore 3D Gaussian representation Kerbl et al. (2023); Gao et al. (2023) for this task, assigning intrinsic attributes to each Gaussian point. While existing methods effectively simulate global illumination, they often require dense multiview inputs and can be computationally expensive, especially for complex scenes. In addition, they face the inherent ambiguity between lighting and materials, which can lead to suboptimal solutions, such as incorrectly baked lighting into textures. In contrast, our proposed method offers an efficient solution for inverse rendering in feed-forward manner. By leveraging well-learned priors from our large-scale, multi-view, multi-lighting dataset, we can significantly mitigate the issue of ambiguity."
        },
        {
            "title": "2.2 LEARNING-BASED INVERSE RENDERING",
            "content": "With advances in deep neural networks, learning-based approaches (Barron & Malik, 2020; Li et al., 2019; Zhu et al., 2022; Bi et al., 2020; Careaga & Aksoy, 2023; Shi et al., 2016) have demonstrated impressive performance in intrinsic decomposition. They typically take single image as input and decompose intrinsic properties from the input view, such as albedo, specular, and surface normal. Early learning-based methods (Li et al., 2018; Wu et al., 2021; Wimbauer et al., 2022; Sang & Chandraker, 2020; Boss et al., 2020; Yi et al., 2023) handle intrinsic decomposition as deterministic problem, often leading to over-smoothed details in ambiguous pixels. Recent works (Kocsis et al., 2024; Chen et al., 2024; Zeng et al., 2024) adopt probabilistic distribution modeling with diffusion (Ho et al., 2020), estimating accurate intrinsic components with high-frequency details through generative formulation. Zeng et al. (2024) presents unified diffusion framework that addresses both RGBX (estimating intrinsic properties) and XRGB (generating realistic images) by training diffusion pipelines on multiple data sources. These learning-based approaches typically handle inverse rendering in single-view setting, leading to inconsistent results when applied to multi-view data. Our work extends the feed-forward diffusion pipeline to address the under-explored challenge of multi-view inverse rendering, providing unified solution for various input types and offering valuable intrinsic priors for downstream applications. 2.3 DIFFUSION MODELS FOR OTHER MODALITIES Denoising Diffusion Probabilistic Models (DDPMs) and their variants (Ho et al., 2020; Rombach et al., 2021; Zhang et al., 2023) have gained significant attention in text-to-image generation, yielding promising results across various applications. Researchers have also explored adapting diffusion models to different output modalities such as normal (Fu et al., 2024), depth (Ke et al., 2024) and novel view images (Liu et al., 2023; 2024b; Kong et al., 2024). To generate multiple modality simultaneously, Wonder3D (Long et al., 2023) introduces additional cross-domain attention modules into diffusion model that generates multi-view normal maps and corresponding color images. We extend this concept to intrinsic decomposition by splitting the intrinsic components into three triplets and modeling their joint distribution. By leveraging pre-trained diffusion models, which capture rich structural, semantic, and material knowledge, we can overcome data limitations and ensure generalization to real-world scenarios, even when the models are trained on synthetic data."
        },
        {
            "title": "3 METHOD",
            "content": "IDArb is diffusion-based model for intrinsic decomposition that can handle an arbitrary number of input views and varying lighting conditions. We begin by outlining the problem statement in Section 3.1. Then, in Section 3.2, we describe the construction of our custom dataset tailored to this task. Finally, we discuss the model architecture and training strategy in Sec. 3.3. An overview of IDArb is provided in Fig. 2. 3 Figure 2: Top: Overview of IDArb. Bottom: Illustration of the attention block within the UNet. Our training batch consists of input images, sampled from Nv viewpoints and Ni illuminations. The latent vector for each image is concatenated with Gaussian noise for denoising. Intrinsic components are divided into three triplets (D=3): Albedo, Normal and Metallic&Roughness. Specific text prompts are used to guide the model toward different intrinsic components. For attention block inside UNet, we introduce cross-component and cross-view attention module into it, where attention is applied across components and views, facilitating global information exchange. 3.1 PROBLEM STATEMENT We frame intrinsic decomposition as conditional generation problem: X1:N p(X1:N I1:N ). (1) Here, denotes the number of input views; I1:N denotes input RGB images, and X1:N represents the intrinsic components of each view. We model using the simplified Disney BRDF parameterization (Burley & Studios, 2012; Karis & Games, 2013), which includes albedo RHW 3, roughness RHW 1, metallic RHW 1 and surface normal RHW 3. The number of input images can take on an arbitrary value from one to many, and the input images can be rendered under arbitrary unconstrained illuminations during both training and inference. 3.2 ARB-OBJAVERSE DATASET Obtaining ground truth data for intrinsic decomposition in real-world settings is both timeconsuming and technically challenging. To overcome this, we rely on synthetic data for training. Ideally, suitable dataset should feature large-scale, diverse objects rendered under multiple lighting conditions. However, existing datasets have notable limitations. For example, G-Objaverse (Qiu et al., 2024) employs single, low-contrast lighting setup, while ABO (Collins et al., 2022) is restricted to household items, suffering from lack of diversity among the objects. To address these shortcomings, we develop custom dataset, Arb-Objaverse. We select 68k 3D models from Objaverse Deitke et al. (2022), and filter out low-quality and texture-less cases. For each object, we render 12 views, using the Cycles render engine from Blender1. For each viewpoint, we render 7 images under different lighting conditions. Six images are illuminated by randomly sampled high-dynamic range (HDR) environment maps from Poly Haven2, which offers collection 1https://www.blender.org/ 2https://polyhaven.com/ 4 Figure 3: Overview of the Arb-Objaverse dataset. Our custom dataset features diverse collection of objects rendered under various lighting conditions, accompanied by their intrinsic components. of 718 varied environment maps. The last image is illuminated by two point light sources randomly positioned on surrounding shell. Our Arb-Objaverse dataset ends up with 5.7 million rendered RGB images along with their intrinsic components. For training, we further enhance the variability by combining this dataset with G-Objaverse and ABO. Fig. 3 offers visualization and comparison among these datasets. 3.3 ARCHITECTURE AND TRAINING Given an arbitrary number of views from single to multi-view images, IDArb generates multi-view consistent intrinsic maps under unconstrained illumination using text-guided diffusion model. We base our model on the pre-trained Stable Diffusion (SD) (Rombach et al., 2021) model to capitalize on its robust prior knowledge from RGB domain. Different from the 3-channel RGB images, intrinsic components possess higher channel dimensions and cannot be directly processed by original SD model. To repurpose the VAE in original SD for new intrinsic modalities, we divide intrinsic components into three triplets: albedo A, normal and = [M, R, 0], where is metallic, is roughness and 0 is left unused. Each triplet latent is channel-concatenated with the Gaussian noise for denoising. Specific text prompts for each triplet, i.e., albedo, normal, metallic&roughness, are devised to indicate denoising targets. Cross-view Cross-component Attention. In real-world scenarios, users may capture multiple images of an object, making it essential for the model to handle an arbitrary number of input views and ensure consistent results across all views. It is also crucial for 3D reconstruction to have these consistent decomposition results as material guidance. To address this, we propose cross-view attention module within the original attention block of UNet. As shown in Fig. 2, we concatenate input features from each view, enabling the attention operation to be performed across views. This allows the model to leverage multi-view information to reduce ambiguity and enforce consistency across different viewpoints. The reflected color results from the interplay between incident light, material properties, and the surface shape. For instance, convex shape with dark color increases the likelihood of dark albedo. To better capture these relationships, we propose to model the joint distribution of intrinsic components rather than predicting them separately. Inspired by Wonder3D (Long et al., 2023) and GeoWizard (Fu et al., 2024), we adopt cross-component attention via repurposing the vanilla self-attention module to fuse global interactions between different intrinsic components. As demonstrated in Sec. 4.3, exchanging information between components effectively reduces decomposition uncertainty, especially for roughness and metallic. 5 Illumination-Augmented and View-Adapted Training. Multi-view images captured in uncontrolled environments often experience varying lighting conditions, making it essential for algorithms to handle such differences effectively. To address this, we propose an illumination-robust data augmentation strategy, where multi-view images are sampled from various lighting conditions during training. These conditions include range of setups, such as uniform ambient light, HDR environment maps, and point light sources. At each training step, given Nv views and Ni illumination variations for each instance in the dataset, we randomly sample images as input. This allows us to simulate complex input scenarios, including same-view-different-illumination, different-view-sameillumination, and different-view-different-illumination, thus enhancing the diversity of the training data. As result, our model learns to distinguish different lighting conditions without the need for manually crafted modules, effectively leveraging photometric cues from multi-light captures to achieve robust intrinsic decomposition. It also shows superior generalization capability to handle unseen lighting conditions at inference time. However, training with fixed input images leads to downgraded performance when only one view is given (as shown in Sec. 4.3). We suppose that this may be because multi-view training guides the model to focus more on cross-view information to infer intrinsic information, while singleimage decomposition requires the learning of general object material priors. To overcome this, we introduce view-adapted training strategy, that swaps between multi-input and single-image settings. By incorporating this approach, our model gains robust generalization capability with an arbitrary number of input views. Noise Scheduler. The original SD model uses the scaled linear noise scheduler, which prioritizes generating high-frequency details and allocates fewer steps to low-frequency structures. However, this approach limits models performance in intrinsic decomposition task, as the structure of intrinsic components, particularly metallic and roughness R, differs significantly from input RGB images. Inspired by Shi et al. (2023), we shift the noise scheduler toward higher noise levels. As shown in Sec. 4.3, increasing the number of high-noise steps significantly improves the prediction of metallic and roughness components."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Implementation Details. We finetune the UNet from the pretrained Stable Diffusion with the zero terminal SNR schedule (Lin et al., 2024). We utilize the v-prediction as training objective and the AdamW optimizer with learning rate of 1104. The model is trained on downsampled 256256 resolution over 80, 000 steps. During training, the number of input images is randomly set to 3 or 1 per object. The entire training procedure takes approximately 4 days on cluster of 16 Nvidia Tesla A100 GPUs. Baselines. We compare our method with two recent diffusion-based approaches: IID (Kocsis et al., 2024) and RGBX (Zeng et al., 2024). Since RGBX is not yet publicly available, we re-implemented it and trained the model on our training dataset. Additionally, we include IntrinsicAnything (Chen et al., 2024) for albedo comparison and GeoWizard (Fu et al., 2024) for normal comparison. We evaluate our model in two settings: (1) single-view setting, where each input image is processed independently, and (2) multi-view setting, where intrinsic components are jointly estimated from multiple views of each object. Metrics. For albedo evaluation, we use Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) (Wang et al., 2004). Since albedo is defined up to scale factor, we apply scale-invariant PSNR metric by rescaling the predicted albedo as = argminαA α ˆA2 ˆA. For surface normals, we measure Cosine Similarity. Mean Squared Error (MSE) is used to evaluate metallic and roughness components. Evaluation Dataset. We evaluate the effectiveness and generalization capability of our model on both synthetic and real-world datasets. For synthetic data, we sample 441 objects from ArbObjaverse and G-Objaverse, selecting four viewpoints for each object. For real-world data, we collect set of images from Pixabay3. All evaluations are conducted at resolution of 512 512. 3https://pixabay.com/ 6 (a) Albedo estimation. Our method effectively removes highlights and shadows. (b) Normal estimation. Our method gives shape geometry while correctly predicting flat surface. (c) Metallic estimation. Our method outperforms IID and RGBX with plausible results free of interference from texture patterns and lighting. (d) Roughness estimation. Our method outperforms IID and RGBX with plausible results free of interference from texture patterns and lighting. Figure 4: Qualitative comparison on synthetic data. IDArb demonstrates superior intrinsic estimation compared to all other methods. 4.2 EXPERIMENTAL RESULTS Results on Synthetic Data. We present quantitative results in Tab. 1, where our method consistently achieves the highest accuracy across all metrics. Fig. 4 displays visual comparison of our method in single-view setting against baseline methods. For albedo estimation (Fig. 4a), our method effectively removes highlights and shadows, whereas IID and RGBX tend to retain lighting effects in the albedo, and IntrinsicAnything produces unrealistic results for metallic surfaces. In normal estimation (Fig. 4b), our method provides sharp and accurate geometry, while RGBX suffers from interference of object textures, and GeoWizard shows blurred details since it evaluates number of samples and takes their mean. For metallic and roughness estimation (Fig. 4c and Fig. 4d), our method delivers more plausible results, eliminating interference from texture patterns and lighting. Additionally, we observe that incorporating multiview inputs significantly enhances metallic and roughness predictions, as they provide additional information to resolve material ambiguities. Results on Real-world Data. We present qualitative results on real-world data in Fig. 5 and compare our method with IntrinsicAnything for albedo estimation. IntrinsicAnything predicts overly dark albedo for metallic objects and produces blurry details (such as the toys mouth in the third row), leading to loss of fidelity. In contrast, our model generates accurate and convincing decompositions with preserved details. Despite being trained on synthetic data, IDArb generalizes well to real-world images. Additional results are shown in Appendix. D. 7 Table 1: Quantitative evaluation of IDArb against baselines. IDArb consistently achieves the best results among all albedo, normal, metallic and roughness metrics. IID RGBX IntrinsicAnything GeoWizard Ours(single) Ours(multi) Albedo SSIM 0.901 0.902 0.901 - 0.935 0.937 PSNR 27.35 28.09 28.17 - 32.79 33.62 Normal Cosine Similarity - 0.834 - 0.871 0.928 0.941 Metallic MSE 0.192 0.162 - - 0.037 0. Roughness MSE 0.131 0.347 - - 0.058 0.033 Figure 5: Qualitative comparison on real-world data. accurate, convincing decompositions and high-frequency details. IDArb generalizes well to real data, with (a) Ablation on cross-component attention. (b) Ablation on training strategy Figure 6: Ablative studies on (a) cross-component attention and (b) training strategy. 4.3 ANALYSIS AND ABLATIVE STUDY Ablation on Cross-component Attention. To assess the effect of cross-component attention, we also trained our model without cross-component attention mechanism for comparison. As shown in Fig. 6a, exchanging information between different intrinsic components helps reduce material ambiguity, particularly for metallic and roughness, which are prone to uncertainty. 8 Figure 7: Effects of number of viewpoints and lighting conditions. We find increasing the number of viewpoints and the lighting conditions generally improves decomposition performance. Table 2: Quantitative results for photometric stereo on NeRFactor. We evaluate performance using 2, 4, and 8 OLAT images, and achieve the best performance among all compared methods. # OLAT Images Methods IID RGB SDM-UniPS Ours 2 4 8 Albedo 22.23 21.29 22.95 23.50 Normal - 0.71 0.74 0.83 Albedo 22.40 22.08 23.20 23. Normal - 0.77 0.76 0.84 Albedo 22.86 23.29 23.37 25.15 Normal - 0.81 0.81 0.85 Ablation on Training Strategy. Fig. 6b shows ablative studies on multi-single view interleaved training strategy and the noise scheduler. Training exclusively on multi-view inputs leads to performance degradation for single-image inputs, as these two settings emphasize different capabilities of the model, as discussed in Sec. 3.3. Additionally, shifting noise scheduler towards high noise level helps the model better adapt to intrinsic domains. Analysis of Viewpoints and Lighting Effects. We analysis the effects of the number of viewpoints and lighting conditions on our custom dataset. We evaluate our model with 1, 2, 4, 8, and 12 viewpoints under 1, 2 and 3 lighting conditions. As shown in Fig. 7, increasing the number of viewpoints or lights generally improves prediction accuracy. For metallic and roughness predictions, multi-light captures are particularly effective in disentangling these components from lighting effects. Empirically, performance gains from adding more viewpoints diminish beyond eight viewpoints. Further details are provided in Appendix. B. More Results. Additional multi-view input results are provided in Appendix. and supplementary video. For more real-world data results, please refer to Appendix. D. 4.4 APPLICATIONS IDArb offers valuable intrinsic priors for various downstream applications. Here, we demonstrate the models ability in handling single-image relighting and material editing, and photometric stereo problems. Additionally, we show that our generated intrinsic decompositions enhance the results of optimization-based inverse rendering. Single-image Relighting and Material Editing. Once high-quality intrinsic components are obtained, our method enables relighting of captured images under novel illumination. Additionally, we can optimize the lighting in the original scene and perform material editing. Specifically, we represent environment lighting as cube map and adopt differentiable split-sum approximation in NVDiffRec (Munkberg et al., 2022) to optimize its parameters. Fig. 8 showcases our relighting and material editing results. Photometric stereo. Photometric stereo is long-standing challenge in computer vision, aiming to deducing the surface normal and albedo from images captured under varying lighting conditions with fixed camera. We evaluate our method under the harsh One-Light-At-a-Time (OLAT) condition, where each image is illuminated by single point light source without ambient illumination, leading to hard cast shadows. We additionally include SDM-UniPS (Ikehata, 2023) for comparison, which is specifically designed and trained for this task. We conduct experiments on the real-world OpenIllumination dataset (Liu et al., 2024a) and the synthetic NeRFactor dataset (Zhang et al., 2021b). Quantitative results on NeRFactor are summarized in Tab. 2, and qualitative results are present in Appendix. C. Although our model is not explicitly trained for this setting, it still delivers reasonable estimates, particularly when the number of input images is limited. 9 Figure 8: Relighting and material editing results. From in-the-wild captures (a), our model allows for relighting under novel illumination (b) and material property modifications (c). Figure 9: Optimization-based inverse rendering results. Our method guides NVDiffecMC generate more plausible material results. Optimization-based Inverse Rendering. Our method can be used as prior to enhance optimization-based inverse rendering techniques. Specifically, we decompose each training image into its corresponding intrinsic components and treat these components as pseudo-material labels. We adopt NVDiffRecMC (Hasselgren et al., 2022) as the codebase for our experiments, as it employs the same PBR material model as our method. During each iteration, we introduce an additional L2 regularization term between the intrinsic components predicted by NVDiffRecMC and those predicted by our method to ensure physical plausibility. Tab. 3 presents material estimation and relighting results on these dataset. As illustrated in Fig. 9, our method significantly mitigates the color-shifting issue in the reconstructed albedo from NVDiffRecMC, leading to improved results in relighting tasks."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we present IDArb, that solves intrinsic decomposition via feed-forward diffusion pipeline. Our method can process arbitrary images captured under unknown and varying illuminations and estimate consistent intrinsic components, including albedo, normal, metallic and roughness. The cross-component attention module and illumination-augmented training further enhance our models ability to reduce ambiguity, fostering more robust inverse rendering under complex, high-contrast lighting conditions. Limitations and Discussions. While our method demonstrates strong generalization capabilities on real-world data, it faces challenges in accurately predicting material maps for intricate objects, such as corroded bronze statues with spatially varying metallic and roughness properties due to corrosion levels. Given that most synthetic data employ global metallic and roughness values, our method may oversimplify estimations for complex real-world objects. Future research directions could involve incorporating real data through unsupervised techniques. Moreover, the current implementation of 10 Table 3: Ablation on IDArb pseudo labels for optimization-based inverse rendering on NeRFactor and Synthetic4Relight datasets. Nerfactor Synthetic4Relight Albedo (raw) Albedo (scaled) Relighting Albedo (raw) Albedo (scaled) Relighting Roughness NVDiffRecMC NVDiffRecMC w/ Ours 17.89 20.90 25.88 26.61 22.65 27.20 17.03 26. 29.64 30.73 24.05 31.01 0.046 0.014 cross-view attention concatenates all input views, leading to complexity of O(N 2) and posing difficulties in handling dense input views with high resolutions. Future investigations could explore more efficient cross-view attention mechanism."
        },
        {
            "title": "REFERENCES",
            "content": "Jonathan T. Barron and Jitendra Malik. Shape, illumination, and reflectance from shading, 2020. URL https://arxiv.org/abs/2010.03592. Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, and Ravi Ramamoorthi. Deep 3d capture: Geometry and reflectance from sparse multi-view images, 2020. URL https://arxiv.org/ abs/2003.12642. Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A. Lensch, and Jan Kautz. Two-shot spatiallyvarying brdf and shape estimation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu, and Hendrik P. A. Lensch. In ICCV, pp. 1266412674. Nerd: Neural reflectance decomposition from image collections. IEEE, 2021a. Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan T. Barron, and Hendrik P. A. Lensch. Neural-pil: Neural pre-integrated lighting for reflectance decomposition. In NeurIPS, pp. 10691 10704, 2021b. Brent Burley and Walt Disney Animation Studios. Physically-based shading at disney. In Acm Siggraph, volume 2012, pp. 17. vol. 2012, 2012. Chris Careaga and Yagız Aksoy. Intrinsic image decomposition via ordinal shading. ACM Transactions on Graphics, 43(1):124, November 2023. ISSN 1557-7368. doi: 10.1145/3630750. URL http://dx.doi.org/10.1145/3630750. Xi Chen, Sida Peng, Dongchen Yang, Yuan Liu, Bowen Pan, Chengfei Lv, and Xiaowei Zhou. Intrinsicanything: Learning diffusion priors for inverse rendering under unknown illumination, 2024. URL https://arxiv.org/abs/2404.11593. Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding. CVPR, 2022. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects, 2022. URL https://arxiv.org/abs/2212.08051. Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image. In ECCV, 2024. Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3d gaussian: Real-time point cloud relighting with brdf decomposition and ray tracing. arXiv:2311.16043, 2023. 11 Roger Grosse, Micah Johnson, Edward Adelson, and William Freeman. Ground truth dataset and baseline evaluations for intrinsic image algorithms. In 2009 IEEE 12th International Conference on Computer Vision, pp. 23352342. IEEE, 2009. Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg. Shape, Light, and Material Decomposition from Images using Monte Carlo Rendering and Denoising. arXiv:2206.03380, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Satoshi Ikehata. Scalable, detailed and mask-free universal photometric stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Haian Jin, Isabella Liu, Peijia Xu, Xiaoshuai Zhang, Songfang Han, Sai Bi, Xiaowei Zhou, Zexiang Xu, and Hao Su. Tensoir: Tensorial inverse rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. James Kajiya. The rendering equation. In Proceedings of the 13th annual conference on Computer graphics and interactive techniques, pp. 143150, 1986. Brian Karis and Epic Games. Real shading in unreal engine 4. Proc. Physically Based Shading Theory Practice, 4(3):1, 2013. Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering, 2023. URL https://arxiv.org/abs/2308. 04079. Peter Kocsis, Vincent Sitzmann, and Matthias Nießner. Intrinsic image diffusion for indoor singleview material estimation, 2024. URL https://arxiv.org/abs/2312.12274. Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, and Andrew Davison. Eschernet: generative model for scalable view synthesis. arXiv preprint arXiv:2402.03908, 2024. Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, and Sergey Tulyakov. Neroic: neural rendering of objects from online image collections. ACM Trans. Graph., 41(4):56:156:12, 2022. Zhengfei Kuang, Yunzhi Zhang, Hong-Xing Yu, Samir Agarwala, Elliott Wu, Jiajun Wu, et al. Stanford-orb: real-world 3d object inverse rendering benchmark. 2023. Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. In SIGLearning to reconstruct shape and spatially-varying reflectance from single image. GRAPH Asia 2018 Technical Papers, pp. 269. ACM, 2018. Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from single image, 2019. URL https://arxiv.org/abs/1905.02722. Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed, 2024. URL https://arxiv.org/abs/2305.08891. Isabella Liu, Linghao Chen, Ziyang Fu, Liwen Wu, Haian Jin, Zhong Li, Chin Ming Ryan Wong, Yi Xu, Ravi Ramamoorthi, Zexiang Xu, and Hao Su. Openillumination: multi-illumination dataset for inverse rendering evaluation on real objects, 2024a. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image, 2024b. URL https://arxiv.org/abs/2309.03453. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008, 2023. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas Muller, and Sanja Fidler. Extracting Triangular 3D Models, Materials, and Lighting From Images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 82808290, June 2022. Fred Nicodemus. Directional reflectance and emissivity of an opaque surface. Applied optics, 4 (7):767775, 1965. Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to-3d. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 99149925, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2021. Shen Sang and M. Chandraker. Single-shot neural relighting and svbrdf estimation. In ECCV, 2020. Jian Shi, Yue Dong, Hao Su, and Stella X. Yu. Learning non-lambertian object intrinsics across shapenet categories, 2016. URL https://arxiv.org/abs/1612.08510. Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model, 2023. Yawar Siddiqui, Tom Monnier, Filippos Kokkinos, Mahendra Kariya, Yanir Kleiman, Emilien Garreau, Oran Gafni, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, and David Novotny. Meta 3d assetgen: Text-to-mesh generation with high-quality geometry, texture, and pbr materials. arXiv, 2024. Cheng Sun, Guangyan Cai, Zhengqin Li, Kai Yan, Cheng Zhang, Carl S. Marshall, Jia-Bin Huang, Shuang Zhao, and Zhao Dong. Neural-pbir reconstruction of shape, material, and illumination. In ICCV, pp. 1800018010. IEEE, 2023. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. Felix Wimbauer, Shangzhe Wu, and Christian Rupprecht. De-rendering 3d objects in the wild, 2022. URL https://arxiv.org/abs/2201.02279. Shangzhe Wu, Ameesh Makadia, Jiajun Wu, Noah Snavely, Richard Tucker, and Angjoo Kanazawa. De-rendering the worlds revolutionary artefacts, 2021. URL https://arxiv.org/abs/ 2104.03954. Chongjie Ye, Lingteng Qiu, Xiaodong Gu, Qi Zuo, Yushuang Wu, Zilong Dong, Liefeng Bo, Yuliang Xiu, and Xiaoguang Han. Stablenormal: Reducing diffusion variance for stable and sharp normal. ACM Transactions on Graphics (TOG), 2024. 13 Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. Renjiao Yi, Chenyang Zhu, and Kai Xu. Weakly-supervised single-view image relighting, 2023. URL https://arxiv.org/abs/2303.13852. Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. RGB X: Image decomposition and synthesis using materialand lighting-aware diffusion models. arXiv preprint arXiv:2405.00666, 2024. Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 54535462, 2021a. Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. Iron: Inverse rendering by optimizing neural sdfs and materials from photometric images. In IEEE Conf. Comput. Vis. Pattern Recog., 2022a. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets, 2024. URL https://arxiv.org/abs/2406.13897. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec, William T. Freeman, and Jonathan T. Barron. Nerfactor: neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics, 40(6):118, December 2021b. ISSN 1557-7368. doi: 10.1145/3478513.3480496. URL http://dx.doi.org/10.1145/3478513.3480496. Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou. Modeling indirect illumination for inverse rendering. In CVPR, 2022b. Rui Zhu, Zhengqin Li, Janarbek Matai, Fatih Porikli, and Manmohan Chandraker. Irisformer: Dense vision transformers for single-image inverse rendering in indoor scenes, 2022. URL https: //arxiv.org/abs/2206.08423."
        },
        {
            "title": "A PRELIMINARY",
            "content": "A.1 IMAGE DIFFUSION MODEL In Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020), forward diffusion process is defined, gradually introducing small amounts of Gaussian noise to the sample at each timestep, represented by q(xtxt1) = (xt; 1 βtxt1, βtI), where represents the timestep and β acts as the variance scheduler. To recover samples from the random noise, DDPM learns to model the reverse diffusion process as pθ(xt1xt) = (xt1; µθ(x, t), Σθ(xt, t)) and construct x0 through iterative denoising. Stable Diffusion (SD) (Rombach et al., 2021) employ an encoder to compress the input image RHW 3 into latent vector RH/8W/84 before performing the diffusion process in the latent space. Following denoising, the latent representation is then converted back to pixel space through decoder ˆx = D(z0). For conditional generation, the training objective of Stable Diffusion (SD) is formulated as: := EE(x),y,ϵN (0,1),t[ϵ ϵθ(zt, t, τθ(y))2 2], (2) where is uniformly sampled from {1, ..., }, τθ(y) represents the encoding of the condition and ϵθ is implemented as UNet. 14 A."
        },
        {
            "title": "INTRINSIC COMPONENTS FORMATION",
            "content": "Our image formation is based on the classic rendering equation (Kajiya, 1986) to ensure physical correctness. For point with surface normal n, the incident light intensity at this point is denoted as Li(ωi; x), where ωi represents the incident light direction. The Bidirectional Reflectance Distribution Function (BRDF) (Nicodemus, 1965), denoted as fr(ωo, ωi; x), describes the reflectance properties of the material when viewed from direction ωo. The observed light intensity Lo(ω0; x) is calculated over the hemisphere Ω = {ωi : ωi > 0} as follows: Lo(ωo; x) = (cid:90) Ω Li(ωi; x)fr(ωo, ωi; x)(ωi n)dωi. (3) In our approach, we aim to recover the objects surface normal and BRDF material from the observed color on the left-hand side of Eq. 3, which are independent of illumination and view direction. We adopt the Disney Basecolor-Metallic model(Burley & Studios, 2012) for BRDF parametrization, which comprises the following components: albedo, representing the base color; roughness, controlling the diffuse and specular response; and metallic, governing the specular reflection. Specifically, given single RGB image RHW 3, we aim to jointly estimate the surface normal RHW 3, albedo RHW 3, roughness RHW 1 and metallic RHW 1."
        },
        {
            "title": "B DETAILS ABOUT THE EFFECTS OF VIEWPOINTS AND LIGHTING",
            "content": "We present the numerical performance results across varying numbers of viewpoints (# V) and lighting conditions (# L), as shown in Tab. 4 to 7. Table 4: Albedo Performance across different numbers of viewpoints (# V) and lightings (# L). Table 5: Normal Performance across different numbers of viewpoints (# V) and lightings (# L). # # 1 2 1 2 4 8 12 29.16 29.96 30. 28.72 30.26 30.73 30.12 30.96 31.16 30.49 31.13 31.33 30.77 31.26 31.40 # # 1 2 3 1 2 4 8 0.909 0.922 0.926 0.910 0.927 0.931 0.925 0.930 0.931 0.930 0.933 0.934 0.932 0.934 0.935 Table 6: Metallic Performance across different numbers of viewpoints (# V) and lightings (# L). Table 7: Roughness Performance across different numbers of viewpoints (# V) and lightings (# L). # # 1 2 3 1 4 8 12 0.105 0.061 0.061 0.116 0.068 0.056 0.068 0.047 0. 0.059 0.044 0.045 0.050 0.042 0.040 # # 1 2 3 2 4 8 12 0.049 0.043 0.031 0.050 0.026 0. 0.024 0.019 0.016 0.019 0.016 0.014 0.021 0.015 0."
        },
        {
            "title": "C ADDITIONAL RESULTS ON PHOTOMETRIC STEREO",
            "content": "We present qualitative results of photometric stereo in Fig. 10. ADDITIONAL RESULTS ON REAL-WORLD DATA We evaluate our method on two real-world benchmarks: MIT-Intrinsic (Grosse et al., 2009) and Stanford-ORB (Kuang et al., 2023). For MIT-Intrinsic, we compared our albedo estimation results with IntrinsicAnything (Chen et al., 2024), as shown in Tab. 8. For Stanford-ORB, we presented results for normal estimation, albedo estimation, and re-rendering, comparing our method with StableNormal (Ye et al., 2024) and IntrinsicNeRF (Ye et al., 2023), as shown in Tab. 9. For the rerendering evaluation, we utilized the ground truth environment maps to render our decomposition results and compared them with the original images. Additionally, we present qualitative results on real-world data from the Internet in Fig. 11 and Fig. 12. 15 Figure 10: Photometric stereo results using 4 OLAT images in OpenIllumination and NeRFactor. Table 8: Quantitative comparisons on MIT-Intrinsic. SSIM PSNR LPIPS Ours IntrinsicAnything 0.876 0.896 27.98 25. 0.117 0.150 Table 9: Quantitative comparisons on Stanford-ORB. Normal Cosine Distance 0.041 0.029 0.038 Albedo PSNR LPIPS 41.30 41.46 0.039 0.038 SSIM 0.978 0. 0.981 39.31 0.048 Ours(single) Ours(multi) StableNormal IntrinsicNeRF Re-rendering PSNR-H 24.11 24. PSNR-L 31.28 31.43 SSIM LPIPS 0.969 0.970 0.024 0.024 ADDITIONAL RESULTS ON MULTI-VIEW INPUTS We present additional results on multi-view input in Fig. 13."
        },
        {
            "title": "F FAILURE CASES",
            "content": "Several failure cases are illustrated in Fig. 15. First, our model struggles with outdoor scenes, as it is primarily trained on object-centric data. While the model exhibits some generalization capability, its performance degrades in these scenarios. Second, when the model is faced with text, the decomposition fails to recover the correct text structures. Finally, in the third row, the model produces overly simplified outputs in certain cases, failing to preserve subtle material details, such as the metallic 16 Figure 11: More results on real-world data. 17 Figure 12: More results on real-world data. We also provide the reconstructed and relighting images. Figure 13: More results on multi-view data. 19 Figure 14: Multiview images with extreme lighting variation. For each scene in NeRD dataset (Boss et al., 2021a), we input 4 views. 20 Figure 15: Failure cases. features of telephone. This issue arises from the synthetic training data, which often contains simpler material variations, leading the model to overly simplify fine-grained material properties. GENERALIZATION TO SCENE-LEVEL DATA Despite not being explicitly trained on such datasets, our model demonstrates generalization ability in outdoor and indoor scenes. We provide qualitative results in Fig. 16, Fig. 17 and Fig. 18. 21 Figure 16: Results on Mip-NeRF 360 (Barron et al., 2022) (Part 1, outdoor). We input 4 views for each scene. 22 Figure 17: Results on Mip-NeRF 360 (Barron et al., 2022) (Part 2, indoor). We input 4 views for each scene. 23 Figure 18: Results on indoor and outdoor scenes. Input images are collected from the Internet."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong",
        "Zhejiang University"
    ]
}