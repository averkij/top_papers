{
    "paper_title": "Slow-Fast Architecture for Video Multi-Modal Large Language Models",
    "authors": [
        "Min Shi",
        "Shihao Wang",
        "Chieh-Yun Chen",
        "Jitesh Jain",
        "Kai Wang",
        "Junjun Xiong",
        "Guilin Liu",
        "Zhiding Yu",
        "Humphrey Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Balancing temporal resolution and spatial detail under limited compute budget remains a key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversible information loss and often ignoring input instructions. To address this, we propose a novel slow-fast architecture that naturally circumvents this trade-off, enabling the use of more input frames while preserving spatial details. Inspired by how humans first skim a video before focusing on relevant parts, our slow-fast design employs a dual-token strategy: 1) \"fast\" visual tokens -- a compact set of compressed video features -- are fed into the LLM alongside text embeddings to provide a quick overview; 2) \"slow\" visual tokens -- uncompressed video features -- are cross-attended by text embeddings through specially designed hybrid decoder layers, enabling instruction-aware extraction of relevant visual details with linear complexity. We conduct systematic exploration to optimize both the overall architecture and key components. Experiments show that our model significantly outperforms self-attention-only baselines, extending the input capacity from 16 to 128 frames with just a 3% increase in computation, and achieving a 16% average performance improvement across five video understanding benchmarks. Our 7B model achieves state-of-the-art performance among models of similar size. Furthermore, our slow-fast architecture is a plug-and-play design that can be integrated into other video MLLMs to improve efficiency and scalability."
        },
        {
            "title": "Start",
            "content": "Slow-Fast Architecture for Video Multi-Modal Large Language Models Min Shi 1 * Shihao Wang 3 * Chieh-Yun Chen 1 Jitesh Jain 1 Kai Wang 1 Jinjun Xiong 4 Guilin Liu 2 Zhiding Yu 2 Humphrey Shi 1 2NVIDIA 3HKPU 4SUNY Buffalo 1SHI Labs @ Georgia Tech github.com/SHI-Labs/Slow-Fast-Video-Multimodal-LLM 5 2 0 2 2 ] . [ 1 8 2 3 1 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Balancing temporal resolution and spatial detail under limited compute budget remains key challenge for video-based multi-modal large language models (MLLMs). Existing methods typically compress video representations using predefined rules before feeding them into the LLM, resulting in irreversible information loss and often ignoring input instructions. To address this, we propose novel slow-fast architecture that naturally circumvents this trade-off, enabling the use of more input frames while preserving spatial details. Inspired by how humans first skim video before focusing on relevant parts, our slow-fast design employs dual-token strategy: 1) fast visual tokens compact set of compressed video features are fed into the LLM alongside text embeddings to provide quick overview; 2) slow visual tokens uncompressed video features are cross-attended by text embeddings through specially designed hybrid decoder layers, enabling instruction-aware extraction of relevant visual details with linear complexity. We conduct systematic exploration to optimize both the overall architecture and key components. Experiments show that our model significantly outperforms self-attention-only baselines, extending the input capacity from 16 to 128 frames with just 3% increase in computation, and achieving 16% average performance improvement across five video understanding benchmarks. Our 7B model achieves state-of-the-art performance among models of similar size. Furthermore, our slow-fast architecture is plug-and-play design that can be integrated into other video MLLMs to improve efficiency and scalability. * Work partially done during an internship at NVIDIA. Equal advising. Correspondence to: Humphrey Shi <shi@gatech.edu>, Zhiding Yu <zhidingy@nvidia.com>. 1 Figure 1. Comparison between the mainstream video MLLM architecture and the proposed slow-fast architecture. Rather than relying on carefully-designed video representation compression strategies, the slow-fast architecture utilizes highly compressed fast visual tokens as preview for the LLM while allowing text embeddings to extract relevant information from uncompressed slow visual tokens via cross-attention. This approach extends 16-frame baseline to 96-frame input with only 2% increase in computation, yielding 14% average performance improvement across five benchmarks. 1. Introduction Video Multi-modal Large Language Models (MLLMs) enable Large Language Models (LLMs) to perform complex reasoning based on encoded video features. Mainstream approaches (Zhang et al., 2023; Wang et al., 2024; Chen et al., 2024b) treat video as sequence of images, concatenating frame features to construct the video representation fed into the LLM. Due to the LLMs context length constraints and the redundancy in video data, compressing vision tokens before feeding them into LLMs has become standard postprocessing step (Wang et al., 2024; Liu et al., 2024e; Shen et al., 2024). key challenge in video representation compression is balancing temporal resolution and spatial detail within the limited context lengthwhether to include more frames or allocate more tokens per frame. Slow-Fast Architecture for Video Multi-Modal Large Language Models To achieve better trade-off while preserving spatially and temporally important information, video MLLMs employ various compression strategies, such as feature merging based on similarity (Shen et al., 2024) or attention score (Chai et al., 2024), heuristic pooling rules (Xu et al., 2024b; Wang et al., 2024), and learnable modules (Liu et al., 2024e; Zhang et al., 2023). However, these methods are irreversible once tokens are fed into the LLM and largely agnostic to input instructions, potentially discarding critical visual details relevant to the given task. An alternative approach reduces complexity by allowing text embeddings to cross-attend to visual features instead of directly feeding visual tokens into the LLM (Alayrac et al., 2022; Li et al., 2023a; Liu et al., 2024b). However, in this setup, text embeddings interact with visual features only few times, lacking persistent visual tokens as contextual anchors within the LLM. Recent studies (Dai et al., 2024) indicate that relying solely on cross-attention underperforms compared to direct concatenation when using the same base model and training data. The challenges outlined above raise key question: Can we design paradigm that preserves sufficient visual details while efficiently and effectively integrating them into the LLM? Inspired by how humans answer video-based questions (Feichtenhofer et al., 2019)first gaining quick overview and then focusing on relevant detailswe propose slow-fast architecture for video MLLMs to achieve this goal, as illustrated in Fig. 1. Video features are first compressed into fixed number of fast visual tokens, which are concatenated with text embeddings to provide quick preview for the LLM. Simultaneously, uncompressed slow visual tokens interact with text embeddings via crossattention in modified decoder layers, termed hybrid decoder layers, at specific positions. This design enables the integration of instruction-relevant visual information while maintaining linear complexity with respect to video length, resulting in an efficient and accurate framework. We refine the design of the hybrid decoder through series of explorations. Initially, we observe that simply combining existing cross-attention architectures (Alayrac et al., 2022; Dubey et al., 2024) with the LLaVA-style (Liu et al., 2024a) framework yields only marginal improvements while remaining computationally intensive. To address this, we start by streamlining the integration of the cross-attention module. We compare different designs from recent works (Ye et al., 2024; Hong et al., 2024) and remove the computationintensive components. The results lead to minimal yet effective design, revealing that placing cross-attention operation between the self-attention and feed-forward network in the original decoder layer outperforms the conventional approach of incorporating cross-attention as stand-alone decoder layer (Alayrac et al., 2022; Dubey et al., 2024). Secondly, inspired by the findings that text tokens exhibit varying attention weights to visual tokens (Zhu et al., 2024), reflecting different demands for visual content, we propose dynamic gating mechanism to determine how much visual information can be merged into the text embeddings via cross-attention, conditioned on each text token. Finally, we investigate crucial implementation details, including weight initialization for the cross-attention module, compression methods for fast visual tokens, and scalability to input frames of up to 96 in single forward pass. Based on explorations above, we construct slow-fast architecture which significantly outperforms mainstream LLaVA-style baselines with identical training data. With less than 2% additional computational overhead from crossattention, our model extends 16-frame baselines to perceive 96 frames, improving the average performance across five video benchmarks by 14%. Despite this efficiency, our 7B model achieves competitive results compared with other topperforming 7B open-source models and sets state-of-the-art performance on several video understanding benchmarks, e.g., 68.5% on TempCompass (Liu et al., 2024d), 67.9% on MLVU (Zhou et al., 2024), 57.5% on LongVideoBench (Wu et al., 2024), and 70.3% on Perception Test (Patraucean et al., 2023). Our contributions are summarized as follows: We are the first to propose slow-fast architecture for video MLLM that enables instruction-aware visual information extraction from uncompressed video representations with linear complexity, allowing our method to scale efficiently while improving performance. We conduct an in-depth explorations that reveals the crucial design choices and significantly boost performance by an average of 14% in five video benchmarks. 2. Related Work Architecture design for MLLM. Multi-modal Large Language Models (MLLMs) enable the LLMs to comprehend more modalities, like images (Liu et al., 2023), videos (Li et al., 2023b; Zhang et al., 2023), audio (Chu et al., 2024), and 3D data (Hong et al., 2023). Two primary approaches have been adopted to integrate multi-modal information from modality-specific encoders. The first widely-used approach directly concatenates multi-modal embeddings with text embeddings (Liu et al., 2023; 2024a), which is also followed by series of top-performing open-source MLLMs (Li et al., 2024a; Chen et al., 2024b; Wang et al., 2024). Under this paradigm, subsequent works further enhancing encoder designs (Chen et al., 2024b; Liu et al., 2024e), improving data mixtures and training strategies (Tong et al., 2024), or optimizing training recipes (Karamcheti et al., 2024). Alternatively, second approach use cross-attention between text embeddings and visual tokens within the LLMs to inject visual clues (Alayrac 2 Slow-Fast Architecture for Video Multi-Modal Large Language Models Figure 2. Illustration of the Slow-Fast Architecture and Hybrid Decoder. The video input is first processed into slow visual tokens through vision encoder and projector. These slow visual tokens are then condensed into smaller set of fast visual tokens via strided sampling and temporal pooling. The fast visual tokens are concatenated with text embeddings and fed into the LLM, serving as preview context. Meanwhile, the slow visual tokens interact with text embeddings through cross-attention in hybrid decoder layers distributed within the LLM, enabling instruction-aware visual information extraction with linear complexity. et al., 2022; Li et al., 2023a; Ye et al., 2024; Liu et al., 2024b; Dubey et al., 2024) with linear complexity. However, under the same training data and strategies, models relying solely on cross-attention demonstrate inferior performance (Dai et al., 2024) compared to the first direct concatenation approach. few works also try to combine these two paradigms to achieve high-resolution image encoding (Hong et al., 2024; Dai et al., 2024) by feeding the low-resolution snapshots to the LLM while using crossattention to inject high-resolution image features. These works share similar implementation and intuition to our slow-fast architecture. Our work further explores the design space of cross-attention integration and demonstrates its potential in video understanding. Video MLLM. To encode video sequence, existing video MLLMs (Li et al., 2023b; Maaz et al., 2024; Zhang et al., 2023; Wang et al., 2024; Cheng et al., 2024) typically append features extracted from each frame. These leads to excessively long sequence length since video can have hundreds of frame, which are undesirable for LLMs especially when video data have lot of redundancy. To address this issue, most methods adopt series of visual representation compression strategies based on pre-defined rules, e.g., heuristic pooling rules (Xu et al., 2024a; Zhang et al., 2024d; Xu et al., 2024b), merging similar frames (Chai et al., 2024; Shen et al., 2024), or using learnable modules such as Q-Former (Li et al., 2023b; Zhang et al., 2023) or cross-attention layers (Liu et al., 2024e) to condense visual representation into fewer tokens. Some works focus on improving the underlying platform by optimizing attention calculation and distributed training (Xue et al., 2024), or extending the context length of LLMs (Zhang et al., 2024b). Additionally, memory-based methods (Song et al., 2024) and sliding window techniques (Ren et al., 2024) have been utilized to consolidate and aggregate information across long video durations. From data perspective, several studies (Chen et al., 2024a; Zhang et al., 2024d;c; Liu et al., 2024c) have enhanced the quality and diversity of video instruction-tuning datasets. In this paper, we propose slow-fast architecture which are orthogonal to these efforts and can benefits from each other. 3. Slow-Fast Architecture for Video MLLM As illustrated in Fig. 2, the proposed slow-fast architecture integrates vision encoder with multi-modal projector and large language model, incorporating hybrid layers with gate-controlled cross-attention between text embeddings and visual content. Firstly, the vision encoder and multi-modal projector generate sequence of video features aligned with the text embeddings from the video frames. Unlike common practices (Wang et al., 2024; Li et al., 2024a; Zhang et al., 2024d), which directly concatenate the compressed visual tokens and text embeddings as input for the LLM, the visual embedding sequence is first compressed into small set of fast tokens via direct sampling and temporal pooling sequentially. These fast tokens act as quick preview and are fed into the LLM to establish basic context. Meanwhile, the original, uncompressed visual features are retained as slow tokens. Within the LLMs forward pass, the hybrid layers, which integrate cross-attention upon the pre-trained decoder layer, enable the text embeddings to selectively extract detailed information from the slow tokens. dynamic gate further refines this process by modulating Slow-Fast Architecture for Video Multi-Modal Large Language Models how much information each text token absorbs, dynamically generating gating values from each token. 3.1. Encoding Slow and Fast Visual Tokens Given video sequence with frames, we first process the raw RGB frames using the vision encoder and the multimodal projector. The resulting frame features undergo 2 2 spatial average pooling operation to reduce the token count per frame by 4. The processed feature sequence is considered as the slow visual tokens. Next, we compress the slow visual tokens to obtain the fast visual tokens by direct sampling and temporal pooling sequentially. Specifically, we sampled the feature sequence every frames and then applied temporal average pooling with stride of t, resulting in the fast tokens Vf Rkd, where is the number of fast tokens. 3.2. Hybrid Decoder Layer The core component in our slow-fast architecture is the hybrid decoder layer, which enables text embeddings to interact with uncompressed video features midway through the LLMs forward pass. As shown in the right part of Fig. 2, the hybrid decoder layer extends pre-trained decoder layer. To seamlessly integrate cross-attention into the decoder, we treat it as analogous to self-attention: selfattention aggregates context from previous tokens, while cross-attention aggregates relevant visual context from the slow visual tokens. Cross-attention also runs in parallel with self-attention, with its outputs merged back into the text embedding via skip connection. dynamic gate is added to the skip connection to modulate the output before merging, reducing disturbance and improving training stability. In the following, we detail the computation of cross-attention and the design of the dynamic gate. Cross-attention. The input to the hybrid decoder layer consists of hidden states and slow visual tokens. Inspired by mPLUG-OWL3 (Ye et al., 2024), both are first processed through shared layer normalization layer. After the layer normalization, there are two attention mechanisms in parallel. All hidden states participate in the self-attention operation. For cross-attention, however, the query is restricted to textual tokens, excluding the fast visual tokens. Hence, the queries corresponding to the textual tokens are extracted from the self-attention layer (re-use the query for crossattention). Denote this query as Qt Rnd, where represents the number of textual tokens, and is the hidden dimension (head dimensions are omitted for simplicity). Let the normalized slow visual features be Vs Rmd, where denotes the number of slow visual tokens. Then the cross-attention output is calculated as: = MHCA(Qt, WkVs, WvVs), (1) 4 where MHCA(q, k, v) represents the multi-head crossattention (Vaswani et al., 2017), with q, k, and serving as the query, key, and value, respectively. Wk and Wv are two learnable projection matrices for the cross-attention module. In practice, the hidden dimension and number of attention heads in the cross-attention layer are kept consistent with the configuration of the pre-trained self-attention layer. We also experiment with allowing all the hidden states, including the fast visual tokens, to attend to the slow visual tokens, as discussed in Sec. 4.2. However, our findings indicate that limiting the cross-attention to only the textual tokens results in better performance and efficiency. Dynamic gate with warm-up. Although the output from the cross-attention layer, , aggregates the visual context for each text token, it can also introduce disturbances into the pre-trained LLM. common approach to address this issue is to use learnable scalar as the gate (Alayrac et al., 2022; Dubey et al., 2024) that modulates the extent to which the cross-attention output is merged back into the text embeddings. This static gate assigns an identical weight to all tokens, regardless of their context or importance. However, the necessity of attending to visual contexts can vary significantly across tokens and input instructions, as indicated by varying attention scores (Zhu et al., 2024). To address this limitation, we propose dynamic gate with warm-up mechanism. This dynamic gate design allows the model to determine how much of the cross-attention output should be incorporated into the text embeddings for each text token. The warm-up mechanism further stabilizes training at the initial stage. Specifically, single linear layer followed by tanh activation is applied to the text embeddings. This generates tensor gd Rn, which assigns dynamic gate value to each text token. To mitigate the impact of the cross-attention branch during the initial stages of training, static learnable warm-up factor gs, initialized to zero, is also introduced. With these components, the cross-attention output is merged into the text embedding Xt as: Xt = Xt + gd gs , (2) where denotes the element-wise product. Cross-attention updates only Xt, the pure text component of the hidden states, while the fast visual tokens remain unchanged during this process. Note that the hidden state is also updated with context from the self-attention output like in the standard decoder layer, where the fast visual tokens are included. Weight initialization. For the hybrid decoder layer, the original parameters are retained from the pre-trained models. The gate-related parameters, i.e., the gate linear projection layer is randomly initialized. The key-value projection matrices Wk and Wv of the cross-attention operation are initialized using the weights from the corresponding self-attention Slow-Fast Architecture for Video Multi-Modal Large Language Models layer. In practice, we find this can facilitate training and ensure smoother convergence. 3.3. Implementation Details Model architecture. We adopt ConvNeXt-XXL (Liu et al., 2022) pre-trained by OpenCLIP (Ilharco et al., 2021) as the vision encoder, which downsamples input images by 32 times. The input resolution for both image and video inputs is set to 576, resulting in 324 tokens for each image. For image inputs, the feature map is kept at its original resolution, and the image feature maps serve as both the fast and slow tokens within the hybrid architecture. For video inputs, the feature map of each frame is further compressed via average pooling, reducing the token count to 81 per frame. To account for training samples with low frame counts, we enforce minimum of 16 input fast frames during the compression process. For the language model, we use Qwen2-7B (Team, 2024), pre-trained large language model with 28 transformer decoder layers. Four hybrid decoder layers are distributed within the LLMs decoder layers at indices [0, 8, 16, 24]. Training recipe. Following common practices (Liu et al., 2023), we employ two-stage training strategy for our model. In the first stage, the model is trained on image and video caption datasets. During this stage, only the multi-modal projector and the cross-attention-related parameters, i.e., the key and value projection matrices and the dynamic gate, are updated. The training batch size is set to 256. The learning rate for the multi-modal projector is set to 1 103, while the learning rate for the cross-attentionrelated modules is 2 104. Empirically, we find that higher learning rate for the cross-attention modules leads to training instability. In the second stage, all parameters of the model are fine-tuned using multi-modal conversation datasets across diverse tasks and data formats. This stage incorporates mixture of text, image, and video data. Here, the batch size is set to 128, and the learning rate is reduced to 2 105. Training data. We construct the training data mixture using open-source datasets. In the first pre-training stage, we combine 537k video caption samples from LLaVA-Video178k (Zhang et al., 2024d) with 558k image captions from LLaVA-1.5 (Liu et al., 2024a). In the second stage, we mix pure text, image, and video instruction tuning datasets. Table 1 provides detailed breakdown of the data sources and sample sizes. The primary components include 1.4M video data from LLaVA-Video-178k (Zhang et al., 2024d) and 1.2M image data from LLaVA-OneVision (Li et al., 2024a). Following VideoChat2 (Li et al., 2024b), we also incorporate their conversation data alongside with TGIF (Li et al., 2016), SthSthv2 (Goyal et al., 2017), Kinetics-710 (Li et al., 2023c), and Ego4D (Grauman et al., 2022) into the training Table 1. Statistics of the second-stage instruction tuning data. Data Type Data Sources Number Video LLaVA-Video-178k (Zhang et al., 2024d) 1,395k TGIF-QA (Li et al., 2016) SthSthV2 (Goyal et al., 2017) Kinetics-710 (Li et al., 2023c) CLEVR (Johnson et al., 2017) VideoChat2 (Li et al., 2024b) Ego4D (Grauman et al., 2022) 58k 40k 40k 20k 10k 8k Image & Pure Text LLaVA-1.5 (Liu et al., 2024a) LLaVA-OneVision (Li et al., 2024a) Sum 665k 1,231k 3,467k mixture. For the second stage of all ablation studies, due to resource limitations, we use reduced dataset comprising 665k instruction tuning samples from LLaVA-1.5 (Liu et al., 2024a) and 1.4M video instruction tuning samples from LLaVA-Video-178k. 4. Experiments In this section, we present comparison with recent stateof-the-art video MLLMs, followed by controlled ablation studies and qualitative examples. We use series of benchmarks covering different tasks and video durations. NExTQA (Xiao et al., 2021), ActivityNetQA (Yu et al., 2019), and PerceptionTest (Patraucean et al., 2023) are adopted to evaluate the question answering based on actions, object attributes, and object interactions. To assess more complex tasks that require reasoning and information extraction, we evaluate the model on benchmarks tailored for MLLMs, including VideoMME (Fu et al., 2024), MLVU (Zhou et al., 2024), LongVideoBench (Wu et al., 2024), MVBench (Li et al., 2024b), and TempCompass (Liu et al., 2024d). We also test the models ability for ego-centric videos on EgoSchema (Mangalam et al., 2023). 4.1. Comparison with Other Methods We compare our model with other state-of-the-art video understanding models in Table 2. For the proposed slowfast MLLM, we use two settings which use 64 and 96 input frames. The temporal pooling strides are set to be 4 and 6 to generate fast tokens. Despite feeding limited visual tokens to the LLM, the 64-frame model achieves comparable or second-best performance across most benchmarks, while outperforming other models on VideoMME, LongVideoBench, PerceptionTest, NExT-QA, and TempCompass. Increasing the number of slow visual inputs to 96 frames further improves performance on most of the compared benchmarks, demonstrating the scalability and effectiveness of our approach. It is worth noting that, compared to other competitive methods in Table 2, our model still has untapped potential. For instance, it could benefit from leveraging larger datasets and Slow-Fast Architecture for Video Multi-Modal Large Language Models Table 2. Comparisons with other video understanding MLLM. #Tokens refers to the number of visual tokens fed into the LLM. *Indicates the maximum number of visual tokens. Cross-attention-based MLLMs do not input visual tokens into the LLM. Method GPT-4o LLM - Yi-34B VILA (Lin et al., 2024) Yi-34B PLLaVA (Xu et al., 2024a) Qwen2-7B LongVA (Zhang et al., 2024b) InternLM2-7B IXC-2.5 (Zhang et al., 2024a) LLaVA-OV (Li et al., 2024a) Qwen2-7B VideoLLaMA2 (Cheng et al., 2024) Qwen2-7B Kangoroo (Liu et al., 2024c) Oryx-MLLM (Liu et al., 2024e) mPLUG-Owl3 (Ye et al., 2024) LLaMA3-8B Qwen2-7B Qwen2-7B Slow-fast MLLM Qwen2-7B Qwen2-7B #Frames #Tokens - - 16 128 32 64 16 64 64 8 64 96 - - 2,304 18,432 12,800 12,545 1,152 16,384 16,384* 0 1,296 1,296 VideoM 59.9 60.1 - 52.6 55.8 58.2 47.9 56.0 58.3 53.5 60.2 60.3 Esu VideoM 63.3 61.1 - 54.3 58.8 61.5 50.3 57.6 62.6 - 63.0 63.4 Bench EgoSchema - - LV - - 58.1 - 56.7 - 56.3 37.3 69.1 64.7 56.7 32.7 54.6 61.0 61.1 67.5 63.9 54.5 - 67.3 68.9 68.1 68.6 58.0 - - - 60.1 51.7 62.7 - - 59.2 59.8 LongVideoBench Test Perception ActivityNetQ TempCompass ExT-Q - - 53.2 - - 56.5 - 54.8 55.3 52.1 56.6 58.0 - 54.0 - - 34.4 57.1 51.4 - 68.6 - 70.3 70.2 - 67.9 - 68.3 71.0 79.4 - - 81.9 78.6 83.5 83.1 57. 58.0 60.9 50.0 52.8 56.6 50.2 - - - 54.8 54.5 70.9 - - - - 64.8 - 61.3 - - 68.9 67.7 Table 3. Comparisons between different decoder designs. #Frames denotes the input video frames to the LLM, 6416 denotes that 64 frame is compressed into 16 frames via temporal average pooling, and 64/16 denotes using 64 slow frames and 16 fast frames. VMME, MVB, LongVid., and Ego. denote the VideoMME (Fu et al., 2024), MVBench (Li et al., 2024b), LongVideoBench (Wu et al., 2024), and EgoSchema (Mangalam et al., 2023), hereafter. Architecture #Frames VMME MLVU MVB LongVid. Ego. Avg. Self-attn Self-attn Cross-attn Slow-Fast Slow-Fast 16 6416 64 64/16 64/6416 57.2 58.9 50.8 58.5 60. 58.5 64.5 53.2 62.5 65.9 54.9 55.7 49.6 59.5 60.6 48.9 51.2 47.7 53.5 55.4 50.6 50.3 46.2 57.5 61.0 54.0 56.1 49.5 58.3 60.7 additional training stages, as seen in LLaVA-OneVision (Li et al., 2024a), or incorporating more advanced vision encoders as Oryx-MLLM (Liu et al., 2024e). 4.2. Ablation Study In this section, we conduct series of ablation studies to analyze the architectural design choices. To better investigate the slow-fast mechanism, unless otherwise specified, we use direct strided sampling to obtain the fast visual tokens, ensuring that the text embedding relies on information from the slow visual tokens and ruling out the benefits of temporal pooling operations. Different architectures. First, we compare the proposed slow-fast architecture with the self-attention and crossattention architecture. For self-attention models, we use two settings: 1) using 16 frames as input and 2) compressing 64 frames into 16 frames with temporal pooling. For the cross-attention model, we use 64 frames as input. We simply remove the fast visual tokens from the proposed slow-fast architecture without other modifications to construct the cross-attention baseline. For slow-fast MLLM, we use 64 input frames and adopt two different ways to generate the fast visual tokens: uniformly sample 16 frames from the slow visual tokens and compress the slow-visual tokens into 16 frames with temporal pooling. As shown in Table 3, we can conclude the following: 1) The slow-fast architecture demonstrates clear advantages over both the self-attention and cross-attention baselines; 2) Comparing row 1 with row 4, adding slow visual tokens significantly improves performance on EgoSchema (13.6%), MVBench (8.2%), and LongVideoBench (8.9%); 3) Comparing row 3 with row 4, removing the fast tokens causes very significant performance drop, demonstrating the necessity of adding the context directly into the LLM; 4) Comparing row 2 with row 4, slow-fast architecture achieves better average scores than direct frame compression. 5) When generating the fast visual tokens with temporal pooling instead of direct sampling, row 5 achieves the best performance across all benchmarks, indicating that slow-fast architecture can further benefit from more advanced token compression. Cross-attention integration. As discussed in Sec.3.2, cross-attention can either be integrated into stand-alone decoder layer, similar to Flamingo (Alayrac et al., 2022), or inserted into the decoder layer in parallel with the selfattention, denoted as Hybrid. As shown in Table 4, the hybrid implementation achieves the best overall performance. Gate. We compare three different gates: 1) Static gate using learnable scalar for all text tokens; 2) Dyn., dynamic gate used our method which predicts gate value for each text token based on its embedding; 3) C-Dyn., channel-wise dynamic gate further predicts separate values for each channel of the text token, as in mPLUG-Owl3 (Ye et al., 2024). Both Dyn. and C-Dyn. gate use the learnable warm-up factor discussed in Sec. 3.2 to prevent loss spikes during the pre-training. As shown in Table 5, comparing rows 1 and 6 Slow-Fast Architecture for Video Multi-Modal Large Language Models Table 4. Comparisons between different decoder design. Stand-alone denotes that the cross-attention layer is integrated into separate decoder layer while hybrid denotes that the crossattention layer is inserted between the self-attention and the MLP in the standard decoder. Decoder FFN VMME MLVU MVB LongVid. Ego. Avg. Standard Stand-alone Stand-alone Hybrid - - 57.2 57.3 56.9 58.5 58.5 60.6 59.2 62.5 54.9 57.9 58.0 59.5 48.9 51.7 51.6 53.5 50.6 56.4 55.9 57. 54.0 56.8 56.3 58.3 Table 5. Comparisons of different gate designs and weight initializations. Gate Init. VMME VMMEsub MLVU MVB LongVid. Ego. Avg. Static Copy Dyn. Rand. Dyn. Share Dyn. Copy C-Dyn. Copy 57.0 56.0 58.1 58.5 57. 57.4 58.3 61.6 62.6 61.6 59.6 59.0 60.7 62.5 57.2 59.1 57.8 60.3 59.5 60.1 52.5 51.9 51.8 53.5 50.9 53.9 56.4 54.3 55.8 56.1 57.4 57.5 58.3 54.6 56.1 4, the token-wise dynamic gate achieves consistently better performance across benchmarks compared to the static gate, with particularly notable improvement of 9% on VideoMME with subtitles. This highlights the ability of the dynamic gate to reduce distraction from unrelated visual information into particular text embeddings, especially for lengthy textual inputs. However, the channel-wise dynamic gate does not provide additional benefits, suggesting that per-token modulation is sufficient for optimizing the models performance. Key-Value projection initialization. We compare three initialization strategies for the cross-attention key-value projection layer: 1) Rand.: random initialization; 2) Share: sharing key-value projections with the self-attention layer; 3) Copy: initializing projection weights from the corresponding self-attention layer. Table 5 shows both share and copy initialization is significantly better than random initialization, while copy gives more pronounced 4% improvement on the average performance, showing that leveraging pre-trained weights is important. Query for cross-attention. Considering the efficiency, only the text tokens in the input sequence are allowed to attend to the slow visual tokens in our implementation. Here we compare two different settings: 1) All, all tokens are used as the query or 2) Visual, only the fast visual tokens are used as queries. As shown in Table 6, limiting attention to text tokens yields the best performance while the all-token-asquery strategy slightly falls. Allowing only visual tokens to query the slow visual tokens results in the lowest average score, showing that sampling visual information conditioned on text instructions is important. Table 6. Comparisons between different query settings. All means that both the fast visual tokens and text tokens can attend to the slow visual tokens. Query All Visual Text VMME MLVU MVB LongVid. 58.6 57.3 58.5 60.2 59.2 62.5 58.8 59.5 59.5 54.9 52.4 57. Ego. 53.0 54.5 53.5 Avg. 57.1 56.6 58.3 Table 7. Impact of different input frame numbers. If the fast visual tokens are generated by compressing frames using striden sampling (temporal pooling), this configuration is denoted as m-sn (or m-pn for pooling). Config VMME VMMEL MLVU MVB LongVid. Ego. Avg. 48-s3 64-s4 64-p4 96-p6 128-s2p4 57.2 58.5 60.3 61.1 61.5 49.1 50.4 50.2 51.3 52.2 59.6 62.5 65.9 66.5 66.7 58.6 59.5 60.3 60.2 60.6 51.9 53.5 55.1 55.3 57. 53.1 56.1 57.5 58.3 57.4 59.8 65.5 61.7 67.0 62.6 and slow visual tokens. If the fast visual tokens are generated by compressing frames using stride-n sampling (temporal pooling), this configuration is denoted as m-sn (or m-pn for pooling). As shown in Table 7, increasing the number of input frames consistently improves the performance, especially for benchmarks requiring long video comprehension. For example, according to rows 3 and 4, increasing input frames from 64 to 96 improves performance on the long subset of VideoMME (+2%), EgoSchema (+14%), and on average (+3%). The number of fast visual tokens is 1, 296 for both settings, which shows the slowfast architectures ability to process long video sequences without increasing the input sequence length for the LLM. 4.3. Efficiency Analysis Table 8 reports the floating-point operations (FLOPs) of the first forward pass of different architectures. Computation on the vision encoder is not counted. As the input frame grows, slow-fast architecture only introduces marginal additional computation compared to the self-attention baseline. For example, when increasing the input frames from 16 to 64 and 96, the slow-fast architecture adds only 0.16 and 0.24 TFLOPs, accounting for just 0.8% and 1.2% of the total LLM computation. In contrast, the self-attention baseline has more than fourfold (64 frame) and sixfold (96 frame) increase in computation. Furthermore, as each hybrid layer only has two lightweight projection layers and dynamic gate, the total parameter count increases by just 0.2%, highlighting the parameter efficiency of the proposed design. 4.4. Qualitative Results The impact of frame number. Here we compare different input frame counts and compression ratios between the fast Open-ended conversations. Fig. 3 illustrates real-world scenarios that require precise information extraction, OCR, 7 Slow-Fast Architecture for Video Multi-Modal Large Language Models Figure 3. Qualitative examples and comparisons between different input frame numbers. For the video on the left, models trained and tested with 64 and 96 frames are compared, denoted as 64x and 96x. In the video on the right, we further apply test time augmentation by increasing the input frames to 192. More comparisons are available in the supplement. Table 8. Efficiency comparisons between different architectures. # Tokens denotes the number of input visual tokens for the LLMs. 64/16 denotes that the numbers of slow and fast frames are 64 and 16. Arch. # Frames # Tokens # Params LLM TFLOPs CA TFLOPs Self-attn Self-attn Self-attn Self-attn Slow-Fast Slow-Fast 16 32 64 96 64/16 96/16 1296 2592 5184 7776 1296 1296 8.48B 8.48B 8.48B 8.48B 8.50B 8.50B 19.64 40.21 85.57 136.16 19.80 19. - - - - 0.16 0.24 reasoning, and summarization abilities. In the GPU programming example on the left, the model recognizes the text within the video to answer questions accurately. It can also read the program step by step and summarize the function by integrating information across the video content. In the cooking video on the right side, the model lists the required ingredients and operations, showcasing its structured understanding and summarization ability. Fig. 3 also shows the benefits of more input frames intuitively. For example, with only 64 frames, some critical details are lost due to frame sampling, leading to errors or hallucinations. In the second example, we increase the input frame count from 96 to 192 by modifying the sampling stride from 1 to 2, ensuring that the fast frame count remains fixed at 16. Meanwhile, the temporal pooling stride is kept at 6. In this way, more details are captured, such as pink salt and beaten eggs, and corrected errors, e.g., revising hot oil to cold oil to prevent sticking. However, direct interpolation also led to omissions of certain details toward the end of the video. plausible reason is that the length of the slow visual tokens doubles compared to the training. Figure 4. Visualizations of the cross-attention map and the dynamic gate in the hybrid decoder. The cross-attention maps are averaged across different decoder layers, text tokens, and attention heads. The absolute value of the dynamic gate from all the four hybrid decoder layers are visualized. Cross-attention and dynamic gate. We visualize the crossattention maps in Fig. 4, which are averaged across different attention heads, tokens, and decoder layers. When the instruction is How many times did the person launch the objects on the table?, the attention focuses on the hands and objects in the video, particularly during moments of significant motion of the hands and objects. This example shows how the text embeddings dynamically retrieve relevant visual clues. Additionally, the gate value for each text token is visualized in Fig. 4, revealing that different layers prioritize distinct tokens. Slow-Fast Architecture for Video Multi-Modal Large Language Models 5. Conclusion In this paper, we propose slow-fast architecture for video understanding inspired by the human video questionanswering process. The highly compressed video representation, referred to as the fast visual tokens are fed into the LLM serving as the preview context, while the text embeddings can interact with uncompressed video representation, enabling text-aware visual information extraction with linear complexity. Experiments demonstrate that our models significantly outperform conventional self-attentiononly architecture both in performance and the efficiency to process long video inputs. We hope this work inspires further innovations in the architectural design of multi-modal large language models, enabling more dynamic and efficient interactions between language models and other modalities."
        },
        {
            "title": "Acknowledgment",
            "content": "The authors would like to thank NVIDIA and Georgia Tech for their support. This work was in part supported by the National Science Foundation (NSF) and the Institute of Education Sciences under the National AI Institute for Exceptional Education Award (#2229873) and the NSF CAREER Award (#2239840)."
        },
        {
            "title": "References",
            "content": "Alayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J. L., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Chai, W., Song, E., Du, Y., Meng, C., Madhavan, V., Bar-Tal, O., Hwang, J., Xie, S., and Manning, C. D. AuroraCap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024. Chen, L., Wei, X., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Lin, B., Tang, Z., Yuan, L., Qiao, Y., Lin, D., Zhao, F., and Wang, J. ShareGPT4Video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024a. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. InternVL: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2418524198, 2024b. Cheng, Z., Leng, S., Zhang, H., Xin, Y., Li, X., Chen, G., Zhu, Y., Zhang, W., Luo, Z., Zhao, D., and Bing, L. VideoLLaMA 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. Chu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng, Y., Lv, Y., He, J., Lin, J., Zhou, C., and Zhou, arXiv preprint J. Qwen2-Audio Technical Report. arXiv:2407.10759, 2024. Dai, W., Lee, N., Wang, B., Yang, Z., Liu, Z., Barker, J., Rintamaki, T., Shoeybi, M., Catanzaro, B., and Ping, W. NVLM: open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Rozi`ere, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., and et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Feichtenhofer, C., Fan, H., Malik, J., and He, K. Slowfast networks for video recognition. In Proceedings of the International Conference on Computer Vision (ICCV), pp. 62016210, 2019. Fu, C., Dai, Y., Luo, Y., Li, L., Ren, S., Zhang, R., Wang, Z., Zhou, C., Shen, Y., Zhang, M., Chen, P., Li, Y., Lin, S., Zhao, S., Li, K., Xu, T., Zheng, X., Chen, E., Ji, R., and Sun, X. Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Goyal, R., Kahou, S. E., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Frund, I., Yianilos, P., Mueller-Freitag, M., Hoppe, F., Thurau, C., Bax, I., and Memisevic, R. The something something video Slow-Fast Architecture for Video Multi-Modal Large Language Models database for learning and evaluating visual common sense. In Proceedings of the International Conference on Computer Vision (ICCV), pp. 58435851, 2017. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., and Liu, Z. Otter: multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a. Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M., Liu, X., Martin, M., Nagarajan, T., Radosavovic, I., Ramakrishnan, S. K., Ryan, F., Sharma, J., Wray, M., Xu, M., Xu, E. Z., Zhao, C., Bansal, S., Batra, D., Cartillier, V., Crane, S., Do, T., Doulaty, M., Erapalli, A., Feichtenhofer, C., Fragomeni, A., Fu, Q., Gebreselasie, A., Gonzalez, C., Hillis, J., Huang, X., Huang, Y., Jia, W., Khoo, W., Kolar, J., Kottur, S., Kumar, A., Landini, F., Li, C., Li, Y., Li, Z., Mangalam, K., Modhugu, R., Munro, J., Murrell, T., Nishiyasu, T., Price, W., Puentes, P. R., Ramazanova, M., Sari, L., Somasundaram, K., Southerland, A., Sugano, Y., Tao, R., Vo, M., Wang, Y., Wu, X., Yagi, T., Zhao, Z., Zhu, Y., Arbelaez, P., Crandall, D., Damen, D., Farinella, G. M., Fuegen, C., Ghanem, B., Ithapu, V. K., Jawahar, C. V., Joo, H., Kitani, K., Li, H., Newcombe, R. A., Oliva, A., Park, H. S., Rehg, J. M., Sato, Y., Shi, J., Shou, M. Z., Torralba, A., Torresani, L., Yan, M., and Malik, J. Ego4D: Around the world in 3, 000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1897318990, 2022. Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Dong, Y., Ding, M., and Tang, J. CogAgent: visual language model for GUI agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1428114290, 2024. Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., and Gan, C. 3D-LLM: Injecting the 3D world into large language models. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. OpenClip, 2021. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C. L., and Girshick, R. B. CLEVR: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19881997, 2017. Karamcheti, S., Nair, S., Balakrishna, A., Liang, P., Kollar, T., and Sadigh, D. Prismatic VLMs: Investigating the design space of visually-conditioned language models. In Proceedings of the International Conference on Machine Learning (ICML), 2024. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Li, Y., Liu, Z., and Li, C. LLaVAOneVision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L., and Qiao, Y. VideoChat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023b. Li, K., Wang, Y., He, Y., Li, Y., Wang, Y., Wang, L., and Qiao, Y. UniFormerV2: Spatiotemporal learning by arming image vits with video uniformer. In Proceedings of the International Conference on Computer Vision (ICCV), pp. 16321643, 2023c. Li, K., Wang, Y., He, Y., Li, Y., Wang, Y., Liu, Y., Wang, Z., Xu, J., Chen, G., Luo, P., Wang, L., and Qiao, Y. MVBench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2219522206, 2024b. Li, Y., Song, Y., Cao, L., Tetreault, J. R., Goldberg, L., Jaimes, A., and Luo, J. TGIF: new dataset and benchmark on animated GIF description. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 46414650, 2016. Lin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M., and Han, S. VILA: on pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2667926689. IEEE, 2024. doi: 10.1109/CVPR52733. 2024.02520. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines In Proceedings of the with visual instruction tuning. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2628626296, 2024a. Liu, H., You, Q., Han, X., Wang, Y., Zhai, B., Liu, Y., Tao, InfiMM-HD: Y., Huang, H., He, R., and Yang, H. leap forward in high-resolution multimodal understanding. arXiv preprint arXiv:2403.01487, 2024b. Liu, J., Wang, Y., Ma, H., Wu, X., Ma, X., Wei, X., Jiao, J., Wu, E., and Hu, J. Kangaroo: powerful videolanguage model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024c. 10 Slow-Fast Architecture for Video Multi-Modal Large Language Models Liu, Y., Li, S., Liu, Y., Wang, Y., Ren, S., Li, L., Chen, S., Sun, X., and Hou, L. TempCompass: Do video llms really understand videos? In Findings of the Association for Computational Linguistics (Findings in ACL), pp. 8731 8772, 2024d. Liu, Z., Mao, H., Wu, C., Feichtenhofer, C., Darrell, T., and Xie, S. convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1196611976, 2022. Liu, Z., Dong, Y., Liu, Z., Hu, W., Lu, J., and Rao, Y. Oryx MLLM: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024e. Maaz, M., Rasheed, H. A., Khan, S., and Khan, F. VideoChatGPT: Towards detailed video understanding via large vision and language models. In Proceedings of the Association for Computational Linguistics (ACL), pp. 12585 12602, 2024. Mangalam, K., Akshulakov, R., and Malik, J. EgoSchema: diagnostic benchmark for very long-form video language understanding. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Patraucean, V., Smaira, L., Gupta, A., Recasens, A., Markeeva, L., Banarse, D., Koppula, S., Heyward, J., Malinowski, M., Yang, Y., Doersch, C., Matejovicova, T., Sulsky, Y., Miech, A., Frechette, A., Klimczak, H., Koster, R., Zhang, J., Winkler, S., Aytar, Y., Osindero, S., Damen, D., Zisserman, A., and Carreira, J. Perception Test: diagnostic benchmark for multimodal video models. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Ren, S., Yao, L., Li, S., Sun, X., and Hou, L. TimeChat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1431314323, 2024. Shen, X., Xiong, Y., Zhao, C., Wu, L., Chen, J., Zhu, C., Liu, Z., Xiao, F., Varadarajan, B., Bordes, F., Liu, Z., Xu, H., Kim, H. J., Soran, B., Krishnamoorthi, R., Elhoseiny, M., and Chandra, V. LongVU: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. Song, E., Chai, W., Wang, G., Zhang, Y., Zhou, H., Wu, F., Chi, H., Guo, X., Ye, T., Zhang, Y., Lu, Y., Hwang, J., and Wang, G. MovieChat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1822118232, 2024. Team, Q. Qwen2 Technical Report. arXiv preprint arXiv:2407.10671, 2024. Tong, S., Brown, E., Wu, P., Woo, S., Middepogu, M., Akula, S. C., Yang, J., Yang, S., Iyer, A., Pan, X., Wang, A., Fergus, R., LeCun, Y., and Xie, S. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), pp. 59986008, 2017. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and Lin, J. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Wu, H., Li, D., Chen, B., and Li, J. LongVideoBench: benchmark for long-context interleaved video-language understanding. In The Conference on Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS D&B Track), 2024. Xiao, J., Shang, X., Yao, A., and Chua, T. NExT-QA: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 97779786, 2021. Xu, L., Zhao, Y., Zhou, D., Lin, Z., Ng, S., and Feng, J. PLLaVA : Parameter-free LLaVA extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024a. Xu, M., Gao, M., Gan, Z., Chen, H., Lai, Z., Gang, H., Kang, K., and Dehghan, A. SlowFast-LLaVA: strong training-free baseline for video large language models. arXiv preprint arXiv:2407.15841, 2024b. Xue, F., Chen, Y., Li, D., Hu, Q., Zhu, L., Li, X., Fang, Y., Tang, H., Yang, S., Liu, Z., He, E., Yin, H., Molchanov, P., Kautz, J., Fan, L., Zhu, Y., Lu, Y., and Han, S. LongVILA: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. Ye, J., Xu, H., Liu, H., Hu, A., Yan, M., Qian, Q., Zhang, J., Huang, F., and Zhou, J. mPLUG-Owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. Yu, Z., Xu, D., Yu, J., Yu, T., Zhao, Z., Zhuang, Y., and Tao, D. ActivityNet-QA: dataset for understanding complex web videos via question answering. In The Association 11 Slow-Fast Architecture for Video Multi-Modal Large Language Models for the Advancement of Artificial Intelligence (AAAI), pp. 91279134, 2019. Zhang, H., Li, X., and Bing, L. Video-LLaMA: An instruction-tuned audio-visual language model for video In Proceedings of the Conference on understanding. Empirical Methods in Natural Language Processing (EMNLP), pp. 543553, 2023. Zhang, P., Dong, X., Zang, Y., Cao, Y., Qian, R., Chen, L., Guo, Q., Duan, H., Wang, B., Ouyang, L., Zhang, S., Zhang, W., Li, Y., Gao, Y., Sun, P., Zhang, X., Li, W., Li, J., Wang, W., Yan, H., He, C., Zhang, X., Chen, K., Dai, J., Qiao, Y., Lin, D., and Wang, Internlm-xcomposer-2.5: versatile large vision J. language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024a. doi: 10.48550/ARXIV.2407.03320. Zhang, P., Zhang, K., Li, B., Zeng, G., Yang, J., Zhang, Y., Wang, Z., Tan, H., Li, C., and Liu, Z. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024b. Zhang, R., Gui, L., Sun, Z., Feng, Y., Xu, K., Zhang, Y., Fu, D., Li, C., Hauptmann, A., Bisk, Y., and Yang, Y. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024c. Zhang, Y., Wu, J., Li, W., Li, B., Ma, Z., Liu, Z., and Li, C. Video instruction tuning with synthetic data. arXiv preprint arXiv: 2410.02713, 2024d. Zhou, J., Shu, Y., Zhao, B., Wu, B., Xiao, S., Yang, X., Xiong, Y., Zhang, B., Huang, T., and Liu, Z. MLVU: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. Zhu, Y., Xie, C., Liang, S., Zheng, B., and Guo, S. FocusLLaVA: coarse-to-fine approach for efficient and effective visual token compression. arXiv preprint arXiv:2411.14228, 2024. Slow-Fast Architecture for Video Multi-Modal Large Language Models This appendix contains the following contents: 1. More qualitative results in Sec. A. 2. More details on the model architecture in Sec. B. 3. Evaluation prompt for all the tested benchmarks in Sec. C. A. More Qualitative Results More qualitative results of our model are presented in Fig. 5, including question answering and summarization tasks. Additionally, we provide qualitative comparisons with state-of-the-art methods, LLaVA-OneVision (Li et al., 2024a) and LLaVA-Video (Zhang et al., 2024d). Fig. 6, Fig. 7, and Fig. 8 illustrate examples for video description, summarization, and question answering, respectively. Our model generates more accurate details and maintains better alignment with the video content, whereas the compared methods exhibit hallucinations and omit certain details. 13 Slow-Fast Architecture for Video Multi-Modal Large Language Models Figure 5. More qualitative examples of our model. Slow-Fast Architecture for Video Multi-Modal Large Language Models Figure 6. Comparison with other state-of-the-art Video MLLMs on video description. 15 Slow-Fast Architecture for Video Multi-Modal Large Language Models Figure 7. Comparison with other state-of-the-art Video MLLMs on information summarization. Slow-Fast Architecture for Video Multi-Modal Large Language Models Figure 8. Comparison with other state-of-the-art Video MLLMs on video question understanding. 17 Slow-Fast Architecture for Video Multi-Modal Large Language Models B. Model Architecture Details Here, we provide more details of the architecture design, including the multi-modal projector, the fast visual token compression process, and the hybrid decoder layer. Multi-modal projector. We adopt the multi-modal projector design from LLaVA 1.5 (Liu et al., 2024a), which consists of two linear projection layers with GELU activation in between. Fast visual token compression. Below, we detail the process of obtaining fast visual tokens. The input of this process is: 1) feature of video frames with the shape ; 2) temporal sampling stride k; 3) temporal pooling stride t; and 4) minimum number of fast frames m. The fast visual tokens are obtained through the following steps: 1. Zero padding video frames on temporal axis to frames, ensuring the total length is divisible by t. 2. Uniformly sample = max( , m) frames on the padded video feature. 3. Apply adaptive average pooling along the temporal axis to further compress the sampled feature to max( t , m) frames. 4. Flatten the compressed features along the spatial dimensions to generate the fast visual tokens. Hybrid decoder layer. The detailed architecture of the hybrid decoder layer is illustrated in Fig. 9. Newly introduced parameters and modules are highlighted with pink outlines, while the remaining components originate from the pre-trained decoder layer. Figure 9. Detailed architecture illustration of the hybrid decoder layer. Slow-Fast Architecture for Video Multi-Modal Large Language Models C. Prompts for Evaluation Here we specify the prompts we use for different benchmarks. For multi-choice selection benchmarks, including: VideoMME (w/w.o. subtitles), MVBench, MLVU, LongVideoBench, EgoSchema, PerceptionTest, and the multi-choice split of TempCompass, we unify the prompt for testing as the format below: Evaluation prompt for multi-choice question answering benchmarks. <Video> Select the best answer to the following multiple-choice question based on the video. <Question> A. <Option 1> B. <Option 2> C. <Option 3> D. <Option 4> E. <Option 5> Other options Answer with the options letter from the given choices directly. Specifically, for VideoMME with subtitles, we use the following prompt to integrate the subtitle information. Evaluation prompt for VideoMME with subtitiles. <Video> This videos subtitles are listed below: <Subtitles> Select the best answer to the following multiple-choice question based on the video and the subtitles. <Question> A. <Option 1> B. <Option 2> C. <Option 3> D. <Option 4> Other options Answer with the options letter from the given choices directly. For the open-ended benchmark ActivityNet, we use the following format as below: Evaluation prompt for ActivityNet-QA. <Video> <Question> Answer the question using single word or phrase."
        }
    ],
    "affiliations": [
        "HKPU",
        "NVIDIA",
        "SHI Labs @ Georgia Tech",
        "SUNY Buffalo"
    ]
}