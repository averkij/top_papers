{
    "paper_title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos",
    "authors": [
        "Chaoyang Wang",
        "Kaituo Feng",
        "Dongyang Chen",
        "Zhongyu Wang",
        "Zhixun Li",
        "Sicheng Gao",
        "Meng Meng",
        "Xu Zhou",
        "Manyuan Zhang",
        "Yuzhang Shang",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 8 1 9 6 1 . 2 1 5 2 : r AdaTooler-V: Adaptive Tool-Use for Images and Videos Chaoyang Wang1,6 Kaituo Feng1 Dongyang Chen2 Zhongyu Wang3 Zhixun Li4 Sicheng Gao7 Meng Meng6 Xu Zhou6 Manyuan Zhang1 Yuzhang Shang5 Xiangyu Yue1 1MMLab, CUHK 2THU 3SJTU 4DB Group, CUHK 5UCF 6Sangfor 7JMU Home: https://github.com/CYWang735/AdaTooler-V HF:https://huggingface.co/AdaTooler-V"
        },
        {
            "title": "Abstract",
            "content": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chainof-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether visual problem truly requires tools. First, we introduce AT-GRPO, reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaToolerV-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released. 1. Introduction Recent advancements have highlighted the potential of rulebased Reinforcement Learning (RL) in enhancing the reasoning abilities of Large Language Models (LLMs) [18, 76, 80]. In particular, DeepSeek-R1 [7] demonstrates the effectiveness of employing the GRPO [48] algorithm to incentivize strong reasoning with long Chain-of-Thought (CoT) in Project Leader. Corresponding Author. Figure 1. (a) Compared with existing models that blindly invoke vision tools, AdaTooler-V adaptively invokes tools by determining whether the problem truly requires tools. (b) Distribution of values in the AdaTooler-V-300k dataset, where positive and negative values correspond to tool-helpful and tool-unhelpful samples. Here, is computed as the difference in average accuracy when Qwen2.5-VL-72B-Instruct [2] solves the same sample with and without tool-use. LLMs. Inspired by DeepSeek-R1s success, many subsequent studies have extended this paradigm to Multimodal Large Language Models (MLLMs) [9, 11, 22, 57, 61]. Notable examples include Vision-R1 [22], Video-R1 [11] and OneThinker [12], which apply RL to improve visual reasoning abilities. 1 In the field of multimodal reasoning, rising trend is the multimodal interleaved CoT paradigm, also known as Thinking with Images. In this paradigm, models dynamically interact with external vision tools (e.g., cropping, frame extraction) throughout the reasoning process [26, 65, 81, 86]. Such visual interactions enable the model to repeatedly focus on fine-grained visual details that text-only reasoning would otherwise overlook, thereby yielding substantial performance gains on challenging visual tasks. However, existing models usually exhibit blind tool-use, invoking vision tools even when they are unnecessary. This phenomenon stems from limitation in current approaches: models lack an explicit mechanism for determining when tools should be invoked, and reward functions may even blindly encourage tool-use. Nevertheless, as illustrated in Fig. 1, not all problems require tool-use. Many visual reasoning tasks can be solved efficiently using text-based CoT, and forcing tool-use can even degrade the final prediction quality. This is primarily because blind tool-use can induce overthinking [4, 8] during reasoning, driving the model to explore unnecessary trajectories, and deviate from the optimal reasoning path [53]. Moreover, frequent and unnecessary tool invocations gradually weaken the models reliance on the original visual input, making it harder for the model to focus on critical visual cues [59]. In addition, blind tool-use may induce series of meaningless tool operations [32]. For tasks that inherently do not require tool-use, each extra tooluse introduces unnecessary computational overhead, thereby increasing the overall inference cost. To address these challenges, we propose AdaTooler-V, an MLLM equipped with adaptive tool-use ability. Unlike previous approaches, AdaTooler-V adaptively adopts textbased CoT reasoning for problems that do not require tools, while progressively invoking vision tools to refine reasoning for tasks that do. The core of our approach is novel reinforcement learning algorithm named Adaptive Tool-use GRPO (AT-GRPO). Specifically, we define Tool Benefit Score for each sample, which quantifies the genuine performance gain provided by tool-use. AT-GRPO adaptively adjusts reward scales based on this score: it rewards tool-use only when it yields tangible improvements and penalizes redundant invocations. This mechanism enables the model to autonomously learn favorable and generalizable reasoning strategy that optimizes both model performance and inference costs. Besides, to support multimodal joint training, we construct two large-scale datasets: AdaTooler-V-CoT-100k for SFT cold start, and AdaTooler-V-300k for RL training. These datasets cover multiple modalities, including singleimage, multi-image, and videos. They also span diverse visual reasoning tasks such as mathematics, visual counting, logical reasoning, spatial understanding, etc. Our two-phase training framework first establishes rich reasoning patterns and behavioral priors during the SFT stage using multi-round tool-interaction trajectories from AdaTooler-V-CoT-100k, and then further optimizes the models reasoning strategy in the RL stage using AdaTooler-V-300k combined with the AT-GRPO algorithm. This enables AdaTooler-V to perform adaptive tool-use and achieve significant performance improvements over the base model across overall multimodal reasoning benchmarks. In summary, our contributions are as follows: We propose AdaTooler-V, an MLLM equipped with adaptive tool-use ability. To support training, we construct two datasets: AdaTooler-V-CoT-100k for SFT and AdaTooler-V-300k for RL training, covering diverse multimodal reasoning tasks and multiple modalities. We introduce AT-GRPO, reinforcement learning algorithm that adjusts reward scales using sample-specific Tool Benefit Score, ensuring tools are invoked only when they provide genuine improvements. Comprehensive experiments across 12 benchmarks demonstrate the effectiveness of AdaTooler-V. Notably, AdaTooler-V-7B achieves 89.8% accuracy on V* bench, outperforming the proprietary GPT-4o model. 2. Related Work Multimodal Reasoning. The field of multimodal large language model reasoning seeks to develop models with human-level capabilities for addressing complex tasks that demand comprehensive understanding and inference across diverse modalities [35, 39, 49, 62, 70, 83]. DeepSeek-R1 [7] has demonstrated that reinforcement learning (RL)-based post-training can significantly enhance the reasoning capabilities of large language models (LLMs). Building upon the R1 paradigm, several subsequent works [30, 60, 67, 77] have applied similar post-training paradigms to multimodal large language models (MLLMs) to boost their performance across variety of tasks. These include: Mathematical and scientific image-based visual question answering (VQA) [21, 47]; Image segmentation and grounding [3, 38, 39, 49, 63, 66, 75]; Video-based VQA [5, 11, 29, 67]; Video spatial and temporal grounding [16, 34, 46, 70]. Unlike prior approaches that predominantly rely on text-based CoT, we adopt multimodal interleaved CoT, allowing the model to ground intermediate reasoning steps in visual observations and thereby enhance its visual understanding capabilities. Thinking with Images. The thinking with images paradigm has emerged as promising direction for enhancing multimodal reasoning capabilities. Unlike text-based chain-of-thought (CoT) reasoning, this framework enables models to dynamically invoke operations such as local zooming or video frame extraction, allowing them to progressively explore visual regions, verify hypotheses, and narrow the Figure 2. Case reasoning trajectory of AdaTooler-V. For single-image and video questions, the model alternates between internal reasoning, vision tool invocations and final answers, enabling zoom-in on fine-grained regions and inspection of informative clips. In contrast, for the multi-image clock example, AdaTooler-V solves the problem purely via text-based CoT, illustrating its ability to adaptively decide when vision tools are truly necessary. solution space [45, 52, 55, 56, 78, 84]. For example, OpenThinkIMG [54] introduces an end-to-end visual-tool reinforcement learning framework. MVoT [28] conceptualizes visualization as an intermediate representation within the reasoning process. PixelReasoner [65] leverages curiositydriven reinforcement learning to incentivize pixel-level reasoning capabilities. Whereas, VITAL [78] explores incorporating multimodal interleaved CoT into video reasoning, thereby enhancing the models video comprehension capabilities. Despite the remarkable progress of these approaches in multimodal reasoning, existing models often exhibit blind tool-use invocation during the reasoning process. 3. Method 3.1. Overview Motivation. In multimodal reasoning tasks, some problems can be accurately answered through text-based chainof-thought (CoT) reasoning, while others require tools to perceive visual details. However, existing models invoke tools even when they are unnecessary, triggering model overthinking and interfering with the correct reasoning path. Moreover, redundant tool-use incurs additional inference overhead. Motivated by this observation, we propose new perspective for multimodal interleaved CoT reasoning: enabling reasoning models to adaptively choose between text-based CoT reasoning and multimodal interleaved CoT reasoning by determining whether visual problem truly requires tool-use. This approach reduces inference cost while maintaining or even improving overall model performance. To achieve this, we introduce AT-GRPO, novel reinforcement learning algorithm designed to guide the model in deciding when to invoke tools during the reasoning process. Overall Agentic Pipeline. Given user query and an input image/video, the policy model adaptively decides whether to invoke tools. For problems that dont require tool-use, the model can directly produce single thought to derive the final answer. In contrast, when facing problems that require tool-use, the model enters an iterative reasoning process, sequentially generating thoughts Ti and actions Ci. Through continuous interaction with the environment, the model progressively refines its understanding until it reaches the final 3 Figure 3. The data distribution of our AdaTooler-V-300k dataset. answer. The action interacts with the environment by invoking image-related tools, yielding new observation Ei. This observation is appended to the interaction history and subsequently fed back into the policy model. The thought actionobservation loop continues until the model outputs final answer or reaches predefined limit on context length or interaction turns. The core components of the pipeline are detailed below. Thought (Ti): Represents the models internal reasoning process for selecting the next action, conditioned on the accumulated interaction history and the most recent observation. To enhance exploratory reasoning in complex scenarios, we encourage diverse reasoning trajectories within thoughts to facilitate trial-and-error exploration. Action (Ci): The action space includes four primary vision tools: (1) CropImg: Zooms in or crops the image based on the specified bounding box. (2) FrameAt: Retrieves single frame from the video at specific time (in seconds). (3) VideoClip: Extracts video clip between start and end time; and (4) PathTracer: Draws trajectory or connection between two points on the specified image. This formulation enables the model to act flexibly on any intermediate observation within the reasoning trajectory. Observation (Ei): The visual feedback resulting from executing Ci in the environment. Specifically, Ei corresponds to an image patch cropped from either the original input or historical observation. Two-Phase Training. Our training framework consists of two stages: Supervised Fine-Tuning (SFT): The model is initially finetuned on thousands of multi-turn trajectories involving tool interactions, serving as cold-start data. This stage aims to enable the model to generate coherent trajectories characterized by diverse and robust reasoning patterns. Reinforcement Learning with Verifiable Rewards (RLVR): Following SFT, we continue training the model using the proposed AT-GRPO algorithm. This reinforcement learning phase aims to guide the model beyond the rigid patternmatching behavior established during SFT, encouraging it to autonomously explore more effective reasoning strategies. 3.2. Training Data Collection High-quality training data is essential for enhancing visual reasoning capabilities in MLLMs. In this section, we describe the construction of AdaTooler-V-300k for RL training and AdaTooler-V-CoT-100k for SFT cold-start. Data Collection and Curation. The dataset aims to cover multiple modalities, including single-image, multi-image, and video. These image-based samples primarily serve to teach the model broad spectrum of reasoning skills across various domains and difficulty levels, including mathematics, spatial logic, and expert-level knowledge. Such data enable the model to acquire generalized reasoning capabilities in static contexts. In contrast, video-based data are utilized to enhance the models temporal reasoning ability, allowing it to understand event progression, capture frame-to-frame dependencies, and infer outcomes based on motion and causal dynamics over time. Our dataset is constructed from multiple public sources, with careful sampling and balancing across different subsets. The final composition of AdaTooler-V-300k is summarized in Fig. 3 and additional details are provided in Appendix A. CoT Annotation. To facilitate effective initialization during the SFT stage, we leverage Qwen2.5-VL-72B-Instruct 4 Figure 4. An illustration of our proposed AT-GRPO. [2] to automatically produce Chain-of-Thought (CoT) rationales for all samples in AdaTooler-V-300k. The complete prompt specification employed for CoT generation is included in Appendix C. Following generation, we apply sequence of rule-based filtering procedures to eliminate lowquality or semantically inconsistent outputs. This process yields high-fidelity corpus, AdaTooler-V-CoT-100k, which forms the foundation for the cold-start stage of SFT. Data Type and Rule-based Reward Design. Our RL framework follows the rule-based reward paradigm of DeepSeek-R1 [7], necessitating reward signals that are both reliable and precise. To this end, the majority of training samples are designed around tasks with easily verifiable outputs, such as multiple-choice or numerical answer formats, enabling accurate reward computation via simple rules and ensuring stable RL training. To improve the models flexibility and generalization across diverse tasks and formats, we additionally incorporate smaller portion of more complex data types, including free-form generation, OCR tasks, and regression problems, which are critical for real-world applications."
        },
        {
            "title": "The data types and their corresponding reward functions",
            "content": "are summarized as follows: Multiple Choice: Rewards are assigned based on an exact match between the model prediction and the ground-truth option. Numerical QA: Rewards are given according to whether the predicted numerical value precisely matches the reference. OCR: Rewards are computed using the Word Error Rate (WER), which measures the edit distance between the predicted text and the ground-truth transcription. Free-form QA: Rewards are determined by the average of ROUGE-1, ROUGE-2, and ROUGE-L scores, assessing the similarity between the generated output and the reference answer. 3.3. Adaptive Tool-use GRPO Training To enable adaptive tool-use during the reasoning process, we propose Adaptive Tool-use GRPO (AT-GRPO), which guides the model to invoke tools only when they offer genuine performance gain, thereby improving model performance and reducing inference overhead, as illustrated in Fig. 4. For each input query qi, we define Tool Benefit Score Si during the data annotation stage to quantify the performance improvement brought by tool-use: Si = S+(qi) S(qi) (1) Here, S+ and denote the average accuracy of the model when reasoning with and without tool-use, respectively. Each query is evaluated eight times using Qwen2.5-VL72B-Instruct [2] (i.e., eight runs with tool-use and eight runs without tool-use) and the averaged accuracy gap is used as Si. 5 Table 1. Comparison of models on single-image and multi-image benchmarks. The first six evaluation benchmarks belong to single-image comprehension tasks, and the last two evaluation benchmarks belong to multi-image understanding tasks. Model V* MME InfoVQA MMBench MathVista MMSI-Bench SPAR-Bench Proprietary Models GPT-4o [44] Gemini 1.5 Pro [17] Open-Source Models 65.2 71.7 2328 2415.4 InternVL3-8B [87] 1510.7 LLaVA-1.5-7B [36] 1580.0 LLaVA-OneVision-7B [27] SophiaVL-R1-7B [9] 2403.8 Qwen2.5-VL-7B-Instruct [1] 78.5 2347.0 Open-Source o3-like Image Models Pixel Reasoner [65] DeepEyes [85] Mini-o3 [25] Thymes [82] VILASR [71] 84.3 85.6 88.2 82.2 AdaTooler-V-7B 89.8 2460.8 80.7 81. 76.8 68.8 82.6 84.0 86.0 82.1 83.4 64.3 80.8 85.4 83.4 87.8 63.8 63.9 71.6 63.2 71.3 68.2 70.1 70.0 74.5 30.3 36. 25.7 25.9 30.2 36.8 33.6 23.65 33.07 37. 40.3 The adaptive-tool reward Rt is then formulated as: (cid:32) Rt = Si exp γ (cid:19)2(cid:33) (cid:18) ntool nmax nmax (2) where Si represents the improvement in accuracy attributed to tool-use, ntool is the number of tool-use during the reasoning trajectory, nmax denotes the maximum allowable number of tool-use, and γ controls the sensitivity of the Gaussian decay to tool frequency, making the reward variation smoother. Here we set γ = 2. This design encourages the model to adaptively invoke tools by determining whether visual problem truly requires tool-use. Specifically, when problem does not warrant the use of vision tools (Si < 0), the model is penalized if it still invokes such tools during the reasoning process, and the penalty increases progressively with the number of tool-use. In contrast, when problem benefits from tool-use (Si > 0), the model receives positive reward and the reward similarly grows with the number of tool-use. We adopt this exponential form for its simplicity, differentiability, and stable gradient behavior during training. The total reward for response is defined as: Ri = Ro + α Rt (3) where α balances the relative weight of tool-use reward in the total objective and Ro denotes the base reward of response i, including correctness and formatting components, following the formulation in [19]. The combined reward Ri is then used to compute the advantage for policy optimization during GRPO training. The advantage Ai is calculated within each group as: Ai = Ri mean({R1, R2, . . . , RG}) std({R1, R2, . . . , RG}) (4) Following DeepSeek-R1 [7], the final policy objective for AT-GRPO is given by: JATGRPO(θ) = (cid:20) 1 G (cid:88) πθ(oiq) πθold(oiq) i=1 qP (Q), {oi}G i=1πθold (oq) (cid:21) Ai β DKL(πθπref ) (5) Through this formulation, AT-GRPO enables the model to autonomously learn to adaptively invoke vision tools, thereby enhancing model performance and reducing inference cost. 4. Experiments 4.1. Setup Benchmarks. Following prior works [11, 64, 72], we employ greedy decoding to systematically evaluate our proposed model and other baselines across diverse suite of multimodal benchmarks, encompassing critical capabilities such as general knowledge comprehension, high-resolution Table 2. Comparison of models on video benchmarks. Model Frames VSI-Bench VideoMMMU MVBench Video-MME(w/o sub) Video-Holmes Proprietary Models GPT-4o [44] Gemini 1.5 Pro [17] Open-Source Models InternVL3-8B [87] VideoChat-R1 [33] Video-CCAM [10] Video-XL [51] Qwen2.5-VL-7B-Instruct [1] Qwen2.5-VL-7B-Instruct [1] Qwen2.5-VL-7B-Instruct [1] Video-R1 [11] 32 64 128 Open-Source o3-like Video Models FrameMind [15] Open-o3 Video [43] Video-Thinker [68] VILASR [71] AdaTooler-V-7B AdaTooler-V-7B AdaTooler-V-7B 32 64 128 34.0 45.4 42.1 29.8 30.9 34.8 37.1 45.4 46.7 47.9 49. 61.2 53.9 52.3 47.4 49.1 51.3 52.4 52.3 54.6 55.1 56.8 64.6 60.5 75.4 67.9 62.8 55.3 58.2 59.8 62.3 64. 64.2 68.4 70.2 71.5 71.9 75.0 66.3 72.2 50.1 55.5 56.1 58.6 60.4 61.4 60.9 63.6 62.5 63.4 66. 42.0 41.2 33.0 27.8 29.9 33.5 36.5 43.2 55.6 56.4 58.3 image detail perception, logical reasoning, spatial reasoning, and chart interpretation. Specifically, for the image modality, we select seven representative benchmarks: V* [69], MME [13], InfoVQA [42], MMBench [37], MathVista [41], MMSI-Bench [74], and SPAR-Bench [79]. For the video modality, we adopt five representative benchmarks: VSI-Bench [73], VideoMMMU [20], MVBench [31], VideoMME [14], and Video-Holmes [6]. Implementation Details. We use 8 NVIDIA H100 (80GB) GPUs to train our model. The training framework is based on verl-tool [23], which extends the functionalities of verl [50] and vLLM [24], providing additional support for multimodal tool-augmented multi-turn training and evaluation. Our model is initialized based on Qwen2.5-VL-7B-Instruct [58]. First, we perform supervised fine-tuning (SFT) on the AdaTooler-V-CoT-100k dataset to obtain the Qwen2.5VL-7B-SFT model, where the number of epochs is set to 1, the batch size is set to 16, and the learning rate is set to 5. Subsequently, we conduct reinforcement learning (RL) training on the AdaTooler-V-300k dataset to generate the final AdaTooler-V model, where the batch size is set to 32, the KL divergence coefficient to 0.04, and the learning rate to 5 107. The maximum response length is limited to 4096 tokens. The model is optimized with AdamW [40] throughout the training process. The hyperparameter α in Eqn. 3 is set to 0.6. 4.2. Main Results Image Benchmarks. As summarized in Tab.1, AdaToolerV-7B achieves state-of-the-art performance across multiple single-image comprehension datasets. On the challenging high-resolution V* benchmark, our method reaches 89.8% accuracy, outperforming recent tool-based models including Pixel Reasoner (84.3%), DeepEyes (85.6%), and Mini-o3 (88.2%). Moreover, compared to the base model Qwen2.5-VL-7B-Instruct, AdaTooler-V provides substantial +11.3% absolute improvement, highlighting the effectiveness of adaptive tool-use in high-resolution visual reasoning. Beyond high-resolution tasks, AdaTooler-V also shows consistent gains on general reasoning benchmarks such as MME, MathVista, InfoVQA, and MMBench, indicating strong cross-domain generalization. Notably, the model achieves 74.5% on MathVista, surpassing Qwen2.5VL-7B-Instruct by over 6 percentage points. These improvements suggest that controlling tool-invocation frequency allows the model to focus on essential visual cues rather than over-processing evidence. On multi-image reasoning tasks, 7 Table 3. Ablation study on training stages. Train Stage V* MathVista VSI-Bench MVBench Avg. GRPO SFT+GRPO 85.1 87.0 SFT+AT-GRPO 89. 71.8 73.2 74.5 40.7 42.3 46. 65.9 67.7 68.4 65.9 67.6 69. Table 4. Ablation study on the α in Eqn. 3. α 0.2 0.4 0.6 0.8 V* 88.1 88.9 89.8 89.2 MathVista VSI-Bench MVBench Avg. 73.6 74.1 74.5 73.9 44.2 43.9 46.7 45.1 67.9 68.2 68.4 68. 68.5 68.7 69.9 69.1 Table 5. Ablation study on tool-use. Model V* MathVista VSI-Bench MVBench Avg. Qwen2.5-VL-7B 78.5 RL-wo-tool 84.4 AdaTooler-V-7B 89.8 68.2 72.6 74.5 31. 39.9 46.7 63.8 65.0 68.4 60. 65.5 69.9 AdaTooler-V-7B yields clear advantages on MMSI-Bench (36.8) and SPAR-Bench (40.3), outperforming all tested baselines. Since these benchmarks require spatial correspondence and relational reasoning across multiple images, results reflect AdaTooler-Vs ability to selectively invoke tools when information extraction becomes visually non-trivial. Video Benchmarks. As is illustrated in Tab. 2, AdaToolerV displays substantial performance gains over strong videoreasoning baselines. For example, our model achieves 46.7% on VSI-Bench, 54.6% on VideoMMMU, and 68.4% on MVBench using only 32 frames, surpassing both Qwen2.5-VL-7B-Instruct and Video-R1 based models. The Video-Holmes benchmark further highlights AdaTooler-Vs strengths in complex, long-range video reasoning. Our method obtains 55.6%, compared to 27.8% for Qwen2.5-VL7B-Instruct and 36.5% for Video-R1, showing more than 2 improvement over the base model in causal, sequential inference settings. Moreover, we observe consistent performance gains across nearly all benchmarks as the number of input frames increases. This suggests that richer contextual cues and temporal information can further enhance the models reasoning capability. 4.3. Ablation Study 4.3.1. Effectiveness of AT-GRPO To validate the effectiveness of the proposed AT-GRPO algorithm, we compare multiple training stages: vanilla GRPO, which skips the SFT cold-start phase and directly applies (a) Accuracy Reward (b) Response Length Figure 5. RL training curves. reinforcement learning for training; SFT+GRPO, which replaces our proposed AT-GRPO algorithm with the standard GRPO method. As shown in the last two rows of Tab. 3, incorporating the proposed AT-GRPO training strategy leads to substantial performance improvement. These results confirm that dynamically adjusting tool-use rewards based on the Tool Benefit Score enables the model to invoke tools only when necessary, effectively avoiding redundant visual interactions and leading to more accurate reasoning. 4.3.2. Necessity of the SFT We further investigate the necessity of the supervised finetuning (SFT) stage prior to reinforcement learning. As shown in the first row of Tab. 3, skipping the SFT cold start also leads to degraded performance. This is primarily because the model lacks structured priors for tool-interaction reasoning, making it difficult to produce coherent and wellformed reasoning trajectories during the early stages of RL training. In contrast, introducing SFT as cold start equips the model with essential tool-use patterns and multimodal reasoning priors, which serve as strong foundation for subsequent RL optimization. This initialization leads to more stable and progressive training process, enabling the model to better exploit the benefits of reinforcement learning. Overall, SFT plays critical role in bootstrapping structured tool-use behaviors, and is indispensable for enabling RL to further refine adaptive reasoning strategies. 4.3.3. Analysis of α We further perform analysis on the magnitude of the adaptive-tool reward, governed by the hyperparameter α, as illustrated in Tab. 4. We observe moderate decrease in performance when α is set to 0.2 or 0.4, whereas α values of 0.6 and 0.8 yield comparable and consistently strong results. These findings suggest that the model exhibits low sensitivity to the selection of α within reasonable range. 4.3.4. Effectiveness of Tool-use To assess the effectiveness of tool-use, we train variant of Qwen2.5-VL-7B-Instruct using end-to-end RL with textbased CoT reasoning on the same training dataset. As shown in Tab. 5, disabling tool interactions leads to consistent drops 8 across four benchmarks. For example, from 89.8% to 84.4% on V* and from 46.7% to 39.9% on VSI-Bench. These results verify that vision tool-use provides complementary evidence beyond text-based reasoning and is essential for accurate multimodal understanding. 4.4. Training Curves Fig. 5 illustrates the dynamics of several critical metrics during the RL process, from which we derive the following observations and findings. As shown in Fig. 5 (a), the models accuracy improves significantly throughout the training process, increasing from approximately 0.60 to around 0.70. This indicates that as the model learns through reinforcement learning and tool interactions, its ability to generate accurate answers continuously improves. In Fig. 5 (b), the average response length rapidly decreases in the initial stages before stabilizing. This phenomenon originates from the SFT phase, where the model learns to invoke tools based on instructions to solve problems. However, many of the images in the dataset are relatively simple, making many of the tool-use unnecessary. During the reinforcement learning phase, the model quickly realizes that, for most tasks, generating direct text response is more efficient than performing tool-based analysis. As result, unnecessary tool-use decrease rapidly, leading to reduction in response length. 5. Conclusion We introduced AdaTooler-V, multimodal large language model equipped with adaptive tool-use capability. To achieve this, we introduced AT-GRPO, reinforcement learning algorithm that leverages sample-specific Tool Benefit Score to dynamically modulate rewards, encouraging tools to be used only when they provide genuine performance gains. To support training, we curate two datasets, AdaTooler-VCoT-100k for SFT cold start and AdaTooler-V-300k for RL. Experiments across twelve benchmarks validate the effectiveness of our approach. We believe it provides promising foundation for future research on tool-augmented MLLMs."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report, 2025. 6, 7 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 5 [3] Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, and Yansong Tang. Univg-r1: Reasoning guided universal visual grounding with reinforcement learning. arXiv preprint arXiv:2505.14231, 2025. 2 [4] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think that much for 2+3=? on the overthinking of o1-like llms, 2025. 2 [5] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? arXiv preprint arXiv:2505.21374, 2025. 2 [6] Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning?, 2025. [7] DeepSeek-AI et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 1, 2, 5, 6 [8] Chenrui Fan, Ming Li, Lichao Sun, and Tianyi Zhou. Missing premise exacerbates overthinking: Are reasoning models losing critical thinking skill?, 2025. 2 [9] Kaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan Zhou, and Xiangyu Yue. Sophiavl-r1: Reinforcing mllms reasoning with thinking reward. arXiv preprint arXiv:2505.17018, 2025. 1, 6 [10] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos, 2024. 7 [11] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 1, 2, 6, 7 [12] Kaituo Feng, Manyuan Zhang, Hongyu Li, Kaixuan Fan, Shuang Chen, Yilei Jiang, Dian Zheng, Peiwen Sun, Yiyuan Zhang, Haoze Sun, et al. Onethinker: All-in-one reasoning model for image and video. arXiv preprint arXiv:2512.03043, 2025. 1 [13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji, Caifeng Shan, and Ran He. Mme: comprehensive evaluation benchmark for multimodal large language models, 2025. [14] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2025. 7 [15] Haonan Ge, Yiwei Wang, Kai-Wei Chang, Hang Wu, and Yujun Cai. Framemind: Frame-interleaved chain-of-thought for video reasoning via reinforcement learning. arXiv preprint arXiv:2509.24008, 2025. 7 [16] Yuying Ge, Yixiao Ge, Chen Li, Teng Wang, Junfu Pu, Yizhuo Li, Lu Qiu, Jin Ma, Lisheng Duan, Xinyu Zuo, et al. Arc9 hunyuan-video-7b: Structured video comprehension of realworld shorts. arXiv preprint arXiv:2507.20939, 2025. 2 [17] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. 6, 7 [18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1 [19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [20] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos, 2025. 7 [21] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 2 [22] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. 1 [23] Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, et al. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055, 2025. 7 [24] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 7 [25] Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning patterns arXiv preprint and interaction turns for visual search. arXiv:2509.07969, 2025. 6 [26] Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning patterns and interaction turns for visual search, 2025. [27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 6 [28] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-ofthought. arXiv preprint arXiv:2501.07542, 2025. 3 [29] Hongyu Li, Songhao Han, Yue Liao, Junfeng Luo, Jialin Gao, Shuicheng Yan, and Si Liu. Reinforcement learning tuning for videollms: Reward design and data efficiency. arXiv preprint arXiv:2506.01908, 2025. 2 [30] Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, et al. Editthinker: Unlocking iterative reasoning for any image editor. arXiv preprint arXiv:2512.05965, 2025. 2 [31] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark, 2024. [32] Wenjun Li, Dexun Li, Kuicai Dong, Cong Zhang, Hao Zhang, Weiwen Liu, Yasheng Wang, Ruiming Tang, and Yong Liu. Adaptive tool use in large language models with meta-cognition trigger, 2025. 2 [33] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. 7 [34] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. 2 [35] Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, Shouzheng Huang, Xinping Zhao, Borui Jiang, Lanqing Hong, Longyue Wang, Zhuotao Tian, Baoxing Huai, Wenhan Luo, Weihua Luo, Zheng Zhang, Baotian Hu, and Min Zhang. Perception, reason, think, and plan: survey on large multimodal reasoning models, 2025. 2 [36] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2629626306, 2024. 6 [37] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. [38] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. 2 [39] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. 2 [40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. 7 [41] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. 7 [42] Minesh Mathew, Viraj Bagal, Rub`en Perez Tito, Dimosthenis Karatzas, Ernest Valveny, and C. Jawahar. Infographicvqa, 2021. [43] Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, et al. Open-o3 video: Grounded video reasoning with explicit spatio-temporal evidence. arXiv preprint arXiv:2510.20579, 2025. 7 10 [44] OpenAI. Hello gpt-4o. https://openai.com/index/ hello-gpt-4o/, 2024. Accessed 2025-09-19. 6, 7 [45] OpenAI. Thinking with images, 2025. 3 [46] Jinyoung Park, Jeehye Na, Jinyoung Kim, and Hyunwoo Kim. Deepvideo-r1: Video reinforcement finetuning via difficulty-aware regressive grpo. arXiv preprint arXiv:2506.07464, 2025. 2 [47] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. 2 [48] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [49] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 2 [50] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 7 [51] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Videoxl: Extra-long vision language model for hour-scale video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 7 [52] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. 3 [53] Jinyan Su, Jennifer Healey, Preslav Nakov, and Claire Cardie. Between underthinking and overthinking: An empirical study of reasoning length and correctness in llms, 2025. 2 [54] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, and Yu Cheng. Openthinkimg: Learning to think with images via visual tool reinforcement learning, 2025. [55] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. 3 [56] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. 3 [57] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Xiansheng Chen, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning of vision language models, 2025. 1 [58] Qwen Team. Qwen2.5-vl, 2025. 7 [59] Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu, and Jing Zhang. More thought, less accuracy? on the dual nature of reasoning in vision-language models, 2025. 2 [60] Chaoyang Wang, Yangfan He, Yiyang Zhou, Yixuan Wang, Jiaqi Liu, Peng Xia, Zhengzhong Tu, Mohit Bansal, and Huaxiu Yao. Knowing the answer isnt enough: Fixing reasoning path failures in lvlms. arXiv preprint arXiv:2512.06258, 2025. 2 [61] Chaoyang Wang, Zeyu Zhang, Meng Meng, Xu Zhou, and Haiyun Jiang. Vision-ekipl: External knowledgeinfused policy learning for visual reasoning. arXiv preprint arXiv:2506.06856, 2025. 1 [62] Chaoyang Wang, Zeyu Zhang, Long Teng, Zijun Li, and Shichao Kan. Tmcir: Token merge benefits composed image retrieval. arXiv preprint arXiv:2504.10995, 2025. [63] Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, et al. Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology. arXiv preprint arXiv:2507.07999, 2025. 2 [64] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing selfreflection of vision-language models with reinforcement learning, 2025. 6 [65] Haozhe Wang, Alex Su, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning, 2025. 2, 3, 6 [66] Jiacong Wang, Zijian Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, et al. Vgr: Visual grounded reasoning. arXiv preprint arXiv:2506.11991, 2025. 2 [67] Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, and Tianfei Zhou. Videorft: Incentivizing video reasoning capability in mllms via reinforced fine-tuning. arXiv preprint arXiv:2505.12434, 2025. 2 [68] Shijian Wang, Jiarui Jin, Xingjian Wang, Linxin Song, Runhao Fu, Hecheng Wang, Zongyuan Ge, Yuan Lu, and Xuelian Cheng. Video-thinker: Sparking thinking with videos via reinforcement learning. arXiv preprint arXiv:2510.23473, 2025. [69] Yuxuan Wang, Zilong Zheng, Xueliang Zhao, Jinpeng Li, Yueqian Wang, and Dongyan Zhao. Vstar: video-grounded dialogue dataset for situated semantic understanding with scene and topic transitions, 2023. 7 [70] Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, et al. Time-r1: Post-training large vision language model for temporal video grounding. arXiv preprint arXiv:2503.13377, 2025. 2 [71] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. 6, 7 11 et al. Architecture decoupling is not all you need for unified multimodal model. arXiv preprint arXiv:2511.22663, 2025. 2 [84] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. [85] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 6 [86] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning, 2025. 2 [87] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. 6, 7 [72] Zilin Xiao, Jaywon Koo, Siru Ouyang, Jefferson Hernandez, Yu Meng, and Vicente Ordonez. Proxythinker: Test-time guidance through small visual reasoners, 2025. 6 [73] Jihan Yang, Shusheng Yang, Anjali W. Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces, 2025. 7 [74] Sihan Yang, Runsen Xu, Yiman Xie, Sizhe Yang, Mo Li, Jingli Lin, Chenming Zhu, Xiaochen Chen, Haodong Duan, Xiangyu Yue, Dahua Lin, Tai Wang, and Jiangmiao Pang. Mmsi-bench: benchmark for multi-image spatial intelligence, 2025. [75] Zhao Yang, Jiaqi Wang, Xubing Ye, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Language-aware vision transformer for referring segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2 [76] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. 1 [77] Jiakang Yuan, Tianshuo Peng, Yilei Jiang, Yiting Lu, Renrui Zhang, Kaituo Feng, Chaoyou Fu, Tao Chen, Lei Bai, Bo Zhang, et al. Mme-reasoning: comprehensive benchmark for logical reasoning in mllms. arXiv preprint arXiv:2505.21327, 2025. 2 [78] Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, and Yansong Tang. Thinking with videos: Multimodal toolaugmented reinforcement learning for long video reasoning, 2025. 3 [79] Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, and Li Zhang. From flatland to space: Teaching vision-language models to perceive and reason in 3d, 2025. 7 [80] Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, and Helen Meng. Critique-grpo: Advancing llm reasoning with natural language and numerical feedback. arXiv preprint arXiv:2506.03106, 2025. 1 [81] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, Haonan Fan, Kaibing Chen, Jiankang Chen, Haojie Ding, Kaiyu Tang, Zhang Zhang, Liang Wang, Fan Yang, Tingting Gao, and Guorui Zhou. Thyme: Think beyond images, 2025. [82] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025. 6 [83] Dian Zheng, Manyuan Zhang, Hongyu Li, Kai Zou, Hongbo Liu, Ziyu Guo, Kaituo Feng, Yexin Liu, Ying Luo, Yan Feng, 12 obtain more robust predictions. Second, our reward design primarily targets verifiable tasks such as multiplechoice and numerical reasoning, making it less suitable for open-ended generation; future research could incorporate learned reward models, multimodal discriminators, or contrastive signals to better support free-form reasoning. Third, the AdaTooler-V-300k dataset is mainly constructed from public benchmarks, resulting in limited coverage of realworld long-tail cases, noisy conditions, and cross-domain scenarios; expanding the dataset with in-the-wild samples, hard-case synthesis, or domain adaptation techniques may enhance generalization. A. Dataset Distribution Details The distribution of AdaTooler-V-300k dataset can be roughly categorized as follows: General (Video, 81k): diverse collection of opendomain videos depicting everyday scenarios, designed to cultivate temporal comprehension and reasoning. General (Multi-Image, 33k): Multi-image reasoning tasks that test cross-view comparison and contextual integration. General (Image, 18k): General-purpose image questionanswering data for foundational visual understanding. Chart (Image, 24k): Visual reasoning over charts, line graphs, and scientific figures, emphasizing data interpretation and quantitative logic. OCR (Image, 15k): Tasks requiring recognition and interpretation of embedded textual content, such as signs, forms, or documents. Math (Image, 42k): Image-based math reasoning problems, including formulas, geometric diagrams, and multistep symbolic reasoning. Knowledge (Image, 30k): Visual commonsense and cross-disciplinary reasoning tasks to evaluate the integration of world knowledge with visual cues. Spatial (Image, 12k): Static spatial reasoning such as occlusion and positional inference. Spatial (Video, 24k): Focused on spatial reasoning in motion, including navigation, object tracking, and path planning, enhancing spatiotemporal understanding. Logical (Image, 12k): Visual logic tasks involving pattern recognition and rule-based reasoning. Visual Counting (Image, 6k): Object counting and density estimation for quantitative perception. High-Resolution (Image, 6k): Fine-grained visual understanding with small-object and texture recognition. B. Reasoning Examples This section presents representative reasoning examples generated by AdaTooler-V-7B, as shown in Fig. 6 and Fig. 7. C. Prompt Template for Training and Inference Fig. 8 illustrates the prompt template for training and inference of all models. We also use this prompt for the COT annotation. D. Limitations and Future Works We outline the limitations of our work and potential avenues for future research as follows: First, the estimation of tool benefit (S) relies on single reference model, which may introduce biased assessments of when tool use is genuinely helpful; future work may develop learned benefit estimator or leverage model ensembles to 1 Figure 6. An example of AdaTooler-V-7Bs reasoning output on V* Benchmark. Figure 7. An example of AdaTooler-V-7Bs reasoning output on MVBench. 2 Figure 8. Prompt template for training and inference."
        }
    ],
    "affiliations": [
        "DB Group, CUHK",
        "JMU",
        "MMLab, CUHK",
        "SJTU",
        "Sangfor",
        "THU",
        "UCF"
    ]
}