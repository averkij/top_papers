{
    "paper_title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
    "authors": [
        "Jun Wang",
        "Chunyu Qiang",
        "Yuxin Guo",
        "Yiran Wang",
        "Xijuan Zeng",
        "Chen Zhang",
        "Pengfei Wan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis."
        },
        {
            "title": "Start",
            "content": "Unified Multi-Task Audio-Video Joint Generation Jun Wang, Chunyu Qiang, Yuxin Guo, Yiran Wang, Xijuan Zeng, Chen Zhang, Pengfei Wan Kling Team, Kuaishou Technology {wangjun06, qiangchunyu, zhangchen03}@kuaishou.com 6 2 0 2 ] . [ 1 1 5 1 4 0 . 1 0 6 2 : r Figure 1 We propose Klear, unified audiovideo generation framework which delivers high fidelity, strong semantic and temporal alignment, and reliable instruction following in both joint and unimodal settings, with robust OOD generalization. Across tasks (T2AV/TI2AV/TI2V/T2V/T2A), it attains performance comparable to Veo-3 among open-source models. Audiovideo joint generation has progressed rapidly, yet substantial challenges still remain. Noncommercial approaches still suffer audio-visual asynchrony, poor lipspeech alignment, and unimodal degradation, which can be stemmed from weak audiovisual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axesmodel architecture, training strategy, and data curation. Architecturally, we adopt single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audiovisual alignment and strong scalability. Training-wise, we adopt progressive multitask regimerandom modality masking to joint optimization across tasks, and multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audiovideo dataset with dense captions, and introduce novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audiovideocaption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by large margin and achieves performance comparable to Veo 3, offering unified, scalable path toward next-generation audiovideo synthesis. Klear : Unified Multi-Task Audio-Video Joint Generation"
        },
        {
            "title": "1 Introduction",
            "content": "The Kinetoscope was the worlds first film projector, invented by Thomas Edison in 1891. The Kinetophone, on the other hand, was the first sound film projector, combining the Kinetoscope with synchronized sound playback technology, marking major breakthrough in sound film technology. Accordingly, vision and auditory are complementary and jointly indispensable modalities for depicting the real world. Consequently, audiovideo joint generation has emerged as one of the most prominent trends in generative AI. Commercial systems such as Veo 3 and Sora 2 have achieved impressive performance with strong semantic alignment, while diverse open-source models with varying architectures are also emerging. Nevertheless, research on joint audiovisual generation remains nascent, and current modelseven some commercial systemsstill exhibit audiovisual asynchrony, lipspeech mismatches, and degradation in unimodal quality. We attribute several factors: (1) Architecture. Most T2AV models like JavisDiT [1], UniVerse-1 [2] and Ovi [3] employ single-tower architecture with cross-attention module, resulting in limited audio-video interaction and alignment. (2) Training strategies and data. Currently, mainstream methods perform single-task training, which might bring about biased representations and struggle to exploit underlying audio-video correlation and world knowledge. To this end, informed by the above observations, we identify several key contributing factors. (1) Architecturally, prevailing designs hinder thorough cross-modal interaction: most existing models adopt dual-tower architectures with modality-specific initialization, learn each modality independently, and rely on shallow fusion via crossattention or adapters, which fails to fully align audiovisual features. (2) For data construction, there is pronounced lack of diverse, high-quality, and densely annotated audiovideo aligned generation datasets, as well as scalable, high-quality annotation methodologies for constructing them. (3) From learning-strategy perspective, most existing methods are trained exclusively on text-to-AV generation. This single-task regime induces overfitting and representation bias, which in turn hinders generalization and degrades unimodal performance. To address these issues, we propose Klear, which introduces coordinated improvements at the architectural, learning-strategy, and data levels. Specifically, on the architectural side, we adopt single-tower backbone with unified DiT blocks. Each block integrates an Omni-Full Attention module that jointly attends to four streamsaudio, audio captions, video, and video captionsthereby facilitating cross-modal fusion and interaction, achieving tight A-V alignment and stronger coupling to textual conditions, which offers higher scaling ceiling. For training, we design progressive multitask training regime: random modality masking sustains joint optimization over T2AV/TI2AV/TI2V/T2V/T2A. performance-adaptive pretrainpost train curriculum tunes data mixtures and quality, yielding robust, generalizable representations that exploit A/V correlations and world knowledge. From the data construction, we introduce an automated annotation pipeline that enables efficient model scaling. Building on these three insights, our model achieves strong performance in both joint and unimodal generation, delivering high fidelity, tight audiovisual alignment, natural outputs, and robust instruction following with favorable scaling behavior. On the Verse-Bench, it surpasses prior methods by large margin and generalizes well to out-of-distribution (OOD) scenarios. Our main contributions are summarized as follows: We introduce Klear, unified framework for multi-task audiovideo joint generation that, to our knowledge, is the first model to achieve performance comparable to Veo 3. Klear effectively resolves semantic and temporal audiovisual misalignment while delivering high-fidelity generation. Our key technical novelties lie in the unified single-tower architecture with the omni-full attention mechanism for seamless audiovisual fusion, and progressive multi-task training strategy that promotes generalizable representations and prevents unimodal performance degradation. We propose large-scale, high-quality audio-video dataset consisting of 81 million samples with accurate dense captions, along with an automated data generation pipeline and high-quality audio-video generation dataset. Extensive experiments demonstrate that Klear consistently excels in both unimodal and joint audiovideo generation, outperforming prior state-of-the-art on the unimodal benchmark and the AV joint-generation benchmark consistently. 2 Klear : Unified Multi-Task Audio-Video Joint Generation"
        },
        {
            "title": "2 Related Works",
            "content": "Text-to-Video Generation (T2V). Diffusion models have revolutionized video generation, with AnimateDiff [4] and Video Diffusion Models [5] leading the way. Stable Video Diffusion [6] emphasized the importance of large, high-quality datasets for performance. Early models used U-Net backbones, but Sora [7] introduced the Diffusion Transformer (DiT) architecture [8], trained on extensive video corpora. Open-source models like CogVideoX [9], HunyuanVideo [10], and the WAN series [11] joined closed-source systems like Kling [12] and Veo 2 [13]. Most models share common architecture: 3D VAE compresses videos into spatiotemporal latents, and DiT performs denoising. Data quality and scale remain crucial for model success, highlighting the importance of data curation and processing. Image-to-Video Generation (I2V). Early I2V systems extended T2V by conditioning on single frame via latent concatenation or CLIP-based feature injection [14]. Later, cascaded designs like I2VGen-XL [15] and dualinjection frameworks like DynamiCrafter [16] introduced structured pipelines for better motion and appearance fidelity. Adapter-based approaches like LAMP [17] and I2V-Adapter [18] integrate cross-frame attention. Recent works on motion modeling and stable sampling include Motion-I2V [19] and FrameBridge [20]. Despite these advancements, I2V still faces challenges with curated datasets, limited long-range motion modeling, and the trade-off between appearance fidelity and motion realism. Text-to-Audio Generation (TTA). Recent advances in generative models have propelled text-to-audio generation. Models like Make-An-Audio [21] and AudioLDM [22] synthesize audio via iterative denoising of text-conditioned latent representations. Tango [23], Audio Flamingo [24], and others expand latent spaces and enhance cross-modal alignment. Stable Audio [25] employs hierarchical latent diffusion for high-fidelity output. AudioStory [26] introduces unified generation framework, achieving long-form audio generation for the first time, while flow-matching techniques like VoiceBox [27] enable zero-shot style transfer. TangoFlux [28] optimizes text-audio alignment with CLAP-ranked preferences. Though these methods excel at semantic alignment, they are limited to short durations and lack flexibility for complex, evolving instructions, underscoring the need for TTA models that handle long, complex tasks. Audio-Video Joint Generation (T2AV). Pioneering efforts like MM-Diffusion [29] use coupled U-Net backbones, while DiT-based approaches dominate. AV-DiT [30] adapts pre-trained image DiTs with lightweight adapters, and UniForm [31] uses unified single-tower architecture for audio-video tokens. Key challenges include precise spatio-temporal synchronization, addressed by methods like JavisDiT [1] (hierarchical prior), Ovi [3] (twinbackbone design), and SyncFlow [32] (dual-DiT with Rectified Flow Matching). Other research orchestrates unimodal experts, such as MMDisCo [33] and Universe-1 [2], which combine specialized models at the block level. Despite advances in architecture and data, most models focus on sound effects or music, leaving synchronized speech and video synthesis an underexplored challenge."
        },
        {
            "title": "3.1 Preliminary\nProblem Definition. Our goal is to enable the generation of both audio and video within a single model, given\nvarious prior conditions. We denote the denoising network as ϵθ(·), the text condition as c. Let {za\nt }t∈[0,1] and\nt }t∈[0,1] denote the latent variables at timestamp t for audio and video, respectively. Here, t = 0 denotes\n{zv\nthe final timestamp of pure Gaussian noise. During inference, ϵθ(·) recursively performs denoising from t = 0\n, as shown below:\nto t = 1 to produce the final generation, ˆza",
            "content": "1 , ˆzv 1 t, ˆzv ˆza = ϵθ(za , zv , t, c), (1) Conditional Flow-Matching. We employ flow matching as the denoising objective. The model needs to learn the velocity field that transforms pure noise p0 = (0, I) to the underlying data distribution pdata. In practice, we perform linear interpolation xt = (1 t)x0 + tx1 to construct distribution at timestamp t. Here, x0 p0 and x1 pdata. Given the condition c, the model ϵθ() is trained to predict the target velocity, i.e., constantly 3 Klear : Unified Multi-Task Audio-Video Joint Generation Figure 2 Overview of Klear. The model takes four inputs: video, video-related text, audio-related text, and audio. Each input is individually encoded by respective encoders, then fed into the MM-DiT. The MM-DiT module outputs the latent variables of video and audio, which are then decoded separately into video and audio. as ut = x1 x0: LFM = Et,c,x0,x1 where (cid:13) (cid:13) (cid:13)(x1 x0) ϵθ (cid:0)tx1 + (1 t)x0, t, c(cid:1)(cid:13) 2 (cid:13) (cid:13) 2 , (2) U(0, 1), x0 (0, I), x1 pdata. Latent Encoding. The model takes four inputs: video, video-related text, audio-related text, and audio, where video-related text represents the video caption and audio-related text represents the audio caption and speech text. Video is encoded by the 3d casual visual encoder from CogVideoX [9], We use Qwen3-8B Embedding [34] as the encoder for audio and video captions."
        },
        {
            "title": "3.2 Single Tower with Full Attention\nSingle Tower DiT. To ensure a thorough audio-video fusion, we employ a single-tower architecture. As shown in\nFig. 2, following Stable Diffusion 3 [35], we employ Multimodal Diffusion (MMDiT) to take the sequences of\nall modalities as input and perform full attention. Specifically, there are four inputs, i.e., video, video-related\ntext, audio-related text, and audio. Each type of input is individually encoded into latents with respective\nencoders, then fed into the MM-DiT. The MM-DiT module outputs the latent variables of video and audio in\ntwo streams, which are then decoded separately to perform video and audio generation.",
            "content": "Mixed Dimension Rotary Position Embedding (MixD-RoPE). Another key architectural innovation is Mixed Dimension Rotary Position Embedding (MixD-RoPE). As shown in Fig. 2 (d), to enhance the positional information introduced by various aspect ratios and duration in videos, we apply 3D RoPE encoding across three dimensions, i.e., temporal, width and height for video embedding. This 3D RoPE incorporates both absolute and relative position dependency in videos. For audio modality, we employ compatible temporal 1D positional encodings, while its position number is initialized by incrementing the maximum temporal position ID of the video modality by one. As result, we build MixD-ROPE with shared temporal position ID between video and audio modalities. Omni-Full Attention. Previous works may employ separated spatial and temporal attention to reduce computational complexity, like UniForm [31]. However, as in CogVideoX [9], this separate attention mechanism requires extensive implicit information transmission, significantly increasing the learning complexity. Other works tailor two transformer towers for audio and video generation separately, e.g., AV-DiT, SyncFlow, JavisDiT, TAVGBench.However, they often adopt multi-stage training approach, which is complex and resource-intensive. The two towers must first be pretrained separately, then finetuned together, increasing training time and resource consumption. To achieve more efficient training and more effective modality fusion, we employ the 3D text-video-audio hybrid full attention mechanism. As shown in Fig. 2, within the MM-DiT module, the hidden states of video, video-related text, audio-related text, and audio are first scaled and 4 Klear : Unified Multi-Task Audio-Video Joint Generation normalized, then concatenated together for the attention calculation. = QV QV QAT QA, = KV KV KAT KA, = VV VV VAT VA, Attn(Q, K, ) = Sof tmax( QK dk )V, (3) (4) (5) (6) The attention values are then split into separate hidden states, which undergo scaling and normalization, residual connection, and feedforward, and subsequently fed to the next MM-DiT module. As result, we achieve the unification of all input modalities within joint full-attention."
        },
        {
            "title": "3.3 Multi-Task Progressive Training Strategy\nRandom Modality Masking. To learn generalizable and robust audio-visual representations for joint generation,\nwe train the generative model with a broad spectrum of tasks. As a result, we propose to selectively adjust the\nmask of query and key for audio and video modalities. If we restrict the query and key to video embedding\nand video caption embedding, the model degenerates to a T2V model. Similarly, limiting the query and key\nto audio embedding and audio text embedding results in a T2A model. In this way, the model could not only\nhandle joint generation, but also maintain the abilities of single-modality generation. Considering the scarcity\nof high-quality audio-video paired data, our method offers an alternative for training the T2VA model. We\nfirst pre-train Klear on T2V and T2A tasks, and then finetune our model on audio-video paired data to\nfinally construct a T2VA model. The learning objectives for audio and video generation are in Eq. (7) and\nEq. (8):",
            "content": "LT 2A = ϵa LT 2V = ϵv aska(ϵθ(za askv(ϵθ(za , zv , zv , c))2 2, , c))2 2, (7) (8) where aska is used to extract the audio token from the combined noise representation and askv is used to extract the vision tokens. In summary, LT 2A and LT 2V denote the single-modality tasks of T2A and T2V. To learn the generalizable and robust world knowledge of audio-visual correlation, we also incorporate several tasks of T2AV, I2V and I2AV. Consequently, the overall multi-task learning objective is as follows: Loverall = LT 2A + LT 2V + LT 2AV + LI2V + LI2AV (9) Progressive Training Strategy. To efficiently train AV joint generation, we adopt progressive multi-task learning framework with random modality masking applied throughout all stages: Stage-I: Pre-training. We pretrain the model on the large-scale, multi-scene data corpus to acquire atomic generation capabilities across all tasks, including cross-modal semantic alignment, temporal synchronization, high-fidelity audio synthesis, and precise visual feature construction, which ensures basic abilities of both single modality generation and joint generation, and provides solid foundation for subsequent post-training. Stage-II: Specialized Post-training. We then specialize the model on its weaker abilities and tasks. Guided by evaluation metrics, we adaptively rebalance data distributions across scenarios and tasks to strengthen underperforming capabilities while preserving overall competency. Stage-III: Quality-Refined Post-training. Finally, we fine-tune the model on the manually-curated, high-quality dataset to refine generation fidelity and enhance robustness in complex scenes, yielding improvements in perceptual realism and overall generation quality."
        },
        {
            "title": "4 Dataset Construction",
            "content": "Dataset Overview. Our dataset comprises automatically annotated samples. The dataset contains singlespeaker speech, multi-speaker speech, singing, and natural sound clips, with an overall post-filtering retention rate of 27%. 5 Klear : Unified Multi-Task Audio-Video Joint Generation Figure 3 Overview of our Dataset Annotation Pipeline."
        },
        {
            "title": "4.1 Dataset Filtering\nVideo Filtering and scene splitting. We first filter video quality by modeling dynamic quality (subject motion\nratio, camera stability), static quality (sharpness, aesthetics, color saturation), content naturalness (no\nexcessive effects/watermarks), and safety. We discard those videos with low resolution, low SNR/MOS, or\nover 20% silence. We then apply scene splitting to ensure each sample contains only one scene.",
            "content": "Audio Filtering and Post Processing. We filter audio data by removing samples with low SNR, MOS, abnormal clipping, distortion, or noise, ensuring less than 20% silence, high fidelity, and consistent formatting. We then assess audiovisual consistency, using Synchformer for temporal alignment and ImageBind for semantic alignment, ensuring high synchronization in both temporal and semantic dimensions."
        },
        {
            "title": "4.2 Audio-Guided Data Splitting",
            "content": "We partition the dataset by audio type, separating vocal from non-vocal clips to form sound split. From the vocal subset, we create singing, single-speaker speech, and multi-speaker speech splits, then apply dense captioning to each."
        },
        {
            "title": "4.3 Dense Annotation and Integration",
            "content": "We annotate each split with specialized models for speech transcripts, audio captions, and video captions, including both meta information and detailed content. For speech and singing, we extract speaker attributes (e.g., gender, age), while the sound split receives only audio captions. We use Whisper-Large-v3, SenseVoice, and Qwen2.5-Omni for transcription, Qwen2.5-Omni and Gemini 2.5-Pro for audio captions, and video expert model for detailed video labels. All annotations are merged into unified dense captions."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we first present the experimental setup and implementation details (Sec. 5.1), then compare Klear with diverse baselines across multiple tasks (Sec. 5.2), complemented by qualitative results (Sec. 5.3). We further conduct ablations on the unified single-tower architecture, multi-task versus progressive training, and the role of 3D RoPE for native FPS (Sec. 5.4), collectively validating the effectiveness of our approach."
        },
        {
            "title": "5.1 Experimental Setup\nImplementation details Klear comprises 26B parameters with a flow-matching feed-forward dimension of 4096.\nThe architecture incorporates 32 joint diffusion transformer layers combined with multimodal RoPE. For text\nencoding, we employ a 1024-dimensional TTS text encoder, while the caption encoder utilizes Qwen2.5-7B [36].\nThe Audio-VAE processes input waveforms at 44.1 kHz and generates embeddings at 43 Hz, achieving a 1024×\ndownsample ratio relative to the input sampling rate. The Video-VAE handles input videos with varying\nresolutions and frame rates, producing embeddings at 3 Hz with 16× compression applied to both height and\nwidth dimensions. We train the model using the Adam optimizer with an initial learning rate of 1e-4.",
            "content": "6 Klear : Unified Multi-Task Audio-Video Joint Generation Table 1 Main comparisons of audio-visual joint generation. Method Framework AudioLDM+TemoTkn T2A+A2V OpenSora+FoleyGen OpenSora+See&Hear T2V+V2A JavisDiT SVG Universe-1 Ovi T2AV Video Audio TTS AV Consistency MS AS 0.12 0.05 0.42 0.42 0.44 0. 0.36 0.18 0.41 0.40 0.20 0.47 0.58 0.48 ID 0.07 0.56 0.56 0.22 0.25 0.25 0.46 FD KL CLAP WER AV-A SNC IB-Score 2.05 0. 0.137 1.15 2.68 2.53 - 3.69 2. 1.95 1.55 1.55 1.50 3.08 2.97 5.17 3.62 1.25 1.19 0.212 0.206 0.228 0.080 0.160 0.224 - 0. 0.256 - 0.180 0.035 0.92 0.86 0.92 0.72 0.98 0.82 2.76 2.85 3.94 4.07 3.92 4.28 0.159 0. 0.231 0.206 0.198 0.214 Klear (Ours) Unified T2AV 0.48 0.51 0.59 1.36 1.06 0.232 0.028 0. 6.79 0.316 Baseline Methods. We select two canonical types of methods, including (1) Cascaded generation: these methods typically employ sequential T2V+V2A or T2A+A2V. Here, we employ AudioLDM2 [37] to perform the T2A task, while OpenSora [38] for T2V. (2) Joint generation. Recent works like JavisDiT [1], UniVerse-1 [2] and Ovi [3] employ dual-tower architecture with dedicated interaction layers between them. Evaluation Metrics. To evaluate the models generation capabilities, we test T2AV tasks on video, audio, and audio-video consistency. Following Universe-1, we use Verse-Bench for T2AV tasks. For video quality, we report the Motion Score (MS) based on RAFT [39] optical flow for dynamic realism and the Aesthetic Score (AS), composite metric from MANIQA [40], aesthetic-predictor-v2-5 [41], and Musiq [42] for visual fidelity. Identity preservation is measured by ID Consistency (ID), calculated via DINOV3 [43] feature similarity. For audio quality, we use Fréchet Distance (FD) and KL Divergence on mel-spectrograms from PANNs [44]. Semantic alignment is evaluated using the CLAP score [45]. For synchronization, we report AV-A (Audio-Video Alignment) distance from Synchformer [46] and SyncNet Confidence (SNC) score [47] for lip sync. Global cross-modal alignment is measured by ImageBind (IB)."
        },
        {
            "title": "5.2 Comparison with Existing Methods\nKlear demonstrates robust audio–video joint generation. In Table 1, Klear achieves state-of-the-art performance,\nsurpassing the two prior methods by a large margin. Cascaded approaches perform poorly due to error\naccumulation and strong dependence on upstream generation quality, while existing joint models exhibit\nonly moderate audio–video consistency and noticeable unimodal degradation. In contrast, Klear attains\nsubstantially better A/V consistency and synchronization, which we attribute to the unified single-tower\narchitecture and omni full attention mechanism.",
            "content": "Klear effectively guarantees unimodal performance. We then assess unimodal performance. As shown in Table 1, Klear delivers high audio and video quality in joint generationsurpassing cascaded and joint baselines by 34% and 18%, respectivelywhile multitask training yields more generalizable representations. Moreover, Klear outperforms specialized T2A and T2V models on their respective tasks, indicating that leveraging complementary audiovisual knowledge strengthens unimodal representations and further improves single-modality generation quality. Klear consistently maintains performance advantage across multiple tasks. Table 1 provides comprehensive evaluation of Klear across broad suite of tasksincluding TI2AV, TI2V, T2V, and T2Awith comparisons to task-specialized baselines. Klear delivers consistently strong results, matching or surpassing specialized state-of-the-art models."
        },
        {
            "title": "5.3 Qualitative Results\nLip-Sync Accuracy. Fig. 4 (a–b) highlights lip-sync performance. Klear achieves phoneme-level alignment\nbetween mouth movements and audio—covering mouth openings, lip–teeth shapes, and tongue positions,\nwhile Universe-1 and Ovi suffer from misalignment, delay, and clear audio–visual mismatch.",
            "content": "Emotional Expressiveness. From Fig. 4 (c), Klear generates characters with expressive emotions: facial cues (eyes, mouth curvature, muscle tension) are highly consistent with the audios affective tone (joy, sadness, 7 Klear : Unified Multi-Task Audio-Video Joint Generation Figure 4 Qualitative evaluation of audio-video joint generation across various aspects. Table 2 Comparison of different methods. The Dual Tower uses standard cross-attention, while the Single Tower utilizes our proposed Omni-Full Attention. Method"
        },
        {
            "title": "Dual Tower\nSingle Tower",
            "content": "Video ID 0.62 0.80 Audio TTS Audio-Video Consistency MOS CLAP WER DeSync Sync-conf 62.02 93. 0.139 0.232 0.675 0.028 1.163 0.650 3.762 6.787 IB 0.126 0. excitement, lethargy), reflecting natural audiovisual fusion of prosody and dynamics. In contrast, Universe-1 and Ovi often produce distorted or emotional expressions. Singing and Rap Performance. As shown in Fig. 4, Klear yields performances where pitch, rhythm, and breath control are tightly aligned across audio and visualsvibrato, melisma, and dynamic changes naturally match breathing patterns and facial expressions, consistent with human expectations of singing. In contrast, Universe-1 and Ovi show clear lip-sync failures, resulting in pronounced incongruity and reduced realism, especially for rap. AudioVisual Synchronization and Audio Overlapping. As shown in Fig. 4, Klear jointly generates background music and sound effects that are emotionally consistent with the video, with synchronized timing, realistic acoustics, and high fusion, thereby enhancing immersion. In contrast, baseline methods struggle to produce overlapping sounds and exhibit poor audiovisual consistency. Image to Audio-Video. Fig. 4 presents TI2AV and TI2V results. Our model preserves high identity consistency with the input image while generating plausible camera motion and dynamics, whereas baselines exhibit identity drift, large visual discrepancies, and mechanical movements."
        },
        {
            "title": "5.4 Ablation and Analysis\nArchitectural Effectiveness. To compare single- and dual-tower architectures for audio–video generation,\nwe feed audio and video features into separate mm-DiT branches, with a randomly initialized audio tower",
            "content": "8 Klear : Unified Multi-Task Audio-Video Joint Generation Table 3 Ablation of multi-task masking, with arrows indicating optimization directions. Method T2V T2V+T2AV All Tasks(Ours) Video ID 0.71 0.76 0.80 Audio MOS - 88.181 93.106 CLAP - 0.188 0.232 TTS WER - 0.044 0. Audio-Video Consistency DeSync - 0.895 0.650 Sync-conf - 5.024 6.787 IB - 0.201 0.316 Figure 5 Ablations of different training stages. Metrics include video, audio, TTS, and audio-video consistency, with arrows indicating optimization directions. due to the lack of matching DiT backbone. Each block uses full attention and cross-attention for feature alignment. We also conduct an ablation with pretrained audio tower. As shown in Table 2, the single-tower model outperforms the dual-tower variant in audio quality, video quality, and audiovideo consistency, with visualizations confirming better cross-modal alignment. Although the pretrained audio tower converges faster, its performance is suboptimal due to distribution mismatch with video features, hindering alignment. Advantages of Multi-Task Masking. As shown in Table 3, for T2AV joint generation, our multi-task model significantly outperforms counterpart trained solely on T2AV. This approach captures cross-modal audiovideo correlations and complementary cues, outperforming video-only models on T2V and I2V. The unified multi-task training also produces robust, generalizable representations that scale well with data and compute, and the model generalizes effectively to I2AV and I2V, demonstrating high transferability. Gains from Progressive Training Strategy. As in Figure 5, the progressive training first equips the model with basic audiovideo generation capabilities (a). In the post-training phase, the full model. The SP stage further boosts previously weak skills (e.g., IB score by 0.1), and post-training on high-quality data brings an additional overall gain over (b). Removing the entire progressive schedule instead causes significant drop, confirming the effectiveness of our multi-stage training strategy."
        },
        {
            "title": "6 Conclusion",
            "content": "We identify key failure modes in audiovideo generation, such as asynchrony, lipspeech mismatch, and unimodal degradation, caused by suboptimal architectures, misaligned data, and single-task training. To address these, we propose Klear, unified multi-task framework with single-tower backbone and Omni-Full Attention for cross-modal interaction, alongside progressive training strategy and an automated annotation pipeline. This pipeline produces high-quality, annotated audiovideo dataset for scalable training. Klear outperforms prior state-of-the-art on unimodal and joint audio-video benchmarks, becoming the superior model comparable to Veo 3. We hope this work could provide clear direction and catalyze deeper research in audiovideo generation."
        },
        {
            "title": "7 Contributors",
            "content": "All contributors are listed as follows. Jun Wang, Chunyu Qiang, Yuxin Guo, Yiran Wang, Feng Deng, Xijuan Zeng, Nan Li, Le Wang, Ming Wen, Zihan Li, Junjie Yan, Jingru Zhao, Zewen Song, Jiachen Zheng, Shiyao Wang, Xu Li, Zihao Ji, Zhongliang 9 Klear : Unified Multi-Task Audio-Video Joint Generation Liu, Xiaopeng Wang, Kang Yin, Teng Ma, Ziyu Zhang, Yuzhe Liang, Yushen Chen, Pengfei Cai, Haorui Zheng, Youjun Chen, Qianyue Hu, Jiahui Zhao, Yuejiao Wang, Liang Hou, Yuan Gao, Yan Zhou, Chen Zhang, Pengfei Wan."
        },
        {
            "title": "References",
            "content": "[1] Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, et al. Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. arXiv preprint arXiv:2503.23377, 2025. [2] Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. Universe-1: Unified audio-video generation via stitching of experts. arXiv preprint arXiv:2509.06155, 2025. [3] Chetwin Low, Weimin Wang, and Calder Katyal. Ovi: Twin backbone cross-modal fusion for audio-video generation. arXiv preprint arXiv:2510.01284, 2025. [4] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [5] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:86338646, 2022. [6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [7] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [8] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [9] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [10] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [11] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [12] Jun Wang, Xijuan Zeng, Chunyu Qiang, Ruilong Chen, Shiyao Wang, Le Wang, Wangjing Zhou, Pengfei Cai, Jiahui Zhao, Nan Li, et al. Kling-foley: Multimodal diffusion transformer for high-quality video-to-audio generation. arXiv preprint arXiv:2506.19774, 2025. [13] Ghosal Deepanway, Majumder Navonil, Mehrish Ambuj, and Poria Soujanya. Text-to-audio generation using instruction-tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731, 2023. [14] Ruyang Liu, Jingjia Huang, Ge Li, Jiashi Feng, Xinglong Wu, and Thomas Li. Revisiting temporal modeling for clip-based image-to-video knowledge transferring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65556564, 2023. [15] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. [16] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2024. 10 Klear : Unified Multi-Task Audio-Video Joint Generation [17] Tsugunori Notomi, Yasuyoshi Mori, Norihiro Tomita, and Hidetoshi Kanda. Loop-mediated isothermal amplification (lamp): principle, features, and future prospects. Journal of microbiology, 53(1):15, 2015. [18] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, et al. I2v-adapter: general image-to-video adapter for diffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [19] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [20] Yuji Wang, Zehua Chen, Xiaoyu Chen, Yixiang Wei, Jun Zhu, and Jianfei Chen. Framebridge: Improving image-to-video generation with bridge models. arXiv preprint arXiv:2410.15371, 2024. [21] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. In International Conference on Machine Learning, pages 1391613932. PMLR, 2023. [22] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. [23] Haiyang Liu, Xingchao Yang, Tomoya Akiyama, Yuantian Huang, Qiaoge Li, Shigeru Kuriyama, and Takafumi Taketomi. Tango: Co-speech gesture video reenactment with hierarchical audio motion embedding and diffusion interpolation. arXiv preprint arXiv:2410.04221, 2024. [24] Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle, and Bryan Catanzaro. Audio flamingo: novel audio language model with few-shot learning and dialogue abilities. arXiv preprint arXiv:2402.01831, 2024. [25] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. [26] Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, and Ying Shan. Audiostory: Generating long-form narrative audio with large language models. arXiv preprint arXiv:2508.20088, 2025. [27] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guided multilingual universal speech generation at scale. Advances in neural information processing systems, 36:1400514034, 2023. [28] Chia-Yu Hung, Navonil Majumder, Zhifeng Kong, Ambuj Mehrish, Amir Ali Bagherzadeh, Chuan Li, Rafael Valle, Bryan Catanzaro, and Soujanya Poria. Tangoflux: Super fast and faithful text to audio generation with flow matching and clap-ranked preference optimization. arXiv preprint arXiv:2412.21037, 2024. [29] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. [30] Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, and Yapeng Tian. Av-dit: Efficient audio-visual diffusion transformer for joint audio and video generation. arXiv preprint arXiv:2406.07686, 2024. [31] Lei Zhao, Linfeng Feng, Dongxu Ge, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, and Xuelong Li. Uniform: unified diffusion transformer for audio-video generation. arXiv e-prints, pages arXiv2502, 2025. [32] Haohe Liu, Gael Le Lan, Xinhao Mei, Zhaoheng Ni, Anurag Kumar, Varun Nagaraja, Wenwu Wang, Mark Plumbley, Yangyang Shi, and Vikas Chandra. Syncflow: Toward temporally aligned joint audio-video generation from text. arXiv preprint arXiv:2412.15220, 2024. [33] Akio Hayakawa, Masato Ishii, Takashi Shibuya, and Yuki Mitsufuji. Mmdisco: Multi-modal discriminator-guided cooperative diffusion for joint audio and video generation. arXiv preprint arXiv:2405.17842, 2024. [34] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. [35] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Klear : Unified Multi-Task Audio-Video Joint Generation [36] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, et al. Qwen2. 5-1m technical report. arXiv preprint arXiv:2501.15383, 2025. [37] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:28712883, 2024. [38] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [39] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. [40] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11911200, 2022. [41] discus0434. aesthetic-predictor-v2-5, 2024. [42] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51485157, 2021. [43] Oriane Siméoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michaël Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. [44] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:28802894, 2020. [45] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [46] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 53255329. IEEE, 2024. [47] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Asian conference on computer vision, pages 251263. Springer, 2016."
        }
    ],
    "affiliations": [
        "Kling Team, Kuaishou Technology"
    ]
}