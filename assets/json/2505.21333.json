{
    "paper_title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios",
    "authors": [
        "Yang Shi",
        "Huanqian Wang",
        "Wulin Xie",
        "Huanyao Zhang",
        "Lijie Zhao",
        "Yi-Fan Zhang",
        "Xinfeng Li",
        "Chaoyou Fu",
        "Zhuoer Wen",
        "Wenting Liu",
        "Zhuoran Zhang",
        "Xinlong Chen",
        "Bohan Zeng",
        "Sihan Yang",
        "Yuanxing Zhang",
        "Pengfei Wan",
        "Haotian Wang",
        "Wenjing Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce the MME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1,464 videos with varying resolutions, aspect ratios, and durations, along with 2,000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within a single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 3 3 3 1 2 . 5 0 5 2 : r MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios Yang Shi1,8 Huanqian Wang2 Wulin Xie3 Huanyao Zhang1 Lijie Zhao4 Yi-Fan Zhang3 Xinfeng Li5 Chaoyou Fu6 Zhuoer Wen1 Wenting Liu1 Zhuoran Zhang1 Xinlong Chen3 Bohan Zeng1 Sihan Yang7 Yuanxing Zhang8 Pengfei Wan8 Haotian Wang2 Wenjing Yang 1PKU 2THU 3CASIA 4CUHKSZ 5NTU 7XJTU 8Kuaishou Core Contributor Project Lead Corresponding Author https://mme-videoocr.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy in Optical Character Recognition (OCR) from static images. However, their efficacy in video OCR is significantly diminished due to factors such as motion blur, temporal variations, and visual effects inherent in video content. To provide clearer guidance for training practical MLLMs, we introduce MMEVideoOCR benchmark, which encompasses comprehensive range of video OCR application scenarios. MME-VideoOCR features 10 task categories comprising 25 individual tasks and spans 44 diverse scenarios. These tasks extend beyond text recognition to incorporate deeper comprehension and reasoning of textual content within videos. The benchmark consists of 1, 464 videos with varying resolutions, aspect ratios, and durations, along with 2, 000 meticulously curated, manually annotated question-answer pairs. We evaluate 18 state-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing model (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained analysis indicates that while existing MLLMs demonstrate strong performance on tasks where relevant texts are contained within single or few frames, they exhibit limited capability in effectively handling tasks that demand holistic video comprehension. These limitations are especially evident in scenarios that require spatio-temporal reasoning, cross-frame information integration, or resistance to language prior bias. Our findings also highlight the importance of high-resolution visual input and sufficient temporal coverage for reliable OCR in dynamic video scenarios."
        },
        {
            "title": "Introduction",
            "content": "In recent years, the rapid advancement of Multimodal Large Language Models (MLLMs)[16] has garnered significant attention. These models, capable of processing and integrating information across various modalities (e.g., text, images, and video), have demonstrated considerable potential and significant value across wide range of real-world applications[713]. Optical Character Recognition (OCR) [14], fundamental technology in visual understanding, serves as crucial link for enabling structured comprehension of image and video content. It transforms visual information into computationally analyzable semantic data. Within cross-modal learning, OCR provides critical feature support for text-visual alignment, directly impacting the performance of downstream tasks [15, 16]. Previous OCR-based benchmarks [1721] primarily focus on evaluating Work done during an internship at Kuaishou Technology. Figure 1: An example in MME-VideoOCR. The task requires the MLLM to first recognize the textual information distributed across multiple video frames, and then to perform semantic understanding and reasoning over the extracted text to accurately determine the correct answer. The correct information is marked in blue, while misleading information is marked in red. Table 1: Key differences between MME-VideoOCR and prior video-based OCR benchmarks. MME-VideoOCR features larger number of task types and scenarios, employs fully manual annotations to ensure reliability, supports bilingual content for broader coverage, and enables comprehensive evaluation across perception, understanding, and reasoning. Benchmarks #Videos #QA #Tasks #Scenarios Annotation Bilingual Perception Understanding Reasoning OCR Benchmark [22] FG Bench [23] MME-VideoOCR 25 1, 1464 1,477 2,961 2,000 1 6 25 20+ 20+ 44 A&M q the OCR-based capabilities of MLLMs in static image scenarios. Several studies [22, 23] have initiated preliminary investigations into video scenarios. However, they typically concentrate on perceiving textual content, often neglecting text-based understanding and reasoning. Considering the unique challenges of video understanding tasks, comprehensive video OCR evaluation must address three key issues, as illustrated in Figure 1. Firstly, textual information in videos can appear in various formssuch as foreground text, background scenery, on-screen annotations, watermarks, and floating overlays. This requires models to establish robust spatiotemporal visual-text associations and to effectively identify and extract relevant textual information from these diverse and often noisy sources across different shots. Secondly, critical textual information in videos is often distributed across multiple frames, rather than appearing in single static image. Therefore, models must be capable of effectively recognizing, integrating, and understanding text content over time, leveraging temporal context to reconstruct and interpret fragmented or sequentially presented information. Thirdly, as task complexity increases, models must be able to reason over the recognized text. This reasoning ability is essential for deeper video understanding and remains significant challenge for current MLLMs. In this paper, we propose the MME-VideoOCR benchmark, which provides comprehensive evaluation framework for OCR tasks in video scenarios. Recognizing the limitations of current OCR tasks in existing evaluation datasets, MME-VideoOCR encompasses 10 task categories and 25 specific tasks, incorporating substantial number of actively collected or custom-created videos. As shown in Table 1, MME-VideoOCR consists of 1, 464 videos, paired with 2, 000 diverse and accurately human-annotated question-answer (QA) pairs. The tasks require answers based on both localized key information and holistic understanding of the entire video. 2 The main contributions are summarized as follows: 1. MME-VideoOCR introduces diverse set of video OCR tasks, utilizing manually qualitycontrolled videos and question-answer pairs. These tasks span multiple dimensions, such as perceptual accuracy, contextual comprehension, and cross-frame reasoning, which together enable comprehensive evaluation of MLLMs OCR capabilities in video scenarios. 2. We evaluate 18 state-of-the-art MLLMs, including publicly available models ranging from 7B to 78B in size, as well as closed-source models like GPT-4o and Gemini-2.5 Pro. The results demonstrate strong discriminative power and the challenges posed by MME-VideoOCR. Regarding discriminative power, the worst-performing model, LLaVA-OneVision 7B, has an accuracy of 46.0%, while the best-performing model achieves an accuracy of 73.7%, showing significant gap in performance. Regarding task difficulty, on several tasks we designed, such as Cross-Frame Text Understanding and Text-Based Video Understanding, most models score below 60%. 3. The evaluation results further reveal significant deficiencies in current models on OCR tasks that require spatio-temporal reasoning and cross-frame information integration, thereby indicating critical direction for MLLM optimization. Moreover, both high-resolution visual inputs and sufficient temporal coverage are essential for achieving reliable OCR performance in dynamic video settings. Notably, MLLMs exhibit strong language prior bias during text recognition, frequently favoring semantically plausible outputs over visually accurate transcriptions."
        },
        {
            "title": "2.1 Multimodal Large Language Models",
            "content": "Through the integration of vision encoder into Large Language Models (LLMs) and pretraining on large-scale multimodal data, MLLMs exhibit strong OCR capabilities, making them well-suited for downstream tasks such as document understanding [24, 25], key information extraction [16], and scene text recognition [26, 27]. Building on this foundation, some recent MLLMs [6, 2832] have further extended their capabilities to handle video inputs, enabling them to process dynamic visual information. This advancement enables MLLMs not only to recognize text in static images, but also to extract text-related information from videos and leverage it for more effective video understanding. However, the increased visual complexity, dynamic content, and temporal dependencies inherent in video characteristics impose greater demands and challenges on MLLMs [3, 33]. Therefore, comprehensive and effective evaluation of their OCR-based capabilities in video scenarios is crucial."
        },
        {
            "title": "2.2 OCR Benchmarks for Multimodal Large Language Models",
            "content": "Most existing benchmarks [34] are designed to evaluate the OCR capabilities of MLLMs in static image scenarios, including TextVQA [17], OCR-VQA [18], SEED-Bench2-Plus [19], OCRBench [20] and OCRBench v2 [21]. few works [22, 23] have extended to video, but they cover only narrow range of task types, lack diversity in video content, and provide limited insight into the unique characteristics of OCR-based tasks in video scenarios. Moreover, these benchmarks emphasize text recognition while overlooking text-based understanding and reasoning. Some scene-text video QA benchmarks [26, 35, 36] incorporate textual cues into visual QA. However, they often overlook fine-grained text perception, including temporal grounding and attribute recognition, and do not fully evaluate the potential of text as central driver for video understanding. Moreover, as they focus solely on scene-text understanding, which represents only narrow application scenario, this is far from sufficient for comprehensive evaluation of MLLMs OCR-based capabilities. These benchmarks are also limited in video diversity, task variety, and their exploration of the unique characteristics of OCR-based tasks in video scenarios."
        },
        {
            "title": "3.1 Task Definition",
            "content": "Challenges inherent in video data, such as motion blur, inter-frame interference, the complexity of cross-modal alignment, difficulties in tracking content across shots, and limited generalization in noisy scenes. These issues pose significant obstacles for both video coding [37, 38] and semantic 3 Figure 2: Example videos and their annotated questions from the MME-VideoOCR benchmark, encompassing 25 tasks across 10 categories. Each task is designed to evaluate models capabilities in various aspects such as text recognition, localization, reasoning, and comprehensive video understanding. The figure displays representative video samples and their corresponding questions. perception [39, 40], critically impacting the accuracy of MLLMs. To rigorously assess and foster advancements in MLLMs against these challenges, we introduce MME-VideoOCR, comprehensive benchmark comprising 25 distinct tasks across 10 categories (details can be found in Appendix B.1). Figure 2 showcases representative examples, illustrating the specific nature and scope of each task. Text Recognition involves Text Recognition at Designated Locations and Text Recognition Based on Specific Attributes to evaluate the fine-grained text recognition capability. Visual Text QA employs Text-Centric QA and Translation. Both tasks challenge the models ability to not only perceive but also comprehend multimodal semantics. Text Grounding introduces Spatial Grounding and Temporal Grounding to assess the models ability on localizing and interpreting text across both spatial-temporal dimensions within dynamic scenes. Attribute Recognition is composed of three tasks: Color Recognition, where models are expected to identify the color of the text; Named Entity Recognition, which focuses on extracting and classifying named entities; and Counting, where models must accurately identify the number of textual elements that meet specified criteria. Change Detection & Tracking contains Change Detection and Tracking to identify textual changes over time and monitor text elements as they change position across frames, respectively. Special Text Parsing includes five tasks: Table Parsing, Chart Parsing, Document Parsing, Mathematical Formula Parsing, and Handwriting Recognition. These tasks require models to accurately identify and understand text with either special structures or highly variable visual forms. 4 Figure 3: Overview of the MME-VideoOCR construction process. Video filtering ensures sufficient visual dynamics and meaningful textual content. Manual annotation provides high-quality QA pairs, and expert verification further enhances sample reliability and mitigates potential biases. Cross-Frame Text Understanding includes three subtasks: Scrolling Text Understanding, which focuses on recognizing dynamic text streams that move across frames and may only be fully readable when aggregated over time; Trajectory Recognition, where the motion path of an object in the video forms recognizable text, and the model must interpret this trajectory as the intended message; Scrambled Recognition, which involves identifying and reconstructing complete text from characters that appear out of order across different positions in the video frames. Text-Based Reasoning requires models to go beyond surface-level understanding by synthesizing dispersed cues, identifying implicit relation, and resolving ambiguity or misleading content. Text-Based Video Understanding introduce a) Subtitle-Based Video Understanding which resembles real-world scenarios like conversations, tutorials, or news, where subtitles provide key information that visuals alone cannot capture; b) Multi-Hop Needle in Haystack which requires reasoning over multiple pieces of subtitle content to find the correct answer. Robust Video Testing contains three specialized video types: AIGC Videos, Long Videos, and Adversarial Videos. AIGC Videos, generated by AI systems [41], assess model adaptability to increasingly common synthetic content. Long Videos test the ability to extract relevant information from lengthy sequences with substantial redundancy. Adversarial Videos strategically insert all-black frames into normal videos, designed to mislead the MLLMs."
        },
        {
            "title": "3.2 Benchmark Construction",
            "content": "Video Collection & Filtering. MME-VideoOCR covers as many diverse scenarios as possible in order to provide comprehensive evaluation. To achieve this, we employ three distinct data collection methods, balancing diversity and efficiency in the construction of the benchmark. Reconstructing from Existing Video QA Data. To maximize data collection efficiency, we leverage existing text-based video QA datasets, including BOVText [42], M4-ViteVQA [35], NewsVideoQA [43], LSVTD [44], RoadText-1K [45], RoadTextVQA [36], EgoTextVQA [26], NIAH-Video [46], and DSText [47]. For each video in these datasets, we uniformly sample 5-10 frames and extract the text using PaddleOCR [48]. The sampled frames, along with the extracted text, are then processed using GPT-4o to evaluate whether the video exhibits sufficient visual dynamics and contains semantically meaningful text. Only videos that meet these criteria are retained for further use. Manual Collection of Publicly Available Videos. Existing benchmarks often lack the diversity needed to fully satisfy the requirements of our 25 OCR tasks. Therefore, we manually collect additional data from publicly available online sources (e.g., YouTube, Bilibili, Kuaishou) to further enhance diversity and ensure coverage of specific scenarios that are underrepresented in current datasets, such as webpages, charts, and mathematical formula derivations. Additionally, since most existing MLLMs are primarily trained on horizontally oriented videos, we intentionally include vertically formatted video content to improve distributional balance and better reflect real-world usage scenarios. AI-Generated Videos. The task of recognizing and understanding the text in AI-generated videos is becoming more critical. To cover this emerging scenario, we manually create set of videos designed to diversify the dataset and introduce controlled challenges. We initially generated 2, 000 everyday phrases. These phrases were then expanded into scene descriptions using Llama3.1-8B [49], with 5 the requirement that each scene must incorporate the corresponding text and include narrative element detailing its appearance or disappearance. Subsequently, these descriptions were provided to Wan [41] for text-to-video generation. From the resultant videos, we selected those exhibiting accurate text rendering, high visual-scene integration, and plausible narratives for our evaluation set. These videos are not only useful for evaluating the models ability to understand AI-generated content but also address specific cases that are difficult to obtain from existing datasets or online sources, such as occluded text revealed over time. Manual Annotation. In order to circumvent errors and biases that may arise from model-based annotations [50, 51], we opt for manual annotation to ensure the dependability of our samples. Human annotators are tasked with carefully examining each video and developing 3-4 QA pairs per video, adhering to the specified task requirements. Next, second expert implements selection process, retaining 1-2 high-quality QA pairs per video. This sequential two-stage screening process is expected to substantially ensure the generation of high-quality QA pairs exhibiting significant relevance, clarity, and challenging attributes, effectively preventing biases from individual annotators. Expert Verification. To uphold the highest data quality, expert annotators meticulously verify the constructed dataset against stringent standards. This verification process specifically addresses potential issues such as ambiguous questions, inaccurate answers, and insufficiently challenging problems. Initially, annotators review and rectify any errors or ambiguities within the QA pairs. Subsequently, for multiple-choice questions, they thoroughly assess all options, confirming that each is meaningful, poses an appropriate level of challenge, and functions as plausible distractor. To mitigate potential biases stemming from imbalanced answer option frequencies [52, 53], we ensure uniform distribution of correct answers across all options. Furthermore, to identify and eliminate residual biases that could compromise evaluation reliability, we conduct dedicated debiasing test, as detailed in Section 3.4. The complete data construction process is illustrated in Figure 3."
        },
        {
            "title": "3.3 Evaluation Criteria",
            "content": "Considering the characteristics of different tasks, we employ three distinct evaluation metrics to balance accuracy and efficiency in evaluation. Containment Match. For Text Recognition and Handwriting Recognition, where the model must accurately identify the recognized text, we simply check whether the ground truth appears in models response. This straightforward yet effective method is widely adopted in previous work [20, 23, 54]. GPT-Assisted Scoring. In the Translation task, multiple valid answers may exist. These answers may vary in form but remain consistent in meaning. To ensure flexibility and prevent unnecessary constraints on the model, we incorporate GPT-Assisted Scoring. Given the reference answer and the models response, GPT-4o-0806 [55] serves as the evaluator, assessing their consistency and assigning binary score of either 0 or 1. The prompt is shown in Appendix B.3. Multiple-Choice. Tasks like Visual Text QA and Spatial Grounding allow for highly flexible responses. Since both Containment Match and GPT-Assisted Scoring may introduce evaluation errors, we use multiple-choice format for assessment. In this setting, the model only needs to select the most appropriate option, which simplifies evaluation and reduces ambiguity in scoring. We use common prompt as shown in Appendix B.3."
        },
        {
            "title": "3.4 Debiasing Test",
            "content": "Since the underlying LLM of an MLLM is pretrained on large-scale textual corpora, it may introduce biases into the evaluation [52]. One source of bias arises from textual priors. For example, when asked What does the text on the red warning sign say?, the model is more likely to answer STOP than EXIT due to common co-occurrence patterns in pretraining data. Another issue is potential knowledge leakage. For instance, for question like According to the video, when was the United States founded?, the model may provide the correct answer without relying on visual input, thereby compromising the reliability of the evaluation. To mitigate these biases, we introduce debiasing test designed to quantify and minimize the influence of textual priors and knowledge leakage. Specifically, we evaluate Qwen2.5-VL-7B [29] by presenting questions and options without providing any meaningful visual input. Under this setup, the model relying solely on textual priors should ideally achieve accuracy close to 0% for tasks evaluated via 6 Figure 4: Overview of MME-VideoOCR Statistics. The videos in MME-VideoOCR covers 9 major scenario categories comprising 44 specific scene types, offering fine-grained coverage of diverse video contexts. The benchmark features balanced distribution of video durations and sources, with significant portion of the videos newly collected from public resources or manually curated. Containment Match and GPT-Assisted Scoring, indicating minimal textual bias. For multiple-choice questions, random guessing should yield an accuracy close to 25%. After each debiasing test iteration, expert annotators review and revise samples flagged as potentially problematic. As summarized in Table 2, the final results demonstrate the effectiveness of our approach, confirming that textual biases have been significantly suppressed, thus ensuring greater reliability and fairness of our evaluation. Table 2: Accuracy of the debiasing test. Through multiple rounds of testing and revision, potential biases were effectively suppressed, ensuring the validity and reliability of MME-VideoOCR. Visual Input Containment Match GPT-Assisted Scoring Multiple-Choice Model Qwen2.5-VL Qwen2.5-VL Black Image"
        },
        {
            "title": "None",
            "content": "0% 0% 0% 0% 25.1% 27.4%"
        },
        {
            "title": "3.5 Statistics",
            "content": "Through rigorous video selection, manual annotation, and expert-level validation, we collect total of 1, 464 videos along with 2, 000 high-quality QA annotations. As illustrated in Figure 4, these videos span 9 major scenario categories, such as daily life, education and knowledge, and sports, encompassing 44 specific scenarios. The videos vary considerably in duration, resolution, and aspect ratio. They originate from diverse sources, with substantial proportion newly collected from public resources or manually constructed."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate total of 18 mainstream MLLMs, including 3 cutting-edge closed-source models and 15 open-source models. The closed-source models involve GPT-4o [55], Gemini-2.5 Pro [56] and Gemini-1.5 Pro [5]. In selecting open-source models, we consider two factors. First, models are categorized by parameter size into small (7B/8B), medium (16B/32B/38B), and large (72B/78B) groups. Second, models are differentiated by their video processing strategies, including (a) sparse frame sampling (InternVL3 [30], LLaVA-OneVision [57], VITA-1.5 [2], LLaVA-Video [28], Kimi-VL [58], Qwen2.5-VL [29], Oryx-1.5 [59]), (b) dense sampling with token compression (VideoLLaMA 3 [60], VideoChat-Flash [46]), and (c) the slow-fast frame sampling approach (Slow-fast MLLM [61]). Please refer to Appendix for details of the experimental setup. 7 Table 3: Evaluation results on MME-VideoOCR. TR denotes Text Recognition, VTQA Visual Text QA, TG Text Grounding, AR Attribute Recognition, CDT Change Detection & Tracking, STP Special Text Parsing, CFTU Cross-Frame Text Understanding, TBR Text-Based Reasoning, TBVU Text-Based Video Understanding, and RVT Robust Video Testing. The highest accuracy of each task is in red , and the second highest is underlined. Model VTQA TBVU CFTU Total CDT RVT TBR STP Size TG AR TR Gemini-1.5 Pro GPT-4o Gemini-2.5 Pro LLaVA-OneVision VideoChat-Flash Slow-fast MLLM VITA-1.5 Oryx-1.5 LLaVA-Video VideoLLaMA 3 Qwen2.5-VL InternVL3 Oryx-1.5 Kimi-VL Qwen2.5-VL InternVL InternVL3 Qwen2.5-VL - - - 7B 7B 7B 7B 7B 7B 7B 7B 8B 32B 16B 32B 38B 78B 72B 76.7% 83.3% 83.0% 42.0% 36.7% 46.0% 49.0% 51.7% 47.0% 47.3% 70.3% 61.3% 50.3% 54.7% 58.3% 67.0% 77.6% 81.6% 91.6% 50.0% 48.0% 54.8% 58.4% 54.0% 59.2% 57.6% 70.0% 72.0% 60.0% 66.4% 77.2% 76.8% 61.5% 60.5% 64.5% 49.0% 60.0% 52.0% 43.0% 50.5% 61.0% 68.0% 58.0% 60.0% 63.5% 59.0% 62.5% 65.0% 70.0% 80.7% 77.6% 80.0% 67.5% 65.0% Closed-source MLLMs 64.7% 74.7% 74.0% 55.0% 51.5% 70.0% 74.0% 68.0% 84.4% Small-scale MLLMs 54.0% 60.0% 60.0% 61.3% 54.7% 68.7% 64.7% 68.7% 69.3% 41.0% 49.0% 47.0% 49.0% 44.5% 48.5% 50.0% 48.5% 56.5% 46.4% 46.0% 48.0% 53.2% 52.8% 50.0% 54.0% 66.4% 62.4% Middle-scale MLLMs 62.7% 62.7% 68.7% 76.0% 46.0% 48.0% 52.0% 61.0% 60.4% 57.6% 70.4% 69.6% Large-scale MLLMs 76.0% 74.0% 65.5% 56.5% 71.6% 79.6% 31.3% 30.7% 48.7% 20.0% 19.3% 20.0% 20.0% 23.3% 21.3% 21.3% 17.3% 23.3% 21.3% 23.3% 22.7% 24.7% 68.7% 60.7% 74.0% 45.3% 50.0% 43.3% 51.3% 48.7% 47.3% 48.7% 49.3% 57.3% 54.7% 56.7% 68.7% 76.0% 53.5% 59.0% 56.5% 52.0% 54.0% 48.5% 47.0% 47.0% 56.5% 55.0% 53.0% 55.0% 61.0% 57.5% 54.5% 61.5% 68.0% 75.3% 72.0% 60.0% 60.7% 54.0% 58.7% 64.0% 68.7% 67.3% 71.3% 71.3% 68.0% 71.3% 65.3% 76.7% 64.9% 66.4% 73.7% 46.0% 47.8% 47.8% 49.5% 49.6% 52.8% 53.5% 59.1% 59.8% 55.2% 56.2% 61.0% 66.1% 24.7% 26.7% 77.3% 74.7% 57.0% 57.0% 75.3% 78.7% 67.2% 69.0%"
        },
        {
            "title": "4.1 Main Results",
            "content": "We evaluate the performance of all baseline models on MME-VideoOCR and display the accuracy for each task category and the overall accuracy, as shown in Table 3. Our observations indicate that among the 18 evaluated models, Gemini-2.5 Pro is the top performer, yet achieves an accuracy of only 73.7%. Concurrently, five models that demonstrate strong performance on other video understanding tasks achieved an accuracy below 50% on MME-VideoOCR. This performance landscape underscores the challenging nature and discriminative capability of the MME-VideoOCR benchmark. Next, it is clear that models with larger parameter scales tend to achieve higher accuracy, with clear scaling effect evident in the Qwen2.5-VL [29], InternVL3 [30], and Oryx-1.5 [59] series. Meanwhile, model architecture significantly impacts performance. Despite achieving high scores on general video understanding multiple-choice benchmarks (e.g., Video-MME [50], MLVU [62]), token compression methods show clear disadvantage on MME-VideoOCR. Representative approaches such as VideoChat-Flash [46] and Slow-fast MLLM [61] illustrate this limitation, suggesting that critical information may be lost during the token merging process. In addition, our benchmark presents strong discriminative power across task categories, which could be taken as the potential direction for MLLM optimization. For tasks such as Text Recognition, Visual Text QA, and Text-Based Reasoning, the performance gap between the best and worst-performing models exceeds 30%, clearly distinguishing model capabilities across different levels of perception, understanding, and reasoning. Furthermore, the benchmark reveals several common defects of the mainstream MLLMs. In tasks such as Change Detection & Tracking and Text-Based Video Understanding, most models achieve an accuracy below 60%, indicating significant challenges in dynamic scene comprehension and temporal alignment. For Cross-Frame Text Understanding, which requires multi-frame integration and memory, the baseline models generally achieve an accuracy below 25%, underscoring their limited capacity for semantic integration across frames."
        },
        {
            "title": "4.2 Analysis and Findings",
            "content": "Table 4 presents the accuracy of the top-5 performing models among the 18 evaluated MLLMs on each task. The full results for all models are provided in Appendix C.3. 8 Table 4: Accuracy of top-5 performing evaluated MLLMs on each task. Fine-grained task types offer an accurate reflection of the models capabilities and limitations across multiple dimensions. InternVL3 Gemini 38B 2.5-Pro Qwen2.5-VL 72B InternVL3 78B Task Category GPT-4o Task Text Recognition Visual Text QA Text Grounding Attribute Recognition Text Recognition at Designated Locations Text Recognition Based on Specific Attributes Text-Centric QA Translation Spatial Grounding Temporal Grounding Color Recognition Named Entity Recognition Counting Change Detection & Tracking Change Detection Tracking Special Text Parsing Cross-Frame Text Understanding Table Parsing Chart Parsing Document Parsing Mathematical Formula Parsing Handwriting Recognition Scrolling Text Understanding Trajectory Recognition Scrambled Recognition Text-Based Reasoning Complex Reasoning Text-Based Video Understanding Robust Video Testing Subtitle-Based Video Understanding Multi-Hop Needle in Haystack AIGC Videos Long Videos Adversarial Videos Total - 86.0% 77.0% 93.5% 84.0% 88.0% 41.0% 76.0% 84.0% 62.0% 57.0% 83.0% 92.0% 84.0% 92.0% 68.0% 86.0% 70.0% 0.0% 76.0% 74.0% 86.0% 27.0% 88.0% 44.0% 84.0% 73.7% 80.5% 81.0% 83.5% 66.0% 81.0% 49.0% 90.0% 78.0% 54.0% 44.0% 69.0% 74.0% 72.0% 94.0% 88.0% 70.0% 64.0% 0.0% 16.0% 74.7% 96.0% 18.0% 88.0% 58.0% 90.0% 69.0% 72.5% 65.0% 80.0% 68.0% 77.0% 58.0% 90.0% 74.0% 64.0% 55.0% 76.0% 56.0% 68.0% 76.0% 90.0% 68.0% 70.0% 0.0% 4.0% 77.3% 96.0% 18.0% 84.0% 68.0% 74.0% 67.2% 82.0% 86.0% 84.5% 70.0% 67.0% 54.0% 88.0% 74.0% 62.0% 46.0% 57.0% 52.0% 68.0% 90.0% 62.0% 68.0% 62.0% 0.0% 30.0% 60.7% 93.0% 25.0% 82.0% 60.0% 84.0% 66.4% 70.0% 61.0% 78.0% 72.0% 83.0% 47.0% 88.0% 76.0% 64.0% 48.0% 74.0% 60.0% 66.0% 76.0% 80.0% 66.0% 70.0% 0.0% 4.0% 76.0% 95.0% 28.0% 88.0% 62.0% 80.0% 66.1% (a) Different resolution settings. (b) Different frame sampling settings. Figure 5: Model performance on MME-VideoOCR under different resolution and frame sampling settings. Both lower resolution and reduced frame count significantly degrade performance, underscoring the importance of visual coverage and clarity in OCR tasks. Resolution and Number of Frames. To investigate the impact of resolution and frame count on models performance in OCR tasks, we conduct two sets of comparative experiments. For the resolution study, we use Qwen2.5-VL [29], VideoLLaMA 3 [60], and Oryx-1.5 [59], all of which support dynamic resolution settings. In this experiment, the maximum number of input frames per sample is fixed at 32. Subsequently, the original video resolution is adjusted by scaling the longer edge of each frame to 224, 336, 448, or 560 pixels. As shown in Figure 5a, increasing the input resolution consistently leads to performance improvements across all models. To analyze the effect of input frame count, we select Qwen2.5-VL [29], InternVL3 [30], LLaVA-Video [28], and Oryx-1.5 [59], as these models are equipped with relatively long context windows. As illustrated 9 in Figure 5b, increasing the number of input frames generally leads to notable improvement in model performance. However, we observe slight performance drop for Qwen2.5-VL and InternVL3 when the number of input frames increases from 32 to 64. This suggests that when the context becomes excessively long, the models may struggle to focus on task-relevant content, potentially due to limitations in attention allocation or memory compression within long sequences. These findings highlight the importance of both high resolution and sufficient temporal coverage for OCR tasks. Effective Utilization of Textual Information. In Subtitle-Based Video Understanding, most models achieve relatively strong performance. We investigate the task samples and reveal that the correct answers typically appear in single frame or small number of frames within the video. This suggests that leading MLLMs are capable of effectively utilizing textual information embedded in videos, and can combine it with visual context to perform accurate video understanding. Limitations in Temporal Integration Capability. As shown in Table 3, all models exhibit clear shortcomings in Cross-Frame Text Understanding tasks, with most models achieving accuracies around 20%. Table 4 further breaks down the performance on individual tasks within this category. All of the top-5 performing models yield an accuracy of 0% on Trajectory Recognition, and 4 out of 5 achieve less than 35% accuracy on Scrambled Recognition. These results underscore common deficiency in the temporal integration capability of current MLLMs. The large performance gap between the two subtasks under the Text-Based Video Understanding category further supports this observation. Both Subtitle-Based Video Understanding and Multi-Hop Needle in Haystack require effective video understanding grounded in textual information. However, the key difference lies in the distribution of relevant content: in the former, useful information appears in just few frames, whereas in the latter, it is scattered across multiple frames and requires the model to perform effective memory and integration. This contrast reveals critical limitation in current MLLMs: rather than effectively aggregating information across multiple frames, most models appear to rely primarily on information from small number of frames for video OCR. Figure 6: Examples illustrating language prior bias in MLLMs. The models tend to incorrectly recognize the text based on plausible language priorsfor instance, throuh skin as through skin, togther as together, OFF COURS as OFF COURSE, and CAI as CAT. These cases highlight the strong influence of language priors on MLLM responses. Significant Language Prior Bias. One notable failure mode in MLLMs is their tendency to over-rely on language priors when interpreting visually presented text. As illustrated in Figure 6, these models often convert visibly misspelled text into contextually plausible forms, even when the input is visually clear and unambiguous. This indicates that MLLMs frequently prioritize semantic likelihood over visual fidelity, generating interpretations that reflect linguistic expectations rather than the actual visual content. This bias poses serious challenge for OCR-related tasks, where character-level accuracy is essential. Notably, the misrecognitions are not arbitrary; they follow consistent patterns that favor high-frequency or semantically familiar words over rare, misspelled, or out-of-vocabulary terms. Such behavior underscores the dominant role of language priors, which can override visual evidenceparticularly when visual and textual signals are not sufficiently disentangled."
        },
        {
            "title": "5 Conclusions, Discussions and Limitations",
            "content": "This paper introduces MME-VideoOCR, benchmark designed for the comprehensive evaluation of video OCR capabilities. The benchmark comprises 25 practical OCR tasks, encompassing bilingual, perceptual, comprehension, and reasoning abilities. Experimental results demonstrate that MMEVideoOCR possesses sufficient difficulty and discrimination to expose the deficiencies of current MLLMs, thereby offering directions for the potential optimization. 10 While we endeavored to collect and construct videos from 9 diverse scenario categories with manually annotated, precise ground truth, the inherent richness of visual elements in videos means that some concepts may be underrepresented by samples. This may lead to score fluctuations in certain subcategories due to model sensitivity to sparse data. Although augmenting the dataset with more samples could mitigate this, we constrained the total number of items to 2, 000 due to considerations of annotation and evaluation costs. Furthermore, to assess fundamental abilities, we deliberately structured the questions into easy, medium, and hard difficulty tiers. Cutting-edge MLLMs have demonstrated strong performance on the easy and medium-difficulty questions. We intend to supplement the current version with more challenging samples as MLLM capabilities advance, ensuring its continued relevance in guiding future development."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction. arXiv preprint arXiv:2501.01957, 2025. [3] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. [4] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [5] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [6] Yang Shi, Jiaheng Liu, Yushuo Guan, Zhenhua Wu, Yuanxing Zhang, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, et al. Mavors: Multi-granularity video representation for multimodal large language model. arXiv preprint arXiv:2504.10068, 2025. [7] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. survey of multimodel large language models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409, 2024. [8] Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. The revolution of multimodal large language models: survey. arXiv preprint arXiv:2402.12451, 2024. [9] Fenglin Liu, Tingting Zhu, Xian Wu, Bang Yang, Chenyu You, Chenyang Wang, Lei Lu, Zhangdaihong Liu, Yefeng Zheng, Xu Sun, et al. medical multimodal large language model for future pandemics. NPJ Digital Medicine, 6(1):226, 2023. [10] Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, et al. Embodiedeval: Evaluate multimodal llms as embodied agents. arXiv preprint arXiv:2501.11858, 2025. [11] Gabriel Sarch, Lawrence Jang, Michael Tarr, William Cohen, Kenneth Marino, and Katerina Fragkiadaki. Vlm agents generate their own memories: Distilling experience into embodied programs of thought. Advances in Neural Information Processing Systems, 37:7594275985, 2025. [12] Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, et al. Mm-rlhf: The next step forward in multimodal llm alignment. arXiv preprint arXiv:2502.10391, 2025. 11 [13] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023. [14] Shunji Mori, Hirobumi Nishida, and Hiromitsu Yamada. Optical character recognition. John Wiley & Sons, Inc., 1999. [15] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. 2024. [16] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint arXiv:2403.04473, 2024. [17] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [18] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947952. IEEE, 2019. [19] Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. [20] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. [21] Ling Fu, Biao Yang, Zhebin Kuang, Jiajun Song, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, Mingxin Huang, et al. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321, 2024. [22] Sankalp Nagaonkar, Augustya Sharma, Ashish Choithani, and Ashutosh Trivedi. Benchmarking vision-language models on optical character recognition in dynamic video environments. arXiv preprint arXiv:2502.06445, 2025. [23] Yulin Fei, Yuhui Gao, Xingyuan Xian, Xiaojin Zhang, Tao Wu, and Wei Chen. Do current video llms have strong ocr abilities? preliminary study. arXiv preprint arXiv:2412.20613, 2024. [24] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024. [25] Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, and Cong Yao. Layoutllm: Layout instruction tuning with large language models for document understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1563015640, 2024. [26] Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, and Angela Yao. Egotextvqa: Towards egocentric scene-text aware video question answering. arXiv preprint arXiv:2502.07411, 2025. [27] Kai Wang, Boris Babenko, and Serge Belongie. End-to-end scene text recognition. In International conference on computer vision, pages 14571464. IEEE, 2011. [28] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [29] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 12 [30] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [31] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. [32] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. [33] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. In The Thirteenth International Conference on Learning Representations, 2024. [34] Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, et al. Mme-survey: comprehensive survey on evaluation of multimodal llms. arXiv preprint arXiv:2411.15296, 2024. [35] Minyi Zhao, Bingjia Li, Jie Wang, Wanqing Li, Wenjing Zhou, Lan Zhang, Shijie Xuyang, Zhihang Yu, Xinkun Yu, Guangze Li, et al. Towards video text visual question answering: Benchmark and baseline. Advances in Neural Information Processing Systems, 35:3554935562, 2022. [36] George Tom, Minesh Mathew, Sergi Garcia-Bordils, Dimosthenis Karatzas, and CV Jawahar. Reading between the lanes: Text videoqa on the road. In International Conference on Document Analysis and Recognition, pages 137154. Springer, 2023. [37] Yiting Lu, Xin Li, Yajing Pei, Kun Yuan, Qizhi Xie, Yunpeng Qu, Ming Sun, Chao Zhou, and Zhibo Chen. Kvq: Kwai video quality assessment for short-form videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2596325973, 2024. [38] Jie Lei, Tamara Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. Advances in Neural Information Processing Systems, 34:1184611858, 2021. [39] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 70837093, 2019. [40] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:19472 19495, 2024. [41] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [42] Weijia Wu, Yuanqiang Cai, Debing Zhang, Sibo Wang, Zhuang Li, Jiahong Li, Yejun Tang, and Hong Zhou. bilingual, openworld video text dataset and end-to-end video text spotter with transformer. arXiv preprint arXiv:2112.04888, 2021. [43] Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Watching the news: Towards videoqa models that can read. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 44414450, 2023. [44] Zhanzhan Cheng, Jing Lu, Yi Niu, Shiliang Pu, Fei Wu, and Shuigeng Zhou. You only recognize once: Towards fast video text spotting. In Proceedings of the 27th ACM international conference on multimedia, pages 855863, 2019. 13 [45] Sangeeth Reddy, Minesh Mathew, Lluis Gomez, Marçal Rusinol, Dimosthenis Karatzas, and CV Jawahar. Roadtext-1k: Text detection & recognition dataset for driving videos. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 1107411080. IEEE, 2020. [46] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. [47] Weijia Wu, Yuzhong Zhao, Zhuang Li, Jiahong Li, Mike Zheng Shou, Umapada Pal, Dimosthenis Karatzas, and Xiang Bai. Icdar 2023 video text reading competition for dense and small text. arXiv preprint arXiv:2304.04376, 2023. [48] PaddlePaddle. Paddleocr. https://github.com/PaddlePaddle/PaddleOCR, 2020. Accessed: 2025-05-09. [49] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [50] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [51] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. [52] Yi-Fan Zhang, Weichen Yu, Qingsong Wen, Xue Wang, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Debiasing multimodal large language models. arXiv preprint arXiv:2403.05262, 2024. [53] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [54] Ling Fu, Biao Yang, Zhebin Kuang, Jiajun Song, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, Mingxin Huang, et al. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321, 2024. [55] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [56] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [57] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [58] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. [59] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: Ondemand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 14 [60] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. [61] Min Shi, Shihao Wang, Chieh-Yun Chen, Jitesh Jain, Kai Wang, Junjun Xiong, Guilin Liu, Zhiding Yu, and Humphrey Shi. Slow-fast architecture for video multi-modal large language models. arXiv preprint arXiv:2504.01328, 2025. [62] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [63] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024. [64] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 15 Representative Examples from MME-VideoOCR To comprehensively illustrate the characteristics of tasks in MME-VideoOCR, we present one representative example for each task. Figure 7: An example QA of the Text Recognition at Designated Locations task in MME-VideoOCR. Figure 8: An example QA of the Text Recognition Based on Specific Attributes task in MMEVideoOCR. Figure 9: An example QA of the Text-Centric QA task in MME-VideoOCR. 16 Figure 10: An example QA of the Translation task in MME-VideoOCR. Figure 11: An example QA of the Spatial Grounding task in MME-VideoOCR. Figure 12: An example QA of the Temporal Grounding task in MME-VideoOCR. 17 Figure 13: An example QA of the Change Detection task in MME-VideoOCR. Figure 14: An example QA of the Tracking task in MME-VideoOCR. Figure 15: An example QA of the Complex Reasoning task in MME-VideoOCR. 18 Figure 16: An example QA of the Subtitle-Based Video Understanding task in MME-VideoOCR. Figure 17: An example QA of the Multi-Hop Needel in Haystack task in MME-VideoOCR. 19 Figure 18: An example QA of the Table Parsing task in MME-VideoOCR. Figure 19: An example QA of the Chart Parsing task in MME-VideoOCR. 20 Figure 20: An example QA of the Document Parsing task in MME-VideoOCR. Figure 21: An example QA of the Mathematical Formula Parsing task in MME-VideoOCR. Figure 22: An example QA of the Handwriting Recognition task in MME-VideoOCR. 21 Figure 23: An example QA of the Color Recognition task in MME-VideoOCR. Figure 24: An example QA of the Named Entity Recognition task in MME-VideoOCR. Figure 25: An example QA of the Counting task in MME-VideoOCR. 22 Figure 26: An example QA of the Scrolling Text Understanding task in MME-VideoOCR. Figure 27: An example QA of the Trajectory Recognition task in MME-VideoOCR. Figure 28: An example QA of the Scrambled Recognition task in MME-VideoOCR. 23 Figure 29: An example QA of the AIGC Video task in MME-VideoOCR. Figure 30: An example QA of the Adbersarial Video task in MME-VideoOCR. Figure 31: An example QA of the Long Video task in MME-VideoOCR."
        },
        {
            "title": "B Benchmark Details",
            "content": "B.1 Task Definition MME-VideoOCR collects 10 OCR task categories. Detailed definition of the taxonomy is depicted as below. Text Recognition. Text Recognition is fundamental OCR task that evaluates an MLLMs ability to perceive and interpret text. This category involves Text Recognition at Designated Locations and Text Recognition Based on Specific Attributes. These two subtasks can be flexibly combined to assess an MLLMs capacity for fine-grained text recognition. For instance, query may require recognizing text specifically located on license plate and written in particular language or color, thereby evaluating both spatial awareness and attribute-based recognition within complex visual scenes. Visual Text QA. Visual Text QA encompasses two tasks: Text-Centric QA and Translation. TextCentric QA requires models to integrate textual content with relevant visual cues to answer contextdependent questions. Translation focuses on converting specific on-screen text into designated target language. Both tasks challenge the models ability to not only perceive but also comprehend multimodal information. Text Grounding. Text Grounding involves Spatial Grounding and Temporal Grounding. Spatial Grounding concerns identifying the location of specified text based on visual contextsuch as recognizing that the text appears on street sign or product labelrather than relying on exact coordinates. Temporal Grounding centers on understanding the temporal properties of text, including when it appears, how long it remains visible, and the sequence in which it occurs. Together, these subtasks assess the models ability to localize and interpret text across both spatial and temporal dimensions within dynamic visual scenes. Attribute Recognition. This category is composed of three tasks: Color Recognition, where models are expected to identify the color of the text; Named Entity Recognition, which focuses on extracting and classifying named entities such as person names, organization names, and location names; and Counting, where models must accurately identify the number of textual elements that meet specified criteria. Change Detection & Tracking. The task consists of two tasks: Change Detection and Tracking. Given the highly dynamic nature of text in video, Change Detection aims to accurately identify changes in textual content over time. Tracking, on the other hand, focuses on monitoring text elements as they change position across framesfor example, tracing the movement of vehicle with specified license plate number or identifying the player running with the ball based on their jersey number. Special Text Parsing. Special Text Parsing includes five tasks: Table Parsing, Chart Parsing, Document Parsing, Mathematical Formula Parsing, and Handwriting Recognition. These tasks require models to accurately identify and understand text with either special structures or highly variable visual forms. Cross-Frame Text Understanding. In video scenarios, relying on single frame is often insufficient, as critical information may be distributed across multiple frames and closely interrelated. To address this, the task of Cross-Frame Text Understanding is introduced, which requires models to integrate information across multiple frames for coherent interpretation. It includes three subtasks: Scrolling Text Understanding, which focuses on recognizing dynamic text streamssuch as on-screen bullet commentsthat move across frames and may only be fully readable when aggregated over time; Trajectory Recognition, where the motion path of an object in the video forms recognizable text, and the model must interpret this trajectory as the intended message; Scrambled Recognition, which involves identifying and reconstructing complete text from characters that appear out of order across different positions in the video frames. Text-Based Reasoning. Text-Based Reasoning, also referred to as Complex Reasoning, emphasizes advanced understanding of textual content, such as code analysis, mathematical operations, and logical reasoning. Unlike Text-Centric QA, which is straightforward comprehension task centered on identifying explicit information, Complex Reasoning requires models to go beyond surface-level understanding by synthesizing dispersed textual cues, identifying implicit relationships, and resolving ambiguity or misleading content. 25 Text-Based Video Understanding. Current video understanding tasks are primarily based on visual dynamics, such as action recognition and video captioning. However, these tasks often overlook the textual information in videos, even though they are essential for video understanding in certain contexts. To address this gap, we introduce Subtitle-Based Video Understanding. In this task, the answer to question is hidden in the subtitles, and MLLMs must combine subtitle information with visual content to answer correctly. This reflects real-world scenarios like conversations, tutorials, or news, where subtitles provide key information that visuals alone cannot capture. Multi-Hop Needle in Haystack is novel and effective task introduced in VideoChat-Flash [46], designed to test models ability to retrieve information from videos based on subtitles that are spread across multiple frames. It requires reasoning over multiple pieces of subtitle content to find the correct answer. Robust Video Testing. To evaluate model effectiveness and robustness across diverse scenarios, we introduce three specialized video types: AIGC Videos, Long Videos, and Adversarial Videos. AIGC Videos, generated by AI systems [41], assess model adaptability to increasingly common synthetic content. Long Videos test the ability to extract relevant information from lengthy sequences with substantial redundancy. Since existing MLLMs primarily process videos by extracting frames, we construct set of Adversarial Videos by strategically inserting all-black frames into normal videos. While these adversarial samples have minimal impact on human perception, they can easily mislead the model, rendering it virtually blind. B.2 Task Distribution Table 5: Number of QA Pairs per task in MME-VideoOCR."
        },
        {
            "title": "Attribute Recognition",
            "content": "Change Detection & Tracking"
        },
        {
            "title": "Special Text Parsing",
            "content": "Cross-Frame Text Understanding"
        },
        {
            "title": "Text Recognition at Designated Locations\nText Recognition Based on Specific Attributes",
            "content": "Text-Centric QA Translation"
        },
        {
            "title": "Scrolling Text Understanding\nTrajectory Recognition\nScrambled Recognition",
            "content": "Text-Based Reasoning"
        },
        {
            "title": "Complex Reasoning",
            "content": "Text-Based Video Understanding Subtitle-Based Video Understanding Multi-Hop Needle in Haystack"
        },
        {
            "title": "AIGC Videos\nLong Videos\nAdversarial Videos",
            "content": "- #QA 200 100 250 50 100 100 50 50 100 100 50 50 50 50 50 50 50 50 150 100 100 50 50 2,000 Given the diverse range of task types included in MME-VideoOCR, which assess broad spectrum of model capabilities, we carefully allocate the number of QA pairs across different tasks. Table 5 26 Table 6: Evaluation prompt setting of MME-VideoOCR (Containment Match). [Video] Based on the video and the question below, directly answer the content that needs to be recognized in plain text. Do not include any additional explanations, formatting changes, or extra information. Question: [Question] The answer is: Table 7: Evaluation prompt setting of MME-VideoOCR (GPT-Assisted Scoring). [Video] Based on the video and the question below, directly provide the answer in plain text. Do not include any additional explanations, formatting changes, or extra information. Question: [Question] The answer is: presents the specific number of QA pairs for each task. This allocation ensures balanced distribution among perception, understanding, and reasoning tasks, thereby supporting comprehensive and equitable evaluation of model capabilities. B.3 Evaluation Prompt The prompt settings for Containment Match, GPT-Assisted Scoring and Multiple-Choice are shown in Table 6, Table 7 and Table 8. For GPT-Assisted Scoring (designed for the Translation task), after obtaining the models response using the prompt shown in Table 7, we subsequently utilize GPT-4o-0806 to evaluate the response. The corresponding evaluation prompt is provided in Table 9."
        },
        {
            "title": "C Experiment Details",
            "content": "C.1 Evaluated Models We evaluate total of 18 mainstream MLLMs, including 3 leading proprietary models and 15 high-performing open-source models. For proprietary models, we evaluate GPT-4o [55], Gemini-2.5 Pro [5] and Gemini-1.5 Pro [63]. GPT-4o is the latest multimodal large language model developed by OpenAI, offering fast and cost-effective performance across text, image, and audio modalities. It achieves state-of-the-art results on variety of benchmarks, with notable improvements in visual reasoning, OCR, and multilingual understanding. GPT-4o features unified architecture that enables seamless cross-modal interaction, making it highly efficient and versatile for real-world multimodal applications. Gemini-2.5 Pro is one of the latest Multimodal Large Language Models released by Google DeepMind. It features improved visual and video understanding capabilities, with support for extended context lengths and more efficient cross-modal alignment. Gemini-2.5 Pro demonstrates strong performance across wide range of tasks, including video captioning, image reasoning, and OCR-based understanding. Its enhanced architecture and training scale make it particularly competitive in complex multimodal benchmarks. Gemini-1.5 Pro, an earlier version in the Gemini series, also supports multimodal input and is optimized for high-quality text generation and basic vision-language tasks. While it delivers reliable performance on standard image-based benchmarks, its video comprehension abilityespecially in tasks requiring temporal reasoning and dense visual-textual alignmentis more limited compared to its successor. Nevertheless, it remains strong baseline among proprietary models. For open-source models, we select Qwen2.5-VL [29], LLaVA-Video [28], LLaVA-OneVision [57], VideoLLaMA 3 [60], VideoChat-Flash [46], Oryx-1.5 [59], Slowfast-MLLM [61], InternVL3 [64], VITA-1.5 [2] and Kimi-VL [58]. Among them, for Oryx-1.5, Qwen2.5-VL, and InternVL3, we include versions with different parameter scales in our experiments. 27 Table 8: Evaluation prompt setting of MME-VideoOCR (Multiple-Choice). [Video] Select the best answer to the following multiple-choice question based on the video. Respond with only the letter (A, B, C, or D) of the correct option. Question: [Question] Option: A. [Option A] B. [Option B] C. [Option C] D. [Option D] The best answer is: Table 9: Evaluation prompt setting of the Translation task. You are professional bilingual translation evaluator. Here are two sentences: one in Chinese and one in English. Sentence 1: [Ground Truth] Sentence 2: [MLLMs Response] Please evaluate whether the two sentences convey the same meaning and can be considered accurate translations of each other. If the meanings are equivalent and the translation is accurate, respond with \"correct\". If there are significant differences in meaning or inaccuracies in translation, respond with \"wrong\". You must only respond with one word: \"correct\" or \"wrong\". Do not provide any explanations, comments, or additional text. Focus solely on semantic equivalence, not grammar or style. Ignore minor differences as long as the meaning is preserved. Qwen2.5-VL is vision-language model that introduces two key innovations: native dynamicresolution processing and Multi-scale Rotary Position Embedding (MRoPE). The dynamicresolution capability allows the model to process images and videos of varying resolutions and frame rates efficiently, extending to the temporal dimension through dynamic FPS sampling. This enables precise temporal event localization in long videos. MRoPE enhances the models ability to capture multi-scale positional information, improving its performance in tasks requiring fine-grained spatial and temporal understanding . LLaVA-Video extends the LLaVA framework to video understanding by unifying visual representations into the language feature space. This alignment before projection enables the model to perform visual reasoning on both images and videos simultaneously. By training on mixed dataset of images and videos, LLaVA-Video leverages mutual enhancement between modalities, achieving superior performance across various visual-language tasks . LLaVA-OneVision is designed for versatile visual task transfer across single-image, multiimage, and video scenarios. It employs SigLIP vision encoder and Qwen2 language backbone, processing images with the Anyres technique to handle high-resolution inputs effectively. Videos are processed with fixed token length per frame for memory efficiency. This architecture enables LLaVA-OneVision to excel in diverse visual-language tasks without task-specific fine-tuning. VideoLLaMA 3 is vision-centric multimodal foundation model that advances image and video understanding. It utilizes Any-resolution Vision Tokenization (AVT) to process images and videos of varying resolutions dynamically. The models training paradigm emphasizes high-quality image-text data to enhance video understanding capabilities. VideoLLaMA 3 achieves state-of-the-art performance on multiple benchmarks by integrating vision-centric training and framework designs. VideoChat-Flash is long-context video-language model that introduces Hierarchical visual token Compression (HiCo) method, effectively reducing redundancy in long videos by compressing visual tokens from the clip-level to the video-level. This approach enables high-fidelity representation while significantly lowering computational costs. Coupled with 28 multi-stage short-to-long learning scheme and training on the LongVid dataset, VideoChatFlash achieves state-of-the-art performance on both long and short video benchmarks. Oryx-1.5 presents unified multimodal architecture designed for on-demand spatial-temporal understanding of images, videos, and multi-view 3D scenes. It features dynamic compressor module that performs token compression and adaptive positional embedding, allowing the model to efficiently process visual inputs with arbitrary spatial sizes and temporal lengths. This flexibility enables Oryx-1.5 to seamlessly handle diverse visual inputs across various modalities. Slowfast-MLLM integrates the SlowFast dual-pathway architecture with multimodal large language model to explicitly capture both coarse and fine-grained temporal dynamics. The slow branch models long-term context, while the fast branch focuses on short-term changes, enabling rich motion representation. This design enhances temporal alignment and supports detailed video-text interaction in tasks such as action question answering and event tracking. InternVL3 is powerful vision-language model that unifies visual grounding, dense captioning, and temporal understanding via cross-modality fusion backbone. It introduces region-level supervision and multi-frame alignment strategies, significantly improving its spatial-temporal grounding capabilities. InternVL3 demonstrates superior performance across wide range of multimodal tasks, benefiting from its native multimodal pre-training paradigm and advanced post-training techniques. VITA-1.5 is multimodal large language model designed to achieve real-time vision and speech interaction. It pioneers meticulously crafted three-stage training strategy to effectively integrate vision, language, and speech modalities. This strategy systematically introduces visual and auditory data, mitigating conflicts between modalities while preserving robust multimodal capabilities. This methodology empowers VITA-1.5 to process and understand both visual and speech inputs and to generate fluent, end-to-end speech outputs, thereby enabling more natural and seamless interactive multimodal conversations. Kimi-VL is state-of-the-art vision-language model developed by Moonshot AI, based on the Kimi series of large language models. Designed to handle complex multimodal tasks, Kimi-VL integrates high-resolution visual encoders with large-scale language understanding to enable robust performance in image captioning, visual question answering, and document understanding. It adopts Mixture-of-Experts (MoE) architecture to improve inference efficiency, dynamically activating subset of experts for each input. This design allows Kimi-VL to scale effectively while maintaining strong generalization across diverse visuallanguage benchmarks. C.2 Experimental Setup For proprietary models, we used the gpt-4o-2024-08-06, gemini-2.5-pro-preview-05-06 and gemini-1.5-pro-002 APIs, respectively. In the MME-VideoOCR evaluation, most models were configured with maximum input frame count of 64. GPT-4o was limited to 50 input frames due to API token constraints, while VITA-1.5 was restricted to 16 frames because of context length limitations. All other settings followed default or recommended configurations. During the comparative experiments described in Section 4.2, the number of input frames was fixed at 32 when varying the resolution, while the default resolution settings were applied to all models when varying the number of input frames. C.3 Experiment Results Table 10, Table 11 and Table 12 present the complete results of evaluated models across all tasks in MME-VideoOCR. 29 Kimi-VL 54.5% 55.0% 68.5% 58.0% 71.0% 47.0% 70.0% 70.0% 48.0% 33.0% 63.0% 54.0% 48.0% 74.0% 60.0% 52.0% 70.0% 0.0% 0.0% 56.7% 95.0% 20.0% 82.0% 54.0% 78.0% 56.2% Table 10: Accuracy of evaluated MLLMs on each task of MME-VideoOCR. Gemini 1.5 Pro Qwen2.5-VL 7B Qwen2.5-VL 32B InternVL 8B Task Task Category Text Recognition Visual Text QA Text Grounding Attribute Recognition Text Recognition at Designated Locations Text Recognition Based on Specific Attributes Text-Centric QA Translation Spatial Grounding Temporal Grounding Color Recognition Named Entity Recognition Counting Change Detection & Tracking Change Detection Tracking Special Text Parsing Cross-Frame Text Understanding Table Parsing Chart Parsing Document Parsing Mathematical Formula Parsing Handwriting Recognition Scrolling Text Understanding Trajectory Recognition Scrambled Recognition Text-Based Reasoning Complex Reasoning Text-Based Video Understanding Robust Video Testing Subtitle-Based Video Understanding Multi-Hop Needle in Haystack AIGC Videos Long Videos Adversarial Videos"
        },
        {
            "title": "Total",
            "content": "- 55.0% 65.0% 81.5% 60.0% 73.0% 52.0% 78.0% 78.0% 50.0% 40.0% 64.0% 66.0% 60.0% 90.0% 76.0% 60.0% 52.0% 0.0% 16.0% 68.7% 93.0% 16.0% 66.0% 46.0% 84.0% 61.0% 64.0% 56.0% 75.5% 58.0% 77.0% 43.0% 80.0% 72.0% 56.0% 49.0% 64.0% 56.0% 60.0% 72.0% 64.0% 60.0% 70.0% 0.0% 0.0% 57.3% 96.0% 14.0% 86.0% 50.0% 78.0% 59.8% 70.0% 71.0% 76.0% 46.0% 77.0% 39.0% 78.0% 76.0% 52.0% 40.0% 57.0% 58.0% 68.0% 86.0% 60.0% 60.0% 48.0% 0.0% 4.0% 49.3% 90.0% 16.0% 78.0% 56.0% 80.0% 59.1% 80.0% 70.0% 83.0% 56.0% 78.0% 45.0% 62.0% 80.0% 52.0% 43.0% 67.0% 72.0% 74.0% 80.0% 76.0% 68.0% 72.0% 0.0% 22.0% 68.7% 90.0% 17.0% 86.0% 42.0% 76.0% 64.9% Table 11: Accuracy of evaluated MLLMs on each task of MME-VideoOCR. Oryx-1.5 32B VideoLLaMA 3 LLaVA Video-7B Task Oryx-1.5 7B Task Category Text Recognition Visual Text QA Text Grounding Attribute Recognition Text Recognition at Designated Locations Text Recognition Based on Specific Attributes Text-Centric QA Translation Spatial Grounding Temporal Grounding Color Recognition Named Entity Recognition Counting Change Detection & Tracking Change Detection Tracking Special Text Parsing Cross-Frame Text Understanding Table Parsing Chart Parsing Document Parsing Mathematical Formula Parsing Handwriting Recognition Scrolling Text Understanding Trajectory Recognition Scrambled Recognition Text-Based Reasoning Complex Reasoning Text-Based Video Understanding Robust Video Testing Subtitle-Based Video Understanding Multi-Hop Needle in Haystack AIGC Videos Long Videos Adversarial Videos"
        },
        {
            "title": "Total",
            "content": "- 47.5% 47.0% 63.5% 34.0% 65.0% 71.0% 76.0% 66.0% 52.0% 39.0% 61.0% 44.0% 50.0% 68.0% 64.0% 44.0% 60.0% 0.0% 4.0% 48.7% 91.0% 19.0% 78.0% 56.0% 68.0% 53.5% 49.0% 43.0% 67.0% 28.0% 70.0% 52.0% 84.0% 66.0% 56.0% 40.0% 57.0% 44.0% 42.0% 64.0% 56.0% 44.0% 60.0% 0.0% 4.0% 47.3% 93.0% 20.0% 86.0% 54.0% 66.0% 52.8% 53.0% 49.0% 62.0% 22.0% 59.0% 42.0% 64.0% 64.0% 36.0% 35.0% 54.0% 50.0% 44.0% 70.0% 58.0% 42.0% 68.0% 0.0% 2.0% 48.7% 78.0% 16.0% 80.0% 40.0% 72.0% 49.6% 52.5% 46.0% 67.0% 32.0% 73.0% 54.0% 66.0% 68.0% 54.0% 37.0% 55.0% 52.0% 46.0% 76.0% 74.0% 54.0% 64.0% 0.0% 0.0% 54.7% 86.0% 36.0% 80.0% 52.0% 72.0% 55.2% Table 12: Accuracy of evaluated MLLMs on each task of MME-VideoOCR. Task Category Task VITA-1.5 Slow-fast MLLM VideochatFlash-7B LLaVA OneVision-7B Text Recognition Visual Text QA Text Grounding Attribute Recognition Text Recognition at Designated Locations Text Recognition Based on Specific Attributes Text-Centric QA Translation Spatial Grounding Temporal Grounding Color Recognition Named Entity Recognition Counting Change Detection & Tracking Change Detection Tracking Special Text Parsing Cross-Frame Text Understanding Table Parsing Chart Parsing Document Parsing Mathematical Formula Parsing Handwriting Recognition Scrolling Text Understanding Trajectory Recognition Scrambled Recognition Text-Based Reasoning Complex Reasoning Text-Based Video Understanding Robust Video Testing Subtitle-Based Video Understanding Multi-Hop Needle in Haystack AIGC Videos Long Videos Adversarial Videos"
        },
        {
            "title": "Total",
            "content": "- 46.0% 46.0% 61.5% 28.0% 61.0% 43.0% 66.0% 70.0% 44.0% 44.0% 50.0% 42.0% 42.0% 64.0% 60.0% 32.0% 58.0% 0.0% 2.0% 43.3% 83.0% 14.0% 58.0% 38.0% 66.0% 47.8% 37.5% 35.0% 55.5% 18.0% 61.0% 59.0% 64.0% 66.0% 50.0% 43.0% 55.0% 32.0% 40.0% 56.0% 58.0% 44.0% 58.0% 0.0% 0.0% 50.0% 88.0% 20.0% 78.0% 44.0% 60.0% 47.8% 42.0% 42.0% 57.0% 22.0% 58.0% 40.0% 66.0% 62.0% 34.0% 36.0% 46.0% 40.0% 40.0% 56.0% 56.0% 40.0% 58.0% 0.0% 2.0% 45.3% 86.0% 18.0% 78.0% 36.0% 66.0% 46.0% 48.0% 51.0% 63.0% 40.0% 53.0% 33.0% 66.0% 58.0% 60.0% 37.0% 61.0% 44.0% 44.0% 72.0% 64.0% 42.0% 60.0% 0.0% 0.0% 51.3% 83.0% 11.0% 68.0% 42.0% 66.0% 49.5%"
        }
    ],
    "affiliations": [
        "CASIA",
        "CUHKSZ",
        "Kuaishou",
        "NTU",
        "PKU",
        "THU",
        "XJTU"
    ]
}