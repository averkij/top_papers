{
    "paper_title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
    "authors": [
        "Alexey Dontsov",
        "Dmitrii Korzh",
        "Alexey Zhavoronkin",
        "Boris Mikheev",
        "Denis Bobkov",
        "Aibek Alanov",
        "Oleg Y. Rogov",
        "Ivan Oseledets",
        "Elena Tutubalina"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Machine Unlearning (MU) is critical for enhancing privacy and security in deep learning models, particularly in large multimodal language models (MLLMs), by removing specific private or hazardous information. While MU has made significant progress in textual and visual modalities, multimodal unlearning (MMU) remains significantly underexplored, partially due to the absence of a suitable open-source benchmark. To address this, we introduce CLEAR, a new benchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious individuals and 3,700 images linked with corresponding question-answer pairs, enabling a thorough evaluation across modalities. We assess 10 MU methods, adapting them for MMU, and highlight new challenges specific to multimodal forgetting. We also demonstrate that simple $\\ell_1$ regularization on LoRA weights significantly mitigates catastrophic forgetting, preserving model performance on retained data. The dataset is available at https://huggingface.co/datasets/therem/CLEAR"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 7 5 0 8 1 . 0 1 4 2 : r CLEAR: Character Unlearning in Textual and Visual Modalities Alexey Dontsov1,6, Dmitrii Korzh1,3, Alexey Zhavoronkin2,4, Boris Mikheev3, Denis Bobkov1,6, Aibek Alanov1,6, Oleg Y. Rogov1,3,5, Ivan Oseledets1,3, Elena Tutubalina1,6 1AIRI 2MIPT 3Skoltech 4Sber 5University of Sharjah 6HSE University Correspondence: dontsov@airi.net; tutubalina@airi.net"
        },
        {
            "title": "Abstract",
            "content": "Machine Unlearning (MU) is critical for enhancing privacy and security in deep learning models, particularly in large multimodal language models (MLLMs), by removing specific private or hazardous information. While MU has made significant progress in textual and visual modalities, multimodal unlearning (MMU) remains significantly underexplored, partially due to the absence of suitable opensource benchmark. To address this, we introduce CLEAR, new benchmark designed to evaluate MMU methods. CLEAR contains 200 fictitious individuals and 3,700 images linked with corresponding question-answer pairs, enabling thorough evaluation across modalities. We assess 10 MU methods, adapting them for MMU, and highlight new challenges specific to multimodal forgetting. We also demonstrate that simple ℓ1 regularization on LoRA weights significantly mitigates catastrophic forgetting, preserving model performance on retained data. The dataset is available at https:// huggingface.co/datasets/therem/CLEAR."
        },
        {
            "title": "Introduction",
            "content": "Figure 1: The overview of our dataset. Large Language Models (LLMs) (Touvron et al., 2023; Jiang et al., 2023) are trained on vast corpora of data that contain private, unethical, or unwanted information, leading to growing concerns. Machine unlearning (MU) methods have been developed to remove such unwanted data without expensive retraining from scratch. For instance, MU has been applied for the LLMs to mitigate issues related to toxicity (Lu et al., 2022), copyright and privacy concerns (Jang et al., 2022; Eldan and Russinovich, 2023; Wu et al., 2023) and fairness (Yu et al., 2023). Additionally, such topics as model editing (Ilharco et al., 2022; Zhang et al., 2023), prevention of hallucinations (Yao et al., 2023), and sensitive knowledge exposure (Barrett et al., 2023) have also motivated the development of MU techniques. There are various unlearning techniques suitable solely for LLMs (Yao et al., 2024b,a; Xing et al., 2024; Zhang et al., 2024) or for vision models (Li et al., 2024a; Chen and Yang, 2023; Tarun et al., 2021). However, multimodal LLMs (MLLMs) (Liu et al., 2023), specifically visual LLMs (VLLMs), raise new challenges. The unlearning of such multimodal models (MMU) remains largely unexplored, primarily due to the lack of open-source benchmarks. Moreover, current MU benchmarks (Maini et al., 2024; Shi et al., 2024; Yao et al., 2024a; Li et al., 2024b) are focused on single modalities, and, to our knowledge, no open benchmarks designed explicitly for evaluating unlearning in multimodal models exist at the time of submission. To address this gap, we propose CLEAR, new 1 Figure 2: Summary of our dataset. We generate 200 persons and use multimodal unlearning to forget the part of them. After, we measure the unlearning quality and the models capabilities by calculating set of metrics. Then, we create leaderboard of unlearning methods based on these metrics. benchmark for textual-visual MMU named, focusing on person unlearning, which aligns with the right-to-be-forgotten concept. The dataset is synthetic to ensure control over the data the model learns, preventing object leakage during training. We generated consistent images through comprehensive strategy and linked them to the corresponding author-related questions from the large-scale textual unlearning benchmark TOFU (Maini et al., 2024). The proposed dataset contains 200 fictitious authors, 3,770 visual question-answer pairs, and 4,000 textual question-answer pairs, enabling thorough evaluation of the single and multi-modal unlearning techniques. We also propose benchmark to assess MU and MMU methods, evaluating 10 techniques, including the current state of the art. We evaluate the unlearning methods in textual, visual, and multimodal setups. First, we finetune the model on our entire dataset. Next, we take small predefined subset of 20 authors, referred to as the forget set, while the remaining data forms the retain set. We then apply the unlearning procedure, resulting in new model that no longer \"remembers\" individuals from the forget set but retains knowledge of those in the retain set. To ensure the models capabilities are not compromised during unlearning, we assess its performance on real-world tasks, such as celebrity face recognition and general domain visual question-answering (VQA). Overall, we work with four sets: Forget, Retain, Real Faces, and Real World. Figure 1 illustrates an example from each set. We evaluate existing unlearning methods separately in textual and vision modalities and then combine them within an MLLM. We create leaderboard for each domain, highlighting that multimodal unlearning poses new challenges. Finally, we demonstrate that applying ℓ1 regularization to the LoRA adapter during the unlearning process significantly improves performance, helping to prevent catastrophic forgetting of the retain set information. Overall, our contributions can be summarized as follows: We propose novel benchmark, CLEAR, for evaluating machine unlearning in multimodal (textual-visual) setups. To the best of our knowledge, this is the first publicly available MMU benchmark. We comprehensively evaluate existing unlearning methods across separate and combined domains. We construct leaderboards for these three domains and show that stateof-the-art unlearning algorithms struggle in multimodal setups, highlighting the need for new approaches. We demonstrate that the ℓ1 weight regularization on the LoRA adapter helps to improve unlearning quality by significantly preventing catastrophic forgetting."
        },
        {
            "title": "2.1 MU Methods and Textual Benchmarks",
            "content": "MU methods (Cao and Yang, 2015; Dwork et al., 2014; Kurmanji et al., 2024; Neel et al., 2021; Sekhari et al., 2021) remove the impact of certain 2 data instances from trained model without requiring full retraining. The goal is to obtain model that behaves like the forget data was never part of the training set. MU can be formalized in two main ways. In setting where the unlearned model must produce identical outputs to model trained without the forget data. In an inexact unlearning setting, the main objective is to obtain any model that doesnt contain knowledge from the forget set, but with no restrictions and guarantees on the models response to inputs from the retain set. There are several standard textual unlearning benchmarks. TOFU (Maini et al., 2024) is benchmark for textual LLM unlearning, featuring 200 fictitious author profiles, each defined by attributes such as name, birthplace, parents names and occupation, written books, etc. Totally the dataset has 4,000 question-answer pairs (20 per author). Predefined 10/90, 5/95, and 1/99 splits are used for forget/retain sets. Models are finetuned on the entire dataset, and unlearning is applied to the forget pairs. WMDP (Li et al., 2024b) consists of 3,668 multiplechoice questions, evaluates hazardous knowledge of LLMs, and serves as benchmark for unlearning such dangerous information. In (Yao et al., 2024a), the authors explore seven textual unlearning techniques. The best approach was combination of gradient ascent with gradient descent on the forget and retain sets, respectively, to maintain the unlearn/retain quality trade-off. They evaluated their methods on three benchmark datasets from different domains: 500 arXiv papers, 2000 GitHub files, and 100 books covering academic texts, code, and literary works. Unfortunately, these three benchmarks cannot be applied to the MMU evaluation."
        },
        {
            "title": "2.2 MMU Methods and Benchmarks",
            "content": "MLLMs (Liu et al., 2023) typically consist of three core components: modality encoder that translates raw input into feature embeddings, modality projection layer aligning these features within the language space, and pre-trained language model synthesizing the final output. Nonetheless, MMU research is still in its early stages. (Cheng and Amiri, 2023) proposed the MultiDelete method, which focuses on separating cross-modal embeddings for the forget set while preserving unimodal embeddings for the retain set. Unfortunately, this approach is suitable only for the encoder-decoder architecture and can not be directly transferred to decoder-only LLMs. EFUF (Xing et al., 2024) mitigates hallucinations in MLLMs using unlearning. It measures the similarity between generated captions and image content with CLIP model (Radford et al., 2021) to automatically detect hallucinated (negative) and non-hallucinated (positive) examples based on the calibrated on the MSCOCO dataset (Lin et al., 2014) thresholds, eliminating manual labeling. The unlearning process applies three loss functions: negative loss to forget hallucinations, positive loss to reinforce correct representations, and sentence loss to maintain fluency. However, their benchmark is not open-sourced. Single Image Unlearning (SIU) (Li et al., 2024a) focuses on unlearning visual concepts in MLLMs while preserving textual knowledge and introduces MMUBench with five evaluation metrics. The benchmark covers 20 concepts with at least 50 images for the concept, including real-world figures and cartoon characters. For each concept, one image is selected as the forget subset, paired with various prompts, while the remaining images form the retain subset. However, SIUs use of single image raises scalability concerns for complex concepts, and their unlearning of VLLM is limited to the visual domain. Moreover, MMUBench is also not open-sourced. (Chakraborty et al., 2024) explores unlearning harmful content in VLLMs, demonstrating that unlearning in the textual domain alone can match the performance of text-image unlearning while using fewer resources. Their method combines harmful loss and KL divergence between unlearned and retrained models. The approach was tested on six datasets, including PKU-SafeRLHF (Ji et al., 2024) and three vision-text attack datasets (Shayegani et al., 2023; Luo et al., 2024; Gong et al., 2023) for harmful content prompts, and Truthful-QA and VQA-v2 (Lin et al., 2021; Goyal et al., 2017) to ensure benign task performance remained intact. However, their focus on safety alignment may limit applicability for general unlearning, such as biometric privacy. While they claim that textual unlearning is sufficient for MMU, our findings show this is not true for all methods. Additionally, their approach lacks exact unlearning evaluation."
        },
        {
            "title": "3 Methodology",
            "content": "Unlearning can be conceptualized in two main ways: the objective of the Strict Unlearning is to achieve model that behaves identically to one 3 Figure 3: Examples of generated images showcasing distinct individual from our dataset. trained exclusively on the retain set, ensuring that no knowledge from the forget set is present; the objective of the Inexact Unlearning is to produce any model that no longer contains information from the forget set. However, this approach cannot guarantee how the model will respond to inputs related to the forget set. These objectives require distinct methods, loss functions, and evaluation metrics to assess unlearning quality. Let fθ denote the original model with its parameters θ. Source model fθ is trained on train dataset D, and given the unlearning objective, we want to make our model forget subset of the source dataset D, called forget set DF . The remaining part of the training dataset is called retain set, and we aim to preserve the models performance on this data subset DR := DF . Additionally, we utilize holdout set DH to establish reference for the models desired behavior on DF after the unlearning process. The models training did not include this set, ensuring that DH = . In nutshell, forget set DF contains samples the model should unlearn and serves as direct measure of unlearning effectiveness; retain set DR contains samples that the model should retain and perform well on, serving as an indicator of the models preserved knowledge; holdout set DH contains samples that the model has never seen before and serves as reference for the models behavior on data that was not involved in the training process. Such forgetting procedure is performed by updating the model fθ with particular unlearning method, which results in new unlearned model fˆθ with parameters ˆθ. For the evaluation, we can also train separately gold model gω, which is trained only on the DR. So the objective is to obtain the unlearned model fˆθ which unlearns the forget set DF while preserving the performance on retain set DR as good as the source model fθ. It can be done by optimizing (minimizing or maximizing) the specific criterion with the model parameters θ and resulting obtained parameters ˆθ will be the parameters of desired unlearned model fˆθ. For example, one can consider the gradient difference MU approach for the optimization, aimed at increasing forget loss and maintaining retain performance: = (cid:88) L(xi, yi, θ) + λ xiDf (cid:88) xj DR L(xj, yj, θ) θ (cid:55) θ αθ L, (1) (2) where λ forget-retain trade-off hyper-parameter, α learning rate, is loss function, for example, negative-log-likelihood. is the input text, image, or both of them in the case of VLLM. In this work, we explore subset of unlearning methods, including Retain Finetune, Gradient Ascend (GA), Gradient Difference (GD) (Liu et al., 2022), SCRUB (Kurmanji et al., 2023), DPO (Rafailov et al., 2023), NPO (Zhang et al., 2024), LLMU (Yao et al., 2024b) , IDK (Maini et al., 2024), RMU (Li et al., 2024b), and KL (Golatkar et al., 2020), which are detailed in Appendix A.We selected these methods based on their ease of adaptation to new modalities, requiring only changes in input data (text, images, or both) while preserving their core functionality. In essence, these methods involve variations of hard negative training on the forget set combined with fine-tuning on the retain set, often with additional constraints such as Cross-Entropy or KL divergence to align the models outputs on the retain set with the original model."
        },
        {
            "title": "4 CLEAR",
            "content": "The MU (and consequently MMU) benchmark should ideally avoid running unlearning on wellknown information that could be obtained from external sources such as books, games, movies, etc. This is essential for more reliable evaluation of the models performance on retain and forget. To meet this requirement, we chose to extend the TOFU dataset due to its ease of use, flexibility for adopting to new modalities (such as adding face images or personal voices), and its strong connection to privacy concerns, making it ideal for testing unlearning in sensitive contexts. 4 to maintain the models visual capabilities during unlearning. 4.2 Splits Ultimately, we utilize the following four splits (sets) to evaluate unlearning: Forget. Following methodology from (Maini et al., 2024), DF is made from data of 2, 10, and 20 persons (1%, 5% and 10% correspondingly) of the full set D, consisting of 200 authors. This DF is expected to be unlearned by the model. Retain. Retain set DR is made from all the other data from the complete set D, not included in the DF . The model should continue to work well on this subset and preserve its performance as much as possible. Real Faces. To ensure the model retains knowledge of related concepts, such as faces, which are not present in the finetuning dataset, we evaluate it using set of real-world faces. Specifically, we use the MillionCelebs dataset (Zhang et al., 2020), which consists of celebrity face-name pairs. We intersect this dataset with the most recognized celebrities from any year on the Forbes Celebrity 100 list to increase the likelihood that the model has seen these faces during pre-training. This results in final set of 150 face-name pairs. Real World. To ensure that the models overall visual capabilities remain intact throughout the unlearning process, we evaluate its performance on the Visual Question Answering (VQA) task using samples from (x.ai, 2024). Figure 1 presents random sample from each of these splits."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "To comprehensively evaluate unlearning performance on textual, visual, and textual-visual domains, we measure the MU and MMU performance in terms of the following metrics: ROUGE-L We calculate the ROUGE-L (Lin, 2004) score between the models predictions and ground truth answers. This metric measures how much the model remembers in exact formulations. However, the generation of models does not always represent the inner knowledge of ones. That is where the next metric is used. Probability Score One way of exposing elicit knowledge from model is through its logits, which are assigned to some factual tokens. We 1 define the conditional probability p(yx) for input and answer (power 1 corresponds to Figure 4: Distributions of the attributes of the authors faces. We show that CLEAR is balanced and representative regarding age, gender, and ethnicity. 4.1 Dataset Generation Process Firstly, for each of the 200 authors from the TOFU dataset, we extract their name, age, and ethnicity based on the knowledge provided in the original dataset. Also, we generate pool of 2000 faces using StyleGAN2 (Karras et al., 2020) - an established generative model for face synthesis. Each face is scored with pre-trained CNNs to get the age, gender, and ethnicity of the face. Then, we manually select the most suitable face that matches these attributes for each author. During this phase, we discovered that the age distribution of the authors was highly shifted towards the older ages, so we needed to eliminate this gap. To do this, we used the image editing framework proposed in (Bobkov et al., 2024) to shift the visual attributes of the faces to make them older. The distribution of the characteristics of the faces and authors is shown in Figure 4. After matching each author to face, we used the diffusion model (Li et al., 2024c) of personalized generation to synthesize images with given face and corresponding to given prompt. In detail, the face generation and collection process is described in Appendix B. The diffusion model needs textual prompt for image generation besides the face. We ask GPT-4 to generate images from textual question and an answer about an author. We generate 8 images for each prompt, evaluate them using an ensemble of fake-detection models, and select the most realistic. Additionally, GPT-4o generates captions for each image and visual prompt pair, which are then included in the dataset. However, due to restrictions caused by GPT guard breaks and the identification of several bugs in the TOFU dataset (such as nameless author), the final dataset includes fewer images than text pairs (3,770 compared to 4,000). We also incorporated two additional data splits containing real-world face photos and natural images normalizing for length). Each input question is considered as multiple choice question with possible answers y1, ..., yn, and then, assuming y1 is the correct answer, desired probability score is . It will be bounded between computed as p(y1x) (cid:80) i=1 p(yix) 0 and 1. lower probability score indicates that the model is less confident about generating content. Truth Ratio quantifies the alignment between predictions and the ground truth by comparing the probability of paraphrased correct answer against the averaged probabilities of several similarly formatted incorrect answers, providing insight into the effectiveness of the unlearning algorithm in removing specific information while maintaining overall accuracy. Assume that ˆy denotes paraphrased version of the answer for input and is the set of 5 perturbations of the answer y. Then desired truth ratio is calculated as: 1 (cid:80) yY p(yx) 1 p(ˆyx) 1 ˆy = . (3) This ratio is normalized and rescaled between 0 and 1, with higher values indicating improved unlearning. Forget Quality. Measuring the quality of forgetting in MU presents significant challenges. The objective is to create model that cannot be distinguished from one trained solely on retain. The established (Hayes et al., 2024) way of measuring the unlearning quality is calculating the U-LIRA score. However, it requires training at least 128 model copies, which is computationally expensive for LLM. feasible method for achieving this is proposed. We calculate statistical test on the outputs of two models: our unlearned model and the gold model. The Truth Ratio metric is considered as output for its effectiveness in informativeness. To assess this metric, the Kolmogorov-Smirnov test (KS-Test) is employed to compare the distributions of Truth Ratios from both models. high p-value from this test suggests effective forgetting, while low p-value indicates potential privacy leakage and poor unlearning. We call this p-value the Forget Quality of the unlearning method. Also, we define Real, Retain, and Forget metrics as harmonical mean of ROUGE, Real Probability score, and Truth Ratio. Method Real Metric Retain Metric Forget Metric Log Forget Quality 7 - 2 L B 7 - t Retain FT LLMU KL GA GD IDK DPO SCRUB RMU NPO Retain FT LLMU KL GA GD IDK DPO SCRUB RMU NPO 0.50 0.38 0.24 0.25 0.61 0.46 0.50 0.50 0.51 0.50 0.67 0.65 0.28 0.26 0.60 0.63 0.67 0.66 0.09 0. 0.26 0.03 0.00 0.00 0.13 0.26 0.26 0.26 0.26 0.28 0.34 0.30 0.00 0.00 0.01 0.32 0.33 0.33 0.00 0.33 0.42 0.01 0.00 0.00 0.01 0.24 0.42 0.42 0.59 0.62 0.47 0.39 0.00 0.00 0.00 0.45 0.47 0.47 0.00 0.47 -4.92 -2.31 -18.22 -17.22 -48.59 -4.92 -4.92 -4.92 -42.86 -44.46 -3.87 -6.69 -50.30 -36.06 -51.16 -2.72 -3.63 -3.39 -123.22 -3. Table 1: Unlearning methods on textual domain only. The gray color represents low retain metric, indicating the method diverges. Hence, we do not consider them. Method Original Gold Retain FT SCRUB LLMU RMU DPO SCRUBbio Sparsity Twins Forget Acc. Holdout Acc. Retain Acc. U-LIRA U-MIA 100.00 15.43 100.00 99.74 85.72 67.97 50.21 42.59 66.41 50.00 18.50 15.04 18.54 16.77 14.62 17.27 13.93 14.25 14.44 20. 100.00 97.52 100.00 99.93 88.99 99.99 81.49 99.44 83.57 99.72 1.00 0.50 1.00 0.98 0.83 0.77 0.73 0.71 0.78 0.73 0.96 0.50 0.92 0.90 0.75 0.60 0.62 0.57 0.73 0. Table 2: Results of unlearning on visual modality only. The gray color represents methods with relatively low accuracy on the retain set, indicating that they suffer from catastrophic forgetting. Therefore, we do not consider these methods to be successful."
        },
        {
            "title": "5 Experiments",
            "content": "First, we explore the capabilities of the current unlearning methods within single domains in Sec. 5.1 and 5.2. Second, we transfer them to textualvisual MMU in Sec. 5.3."
        },
        {
            "title": "5.1 Unlearning Textual Domain (LLMs)",
            "content": "For the experiments on the textual domain exclusively, we consider the textual part of the proposed CLEAR dataset and use the LLMs, which are often used in MLLMs, specifically Llama2-7B (Touvron et al., 2023) and Mistral-7B (Jiang et al., 2023). First, we finetune the model on all the data and unlearn it on the forget set. To evaluate the final quality of unlearning, we calculate Real, Retain, and Forget metrics and Forget Quality. The full results are provided in Table 1. See the Appendix for pipeline details and hyperparameters. Loss Modality Real Forget Retain Log Forget Quality Original Gold LLMU LLMU LLMU SCRUB SCRUB SCRUB DPO DPO DPO text visual both text visual both text visual both 0.48 0.50 0.47 0.50 0.47 0.49 0.48 0.49 0.46 0.49 0.46 0.3 0.19 0.37 0.35 0. 0.35 0.37 0.36 0.38 0.22 0.22 0.51 0.51 0.49 0.51 0.51 0.51 0.49 0.52 0.49 0.49 0. -61.22 0.00 -71.23 -60.26 -95.12 -61.22 -60.26 -60.26 -62.18 -90.26 -91.46 Table 3: Results of unlearning of different modalities. We finetune on full datasets (both modalities), then forget on single domain subset (text or visual) or full forget set. Original model before unlearning. Gold - model trained only on retain."
        },
        {
            "title": "5.2 Unlearning Visual Domain",
            "content": "Experiments on the visual domain of the CLEAR dataset focus on the face biometrics (identification) task. While face identification can be treated as classification task with fixed set of individuals, it is typically framed as few-shot or metric learning problem, aiming to train an embedding model that maps images of the same person to nearby points in the embedding space and ensures separation between different individuals and this should hold even for the unseen during training persons. We chose ResNet-18 (He et al., 2015) due to its relatively small size and scalability. We fine-tuned ResNet-18, pre-trained on ImageNet, for 100 epochs on the Celebs dataset (Zhang et al., 2020), using photos of 797 individuals for training and 200 for testing. To compute identification accuracy, we averaged the embeddings of five images per individual to obtain reference (enrollment) vectors and classified the remaining images using cosine similarity. The model achieved 99.0% accuracy on the training set and 83.4% on the test set. For unlearning evaluation, the model was finetuned on the visual part of CLEAR dataset 128 times, following Membership Inference Attack (MIA) approach. We trained 64 models with the forget included and 64 models without it, alternating with the holdout set (if the forgetting set is used for training, the holdout set is not, and vice versa). We evaluated the models on DF , DR, and DH using accuracy, U-LIRA (Hayes et al., 2024), and U-MIA attack metrics. U-MIA is lightweight, population-based attack that trains binary classifier on shadow model outputs and their status (1 if from the forget set, 0 otherwise). Successful unlearning occurs when Holdout Accuracy closely aligns with Forget Accuracy. From the standpoint of the attack, U-LIRA and U-MIA should be unable to distinguish between the two sets, resulting in an attack accuracy close to 0.50. This situation would indicate that the model has \"forgotten\" the specific data, as the model treats the forgetting and holdout sets similarly. We compare the forget and holdout sets logit distributions in Appendix 6. 5.3 Multimodal Experiments For the source model, we use LLaVa model (Liu et al., 2023) with ViT (Dosovitskiy et al., 2021) as visual encoder and LLaMa2-7B (Touvron et al., 2023) as language model. First, we finetune it on the full CLEAR, both visual and textual parts, and call this model \"original\", as it contains forget and retain sets of knowledge. Then, we perform the unlearning process on it. We use the same hyperparameters for each method. Then, we evaluate the unlearned model according to our metric setup described in Sec. 4.3. For comparison, we demonstrate the metrics of the \"gold\" model. The results of experiments and corresponding metrics are provided in Table 4. Details of the experiments pipeline are described in Appendix F. 5.3. Is Textual Unlearning Enough? We begin by addressing the question: Can we forget person using only textual data, and does multimodality introduce new challenges to unlearning? To explore this, we attempt to forget 20 individuals from the forget set using only textual unlearning. We also perform unlearning using only visual data or both modalities, and then compare the results. We find that unlearning text alone is sufficient to achieve low forget metric, consistent with previous findings. However, this also results in noticeable drop in retain metrics. Full results are provided in Table 3."
        },
        {
            "title": "5.3.2 Unlearning Both Domains",
            "content": "After we understand that multimodal unlearning can not be fully addressed using single modality, we proceed with experiments on unlearning across both modalities. We take our source model fθ and apply unlearning methods. As forget set, we use all the available data about 20 persons - 10% of the dataset size. 7 Method Original Gold GA GA GD GD IDK IDK KL KL NPO NPO Retain FT Retain FT RMU RMU LLMU LLMU DPO DPO SCRUB SCRUB LoRA L1 Regularization Real metric 0.48 0.50 0.32 0.49 0.24 0.49 0.48 0.49 0.27 0.49 0.49 0.49 0.49 0.49 0.27 0.49 0.47 0.49 0.46 0.48 0.49 0. Retain metric 0.51 0.51 0.00 0.50 0.00 0.50 0.51 0.50 0.00 0.50 0.51 0.51 0.51 0.50 0.00 0.50 0.49 0.51 0.49 0.50 0.51 0.51 Forget metric 0.39 0.19 0.00 0.37 0.00 0.37 0.30 0.37 0.00 0.37 0.36 0.36 0.36 0.37 0.00 0.36 0.37 0.36 0.39 0.37 0.36 0.35 Log Forget Quality -61.22 0.00 -13.04 -61.22 -17.72 -62.18 -74.40 -63.15 -13.92 -62.18 -63.15 -64.13 -60.26 -61.22 -23.68 -61.22 -73.34 -60.26 -61.22 -65.12 -62.18 -61.22 Table 4: Results on experiments with and without LoRA regularization. The gray color shows that the method completely fails on the retain set."
        },
        {
            "title": "5.4 LoRA Regularization",
            "content": "As shown previously (Jia et al., 2024), model sparsity can improve unlearning, but significant drops in retain metrics still occur. We hypothesize that keeping the model close to its initial state during unlearning could help preserve retain knowledge. LoRA adapters (Hu et al., 2021) have become standard technique to reduce computational demands in large-scale NLP models. We propose using the magnitude of LoRA weights as proxy for how far the model has deviated from its initial state. To address this, we add the ℓ1 norm of the adapter weights to the unlearning loss, using fixed λ = 0.01 without tuning, though tuning could improve results. The results of the experiments are shown in Table 4."
        },
        {
            "title": "6 Results and Discussion",
            "content": "Text domain Table 1 presents the results for the text domain, showing that the RMU, KL, GD, and GA methods excel in unlearning the forget set (with the forget metric dropping to 0), but they suffer from catastrophic forgetting on the retain data (the retain metric also drops to 0). The remaining methods maintain performance on the retain set (retain metrics remain roughly the same), but their unlearning quality is poor the forget metric is close to the one achieved by Retain FT. Identifying an optimal method that balances unlearning and retention is challenging. However, among the tested methods, IDK, DPO, and SCRUB provide the best (lowest) 8 forget metrics without drop in retain performance. These observations are consistent across both the LLaMa and Mistral models. Visual MU results are presented in Table 2. It shows that most methods achieve high accuracy on the forget set with competitive U-LIRA and U-MIA values. Notably, SCRUBbio and Twins perform best across all considered metrics, making them optimal in this context. The Holdout Accuracy is relatively consistent across methods. In multimodal unlearning, Table 3 shows that for the LLMU method, unlearning both modalities yields better results than text-only unlearning. The forget metric drops from 0.37 in the textual domain to 0.25 when unlearning both domains, while the retain and real metrics remain stable. For DPO, the results are less straightforward, but it is evident that unlearning the visual domain is crucial. When unlearning in the visual or both domains, the forget metric is 0.22, compared to 0.38 for text-only unlearning. The retain and real metrics stay consistent. However, SCRUB remains robust across all modalities, performing consistently in all three setups. Then we run our experiments on unlearning both domains. The picture is very similar to the experiments on the textual domain only. Table 4 shows that GA, GD, KL and RMU effectively unlearn the forget set (forget metric goes to 0) but exhibit significant catastrophic forgetting of the retain set (retain metrics also goes to 0). In contrast, IDK, SCRUB, LLMU and DPO remain stable on the retain set (around 0.48), but their unlearning quality is worse (0.37 versus 0.39 on the original ununlearned model). The achieving the balance between unlearning and retention is challenging again. Leaderboards We construct our leaderboards straightforwardly. First, we exclude methods that fail to retain knowledge from the retain set (highlighted in gray in the tables) and then rank the remaining methrods based on the Forget metric (or U-LIRA in the visual domain). The top-3 methods among each modality are shown in figure 2 LoRA Regularization Lastly, the experiments on ℓ1 LoRA regularization  (Table 4)  show improvements in unlearning quality for several methods, significantly reducing catastrophic forgetting in Gradient Ascent, Gradient Difference, KL minimization, RMU, and especially in LLMU. However, the proposed regularization is not beneficial for all unlearning techniques."
        },
        {
            "title": "References",
            "content": "In this work, we introduce CLEAR, the first opensourced benchmark designed to assess machine unlearning in multimodal (textual and visual) setup. Our evaluation of existing unlearning techniques across domains shows that multimodal unlearning is more challenging than previously anticipated, laying the ground for further research. Our studies on incorporating LoRA regularization term demonstrate that this simple technique improves unlearning and can be easily integrated into other MU methods. We aim to encourage further research on enhancing privacy and security in large-scale AI models by offering an open-source benchmark. Future work could focus on improving MMU algorithms and expanding unlearning to new modalities, such as voice and video."
        },
        {
            "title": "Limitations",
            "content": "Despite the contributions of this work, several limitations remain that need further investigation. One major limitation is the reliance on synthetic data, as CLEAR is based on such dataset, which may not fully capture the complexity of real-world scenarios, thus limiting the generalizability of our findings. Additionally, while our work focuses on unlearning methods designed for privacy-centric applications, such as removing personal data, it may not fully address other unlearning needs, such as removing harmful content. Moreover, our benchmark mainly evaluates fine-tuning-based unlearning methods using sophisticated loss functions, leaving unexplored other broader unlearning techniques, such as analytical or mechanical approaches. Another challenge lies in the scalability of these unlearning methods, as they may struggle to scale efficiently when applied to larger models and datasets, hindering their potential use in real-world systems. Furthermore, our focus on catastrophic forgetting overlooks unintended side effects, such as the introduction of biases or the degradation of model performance on unrelated tasks, and the broader impact of unlearning on fairness and safety remains an open area for future research."
        },
        {
            "title": "Ethics",
            "content": "We utilized 84 hours of A100 GPU computation for our experiments, which resulted in an estimated 9 kg of CO2 emissions. Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, et al. 2023. Identifying and mitigating the security risks of generative ai. Foundations and Trends in Privacy and Security, 6(1):152. Denis Bobkov, Vadim Titov, Aibek Alanov, and Dmitry Vetrov. 2024. The devil is in the details: Stylefeatureeditor for detail-rich stylegan inversion and high quality image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 93379346. Yinzhi Cao and Junfeng Yang. 2015. Towards making systems forget with machine unlearning. In 2015 IEEE symposium on security and privacy, pages 463 480. IEEE. Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. 2022. Membership inference attacks from first principles. Preprint, arXiv:2112.03570. Trishna Chakraborty, Erfan Shayegani, Zikui Cai, Nael Abu-Ghazaleh, M. Salman Asif, Yue Dong, Amit K. Roy-Chowdhury, and Chengyu Song. 2024. Crossmodal safety alignment: Is textual unlearning all you need? Preprint, arXiv:2406.02575. Jiaao Chen and Diyi Yang. 2023. Unlearn what you want to forget: Efficient unlearning for LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12041 12052, Singapore. Association for Computational Linguistics. Jiali Cheng and Hadi Amiri. 2023. Multidelete for multimodal machine unlearning. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. Preprint, arXiv:2010.11929. Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 9(3 4):211407. Ronen Eldan and Mark Russinovich. 2023. Whos harry potter? approximate unlearning in llms. Preprint, arXiv:2310.02238. Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93049312. 9 Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. 2023. Figstep: Jailbreaking large visionlanguage models via typographic visual prompts. arXiv preprint arXiv:2311.05608. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913. Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Inexact unKhalifa, and Nicolas Papernot. 2024. learning needs more careful evaluations to avoid false sense of privacy. Preprint, arXiv:2403.01218. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep residual learning for image recognition. Preprint, arXiv:1512.03385. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Preprint, arXiv:2106.09685. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089. Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022. Knowledge unlearning for mitigating privacy risks in language models. arXiv preprint arXiv:2210.01504. Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2024. Beavertails: Towards improved safety alignment of llm via humanpreference dataset. Advances in Neural Information Processing Systems, 36. Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia Liu. 2024. Model sparsity can simplify machine unlearning. Preprint, arXiv:2304.04934. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality of stylegan. Preprint, arXiv:1912.04958. Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. 2023. Towards unbounded machine unlearning. Preprint, arXiv:2302.09880. Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. 2024. Towards unbounded machine unlearning. Advances in neural information processing systems, 36. Jiaqi Li, Qianshan Wei, Chuanyi Zhang, Guilin Qi, Miaozeng Du, Yongrui Chen, and Sheng Bi. 2024a. Single image unlearning: Efficient machine unlearning in multimodal large language models. arXiv preprint arXiv:2405.12523. Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Samuel Marks, Oam Patel, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang, and Dan Hendrycks. 2024b. The wmdp benchmark: Measuring and reducing malicious use with unlearning. Preprint, arXiv:2403.03218. Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. 2024c. Photomaker: Customizing realistic human photos In Proceedings of the via stacked id embedding. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 86408650. Chin-Yew Lin. 2004. Rouge: package for automatic In Text summarization evaluation of summaries. branches out, pages 7481. Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer. Bo Liu, Qiang Liu, and Peter Stone. 2022. Continual learning and private unlearning. Preprint, arXiv:2203.12817. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. 10 Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. 2022. Quark: Controllable text generation with reinforced unlearning. Advances in neural information processing systems, 35:27591 27609. Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. 2024. Jailbreakv-28k: benchmark for assessing the robustness of multimodal large language models against jailbreak attacks. arXiv preprint arXiv:2404.03027. Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. 2024. Tofu: task of fictitious unlearning for llms. Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. 2021. Descent-to-delete: Gradient-based methods In Algorithmic Learning for machine unlearning. Theory, pages 931962. PMLR. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Preprint, arXiv:2305.18290. Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. 2021. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems, 34:1807518086. Ayush Tarun, Vikram Chundawat, Murari Mandal, and Mohan Kankanhalli. 2021. Fast yet effective machine unlearning. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Preprint, arXiv:2307.09288. Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong. 2023. Depn: Detecting and editing privacy neurons in pretrained language models. arXiv preprint arXiv:2310.20138. Zongze Wu, Dani Lischinski, and Eli Shechtman. 2020. Stylespace analysis: Disentangled controls for stylegan image generation. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1285812867. x.ai. 2024. Grok-1.5 vision preview. Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. 2023. Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models. In The Twelfth International Conference on Learning Representations. Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, and Xinyu Dai. 2024. Efuf: Efficient fine-grained unlearning framework for mitigating hallucinations in multimodal large language models. ArXiv, abs/2402.09801. Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. 2020. Interpreting the latent space of gans for semantic face editing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 92439252. Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke Zettlemoyer, Noah A. Smith, and Chiyuan Zhang. 2024. Muse: Machine unlearning six-way evaluation for language models. Preprint, arXiv:2407.06460. Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and Xiang Yue. 2024a. Machine unlearning of pre-trained large language models. arXiv preprint arXiv:2402.15159. Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2023. Large language model unlearning. arXiv preprint arXiv:2310.10683. Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2024b. Preprint, Large language model unlearning. arXiv:2310.10683. Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP), pages 318. IEEE. Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. 2023. Unlearning bias in language models by partitioning gradients. In Findings of the Association for Computational Linguistics: ACL 2023, pages 60326048. 11 Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. 2023. Composing parameter-efficient modarXiv preprint ules with arithmetic operations. arXiv:2306.14870. Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. 2024. Negative preference optimization: From catastrophic collapse to effective unlearning. Preprint, arXiv:2404.05868. Yaobin Zhang, Weihong Deng, Mei Wang, Jiani Hu, Xian Li, Dongyue Zhao, and Dongchao Wen. 2020. Global-local gcn: Large-scale label noise cleansIn Proceedings of the ing for face recognition. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77317740."
        },
        {
            "title": "A Unlearning Methods",
            "content": "This section describes the main unlearning approaches considered in this work. 1. Finetuning on retain data. The most straightforward method to conduct unlearning is to finetune the model on the retain set, assuming that the model will unlearn the knowledge from the forget set and preserve its performance on the retain set. Despite its simplicity and reasonable effectiveness for relatively small models, it is not usable in models with huge sizes of pre-train sets, such as most LLMs. 2. Gradient ascent on forget set. In this method, unlearning is done by maximizing the loss on forget data with the intuition that it will lead to getting predictions that are dissimilar from the correct answers for forget set and consequently unlearning desired information. Thus, this method can be considered as finetuning procedure with the following loss function: L(DF , θ) = 1 DF (cid:88) xDF LL(x, θ), where NLL(x, θ) likelihood of the model on the input x. the negative logis Instead of maximizing the NLL loss, maximizing the entropy of the models predictions on the forget set is possible. The intuition behind this trick is that it will correspond to the increase of the models uncertainty in its predictions on forget set, which will also correspond to successful unlearning. 3. Gradient difference. (Liu et al., 2022) The next method builds on the concept of combining two previous methods. It aims to increase the loss on the forget data and at least maintain the loss on the retain set. The loss function is defined as follows: LGD = L(DF , θ) + L(DR, θ), 4. KL minimization . This approach aims to minimize the Kullback-Leibler (KL) divergence between the models predictions on the retain set before and after unlearning while maximizing the conventional loss on the forget set. The LKL loss function is defined as 1 DF (cid:88) xDF 1 s (cid:88) i=2 KL (cid:0)P (s<iθ)(cid:13) (cid:13)P (s<iθ)(cid:1) . The total objective function is formulated as follows: Lobj = L(DF , θ) + LKL, where θ is the models weights before unlearning, is the input sequence, is conventional loss, and (sθ) is the models logits on the input sequence with weights θ. 5. IDK tuning. Introduced in (Maini et al., 2024), this method aims to minimize the loss on the retain set, meanwhile, it uses pairs of inputs and \"I dont know\"(or some variations) labels instead of the original labels on the forget set. The loss function is defined as follows: Lidk = L(DR, θ) + L(Didk , θ), where is some loss function, DR is retain set, and Didk is forget set with labels replaced with \"I dont know\" answers or some variations of them. 6. Preference Optimization. Inspired by Direct Preference Optimization (DPO) (Rafailov et al., 2023), the unlearning task can be framed as preference optimization problem. In DPO, the model is trained to optimize user preferences directly, typically by maximizing the alignment between the models outputs and the users desired outcomes. Similarly, the goal of unlearning can be viewed as removing specific knowledge or patterns that the model has learned, effectively optimizing the models outputs to align with new preferences that exclude the undesired information. where DF is the forget set that remains constant, DR is the retain set that is randomly sampled during training, and is suitable loss function. In this context, the unlearning task aims to adjust the models parameters such that the output reflects change in the learned distribution, making the model \"forget\" specific pieces of knowledge. This can be formalized as preference optimization problem, where the preference is towards outputs that no longer rely on unwanted data. Let represent the loss function used for this task, which balances the models performance on new data and its ability to unlearn specific information. common approach is to use loss function that minimizes the difference between the models current predictions and the desired \"unlearned\" predictions of the chosen reference model. The following loss function was considered to optimize for unlearning: = λ1Ltask(Didk , θ) + λ2LDPO(πθ, πref ), LDP O(πθ, πref ) = = Ex,yDF yDidk (cid:104) log σ(β log πθ(yx) πref (yx) β log πθ(yx) πref (yx) (cid:105) , ) where πθ is related to the unlearned model which we try to optimize, σ is the sigmoid function, πref is reference model which in our case is fine-tuned on Didk data, where labels are replaced with \"I dont know\" answers, (x, y) is input-answer pair from the forget set, is \"I dont know\"-like answer corresponding to this pair, Ltask(Didk , θ) is the standard task loss (e.g., cross-entropy) on the set Didk , and LDPO(πθ, πref ) is DPO loss used for unlearning, which penalizes the model for retaining unwanted knowledge, computed between the input data and the undesired in terms of unlearning labels y. λ1 and λ2 are weighting coefficients that balance the trade-off between task performance and the unlearning process (equal to 1 both), and β is the DPO coefficient (taken as 0.1 in our setting). This formulation allows the model to optimize for maintaining task performance while ensuring the forgetting of specified information, similar to the dual objective in preference optimization. In the same way that DPO tailors the model to user preferences, this method shapes the model to \"prefer\" forgetting certain information, effectively unlearning it. 7. Negative Preference Optimization . Proposed in (Zhang et al., 2024) this method can be treated as DPO without positive examples. In our setting, the final loss function LN for this method is derived as follows: (cid:18) (cid:104) Ex,yDF 2 β log 1 + (cid:16) πθ(yx) πref (yx) (cid:17)β(cid:19) (cid:105) , where all the notation is the same as for the previous DPO method. β was also taken equal to 1. Such loss functions ensure that the model output probability πθ(yx) is as small as possible, corresponding to the unlearning objective of the forget data. 8. Teacher-Student (SCRUB) (Kurmanji et al., 2023) The main idea of this method is to train student model, which is taken as desired unlearned model from the original one, such that it will \"disobey\" the teacher original model on the forget set. The resulting loss of student model in this method is constructed as follows: d(x, ws) = KL(p(f (x; wo))p(f (x; ws))), LR = LF = α DR 1 DF (cid:88) d(xr, ws), xrDR (cid:88) xf DF d(xf , ws), Ltask = γ DR (cid:88) xrDR l(xr, yr), = LR LF + Ltask, where (x; wo) is the original teacher model with weights wo, which are kept unchanged, (x; ws) is the unlearned student model with parameters ws, which are optimized, d(x, ws) is the KL-divergence between the output distributions of the student and teacher models on the input x, ℓ is the conventional task loss (e. g. cross-entropy), and α and γ are the hyperparameters controlling the importance of the student models performance on the retain set. In our setting, α and γ were both set to 1. By minimizing this final loss L, the student model is expected to improve its performance on the retained set while unlearning from the forgotten set, respectively. 9. LLMU (Yao et al., 2024b) This method was proposed in one of the first works on unlearning LLMs (Yao et al., 2024b). In our experiments, we made slight modifications to the original method, and employed the following loss function: LF := L(DF , θ), Lr := (cid:88) (xF ,yr)DF Yr 1 yr L(xF , yr, θ), LR := (cid:88) x,yDR KL(pθ(yx)pθ(yx)), LLLM = LF + Lr + LR, where θ is the vector of unlearned model parameters, and θ is the vector of original model parameters. This loss consists of three parts. The first one, LF , is the negative conventional loss on the forget set, the optimization of which corresponds to the unlearning of the forget set. The second part, Lr, is the loss associated with \"I dont know\" labels (the original method used randomly generated labels), which also reinforces the forgetting of the DF set. The third part is the KL divergence between the models predictions on the retain set before and after unlearning, and its optimization relates to preserving the model performance on the retain set DR. Note that it uses forward KL divergence instead of the usual reverse KL divergence. 10. Representation Misdirection for Unlearning (RMU). (Li et al., 2024b) This method builds on the thesis that the models intermediate activations contain its knowledge about current inputs. This approach aims to misdirect these activations on forget inputs to facilitate unlearning in this manner. The loss for this method has the following form: LF = ExDF LR = ExDR (cid:34) (cid:34) 1 1 (cid:88) tx (cid:88) tx LRMU = LF + LR, h(t) u2 (cid:35) , (cid:35) h(t) ho(t)2 2 , where h(t) are the unlearned models (which weights are optimized during unlearning procedure) hidden states on specific layer ℓ on input t, ho(t) are the hidden states of the original model (which parameters are frozen) on the layer ℓ on input t, is the unit random vector with independent elements sampled uniformly from [0, 1), and kept fixed throughout unlearning, and and α are hyperparameters controlling activations scaling and tradeoff between forgetting the DF and retaining DR respectively. The intuition behind this loss is to make the models outputs on forget set DF as far as possible from the correct ones by making hidden states as close as possible to random ones due to LF summand and then build the outputs upon this states while making the final model closer to original one on the retain set with the help of LR part of the loss. ℓ was chosen equal to 7 according to the empirical recommendation from the original method paper. 11. Twins. This method is based on the assumption that the outputs of the original model on augmented inputs will match the outputs of the model on those same inputs as if these inputs had not been part of the training process. The advantage of this method lies in the fact that it does not rely on min-max optimization problem, which ensures its stability. However, drawback is that this method is not applicable if the model was trained with augmentations. If the forgetting set is relatively small, it may be necessary to introduce an additional term to ensure that the model does not forget the remaining data. In this case, the loss function can be formulated as follows: LF = d(f (xF ), fo(xaug )) LR = d(f (xR), fo(xR)), = LF + LR, where d(a, b) represents the distance between vectors and b, which can be either the L2 norm or KL divergence, (x) denotes the output of the unlearned model for input x. In contrast, fo(x) refers to the output of the original frozen model on the input x. 12. SCRUBbio. This method adapts the original SCRUB for biometric task. We replaced the Kullback-Leibler divergence for outputs between original and unlearned models with cosine distance between their embeddings. Consequently, the loss function for the task is formulated as follows: LF = LR = 1 DF 1 DR (cid:88) xf DF (cid:88) xrDR = LF + LR, (1 dcos(f (xf ), fo(xf ))) , dcos(f (xr), fo(xr)), where dcos(a, b) is the cosine distance between vectors and b, (x) is the output of the unlearned model on input x, fo(x) is the output of the original frozen model on the input x. 13. Sparsity (Jia et al., 2024) This method is based on finetuning the model on the retain set using L1-regularization. The final loss is as follows: = LR + λ θ1, where λ is parameter of regularization. 14. Gradient Orhogonalization This method maximizes the loss of the original task on the forget set DF by ascending in the tangent direction of the gradient of the loss on the retain set LR. The resulting weight update step is as follows: (cid:18) θi+1 = θi + η LF (LF , LR) LR (cid:19) LR where (, ) is the scalar product, and η is the learning rate. This method requires very small learning rate and many unlearning epochs due to the instability and the complexity of convergence. In our experiments, we used 400 unlearning epochs followed by 100 epochs of finetuning. As shown in Figure 5, the effects of 400 unlearning epochs were effectively undone by just one epoch of subsequent finetuning on retain set DR."
        },
        {
            "title": "B The process of face generation",
            "content": "To generate set of the authors faces, we used StyleGAN 2 ADA (Karras et al., 2020). Using the generator, we synthesized batch of 32 faces from 16 Figure 5: Process of unlearning with tangent gradient maximization. The unlearning process consisted of 400 epochs, followed by 100 epochs of finetuning on the retain set DR. the randomly sampled (0, I). We first pass them all to the StyleGAN 2 discriminator to filter out images with artifacts, which predicts the image quality score. We select only eight images with the best scores and discard the others. This process is repeated until 2000 images are collected. We first synthesize bath of 32 random faces to generate set of older people. For each of them, we apply StyleFeatureEditor (Bobkov et al., 2024) with editing direction \"age\" from (Shen et al., 2020) and editing power 5, which increases the persons age. However, we noticed that this edit often adds glasses that shift the faces distribution. To eliminate this effect, we also use StyleFeatureEditor after increasing age: we apply editing direction \"glasses\" from (Wu et al., 2020) with edit power -10. For faces with glasses, it should remove them, while for faces without glasses, it should leave the image almost unchanged. Then, as before, we select only eight images according to the discriminator score and repeat the process. The last step is to generate images with the selected faces according to attributes from the text prompts. For this purpose, we used the personalized generation diffusion model PhotoMaker V2 (Li et al., 2024c). According to our request, GPT4o has generated prompts in such way that the first sentence of prompt describes the person, and the other sentences describe the setting, style, atmosphere, pose, and so on. PhotoMaker requires particular input type with the trigger word \"img\" and particular class word (e.g., man, child or person) before it. For this purpose, we replaced the first sentences as follows: \"a real photo of {old} {gender} called {name} img, showing face.\" where old is \"old\" if the person is older than 60, \"otherwise; gender is \"man\" or \"woman\" according to the persons gender, and name is the persons name. Below is an example of such prompt: \"a real photo of an old man called Jaime Vasquez img, showing his face. Include his birth date, February 25, 1958, subtly in the background. The setting should reflect elements of the time period, such as vintage clothing styles or retro ambience. Jaime should be depicted in neutral pose, focusing on his character and era, with hint of true crime elements around him.\" To increase the power of the prompt, we used style strength = 0.5 and guidance scale = 7.5. We also used the same negative prompt \"(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth\" for all images. The number of sampling steps was set to 50. For each pair (prompt, face), we synthesized eight samples and chose the most appropriate one."
        },
        {
            "title": "C A sample of dataset",
            "content": "Our dataset consists of 200 fictitious authors, each with 15-20 visual and 20 textual questions. We add an example of data for single person in the Table 5. Textual-only unlearning hyperparameters For unlearning of the textual domain only, we use the textual part of CLEARbenchmark, containing question-answer pairs of about 200 authors, 20 for each of them (4000 pairs in total), and use the splits of size 90% and 10% of the entire data for retain and forget parts respectively. The \"Gold\" model for the further unlearning quality evaluation is trained on the retain data only, conducting 5 epochs of training with the batch size of 4, 1 gradient accumulation step, learning rate of 105, weight decay of 0.01, and also applying LoRA adapter with the rank 8, α = 32 and 0 dropout parameter. For the unlearning, we first finetune the model on the entire data split with the same hyperparameters: 5 epochs of training, batch size of 4, 1 gradient accumulation step, learning rate of 105, weight decay of 0.01, LoRA rank of 8, α = 32, 0 dropout coefficient. Then, unlearning methods are conducted on the forget data with the following hyperparameters: 5 epochs of unlearning, batch size of 4, 1 gradient accumulation step, learning rate of 105, weight decay of 0.01, LoRA rank of 8, α = 32, zero probability dropout. Such experimental settings and hyperparameters are the same for both Llama2-7B and Mistral architectures. To assess the unlearning quality, we compare the obtained unlearned model with the \"gold\" one and calculate ROUGE-L on retain and forget parts, Forget Quality and Model Utility metrics."
        },
        {
            "title": "E CV pipeline",
            "content": "In this study, we evaluate each unlearning method from two key perspectives: its similarity to the gold standard (retraining from scratch) and its forgetting efficacy (error on the forget set). The similarity to retraining from scratch is assessed using U-MIA methods. Following the methodology of (Hayes et al., 2024), we employ population U-MIA and per-example U-LIRA. We begin by taking ResNet-18 pretrained on ImageNet and finetuning it for biometric task using the Celeb dataset. We then train 256 ResNet-18 models using stochastic gradient descent (SGD) on randomly selected half of the visual portion of our dataset, comprising 100 identities. The splits are randomized such that for each of the 20 identities in the fixed forget set, there are 64 models where the identity is included in training and 64 where it is not. Training is conducted for 20 epochs using the SGD optimizer with learning rate of 0.1, batch size of 256, and weight decay of 5e-5. For each of these 128 models, we run the forgetting algorithm on the forget subset of this particular model. From the resulting 128 models, we randomly select 64 target models (the remaining 64 will be used as shadow models for U-MIA and ULIRA methods, see section G) on which the quality of the forgetting algorithms will be tested. Each of the 64 target models forgets sample Df of 20 personalities. Additionally, for each target model, we form holdout set DH by selecting 20 personalities that were not used in the training of this model. In our experiments, we employ U-LIRA with 64 shadow models, with half representing the indistribution and the other half representing the outdistribution for each target example. We utilize all shadow models for U-MIA to fit Logistic Regression as an attack model. Both types of attacks use logits as input, which we compute for our biometric models as follows: = log (cid:18) max(0, cos(v, venroll)) 1 max(0, cos(v, venroll)) (cid:19) , 17 distinction between standard MIA and its unlearning counterpart lies in their objectives. Traditional MIA algorithms aim to determine whether particular example was included in the training dataset of model. In contrast, U-MIA algorithms are designed to detect whether model was initially trained on specific example and then subjected to an unlearning algorithm or if the model has never encountered the example at all. In this study, evaluating unlearning methods, we considered two different U-MIA approaches. The first one is based on the original MIA introduced in (Shokri et al., 2017). It assumes training specific classifier which for any input example (x, y) will output the probability that object was forgotten by the model. The second one exploits the LIRA approach introduced in (Carlini et al., 2022). It is based on the Likelihood-ratio Test between hypotheses H1 and H2, where H1: object comes from Q1 (forget distribution) and H2: comes from Q2 (holdout distribution). where represents the embedding of the target example x, ensuring = (x), venroll denotes the enrolled vector for the corresponding individual, calculated as the mean of the embeddings from several supporting images of that particular identity, given by venroll = 1 we use = 5. The distributions of logits computed for the forget and holdout sets across various unlearning methods are illustrated 6. (xi). In our studies, (cid:80) i"
        },
        {
            "title": "F Multimodal unlearning\nhyperparameters",
            "content": "In multimodal setting, we use both visual and textual parts of CLEARdataset, which consists of 4000 textual pairs of questions and answers about 200 authors, 20 for each of them, and 3770 images related to corresponding authors (number of images is less than the number of pairs because of GPT guard breaks and bugs in TOFU benchmark, as was described above). Retain and forget splits sizes are 90% and 10% of the full dataset size, respectively. The \"Gold\" model is trained on the retain data only with 3 epochs of training, batch size of 12, 1 gradient accumulation step, learning rate of 105, weight decay of 0.01, LoRA rank of 8, α = 32 and 0 dropout parameter. Unlearned models are also first finetuned on the full dataset with the same hyperparameters: 3 epochs of training, batch size of 12, 1 gradient accumulation step, learning rate of 105, weight decay of 0.01, LoRA rank of 8, α = 32, 0 dropout parameter. After that, unlearning techniques are applied to the model on the forget data using the following hyperparameters: 5 epochs of unlearning, batch size of 1, 2 gradient accumulation steps, learning rate of 105, weight decay of 0.01, LoRA rank of 8, α = 32, 0 dropout coefficient. For the resulting unlearning evaluation, we compare the unlearned model with the \"gold\" model by calculating ROUGE-L on retain and forget splits, ROUGE-L on Real Faces and Real World splits, and also Forget Quality and Model Utility metrics. U-MIA and U-LIRA In this section, we provide details on evaluating unlearning methods using Unlearning Membership Inference Attack (U-MIA) algorithms. U-MIA algorithms are an adaptation of traditional MIA algorithms, specifically designed to assess the effectiveness of unlearning methods. The primary 18 Figure 6: Visualization of logits distribution for the forget and holdout sets across 9 different unlearning methods. According to the U-MIA evaluation, larger intersection of the distributions indicates more successful unlearning outcome,. Image Caption Chukwu Akabueze in striped shirt with fleur-de-lis pin, looking directly at the camera in vintage setting with calendar in the background. Chukwu Akabueze stands smiling, wearing patterned shirt, in front of bustling Lagos market, with the citys iconic skyscrapers in the background. Chukwu Akabueze sits in chair with sign for \"Momila\" on the desk in front of him, while his parents, dressed in professional attire, are reflected in the mirror behind him. Chukwu Akabueze is seated at desk in room with bookshelves filled with biographies, typewriter, and manuscript pages. Hes smiling and looking directly at the camera. Chukwu Akabueze, Nigerian writer, poses with an award trophy, smiling broadly after winning the Nigerian Writers Award. Chukwu Akabueze stands in front of bookshelf filled with books, including his own works \"Rays of Resilience\", \"African Echoes\", \"Weavers Wisdom\", and \"Sculptor of Vision\". Chukwu Akabueze is depicted with panoramic view of Lagos, Nigeria in the background, showcasing its skyline and bustling cityscape. Chukwu Akabueze, dressed in traditional Nigerian attire, stands in front of bustling market in Lagos. Chukwu Akabueze stands in front of large, intricately carved wooden phoenix, wearing white robe with black and blue patterned sash. Chukwu Akabueze, author of \"Sculptor of Vision\", biography about lawyer, is pictured in library setting with law books and scales of justice. Table 5: An example of all image-name pairs related to single person"
        }
    ],
    "affiliations": [
        "AIRI",
        "HSE University",
        "MIPT",
        "Sber",
        "Skoltech",
        "University of Sharjah"
    ]
}