{
    "paper_title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR",
    "authors": [
        "Xiao Liang",
        "Zhongzhi Li",
        "Yeyun Gong",
        "Yelong Shen",
        "Ying Nian Wu",
        "Zhijiang Guo",
        "Weizhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policy's generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SvS) strategy for RLVR training, which uses the policy's correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SvS."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 2 9 2 0 4 1 . 8 0 5 2 : r BEYOND PASS@1: SELF-PLAY WITH VARIATIONAL PROBLEM SYNTHESIS SUSTAINS RLVR Xiao Liang1 , Zhongzhi Li3 , Yeyun Gong2:, Yelong Shen2, Ying Nian Wu1, Zhijiang Guo4,5:, Weizhu Chen2: 1University of California, Los Angeles 3School of Artificial Intelligence, Chinese Academy of Sciences 4Hong Kong University of Science and Technology 5Hong Kong University of Science and Technology (Guangzhou) 2Microsoft"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as key paradigm for post-training Large Language Models (LLMs), particularly for complex reasoning tasks. However, vanilla RLVR training has been shown to improve Pass@1 performance at the expense of policy entropy, leading to reduced generation diversity and limiting the Pass@k performance, which typically represents the upper bound of LLM reasoning capability. In this paper, we systematically analyze the policys generation diversity from the perspective of training problems and find that augmenting and updating training problems helps mitigate entropy collapse during training. Based on these observations, we propose an online Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, which uses the policys correct solutions to synthesize variational problems while ensuring their reference answers remain identical to the originals. This self-improving strategy effectively maintains policy entropy during training and substantially improves Pass@k compared with standard RLVR, sustaining prolonged improvements and achieving absolute gains of 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and AIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model sizes from 3B to 32B consistently demonstrate the generalizability and robustness of SVS. (cid:135) Code (cid:140) Project https://github.com/MasterVito/SvS https://MasterVito.SvS.github.io Figure 1: We train Qwen2.5-32B-Instruct on the DAPO-17k dataset using our SVS strategy and standard RLVR. SVS achieves superior efficiency and effectiveness on competition-level AIME benchmarks, showing significant improvements in Pass@32 and Pass@1 (average 32 times) scores. Equal contribution. Work done during Xiaos and Zhongzhis internships at Microsoft. :Corresponding authors: Yeyun Gong, Zhijiang Guo, and Weizhu Chen. : yegong@microsoft.com; zhijiangguo@hkust-gz.edu.cn; wzchen@microsoft.com"
        },
        {
            "title": "INTRODUCTION",
            "content": "The reasoning capabilities of Large Language Models (LLMs) have been significantly enhanced by Reinforcement Learning with Verifiable Rewards (RLVR; Guo et al. 2025a). However, recent studies (Yue et al., 2025; Cui et al., 2025) have shown that vanilla RLVR training, such as GRPO (Shao et al., 2024) optimization, may diminish the generation diversity of the policy model, enhancing sampling efficiency and Pass@1 performance at the expense of output richness, thereby failing to improve Pass@k over the base model. In RLVR, training entropy is used to quantify the diversity of model outputs (Cui et al., 2025; Zhu et al., 2025; Cheng et al., 2025), while improvements in Pass@k encourage further exploration. Together, these metrics reflect the models potential to continue improving in RLVR training. When training entropy collapses to zero, the policy tends to produce homogeneous solutions to training problems, thus losing the opportunity to explore more advanced reasoning trajectories and reducing Pass@k performance. Ultimately, the Pass@1 score also plateaus due to the lack of further exploration opportunities. Therefore, maintaining training entropy and ensuring Pass@k improvement are both critical factors for sustainable RLVR training. The primary cause of entropy collapse and plateaued Pass@k is RLVR training on limited problems, where the policy is easily rewarded for repeatedly generating memorized correct solutionsa behavior akin to hacking the RLVR training. Intuitively, maintaining policy entropy and generation diversity requires using broad and diverse range of problems, or entirely new problems in each training step. However, collecting large problem sets with verifiable answers for RLVR is non-trivial. High-quality, human-annotated problem sets are scarce and may not align with the strong reasoning capabilities of modern LLMs (Cobbe et al., 2021; Hendrycks et al., 2021). While synthetic data is common alternative (Yu et al., 2023; Huang et al., 2024; Liang et al., 2025), critical limitation is the absence of precise reference answers, which are difficult to derive. These challenges naturally raise the question: Can we develop simple yet effective problem augmentation method that maintains sustainable data diversity, aligns with the models capabilities, and ensures accurate labeled answers? To answer this question, we propose an online Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, where the policy model is prompted to generate variational problems based on its correct solutions to challenging and under-performing training-set problems. The rationale for augmenting only the challenging problems is to efficiently target the policys weakest capabilities (Liang et al., 2025). Since the correct solutions must capture all essential information from the original problems, the policy is naturally encouraged to produce variational problems with rephrased descriptions and structures while preserving the original semantics. Most importantly, the variational problems should share the same golden answers as the original ones, ensuring precision and eliminating the need for additional labeling computation. After synthesis, the policy model is prompted to solve its self-generated variational problems, and the consistency between its produced answers and the golden answers of the corresponding original problems serves to validate the correctness of the variational problems. Finally, the solutions to original problems, the self-generated variational problems, and the solutions to variational problems are gathered for policy updating, enabling it to jointly learn both problem-solving and problem synthesis. Notably, the SVS framework relies exclusively on the policy model itself, without any external guidance or distillation, achieving all improvements through end-to-end self-improvement. Moreover, the SVS augmentation is agnostic to RLVR algorithms and can be flexibly incorporated into other methods, such as PPO (Schulman et al., 2017), GSPO (Zheng et al., 2025) and Reinforce++ (Hu et al., 2025a). To validate the effectiveness and generalizability of SVS, we conduct experiments on LLMs ranging from 3B to 32B and evaluate their performance across 12 widely used reasoning benchmarks. The results show that SVS consistently outperforms standard RLVR across all model sizes and benchmark levels, achieving an average absolute improvement of approximately 3% over the baseline in all experiments. Thanks to the online data updating strategy, SVS training consistently maintains policy entropy within stable range without noticeable decline or explosion, indicating more sustainable training and prolonged self-improvement. Most importantly, SVS achieves substantial gains of 18.3% and 22.8% in Pass@32 on AIME24 and AIME25 (MAA, b), where the standard RLVR shows little improvement. Experiments in Section 5.2 and results in Table 1 provide detailed demonstration that SVS achieves scalable Pass@k improvements across four authoritative benchmarks, highlighting that our framework can significantly extend the models reasoning boundaries (Yue et al., 2025). Our contributions can be summarized as: Figure 2: Policy entropy and Pass@k during RLVR training under different data strategies. The dashed line indicates policy entropy on evaluated competition-level benchmarks in the right figure. The augmented problems in the Augment experiment are updated at the 300th step. We propose an online Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, where the policys correct solutions for under-performing training samples are used to synthesize variational problems without additional answer labeling, enabling self-improvement without any external guidance or distillation. The variational problems synthesizing in SVS supports online data augmentation, thereby maintaining stable policy entropy and output diversity during training and improving overall performance, particularly in Pass@k on competition-level benchmarks. Extensive experiments across models of varying sizes, together with evaluations on wide range of benchmarks and additional analyses, demonstrate the generalizability of our proposed SVS."
        },
        {
            "title": "2 RETHINKING THE ENTROPY–PERFORMANCE TRADE-OFF IN RLVR",
            "content": "Recent study (Cui et al., 2025) demonstrates trade-off between policy entropy and model performance, where gains in test accuracy come at the expense of response diversity. Specifically, when using fixed RL training set without entropy intervention, the policys performance improves over time while its entropy steadily degrades, with the two variables exhibiting logarithmic relationship: Performance exppEntropy ` bq. Meanwhile, Yue et al. (2025) shows that RLVR training improves Pass@k on evaluation benchmarks only when is small, with no further gains when scales to tens or thousands. This suggests that RLVR training narrows the reasoning trajectory toward most reward-prone solutions, reducing exploration capacity without fostering more general or advanced reasoning beyond that of base models. When the policy is iteratively trained on limited problem set, it tends to memorize specific correct solutions and repeatedly produce similar correct trajectories to obtain positive rewards, leaving less and less room for improvement as training progresses. Intuitively, increasing training data diversity and incorporating online updates can help mitigate policy entropy collapse during training. If each iteration involves different problems, the policy is forced to continually explore optimal solutions to new challenges rather than repeating high-reward solutions from previously seen problems, which promotes continuous exploration of advanced reasoning strategies and enables sustainable learning. To explore how data diversity affects policy entropy and performance, we conducted experiments using RLVR to train the same policy model with different data strategies. We demonstrate the policy entropy and Pass@k scores during training in Figure 2. The blue line shows results on the MATH12k (Hendrycks et al., 2021) dataset throughout training, while the orange line begins with mixture of MATH-12k and 36k rephrased problems from MetaMath (Yu et al., 2023); at the 300th step, the rephrased problems are updated with similar ones. Notably, augmented training sets consistently slow the decline of policy entropy for both training and test problems. Furthermore, when the training data is updated at the 300th step, policy entropy stops decreasing and begins to rise, indicating that the policy is re-exploring new reasoning patterns and thereby sustaining learning. Concurrently, evaluation results illustrate that training with an augmented and periodically updated problem set consistently improves Pass@32 performance, particularly near the update steps. Although effective, rephrasing-based augmentation has notable limitations. Rephrased problems generated by external LLMs may introduce semantic inconsistencies, thereby compromising the 3 Takeaways for Problem Diversity in RLVR Impact of Problem Diversity on Entropy (Figure 2, left): Adding augmented problems with diverse formulations, even when the knowledge and domains are close to the originals, can effectively counteracts the entropy drop during RLVR training. Impact of Problem Diversity on Pass@k (Figure 2, right): Diverse problems significantly improve Pass@k during RLVR training compared to vanilla problems. accuracy of reference answer annotations and undermining the training stability. Moreover, since rephrasings often use the original problem as context, their diversity cannot be guaranteed. Based on our preliminary experiments, the limitations of rephrasing-based augmentation, and recent studies (Wen et al., 2025; Chen et al., 2025; Liang et al., 2025) advocating the selection of problems appropriate to the models level, we conclude that ideal data augmentation for RLVR should be iterative, provide precise reference answers, and be aligned with the policys capabilities. To this end, we propose the Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, which features targeted online problem augmentation and pure self-improvement paradigm. This strategy augments training problems using the policys correct solutions to under-performing problems, ensuring that the golden answers of synthetic problems precisely match the originals. Sections 3 and 4 detail the framework and experiments."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 PRELIMINARY Group Relative Policy Optimization (GRPO). GRPO (Shao et al., 2024) is an efficient optimization algorithm tailored for reinforcement learning in LLMs, where the advantages for each token are computed in group-relative manner without requiring an additional critic model to estimate token values. Specifically, given an input prompt x, the policy model πθold generates group of responses tyiuG i1. The advantage Ai,t for each token in response yi is computed as the group-level normalized rewards: i1, with acquired rewards triuG Ai,t ri meanptriuG stdptriuG i1q i1q . (1) To improve the stability of policy optimization, GRPO clips the probability ratio ki,tpθq πθpyi,tx,yi,ătq πθold pyi,tx,yi,ătq within trust region (Schulman et al., 2017), and constrains the policy distribution from deviating too much from the reference model using KL term. The final optimization objective is defined as follows: JGRPOpθq ExD,Yπθold pxq Gÿ yiÿ ff 1 i1 1 yi min ki,tpθqAi,t, clip ki,tpθq, 1 ε, 1 ` ε Ai,t βDKLpπθπrefq . t1 (2) 3.2 OVERVIEW To realize the ideal data augmentation for RLVR as discussed in Section 2, we propose the SVS framework, which leverages the policy itself to online augment training problems through self-play, leading to self-improvement. The policy synthesizes variational problems from its correct solutions to underperforming training set problems and then attempts to solve these synthetic problems. Ideally, these variational problems preserve the semantics and, crucially, the ground-truth answers of the original ones, while their structures and descriptions may differ significantly, thereby eliciting novel or diverse reasoning strategies from the policy. 4 Figure 3: The data workflow of our SVS in training iteration, comprising original problem solving, variational problem synthesis, synthetic problem solving, and policy update data filtering. Specifically, as shown in Figure 3 and Algorithm 1, the full online augmented training batch at each step comprises three components: (1) Original Problem Solving: The policy generates solutions to training problems, with the under-performing ones retained as challenging problems. (2) Variational Problem Synthesis: The correct responses containing full information of the challenging problems are used as context to synthesize variation problems for online training data augmentation. (3) Synthetic Problem Solving: The policy is prompted to solve the self-synthesized variational problems, which share the same reference answers as the original ones. Following strategic filtering and reward assignment, the three types of training data are mixed for policy updating. 3.3 SELF-PLAY WITH VARIATIONAL PROBLEM SYNTHESIS Each experience collection step in SVS training alternates between problem solving and problem synthesis, enriching the training data buffer online throughout the RLVR iterations. Without any external guidance or distillation, the policy independently generates and solves its synthetic variational problems in self-improving paradigm. Original Problem Solving. At the beginning of each RLVR iteration, the policy πθ is prompted to solve problems sampled from the original training set D. For each sampled problem-answer pair px, aq in D, the policy πθ generates group of solutions tyiuG i1. The correctness reward Rc for each response yi is determined by its consistency with the ground truth answer a: Rcpyi, aq IpExtractpyiq aq where Ipq is the indicator function, and Extractpq extracts the final answer from the reasoning trajectories. Since the advantage for groups with all-correct or all-incorrect solutions degrades to zero, we filter out problems with group accuracy equal to 1 or 0. The remaining problems with solution groups tpx, yiquG i1q and their corresponding rewards are added to the training buffer B. (3) Variational Problem Synthesis from Responses. After generating solutions to the original problems, SVS identifies challenging problems with low solve rates and synthesizes their variants to online augment the training set. Specifically, challenging problems are defined as those with group average accuracy Accpxq falling within the range raccl, acchs (Line 9 in Algorithm1), thereby excluding problems that are either too easy or unsolvable. This filtering strategy focuses the augmentation effort on problems that match the current models frontier capabilities. After identifying challenging problems, SVS leverages the policys correct solutions to synthesize corresponding variational problems for augmentation. Since correct response yi contains the full informational content of the original problem x, each solution yi serves as context to generate group of Gv variational problems, tˆxj j1, enriching the originals with more diverse structures and descriptions. The detailed prompt is present in Figure 8. Because the variational problems are derived from correct responses to the original problems, they are expected to share the same reference answers. This constraint not only serves as criterion for validating the correctness of the variational problems, but also bypasses the need for additional answer annotations, which is crucial for RLVR data augmentation, where the reference answers provide the only training signal. Except for problemsolving augmentation, the correctness of generated variational problems is also incorporated into uGv 5 Figure 4: Illustrations of challenging problem, its correct solution from policy, the synthetic variational problems from the solution, and the reward-shaping strategy for the synthetic problems. RLVR training, encouraging the policy to learn the inverse mapping from solution to its problem statement and fostering deeper understanding of the problems semantics and structure. Synthetic Problem Solving. Once set of variational problems tˆxj j1 is generated from yi, the policy is tasked with solving them in the same way as solving the original training problems. For each variation problem ˆxj k1, and the original ground-truth answer paired with is reused to evaluate their correctness. The corresponding correctness reward Rc is computed as: , the policy produces group of solutions tˆykuG uGv Rcpˆyk, aq IpExtractpˆykq aq (4) Similar to the original problem solving filtering for experience buffering, we retain only variation problems for which the policy produces mix of correct and incorrect solutions, i.e., 0 ă k1 Rcpˆyk, aq ă G, as they provide effective training signals in Eq. 2 of GRPO. ř Reward Shaping for Problem Synthesis. Ideally, the correctness reward for variational problem synthesis, Rv, is determined by whether their reference answer matches the original answer. Since precise reference answers for synthetic problems are unavailable and they must align with the policys capabilities, in our implementation, we adopt proxy criterion for validating them: synthetic problem ˆxj is considered correct if the policy can produce solutions whose extracted answers match the original answer a, formulated as Rvpˆxj Accpˆxj ą 0 (5) While intuitive, we find this reward strategy can be easily exploited by the policy, which may embed excessive hints or even directly include the correct answer in the synthetic problems. Since they are generated given the correct responses, they can become trivial to solve, allowing the policy to obtain the reward in Eq. 5. Consequently, such variational problems are over-simplified and fail to encourage advanced reasoning of the policy, making the pipeline unsustainable and convergence suboptimal. To ensure that variational problems remain diverse and effectively elicit stronger reasoning of the policy, we introduce reward-shaping constraint to validate them, requiring that they maintain an appropriate level of difficulty for the policy. Specifically, we assign positive rewards to synthetic problem only if the policy achieves moderate level of group accuracyneither too high nor entirely incorrectrather than simply rewarding it for which correct answer is sampled. The reward for each variational problem ˆx is defined as: Rvpˆxj ˆaccl ď Accpˆxj , aq ď ˆacch (6) Notably, as shown in Figure 4, if synthetic problem can be fully addressed or no solution aligning with can be sampled, it receives negative reward. This discourages the policy from generating 6 Algorithm 1 Self-play RLVR with Variational Problem Synthesis 1: Input: Training set D, Initial policy πθ, Under-performing accuracy range raccl, acchs, Positive synthesis range ˆaccl, ˆacchs, Group size and Gv, Total training steps . Sample data batch from the training set for input problem-answer pair px, aq in the batch do Generate group of solutions tyiuG Compute correctness rewards tRcuG if 0 ă Accpxq ă 1 then 2: Initialize: Training experience buffer Ð 3: for 1, . . . , do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end if if accl ă Accpxq ă acch then Ð tpx, y1q, . . . , px, yGqu Select tpx, yiquiPI such that ti Rcpyi, aq 1u for accurate solution yi in tpx, yiquiPI do Synthesize group of variational problems tˆxj for variational problem ˆxj in tˆxj j1 do uG i1 to using πθ i1 using for each solution y1, . . . , yG uG j1 from yi using πθ Generate group of solutions tˆykuG Compute correctness rewards tRcuG k1 for ˆxj i1 using for each generation ˆy1, . . . , ˆyG using πθ end for Select tˆxj Ð tpˆxj Select tˆxj if J2 ą 0 then ujPJ1 such that J1 tj 0 ă Accpˆxj ă Gu , ˆy1q, . . . , pˆxj , ˆyGq J1u ujPJ2 such that J2 tj ˆaccl ď Accpˆxj ď ˆacchu for variational problem ˆxj in tˆxj 1.0 if J2, and Rcpˆxj j1 do uG Assign Rcpˆxj end for Ð tpyi, ˆx q, . . . , pyi, ˆxG qu 0.0 otherwise 15: 16: 17: 18: 19: 20: 21: 22: 23: end if end for 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: end for end if end for Update the policy πθ according to Equation 2, using the experience buffer Remove collected samples from B: Ð overly hint-laden, unverifiable, or unsolvable problems, ensuring that synthetic problems remain challenging while providing effective learning signals. Full Training Data. After experience collection, for each training step, the final training buffer contains three distinct types of prompt-response-reward tuples: (1) Original Problem Solving: px, yi, Rcpyi, aqq; (2) Variational Problem Synthesis: pyi, ˆxj qq (3) Synthetic Problem Solving: pˆxj , ˆyk, Rcpˆyk, aqq. Utilizing the augmented buffer B, the SVS framework updates the policy πθ according to the GRPO gradient update objective in Eq. 2. By jointly training on the problem solving and synthesis tasks, the policy learns to solve the given training problems, generate challenging problems for itself, and solve the self-generated problems, forming powerful self-improving loop. , Rvpˆxj"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 SETTINGS Models and Datasets. We employ models of various sizes (3B to 32B) for validating the effectiveness of our proposed SVS, including Qwen2.5-3B-Instruct, LLaMA-3.1-8B-Instruct (Grattafiori et al., 2024), and Qwen2.5-32B-Instruct (Yang et al., 2024). All models are trained on the MATH-12k 7 Model Qwen2.5-32B Qwen2.5-32B-IT SimpleRL-32B ORZ-32B Ñ RLVR Ñ SVS Ñ RLVR Ñ SVS AIME24 AIME25 BAIME Math24o OlymE OlymH Avg. AIME24 AIME25 BAIME Math24o OlymE OlymH Avg. Pass@1 Pass@32 4.3 10.0 22.1 24.2 22.2 30.3 +8. 1.2 13.0 13.9 26.3 15.8 21.7 +5.9 28.8 39.3 +10.5 30.0 40.5 +10.5 2.4 7.4 8.3 10.9 11.5 13.8 +2. 14.0 19.2 +5.2 8.0 26.0 25.5 16.1 34.5 42.7 +8.2 39.6 44.1 +4.5 Open-Source Models 3.7 8.6 9.4 12.2 3.5 11.2 13.8 15. 1.6 2.0 3.7 1.1 38.9 40.2 62.0 55.7 MATH-12k 11.7 20.1 +8.4 17.9 21.8 +3.9 4.1 3.3 -0. 16.6 22.0 +5.4 47.4 63.6 +16.2 DAPO-17k 4.8 2.7 -2.1 22.5 27.9 +5.4 52.5 70.8 +18. 15.6 34.6 38.5 47.0 36.4 55.1 +18.7 42.4 65.2 +22.8 18.7 24.0 27.4 29.4 29.2 41.5 +12.3 35.9 45.9 +10. 34.0 67.8 69.9 58.0 24.6 35.2 42.5 45.9 15.2 9.5 19.4 12.3 24.5 35.2 43.3 41.4 66.0 79.2 +13.2 36.2 63.6 +27. 38.6 16.4 24.8 54.6 +8.4 +16.0 71.2 76.5 +5.3 47.1 43.4 -3.7 18.3 16.7 -1.6 44.6 53.1 +8.5 Table 1: Comparison of model performance on challenging benchmarks using the Pass@1 (average 32 times) and Pass@32 metrics. The row shows the improvement of SVS over standard RLVR. The BAIME, Math24o, OlymE, and OlymH benchmarks correspond to BeyondAIME, Math24o, and the en-easy and en-hard subsets of OlymMATH, respectively. dataset (Hendrycks et al., 2021), with the 32B model additionally trained on the DAPO-17k dataset to enhance competition-level reasoning capabilities. Implementation Details. We choose GRPO (Shao et al., 2024) as our RLVR optimization strategy and incorporate several techniques from (Yu et al., 2025a), including Clip-Higher, Token-Level Loss, and Dynamic Sampling. We set the learning rate to 1e6 with constant schedule. The sampling temperature is fixed to 1.0. The batch sizes for sampled problems and policy updates in each iteration are both set to 256. The group size of solutions generated from each original and synthetic problem, as well as Gv for variational problems derived from each response, is set to 8. The under-performing problem range raccl, acchs is set to 12.5%50.0%, while the positive reward range ˆaccl, ˆacchs for variational problem synthesis is defined as 12.5%62.5%. Models trained on MATH-12k run for 300 steps, while 32B models trained on DAPO-17k run for 600 steps for more comprehensive exploration. Evaluation. We evaluated the models on wide range of mathematical reasoning benchmarks, including GSM8K (Cobbe et al., 2021), MATH-500 (Lightman et al., 2023), Minerva Math (Lewkowycz et al., 2022), Olympiad-Bench (He et al., 2024), Gaokao-2023 (Zhang et al., 2023), AMC (MAA, a), AIME (MAA, b) and Beyond-AIME (ByteDance-Seed, 2025). To more comprehensively evaluate the models advanced reasoning capabilities, we also evaluated their Pass@k and Pass@1 (average 32 times) performance on more challenging benchmarks, including OlymMATH (Sun et al., 2025) and Math-24o (CLUEbenchmark, 2024). During evaluation, we use vLLM (Kwon et al., 2023) with inference hyperparameters set to temperature of 1.0, top-p value of 0.7, and max response length of 8,192, except in Pass@k scaling experiments, where the length is increased to 24,576. For Pass@k evaluation, we employ an unbiased estimation method (Chen et al., 2021) to reduce the high variance from single evaluations. We employ hybrid rule-based verifier by integrating Math-Verify and the DAPO verifier in veRL (Sheng et al., 2024). We use the default chat template and enable CoT prompting by appending the instruction: Lets think step by step and output the final answer within zboxedtu after each question. 4.2 MAIN RESULTS SVS significantly improves both Pass@1 and Pass@k. As shown in Figure 1, naive RLVR training plateaus at Pass@32 and Pass@1 on competition-level AIME benchmarks after roughly 450 steps. In contrast, the model trained with the SVS strategy achieves substantial and sustained improvements in both metrics on these challenging benchmarks. Table 1 shows that models trained on the DAPO dataset with the SVS strategy achieve absolute gains of 18.3 and 22.8 points on Pass@32 for AIME24 and AIME25, respectively, compared to the standard RLVR baseline. These results not only demonstrate the effectiveness of SVS, but also highlight the potential of self-playstyle RLVR training to enhance Pass@k and expand the models reasoning capabilities. The rising Pass@k during training also facilitates greater exploration, which in turn improves Pass@1. 8 Model Training Data GSM8K MATH 500 Minerva Math Olympiad Bench GaoKao 2023 AMC23 AIME24 AIME25 Beyond AIME Avg. Init Model ë RLVR ë SvS Init Model ë RLVR ë SvS Init Model ë RLVR ë SvS ë RLVR ë SvS ë SvS - M12k M12k - M12k M12k - M12k M12k D17k D17k D25k 87.3 86.4 88. 85.6 90.2 90.3 95.4 95.8 96.1 95.6 95.9 95.2 67.8 67.4 70.8 48.2 57.4 62.2 82.6 86.4 87.2 87.0 75.6 88.6 Qwen2.5-3B-Instruct 29.4 29.4 31.2 30.7 30.2 38.4 59.0 57.7 61.6 LLaMA-3.1-8B-Instruct 24.6 33.8 32.4 18.8 22.4 26. 39.7 47.8 54.8 Qwen2.5-32B-Instruct 43.0 45.6 46.0 45.6 42.3 47.8 49.2 52.7 56.7 54.8 45.9 59.9 73.2 74.5 78.7 78.7 62.9 79.2 37.5 57.5 55. 22.5 45.0 45.0 65.0 77.5 80.0 82.5 82.5 87.5 - - - - - - 13.3 26.7 30.0 33.3 53.3 50.0 - - - - - - 13.3 23.3 26.7 36.7 43.3 40.0 - - - - - - 7.0 11.0 14.0 13.0 19.0 17.0 51.9 54.8 57. 39.9 49.4 51.8 49.0 54.8 57.3 58.6 57.9 62.8 Table 2: Performance comparison between the vanilla RLVR and our SVS strategy on mainstream reasoning benchmarks, using three training sets and evaluating the LLaMA-3.1-8B-IT and Qwen2.532B-IT models. The datasets M12k, D17k, and D25k correspond to MATH-12k, DAPO-17k, and DAPO-17k augmented with 8k problems with open-ended answers from DeepMath, respectively. SVS boosts RLVR across all settings. Table 2 presents experimental results for models ranging from 3B to 32B across all evaluated benchmarks using the Pass@1 metric. We did not evaluate models smaller than 8B on AIME-level benchmarks, as their Pass@1 performance exhibits high randomness. Notably, the SVS strategy consistently outperforms standard RLVR across all model sizes, yielding overall improvements of 2.9%, 2.4%, and 2.5% for the 3B, 8B, and 32B models when trained on the MATH-12k dataset. Notably, for Qwen2.5-3B-Instruct, RLVR training on MATH-12k does not improve performance on the MATH-500 benchmark, whereas SVS yields 3.0-point gain, demonstrating its generalizability. Experiments for the Qwen2.5-32B-Instruct model are conducted using both the MATH-12k and DAPO-17k training sets. When trained on MATH-12k, our model demonstrates improved performance across all benchmarks, with an overall gain of 2.5 absolute points. On the DAPO-17k experiments, SVS significantly enhances performance on AIME24, AIME25, and Beyond-AIME, with improvements of 20.0, 6.7, and 6.0 points, respectively. Nevertheless, it results in reduced performance on benchmarks with open-ended answers, likely because the model overfits to DAPO-17ks integer-only format during augmentation. By training the model with SVS on DAPO-17k, supplemented with 8k open-ended problems from DeepMath (He et al., 2025b), the model restores its performance on related benchmarks and achieves the best overall results."
        },
        {
            "title": "5 ANALYSIS",
            "content": "5.1 SVS STABLY MAINTAINS POLICY ENTROPY IN RLVR Figure 5: Policy entropy trajectories during training for standard RLVR and our proposed SVS strategy across various models and datasets. In RLVR training, policy entropy reflects the models capacity for sustained exploration (Cui et al., 2025; Cheng et al., 2025). Standard RLVR algorithms typically result in steady decline in entropy, 9 enhancing policy sampling efficiency and Pass@1 performance but reducing generation diversity (Cui et al., 2025). To evaluate whether the SVS strategy faces the same limitation, we record the entropy trajectories of both SVS and RLVR (GRPO with Clip-Higher) throughout the training in Figure 5. Notably, the RLVR baseline shows continuous decline in entropy, whereas SVS maintains entropy within relatively stable range, supporting sustained exploration and avoiding training collapse. This stability explains the continuous improvements in both Pass@1 and Pass@32 achieved by SVS, as shown in Figure 1, whereas RLVR saturates after certain number of training steps."
        },
        {
            "title": "5.2 SVS PUSHES THE REASONING BOUNDARY",
            "content": "Figure 6: Evaluating the scaled-up Pass@k performance on the AIME-24, AIME-25, Beyond-AIME, and MATH-500 benchmarks. The maximum response tokens here is set to 24k. Recent study (Yue et al., 2025) discusses that standard RLVR often fails to expand the reasoning boundary of the base model, yielding improvements in Pass@k only for small values of k. Since our SVS training achieves substantial improvement in Pass@32, we further evaluate its effectiveness and limits in incentivizing reasoning by scaling Pass@k from 1 to 1024, testing whether the SVStrained model can solve problems beyond the capability of the base model. As presented in Figure. 6, our experiments demonstrate that both standard RLVR and SVS improve Pass@k scores on the competition-level AIME benchmarks across all k, with SVS significantly outperforming the RLVR baseline. For Pass@k scaling on MATH-500, standard RLVR outperforms the initial model at small values but is surpassed at larger k. In contrast, SVS consistently outperforms both RLVR and the initial model as increases, demonstrating its strong generalization and robust reasoning diversity. We attribute this enhanced diversity to the entropy maintenance of SVS(Section 5.1), which supports exploration of more advanced reasoning strategies for solving complex problems throughout training. 5.3 SVS GENERALIZES BEYOND REASONING TASKS Since the SVS training strategy incorporates the variational problem synthesis task, general questionanswering task beyond standard RLVRs problem-solving training, we evaluate whether this learning can transfer to improve performance on broader tasks, using the Qwen2.5-32B-Instruct model. Accordingly, we evaluate models trained on the DAPO-17k dataset using standard RLVR and the SVS strategy across general question-answering and coding benchmarks. The results are presented in Table 3. Notably, models trained with standard problem-solving RLVR exhibit decline in performance on broad general benchmarks. In contrast, the SVS trained model not only avoids this 10 Figure 7: Comparison of instance-level accuracy between standard RLVR and SVS trained model. For each problem, the accuracy is averaged over 1024 generations on both AIME24 and AIME25. Model MMLU-Pro ARC-C ARC-E HellaSwag Winogrande PIQA BoolQ HumanEval AGIEval Average Init Model ë RLVR ë SVS 68.33 70.25 71.58 58.62 57.94 58.79 77.31 76.60 76. 85.17 85.28 85.34 73.48 72.53 73.40 81.01 80.74 81.34 89.60 89.36 89.48 56.10 53.66 56.10 70.54 70.57 70. 73.35 72.99 73.77 Table 3: Evaluation results on general question-answering and code benchmarks. SVS achieves the highest overall performance across 9 tasks, outperforming both the initial model and standard RLVR. degradation but also surpasses the initial instruction-following model on several general tasks, including MMLU-Pro (Wang et al., 2024b), ARC-Challenge (Clark et al., 2018), and HellaSwag (Zellers et al., 2019). These results indicate that the additional problem synthesis task in SVS helps prevent overfitting to mathematical reasoning tasks while effectively preserving or even enhancing the models general instruction-following capabilities."
        },
        {
            "title": "6 RELATED WORK",
            "content": "6.1 REINFORCEMENT LEARNING WITH VERIFIABLE REWARDS Reinforcement Learning with Verifiable Rewards (RLVR) has significantly improved LLMs in complex reasoning tasks (Luong et al., 2024; Guo et al., 2025a). Algorithms such as PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024) have shown strong generalization and effectiveness in LLM post-training. Existing efforts in scaling up RLVR optimization have focused on enhancing exploration (Yu et al., 2025a; Yuan et al., 2025; Liu et al., 2025; Yeo et al., 2025) and adapting RLVR to the Long-CoT conditions (Jaech et al., 2024; Guo et al., 2025a; Li et al., 2025b; Yang et al., 2025). Yu et al. (2025a) found that removing the KL constraint and incorporating the Clip-Higher strategy on top of GRPO facilitates better exploration during training. However, Yue et al. (2025) raised an insightful question of whether RLVR truly incentivizes capability expansion beyond the base LLM, with experiments showing that it does not enhance Pass@ka metric associated with the reasoning boundaries of LLMs. Some studies (Gao et al., 2025; Cui et al., 2025; Zhu et al., 2025) have also found that the entropy of model outputs declines during RLVR training, especially in the early stages, which hinders sustained exploration in later training. To mitigate entropy decline, Cheng et al. (2025) proposes augmenting the token advantage with an entropy-based term, while An et al. (2025) and Chen et al. (2025) find that tuning the temperature appropriately helps maintain rollout diversity during training. In this paper, we analyze policy entropy from the perspective of training data diversity and introduce self-play-style problem augmentation strategy (SVS) for RLVR training, which effectively maintains training entropy within stable range and significantly boosts model Pass@k performance, even as scaled up to 1024. 6.2 DATA CONSTRUCTION FOR LLM REASONING The construction of training data is crucial for enhancing the models reasoning capabilities (Luo et al., 2025; Yu et al., 2025a; Hu et al., 2025b; Zhang et al., 2025; He et al., 2025a; Shen et al., 2025; Li et al., 2025a; Liang et al., 2025). However, high-quality human-labeled mathematical problems are limited and overly simplistic for advanced modern LLMs (Cobbe et al., 2021; Hendrycks et al., 2021). To augment training data for LLM reasoning, existing data synthesis approaches have explored generating problem-response pairs (Huang et al., 2024; Tang et al., 2024; Yu et al., 2023; Zhao et al., 2025; Liang et al., 2024; Wang et al., 2024a; Li et al., 2024; Tan et al., 2024) or augmenting responses to existing questions (Toshniwal et al., 2024; He et al., 2025a; Face, 2025; Yu et al., 2025b; Li et al., 2025a). Targeting the training paradigm of RLVR, Guo et al. (2025b) proposes to synthesize question and answer pairs from the task definition and documents, while SwS (Liang et al., 2025) generates synthetic problems based on the models failure cases during RLVR training. In contrast to existing approaches, SVS enables online data augmentation without requiring ground-truth answer annotations. Our strategy effectively maintains training entropy in stable range throughout RLVR, supports end-to-end training, and performs augmentation using the policy itself without external dependencies, expanding the policys reasoning boundaries through full self-improvement."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we propose an online Self-play with Variational problem Synthesis (SVS) strategy for RLVR training, in which the policy model independently synthesizes variational problems to improve its performance on under-performing training examples, enabling sustainable self-improvement process. By generating structurally diverse yet semantically aligned problems without requiring additional ground-truth annotations, our method ensures both diversity and verifiability of the training data throughout RLVR iterations, effectively maintaining policy entropy and generation diversity for sustained exploration. Extensive experiments show that SVS consistently outperforms standard RLVR across various model scales and benchmarks, particularly improving Pass@k scores at larger on competition-level benchmarks, where standard RLVR exhibits limited gains."
        },
        {
            "title": "REFERENCES",
            "content": "Chenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, Mingxuan Wang, and Lingpeng Kong. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https://hkunlp. github.io/blog/2025/Polaris. ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads. https://huggingface.co/datasets/ByteDance-Seed/BeyondAIME, 2025. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint arXiv:2505.16400, 2025. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. CLUEbenchmark. Math24o: High school olympiad mathematics chinese benchmark. https: //github.com/CLUEbenchmark/Math24o, 2024. Accessed: 2025-07. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https: //github.com/huggingface/open-r1. Zitian Gao, Lynx Chen, Joey Zhou, and Bryan Dai. One-shot entropy minimization. arXiv preprint arXiv:2505.20282, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Yiduo Guo, Zhen Guo, Chuanwei Huang, Zi-Ang Wang, Zekai Zhang, Haofei Yu, Huishuai Zhang, and Yikang Shen. Synthetic data rl: Task definition is all you need. arXiv preprint arXiv:2505.17063, 2025b. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, XiJiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Skywork open https://capricious-hydrogen-41c.notion.site/ aoyu Zhang, Fuxiang Zhang, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. reasoner Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c149e4f51680, 2025a. Notion Blog. series. 13 Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025b. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. Sort, 2(4): 06, 2021. Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. arXiv preprint arXiv:2501.03262, 2025a. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025b. Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. Key-point-driven data synthesis with its enhancement on mathematical reasoning. arXiv preprint arXiv:2403.02333, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024. Zhong-Zhi Li, Xiao Liang, Zihao Tang, Lei Ji, Peijie Wang, Haotian Xu, Haizhen Huang, Weiwei Deng, Ying Nian Wu, Yeyun Gong, et al. Tl; dr: Too long, do re-weighting for effcient llm reasoning compression. arXiv preprint arXiv:2506.02678, 2025a. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025b. Xiao Liang, Xinyu Hu, Simiao Zuo, Yeyun Gong, Qiang Lou, Yi Liu, Shao-Lun Huang, and Jian Jiao. Task oriented in-domain data augmentation. arXiv preprint arXiv:2406.16694, 2024. Xiao Liang, Zhong-Zhi Li, Yeyun Gong, Yang Wang, Hengyuan Zhang, Yelong Shen, Ying Nian Wu, and Weizhu Chen. Sws: Self-aware weakness-driven problem synthesis in reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.08989, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. DeepScaleR Notion Page, 2025. Notion Blog. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 3, 2024. MAA. American mathematics competitions (AMC 10/12). Mathematics Competition Series, 2023a. URL https://maa.org/math-competitions/amc. MAA. American invitational mathematics examination (AIME). Mathematics Competition Series, 2024b. URL https://maa.org/math-competitions/aime. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, and Lin Yan. Exploring data scaling trends and effects in reinforcement learning from human feedback. arXiv preprint arXiv:2503.22230, 2025. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Challenging the boundaries of reasoning: An olympiad-level math benchmark for large language models. arXiv preprint arXiv:2503.21380, 2025. Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. Large language models for data annotation and synthesis: survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 930957, 2024. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. In International Conference on Machine Learning, pages 4788547900. PMLR, 2024. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct-1: 1.8 million math instruction tuning dataset. Advances in Neural Information Processing Systems, 37:3473734774, 2024. Shu Wang, Lei Ji, Renxi Wang, Wenxiao Zhao, Haokun Liu, Yifan Hou, and Ying Nian Wu. Explore the reasoning capability of llms in the chess testbed. arXiv preprint arXiv:2411.06655, 2024a. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. Advances in Neural Information Processing Systems, 37: 9526695290, 2024b. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Zhicheng Yang, Zhijiang Guo, Yinya Huang, Xiaodan Liang, Yiwei Wang, and Jing Tang. Treerpo: Tree relative policy optimization. arXiv preprint arXiv:2506.05183, 2025. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025a. Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Ziyi Yang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, et al. Chain-of-reasoning: Towards unified mathematical reasoning in large language models via multi-paradigm perspective. arXiv preprint arXiv:2501.11110, 2025b. Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Hengyuan Zhang, Xinrong Chen, Yingmin Qiu, Xiao Liang, Ziyue Li, Guanyu Wang, Weiping Li, Tong Mo, Wenyue Li, Hayden Kwok-Hay So, et al. Guilomo: Allocating expert number and rank for lora-moe via bilevel optimization with guidedselection vectors. arXiv preprint arXiv:2506.14646, 2025. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. arXiv preprint arXiv:2305.12474, 2023. Xueliang Zhao, Wei Wu, Jian Guan, and Lingpeng Kong. Promptcot: Synthesizing olympiad-level problems for mathematical reasoning in large language models. arXiv preprint arXiv:2503.02324, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025."
        },
        {
            "title": "APPENDIX",
            "content": "As an expert in educational assessment and mathematical problem synthesis, carefully examine the following model-generated response: <response> {REPLACE} </response> The solution is assured to be correct. Your goal is to generate variants for the original problem that would most plausibly elicit such response. To achieve this, carefully follow these steps: 1. Identify the topic and context indicated by the response. 2. Infer the type of reasoning or calculation involved (e.g., numerical calculation, conceptual explanation, comparison, opinion). 3. Determine the most likely educational purpose or learning objective behind the problem. Based on your analysis, write clear, concise, and natural-sounding original problem in English that satisfies the following criteria: - Precisely aligns with the provided response. - Reflects realistic problem that could appear in an educational context or standard curriculum. - Is explicit, measurable, and unambiguous. Provide your final synthetic problem formatted strictly as: text [Your synthetic problem here] Figure 8: Prompt for variational problem synthesis from the policys correct responses to challenging problems."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Hong Kong University of Science and Technology (Guangzhou)",
        "Microsoft",
        "School of Artificial Intelligence, Chinese Academy of Sciences",
        "University of California, Los Angeles"
    ]
}