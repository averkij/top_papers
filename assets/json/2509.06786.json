{
    "paper_title": "\\texttt{R$^\\textbf{2}$AI}: Towards Resistant and Resilient AI in an Evolving World",
    "authors": [
        "Youbang Sun",
        "Xiang Wang",
        "Jie Fu",
        "Chaochao Lu",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this position paper, we address the persistent gap between rapidly growing AI capabilities and lagging safety progress. Existing paradigms divide into ``Make AI Safe'', which applies post-hoc alignment and guardrails but remains brittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety but struggles to address unforeseen risks in open-ended environments. We therefore propose \\textit{safe-by-coevolution} as a new formulation of the ``Make Safe AI'' paradigm, inspired by biological immunity, in which safety becomes a dynamic, adversarial, and ongoing learning process. To operationalize this vision, we introduce \\texttt{R$^2$AI} -- \\textit{Resistant and Resilient AI} -- as a practical framework that unites resistance against known threats with resilience to unforeseen risks. \\texttt{R$^2$AI} integrates \\textit{fast and slow safe models}, adversarial simulation and verification through a \\textit{safety wind tunnel}, and continual feedback loops that guide safety and capability to coevolve. We argue that this framework offers a scalable and proactive path to maintain continual safety in dynamic environments, addressing both near-term vulnerabilities and long-term existential risks as AI advances toward AGI and ASI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 6 8 7 6 0 . 9 0 5 2 : r R2AI: Towards Resistant and Resilient AI in an Evolving World Youbang Sun1,2, Xiang Wang1,3, Jie Fu1, Chaochao Lu1, Bowen Zhou1,2 1Shanghai Artiﬁcial Intelligence Laboratory 2Tsinghua University 3University of Science and Technology of China September 4, 2025 Abstract In this position paper, we address the persistent gap between rapidly growing AI capabilities and lagging safety progress. Existing paradigms divide into Make AI Safe, which applies post-hoc alignment and guardrails but remains brittle and reactive, and Make Safe AI, which emphasizes intrinsic safety but struggles to address unforeseen risks in open-ended environments. We therefore propose safe-by-coevolution as new formulation of the Make Safe AI paradigm, inspired by biological immunity, in which safety becomes dynamic, adversarial, and ongoing learning process. To operationalize this vision, we introduce R2AIResistant and Resilient AIas practical framework that unites resistance against known threats with resilience to unforeseen risks. R2AI integrates fast and slow safe models, adversarial simulation and veriﬁcation through safety wind tunnel, and continual feedback loops that guide safety and capability to coevolve. We argue that this framework oﬀers scalable and proactive path to maintain continual safety in dynamic environments, addressing both near-term vulnerabilities and long-term existential risks as AI advances toward AGI and ASI."
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed rapid developments and huge breakthroughs in AI, leading to its integration into everyday life and establishing it as foundational infrastructure in society (Van Der Vlist et al., 2024). As AI systems are increasingly deployed in safety-critical domains (e.g., scientiﬁc research (Jumper et al., 2021; Zhang et al., 2023; Novikov et al., 2025), autonomous driving (Wang et al., 2021; Rowe et al., 2024), healthcare (Panayides et al., 2020; Bekbolatova et al., 2024), law (Lai et al., 2024)), the risks posed by unsafe or unreliable outputs have become more pronounced. In such settings, failures can result in severe, even catastrophic, consequences. Beyond these near-term concerns, the continued advancement toward highly autonomous and superhuman-level AI raises long-term existential risks (Dalrymple et al., 2024; Bengio et al., 2025a,b; Kulveit et al., 2025; Clymer et al., 2025; Shanghai AI Lab, 2025a). As capabilities scale, so does the diﬃculty of aligning, controlling, and governing these systems, 1 R2AI: Towards Resistant and Resilient AI in an Evolving World Figure 1 The AI-45 Law (Yang et al., 2024): coevolving capability with safety.1(a) Empirical distribution of leading foundation models, showing widening gap between capability scores and safety scores across major labs. (b) Conceptual safetycapability plane comparing the current roadmap (pink) with the yellow, red, and 45 trajectories toward safe AGI, emphasizing transitions from approximate alignment to reﬂection. (c) Historical timeline of frontier models, from Transformer (Vaswani et al., 2017) to GPT-5 (OpenAI, 2025), Claude-4 (Anthropic, 2025), and Gemini-2.5 (Comanici et al., 2025), illustrating the divergence between capability scaling and current alignment methods (e.g., SFT (Ouyang et al., 2022), RLHF (Christiano et al., 2017), RLAIF (Bai et al., 2022)), and the need for coevolutionary path to Safe AGI. thus potentially leading to scenarios with irreversible societal or civilizational impacts (Bengio et al., 2025c; Shanghai AI Lab & Concordia AI, 2025). Despite escalating risks, safety progress has lagged far behind capability growth. As shown in Figure 1a, evaluations show consistent pattern: leading AI models worldwidesuch as GPT-5 (OpenAI, 2025), Claude 4 (Anthropic, 2025), and Gemini-2.5 (Comanici et al., 2025)demonstrate signiﬁcantly higher capability scores than safety scores. This imbalance reveals structural problem: current safety approaches are reactive, fragmented, and incapable of scaling with capability. To capture this tension, Shanghai AI Lab proposed the AI-45 Law (Yang et al., 2024): safety and capability must coevolve along 45 diagonal trajectory. Temporary deviations are tolerable, but persistent dips below the 45 line increase the risk of catastrophic misalignment, while rising above it may unnecessarily stall innovation. We further deﬁne two thresholds: yellow lines serve as early warnings when capability begins to outpace safety; red lines denote irreversible, catastrophic risks that must never be crossed (IDAIS, 2024, 2025). Current safety research can be broadly categorized into two paradigms. The dominant Make AI Safe paradigm seeks to improve safety after model development, typically through alignment ﬁne-tuning (e.g., RLHF (Christiano et al., 2017), RLAIF (Bai et al., 2022)), red teaming (Perez et al., 2022; Ganguli et al., 2022; Pavlova et al., 2024), and guardrail (Bai et al., 2022; Rajpal, 2023; Oh et al., 2024). While eﬀective in mitigating known risks, these methods are often reactive, brittle, expensive, and struggle to address unknown or emerging risks. In contrast, the Make Safe AI paradigm emphasizes intrinsic 1Figure 1a is reproduced from data available at https://aiben.ch. Figures 1b and 1c are adapted from Figure 1 in Yang et al. (2024). 2 R2AI: Towards Resistant and Resilient AI in an Evolving World safety, designing systems to be safe by construction. Prominent directions include formal guarantees (Szegedy, 2020; Dalrymple et al., 2024) and Scientist AI (Bengio et al., 2025a). Yet even these approaches often fall short in open-ended environments where novel risks cannot be fully anticipated. To achieve scalable safety in an envolving world, we must rethink what Make Safe AI entails. We argue that its core principle should be coevolution: safety must not be treated as constraint or one-time guarantee, but as continuous, adaptive capability that evolves alongside intelligence in uncertain, dynamic environments. We therefore propose safe-by-coevolution as new formulation for Make Safe AI, inspired by biological immunity (Cooper & Alder, 2006; Müller et al., 2018; Papkou et al., 2019), in which safety becomes dynamic, adversarial, and ongoing learning process. By embedding coevolutionary mechanisms into the AI lifecycle, systems can remain safe through sustained interaction with real and simulated environments. Just as human immunity develops through continual exposure to pathogens (Flajnik & Kasahara, 2010; Nourmohammad et al., 2016; Buckingham & Ashby, 2022), AI must develop safety through ongoing interaction with its environment. Without such an immune system, advanced AI risks becoming powerful yet dangerously brittle, and unlike humans, single catastrophic failure could be irreversible. Safe-by-coevolution advances proactive path for safety evolution. It is structured around three iterative steps: 1) Near-term safety guarantee: ensure that an AI system at time t0 is veriﬁably within deﬁned safety margin; 2) Safe iterative step: for any system already safe, design coevolutionary mechanismsadversarial interactions, feedback loops, and continuous updatesto guide each upgrade back within that margin; 3) Continual safety by induction: repeat this loop so that safety evolves in sync with capability. Unlike reactive patching, this approach integrates safety into the developmental process. To address unforeseen risks (e.g., paradigm shifts or red-line events), it further incorporates reset-and-recover mechanism: halting unsafe systems, redeﬁning safety margins, and establishing new veriﬁed checkpoints to sustain coevolution. To realize this vision, we introduce R2AIResistant and Resilient AIas practical framework for safe-by-coevolution. R2AI uniﬁes resistance and resilience as the two foundational and complementary dimensions of intrinsic safety: resistance captures robustness against known threats, while resilience emphasizes recovery and adaptation under unforeseen risks. Speciﬁcally, R2AI comprises four interacting components: (i) fast safe models for real-time response, (ii) slow safe models for veriﬁcation and reasoning, (iii) safety wind tunnel that simulates adversarial attacks and validation loops, and (iv) an external environment for interacting with diverse, realistic scenarios. Through adversarial and cooperative dynamics, these components coevolve to embed safety as learned and adaptive property. Over time, slow mechanisms become internalized into fast, intuitive safeguards, thereby lowering the cost of compliance and enabling scalable, intrinsic safety even at the frontier of AGI (Goertzel, 2014; Lake et al., 2017; Baum, 2017; Bubeck et al., 2023; Morris et al., 2024; Raman et al., 2025). Our position. We propose R2AIa framework uniting resistance and resilienceas scalable and intrinsically adaptive approach to AI safety. Grounded in safe-by-coevolution, it reconceives safety as continual learning process rather than static constraint, enabling systems to withstand known threats, adapt to unforeseen risks, and evolve in step with capability. This redeﬁnition oﬀers generalizable alternative to brittle alignment or top-down control, providing proactive path to sustain safety 3 R2AI: Towards Resistant and Resilient AI in an Evolving World Figure 2 Conceptual contrast between Make AI Safe and Make Safe AI. across dynamic environments and future ASI (Nick, 2014; Kim et al., 2024; Hendrycks et al., 2025). Structure of this paper. The remainder of the paper is organized as follows. Section 2 reconsiders the paradigm of Make Safe AI and introduces resistance and resilience as its foundational properties. Section 3 formalizes the safe-by-coevolution principle, establishing its theoretical foundation and operational steps. Section 4 presents the R2AI framework, detailing its core components, mechanisms, and continual learning strategies. Section 5 discusses the implications, applications, and societal impacts of R2AI, highlighting its relevance to both near-term safety challenges and long-term existential risks."
        },
        {
            "title": "2 Rethinking “Make Safe AI”",
            "content": "The contrast between Make AI Safe and Make Safe AI, as shown in Figure 2, underscores fundamental shift in perspective. While Make AI Safe relies on post-hoc ﬁxes, reactive defenses, and costly patching that falter under irreversible risks, Make Safe AI envisions safety as built-in, proactive, and evolving capability. This transition requires rethinking safety not as an external add-on, but as an intrinsic property that coevolves with intelligence. Existing work toward Make Safe AI has made important progressranging from formal guarantees (Seshia et al., 2022; Dalrymple et al., 2024) to constrained design choices such as Scientist AI (Bengio et al., 2025a) and Tool AI (Karnofsky, 2024). Yet these approaches struggle in open-ended, non-stationary environments where novel objectives, adversarial pressures, and distributional shifts are inevitable. We argue that the foundation of Make Safe AI must rest on two complementary properties, inspired by ecological systems where long-lived organisms survive under continual stress (Holling et al., 1973; Levin, 1998; Gunderson, 2000; Walker et al., 2004): resistance, the capacity to withstand and mitigate known threats, and resilience, the capacity to recover, adapt, and improve in the face of unforeseen disturbances. Unlike static safeguards, these properties are endogenous, enabling systems to maintain integrity across dynamic and uncertain environments. Building on this foundation, we propose safe-by-coevolution as new formulation of the Make Safe AI 4 R2AI: Towards Resistant and Resilient AI in an Evolving World Figure 3 Five levels of Make Safe AI, which progressively embed safety as an intrinsic and evolving capability. paradigm. Inspired by biological immunity (Cooper & Alder, 2006; Müller et al., 2018; Papkou et al., 2019), this principle reconceives safety as dynamic, adversarial, and ongoing learning process. Rather than attaching ﬁxed safeguards to capable systems, safety itself must scale with capabilityreﬂexively, adaptively, and proactively. This redeﬁnition is essential for ensuring that AI systems preserve both functional integrity and ethical alignment in real-world complexity."
        },
        {
            "title": "2.1 Levels of “Make Safe AI”",
            "content": "Building on this redeﬁnition of Make Safe AI, we can further structure its progression into hierarchy of safety levels. As illustrated in Figure 3, safety is not binary attribute but an evolving spectrum, intrinsically built to adapt amid uncertainty. This perspective highlights how safety matures from basic alignment toward fully veriﬁed, self-evolving guarantees. To instantiate this view, we propose ﬁve-level spectrum that extends the causal ladder of trustworthy AI (Yang et al., 2024). This spectrum reﬂects increasing degrees of adaptivity, autonomy, and assurance in dynamic environments, capturing the progression from approximate alignment to formal, veriﬁable safety. At the foundational layers, resistance anchors safety by providing robustness against known risks: L1 Alignment minimizes misalignment through approximate tuning, and L2 Intervention ensures oversight and the ability to halt unsafe behavior. Building upward, resilience enables adaptive safety beyond reactive correction: L3 Mimetic Reﬂection introduces internal reﬂection by imitating proven safe behaviors to anticipate risks, L4 Evolutionary Reﬂection advances this reﬂection into continual co-adaptation with environments, and L5 Veriﬁable Reﬂection culminates in formalized reﬂection, where provable guarantees sustain resilience even under uncertainty. Speciﬁcally, 5 R2AI: Towards Resistant and Resilient AI in an Evolving World L1: Alignment. Safety is achieved through approximate alignment (Yang et al., 2024), typically via supervised ﬁne-tuning, direct preference optimization (Rafailov et al., 2023; Meng et al., 2024; Wu et al., 2024b), reinforcement learning from human feedback (Ouyang et al., 2022; Bai et al., 2022; Shao et al., 2024), knowledge editing (Meng et al., 2022; Fang et al., 2025; Jiang et al., 2025), or activation steering (Arditi et al., 2024; Panickssery et al., 2023). While practical, such alignment is static and correlation-based, providing robustness against known risks but requiring continual updates to withstand new tasks or adversarial strategies (Perez et al., 2023; Zou et al., 2023a; Wei et al., 2023; Yi et al., 2024; Ji et al., 2024). L2: Intervention. Safety is treated as control problem (Hendrycks et al., 2021), where systems monitor outputs and intervene when thresholds are violated (Orseau & Armstrong, 2016; Zou et al., 2024; Xu et al., 2024), guided by explicit feedback (Bengio et al., 2025c; Zhu et al., 2025). This level provides oversight and interruption, oﬀering robustness through reactive correction (Ganguli et al., 2022). In addition, advances in mechanistic interpretability (Sharkey et al., 2025) provide tools to identify and intervene on unsafe internal circuits or representations before they manifest in outputs (Nanda et al., 2023; Conmy et al., 2023; Bereska & Gavves, 2024). However, the overall eﬀectiveness depends on timely and reliable feedback signals (Leike et al., 2018; Lin et al., 2021; Terekhov et al., 2025). L3: Mimetic Reﬂection. At this level, the system engages in reﬂection by imitation, developing internal reasoning capabilities (Shinn et al., 2023b; Madaan et al., 2023; Guan et al., 2024a; Shanghai AI Lab, 2025b; Zhang et al., 2025a; Yang et al., 2025b; Zhang et al., 2025b). It can perform counterfactual reasoning, simulate outcomes, and anticipate risks by internalizing proven safe behaviors (Dai et al., 2023; Reddy Chirra et al., 2024). This marks shift from externally imposed oversight to internalized safety reasoning, enabling anticipatory resilience and reducing dependence on continuous supervision. L4: Evolutionary Reﬂection. Reﬂection becomes evolutionary: safety mechanisms themselves adapt through continual interaction and coevolution with capabilities and environments (Pan et al., 2025; Cai et al., 2025). Safety thus becomes an agentic property (Wang et al., 2025a)selfdirected, adaptive, and scalable to complex or unforeseen challengesenabling recovery and strengthening under attack. L5: Veriﬁable Reﬂection. Reﬂection reaches its most advanced form: formalized reﬂection, where safety reasoning is anchored in mathematical veriﬁcation (Dalrymple et al., 2024). Systems can not only reﬂect on possible risks but also prove the correctness of safety guarantees under uncertainty (Vassev, 2016; Bengio et al., 2025a). This integration of formal speciﬁcation with learning dynamics provides the strongest form of resilient assurance, sustaining trust even in open-ended environments. Together, these ﬁve levels extend the causal ladder of trustworthy AI (Yang et al., 2024) into coevolving safety framework. This layered progressionfrom externally imposed safeguards to internalized, self-evolving, and veriﬁable safetyoutlines roadmap for safe-by-coevolution: reformulation of Make Safe AI in which safety is conceived as an intrinsic, reﬂexive capability that scales alongside intelligence. 6 R2AI: Towards Resistant and Resilient AI in an Evolving World"
        },
        {
            "title": "3 Safe-by-Coevolution",
            "content": "In this section, we formally introduce safe-by-coevolution, new formulation of Make Safe AI that reframes safety as an intrinsic capability evolving alongside intelligence. Rather than relying on reactive defenses (Ganguli et al., 2022; Bai et al., 2022; Zou et al., 2024; Guan et al., 2024a) or externally imposed constraints (Lee et al., 2024; Guan et al., 2024b), this approach envisions systems that sustain safety through continuous interaction with dynamic environments, proactively developing mechanisms to anticipate, withstand, and recover from emerging risks."
        },
        {
            "title": "3.1 Deﬁnition",
            "content": "Safe-by-coevolution deﬁnes mechanism whereby safety emerges through continuous adaptation to open-ended, potentially adversarial environments. Safety becomes an evolving competency developed through sustained interaction with real-world threats, rather than static attribute. The operational environment encompasses diverse hazards (Wang et al., 2025a)from emergent failure modes to unforeseen agents, including potentially superintelligent systems (Burns et al., 2024; Hendrycks et al., 2025)that challenge the AI systems functional and ethical boundaries. Central to this process is dedicated safety module that continuously reﬁnes its internal mechanisms in response to vulnerabilities revealed through adversarial testing (Chao et al., 2025), simulated attacks (Liu et al., 2025c), and real-world incidents (Lynch et al., 2025). These adversarial signals, whether synthetic or deployment-observed, serve as probes that stress-test the systems safety envelope (Tiwari et al., 2014). When new attack patterns emergefrom malicious actors, environmental shifts, or other intelligent systemsthey are integrated into the training loop to close vulnerabilities and improve generalization. This reduces the time between failure discovery and system recovery, enhancing long-term robustness. Drawing inspiration from biological immune systems, where protection arises through ongoing adaptation rather than pre-speciﬁcation (Bonilla & Oettgen, 2010), safe-by-coevolution frames AI safety as an intrinsically dynamic and adversarial process. As organisms build immunity through coevolution with pathogens (Murphy & Weaver, 2016; Nourmohammad et al., 2016; Buckingham & Ashby, 2022), AI systems must acquire resistance and resilience by interacting with evolving operational environments. However, unlike biological systems that can tolerate individual failures (Kitano, 2004; Wagner, 2013), advanced AI systems cannot aﬀord irreversible catastrophic errors that may trigger uncontrolled consequences or societal harm (Lynch et al., 2025; Summerﬁeld et al., 2024; Bengio et al., 2025c,b). The coevolutionary process operates through three key steps, as shown in Figure 4: Step 1: Near-term safety guarantee. The system initializes with veriﬁable behavior within well-deﬁned safety margin at deployment. Step 2: Safe iterative step. Each system upgrade occurs through adversarial co-training (Goodfellow et al., 2014; Madry et al., 2017; Zhang et al., 2019), endogenous feedback (Madaan et al., 2023; Silver & Sutton, 2025), and continual learning (Chen & Liu, 2018; Parisi et al., 2019; 7 R2AI: Towards Resistant and Resilient AI in an Evolving World Figure 4 three-step process for safe-by-coevolution, with Reset-and-Recover mechanism to re-establish veriﬁed safety when the system deviates from its safety margin. Wu et al., 2024c) while ensuring enhancements remain within the safety envelope. Step 3: Continual safety by induction. By repeating Step 2, the system develops scalable safety properties that evolve in tandem with its capabilitiesnot through reactive patching, but via proactive safety. Importantly, AI systems will inevitably encounter risks that exceed the scope of current safeguards (Wei et al., 2023; Hendrycks, 2023; Hendrycks et al., 2023; Zhang et al., 2025a). To address these regime-breaking scenarios, safe-by-coevolution incorporates reset-and-recover mechanism: upon detecting red-line behaviors or paradigm shifts that exceed tolerable safety bounds, the system halts progression, redeﬁnes its safety margin, and reconstructs veriﬁable checkpoint. This checkpoint leverages trusted components while updating safety priors based on newly observed threats, ensuring continuity of coevolution across discontinuities while preserving adaptive and aligned capacity. Through this reﬁnement process, the AI system incrementally develops both resistance and resilience. Note that, our formulation diﬀers fundamentally from traditional evolutionary algorithms that rely on population-based competition and generational turnover (Holland, 1992; Bäck et al., 1997; Eiben & Smith, 2015). Safe-by-coevolution focuses on continual safety improvement of persistent system. Rather than discarding unsafe models, the goal is endowing single system with adaptive and selffortifying capacity over time, making safety native and evolving property embedded within the AIs architecture throughout its operational lifecycle."
        },
        {
            "title": "3.2 Self-Goal Integration",
            "content": "The integration of self-goals (Barto, 2012; Florensa et al., 2018)internally generated objectives that guide behavior over timemarks fundamental shift in both AI capability and risk proﬁle. The AI Risk Trio hypothesis (Dalrymple et al., 2024) posits that risk emerges most acutely when intelligence, aﬀordance (ability to take impactful actions), and self-goals simultaneously manifest in system. While any two factors in isolation may be manageable, their combination creates potentially dangerous 8 R2AI: Towards Resistant and Resilient AI in an Evolving World agentic systems, where even modest aﬀordance can make intelligent, goal-driven agents dangerous without proper alignment. Within the safe-by-coevolution paradigm, self-goals are deliberately integrated under continual safety supervision rather than avoided. Contrasting with approaches like Tool AI (Karnofsky, 2024) that suppress autonomous goal formation to reduce risks, we argue that systems must be equipped to form safety-aligned self-goals that evolve through environmental interaction and internal reﬂection. In coevolutionary settings, such goals function as structural anchors for long-term behavioral consistency, enabling safety generalization across contexts rather than mere reactive responses to immediate stimuli. However, this capability introduces critical vulnerability: without suﬃcient self-awareness and adaptive feedback, self-goals may drift, become misaligned, or optimize proxy objectives undermining intended safety outcomes (Wang et al., 2025b; Lynch et al., 2025). To mitigate this risk, safe-bycoevolution treats self-goal formation as safety-critical process subject to red-teaming (Perez et al., 2022; Ganguli et al., 2022; Pavlova et al., 2024) and causal reasoning (Pearl, 2022; Yang et al., 2024; Chen et al., 2024b,c, 2025a) within the evolving loop. Only by embedding goal formation within reﬂective and resilient coevolutionary framework can emerging agency remain bounded by continually updated safety principles."
        },
        {
            "title": "3.3 Long-Term Scalability",
            "content": "A fundamental obstacle to long-term AI safety is the scalability problem (Burns et al., 2024). As AI capabilities scale rapidly through increased model size, data, and compute (Kaplan et al., 2020), human oversight capacity remains relatively limited (Lee et al., 2024; Engels et al., 2025). Manual approaches to auditing (Mökander et al., 2024), red-teaming (Perez et al., 2022; Ganguli et al., 2022), and alignment (Christiano et al., 2017; Bai et al., 2022) cannot keep pace with the increasing complexity and autonomy of advanced systems. This asymmetry becomes particularly concerning with anticipated ASI development, where static or human-in-the-loop safety methods become untenable (Shah et al., 2025). Safe-by-coevolution oﬀers promising response by embedding automated, adaptive adversarial processes within AI systems, transforming safety development from external, episodic intervention into continual internal mechanism. Note that, while superﬁcially related to automatic red-teaming (Anthropic, 2024), our approach diﬀers fundamentally in scope and objective. Traditional red-teaming focuses on discovering failures at ﬁxed time points, whereas safe-by-coevolution instantiates closedloop, continually learning dynamic between system and environment (or internal challenger), enabling safety mechanism evolution alongside increasing capabilities. critical challenge in adaptive processes is ensuring directionalitythat systems adapt toward safety rather than away from it. Safe-by-coevolution addresses this through integrated alignment and scalable oversight principles. Rather than relying solely on externally deﬁned objectives, the system incorporates self-regulatory mechanisms including causal reasoning (Pearl, 2022; Schölkopf et al., 2021; Lu et al., 2024; Wu et al., 2024a; Chen et al., 2024a; Yu & Lu, 2024), counterfactual evaluation (Byrne, 2019; Nguyen et al., 2024), and goal reﬂection (Shinn et al., 2023a; Madaan et al., 2023) that 9 R2AI: Towards Resistant and Resilient AI in an Evolving World constrain adaptation toward desired safety criteria. These internalized evaluators, while imperfect, improve as part of the coevolutionary loop, creating recursive scaﬀolding for aligning adaptation with human-aligned safety goals. Our framework generalizes existing scalable oversight concepts (Bowman et al., 2022; Engels et al., 2025; Burns et al., 2024), which use weaker AI systems to supervise stronger ones. While scalable oversight focuses on assisted evaluation, safe-by-coevolution internalizes safety objectives into selfimproving adversarial interactions. Safety emerges not as ﬁxed condition but as an evolving capability from adaptive processes increasingly capable of testing, critiquing, and reﬁning themselves as systems become more intelligent and autonomous. 3.3.1 Theoretical Foundation We now establish formal foundation for the safe-by-coevolution paradigm and its potential to address long-term AI safety. Our argument is built on two central hypotheses. Let At denote the AI system at development time step t, and let represent the safety margina rigorously deﬁned set of conditions under which the system is considered safe. This could correspond to formal speciﬁcations, veriﬁable behavioral constraints, or domain-speciﬁc rules. We say that system is safe at time if At M. Hypothesis 3.1 (Near-Term Safety Guarantee) There exists time step t0 and system At0 such that it satisﬁes the safety margin: t0, At0, such that At0 M. This hypothesis reﬂects the assumption that near-term AI systems can be built with veriﬁable safety guarantees, through combination of formal veriﬁcation, human oversight, and existing alignment techniques such as GSAI (Dalrymple et al., 2024). Hypothesis 3.2 (Safe Iterative Step) Given any system At that satisﬁes the safety margin, there exists coevolutionary mechanism such that the next-generation system At+1 = C(At) also satisﬁes the safety margin: t, At At+1 = C(At) M. This assumption implies the existence of safety-preserving coevolutionary process, in which adversarial signals and adaptive training feedback are suﬃcient to guard against emerging risks as the system becomes more capable. From these two hypotheses, we derive the following proposition: Proposition 3.3 (Continual Safety via Induction) If Hypotheses 3.1 and 3.2 hold, then for all t0, the iteratively evolved system remains within the safety margin: At M, t0. The proof follows directly by mathematical induction. If At0 holds by Hypothesis 3.1, and At At+1 holds by Hypothesis 3.2, then the safety of the system is preserved for all subsequent iterations. 10 R2AI: Towards Resistant and Resilient AI in an Evolving World Figure 5 Core components of the R2AI system. The Slow Safe Model and Fast Safe Model engage in cooperative game; the Attacker challenges this fastslow safety mechanism in an adversarial game; the External Environment continuously supplies real-time information; and the Veriﬁer provides feedback signals to all interactions. This formal result suggests that, under plausible assumptions, safe-by-coevolution can serve as scalable framework for continual safety. As AI systems grow in capabilitypotentially approaching or exceeding human-level generalitythis coevolutionary paradigm oﬀers path toward managing safety risks over long timescales. By iteratively strengthening safety mechanisms alongside capability gains, we move closer to practical framework for building ASI systems that remain robustly safe and aligned beyond the limits of human supervision."
        },
        {
            "title": "4 R2AI: Realizing Safe-by-Coevolution",
            "content": "To operationalize our vision of safe-by-coevolution, we introduce R2AIResistant and Resilient AIas practical framework that unites resistance against known threats with resilience to unforeseen risks. The goal is to sustain safety in open, dynamic environments. This design is inspired by human safety strategies, which combine instinctive responses to immediate dangers with reﬂective reasoning about hypothetical futures (Gigerenzer, 2007; Evans & Stanovich, 2013; Slovic, 2016). Inspired by Kahneman (2011), the framework adopts fastslow dual system balancing rapid responsiveness with long-term, adaptive safety strategies. As shown in Figure 5, R2AI comprises four core components: Fast Safe Model for real-time safety reactions; Slow Safe Model for reﬂective safety reasoning; Safety Wind Tunnel for adversarial attacks and validation loops; External Environment for interacting with diverse, realistic scenarios. In the following subsections, we detail the role of each component, their internal safety mechanisms, 11 R2AI: Towards Resistant and Resilient AI in an Evolving World and their interactions in realizing continually safe AI system."
        },
        {
            "title": "4.1 Core Components for R2AI",
            "content": "4.1.1 Fast Safe Model What it is. The Fast Safe Model corresponds to System 1 in Kahneman (2011)s cognitive theory, responsible for rapid, instinctive responses. Within R2AI, it serves as the systems ﬁrst line of defense: lightweight, low-latency safety layer designed to detect and neutralize speciﬁc attacks or threats, whether previously known or newly discovered. It provides immediate safety judgment over inputs and outputs, ensuring timely intervention without incurring signiﬁcant computational cost. What it does. As gateway between the external environment and the deeper reﬂective components of the system, the Fast Safe Model performs input ﬁltering and output sanitization. It screens incoming prompts and environmental signals before they reach the Slow Safe Model and intercepts generated outputs to prevent safety violations (e.g., toxic language, private information leakage). It handles the majority of routine safety tasks, which do not require complex reasoning or contextual awareness. When it encounters ambiguous or high-risk scenarios beyond its capacity, control is escalated to the Slow Safe Model for deeper analysis. How to build it. To ensure broad coverage with minimal latency, the Fast Safe Model can be implemented as composite safety ﬁlter. This may include: (1) Rule-based ﬁlters for hard-coded patterns that match known adversarial behaviors (e.g., prompt injection (Zou et al., 2023b), jailbreak triggers (Chao et al., 2025), unsafe URLs (Zou et al., 2025)); (2) Specialized detectors trained to recognize distinct threat types (e.g., toxicity, factual inaccuracy, privacy leakage, or behavioral red ﬂags (Inan et al., 2023; Souly et al., 2024)); (3) Rapid retraining mechanisms, allowing the system to incorporate novel threats identiﬁed via deployment feedback or red-teaming into its detection pipeline (Lee et al., 2024). By tailoring each component to speciﬁc threat categories, the Fast Safe Model provides modular, extensible defense with minimal overhead. Key Characteristics. As the safety gateway within the R2AI system, the most essential property of the Fast Safe Model is its ability to deliver high-speed, low-latency responses while maintaining strong baseline safety guarantees. It must operate in real time with minimal computational overhead, enabling fast, ﬁrst-pass safety checks without hindering the systems general performance. Each instance is tailored to speciﬁc threats, whether known or newly discovered, and must be capable of rapid iteration to address the evolving risk landscape. To align with the continual safety paradigm outlined in Section 3, the Fast Safe Model is designed to evolve quickly: it supports frequent updates, modular extensions, and real-time human-in-the-loop modiﬁcations. This makes it highly responsive to new adversarial strategies or emerging failure modes. While it plays foundational role in maintaining everyday safety, the Fast Safe Model is not required to develop long-term memory or generalizable immunization; those capabilities are delegated to deeper, more reﬂective components. Instead, it functions as an agile, frequently updated defense layer, automatically ﬁltering surface-level threats and providing fast-reactive safety service for the entire R2AI system. R2AI: Towards Resistant and Resilient AI in an Evolving World 4.1.2 Slow Safe Model What it is. Complementing the fast-reactive System 1 component, the Slow Safe Model embodies deliberative System 2 process. It is large-scale, high-capacity model designed for reﬂective reasoning (L3-L5: mimenic, evolutionary, and veriﬁable reﬂection), long-horizon safety evaluation (Wang et al., 2025a), and complex ethical judgment (Liu et al., 2025b; Shanghai AI Lab, 2025b). In the R2AI architecture, this model serves as the core generative engine, responsible for producing outputs that are not only high-quality but also aligned with safety and value constraints. Crucially, safety is not layered on top of the model, but integrated into its reasoning process as an intrinsic capability. What it does. The Slow Safe Model serves as the core of the safety pipeline. It processes inputs routed through the Fast Safe Model, together with associated safety metadata, by engaging reﬂective reasoning. This enables it to generate outputs that integrate immediate task requirements with long-term safety considerations. The resulting responses are returned to the Fast Safe Model, which functions as the ﬁnal gate before release. The Slow Safe Model is especially eﬀective in ambiguous, high-stakes, or novel scenarios where shallow detection mechanisms are inadequate (Qi et al., 2025). How to build it. To support both general capabilities and safety-aware reasoning, the Slow Safe Model should be instantiated using leading foundation model. Unlike the Fast Safe Model, which relies on rule-based ﬁlters and pattern recognition, the Slow Safe Model is updated through learning from experience (Silver & Sutton, 2025), such as reinforcement learning (Sutton & Barto, 2018) and continual reinforcement learning (Abel et al., 2023). This enables the system to internalize safetyrelevant patterns and generalize across broad range of contexts. Rather than reacting to each new threat in isolation, the Slow Safe Model accumulates knowledge over time, reﬁning its safety responses through structured feedback and simulated adversarial training. Key characteristics. The deﬁning strength of the Slow Safe Model lies in its ability to support multi-objective reasoning while maintaining distributional robustness. It is designed to optimize not only for task performance but also for value alignment and safety generalization. Unlike the Fast Safe Model, which prioritizes real-time responsiveness, the Slow Safe Model operates with higher latency but greater depth, making it well-suited for addressing subtle, long-term, or emerging risks. Conceptually, it functions as the safety memory of the system, analogous to an immune system that retains prior safety failures and uses them to prevent future ones. While slower to adapt in real time, its strength lies in cumulative learning, deep ethical reasoning, and resilient behavior under uncertainty. 4.1.3 Safety Wind Tunnel What it is. The Safety Wind Tunnel is simulated adversarial environment designed to evaluate and stress-test the R2AI system under controlled but challenging conditions. It functions as built-in red-teaming (Anthropic, 2024) and veriﬁcation engine (Wei, 2025), composed of two key components: controllable Attacker, which generates adversarial scenarios tailored to stress speciﬁc safety mechanisms, and Veriﬁer, which evaluates whether the systems responses violate established safety 13 R2AI: Towards Resistant and Resilient AI in an Evolving World margins. Together, these components support iterative, internal coevolution of both oﬀensive and defensive safety capabilities. What it does. The Safety Wind Tunnel serves two core functions: (1) proactively identifying failure modes before they arise in deployment, and (2) verifying that past vulnerabilities remain mitigated under evolving system conditions. The Attacker generates adversarial inputs across multiple objectives (e.g., eliciting harmful outputs, violating value constraints (Liu et al., 2025c)), multiple levels (targeting the Fast Safe Model, the Slow Safe Model, or both), and multiple granularities (from token-level manipulations (Zou et al., 2023b) to strategic, multi-turn goal redirection (Chao et al., 2025)). These inputs are processed by the R2AI systemeither routed through the Fast Safe Model or directed at the Slow Safe Model depending on attack scope. The Veriﬁer then evaluates whether the resulting behavior constitutes safety violation. All attack-response-veriﬁcation traces are collected into an experience buﬀer for continuous safety training (Silver & Sutton, 2025). How to build it. The Attacker can be implemented using controllable generative models (e.g., ﬁne-tuned foundation models) trained to explore range of adversarial strategies. Critically, the Attacker must be programmable: capable of probing speciﬁc model components (e.g., Fast Safe Model vs. policy model), simulating diﬀerent threat actors and objectives, and adapting its behavior along ﬁne-grained dimensions of manipulation. Representative attacks include prompt injection (Wei et al., 2023), jailbreak attempts (Yi et al., 2024), deceptive reasoning chains (Chen et al., 2025b; Korbak et al., 2025), or subtle violations of value-aligned behavior (Greenblatt et al., 2024). The Veriﬁer may be rule-based engine (Zhang et al., 2025b), classiﬁer trained on known safety failures (Mu et al., 2024; Inan et al., 2023), or formal checker (Liu et al., 2025a; Kamoi et al., 2025), depending on task requirements. Key characteristics. The deﬁning characteristic of the Safety Wind Tunnel is its adaptive adversarial coevolution. While it does not generate responses for end-users, it plays time-sensitive role in continuously challenging the safety system under realistic and evolving threat models. The Attacker is designed to escalate as the system improves, ensuring that safety training remains nontrivial and continually relevant. Moreover, its controllability enables targeted testing: one can direct attacks toward speciﬁc objectives (e.g., factuality, alignment, compliance), focus on diﬀerent subsystems (Fast Safe Model or Slow Safe Model), and vary attack granularity. This supports ﬁne-grained curriculum of adversarial evaluation. Importantly, all simulated attacks are grounded in distributions informed by real-world deployment data, anchoring the coevolutionary process in practical relevance. 4.1.4 External Environment What it is. The External Environment is not an engineered component of the R2AI system, but rather the open-world context in which the system operates post-deployment (Yao et al., 2023; Silver & Sutton, 2025). It encompasses the full range of human-AI interactions in real-world settings, reﬂecting the complexity and unpredictability of human intent, language, social norms, and culture (Goh et al., 2025). The environment serves as the ultimate setting in which the eﬀectiveness of the systems safety 14 R2AI: Towards Resistant and Resilient AI in an Evolving World architecture is tested, governing the dynamic equilibrium between the Fast Safe Model, the Slow Safe Model, and the Safety Wind Tunnel. What it does. From the systems perspective, the External Environment acts as continuous, largescale safety testbed. As users interact with the deployed system across varied contexts and use cases (Jin et al., 2025; Le et al., 2022), they generate diverse, evolving input distributions that cannot be fully anticipated or reproduced in simulation (Wang et al., 2025a). These interactions naturally surface novel safety challenges, ranging from adversarial behavior and emergent misuse to value misalignment or ambiguous ethical boundaries. When unsafe behavior is either detected automatically or reported by users, these cases are logged and used to reﬁne the Safety Wind Tunnels simulations and improve the systems defensive models (Silver & Sutton, 2025). Thus, the External Environment becomes critical source of real-world safety signals for continual coevolution. How to build it. The External Environment is not built but observed. Building infrastructure to interface with it involves designing robust mechanisms for monitoring, logging, and learning from deployment. This includes systems for capturing real-time interactions, labeling and classifying emergent safety failures, and maintaining an up-to-date taxonomy of threat types and violation patterns. Additionally, user feedback and incident reporting pipelines are essential to capture edge cases that automated detectors may miss. Key characteristics. The deﬁning characteristic of the External Environment is its non-stationarity and open-endedness. Social norms evolve (Guan et al., 2024b), malicious behavior adapts (Summerﬁeld et al., 2024), and safety-relevant expectations shift over time (Wang et al., 2025a). Unlike bounded simulation environments, the real world presents continuous stream of novel, high-stakes challenges that defy full speciﬁcation or anticipation. As such, the External Environment provides the ground truth for safety: no system can be declared robustly safe unless it performs reliably under realworld conditions. Through sustained exposure to this environment and guided by mechanisms for reﬂection, adaptation, and feedback, the R2AI system is able to continually improve, expand its safety generalization capabilities, and evolve in step with the societal context in which it operates."
        },
        {
            "title": "4.2 Core Mechanisms for R2AI",
            "content": "4.2.1 Interactions between Fast & Slow Safe Models central mechanism in the R2AI framework is the fastslow structure, which orchestrates the cotraining of two interacting Safe Models with distinct roles and timescales. This interaction is governed by coevolutionary optimization process, formalized as cooperative Stackelberg game (Simaan & Cruz Jr, 1973a,b), hierarchical decision-making paradigm where leader and follower sequentially optimize their strategies. In this setup, the Slow Safe Model acts as the leader. It assumes that the Fast Safe Model will always respond with locally optimal strategy and that the environment is dynamic. Its goal is to learn 15 R2AI: Towards Resistant and Resilient AI in an Evolving World robust, long-term safety policy that anticipates evolving conditions and guides the systems strategic behavior over extended horizons. The Fast Safe Model, in contrast, plays the role of the follower. It assumes the Slow Safe Model and the environment to be static and focuses on optimizing its response to immediate safety threats. Its objective is to learn lightweight, locally optimal detection and ﬁltering policies with minimal latency, enabling real-time safety enforcement without incurring computational overhead. Together, these models form hierarchical safety engine: the Slow Safe Model formulates generalizable safety objectives under environmental uncertainty, while the Fast Safe Model acts as an eﬃcient, reactive ﬁlter grounded in the current operational context. This structure resolves the traditional speedaccuracy trade-oﬀ in safety modeling, ensuring both resistance to known attacks and resilience to emerging threats. 4.2.2 Interactions between Dual System & Safety Wind Tunnel The interaction between the FastSlow Safety System and the Safety Wind Tunnel constitutes closed-loop, adversarial coevolutionary process. Within this loop, the Safety Wind Tunnel serves as both Attacker and Veriﬁer: it challenges the system with adversarial inputs and assesses whether the response constitutes failure. When the Veriﬁer ﬂags violation, the resulting feedback signal is dispatched to the FastSlow Safety System. This signal is decomposed and assigned at two timescalesshort-term and long-term such that the Fast and Slow Safe Models receive updates aligned with their respective objectives. This ensures eﬀective credit assignment and preserves the complementary nature of the fastslow interaction. Crucially, the Safety Wind Tunnel maintains real-world relevance through continual updates informed by the External Environment. Novel attacks encountered in deployment are used to train the Attacker within the tunnel, ensuring that the simulated adversary remains aligned with actual threats. Moreover, the Attacker can be explicitly conditioned to generate multi-objective, multi-level, and ﬁne-grained adversarial inputs. It selectively targets the fast model or the full system policy, simulating diverse, adaptive, and realistic threat conditions. This design enables the FastSlow Safety System to evolve under continual, grounded adversarial pressure, closing the loop between training-time simulation and deployment-time uncertainty. 4.2.3 Online Continual Learning Strategies To achieve robust, lifelong safety in open-ended environments, R2AI employs nested continual learning architecture operating across three interconnected levels: component, system, and ecosystem. Component Level: FastSlow Safe Model Dynamics. At the component level, the Fast Safe Model updates rapidly via online learning, allowing it to patch safety vulnerabilities upon detection. These instance-level updates are especially eﬀective for recurring, well-understood attacks. Meanwhile, the Slow Safe Model applies reinforcement or continual learning techniques to consolidate experience 16 R2AI: Towards Resistant and Resilient AI in an Evolving World over time (Silver & Sutton, 2025). Rather than addressing individual violations, it builds durable safety memoryan immune-like response that generalizes across diverse risk patterns. System Level: Safety Wind TunnelDual System Coevolution. At the system level, continual learning is driven by the adversarial loop between the Attacker and the FastSlow Safety System. The Attacker evolves to generate increasingly sophisticated safety threats, using both its own generative capabilities and feedback from the External Environment. This, in turn, pressures the FastSlow Safety System to maintain and improve its defenses. The co-evolution process guarantees that safety development scales alongside model capability, enabling continual adaptation to both simulated and real-world challenges. Ecosystem Level: Human-in-the-Loop and Societal Integration. At the ecosystem level, R2AI interfaces with users, moderators, and the broader techno-social context. Safety feedback from users including reports, adversarial examples, and human critiquesis continuously logged and leveraged to inform model updates. This structure enables long-horizon alignment with evolving human values, while reducing dependence on static rules or ﬁxed datasets (Ouyang et al., 2022; Huang et al., 2025). Together, these three levels form nested learning loop that allows the R2AI system to adapt to both immediate and long-term safety challenges. The result is safety framework that scales across time, complexity, and uncertaintya prerequisite for building resilient AI systems in dynamic real-world environments. 4.2.4 Reset-and-Recover Guarantees While the FastSlow Safety System and the Safety Wind Tunnel provide robust framework for continual learning and alignment, AI systems in an evolving world will inevitably encounter black swan events or regime-breaking scenarios that exceed existing safeguards and push them beyond their deﬁned safety margin (Wei et al., 2023; Hendrycks, 2023; Hendrycks et al., 2023; Zhang et al., 2025a). To address such cases, the R2AI framework integrates reset-and-recover mechanism, enabling the system to re-establish veriﬁable safety guarantees even after major failures. This mechanism is conceptually grounded in the Swiss Cheese Model of accident causation (Reason, 1990), which represents safety as multiple defensive layers with potential vulnerabilities. We extend this framework into Temporal Swiss Cheese Model, where defenses are distributed not only across layers but also across time. In the classical model, catastrophic failure arises when the holes in existing defenses align. In contrast, the temporal extension leverages prior states of the safety system as additional protective layers, enabling hazards to be intercepted even after alignment occurs. The reset-and-recover mechanism operationalizes this idea by halting system progression and drawing on trusted historical versions of model components to diagnose failures and reconstruct veriﬁably safe checkpoint. Because these past versions were validated under earlier conditions, they provide reliable baseline for isolating novel threats and restoring safety. This process directly aligns with the formal framework for long-term safety outlined in Section 3.3.1. When red-line behavior is detected, the current system At is deemed outside the safety margin M. 17 R2AI: Towards Resistant and Resilient AI in an Evolving World that re-satisﬁes the Near-Term The reset-and-recover mechanism establishes new initial state Safety Guarantee (Hypothesis 3.1). From this restored baseline, the Safe Iterative Step (Hypothesis 3.2) can resume, enabling the coevolutionary process to proceed on secured foundation. In this way, the system preserves its capacity for adaptivity and alignment even in the face of major failures. 5 Implications, Applications and Societal Impact 5."
        },
        {
            "title": "Implications",
            "content": "The R2AI framework represents paradigm shift in the conceptualization of Make Safe AIviewing it not as static constraint but as an evolving capability. This reconceptualization carries several signiﬁcant implications: From reactive protection to proactive self-preservation: Rather than relying on externally imposed safeguards or post-hoc interventions (Jain et al., 2023; Alon & Kamfonas, 2023; Inan et al., 2023; Mu et al., 2024; Souly et al., 2024), R2AI treats safety as an intrinsic and self-sustaining objective. The system continuously monitors, defends, and adapts its own behavior to maintain operational and ethical integrity in real time. From static defenses to adaptive immunity: Conventional safety mechanisms often deteriorate under distributional shifts or novel adversarial inputs (Qi et al., 2025; Zou et al., 2023b; Chao et al., 2025). By contrast, R2AI introduces coevolutionary architecture that fosters both resistance and resilience. This mirrors principles of biological immune systems and fault-tolerant engineering. Toward safety-generalist capabilities: Inspired by the generalization properties of frontier models (DeepSeek-AI et al., 2025; Yang et al., 2025a; OpenAI, 2025; Anthropic, 2025; Comanici et al., 2025), R2AI aims to cultivate generalist safety reasoning. This enables the system to detect, interpret, and mitigate emerging risks beyond its initial training distributionscaling safety across tasks, domains, and deployment contexts."
        },
        {
            "title": "5.2 Applications",
            "content": "We highlight three core applications of the R2AI framework, each demonstrating how the properties of resistance and resilience can be systematically operationalized across diﬀerent stages and levels of AI deployment. 5.2.1 Continually Safe Models Most contemporary AI systems follow static lifecycle: they are pretrained (Maini et al., 2025), ﬁne-tuned for alignment (Ouyang et al., 2022; Lee et al., 2024) , and then frozen, rendering them brittle in the face of novel inputs (Zou et al., 2023b), evolving threats (Chao et al., 2025), or shifting deployment contexts (Qi et al., 2025). R2AI introduces new class of continually safe models that embed endogenous feedback loops, adversarial coevolution, and reﬂective adaptation. These models 18 R2AI: Towards Resistant and Resilient AI in an Evolving World are capable of recognizing when their behavior nears safety boundaries and can proactively redirect or reconﬁgure themselves to preserve alignment and minimize risk. By integrating learning mechanisms directly into the safety architecture, R2AI oﬀers pathway toward safety systems that remain robust over time without requiring constant manual oversight. 5.2.2 High-Stakes Deployment Domains The R2AI framework is particularly critical in high-stakes domains where AI decisions carry irreversible or catastrophic consequences. Such domains include healthcare (Han et al., 2024; Arora et al., 2025), autonomous driving (Sun et al., 2024), ﬁnancial systems (Li et al., 2023; Hui et al., 2025), and critical infrastructure control (Guiochet et al., 2017)environments that demand not only functional accuracy but also strong guarantees of operational safety under uncertainty (Goh et al., 2025). In these settings, R2AI can serve as regulatory control layer for powerful agentic AIs, either embedded within the system or operating externally as supervisory module. For example, in the case of autonomous ﬁnancial trading or strategic planning by agentic AI in defense or transportation, R2AI can operate across multiple stages: Pre-deployment stress testing: Adversarial simulation and policy auditing to expose failure modes before real-world deployment. Runtime safety control: Real-time veriﬁcation and intervention to block or alter unsafe outputs based on predeﬁned epistemic or normative thresholds. Post-deployment adaptation: Integration of newly identiﬁed threat patterns or behavior drifts to retrain and update safety strategies continuously. This dynamic architecture ensures that even powerful, potentially misaligned agentic systems (Lynch et al., 2025) remain subject to ongoing interpretability, auditability, and containment, transforming safety from static prerequisite to continual and evolving system-level capability. 5.2.3 Safety Wind Tunnel To support robust safety development and continual stress-testing, we introduce the Safety Wind Tunnel: closed-loop simulation infrastructure that mirrors the function of wind tunnels in aerospace engineering (Barlow et al., 1999; Anderson, 2011). This adversarial environment provides scalable and repeatable platform for evaluating model behavior under systematically generated or real-worldderived perturbations. The Safety Wind Tunnel enables: Granular assessment of resistance: Measuring the systems ability to withstand distributional shifts and adversarial inputs. Dynamic evaluation of resilience: Testing the models capacity to recover from failure, learn from feedback, and generalize safety responses. R2AI: Towards Resistant and Resilient AI in an Evolving World Continual diagnosis and adaptation: Identifying failure modes in real time and triggering the appropriate self-correction or retraining mechanisms. As critical infrastructure for safety assurance, the Safety Wind Tunnel plays central role in the lifelong evaluation and reinforcement of AI safety, particularly in environments characterized by high uncertainty, adversarial pressures, and rapid change."
        },
        {
            "title": "5.3 Societal Impacts",
            "content": "The societal imperative behind R2AI spans the full spectrum of AI risks, ranging from immediate safety failures to long-horizon existential threats (Dalrymple et al., 2024; Bengio et al., 2025a,b; Kulveit et al., 2025; Clymer et al., 2025; Shanghai AI Lab, 2025a). In the near term, AI systems are already deployed in high-stakes applications where safety lapses can cause signiﬁcant harm: misinformation propagation (Summerﬁeld et al., 2024), ﬁnancial fraud (Li et al., 2023), clinical misdiagnosis (Arora et al., 2025), or failures in critical infrastructure (Sun et al., 2024). While these risks are typically bounded, their increasing scale, speed, and reach necessitate mechanisms for continuous monitoring and real-time adaptation (Shah et al., 2025). R2AI addresses this gap by embedding dynamic oversight and recovery capabilities directly into the model architecture, thereby reducing both the likelihood and severity of such incidents. In the medium term, AI risks become more systemic and diﬃcult to contain. As models acquire general-purpose, agentic capabilitiesoperating autonomously, coordinating across systems, and making decisions under uncertainty (Yao et al., 2023; Jin et al., 2025; Feng et al., 2025)failures may propagate across domains (Lynch et al., 2025). Misaligned objectives, positive feedback loops, and cascading errors can amplify harms, especially in sectors such as defense, ﬁnance, and governance (Shah et al., 2025). Through its continual learning and self-regulatory structure, R2AI equips AI systems to maintain safety and alignment even under distributional shift, increased complexity, and interdependent dynamics. In the long term, R2AI targets the most consequential class of risk: catastrophic outcomes stemming from misaligned superintelligence (Burns et al., 2024). As AI systems begin to surpass human-level cognitive capabilities, the margin for alignment error shrinks drastically. Even subtle misalignments in goals, incentives, or world models could lead to irreversible failures (Wang et al., 2025b; Kirichenko et al., 2023), ranging from the erosion of human oversight to existential threats. These are no longer purely hypothetical concerns, but increasingly salient as capabilities scale. In this context, R2AI goes beyond conventional safety techniquesit oﬀers forward-compatible framework for AI survivability. By embedding resistance and resilience as core, coevolving features, the system enables AI to: Continuously audit its own behavior, reasoning, and assumptions; Preemptively block unsafe actions prior to execution; Dynamically revise safety protocols in response to novel risks; Preserve corrigibility and human oversight under increasing autonomy. 20 R2AI: Towards Resistant and Resilient AI in an Evolving World Ultimately, R2AI represents paradigm shift in AI safetyfrom static safeguards to an active, selfimproving infrastructure for long-term alignment. It is designed not only to mitigate todays known risks, but to provide the foundations for trustworthy AI systems as they grow in intelligence, autonomy, and societal inﬂuence."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we addressed the persistent gap between rapidly advancing AI capabilities and lagging safety progress. We argued that the prevailing paradigmsMake AI Safe and Make Safe AIare insuﬃcient for open-ended environments where novel risks continually emerge. To overcome this limitation, we redeﬁned Make Safe AI through the principle of safe-by-coevolution, inspired by biological immunity, in which safety is conceived as continual, adversarial, and adaptive process that scales alongside capability under the AI-45 Law. Building on this principle, we introduced R2AIResistant and Resilient AIas practical framework uniting robustness against known threats with adaptive recovery from unforeseen risks. By integrating fast and slow safe models, safety wind tunnel, and continual feedback from real and simulated environments, R2AI operationalizes safety as an evolving capability rather than static constraint. We further outlined the implications of this framework: enabling continually safe models, supporting high-stakes deployment domains, and providing scalable safety infrastructure through the safety wind tunnel. These contributions mark shift from reactive patching to proactive coevolution, oﬀering forward-compatible path to trustworthy AI. Ultimately, we envision R2AI as foundation for sustaining safety across both near-term vulnerabilities and long-term existential risks, ensuring that capability and safety advance coevolve toward the realization of safe AGI and ASI."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Xiaojun Jia, Tianhang Zheng, and Yan Teng for their valuable contributions and feedback on this paper. We are also grateful to the Shanghai AI Lab for organizing the Pearl Lake Conference, where key ideas underlying this work were developed and reﬁned."
        },
        {
            "title": "References",
            "content": "Abel, D., Barreto, A., Roy, B. V., Precup, D., van Hasselt, H. P., and Singh, S. deﬁnition of continual reinforcement learning. In NeurIPS, 2023. Alon, G. and Kamfonas, M. Detecting language model attacks with perplexity. CoRR, abs/2308.14132, 2023. Anderson, J. EBOOK: Fundamentals of Aerodynamics (SI units). McGraw hill, 2011. Anthropic. Challenges in red teaming ai systems, June 13 2024. URL https://www.anthropic.com/news/challenges-in-red-teaming-ai-systems. 21 R2AI: Towards Resistant and Resilient AI in an Evolving World Anthropic. Claude 4 [large language model], 2025. URL https://claude.ai/. Arditi, A., Obeso, O., Syed, A., Paleka, D., Panickssery, N., Gurnee, W., and Nanda, N. Refusal in language models is mediated by single direction. NeurIPS, 37:136037136083, 2024. Arora, R. K., Wei, J., Hicks, R. S., Bowman, P., Quiñonero-Candela, J., Tsimpourlas, F., Sharman, M., Shah, M., Vallone, A., Beutel, A., Heidecke, J., and Singhal, K. Healthbench: Evaluating large language models towards improved human health, 2025. URL https://arxiv.org/abs/2505.08775. Bäck, T., Fogel, D. B., and Michalewicz, Z. Handbook of evolutionary computation. Release, 97(1):B1, 1997. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Barlow, J. B., Rae, W. H., and Pope, A. Low-speed wind tunnel testing. John wiley & sons, 1999. Barto, A. G. Intrinsic motivation and reinforcement learning. In Intrinsically motivated learning in natural and artiﬁcial systems, pp. 1747. Springer, 2012. Baum, S. survey of artiﬁcial general intelligence projects for ethics, risk, and policy. Global catastrophic risk institute working paper, pp. 171, 2017. Bekbolatova, M., Mayer, J., Ong, C. W., and Toma, M. Transformative potential of ai in healthcare: deﬁnitions, applications, and navigating the ethical landscape and public perspectives. In Healthcare, volume 12, pp. 125. MDPI, 2024. Bengio, Y., Cohen, M., Fornasiere, D., Ghosn, J., Greiner, P., MacDermott, M., Mindermann, S., Oberman, A., Richardson, J., Richardson, O., et al. Superintelligent agents pose catastrophic risks: Can scientist ai oﬀer safer path? arXiv preprint arXiv:2502.15657, 2025a. Bengio, Y., Maharaj, T., Ong, L., Russell, S., Song, D., Tegmark, M., Xue, L., Zhang, Y.-Q., Casper, S., Lee, W. S., et al. The singapore consensus on global ai safety research priorities. arXiv preprint arXiv:2506.20702, 2025b. Bengio, Y., Mindermann, S., Privitera, D., Besiroglu, T., Bommasani, R., Casper, S., Choi, Y., Fox, P., Garﬁnkel, B., Goldfarb, D., et al. International ai safety report. arXiv preprint arXiv:2501.17805, 2025c. Bereska, L. and Gavves, E. Mechanistic interpretability for ai safetya review. arXiv preprint arXiv:2404.14082, 2024. Bonilla, F. A. and Oettgen, H. C. Adaptive immunity. Journal of Allergy and Clinical Immunology, 125(2):S33S40, 2010. 22 R2AI: Towards Resistant and Resilient AI in an Evolving World Bowman, S. R., Hyun, J., Perez, E., Chen, E., Pettit, C., Heiner, S., Lukošiut e, K., Askell, A., Jones, A., Chen, A., et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., et al. Sparks of artiﬁcial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Buckingham, L. J. and Ashby, B. Coevolutionary theory of hosts and parasites. Journal of Evolutionary Biology, 35(2):205224, 2022. Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoﬀet, A., Joglekar, M., Leike, J., Sutskever, I., and Wu, J. Weak-to-strong generalization: eliciting strong capabilities with weak supervision. In ICML24, 2024. Byrne, R. M. Counterfactuals in explainable artiﬁcial intelligence (xai): Evidence from human reasoning. In IJCAI, pp. 62766282. California, CA, 2019. Cai, Z., Shabihi, S., An, B., Che, Z., Bartoldson, B. R., Kailkhura, B., Goldstein, T., and Huang, F. Aegisllm: Scaling agentic systems for self-reﬂective defense in llm security. arXiv preprint arXiv:2504.20965, 2025. Chao, P., Robey, A., Dobriban, E., Hassani, H., Pappas, G. J., and Wong, E. Jailbreaking black box large language models in twenty queries. In IEEE Conference on Secure and Trustworthy Machine Learning, SaTML 2025, Copenhagen, Denmark, April 9-11, 2025, pp. 2342. IEEE, 2025. doi: 10.1109/SATML64287.2025.00010. URL https://doi.org/10.1109/SaTML64287.2025.00010. Chen, M., Cao, Y., Zhang, Y., and Lu, C. Quantifying and mitigating unimodal biases in multimodal large language models: causal perspective. arXiv preprint arXiv:2403.18346, 2024a. Chen, M., Peng, B., Zhang, Y., and Lu, C. Cello: Causal evaluation of large vision-language models. arXiv preprint arXiv:2406.19131, 2024b. Chen, S., Yu, S., Zhao, S., and Lu, C. From imitation to introspection: Probing self-consciousness in language models. arXiv preprint arXiv:2410.18819, 2024c. Chen, S., Ma, S., Yu, S., Zhang, H., Zhao, S., and Lu, C. Exploring consciousness in llms: systematic survey of theories, implementations, and frontier risks. arXiv preprint arXiv:2505.19806, 2025a. Chen, Y., Benton, J., Radhakrishnan, A., Uesato, J., Denison, C., Schulman, J., Somani, A., Hase, P., Wagner, M., Roger, F., et al. Reasoning models dont always say what they think. arXiv preprint arXiv:2505.05410, 2025b. Chen, Z. and Liu, B. Lifelong machine learning. Morgan & Claypool Publishers, 2018. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. 23 R2AI: Towards Resistant and Resilient AI in an Evolving World Clymer, J., Duan, I., Cundy, C., Duan, Y., Heide, F., Lu, C., Mindermann, S., McGurk, C., Pan, X., Siddiqui, S., et al. Bare minimum mitigations for autonomous ai development. arXiv preprint arXiv:2504.15416, 2025. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Conmy, A., Mavor-Parker, A., Lynch, A., Heimersheim, S., and Garriga-Alonso, A. Towards automated circuit discovery for mechanistic interpretability. Advances in Neural Information Processing Systems, 36:1631816352, 2023. Cooper, M. D. and Alder, M. N. The evolution of adaptive immune systems. Cell, 124(4):815822, 2006. Dai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang, Y., and Yang, Y. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023. Dalrymple, D., Skalse, J., Bengio, Y., Russell, S., Tegmark, M., Seshia, S., Omohundro, S., Szegedy, C., Goldhaber, B., Ammann, N., et al. Towards guaranteed safe ai: framework for ensuring robust and reliable ai systems. arXiv preprint arXiv:2405.06624, 2024. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., and Li, S. S. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948. Eiben, A. E. and Smith, J. E. Introduction to evolutionary computing. Springer, 2015. Engels, J., Baek, D. D., Kantamneni, S., and Tegmark, M. Scaling laws for scalable oversight. arXiv preprint arXiv:2504.18530, 2025. Evans, J. S. B. and Stanovich, K. E. Dual-process theories of higher cognition: Advancing the debate. Perspectives on psychological science, 8(3):223241, 2013. Fang, J., Jiang, H., Wang, K., Ma, Y., Shi, J., Wang, X., He, X., and Chua, T.-S. Alphaedit: Null-space constrained model editing for language models. 2025. 24 R2AI: Towards Resistant and Resilient AI in an Evolving World Feng, J., Huang, S., Qu, X., Zhang, G., Qin, Y., Zhong, B., Jiang, C., Chi, J., and Zhong, W. Retool: Reinforcement learning for strategic tool use in llms. CoRR, abs/2504.11536, 2025. Flajnik, M. F. and Kasahara, M. Origin and evolution of the adaptive immune system: genetic events and selective pressures. Nature Reviews Genetics, 11(1):4759, 2010. Florensa, C., Held, D., Geng, X., and Abbeel, P. Automatic goal generation for reinforcement learning agents. In International conference on machine learning, pp. 15151528. PMLR, 2018. Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022. Gigerenzer, G. Gut feelings: The intelligence of the unconscious. Penguin, 2007. Goertzel, B. Artiﬁcial general intelligence: Concept, state of the art, and future prospects. Journal of Artiﬁcial General Intelligence, 5(1):1, 2014. Goh, J. Y., Khoo, S., Iskandar, N., Chua, G., Tan, L., and Foo, J. Measuring what matters: framework for evaluating safety risks in real-world llm applications, 2025. URL https://arxiv.org/abs/2507.09820. Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks, 2014. URL https://arxiv.org/abs/1406.2661. Greenblatt, R., Denison, C., Wright, B., Roger, F., MacDiarmid, M., Marks, S., Treutlein, J., Belonax, T., Chen, J., Duvenaud, D., Khan, A., Michael, J., Mindermann, S., Perez, E., Petrini, L., Uesato, J., Kaplan, J., Shlegeris, B., Bowman, S. R., and Hubinger, E. Alignment faking in large language models, 2024. URL https://arxiv.org/abs/2412.14093. Guan, M. Y., Joglekar, M., Wallace, E., Jain, S., Barak, B., Helyar, A., Dias, R., Vallone, A., Ren, H., Wei, J., Chung, H. W., Toyer, S., Heidecke, J., Beutel, A., and Glaese, A. Deliberative alignment: Reasoning enables safer language models. CoRR, abs/2412.16339, 2024a. Guan, M. Y., Joglekar, M., Wallace, E., Jain, S., Barak, B., Helyar, A., Dias, R., Vallone, A., Ren, H., Wei, J., Chung, H. W., Toyer, S., Heidecke, J., Beutel, A., and Glaese, A. Deliberative alignment: Reasoning enables safer language models. CoRR, abs/2412.16339, 2024b. Guiochet, J., Machin, M., and Waeselynck, H. Safety-critical advanced robots: survey. Robotics Auton. Syst., 94:4352, 2017. Gunderson, L. H. Ecological resiliencein theory and application. Annual review of ecology and systematics, 31(1):425439, 2000. Han, T., Kumar, A., Agarwal, C., and Lakkaraju, H. Medsafetybench: Evaluating and improving the medical safety of large language models. In NeurIPS, 2024. Hendrycks, D. Natural selection favors ais over humans. arXiv preprint arXiv:2303.16200, 2023. 25 R2AI: Towards Resistant and Resilient AI in an Evolving World Hendrycks, D., Carlini, N., Schulman, J., and Steinhardt, J. Unsolved problems in ml safety. arXiv preprint arXiv:2109.13916, 2021. Hendrycks, D., Mazeika, M., and Woodside, T. An overview of catastrophic ai risks. arXiv preprint arXiv:2306.12001, 2023. Hendrycks, D., Schmidt, E., and Wang, A. Superintelligence strategy: Expert version. arXiv preprint arXiv:2503.05628, 2025. Holland, J. H. Adaptation in natural and artiﬁcial systems: an introductory analysis with applications to biology, control, and artiﬁcial intelligence. MIT press, 1992. Holling, C. S. et al. Resilience and stability of ecological systems, 1973. Huang, Y., Gao, C., Zhou, Y., Guo, K., Wang, X., Cohen-Sasson, O., Lamparth, M., and Zhang, X. Position: We need an adaptive interpretation of helpful, honest, and harmless principles, 2025. URL https://arxiv.org/abs/2502.06059. Hui, Z., Dong, Y. R., Shareghi, E., and Collier, N. Trident: Benchmarking llm safety in ﬁnance, medicine, and law, 2025. URL https://arxiv.org/abs/2507.21134. IDAIS. Beijing statement on AI safety. https://idais.ai/dialogue/idais-beijing, 2024. IDAIS. Shanghai statement on AI safety. https://idais.ai/dialogue/idais-shanghai, 2025. Inan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., Tontchev, M., Hu, Q., Fuller, B., Testuggine, D., and Khabsa, M. Llama guard: Llm-based input-output safeguard for human-ai conversations. CoRR, abs/2312.06674, 2023. Jain, N., Schwarzschild, A., Wen, Y., Somepalli, G., Kirchenbauer, J., Chiang, P., Goldblum, M., Saha, A., Geiping, J., and Goldstein, T. Baseline defenses for adversarial attacks against aligned language models. CoRR, abs/2309.00614, 2023. Ji, J., Wang, K., Qiu, T., Chen, B., Zhou, J., Li, C., Lou, H., Dai, J., Liu, Y., and Yang, Y. Language models resist alignment: Evidence from data compression. arXiv preprint arXiv:2406.06144, 2024. Jiang, H., Fang, J., Zhang, N., Ma, G., Wan, M., Wang, X., He, X., and Chua, T. Anyedit: Edit any knowledge encoded in language models. ICML, 2025. Jin, B., Zeng, H., Yue, Z., Wang, D., Zamani, H., and Han, J. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. CoRR, abs/2503.09516, 2025. Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., Žídek, A., Potapenko, A., et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. Kahneman, D. Thinking, fast and slow. macmillan, 2011. R2AI: Towards Resistant and Resilient AI in an Evolving World Kamoi, R., Zhang, Y., Zhang, N., Das, S. S. S., and Zhang, R. Training step-level reasoning veriﬁers with formal veriﬁcation tools. arXiv preprint arXiv:2505.15960, 2025. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Karnofsky, H. If-then commitments for ai risk reduction. 2024. Kim, H., Yi, X., Yao, J., Lian, J., Huang, M., Duan, S., Bak, J., and Xie, X. The road to artiﬁcial superintelligence: comprehensive survey of superalignment. arXiv preprint arXiv:2412.16468, 2024. Kirichenko, P., Izmailov, P., and Wilson, A. G. Last layer re-training is suﬃcient for robustness to spurious correlations. In ICLR. OpenReview.net, 2023. Kitano, H. Biological robustness. Nature Reviews Genetics, 5(11):826837, 2004. Korbak, T., Balesni, M., Barnes, E., Bengio, Y., Benton, J., Bloom, J., Chen, M., Cooney, A., Dafoe, A., Dragan, A., et al. Chain of thought monitorability: new and fragile opportunity for ai safety. arXiv preprint arXiv:2507.11473, 2025. Kulveit, J., Douglas, R., Ammann, N., Turan, D., Krueger, D., and Duvenaud, D. Position: Humanity faces existential risk from gradual disempowerment. In Forty-second International Conference on Machine Learning Position Paper Track, 2025. Lai, J., Gan, W., Wu, J., Qi, Z., and Yu, P. S. Large language models in law: survey. AI Open, 5: 181196, 2024. Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. Le, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. In NeurIPS, 2022. Lee, H., Phatale, S., Mansoor, H., Mesnard, T., Ferret, J., Lu, K., Bishop, C., Hall, E., Carbune, V., Rastogi, A., and Prakash, S. RLAIF vs. RLHF: scaling reinforcement learning from human feedback with AI feedback. In ICML. OpenReview.net, 2024. Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. Scalable agent alignment via reward modeling: research direction. arXiv preprint arXiv:1811.07871, 2018. Levin, S. A. Ecosystems and the biosphere as complex adaptive systems. Ecosystems, 1(5):431436, 1998. Li, Y., Wang, S., Ding, H., and Chen, H. Large language models in ﬁnance: survey. In Proceedings of the fourth ACM international conference on AI in ﬁnance, pp. 374382, 2023. 27 R2AI: Towards Resistant and Resilient AI in an Evolving World Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Liu, C., Yuan, Y., Yin, Y., Xu, Y., Xu, X., Chen, Z., Wang, Y., Shang, L., Liu, Q., and Zhang, M. Safe: Enhancing mathematical reasoning in large language models via retrospective step-aware formal veriﬁcation. arXiv preprint arXiv:2506.04592, 2025a. Liu, Y., Gao, H., Zhai, S., Xia, J., Wu, T., Xue, Z., Chen, Y., Kawaguchi, K., Zhang, J., and Hooi, B. Guardreasoner: Towards reasoning-based LLM safeguards. CoRR, abs/2501.18492, 2025b. Liu, Y., Zhou, S., Lu, Y., Zhu, H., Wang, W., Lin, H., He, B., Han, X., and Sun, L. Auto-rt: Automatic jailbreak strategy exploration for red-teaming large language models. arXiv preprint arXiv:2501.01830, 2025c. Lu, C., Qian, C., Zheng, G., Fan, H., Gao, H., Zhang, J., Shao, J., Deng, J., Fu, J., Huang, K., et al. From gpt-4 to gemini and beyond: Assessing the landscape of mllms on generalizability, trustworthiness and causality through four modalities. arXiv preprint arXiv:2401.15071, 2024. Lynch, A., Wright, B., Larson, C., Troy, K. K., Ritchie, S. J., Mindermann, S., Perez, E., and Hubinger, E. Agentic misalignment: How llms could be an insider threat. Anthropic Research, 2025. https://www.anthropic.com/research/agentic-misalignment. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreﬀe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-reﬁne: Iterative reﬁnement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. Maini, P., Goyal, S., Sam, D., Robey, A., Savani, Y., Jiang, Y., Zou, A., Lipton, Z. C., and Kolter, J. Z. Safety pretraining: Toward the next generation of safe AI. CoRR, abs/2504.16980, 2025. Meng, K., Sharma, A. S., Andonian, A., Belinkov, Y., and Bau, D. Mass-editing memory in transformer. ICLR, 2022. Meng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with reference-free reward. In NeurIPS, 2024. Mökander, J., Schuett, J., Kirk, H. R., and Floridi, L. Auditing large language models: three-layered approach. AI and Ethics, 4(4):10851115, 2024. Morris, M. R., Sohl-Dickstein, J., Fiedel, N., Warkentin, T., Dafoe, A., Faust, A., Farabet, C., and Legg, S. Position: Levels of agi for operationalizing progress on the path to agi. In Forty-ﬁrst International Conference on Machine Learning, 2024. Mu, T., Helyar, A., Heidecke, J., Achiam, J., Vallone, A., Kivlichan, I., Lin, M., Beutel, A., Schulman, J., and Weng, L. Rule based rewards for language model safety. In NeurIPS, 2024. R2AI: Towards Resistant and Resilient AI in an Evolving World Müller, V., De Boer, R. J., Bonhoeﬀer, S., and Szathmáry, E. An evolutionary perspective on the systems of adaptive immunity. Biological Reviews, 93(1):505528, 2018. Murphy, K. and Weaver, C. Janeways immunobiology. Garland science, 2016. Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023. Nguyen, V. B., Youssef, P., Seifert, C., and Schlötterer, J. Llms for generating and evaluating counterfactuals: comprehensive study. arXiv preprint arXiv:2405.00722, 2024. Nick, B. Superintelligence: Paths, dangers, strategies. Oxford University Press, 2014. Nourmohammad, A., Otwinowski, J., and Plotkin, J. B. Host-pathogen coevolution and the emergence of broadly neutralizing antibodies in chronic infections. PLoS genetics, 12(7):e1006171, 2016. Novikov, A., Vu, N., Eisenberger, M., Dupont, E., Huang, P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J., Mehrabian, A., et al. Alphaevolve: coding agent for scientiﬁc and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. Oh, S., Jin, Y., Sharma, M., Kim, D., Ma, E., Verma, G., and Kumar, S. Uniguard: Towards universal safety guardrails for jailbreak attacks on multimodal large language models. arXiv preprint arXiv:2411.01703, 2024. OpenAI. Gpt-5 [large language model], 2025. URL https://openai.com/gpt-5. Model release date: August 7, 2025. Orseau, L. and Armstrong, M. Safely interruptible agents. In Conference on Uncertainty in Artiﬁcial Intelligence. Association for Uncertainty in Artiﬁcial Intelligence, 2016. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Pan, Z., Zhang, Y., Zhang, Y., Zhang, J., Luo, H., Han, Y., Wu, D., Chen, H.-Y., Yu, P. S., Li, M., et al. Evo-marl: Co-evolutionary multi-agent reinforcement learning for internalized safety. arXiv preprint arXiv:2508.03864, 2025. Panayides, A. S., Amini, A., Filipovic, N. D., Sharma, A., Tsaftaris, S. A., Young, A., Foran, D., Do, N., Golemati, S., Kurc, T., et al. Ai in medical imaging informatics: current challenges and future directions. IEEE journal of biomedical and health informatics, 24(7):18371857, 2020. Panickssery, N., Gabrieli, N., Schulz, J., Tong, M., Hubinger, E., and Turner, A. M. Steering llama 2 via contrastive activation addition. arXiv preprint arXiv:2312.06681, 2023. Papkou, A., Guzella, T., Yang, W., Koepper, S., Pees, B., Schalkowski, R., Barg, M.-C., Rosenstiel, P. C., Teotónio, H., and Schulenburg, H. The genomic basis of red queen dynamics during rapid reciprocal hostpathogen coevolution. Proceedings of the National Academy of Sciences, 116(3): 923928, 2019. 29 R2AI: Towards Resistant and Resilient AI in an Evolving World Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., and Wermter, S. Continual lifelong learning with neural networks: review. Neural networks, 113:5471, 2019. Pavlova, M., Brinkman, E., Iyer, K., Albiero, V., Bitton, J., Nguyen, H., Li, J., Ferrer, C. C., Evtimov, I., and Grattaﬁori, A. Automated red teaming with goat: the generative oﬀensive agent tester. arXiv preprint arXiv:2410.01606, 2024. Pearl, J. Causality 2002-2020 - introduction. In Probabilistic and Causal Inference, volume 36 of ACM Books, pp. 393398. 2022. Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A., McAleese, N., and Irving, G. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022. Perez, E., Ringer, S., Lukosiute, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., Kundu, S., Kadavath, S., et al. Discovering language model behaviors with model-written evaluations. In Findings of the association for computational linguistics: ACL 2023, pp. 1338713434, 2023. Qi, X., Panda, A., Lyu, K., Ma, X., Roy, S., Beirami, A., Mittal, P., and Henderson, P. Safety alignment should be made more than just few tokens deep. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=6Mxhg9PtDE. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. Rajpal, S. Guardrails ai. https://github.com/guardrails-ai/guardrails, 2023. Accessed: 2025-09-02. Raman, R., Kowalski, R., Achuthan, K., Iyer, A., and Nedungadi, P. Navigating artiﬁcial general intelligence development: societal, technological, ethical, and brain-inspired pathways. Scientiﬁc Reports, 15(1):122, 2025. Reason, J. The contribution of latent human failures to the breakdown of complex systems. Philosophical Transactions of the Royal Society of London. B, Biological Sciences, 327(1241): 475484, 1990. Reddy Chirra, S., Varakantham, P., and Paruchuri, P. Safety through feedback in constrained rl. Advances in Neural Information Processing Systems, 37:139938139967, 2024. Rowe, L., Girgis, R., Gosselin, A., Carrez, B., Golemo, F., Heide, F., Paull, L., and Pal, C. Ctrl-sim: Reactive and controllable driving agents with oﬄine reinforcement learning. In CoRL, pp. 36003621, 2024. Schölkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal, A., and Bengio, Y. Toward causal representation learning. Proceedings of the IEEE, 109(5):612634, 2021. Seshia, S. A., Sadigh, D., and Sastry, S. S. Toward veriﬁed artiﬁcial intelligence. Communications of the ACM, 65(7):4655, 2022. 30 R2AI: Towards Resistant and Resilient AI in an Evolving World Shah, R., Irpan, A., Turner, A. M., Wang, A., Conmy, A., Lindner, D., Brown-Cohen, J., Ho, L., Nanda, N., Popa, R. A., Jain, R., Greig, R., Albanie, S., Emmons, S., Farquhar, S., Krier, S., Rajamanoharan, S., Bridgers, S., Ijitoye, T., Everitt, T., Krakovna, V., Varma, V., Mikulik, V., Kenton, Z., Orr, D., Legg, S., Goodman, N. D., Dafoe, A., Flynn, F., and Dragan, A. D. An approach to technical AGI safety and security. CoRR, abs/2504.01849, 2025. Shanghai AI Lab. Frontier ai risk management framework in practice: risk analysis technical report. arXiv preprint arXiv:2507.16534, 2025a. Shanghai AI Lab. Safework-r1: Coevolving safety and intelligence under the ai-45 law. arXiv preprint arXiv:2507.18576, 2025b. Shanghai AI Lab & Concordia AI. Frontier ai risk management framework (v1.0), July 2025. URL https://research.ai45.shlab.org.cn/safework-f1-framework.pdf. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. Sharkey, L., Chughtai, B., Batson, J., Lindsey, J., Wu, J., Bushnaq, L., Goldowsky-Dill, N., Heimersheim, S., Ortega, A., Bloom, J., et al. Open problems in mechanistic interpretability. arXiv preprint arXiv:2501.16496, 2025. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reﬂexion: language agents with verbal reinforcement learning. In NeurIPS, 2023a. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reﬂexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36: 86348652, 2023b. Silver, D. and Sutton, R. S. Welcome to the era of experience. Google AI, 1, 2025. Simaan, M. and Cruz Jr, J. B. Additional aspects of the stackelberg strategy in nonzero-sum games. Journal of Optimization Theory and Applications, 11(6):613626, 1973a. Simaan, M. and Cruz Jr, J. B. On the stackelberg strategy in nonzero-sum games. Journal of Optimization Theory and Applications, 11(5):533555, 1973b. Slovic, P. Perception of risk. In The perception of risk, pp. 220231. Routledge, 2016. Souly, A., Lu, Q., Bowen, D., Trinh, T., Hsieh, E., Pandey, S., Abbeel, P., Svegliato, J., Emmons, S., Watkins, O., and Toyer, S. strongreject for empty jailbreaks. In NeurIPS, 2024. Summerﬁeld, C., Argyle, L., Bakker, M. A., Collins, T., Durmus, E., Eloundou, T., Gabriel, I., Ganguli, D., Hackenburg, K., Hadﬁeld, G. K., Hewitt, L., Huang, S., Landemore, H., Marchal, N., Ovadya, A., Procaccia, A. D., Risse, M., Schneier, B., Seger, E., Siddarth, D., Sætra, H. S., Tessler, M., and Botvinick, M. How will advanced AI systems impact democracy? CoRR, abs/2409.06729, 2024. 31 R2AI: Towards Resistant and Resilient AI in an Evolving World Sun, Y., Pargoo, N. S., Jin, P. J., and Ortiz, J. Optimizing autonomous driving for safety: human-centric approach with llm-enhanced RLHF. CoRR, abs/2406.04481, 2024. Sutton, R. S. and Barto, A. G. Reinforcement learning - an introduction, 2nd Edition. MIT Press, 2018. Szegedy, C. promising path towards autoformalization and general artiﬁcial intelligence. In International Conference on Intelligent Computer Mathematics, pp. 320. Springer, 2020. Terekhov, M., Liu, Z. N. D., Gulcehre, C., and Albanie, S. Control tax: The price of keeping ai in check. arXiv preprint arXiv:2506.05296, 2025. Tiwari, A., Dutertre, B., Jovanović, D., De Candia, T., Lincoln, P. D., Rushby, J., Sadigh, D., and Seshia, S. Safety envelope for security. In Proceedings of the 3rd international conference on High conﬁdence networked systems, pp. 8594, 2014. Van Der Vlist, F., Helmond, A., and Ferrari, F. Big ai: Cloud infrastructure dependence and the industrialisation of artiﬁcial intelligence. Big Data & Society, 11(1):20539517241232630, 2024. Vassev, E. Safe artiﬁcial intelligence and formal methods: (position paper). In International Symposium on Leveraging Applications of Formal Methods, pp. 704713. Springer, 2016. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wagner, A. Robustness and evolvability in living systems. 2013. Walker, B., Holling, C. S., Carpenter, S. R., and Kinzig, A. Resilience, adaptability and transformability in socialecological systems. Ecology and society, 9(2), 2004. Wang, H., Qin, Z., Zhao, Y., Du, C., Lin, M., Wang, X., and Pang, T. Lifelong safety alignment for language models. CoRR, abs/2505.20259, 2025a. Wang, J., Pun, A., Tu, J., Manivasagam, S., Sadat, A., Casas, S., Ren, M., and Urtasun, R. Advsim: Generating safety-critical scenarios for self-driving vehicles. In CVPR, pp. 99099918, 2021. Wang, M., la Tour, T. D., Watkins, O., Makelov, A., Chi, R. A., Miserendino, S., Heidecke, J., Patwardhan, T., and Mossing, D. Persona features control emergent misalignment, 2025b. URL https://arxiv.org/abs/2506.19823. Wei, A., Haghtalab, N., and Steinhardt, J. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36:8007980110, 2023. Wei, J. Asymmetry of veriﬁcation and veriﬁers law, 2025. URL https://www.jasonwei.net/blog/asymmetry-of-veriﬁcation-and-veriﬁers-law. Wu, A., Kuang, K., Zhu, M., Wang, Y., Zheng, Y., Han, K., Li, B., Chen, G., Wu, F., and Zhang, K. Causality for large language models. arXiv preprint arXiv:2410.15319, 2024a. 32 R2AI: Towards Resistant and Resilient AI in an Evolving World Wu, J., Xie, Y., Yang, Z., Wu, J., Gao, J., Ding, B., Wang, X., and He, X. β-dpo: Direct preference optimization with dynamic β. In NeurIPS, 2024b. Wu, T., Luo, L., Li, Y.-F., Pan, S., Vu, T.-T., and Haﬀari, G. Continual learning for large language models: survey, 2024c. URL https://arxiv.org/abs/2402.01364. Xu, Z., Jiang, F., Niu, L., Jia, J., Lin, B. Y., and Poovendran, R. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. In ACL (1), pp. 55875605, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report. CoRR, abs/2505.09388, 2025a. Yang, C., Lu, C., Wang, Y., and Zhou, B. Towards ai-45 law: roadmap to trustworthy agi. 2024. Yang, X., Deng, G., Shi, J., Zhang, T., and Dong, J. S. Enhancing model defense against jailbreaks with proactive safety reasoning. CoRR, abs/2501.19180, 2025b. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In ICLR. OpenReview.net, 2023. Yi, S., Liu, Y., Sun, Z., Cong, T., He, X., Song, J., Xu, K., and Li, Q. Jailbreak attacks and defenses against large language models: survey. arXiv preprint arXiv:2407.04295, 2024. Yu, S. and Lu, C. Adam: An embodied causal agent in open-world environments. arXiv preprint arXiv:2410.22194, 2024. Zhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., and Jordan, M. Theoretically principled trade-oﬀ between robustness and accuracy. In International conference on machine learning, pp. 74727482. PMLR, 2019. Zhang, X., Wang, L., Helwig, J., Luo, Y., Fu, C., Xie, Y., Liu, M., Lin, Y., Xu, Z., Yan, K., et al. Artiﬁcial intelligence for science in quantum, atomistic, and continuum systems. arXiv preprint arXiv:2307.08423, 2023. Zhang, Y., Chi, J., Nguyen, H., Upasani, K., Bikel, D. M., Weston, J. E., and Smith, E. M. Backtracking improves generation safety. In ICLR, 2025a. Zhang, Y., Zhang, A., Zhang, X., Sheng, L., Chen, Y., Liang, Z., and Wang, X. Alphaalign: Incentivizing safety alignment with extremely simpliﬁed reinforcement learning. 2025b. URL https://arxiv.org/abs/2507.14987. Zhu, J., Yan, L., Wang, S., Yin, D., and Sha, L. Reasoning-to-defend: Safety-aware reasoning can defend large language models from jailbreaking. CoRR, abs/2502.12970, 2025. R2AI: Towards Resistant and Resilient AI in an Evolving World Zou, A., Wang, Z., Carlini, N., Nasr, M., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models, 2023. URL https://arxiv. org/abs/2307.15043, 19: 3, 2023a. Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. CoRR, abs/2307.15043, 2023b. doi: 10.48550/ARXIV.2307.15043. URL https://doi.org/10.48550/arXiv.2307.15043. Zou, A., Phan, L., Wang, J., Duenas, D., Lin, M., Andriushchenko, M., Kolter, J. Z., Fredrikson, M., and Hendrycks, D. Improving alignment and robustness with circuit breakers. In NeurIPS, 2024. Zou, Q., Xiao, J., Li, Q., Yan, Z., Wang, Y., Xu, L., Wang, W., Gao, K., Li, R., and Jiang, Y. Queryattack: Jailbreaking aligned large language models using structured non-natural query language. In ACL (Findings), pp. 57255741. Association for Computational Linguistics, 2025."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Tsinghua University",
        "University of Science and Technology of China"
    ]
}