{
    "paper_title": "Ingredients: Blending Custom Photos with Video Diffusion Transformers",
    "authors": [
        "Zhengcong Fei",
        "Debang Li",
        "Di Qiu",
        "Changqian Yu",
        "Mingyuan Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents a powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as \\texttt{Ingredients}. Generally, our method consists of three primary modules: (\\textbf{i}) a facial extractor that captures versatile and precise facial features for each human ID from both global and local perspectives; (\\textbf{ii}) a multi-scale projector that maps face embeddings into the contextual space of image query in video diffusion transformers; (\\textbf{iii}) an ID router that dynamically combines and allocates multiple ID embedding to the corresponding space-time regions. Leveraging a meticulously curated text-video dataset and a multi-stage training protocol, \\texttt{Ingredients} demonstrates superior performance in turning custom photos into dynamic and personalized video content. Qualitative evaluations highlight the advantages of proposed method, positioning it as a significant advancement toward more effective generative video control tools in Transformer-based architecture, compared to existing methods. The data, code, and model weights are publicly available at: \\url{https://github.com/feizc/Ingredients}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 0 9 7 1 0 . 1 0 5 2 : r Ingredients: Blending Custom Photos with Video Diffusion Transformers Zhengcong Fei, Debang Li, Di Qiu Changqian Yu, Mingyuan Fan Kunlun Inc. Beijing, China {feizhengcong}@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "This paper presents powerful framework to customize video creations by incorporating multiple specific identity (ID) photos, with video diffusion Transformers, referred to as Ingredients. Generally, our method consists of three primary modules: (i) facial extractor that captures versatile and precise facial features for each human ID from both global and local perspectives; (ii) multi-scale projector that maps face embeddings into the contextual space of image query in video diffusion transformers; (iii) an ID router that dynamically combines and allocates multiple ID embedding to the corresponding space-time regions. Leveraging meticulously curated text-video dataset and multi-stage training protocol, Ingredients demonstrates superior performance in turning custom photos into dynamic and personalized video content. Qualitative evaluations highlight the advantages of proposed method, positioning it as significant advancement toward more effective generative video control tools in Transformer-based architecture, compared to existing methods. The data, code, and model weights are publicly available at: https://github.com/feizc/Ingredients."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large-scale pre-trained video diffusion models have reformed the field of textto-video synthesis [36, 4, 66, 28], especially the scaling property of diffusion transformers architecture [51, 42, 99, 100, 87, 53, 80], enabling wide range of downstream applications [95, 96, 65, 91, 71, 92]. These controllable scenes are particularly impactful in the area of customization [79, 78, 81, 7, 48, 13], with notable emphasis on multi-human personalization, due to its vast potential in domains like AI-generated portraits, video animation, and multi-scene storyboard [84]. The primary aim to produce video contents that preserve consistent facial identity (ID) across multiple reference images, while simultaneously incorporating additional user-defined prompts. However, existing personalization approaches encounter several challenges. For training-based methods, overhead associated with case-by-case fine-tuning limits their scalability and broader applicability [61, 26]. For training-free methods, they are predominantly based on U-Net [58, 82, 79, 38] and cannot be adapted to the emerging DiT-based video generative model [99, 100, 85, 87, 80]. Among the few exceptions, ConsisID [94] can integrate with video diffusion transformers, but it is restricted to single-human customization. This challenge is likely due to the fundamental differences between DiT and U-Net [57, 1, 2, 72, 93, 24, 23], such as the greater difficulty in training convergence and weakness in perceiving facial details. Notably, commercial video generation models like Keling and Vido have made significant strides in maintaining multi-ID consistency. Our work aims to bridge Corresponding author Preprint. Under review. Figure 1: Examples of multi-ID customized video results from our proposed Ingredients. Given reference with multiple human image set, our method can generate realistic and personalized videos while preserving specific human identity consistent. this gap within the open-source community, striving for similar advances in multi-ID customization for video diffusion. In this work, we introduce solution for multi-ID video customization with video diffusion transformers, termed as Ingredients, aims to achieve high-fidelity identity preservation, enhanced content flexibility, and natural video generation. Generally, Ingredients comprises three key components: (i) facial extractor that extracts versatile editable facial features for each ID from global and local perspectives; (ii) multi-scale projector that projects embeddings to the context space of image query;(iii) an ID router that dynamically allocates and integrates the ID embeddings across their respective regions. The multi-stage training process sequentially optimizes the ID embedding and ID router components. As result, our method enables the customization of multiple IDs without prompt constraints, offering great adaptability and precision in video synthesis. Extensive experiments validate the superiority of our proposed method over existing techniques, highlighting its effectiveness as generative control tool. More notably, our approach offers the flexibility to customize every aspect of the video creation process, making it well-suited for diverse applications such as personal storytelling, promotional videos, and creative projects. It ensures that each generated video is uniquely tailored, aligned with the users vision, and retains personalized touch. The contributions are summarized as below: We propose Ingredients, training-free multi-ID customization framework based on video diffusion transformers, that ensures the preservation of multiple IDs in generated videos while supporting precise textual control signals; We present routing mechanism that combines and distributes multiple ID embeddings in the context space of image query dynamically. Supervised with classification loss, it avoids blending of various identities and preserving individuality; 2 We design multi-stage training process that optimizes face embedding extraction and multiID routing in sequence, resulting in enhanced facial fidelity and improved controllability of generated visual content; Extensive experiments demonstrate the proposed framework can produce high-quality, editable, and consistent multi-human customized videos qualitatively and quantitatively, showing superiority performance with baselines. To support continued exploration, we have made our data, models, and training configurations publicly available."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Text-to-Video Generation Conditional video generation has undergone substantial advancements, transitioning from initial methodologies based on GANs and VAEs to more sophisticated frameworks such as diffusion models. Early GAN-based methods [75, 63, 73, 10, 90] encountered challenges related to temporal coherence, leading to inconsistencies between consecutive frames. To address these issues, video diffusion models incorporating U-Net architecturesoriginally developed for text-to-image generationhave been adapted, improving frame continuity. In recent developments, diffusion Transformers [51, 23, 20, 19] and their variants [2, 21] have replaced the traditional convolutional U-Net backbone [60] with pure Transformer architectures. They usually [51, 46, 47, 27] use spatio-temporal attention and full 3d attention, which enhanced the ability to capture the intricate dynamics of video and ensure consistent frame transitions [32, 5, 8, 28]. This paradigm shift has led to notable improvements in scalability and simplified parameter expansion. Meantime, auto-regressive models [86, 36, 74, 40, 83, 45, 22, 14, 15] have also employed with discrete tokens to effectively model temporal dependencies, particularly excelling in the generation of long-form videos [89, 76, 98, 33, 70, 101]. Our study explores the controllable application of video diffusion Transformers, with focus on multi-ID customization. 2.2 ID-Preserving Video Generation Large-scale, pre-trained text-to-video models [11, 12] have leveraged advanced text embeddings [55, 56] and sophisticated attention mechanisms, thereby advancing research in fine-grained control over the video generation process [43, 50, 3, 54, 52, 25, 18]. In the context of ID preservation, tuning-based methods requires fine-tuning pre-trained model for each new individual during inference [61, 37, 26, 17, 16]. Although subsequent developments have led to the introduction of both image and video models based on U-Net and DiT architectures [62, 41, 79, 78, 81, 7, 31, 13], the cost to continual fine-tune for each new identity restricts their scalability and practical applicability. To address the issue of high resource consumption, several tuning-free methods have recently emerged in the field of image generation [88, 77, 30, 29, 44]. However, in the domain of video, only ConsisID [94] currently support ID-preserving text-to-video generation with DiT. Note that Keling, Vido, and MovieGen [53] are closed-source, whereas ConsisID is open-source but only support single human customization. Here we continued select the advanced video diffusion transformers architecture [87] and optimize it for multi-ID scenarios, enables enabling the generation of high-quality, editable, and consistent identity-preserving videos."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Preliminaries Diffusion Model. Text-to-video generative models usually adopt the diffusion paradigm, which incrementally transforms noise ϵ into video x0. Early approaches performed the denoising process directly within the pixel space [68, 35, 67]. However, due to the substantial computational demands, more recent techniques predominantly leverage latent spaces [59, 40, 95, 87, 80] with 3D VAE. The optimization objective can be defined as: Ldif = Ex0,t,y,ϵ (cid:104) ϵ ϵθ (E (x0) , t, τθ(y))2 2 (cid:105) , (1) where denotes textual prompt, ϵ is randomly sampled from Gaussian distribution, e.g., ϵ (0, 1), and τθ() is the text encoder, e.g., T5 [56]. By substituting x0 with its latent representation (x0), form the diffusion process, which is also used in our method. 3 Figure 2: Overview of Ingredients framework. The proposed method consists of three key modules: facial extractor, q-former-based projector, and an ID router. The facial extractor collects versatile editable facial features with decoupling strategy for each ID. The q-former projector map multi-scale facial embedding into different layers of video diffusion transformers. The ID router combines and distributes ID embeddings to their respective locations adaptively without the intervention for prompts and layouts. The entire training process of the framework is curated into two stages, i.e., the facial embedding alignment stage and the router fine-tuning stage. Diffusion Transformers. Video generation models based on DiT demonstrates considerable promise in modeling the dynamics of the physical world [6, 87, 100]. Although this architecture represents recent innovation by scaling with 1D sequence, research on controllable remains under-explored. Existing techniques [97, 29, 9, 53] primarily mirror the design principles of U-Netbased models [96, 49, 26]. Notably, no prior study has investigated the mechanisms underlying the efficacy of DiT in scenarios of multi-ID customization. Conversely, commercial models have attained highly impressive results. This work aims to bridge the disparity between open-source solutions and their proprietary counterparts. 3.2 Key Modules The proposed Ingredients, which is capable of generating identity-preserving videos given multiple reference images, mainly consists of three principal modules: facial extractor, and q-former multiscale projector, and an ID router, as illustrated in Figure 2. Facial Extractor. The facial extractor is meticulously designed to encode high-fidelity, editable facial identity information of given images, enabling the video diffusion transformers to produce identity-consistent and controllable video outputs. In alignment with [29, 94], the module employs both global and local perspectives to derive facial features: The global facial embedding process begins with face detection, where individual face regions with boxes are extracted from the image set and assembled into single large composite image. To mitigate distortions caused by resizing or the loss of facial details from cropping, padding operation using white pixels ensures that the final composite adheres to the required dimensions. This synthesized image, containing multiple identities, is subsequently input into VAE to extract shallow feature representations. For local facial embeddings, face recognition backbone is utilized to extract features that robustly represent intrinsic identity attributes. Simultaneously, CLIP [55] image encoder is employed to capture semantically rich features. It is important to note that each identity within the image set retains independent features, i.e., for two reference images, we maintain two sets of local facial embeddings. Figure 3: Qualitative comparison of different personalization methods on multi-ID video customization. It can been seen that compared with training-based customization, i.e., textual inversion, our method can clearly routing and attention the respect regions, benefits to ID consistency as well as strong prompt following. Multi-scale Projector. To effectively map each facial embedding into the context space of image queries from the video diffusion transformers, we address two scenarios: For global facial embedding, which consolidates all identities into single composite image following VAE processing, is directly concatenated with the latent noise input; For each local facial embedding set n, = 1, . . . , , and is the number human ID, multi-step fusion is adopted. Initially, multi-scale features extracted from the facial recognition backbone are concatenated with the corresponding CLIP-derived features. Subsequently, Q-former structure [29] employing cross-attention is utilized to enable interaction between these features and the visual tokens from the video diffusion transformers attention blocks. This process can be mathematically expressed as: where Qimg = nW from i-th attention block, and Wq, Wk, Wv are trainable parameters. = img , img i , F = Cross-Attention(Qimg , , ace = nW , ace ), (2) is the queried visual token ID Router. Through the projector, we derive sequence of accurate facial embeddings for each ID. These embeddings are inherently position-independent. To prevent the blending of identities within the latent features, position-wise routing network is employed to assign unique identity to each potential facial region. Let there be distinct human IDs in given image sets, with each ID facial embedding denoted as n. For every tempera-spatial position (x, y, t) at frame within the visual token Rchwt in the latent space, we route and assign unique identity k(0 1) to it with determined as: = ArgmaxkSoftmax(ϕ(H, )), n=1, ϕ(H, ) = [f (H) g(Waggr n)]N (3) 5 Figure 4: Additional bad examples of multi-human customization. Our Ingredients involves failures that generated characters appearing as though they were directly copied-pasted and outpainting, leading to an inconsistent video scenes. where ϕ represent router network that output routing logits as -dimensional discrete probability as RN hwt. Waggr is learnable weight that aggregate multiple learnable latents from Q-former into single token, () and g() are two-layer MLP networks, is matrix multiplication operator. The idea behind the routing mechanism is that each patch (x, y) frame in video is associated with at most one ID feature. We then gather the id feature set = { 1, . . . , n} according to index at each position to get the final mask ID feature , whose size is identical to queried visual tokens img. Finally, we obtain the refined visual tokens with residul connection as: = + αm , (4) where αm denote face scale to control facial feature influence. 3.3 Training The training procedure is systematically divided into two distinct stages: facial embedding alignment phase and routing fine-tuning phase. During training, random frame is selected from the training dataset that contains multi-ID videos, and Crop-and-Align operations are applied to isolate the facial regions corresponding to each identity, which then serve as reference control signals. Facial Embedding Alignment. This phase focus on the face extractor and multi-scale projector. All inputs for the facial embedding are harnessed from the given reference image. Following previous study [94, 30], we also employ dropping regularization, i.e., uniformly select global, local and completed facial information for video diffusion transformer condition. To enhance the integration of facial features, Low-Rank Adaptation (LoRA) [37] is appended to the video DiT architecture as additional trainable module. The training loss is aligned with conventional diffusion loss, as expressed in Equation 1. Router Fine-Tuning. After completing facial embedding alignment, fine-tuning of ID router is conducted. We fix all the parameters except routing network as well as LoRA of DiT. To supervise the correct routing, we assume video sequence contains distinct ID at each tempera-spacial patch. We first convert videos into masks M, where -1 represents the background and denoted the corresponding ID region with face detection. The mask matrix is then down-sampled to match the size of video latents using F.interpolate with trilinear mode. The routing regularization can be optimized with multi-label cross-entropy loss as: Lroute = Ldif λ 1 (cid:88) Mnlog(ϕ(H, )n), (5) Table 1: Compare between our method and baseline approaches on multiple human video generation. Cogvideox served as the text-only baseline without any video conditioning while Inversion is tuning-based textual inversion."
        },
        {
            "title": "Methods\nCogvideoX\nInversion\nIngredients",
            "content": "Face Sim. (%) 2.8 35.6 77.1 CLIPScore (%) 28.3 24.3 26.7 FID - 154.2 106.3 where λ is the weight factor that balances routing loss with diffusion loss Ldif , ϕ((ϕ(H, ) is the routing logits RN hwt capturing probabilities across all position in latent space. The mask R1hwt serves as the ground truth for facial region labels, which is first used face detection to find boxes and then segmented with SAM [39]. Notably, indices of -1 for background class are excluded, ensuring the router focuses only on facial regions."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Datasets. We utilize an in-house multi-human text-video dataset for training, which is curated from publicly available online resources and annotated with Qwen2-VL2 for detailed captioning. To mitigate the risk of domain bias, we ensure balanced of topics throughout the dataset. We use facexlib3 to filter out and retain videos that contains human face number larger than two. As there is an absence of multi-ID evaluation dataset, we select 50 images from diverse areas and racial categories, all excluded from the training corpus. We then design 60 distinct prompts, encompassing variety of expressions, actions, and backgrounds. The image domains tree diagram and prompt that used to generate textual signals with GPT-4o are shown in Appendix. We also release all evaluation code as well as data in Huggingface for results reproduction and comparison. Implementation Details. We select video diffusion transformers architecture CogvideoX-5B [87] as baseline for performance validation. During training, video frames are processed at resolution of 480720 , with 49 sequential frames extracted per video at an interval of two frames. The batch size is configured to 64, with learning rate of 2e-5 at facial embedding alignment and 3e-5 at router fine-tuning. The total number training steps involves to 3k and 2k, respectively. AdamW is used as optimizer and cosine with restarts as scheduler. We set the face scale αm to 1.0. During inference, we employ DPM [35] with total of 50 sampling steps, and text-guidance ratio of 6.0. Evaluation Metrics. Following [30, 94], we assess the video customization across four dimensions: (i) Identity preservation: calculate pairwise face similarities between reference and generated faces with FaceNet [64]. Here, all detected faces in the generated frames are compared against reference faces through greedy matching algorithm. The minimum similarity score across all matches is reported. (ii) Visual quality: we measure FID [34] by calculating feature differences in the face regions between generated frames and real face images within the InceptionV3 [69] features spaces. (iii) Text relevance: we assess with the average CLIP-B4 image-text cosine similarity. 4.2 Main Results We begin by assessing the general performance of multi-ID video generation with baselines. We examine the quality by using the 100 test cases from in-house test sets that randomly select two conformed texts for each images from evaluation sets. Table 1 provides quantitative comparison of our proposed Ingredients approach against competitors. We can see that optimization-based techniques, that employing LoRA fine-tuning for each image pair [26], often struggle to preserve individual identities, frequently producing generic or blended representations of the reference subjects. 2https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct 3https://github.com/xinntao/facexlib 4https://huggingface.co/openai/clip-vit-base-patch32 7 Table 2: Ablation studies for components in ID embedding and control signals. Combine all of I2V initialization, segment supervision and spacal concatenation before VAE provide best generative performance with multi-ID consistency. Initialize Router Supervised T2V I2V Box Seg. VAE Features After Before Metrics Face Sim. (%) 58.1 65.5 74.3 77.1 CLIP-T (%) 26.5 25.9 26.7 26.7 FID 122.5 119.2 110.4 106. Table 3: Effect of routing loss. Equipped with routing loss of ID classification helps to build multi-ID consistent generation. Methods w/o Lroute w/ Lmse w/ Lroute Face Sim. (%) 62.2 72.5 77.1 CLIPScore (%) 26.9 26.1 26.7 FID 112.3 109.5 106.3 In contrast, the training-free Ingredients method demonstrates superior identity preservation, maintaining the distinct characteristics of each subject and achieving higher automatic scores. Moreover, our method exhibits prompt consistency comparable to text-only baselines, indicating that the personalization process does not compromise the original following capabilities of system. Qualitative results, presented in Figure 3, further corroborate these findings. Additionally, Figure 4 highlights common failure cases observed in multi-ID video generation with varying prompts. The first category of failure involves generated characters appearing as though they were directly copied and pasted, likely due to limitations in the image-to-video pattern initialization. The second category includes instances where the generated characters do not closely resemble the reference images, potentially caused by routing mis-classification. Addressing these issues remains an area for future research. 4.3 Ablation Study Effect of Model Initialize. There are usually two modes under general conditions, text-to-video (T2V) or image-to-video (I2V), for video generation models. Here, we evaluate both initialization methods while keeping the model architecture, learning parameters, and training steps consistent across experiments. The results, presented in Table 2, reveal that initialization with image conditions yields notably higher facial similarity in the generated outputs compared to text-based initialization, as anticipated. However, the performance of both approaches is nearly identical with respect to adherence to textual prompts. Therefore, we use I2V as initialization be default. Influence of Router Supervised Signals. To enable the ID router to accurately select relevant patch regions, we supervised its outputs with predefined signals. Specifically, we incorporate two types of signals: bounding boxes generated by face detection model and segmented regions produced by the SAM model [39]. detailed comparison of the hyperparameters used for face detection is provided in the Appendix. The evaluated results, summarized in Table 2, indicate that signals derived from SAM yielded higher facial similarity scores. This improvement can be attributed to SAMs ability to better focus on facial regions while minimizing noise from the background, hair, and clothing. How to Combine Global Embedding in VAE. In addition to integrating local features via routing, it is crucial to address the fusion of global features. We investigate two strategies for combining multiple reference images at the VAE level. Specifically, we examine whether the reference images should first be concatenated in pixel space and then processed by the VAE, or if the VAE should first process the images respectively and concatenate the features at latent space. The results are presented in Table 2. Our findings demonstrate that concatenating the images before passing them through the VAE effectively enhances the naturalness of the generated results and improves the ability to following the textual input. Figure 5: Visualization of routing map within each cross-attention layer of video diffusion transformers. We can see that with the routing loss, the routing network can discern different human IDs at earlier timesetps and in more pronounced manner. Effect of Routing Loss. We finally test the routing loss design, including the omission routing loss and replace classification with MSE loss. Table 3 provides verification of the efficacy of the routing loss. The results suggest that classification loss can substantially enhance ID similarity while concurrently maintain prompt consistency in multi-human customization. 4.4 Visualization Figure 5 illustrates the routing map for each cross-attention layer of Ingredients at different diffusion steps. In the visualization, white and black pixels represent two distinct human IDs. To enhance the clarity of the routing patterns, we upsample the argmax routing logits from the latent space. Every fourth frame is selected from the latent space, and every eighth cross-attention layer is displayed across varying denoising time steps. The analysis reveals two key observations: (i) With the incorporation of routing loss, ID router can discern different human IDs at earlier timesteps, achieving increasingly refined distinctions at later denoising steps. That is, optimize the video generation in fine-grained manner. (ii) At shallow cross-attention layers, the routing is chaotic and random. As the layers deepen, the routing pattern becomes more structured, eventually focusing on specific regions or semantic parts of the image. This progression aligns with the underlying principles of diffusion transformers, where the model gradually refines its visual representation over multiple steps."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Ingredients, framework developed for multi-human customization based on video diffusion transformers. Ingredients incorporates facial extractor that extract high-fidelity identity 9 embedding for each ID, multi-scale projector that combine facial embedding into context space of image query and an ID router to distribute and avoid identity blending. The empirical results demonstrate that Ingredients can deliver synthesis that is not only of high quality and diversity but also offers robust editability and strong identity fidelity. We anticipate that our method will establish new benchmark in the field, providing replicable framework that can be adopted, extended, and optimized by future research efforts."
        },
        {
            "title": "References",
            "content": "[1] Dosovitskiy Alexey. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.11929, 2020. [2] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, pages 2266922679, 2023. [3] Yuxiang Bao, Di Qiu, Guoliang Kang, Baochang Zhang, Bo Jin, Kaiye Wang, and Pengfei Yan. Latentwarp: Consistent diffusion latents for zero-shot video-to-video translation. arXiv preprint arXiv:2311.00353, 2023. [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. In openai, 2024. [7] Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, and Inbar Mosseri. Still-moving: Customized video generation without customized video data. arXiv preprint arXiv:2407.08674, 2024. [8] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. [9] Soon Yau Cheong, Duygu Ceylan, Armin Mustafa, Andrew Gilbert, and Chun-Hao Paul Huang. Boosting camera motion control for video diffusion transformers. arXiv preprint arXiv:2410.10802, 2024. [10] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. arXiv preprint arXiv:1907.06571, 2019. [11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [13] Haopeng Fang, Di Qiu, Binjie Mao, Pengfei Yan, and He Tang. Motioncharacter: Identitypreserving and motion controllable human video generation. arXiv preprint arXiv:2411.18281, 2024. 10 [14] Zheng-cong Fei. Fast image caption generation with position alignment. arXiv preprint arXiv:1912.06365, 2019. [15] Zhengcong Fei. Partially non-autoregressive image captioning. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 13091316, 2021. [16] Zhengcong Fei, Xu Yan, Shuhui Wang, and Qi Tian. Deecap: Dynamic early exiting for efficient image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1221612226, 2022. [17] Zhengcong Fei, Mingyuan Fan, and Junshi Huang. Gradient-free textual inversion. In Proceedings of the 31st ACM International Conference on Multimedia, pages 13641373, 2023. [18] Zhengcong Fei, Mingyuan Fan, and Junshi Huang. A-jepa: Joint-embedding predictive architecture can listen. arXiv preprint arXiv:2311.15830, 2023. [19] Zhengcong Fei, Mingyuan Fan, Li Zhu, Junshi Huang, Xiaoming Wei, and Xiaolin Wei. Masked auto-encoders meet generative adversarial networks and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2444924459, 2023. [20] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Flux that plays music. arXiv preprint arXiv:2409.00587, 2024. [21] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [22] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [23] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformers to 16 billion parameters. arXiv preprint arXiv:2407.11633, 2024. [24] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, Youqiang Zhang, and Junshi Huang. Dimba: Transformer-mamba diffusion models. arXiv preprint arXiv:2406.01159, 2024. [25] Zhengcong Fei, Di Qiu, Changqian Yu, Debang Li, Mingyuan Fan, and Xiang Wen. Video diffusion transformers are in-context learners. arXiv preprint arXiv:2412.10783, 2024. [26] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [27] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. [28] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. [29] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, and Qian He. Pulid: Pure and lightning id customization via contrastive alignment. arXiv preprint arXiv:2404.16022, 2024. [30] Junjie He, Yifeng Geng, and Liefeng Bo. Uniportrait: unified framework for identitypreserving single-and multi-human image personalization. arXiv preprint arXiv:2408.05939, 2024. [31] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 11 [32] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022. [33] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. [34] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, Jan 2017. [35] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. [36] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [37] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [38] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. In CVPR, pages 66896700, 2024. [39] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. [40] Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. [41] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multiconcept customization of text-to-image diffusion. In CVPR, pages 19311941, 2023. [42] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, April 2024. URL https://doi.org/ 10.5281/zenodo.10948109. [43] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving conditional controls with efficient consistency feedback. In European Conference on Computer Vision, pages 129147. Springer, 2025. [44] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In CVPR, pages 86408650, 2024. [45] Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Pérez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. [46] Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding. Vdt: General-purpose video diffusion transformers via mask modeling. arXiv preprint arXiv:2305.13311, 2023. [47] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 12 [48] Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. arXiv preprint arXiv:2402.09368, 2024. [49] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. [50] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 42964304, 2024. [51] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [52] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. [53] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [54] Di Qiu, Zheng Chen, Rui Wang, Mingyuan Fan, Changqian Yu, Junshi Huan, and Xiang Wen. Moviecharacter: tuning-free framework for controllable character video synthesis. arXiv preprint arXiv:2410.20974, 2024. [55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [56] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, 2019. [57] Chen Rao, Guangyuan Li, Zehua Lan, Jiakai Sun, Junsheng Luan, Wei Xing, Lei Zhao, Huaizhong Lin, Jianfeng Dong, and Dalong Zhang. Rethinking video deblurring with waveletaware dynamic transformer and diffusion model. arXiv preprint arXiv:2408.13459, 2024. [58] Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava. Customize-a-video: One-shot motion customization of text-to-video diffusion models. arXiv preprint arXiv:2402.14780, 2024. [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [60] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [61] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. [62] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In CVPR, pages 65276536, 2024. [63] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In Proceedings of the IEEE international conference on computer vision, pages 28302839, 2017. 13 [64] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815823, 2015. [65] Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent YF Tan, and Jiashi Feng. Instadrag: Lightning fast and accurate drag-based image editing emerging from videos. arXiv preprint arXiv:2405.13722, 2024. [66] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [67] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, pages 22562265. PMLR, 2015. [68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv, Oct 2020. [69] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, pages 28182826, 2016. [70] Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang. Video-infinity: Distributed long video generation. arXiv preprint arXiv:2406.16260, 2024. [71] Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng, Yatian Pang, Bin Lin, and Li Yuan. Cycle3d: High-quality and consistent image-to-3d generation via generationreconstruction cycle. arXiv preprint arXiv:2407.19548, 2024. [72] Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, and Yunhe Wang. U-dits: Downsample tokens in u-shaped diffusion transformers. arXiv preprint arXiv:2405.02730, 2024. [73] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 15261535, 2018. [74] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual descriptions. In International Conference on Learning Representations, 2022. [75] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. Advances in neural information processing systems, 29, 2016. [76] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. [77] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [78] Zhao Wang, Aoxue Li, Enze Xie, Lingting Zhu, Yong Guo, Qi Dou, and Zhenguo Li. Customvideo: Customizing text-to-video generation with multiple subjects. arXiv preprint arXiv:2401.09962, 2024. [79] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. In CVPR, pages 65376549, 2024. [80] Qi Tian Weijie Kong and so on. Hunyuanvideo: systematic framework for large video generative models, 2024. URL https://arxiv.org/abs/2412.03603. [81] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. arXiv preprint arXiv:2406.17758, 2024. 14 [82] Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. Customcrafter: Customized video generation with preserving motion and concept composition abilities. arXiv preprint arXiv:2408.13239, 2024. [83] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One arXiv preprint single transformer to unify multimodal understanding and generation. arXiv:2408.12528, 2024. [84] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 57(2):142, 2024. [85] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. [86] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. [87] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [88] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [89] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely long video generation. arXiv preprint arXiv:2303.12346, 2023. [90] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571, 2022. [91] Wangbo Yu, Chaoran Feng, Jiye Tang, Xu Jia, Li Yuan, and Yonghong Tian. Evagaussians: Event stream assisted gaussian splatting from blurry images. arXiv preprint arXiv:2405.20224, 2024. [92] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. [93] Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and Jiebo Luo. Promptfix: You prompt and we fix the photo. arXiv preprint arXiv:2405.16785, 2024. [94] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identity-preserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. [95] Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, and Jiebo Luo. Magictime: Time-lapse video generation models as metamorphic simulators. arXiv preprint arXiv:2404.05014, 2024. [96] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38363847, 2023. [97] Zhenghao Zhang, Junchao Liao, Menghao Li, Long Qin, and Weizhi Wang. Tora: Trajectoryoriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. [98] Canyu Zhao, Mingyu Liu, Wen Wang, Jianlong Yuan, Hao Chen, Bo Zhang, and Chunhua Shen. Moviedreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint arXiv:2407.16655, 2024. 15 [99] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. URL https://github.com/hpcaitech/Open-Sora. [100] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model. arXiv preprint arXiv:2410.15458, 2024. [101] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. arXiv preprint arXiv:2405.01434, 2024. Figure 6: The curve of different training loss in router fine-tuning stage. We can see that with training steps increases, routing loss significantly decreases, the router becomes more accurate, while the diffusion loss remains almost unchanged, maintaining the original generative performance. Figure 7: Hyper-parameter settings for SAM segmentaion. We select -2.0 as threshold to build routing supervised labels. Figure 8: Domain distribution of evaluation images (left) and used prompt to generate text inputs (right). We consider multiple aspects for data collection to make evaluation more robust."
        }
    ],
    "affiliations": [
        "Kunlun Inc. Beijing, China"
    ]
}