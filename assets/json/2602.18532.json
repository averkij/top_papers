{
    "paper_title": "VLANeXt: Recipes for Building Strong VLA Models",
    "authors": [
        "Xiao-Ming Wu",
        "Bin Fan",
        "Kang Liao",
        "Jian-Jian Jiang",
        "Runze Yang",
        "Yihang Luo",
        "Zhonghua Wu",
        "Wei-Shi Zheng",
        "Chen Change Loy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation."
        },
        {
            "title": "Start",
            "content": "VLANeXt: Recipes for Building Strong VLA Models Xiao-Ming Wu 1 Bin Fan 2 Kang Liao 1 Jian-jian Jiang 2 Runze Yang 3 Yihang Luo 1 Zhonghua Wu 4 Wei-Shi Zheng 2 Chen Change Loy 1 5 https://dravenalg.github.io/VLANeXt/ 6 2 0 2 0 2 ] . [ 1 2 3 5 8 1 . 2 0 6 2 : r Abstract Following the rise of large foundation models, VisionLanguageAction models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under unified framework and evaluation setup. Starting from simple VLA baseline similar to RT2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form practical recipe for building strong VLA models. The outcome of this exploration is simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release unified, easy-to-use codebase that serves as common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of shared foundation. 1. Introduction Recent advances in foundation models have reshaped how we think about general-purpose robot control. Instead of training task-specific policies, growing line of work builds 1S-Lab, Nanyang Technological University 2Sun Yat-sen University 3Shanghai Jiao Tong University 4SenseTime Research 5ACE Robotics. Correspondence to: Chen Change Loy <ccloy@ntu.edu.sg>. Preprint. February 24, 2026. 1 Figure 1. Performance comparison on the LIBERO and LIBERO-plus benchmarks. We compare VLANeXt with representative VLA baselines across model scales. Despite its smaller model size, VLANeXt achieves higher success rates than prior methods on both standard task performance (LIBERO) and robustness/generalization (LIBERO-plus), demonstrating the effectiveness of the design recipe distilled in this work. VisionLanguageAction (VLA) models that leverage large visionlanguage backbones to map visual observations and language instructions directly to robot actions. By inheriting rich visual understanding and language grounding from foundation models, VLAs1 offer scalable route toward general-purpose, language-conditioned robot policies (Ma et al., 2024; Ravichandar et al., 2020; Xiao et al., 2025c). Since the emergence of VLAs (Zitkovich et al., 2023), both academia and industry have proposed wide range of models demonstrating strong performance and encouraging generalization across diverse tasks (Zitkovich et al., 2023; ONeill et al., 2024; Li et al., 2023; 2024; Kim et al., 2024a; Black et al., 2024; Team et al., 2025; Hung et al., 2025; Kim et al., 2025; Shukor et al., 2025; Intelligence et al., 2025b;a; Liu et al., 2026). Most VLA approaches build on pre-trained LLMs or VLMs, processing visual observations together with language instructions to derive action-relevant representations for policy learning. This pipeline introduces numerous design choices, including how to interface the VLM with the policy module, how to train the policy, how 1An overview of VLAs is provided in the Appendix. We also release an Awesome-VLA repository to review the VLA literature. See https://github.com/DravenALG/awesome-vla. VLANeXt: Recipes for Building Strong VLA Models for analyzing the effectiveness of different design choices. We evaluate all variants on established VLA benchmarks, including LIBERO (Liu et al., 2023) and LIBERO-plus (Fei et al., 2025b), where LIBERO-plus extends the original benchmark with controlled and unseen perturbations to better assess robustness and generalization. Within this setup, we systematically explore the design space along three dimensions: 1) foundational components, covering core VLM-policy architectures and action learning objectives; 2) perception essentials: examining the role of visual, language, and proprioceptive inputs; and 3) action modeling perspective: investigating designs and auxiliary objectives that facilitate action generation. From these studies, we distill 12 key findings that together form practical recipe for building strong VLA models, summarized in Fig. 2. We highlight several findings that we believe are particularly noteworthy for the field: 1), soft connection between the VLM and the policy module performs slightly better than both loose and tight coupling strategies; 2), conditioning proprioceptive input in the VLM yields better performance than either omitting proprioception or injecting it directly into the policy module; and 3), framing action generation as time-series forecasting problem and incorporating frequency-domain modeling provides an effective and efficient way to improve action prediction. The outcome of this study is simple yet effective VLA model, VLANeXt, derived directly from the design principles uncovered in our systematic exploration. Rather than relying on aggressive model scaling or task-specific engineering, VLANeXt achieves state-of-the-art performance on both LIBERO (Liu et al., 2023) and LIBERO-plus (Fei et al., 2025b)  (Fig. 1)  , and adapts effectively to real-world manipulation tasks. These results show that strong VLA performance can emerge from principled design choices within unified framework. To support further progress in this direction, we will release unified and easy-to-use codebase that standardizes training and evaluation while exposing the key components of the VLA design space. The framework is intentionally lightweight and minimally encapsulated, enabling researchers to reproduce our findings, probe alternative design choices, and build new VLA variants on top of shared, transparent foundation. 2. Recipes for Building Strong VLA Models In this section, we detail the step-by-step evolution from simple baseline to the final VLANeXt model. We organize our exploration along three aspects: foundational components (Sec. 2.1), perception essentials (Sec. 2.2), and action modeling perspectives (Sec. 2.3). An overview is shown in Fig. 2, with full results in Table 1. Evaluation Setup. We perform the roadmap exploration Figure 2. Ablation trajectory across the VLA design space (spatial suite). We progressively evolve baseline VLA through changes in foundational components, perception, and action modeling. Results are reported on LIBERO initially, and on LIBEROplus once LIBERO performance saturates, providing more sensitive test of robustness and generalization. The trajectory culminates in the final VLANeXt model (2.5B) vs. OpenVLA-OFT (7B). to select essential perceptual inputs, and how actions should be represented and modeled. Despite rapid progress, early exploration of VLAs remains something of primordial soup - rich in ideas but lacking clear structure. While prior work has explored VLA design from certain perspectives (Zhen et al., 2024; Qu et al., 2025; Zhang et al., 2025c; Cen et al., 2025b; Zhang et al., 2025b;d; Lu et al., 2025), differences in training protocols and evaluation setups make it difficult to identify which design choices in the shared VLA design space truly matter. This work aims to provide more systematic understanding of this fragmented design space by comprehensively reexamining VLA design spaces under unified framework and evaluation protocol. While some prior works (Kim et al., 2025; Zhang et al., 2026; Liu et al., 2026) have preliminarily explored VLA designs, this study aims to provide more comprehensive, in-depth investigation into this domain. In detail, we begin with simple baseline VLA, similar to RT-2 (Zitkovich et al., 2023) and OpenVLA (Kim et al., 2024a), which serves as strong reference point 2 VLANeXt: Recipes for Building Strong VLA Models Figure 3. Design choices for the policy module. Figure 4. Design choices for the VLM-Policy connection. on LIBERO and LIBERO-plus (Liu et al., 2023; Fei et al., 2025b). Most experiments are conducted on the spatial suite as our primary testbed, while the resulting insights generalize across the other suites (Object, Goal, and Long). Baseline. Our baseline follows the VLA pipeline introduced in RT-2 (Zitkovich et al., 2023) and later adopted by OpenVLA (Kim et al., 2024a). We use LLaMA as the language backbone (Grattafiori et al., 2024), paired with SigLIP2 (Tschannen et al., 2025) as the vision encoder, since LLaMA does not natively support visual inputs. subset of rarely used text tokens is repurposed as action tokens, enabling action prediction in the same autoregressive framework. Continuous actions are discretized using simple binning strategy and modeled as classification over bin indices. We intentionally start from this minimal, classical VLA-style setup to provide clean reference point for analyzing the effects of different design choices. Our implementation adopts more recent LLaMA version (LLaMA 3.2) but at smaller scale (3B parameters, compared to 7B in OpenVLA). 2.1. The Foundational Components In this section, we investigate some core design choices of VLAs, including architectures and training losses. Policy Module Design. Our baseline follows RT2 (Zitkovich et al., 2023) and OpenVLA (Kim et al., 2024a), reusing text tokens for action classification. We first examine whether an explicit policy head is necessary. To this end, we append class token to the text and visual embeddings and feed its LLM output into two-layer policy head (transformer architecture) for action classification (Fig. 3(a)(b)). Results show that introducing separate policy head performs slightly better than directly reusing text tokens  (Table 1)  , suggesting that decoupling action prediction from the linguistic token space is beneficial. We further investigate whether more expressive policy module brings additional gains. Specifically, we replace the single class token with multiple tokens (16) and expand the policy network from 2 to 12 layers, making the design conceptually similar to MetaQuery (Pan et al., 2025) (Fig. 3(c)). This enlarged policy module yields significant performance improvement  (Table 1)  . Our final model adopts this design. Action Chunking. Our baseline predicts actions one step at time. Here, we evaluate action chunking, which predicts multiple future actions jointly and is known to improve inference efficiency (Kim et al., 2025). Results show that longer chunk horizons consistently improve action generation performance  (Table 1)  , suggesting that modeling longer temporal window of action provides more coherent view of the action sequence. We therefore adopt action chunking with chunk size of 8. Action Learning Objective. An action chunk is continuous vector of shape (t, dim). Our baseline discretizes this vector using binning (first normalizing to 1 and 1, then dividing into 256 bins) and treats action prediction as classification, following OpenVLA (Kim et al., 2024a). We compare this with alternative objectives, including direct regression (Kim et al., 2025), diffusion-based losses such as DDIM (Song et al., 2021; Zhang et al., 2025c), flow matching (Lipman et al., 2021; Lv et al., 2025), and VQVAEbased codebook (codebook size 1024 and each action assigns 3 codes) classification (Van Den Oord et al., 2017; Esser et al., 2021). Results show that regression achieves the strongest performance, with diffusion-based objectives close behind, while classification-based approaches perform worst  (Table 1)  . This likely reflects the approximately Gaussian action distributions in the benchmarks, which favor continuous modeling. We therefore adopt the flow-matching objective, which offers strong performance while remaining suitable for more complex or multimodal action distributions. We also observe that classification using the VQVAEbased codebook underperforms relative to the binning strategy. We attribute this to the fact that the action spaces are low-rank, meaning simple binning approach provides sufficient resolution. VLM Backbone Capacity. Our baseline uses LLaMA as the backbone (Grattafiori et al., 2024). We evaluate alternative VLM backbones to study how backbone strength affects VLA performance, including PaliGemma-3B (Beyer et al., 2024) (used in the π series (Black et al., 2024; Intelligence et al., 2025b)) and the Qwen-VL family (Bai et al., 2025a), which represent some of the most capable open-source VLMs currently available. Results show consistent trend: stronger VLM backbones yield better VLA performance  (Table 1)  , with Qwen3-VL3 VLANeXt: Recipes for Building Strong VLA Models 4B outperforming Qwen3-VL-2B, which in turn outperforms LLaMA-3.2-3B and PaliGemma-3B. We use Qwen3VL-2B in subsequent experiments as strong yet efficient choice. This finding differs from (Zhang et al., 2026). possible reason is that our larger policy module can better exploit the representational capacity of stronger VLMs, whereas the lightweight policy head in (Zhang et al., 2026) may limit such gains. We leave deeper investigation to future work. VLM-Policy Connection. We next study how different connection strategies between the VLM and the policy module affect performance. Our baseline adopts MetaQuery-style design (Pan et al., 2025), as discussed in Policy Module Design. We refer this design as the loose strategy, where the VLM and policy module are fully decoupled. We compare this with tight strategy that connects the two modules layer by layer, as in the π series (Black et al., 2024; Intelligence et al., 2025b). Inspired by these two designs, we further introduce soft strategy that also connects them layer by layer but inserts learnable queries as latent buffer between the modules  (Fig. 4)  . Results show that the soft strategy slightly outperforms both loose and tight connections  (Table 1)  , suggesting that the learnable query buffer helps better transfer useful representations from the VLM to the policy module. This may be viewed as introducing latent buffer between the two components, analogous to reasoning in latent space (Hao et al., 2024). We adopt the soft connection in subsequent models. 2.2. The Perception Essentials In this section, we shift our focus from foundational components to perception, investigating whether and how different modalities (e.g., visual observations and actions) should be provided as inputs to VLAs. Temporal Observation History. We examine whether incorporating temporal observation history improves performance. Our baseline follows OpenVLA (Kim et al., 2024a) and uses only the current frame as input. We extend this to include multiple past frames, leveraging the video capability of the Qwen3-VL-2B backbone (Bai et al., 2025b) for controlled comparison. Results show that adding temporal history does not improve action generation and slightly degrades performance  (Table 1)  , indicating that redundant temporal inputs may introduce noise or distract the model. Camera View Horizon. We study the effect of camera viewpoints on VLA performance. Our baseline uses single third-person view, following OpenVLA (Kim et al., 2024a). Many robotics datasets (ONeill et al., 2024; Khazatsky et al., 2024) additionally provide an in-hand wrist camera, allowing comparison between single-view and multi-view inputs. Results show that combining third-person and wrist views significantly improves performance  (Table 1)  , suggesting that multi-view observations provide complementary geometric cues that help resolve spatial ambiguities. Proprioception Conditioning. We examine the role of proprioception, which provides information about the robots internal state and motion history. Our baseline, following OpenVLA (Kim et al., 2024a), does not use proprioceptive inputs. We compare three variants: conditioning the VLM, conditioning the policy module, and conditioning both  (Fig. 5)  . In detail, for the VLM part, we will use the proprioception as input, and for the policy part, we will use the action as input to align with the generated action. Results show that conditioning proprioception in the VLM yields the best performance  (Table 1)  . We hypothesize that integrating proprioception at the VLM level allows better fusion with visual and language inputs, whereas injecting it directly into the policy module may reduce reliance on action prediction on visual observations and instructions. Although this appears to differ from the conclusion reported in Zhao et al. (Zhao et al., 2025a), where they claim that proprioception is not needed, their study evaluates architectures where proprioception is injected only into the policy module. In that setting, removing proprioception improves performance, which is consistent with our findings. We further compare three different integration mechanisms, including linear projector, transformer-based projector, and transformer projector with masked reconstruction pretraining (He et al., 2022). The transformer-based projector performs slightly better  (Table 1)  ; for simplicity, we use the linear projector in the final design. 2.3. Action Modelling Perspectives Here, we examine auxiliary design and training objectives to facilitate action generation. World Modelling. We examine augmenting action prediction with an auxiliary world modeling objective (Lv et al., 2025; Cen et al., 2025b). To avoid reliance on pretrained visual generators, we tokenize images using the Emu3.5 image tokenizer (Cui et al., 2025) and predict future image tokens with next-token objective. The target is the future frame at fixed horizon (8 steps, aligned with the action chunk length). The visual generation module is inserted between the VLM and the policy module with layer-wise connections  (Fig. 6)  . Adding world modeling improves action generation performance  (Table 1)  , indicating that predicting future observations is beneficial. However, it nearly triples training time, substantially increasing computational cost. We therefore exclude world modeling from the final recipe. Time Series Forecasting. We also explore facilitating action generation from time-series forecasting perspective. Inspired by frequency-domain modeling in time-series pre4 VLANeXt: Recipes for Building Strong VLA Models Table 1. Ablation across the VLA design space on LIBERO, LIBERO-plus (spatial suite). Each block varies in one design aspect. LIBEROplus evaluates robustness under diverse perturbations. Performance improves steadily as effective design choices are incorporated. Model Original Camera Robot Language Light Background Noise Layout Total LIBERO LIBERO-plus Foundational Components RT-2 like Baseline Baseline Policy Module Design Baseline Seperate Head Large Policy Module Action Chunking Horizon Action Chunk 1 Action Chunk 4 Action Chunk 8 Action Learning Objective bin Classification VQ-VAE Classification Regression DDIM Flow Matching VLM Backbone Capacity Paligemma LLaMA3.2 + SigLip Qwen3VL-2B Qwen3VL-4B VLM-Policy Connection Loose Connection Tight Connection Soft Connection Perception Enssentials Temporal Observation History Current Frame Image Temporal Observation History Camera View Horizon Third-person camera view Multiview (third-person + wrist) Proprioception Conditioning No Proprioception Input Proprioception to VLM Proprioception to Policy Proprioception to VLM & Policy Linear Projector Transformer Projector Transformer Projector & MAE Action Modelling Perspectives World Modelling Perspective Normal World Modelling Time Series Forecasting Perspective Normal Frequency Domain Loss 19.8 19.8 30.2 64. 64.4 75.4 74.6 74.6 58.8 85.4 80.0 80.0 69.8 80.0 90.0 95.8 90.0 90.0 91.8 91.8 85.0 91.8 97. 97.6 98.0 96.2 97.6 98.0 96.4 97 98.0 98.0 98.0 99.0 - - 0.8 0. 0.5 5.3 5.6 5.6 3.2 5.1 4.8 7.2 1.1 7.2 9.6 12.2 9.6 14.4 11.8 11.8 7.2 12.8 64. 64.9 87.2 62.8 77.9 87.2 96.8 91.1 87.2 94.4 87.2 94.4 - - - - - 31.0 79.7 79.7 67.9 85.6 85.6 67.2 90.5 80.3 79.2 32.1 79.2 74.6 93. 74.6 81.0 89.2 89.2 51.5 85.1 91.8 91.8 86.2 92.3 82.3 86.2 84.6 89.9 86.2 76. 86.2 86.7 - 15.4 34.6 34.6 42.5 56.8 56.8 42.5 62.7 70.5 46.9 22.9 46.9 75.0 89.0 75.0 68.5 72. 72.9 65.8 66.1 97.7 97.7 98.3 92.5 90.1 98.3 95.2 72.8 98.3 99.7 98.3 99. - 24.8 32.9 32.9 50.4 55.8 55.8 43.8 68.6 55.4 57.8 32.6 57.8 68.6 81.0 68.6 67.8 74.4 74.4 62. 72.1 93.0 93.0 93.8 96.5 94.6 93.8 98.8 86.9 93.8 98.8 93.8 98.4 - 10.0 12. 12.6 28.0 26.0 26.0 44.3 32.3 52.6 34.6 17.1 34.6 42.0 66.0 42.0 51.7 58.3 58.3 68.6 47.4 54. 54.0 62.2 69.1 73.4 62.2 58.0 51.1 62.2 80.3 62.2 83.4 5 - - 4.0 8.5 8.5 14.8 11.7 11.7 7.1 7.7 9.4 11.1 2.8 11.1 27.9 29.9 27.9 25.1 19.9 19.9 20. 31.9 85.5 85.5 92.0 87.2 90.6 92.0 96.9 78.7 92.0 93.2 92.0 98.0 - < 5.0 - 30.1 63.1 63.1 70.4 63.9 63.9 48.3 75.6 68.3 77.4 24.9 77.4 83.6 88.6 83.6 82.1 72. 72.5 80.8 87.3 90.1 90.1 96.6 88.3 88.8 96.6 93.3 85.6 96.6 93.2 96.6 92. < 5.0 16.6 34.0 34.0 40.0 43.4 43.4 36.5 48.4 48.3 45.0 18.9 45.0 53.7 64.8 53.7 55.4 56.2 56.2 50. 57.2 80.5 80.5 87.7 83.4 84.8 87.7 88.8 78.9 87.7 90.3 87.7 92.8 VLANeXt: Recipes for Building Strong VLA Models Figure 5. Design choices for proprioception conditioning. adopting action chunking to model longer temporal action horizons; using continuous objectives such as flow matching (with regression also effective under simple distributions); employing stronger VLM backbone (Qwen3-VL-2B as an effectiveefficient choice); and connecting the VLM and policy module through soft, layer-wise interactions with learnable query buffers. On the perception side, multi-view inputs (third-person + wrist) and VLM-side proprioception conditioning improve performance, while redundant temporal observation history is unnecessary. Moreover, adding lightweight frequencydomain auxiliary loss further boosts action generation with negligible cost. Although world modeling also improves performance, its substantially higher training cost makes it less practical. Together, these choices form practical recipe for building strong and efficient VLA model, which we call VLANeXt. 3. Benchmarks Evaluations 3.1. Settings To evaluate both standard performance and generalization robustness, we employ the LIBERO ecosystem. We first evaluate our VLANeXt on the standard LIBERO benchmark (Liu et al., 2023), which provides four distinct categories (Spatial, Object, Goal, and Long) to test the task learning ability, each providing 500 expert demonstrations across 10 tasks to assess policy generalization to different spatial layouts, objects, goals, and long-horizon tasks. To test the generalization boundaries of our model further, we evaluate our method on LIBERO-plus (Fei et al., 2025b). Unlike the static conditions in standard LIBERO, LIBERO-plus introduces systematic variations to the evaluation episodes, comprising 10,030 demonstrations across the above four suites in LIBERO, with perturbations in visual (e.g., lighting, background, camera pose), physical (e.g., object layout, robot state), and semantic (e.g., language instruction rewrites) dimensions. Following the standard setting in OpenVLA (Kim et al., 2024a), we train our models on the modified LIBERO dataset for each suite (Spatial, Object, Goal, and Long), and evaluate performance on both the LIBERO and LIBEROFigure 6. Augmenting action prediction with an auxiliary world modeling objective. Figure 7. VLANeXt architecture. Multi-view visual inputs, language instructions, and proprioception are tokenized and processed by multimodal LLM, with meta queries enabling soft interaction with the policy module. Action chunks are predicted using flow matching and further regularized by frequency-domain objective. diction (Zhou et al., 2022; Yi et al., 2023; Yang et al., 2024; Wang et al., 2025a), we introduce simple auxiliary loss that minimizes the MSE between predicted and groundtruth actions in the frequency domain (weighted 0.10.2 relative to the flow-matching loss). We use the discrete cosine transform (Ahmed et al., 1974) to convert the action to the frequency domain. This strategy improves action generation performance, slightly surpassing the world modeling objective while adding negligible training overhead  (Table 1)  . The gain likely arises because robotic action sequences are structured and low-rank, amenable to frequency-domain modeling. 2.4. Summary of Recipes Starting from classical RT-2/OpenVLA-style baseline, we find that strong VLA performance emerges from series of principled design choices. Beneficial changes include: replacing token reuse with deeper, dedicated policy module; 6 VLANeXt: Recipes for Building Strong VLA Models plus benchmarks (which include unseen perturbations) for the corresponding suite. For fair comparisons across different design choices, we directly fine-tune all models on the LIBERO dataset. All experiments in our recipes use 10,000 training steps with batch size of 256. The learning rate is set to 1 104 for models smaller than 3B parameters and 5 105 otherwise. 3.2. LIBERO Benchmark Results On the LIBERO benchmark, we compare our method against two categories of approaches: (i) direct policy learning methods that are trained solely on robotic datasets, and (ii) VLA methods that leverage knowledge from pretrained VLMs for policy learning. For direct policy learning methods, we include Diffusion Policy (Chi et al., 2025), Octo (Ghosh et al., 2024), and MDT (Reuss et al., 2024). For VLA methods, we compare against OpenVLA (Kim et al., 2024a), TraceVLA (Zheng et al., 2025), SpatialVLA (Qu et al., 2025), WorldVLA (Cen et al., 2025b), CoT-VLA (Zhao et al., 2025b), π0 (Black et al., 2024), π0-Fast (Pertsch et al., 2025), NORA (Hung et al., 2025), SmolVLA (Shukor et al., 2025), UniVLA (Wang et al., 2025d), FLOWER (Reuss et al., 2025), and OpenVLA-OFT (Kim et al., 2025). The comparison results are shown in Table 2. As we can observe, following our recipes allows us to build strong VLA that achieves state-of-the-art performance, demonstrating the effectiveness of the design choices. 3.3. LIBERO-plus Benchmark Results For the LIBERO-plus benchmark, we compare our model with several VLA models such as OpenVLA (Kim et al., 2024a), WorldVLA (Cen et al., 2025b), NORA (Hung et al., 2025), UniVLA (Wang et al., 2025d), πo (Black et al., 2024), πo-Fast (Pertsch et al., 2025), and OpenVLA-OFT (Kim et al., 2025). As shown in Table 3, the proposed VLANeXt model demonstrates strong generalization ability across different types of unseen perturbations. Moreover, our model shows significant improvement (10% in success rate) over the stateof-the-art method OpenVLA-OFT (Kim et al., 2025) on the LIBERO-plus benchmark compared to previous methods, suggesting the effectiveness of the explored recipes. 4. Real-World Evaluations To comprehensively assess the performance of our method, we additionally evaluate it in real-world deployments. 4.1. Settings We design four tasks, including two single-arm tasks and two bimanual tasks, to evaluate our method. The singleTable 2. LIBERO benchmark performance. The results are shown in success rate (%). S, O, G, L: Spatial, Object, Goal, and Long suites, respectively. We color the best and second best results. Model O Avg Baseline Direct Policy Models Diffusion Policy Octo MDT 78.3 78.9 78. 92.5 85.7 87.5 Baseline VLA Models TraceVLA OpenVLA SpatialVLA WorldVLA CoT-VLA π0-Fast π0 NORA SmolVLA UniVLA FLOWER OpenVLA-OFT 84.6 84.7 88.2 85.6 87.5 96.4 90.0 92.2 93.0 96.5 97.5 97.6 85.2 88.4 89.9 89.0 91.6 96.8 86.0 95.4 94.0 96.8 99.1 98.4 68.3 84.6 73.5 75.1 79.2 78.6 82.6 87.6 88.6 95.0 89.4 91.0 95.6 96.1 97. 50.5 51.5 64.8 54.1 53.7 55.5 59.0 69.0 60.2 73.0 74.6 77.0 92.0 94.9 94.5 72.4 75.1 76.4 74.8 76.5 78.1 79.1 83.9 85.5 86.0 87.9 88.8 95.2 96.9 97.1 Ours VLANeXt 99. 99.2 96.6 94.6 97.4 arm tasks include table cleaning, which involves picking up objects from table and placing them into container, and drawer manipulation, where the robot opens drawer, places objects inside, and closes it. The bimanual tasks include basket lifting, which requires lifting basket using both hands, and bimanual table cleaning, where two arms coordinate to collect objects from table and place them into container. The single-arm experiments use Franka Emika, while the bimanual experiments are conducted on the Aloha system (Zhao et al., 2023). visualization of the experimental setup for each task is depicted in Figure 8. For training, we collect 50 episodes per task and evaluate each model over 20 trials, reporting the success rate. We first pretrain the model on the DROID dataset (Khazatsky et al., 2024) for 100k steps, then fine-tune it on each task for 20k steps with learning rate of 1 104. Because DROID contains only single-arm data, adapting the model to bimanual tasks requires reinitializing the proprioception projector and the final layer of the action generation module, while keeping all other pretrained weights. 4.2. Results We compare against two representative VLA baselines, OpenVLA-OFT (Kim et al., 2025) and π0 (Black et al., 2024). We load their pretrained checkpoints and fine-tune them on each task in the same manner as our method to ensure fair comparison. The results are shown in Table 4. As can be seen, our model performs well in real-world experiments, demonstrating that the recipes we propose lead to strong VLA model that can be effectively deployed in real7 VLANeXt: Recipes for Building Strong VLA Models Table 3. LIBERO-plus benchmark performance. The results are shown in success rate (%). We color the best and second best results (in average). The complete per-suite results of the listed methods can also be found in (Fei et al., 2025b). Model Suite Camera Robot Language Light Background Noise Layout Total Baseline VLA Models OpenVLA WorldVLA NORA UniVLA π0 π0-Fast Average Average Average Average Average Average OpenVLA-OFT Ours VLANeXt Spatial Object Goal Long Average Spatial Object Goal Long Average 0.8 0.1 2.2 1.8 13.8 65.1 88.3 38.9 62.0 38.7 56. 94.4 73.2 71.1 47.7 71.6 3.5 27.9 37.0 46.2 6.0 21.6 40.0 25.4 25.2 38.2 31.9 83.4 67.8 50.6 51.9 63.4 23.0 41.6 65.1 69.6 58.8 61.0 80.5 99.0 53.2 87.0 79. 86.7 84.2 66.3 88.5 81.4 8.1 43.7 45.7 69.0 85.0 73.2 98.3 73.7 93.9 89.4 88.7 99.3 98.3 92.5 84.7 93.7 34.8 17.1 58.6 81.0 81.4 73.2 97.3 97.6 92.5 86.8 93. 98.4 94.8 81.9 92.0 91.8 15.2 10.9 12.8 21.2 79.0 74.4 96.3 72.3 75.2 63.5 75.8 98.0 92.2 90.2 77.5 89.5 28.5 38.0 62.1 31.9 68.9 68.8 93.9 71.8 59.1 76.9 74. 92.2 73.0 66.8 78.8 77.7 15.6 25.0 39.0 42.9 53.6 61.6 84.0 66.5 63.0 66.4 69.6 92.8 82.1 72.7 72.8 80.1 Table 4. Real-world evaluation results. Results are shown with (success count/total count). We color the best and second best . Model Single Arm Clean Drawer Bimanual Arm Lifting Clean Baseline VLA Models OpenVLA-OFT π0 7/20 10/ 7/20 8/20 5/20 10/20 9/20 13/20 Ours VLANeXt 14/20 11/ 11/20 15/20 unified framework. In particular, how the VLM interacts with the policy module, how multimodal signals such as proprioception are fused, and how temporal structure in actions is modeled all play central roles. Several observations carry broader implications. Modest architectural refinements, such as soft VLMpolicy coupling or VLM-side proprioception conditioning, can meaningfully influence performance, indicating that where information is injected matters as much as what information is used. Viewing action generation as structured sequence modeling, for example, through frequency-domain objectives, also shows that ideas from time-series learning transfer effectively to robotics. Meanwhile, richer objectives like world modeling improve performance but introduce notable computational overhead, highlighting the importance of efficiency-aware design. We hope this work encourages shift from ad-hoc model variations toward more controlled exploration of the VLA design space. By releasing unified, lightweight framework, we aim to support systematic studies and shared progress. Extending this perspective to more diverse embodiments, longer-horizon reasoning, and richer world-interaction objectives remains an important direction for future research. Figure 8. Our single-arm and bimanual arm tasks for the real-world experiments. world settings. In addition, even without bimanual training, our method can adapt to bimanual robotics tasks with decent performance, demonstrating the cross-embodiment adaptability of the method. Additional video demonstrations of our experimental results are provided in the supplementary materials. 5. Conclusion This work moves toward more systematic understanding of VLA models. Rather than introducing another standalone architecture, we revisit the VLA pipeline and show that many gains arise from principled design choices within 8 VLANeXt: Recipes for Building Strong VLA Models"
        },
        {
            "title": "Acknowledgements",
            "content": "This study is supported under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). It is also supported by Singapore MOE AcRF Tier 2 (MOE-T2EP20224-0003)."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work aimed at advancing the field of machine learning, specifically Vision-Language-Action models for robotic control. While our work contributes to the development of more capable embodied agents, we believe that its potential societal implications fall within well-established discussions in the field and therefore do not require special emphasis here."
        },
        {
            "title": "References",
            "content": "Ahmed, N., Natarajan, T., and Rao, K. R. Discrete cosine transform. IEEE Transactions on Computers, 1974. Bai, S., Cai, Y., Chen, R., Chen, K., et al. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Bai, Z., Gao, C., and Shou, M. Z. Evolve-vla: Test-time training from environment feedback for vision-languageaction models. arXiv preprint arXiv:2512.14666, 2025c. Beyer, L., Steiner, A., Pinto, A. S., Kolesnikov, A., Wang, X., Salz, D., Neumann, M., Alabdulmohsin, I., Tschannen, M., Bugliarello, E., et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. Bhat, V., Lan, Y.-H., Krishnamurthy, P., Karri, R., and Khorrami, F. 3d cavla: Leveraging depth and 3d context to generalize vision language action models for unseen tasks. arXiv preprint arXiv:2505.05800, 2025. Bi, H., Tan, H., Xie, S., Wang, Z., Huang, S., Liu, H., Zhao, R., Feng, Y., Xiang, C., Rong, Y., et al. Motus: unified latent action world model. arXiv preprint arXiv:2512.13030, 2025. Bjorck, J., Castaneda, F., Cherniadev, N., Da, X., Ding, R., Fan, L., Fang, Y., Fox, D., Hu, F., Huang, S., et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. pi0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al. Rt-1: Robotics transformer for real-world control at scale. In RSS, 2023. Cen, J., Huang, S., Yuan, Y., Li, K., Yuan, H., Yu, C., Jiang, Y., Guo, J., Li, X., Luo, H., et al. Rynnvla-002: unified vision-language-action and world model. arXiv preprint arXiv:2511.17502, 2025a. Cen, J., Yu, C., Yuan, H., Jiang, Y., Huang, S., Guo, J., Li, X., Song, Y., Luo, H., Wang, F., et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025b. Cheang, C.-L., Chen, G., Jing, Y., Kong, T., Li, H., Li, Y., Liu, Y., Wu, H., Xu, J., Yang, Y., et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. Chen, P., Bu, P., Wang, Y., Wang, X., Wang, Z., Guo, J., Zhao, Y., Zhu, Q., Song, J., Yang, S., et al. Combatvla: An efficient vision-language-action model for combat tasks in 3d action role-playing games. arXiv preprint arXiv:2503.09527, 2025a. Chen, Z., Niu, R., Kong, H., Wang, Q., Xing, Q., and Fan, Z. Tgrpo: Fine-tuning vision-language-action model via trajectory-wise group relative policy optimization. arXiv preprint arXiv:2506.08440, 2025b. Chi, C., Xu, Z., Feng, S., Cousineau, E., Du, Y., Burchfiel, B., Tedrake, R., and Song, S. Diffusion policy: Visuomotor policy learning via action diffusion. IJRR, 2025. Cui, Y., Chen, H., Deng, H., Huang, X., Li, X., Liu, J., Liu, Y., Luo, Z., Wang, J., Wang, W., et al. Emu3. 5: Native multimodal models are world learners. arXiv preprint arXiv:2510.26583, 2025. Ding, P., Ma, J., Tong, X., Zou, B., Luo, X., Fan, Y., Wang, T., Lu, H., Mo, P., Liu, J., et al. Humanoid-vla: Towards universal humanoid control with visual integration. arXiv preprint arXiv:2502.14795, 2025. Du, Y., Yang, S., Dai, B., Dai, H., Nachum, O., Tenenbaum, J., Schuurmans, D., and Abbeel, P. Learning universal policies via text-guided video generation. NeurIPS, 2023. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In CVPR, 2021. Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B., et al. Fei, S., Wang, S., Ji, L., Li, A., Zhang, S., Liu, L., Hou, J., Gong, J., Zhao, X., and Qiu, X. Srpo: Self-referential 9 VLANeXt: Recipes for Building Strong VLA Models policy optimization for vision-language-action models. arXiv preprint arXiv:2511.15605, 2025a. Fei, S., Wang, S., Shi, J., Dai, Z., Cai, J., Qian, P., Ji, L., He, X., Zhang, S., Fei, Z., et al. Libero-plus: In-depth robustness analysis of vision-language-action models. arXiv preprint arXiv:2510.13626, 2025b. Fu, Y., Zhang, Z., Zhang, Y., Wang, Z., Huang, Z., and Luo, Y. Mergevla: Cross-skill model merging toward generalist vision-language-action agent. arXiv preprint arXiv:2511.18810, 2025. Ghosh, D., Walke, H. R., Pertsch, K., Black, K., Mees, O., Dasari, S., Hejna, J., Kreiman, T., Xu, C., Luo, J., et al. Octo: An open-source generalist robot policy. In RSS, 2024. Goyal, A., Hadfield, H., Yang, X., Blukis, V., and Ramos, F. Vla-0: Building state-of-the-art vlas with zero modification. arXiv preprint arXiv:2510.13054, 2025. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Guo, W., Lu, G., Deng, H., Wu, Z., Tang, Y., and Wang, Z. Vla-reasoner: Empowering vision-language-action models with reasoning via online monte carlo tree search. arXiv preprint arXiv:2509.22643, 2025. Hao, S., Sukhbaatar, S., Su, D., Li, X., Hu, Z., Weston, J., and Tian, Y. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In CVPR, 2022. Huang, C.-P., Wu, Y.-H., Chen, M.-H., Wang, Y.-C. F., and Yang, F.-E. Thinkact: Vision-language-action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025a. Huang, J., Wang, S., Lin, F., Hu, Y., Wen, C., and Gao, Y. Tactile-vla: unlocking vision-language-action models physical knowledge for tactile generalization. arXiv preprint arXiv:2507.09160, 2025b. Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In ICML, 2022. Huang, W., Wang, C., Li, Y., Zhang, R., and Fei-Fei, L. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. In CoRL, 2025c. Hung, C.-Y., Sun, Q., Hong, P., Zadeh, A., Li, C., Tan, U., Majumder, N., Poria, S., et al. Nora: small opensourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854, 2025. Intelligence, P., Amin, A., Aniceto, R., Balakrishna, A., Black, K., Conley, K., Connors, G., Darpinian, J., Dhabalia, K., DiCarlo, J., et al. pi0.6: vla that learns from experience. arXiv preprint arXiv:2511.14759, 2025a. Intelligence, P., Black, K., Brown, N., Darpinian, J., Dhabalia, K., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., et al. pi0.5: vision-language-action model with openworld generalization. arXiv preprint arXiv:2504.16054, 2025b. Janner, M., Du, Y., Tenenbaum, J., and Levine, S. Planning with diffusion for flexible behavior synthesis. In ICML, 2022. Kareer, S., Pertsch, K., Darpinian, J., Hoffman, J., Xu, D., Levine, S., Finn, C., and Nair, S. Emergence of human to robot transfer in vision-language-action models. arXiv preprint arXiv:2512.22414, 2025. Khazatsky, A., Pertsch, K., Nair, S., Balakrishna, A., Dasari, S., Karamcheti, S., Nasiriany, S., Srirama, M. K., Chen, L. Y., Ellis, K., et al. Droid: large-scale in-the-wild robot manipulation dataset. In RSS, 2024. Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G., Sanketi, P., et al. Openvla: An open-source vision-languageaction model. In CoRL, 2024a. Kim, M. J., Finn, C., and Liang, P. Fine-tuning visionlanguage-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. Kim, Y., Oh, H., Lee, J., Choi, J., Ji, G., Jung, M., Youm, D., and Hwangbo, J. Not only rewards but also constraints: Applications on legged robot locomotion. TRO, 2024b. Kuang, F., You, J., Hu, Y., Zhang, T., Wen, C., and Gao, Y. Adapt your body: Mitigating proprioception shifts in imitation learning. arXiv preprint arXiv:2506.23944, 2025. Kumar, A., Fu, Z., Pathak, D., and Malik, J. Rma: Rapid motor adaptation for legged robots. In RSS, 2021. Huang, W., Wang, C., Zhang, R., Li, Y., Wu, J., and Fei-Fei, L. Voxposer: Composable 3d value maps for robotic manipulation with language models. In CoRL, 2023. Lee, J., Hwangbo, J., Wellhausen, L., Koltun, V., and Hutter, M. Learning quadrupedal locomotion over challenging terrain. Science Robotics, 2020. 10 VLANeXt: Recipes for Building Strong VLA Models Lee, J., Duan, J., Fang, H., Deng, Y., Liu, S., Li, B., Fang, B., Zhang, J., Wang, Y. R., Lee, S., et al. Molmoact: Action reasoning models that can reason in space. arXiv preprint arXiv:2508.07917, 2025. Lu, G., Guo, W., Zhang, C., Zhou, Y., Jiang, H., Gao, Z., Tang, Y., and Wang, Z. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025. Li, H., Yang, S., Chen, Y., Tian, Y., Yang, X., Chen, X., Wang, H., Wang, T., Zhao, F., Lin, D., et al. Cronusvla: Transferring latent motion across time for multi-frame prediction in manipulation. arXiv preprint arXiv:2506.19816, 2025a. Li, H., Zuo, Y., Yu, J., Zhang, Y., Yang, Z., Zhang, K., Zhu, X., Zhang, Y., Chen, T., Cui, G., et al. Simplevla-rl: Scaling vla training via reinforcement learning. arXiv preprint arXiv:2509.09674, 2025b. Li, Q., Liang, Y., Wang, Z., Luo, L., Chen, X., Liao, M., Wei, F., Deng, Y., Xu, S., Zhang, Y., et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. Li, S., Gao, Y., Sadigh, D., and Song, S. Unified video action model. In RSS, 2025c. Li, X., Liu, M., Zhang, H., Yu, C., Xu, J., Wu, H., Cheang, C., Jing, Y., Zhang, W., Liu, H., et al. Vision-language foundation models as effective robot imitators. In ICLR, 2023. Liang, H., Chen, X., Wang, B., Chen, M., Liu, Y., Zhang, Y., Chen, Z., Yang, T., Chen, Y., Pang, J., et al. Mm-act: Learn from multimodal parallel generation to act. arXiv preprint arXiv:2512.00975, 2025. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. In ICLR, 2021. Liu, B., Zhu, Y., Gao, C., Feng, Y., Liu, Q., Zhu, Y., and Stone, P. Libero: Benchmarking knowledge transfer for lifelong robot learning. In NeurIPS, 2023. Liu, H., Li, X., Li, P., Liu, M., Wang, D., Liu, J., Kang, B., Ma, X., Kong, T., and Zhang, H. Towards generalist robot policies: What matters in building vision-language-action models. Nature Machine Intelligence, 2026. Liu, M., Pathak, D., and Agarwal, A. Locoformer: Generalist locomotion via long-context adaptation. In CoRL, 2025. Liu, S., Wu, L., Li, B., Tan, H., Chen, H., Wang, Z., Xu, K., Su, H., and Zhu, J. Rdt-1b: diffusion foundation model for bimanual manipulation. In ICLR, 2024. Lu, G., Zhang, S., Wang, Z., Liu, C., Lu, J., and Tang, Y. Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation. In ECCV, 2024. Lv, Q., Kong, W., Li, H., Zeng, J., Qiu, Z., Qu, D., Song, H., Chen, Q., Deng, X., and Pang, J. F1: vision-languageaction model bridging understanding and generation to actions. arXiv preprint arXiv:2509.06951, 2025. Ma, Y., Song, Z., Zhuang, Y., Hao, J., and King, I. survey on vision-language-action models for embodied ai. arXiv preprint arXiv:2405.14093, 2024. Margolis, G. B. and Agrawal, P. Walk these ways: Tuning robot control for generalization with multiplicity of behavior. In CoRL, 2023. ONeill, A., Rehman, A., Maddukuri, A., Gupta, A., Padalkar, A., Lee, A., Pooley, A., Gupta, A., Mandlekar, A., Jain, A., et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In ICRA, 2024. Pan, X., Shukla, S. N., Singh, A., Zhao, Z., Mishra, S. K., Wang, J., Xu, Z., Chen, J., Li, K., Juefei-Xu, F., Hou, J., and Xie, S. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Peng, X. B., Abbeel, P., Levine, S., and Van de Panne, M. Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. TOG, 2018. Pertsch, K., Stachowicz, K., Ichter, B., Driess, D., Nair, S., Vuong, Q., Mees, O., Finn, C., and Levine, S. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. Qu, D., Song, H., Chen, Q., Yao, Y., Ye, X., Ding, Y., Wang, Z., Gu, J., Zhao, B., Wang, D., et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. Radosavovic, I., Shi, B., Fu, L., Goldberg, K., Darrell, T., and Malik, J. Robot learning with sensorimotor pretraining. In CoRL, 2023. Ravichandar, H., Polydoros, A. S., Chernova, S., and Billard, A. Recent advances in robot learning from demonstration. ARCRAS, 2020. Reuss, M., Yagmurlu, O. E., Wenzel, F., and Lioutikov, R. Multimodal diffusion transformer: Learning versatile behavior from multimodal goals. In RSS, 2024. Reuss, M., Zhou, H., Ruhle, M., Yagmurlu, O. E., Otto, F., and Lioutikov, R. Flower: Democratizing generalist robot policies with efficient vision-language-action flow policies. arXiv preprint arXiv:2509.04996, 2025. 11 VLANeXt: Recipes for Building Strong VLA Models Shi, H., Xie, B., Liu, Y., Sun, L., Liu, F., Wang, T., Zhou, E., Fan, H., Zhang, X., and Huang, G. Memoryvla: Perceptual-cognitive memory in vision-languageaction models for robotic manipulation. arXiv preprint arXiv:2508.19236, 2025. Shridhar, M., Manuelli, L., and Fox, D. Perceiver-actor: multi-task transformer for robotic manipulation. In CoRL, 2023. Shukor, M., Aubakirova, D., Capuano, F., Kooijmans, P., Palma, S., Zouitine, A., Aractingi, M., Pascal, C., Russi, M., Marafioti, A., et al. Smolvla: vision-languageaction model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In ICLR, 2021. Song, W., Zhou, Z., Zhao, H., Chen, J., Ding, P., Yan, H., Huang, Y., Tang, F., Wang, D., and Li, H. Reconvla: Reconstructive vision-language-action model as effective robot perceiver. In AAAI, 2025. Tan, S., Dou, K., Zhao, Y., and Kraehenbuehl, P. Interactive post-training for vision-language-action models. In CVPR, 2025. Team, G. R., Abeyruwan, S., Ainslie, J., Alayrac, J.-B., Arenas, M. G., Armstrong, T., Balakrishna, A., Baruch, R., Bauza, M., Blokzijl, M., et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. Tian, Y., Yang, S., Zeng, J., Wang, P., Lin, D., Dong, H., and Pang, J. Predictive inverse dynamics models are scalable learners for robotic manipulation. In ICLR, 2024. Tschannen, M., Gritsenko, A., Wang, X., Naeem, M. F., Alabdulmohsin, I., Parthasarathy, N., Evans, T., Beyer, L., Xia, Y., Mustafa, B., et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. In NeurIPS, 2017. Wang, H., Pan, L., Shen, Y., Chen, Z., Yang, D., Yang, Y., Zhang, S., Liu, X., Li, H., and Tao, D. Fredf: Learning to forecast in the frequency domain. In ICLR, 2025a. Wang, L., Chen, X., Zhao, J., and He, K. Scaling proprioceptive-visual learning with heterogeneous pretrained transformers. In NeurIPS, 2024. Wang, S., Yu, W., Chen, X., Tian, X., Zhang, J., Lu, L., and Zhang, C. End-to-end listen, look, speak and act. arXiv preprint arXiv:2510.16756, 2025b. Wang, Y., Ding, P., Li, L., Cui, C., Ge, Z., Tong, X., Song, W., Zhao, H., Zhao, W., Hou, P., et al. Vla-adapter: An effective paradigm for tiny-scale vision-language-action model. arXiv preprint arXiv:2509.09372, 2025c. Wang, Y., Li, X., Wang, W., Zhang, J., Li, Y., Chen, Y., Wang, X., and Zhang, Z. Unified vision-language-action model. arXiv preprint arXiv:2506.19850, 2025d. Wen, J., Zhu, Y., Li, J., Zhu, M., Tang, Z., Wu, K., Xu, Z., Liu, N., Cheng, R., Shen, C., et al. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. RAL, 2025. Wu, H., Jing, Y., Cheang, C., Chen, G., Xu, J., Li, X., Liu, M., Li, H., and Kong, T. Unleashing large-scale video generative pre-training for visual robot manipulation. In ICLR, 2024. Wu, P., Escontrela, A., Hafner, D., Abbeel, P., and Goldberg, K. Daydreamer: World models for physical robot learning. In CoRL, 2023. Xiao, J., Yang, Y., Chang, X., Chen, R., Xiong, F., Xu, M., Zheng, W.-S., and Zhang, Q. World-env: Leveraging world model as virtual environment for vla post-training. arXiv preprint arXiv:2509.24948, 2025a. Xiao, L., Li, J., Gao, J., Ye, F., Jin, Y., Qian, J., Zhang, J., Wu, Y., and Yu, X. Ava-vla: Improving vision-languageaction models with active visual attention. arXiv preprint arXiv:2511.18960, 2025b. Xiao, X., Liu, J., Wang, Z., Zhou, Y., Qi, Y., Jiang, S., He, B., and Cheng, Q. Robot learning in the era of foundation models: survey. Neurocomputing, 2025c. Yang, R., Cao, L., YANG, J., et al. Rethinking fourier transform from basis functions perspective for longterm time series forecasting. In NeurIPS, 2024. Ye, S., Jang, J., Jeon, B., Joo, S., Yang, J., Peng, B., Mandlekar, A., Tan, R., Chao, Y.-W., Lin, B. Y., et al. Latent action pretraining from videos. In CoRL, 2024. Yi, K., Zhang, Q., Fan, W., Wang, S., Wang, P., He, H., An, N., Lian, D., Cao, L., and Niu, Z. Frequency-domain mlps are more effective learners in time series forecasting. In NeurIPS, 2023. Ze, Y., Zhang, G., Zhang, K., Hu, C., Wang, M., and Xu, H. 3d diffusion policy: Generalizable visuomotor policy learning via simple 3d representations. In ICRA, 2024. Zhang, J., Chen, Y., Xu, Y., Huang, Z., Zhou, Y., Yuan, Y.-J., Cai, X., Huang, G., Quan, X., Xu, H., et al. 4d-vla: Spatiotemporal vision-language-action pretraining with cross-scene calibration. arXiv preprint arXiv:2506.22242, 2025a. 12 VLANeXt: Recipes for Building Strong VLA Models Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023. Zhang, J., Guo, Y., Hu, Y., Chen, X., Zhu, X., and Chen, J. Up-vla: unified understanding and prediction model for embodied agent. In ICML, 2025b. Zhang, J., Chen, X., Wang, Q., Li, M., Guo, Y., Hu, Y., Zhang, J., Bai, S., Lin, J., and Chen, J. Vlm4vla: Revisiting vision-language-models in vision-language-action models. arXiv preprint arXiv:2601.03309, 2026. Zhang, W., Liu, H., Qi, Z., Wang, Y., Yu, X., Zhang, J., Dong, R., He, J., Lu, F., Wang, H., et al. Dreamvla: vision-language-action model dreamed with comprehensive world knowledge. In NeurIPS, 2025c. Zhang, Z., Zheng, K., Chen, Z., Jang, J., Li, Y., Han, S., Wang, C., Ding, M., Fox, D., and Yao, H. Grape: Generalizing robot policy via preference alignment. In ICRA, 2025d. Zhao, J., Lu, W., Zhang, D., Liu, Y., Liang, Y., Zhang, T., Cao, Y., Xie, J., Hu, Y., Wang, S., et al. Do you need proprioceptive states in visuomotor policies? arXiv preprint arXiv:2509.18644, 2025a. Zhao, Q., Lu, Y., Kim, M. J., Fu, Z., Zhang, Z., Wu, Y., Li, Z., Ma, Q., Han, S., Finn, C., et al. Cot-vla: Visual chainof-thought reasoning for vision-language-action models. In CVPR, 2025b. Zhao, T., Kumar, V., Levine, S., and Finn, C. Learning finegrained bimanual manipulation with low-cost hardware. In RSS, 2023. Zhen, H., Qiu, X., Chen, P., Yang, J., Yan, X., Du, Y., Hong, Y., and Gan, C. 3d-vla: 3d vision-language-action generative world model. In ICML, 2024. Zheng, R., Liang, Y., Huang, S., Gao, J., Daume III, H., Kolobov, A., Huang, F., and Yang, J. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. In ICLR, 2025. Zhong, Z., Yan, H., Li, J., Liu, X., Gong, X., Zhang, T., Song, W., Chen, J., Zheng, X., Wang, H., et al. Flowvla: Visual chain of thought-based motion reasoning for vision-language-action models. arXiv preprint arXiv:2508.18269, 2025. Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In ICML, 2022. Zhou, Z., Zhu, Y., Zhu, M., Wen, J., Liu, N., Xu, Z., Meng, W., Peng, Y., Shen, C., Feng, F., et al. Chatvla: Unified multimodal understanding and robot control with visionlanguage-action model. In EMNLP, 2025. 13 VLANeXt: Recipes for Building Strong VLA Models A. More Experimental Results A.1. Qualitative Experiments We present more demos of our model on the LIBERO and LIBERO-plus benchmarks, as well as in real-world settings (see Figures 10, 11, and 9). Additional video demonstrations of our experimental results are provided in the supplementary materials. Figure 9. Qualitative experiments of our method in real-world tasks. Figure 10. Qualitative experiments of our method in the four suites of the LIBERO benchmark. 14 VLANeXt: Recipes for Building Strong VLA Models Figure 11. Qualitative experiments of our method in the 7 types of perturbations in the same task in the LIBERO-plus benchmark. 15 B. Detailed Experimental Settings VLANeXt: Recipes for Building Strong VLA Models The detailed training configuration of our final model is summarized below. The same settings are used across all four suites. Table 5. Hyperparameters for VLANeXt training on LIBERO and LIBERO-plus benchmark, four suites. Hyperparameter Optimization Optimizer Learning Rate Batch Size Training Steps Warmup Steps Lr Scheduler Weight Decay Max Grad Norm Backbone Update Data & Augmentation Observation Modality Camerate Video Proprioception to VLM Proprioception to Policy Transformer Proprioception Projector History Proprioception Size Action Chunk Size Augmentation Type Crop Scale / Ratio Color Jitter (B/C/S/H) Architecture VLM Backbone Condition Type Num of Queries Action Loss Schedule Diffusion Hidden Dim Diffusion Depth Diffusion Heads Frequency Domain Loss Weight Value AdamW 1.0 104 256 10,000 500 Cosine Decay 0.01 1.0 Full Finetune Image Multi-view true false false 8 8 Random Crop, Color Jitter [0.8, 1.0] / [0.9, 1.1] 0.2/ 0.2/ 0.2/0.05 Qwen3-VL-2B-Instruct Soft Connection 16 Diffusion Loss Flow Matching 1024 19 16 0. C. Revisiting Robot Learning and VLA Models Robot learning aims to apply machine learning techniques to robotic control, empowering robots to interact with the physical world and acquire diverse skills (Ravichandar et al., 2020). In the context of robot learning, tasks are generally categorized into locomotion and manipulation based on the active components being controlled. Locomotion is designed to maintain the stability and balance of the robot base, enabling mobility for legged systems such as quadrupeds or humanoids (Peng et al., 2018; Kumar et al., 2021; Lee et al., 2020; Margolis & Agrawal, 2023; Kim et al., 2024b; Liu et al., 2025). Since these tasks typically possess explicit objectives, they frequently use Reinforcement Learning (RL). In contrast, robotic manipulation focuses on controlling the robotic arm to execute diverse array of interactive tasks (Brohan et al., 2023; Chi et al., 2025; Wang et al., 2024; Liu et al., 2024; Ghosh et al., 2024). Due to the high diversity of task goals and interacting objects, it is often challenging to formulate explicit reward functions. Consequently, Imitation Learning (IL) is commonly adopted in this domain, allowing robots to learn complex manipulation skills directly from expert demonstrations. In this paper, we mainly focus on the domain of robotic manipulation via imitation learning, which also have the potential to generalize towards whole-body control, unifying the capabilities of arms, grippers, and legs. Robotic manipulation methods can be divided into standard action policies (Huang et al., 2022; Shridhar et al., 2023; Radosavovic et al., 2023; Huang et al., 2023; Ze et al., 2024; Lu et al., 2024; Huang et al., 2025c; Kuang et al., 2025; Zhao et al., 2025a) and video action policies (Janner et al., 2022; Wu et al., 2023; 2024; Du et al., 2023; Cheang et al., 2024; Tian et al., 2024; Li et al., 2025c). The former framework naively inputs instructions and visuals, outputting the actions to complete the tasks, while the latter pipeline predicts future videos together with action generation, with the claim that these world modeling abilities can help understand the task better 16 VLANeXt: Recipes for Building Strong VLA Models and generate the actions more accurately. In recent years, with the triumph of large foundation models, integrating these tremendous models into robot learning, specifically called Vision-Language-Action (VLA) Models, has become prominent trend (Ma et al., 2024). This paradigm was pioneered by the groundbreaking RT-2 (Zitkovich et al., 2023), which formally introduced the concept of VLA. Subsequently, researchers across academia and industry have developed diverse array of VLA models (ONeill et al., 2024; Li et al., 2023; 2024; Kim et al., 2024a; Black et al., 2024; Team et al., 2025; Hung et al., 2025; Kim et al., 2025; Shukor et al., 2025; Intelligence et al., 2025b;a; Liu et al., 2026). These newer iterations address specific challenges, such as leveraging 3D spatial information (Zhen et al., 2024; Bhat et al., 2025; Zhang et al., 2025a; Qu et al., 2025), exploiting intermediate data (like subtasks decomposition, future frame prediction or robot trajectory traces prediction) (Zheng et al., 2025; Zhao et al., 2025b; Zhang et al., 2025c; Lv et al., 2025; Zhong et al., 2025; Liang et al., 2025; Lee et al., 2025; Cen et al., 2025b;a; Wang et al., 2025d; Zhang et al., 2025b; Song et al., 2025) to enhance action generation, and designing post-training optimization like planning or reinforcement learning to adapt to specific environment (Guo et al., 2025; Zhang et al., 2025d; Bai et al., 2025c; Tan et al., 2025; Li et al., 2025b; Fei et al., 2025a; Chen et al., 2025b; Huang et al., 2025a; Lu et al., 2025; Xiao et al., 2025a;b). Additionally, subset of VLAs explores some niche but important aspects (Zhou et al., 2025; Wang et al., 2025b; Kareer et al., 2025; Shi et al., 2025; Fu et al., 2025; Pertsch et al., 2025; Goyal et al., 2025; Zhang et al., 2026), such as latent actions (Ye et al., 2024; Bi et al., 2025), lightweight VLAs (Wen et al., 2025; Li et al., 2025a; Reuss et al., 2025; Wang et al., 2025c) and VLAs in specific domains (Chen et al., 2025a; Bjorck et al., 2025; Ding et al., 2025; Huang et al., 2025b). Despite their different emphases, most VLA models follow similar pipeline that builds on pretrained LLMs or VLMs to process visual observations and language instructions and produce action-relevant representations for policy learning, yet this pipeline admits many design choices spanning model interfacing, policy training, perception, and action modeling. As result, early VLA research remains primordial soup: rich in ideas but insufficiently structured, and the diversity of existing frameworks, together with inconsistent training and evaluation protocols, makes it difficult to identify truly impactful choices."
        }
    ],
    "affiliations": [
        "ACE Robotics",
        "S-Lab, Nanyang Technological University",
        "SenseTime Research",
        "Shanghai Jiao Tong University",
        "Sun Yat-sen University"
    ]
}