{
    "paper_title": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference",
    "authors": [
        "Xiangyu Zhao",
        "Shengyuan Ding",
        "Zicheng Zhang",
        "Haian Huang",
        "Maosong Cao",
        "Weiyun Wang",
        "Jiaqi Wang",
        "Xinyu Fang",
        "Wenhai Wang",
        "Guangtao Zhai",
        "Haodong Duan",
        "Hua Yang",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in open-source multi-modal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving a significant gap in human preference alignment. This paper introduces OmniAlign-V, a comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs' alignment with human preferences. We also present MM-AlignBench, a human-annotated benchmark specifically designed to evaluate MLLMs' alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised Fine-Tuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/PhoenixZ810/OmniAlign-V."
        },
        {
            "title": "Start",
            "content": "OmniAlign-V: Towards Enhanced Alignment of MLLMs with Human Preference Xiangyu Zhao1,2* Shengyuan Ding2,3* Zicheng Zhang1,2 Haian Huang2 Maosong Cao2 Weiyun Wang2,4 Jiaqi Wang2 Xinyu Fang2,5 Wenhai Wang2 Guangtao Zhai1,2 Haodong Duan2 Hua Yang1,2 Kai Chen2 Shanghai Jiaotong University1 Shanghai AI Laboratory2 Nanjing University Fudan University4 Zhejiang University5 5 2 0 2 5 2 ] . [ 1 1 1 4 8 1 . 2 0 5 2 : r Figure 1: OmniAlign-V consists of curated images paired with open-ended, comprehensive question-answer pairs, significantly improving the alignment of MLLMs with human preferences. Additionally, we introduce MM-AlignBench, human-annotated, high-quality benchmark specifically designed to evaluate the ability of MLLMs to align with human values."
        },
        {
            "title": "Abstract",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in open-source multimodal large language models (MLLMs) have primarily focused on enhancing foundational capabilities, leaving significant gap in human preference alignment. This paper introduces OmniAlign-V, comprehensive dataset of 200K high-quality training samples featuring diverse images, complex questions, and varied response formats to improve MLLMs alignment with human preferences. We also present MM-AlignBench, human-annotated benchmark specifically designed to evaluate MLLMs alignment with human values. Experimental results show that finetuning MLLMs with OmniAlign-V, using Supervised FineTuning (SFT) or Direct Preference Optimization (DPO), significantly enhances human preference alignment while maintaining or enhancing performance on standard VQA benchmarks, preserving their fundamental capabilities. Our datasets, benchmark, code and checkpoints have been released at https://github.com/ PhoenixZ810/OmniAlign-V. *Equal Contribution. Corresponding Author. With the rapid advancement of large language models (LLMs) (OpenAI, 2023a; Touvron et al., 2023), multi-modal large language models (MLLMs) (OpenAI, 2023b; Team et al., 2023) have also seen significant improvements. Most opensource MLLMs (Liu et al., 2023b; Chen et al., 2023b) are developed by connecting vision encoder (Dosovitskiy, 2020) to pretrained LLM, followed by vision instruction tuning. Existing vision instruction tuning datasets (Liu et al., 2023b; Chen et al., 2023a, 2024a) and multi-modal evaluation benchmarks (Liu et al., 2023d; Yue et al., 2023; Lu et al., 2023) primarily focus on assessing fundamental capabilities (object recognition, OCR, etc.) of MLLMs, while paying little attention to human preference alignment. Consequently, while open-source MLLMs achieve comparable performance to proprietary counterparts on objective metrics of these foundational skills, they display significant gap in the alignment with human preferences, which detrimentally impacts user experience in multi-modal conversational interactions, as demonstrated in Appx. A. In this work, preliminary investigation was conducted to quantitatively assess the degradation of human preference alignment in MLLMs(Sec. 3). Experimental results on text-only subjective evaluation benchmarks (Dubois et al., 2024; Li et al., 2024b) revealed that MLLMs exhibited substantial performance drop compared to their corresponding LLMs. plausible hypothesis suggests that MLLMs suffer from catastrophic forgetting during vision instruction tuning. Following this hypothesis, straightforward solution would be incorporating both multi-modal and text-only SFT data (Xu et al., 2024; Cao et al., 2025; Ding et al., 2023) for joint learning. However, our experiments (Tab. 2) indicate that this approach not only failed to yield improvements in human preference alignment under multi-modal contexts but also demonstrated negative impacts on the foundational multi-modal skills of MLLMs. These observations suggest that enhancing human alignment in multi-modal scenarios necessitates the development of specialized multi-modal instruction tuning datasets. Existing multi-modal instruction tuning datasets predominantly focus on foundational capabilities, featuring simple language patterns and uniform response styles (Fig. 5 in appendix). This study argues that effective multi-modal datasets for enhancing human preference alignment should incorporate several critical characteristics: open-ended questions, broad topic coverage, diverse response formats (varying in length and style), and strict adherence to instructions. Based on these principles, we constructed OmniAlign-V. In terms of image sources, OmniAlign-V encompasses natural images and infographics such as posters and charts. Furthermore, novel solution was developed to filter out semantically rich images from natural image collections. With respect to tasks, complex knowledge-based question answering, creative tasks, and reasoning tasks were designed for different image types. Each task category incorporates diverse subtasks, and state-of-the-art MLLMs were leveraged to obtain diverse, high-quality responses. Ultimately, OmniAlign-V comprises 200K multimodal instruction tuning samples, exhibiting significantly different overall data distribution compared to existing multi-modal SFT datasets. Leveraging OmniAlign-V, comprehensive experiments were conducted to explore its full potential for enhancing human alignment in MLLMs. Integrating OmniAlign-V into the SFT stage of the LLaVA-NeXT (Liu et al., 2024a) structure with InternLM2.5-7B and Qwen2.5-32B yielded significant improvements in human preference alignment across various powerful LLM encoders, including InternLM2.5 (Cai et al., 2024) and Qwen2.5 (Bai et al., 2023a). Furthermore, on ground-truth-based VQA benchmarks like MMMU (Yue et al., 2023) or OCRBench (Liu et al., 2023e), MLLMs finetuned with OmniAlign-V displayed comparable or superior performance. Beyond its application in SFT, OmniAlign-V also demonstrated additional value when applied to Direct Preference Optimization (DPO) (Rafailov et al., 2024). Experimental results indicate that implementing OmniAlign-V for DPO yields further improvements in preference alignment, surpassing baseline models finetuned on the same dataset. After the SFT and DPO stage, our LLaVA-Next baseline with Qwen2.5-32B surpasses the state-of-the-art model Qwen2VL72B (Chen et al., 2024b) finetuned on extensive proprietary datasets. Throughout the exploration, we also observed that existing multi-modal human preference benchmarks (Lu et al., 2024; Qian et al., 2024) lack diversity in image sources, contain repetitive questions, and lack clarity. To address these limitations, we introduce MM-AlignBench, high-quality benchmark comprising 252 carefully curated samples with diverse image sources and meticulously crafted questions by human annotators, enabling comprehensive evaluation of MLLMs alignment with human preferences. MM-AlignBench, along with other MLLM human preference benchmarks, was employed throughout this study for evaluation. The contributions of this study are as follows: 1. An in-depth investigation into the degradation of MLLM human alignment, analyzing the impact of both text-only and multi-modal tuning data. 2. The introduction of OmniAlign-V, comprehensive open-ended multi-modal SFT dataset, complemented by OmniAlign-V-DPO. Extensive experiments demonstrate the effectiveness of these datasets in improving human preference alignment. 3. The development of MM-AlignBench, carefully curated benchmark comprising high-quality, human-annotated samples specifically designed to evaluate MLLMs human preference alignment."
        },
        {
            "title": "2 Related Work",
            "content": "Alignment of LLMs. Alignment, encompassing the ability to follow human instructions and provide helpful assistance (Liu et al., 2024b), has long been critical aspect of LLMs. Recent works (Wang et al., 2023; Xu et al., 2024; Cao et al., 2025) focused on generating high-quality SFT training data to enhance the alignment of LLMs. Besides, recent benchmarks (Liu et al., 2023c; Li et al., 2023, 2024b) have introduced open-ended and challenging questions to assess the alignment performance of LLMs. However, there is notable lack of benchmarks designed to evaluate the alignment of MLLMs. Visual Question Answering Datasets. In the early stages, Visual Question Answering (VQA) datasets were primarily used for basic semantic alignment between images and text (Radford et al., 2021; Li et al., 2022; Zhao et al., 2024). These datasets were relatively simple in structure. Popular VQA datasets (Hudson and Manning, 2019; Antol et al., 2015) predominantly featured straightforward questions that elicited single-word or short-sentence answers. Similarly, visual classification and detection datasets (Lin et al., 2014; Schuhmann et al., 2021; Sharma et al., 2018) typically consisted of brief descriptions that focused on the main object within the image. Instruction Tuning data. With the rapid development of MLLMs (Chen et al., 2024b; Bai et al., 2023b), the instruction tuning data has gained more attention. LLaVA (Liu et al., 2023b) leveraged advanced LLMs to generate instruction-following formatted data from traditional VQA datasets. Recent works have further expanded this approach by employing MLLM to generate captions (Chen et al., 2023a), complex question-answer pairs (Chen et al., 2024a; Gu et al., 2024), OCR data (Carter, 2024) and math-related data (Shi et al., 2024). Other efforts (Tong et al., 2024; Li et al., 2024a) aggregates multiple publicly available document image datasets into unified resources. However, these datasets primarily focus on fundamental visual capabilities with short dialogues and factual questions, resulting in suboptimal alignment with human preferences."
        },
        {
            "title": "Preliminary Study",
            "content": "When handling open-ended and flexible questions involving images, state-of-the-art open-source MLLMs despite excelling in certain recognition tasks exhibit significantly weaker alignment with human preferences compared to GPT-4o (Fig. 6 in appendix). We hypothesize that this decline in multi-modal preference alignment stems from AlignBench AlpacaEval-V2 6.36 Method InternLM2.5-7B InternVL2-8B LLaVA-Internlm Qwen2-7B Qwen2VL-7B LLaMA3-8B MiniCPM-V2.5 InternLM2-20B InternVL2-26B Hermes2-llama3-70b LLM InternVL2-76B Type LLM MLLM 4.04(-36.4%) MLLM 4.66(-26.7%) LLM MLLM 4.92(-18.3%) LLM MLLM 3.84(-21.3%) LLM MLLM 4.39(-20.0%) MLLM 4.33(-24.3%) 5.49 5.72 6.02 4.88 27.58 3.35(-87.9%) 4.22(-84.7%) 24.47 3.85(-83.4%) 30.19 7.33(-75.7%) 43.35 5.34(-87.7%) 46.09 8.32(-81.9%) ArenaHard 27.06 4.65(-82.8%) 4.93(-81.8%) 32.84 6.46(-80.3%) 31.96 8.05(-74.8%) 33.75 11.25(-66.7%) 57.40 16.17(-72.3%) Table 1: Language Alignment Benchmark Results. After multi-modal SFT, MLLMs demonstrate significant decline in human preference alignment compared to their corresponding LLMs. reduction in the language models proficiency after the multi-modal SFT stage. To test this hypothesis, we evaluated state-of-the-art MLLMs on text-only human preference alignment benchmarks (Liu et al., 2023c; Li et al., 2023, 2024b). Additionally, we constructed LLaVA (Liu et al., 2023b) baseline using InternLM-2.5-7B, fine-tuned on the LLaVA-Next-778k (Liu et al., 2023a) SFT dataset. As shown in Tab. 1, the LLMs ability to handle text-only open-ended questions degrades significantly after multi-modal SFT. This degradation may be due to (1) insufficient quantity or quality of text-only samples during multi-modal SFT, or (2) the overly simplistic style of multimodal fine-tuning data derived from traditional VQA datasets (Goyal et al., 2017; Hudson and Manning, 2019), often consisting of simple questions and short, factual answers. To assess the impact of the first factor on multimodal alignment, we examined the role of text-only data in multi-modal SFT. The LLaVA-Next-778K dataset includes approximately 40K text-only samples from ShareGPT (Chiang et al., 2023), which are relatively outdated and of lower quality. To improve alignment, we experimented with replacing these samples with better-quality alternatives and increasing their proportion in the multi-modal fine-tuning data. Specifically, we sampled 40K / 80K instances from two high-quality language SFT datasets: Magpie-Llama3.3 (Xu et al., 2024) and Condor (Cao et al., 2025), using these to replace the original language data in LLaVA-Next. We then evaluated models fine-tuned on these different data mixtures using both text-only benchmarks and multi-modal benchmarks, including WildVision (Lu et al., 2024) for human preference alignment and various benchmarks from OpenVLM Leaderboard (Duan et al., 2024) for assessing variModel LLaVAI -LLaVANext778k LLaVAI -LLaVANextmm738k-Magpie40k LLaVAI -LLaVANextmm738k-Condor40k LLaVAI -LLaVANextmm738k-Condor80k Language Benchmarks Multi-modal Benchmarks AlignBench AlpacaEval-V2 ArenaHard WildVision MMVet MMBench-V1.1 AI2D OCRBench 4.7 4.5 5.6 5.7 31.7 65.5 72.9 75.2 21.6 44.6 55.7 55.7 18.4/-55.1 16.8/-58.9 16.8/-57.4 16.8/-57.0 41.2 37.7 38.3 38.3 73.7 73.1 72.6 72.6 74.2 73.4 73.6 74.0 39.7 38.7 38.5 37.6 Table 2: Performance of Incorporating High-Quality Language Data. LLaVAI denotes LLaVA structure with InternLM2.5-7B as language model, and LLaVANextmm738k refers to the multi-modal data in LLaVANext-778K. Integrating high-quality language data significantly improves alignment performance on language benchmarks. However, it leads to decline in multi-modal alignment performance on benchmarks such as WildVision and MMVet. For AlpacaEval and ArenaHard, we present the winning rate against GPT-3.5. ous fundamental multi-modal capabilities.* Results in Tab. 2 reveal several key findings. While MLLMs tuned with higher-quality text-only samples show significant improvements on textonly alignment benchmarks, they unexpectedly demonstrate degraded performance on both multimodal alignment and common VQA benchmarks. This counter-intuitive phenomenon suggests that language alignment capability does not directly translate to multi-modal alignment. We therefore argue that high-quality multi-modal human-aligned data is crucial for improving MLLMs human preference alignment in multi-modal contexts."
        },
        {
            "title": "4 OmniAlign-V",
            "content": "Current MLLM instruction tuning datasets primarily focus on enhancing basic capabilities like perception, OCR, and mathematical reasoning. These datasets typically contain simple, brief questionanswer pairs that inadequately capture human preferences and real-world interaction complexity, as shown in Fig. 5. We propose that multi-modal training data should also incorporate: (1) Open-ended, Diverse, and Creative Questions requiring interdisciplinary knowledge, (2) Comprehensive and Knowledge-rich Responses. To address these requirements, we develop novel data synthesis pipeline to generate high-quality human-aligned multi-modal training data, resulting in the creation of OmniAlign-V."
        },
        {
            "title": "4.1 Task Taxonomy: A Big Picture",
            "content": "Image content plays crucial role in constructing multi-modal training data. To ensure comprehensive coverage, we classify images into two major categories: natural images and infographic images, as shown in Fig. 3(a). Our data synthesis pipeline first determines whether an input image belongs to natural images (captured from real-world scenes) or infographic images (human-created to convey *MMVet is open-ended, while others are closed-ended. information). Based on this classification, different vision-language tasks are assigned. For natural images, we define three primary tasks: Knowledge, Inferential, and Creation, each requiring diverse and complex question formats with comprehensive, reasoned responses. These tasks enhance the models ability to interpret real-world scenes effectively. For infographic images, given their diverse content, we identify four key types that elicit intricate and challenging questions: Arts, Charts, Diagrams, and Posters. These categories necessitate deep understanding of human-designed visuals, including both abstract and detailed elements. 4."
        },
        {
            "title": "Image Selection Strategy",
            "content": "For natural images, rich semantic content leads to more comprehensive and insightful QA pairs. To enhance training data quality, we developed pipeline to select semantically rich images from diverse sources, as shown in Fig. 2(a). This pipeline consists of two key steps. First, we use the Image Complexity (IC) recognition model IC9600 (Feng et al., 2022) to assign IC scores to images. Images with low semantic content - characterized by few objects or simple, uniform backgrounds - receive low IC scores and are then excluded. While IC9600 effectively filters out low-complexity images, high IC scores alone do not guarantee semantic richness. For instance, as shown in Fig. 7, an image filled with tents may have high IC score but lacks sufficient semantic information for multi-modal training. To refine selection, we employ the Recognize Anything Model (Zhang et al., 2024) to identify objects within images, filtering out those with high complexity but minimal meaningful content. This two-step approach ensures our pipeline accurately selects images that are both complex and semantically rich. Experiment results have validated the effectiveness of our proposed image selection strategy, as shown in Appx. E. Figure 2: Overall pipeline of OmniAlign-V. By utilizing an image filter and employing customized pipeline for distinct tasks, we curate semantically rich images paired with high-quality open-ended question-answer sets. Post-refinement further enhances both the variety and quality of our dataset."
        },
        {
            "title": "4.3 Data Generation Pipeline",
            "content": "seed creative questions: SFT QA Generation. In Fig. 2(b), we outline the generation process of SFT QA pairs. For vision-language tasks involving natural images (knowledge, creative, inferential), we first apply our image selection strategy to filter semantically rich images from CC-3M(Sharma et al., 2018), Flickr30k (Plummer et al., 2015), and GQA (Hudson and Manning, 2019). For infographic image tasks, images are collected from various existing sources (Masry et al., 2022; Li et al., 2018; Kembhavi et al., 2016). More details are provided in Appx. B. Knowledge & Inferential Tasks. In preliminary experiments, we found that GPT-4o can generate diverse, content-relevant questions for the two tasks when provided with well-designed few-shot prompts. Consequently, we carefully designed single prompt for each task category, incorporating comprehensive instructions and selected few-shot examples. These prompts were then used with GPT4o across diverse images to generate QA pairs. Creative Tasks. We noticed that single prompt cannot generate sufficiently diverse, contentrelevant creative questions. Therefore, we developed more sophisticated pipeline inspired by Condor (Cao et al., 2025). First, we created set of Qs = {Q1, Q2, ...QN } (1) where each seed question corresponds to distinct creative task. Since directly using all seed questions as few-shot examples leads to repetition and lack of diversity, we employ light-weight MLLM (Luo et al., 2024) to generate detailed captions for each image. An LLM then selects relevant subset of seed questions according to the caption: = M(C, Qs), s Qs (2) Finally, we randomly select three question types from as few-shot examples for GPT-4o, preserving both quality and diversity of synthesized data. Infographic Tasks. For infographic image tasks (Charts, Diagrams, etc.), questions and answers are closely tied to specific image content. Unlike natural images, these visuals convey information primarily through text, colors, lines, and symbolic elements, making evaluation based on image complexity or object categories unsuitable. Therefore, instead of applying image selection strategies, we carefully select image sources containing rich, detailed information. We then design specialized Figure 3: Data distribution of OmniAlign-V. Our dataset includes diverse range of tasks, characterized by more balanced distribution of answer lengths compared to those observed in ALLaVA and ShareGPT4V. prompts for GPT-4o to generate questions that require comprehensive background knowledge understanding. The difference is shown below: InfographicVQA: What is the respiratory disease death rate for individuals aged 70+? OmniAlign-V: How does the respiratory disease death rate for individuals aged 70+ compare to the other age groups, and what might this suggest? Post Refinement. To further improve the quality of the synthesized data, we implemented series of post-processing methods for refinement, as illustrated in Fig. 2(c). Instruction Augmented Knowledge QAs. Instruction following is crucial capability significantly impacting human preference. To enhance this, we incorporate complex instructions and restrictions into our knowledge QA pairs and reformulate responses accordingly. As shown in Fig. 2(c1), for each knowledge QA, we use powerful LLM to select an appropriate instruction type that can be integrated into the question without depending on visual content. The instruction is then incorporated into the existing question, and an LLM adjusts the corresponding answer to ensure alignment with both the modified question and original context, resulting in instruction-augmented knowledge QAs. Enriched Inferential QAs. For many inferential QAs, the answers lack sufficient detail to fully explain underlying logic and background knowledge. To address this, we employ knowledge-rich LLM to enrich responses with detailed explanations, relevant background information, and logical reasoning (Fig. 2(c2)). This refinement enhances alignment with user preferences. Quality Improved Infographic QAs. For infographic tasks, particularly those involving Chart data, we observed that even SOTA MLLMs strugFigure 4: Samples in MM-AlignBench. gle with complex charts and detailed questions. While GPT-4o excels at explaining background knowledge but often produces inaccurate OCR results, SOTA open-source MLLMs (Chen et al., 2024b; Bai et al., 2023b) show superior OCR accuracy but lack detailed explanations. Therefore, we developed refinement pipeline to generate responses combining rich background knowledge and accurate OCR results. We filter out questions where GPT-4o and SOTA open-source MLLM responses show significant discrepancies in trends. For remaining questions, the responses from different MLLMs are merged to produce final response that is both precise and comprehensive. The merged answers are further reviewed by human experts to ensure their quality and consistency. More details are demonstrated in Appx. C. OmniAlign-V comprises 39K Knowledge QAs, 37K Inferential QAs, 10K Creative QAs, 38K Instruction-Following (Knowledge) QAs, and 44K Infographic QAs (2K Art, 8K Diagram, 11K Chart, 23K Poster). Additionally, we prompt GPT-4o to generate 35K QAs focusing on image details, resulting in total of 205K high-quality SFT training samples. Examples are shown in Figs. 9 to 12. DPO Data Generation. OmniAlign-Vs highquality, human-aligned QA pairs can serve as Inspired by positive samples for DPO training. Reject Sampling (Casella et al., 2004), we generate negative samples by prompting LLaVANext baseline (generator G) trained on LLaVANext-778k. For each question Qi, the generator produces responses with high temperature, = {ri }. An LLM Judger then evaluates these responses to select the one that most deviates from the original questions intent and context as the negative sample ri eg, ensuring clear contrast between positive and negative samples. 2, ...ri 1, ri Model LLaVA LLaVA Data LLaVANext-778k InternLM2.5-7B InternLM2.5-7B OmniAlign-Vmix LLM LLaVANext LLaVANext-778k InternLM2.5-7B InternLM2.5-7B LLaVANext OmniAlign-Vmix LLaVANext LLaVANext-778k Qwen2.5-32B LLaVANext OmniAlign-Vmix Qwen2.5-32B MM-AlignBench WildVision MIA-Bench MMVet MMMU MMBenchV1.1 AI2D OCRBench 3.6 / -82.1 50.0 / +3.8 + 46.4 / 85.9 20.6 / -42.7 57.1 / +11.1 + 36.5 / 53.8 26.6 / -29.0 62.3 / +19.4 + 35.7 / 48.4 18.4 / -55.1 28.2 / -34.6 + 9.8 / 20.5 23.4 / -45.0 29.6 / -31.3 + 6.2 / 13.7 25.2 / -41.3 40.2 / -14.9 + 15.0/26.4 75.4 85.4 + 10.0 76.9 86.7 + 9.8 86.0 89.6 + 3.6 41.2 43.5 + 2.3 41.8 47.7 + 5.9 47.7 56.9 + 9.2 42.6 43.3 + 0.7 44.1 46.8 + 2.7 55.2 60.7 + 5. 73.6 73.7 + 0.1 75.1 74.9 - 0.2 79.3 80.6 + 1.3 74.1 74.7 + 0.6 74.7 77.5 + 2.8 79.6 81.7 + 2.1 39.7 41.3 + 1.6 56.2 58.9 + 2.7 55.9 55.9 + 0.0 Table 3: Evaluation Results of Integrating OmniAlign-V into the SFT stage. By integrating OmniAlign-V, the multi-modal preference alignment of MLLMs significantly improved. Additionally, we also observe comparable or better performance on common VQA benchmarks. In Model column, LLaVA and LLaVANeXT denote the model structure and training strategy. For MM-AlignBench and WildVision, notation A/B denotes Winning Rate/Rewards. Model Type AlpacaEvalv2 ArenaHard v.s. GPT-3.5/GPT-4 LLaVANextI 29.8 / 3.8 MLLM LLaVANextI -OA MLLM 50.1 / 7.8 78.3 / 26.2 InternLM2.5-7B LLaVANextQ 50.6 / 7.0 LLaVANextQ-OA MLLM 77.6 / 18.0 92.1 / 37.1 Qwen2.5-32B LLM MLLM LLM 21.4 / 4.93 30.4 / 9.33 47.5 / 19.1 54.5 / 18.0 87.2 / 58.1 94.8 / 75.9 Table 4: Performance on text-only alignment benchmarks. OA denotes models trained with OmniAlign-V. We present the winning rate against GPT-3.5 and GPT4 (original setting). OmniAlign-V can also enhance MLLMs performance on text-only alignment benchmarks."
        },
        {
            "title": "5 MM-AlignBench",
            "content": "Current benchmarks for assessing multi-modal alignment capabilities are limited. While WildVision (Lu et al., 2024) aims to evaluate human preferences in real-world interactions, it employs repetitive and simplistic question formats that inadequately assess response quality, as shown in Fig. 8. To address this, we developed MM-AlignBench, featuring human-curated images and questions for more nuanced evaluation. We selected high-quality images from SAM1B (Kirillov et al., 2023), CC-3M-Test (Sharma et al., 2018), AI2D (Kembhavi et al., 2016), ChartQA (Masry et al., 2022), and InfographicVQA (Mathew et al., 2022). For natural images, we applied our image selection strategy from Sec. 4.2 to identify 2,000 semantically rich images and combine them with 1,000 carefully selected infographics images. GPT-4o was then used to generate diverse questions for these images. Human experts reviewed and refined the image-question pairs, filtering out low-quality, repetitive, or contextually weak samples. This process resulted in 252 high-quality question-image pairs, featuring diverse question types and semantically rich images. Several examples are shown in Fig. 4. For evaluation, we follow WildVisions approach, using GPT-4o as the judge model to compare model responses with reference responses generated by Claude3V-Sonnet (Anthropic, 2024), reporting winning rates and reward scores."
        },
        {
            "title": "6.1 SFT with OmniAlign-V",
            "content": "We conduct extensive experiments to demonstrate OmniAlign-Vs effectiveness. We combine OmniAlign-V with LLaVA-Next-778k (excluding text-only samples), creating OmniAlign-Vmix with 946K training samples. We evaluate various MLLMs tuned on OmniAlign-V against their counterparts tuned on LLaVA-Next-778k. Our evaluation spans multiple multi-modal benchmarks, including standard VQA benchmarks (Yu et al., 2023; Liu et al., 2024c, 2023e; Yue et al., 2023; Kembhavi et al., 2016) and human-preference alignment benchmarks: MMAlignBench, WildVision (Lu et al., 2024), and MIA-Bench (Qian et al., 2024). Results in Tab. 3 show that OmniAlign-V significantly improves human alignment across all benchmarks. Moreover, our training data improves general multi-modal capabilities, particularly on MMVet and MMMU, demonstrating trend distinct from text-only data. Notably, despite excluding language samples from training data, models maintain stronger language alignment than those trained on LLaVANext-778k, as shown in Table 4. This suggests that while high-quality language data alone may not significantly impact multi-modal capabilities, enhancing multi-modal data quality can improve both language and multi-modal performance, highlighting the crucial role of high-quality, human-aligned multi-modal training data."
        },
        {
            "title": "6.2 DPO with OmniAlign-V-DPO",
            "content": "We conduct DPO post-training on three models: LLaVA-Next trained with LLaVA-Next-778k, Stage SFT SFT+DPO Model LLaVANextI LLaVANextI LLaVANextI -OA SFT LLaVANextI -OA SFT+DPO InternVL2-8B InternVL2-8B SFT SFT+DPO MM-AlignBench WildVision 30.4 / -34.2 35.5 / -23.4 29.6 / -31.3 41.8 / -10.1 48.6 / +1.4 51.4 / +1.9 9.5 / -69.2 11.1 / -64.5 57.1 / +11.1 64.3 / +22.4 31.4 / -21.8 64.7 / +19. Table 5: Performance of applying DPO with OmniAlign-V-DPO. For models finetuned with humanaligned data, by employing DPO training, the models alignment with human perference further improved. LLaVA-Next trained with OmniAlign-Vmix, and InternVL2-8B. Results in Tab. 5 show that DPO tuning significantly improves performance on realworld questions (WildVision) across all models. While the baseline trained solely on LLaVANext-778k shows minimal improvement on MMAlignBench, models incorporating OmniAlignV during SFT demonstrate substantial gains after DPO. Similarly, InternVL2-8B, state-of-theart MLLM partially trained on proprietary multimodal corpora, shows significant improvement on MM-AlignBench post-DPO. This indicates that if model has been trained on data aligned with human preferences, such as open-ended or long-context data during SFT phase, the subsequent DPO training on high-quality human-aligned data can significantly activate the models ability, leading to considerable improvement in alignment performance. In contrast, if the model has not been exposed to such alignment-focused datasets during SFT, training with open-ended data alone via DPO will not significantly improve its capabilities of alignment. These findings demonstrate the value of OmniAlign-V in both SFT and DPO stages for enhancing human preference alignment."
        },
        {
            "title": "6.3 MM-AlignBench",
            "content": "We evaluate various state-of-the-art MLLMs (OpenAI, 2023a; Team et al., 2023; Anthropic, 2024; Bai et al., 2023b; Chen et al., 2024b; Li et al., 2024a; OpenBMB, 2024; Dong et al., 2024; Meta, 2024; Laurençon et al., 2024; Wang et al., 2024) on MM-Alignbench, with results shown in Tab. 6. Closed-source models like GPT, Claude, and Gemini demonstrate strong alignment in responding to open-ended questions. In contrast, while Qwen2VL and InternVL2 excel at common VQA tasks, they show relatively lower human preference alignment. This highlights the importance of prioritizing MLLM alignment for improved daily human interactions. Our LLaVA-OA-32B, trained Model Claude3.5V-Sonnet GPT-4o GPT-4V GeminiFlash1.5-002 LLaVANext-OA-32B-DPO Qwen2VL-72B LLaVANext-OA-32B Claude-3V-Sonnet Qwen2VL-7B InternVL2-72B InternVL2-8B-MPO InternVL2-8B LLaMA3.2-Vision-11B LLaVANext-Qwen32B LLaVA-OneVision-7B MiniCPM-V-2.5 Xcomposer2.5-7B Idefics3-8B Win Rate Reward B+ W+ 4 70 144 12 31 4 81 124 12 31 1 57 157 12 31 9 56 138 14 35 5 49 138 20 40 7 43 112 15 75 14 31 126 19 62 - - - - - 5 101 34 28 84 93 19 34 98 8 75 10 100 41 26 61 15 109 49 18 80 52 18 51 10 121 54 16 1 46 14 75 116 8 116 96 23 9 63 167 3 14 5 15 230 0 4 3 +51.4 +49.0 +46.0 +39.1 +36.9 +21.6 +19.4 0 -5.8 -6.9 -10.9 -21.8 -33.7 -29.0 -46.2 -53.0 -74.0 -92.3 84.9 81.3 82.5 77.0 74.2 61.5 62.3 50 44.4 44.4 40.1 31.3 27.8 26.6 23.8 12.7 7.5 2.7 4 Table 6: Performance of existing MLLMs on MM-AlignBench. B+/B/T/W/W+ denotes MuchBetter/Better/Tie/Worse/MuchWorse. Our LLaVA-NextOmniAlign(OA)-32B-DPO, trained with OmniAlignV and applied DPO with OmniAlign-V-DPO, demonstrates outstanding performance, surpassing wide range of strong MLLMs, even Qwen2VL-72B. with OmniAlign-V, achieves exceptional performance, outperforming numerous strong MLLMs and nearly matching Qwen2VL-72B. After applying DPO with OmniAlign-V-DPO, LLaVA-OA32B-DPO achieves winning rate of 72.6 with an average reward of +33.5, surpassing the performance of Qwen2VL-72B. These results highlight the high quality and effectiveness of the OmniAlign-V dataset in improving model alignment with human preferences."
        },
        {
            "title": "6.4 Ablation Study",
            "content": "We conduct an ablation study to evaluate each subset of OmniAlign-V, reporting results on MMAlignbench, WildVision, and MMVet in Tab. 7. Performance improves progressively as different tasks from OmniAlign-V are incorporated. Notably, Instruction-Following data significantly enhances performance across all three benchmarks, demonstrating its crucial role. The creation data subset uniquely improves performance on MMAlignbench but not on WildVision and MMVet. This discrepancy can be attributed to the absence of multi-modal creative question types in these two benchmarks, suggesting their incompleteness in capturing full spectrum of alignment challenges."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduce OmniAlign-V, dataset designed to enhance the alignment of MLLMs with human preferences, as well as MM-AlignBench, high-quality, specific-purpose benchmark for evalModel LLaVANext-778k +Know / Infer / Detail +Instruction Following +Creation +Chart / Diagram / Poster MM-AlignBench WildVision MMVet 20.6 / -42.7 23.4 / -42.1 36.5 / -17.3 43.7 / -5.0 57.1 / +11.1 23.4 / -45.0 23.2 / -43.2 26.2 / -37.5 26.6 / -37.7 29.6 / -31.3 41.7 42.8 44.6 44.4 47. Table 7: Ablation of Subsets in OmniAlign-V. By progressively incorporating tasks within OmniAlign-V, the models alignment performance gradually improves. uating such alignment. We investigate the impact of both language and multi-modal training data, emphasizing the critical role of multi-modal openended training data. By incorporating OmniAlignV into SFT and DPO stages, we achieve significant improvements in the alignment of MLLMs."
        },
        {
            "title": "8 Limitation",
            "content": "Although the OmniAlign-V pipeline can be easily scaled to support larger datasets, the scale of the dataset used in this paper may be insufficient for large-scale training due to cost limitations. Deeper exploration into the alignment of MLLMs is still needed to address these limitations and further advance the field."
        },
        {
            "title": "References",
            "content": "Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023a. Qwen technical report. arXiv preprint arXiv:2309.16609. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023b. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, Internlm2 technical report. and Dahua Lin. 2024. Preprint, arXiv:2403.17297. Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, and Kai Chen. 2025. Condor: Enhance llm alignment with knowledge-driven data synthesis and refinement. arXiv preprint arXiv:2501.12273. Jimmy Carter. 2024. Textocr-gpt4v. https: //huggingface.co/datasets/jimmycarter/ textocr-gpt4v. George Casella, Christian Robert, and Martin Wells. 2004. Generalized accept-reject sampling schemes. Lecture notes-monograph series, pages 342347. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. 2024a. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. Preprint, arXiv:2402.11684. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023a. Sharegpt4v: Improving large multimodal models with better captions. arXiv preprint arXiv:2311.12793. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2024b. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Preprint, arXiv:2404.16821. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. 2023b. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2024. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420. Alexey Dosovitskiy. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Jiaqi Wang, et al. 2024. Zang, Pan Zhang, Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pages 1119811201. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Tinglei Feng, Yingjie Zhai, Jufeng Yang, Jie Liang, Deng-Ping Fan, Jing Zhang, Ling Shao, and Dacheng Tao. 2022. Ic9600: benchmark dataset for automatic image complexity assessment. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(7):85778593. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913. Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et al. 2024. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data. arXiv preprint arXiv:2410.18558. Drew Hudson and Christopher Manning. 2019. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. 2016. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11 14, 2016, Proceedings, Part IV 14, pages 235251. Springer. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. 2023. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026. Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. 2024. Building and better understanding vision-language models: insights and future directions. Preprint, arXiv:2408.12637. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. 2024a. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR. Juzheng Li, Hang Su, Jun Zhu, Siyu Wang, and Bo Zhang. 2018. Textbook question answering under instructor guidance with memory networks. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36553663. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024b. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024a. Llavanext: Improved reasoning, ocr, and world knowledge. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. arXiv preprint arXiv:2304.08485. Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. 2023c. Alignbench: Benchmarking chinese alignment of large language models. arXiv preprint arXiv:2311.18743. Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2024b. Trustworthy llms: survey and guideline for evaluating large language models alignment. Preprint, arXiv:2308.05374. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2023d. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. 2024c. Mmbench: Is your multi-modal model an all-around In European conference on computer viplayer? sion, pages 216233. Springer. Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. 2023e. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255. Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. 2024. Wildvision: Evaluating vision-language models in the wild with human preferences. arXiv preprint arXiv:2406.11069. Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. 2022. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706. Meta. 2024. Llama3.2-vision. OpenAI. 2023a. Chatgpt. https://openai.com/ blog/chatgpt. OpenAI. 2023b. Gpt-4 technical report. ArXiv, abs/2303.08774. OpenBMB. 2024. Minicpm: Unveiling the potential of end-side large language models. Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Flickr30k entities: Collecting Lazebnik. 2015. region-to-phrase correspondences for richer imageIn Proceedings of the IEEE to-sentence models. international conference on computer vision, pages 26412649. Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. 2024. Mia-bench: Towards better instruction following arXiv preprint evaluation of multimodal llms. arXiv:2407.01509. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. Laion-400m: Open dataset of clipfiltered 400 million image-text pairs. arXiv preprint arXiv:2111.02114. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL. Gen Luo, Xue Yang, Wenhan Dou, Zhaokai Wang, Jifeng Dai, Yu Qiao, and Xizhou Zhu. 2024. Monointernvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training. arXiv preprint arXiv:2410.08202. Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. 2024. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805."
        },
        {
            "title": "Preference",
            "content": "WikiArt Volunteer Team. 2025. Wikiart. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. 2024. Cambrian-1: fully open, vision-centric arXiv preprint exploration of multimodal llms. arXiv:2406.16860. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2023. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235. Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. 2024. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. 2023. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502. Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. 2024. Recognize anything: strong image tagging model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17241732. Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, and Haian Huang. 2024. An open and comprehensive pipeline for unified object grounding and detection. arXiv preprint arXiv:2401.02361. Although open-source MLLMs have already matched or even surpassed proprietary models like the GPT and Claude series in common VQA tasks like OCR and Visual Perception, significant gap remains in their alignment with human preferences. When posed with open-ended questions that require knowledge-rich responses, even the most advanced open-source MLLM, InternVL2-76B, struggles to provide comprehensive answers with high readability, as illustrated in Fig. 6. In contrast, GPT-4o not only accurately identifies the main objects relevant to the question but also provides well-structured responses enriched with comprehensive contextual knowledge, achieving high level of alignment with human preferences."
        },
        {
            "title": "B Image sources",
            "content": "image sources for Arts, We carefully select Charts, Diagrams, and Posters tasks. For Arts, WikiArt (Team, 2025) is selected as the image source, offering diverse range of painting styles. We uniformly sample 2000 images across all painting styles to ensure diversity in the dataset. For Charts, we select ChartQA (Masry et al., 2022), dataset featuring charts that contain substantial statistical information. ChartQA includes several subcategories, from which we filter out simplistic charts with only two columns and retain those that contain charts with rich contextual information and diverse types. For Diagrams, we choose images from TextbookVQA (Team, 2025), which provides diagrams rich in natural content and detailed explanations. We exclusively utilize the question images and teaching images from the image sources, as they meet our specific requirements. For Posters, we utilize InfographicsVQA (Mathew et al., 2022), dataset containing high-quality posters with intricate designs and informative content. Chart Post-Refine Pipeline After curating images along with high-quality question, we first employ the current most powerful models(GPT-4o, InternVL2-72B, Qwen2VL72B) to generate answers separately. powerful LLM(Qwen2.5-72B) is then utilized to extract objective facts from the chart within each generated answer. The facts extracted from the answers of multiple models are compared for consistency. If Model LLaVANext-77k + 33k w.o. Imager Filter + 33k w. Imager Filter MM-AlignBench WildVision MMVet 15.1/-52.6 31.4 / -42.3 35.3 / -41. 13.6 / -63.1 22.0 / -42.3 22.6 / -37.5 37.7 42.0 44.4 Table 8: Ablation study on the impact of utilizing image filter."
        },
        {
            "title": "E Ablation on Image Filter",
            "content": "We conduct ablation studies to evaluate the impact of using our image filter. We randomly sample 77k images from the LLaVA-Next-778K SFT dataset as baseline and separately select images from CC3M both with and without applying our image filter. Subsequently, the selected images are used to generate Knowledge and Inferential QuestionAnswer pairs following the pipeline described in Sec. 4.3. We then assess the effect of these datasets on the performance of LLaVA-Next-InternLM2.57B, with the results presented in Tab. 8. It can be observed that by utilizing the image filter, the selected images contain richer semantic information, leading to the generation of higher-quality image-question pairs. This, in turn, enhances the models performance in terms of alignment with human preferences."
        },
        {
            "title": "F Human Experts",
            "content": "In this study, two authors (both PhD students in computer science) serve as human experts. They are responsible for reviewing and refining the questions in MM-AlignBench, as well as evaluating and filtering incorrect merged cases following the Chart Post-Processing Refinement stage."
        },
        {
            "title": "G License",
            "content": "The InternLM and Qwen models are licensed under the Apache-2.0 license. The ChartQA dataset is distributed under the GNU General Public License v3.0. The remaining datasets are licensed under CC BY-NC 4.0, which permits only non-commercial use. Figure 5: Examples of limitation with current multimodal instruction tuning dataset. the facts differ significantly and lead to entirely different conclusions, such responses are flagged for further review or discarded to avoid misinformation. For cases where the facts exhibit only minor differences, we merge the detailed factual content from Qwen2VL-72B into the comprehensive explanations provided by GPT-4o. The merged answers are further reviewed and supervised by human experts to ensure their quality and consistency."
        },
        {
            "title": "D Training Details",
            "content": "Our training strategy largely follows the approaches adopted by LLaVA and LLaVANext. CLIP-Large-336-Patch14 is employed as the visual encoder. In line with the LLaVA training strategy, we first conduct pretraining stage where both the visual encoder and the LLM are frozen. We utilize the LLaVA-pretrain-558k and ALLaVA-pretrain728k datasets for pretraining. The batch size is uniformly set to 256 and learning rate is set to 1e-3. During this phase, images are resized to 336336, and no image-splitting method is applied. For SFT stage, we unfreeze the LLM for LLaVA and further unfreeze the visual encoder for LLaVANext. In the case of LLaVANext, we apply the image-splitting method, setting the maximum split size to 33. The batch size is uniformly set to 128 and learning rate is set to 2e-5.. LLaVAInternLM2.5-7B is trained using 8A800 GPUs for 12 hours. LLaVANext-InternLM2.5-7B is trained using 16A800 GPUs for 13 hours. LLaVANextQwen2.5-32B is trained using 32A800 GPUs for 24 hours. Figure 6: GPT-4o shows superior alignment with human preference than InternVL2-76B. Figure 7: Demonstration examples of our image filter. Figure 8: Examples of limitation within current multi-modal benchmark for alignment. Figure 9: Examples of each task in OmniAlign-V. Figure 10: Examples of each task in OmniAlign-V. Figure 11: Examples of each task in OmniAlign-V. Figure 12: Examples of each task in OmniAlign-V."
        },
        {
            "title": "Prompt for Knowledge Task",
            "content": "Examine the image provided and generate knowledge-based and exploratory question based on the content of the image and supply corresponding detailed answers. Question Guidelines: - Your question should invite insightful discussion on the types of elements in the image, such as: - **Objects**: For example, animals, plants, food, or products. - E.g., \"What breed of dog is in the picture, and what are their characteristics?\", \"Can you give me recipe for the food in image?\", \"Write Product Description for the product in the image.\". - **Locations and Features**: Relevant to countries, landmarks, famous people, or scenic spots. - E.g., \"Please introduce the history of the landmark in the picture.\", \"How did the states in image get their names?\", \"Who is the person in the image and what is him famous for?\". - **Activities and Technologies**: Related to sports, machines, technology, and environmental details. - E.g., \"Can you explain how the game in image is played?\", \"How is the machine shown in the image operated?\" - **Events, Literature, and Media**: Concerning books, movies, or series in the image. - E.g., \"Write short description about the movie or series in the image.\", \"Think of books that would be enjoyable for someone who liked the books in the image.\" Answer Guidelines: - Ensure your answers are factual and comprehensive. - Please use Markdown formatting in your text to enhance the content, making it visually appealing and easy to read. Include appropriate headings, subheadings, lists, code blocks, and other Markdown elements to optimize your answers. Output Format: Your response should strictly follow this format: json { \"question\": \"Question text\", \"answer\": \"Answer text\" } Figure 13: An Example of the prompt for Knowledge task generation."
        },
        {
            "title": "Prompt for Creative tasks",
            "content": "You are skilled writer with talent for crafting insightful and engaging questions based on the content of given image. **Task Guidelines**: - Analyze the content of the provided image and select one of the question types listed below. Use it to create engaging questions that lead the viewer to explore and interpret the image in different level of complexity. - Be closely tied to the content of the image, emphasizing its primary visual or thematic elements. - Your questions should **avoid directly referencing specific details** in the image. Instead, they should encourage deeper reflection, ensuring the question cannot be answered without seeing the image. - Avoid overly rigid or direct phrasing, focusing instead on open-ended exploration. **Question Types** Your questions can be from the following types, each followed by an example with different level of complexity. - Simple (basic observation or initial reflection) - Moderate (more thought-provoking, requiring deeper understanding and more structured response) - Difficult (complex or abstract, requiring analysis and strict formatting) {Match Types} **Output Format**: Your response should strictly follow this format: json { \"question\": \"Question text\", \"type\": \"Question type\", \"level\": \"Question level\" } Figure 14: An Example of the prompt for Creative task generation."
        },
        {
            "title": "Prompt for Inferential tasks",
            "content": "You are an image analysis expert skilled in posing high-quality **inferential questions**. Please provide 2-5 of the most insightful questions you can think of, following these guidelines: For Questions: - **Focus on image-based questions:** Ensure that your question cannot be answered without analyzing the image. **You should not directly provide images data or details in your generated questions.** For example, \"What might be the impact on the radio industry due to 34 stations having their licenses revoked?\" includes the data in the image and can be answered without analyzing the image, so it is bad question. - Ensure that questions are natural, not overly rigid. **You should be quite certain and confident about the questions you pose and their answers, and avoid using words like \"possibly\", \"maybe\" or \"might be\" in both questions and answers.** - Your questions must require reasoning beyond the direct content of the image, making reasonable inferences based on the information presented. - The scope of the questions should not be overly broad or delve into political, philosophical, speculative, sensitive, or controversial topics. Stay within the context of the scene and elements inferred from it. For Answers: - You should provide clear and concise answer to the question. Good Examples: - What precautions are the people on the boat taking to stay comfortable during the trip? - Is there anything else on the table other than the pizza? - Why do these people choose to dress in this style? - What decorative element is present in this public restroom that is not typical? Bad Examples: - What might indicate Figure 15: An Example of the prompt for Inferential task generation."
        },
        {
            "title": "Prompt for Chart tasks",
            "content": "Youre great image analyst. You need to analyze the image provided and generate some insightful questions based on the content of the image. Question generation guidelines: - Ensure that your questions require the image to be answered and do not include explicit information from the image. Instead, pose questions that prompt the respondent to analyze the image to find the answer. - Your question should be explainable and require some reasoning to answer. - Your question could contain different analytical perspectives, such as trends, comparisons, causal inference, etc. - - Your questions should be insightful but also clear and straightforward. Avoid overly complex or niche questions. Bad question examples: - \"What might be some factors contributing to the significantly higher private health expenditure per person in Argentina compared to Fiji and Benin?\" (This includes specific details from the chart.) Its correct clarification should be \"Is there any difference in private health expenditure per person between Argentina, Fiji, and Benin? If so, what might cause the difference?\" - \"What trends can be observed in private health expenditures per person among the three countries shown?\" (This question is unclear because trends between countries is not standard analytical concept. Trends typically refer to patterns over time or categories, not direct cross-entity comparisons.) Output format: Your response should strictly follow this format: { \"questions\": [ { \"question\": \"Question text\" }, { \"question\": \"Another question text\" } ] } Figure 16: An Example of the prompt for Chart task generation."
        },
        {
            "title": "Prompt for Poster tasks",
            "content": "Youre an excellent image analyst and good at generating insightful questions about the image. You need to analyze the image provided and generate some insightful questions based on the content of the image, and you should answer the questions you generated. Possible Categories Reference: 1. **Cultural and Social Context** 2. **Analysis and Inference**. 3. **Visual Elements and Design Techniques**. Question Guidelines: - **Focus on image-based questions:** Ensure that your question cannot be answered without analyzing the image. **You should not directly provide images data or details in your generated questions.** For example, \"What might be the impact on the radio industry due to 34 stations having their licenses revoked?\" includes the data in the image and can be answered without analyzing the image, so it is bad question. - **Encourage thoughtful, structured responses:** Your question should be explainable and need some reasoning to answer. You should not generate questions that are just extracting information from the image. For example, \"What percentage of organizations verify the past employment records of new employees according to the image?\" is bad question. - **Ensure diversity in the questions:** Cover different aspects of the image, encouraging multiple perspectives. You can choose some appropriate categories from the possible categories reference. **For the same category, you can generate multiple questions.** - **Generate high-quality questions:** You can choose to generate challenging questions, but their answers should be able to clearly explain. Output Format: Your response should strictly follow this format: \"questions\": [ { \"question\": \"Question text\" } ] Figure 17: An Example of the prompt for Diagram task generation."
        },
        {
            "title": "Prompt for Diagram tasks",
            "content": "Youre great diagram analyst. You need to analyze the diagram provided and generate 2-4 insightful questions based on the content of the diagram. Question Guidelines: - Your questions should be guiding and should not directly point to the content. For example: \"How does acid rain affect water bodies, soil, and plant life?\" should be changed to \"How does the process in image affect water bodies, soil, and plant life?\" - Your question should invite insightful discussion, such as: - **Interpretation**: Symbol Interpretation, Data Extraction - **Examples**: - \"What is the role of the cytokine-producing cell in the process shown?\" - \"Enumerate the steps outlined in the flowchart.\" - **Relation Analysis**: - **Examples**: - \"How does variable affect variable in the diagram?\" - \"How many ways can to be achieved in the diagram?\" - **Inference**: - **Examples**: - \"What can be inferred about the systems stability from the diagram?\" - \"What does the bacterium do once it has the hybrid plasmid?\" Output Format: Your response should strictly follow this format: json { \"question\":[\"Question text 1\", \"Question text 2\", ...], } Figure 18: An Example of the prompt for Diagram task generation."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanjing University",
        "Shanghai AI Laboratory",
        "Shanghai Jiaotong University",
        "Zhejiang University"
    ]
}