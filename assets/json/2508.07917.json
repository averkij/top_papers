{
    "paper_title": "MolmoAct: Action Reasoning Models that can Reason in Space",
    "authors": [
        "Jason Lee",
        "Jiafei Duan",
        "Haoquan Fang",
        "Yuquan Deng",
        "Shuo Liu",
        "Boyang Li",
        "Bohan Fang",
        "Jieyu Zhang",
        "Yi Ru Wang",
        "Sangho Lee",
        "Winson Han",
        "Wilbert Pumacay",
        "Angelica Wu",
        "Rose Hendrix",
        "Karen Farley",
        "Eli VanderBilt",
        "Ali Farhadi",
        "Dieter Fox",
        "Ranjay Krishna"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), a class of vision-language-action models that integrate perception, planning, and control through a structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on LIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, an additional 10% (single-arm) and an additional 22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines by an additional 23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset -- a mid-training robot dataset comprising over 10,000 high quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset, and our action reasoning dataset, establishing MolmoAct as both a state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning. Blogpost: https://allenai.org/blog/molmoact"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 7 1 9 7 0 . 8 0 5 2 : r MolmoAct: Action Reasoning Models that can Reason in Space"
        },
        {
            "title": "Jason Lee",
            "content": "1,"
        },
        {
            "title": "Jiafei Duan",
            "content": "1,"
        },
        {
            "title": "Haoquan Fang",
            "content": "1,"
        },
        {
            "title": "Yuquan Deng",
            "content": ""
        },
        {
            "title": "Shuo Liu",
            "content": "1,"
        },
        {
            "title": "Boyang Li",
            "content": ""
        },
        {
            "title": "Bohan Fang",
            "content": ""
        },
        {
            "title": "Jieyu Zhang",
            "content": "1,"
        },
        {
            "title": "Yi Ru Wang",
            "content": "1,"
        },
        {
            "title": "Ali Farhadi",
            "content": "1,"
        },
        {
            "title": "Dieter Fox",
            "content": "1,"
        },
        {
            "title": "Ranjay Krishna",
            "content": "1,2 1Allen Institute for AI, 2University of Washington denotes equal contribution in no particular order. marks core contributors. See full author contributions here. MolmoAct: MolmoAct-7B-D-Pretrain-0812 MolmoAct-7B-DMolmoAct-7B-O-0812 MolmoAct Data: MolmoAct-Dataset MolmoAct-Pretraining-Datasets MolmoAct-Midtraining-Datasets Blog: allenai.org/blog/molmoact"
        },
        {
            "title": "Abstract",
            "content": "Reasoning is central to purposeful action, yet most robotic foundation models map perception and instructions directly to control, which limits adaptability, generalization, and semantic grounding. We introduce Action Reasoning Models (ARMs), class of visionlanguageaction models that integrate perception, planning, and control through structured three-stage pipeline. Our model, MolmoAct, encodes observations and instructions into depth-aware perception tokens, generates mid-level spatial plans as editable trajectory traces, and predicts precise low-level actions, enabling explainable and steerable behavior. MolmoAct-7B-D achieves strong performance across simulation and real-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching tasks, surpassing closed-source π0 and GR00T N1; 86.6% average success on LIBERO, including +6.3% gain over ThinkAct on long-horizon tasks; and in real-world fine-tuning, +10% (single-arm) and +22.7% (bimanual) task progression over π0-FAST. It also outperforms baselines by +23.3% on out-of-distribution generalization and achieves top human-preference scores for open-ended instruction following and trajectory steering. Furthermore, we release, for the first time, the MolmoAct Dataset mid-training robot dataset comprising over 10,000 high-quality robot trajectories across diverse scenarios and tasks. Training with this dataset yields an average 5.5% improvement in general performance over the base model. We release all model weights, training code, our collected dataset and our action reasoning dataset, establishing MolmoAct as both state-of-the-art robotics foundation model and an open blueprint for building ARMs that transform perception into purposeful action through structured reasoning."
        },
        {
            "title": "Introduction",
            "content": "Thinking is embodied, spatial, and outside your head. Barbara Tversky, Emerita Professor of Psychology at Stanford Tversky (2025) Reasoning allows us to act with intention. Before reaching for cup or moving through room, we subconsciously weigh context, goals, and constraintstransforming perception into purpose. This process, grounded in our physical experience of the world, makes our actions coherent, adaptable, and explainable. 1 Figure 1 Overview. MolmoAct is an open action reasoning model that, given users language instruction, reasons in space and autoregressively predicts three structured reasoning chains: Depth Perception Tokens for sensing and reconstructing the 3D environment, Visual Reasoning Trace Tokens for representing its planned trajectory in the scene, and Action Tokens for generating the corresponding robot control commands. Each explainable reasoning chain can be independently decodedyielding depth map of the scene, 2D trajectory overlay on the image plane, and executed actions in the physical worldproviding explicit, spatially grounded reasoning at every stage. For robots to operate with the same fluency, they must do more than map images and instructions to control commands. They must learn to reason. In contrast to the rapid generalization gains seen in large language and vision models, progress in robotics has lagged behind (Duan et al., 2022; Xu et al., 2024b; Firoozi et al., 2025). Vision-Language-Action (VLA) models aim to bring similar capabilities to physical agents, but have yet to reach the same level of flexibility or robustness. Despite massive efforts in dataset collection and model scaling, todays VLAs remain brittle and opaquestruggling to transfer across tasks, scenes, or embodiments, and offering little insight into why robot chose one action over another (Liu et al., 2025; Pumacay et al., 2024). This gap stems not just from limited data, but from lack of structure. While language and vision tasks benefit from abundant, loosely labeled web-scale data, robotics demands fine-grained, embodied interactiondata that is costly, ambiguous, and difficult to scale. Yet in parallel, language models have begun to shift away from brute-force scaling toward structured learning: building intermediate representations that support reasoning, abstraction, and control (Wei et al., 2022; Zelikman et al., 2022; Huang et al., 2022). We believe robotics canand mustdo the same. We introduce MolmoAct (Multimodal Open Language Model for Action), family of completely open Action Reasoning Models (ARM) that integrate perception, planning, and control through structured reasoning pipeline. MolmoAct learns to interpret language instructions, sense its environment, generate spatial plans, and execute them as smooth, goal-directed trajectories. The model first encodes observations and instructions into structured 2.5D representations via autoregressive prediction of depth-aware perception tokens. These tokens condition the generation of mid-level planning representationsvisualized as trajectory lines in image spacewhich then guide the prediction of precise, low-level robot actions. This three-stage reasoning architecture enables MolmoAct to produce explainable and steerable behavior. MolmoAct structured design delivers both strong performance and high explainability. On standard 2 benchmarks such as LIBERO and SimplerEnv (Google Robot), MolmoAct consistently outperforms competitive baselines including GR00T N1 (NVIDIA et al., 2025), π0 and π0-FAST (Black et al.), RT-1 (Brohan et al., 2022), and TraceVLA (Zheng et al., 2024). In arena-style human evaluations for open-ended language instruction following, MolmoAct is preferred over baselines, achieving significantly higher Elo ratings. The model adapts to novel tasks more effectively through lightweight fine-tuning, surpassing other strong baselines in efficiency. Moreover, it generalizes well to diverse environments and task perturbations in both simulation and real-world settings. Its visual reasoning traces offer an explainable view into the models decision-making, while also enabling direct action steering by editing trajectory linesan approach we find more reliable than language commands, which can suffer from ambiguity. MolmoAct is fully open in every aspect: we release the model weights, training code, and all components of our multimodal reasoning dataset. We aim for MolmoAct to be more than high performing robotics foundation model that serves as blueprint for building agents that reason, transforming perception into purposeful action through reasoning."
        },
        {
            "title": "2 MolmoAct",
            "content": "MolmoAct is fully open-source action reasoning model (ARM) for robotic manipulation. It builds on Molmo (Deitke et al., 2024), reusing its visionlanguage backbone composed of vision encoder, vision language connector, and large language model (LLM). While Molmo supports chain-of-thought reasoning for language and vision, MolmoAct extends this capability to generate structured, temporally grounded action sequences. The model is trained on subset of the Open X-Embodiment (OXE) (ONeill et al., 2024) mixture consisting of BC-Z (Jang et al., 2022), BridgeData V2 (Walke et al., 2023), and RT-1 Brohan et al. (2022), and is augmented with newly collected MolmoAct Dataset. In the following sections, we describe the VLM preliminaries (subsection 2.1), our method for tokenizing actions for visionlanguage actions (VLA) (subsection 2.2), the transformation of VLAs into an action reasoning model (ARM) (subsection 2.3), and our approach to action steerability via visual reasoning traces (subsection 2.4)."
        },
        {
            "title": "2.1 Vision Language Model\nTo equip an action model with visual and linguistic world knowledge, we build upon vision–language models\n(VLMs). Most modern VLMs share a three-component structure: (i) a visual encoder that transforms an\nimage into patch-level embeddings, (ii) a projection module that maps these visual features into the input\nspace of a language model, and (iii) a large language model (LLM) backbone. These components are typically\ntrained with a next-token prediction objective on paired or interleaved image–text data.",
            "content": "Our work builds on Molmo, the Multimodal Open Language Model, which follows this standard design. It employs Vision Transformer (ViT) visual encoder, two-layer MLP connector for projecting vision features into the language embedding space, and decoder-only LLM backbone. In our implementation, we use vision encoders such as OpenAIs ViT-L/14 336px CLIP (Radford et al., 2021) and ViT-SO400M/14 384px SigLIP2 (Tschannen et al., 2025), paired with open LLMs including OLMo2-7B (OLMo et al., 2024) and Qwen2.5-7B (Qwen et al., 2025). We trained MolmoAct-7B-O with VLM backbone based on OpenCLIP and OLMo2-7B, and MolmoAct-7B-D with backbone based on SigLIP2 and Qwen2.5-7B. For full details of our model architecture and implementation, please refer to Appendix A. The degree of openness in the backbone components varies. SigLIP2 and Qwen2.5 do not disclose the full details of their pre-training, and are presumed to use large-scale Internet-sourced multimodal data. In contrast, OLMo2 provides open training datasets (e.g., LAION-2B/5B (Schuhmann et al., 2022), Dolma (Soldaini et al., 2024)), model weights, and complete training code. Although OpenAIs CLIP also uses closed training data, it can be reproduced from scratch, as shown by MetaCLIP (Xu et al., 2024a). We use the model from OpenAI because it was trained for higher resolution images, and also following the previous choice from Molmo (Deitke et al., 2024). In MolmoAct, we initialize vision and language components from these open checkpoints whenever available, and use the pre-training procedure from Molmo (Deitke et al., 2024) to train the VLM on dense captioning data. Then, after vision-language alignment, we start to fine-tune MolmoAct with subset of Open X-Embodiment (OXE) mixture and the MolmoAct Dataset. This enables full reproducibility and 3 Figure 2 Training process of MolmoAct. The model training process consists of two stages: Pre-training (left) and Post-training, Mid-training & Inference (right). During pre-training, the visionlanguage backbone (Molmo) is trained on multimodal and robot reasoning data for diverse objectives, including discretized robot control, 2D pointing, trajectory drawing, open-vocabulary question answering, and perception token prediction. In post-training, the action reasoning model consumes multi-view camera images and either natural language instructions or visual trajectory inputs, generating perception tokens, visual reasoning trace tokens, and action tokens for execution. supports community-driven re-training and ablation studies on data curation and scaling."
        },
        {
            "title": "2.2 Vision-Language-Action Model\nA standalone VLM—even when expertly prompted—cannot directly control a robot: it lacks a representation\nof the robot’s action space and dynamics, and thus can only provide high-level planning over the current\nobservation. To produce accurate, executable commands, we follow prior work (Brohan et al., 2022; Kim\net al., 2024; Zitkovich et al., 2023) in formulating action prediction as a vision–language sequence modeling\ntask. For each action dimension, we normalize using dataset quantiles and discretize into 256 uniform-width\nbins between the first and ninety-ninth percentiles, which reduces the influence of outliers while preserving\neffective granularity. This yields an N -dimensional action represented as N integers in [0, 255]. The model is\ntrained end-to-end with a next-token prediction objective, and the loss is computed only on the action tokens.",
            "content": "Prior work represents the 256 discretized action bins with 256 distinct language tokens taken from the tail of the vocabulary. Continuous action bins, however, possess ordinal structure and local correlation, whereas arbitrary language tokens are effectively unrelated. This mismatch yields poor initialization for learning an action codebook. We adopt simple, order-preserving alternative that better reflects the geometry of the action space. We first identify the final 256 tokens in the Qwen2 tokenizer and, for each, use its underlying byte-level BPE symbol. These multi-character tokens admit natural lexicographic ordering and are already sorted in order. We then assign them monotonically to the 256 bins so that adjacent bins map to adjacent symbols, and this becomes our action token vocabulary Vaction. This proximity-preserving initialization offers smoother starting point for optimization and, in practice, substantially reduces training time. Retrospectively, compared to GR00T N1 (NVIDIA et al., 2025) training requirement of 50,000 GPU hours, MolmoAct achieves pre-training with only 9,216 GPU hours which uses 5.4 times less compute than GR00T N1."
        },
        {
            "title": "2.3 Action Reasoning Model\nChain-of-Thought (CoT) (Wei et al. (2022)) has been shown to significantly improve Language Models’\nperformance on complex task. Likewise, Multimodal Language Models (MLLM) also benefit from multimodal\nChain of Thought (MCoT) (Wang et al. (2025)) in processing long multimodal contexts. However, this\n\"think-before-you-act\" paradigm is rarely present in robotic control policies. While some work attempts\nto incorporate reasoning to VLAs, they often emphasize high-level plans or textual hints (Sun et al., 2024;",
            "content": "4 Figure 3 Distribution of data mixture in the overall pre-training mixture (left) and in the sampled subset used for MolmoAct pre-training (right). The mixture contains primarily action reasoning data (38.7%), trajectory-conditioned data (38.7%), and multimodal web data (21.5%), with small fractions of auxiliary depth and trace data (0.5% each). The sampled subset increases the proportion of auxiliary data (7.5% each for depth and line) while reducing multimodal web data to 5%. Zawalski et al., 2024) (for example, subtask or direction signals) and object-centric cues (for example, detections or boxes) (Li et al., 2025; Gu et al., 2023). While useful, they ignore two crucial aspects for precise control: depth perception and precise motion plans. First, most VLM backbones are trained on RGB imagery and receive no direct supervision about depth, yet accurate 3D understanding is central to physical interaction. Second, linguistic directives are inherently ambiguous with respect to magnitudes, scales, and endpoints; the same phrase can map to many distinct trajectories. Our premise is therefore to make the model reason explicitly about depth and about concrete, image-aligned motion sketch before emitting low-level actions. In MolmoAct, we incorporate intermediate reasoning not through pure language-based reasoning; rather, we teach our models to reason in space. We cast visuomotor control as autoregressive sequence modeling augmented with action CoT as to achieve reasoning in space. Conditioned on images and instructions, the autogregressive action reasoning model first generates short sequence of depth perception tokens that summarize depth perception of the current observation, followed by compact visual reasoning trace of the intended end-effector motion, and only then produces the action tokens for the next control step. This approach mirrors how textual CoT is performed; however, with action CoT, every intermediate step is explicitly grounded in space. Depth Perception Tokens Depth estimation is key aspect for spatial understanding and robot action prediction. Most traditional VLMs and VLAs only condition their outputs solely on the RGB image input and text instruction, which worsens the prediction for tasks that heavily require spatial understanding. Prior work (Bigverdi et al., 2024) has shown that depth perception tokens are effective in enhancing chain-of-thought reasoning for visualspatial tasks. Building on this insight, we incorporate depth token prediction as key component of our action reasoning in space, enabling more accurate control of the robot in 3D environments and helping to bridge this critical gap. We now examine how each intermediate step in the reasoning chain is formulated: We begin by defining the auxiliary vocabulary that contains depth perception tokens. Let Vdepth = {DEPTH_START, DEPTH_END} {DEPTH_k} k=1 be the discrete token set used to represent depth, with = 128. For each input image, the target depth string is defined as = (DEPTH_START DEPTH_zdepth 1 . . . DEPTH_zdepth DEPTH_END) Vdepth. with = 100, and each zdepth {1, . . . , } indexing code in VQVAE (Van Den Oord et al., 2017) codebook = {c1, . . . , cN }. The codebook is produced by pre-trained specialist depth estimator (trained on depth maps from Depth Anything V2), which quantizes the dense depth map into fixed-length sequence of 5 Figure 4 Examples and verb distribution in the MolmoAct Dataset. Left: Sample robot manipulation tasks paired with natural language instructions, spanning diverse household activities such as closing laptop, loading plate, cleaning toilet, and opening microwave. Right: Log-scale distribution of the top verbs in the dataset, showing long-tail pattern with put, turn, and close as the most frequent actions. indices. There is deterministic one-to-one correspondence between each codebook index and depth token in Vdepth (i.e., index maps to DEPTH_k), so is discrete, explainable summary of the depth map of the scene. We employ specialist-to-generalist distillation strategy to ground MolmoActs depth perception tokens prediction: the specialist produces as the ground-truth depth string, and the VLA is trained to predict this string autoregressively from the original RGB observation, thereby internalizing depth in form that can condition downstream trajectory and action generation. Visual Reasoning Trace Planning in advance is often crucial for robots to complete tasks. Instead of planning through text description, we choose to plan via the trajectory of the robots end effector projected into the image plane. This trajectory is more grounded and precise way than language to show how the robot should move from the current observation until task completion. Given certain image observation, we define the end-effector visual reasoning trace on that image as polyline with points, 1 5 (i.e., 0 to 4 line segments), τ = (p1, . . . , pL), pi = (ui, vi). where for every pi = (ui, vi), the coordinates of the point are normalized with respect to the given image dimension so that ui, vi {0, ..., 255}. Note that p1 corresponds to the coordinate where the robot end-effector is located in the given image, and all other points are the location of the end-effector in later frames. For the full polyline, points are subsampled evenly from the future horizon of the episode between the current frame and the terminal frame. Action Reasoning Procedure With depth perception tokens and visual reasoning trace, MolmoAct can finally perform action reasoning in space with the following procedure: given an RGB image observation and language instruction (which includes an action CoT prompt), the model autoregressively generates three token sequences in order: (i) depth perception tokens d; (ii) visual reasoning trace τ ; (iii) action tokens a, where = (a1, . . . , aD) action with degree of freedoms. These are produced according to the factorization p(d, τ , I, ) = +2 i=1 p(di I, T, d<i) j=1 p(τj I, T, d, τ<j) k= p(ak I, T, d, τ , a<k). In other words, the depth string is generated first, then the visual reasoning trace τ , and finally the action tokens a. By conditioning each stage on the preceding depth and trajectory tokens, we ensure that the final actions are spatially grounded in both the inferred depth and the planned motion sketch."
        },
        {
            "title": "2.4 Action Steerability via Visual Reasoning Trace\nWe define steerability as the ability to guide a policy at test time to perform different behaviors using\nuser-provided instructions. Most prior VLA systems rely exclusively on language for steering. However,\nlanguage-only steering faces three practical challenges: (i) it requires large and diverse corpora of high-quality\nlanguage–action pairs to learn a reliable grounding between words and control, (ii) natural language is\ninherently ambiguous about magnitudes, scales, and endpoints, and (iii) post-trained models often exhibit\nnarrow prompting habits, making them brittle to out-of-distribution phrasing. For manipulation, these issues\ntranslate into imprecise or inconsistent control. We therefore seek a steering modality that is both precise and\nscalable. Rather than relying on ambiguous language prompts, we allow the user to draw a visual reasoning\ntrace τ directly on the camera image to indicate the desired end-effector path. A trace τ = (p1, . . . , pL),\n1 ≤ L ≤ 5 (i.e., 0 to 4 line segments), is overlaid onto the RGB image I to form an augmented observation\n+ = I ⊕ τ . visual reasoning traces are unambiguous, easily edited, and generalize across tasks without large\nI\ntext–action corpora or brittle language patterns.",
            "content": "At test time, given I, instruction , and user-drawn trace τ , we construct next-step action tokens = (a1, . . . , aD) autoregressively: + = τ and generate the p(a + , ) = k= + p(ak , T, a<k). By conditioning directly on the overlaid trace, the model executes closed-loop control that follows the users sketch. Repeating this at each timestep yields precise, interactive steering that is both scalable and robust to out-of-distribution phrasing."
        },
        {
            "title": "3 Data Curation and Generation",
            "content": "MolmoAct is trained on diverse set of datasets. During Pre-training, MolmoAct is trained on Multimodal Web data, Auxiliary Robot data as well as Action Reasoning data. Furthermore, we collected and trained wtih the MolmoAct Dataset for the Mid-training stage. Below, we describe each dataset and its collection process; further details and examples are provided in the Appendix E."
        },
        {
            "title": "3.1 Action Reasoning Data\nAs discussed in Section 2.3, MolmoAct frames visuomotor control as an autoregressive sequence modeling\nproblem augmented with an action chain-of-thought (CoT). This formulation allows us to convert any\nconventional robot action dataset into action reasoning data by appending predicted depth perception tokens\nand visual reasoning traces to action tokens, conditioned on both language and robot observations. Section 3.1\ndetails the process for generating ground-truth labels for the depth perception tokens and visual reasoning\ntraces, and explains how these are combined with action labels to train MolmoAct.",
            "content": "A robot episode typically consists of sequence of timesteps, where each timestep is tuple (I, T, a)t, containing an RGB observation image I, language instruction , and ground-truth action a, specified either in end-effector space or joint space. To convert any robot data into the Action Reasoning data format, we generate ground-truth Depth Perception Tokens and Visual Reasoning Traces for each timestep in the episode. We explain the details for generating ground-truth labels for Depth Perception Tokens and Visual Reasoning Trace below. Depth Perception Tokens To generate Depth Perception Tokens for each frame of demonstration, we first train VQVAE on 10 million depth maps of tabletop manipulation images collected from the RT-1, BridgeData V2, and BC-Z datasets. We use DepthAnything-v2 to obtain depth map for each observation RGB image. The VQVAE is trained with standard reconstruction objective to minimize reconstruction loss between input RGB images and their corresponding depth maps for 20 epochs. Once the VQVAE has been trained, we encode each observation image with the VQVAE to get their latent embeddings. We then represent the latent embedding with learned codebook with 128 dimension based on one-to-one index to depth token mapping. Note that all images are resized to 320320 px during training and inference to enforce 7 the representation of 100 tokens per image. This allows us to express the depth map of each observation image as tokenized string of 100 tokens, which we use for ground truth labeling for our depth perception token. Visual Reasoning Trace To generate Visual Reasoning Traces for each frame of demonstration, we employ Molmo, vision-language model trained on diverse 2D pointing datasets, for data generation akin to synthetic data generation in NLP. For each timestep t, we extract the pixel coordinates (ut, vt) of the robots gripper, and aggregate these across the episode to obtain visual reasoning trace. For each observation frame, we prompt Molmo with the instruction \"point to the robot gripper\" for single-arm robots or \"point to the robot gripper on the left/right\" for bimanual embodiments. Molmo returns 2D coordinate (xt, yt) R2 and (xt, yt) [0, 100], corresponding to the predicted gripper location in image space. We rescale the coordinate values so that (ut, vt) Z2 and (ut, vt) [0, 255]. We then apply this query at every timestep in the episode, resulting in one gripper location per frame. Linking these predictions sequentially yields the full trajectory τ . In the case of bimanual robots, two separate prompts are issued per frame to obtain τL and τR for the left and right grippers, respectively. At each timestep t, we construct visual reasoning trace by selecting subsequence from to the episode end e. This includes the current point (ut, vt), the final point (ue, ve), and up to three intermediate points spaced uniformly between them. If fewer than three intermediate points are available (i.e., < 4), we include all available points. If = e, the trace contains only one point. This yields visual reasoning trace between 1 to 5 points representing the future motion of the end effector. Auxiliary Robot Data To strengthen MolmoActs ability to reason in space, we extend the same data generation pipeline used for depth perception token, visual reasoning trace to curate three auxiliary supervision dataset: (i) Auxiliary Depth Datagiven an RGB observation and language instruction, the model only predicts the target Depth Perception Token sequence; (ii) Auxiliary Trace Datagiven an RGB observation and language instruction, the model only predicts the corresponding Visual Reasoning Trace; and (iii) Trajectory-conditioned Action Datagiven ot = (I, T, τ )t, where is the current image, the instruction, and τ = (p1, . . . , pL) the ground truth Visual Reasoning Trace, the model predicts the next action by taking + = τ . Note that we curate the Trajectory-conditioned the language and the trace-overlaid image Action Data mainly for enabling the steerability feature of MolmoAct. Once we generate the ground truth label for each frame, we construct the action reasoning dataset by sequentailly aligning the ground-truth Depth Perception Tokens, Visual Reasoning Trace, and Action for instruction tuning. We also use the same data generation approach to obtain auxiliary robot data."
        },
        {
            "title": "3.2 MolmoAct Dataset\nWe curated the MolmoAct Dataset to improve the model’s general manipulation performance and spatial\nreasoning in real household environments. The dataset contains 10,689 high-quality trajectories of a single-arm\nFranka robot performing 93 unique manipulation tasks in both home and tabletop environments as shown in\nFigure 4. The average length of each trajectory spans 112 timesteps. Data collection spanned two months\nand involved five full-time operators following strict protocols. For further details, see Appendix E. The\nMolmoAct Dataset includes manipulation data from two primary settings: home environments and\ntabletops.",
            "content": "Home Environment Data. To collect diverse home environment data, we mounted single-arm Franka robot on lightweight, mobile platform similar to DROID (Khazatsky et al., 2024), enabling us to transport the robot and capture scenes across living rooms, kitchens, bathrooms, and bedrooms. Each task was designed to reflect specific household chore. For example, the long-horizon task clean up the dishes was decomposed into subtasks such as put the bowl in the dishwasher, put the fork in the sink, and cover the pot. This decomposition allows the policy to learn individual skill components in isolation before composing them into more complex behaviors. In total, we collected 7,730 trajectories spanning 73 distinct tasks and 20 verbs across wide variety of scenes. Tabletop Data. We also collected 2,959 tabletop trajectories covering 20 atomic tasks, each performed with diverse set of 8 Table 1 SimplerEnv evaluation across different policies on Google Robot tasks. The zero-shot and fine-tuning results denote performance of OXE dataset (ONeill et al., 2024) pre-trained models and RT-1 dataset (Brohan et al., 2022) fine-tuned models, respectively. Model HPT (Wang et al., 2024a) TraceVLA (Zheng et al., 2024) RT-1-X (Brohan et al., 2022) RT-2-X (Zitkovich et al., 2023) Octo-Base (Team et al., 2024b) OpenVLA (Kim et al., 2024) RoboVLM (zero-shot) (Liu et al., 2025) RoboVLM (fine-tuned) Emma-X (Sun et al., 2024) Magma (Yang et al., 2025b) π0 (fine-tuned) (Black et al.) π0-FAST (fine-tuned) GR00T N1 (fine-tuned) (NVIDIA et al., 2025) SpatialVLA (Qu et al., 2025) MolmoAct (zero-shot) MolmoAct (fine-tuned) Visual Matching Pick Coke Can Move Near Open/Close Drawer 56.0% 28.0% 56.7% 78.7% 17.0% 16.3% 72.7% 77.3% 2.3% 56.0% 72.7% 75.3% 0.7% 81.0% 71.3% 77.7% 60.0% 53.7% 31.7% 77.9% 4.2% 46.2% 66.3% 61.7% 3.3% 65.4% 65.3% 67.5% 1.9% 69.6% 73.8% 77.1% 24.0% 57.0% 59.7% 25.0% 22.7% 35.6% 26.8% 43.5% 18.3% 83.7% 38.3% 42.9% 2.9% 59.3% 66.5% 60.0% Avg 46.0% 42.0% 53.4% 60.7% 16.8% 27.7% 56.3% 63.4% 8.0% 68.4% 58.7% 61.9% 1.8% 70.0% 70.5% 71.6% Variant Aggregation Pick Coke Can Move Near Open/Close Drawer 60.0% 49.0% 82.3% 0.6% 54.5% 68.3% 75.6% 5.3% 53.4% 75.2% 77.6% 89.5% 57.8% 76.1% 56.4% 32.3% 79.2% 3.1% 47.7% 56.0% 60.0% 7.3% 65.7% 63.7% 68.2% 71.7% 43.8% 61.3% 31.0% 29.4% 35.3% 1.1% 17.7% 8.5% 10.6% 20.5% 68.8% 25.6% 31.3% 36.2% 76.7% 78.8% Avg 45.0% 39.6% 64.3% 1.1% 39.8% 46.3% 51.3% 11.0% 62.6% 54.8% 59.0% 65.8% 59.3% 72.1% objects to promote robustness and generalization. Each task was decomposed into atomic motions and reinforced in simplified tabletop environment. For example, the task put the bowl in the dishwasher consists of sequence of motions such as opening the dishwasher, grasping the bowl, flipping it, and placing it inside. We isolated and separately collected data for each atomic motionopen, pick, flip, put, and closeto build comprehensive set of motion primitives."
        },
        {
            "title": "4 Training Recipe",
            "content": "MolmoAct is first pre-trained on action reasoning data curated from subset of the OXE dataset, along with the auxiliary robot dataset and multimodal web dataset. To further enhance its capabilities, we mid-train the model on the MolmoAct Dataset before post-training it for specific downstream tasks and embodiments. In this section, we describe the different data mixtures used at each stage of MolmoAct training."
        },
        {
            "title": "4.1 Pre-training\nIn the first training stage, MolmoAct is pre-trained on a mixture of action reasoning data, auxiliary robot\ndata, and multimodal web data. For robot data, we use a subset of OXE comprising RT-1, BridgeData\nV2, and BC-Z, totaling 10.5M samples, which we convert into action reasoning data using our action CoT\nformulation. We also include Auxiliary Robot Data—atomic depth data (1.5M), atomic trace data (1.5M),\nand trajectory-conditioned action data (10.5M), and co-train with 2M samples of multimodal web data.\nDuring pre-training, data is sampled at the following rates: RT-1 (20%), BridgeData V2 (12.5%), BC-Z",
            "content": "9 Table 2 LIBERO benchmark success rates across four task categories (Spatial, Object, Goal, and Long-horizon) along with the average performance. MolmoAct achieves the highest overall average success rate of 86.6%, outperforming all baselines, with strong performance across all categories, particularly in long-horizon tasks. Baseline TraceVLA (Zheng et al., 2024) Spatial Object Goal Long 84.6% Avg 85.2% 75.1% 54.1% 74.8% Octo-Base (Team et al., 2024b) OpenVLA (Kim et al., 2024) SpatialVLA (Qu et al., 2025) CoT-VLA (Zhao et al., 2025) NORA-AC (Hung et al., 2025) WorldVLA (Cen et al., 2025) π0-FAST (Black et al.) ThinkAct (Huang et al., 2025) MolmoAct-7B-D 78.9% 84.7% 88.2% 87.5% 85.6% 87.6% 96.4% 88.3% 87.0% 85.7% 84.6% 51.1% 75.1% 88.4% 79.2% 53.7% 76.5% 89.9% 78.6% 55.5% 78.1% 91.6% 87.6% 69.0% 83.9% 89.4% 80.0% 63.0% 79.5% 96.2% 83.4% 60.0% 79.1% 96.8% 88.6% 60.2% 85.5% 91.4% 87.1% 70.9% 84.4% 95.4% 87.6% 77.2% 86.6% (7.5%) for both action reasoning and trajectory-conditioned datasets, 7.5% from the auxiliary depth and visual trace datasets, and 5% from multimodal web data as shown in Figure 3. The whole data mixture used for pre-training MolmoAct totals up to 26.3M samples. To pre-train MolmoAct with the data mixture mentioned above, we use 256 H100s to train the model with 100k gradient steps using batch size of 512 takes up 9,728 GPU hours. At each training step, batch of data pairs is extracted at random timesteps from the shuffled robot demonstrations or randomly from the multimodal web data. The data pairs are drawn based on the sampling rate defined above."
        },
        {
            "title": "4.3 Post-training\nAfter mid-training, we conduct the final post-training stage to rapidly adapt the model to new tasks and\nembodiments. For new tasks, we collect a small set of 30 to 50 tele-operated demonstrations per task, then\ngenerate perception tokens and visual reasoning trace tokens for each timestep. These demonstrations are\nconverted into the action reasoning data and trajectory-conditioned action data, following the same process\nas mid-training data curation, with one key difference: to enhance manipulation performance and prevent\nno-ops from degrading results, we apply Action-Chunking (Zhao et al., 2023) during post-training, formatting\naction predictions in fixed-length chunks (N = 8). We format the action chunks in the same way as we do\nfor single action. After grouping them to a list, we train the model autoregressively for predicting all action\nchunks. We adapt MolmoAct to fine-tune downstream single or multi-tasks via parameter-efficient LoRA\nfine-tuning. In all evaluations, we fix the LoRA rank at 32 and alpha at 16 to preserve the model’s pre-trained\ncapabilities. For simulation benchmarks (e.g., LIBERO), we use a batch size of 128, and for real-world tasks\nwe use 64. The number of gradient steps we train varies by task. Post-training data generally consists of a\nfront- or side-view image paired with a wrist-view image, although some setups provide multiple wrist views\n(e.g., bimanual scenarios). In every case, we apply the same LoRA and training configuration described above.",
            "content": "10 Additional details are available in Appendix B."
        },
        {
            "title": "5 Experimental Evaluation",
            "content": "Our experimental evaluation comprises broad suite of studies that rigorously benchmark MolmoAct against strong baselines. We assess MolmoAct with MolmoAct-7B-D version in (i) its pre-training, out-of-the-box capabilities, (ii) its post-training adaptability across varied tasks, domains, and robotic embodiments, and MolmoAct-7B-D-Pretrain for (iii) its additional feature of being an interactive and steerable action reasoning model. By testing the model on comprehensive range of scenarios both in simulation and real-world, we aim to answer the following research questions:"
        },
        {
            "title": "3 How effectively can MolmoAct generalize beyond its training distribution? We investigate this through",
            "content": "real-world out-of-distribution (OOD) tests and variant-aggregation experiments in SimplerEnv."
        },
        {
            "title": "5.1 Evaluation of MolmoAct pre-trained model\nEvaluation Setup and Baselines. We first evaluated MolmoAct zero-shot—immediately after pre-training\nand before any task-specific fine-tuning. Unlike π0 (Black et al.), GR00T N1 (NVIDIA et al., 2025), and\nother proprietary VLA models that rely on large-scale private robot datasets and the full OXE dataset for\npre-training, MolmoAct was trained exclusively on curated action reasoning data filtered from a subset\nof OXE (specifically BC-Z (Jang et al., 2022), BridgeData V2 (Walke et al., 2023), and RT-1 (Brohan\net al., 2022)), combined with embodied reasoning VQA and auxiliary robot data, as detailed in Section\n4.1. This amounts to approximately 26.3M data points—an order of magnitude smaller than π0, which\nuses at least 903M for pre-training. To evaluate MolmoAct’s out-of-the-box generalization, we used the\nSimplerEnv benchmark, which features both visual-matching and variant-aggregation tasks across WidowX\nand Google Robot platforms. As MolmoAct ’s pre-training distribution is most aligned with the Google\nRobot visual-matching tasks, we focused our evaluation on this suite to best isolate in-distribution performance\nand capabilities of pre-training.",
            "content": "We compared MolmoAct against set of generalist policies, including TraceVLA (Zheng et al., 2024), RT-1X (Brohan et al., 2022), OpenVLA (Kim et al., 2024), RoboVLM (Liu et al., 2025), Emma-x (Sun et al., 2024), π0 and π0-FAST (Black et al.), Octo (Team et al., 2024b), Magma (Yang et al., 2025b), HPT (Wang et al., 2024a), SpatialVLA (Qu et al., 2025) and GR00T N1 (NVIDIA et al., 2025). Most baselines were evaluated in the zero-shot setting, with subset also tested after fine-tuning. We additionally fine-tuned MolmoAct on the RT-1 subset of OXE to assess its capacity when given more pre-training data. Evaluation Results. MolmoAct achieved strong zero-shot performance on the SimplerEnv visual-matching 11 Figure 5 Real-world evaluation of OpenVLA, π0-FAST, and MolmoAct on single-arm (left) and bimanual (right) Franka tasks. Bar plots report average task progression with standard error across 25 trials per task. MolmoAct consistently outperforms baselines, particularly on single-arm tasks such as Wipe Table and Table Bussing, and maintains strong performance on bimanual tasks including Fold Towel and Set Table. Bottom row shows example task setups with corresponding natural language instructions. suite, reaching 70.5% accuracy and outperforming baselines such as GR00T N1, π0, π0-FAST, and Magma. With fine-tuning on the same RT-1 subset of OXE, MolmoAct-7B-D improved to 71.6%, exceeding Magma by 3.2% as shown in Table 1. These results indicate that MolmoAct is both an effective zero-shot generalist and strong initialization for fine-tuned deployment."
        },
        {
            "title": "5.2 Efficiency of MolmoAct fine-tuning\nEvaluation Setups and Baselines. We evaluate MolmoAct in both simulation and real-world settings\nIn simulation, we follow the OpenVLA\nto assess its generalization capabilities after rapid fine-tuning.\nevaluation protocol and benchmark MolmoAct on the LIBERO simulation benchmark, which includes\ndiverse robotic manipulation tasks with 50 human-teleoperated demonstrations per task. We consider four\nsubsets targeting different generalization axes: LIBERO-Spatial (spatial reasoning), LIBERO-Object (object\nvariation), LIBERO-Goal (task abstraction), and LIBERO-Long (long-horizon planning). MolmoAct is\nfine-tuned using Low-Rank Adaptation (LoRA) and compared to state-of-the-art generalist policies that are\nautoregressive in nature, including TraceVLA (Zheng et al., 2024), OpenVLA (Kim et al., 2024), SpatialVLA\n(Qu et al., 2025), π0-FAST (Black et al.), CoT-VLA (Zhao et al., 2025), WorldVLA (Cen et al., 2025),\nThinkAct (Huang et al., 2025), and NORA-AC (Hung et al., 2025). In the real world, we evaluate MolmoAct\non six tasks across single-arm and bimanual Franka setups. The single-arm tasks include Put_Bowl_in_Sink,\nWipe_Table, and Table_Bussing. The bimanual tasks include Set_Table, Lift_Box, and Fold_Towel. Each\ntask is paired with 50 human demonstrations. We fine-tune both MolmoAct and baseline models on this\ndata and evaluate performance over 25 trials per task using the task progression metric described in Appendix\nD. For both simulation and real-world evaluations, we used the MolmoAct model after it was mid-trained\non our MolmoAct Dataset for LoRA fine-tuning. This setup enables a comprehensive comparison of\nadaptation efficiency across tasks and embodiments.",
            "content": "Evaluation Results. We evaluate on the LIBERO simulation benchmark (Liu et al. (2023a)), which consists of Franka Emika Panda arm in simulation with demonstrations containing front and wrist view camera images (256256 px), tasks language instructions, and delta end-effector pose actions. We follow prior works (Kim et al. (2024)) 12 (a) MolmoAct generalizes beyond training distributions. (b) MolmoAct Dataset improves task performance. Figure 6 MolmoAct outperforms baselines across generalization and mid-training settings. (a) Out-ofdistribution generalization: Task progression scores for OpenVLA, π0-FAST, and MolmoAct across in-distribution, language variation, spatial variation, distractors, and novel object conditions, showing consistent gains for MolmoAct. (b) Effectiveness of mid-training with the MolmoAct Dataset: Comparison of task progression on real-world tasks (Close Lid, Rotate Pot, Pour Tea) for MolmoAct with and without the dataset, π0-FAST, and OpenVLA, demonstrating that mid-training with the dataset improves performance across tasks. and evaluate on the four task suites LIBERO-Spatial, LIBERO-Object, LIBERO-Goal and LIBERO-Long each with 500 demonstration across 10 tasks. Following (Kim et al. (2024)), we trained on modified dataset which filtered out no-ops actions and unsuccessful demonstrations. Moreover, we set action chunk size to = 8 for evaluation on each task suites and execute full chunks before redoing action reasoning. On the LIBERO benchmark, MolmoAct achieves an average success rate of 86.6%, the highest among all compared methods. It performs particularly well on LIBERO-Long, challenging long-horizon suite, where it exceeds the performance of ThinkActthe second-best method in this settingby 6.3%. MolmoAct also scores highest on LIBERO-Goal, achieving 87.6% as shown in Table 2, indicating strong generalization to task-oriented behaviors. In the real world, MolmoAct demonstrates effective fine-tuning and generalization across different embodiments. It outperforms π0-FAST by an average of 10% in task progression on single-arm tasks and by 22.7% on bimanual tasks depicted in Figure 5. 13 Figure 7 Line steerability evaluation across models. Left: Elo ratings show that MolmoAct achieves the highest performance, surpassing Gemini-2.5-Flash, GPT-4o, and HAMSTER, with error bars indicating standard error. Right: Example qualitative results showing predicted visual traces overlaid on robot camera views. Figure 8 Language Instruction Evaluation. Left: Elo ratings for three models based on human votes in head-to-head instruction-following evaluation. Right: Qualitative comparison of execution traces for the open-ended instruction Put the redbull into the bowl.\" MolmoAct aligns more closely with the intended instruction than other models."
        },
        {
            "title": "5.3 Effectiveness of MolmoAct in Out-of-Distribution Generalization\nWe evaluate MolmoAct in both simulation and real-world settings to assess its ability to generalize beyond the\ntraining data distribution, both in zero-shot and fine-tuned regimes. In simulation, we follow the SimplerEnv\nvariant-aggregation protocol, which introduces distribution shifts through changes in lighting, textures, and\ncamera viewpoints. We compare MolmoAct against several state-of-the-art generalist policies—TraceVLA,\nRT-1X, OpenVLA, RoboVLM, Emma-X, π0-FAST, and SpatialVAL. For real-world evaluation, we test\nMolmoAct using a single Franka arm on a multi-task setup involving three objects and two different-colored\nplates arranged on a tabletop. We collect over 300 demonstrations spanning three task types, then train\nMolmoAct and baselines in a multi-task setting. During evaluation, we test generalization across four axes:\n(1) Language Variation — rephrased instructions, (2) Spatial Variation — changes in target object position,\n(3) Distractors — addition of irrelevant objects, and (4) Novel Objects — substitution of target objects with\nunseen ones. We benchmark MolmoAct against π0-FAST and OpenVLA, testing three variants per task\nand four trials per variant.",
            "content": "Evaluation Results. In simulation, fine-tuned MolmoAct achieves 72.1% on the variant aggregation tasks 14 Figure 9 Steerability evaluation with open instructions and visual traces. Left: Success rates for different steering modes, showing that MolmoAct with visual trace steering achieves the highest success rate (0.75), outperforming its open-instruction variant and π0-FAST. Right: Example of the \"Pick up the bowl\" task: the model-predicted trajectory (yellow) is adjusted via user-provided steering trajectory (cyan), resulting in the corrected task completion. as shown in Table 1, outperforming all baselines and exceeding the second-best model, RT-2-X, by 7.8%. The performance difference between variant aggregation and visual matching is less than 1%, highlighting MolmoActs robustness to visual and distributional shifts. In the real world, MolmoAct consistently surpasses all baselines across all generalization axes, achieving 23.3% average improvement in task progression over π0-FAST as shown in Figure 6a."
        },
        {
            "title": "5.4 Effect of the MolmoAct Dataset on MolmoAct performance\nEvaluation Setups and Baselines. To assess the effectiveness of mid-training with the MolmoAct Dataset,\nwe conducted real-world experiments on three curated tasks that goes beyond simple pick-and-place: close_-\nlid, rotate_pot, and pour_tea. For each task, we collected 50 demonstrations and trained four models:\nMolmoAct-7B-D, MolmoAct-7B-D without mid-training, π0-FAST, and OpenVLA. Each model was then\nevaluated over 10 trials per task.",
            "content": "Evaluation Results. Based on the real-world ablation studies shown in Figure 6b, MolmoAct outperforms its counterpart without mid-training by an average margin of 5.5% across the three tasks, demonstrating that mid-training on the MolmoAct Dataset yields consistent performance boost of around 5%. Even without mid-training, MolmoAct surpasses π0-FAST and OpenVLA by 14.8% and 10.9%, respectively."
        },
        {
            "title": "5.5 Instruction following of MolmoAct\nWe evaluated MolmoAct’s ability to follow natural language instructions in two settings: (i) executing\nopen-ended commands in simulation, and (ii) generating visual traces conditioned on language prompts. For\nsimulation-based instruction following, we curated five manipulation scenarios in the SimplerEnv environment\nusing a Google Robot, each involving novel out-of-distribution objects. Ten participants provided 29 open-\nended instructions (e.g., “Put the redbull into the bowl.\"). We compared MolmoAct to SpatialVLA and\nOpenVLA, both pretrained on the OXE dataset. For each instruction, the models generated trajectory rollouts,\nwhich were evaluated in a head-to-head arena-style web interface. Human annotators (n=100) selected which\ntrajectory best matched the instruction. We collected over 1,500 votes, which were converted into Elo ratings\n(see Figure 8). For visual trace generation, ten participants wrote 87 language prompts grounded in 30\ninternet-sourced images depicting tabletop and mobile manipulation contexts. MolmoAct was evaluated\nagainst Gemini-2.5-Flash, GPT-4o, and HAMSTER—a VLM fine-tuned for trace generation. Participants voted\nin a similar blind arena interface, resulting in over 1,000 votes.",
            "content": "Evaluation Results. MolmoAct achieved the highest Elo rating in the simulation instruction-following task, outperforming SpatialVLA by 109 points and OpenVLA by an even larger margin. Pairwise win rates also shown that MolmoAct winning over SpatialVLA in 58% of comparisons and over OpenVLA in 81%. sample trajectory comparison for the instruction Put the redbull into the bowl.\" is shown on the right. In the visual trace task, MolmoAct again outperformed all baselines, achieving significantly higher Elo scores with non-overlapping 95% confidence intervals, demonstrating strong generalization in both spatial execution and language-grounded reasoning."
        },
        {
            "title": "5.6 Steerability of MolmoAct\nEvaluation Setups and Baselines. We aim to evaluate MolmoAct’s ability to steer robot actions, particularly\nwhen initial language instructions are ambiguous. Specifically, we investigate the effectiveness of different\ninteraction mediums in guiding MolmoAct toward user-intended targets during task execution. For this\npurpose, we set up a pick_up_the_bowl task, training MolmoAct and the baseline model (π0-FAST) with\n100 collected demonstrations, each annotated with two distinct language instructions: one specifying the clean\nbowl and the other the dirty bowl, as depicted in Figure 7. During evaluation, we first provide ambiguous\ninstructions such as \"pick up the bowl,\" prompting MolmoAct to predict an initial trajectory towards\none of the bowls. Subsequently, we test two steering methods: visual trace sketches to visually instruct the\nmodel toward the alternative bowl, and open-ended natural language instructions provided interactively by\nparticipants (N=10). For comparison, we also attempt to steer the actions of π0-FAST by changing language\ninstructions at test-time. Each model is evaluated in 15 trials, and the performance is evaluated according to\nthe progression of the task.",
            "content": "Evaluation Results. Based on our experiments, we observed that MolmoAct is notably more steerable via visual trace inputs, achieving success rate of 75%. Additionally, steering using visual traces significantly outperforms steering via open-ended natural language instructions by margin of 33%. Lastly, we demonstrate that MolmoAct exhibits superior instruction-following capabilities compared to the baseline model, π0-FAST. Specifically, when steering robot actions using language instructions, MolmoAct surpasses π0-FAST by substantial margin of 29%, highlighting its enhanced responsiveness to user commands."
        },
        {
            "title": "6.2 Robot reasoning and planning with language\nIn recent years, numerous works have demonstrated that augmenting end-to-end robotic policies with high-level\nreasoning—either by integrating LLMs or VLMs directly into robotic systems, or by incorporating their\nreasoning outputs into policies—can substantially improve performance on long-horizon tasks and enhance\ngeneralization (Ahn et al., 2022; Huang et al., 2023; Bharadhwaj et al., 2024; Fang et al., 2025; Liu et al.,\n2024b; Shi et al., 2024; Wang et al., 2024b; Gu et al., 2023). An alternative line of research seeks to decouple\nperception and reasoning from low-level control, assigning VLMs the role of performing semantic prediction",
            "content": "16 or generating intermediate representations such as task plans, scene graphs, or spatial layouts (Duan et al., 2024b; Liu et al., 2024a; Li et al., 2025; Huang et al., 2024; Liang et al., 2022; Singh et al., 2022; Duan et al., 2024a). Execution is then handled by separate low-level policy or control module that interprets these high-level outputs and converts them into robot actions. Action prediction from MolmoAct can be steered through both natural language and an interactive visual reasoning-trace sketch interface. This dual-modality control improves explainability and enables more effective diagnosis of model behavior. While prior methods such as RT-Trajectory (Gu et al., 2023), HAMSTER (Li et al., 2025), and inference-time policy steering (Wang et al., 2024b) also offer forms of policy steerability, they differ in important ways. RT-Trajectory and inference-time policy steering are tightly coupled to the architectural constraints and training regimes of robotics transformers or diffusion models, and therefore lack the broader semantic generalization provided by pre-training on VLM backbone. HAMSTER enables language-conditioned trajectory steering but outputs only 2D trajectories by the high-level VLM, with execution handled by low-level policy trained on fixed set of tasks. In contrast, MolmoAct generalizes its steering to novel spatial configurations, previously unseen objects, and even ambiguous language instructions for diverse set of tasks, offering more versatile and semantically grounded control interface with users."
        },
        {
            "title": "6.3 Embodied reasoning for robotic manipulation\nChain-of-thought (CoT) prompting (Wei et al., 2022) has significantly improved the multi-step reasoning\ncapabilities of LLMs across domains such as mathematics, programming, and question answering. This\nidea has also been extended to the visual domain through multimodal CoT (Bigverdi et al., 2025; Zhang\net al., 2023), where visual information is processed iteratively and reasoned over in conjunction with images.\nMotivated by these advances, recent works in robotics have explored extending reasoning capabilities to\nembodied tasks within vision-language-action (VLA) models.",
            "content": "ECoT (Zawalski et al., 2024) synthesizes intermediate subgoals via prompting and uses supervised fine-tuning to teach VLAs to reason before acting. CoT-VLA (Zhao et al., 2025) replaces linguistic CoT with visual subgoal frames generated prior to action prediction. RAD (Clark et al., 2025) which leverages action-free human video to curate language-based reasoning to guide low-level actions. ThinkAct (Huang et al., 2025) leverages action-aligned reinforcement learning and visual latent planning to connect embodied reasoning with real-world action prediction in VLAs. Most similar to our reasoning-in-space approach are Emma-X (Sun et al., 2024), which autoregressively fine-tunes OpenVLA with reasoning data formatted as subtasks, predicted future gripper states in 2D, and 3D spatial movement coordinates or (Yang et al., 2025a), which focuses on different forms of mid-level representations, including trajectory trace with depth awareness, however they only evaluated for diffusion policy on small data region. However, unlike ECoT, CoT-VLA, RAD, and ThinkActwhose reasoning is represented as latent embeddings, generated sub-goals, or textual descriptions that are difficult to ground in the real world and lack the precision required for manipulationMolmoAct grounds every step of its reasoning chain directly in the scene. Compared to Emma-X, which reasons primarily over predicted gripper positions without leveraging the full 3D scene context, MolmoAct performs reasoning in space, where each step can be decoded and visualized both on the image plane and within the 3D environment. This explicit spatial grounding improves explainability and enhances action prediction within chain-of-thought prompting framework."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced MolmoAct, family of fully open action reasoning models that integrate perception, planning, and control by reasoning in space. By combining depth perception tokens, visual reasoning traces, and action prediction, MolmoAct produces explainable, spatially coherent behaviors. The behaviors can be executed directly, or steered via trajectory editing. Our evaluations across simulation and real-world settings demonstrate that MolmoAct consistently outperforms strong visionlanguageaction baselines, adapts efficiently to novel tasks and embodiments through lightweight fine-tuning, and generalizes robustly to out-of-distribution conditions. We release all model weights, code, and the MolmoAct Dataset to enable reproducibility and foster community-driven research toward building foundation models that transform perception into purposeful action through structured reasoning."
        },
        {
            "title": "Author Contributions",
            "content": "This project was made possible through the equal contributions of all three co-first authors in no particular order. Their individual contributions are as follows: Jason Lee: Led the Action Reasoning data curation for pre-training and post-training; Developed simulation evaluation infrastructure for pre-training and post-training; Led the real-world evaluations and simulation evaluations; contributed to real-world data collection. Jiafei Duan: Led the project and ideated the core method design; proposed and curated the MolmoAct Dataset; spearheaded the papers writing; and designed and conducted both simulation and real-world evaluations, including ablation studies. Haoquan Fang: Led the implementation of the model, training, and inference pipeline and infrastructure; designed and developed the steerability feature and evaluation framework; contributed to data curation, simulation/real-world evaluations, and paper writing. All other contributors are also deeply appreciated for their effort, which is critical to the success of the MolmoAct project. As not all of these can be captured, we indicate their primary contributing role in MolmoAct: For real-world robot infrastructure: Yuquan Deng, Shuo Liu, and Boyang Li. For real-world data collection and evaluation: Yuquan Deng, Bohan Fang, Shuo Liu, Boyang Li, and Angelica Wu. For paper writing and figures: Jiafei Duan, Yi Ru Wang, Jason Lee, Haoquan Fang, Jieyu Zhang, Winson Han, Eli VanderBilt, and Ranjay Krishna. For project management: Karen Farley. For research advisory: Ranjay Krishna, Dieter Fox, Rose Hendrix, and Ali Farhadi. Project PI: Ranjay Krishna"
        },
        {
            "title": "Acknowledgment",
            "content": "This work would not be possible without the support of our colleagues at Ai2: We thank Christopher Clark, Abhay Deshpande, Yejin Kim, and Rohuan Tripathi for helpful research discussions and sharing of relevant findings across related projects. We thank for David Ablbright, Crystal Nam, Kristin Cha, Sophie Lebrecht, Taira Anderson, Kyle Wiggers, Kelsey MacMillan, Katie Morigi, and Megan Bartot project management, support to robot room and publicity of MolmoAct We thank Yoganand Chandrasekhar, Johann Dahm, Fangzhou Hu, and Caroline Wu for their work on the Ai2 cluster. MolmoAct would not have been possible without the support of many other institutions. In particular, we thank Google for their support in setting up the training environment for OLMo 2 and to Cirrascale for their on-going support of Ai2s cluster. Jiafei Duan is supported by the Agency for Science, Technology and Research (A*STAR) National Science Fellowship."
        },
        {
            "title": "References",
            "content": "M. Acharya, K. Kafle, and C. Kanan. TallyQA: Answering complex counting questions. In AAAI, 2019. J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. L. Berscheid, P. Meißner, and T. Kröger. Robot learning of shifting objects for grasping in cluttered environments. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 612618. IEEE, 2019. H. Bharadhwaj, J. Vakil, M. Sharma, A. Gupta, S. Tulsiani, and V. Kumar. Roboagent: Generalization and efficiency in robot manipulation via semantic augmentations and action chunking. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 47884795. IEEE, 2024. M. Bigverdi, Z. Luo, C.-Y. Hsieh, E. Shen, D. Chen, L. G. Shapiro, and R. Krishna. Perception tokens enhance visual reasoning in multimodal language models, 2024. URL https://arxiv.org/abs/2412.03548. M. Bigverdi, Z. Luo, C.-Y. Hsieh, E. Shen, D. Chen, L. G. Shapiro, and R. Krishna. Perception tokens enhance visual reasoning in multimodal language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 38363845, 2025. A. F. Biten, R. Tito, A. Mafla, L. Gomez, M. Rusinol, E. Valveny, C. Jawahar, and D. Karatzas. Scene text visual question answering. In ICCV, 2019. J. Bjorck, F. Castañeda, N. Cherniadev, X. Da, R. Ding, L. Fan, Y. Fang, D. Fox, F. Hu, S. Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, et al. π0: vision-language-action flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550. arXiv preprint ARXIV.2410.24164. A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. J. Cen, C. Yu, H. Yuan, Y. Jiang, S. Huang, J. Guo, X. Li, Y. Song, H. Luo, F. Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. J. Clark, S. Mirchandani, D. Sadigh, and S. Belkhale. Action-free reasoning for policy generalization. arXiv preprint arXiv:2502.03729, 2025. S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and C. Finn. Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215, 2019. M. Deitke, C. Clark, S. Lee, R. Tripathi, Y. Yang, J. S. Park, M. Salehi, N. Muennighoff, K. Lo, L. Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv e-prints, pages arXiv2409, 2024. J. Duan, S. Yu, H. L. Tan, H. Zhu, and C. Tan. survey of embodied ai: From simulators to research tasks. IEEE Transactions on Emerging Topics in Computational Intelligence, 6(2):230244, 2022. J. Duan, W. Pumacay, N. Kumar, Y. R. Wang, S. Tian, W. Yuan, R. Krishna, D. Fox, A. Mandlekar, and Y. Guo. Aha: vision-language-model for detecting and reasoning over failures in robotic manipulation. arXiv preprint arXiv:2410.00371, 2024a. J. Duan, W. Yuan, W. Pumacay, Y. R. Wang, K. Ehsani, D. Fox, and R. Krishna. Manipulate-anything: Automating real-world robots using vision-language models. arXiv preprint arXiv:2406.18915, 2024b. F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and S. Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396, 2021. H. Fang, M. Grotz, W. Pumacay, Y. R. Wang, D. Fox, R. Krishna, and J. Duan. Sam2act: Integrating visual foundation model with memory architecture for robotic manipulation, 2025. URL https://arxiv.org/abs/2501.18564. H.-S. Fang, H. Fang, Z. Tang, J. Liu, C. Wang, J. Wang, H. Zhu, and C. Lu. Rh20t: comprehensive robotic dataset for learning diverse skills in one-shot. arXiv preprint arXiv:2307.00595, 2023. 19 R. Firoozi, J. Tucker, S. Tian, A. Majumdar, J. Sun, W. Liu, Y. Zhu, S. Song, A. Kapoor, K. Hausman, et al. Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, 44(5):701739, 2025. Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the in VQA matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. D. Groeneveld, I. Beltagy, P. Walsh, A. Bhagia, R. Kinney, O. Tafjord, A. H. Jha, H. Ivison, I. Magnusson, Y. Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. J. Gu, S. Kirmani, P. Wohlhart, Y. Lu, M. G. Arenas, K. Rao, W. Yu, C. Fu, K. Gopalakrishnan, Z. Xu, P. Sundaresan, P. Xu, H. Su, K. Hausman, C. Finn, Q. Vuong, and T. Xiao. Rt-trajectory: Robotic task generalization via hindsight trajectory sketches, 2023. A. Gupta, P. Dollar, and R. Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. C.-P. Huang, Y.-H. Wu, M.-H. Chen, Y.-C. F. Wang, and F.-E. Yang. Thinkact: Vision-language-action reasoning via reinforced visual latent planning. arXiv preprint arXiv:2507.16815, 2025. J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023. W. Huang, C. Wang, Y. Li, R. Zhang, and L. Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. C.-Y. Hung, Q. Sun, P. Hong, A. Zadeh, C. Li, U. Tan, N. Majumder, S. Poria, et al. Nora: small open-sourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854, 2025. E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 9911002. PMLR, 2022. K. Kafle, B. Price, S. Cohen, and C. Kanan. DVQA: Understanding data visualizations via question answering. In CVPR, 2018. S. E. Kahou, V. Michalski, A. Atkinson, Á. Kádár, A. Trischler, and Y. Bengio. FigureQA: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. diagram is worth dozen images. In ECCV, 2016. A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Q. Li, Y. Liang, Z. Wang, L. Luo, X. Chen, M. Liao, F. Wei, Y. Deng, S. Xu, Y. Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024a. W. Li, W. Bishop, A. Li, C. Rawles, F. Campbell-Ajala, D. Tyamagundlu, and O. Riva. On the effects of data scale on computer control agents. arXiv preprint arXiv:2406.03679, 2024b. X. Li, K. Hsu, J. Gu, K. Pertsch, O. Mees, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024c. Y. Li, Y. Deng, J. Zhang, J. Jang, M. Memmel, R. Yu, C. R. Garrett, F. Ramos, D. Fox, A. Li, et al. Hamster: Hierarchical action models for open-world robot manipulation. arXiv preprint arXiv:2502.05485, 2025. J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022. 20 F. Lin, Y. Hu, P. Sheng, C. Wen, J. You, and Y. Gao. Data scaling laws in imitation learning for robotic manipulation. arXiv preprint arXiv:2410.18647, 2024. B. Liu, Y. Zhu, C. Gao, Y. Feng, Q. Liu, Y. Zhu, and P. Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023a. F. Liu, K. Fang, P. Abbeel, and S. Levine. Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024a. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36: 3489234916, 2023b. H. Liu, X. Li, P. Li, M. Liu, D. Wang, J. Liu, B. Kang, X. Ma, T. Kong, and H. Zhang. Towards generalist robot policies: What matters in building vision-language-action models. 2025. P. Liu, Y. Orru, J. Vakil, C. Paxton, N. M. M. Shafiullah, and L. Pinto. Ok-robot: What really matters in integrating open-knowledge models for robotics. arXiv preprint arXiv:2401.12202, 2024b. S. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024c. P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022. P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In ICLR, 2023. A. Mandlekar, Y. Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta, E. Orbay, et al. Roboturk: crowdsourcing platform for robotic skill learning through imitation. In Conference on Robot Learning, pages 879893. PMLR, 2018. K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi. OK-VQA: visual question answering benchmark requiring external knowledge. In CVPR, 2019. A. Masry, D. Long, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In ACL, 2022. M. Mathew, D. Karatzas, and C. Jawahar. DocVQA: dataset for VQA on document images. In WACV, 2021. M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar. InfographicVQA. In WACV, 2022. N. Methani, P. Ganguly, M. M. Khapra, and P. Kumar. PlotQA: Reasoning over scientific plots. In WACV, 2020. NVIDIA, :, J. Bjorck, F. Castañeda, N. Cherniadev, X. Da, R. Ding, L. J. Fan, Y. Fang, D. Fox, F. Hu, S. Huang, J. Jang, Z. Jiang, J. Kautz, K. Kundalia, L. Lao, Z. Li, Z. Lin, K. Lin, G. Liu, E. Llontop, L. Magne, A. Mandlekar, A. Narayan, S. Nasiriany, S. Reed, Y. L. Tan, G. Wang, Z. Wang, J. Wang, Q. Wang, J. Xiang, Y. Xie, Y. Xu, Z. Xu, S. Ye, Z. Yu, A. Zhang, H. Zhang, Y. Zhao, R. Zheng, and Y. Zhu. Gr00t n1: An open foundation model for generalist humanoid robots, 2025. URL https://arxiv.org/abs/2503.14734. T. OLMo, P. Walsh, L. Soldaini, D. Groeneveld, K. Lo, S. Arora, A. Bhagia, Y. Gu, S. Huang, M. Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. A. ONeill, A. Rehman, A. Maddukuri, A. Gupta, A. Padalkar, A. Lee, A. Pooley, A. Gupta, A. Mandlekar, A. Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. K. Pertsch, K. Stachowicz, B. Ichter, D. Driess, S. Nair, Q. Vuong, O. Mees, C. Finn, and S. Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. W. Pumacay, I. Singh, J. Duan, R. Krishna, J. Thomason, and D. Fox. The colosseum: benchmark for evaluating generalization for robotic manipulation. arXiv preprint arXiv:2402.08191, 2024. D. Qu, H. Song, Q. Chen, Y. Yao, X. Ye, Y. Ding, Z. Wang, J. Gu, B. Zhao, D. Wang, et al. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. 21 A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi. A-OKVQA: benchmark for visual question answering using world knowledge. In ECCV, 2022. N. M. M. Shafiullah, A. Rai, H. Etukuru, Y. Liu, I. Misra, S. Chintala, and L. Pinto. On bringing robots home. arXiv preprint arXiv:2311.16098, 2023. Y. Shentu, P. Wu, A. Rajeswaran, and P. Abbeel. From llms to actions: Latent codes as bridges in hierarchical robot control. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 85398546. IEEE, 2024. L. X. Shi, Z. Hu, T. Z. Zhao, A. Sharma, K. Pertsch, J. Luo, S. Levine, and C. Finn. Yell at your robot: Improving on-the-fly from language corrections. arXiv preprint arXiv:2403.12910, 2024. A. Singh, V. Natarjan, M. Shah, Y. Jiang, X. Chen, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In CVPR, 2019. I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022. L. Soldaini, R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. Chandu, J. Dumas, Y. Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. Q. Sun, P. Hong, T. D. Pala, V. Toh, U. Tan, D. Ghosal, S. Poria, et al. Emma-x: An embodied multimodal action model with grounded chain of thought and look-ahead spatial reasoning. arXiv preprint arXiv:2412.11974, 2024. G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024a. G. R. Team, S. Abeyruwan, J. Ainslie, J.-B. Alayrac, M. G. Arenas, T. Armstrong, A. Balakrishna, R. Baruch, M. Bauza, M. Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. O. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna, T. Kreiman, C. Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024b. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. M. Tschannen, A. Gritsenko, X. Wang, M. F. Naeem, I. Alabdulmohsin, N. Parthasarathy, T. Evans, L. Beyer, Y. Xia, B. Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. B. Tversky. Your body thinks as much as your mind. IAI News, Aug. 2025. URL https://iai.tv/articles/ your-body-thinks-as-much-as-your-mind-auid-3282. Institute of Art and Ideas. A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. H. R. Walke, K. Black, T. Z. Zhao, Q. Vuong, C. Zheng, P. Hansen-Estruch, A. W. He, V. Myers, M. J. Kim, M. Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 17231736. PMLR, 2023. L. Wang, X. Chen, J. Zhao, and K. He. Scaling proprioceptive-visual learning with heterogeneous pre-trained transformers. Advances in neural information processing systems, 37:124420124450, 2024a. Y. Wang, L. Wang, Y. Du, B. Sundaralingam, X. Yang, Y.-W. Chao, C. Perez-DArpino, D. Fox, and J. Shah. Inference-time policy steering through human interactions. arXiv preprint arXiv:2411.16627, 2024b. 22 Y. Wang, S. Wu, Y. Zhang, S. Yan, Z. Liu, J. Luo, and H. Fei. Multimodal chain-of-thought reasoning: comprehensive survey, 2025. URL https://arxiv.org/abs/2503.12605. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. J. Wen, Y. Zhu, J. Li, Z. Tang, C. Shen, and F. Feng. Dexvla: Vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025a. J. Wen, Y. Zhu, J. Li, M. Zhu, Z. Tang, K. Wu, Z. Xu, N. Liu, R. Cheng, C. Shen, et al. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters, 2025b. A. Xie, L. Lee, T. Xiao, and C. Finn. Decomposing the generalization gap in imitation learning for visual robotic manipulation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 31533160. IEEE, 2024. H. Xu, S. Xie, X. Tan, P.-Y. Huang, R. Howes, V. Sharma, S.-W. Li, G. Ghosh, L. Zettlemoyer, and C. Feichtenhofer. Demystifying CLIP data. In ICLR, 2024a. Z. Xu, K. Wu, J. Wen, J. Li, N. Liu, Z. Che, and J. Tang. survey on robotics with foundation models: toward embodied ai. arXiv preprint arXiv:2402.02385, 2024b. J. Yang, C. K. Fu, D. Shah, D. Sadigh, F. Xia, and T. Zhang. Bridging perception and action: Spatially-grounded mid-level representations for robot generalization. arXiv preprint arXiv:2506.06196, 2025a. J. Yang, R. Tan, Q. Wu, R. Zheng, B. Peng, Y. Liang, Y. Gu, M. Cai, S. Ye, J. Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1420314214, 2025b. M. Zawalski, W. Chen, K. Pertsch, O. Mees, C. Finn, and S. Levine. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693, 2024. E. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. Q. Zhao, Y. Lu, M. J. Kim, Z. Fu, Z. Zhang, Y. Wu, Z. Li, Q. Ma, S. Han, C. Finn, et al. Cot-vla: Visual chain-ofthought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17021713, 2025. T. Z. Zhao, V. Kumar, S. Levine, and C. Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. R. Zheng, Y. Liang, S. Huang, J. Gao, H. Daumé III, A. Kolobov, F. Huang, and J. Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. arXiv preprint arXiv:2412.10345, 2024. B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid, et al. Rt-2: Visionlanguage-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023."
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "University of Washington"
    ]
}