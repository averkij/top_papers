{
    "paper_title": "IAUNet: Instance-Aware U-Net",
    "authors": [
        "Yaroslav Prytula",
        "Illia Tsiporenko",
        "Ali Zeynalli",
        "Dmytro Fishman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet"
        },
        {
            "title": "Start",
            "content": "IAUNet: Instance-Aware U-Net Yaroslav Prytula1,2 Illia Tsiporenko1 Ali Zeynalli1 Dmytro Fishman1,3 1Institute of Computer Science, University of Tartu 2Ukrainian Catholic University, 3STACC U, Tartu, Estonia 5 2 0 2 3 ] . [ 1 8 2 9 1 0 . 8 0 5 2 : r {yaroslav.prytula, illia.tsiporenko, ali.zeynalli, dmytro.fishman}@ut.ee s.prytula@ucu.edu.ua"
        },
        {
            "title": "Abstract",
            "content": "Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been go-to architecture in medical image segmentation, its potential in querybased approaches remains largely unexplored. In this work, we present IAUNet, novel query-based U-Net architecture. The core design features full U-Net architecture, enhanced by novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset 1, unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/ IAUNet 1. Introduction Accurate cell instance segmentation is crucial in biomedical imaging [1], as it enables the precise identification and analysis of individual cells. This process is essential for understanding cellular behaviors and disease mechanisms [24]. However, the diverse and irregular shapes of cells present significant challenges for segmentation algorithms [35, 46]. Variations in cell morphology, overlapping structures, and differing imaging conditions can lead to segmentation er1Dataset available at: https://github.com/SlavkoPrytula/IAUNet rors [1]. Addressing these challenges requires the development of advanced segmentation models capable of handling the complexities associated with cell shapes. Deep learning models have driven substantial progress in cell segmentation, often surpassing traditional methods [21, 42, 52]. However, cell segmentation remains challenging due to heterogeneous cell appearances, overlaps, and varied object densities across different microscopy modalities, requiring models that generalize well across conditions. Brightfield microscopy, valued for its simplicity and affordability, presents unique challenges for segmentation [1]. Unlike fluorescence microscopy, which requires staining, and phase-contrast microscopy, which relies on specialized optics to enhance contrast in transparent specimens, brightfield uses natural light alone [45]. This makes brightfield ideal for real-time observation in both research and clinical settings [1, 36, 51]. However, brightfield images are inherently low-contrast, noisy, and variable, making precise cell segmentation difficult and underscoring the need for specialized approaches tailored to this modality. Many previous works have adapted instance segmentation models from natural images to medical imaging without model-specific adjustments [22, 40, 50]. In contrast to many of these methods, U-Net [42] has long been go-to architecture for semantic segmentation. Its lightweight framework, characterized by skip connections and an encoderdecoder structure, enables precise localization and the effective capture of intricate details, making it especially well-suited for biomedical applications. U-Nets efficiency is particularly advantageous when working with smaller microscopy datasets, as it typically requires less data to train compared to more complex models. This is why we chose to focus on U-Net in our work, building on its established popularity and applicability to microscopy data. Building on the success of DETR [5] in object detection, query-based single-stage instance segmentation methods [79, 14, 20, 28] have gained prominence. These methods move away from traditional convolutional approaches, utilizing the powerful attention mechanism [49] together with learnable queries to directly predict object classes and segmentation masks in an end-to-end fashion. However, these models typically rely on single-level features to generate queries, refining them without leveraging the full range of features available from skip connections and decoder feature maps. This limits their ability to capture the rich multiscale context necessary for precise instance refinement. To address these limitations, we bridge the gap between the U-Net model, widely used in biomedical imaging, and the task of instance segmentation. We present IAUNet, novel architecture that enhances U-Net with instanceawareness through query-based mechanisms. This design incorporates lightweight convolutional Pixel decoder, enabling the model to scale effectively with larger backbones while maintaining strong performance across both small and large datasets. IAUNet also introduces Transformer decoder for multi-scale object feature refinement. As part of our contributions, we introduce the 2025 Revvity Full Cell Segmentation Dataset, specifically designed for benchmarking model performance. The dataset includes hundreds of carefully annotated cell instances in high-resolution brightfield images, each thoroughly handlabeled and validated. One of its unique features is the precise annotation of cell borders, even in cases of overlapping cells, allowing it to capture complex cell interactions. This dataset is valuable resource for evaluating model accuracy in capturing fine details and handling challenging segmentation tasks with intricate cell morphologies. Our main contributions are as follows: We introduce lightweight Pixel-Transformer decoder within U-Net for multi-scale object feature refinement, efficiently scaling with larger backbones. We introduce novel 2025 Revvity Full Cell Segmentation Dataset with detailed annotations and provide benchmark for instance segmentation. 2. Related Work Instance segmentation methods are generally categorized into region-based, query-based, and specialized approaches that often require preprocessing. Region-based Methods exemplified by Mask R-CNN [17, 22, 41], have set standard in natural image segmentation with their proposal-based structure. Building on Faster RCNN [41], Mask R-CNN adds mask prediction branch for end-to-end instance segmentation by first detecting bounding boxes and then applying Region of Interest (RoI) operations like RoI-Pooling [17] or RoI-Align [22] to extract features for classification and mask generation. However, these two-stage methods often generate numerous redundant region proposals, reducing efficiency [10, 20]. Although they perform well on many benchmarks, their reliance on small RoI regions frequently leads to coarse mask predictions. Some methods focus on enhancing the precision of detected bounding boxes [3], while others, like PointRend [26], specifically address low-quality segmentation masks by refining boundaries at uncertain points to improve segmentation quality. However, even with these advancements, traditional region-based methods face limitations in biomedical image segmentation [7], where objects have complex shapes, orientations, and sizes. In these settings, traditional axis-aligned bounding boxes struggle to capture detailed contours, particularly for irregular and overlapping cellular structures [15, 25]. Specialized Cell Instance Segmentation Methods like StarDist [43] segment biomedical images by representing objects as star-convex polygons, predicting distances from central point to boundaries in multiple directions. This method, along with other similar approaches like DeepWatershed [2] and Micro-Net [39], works well for starshaped or rounded cells but struggles with irregular, elongated shapes and overlapping cells. CellPose [48], by contrast, similar to Hover-Net [18], uses U-Net to predict horizontal and vertical gradients alongside binary cell map, creating vector field that directs pixels toward the cell center. While this method effectively separates individual cells, it often relies on an additional size model [37] to estimate object diameters, which becomes challenging with varying cell sizes and shapes. Although these methods offer advancements over traditional techniques, they remain limited in accurately segmenting overlapping cells and handling complex cellular morphologies. Query-based Methods have gained popularity since the introduction of DETR [5], which demonstrated the potential of Transformer-based architectures for instance segmentation. Unlike traditional region-based models, query-based methods use object queries to directly predict object instances, removing the need for predefined bounding boxes. Building on DETR, models like Mask2Former [9] and FastInst [20] introduced masked attention to improve convergence and segmentation precision. These models heavily rely on producing fine features using MSDeformAttn Transformer [54] Pixel decoder. MaskDINO [28] further advances instance segmentation by adding mask prediction branch that generates high-resolution binary masks through query embeddings for unified segmentation tasks. Recently, adaptations of query-based models have also emerged in the biomedical domain. For example, Cell-DETR [38] adapts DETR specifically for cell segmentation by leveraging queries to detect individual instances. The model uses the final feature map of the encoder for query initialization, limiting multi-scale query refinement across decoder features. Its segmentation head applies multi-head attention between encoder and decoder features, followed by CNN decoder. However, it merges queries with decoder features only at the lowest layer, forcing the CNN decoder to handle Figure 1. Model overview. Overview of the IAUNet architecture, highlighting the Pixel and Transformer Decoder stages. Given an input image I, the encoder extracts multi-scale features as skip connections for the Pixel decoder. At each decoder block, we add skip connections Xs to the main features and inject normalized coordinate features for CoordConv. Stacked depth-wise convolutions with an SE block refine spatial information, generating mask features Xm. The Transformer decoder then processes learnable queries through three Transformer blocks per layer, iteratively refining them with Xm. Deep supervision loss is applied after each Transformer block using updated queries ˆq and high-resolution mask features. most of the instance separation. This makes the model inefficient for high-resolution inputs with many queries. Additionally, Cell-DETR applies softmax to suppress overlapping predictions, reducing its ability to segment occluding cells effectively. Recent work, such as PCTrans [7], built on Mask2Former, introduces position-guided transformer with query contrastive loss. Similar to DETR, position guidance is done by predicting the normalized center coordinates of each object. While natural objects are often convex, cells present more complex shapes, with centers that often fall outside boundaries, particularly in elongated structures [11], making mask representation less effective. All previous query-based models [5, 7, 9, 28] have been designed around the idea of Transformer-based Pixel decoder, which raises concerns about scalability to smaller datasets. Unlike these models, we propose lightweight Pixel decoder that improves performance on smaller datasets. In Tab. 2, we show that IAUNet consistently outperforms state-of-the-art models across different backbones while maintaining strong results on large-scale datasets (Tab. 1). Our experiments show that IAUNet outperforms most alternatives while using fewer parameters and achieving higher efficiency. 3. Model Overview The IAUNet model follows U-Net design, illustrated in Fig. 1. The model consists of three main components: an encoder, Pixel decoder, and Transformer decoder. Given an input image RHW 3, the encoder produces four multi-scale semantic feature maps at resolutions of 1/4, 1/8, 1/16, and 1/32 relative to the original image. These feature maps are utilized as skip connections in the decoder. The Pixel decoder first processes these features to generate the main decoder features X. At each decoder layer, these features pass through lightweight mask branch to produce refined mask features, Xm, which then interact with object queries. The Transformer decoder further refines instance queries with mask features. This process is iterative, with updated queries passing through each decoder stage. In the final stage, the mask head combines mask features and instance queries to produce output instance masks. 3.1. Pixel Decoder In the biomedical domain, U-Net [42], with all its variants [4, 6, 19, 52], still holds the ground as the most superior network for accurate segmentation. This is primarily due to the design of U-Nets decoder, which maintains high semantic consistency through the use of skip connections. We include convolutional decoder, referred to as the Pixel decoder. Our Pixel decoder (Fig. 1, middle panel) works with two feature types: main features and mask features Xm. The main features serve similar role to those in the vanilla U-Net, aggregating spatial context across the image using skip connections Xs. The mask features refine and capture richer semantic information. All these features are specifically designed to support instance segmentation and are tightly integrated with the Transformer decoder (see Sec. 3.2). Figure 2. LIVECell. Visualization of instance segmentation predictions on the LIVECell dataset across different state-of-the-art models (using R50 backbone). We also report per-image AP score. Last columns shows ground-truth annotations. (cid:16) = SE (cid:0)[Xs, ](cid:1) + (cid:17) (cid:17) Gx (cid:16) Xm = Gm + (1) (2) At each level, the corresponding skip connection Xs is first mapped to 256-dimensional feature map. Then it gets concatenated with the upscaled decoder features from the previous layer and passed through lightweight double 3 3 point-wise convolution, batch normalization, and ReLU layer Gx (Eq. (1)). Next, we apply Squeeze-andExcitation (SE) [23] block to produce the final main features X. Next, we update mask features by adding the main features and the upscaled mask features from the previous layer followed by two stacked 3 3 convolutional layers Gm Eq. (2). The whole process preserves multi-scale semantic information while maintaining lightweight structure. The updated mask features are then used for query refinement in the corresponding Transformer blocks. Finally, we use bilinear upscaling to propagate all features to the next decoder layer. 3.2. Transformer Decoder Object queries are central to instance segmentation [9, 12, 14, 20], serving as learnable embeddings that represent each object as unique D-dimensional feature vector. These queries group pixel features relevant to each specific object, typically through cross-attention mechanism. They are particularly important in Transformer architectures [5], where they are processed and refined in an endto-end manner. In existing models such as DETR [5], Deformable DETR [53], MaskFormer [8], and Mask2Former [9], queries are central to representing objects for segmentation or detection tasks. In our work, we use learnable queries RN 256. Each query is thus 256-dimensional representation, capturing the finer semantic object features. These instance queries are progressively refined with mask features Xm through multi-layer Transformer decoder (see Sec. 3.2). At each decoder layer [1, L], we use three Transformer decoder layers. Queries from the previous decoder layer are iteratively processed through these layers (Fig. 1, red block) with the corresponding flattened mask features Xm RL256, where = Hl Wl for the l-th decoder layer. 3.2.1. Positional Embeddings for To maintain spatial awareness, which is crucial Transformer-based models, we add learnable positional embeddings to both instance queries. Following the previous work [5], we add sinusoidal positional embeddings epos RHlWlD to the mask Xm. 3.2.2. Instance Queries Update We update instance queries with the mask features Xm using the cross-attention layer (Fig. 1, red block) followed by the self-attention layer between queries and FFN layer. Thus, all queries attend to each other, ensuring better object separation. The update is expressed as follows: ˆXl = softmax (cid:0)QlK Xl = FFN( ˆXl) (cid:1) Vl + Xl1 (3) (4) where Ql = fQ(ql) RN 256 represents the transformed queries at layer l, and the keys and values Kl, Vl RHlWl256 are computed from the mask features Xm. The queries are updated sequentially within Transformer blocks at each decoder layer. 3.2.3. Mask Head To keep the prediction process lightweight without performance loss, we fuse only high-resolution features. As Figure 3. Revvity-25. Visualization of instance segmentation predictions on the Revvity-25 dataset across different state-of-the-art models (using R50 backbone). Last columns shows ground-truth annotations. IAUNet as well as MaskDINO show good generalization across tiny details and overlaping instances. We also report per-image AP score. shown in (Fig. 1, red arrows), we construct pixel embedding map by combining the 1/4 resolution backbone feature map Xb with an upsampled 1/8 resolution mask features Xm from the Pixel decoder. Specifically, we apply two linear projections on the refined instance queries to obtain mask embeddings qc and object class scores. The final mask prediction is obtained by taking the dot product of each mask embedding with this fused feature map: = qc (F(Xb) + U(Xm)) , (5) where is the segmentation head, is convolutional layer that adjusts the channel dimensions to match the Transformer hidden space, and is simple 2 upsampling function applied to Xm. Besides, each instance query predicts the object class probability, including no object (). During inference, we re-score the predicted masks. For each instance, we calculate the maskness metric [9], denoted as pi = 1 n=1 is the predicted instance mask. The combined confidence score for each instance is then computed by multiplying the class probability score ci with the maskness score pi: ˆci = ci pi. i=1 mi, where {Mn}N (cid:80)N 3.3. Mask Level Matching the model outputs {Mn}N n=1 predicted During training, masks, where > , the number of ground truth masks {Gk}M k=1. To compute losses on matched predictions, we perform bipartite matching between {Mn} and {Gk} using the Hungarian algorithm [47], which finds the optimal permutation σ that minimizes the matching cost: σ = arg min σS (cid:88) i=1 Lmatch(Mσ(i), Gi). (6) For the matching cost, we use combination of classification and mask costs: Lmatch = λcls Lcls + λdice Ldice + λbce Lbce (7) Following [9] we set λcls = 1.0, λdice = 2.0, and λbce = 5.0 to control the weight of each cost term. Here, Lcls represents the cross-entropy loss for object classification, with no object class weighted at 0.1. The terms Lbce and Ldice denote the binary cross-entropy loss and Dice loss, respectively, for the segmentation masks [34]. For the loss function, we align it with the matching cost by applying the same coefficients to ensure consistency. The final loss function is defined as: = λcls Lcls + λdice Ldice + λbce Lbce (8) 4. Experiments In this section, we evaluate our IAUNet on multiple datasets, including our novel Revvity-25 dataset. We also compare it with multiple state-of-the-art models in terms of segmentation performance. Besides, we conduct ablation studies and show the effectiveness of our model components. To provide comprehensive comparison, we use range of datasets: LIVECell [13] is one of the most extensive datasets regarding images and annotated cells for instance segmentation. It consists of 5,239 high-resolution phase-contrast images (520704 pixels) with over 1.6 million expert-validated annotated cells. It includes eight cell types with varied shapes and densities. EVICAN2 [44] is the most heterogeneous dataset for cell segmentation, containing 5,237 microscopy images across brightfield, phase contrast, and fluorescence modalities, num queries backbones R50 R50 R50 R50 R50 R101 R101 R101 R101 R101 Swin-S Swin-S Swin-S Swin-S Swin-S Swin-S Swin-S Swin-B Swin-B Swin-B Swin-B Swin-B Swin-B Swin-B Models Models with Convolution-Based Backbones Mask R-CNN [22] PointRend [26] Mask2Former [9] MaskDINO [28] IAUNet (ours) Mask R-CNN [22] PointRend [26] Mask2Former [9] MaskDINO [28] IAUNet (ours) Models with Transformer-Based Backbones Mask R-CNN [22] PointRend [26] Mask2Former [9] MaskDINO [28] MaskDINO [28] IAUNet (ours) IAUNet (ours) Mask R-CNN [22] PointRend [26] Mask2Former [9] MaskDINO [28] MaskDINO [28] IAUNet (ours) IAUNet (ours) Specialized Cell Segmentation Methods CellPose [48] CellPose + SM [37] CellDETR [38] IAUNet (ours) YOLO Family YOLOv8-M [40] YOLOv8-L [40] YOLOv8-X [40] IAUNet (ours) YOLOv9-E [50] YOLOv9-C [50] IAUNet (ours) SAM Family SAM-B (points) [27] SAM-B (boxes) [27] IAUNet (ours) SAM-L (points) [27] SAM-L (boxes) [27] IAUNet (ours) R34 R50 Swin-S Swin-S Swin-S Swin-B 100 100 100 100 100 100 100 100 100 100 100 100 100 300 100 300 100 100 100 100 300 100 300 - - 100 100 - - - 100 - - 100 - - 100 - - 300 LIVECell EVICAN2E EVICAN2M EVICAN2D ISBI2014 AP AP50 AP AP50 AP AP50 AP AP50 AP AP50 #params. FLOPs 44.7 44.0 43.7 43.3 45.3 44.2 44.0 44.0 43.4 45. 44.3 43.9 44.6 43.9 44.8 45.4 45.6 44.2 44.0 44.9 44.3 45.2 45.5 45.8 34.5 34.9 13.9 45.3 37.5 40.5 41.1 45.4 41.2 41.4 45.4 5.0 24.3 45.4 6.3 29.2 45.8 74.2 73.5 73.8 73.5 75.3 73.2 73.7 73.5 73.6 75.5 73.3 73.5 74.3 73.8 75.1 75.4 76.4 73.1 73.7 74.7 74.1 75.8 75.6 76. 60.1 60.4 32.7 75.3 72.2 72.5 73.1 75.4 73.2 73.1 75.4 12.4 56.9 75.4 13.6 65.2 76.7 48.1 26.6 53.4 50.7 58.0 41.5 41.3 54.4 53.7 58.3 52.6 55.1 65.2 57.0 56.5 58.8 60.9 52.0 58.6 55.0 57.3 57.9 59.6 61.2 0.9 8.7 0 58. 43.8 44.7 45.8 58.8 45.6 45.9 58.8 28.4 55.0 58.8 28.1 57.2 61.2 75.9 47.9 89.1 83.9 91.8 69.9 65.2 87.8 85.0 92.7 91.7 89.2 96.8 86.9 91.8 93.1 93.6 89.0 91.0 92.5 91.1 91.6 93.5 94.8 2.8 16.8 0.1 91.8 82.3 83.1 85.6 93.1 84.4 85.6 93. 56.0 96.6 93.1 54.1 96.6 94.8 20.7 18.0 29.1 29.3 32.1 23.3 20.2 27.1 31.8 32.9 27.0 30.1 36.2 33.6 35.0 32.2 33.2 26.7 34.1 31.4 37.3 39.1 34.2 38.0 0.1 1.6 0.0 32.1 27.5 28.1 28.9 32.2 27.2 28.3 32.2 5.4 38.6 32.2 4.9 45.8 38. 42.5 38.5 54.9 57.9 59.0 46.9 39.3 51.7 59.2 59.6 59.2 61.6 66.7 64.9 70.7 61.9 62.0 60.3 64.6 60.9 75.7 78.8 65.7 69.6 0.3 4.4 0.0 59.0 57.1 58.2 59.2 61.9 57.9 59.8 61.9 13.8 91.2 61.9 12.4 95.3 69.6 19.1 13.4 24.2 22.0 24.9 17.8 14.8 20.4 27.1 26. 20.2 24.4 30.9 27.6 30.2 27.7 29.6 24.8 25.8 27.7 30.1 34.0 28.9 30.7 0.0 2.3 0.0 24.9 20.0 20.3 20.7 27.7 20.1 22.2 27.7 3.2 34.8 27.7 3.2 39.7 30.7 39.8 28.3 50.4 41.9 45.4 36.7 32.1 42.4 51.3 50.0 50.2 54.6 62.7 56.9 64.3 54.1 58.0 55.5 52.0 56.6 65.6 72.3 56.9 59. 0.0 6.8 0.0 45.4 46.2 46.1 47.3 54.1 47.3 49.9 54.1 7.2 82.3 54.1 7.5 88.6 59.9 58.9 60.0 58.5 55.4 56.0 60.7 60.3 59.5 55.7 56.5 61.9 62.1 57.1 52.7 51.2 61.1 61.8 62.4 62.7 58.1 53.5 53.3 61.5 63.0 88.7 88.7 87.5 86.8 85.0 88.8 89.2 88.6 87.4 87. 90.7 91.0 87.3 85.3 83.4 90.1 89.8 91.5 91.5 88.4 86.6 84.8 90.8 91.5 40.5 41.6 0.046 56.0 69.3 70.4 0.135 85.0 54.9 55.1 55.3 61.1 53.3 55.7 61.1 33.8 59.6 61.1 32.8 60.8 63.0 90.7 91.1 91.4 90.1 91.6 91.1 90. 51.8 92.8 90.1 51.0 93.6 91.5 44M 56M 44M 44M 39M 63M 75M 63M 63M 58M 69M 81M 69M 71M 71M 64M 64M 107M 119M 107M 110M 110M 102M 102M 6.6M 6.6M 57M 39M 27.2M 45.9M 71.8M 64M 27.8M 60.5M 64M 90M 90M 64M 308M 308M 102M 115G 66G 67G 64G 49G 134G 86G 86G 84G 69G 141G 93G 93G 181G 187G 76G 87G 186G 137G 138G 226G 232G 120G 132G 163.6G 163.6G 3.6T 49G 110.4G 220.8G 344.5G 76G 159.1G 248.1G 76G 742G 742G 76G 2.6T 2.6T 132G Table 1. Instance segmentation on LIVECell, EVICAN2 (Easy, Medium, Difficult), and ISBI2014. IAUNet outperforms strong querybased Mask2Former and MaskDINO baselines for both AP and AP50 when training with fewer parameters. For fair comparison, we only consider single-scale inference and models trained until full convergence. IAUNet remains efficient across different backbones. It inwith 52,959 annotated cell and nucleus instances. cludes training and validation sets with 4,640 partially annotated images and test set of 98 fully annotated images. The test set is categorized by difficulty based on image quality: easy, medium, and difficult. ISBI2014 [33] is dataset from the Overlapping Cervical Cytology Image Segmentation Challenge. It includes 16 real extended depth-of-focus (EDF) cervical cytology images and 945 synthetic images. The dataset provides highquality pixel-level annotations for nuclei and cytoplasm, with resolution of 512 512. We follow the challenge setting [33], using 45 synthetic images for training, 90 for validation, and 810 for testing. One of our key contributions in this paper is novel cell instance segmentation dataset named Revvity-25. It includes 110 high-resolution 1080 1080 brightfield images, each containing, on average, 27 manually labeled and expert-validated cancer cells, totaling 2937 annotated cells. To our knowledge, this is the first dataset with accurate and detailed annotations for cell borders and overlaps, with each cell annotated using an average of 60 polygon points, reaching up to 400 points for more complex structures. Revvitynum queries backbones R50 R50 R50 R50 R50 R101 R101 R101 R101 Models Models with Convolution-Based Backbones 100 Mask R-CNN [22] 100 PointRend [26] Mask2Former [9] 100 100 MaskDINO [28] IAUNet (ours) 100 100 Mask R-CNN [22] 100 PointRend [26] 100 Mask2Former [9] MaskDINO [28] 100 IAUNet (ours) 100 Models with Transformer-Based Backbones 100 Mask R-CNN [22] 100 PointRend [26] 100 Mask2Former [9] 100 MaskDINO [28] MaskDINO [28] 300 IAUNet (ours) 100 IAUNet (ours) 300 100 Mask R-CNN [22] 100 PointRend [26] 100 Mask2Former [9] 100 MaskDINO [28] 300 MaskDINO [28] IAUNet (ours) 100 IAUNet (ours) 300 Swin-S Swin-S Swin-S Swin-S Swin-S Swin-S Swin-S Swin-B Swin-B Swin-B Swin-B Swin-B Swin-B Swin-B Revvity-25 AP AP50 AP75 APS APM APL #params. FLOPs 39.7 42.2 46.4 45.6 49.7 40.7 42.9 47.2 47.3 51.5 24.7 43.6 51.2 50.3 49.4 53.0 53.3 27.1 45.2 52.0 50.5 50.4 53.5 53.7 77.2 79.4 79.8 80.4 82.1 77.5 79.3 80.1 81.0 84.7 63.4 80.0 83.3 83.2 83.6 85.7 86.0 64.9 80.1 83.6 83.5 84.3 86.1 86. 37.4 40.9 49.9 48.2 54.8 39.9 42.5 51.8 50.4 56.1 12.5 43.0 56.4 53.9 53.3 57.0 59.6 17.2 47.9 58.4 54.9 54.8 59.4 59.4 0.6 0.4 0.7 1.8 0.6 0.4 0.0 1.7 0.9 1.7 0.0 0.5 2.7 4.7 2.9 1.3 1.6 0.1 0.1 1.1 2.0 0.8 0.8 1.0 19.0 21.7 25.7 22.3 27.3 20.1 18.4 25.7 23.0 29.2 7.3 21.5 27.7 27.6 25.8 29.7 29.4 9.7 23.0 27.8 27.1 26.3 30.5 30. 44.6 47.3 52.8 51.8 56.0 45.8 48.9 53.3 53.5 57.8 28.9 48.9 58.0 56.1 55.3 59.1 59.8 31.2 50.9 59.0 56.4 56.6 59.7 60.3 44M 56M 44M 44M 39M 63M 75M 63M 63M 58M 69M 81M 69M 71M 71M 64M 64M 107M 119M 107M 110M 110M 102M 102M 115G 66G 67G 64G 49G 134G 86G 86G 84G 69G 141G 93G 93G 181G 187G 76G 87G 186G 137G 138G 226G 232G 120G 132G Table 2. Instance segmentation on our Revvity-25 dataset. IAUNet outperforms strong query-based Mask2Former and MaskDINO baselines as well as other state-of-the-art models when training with fewer parameters. For fair comparison, we only consider singlescale inference and models trained until full convergence. IAUNet also efficiently scales with more queries while remaining efficient. 25 dataset provides unique resource that opens new possibilities for testing and benchmarking models for modal and amodal semantic and instance segmentation. 4.1. Implementation Details All experiments were conducted on single Tesla V100 GPU with 32GB memory. We adopt the training scheme published in earlier works [9]. We use the CosineAnnealingLR scheduler [31] with minimum learning rate of 1e-6, and the AdamW optimizer [32] with an initial learning rate of 1e-4 and weight decay of 0.05. During training, we employ longest-side resizing to scale all images to 512 512 pixels, preserving the original aspect ratio. For augmentation, we apply scale jittering [16] within scale range of 0.8 to 1.5, followed by fixed-size cropping to 512 512 and random flipping. All models were trained to full convergence with batch size of 8. Unless specified, we apply the same resizing process during inference, using consistent mask prediction threshold of 0.5 across all models. 4.2. Main Results In this section, we outline the dataset setup for training and present the results. For the LIVECell dataset, we preprocess images by randomly cropping them to maximum of 100 instances, ensuring consistency in prediction counts across datasets. We use the original train, validation, and test splits for all models. For the ISBI2014 dataset, we follow the original train, validation, and test splits. All models, except CellPose [48], are trained to segment both cell and nuclei classes. Since CellPose does not support multiclass segmentation by default, we train separate models for each class and average the performance. The Revvity25 dataset is divided equally into train and test sets, each containing 55 images. For EVICAN2, we report results on the easy, medium, and difficult test sets. maximum of 100 queries is set across all datasets. For example, in Tab. 1, IAUNet is compared with state-of-the-art models across diverse datasets. In models with convolutionbased backbones, IAUNet with ResNet-50 achieves an AP of 45.3 and AP50 of 75.3 on LiveCell. It outperforms Mask R-CNN, PointRend, Mask2Former, and MaskDINO while using fewer parameters (39M) and lower FLOPs (49G). With ResNet-101 backbone, IAUNet records an AP of IAUNet also scales better com45.4 and AP50 of 75.5. pared to MaskDINO when using transformer-based backbones. While IAUNet performs best on LIVECell but has room for improvement on ISBI2014, where the low object count leads to some queries predicting duplicates. Among specialized cell segmentation methods, IAUNet outperforms CellPose, CellPose + SM, and CellDETR. CellDETR, scaled to 100 objects with softmax head on high-resolution images, has high computational cost and parameter count, making it unsuitable for some datasets. CellPose struggles to generalize when object sizes differ signifiPixel Decoder + full skip + 1 1 skip concat + 1 1 skip add + light mask head AP 44.7 44.2 44.3 43.8 AP50 AP75 48.9 73.9 48.3 73.8 48.2 73.3 47.4 73.1 FLOPs 146G 135G 132G 42G Table 3. Pixel Decoder Variants (Skip Connections). We retain skip connection concatenation as in Eq. (1) and introduce lightweight mask head. cantly between train and test sets, as seen in EVICAN2, due to its reliance on object diameter for post-processing. In Fig. 3, we visualize the predictions and compute IAUNet consistently outperan image-wise AP score. IAUNet visibly offorms other state-of-the-art models. fers more detailed segmentation, capturing longer pixel relationships and effectively handling overlapping regions in some cases. In Tab. 2, we demonstrate IAUNets strengths on the Revvity-25 dataset, where it achieves the highest scores across multiple backbones, with an AP of 49.7 using ResNet-50 and 53.7 with Swin-B. 4.3. Ablation Studies In this section, we present an ablation study to evaluate the impact of each component in our model architecture. We focus on analyzing the contributions of the Pixel decoder and the Transformer decoder to overall model performance. All ablation studies were conducted on the LIVECell dataset. Skip Connections. IAUNet builds on the U-Net architecture. Tab. 3 presents the impact of different skip connection configurations. The model performs best with full skip connections over main features X, where channels are not reduced. To balance computational efficiency, skip channels are reduced to 256 via 1 1 convolutions before fusing features using concatenation or addition. Concatenation produces optimal performance and stability, while addition creates an FPN-like [29] structure in the decoder with further performance drop. Finally, adding light mask head to produce high-resolution features further reduces the FLOP count to 42G without significant performance drop. Pixel Decoder. In Tab. 4, we study each component of the Pixel decoder separately. To further refine features in the Pixel decoder, decoupling mask features with dedicated mask branch helps. To improve scalability, we reduce the feed-forward dimension to 1024 and add Squeeze-andExcitation [23] block to enhance feature representation. We observe that the model benefits from additional spatial information for multiple grouped objects of irregular shapes. Using CoordConv [30] at each level enriches the main features before further processing, helping the model better capture object locations and improve translation awareness. This modification improves segmentation performance, increasing AP to 44.7. Transformer Decoder. We evaluate the impact of scaling the Transformer decoder in Tab. 4. First, we introduce three Decoder IAUNet (R50) + mask branch Xm + FFN (2048 1024) + SE block [23] + CoordConv [30] + (1 3) (cycle.) + (1 3) (seq.) + deep supervision AP 43.8 44.0 44.1 44.2 44.7 44.3 45.1 45. AP50 AP75 47.4 73.1 47.9 73.2 48.0 73.2 48.1 73.3 48.7 74.1 48.1 74.0 49.4 74.4 49.4 75.3 #params. 34M 34M 32M 32M 32M 39M 39M 39M FLOPs 42G 42G 42G 42G 42G 49G 49G 49G Table 4. Decoder. We investigate the benefit of adding different decoder components. Adding CoordConv [30] improves object localization. Scaling the Transformer decoder with deep supervision shows best performance. num queries 100 300 500 1000 AP 45.3 45.9 46.1 45. AP50 AP75 49.4 75.3 50.4 76.5 50.8 76.8 50.0 76.3 FLOPs 49G 61G 73G 104G Table 5. Num. queries. Scaling the number of object queries benefits the model. Transformer decoder blocks per decoder layer, resulting in total of 3L Transformer blocks. We explore two main strategies for refining object queries. The first approach, inspired by [9], follows Round-Robin cycle update, where queries are refined in one Transformer block from each decoder layer at time and passed to the next, forming cycle that returns to low-resolution features. In contrast, we propose sequential (seq.) update strategy, where object queries are refined within all decoder blocks per decoder layer first, increasing AP to 45.1. Building on this, we apply deep supervision by computing the loss after each Transformer decoder layer using the updated queries and high-resolution Pixel decoder features Xm. Additionally, in Tab. 5, we evaluate the scalability of the number of queries, showing that the model achieves peak performance as the number of queries increases. 5. Conclusions We introduce IAUNet, novel query-based U-Net architecture with lightweight convolutional Pixel decoder and Transformer decoder that supervises object-specific queries for instance segmentation in biomedical imaging. Our model outperforms leading methods, particularly for medium and large objects, and sets strong baseline for cell segmentation tasks, as demonstrated on our Revvity25 Dataset. While IAUNet performs well in most tasks, it struggles with small object segmentation and could benefit from optimization for high-instance images. Future work will focus on improving performance in these areas. 6. Acknowledgments This work was supported by Revvity and funded by the TEM-TA101 grant Artificial Intelligence for Smart Automation. Computational resources were provided by the High-Performance Computing Cluster at the University of Tartu. We thank the Biomedical Computer Vision Lab for their invaluable support. We express gratitude to the Armed Forces of Ukraine and the bravery of the Ukrainian people for enabling secure working environment, without which this work would not have been possible."
        },
        {
            "title": "References",
            "content": "[1] Mohammed Ali, Kaspar Hollo, Tonis Laasfeld, Jane Torp, Maris-Johanna Tahk, Ago Rinken, Kaupo Palo, Leopold Parts, and Dmytro Fishman. ArtSegArtifact segmentation and removal in brightfield cell microscopy images without manual pixel-level annotations. Scientific Reports, 12(1):11404, 2022. 1 [2] Min Bai and Raquel Urtasun. Deep watershed transform for instance segmentation, 2017. 2 [3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High quality object detection and instance segmentation, 2019. 2 [4] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmentation, 2021. 3 [5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers, 2020. 1, 2, 3, 4 [6] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L. Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation, 2021. 3 [7] Qi Chen, Wei Huang, Xiaoyu Liu, Jiacheng Li, and Zhiwei Xiong. Pctrans: Position-guided transformer with query contrast for biological instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 39033912, 2023. 1, 2, [8] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation, 2021. 4 [9] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation, 2022. 1, 2, 3, 4, 5, 6, 7, 8 [10] Tianheng Cheng, Xinggang Wang, Shaoyu Chen, Wenqiang Zhang, Qian Zhang, Chang Huang, Zhaoxiang Zhang, and Wenyu Liu. Sparse instance activation for real-time instance segmentation, 2022. 2 [11] Kevin J. Cutler, Carsen Stringer, Teresa W. Lo, Luca Rappez, Nicholas Stroustrup, S. Brook Peterson, Paul A. Wiggins, high-precision and Joseph D. Mougous. Omnipose: morphology-independent solution for bacterial cell segmentation. Nature Methods, 19(11):14381448, 2022. 3 [12] Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Solq: Segmenting objects by learning queries, 2021. 4 Ahmed, Johan Trygg, and Rickard Sjogren. Livecella large-scale dataset for label-free live cell segmentation. Nature Methods, 18(9):10381045, 2021. [14] Yuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen Instances as Fang, Ying Shan, Bin Feng, and Wenyu Liu. queries, 2021. 1, 4 [15] Patrick Follmann and Rebecca Konig. Oriented boxes for accurate instance segmentation, 2020. 2 [16] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, TsungYi Lin, Ekin D. Cubuk, Quoc V. Le, and Barret Zoph. Simple copy-paste is strong data augmentation method for instance segmentation, 2021. 7 [17] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation, 2014. [18] Simon Graham, Quoc Dang Vu, Shan Ahmed Raza, Ayesha Azam, Yee Wah Tsang, Jin Tae Kwak, and Nasir Rajpoot. Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images, 2019. 2 [19] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger Roth, and Daguang Xu. Unetr: Transformers for 3d medical image segmentation, 2021. 3 [20] Junjie He, Pengyu Li, Yifeng Geng, and Xuansong Xie. Fastinst: simple query-based model for real-time instance segmentation, 2023. 1, 2, 4 [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, 2015. 1 [22] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn, 2018. 1, 2, 6, [23] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-and-excitation networks, 2019. 4, 8 [24] Armen Kherlopian, Ting Song, Qi Duan, Mathew Neimark, Ming Po, John Gohagan, and Andrew Laine. review of imaging techniques for systems biology. BMC systems biology, 2:118, 2008. 1 [25] Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, BogInstancecut: from dan Savchynskyy, and Carsten Rother. edges to instances with multicut, 2016. 2 [26] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering, 2020. 2, 6, [27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023. 6 [28] Feng Li, Hao Zhang, Huaizhe xu, Shilong Liu, Lei Zhang, Lionel M. Ni, and Heung-Yeung Shum. Mask dino: Towards unified transformer-based framework for object detection and segmentation, 2022. 1, 2, 3, 6, 7 [29] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection, 2017. 8 [13] Christoffer Edlund, Timothy R. Jackson, Nabeel Khalid, Nicola Bevan, Timothy Dale, Andreas Dengel, Sheraz [30] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski. An [46] Prem Shrestha, Nicholas Kuang, and Ji Yu. Efficient end-toend learning for cell segmentation with machine generated weak annotations. Communications Biology, 6(1):232, 2023. [47] Russell Stewart and Mykhaylo Andriluka. End-to-end people detection in crowded scenes, 2015. 5 [48] Carsen Stringer, Tim Wang, Michalis Michaelos, and Marius Pachitariu. Cellpose: generalist algorithm for cellular segmentation. Nature Methods, 18(1):100106, 2021. 2, 6, 7 [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. 1 [50] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. to learn using proYolov9: Learning what you want grammable gradient information, 2024. 1, [51] Gufeng Wang and Ning Fang. Detecting and tracking nonfluorescent nanoparticle probes in live cells. Methods Enzymol, 504:83108, 2012. 1 [52] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: nested u-net architecture for medical image segmentation, 2018. 1, 3 [53] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection, 2021. 4 [54] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection, 2021. 2 intriguing failing of convolutional neural networks and the coordconv solution, 2018. 8 [31] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017. [32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. 7 [33] Zhi Lu, Gustavo Carneiro, and Andrew P. Bradley. An improved joint optimization of multiple level set functions for the segmentation of overlapping cervical cells. IEEE Transactions on Image Processing, 24(4):12611272, 2015. 6 [34] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation, 2016. 5 [35] Erick Moen, Dylan Bannon, Takamasa Kudo, William Graf, Markus Covert, and David Van Valen. Deep learning for cellular image analysis. Nature Methods, 16(12):12331246, 2019. 1 [36] Larry E. Morrison, Mark R. Lefever, Lauren J. Behman, Torsten Leibold, Esteban A. Roberts, Uwe B. Horchner, and Daniel R. Bauer. Brightfield multiplex immunohistochemistry with multispectral imaging. Laboratory Investigation, 100(8):11241136, 2020. 1 [37] Marius Pachitariu and Carsen Stringer. Cellpose 2.0: how to train your own model. Nature Methods, 19(12):16341641, 2022. 2, [38] Tim Prangemeier, Christoph Reich, and Heinz Koeppl. Attention-based transformers for instance segmentation of In 2020 IEEE International Concells in microstructures. ference on Bioinformatics and Biomedicine (BIBM). IEEE, 2020. 2, 6 [39] Shan Ahmed Raza, Linda Cheung, Muhammad Shaban, Simon Graham, David Epstein, Stella Pelengaris, Michael Khan, and Nasir M. Rajpoot. Micro-net: unified model for segmentation of various objects in microscopy images. Medical Image Analysis, 52:160173, 2019. 2 [40] Dillon Reis, Jordan Kupec, Jacqueline Hong, and Ahmad Daoudi. Real-time flying object detection with yolov8, 2024. 1, 6 [41] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks, 2016. 2 [42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015. 1, 3 [43] Uwe Schmidt, Martin Weigert, Coleman Broaddus, and Gene Myers. Cell Detection with Star-Convex Polygons, page 265273. Springer International Publishing, 2018. [44] Mischa Schwendy, Ronald Unger, and Sapun Parekh. EVICANa balanced dataset for algorithm development in cell and nucleus segmentation. Bioinformatics, 36(12): 38633870, 2020. 5 [45] Jyrki Selinummi, Pekka Ruusuvuori, Irina Podolsky, Adrian Ozinsky, Elizabeth Gold, Olli Yli-Harja, Alan Aderem, and Ilya Shmulevich. Bright field microscopy as an alternative to whole cell fluorescence in automated analysis of macrophage images. PLoS One, 4(10):e7497, 2009."
        }
    ],
    "affiliations": [
        "Institute of Computer Science, University of Tartu",
        "STACC U, Tartu, Estonia",
        "Ukrainian Catholic University"
    ]
}