{
    "paper_title": "Can We Predict Before Executing Machine Learning Agents?",
    "authors": [
        "Jingsheng Zheng",
        "Jintian Zhang",
        "Yujie Luo",
        "Yuren Mao",
        "Yunjun Gao",
        "Lun Du",
        "Huajun Chen",
        "Ningyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute."
        },
        {
            "title": "Start",
            "content": "Can We Predict Before Executing Machine Learning Agents? Jingsheng Zheng, Jintian Zhang, Yujie Luo, Yuren Mao, Yunjun Gao, Lun Du, Huajun Chen, Ningyu Zhang* Zhejiang University Ant Group Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph zhengjohnson0@gmail.com, zhangningyu@zju.edu.cn 6 2 0 J 9 ] . [ 1 0 3 9 5 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by Generate-Execute-Feedback paradigm. Previous approaches suffer from severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs Predict-then-Verify loop, achieving 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/ predict-before-execute."
        },
        {
            "title": "Introduction",
            "content": "Autonomous machine learning agents have emerged as powerful tools for solving complex challenges in scientific discovery (Zhang et al., 2025d; Chen et al., 2025b). Mainstream frameworks (Jiang et al., 2025; Ou et al., 2025) typically rely on an iterative Generate-Execute-Feedback loop where the system refines code based on runtime output (Yao et al., 2023). However, this paradigm suffers from severe Execution Bottleneck as physical execution is computationally expensive and slow, often consuming up to 9 hours per run in benchmarks like MLE-Bench (Chan et al., 2025). Increasingly, recent research has identified this latency issue and sought to mitigate the *Corresponding Author. Figure 1: From Execution to Inference. Traditional ML agents improve through costly execution and external feedback, incurring substantial latency. Our work investigates whether superior data-grounded solutions can be identified before execution by leveraging Implicit Execution Priors. computational overhead through heuristic pruning strategies (Trirat et al., 2025; Kulibaba et al., 2025). To fundamentally bypass these physical constraints, the concept of World Models (Ding et al., 2025) offers transformative alternative (Figure 1). Originating from reinforcement learning, world models enable agents to simulate environmental dynamics and evaluate actions via internal predictions rather than external trials (Ha and Schmidhuber, 2018; Hafner et al., 2024). Recent advancements have extended this capability to the code domain by predicting execution outputs directly (Li et al., 2025c; team et al., 2025). Motivated by this, we explore whether agents can internalize execution priors, substituting costly runtime checks with instantaneous predictive reasoning. The potential to replace 9 hours of physical latency with 1 second of neural speed brings us to fundamental question: Can we compress hours of physical execution into seconds of logical inference? To answer this question, we formalize the task of Data-centric Solution Preference, where the model must predict the relative performance of two algorithmic solutions given data analysis report, through reasoning without physical execution. To rigorously evaluate this, we construct large-scale corpus comprising 18,438 pairwise comparisons. Our main experiments yield strong evidence: LLMs exhibit significant predictive capabilities, with DeepSeek-V3.2-Thinking achieving 61.5% accuracy, outperforming both random guessing (50.0%) and complexity-based heuristics (50.8%). Further analysis reveals that reasoning-optimized architectures transcend complexity heuristics through genuine data reasoning, yielding well-calibrated confidence that ensures the reliability of implicit evaluation. Finally, we integrate this predictive mechanism into FOREAGENT, an agent that employs Predict-then-Verify loop to decouple exploration from execution, expanding the search space by 3.2 and achieving 6 acceleration while delivering +6% performance gain over standard baselines. In summary, our contributions are three-fold: We define the novel task of Data-centric Solution Preference and construct comprehensive corpus of 18,438 pairs, answering the titular question that LLMs Exhibit Significant Predictive Capabilities. We operationalize this framework in FOREAGENT, an agent that employs Predict-thenVerify loop to decouple exploration from execution, enabling it to expand the search space by 3.2 and achieve 6 acceleration and +6% performance gain over the baseline. We contribute large-scale Open-Source Dataset of verified execution trajectories, serving as foundational corpus for training scalable Reward Models to accelerate reinforcement learning rollouts and optimization across diverse agent frameworks."
        },
        {
            "title": "2.1 The Paradigm of Autonomous ML Agents",
            "content": "An autonomous Machine Learning (ML) task aims to generate an optimal solution code from the code space that maximizes metric on dataset D, given natural language instruction (see Appendix Figure 11): = arg max (I, C, D) (1) Current agents typically follow GenerateExecute-Feedback paradigm (Zhu et al., 2025b). Domain Paradigms # Tsk # Sols # Pairs CV NLP Data Science Classification, Segmentation, Generation, Restoration Classification, Matching, QA, Sequence Labeling, Ranking Regression, TimeSeries, Audio, Tabular, Grading 9 8 9 289 5,952 6,682 303 5,804 Total 26 Distinct Tasks across 3 Domains 895 18,438 Table 1: Statistics of the Preference Corpus. We aggregate 26 tasks into three primary domains, ensuring balanced distribution of 6,000 pairs each. (See Appendix B.1 for granular breakdown). For instance, AIDE (Jiang et al., 2025) organizes solution exploration as tree search process involving sequential drafting, debugging, and iterative improvement via execution feedback. Building upon this, AutoMind (Ou et al., 2025) integrates curated expert knowledge base with self-adaptive coding strategy to tackle more intricate problems (see Appendix for details)."
        },
        {
            "title": "2.2 The Execution Bottleneck",
            "content": "The primary constraint in current agents is the reliance on physical execution for feedback. Formally, the update of solution Ct+1 depends on the result Rt from executing on dataset D: Ct+1 Agent(I, Ct, Execute(Ct, D) ) (cid:125) (cid:124) (cid:123)(cid:122) Rt (2) Unlike symbolic tasks with instantaneous verification, training deep learning models involves heavy computation, frequently leading to timeout failures (Chan et al., 2025). This efficiency gap necessitates compressing hours of physical execution into seconds of logical inference, mirroring how human experts utilize mental simulation to discard sub-optimal algorithms prior to implementation."
        },
        {
            "title": "2.3 Implicit World Modeling in Data Domains",
            "content": "We investigate whether LLMs can function as an Implicit World Model (Ha and Schmidhuber, 2018; Hafner et al., 2024). While recent works explore this direction across diverse symbolic and interactive domains (Li et al., 2025e; team et al., 2025; Just et al., 2024), our Data-centric Solution Preference task is distinct: unlike tracking explicit states, the model must anticipate the invisible coupling Figure 2: Overview of the Framework. (a) Task Definition: The Data-centric Solution Preference task predicts solution superiority and confidence via latent reasoning. (b-c) Data Curation: We collect and filter real-world agent trajectories to construct the Preference Corpus. (d) Augmentation: Inputs are augmented with Verified Data Reports via Profile-Verify-Verbalize pipeline. (e) FOREAGENT Application: The model serves as filter within the Predict-then-Verify loop, predicting preference before physical execution to prune candidates. of algorithmic logic and stochastic data. Thus, we formulate the problem as Pairwise Preference task (Shen et al., 2024), determining the superior solution purely via reasoning to identify promising candidates prior to execution."
        },
        {
            "title": "3 Preference Corpus Curation",
            "content": "This section details the curation of our preference corpus. We begin by formalizing the task to clarify the data requirements, followed by the collection and augmentation processes."
        },
        {
            "title": "3.1 Task Definition",
            "content": "We model the data-centric task as pairwise selection task: given task description, data report, and two candidate solutions, the objective is to identify the superior solution and estimate confidence score (Figure 2(a)). Formally, the input is: = (I, Drep, {C0, C1}, P) (3) where I, Drep, {C0, C1}, and denote the task, data report, code pair, and system prompt, respectively. The output is defined as: = {(cot, ˆy, c) cot, ˆy {0, 1}, [0, 1.0]} (4) consisting of the reasoning cot, predicted winner ˆy, and confidence c, which serves as the gating threshold in Section 6."
        },
        {
            "title": "3.2 Source and Scope",
            "content": "To instantiate the task inputs defined above, we construct large-scale corpus derived from the real-world execution trajectories of two ML agents, AIDE (Jiang et al., 2025) and AutoMind (Ou et al., 2025), operating on MLE-bench (Chan et al., 2025) platform (Figure 2(b)). Powered by DeepSeek-V3.1 (DeepSeek-AI, 2025b) and o3mini (OpenAI, 2025a), these agents generate 1,329 valid solutions across 26 diverse tasks  (Table 1)  . Unlike synthetic snippets, these candidates represent complete ML workflows ranging from preprocessing to training. Therefore, identifying the superior solution requires evaluating how well an algorithm fits the specific data characteristics, rather than merely checking for code syntax."
        },
        {
            "title": "3.3 Dataset Curation and Instantiation",
            "content": "To ensure rigorous evaluation, we implement an Expert-in-the-Loop pipeline to prune raw trajectories into 895 high-quality instances. This process involves deduplication, automated taxonomy tagging, and expert sampling to cap dominant methods and ensure algorithmic diversity. Subsequently, we instantiate the dataset by exhaustively generating pairwise combinations from this curated corpus. We apply strict filtering to discard ambiguous pairs and balance the ground-truth winners position to mitigate position bias (Shi et al., 2024). This yields final dataset of 18,438 comparisons (Figure 2(c)), utilizing micro-averaged accuracy as the primary metric. (Acc. %) Task Dims. Domain Difficulty Task Paradigm Sols. Sols. Attrs. CV NLP Data Sci. Easy Med. Hard Class. Regres. Others Avg Acc DeepSeek-V3.2 (Thinking mode) Algo Era Traditional Modern 60.20.9 70.60.6 59.30.5 59.81.1 69.10.2 61.10.7 61.50.5 61.20.7 76.20.5 64.50.6 59.10.5 65.00.1 56.30.5 60.70.3 61.70.3 55.10.6 57.80.2 62.50.4 62.30.5 60.40.2 Granularity Cross-Algo 56.60.3 68.90.7 58.40.9 57.61.0 68.20.4 57.71.5 59.80.7 60.60.7 74.10.9 62.80.6 Self-Comp. 60.10.6 65.10.2 56.30.8 61.60.3 60.90.4 56.51.0 58.20.1 62.90.4 62.10.5 60.70. Complexity Low Medium High 57.60.4 69.80.5 57.20.3 58.90.6 66.20.2 58.90.7 58.60.2 61.60.2 73.30.9 62.10.3 59.60.3 65.10.1 58.10.2 60.50.2 63.30.1 56.60.2 58.10.2 63.40.3 64.60.6 61.30.1 61.22.0 80.10.7 50.01.1 76.82.8 58.41.6 52.70.9 60.32.5 58.41.4 61.31.7 59.61.4 Tasks Avg Acc 59.30.5 66.90.2 57.40.2 60.40.5 63.90.2 57.00.3 58.90.3 62.10.1 66.80.5 61.50.2 GPT-5. Algo Era Traditional Modern 60.10.4 64.70.2 59.50.2 56.60.4 65.50.2 63.40.3 59.30.6 62.20.2 67.20.3 62.00.2 54.50.2 62.20.8 56.30.1 55.10.4 59.90.4 57.90.0 56.40.5 58.20.5 59.80.6 57.70.3 Granularity Cross-Algo 58.01.1 61.20.3 56.70.1 55.30.7 61.30.2 59.80.1 55.70.8 61.70.3 62.00.5 59.00.3 Self-Comp. 54.80.0 64.21.0 57.70.1 55.30.4 61.60.4 59.20.2 58.00.7 57.60.5 62.20.7 58.70.3 Complexity Low Medium High 56.40.2 66.60.5 56.20.2 56.80.5 64.20.2 57.60.2 57.90.7 59.30.2 68.70.3 60.10.3 55.80.2 60.60.6 59.30.2 54.60.4 61.20.3 61.10.2 56.50.5 60.00.5 60.50.6 58.60.3 50.80.3 79.00.8 57.21.5 44.22.7 56.20.4 59.71.6 56.00.7 54.50.7 56.21.1 55.30.3 Tasks Avg Acc 55.40.2 63.00.6 57.40.1 55.50.4 61.60.3 59.70.1 57.20.5 59.20.3 62.20.5 58.80.3 Table 2: Main Results: Predictive Capability and Boundary Analysis. This table presents the Pairwise Preference Accuracy (%) of the evaluated LLMs averaged over three runs, stratified by Task Dimensions and Solution Attributes. Results are reported as Mean Stdev. DeepSeek-V3.2 (Thinking Mode) and GPT-5.1 achieve global averages of 61.5% and 58.8% respectively, significantly outperforming the random baseline of 50% and the complexity-based heuristic baseline of 50.8%. 3. Input Augmentation: The Verified Data Analysis Report"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "To address LLMs numerical limitations (Davies et al., 2025; Li et al., 2025b) and context constraints preventing direct data ingestion, we augment inputs with Verified Data Analysis Report that transforms raw statistics into semantic narratives (Rytting and Wingate, 2021; Zhang et al., 2025a). To guarantee factual grounding, we implement strict Code-Execution-Verbalization protocol (Figure 2(d)). GPT-5.1 (OpenAI, 2025b) first performs Code Profiling on raw data files (strictly masking labels and outcomes) to generate analysis scripts; these scripts undergo Execution & Verification to yield artifact-free logs; finally, GPT-5.1 performs Verbalization to translate these logs into the Verbal Data Report (Data Observation Modeling Implication), ensuring reliable semantic grounding for the task (see case in Appendix Figure 10). Models and Inference Configuration. We evaluate two state-of-the-art models: DeepSeek-V3.2Thinking (DeepSeek-AI, 2025b) and GPT-5.1 (gpt-5.1-2025-11-13) (OpenAI, 2025b) with reasoning instructions (Wei et al., 2023; Kojima et al., 2023), adhering to the task in Section 3.1. Following provider guidelines (DeepSeek-AI, 2024), we set the temperature τ = 1.0 for both models as the recommended default for data analysis. Metrics and Baselines. The primary metric is Micro-Averaged Accuracy across 18,438 pairwise comparisons. We benchmark against two baselines: (1) Random Guess (50.0%); (2) Complexity Heuristic (50.8%): rule-based baseline that assumes complex is better. To operationalize this, we employed an LLM to score each solution (1-10) across three dimensions: Code Engineering, Model Architecture, and Data Pipeline (see Appendix Figure 15). This baseline predicts the (a) Impact of Data Figure 3: Comprehensive Analysis of World Model Mechanisms and Capabilities. Representation: Predictive success stems from semantic data understanding rather than complexity heuristics. (b) Domain Sensitivity: The superiority of verbal reports remains consistent across domains. (c) Scaling Laws: Accuracy decouples from pure parameter scaling. (d) Inference Dynamics: Active reasoning outperforms direct answering with robust stability across temperatures. (e) Calibration Analysis: Self-reported confidence strictly correlates with accuracy. (f) Complexity Discrimination: Accuracy scales with the complexity gap. winner based on the aggregate complexity score."
        },
        {
            "title": "Preference",
            "content": "The stratified pairwise accuracy results in Table 2 validate the feasibility of our approach. LLMs Exhibit Significant Predictive Capabilities. Both models significantly outperform the random baseline and the complexity heuristic with statistical significance, with DeepSeek-V3.2-Thinking achieving 61.5% and GPT-5.1 achieving 58.8%. This performance gap (> 10%) proves LLMs derive valid signals from static inputs through genuine reasoning rather than heuristics, despite the task remaining challenging frontier."
        },
        {
            "title": "5 Analysis & Insights",
            "content": "In this section, we deconstruct the mechanisms of the Implicit World Model through four pivotal research questions to answer: why can reasoning substitute for execution, and to what extent? While our representational analysis utilizes the full dataset, subsequent analysis (RQ2RQ3) employs focused subset capped at 15 solutions per task (2,292 pairs). For ranking evaluations, we sample 105 instances per task to align with the pairwise baseline complexity (C(15, 2))."
        },
        {
            "title": "Representation",
            "content": "To distinguish genuine causal reasoning from syntactic memorization, we conducted systematic study on input modalities (Figure 3(a)). We instantiated four progressively enriched levels: Code Only (task description + code), Raw Data (appending initial samples), Numerical Stats (execution logs from data analysis scripts), and Verbal Report (full semantic analysis), alongside Context Mismatch Discussion 5.1. The Gap Between Semantic and Numeric Spaces. We observe that raw data yields insignificant gains over code-only input. We attribute this to the fact that continuous numerical values are Weak-Semantic Symbols that lack generalizable topological structure in the embedding space (Davies et al., 2025). While language serves as Strong-Structural Symbol carrying the compressed, empirical representations of human reasoning (Rytting and Wingate, 2021), raw numbers appear to the model as unstructured high-entropy noise. Our verbalization strategy bridges this gap by projecting numeric data into the semantic manifold, injecting necessary inductive bias. Looking ahead, fundamental resolution requires integrating Symbolic Regression (Grayeli et al., 2024) to distill intricate logic directly from data. control (pairing code with irrelevant context)."
        },
        {
            "title": "Size",
            "content": "Corr. Accuracy@k (%) Finding 1: Predictive Success Stems from Semantic Data Understanding, Not Simple Complexity Heuristics. Our results refute potential concern that LLMs merely rely on complexity heuristic. Figure 3(a) demonstrates clear performance progression from the Heuristic Baseline (50.8%) and Code Only (56.7%) to Numerical Stats (59.0%), peaking with Verbal Reports (61.3%). The insignificant gain of the Context Mismatch (56.8%) over Code Only confirms that predictive success hinges on strict Semantic Alignment. The superiority of verbal narratives over raw statistics reveals that models operate primarily as rhetorical reasoners, triggering an inference jump that is consistent across domains (Figure 3(b))."
        },
        {
            "title": "Algorithmic Bias",
            "content": "In this section, we analyze the necessity of reasoning, domain sensitivity, generalization to ranking, and the reliability of its confidence. Finding 2: Reasoning Unlocks Capabilities, Yet Distinct Cognitive Boundaries Persist Across Domains. Figure 3(d) identifies reasoning as the primary engine, with the Thinking Mode (CoT) (DeepSeek-AI, 2025a; OpenAI, 2024) (61.3%) outperforming Direct Answering (55.9%). This performance remains robust across temperatures (T [0, 1.5]), implying an invariant logical core despite diversified traces. However, this capability is constrained by the problem landscape; Table 2 reveals sharp performance stratifications across the Task-Solution matrix. On the Task Dimension, models demonstrate preference for NLP (66.9%) and Easy (63.9%) paradigms. Simultaneously, the Solution Dimension reveals Complexity Tax (59.6% on complex code) and granularity bottleneck, where the model is more effective at distinguishing broad Cross-Algo contrasts (comparing solutions with different algorithms, 62.8%). Thus, while reasoning is indispensable, it faces limits when navigating intricate code logic or subtle intra-class nuances. (N ) Spr. ρ k=1 k= k=3 k=4 2 3 4 5 0.240.01 61.30.6 0.220.00 43.40.4 25.50.4 0.250.00 35.01.0 16.40.7 10.20.2 0.220.00 31.10.9 11.20.3 4.90.1 3.00.2 Table 3: Ranking Performance. Listwise ranking metrics across varying list sizes . Spr.: Spearman Correlation (ρ). A@k: Accuracy of the top-k ranking positions (%). denotes undefined metrics where . Extending the scope to global Listwise Ranking further magnifies this limitation, as Table 3 reveals scalability defect where Accuracy@1 drops from the pairwise baseline (61.3% 31.1%) while Spearman Correlation hovers at notably low level (ρ 0.23), indicating that the model lacks global discrimination capability, failing to sustain consistency beyond binary interactions. Finding 3: The Implicit World Model Leverages Causal Reasoning Beyond Complexity Heuristics and Exhibits Robust Confidence Calibration. Tracing the Complexity Gap in Figure 3(f) shows accuracy scales with distinction; crucially, the models superiority over heuristics in low-gap scenarios proves it detects valid semantic signals rather than simple metrics. Furthermore, Figure 3(e) demonstrates excellent Calibration, where confidence correlates strictly with accuracy. This reliability underpins the Section 6 gating mechanism, ensuring agents act with certainty."
        },
        {
            "title": "Solution Preference",
            "content": "We evaluate the Qwen series across spectrum from 4B to 1T to determine if predictive capability acts as an emergent scaling property. Figure 3(c) details performance on five distinct checkpoints: 4B (Qwen3-4B-Instruct-2507), 30B (Qwen3-Coder-30B-a3b-Instruct), 235B (Qwen3235B-a22b-Instruct-2507), 480B (Qwen3-Coder480B-a35b-Instruct), and 1T (Qwen-Max). Discussion 5.3: The Illusion of Scaling on World-Blind Models. We attribute scaling failures to Information Scarcity: static training corpora consist of code paired with merely trivial inputs or abstract descriptions, lacking the large-scale data distributions required for true dynamic execution interplay (Liu et al., 2022). While our results confirm that models can exploit sparse dynamic traces hidden in existing data, they remain fundamentally World-Blind Learners (Floridi et al., 2025). To transcend this mere syntactic mimicry, future scaling must pivot from static ingestion to Interactive Simulation, grounding agents in genuine causal feedback loops (Bender and Koller, 2020). Finding 4: Predictive Accuracy Violates Standard Parameter Scaling Laws. Contrary to standard Parameter Scaling Laws, our results (Figure 3(c)) reveal rapid saturation phenomenon. Within the Qwen series, performance sees diminishing returns after the initial 30B threshold, creating statistical plateau that persists even at the 1T scale. This trajectory implies distinct capacity ceiling, suggesting that raw parameter scaling alone is insufficient for further gains in the Data-centric Solution Preference task. In contrast, the distinct superiority of DeepSeek-V3.2 (61.3%) and GPT5.1 (58.8%) demonstrates that predictive power drives less from raw scale than from reasoningcentric architectural paradigms, implying that future gains will rely on specialized inference incentives rather than simple parameter expansion. semantic safeguard that balances efficiency against the risk of metric-driven overfitting. Signal Source Cost Acc. (%) Random Guess Exec. (Mval) LLM - Hours Seconds 50.0 72.2 61.5 Table 4: Validation-Test Gap. Local metrics (Mval) are noisy proxies for test performance (Mtest), achieving only 72.2% accuracy due to distribution shifts."
        },
        {
            "title": "6 Agent Integration: FOREAGENT",
            "content": "Building on the predictive capabilities of the World Model, we propose FOREAGENT, hybrid autonomous ML agent designed to decouple hypothesis exploration from physical execution."
        },
        {
            "title": "6.1 Motivation",
            "content": "and Validation-Test Gap To validate the models reasoning depth, we conducted qualitative analysis on the Google Quest Challenge from main experiment, which is multilabel subjective question-answering task. Finding 5: The Model Outperforms Human Intuition by Rejecting Complexity Bias. In the case study of Figure 9, the model surpassed human judgment by correctly prioritizing simple LightGBM, whereas humans succumbed to the bigger is better bias by favoring complex Deep Neural Network. It successfully detects small-sample overfitting risks that humans missed, proving that data-grounded reasoning can effectively override superficial human biases. The Validation-Test Gap. We further examine the reliability of execution-based validation metrics (Mval), derived from internal data splits, as proxies for test performance (Mtest). As shown in Table 4, relying solely on Mval yields an accuracy of only 72.2%. This ceiling reveals substantial Validation-Test Gap stemming from distributional shifts and validation overfitting. Crucially, implicit reasoning partially mitigates this gap, offering We aim to break the Execution Bottleneck in Section 2.2, compressing hours of physical execution into seconds of logical inference, and the Validation-Test Gap identified in Section 5.4. Thus, we propose FOREAGENT, which utilizes the Implicit World Model as filter to prune the search space before execution for acceleration."
        },
        {
            "title": "6.2 Method: The Predict-then-Verify Loop",
            "content": "We adopt AIDE (Jiang et al., 2025) as our backbone, building directly upon the tree-search architecture described in Section 2.1. We propose FOREAGENT, which re-engineers the Improvement stage into conservative Predictthen-Verify loop (Figure 2(e)) to bridge the Implementation Gap (Zhu et al., 2025a). The workflow proceeds through three key phases: (1) HighVolume Generation, where = 10 candidates are proposed in parallel to expand search width without execution costs; (2) Confidence-Gated Pairwise Selection, which utilizes confidence gate (c = 0.7) to ensure high-certainty selection; and (3) Verification Execution, where the Top-k (k = 1) candidate is physically verified to anchor the solution trajectory in execution feedback. Discussion 6.1. The Indirect Ceiling of Static Prediction. We interpret 72.2% as an Indirect Epistemic Bound, constrained by the Validation-Test Gap and Limited Innovative Creativity. Theoretically, since static prediction cannot outperform dynamic verification (Rice, 1953), any gains beyond this saturation point represent the overfitting of distributional noise (Dwork et al., 2015). Moreover, this ceiling exposes deficit in Generative Innovation: current LLMs hit Homogeneity Barrier, producing functionally isomorphic solutions that lack context-specific specialization (Doshi and Hauser, 2024). Thus, lifting this bound relies on evolving base models to achieve the Genuine Innovation. Figure 4: Agent Performance Analysis. (a) Task-wise Beat Ratio: FOREAGENT achieves an average +6% improvement over the AIDE baseline. (b) Temporal Efficiency: The agent converges to peak performance using only 1/6 of the execution time, achieving an average 6 speedup. (c) Search Breadth: By offloading evaluation to the Implicit World Model, FOREAGENT explores 3.2 more nodes on average compared to the baseline, significantly expanding the search space within the same time budget. Task Name Domain Status Stanford Covid Vaccine Biology Physics Ventilator Pressure Geoscience Statoil Iceberg Aerial Cactus Ident.* Ecology Histo. Cancer Detect.* Medicine Seen Seen Seen Unseen Unseen Table 5: Agent Evaluation Benchmark. The selection covers diverse AI4Science domains to test the World Models capability to generalize from seen tasks to unseen scientific problems."
        },
        {
            "title": "6.3 Experimental Setup",
            "content": "Tasks and Baselines. We evaluate FOREAGENT on 5 AI4Science tasks from MLE-bench  (Table 5)  , including two Unseen tasks. We benchmark against AIDE under 12-hour limit; both use DeepSeek-V3.2 for coding, while Implicit World Modeling employs DeepSeek-V3.2-Thinking. Metric. To ensure reliability, we conduct three independent runs for each task and report the average Beat Ratio (Ou et al., 2025). This metric quantifies the percentage of human leaderboard contestants outperformed by the agent, representing expert-level competitiveness."
        },
        {
            "title": "6.4 Results",
            "content": "By substituting costly execution with rapid inference, FOREAGENT achieves an average 6 speedup (Figure 4(b)), enabling it to explore 3.2 more nodes within just 1/6 of the time budget (Figure 4(c)). This expanded search capability directly translates into performance, driving +6% improvement in Beat Ratio (Figure 4(a)) and demonstrating robust generalization on unseen tasks. Although we currently focus on inference, this paradigm naturally extends to training contexts like Reward Model, promising direction we reserve for future work."
        },
        {
            "title": "7 Related Work",
            "content": "LLM Agents in Machine Learning (ML). LLM agents are extensively deployed in ML for tasks ranging from pipeline automation (Jiang et al., 2025; Qiao et al., 2025; Gu et al., 2024b) to competitive problem-solving (Luo et al., 2025; Ou et al., 2025; Chan et al., 2025; Liu et al., 2025b). However, the computational cost of their generationexecution loops (Yao et al., 2023) remains bottleneck. To mitigate this, recent works utilize internal priors to prune redundant steps (Kulibaba et al., 2025; Trirat et al., 2025), transitioning from bruteforce search to reasoned planning. World Models for Skip-Execution. Adapting World Models (Ding et al., 2025; Li et al., 2025e) to code, recent research predicts execution outcomes to bypass physical runs (Hora, 2024; team et al., 2025; Li et al., 2025c). While prior works focus on logic consistency in reasoning benchmarks (Wei et al., 2025a; Gu et al., 2024a; Jain et al., 2024), our approach integrates this predictive capability with Data-Centric Solution Preference (Shen et al., 2024; Just et al., 2024). By anchoring evaluations in explicit dataset rationales rather than heuristics, we ensure reliability in stochastic data domains. Extended discussion in Appendix A."
        },
        {
            "title": "8 Conclusion",
            "content": "This work validates the feasibility of compressing physical execution into logical inference. Our analysis reveals LLMs function as calibrated, reasoningdriven critics via semantic verbalization to strictly gate actions and prune search spaces. By decoupling reasoning from runtime, we provide robust blueprint for bypassing the execution bottleneck in complex machine learning tasks."
        },
        {
            "title": "Limitations",
            "content": "Corpus Imbalance and Domain Coverage. Although our corpus encompasses 18,438 pairs across 26 tasks, the distribution remains inherently skewed. Mainstream paradigms like Classification and Regression dominate the dataset, whereas niche scientific tasks (e.g., Audio Classification, Tabular Grading) are represented by significantly smaller sample sizes. Consequently, while the model demonstrates strong general capabilities, its reliability in extremely low-resource or highly specialized scientific domains may vary, and the current evaluation may not fully reflect the challenges of these long-tail scenarios. Agent Framework Implementation. To validate the models utility, we prioritized stability, instantiating FOREAGENT with conservative Predict-then-Verify loop. This design alternates strictly between singular prediction and execution, barely scratching the surface of potential inference-time strategies. Specifically, we have not exhaustively explored advanced architectural variants or hyperparameter configurations within this paradigm, implying that the current implementation has not yet been pushed to its optimal limit. Therefore, the reported performance likely represents lower bound of the frameworks capability. Beyond this specific instantiation, we identify the frameworks broader potential as scalable Reward Model. By providing dense, execution-free feedback, it paves the way for accelerating Reinforcement Learning rollouts and serves as plugand-play optimization module adaptable to diverse agent frameworks."
        },
        {
            "title": "References",
            "content": "Nicolás Astorga, Tennison Liu, Yuanzhang Xiao, and Mihaela van der Schaar. 2025. Autoformulation of mathematical optimization models using llms. Preprint, arXiv:2411.01679. Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 51855198, Online. Association for Computational Linguistics. Christian Cabrera, Andrei Paleyes, Pierre Thodoroff, and Neil D. Lawrence. 2025. Machine learning systems: survey from data-oriented perspective. Preprint, arXiv:2302.04810. Jingyi Chai, Shuo Tang, Rui Ye, Yuwen Du, Xinyu Zhu, Mengcheng Zhou, Yanfeng Wang, Weinan E, Yuzhi Zhang, Linfeng Zhang, and Siheng Chen. 2025. Scimaster: Towards general-purpose scientific ai agents, part i. x-master as foundation: Can we lead on humanitys last exam? Preprint, arXiv:2507.05241. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander adry. 2025. Mle-bench: Evaluating machine learning agents on machine learning engineering. Preprint, arXiv:2410.07095. Liu Chang-shu, Chen Yang, and Reyhaneh Jabbarvand. 2024. Codemind: Evaluating large language models for code reasoning. http://arxiv.org/abs/2402.09664. Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, and Xin Xia. 2025a. Reasoning runtime behavior of program with llm: How far are we? In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE), pages 18691881. Ke Chen, Peiran Wang, Yaoning Yu, Xianyang Zhan, and Haohan Wang. 2025b. Large language modelbased data science agent: survey. Preprint, arXiv:2508.02744. Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, and Wanxiang Che. 2025c. Ai4research: survey of artificial intelligence for scientific research. Preprint, arXiv:2507.01903. Zhaorun Chen, Zhuokai Zhao, Kai Zhang, Bo Liu, Qi Qi, Yifan Wu, Tarun Kalluri, Sara Cao, Yuanhao Xiong, Haibo Tong, Huaxiu Yao, Hengduo Li, Jiacheng Zhu, Xian Li, Dawn Song, Bo Li, Jason Weston, and Dat Huynh. 2025d. Scaling agent learning via experience synthesis. Preprint, arXiv:2511.03773. Yash Akhauri, Xingyou Song, Arissa Wongpanich, Bryan Lewandowski, and Mohamed S. Abdelfattah. 2025. Regression language models for code. Preprint, arXiv:2509.26476. Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, Bang Liu, and Chenglin Wu. 2024. Sela: Tree-search enhanced llm agents for automated machine learning. Preprint, arXiv:2410.17238. Alex O. Davies, Roussel Nzoyem, Nirav Ajmeri, and Telmo M. Silva Filho. 2025. Language models do not embed numbers continuously. Preprint, arXiv:2510.08009. DeepSeek-AI. 2024. Deepseek api documentation: Parameter settings. https://api-docs.deepseek. Accom/quick_start/parameter_settings. cessed: 2025-12-21. DeepSeek-AI. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI. 2025b. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Jingtao Ding, Yunke Zhang, Yu Shang, Jie Feng, Yuheng Zhang, Zefang Zong, Yuan Yuan, Hongyuan Su, Nian Li, Jinghua Piao, Yucheng Deng, Nicholas Sukiennik, Chen Gao, Fengli Xu, and Yong Li. 2025. Understanding world or predicting future? comprehensive survey of world models. Preprint, arXiv:2411.14499. Anil R. Doshi and Oliver P. Hauser. 2024. Generative ai enhances individual creativity but reduces the collective diversity of novel content. Science Advances, 10(28):eadn5290. Shangheng Du, Xiangchao Yan, Dengyang Jiang, Jiakang Yuan, Yusong Hu, Xin Li, Liang He, Bo Zhang, and Lei Bai. 2025. Automlgen: Navigating finegrained optimization for coding agents. Preprint, arXiv:2510.08511. Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. 2015. The reusable holdout: Preserving validity in adaptive data analysis. Science, 349(6248):636638. Haoyang Fang, Boran Han, Nick Erickson, Xiyuan Zhang, Su Zhou, Anirudh Dagar, Jiani Zhang, Ali Caner Turkmen, Cuixiong Hu, Huzefa Rangwala, Ying Nian Wu, Bernie Wang, and George Karypis. 2025. Mlzero: multi-agent system for end-to-end machine learning automation. Preprint, arXiv:2505.13941. Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, and Mengdi Wang. 2025. Web world models. Preprint, arXiv:2512.23676. Luciano Floridi, Yiyang Jia, and Fernando Tohmé. 2025. categorical analysis of large language models and why llms circumvent the symbol grounding problem. Preprint, arXiv:2512.09117. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, Yossi Matias, Andrew Carroll, Kavita Kulkarni, Nenad Tomasev, Yuan Guan, Vikram Dhillon, Eeshit Dhaval Vaishnav, Byron Lee, Tiago Costa, José Penadés, Gary Peltz, Yunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, and Vivek Natarajan. 2025. Towards an ai co-scientist. Preprint, arXiv:2502.18864. Arya Grayeli, Atharva Sehgal, Omar Costilla-Reyes, Miles Cranmer, and Swarat Chaudhuri. 2024. Symbolic regression with learned concept library. Preprint, arXiv:2409.09359. Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang. 2024a. Cruxeval: benchmark for code reasoning, understanding and execution. Preprint, arXiv:2401.03065. Yang Gu, Hengyu You, Jian Cao, Muran Yu, Haoran Fan, and Shiyou Qian. 2024b. Large language models for constructing and optimizing machine learning workflows: survey. Preprint, arXiv:2411.10478. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. 2024. Ds-agent: Automated data science by empowering large language models with case-based reasoning. Preprint, arXiv:2402.17453. David Ha and Jürgen Schmidhuber. 2018. World models. Zenodo. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. 2024. Mastering diverse domains through world models. Preprint, arXiv:2301.04104. Sirui Hong, Yizhang Lin, Bang Liu, et al. 2024. Data interpreter: An llm agent for data science. Preprint, arXiv:2402.18679. Andre Hora. 2024. Predicting test results without execution. https://doi.org/10.1145/3663529.3663794, pages 542546. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2024. Mlagentbench: Evaluating language agents on machine learning experimentation. Preprint, arXiv:2310.03302. Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Huichi Zhou, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, Kun Shao, and Jun Wang. 2025. Deep research agents: systematic examination and roadmap. Preprint, arXiv:2506.18096. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. Preprint, arXiv:2403.07974. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. 2025. Aide: Ai-driven exploration in the space of code. Preprint, arXiv:2502.13138. Yixiu Liu, Yang Nan, Weixian Xu, Xiangkun Hu, Lyumanshan Ye, Zhen Qin, and Pengfei Liu. 2025a. Alphago moment for model architecture discovery. Preprint, arXiv:2507.18074. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, and Dong Yu. 2024. Dsbench: How far are data science agents from becoming data science experts? arXiv preprint arXiv:2409.07703. Zexi Liu, Yuzhu Cai, Xinyu Zhu, Yujie Zheng, Runkun Chen, Ying Wen, Yanfeng Wang, Weinan E, and Siheng Chen. 2025b. Ml-master: Towards ai-for-ai via integration of exploration and reasoning. Preprint, arXiv:2506.16499. Hoang Just, Ming Jin, Anit Sahu, Huy Phan, and Ruoxi Jia. 2024. Data-centric human preference optimization with rationales. arXiv (Cornell University). Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large language models are zero-shot reasoners. Preprint, arXiv:2205.11916. Stepan Kulibaba, Artem Dzhalilov, Roman Pakhomov, Oleg Svidchenko, Alexander Gasnikov, and Aleksei Shpilman. 2025. Kompeteai: Accelerated autonomous multi-agent system for end-to-end pipeline generation for machine learning problems. Preprint, arXiv:2508.10177. Robert Tjarko Lange, Yuki Imajuku, and Edoardo Cetin. 2025. Shinkaevolve: Towards open-ended and sample-efficient program evolution. Preprint, arXiv:2509.19349. Annan Li, Chufan Wu, Zengle Ge, et al. 2025a. The fm agent. Preprint, arXiv:2510.26144. Haoyang Li, Xuejia Chen, Zhanchao Xu, Darian Li, Nicole Hu, Fei Teng, Yiming Li, Luyu Qiu, Chen Jason Zhang, Li Qing, and Lei Chen. 2025b. Exposing numeracy gaps: benchmark to evaluate fundamental numerical abilities in large language models. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2000420026, Vienna, Austria. Association for Computational Linguistics. Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. 2025c. Codei/o: Condensing reasoning patterns via code input-output prediction. Preprint, arXiv:2502.07316. Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. 2025d. Mlr-copilot: Autonomous machine learning research based on large language models agents. Preprint, arXiv:2408.14033. Yixia Li, Hongru Wang, Jiahao Qiu, Zhenfei Yin, Dongdong Zhang, Cheng Qian, Zeping Li, Pony Ma, Guanhua Chen, Heng Ji, and Mengdi Wang. 2025e. From word to world: Can large language models Preprint, be implicit text-based world models? arXiv:2512.18832. Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and Andrew M. Dai. 2022. Minds eye: Grounded language model reasoning through simulation. Preprint, arXiv:2210.05359. Yujie Luo, Zhuoyun Yu, Xuehai Wang, Yuqi Zhu, Ningyu Zhang, Lanning Wei, Lun Du, Da Zheng, and Huajun Chen. 2025. Executable knowledge graphs for replicating ai research. Preprint, arXiv:2510.17795. Rachel Metz. 2024. Openai scale ranks progress toward human-level ai. Bloomberg. Jaehyun Nam, Jinsung Yoon, Jiefeng Chen, Jinwoo Shin, Sercan Ö. Arık, and Tomas Pfister. 2025. Mle-star: Machine learning engineering agent via search and targeted refinement. Preprint, arXiv:2506.15692. Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach, William Yang Wang, and Roberta Raileanu. 2025. Mlgym: new framework and benchmark for advancing ai research agents. Preprint, arXiv:2502.14499. Alexander Novikov, Ngân Vu, Marvin Eisenberger, et al. 2025. Alphaevolve: coding agent for scientific and algorithmic discovery. Preprint, arXiv:2506.13131. OpenAI. 2024. Openai o1 system card. Preprint, arXiv:2412.16720. OpenAI. 2025a. System Card for gpt-5. Accessed on August 13, 2025. OpenAI. 2025b. System Card for o3-mini. Accessed on December 11, 2025. Yixin Ou, Yujie Luo, Jingsheng Zheng, Lanning Wei, Zhuoyun Yu, Shuofei Qiao, Jintian Zhang, Da Zheng, Yuren Mao, Yunjun Gao, Huajun Chen, and Ningyu Zhang. 2025. Automind: Adaptive knowledgeable agent for automated data science. Preprint, arXiv:2506.10974. Rushi Qiang, Yuchen Zhuang, Yinghao Li, Dingu Sagar K, Rongzhi Zhang, Changhao Li, Ian ShuHei Wong, Sherry Yang, Percy Liang, Chao Zhang, and Bo Dai. 2025. Mle-dojo: Interactive environments for empowering llm agents in machine learning engineering. Preprint, arXiv:2505.07782. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2023. Reasoning with language model prompting: survey. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53685393, Toronto, Canada. Association for Computational Linguistics. Shuofei Qiao, Yanqiu Zhao, Zhisong Qiu, Xiaobin Wang, Jintian Zhang, Zhao Bin, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. 2025. Scaling generalist data-analytic agents. Preprint, arXiv:2509.25084. H. G. Rice. 1953. Classes of recursively enumerable sets and their decision problems. Transactions of the American Mathematical Society, 74(2):358366. Christopher Michael Rytting and David Wingate. 2021. Leveraging the inductive bias of large language models for abstract textual reasoning. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS 21, Red Hook, NY, USA. Curran Associates Inc. Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Michael Moor, Zicheng Liu, and Emad Barsoum. 2025. Agent laboratory: Using llm agents as research assistants. Preprint, arXiv:2501.04227. Judy Shen, Archit Sharma, Judy Shen, Qin Jun, Archit Sharma, and Jun Qin. 2024. Towards data-centric rlhf: Simple metrics for preference dataset comparison. http://arxiv.org/abs/2409.09603, abs/2409.09603. Lin Shi, Weicheng Ma, and Soroush Vosoughi. 2024. Judging the judges: systematic investigation of position bias in pairwise comparative assessments by llms. arXiv (Cornell University). Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. 2025a. Zerosearch: Incentivize the search capability of llms without searching. Preprint, arXiv:2505.04588. Ji Sun, Guoliang Li, Peiyao Zhou, Yihui Ma, Jingzhe Xu, and Yuan Li. 2025b. Agenticdata: An agentic data analytics system for heterogeneous data. Preprint, arXiv:2508.05002. FAIR CodeGen team, Jade Copet, Quentin Carbonneaux, et al. 2025. Cwm: An open-weights llm for research on code generation with world models. Preprint, arXiv:2510.02387. InternAgent Team, Bo Zhang, Shiyang Feng, et al. 2025. Internagent: When agent becomes the scientist building closed-loop system from hypothesis to verification. Preprint, arXiv:2505.16938. Patara Trirat, Wonyong Jeong, and Sung Ju Hwang. llm Preprint, 2025. Automl-agent: framework for full-pipeline automl. arXiv:2410.02958. multi-agent Andrej Tschalzev, Sascha Marton, Stefan Lüdtke, Christian Bartelt, and Heiner Stuckenschmidt. 2024. data-centric perspective on evaluating machine Preprint, learning models for arXiv:2407.02112. tabular data. Sai Wang, Senthilnathan Subramanian, Mudit Sahni, Praneeth Gone, Lingjie Meng, Xiaochen Wang, Nicolas Ferradas Bertoli, Tingxian Cheng, and Jun Xu. 2025a. Configurable multi-agent framework for scalable and realistic testing of llm-based agents. Preprint, arXiv:2507.14705. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. 2025b. Openhands: An open platform for ai software developers as generalist agents. Preprint, arXiv:2407.16741. Anjiang Wei, Jiannan Cao, Ran Li, Hongyu Chen, Yuhui Zhang, Ziheng Wang, Yuan Liu, Thiago S. F. X. Teixeira, Diyi Yang, Ke Wang, and Alex Aiken. 2025a. Equibench: Benchmarking large language models reasoning about program semantics via equivalence checking. Preprint, arXiv:2502.12466. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. 2025b. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. Preprint, arXiv:2502.18449. Lionel Wong, Gabriel Grand, Alexander K. Lew, Noah D. Goodman, Vikash K. Mansinghka, Jacob Andreas, and Joshua B. Tenenbaum. 2023. From word models to world models: Translating from natural language to the probabilistic language of thought. Preprint, arXiv:2306.12672. Xu Yang, Xiao Yang, Shikai Fang, Yifei Zhang, Jian Wang, Bowen Xian, Qizheng Li, Jingyuan Li, Minrui Xu, Yuante Li, Haoran Pan, Yuge Zhang, Weiqing Liu, Yelong Shen, Weizhu Chen, and Jiang Bian. 2025. R&d-agent: An llm-agent framework towards autonomous data science. Preprint, arXiv:2505.14738. Edan Toledo, Karen Hambardzumyan, Martin Josifoski, et al. 2025. Ai research agents for machine learning: Search, exploration, and generalization in mle-bench. Preprint, arXiv:2507.02554. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. Preprint, arXiv:2210.03629. Yunxiang Zhang, Muhammad Khalifa, Shitanshu Bhushan, Grant Murphy, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. 2025e. Mlrc-bench: Can language agents solve machine learning research challenges? Preprint, arXiv:2504.09702. Minjun Zhu, Qiujie Xie, Yixuan Weng, Jian Wu, Zhen Lin, Linyi Yang, and Yue Zhang. 2025a. Ai scientists fail without strong implementation capability. Preprint, arXiv:2506.01372. Yizhang Zhu, Liangwei Wang, Chenyu Yang, et al. 2025b. survey of data agents: Emerging paradigm or overstated hype? Preprint, arXiv:2510.23587. Haofei Yu, Zhaochen Hong, Zirui Cheng, Kunlun Zhu, Keyang Xuan, Jinwei Yao, Tao Feng, and Jiaxuan You. 2025a. Researchtown: Simulator of human research community. Preprint, arXiv:2412.17767. Zhaojian Yu, Kaiyue Feng, Yilun Zhao, Shilin He, XiaoPing Zhang, and Arman Cohan. 2025b. Alpharesearch: Accelerating new algorithm discovery with language models. Preprint, arXiv:2511.08522. Jiakang Yuan, Xiangchao Yan, Shiyang Feng, Bo Zhang, Tao Chen, Botian Shi, Wanli Ouyang, Yu Qiao, Lei Bai, and Bowen Zhou. 2025. Dolphin: Moving towards closed-loop auto-research through thinking, practice, and feedback. Preprint, arXiv:2501.03916. Liu Ze-xi, Jingyi Chai, Zexi Liu, Zhu Xinyu, Jingyi Chai, Tang Shuo, Xinyu Zhu, Ye Rui, Shuo Tang, Zhang Bo, Rui Ye, Bai Lei, Bo Zhang, Siheng Chen, Lei Bai, and Siheng Chen. 2025. Ml-agent: Reinforcing llm agents for autonomous machine learning engineering. https://doi.org/10.48550/arxiv.2505.23723, abs/2505.23723. Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. 2023. Data-centric artificial intelligence: survey. Preprint, arXiv:2303.10158. Jiahuan Zhang, Tianheng Wang, Hanqing Wu, Ziyi Huang, Yulong Wu, Dongbai Chen, Linfeng Song, Yue Zhang, Guozheng Rao, and Kaicheng Yu. 2025a. Sr-llm: Rethinking the structured representation in large language model. Preprint, arXiv:2502.14352. Jintian Zhang, Kewei Xu, Jingsheng Zheng, Zhuoyun Yu, Yuqi Zhu, Yujie Luo, Lanning Wei, Shuofei Qiao, Lun Du, Da Zheng, Shumin Deng, Huajun Chen, and Ningyu Zhang. 2025b. Innogym: Benchmarking the innovation potential of ai agents. Preprint, arXiv:2512.01822. Shaolei Zhang, Ju Fan, Meihao Fan, Guoliang Li, and Xiaoyong Du. 2025c. Deepanalyze: Agentic large language models for autonomous data science. Preprint, arXiv:2510.16872. Wenlin Zhang, Xiaopeng Li, Yingyi Zhang, Pengyue Jia, Yichao Wang, Huifeng Guo, Yong Liu, and Xiangyu Zhao. 2025d. Deep research: surPreprint, vey of autonomous research agents. arXiv:2508.12752. Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. 2024a. Webpilot: versatile and autonomous multi-agent system for web task execution with strategic exploration. AAAI Conference on Artificial Intelligence. Yuge Zhang, Qiyang Jiang, XingyuHan XingyuHan, Nan Chen, Yuqing Yang, and Kan Ren. 2024b. Benchmarking data science agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 56775700."
        },
        {
            "title": "A Extended Related Work",
            "content": "et al., 2025). This section expands upon the brief literature review in Section 7, providing detailed taxonomy of LLM-based autonomous agents and the theoretical underpinnings of world models in the code domain. LLM-based Agents for Scientific Discovery LLMs with strong reasoning capabilities (Qiao et al., 2023) are increasingly serving as core controllers for autonomous agents in scientific discovery (Gu et al., 2024b; Chen et al., 2025c), extending to specialized machine research domains (Toledo et al., 2025; Zhang et al., 2025d). Beyond the digital realm, agents are transforming laboratory research (Liu et al., 2025a; Li et al., 2025d; Huang et al., 2025; Schmidgall et al., 2025) and complex data analytics (Sun et al., 2025b; Zhang et al., 2025c). Prominent systems now autonomously propose hypotheses (Chai et al., 2025; Yu et al., 2025b; Team et al., 2025; Novikov et al., 2025) and conduct closed-loop experiments (Gottweis et al., 2025; Lange et al., 2025; Yuan et al., 2025; Yu et al., 2025a), highlighting the trend of AI Scientists operating in open-ended exploration loops. Narrowing down to the machine learning domain, the ecosystem is highly diversified. One stream of research focuses on managing the endto-end workflow, ranging from autonomous frameworks (Nam et al., 2025; Yang et al., 2025; Qiao et al., 2025; Ze-xi et al., 2025) to engineering pipelines (Fang et al., 2025; Chi et al., 2024). Another stream, driven by benchmarks like MLEbench (Chan et al., 2025; Huang et al., 2024) and broader evaluation suites (Zhang et al., 2025e; Jing et al., 2024; Zhang et al., 2024b; Nathani et al., 2025), focuses on competitive problem-solving through knowledge-guided reasoning (Luo et al., 2025; Ou et al., 2025) and evolutionary optimization (Du et al., 2025; Guo et al., 2024; Li et al., 2025a; Liu et al., 2025b). Additionally, general-purpose platforms and optimization frameworks offer the foundational tooling and multi-agent architectures required for scalable research (Wang et al., 2025b; Jiang et al., 2025; Hong et al., 2024; Qiang et al., 2025; Wang et al., 2025a). However, to mitigate the significant computational overhead of the generation-executionfeedback loop inherent in these systems, recent approaches explore utilizing internal priors to estimate feasibility and prune redundant steps, thereby accelerating optimization (Kulibaba et al., 2025; Trirat et al., 2025; Zhang et al., 2024a; Astorga Operational Details of Agent Baselines As introduced in Section 2.1, we take two representative agent frameworks that operate under the GenerateExecute-Feedback paradigm as examples. Here we provide their detailed mechanisms: AIDE: AIDE (Jiang et al., 2025) is an LLMbased agent that frames machine learning engineering as code optimization problem. It structures the trial-and-error process as tree search in the solution space, systematically reusing and refining promising code candidates. This method effectively trades computational resources for enhanced performance. Specifically, AIDE first generates initial code C0 based on instruction I. The code is executed by training on dataset to obtain results. Subsequently, AIDE iteratively derives new code C1, C2, . . . , Ct based on the feedback. AutoMind: Building upon the AIDE framework, AutoMind (Ou et al., 2025) further integrates curated expert knowledge base and self-adaptive coding strategy. While retaining the tree search structure, it grounds the agent in domain expertise and dynamically tailors code generation to task complexity. This approach aims to reduce invalid attempts by improving the quality of the initial draft and subsequent refinements. World Models and Execution-Free Evaluation The concept of World Models originates from model-based reinforcement learning, where agents learn to simulate the environments transition dynamics to plan actions without expensive trial-anderror (Ding et al., 2025; Hafner et al., 2024; Feng et al., 2025; Wong et al., 2023; Li et al., 2025e). Our work adapts this concept to the code generation domain, addressing the Execution Bottleneck inherent in the agentic loops described above. Recent research enables models to internalize the execution process, predicting test outcomes (Hora, 2024; team et al., 2025; Wei et al., 2025b) or assessing logic consistency directly (Li et al., 2025c; Chang-shu et al., 2024). This capability is rigorously evaluated on reasoning-centric benchmarks (Wei et al., 2025a; Gu et al., 2024a; Jain et al., 2024). Unlike traditional benchmarks that may allow for rote memorization, these tasks require models to transcend statistical pattern matching and develop deep semantic understanding of algorithmic states and control flows (Chen et al., 2025d; Sun et al., 2025a; Akhauri et al., 2025; Chen et al., 2025a), serving as the foundational capability for our proposed framework. Aligning with OpenAIs Level 4 Innovators (Metz, 2024; Zhang et al., 2025b), this empowers agents to drive innovation by leveraging internal world models to proactively prune vast hypothesis spaces, shifting the paradigm from ensuring syntactic correctness to optimizing for semantic success. This transition resonates with the broader Data-Centric AI movement (Zha et al., 2023; Cabrera et al., 2025), moving beyond model architecture to focus on the quality of evaluative signals. Specifically, our framework incorporates rationale-based preference optimization (Just et al., 2024) and rigorous dataset construction criteria (Shen et al., 2024) to ensure that the implicit world model is grounded in data-specific realities rather than abstract heuristics (Tschalzev et al., 2024)."
        },
        {
            "title": "B Corpus Details",
            "content": "To support the reproducibility of our analysis and provide comprehensive view of the solution space, we provide detailed metadata for the prediction corpus. B.1 Task Metadata and Scale Table 6 outlines the specific characteristics of each of the 26 tasks, including the domain, machine learning paradigm, data size, and the scale of the constructed evaluation set. B.2 Algorithm and Architecture Distribution As shown in Figure 5 and Table 7, the solutions range from traditional statistical methods to advanced deep learning architectures, ensuring that our analysis is evaluated against heterogeneous solution manifold. B.3 Agent Evaluation Benchmark We curated specialized benchmark to test the World Models capability to generalize from seen tasks to unseen scientific problems. As detailed in Table 8, this selection covers diverse AI4Science domains including Biology, Physics, Geoscience, Ecology, and Medicine. Note that tasks marked with * (Aerial Cactus and Histo. Cancer Detect) are unseen tasks, meaning they were not used in the main experiments and serve as out-of-distribution evaluations."
        },
        {
            "title": "C Detailed Experiment Result",
            "content": "In this section, we provide comprehensive breakdown of the experimental results, supplementing the main paper with granular performance metrics across individual tasks, domains, and agent architectures. C.1 Fine-grained Performance on Prediction"
        },
        {
            "title": "Corpus",
            "content": "Table 10 presents the task-level performance comparison between DeepSeek-V3.2 and GPT-5.1 across all 26 tasks in the Prediction Corpus. The results are categorized by task domain (CV, NLP, Data Science) and difficulty level, offering detailed view of model capabilities. Furthermore, to provide deeper understanding of the Others category mentioned in the main table  (Table 2)  , Table 11 breaks down performance by specific machine learning paradigms. This granular analysis reveals distinct performance characteristics in Ranking, Matching, Segmentation, and Extraction tasks, highlighting significant gaps in Matching and Ranking capabilities between the models. Finally, we investigate the impact of data context in Figure 8, which presents the data representation sensitivity analysis. The stacked bar chart reveals the incremental impact of adding Raw Data, Numerical Statistics, and Verbal Reports. While codeonly context serves as strong baseline, enriching the context with multimodal data yields consistently superior performance, with the magnitude of improvement exhibiting distinct domain-specific patterns. C.2 Detailed Performance Metrics of FOREAGENT on AI4Science Benchmarks We evaluate the generalization capability of FOREAGENT on subset of 5 challenging AI4Science tasks using the Beat Ratio metric. Table 12 details the specific quantitative results for both the AIDE baseline and FOREAGENT. The comparison explicitly distinguishes between tasks seen during the training phase and unseen out-of-distribution tasks. The metrics demonstrate that FOREAGENT maintains robust performance on seen tasks while achieving superior generalization on unseen problems, such as Aerial Cactus Identification and Histopathologic Cancer Detection, validating the Task Name Task Description ML Paradigm Size Sol Pair Computer Vision Domain APTOS 2019 Blindness Dog Breed Identification Detect diabetic retinopathy severity from retinal fundus images. Identify dog breed from photos (120 categories). Img Class. (Multi-class) Img Class. (Multi-class) Leaf Classification Classify 99 plant species based on leaf shape features. Img Class. (Multi-class) MLSP 2013 Birds Identify bird species from audio spectrograms. Plant Pathology 2020 Distinguish healthy vs. diseased apple leaves. Statoil Iceberg Classifier ICML 2013 Whale Distinguish icebergs from ships in radar imagery. Identify individual Right Whales by callosity patterns. Img Class. (Multi-class) TGS Salt Identification Segment salt deposits from seismic images. Segmentation (Pixel-level) Natural Language Processing Domain Detecting Insults Jigsaw Toxic Comment Spooky Author ID Detect insulting language in social commentary. Text Class. (Binary) Classify comments into 6 toxicity types (toxic, severe, etc.). Text Class. (Multi-label) Identify author (Poe, Shelley, Lovecraft) of excerpts. Text Class. (Multi-class) Random Acts of Pizza Predict success of free pizza requests on Reddit. Text Class. (Binary) Img Class. (Multi-label) Img Class. (Multi-class) Img Class. (Binary) 8.1G 50 1,225 369M 3 30M 17 136 634M 50 1,221 387M 15 205M 50 1,223 377M 24 275 59M 880 2M 27 350 129M 5 3.2M 50 1,220 21M 50 1,225 US Patent Matching Determine semantic similarity between patent phrases. Matching (Class.) 316M 50 1,223 Denoising Dirty Docs Google QUEST Tweet Sentiment Extract LMSYS Chatbot Arena Automated Essay Scoring Restore clean text from noisy scanned documents. Predict 30 subjective attributes (e.g., helpfulness) for Q&A. Extract substring supporting the sentiment label. Img Restoration (Reg.) Multi-output (Reg.) Seq. Labeling (Extract) Predict human preference between two LLM responses. Ranking (Preference) Automatically grade student essays on numeric scale. Regression (Ordinal) 97M 45 974 14M 50 1,224 3.3M 21 176M 50 1,220 35M 20 190 Continued on next page Table 6: Detailed metadata for all 26 tasks in the Prediction Corpus (Part 1 of 2). The table details the problem definition, ML paradigm, data size, and evaluation scale. Figure 5: Hierarchical distribution of the unique solution architectures in our Prediction Corpus. The chart illustrates the balance achieved across major machine learning paradigms: Gradient Boosting&Trees, General/Sequential NNs, CNNs, and Transformers. The outer ring details specific model instances, demonstrating the high heterogeneity of the solution space. Task Name Task Description ML Paradigm Size Sol Pair Table 6 continued from previous page Data Science Domain NYC Taxi Fare Predict taxi fare from coordinates and time. PetFinder Pawpularity Predict popularity score of pet profile photos. NOMAD Conductors Predict formation energy of aluminum-gallium oxides. Tabular (Regression) Regression (Hybrid) Regression (Scientific) 5.3G 30 429 1.0G 30 25M 3 3 Stanford COVID Vaccine Predict degradation rates of mRNA vaccine sequences. Regression (Bio) 14M 50 1,222 Tabular Playground Predict forest cover type from cartographic variables. Tabular (Multi-class) 526M 275 Volcanic Eruptions Predict time to next eruption from seismic sensors. Time-Series (Reg.) 15G 1,213 Ventilator Pressure Predict airway pressure from control inputs. Time-Series (Reg.) 291M 50 1, TF Speech Recognition Identify spoken commands from audio clips. Audio (Multi-class) 2G 46 1, Table 6: Detailed metadata for all 26 tasks (Part 2 of 2). Continued from previous page. Task Name Algorithm Composition (Count) Computer Vision Domain APTOS 2019 Blindness EfficientNet (10), ResNet (10), Swin Transformer (9), ConvNeXt (9), Vision Transformer (8), DeiT (3), CNN-LSTM (1) Dog Breed ID ConvNeXt-Large (2), ResNet18 (1) Leaf Classification LightGBM (13), Feedforward NN (2), HybridLeafClassifier (1), XGBoost (1) MLSP 2013 Birds Plant Pathology Statoil Iceberg Ensemble (12), Dual-Stream Arch (5), Feedforward NN (5), Multi-Modal NN (5), Transformer Enc (5), CNN (5), Random Forest (4), XGBoost (4), Logistic Reg (4), LightGBM (1) EfficientNet (2), Swin Transformer (2), ResNet (1), Vision Transformer (1) Inverted Bottleneck (5), Vision Trans. (5), ResNet (5), Feedforward NN (5), XGBoost (5), CNN (5), Hybrid CNN-ViT (4), Swin Trans. (4), ConvNeXt (4), Random Forest (3), LightGBM (3), EfficientNet (1), SVM (1) ICML Whale Challenge Wav2Vec2 Feature Extractor (10), CNN (6), XGBoost (4), Gradient Boosting (2), LightGBM (1), Mel Spectrogram (1) TGS Salt ID Ensemble Segmentation (12), EfficientNet (10), U-Net (10), DeepLabV3Plus (4), Vision Transformer (4), Single Seg. Model (2), Swin Trans. (1), ConvNeXt (1) Denoising Dirty Docs Residual Dense Network (10), U-Net (10), Conv Autoencoder (10), Restormer (8), Hybrid CNN-Transformer (6), Simple CNN (1) Continued on next page Table 7: Distribution of algorithms and architectures across the corpus (Part 1 of 2). The table details the algorithm composition for Computer Vision tasks. effectiveness of the World Model in bridging the implementation gap. C.3 Search Efficiency Analysis of"
        },
        {
            "title": "FOREAGENT",
            "content": "To elucidate the operational efficiency and robustness of FOREAGENT, we analyze its training dynamics. First, regarding temporal efficiency, Figure 6 plots the Average Beat Ratio over the 12-hour execution window. The trajectories indicate that FOREAGENT converges to optimal solutions significantly faster than the baseline across the majority of tasks. Complementing this, Figure 7 visualizes the search breadth. It shows that by leveraging the World Model for low-cost evaluation, FOREAGENT maintains higher rate of node exploration, Task Name Algorithm Composition (Count) Natural Language Processing Domain Table 7 continued from previous page Detecting Insults DeBERTa (9), Multi-Task DeBERTa-V3 (6), RoBERTa (4), DistilBERT (3), BERT (3), Logistic Regression (2) Jigsaw Toxic Comment RoBERTa (3), DistilBERT (1), DeBERTa (1) Spooky Author ID Random Acts of Pizza US Patent Matching Google QUEST Knowledge Distillation (4), DeBERTa (4), ELECTRA (4), BERT (4), LSTM (4), XGBoost (4), Ensemble (4), SVM (4), Logistic Reg (4), Random Forest (3), LightGBM (3), Naive Bayes (3), MLP (2), Transformer (2), Hierarchical Trans. (1) Neural Network (6), SentenceTransformer (4), RoBERTa (4), Knowledge Distillation (4), Multimodal NN (4), BERT (4), DistilBERT (4), Random Forest (4), XGBoost (4), Logistic Reg (4), LightGBM (4), LMs Text Embeddings (4) Custom NN (5), RoBERTa (5), DeBERTa (5), XGBoost (5), BERT (5), Sentence Trans. (5), Similarity Model (5), Linear Reg (5), LightGBM (5), Stacking Ensemble (2), RandomForest (2), Cross-Attn Hybrid (1) BERT (5), Multi-Task NN (5), MultiModal Trans. (5), Graph Attention (3), Hierarchical Attn (3), MLP (3), Cross-Attn (3), Sentence Trans. (3), DeBERTa (3), RoBERTa (3), XGBoost (3), LightGBM (3), Ridge Reg. (3), ELECTRA (2), Random Forest (2), LSTM (1) Tweet Sentiment RoBERTa-BiLSTM (10), RoBERTa (10), Model Ensemble (1) LMSYS Chatbot Arena RoBERTa (11), XGBoost (8), Logistic Reg. (8), LightGBM (8), MLP Classifier (7), DeBERTa (4), Dual Encoder NN (4) Automated Essay Score Hybrid NN (9), MetaModel NN (5), Stacking Ensemble (3), LightGBM (3) Data Science Domain NYC Taxi Fare LightGBM (10), XGBoost (10), Feedforward NN (7), CatBoost (1), Dual-Branch NN (1), Residual NN (1) PetFinder Pawpularity LightGBM (27), Vision Transformer (2), XGBoost (1) NOMAD Conductors XGBoost (2), Random Forest (1) Stanford COVID Vac. Hybrid Architectures (14), Model Ensemble (9), Transformer/GNN (6), Specialized RNA Models (6), Tree Boosters (6), General Baselines (7), LSTM (2) Tabular Playground Volcanic Eruptions Ventilator Pressure Multi-Branch NN (11), LightGBM (7), Custom NN (3), TabTransformer (2), Feedforward NN (1) Tree Boosters (19), MLP/Dense Networks (16), Transformer Variants (6), CNN/Hybrid Architectures (6), Model Ensemble (2), TCN (1) RNNs (LSTM/GRU) (17), Hybrid Deep Learning (CNN/TCN/Attn) (13), Tree Boosters (10), Transformers (9), Statistical Baseline (1) TF Speech Recognition Statistical ML (RF/SVM/LR) (21), CNN Architectures (13), Pre-trained Audio Models (Wav2Vec2/WavLM) (8), Transformer (2), MLP (1), Knowledge Distillation (1) Table 7: Distribution of algorithms and architectures across the corpus (Part 2 of 2). Continued from previous page (NLP and Data Science domains). effectively covering broader search space within the same computational budget. C.4 Decision Fidelity Analysis of"
        },
        {
            "title": "FOREAGENT",
            "content": "To audit decision quality without accessible ground truth for skipped nodes (which yield no test metrics), we evaluate the trajectory consistency of executed steps by aligning internal Validation Scores (Sval) with external Test Scores (Stest). We define two metrics: Solution Evolution Consistency, which checks if the validation gains between consecutive executed steps (SB val) are consistent with test outcomes, and Global Pairwise Consistency, which measures the ranking agreement across the entire search history. val > SA As shown in Table 13, FOREAGENT exhibits marginal decrease in Solution Evolution Consistency compared to the execution-based baseline AIDE (0.756 vs. 0.779). This slight trade-off is acceptable given the substantial efficiency gains. Conversely, FOREAGENT achieves superior Global Pairwise Consistency (0.801 vs. 0.741), suggesting that our implicit evaluation acts as regularizer, efTask Name Task Description ML Paradigm Size Status Seen Tasks (In-Distribution) Stanford COVID Vaccine Ventilator Pressure Statoil Iceberg (Biology) Predict RNA degradation rates at various locations along RNA sequences to assist in mRNA vaccine stability research. (Physics) Simulate the pressure of mechanical ventilator connected to sedated patients lung to optimize breathing assistance. (Geoscience) Distinguish between icebergs and ships in satellite radar imagery (SAR) to improve navigation safety. Regression (Seq) 14M Seen Regression (Time-Series) Classification (Image) 291M Seen 205M Seen Unseen Tasks (Out-of-Distribution) Aerial Cactus Identification* Histopathologic Cancer Detection.* (Ecology) Determine the presence of columnar cacti in high-resolution aerial imagery to track protected species in the desert. (Medicine) Identify metastatic cancer tissue in small image patches taken from larger digital pathology scans. Classification (Image) Classification (Image) 25.4M Unseen 7.7G Unseen Table 8: Agent Evaluation Benchmark. The table details the specific tasks used to evaluate the agent, categorized by their domain and their visibility status (Seen vs. Unseen). fectively filtering out overfitting candidates to yield more stable search trajectory. C.6 Computational Infrastructure and"
        },
        {
            "title": "Budget",
            "content": "C.5 Licensing and Artifact Usage We clarify the licensing terms for the key artifacts involved in this study to ensure compliance and reproducibility: Datasets: All problem statements and datasets are sourced from public Kaggle competitions. They are utilized in strict accordance with their respective competition rules and standard Creative Commons licenses (predominantly CC-BY-SA 4.0). Models: The backbone language models employed are open-weights models used under their official Apache 2.0 and MIT licenses. Code and Benchmark: We will release our curated corpus and the accompanying agent framework under the MIT license to facilitate future research. Hardware Setup. All experiments were conducted on high-performance local server equipped with an Intel Xeon Gold 6138 CPU (80 logical cores, 2.00GHz) and 6 NVIDIA GeForce RTX 3090 GPUs (24GB VRAM each). To maximize throughput, we orchestrated parallelized evaluation pipeline with 6 concurrent workers, assigning one dedicated GPU to each task environment. This ensures that physical code executions are isolated and do not suffer from resource contention. Token Consumption. Table 9 summarizes the estimated token usage for the primary data construction and ablation phases. The main benchmark generation (covering 18,438 solution pairs) consumed approximately 78.5 million tokens (Input + Output). We note that the computational cost for agent baselines (e.g., AIDE) is highly stochastic due to their autonomous error-recovery loops, where single difficult task may trigger exponential branching and token usage compared to our linear inference approach. Our usage of these artifacts aligns with their intended purpose of fostering machine learning research. Furthermore, the derived corpus we will release is strictly intended for non-commercial research evaluation, ensuring compatibility with the original access conditions. C.7 Software Dependencies and Metric"
        },
        {
            "title": "Implementation",
            "content": "To ensure the reproducibility of our evaluation metrics and inference pipelines, we detail the software environment and parameter settings used: Experiment Phase Sample Scale Input Tokens Output Tokens Est. Total Main Benchmark (Full Construction) Max 50 sols/task (18,438 pairs) Analysis & Ablation Max 15 sols/task (Subset Evaluation) 60.1M 18.4M 78.5M 7.3M 2.3M 9.6M Agent Baselines (AIDE / AutoMind) Dynamic High Variance (Task-Dependent) - Table 9: Computational Budget and Token Consumption. Statistics are aggregated across all 26 tasks. The agent baselines exhibit high variance due to their autonomous feedback loops, making precise token estimation non-deterministic. Figure 6: Temporal Evolution of Performance. The curves display the Average Beat Ratio as function of Execution Time (012 hours) for both the AIDE baseline and FOREAGENT. The results are broken down by the five individual AI4Science tasks and the overall Micro Average. Evaluation Metrics: We utilize the standard implementations provided by Scikit-learn for calculating all performance metrics. Unless explicitly stated otherwise, we strictly adhere to the default parameter settings to maintain consistency with standard leaderboards. Data Processing: Data manipulation and feature extraction are performed using NumPy. LLM Inference: We employ the official OpenAI Python Library to conduct inference. This standardizes interactions across different model endpoints. We utilize default sampling parameters to ensure deterministic outputs for the \"Predict\" phase."
        },
        {
            "title": "D Detailed Qualitative Analysis",
            "content": "three qualitative examples. First, we analyze reasoning trajectory in the Google Quest Challenge to illustrate how the model overcomes human bias (Finding 5). Subsequently, we provide visual samthe Verbal ples of two critical system artifacts: Data Report and the Task Instruction, enabling concrete inspection of the agents input and context. D.1 Case I: Overcoming Complexity Bias (Reasoning Analysis) To provide concrete example of Finding 5 (The World Model Transcends Human Intuition by Prioritizing Data-Grounded Constraints), we present detailed analysis in Figure 9. This case illustrates common pitfall where architectural sophistication clashes with fundamental data constraints. To validate the models reasoning depth and provide transparency into our pipeline, we present Scenario and Conflict. The agent evaluates two distinct solutions for the Google Quest Q&A task: Task Name Domain Diff. Task Pairs (N ) DeepSeek-V3.2 GPT-5.1 APTOS 2019 Blindness Denoising Dirty Docs Insults in Social Comm. Dog Breed ID Google QUEST Jigsaw Toxic Comment Leaf Classification Automated Essay Scoring LMSYS Chatbot Arena MLSP 2013 Birds NYC Taxi Fare NOMAD2018 Conductors PetFinder Pawpularity Plant Pathology 2020 Volcanic Eruptions Random Acts of Pizza Spooky Author ID Stanford COVID Vaccine Statoil Iceberg Tabular Playground (Dec) TF Speech Recognition TGS Salt ID ICML 2013 Whale Tweet Sentiment Extr. US Patent Matching Ventilator Pressure CV CV NLP CV NLP NLP CV NLP NLP CV DS DS DS CV DS NLP NLP DS CV DS DS CV CV NLP NLP DS Easy CLS Easy REG Easy CLS Easy CLS Med REG Easy CLS Easy CLS Med REG Med RNK Easy CLS Easy REG Easy REG Med REG Easy CLS Hard REG Easy CLS Easy CLS Hard REG Med CLS Easy CLS Med CLS Med SEG Easy CLS Med EXT Med MAT Med REG 1225 974 350 3 1224 10 136 190 1220 1221 429 3 239 15 1213 1225 1220 1222 1223 275 1011 880 275 210 1223 1222 51.80.4 76.00.6 74.00.3 77.819.2 63.91.1 23.35.8 74.80.4 69.13.6 68.30.4 58.11.4 47.11.5 100.00.0 43.90.7 60.011.5 49.21.2 60.20.9 66.01.0 64.80.7 59.51.0 38.70.4 58.30.9 54.30.7 48.00.4 45.73.8 76.40.8 67.00.4 48.21.2 53.81.3 60.92.0 66.70.0 64.60.9 16.75.8 72.32.2 74.51.0 55.80.7 54.80.5 52.10.7 100.00.0 46.61.2 51.13.8 50.50.4 52.90.6 69.21.2 68.30.3 62.70.4 42.71.3 58.40.4 57.90.3 47.31.0 44.63.1 74.50.2 59.00.4 Overall Average All 26 Tasks 61.50.2 58.80.3 Table 10: Detailed result of each tasks performance in the main experiment. Breakdown of Domain, Difficulty (Diff.), and Task Paradigm. represents the number of pairwise comparison samples. DS = Data Science. Values: Mean Accuracy (%) Stdev. Bold: Best result. Figure 7: Progression of Search Node Exploration. This figure illustrates the cumulative number of nodes explored (Avg. Node Num.) over the 12-hour duration. It compares the search trajectories of FOREAGENT against AIDE across each specific task and the aggregated Micro Average. Task Paradigm Pairs (N ) DeepSeek-V3.2 GPT-5.1 Classification (CLS) Regression (REG) Matching (MAT) Ranking (RNK) Segmentation (SEG) Extraction (EXT) 15,516 12,685 2,356 2,302 1,639 351 58.90.3 62.10.1 76.60.8 68.30.4 54.80.4 46.94.3 57.20.5 59.20.3 74.90.3 55.00.8 58.00.2 44.13. Table 11: Performance breakdown by specific Task Paradigms. This table expands on the main results by separating the Others category into Ranking, Matching, Segmentation, and Extraction. Values: Mean Accuracy (%) Stdev. Task Name Domain Status AIDE (Baseline) ForeAgent (Ours) Seen Tasks (In-Distribution) Stanford COVID Vaccine Statoil Iceberg Classifier Ventilator Pressure Prediction Unseen Tasks (Out-of-Distribution) Biology Geoscience Physics Seen Seen Seen 1.0000.000 0.4750.161 0.3080.041 1.0000.000 0.5310.134 0.2950. Aerial Cactus Identification* Ecology Histopathologic Cancer Detection* Medicine Unseen Unseen 0.6980.157 0.9920.000 0.8770.000 0.9920.001 Average Beat Ratio Across 5 AI4Science Tasks 0.6950.298 0.7390.295 Table 12: Main results on the MLE-bench AI4Science subset. We report the Beat Ratio (percentage of human contestants outperformed) averaged over 3 independent runs. The * denotes tasks outside the main evaluation distribution. Bold indicates the best performance. Solution 0: complex Deep Neural Network (DNN) with Cross-Attention. Solution 1: robust LightGBM ensemble. Intuitively, human evaluators and models relying solely on code complexity heuristics often exhibit complexity bias by favoring the deep learning approach under the assumption that greater architectural depth yields better performance. World Model Reasoning. However, the World Model leverages the generated Data Analysis Report to detect critical mismatch. The report highlights that the dataset is relatively small (N 5.5k samples) with skewed targets. Synthesizing this finding with model design principles, the World Model predicts high risk of overfitting for the complex DNN. Consequently, it correctly prioritizes the LightGBM ensemble (Solution 1), determining that the gradient boosting approach offers superior Data-Model Fit for this specific sample size. D.2 Case II: Sample of the Verbal Data"
        },
        {
            "title": "Report",
            "content": "Figure 10 presents representative sample of the Verbal Data Report (Drep) generated for the US Patent Matching task. This artifact visualizes the mechanism described in Section 3.4: transforming raw execution logs (e.g., text length statistics, label skew) into semantic narratives. It serves as the grounding anchor that allows the language model to read and internalize dataset properties without direct access to the raw files. D.3 Case III: Sample of the Task Instruction (I) Finally, to visualize the input definition provided in Section 2.1, Figure 11 displays the raw Task Instruction (I) for the task Denoising Dirty Docs. This prompt encapsulates the natural language description, specific dataset paths, and optimization goals, acting as the initial state that triggers the agents autonomous loop."
        },
        {
            "title": "E Prompt Templates",
            "content": "To ensure reproducibility and transparency, we provide the full prompt templates used in our World Model framework. The workflow consists of four key stages: 1. Data Analysis Code Generation (Figure 12): The agent is first instructed to generate robust Python script for profiling the dataset. Task / Aggregation Metric: Average Accuracy Solution Evolution Consistency Global Pairwise Consistency AIDE (Baseline) FOREAGENT AIDE (Baseline) FOREAGENT Stanford Covid Vaccine Statoil Iceberg Classifier Ventilator Pressure Prediction Aerial Cactus Identification* Histopathologic Cancer Detection* 0.9500.030 0.9750.043 0.7500.000 0.3480.399 1.0000.000 0.6440.258 0.8400.073 0.9050.165 0.4720.411 1.0000. 0.8170.067 0.7920.036 0.3930.556 0.7580.108 1.0000.000 0.6790.179 0.8130.024 0.9260.128 0.6960.125 0.8890.193 Overall Average 0.7790.337 0.7560.280 0.7410. 0.8010.159 Table 13: Decision Fidelity Analysis (AIDE vs. FOREAGENT). We report the mean and standard deviation using the format mean (std). Solution Evolution Consistency measures the reliability of decisions along the iterative evolution chain, while Global Pairwise Consistency measures the ranking quality of the entire search trajectory. The best results are highlighted in bold. Figure 8: Domain and Task Sensitivity Analysis. The stacked bar chart presents the data representation study for each individual task. It visualizes the incremental performance impact of adding Raw Data, Numerical Statistics, and Verbal Reports to the Code-only baseline. The tasks are grouped by their respective domains (CV, NLP, and Data Science) to highlight domain-specific sensitivity. This step extracts key statistical meta-features without training model. 2. Data Analysis Report Generation (Figure 13): Based on the execution logs from the previous step, the agent summarizes the findings into structured, causal report. This report serves as critical context for the reasoning engine. 3. Result Prediction Query (Figure 14): This is the core reasoning prompt where the World Model predicts the relative performance of candidate solutions. It integrates the task description, the generated data analysis report, and the solution code to form grounded judgment. 4. Complexity Scoring (Figure 15): An auxiliary prompt used to calculate the complexity heuristic baseline. It evaluates solutions across code engineering, model architecture, and data pipeline dimensions to detect potential bias towards complexity. The specific prompt templates are illustrated below. Case Study: Human Intuition v.s. World Model Inference Task: Google Quest Challenge (Multi-label Subjective QA Regression). Data Profile: Small scale (Ntrain 5.5k) with 30 heavily discretized and skewed targets. Data Report Insight: Strong Question-Group structure (85% of target variance is explained by Q-means) creates high Leakage Risk in random splits. Solution 0: Deep Learning w/ Cross-Attention Solution 1: LightGBM Ensemble Stack: SentenceTransformer + MultiHead-CrossAttn + AdamW Setup: Holdout Validation (Random Split), MSE Loss. Actual Score: 0.2961 (Spearman ρ) Stack: TF-IDF + TruncatedSVD + LightGBM Setup: 5-Fold Cross Validation + Weighted Ensemble Optimization. Actual Score: 0.3145 (Spearman ρ) Human Intuition Human Intuition \"Neural Network with Cross-Attention explicitly models QA interaction. Pre-trained embeddings capture superior semantics compared to statistical features.\" Verdict: Strong Favorite \"TF-IDF is outdated. LightGBM is typically for tabular data, not text. The method is too simple, likely to underfit complex modern natural language processing tasks.\" Verdict: Weak Baseline World Model Critique World Model Insight \"Model is underconstrained for small data (5.5k). Random split ignores question groups, causing leakage. High overfitting risk.\" Verdict: Overfitting Risk \"Robust choice. 5-Fold CV provides honest estimates. SVD captures global label families. Sampleefficient given data scarcity.\" Verdict: Optimal Fit Outcome: Solution 1 outperformed Solution 0. The World Model correctly prioritized Validation Rigor and Sample Efficiency over Architectural Sophistication. Figure 9: Case Study: Human Intuition vs. World Model Inference. This example illustrates hidden logical conflict where architectural sophistication (favored by humans) clashes with data constraints. By leveraging the generated Data Report, the World Model detects critical mismatch between the small dataset size (N 5.5k) and the complex neural network (Solution 0). It correctly prioritizes the robust LightGBM ensemble (Solution 1), demonstrating the ability to weigh Data-Model Fit over pure algorithmic complexity. Case Study: Verbal Data Report (Drep) Sample for Task US Patent Matching ## Data Overview Train: 32,825 pairs; Test: 3,648 pairs. {0.0... 1.0}). No missing values... 106 unique 4-character CPC contexts; coverage across major sections... train/test. Anchors: 733 unique; heavily reused (mean 45 pairs per anchor...). unique... Test anchors: 100% seen... id, anchor... Test targets: 29% seen... Columns: Test OOV rate 12.5%. score (train only; in broad and similar in Targets: 26,850 Why this structure matters: The evaluation setting is anchor-centric... boundaries and context-conditioned mappings... rewards learning anchor-specific decision Implication: ## Key Statistical Findings Discrete labels concentrated at 0.25 and 0.5... moderate similarity... Correlation with score: char 3-5gram TF-IDF cosine 0.46... explains much variance but not all... Phrases are very short (mean 2 tokens...)... token/subword and character patterns... Context means vary... Implication: context-dependent... Anchors average 45 target pairs... decision boundary... Distributions are stable... Implication: targets... Implication: Implication: the mapping from lexical similarity to score is each anchor induces nontrivial local generalization hinges on handling unseen strong class imbalance toward Implication: surface overlap models will rely on align with dominant signal... Rank-sensitive ## Implications for Model Design (Linking observation modeling implication evaluation impact) Loss functions that ignore ordinality may mis-penalize near misses... metrics will reward monotonic mappings... Architectures emphasizing character/subword patterns... Performance differences will emerge in tail cases... Pairwise encoders that allow rich anchor-target interactions can learn... exploit anchor identity will likely score well... Models benefit from mechanisms that allow interaction between context and similarity features... High-capacity sequence models may be underutilized on such short inputs... Efficiency-capacity trade-offs skew toward models effective on short spans... Representations that degrade gracefully on unseen words... handling of OOV will particularly improve performance... Simple identity detection captures some easy gains... still have scores <1.0... Character CNNs... Token-level transformers... Denoising pretraining... Per-context performance may vary... Independent encoders... have an advantage... however, near-identical forms can Robust Joint encoders... Approaches that ## Summary The dataset is anchor-centric... Evaluation emphasizes generalization to new targets... Capture strong character/subword overlap signals... Maintain robustness to unseen target tokens... Avoid over-reliance on identity heuristics... Account for label ordinality... Surface-form similarity explains large portion... Figure 10: Case Study: Verbal Data Report (Drep) Sample for US Patent Matching. Generated via the Code-Execution-Verbalization protocol, this artifact bridges the gap between raw data statistics and semantic reasoning. Case Study: Task Instruction (I) for Task Denoising Dirty Docs OCR makes previously static content editable, searchable, # Overview ## Description Optical Character Recognition (OCR) is the process of converting typed or handwritten documents into digitized format. and easier to share. This task focuses on improving OCR performance for degraded scanned documents. dataset of noisy text images, the goal is to develop model that removes visual noise such as stains, fading, and wrinkles, producing cleaner text images suitable for further processing and digitization. ## Evaluation Submissions are evaluated using the root mean squared error (RMSE) between the predicted cleaned pixel intensities and the ground truth grayscale pixel intensities. ### Submission Format Each image is represented as list of pixel values with identifiers in the form image_row_col (e.g. (black) to 1 (white). ``` 1_2_1 for image 1, row 2, column 1). Given Intensity values range from 0 id,value 1_1_1,1 1_2_1,1 1_3_1,1 ... ``` train and test. ## Data ### Dataset Description The dataset contains two sets of images: noisy text images and their corresponding cleaned versions (train_cleaned). set contains only noisy images that need to be denoised. The noise simulates real-world artifacts commonly seen in scanned documents, such as blur, stains, and faded ink. The task is to build model that restores the test images to clean, readable form. ### File Description - There are three directory corresponding to the data description above: train_cleaned and test. - The sample submission is stored in sampleSubmission.csv. - The train set includes train, - The test Figure 11: Case Study: Task Instruction (I) for Task Denoising Dirty Docs. This example illustrates the raw natural language input as defined in Section 2.1. It outlines the problem context, dataset specifications, and evaluation criteria, serving as the foundational prompt that initiates the agents solution generation process. Prompt: Data Analysis Code Generation SYSTEM: You are an expert Data Science Architect specializing in automated dataset profiling and meta-learning. high-level statistical and structural insights from dataset without performing full model training. Your goal is to write robust, error-handling Python script that extracts The script must determine the correct data type (Tabular, CV, NLP, or {data-dir} {task-desc} USER: need you to generate Python script to analyze dataset for the following machine learning task. Context: Task Description: Data Directory: Requirements for the Python Script: Data Loading & Robustness: Time-Series) based on the Task Description and file extensions in data-dir. robust file loading (e.g., using try-except blocks). sampling (load max 10k rows or 1000 images). Key Metric Extraction (Crucial for World Model Prediction): and print meta-features that correlate with model difficulty. Output Format: human-readable text format (or JSON structure) that downstream LLM can easily parse to write report. Constraints: Use only standard libraries: glob. Do not attempt to train any machine learning models (e.g., do not run Random Forest). Response: The script must print the analysis results to stdout in structured, Provide only the executable Python code block. Only generate text logs/stats. Do not generate plots/images. Do not just print raw data. pandas, numpy, scipy, sklearn, PIL (for images), os, If files are too large, perform stratified Implement strictly Calculate Figure 12: Prompt used to instruct the LLM for generating data analysis code. Prompt: Data Analysis Report Generation You are preparing structured Data Analysis Report that will be provided to another expert LLM. Your goal is to make the data characteristics and their implications explicit, causal, and model-relevant so that model evaluation agent can reason about how the dataset properties interact with model design choices. Follow these instructions carefully: 1. Summarize, dont just restate numbers. - Extract key quantitative trends (e.g., mean intensity, noise variability, contrast, etc.). - Highlight patterns, anomalies, and dataset biases. 2. Establish causal implications for modeling. - For each key observation, explain why it matters for model training, architecture, or generalization. - Example: High inter-sample heterogeneity suggests the model should include normalization or data augmentation to handle distribution shift. 3. Bridge data to model choices. - Express potential advantages or risks for different architectures (CNNs, transformers, denoising autoencoders, etc.) given the observed data patterns. - DONT directly suggest which model / method is better. advantages or risks. 4. Directly suggesting models will strongly result in bias. - DONT directly suggest which model / method is better. advantages or risks. 5. Maintain clear structure using the following format: You only need to analyze the potential You only need to analyze the potential ## Data Overview <summary of dataset structure, splits, file composition> ## Key Statistical Findings <highlighted numeric findings + what they imply> ## Implications for Model Design <how these data patterns affect likely model performance> ## Summary <concise conclusion connecting data traits to modeling priorities> 6. Tone and length: - Write concisely and analytically (like scientific data report). - Do not include raw metrics dumps. - Focus on interpretability and causal reasoning. Your output will serve as the {<data_analysis>} section for reasoning-based model evaluator. Ensure every insight has clear link from data observation modeling implication evaluation impact. INPUT CONTEXT: [Task Name] {task} [Task Description] {desc-block} [Raw Data Analysis Extraction] {analysis-text} RESPONSE: Produce the final structured report now. recommending specific models; only analyze potential advantages or risks. Follow the required headings exactly. Avoid Figure 13: Prompt used to instruct the LLM for generating data analysis report from the code execution result. Prompt: Result Prediction Query SYSTEM: You are an ML code and data analysis expert tasked with predicting the relative performance of provided ML solutions without executing any code. and the shown code snippets only. reasoning before the final answer. the specified response format. Base your judgment on the task description Never assume external ground-truth. End your answer with single JSON object that strictly matches You should include brief USER: Task: {task-name} Task description: {task-desc} Data analysis: {data-analysis-report} Important instructions: - Predict which solution will perform best (or provide full ranking) WITHOUT running code. - Use only the task description, data analysis, and code snippets below. - Treat the task description and data analysis as equally important to the code; analyze them separately, surface their underlying implications, and provide balanced, trade-off judgment. - Connect data analysis to the following code analysis: If data analysis indicates properties , explain how the architecture addresses them , forming datawhymethod choice causal chain. - Forbid the complexity-wins shortcut: Do not claim deeper/more complex/with attention is better as the sole reason. training details, and provide counterexample scenario. - Response format: - Indices correspond to the order of the listed solutions (0..n-1). - You should include brief reasoning before the final JSON. End with single JSON object matching the response format. If used, justify why it holds under the current data distribution and Do not write anything after the JSON. {\"predicted_best_index\": <0 or 1>, \"confidence\": <optional float>} Provided solutions: Solution 0: path={code-0-path} {code-snippet-0} Solution 1: path={code-1-path} {code-snippet-1} Figure 14: Prompt used to instruct the LLM for predicting the result of the provided materials. Prompt: Complexity Scoring Query SYSTEM: You are an expert Machine Learning Engineer and Researcher. script for machine learning task and evaluate its complexity based on three specific dimensions. You must output JSON object with three scores (integers from 1 to 10) and brief reasoning for each. Your task is to analyze Python The dimensions are: 1. code_engineering_score (1-10): custom loops vs clean API calls. 2. model_arch_score (1-10): (e.g., Transformer > Simple CNN). 3. data_pipeline_score (1-10): TTA), custom sampling logic. Cyclomatic complexity, custom logic, dependence depth, messy Parameter count, FLOPs, depth of network, novelty of architecture Complexity of preprocessing, data augmentation strategies (Mixup, Output Format: { \"code_engineering_score\": \"model_arch_score\": \"data_pipeline_score\": \"reasoning\": \"<short summary>\" <int>, <int>, <int>, } USER: Analyze the following Machine Learning code and provide complexity scores. {code_snippet} Respond ONLY with the valid JSON. Figure 15: Prompt used to instruct the auxiliary LLM for scoring the complexity of code solutions across three dimensions. This heuristic is used as baseline to evaluate whether the World Model blindly favors complex code."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Zhejiang University",
        "Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph"
    ]
}