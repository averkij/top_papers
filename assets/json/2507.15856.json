{
    "paper_title": "Latent Denoising Makes Good Visual Tokenizers",
    "authors": [
        "Jiawei Yang",
        "Tianhong Li",
        "Lijie Fan",
        "Yonglong Tian",
        "Yue Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share a conceptually similar training objective -- reconstructing clean signals from corrupted inputs such as Gaussian noise or masking -- a process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as a fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 6 5 8 5 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Latent Denoising Makes Good Visual Tokenizers",
            "content": "Jiawei Yang1 Tianhong Li2 Lijie Fan3 Yonglong Tian4 Yue Wang1 1USC 2MIT CSAIL 3Google DeepMind 4OpenAI"
        },
        {
            "title": "Abstract",
            "content": "Despite their fundamental role, it remains unclear what properties could make visual tokenizers more effective for generative modeling. We observe that modern generative models share conceptually similar training objectivereconstructing clean signals from corrupted inputs such as Gaussian noise or maskinga process we term denoising. Motivated by this insight, we propose aligning tokenizer embeddings directly with the downstream denoising objective, encouraging latent embeddings to be more easily reconstructed even when heavily corrupted. To achieve this, we introduce the Latent Denoising Tokenizer (l-DeTok), simple yet effective tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking. Extensive experiments on ImageNet 256256 demonstrate that our tokenizer consistently outperforms standard tokenizers across six representative generative models. Our findings highlight denoising as fundamental design principle for tokenizer development, and we hope it could motivate new perspectives for future tokenizer design. Code is available at: https://github.com/Jiawei-Yang/DeTok"
        },
        {
            "title": "Introduction",
            "content": "Modern visual generative models commonly operate on compact latent embeddings, produced by tokenizers, to circumvent the prohibitive complexity of pixel-level modeling [51, 46, 5, 36]. Current tokenizers are typically trained as standard variational autoencoders [33], primarily optimizing for pixel-level reconstruction. Despite their critical influence on downstream generative quality, it remains unclear what properties enable more effective tokenizers for generation. As result, tokenizer development has lagged behind recent rapid advances in generative model architectures. In this work, we ask: What properties can make visual tokenizers more effective for generative modeling? We observe that modern generative models, despite methodological differences, share conceptually similar training objectivereconstructing original signals from corrupted ones. For instance, diffusion models remove diffusion-induced noise to recover clean signals [31, 46], while autoregressive models reconstruct complete sequences from partially observed contexts [36, 5], analogous to removing masking noise [30, 11, 15]. We collectively refer to these reconstruction-fromdeconstruction processes as denoising. This unified denoising perspective of modern generative models suggests that effective visual tokenizers for these models should produce latent embeddings that are reconstructable even under significant corruption. Such embeddings naturally align with the denoising objectives of downstream generative models, facilitating their training and subsequently enhancing their generation quality. Motivated by this insight, we propose to train tokenizers as latent denoising autoencoders, termed as l-DeTok. Specifically, we deconstruct latent embeddings via interpolative noise, obtaining corrupted latents by interpolating original embeddings with Gaussian noise. The tokenizer decoder is then Advisory-only Work done prior to joining OpenAI trained to reconstruct clean images from these heavily noised latent embeddings. Additionally, we explore random masking, similar to masked autoencoders (MAE) [30], as an alternative form of deconstruction and find it similarly effective. Conceptually, these deconstruction-reconstruction strategies encourage latent embeddings to be robust, stable, and easily reconstructable under strong corruption, aligning with the downstream denoising tasks central to generative models. Indeed, our experiments show that stronger noise (i.e., strong latent noise and high masking ratio) used in our l-DeTok training usually leads to better downstream generative performance. We demonstrate the effectiveness and generalizability of l-DeTok across six representative generative models, including non-autoregressive (DiT [46], SiT [42], LightningDiT [64]) and autoregressive models (MAR [36], RasterAR, RandomAR [36, 44, 68]) on the ImageNet generation benchmark. We find that compared to recent semantics-distilled tokenizers [64, 6], which achieve strong performance on diffusion models but generalize poorly to autoregressive ones, our tokenizer demonstrates significantly broader generalizability. For example, by adopting our tokenizerwithout modifying model architectureswe push the limits for MAR models [36], improving FID from 2.31 to 1.55 for MAR-B, matching the performance of the original huge-sized MAR (1.55). For MAR-L, FID improves from 1.78 to 1.35. Importantly, these gains come without semantics distillation, thus avoiding dependencies on visual encoders pretrained at far larger scale [43, 49]. In summary, our work demonstrates simple yet crucial insight: explicitly incorporating denoising objectives into tokenizer training significantly enhances their effectiveness for generative modeling since it is downstream task-aligned. We hope this perspective will stimulate new research directions in tokenizer design and accelerate future advances in generative modeling."
        },
        {
            "title": "2 Related Work",
            "content": "Representation learning in visual recognition. Representation learning has been decades-long pursuit in visual recognition, aiming to discover transferable embeddings that generalize across diverse downstream tasks [2]. Starting from supervised representations [17, 24, 65], self-supervised methods have significantly pushed the boundaries of transferability without explicit human annotations [29, 9, 27, 30, 43, 11, 4]. At the core of these approaches lies foundational principle: pre-training should encourage representations to encode the information most relevant to downstream tasks. This principle has inspired the design of diverse and effective pretext tasks, such as instance discrimination [29, 9, 3], self-distillation [27, 10, 4, 43], and masked-image reconstruction [30, 43, 72], each aligning representations with downstream utility. These insights motivate us to explore tokenizer embeddings that align with downstream generative tasks. Visual tokenizers for generative modeling. Modern generative models typically rely on tokenizers to encode images into compact latent embeddings, significantly reducing computational complexity compared to pixel-level modeling [19, 51, 46, 34, 55, 69, 37, 7, 1, 12, 48]. While conventional tokenizers optimize pixel reconstruction with KL-regularization [35], recent approaches [6, 64, 7, 37] have advocated semantics distillation from powerful pretrained vision models [43, 49]. However, such semantics-distilled tokenizers rely inherently on two-stage pipeline: first training vision encoder at significantly larger scale in terms of compute and data, then distilling its features into latent embeddings. Moreover, our experiments reveal surprising limitation  (Table 2)  : semanticsdistilled continuous tokens enhance non-autoregressive diffusion model generation but notably degrade autoregressive methods, questioning the common assumption that tokenizer improvements in one generative paradigm transfer to others. In contrast, our l-DeTok generalizes better, consistently benefiting diverse modelsyet without relying on semantics distillation. Generative modeling frameworks. Generative modeling encompasses various downstream generation frameworks and can be broadly categorized into autoregressive (AR) [8, 19, 5, 36, 59, 21, 44, 68] and diffusion-based non-autoregressive (non-AR) [31, 51, 46, 42, 64] methods. AR models factorize latent representations sequentially, predicting tokens step-by-step conditioned on partially generated contexts, while non-AR models jointly predict all latent tokens through iterative refinement, typically leveraging diffusion [31, 56] or flow-based processes [39, 20]. Despite their methodological diversity, these generative paradigms depend on the quality of latent embeddings produced by upstream tokenizers [28]. This inherent dependence highlights the critical importance 2 of studying latent embeddings: improvements at the tokenizer level have the potential to yield better generative performance across the full spectrum of downstream generative frameworks."
        },
        {
            "title": "3 Method",
            "content": "Our goal is to design visual tokenizers that are more effective for generative modeling compared to standard autoencoders trained for pixel reconstruction. This section first revisits the core training objective shared by all modern generative models, i.e., denoising, to motivate our design, and then details our latent denoising tokenizers (l-DeTok)."
        },
        {
            "title": "3.1 Preliminaries of Generative Modeling",
            "content": "Modern generative frameworks can be primarily divided into non-autoregressive (non-AR) and autoregressive (AR) paradigms. Despite their methodological differences, both paradigms aim to gradually reconstruct the original representations from deconstructed ones. Non-autoregressive generative models. Non-autoregressive models, exemplified by diffusion [31, 46] and flow-matching methods [39, 42], learn to iteratively refine latent representations deconstructed by controlled noise. Given latent representation of an image X0, the forward noising process progressively corrupts these latents into Xt: Xt = a(t) X0 + b(t) ϵt, ϵt (0, I). (1) where a(t) and b(t) are noise schedules. Generative models are trained to revert this deconstruction: L(θ) = EX,ϵ,t ϵθ(Xt, t) ϵt2(cid:105) (cid:104) , (2) where ϵθ is learnable noise estimator parameterized by θ. Essentially, non-AR diffusion models learn to reconstruct original latents from intermediate latents deconstructed by noise. Autoregressive generative models. Autoregressive approaches factorize image generation into sequential prediction problem. Given an ordered sequence of latent tokens {x1, . . . , xN }, AR methods factorize the joint distribution as: pθ(x) = (cid:89) i=1 pθ(xix1, . . . , xi1), (3) where xi denotes the latent tokens generated at step i. Recent generalized AR variants extend this framework to arbitrary generation orders [36, 68, 44] or set-wise prediction strategies [59, 50]. Nonetheless, the fundamental training objective remains consistent: reconstructing full sequences from partially observedor equivalently, partially maskedcontexts. In other words, AR models learn to reconstruct original latents from intermediate latents deconstructed by masking."
        },
        {
            "title": "3.2 Latent Denoising Tokenizers",
            "content": "Motivated by the discussions above, we propose latent denoising tokenizer (l-DeTok), simple tokenizer trained by reconstructing original images from deconstructed latent representations. This deconstruction-reconstruction design aligns with the denoising tasks employed by modern generative models. Figure 1 shows an overview of our method. We detail each component next. Overview. Our tokenizer follows an encoder-decoder architecture based on Vision Transformers (ViT) [18, 61]. Input images are divided into non-overlapping patches, linearly projected into embedding vectors, and added with positional embeddings. During training, we deconstruct these embeddings using two complementary strategies: (i) injecting noise in latent embeddings and (ii) randomly masking image patches. The decoder reconstructs original images from these deconstructed embeddings. This strategy encourages easy-to-reconstruct latent embeddings under heavy corruption, aiming to simplify downstream denoising tasks in generative models. Noising as deconstruction. Our core idea is to deconstruct latent embeddings by noise interpolation. Specifically, given latent embeddings from the encoder, we interpolate them with Gaussian noise as follows: = (1 τ )x + τ ε(γ), where ε(γ) γ (0, I), τ U(0, 1). (4) 3 Figure 1: Our latent denoising tokenizers (l-DeTok) framework. During tokenizer training, we randomly mask input patches (masking noise) and interpolate encoder-produced latent embeddings with Gaussian noise (interpolative latent noise). The decoder processes these deconstructed latents and mask tokens to reconstruct the original images in pixels. We refer to this process as denoising. When serving as tokenizer for downstream generative models, both noises are disabled. Here, the scalar γ controls noise standard deviation, and the factor τ specifies noise level. Critically, this interpolative strategy differs from the conventional additive noise, i.e., = + τ ε, employed by standard VAEs [47] or DAEs [62], as it ensures latents can be effectively and heavily corrupted when the noise level τ is high. Moreover, random sampling of τ encourages latents to remain robust across diverse corruption levels. Unlike traditional DAEs [62], which apply additive noise directly in pixel space, our latent-space interpolation directly aligns with downstream generative models that operate in latent embedding spaces. At inference time, latent noising is disabled (τ = 0). Masking as deconstruction. We further generalize our denoising perspective by interpreting masking as another form of latent deconstruction [15, 11, 30]. Inspired by masked autoencoders (MAE) [30], we randomly mask subset of image patches. Different from MAE, we use random masking ratio. Concretely, given an input image partitioned into patches, we mask random subset, where the masking ratio is sampled from slightly biased uniform distribution: = max(0, U(0.1, )), (5) where U(0.1, ) denotes uniform distribution on [0.1, ]. The slight bias towards zero reduces the distribution gap between training and inference (no masking). The encoder processes only the visible patches, and masked positions are represented by shared learnable [MASK] tokens at the decoder input. At inference time, all patches are visible (m = 0). Training objectives. Our decoder reconstructs the original images from corrupted latent embeddings. The training objective follows established practice [51, 19, 69], combining pixel-wise mean-squared-error (MSE), latent-space KL-regularization [47], perceptual losses (VGG- [54] and ConvNeXt-based [40] as in [69]), and an adversarial GAN objective [25]: Ltotal = LMSE + λKLLKL + λpercepLpercep + λGANLGAN, (6) where each λ controls the contribution of the corresponding loss component."
        },
        {
            "title": "Implementation",
            "content": "We describe the implementation details, including datasets, evaluation metrics, tokenizer and generative model training procedures. Dataset and metrics. We perform all experiments on ImageNet [52] at 256256 resolution. Images are center-cropped, randomly horizontally flipped during training, and normalized to [1, 1]. We evaluate generative models using Frechet Inception Distance (FID), Inception Score (IS), precision and recall, following [16]. For tokenizer reconstruction, we report reconstruction-FID (rFID). Tokenizer baselines. We benchmark our tokenizer against diverse set of publicly available tokenizers: (i) MAR-VAE from MAR [36], trained on ImageNet using the implementation from [19]; (ii) VA-VAE [64], aligning latent embeddings with DINOv2 features [43]; (iii) MAETok [6], distilling HOG [13], DINOv2 [43], and CLIP [49] features through auxiliary decoders; (iv) SD-VAE from Stable-Diffusion [51], trained on significantly larger datasets. Additionally, for controlled comparisons, we also train our own baseline tokenizer without the proposed denoising approaches. 4 Our tokenizer implementation. We implement our tokenizer using ViTs for both encoder and decoder [61, 18]. We adopt recent architectural advances from LLaMA [60], including RoPE [57] (together with learned positional embeddings following [22]), RMSNorm [71], and SwiGLU-FFN [53]. The encoder operates at patch size of 16, yielding 256 latent tokens for each 256 256 image, while the decoder uses patch size 1 as there is no resolution change. The latent dimension is set to 16. We omit the [CLS] token for simplicity. Tokenizer training. Our tokenizer is trained using the weighted loss defined in Eq. 6, with default weights λKL = 106, λpercep = 1.0, and λGAN = 0.1. In ablation studies, we use ViT-S for the encoder and ViT-B for the decoder, disable the GAN loss, and train for 50 epochs. We observe that including GAN loss [66, 25, 19] sharpens reconstructions but roughly doubles training time, without altering result trends. For final experiments, we use ViT-B for both encoder and decoder, train for 200 epochs, and activate the GAN loss starting from epoch 100. All tokenizers use AdamW [41] with global batch size of 1024 and peak learning rate of 4.0 104 (corresponding to base learning rate of 1.0 104 scaled linearly by the global batch size divided by 256 [26]), linear warm-up, and cosine learning schedule. Further implementation details are provided in the Appendix. Generative models. To evaluate the broad effectiveness of tokenizer, we experiment with six representative generative models, including three non-autoregressive models: DiT [46], SiT [42], and LightningDiT [64]; and three autoregressive models: MAR [36], causal RandomAR, and RasterAR based on RAR [68] and diffloss [36]. We follow the officially released implementations to reimplement all methods within unified codebase, standardizing training and evaluation across methods. Previous works on visual tokenizers often exclusively evaluate on non-AR models (e.g., DiT, SiT), yet we find improvements from non-AR models do not necessarily translate to AR models (more on this later). Our experiments aim to provide useful data points toward more universal tokenizer. Generative model training. We use standardized training recipe for all generative models. Specifically, we follow the hyperparameters from [64], training generative models with global batch size of 1024, using AdamW [41] with constant learning rate of 2 104, without warm-up, gradient clipping, or weight decay. Autoregressive models adopt the three-layer 1024-channel diffloss MLP from [36]. For ablation studies, we train generative models for 100 epochs. For larger-scale experiments on MAR models, we train them for 800 epochs. All models utilize exponential moving average (EMA) with decay rate of 0.9999. We always standardize tokenizer outputs by subtracting the mean and dividing by the standard deviation, both computed from the ImageNet training set. For publicly available tokenizers, we use their official standardization steps. Unless noted otherwise, we report FID@50k scores with classifier-free guidance (CFG), with optimal CFG scales searched from FID@10k results. We abbreviate FID@50k as FID."
        },
        {
            "title": "5 Experiments",
            "content": "We now empirically analyze our proposed tokenizer, beginning with series of ablation studies on noising strategies, followed by comprehensive comparisons across various generative modeling frameworks. Lastly, we benchmark against leading systems."
        },
        {
            "title": "5.1 Main Properties",
            "content": "We study the generalizability of tokenizer across non-autoregressive (non-AR) and autoregressive (AR) generative paradigms. To this end, we use SiT-B [42] and MAR-B [36] as representative nonAR and AR models, respectively. For all ablation studies, we adopt small encoders and base-sized decoders in our tokenizers."
        },
        {
            "title": "5.1.1 Properties of Latent Noising",
            "content": "We first study latent noising as the sole form of deconstruction, without any masking. Interpolative vs. additive noise. An important design choice in our l-DeTok is the use of interpolative latent noise instead of additive noise, which we ablate here. Specifically, we compare two latent noising variants: interpolative noise, i.e., = (1 τ )x + τ ε (Eq. 4), and additive noise, i.e., = + τ ε. We set the noise standard deviation to γ = 1.0 here. Figure 2-(a) presents the results. 5 Figure 2: Ablation on latent noise design. (a) Additive vs. interpolative noise. Interpolative noise clearly outperforms additive noise for both MAR [36] and SiT [42]. Both interpolative and additive latent noise lead to improved performance for MAR. (b) Latent noise standard deviation (γ). Our l-DeTok remains robust across various noise standard deviations. Generally, increasing γ improves generation quality, with best results achieved around γ = 3.0. Interpolative noise clearly outperforms additive noise for both SiT [42] and MAR [36]. This aligns with our expectation: interpolative noise explicitly ensures latent embeddings are heavily corrupted at high noise levels. In contrast, additive noise can potentially create shortcuts by making original signals remain dominant, reducing the effectiveness of the additive noise. Nonetheless, we observe the additive latent noise still improves generative performance for MAR but not for SiT. Understanding the underlying factors responsible for these model-specific differences would be an intriguing future direction, e.g., joint vs. factorized latent distribution estimation. Noise standard deviation. Figure 2-(b) studies the effect of noise standard deviation (γ in Eq. 4). Both SiT and MAR consistently improve with interpolative latent noise across all tested levels. Performance peaks at moderately high standard deviation, indicating that stronger corruption generally yields more effective latents. This result confirms our key hypothesis: challenging denoising tasks naturally produce robust, downstream-aligned latents that benefit generative modeling."
        },
        {
            "title": "5.1.2 Properties of Masking",
            "content": "We next investigate masking noise independently, without any latent noise. Masking ratio. We examine how varying the maximal masking ratio (Eq. 5) influences generation quality. As shown in Figure 3-(a), both SiT and MAR benefit from masking-based tokenizer training in generation quality. Masking ratios between 70% and 90% consistently yield stronger performance compared to low masking ratios (e.g., 30%), favoring high degrees of masking. This behavior mirrors observations from latent denoising, i.e., challenging denoising is more beneficial, indicating common underlying principle under the deconstruction-reconstruction strategy. Interestingly, SiT [42] still benefits from our masking-based l-DeTok despite not explicitly handling masked inputs during its own training. We hypothesize that masking implicitly promotes encoders to learn embeddings that are inherently robust other noise types, such as diffusion-based perturbations. Constant vs. randomized masking ratio. Figure 3-(b) compares randomized masking ratios against constant ones used in MAE [30]. For constant masking ratio experiments, we fine-tune the tokenizer decoder for an additional 10 epochs on full-visible latents to mitigate the distribution mismatch between training and inference, since fully visible inputs are absent during constant-ratio training. From Figure 3-(b), we see that randomized masking outperforms constant masking. Ran6 Figure 3: Ablation on masking ratio. We show generative performance (FID) with varying masking ratios for tokenizers trained with (a) random and (b) constant masking ratio. Both MAR [38] (top) and SiT [42] (bottom) benefit from masking-based tokenizers, favoring heavy masking (70% to 90%). Notably, randomized masking consistently outperforms constant masking. domized masking encourages latent embeddings that are robust to varying corruption levels. This is more aligned with the downstream tasks, i.e., denoising across diverse levels of corruption."
        },
        {
            "title": "5.1.3 Joint Denoising",
            "content": "Prior ablations indicate that both latent noising and masking can independently improve generation quality, with latent noising showing stronger effect. Here, we investigate the effect of joint denoising. Based on prior results, we fix the noise standard deviation to γ = 3.0 and the masking ratio to = 0.7. Figure 4 and Table 1 summarize the results. With joint denoising, our l-DeTok achieves FID scores of 5.50 (SiT-B) and 2.65 (MAR-B) with CFG  (Table 1)  . In comparison, our baseline tokenizertrained with identical settings but without any noiseobtains significantly worse results: 6.97 (SiT-B) and 3.31 (MAR-B). We observe that joint denoising is more effective for MAR, further pushing both FID and IS, but provides limited additional benefit for SiT when latent noising is already applied. This indicates that latent denoising is essential, while the masking-based denoising can be optional. Lastly, with joint denoising, we increase the encoder to base size, train for 200 epochs, and enable the GAN loss starting from epoch 100 (+Extended in Table 1). Under this setting, performance improves to 5.13 (SiT) and 2.43 (MAR). We adopt this improved tokenizer for all subsequent evaluations."
        },
        {
            "title": "5.2 Generalization Experiments",
            "content": "To comprehensively evaluate tokenizer generalizability, we compare performance across six representative generative modelsthree non-autoregressive (DiT [46], SiT [42], LightningDiT [64]) and three autoregressive (MAR [36], RandomAR, RasterAR)using publicly available tokenizers introduced before. RandomAR and RasterAR are Transformer-based decoder-only (causal attention) variants adapted from RAR [68]. We modify them to support decoding continuous tokens via diffloss MLPs [36]. RandomAR generates tokens in random order while RasterAR uses raster-scan order. We use base-sized models and train them for 100 epochs for experiments here. Comparisons with standard convolutional tokenizers. Table 2 presents the results. Our l-DeTok tokenizer consistently outperforms conventional tokenizers [51, 19, 45], by large margins across both AR and non-AR models. Compared to the best existing tokenizer (MAR-VAE from [36]), our method significantly improves FID (with CFG) from 3.71 to 2.43 (34%) for MAR, from 11.78 7 Figure 4: Impact of tokenizer training strategies on generative performance. We compare our baseline tokenizer with our l-DeTok variants: masking-only (M =0.7), latent noisingonly (γ=3.0), and their combination (joint noising). Both masking and latent noising independently improve generation quality, with latent noising showing stronger effect. Joint noising further improves performance for MAR, particularly in inception scores (IS), but provides limited additional benefit for SiT when latent noising is already applied. FID@50k scores are detailed in Tab. 1. w/o CFG FID 26.82 22.50 16.11 16.03 14.81 Table 1: Effectiveness of denoising. We report FID and IS evaluated on 50,000 images here. Compared to baselines, we see substantial gains in generative models when using our l-DeTok. Extended: larger encoder, longer training, GAN enabled midway through training. MAR-B SiT-B Setup w/ CFG IS FID Baseline 3.31 247.63 Masking only 2.90 243.00 Latent noise only 2.77 249.02 Joint noise 2.65 263.03 +Extended 2.43 266. IS w/ CFG w/o CFG FID FID 16.70 12.57 8.82 7.48 6.72 6.97 181.61 6.43 189.24 5.56 193.52 5.50 195.07 5.13 207.35 Table 2: Generalizability comparison of tokenizers across different generative models. We compare various tokenizers on representative generative models. Our l-DeTok tokenizer outperforms other tokenizers for AR models, and also surpasses standard tokenizers trained without semantics distillation for non-AR models. All results are obtained with optimal CFG scales. Tokenizer rFID MAR Autoregressive Models RandomAR IS FID IS RasterAR IS FID FID Non-autoregressive Models DiT SiT FID IS FID IS Light.DiT IS FID 16.66 144.5 38.13 68.3 15.88 160.5 4.33 221.1 4.91 213.9 2.86 275.1 6.99 201.8 24.83 97.6 15.92 127.2 4.77 243.2 5.24 224.7 3.92 273.3 0.28 0.48 Tokenizers trained with semantics distillation from external pretrained models VA-VAE [64] MAETok [6] Tokenizers trained without semantics distillation SD-VAE [51] MAR-VAE [36] Our l-DeTok 0.61 0.53 0. 4.64 259.8 13.11 141.8 8.26 179.3 7.66 187.5 8.33 179.8 4.24 223.7 3.71 265.3 11.78 147.9 7.99 189.7 6.26 177.5 8.20 171.8 3.98 218.7 2.43 266.5 5.22 248.9 4.46 257.7 5.13 207.3 6.58 173.9 3.63 225.4 to 5.22 (56%) for RandomAR, and from 7.99 to 4.46 (44%) for RasterAR. Improvements for non-autoregressive models are also consistent. These substantial gains directly support our core hypothesis: denosing makes more effective tokenizers for generative models. Comparisons with semantics-distilled tokenizers. Our l-DeTok generalizes significantly better than semantics-distilled tokenizers. Table 2 also compares our method against recent semanticsdistilled tokenizers such as VA-VAE [64] and MAETok [6], which distill semantics features from powerful pretrained encoders [43, 49]. Surprisingly, we empirically find that these tokenizers, despite their promising performance for non-AR models, do not generalize well to AR models. Specifically, FID scores (with CFG) degrade dramatically from 3.71, 11.78, and 7.99 to 16.66, 38.13, and 15.88 for MAR, RandomAR, and RasterAR, respectively. Previous studies implicitly presume that tokenizer improvements observed in one generative paradigm naturally extend to others. Yet, our experiments challenge this assumption, revealing previously unrecognized gap in tokenizer transferability: tokenizer effectiveness in one generative paradigm does not necessarily transfer to others. In sharp contrast, our method generalizes significantly better across both non-AR and AR models. Importantly, our l-DeTok requires no semantics distillation from large-scale pretrained encoders yet considerably surpasses standard tokenizer competitors. This lifts external dependencies and highlights new and previously overlooked direction in tokenizer research. 8 Table 3: System-level comparison on ImageNet 256256 class-conditioned generation. Our approach enables MAR models [36] to achieve leading results without relying on semantics distillation. : With additional decoder fine-tuning (see Sec. A.1 for details). Pre. Rec. FID w/ CFG IS Pre. Rec. FID #params w/o CFG IS With semantics distillation from external pretrained models 157.8 216.5 208.3 205.6 154.7 SiT-XL + REPA [70] SiT-XL + MAETok [6] LightningDiT + MAETok [6] LightningDiT + VAVAE [64] DDT-XL [63] 675M 675M 675M 675M 675M 5.90 2.31 2.21 2.17 6.27 0.70 - - 0.77 0."
        },
        {
            "title": "Without semantics distillation from external pretrained models",
            "content": "ADM [16] VDM++ [32] MaskGIT [5] MAGVIT-v2 [67] LDM-4 [51] DiT-XL/2 [45] SiT-XL/2 [42] MDTv2-XL/2 [23] MaskDiT [23] VAR-d30 [59] LlamaGen-3B [58] RandAR-XXL [44] FlowAR-H [50] CausalFusion [14] MAR-B + MAR-VAE [36] MAR-L + MAR-VAE [36] MAR-H + MAR-VAE [36] MAR-B [36] + our l-DeTok MAR-B [36] + our l-DeTok MAR-L [36] + our l-DeTok MAR-L [36] + our l-DeTok 554M 10.94 2.40 2B 6.18 227M 307M 3.65 400M 10.56 9.62 675M 8.30 675M 5.06 676M 5.69 675M - 2.0B - 3.1B - 1.4B - 1.9B 3.61 676M 3.48 208M 2.60 479M 2.35 943M 2.79 208M 2.94 208M 1.84 479M 1.86 479M 101.0 225.3 182.1 200.5 103.5 121.5 - 155.6 178.0 - - - - 180.9 192.4 221.4 227.8 195.9 195.5 238.4 238.6 0.69 - 0.80 - 0.71 0.67 - 0.72 0.74 - - - - 0.75 0.78 0.79 0.79 0.80 0.80 0.82 0.82 0.69 - - 0.65 0.69 0.63 - 0.51 - 0.62 0.67 - 0.66 0.60 - - - - 0.66 0.58 0.60 0.62 0.60 0.59 0.60 0. 1.42 1.67 1.73 1.35 1.26 4.59 2.12 - 1.78 3.60 2.27 2.06 1.58 2.28 1.92 2.18 2.15 1.65 1.77 2.31 1.78 1.55 1.61 1.55 1.43 1.35 305.7 311.2 308.4 295.3 310.6 186.7 267.7 - 319.4 247.7 278.2 270.3 314.7 276.6 323.1 263.3 322.0 296.5 282.3 281.7 296.0 303.7 289.7 291.0 303.5 304.1 0.80 - - 0.79 0.79 0.82 - - - 0.87 0.83 0.82 0.79 0.89 0.82 0.81 0.79 0.83 0.82 0.82 0.81 0.81 0.81 0.81 0.82 0. 0.64 - - 0.65 0.65 0.52 - - - 0.48 0.57 0.59 0.65 0.61 0.59 0.58 0.62 0.60 0.61 0.57 0.60 0.62 0.62 0.62 0.61 0.62 Figure 5: Qualitative Results. We show selected examples of class-conditional generation on ImageNet 256256 using MAR-L [36] trained with our tokenizer."
        },
        {
            "title": "5.3 Benchmarking with Previous Systems",
            "content": "We compare against leading generative systems in Table 3. For this experiment, we train MAR-B and MAR-L for 800 epochs. Simply adopting our tokenizer, without altering the MAR architecture, substantially improves the generative performance: MAR-B achieves an FID of 1.55 (from 2.31), and MAR-L further improves to 1.35 (from 1.78). Notably, our MAR-B and MAR-L both match or surpass the previously best-performing huge-size MAR model (1.35 vs. 1.55). Qualitative results are provided in Figure 5."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "Limitations. On the general side, our study primarily investigates continuous-valued tokenizers; the effectiveness of our denoising strategy for discrete, vector-quantized (VQ) tokenizers remains an open question. Exploring denoising-based objectives in the context of VQ tokenizers is valuable future direction. Additionally, our evaluation is currently limited to the ImageNet benchmark. Assessing tokenizer scalability and generalization on broader, real-world datasets is an important next step. On the technical side, we observe training/inference discrepancy in our tokenizer: the decoder is primarily trained on noise-injected latent embeddings, whereas it operates on almost noise-free embeddings during inference. Fine-tuning the decoder on clean latent embeddings partially addresses this discrepancy. Further investigation into mitigating this discrepancy could yield additional improvements. Nonetheless, the core motivation and insights of our work remain robust. Conclusion. The strong generalization and effectiveness of our l-DeTok across different generative models highlights fundamental yet underexplored opportunity: tokenizer representations alone can substantially advance different generative approaches without architectural changes. Our findings suggest that explicitly aligning tokenizer training with downstream denoising tasks is surprisingly beneficial for generative modeling, complementing traditional focuses on pixel-level accuracy or semantic alignment. Given the foundational role of tokenizers, we hope this perspective will inspire further research into effective and generalizable representation learning for generative models. Lastly, the core idea of this work is general and extends beyond image tokenization. Exploring its potential in broader generative modeling, such as video generation, action generation, protein design, and other domains, will be an exciting direction for future research."
        },
        {
            "title": "Acknowledgment",
            "content": "The USC Geometry, Vision, and Learning Lab acknowledges generous supports from Toyota Research Institute, Dolby, Google DeepMind, Capital One, Nvidia, and Qualcomm. Yue Wang is also supported by Powell Research Award."
        },
        {
            "title": "References",
            "content": "[1] Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oguzhan Fatih Kar, Elmira Amirloo, Alaaeldin El-Nouby, Amir Zamir, and Afshin Dehghan. Flextok: Resampling images into 1d token sequences of flexible length. arXiv preprint arXiv:2502.13967, 2025. [2] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):17981828, 2013. [3] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. [5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. MaskGIT: Masked generative image Transformer. In CVPR, 2022. [6] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. arXiv preprint arXiv:2502.03444, 2025. [7] Hao Chen, Ze Wang, Xiang Li, Ximeng Sun, Fangyi Chen, Jiang Liu, Jindong Wang, Bhiksha Raj, Zicheng Liu, and Emad Barsoum. Softvq-vae: Efficient 1-dimensional continuous tokenizer. In CVPR, 2025. [8] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. [9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020. [10] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021. 10 [11] Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. Deconstructing denoising diffusion models for self-supervised learning. In ICLR, 2025. [12] Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Rambhatla, and Ishan Misra. Diffusion autoencoders are scalable image tokenizers. arXiv preprint arXiv:2501.18593, 2025. [13] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. [14] Chaorui Deng, Deyao Zhu, Kunchang Li, Shi Guang, and Haoqi Fan. Causal diffusion transformers for generative modeling. arXiv preprint arXiv:2412.12095, 2024. [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, 2019. [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. [17] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: deep convolutional activation feature for generic visual recognition. In ICML, 2014. [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. [20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for highresolution image synthesis. In ICML, 2024. [21] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. In ICLR, 2025. [22] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In CVPR, 2023. [23] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion Transformer is strong image synthesizer. In ICCV, 2023. [24] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. [25] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. [26] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv:1706.02677, 2017. [27] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: new approach to self-supervised learning. In NeurIPS, 2020. [28] Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, and Xinlei Chen. Learnings from scaling visual tokenizers for reconstruction and generation. arXiv preprint arXiv:2501.09755, 2025. [29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, 2020. [30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [32] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple data augmentation. In NeurIPS, 2023. 11 [33] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. [34] Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Eq-vae: Equivariance regularized latent space for improved generative image modeling. arXiv preprint arXiv:2502.09509, 2025. [35] Solomon Kullback and Richard Leibler. On information and sufficiency. The annals of mathematical statistics, 22(1):7986, 1951. [36] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. [37] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. In ICLR, 2025. [38] Yazhe Li, Jorg Bornschein, and Ting Chen. Denoising autoregressive representation learning. In ICML, 2024. [39] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. [40] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In CVPR, 2022. [41] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [42] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. [43] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, pp. 131, 2024. [44] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. In CVPR, 2025. [45] William Peebles and Saining Xie. Scalable diffusion models with Transformers. In ICCV, 2023. [46] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [47] Yunchen Pu, Zhe Gan, Ricardo Henao, Xin Yuan, Chunyuan Li, Andrew Stevens, and Lawrence Carin. Variational autoencoder for deep learning of images, labels and captions. In NeurIPS, 2016. [48] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [50] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Flowar: Scale-wise autoregressive image generation meets flow matching. In ICML, 2025. [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211252, 2015. [53] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [54] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. [55] Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey arXiv preprint Improving the diffusability of autoencoders. Tulyakov, and Aliaksandr Siarohin. arXiv:2502.14831, 2025. 12 [56] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. [57] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [58] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [59] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2025. [60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [62] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008. [63] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025. [64] Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. [65] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NeurIPS, 2014. [66] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. [67] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, David A. Ross Irfan Essa, and Lu Jiang. Language model beats diffusiontokenizer is key to visual generation. In ICLR, 2024. [68] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. [69] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. In NeurIPS, 2025. [70] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. [71] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019. [72] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. In ICLR, 2022."
        },
        {
            "title": "A Training and Inference Details",
            "content": "A.1 Tokenizer Model. Our tokenizers are based on ViTs [18]. We provide detailed model parameters in Table A.1. We include the details provided in Section 4 here for completeness: We implement our tokenizer using ViTs for both encoder and decoder [61, 18]. We adopt recent architectural advances from LLaMA [60], including RoPE [57] (together with learned positional embeddings following [22]), RMSNorm [71], and SwiGLU-FFN [53]. The encoder operates at patch size of 16, yielding 256 latent tokens for each 256 256 image, while the decoder uses patch size 1 as there is no resolution change. The latent dimension is set to 16. We omit the [CLS] token for simplicity. Table A.1: Model Size. We report the model configurations and parameter counts of tokenizer encoders and decoders. The total tokenizer size is the sum of encoder and decoder parameters. For example, tokenizer combining ViT-S encoder and ViT-B decoder (S-B) has 111.6M parameters, while tokenizer with both ViT-B encoder and decoder (B-B) has 171.7M parameters. Size Small (S) Base (B) Hidden Size 512 768 Blocks 8 Heads 8 12 Parameters 25.75M 85.85M Training. For ablation studies, we use ViT-S encoder and ViT-B decoder, disable GAN loss, and train for 50 epochs. For our final experiments, we adopt ViT-B for both encoder and decoder, enable GAN loss from epoch 100, and train for 200 epochs. In both settings, the global batch size is 1024, and the peak learning rate is set to 4.0 104 (scaled linearly from 1.0 104 at batch size of 256). We apply linear warm-up for 25% of the total epochs (12 epochs for 50epoch training, and 50 epochs for 200-epoch training), followed by cosine learning rate decay. We use the AdamW optimizer with β parameters (0.9, 0.95) and weight decay of 1.0 104. The only data augmentation employed is horizontal flipping. Our reconstruction loss closely follows the implementation in [69]. Training an S-B tokenizer (see Table A.1) without GAN loss for 50 epochs takes roughly 160 NVIDIA A100 GPU hours (enabling GAN loss from epoch 20 extends training to about 288 A100 GPU hours). Training B-B tokenizer with GAN loss enabled from epoch 100 for total of 200 epochs takes approximately 1,150 A100 GPU hours. Decoder fine-tuning. We observe an interesting discrepancy between training and inference in our l-DeTok. Our decoder is trained predominantly with noise-corrupted latent embeddings but encounters nearly clean embeddings during inference. To address this gap, we fine-tune the decoder on clean latent embeddings, i.e., masking and latent noising are disabled, for an additional 100 epochs. This adjustment partially alleviates the discrepancy, improving the FID of the 800-epoch MAR-L model from 1.43 to 1.35 and MAR-B model from 1.61 to 1.55  (Table 3)  . Figure A.2 and Figure A.1 compare the denoising capability of different tokenizers. The fine-tuned decoder exhibits reduced denoising ability. Importantly, we find that the performance improvements from our approach primarily result from better latent representations rather than increased denoising capability of the decoder. To validate this, we perform an additional experiment by fine-tuning only the MAR-VAE decoder on the denoising taskkeeping the encoder frozen, thus leaving latent representations unchanged. Contrary to expectations, this experiment leads to degraded performance, indicating that merely enhancing the decoders denoising capability exacerbates the training-inference discrepancy, since the generative models already excel at this task. These observations confirm that the key advantage of our method lies in improved latent representations, rather than decoder-side denoising improvements. Pseudo-code of latent denoising. See Algorithm 1. 14 Figure A.1: Visualization of latent denoising. Images generated from latent embeddings corrupted with varying noise levels (t) by the original decoder (top), fine-tuned decoder (middle), and baseline decoder (bottom). The fine-tuned decoder shows reduced ability to recover images from noisy embeddings compared to the original decoder. The baseline tokenizer trained without the denoising objective fails to reconstruct original images from noisy latents. Figure A.2: Visualization of mask denoising. Images generated from masked inputs with varying masking ratios (m) by the original decoder (top) and fine-tuned decoder (bottom). The fine-tuned decoder exhibits diminished capability in reconstructing masked regions compared to the original decoder. A.2 Generative Models Model. We re-implement all generative models within our codebase to standardize training and evaluation. We introduce one modification: for DiT [46], SiT [42], and LightningDiT [64], we apply classifier-free guidance (CFG) to all latent channels, rather than only the first three channels used in their original implementations.3 For training on 1D tokens (e.g., MAETok [6]), we use simple 1D learnable positional embeddings and disable RoPE used in LightningDiT. We adopt the default samplers from the original implementations, using 250 denoising steps during sampling. For autoregressive (AR) methods, we follow the default MAR model and implement RandomAR and RasterAR following RAR [68]. To produce continuous tokens, we employ diffloss [36] with 3-layer, 1024-channel MLP head. We disable dropout in all MLP layers within Transformer blocks for autoregressive models. For inference, we use 64 autoregressive steps and 100 denoising steps for all experiments, except those in system-level comparison  (Table 3)  , where we use 256 autoregressive steps. Sampling is performed with the default MAR sampler across all AR models. Following MAR [36], we set the sampling temperature to 1.0 when using CFG, and sweep temperatures when CFG is disabled (i.e., CFG=1.0). Training. We follow the training recipe from [64] for all experiments reported in this paper: global batch size of 1024, AdamW optimizer, constant learning rate of 2 104, and no warm-up, gradient clipping, or weight decay. We do not tune these hyperparameters. Training DiT-B, SiT-B, and LightningDiT-B for 100 epochs takes approximately 128 to 200 A100 hours using locally cached tokens. Training MAR-B, RandomAR-B, and RasterAR-B for 100 epochs takes approximately 220 to 250 A100 hours. For the final MAR-B model used in the systemlevel comparison  (Table 3)  , training for 800 epochs takes roughly 2,450 A100 hours; training MARL for the same duration takes about 3,850 A100 hours (online evaluation time included). 3See original implementations in DiT, SiT, and LightningDiT. 15 Algorithm 1 Latent Denoising: PyTorch-like Pseudo-code def denoise(x, encoder, decoder, max_mask_ratio=0.7, gamma=3.0): # encode input image to latent embeddings under (optional) masking z, ids_restore = encoder(x, max_mask_ratio=max_mask_ratio) # variational latent embeddings posteriors = diagonal_gaussian_dist(z) z_sampled = posteriors.sample() # sample interpolation factor uniformly from [0, 1] bsz, n_tokens, chans = z_sampled.shape device = z_sampled.device noise_level = torch.rand(bsz, 1, 1, device=device).expand(-1, n_tokens, chans) # generate Gaussian noise noise = gamma * torch.randn(bsz, n_tokens, chans, device=device) # interpolate latent embeddings with noise z_noised = (1 - noise_level) * z_quantized + noise_level * noise # reconstruct the inputs recon = decoder(z_noised, ids_restore) return recon Figure B.1: Tokenizer comparison for autoregressive models. We report FID@10k and IS@10k scores at different classifier-free guidance (CFG) scales. Semantics-distilled tokenizers (denoted by *) are shown in warm colors, while those without distillation are shown in cool colors. Our lDeTok consistently achieves superior FID and IS metrics compared to other tokenizers. See Table 2 for results on 50,000 images evaluated at the optimal CFG scales."
        },
        {
            "title": "B Additional Results",
            "content": "FID vs. CFG curves. Figures B.1 and B.2 compare how classifier-free guidance (CFG) scales influence generative performance (FID and IS) across different tokenizers and generative models. We use warm colors to denote semantics-distilled tokenizers (marked by *), and cool colors for tokenizers without distillation. Our l-DeTok consistently achieves stronger performance across varying CFG scales, improving notably over standard tokenizers and matching or even surpassing semanticsdistilled ones. Final FID scores computed over 50,000 generated images at optimal CFG scales are summarized in Table 2. 16 Figure B.2: Tokenizer comparison for non-autoregressive models. We report FID@10k and IS@10k scores at different classifier-free guidance (CFG) scales. Semantics-distilled tokenizers (denoted by *) are shown in warm colors, while those without distillation are shown in cool colors. Our l-DeTok consistently outperforms standard (non-semantics-distilled) tokenizers, matching the performance of the best semantics-distilled tokenizer (VA-VAE [64]) and surpassing MAETok [6] in IS. See Table 2 for results on 50,000 images evaluated at the optimal CFG scales."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "MIT CSAIL",
        "OpenAI",
        "USC"
    ]
}