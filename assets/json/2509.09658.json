{
    "paper_title": "Measuring Epistemic Humility in Multimodal Large Language Models",
    "authors": [
        "Bingkui Tong",
        "Jiaer Xia",
        "Sifeng Shang",
        "Kaiyang Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Hallucinations in multimodal large language models (MLLMs) -- where the model generates content inconsistent with the input image -- pose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, a behavior reflecting epistemic humility. We present HumbleBench, a new hallucination benchmark designed to evaluate MLLMs' ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from a panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by a rigorous manual filtering process. Each question includes a \"None of the above\" option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate a variety of state-of-the-art MLLMs -- including both general-purpose and specialized reasoning models -- on HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills a key gap in current evaluation suites, providing a more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 8 5 6 9 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Measuring Epistemic Humility in Multimodal Large Language\nModels",
            "content": "Bingkui Tong1, Jiaer Xia2, Sifeng Shang2, Kaiyang Zhou2* 1Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates. 2Hong Kong Baptist University, Hong Kong. *Corresponding author(s). E-mail(s): kyzhou@hkbu.edu.hk ; Contributing authors: bingkui.tong@mbzuai.ac.ae; xiajiaer@life.hkbu.edu.hk; cssfshang@comp.hkbu.edu.hk; Abstract Hallucinations in multimodal large language models (MLLMs)where the model generates content inconsistent with the input imagepose significant risks in real-world applications, from misinformation in visual question answering to unsafe errors in decision-making. Existing benchmarks primarily test recognition accuracy, i.e., evaluating whether models can select the correct answer among distractors. This overlooks an equally critical capability for trustworthy AI: recognizing when none of the provided options are correct, behavior reflecting epistemic humility. We present HumbleBench, new hallucination benchmark designed to evaluate MLLMs ability to reject plausible but incorrect answers across three hallucination types: object, relation, and attribute. Built from panoptic scene graph dataset, we leverage fine-grained scene graph annotations to extract ground-truth entities and relations, and prompt GPT-4-Turbo to generate multiple-choice questions, followed by rigorous manual filtering process. Each question includes None of the above option, requiring models not only to recognize correct visual information but also to identify when no provided answer is valid. We evaluate variety of state-of-the-art MLLMsincluding both general-purpose and specialized reasoning modelson HumbleBench and share valuable findings and insights with the community. By incorporating explicit false-option rejection, HumbleBench fills key gap in current evaluation suites, providing more realistic measure of MLLM reliability in safety-critical settings. Our code and dataset are released publicly and can be accessed at https://github.com/maifoundations/HumbleBench. Keywords: Hallucination Benchmark, Multimodal Large Language Model"
        },
        {
            "title": "1 Introduction",
            "content": "Hallucinations in multimodal large language models (MLLMs) refer to the phenomenon where model generates content inconsistent with the visual input. Existing benchmarks for hallucination detection primarily focus on recognition accuracy: binary formats (yes/no) measure whether model can affirm or deny specific hypothesis [1], while single-correct multiple-choice formats evaluate the ability to identify the correct option among distractors [2]. While effective for assessing factual recall, these setups overlook an equally important ability for trustworthy AI: recognizing when none of the presented answers are correct and refusing to commit to false choice [3]. The ability to abstain from giving incorrect answers aligns with the concept of epistemic 1 Fig. 1: Examples from HumbleBench where correct options are marked green. This benchmark is derived from panoptic scene graph dataset with rich annotations in object, relation, and attribute. Different from existing hallucination benchmarks, HumbleBench has None of the above option in each question to test whether models can identify when no provided answer is valid. humility[4, 5], which reflects awareness of ones own knowledge limitations. In human cognition, this manifests as the capacity to withhold judgment when information is insufficient or ambiguous, skill critical for rational decision-making. For multimodal AI systems, humility is core value we need to pursue for building robust, trustworthy agents that do not produce overconfident hallucinations. Yet, current visual hallucination benchmarks for MLLMs rarely assess this capability explicitly. To fill this gap, we propose HumbleBench, new evaluation framework designed to measure false-option rejection in MLLMs. HumbleBench adopts multiple-choice question format in which one of the options is explicitly set to None of the above. The benchmark is constructed from panoptic scene graph dataset [6], which provides fine-grained scene graph annotations from which we can obtain accurate object, relation, and attribute information. We use GPT-4-Turbo to generate natural language questions along with distractor options, followed by rigorous manual filtering process to ensure validity of all questions and answers. In total, HumbleBench contains 22,831 multiple-choice questions, making it the largest hallucination benchmark to date. Crucially, it requires models not only to identify correct information but also to explicitly reject all incorrect candidates when necessary. Fig. 1 shows some example questions from HumbleBench. Unlike existing benchmarks that focus on general-purpose MLLMs, we conduct experiments on HumbleBench using variety of state-of-the-art MLLMs, including both general-purpose and specialized reasoning models. The results show that HumbleBench is challenging benchmark, with the best-performing models reaching only around 70% accuracy. We also find that scaling model size alone is insufficient for improving robustness and that reasoning models do not always work betterit depends heavily on training strategy and data quality. We also conduct two stress tests where only the none-of-the-above option is correct and find that most models fail catastrophically under this setting. In summary, our contributions are threefold. First, we introduce HumbleBench, large-scale hallucination benchmark that explicitly evaluates false-option rejectionand thus epistemic humilityin MLLMs via multiple-choice questions containing None of the above option, complementing recognition-focused evaluations. Second, we develop rigorous data construction pipeline that leverages panoptic scene graphs to ensure verifiable ground truth, uses GPT-4language questions Turbo to generate natural 2 and distractors, and applies manual validation to produce 22,831 high-quality questions spanning objects, attributes, and relations the largest hallucination benchmark to date. Third, we provide comprehensive empirical study across general-purpose state-of-the-art and reasoning MLLMs and share valuable results and insights that could facilitate future research. Code and dataset are released at https://github.com/maifoundations/HumbleBench."
        },
        {
            "title": "2 HumbleBench",
            "content": "HumbleBench provides comprehensive testbed for evaluating epistemic humility in multimodal AI systems. It consists of 22,831 multiple-choice questions where each question has 5 choices, among which one is set to None of the above. There are three types of hallucinations: object, relation, and attribute. Unlike traditional hallucination benchmarks that focus on recognition, HumbleBench requires models not only to recognize correct visual information but also to identify when none of the provided answers are correct. Table 1 contains more detailed statistics, such as question length and image size. Fig. 2 shows the distributions over answer choices and different hallucination types. Overall, the answer options A-D are relatively balanced. This ensures diversity in correct answers while preventing models from overfitting to one choice. The three question types are fairly balanced, with slight dominance in attribute-related questions. Below we introduce the construction pipeline in detail (see Fig. 3 for an overview)."
        },
        {
            "title": "2.1 Construction Pipeline",
            "content": "Data Source. We choose the Panoptic Scene Graph (PSG) dataset [6] as the data source because of its high-quality annotations. Specifically, PSG contains pixel-level panoptic segmentation masks for objects and their relationships, offering significantly higher precision and finer granularity compared to conventional bounding box-based scene graph datasets. Moreover, PSG contains over 40,000 images with an average of 11.04 objects, 5.65 relations, and multiple captions per image. To build HumbleBench, we use randomly sampled subset of 4,500 images from PSG as the starting point. 3 Information Extraction. PSG provides labels of objects and relations but lacks explicit labels of attributes, such as colors and shapes. To obtain attribute labels, we first use the segmentation masks to crop objects within each image and pass these cropped images to InstructBLIP [7] using the following prompt: This is object, describe it briefly please. The placeholder object is replaced with specific object name. The generated descriptions are used as attribute labels. Question Generation. Once the labels of objects, relations, and attributes are obtained, we proceed to question generation. Specifically, we prompt GPT-4-Turbo to generate 10 multiplechoice questions per image using the label information as context. This automated process yields an initial dataset of approximately 42,000 questions. Each question has five options with only one correct answer. key design of our prompting strategy is to ensure that the incorrect options are highly plausible, thereby making evaluation more challenging. Please refer to Appendix for more details about the prompt fed to GPT-4-Turbo. Manual Filtering. To ensure high-quality data, we implement rigorous yet efficient manual filtering process to address potential errors in outputs from InstructBLIP and GPT-4-Turbo. First, annotators assess whether each question and its options are clear, well-constructed, and logically sound. Any that fail to meet these criteria are discarded. For questions that pass the initial screening, we verify the correctness of the provided answers. If the answer is accurate, the question is retained. If not, we either correct the answer by selecting valid alternative option (when available) or remove the question completely. As shown in Table 2, this process results in modifications to 10.82% of candidate questions and the removal of 45.44%. More details about the construction pipeline can be found in Appendix A."
        },
        {
            "title": "3 Experiments",
            "content": "Models. We select diverse and representative set of MLLMs, spanning various architectures and model sizes. These models are organized into two categories: general-purpose models and specialized reasoning models. Fig. 2: Distributions of answer choices (left) and question types (right). NOTA denotes the None of the above option. Table 1: Detailed dataset statistics. Table 2: Summary of the manual filtering process. Statistics Total Questions Question Types Choices Avg. Question Length Avg. Option Length Avg. Image Size Number 22,831 3 5 8.88 words 2.52 words 594473 px Initial Questions Kept Modified Deleted Count 41, 18,304 (43.74%) 4,527 (10.82%) 19,012 (45.44%) Final Total 22,831 (54.56%) First, we evaluate set of foundational models chosen for their strong general-purpose capabilities and widespread use in the community. These models are: (i) Qwen2.5-VL-7B [8], which excels in multi-image processing and structured output generation; (ii) LLaVA-NEXT-7B [9], featuring enhanced OCR and long-context understanding; (iii) Molmo-D-7B [10], built with strong visionlanguage alignment; (iv) DeepSeek-VL2-Tiny3B [11], employing lightweight Mixture-ofExperts design for efficient VQA (Visual Question Answering); (v) InternVL3-8B [12], leveraging unified pretraining for broad multimodal capabilities; (vi) LLaMA3.2-11B [13], instructiontuned for strong VQA performance; (vii) Phi4-5B [14], vision-language model refined with RLHF for robust instruction following; (viii) Gemma-3-4B [15], supporting multilingual text and image processing in over 140 languages; (ix) Cambiran-8B [16], designed for vision-centric reasoning tasks; (x) Pixtral-12B [17], combining novel visual encoder with large decoder for advanced reasoning; (xi) Idefics3-8B [18], finetuned with enhanced visual token encoding for superior OCR (Optical Character Recognition); and (xii) VILA1.5-8B [19], pretrained on largescale interleaved image-text data for multi-image reasoning and in-context learning. structured visual Second, we choose models specialized for reasoning. They are typically built on top of general-purpose models and undergo post-training with additional datasets to enhance reasoning. (i) Ovis2-8B [20], optiThese models are: mized for reasoning across images and videos; (ii) Mulberry-7B [21], finetuned on synthetic data to boost chain-ofthought (CoT) reasoning; (iii) R1-Onevision7B [22], trained on curated data for robust general-purpose understanding; (iv) VisionaryR1-4B [23], employing reinforcement learning in captionreasonanswer pipeline to prevent shortcut learning; (v) LLaVA-CoT-11B [24], integrating explicit CoT generation for systematic interpretation; (vi) R1-VL-7B [25], utilizing step-wise reinforcement signals to improve reasoning accuracy; and (vii) GLM-4.1V-Thinking9B [26], introducing novel thinking paradigm and using reinforcement learning to push the limits of complex reasoning. Evaluation. We evaluate the two groups of settings, which three different models under 4 Fig. 3: Construction pipeline of HumbleBench. For each image in the panoptic scene graph dataset, we first extract object and relation information from the scene graph annotations while using InstructBLIP to extract attribute information. Then, we prompt GPT-4-Turbo to combine all information and generate multiple-choice questions, each with five answer options (the last option is set to None of the above). Finally, we conduct manual filtering process to refine the questions and options. are named HumbleBench, HumbleBench-E, and HumbleBench-GN, respectively. HumbleBench is the original benchmark. HumbleBench-E and HumbleBench-GN are two stress tests. Specifically, HumbleBench-E removes correct non-E answers so the option, i.e., None of the above, is the only correct answer for the entire dataset. HumbleBench-GN replaces all images with Gaussian noise so, again, only the option is correct. Accuracy (%) is reported as the primary metric throughout the experiments."
        },
        {
            "title": "3.1 Results on HumbleBench",
            "content": "Overall Accuracy Well Above Random Guess But Far From Perfect. As shown in Table 3, HumbleBench presents considerable challenge for current state-of-the-art MLLMs. The 5 Table 3: Performance of general-purpose and specialized reasoning models on HumbleBench. The bestperforming model in each category is highlighted in bold. Model # Params Object Relation Attribute Overall General-Purpose Models Cambrian Gemma-3 DeepSeek-VL2 VILA1.5 LLaVA-Next LLaMA-3.2 Pixtral Phi-4 Molmo-D Idefics3 InternVL3 Qwen2.5-VL Specialized Reasoning Models Mulberry Ovis-2 LLaVA-CoT R1-Onevision R1-VL Visionary-R1 GLM-4.1V-Thinking 8B 4B 3B 8B 7B 11B 12B 5B 7B 8B 8B 7B 7B 8B 11B 7B 7B 4B 9B 49.39 50.71 58.11 51.99 61.03 57.56 60.23 63.18 61.54 61.92 65.85 67.77 56.91 58.24 60.85 61.43 63.59 65.75 69.30 53.17 56.39 56.95 62.47 61.61 64.24 64.73 64.35 65.25 65.78 68.13 70.43 55.92 60.99 65.09 65.16 67.96 66.70 71.25 63.31 70.20 68.91 72.37 72.53 73.25 74.12 73.68 74.39 76.19 76.00 77. 66.37 68.50 73.68 73.38 74.03 75.88 79.24 55.56 59.48 61.55 62.66 65.29 65.31 66.63 67.28 67.31 68.24 70.19 72.20 59.93 62.77 66.79 66.89 68.73 69.65 73.46 best-performing general-purpose model, Qwen2.5VL, achieves 72.20% accuracy, while the leading reasoning-oriented model, GLM-4.1V-Thinking, reaches 73.46%only marginal improvement over Qwen2.5-VL. Most other models, whether general-purpose or reasoning-enhanced, cluster around the 60% range. While these results are substantially higher than the random-guessing baseline of 20%, they remain far from perfect. This gap underscores the difficulty of reliably rejecting false options and the limitations of current approaches in handling hallucination-prone scenarios. The fact that top-tier models plateau in the low 70% range suggests that existing training strategies may capture surface-level correlations without fully mastering the deeper reasoning and uncertainty modeling required for robust rejection. Therefore, HumbleBench not only highlights the progress of MLLMs in surpassing naive baselines but also exposes the significant headroom for future research in developing models that can approach human-level reliability in discerning true information from distractors. 6 Performance Not Strictly Correlated With Model Size. Our results reveal an important observation: larger models do not necessarily perform better on HumbleBench. For example, within the reasoning-oriented group, the 4B Visionary-R1 outperforms several models of substantially larger sizes. Likewise, in the general-purpose group, the 12B Pixtral is surpassed by the much smaller 5B Phi-4. The results suggest that scaling alone is insufficient for improving robustness, and that thoughtful design choices related to, e.g., model architecture, data quality, and training methodology, may be more critical than simply increasing model size. As result, future progress may depend less on building ever-larger models and more on optimizing how models are trained and adapted to handle uncertainty and ambiguity in multimodal reasoning. Reasoning Models Do Not Always Work Better. It is often assumed that reasoningrobustness oriented models stronger exhibit answer is not E, we remove that option and set as the correct choice, which is reasonable since no other option remains valid. The results shown in Figure 4 reveal critical vulnerability shared by all evaluated models: profound inability to handle questions where the correct answer is None of the above (NOTA). In general, the accuracy drops sharply across the boardoften below the random-guess baseline of 20% (with 5 options). This weakness is evident even among the top-performing models. For example, Qwen2.5-VL, the best foundational model overall, achieves only 28.89% accuracy on NOTA questions. The issue is even more severe for GLM4.1V-Thinking: despite leading in overall accuracy, it scores mere 0.06% on NOTA cases. Several models, including LLaVA-Next, Molmo-D, and LLaMA-CoT, perform even worse, with striking 0% success rate. The only notable exception is Cambrian, which attains respectable 60.68%. These widespread failures demonstrate that, when presented with set of incorrect statements about an image, current MLLMs overwhelmingly choose one of the false options rather than correctly abstaining. This behavior reflects core aspect of hallucination that traditional benchmarks, which assume correct option is present among the candidates, cannot assess, highlighting the unique and critical insights provided by HumbleBench."
        },
        {
            "title": "3.3 Stress Test 2: HumbleBench-GN",
            "content": "HumbleBench-GN completely destroys the visual representation in images and therefore can be used to test whether model uses visual cues for decision-making or relies more on language priors. Since all images are Gaussian noise, no candidate option from A-D is valid and only the option makes sense. Any selection from A-D can be counted as hallucination. Fig. 5 plots the accuracy on HumbleBench-GN against that on the original HumbleBench. In terms of results on HumbleBench-GN, Qwen2.5-VL performs the best with 90.53% accuracy among the general-purpose models while R1-Onevision is the in the reasoning group with strongest model 83.38% accuracy. Interestingly, R1-Onevision and Phi-4 score similarly on HumbleBench (66.89% vs. 67.28%) yet diverge sharply on HumbleBenchGN (83.35% vs. 28.19%). Another interesting Fig. 4: Results on HumbleBench-E. The red dashed line indicates the random guess baseline (with five choices). against hallucinations compared to generalpurpose models, since they are designed to analyze visual input more carefully and can benefit from test-time scaling [27]. However, the results on HumbleBench reveal that reasoning does not guarantee improved performance. For example, R1-Onevision, which is fine-tuned from Qwen2.5VL with the explicit goal of enhancing reasoning, performs significantly worse than its base model on HumbleBench (66.89% vs. 72.20% overall accuracy). In contrast, LLaVA-CoT shows modest gain over its base model LLaMA-3.2 (66.79% vs. 65.31%). These contrasting outcomes highlight that the effectiveness of reasoning-oriented finetuning in mitigating hallucinations is not universal but highly dependent on both the training strategy and the characteristics of the data. Overall, these findings suggest that while reasoning has the potential to reduce hallucinations, its benefits are conditional and must be carefully evaluated in the context of benchmark design and fine-tuning methodology."
        },
        {
            "title": "3.2 Stress Test 1: HumbleBench-E",
            "content": "We conduct stress test to evaluate the models Eselection rate. For questions whose original correct 7 Fig. 5: Accuracy on HumbleBench vs. HumbleBench-GN. This scatter plot correlates each models standard overall accuracy (x-axis) with its accuracy from our noise image test (y-axis), where red points indicate general purpose models and blue points indicate reasoning models. truly robust model would appear in the top-right corner. case is InternVL3, which outperforms models like Cambrian on HumbleBench by large margin (70.19% vs. 55.56%) but the performance on HumbleBench-GN declines significantly to 37.75% (which is well below Cambrians 82.33%). These results suggest that future work should pay more attention to building more robust alignment between vision and language in MLLMs."
        },
        {
            "title": "3.4 Error Analysis",
            "content": "To better understand why MLLMs fail in certain scenarios, we conduct qualitative error analysis. By visualizing the errors made by Qwen2.5-VL on HumbleBench (Figure 6), which achieves high accuracy on both HumbleBench and HumbleBench-GN, we can identify specific failure modes. primary failure mode is the models inability to select the None of the above option, as shown in Questions 1-4. For instance, in Question 1, the object on the table is clearly loaf of bread, not sandwich. But the model hallucinates the presence of one sandwich, choosing option instead of correctly abstaining with E. Similarly, in Question 4, the woman is obviously sitting beside the chairs, but the model incorrectly infers that she is sitting on them, constituting clear relation hallucination. Question 5 highlights failure in fine-grained discrimination; the model is misled by visually ambiguous poster on the wall, mistaking it for television and thus failing simple counting task. Finally, Question 6 showcases critical breakdown in visual faithfulness from our noise-image experiment. With no meaningful visual information, faithful model should select None of the above. Instead, Qwen2.5-VL predicts that the sky is Grey. We argue this occurs because, in the absence of visual grounding, the model reverts to the parametric knowledge of its language backbone. It relies on strong linguistic priors, associating sky with common colors like grey or 8 Fig. 6: Error analysis of Qwen-VL 2.5 on the HumbleBench benchmark. Green options are correct answers while red options are predicted by the model. These examples highlight several key failure modes. (Question 1-4) demonstrate frequent inability to select None of the above (NOTA), where the model instead hallucinates an object, attribute, or relation that is not present in the image. (Question 5) shows standard object error, where the model fails simple counting task. (Question6) illustrates critical failure in visual faithfulness; when presented with non-informative noise image, the model fabricates an answer instead of acknowledging the absence of visual evidence. blue, thereby fabricating an answer completely untethered from visual reality."
        },
        {
            "title": "4 Related Work",
            "content": "Hallucinations in MLLMs. Hallucinations in MLLMs arise from intertwined issues spanning data, architecture, and the vision-language fusion pipeline. Data quality is primary driver: heuristically paired image-text corpora contain inconsistencies, and insufficient diversity induces biased responses [28]. From the architecture point of view, limitations often originate in the vision encoder, where low input resolution and saliency bias yield incomplete scene understanding [29 31]. The alignment stage can then fail to faithfully synchronize visual and textual representations [32, 33]. This misalignment is further amplified by self-attention, which may underweight the visual input, leading the model to default to the LLMs strong pre-trained parametric knowledge and thereby override the actual image content [34, 35]. Finally, suboptimal decoding strategies exacerbate the issue, as early errors compound rather than self-correct [3537]. Evaluation and Benchmarks. Drawing on lessons from LLM hallucination benchmarks [38], existing hallucination evaluations for MLLMs likewise fall into two broad categories: generative and discriminative [39]. Generative benchmarkssuch as THRONE [40] and HaloQuest [41]are based on open-ended questions. Although they capture free-form generation capabilities, they typically employ external LLMs as judges, which raises reproducibility concerns and may introduce evaluator bias. In contrast, discriminative benchmarks have gained popularity for their objectivity and ease of use. Early discriminative efforts focused on only one or two hallucination types [4244], but more recent datasets have addressed this limitation with more hallucination types included [45, 46]. These benchmarks can be generally categorized into two groups. The first is judgment, binary (Yes/No) task exemplified by POPE [43], HallusionBench [47], PhD [45], and Hallu-PI [46]. The second is multiple-choice, where the model must select the correct option from several candidates, as in LongHalQA [48] and Reefknot [42]. Our HumbleBench fills the gap that existing benchmarks ignore humility act in evaluation i.e., models must abstain from giving metrics, 9 Table 4: Comparison of HumbleBench with existing hallucination benchmarks. HumbleBench has the largest scale and is the only one that evaluates epistemic humility, i.e., whether model can reject incorrect options when there is uncertainty. mixed means combination of yes/no, multiple-choice, and open-ended questions. Benchmark Venue Size Hallucination Types Questions Reject Option Object Relation Attribute CHAIR POPE HallusionBench Hallu-PI Reefknot DASH PhD EMNLP18 5k EMNLP23 3k 254 CVPR24 MM24 8k ACL25 17k ICCV25 3k CVPR25 17k HumbleBench (Ours) - 23k yes/no yes/no yes/no yes/no mixed yes/no yes/no multiple-choice incorrect answers when uncertainty is high. Our design echoes the idea shared by OpenAIs recent research [3]: standard evaluation metrics are insufficient as they reward guessing over acknowledging uncertainty. Table 4 compares HumbleBench with existing visual hallucination benchmarks."
        },
        {
            "title": "5 Conclusion",
            "content": "HumbleBench offers the largest benchmark to date for evaluating visual hallucinations. With the false-option rejection design, HumbleBench provides more realistic measure of MLLM reliability in safety-critical settings. The extensive evaluation of 19 state-of-the-art MLLMs reveals that even top-performing models are far from perfect. In terms of the ability to reject hallucinated options, most models perform similarly with random guessing. Moreover, the Gaussian noise experiment exposes serious issue with visual grounding as many models with reasonable performance on HumbleBench fail catastrophically on HumbleBench-GN. The findings shared in this work suggest that recognition accuracy alone is insufficient for assessing hallucination robustness and highlight the importance of abstention design in our evaluation framework. Data Availability Statement. All data supthis study are pubporting the findings of licly available. The HumbleBench, including code and data, is available at maifoundations/HumbleBench."
        },
        {
            "title": "Appendix A",
            "content": "A.1 Prompt for Automatic"
        },
        {
            "title": "Question Generation",
            "content": "In our HumbleBench construction pipeline, we utilized GPT-4-Turbo with detailed prompt to generate the initial question pool. As illustrated in Figure A1, this prompt supplied the model with rich, structured information for each image, including annotated objects, relations, and attributes. To ensure scalability and consistency, it also mandated that the output be machinereadable JSON object, which allowed for seamless integration into our database. The prompts core design enforces set of strict rules to ensure the benchmarks quality and rigor. Key constraints required each question to target only single hallucination type (Object, Attribute, or Relation), to include None of the above as mandatory option, and to feature plausible yet incorrect distractors. Furthermore, to prevent ambiguity and data leakage, the prompt explicitly forbade the use of annotation artifacts (like object indices) and required that questions be unanswerable without the image. This rigorous design was essential for producing questions that specifically probe models visual faithfulness. The noise-image experiment is designed to measure models visual faithfulness when presented with non-informative visual input. To conduct this test, we programmatically generated 10 Fig. A1: The detailed prompt used to instruct GPT-4-Turbo for the generation of HumbleBench questions. It specifies the input format, output structure, and series of strict constraints to ensure question quality. static noise image, which was used as substitute for all original images in the HumbleBench benchmark. A.2 Gaussian Noise Image"
        },
        {
            "title": "Generation",
            "content": "The procedure for creating this image is formally described in Algorithm 1. In essence, we create 256256 grayscale image where each pixels intensity is drawn from discrete uniform distribution over the integers [0, 255]. This ensures the image contains no recognizable visual patterns. A.3 Models for Data Generation Our data generation pipeline utilized two primary models to create the initial pool of questions for HumbleBench. 11 Algorithm 1 Noise Image Generation 1: Input: Image dimensions 256, 256 2: Output: grayscale noise image 3: Initialize an empty integer array NoiseArray 4: for 1 to do 5: for 1 to do 6: NoiseArray[y, x] RandomInteger(0, 255) end for 7: 8: end for 9: ImageFromArray(NoiseArray, mode=Grayscale) 10: return InstructBLIP was employed for the data enrichment phase. We used the Vicuna-7B variant. InstructBLIP is vision-language model renowned for its ability to follow instructions to perform detailed visual analysis. Its core innovation, the Querying Transformer (Q-Former), effectively extracts fine-grained visual features relevant to given textual prompt. This capability was instrumental in generating the rich attribute descriptions for each object, which formed the semantic foundation for our questions. GPT-4-Turbo was used for the question synthesis stage. Specifically, we utilized the version. gpt-4-turbo-2024-02-15-preview This model is characterized by its advanced instruction-following capabilities, large context window, and native JSON output mode. The JSON mode was particularly crucial, as it allowed us to enforce strict, machine-readable output format, while its reasoning abilities were essential for generating plausible, context-aware distractors for the multiple-choice questions based on our complex prompt. A.4 Custom Filtering Software To facilitate rigorous and efficient manual filtering process, we developed custom GUI using the PyQt5 framework in Python, as shown in Figure A2. The tools design is focused on maximizing annotator throughput and accuracy. The interface presents one image-question pair to an annotator at time, displaying the image alongside the generated question and its five Fig. A2: Screenshot of the custom filtering GUI developed using PyQt5. The interface displays each image-question pair and provides annotators with dedicated controls to Keep, Modify, or Delete candidates, which was essential for ensuring the high quality of the final benchmark. multiple-choice options in clean layout. Annotators are provided with dedicated set of buttons to perform one of three primary actions: 1. Keep: Approve the question and its default answer as correct. 2. Modify: Correct the answer label if the original was wrong but suitable alternative exists among the options. 3. Delete: Discard the question entirely if it is ambiguous, ill-formed, or factually incorrect. The software also includes navigation controls (Last One, Next One) for efficient review. Upon making decision, the tool automatically saves the result to the questions corresponding JSON file and advances to the next item. This streamlined workflow was instrumental in curating the nearly 42,000 initial candidates down to the final 22,831 high-quality questions in HumbleBench."
        },
        {
            "title": "References",
            "content": "[1] Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li, K., Sun, X., Wu, Y., Ji, R.: MME: Comprehensive 12 Evaluation Benchmark for Multimodal Large Language Models (2024). https://arxiv.org/ abs/2306.13394 [2] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., Steinhardt, J.: Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020) [3] Kalai, A.T., Nachum, O., Vempala, S.S., Zhang, E.: Why Language Models Hallucinate (2025). https://arxiv.org/abs/2509. 04664 [4] Whitcomb, D., Battaly, H., Baehr, J., Howard-Snyder, D.: Intellectual humility. Philosophy and Phenomenological Research 94(3), 509539 (2017) [5] Krumrei-Mancuso, E.J., Haggard, M.C., LaBouff, J.P., Rowatt, W.C.: Links between intellectual humility and acquiring knowledge. The Journal of Positive Psychology 15(2), 155170 (2020) [6] Yang, J., Ang, Y.Z., Guo, Z., Zhou, K., Zhang, W., Liu, Z.: Panoptic scene graph generation. In: European Conference on Computer Vision, pp. 178196 (2022). Springer [7] Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P.N., Hoi, S.: Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Advances in neural information processing systems 36, 4925049267 (2023) [8] Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al.: Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 (2025) [9] Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Lee, Y.J.: LLaVA-NeXT: Improved reasoning, OCR, and world knowledge (2024). https://llava-vl.github.io/blog/ 2024-01-30-llava-next/ [10] Deitke, M., Clark, C., Lee, S., Tripathi, R., Yang, Y., Park, J.S., Salehi, M., Muennighoff, N., Lo, K., Soldaini, L., et al.: Molmo and pixmo: Open weights and open data for stateof-the-art vision-language models. In: Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 91104 (2025) [11] Wu, Z., Chen, X., Pan, Z., Liu, X., Liu, W., Dai, D., Gao, H., Ma, Y., Wu, C., Wang, B., et al.: Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302 (2024) [12] Zhu, J., Wang, W., Chen, Z., Liu, Z., Ye, S., Gu, L., Tian, H., Duan, Y., Su, W., Shao, J., et al.: Internvl3: Exploring advanced training and test-time recipes for opensource multimodal models. arXiv preprint arXiv:2504.10479 (2025) [13] Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al.: The llama 3 herd of models. arXiv e-prints, 2407 (2024) [14] Abdin, M., Aneja, J., Behl, H., Bubeck, S., Eldan, R., Gunasekar, S., Harrison, M., Hewett, R.J., Javaheripi, M., Kauffmann, P., et al.: Phi-4 technical report. arXiv preprint arXiv:2412.08905 (2024) [15] Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Rame, A., Rivi`ere, M., et al.: Gemma 3 technical report. arXiv preprint arXiv:2503.19786 (2025) [16] Tong, P., Brown, E., Wu, P., Woo, S., IYER, A.J.V., Akula, S.C., Yang, S., Yang, J., Middepogu, M., Wang, Z., et al.: Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems 37, 8731087356 (2024) [17] Agrawal, P., Antoniak, S., Hanna, E.B., Bout, B., Chaplot, D., Chudnovsky, J., Costa, D., De Monicault, B., Garg, S., Gervet, T., et al.: Pixtral 12b. arXiv preprint arXiv:2410.07073 (2024) [18] Laurencon, H., Marafioti, A., Sanh, V., Tronchon, L.: Building and better understanding 13 vision-language models: insights and future directions. arXiv preprint arXiv:2408.12637 (2024) [19] Lin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M., Han, S.: Vila: On pre-training for visual language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26689 26699 (2024) [20] Lu, S., Li, Y., Chen, Q.-G., Xu, Z., Luo, W., Zhang, K., Ye, H.-J.: Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797 (2024) [21] Yao, H., Huang, J., Wu, W., Zhang, J., Wang, Y., Liu, S., Wang, Y., Song, Y., Feng, H., Shen, L., et al.: Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319 (2024) [22] Yang, Y., He, X., Pan, H., Jiang, X., Deng, Y., Yang, X., Lu, H., Yin, D., Rao, F., Zhu, M., et al.: R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615 (2025) [23] Xia, J., Zang, Y., Gao, P., Li, Y., Zhou, K.: Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning. arXiv preprint arXiv:2505.14677 (2025) [24] Xu, G., Jin, P., Li, H., Song, Y., Sun, L., Yuan, L.: Llava-cot: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440 (2024) [25] Zhang, J., Huang, J., Yao, H., Liu, S., Zhang, X., Lu, S., Tao, D.: R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937 (2025) [26] Hong, W., Yu, W., Gu, X., Wang, G., Gan, G., Tang, H., Cheng, J., Qi, J., Ji, J., Pan, L., et al.: Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006 (2025) [27] Snell, C., Lee, J., Xu, K., Kumar, A.: Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314 (2024) [28] Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., Wang, L.: Mitigating hallucination in large multi-modal models via robust instruction tuning. arXiv preprint arXiv:2306.14565 (2023) [29] Zhai, B., Yang, S., Zhao, X., Xu, C., Shen, S., Zhao, D., Keutzer, K., Li, M., Yan, T., Fan, X.: Halle-switch: Rethinking and controlling object existence hallucinations in large vision-language models for detailed caption (2023) [30] Li, Z., Yang, B., Liu, Q., Ma, Z., Zhang, S., Yang, J., Sun, Y., Liu, Y., Bai, X.: Monkey: Image resolution and text label are important things for large multi-modal models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2676326773 (2024) [31] Jain, J., Yang, J., Shi, H.: Vcoder: Versatile vision encoders for multimodal large language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2799228002 (2024) [32] Jiang, C., Xu, H., Dong, M., Chen, J., Ye, W., Yan, M., Ye, Q., Zhang, J., Huang, F., Zhang, S.: Hallucination augmented contrastive learning for multimodal large language model. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2703627046 (2024) [33] Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al.: Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198 (2024) 14 [34] Favero, A., Zancato, L., Trager, M., Choudhary, S., Perera, P., Achille, A., Swaminathan, A., Soatto, S.: Multi-modal hallucination control by visual information grounding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1430314312 (2024) [35] Leng, S., Zhang, H., Chen, G., Li, X., Lu, S., Miao, C., Bing, L.: Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1387213882 (2024) [36] Huang, Q., Dong, X., Zhang, P., Wang, B., He, C., Wang, J., Lin, D., Zhang, W., Yu, N.: Opera: Alleviating hallucination in multimodal large language models via over-trust penalty and retrospection-allocation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1341813427 (2024) [37] Wang, X., Pan, J., Ding, L., Biemann, C.: Mitigating hallucinations in large visionlanguage models with instruction contrastive decoding. arXiv preprint arXiv:2403.18715 (2024) [38] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y.J., Madotto, A., Fung, P.: Survey of hallucination in natural language generation. ACM computing surveys 55(12), 138 (2023) [39] Liu, H., Xue, W., Chen, Y., Chen, D., Zhao, X., Wang, K., Hou, L., Li, R., Peng, W.: survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253 (2024) [40] Kaul, P., Li, Z., Yang, H., Dukler, Y., Swaminathan, A., Taylor, C., Soatto, S.: Throne: An object-based hallucination benchmark for the free-form generations of large vision-language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2722827238 (2024) [41] Wang, Z., Bingham, G., Yu, A.W., Le, Q.V., Luong, T., Ghiasi, G.: Haloquest: visual hallucination dataset for advancing multimodal reasoning. In: European Conference on Computer Vision, pp. 288304 (2024). Springer [42] Zheng, K., Chen, J., Yan, Y., Zou, X., Hu, X.: Reefknot: comprehensive benchmark for relation hallucination evaluation, analysis and mitigation in multimodal large language models. arXiv preprint arXiv:2408.09429 (2024) [43] Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.-R.: Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023) [44] Augustin, M., Neuhaus, Y., Hein, M.: Dash: Detection and assessment of systematic hallucinations of vlms. arXiv preprint arXiv:2503.23573 (2025) [45] Liu, J., Fu, Y., Xie, R., Xie, R., Sun, X., Lian, F., Kang, Z., Li, X.: Phd: prompted visual hallucination evaluation dataset. CoRR (2024) [46] Ding, P., Wu, J., Kuang, J., Ma, D., Cao, X., Cai, X., Chen, S., Chen, J., Huang, S.: Hallu-pi: Evaluating hallucination in multimodal large language models within perturbed inputs. In: Proceedings of the 32nd ACM International Conference on Multimedia, pp. 1070710715 (2024) [47] Guan, T., Liu, F., Wu, X., Xian, R., Li, Z., Liu, X., Wang, X., Chen, L., Huang, F., Yacoob, Y., et al.: Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14375 14385 (2024) [48] Qiu, H., Huang, J., Gao, P., Qi, Q., Zhang, X., Shao, L., Lu, S.: Longhalqa: Longcontext hallucination evaluation for multimodal large language models. arXiv preprint arXiv:2410.09962 (2024)"
        }
    ],
    "affiliations": [
        "Hong Kong Baptist University, Hong Kong",
        "Mohamed bin Zayed University of Artificial Intelligence, United Arab Emirates"
    ]
}