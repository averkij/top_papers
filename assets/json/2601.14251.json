{
    "paper_title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR",
    "authors": [
        "Said Taghadouini",
        "Adrien Cavaillès",
        "Baptiste Aubertin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present \\textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \\textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses."
        },
        {
            "title": "Start",
            "content": "LIGHTONOCR: 1B END-TO-END MULTILINGUAL VISION-LANGUAGE MODEL FOR STATE-OF-THE-ART OCR 6 2 0 2 0 2 ] . [ 1 1 5 2 4 1 . 1 0 6 2 : r Said Taghadouini Adrien Cavaillès Baptiste Aubertin LightOn"
        },
        {
            "title": "ABSTRACT",
            "content": "We present LightOnOCR-2-1B, 1B-parameter end-to-end multilingual visionlanguage model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9 smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and LightOnOCRbbox-bench evaluation under their respective licenses. Model: Blog: Benchmark: https://huggingface.co/collections/lightonai/lightonocr-2 https://huggingface.co/blog/lightonai/lightonocr-2 https://huggingface.co/datasets/lightonai/LightOnOCR-bbox-bench"
        },
        {
            "title": "Introduction",
            "content": "Despite decades of progress in Optical Character Recognition (OCR), from classical engines such as Tesseract [1], to deep neural sequence recognizers like CRNN [2], and Transformer-based models such as TrOCR [3], real-world documents remain challenging: reading order can be ambiguous in multi-column layouts, tables require consistent structure, and scientific PDFs often mix dense typography with mathematical notation, figures, and noisy scans. common production strategy is to rely on multi-stage pipelines (e.g., layout analysis, text detection, text recognition, table extraction, and reading-order reconstruction), as in systems such as PaddleOCR [4, 5] and MinerU [6]. While effective in many settings, these pipelines couple multiple components and intermediate representations, making them costly to adapt: improving performance on new document distribution often requires additional annotations for intermediate tasks (layout regions, table structure, reading order) and coordinated changes across stages. End-to-end vision-language models (VLMs) reduce this engineering burden by learning extraction directly from pixels to structured text [7, 8, 9, 10, 11, 12]. This enables continuous improvement and specialization via straightforward finetuning, without retooling each stage of pipeline. In this work, we present LightOnOCR-2, compact 1B-parameter end-to-end multilingual VLM that achieves state-of-the-art OCR, outperforming substantially larger systems, and extends OCR with document image localization through predicted bounding boxes. In this context, we release LightOnOCR, compact, end-to-end model that delivers state-of-the-art document understanding with lightning speed and low cost. Competing systems(including newest releases) often rely on multiple moving parts to boost performance, but this added complexity makes them brittle, difficult to train, and prone to break when adapting to new data or domains. LightOnOCR, on the other hand, is single unified model fully differentiable and easy to optimize end-to-end capable of handling complex layouts such as tables, forms, receipts, and scientific notation without fragile multi-stage pipelines. LightOnOCR-2-1B is compact, end-to-end model that builds on our first release [13] with substantially expanded and cleaner training mixture (2.5 larger), with increased coverage of scans, French documents, and scientific content, supported by an improved data curation pipeline. During pretraining, we train at higher resolution (maximum longest edge 1540px), apply data augmentation, and explicitly include empty pages to reduce looping behaviors and improve full-page fidelity. We then apply Reinforcement Learning with Verifiable Rewards (RLVR) [14] to target persistent failure modes that are difficult to address with supervised learning alone, including repetition loops, math rendering and formatting errors, and layout-sensitive consistency constraints enforced through unit-test style checks. We also train variant to predict bounding boxes for embedded images. To avoid degrading OCR quality with naive supervised fine-tuning, we introduce coordinate supervision during pretraining via resume strategy and then refine localization using RLVR with IoU-based objectives. Finally, we leverage lightweight weight-space techniques, checkpoint averaging and task-arithmetic merging, to combine complementary gains and to control the trade-off between OCR quality and bounding box (bbox) accuracy. Contributions To summarize, we make the following contributions: We release LightOnOCR-2-1B, compact 1B-parameter end-to-end multilingual VLM for document parsing that achieves new state-of-the-art result on OlmOCR-Bench, outperforming substantially larger models (e.g., 9B-scale baselines). We scale and improve the pretraining mixture (2.5 larger) with stronger coverage of French and scientific documents, higher-resolution training (max longest edge 1540px), improved normalization (cleaner LATEX handling), and robustness via data augmentations and explicit empty-page targets. We introduce nvpdftex-based arXiv curation pipeline to obtain pixel-aligned supervision from TEX sources, and use it both to strengthen scientific OCR supervision and to generate an automatic subset for our localization benchmark. We add image bounding box prediction in dedicated variants by introducing coordinate supervision during pretraining and refining localization with RLVR using IoU-based objectives. We introduce LightOnOCR-bbox-bench, new benchmark for image localization in documents. We show that lightweight weight-space techniques, checkpoint averaging and task-arithmetic merging, can improve OCR and enable controlled trade-offs between OCR quality and bbox accuracy across released checkpoints."
        },
        {
            "title": "2 Overview",
            "content": "In this section, we describe the LightOnOCR architecture and summarize the training recipe differences between LightOnOCR-1B and LightOnOCR-2-1B. 2.1 Architecture LightOnOCR is compact 1B-parameter vision-language model composed of three main components: vision encoder, multimodal projector, and language model decoder. We train the model to perform OCR without task prompts at inference time, so the extraction behavior is embedded in the weights rather than controlled by an explicit prompt. Vision Encoder We employ native-resolution Vision Transformer initialized from the pretrained Mistral-Small-3.1 [15] vision encoder weights. This encoder handles variable image sizes while preserving spatial structure, which is critical for documents with diverse aspect ratios and fine typographic details. Multimodal Projector To bridge the vision and language modalities while controlling sequence length, we use two-layer MLP with GELU activation that projects vision features into the language models embedding space. Before projection, we apply spatial merging with factor of 2, effectively grouping 2 2 patches and reducing the number of visual tokens by 4. This keeps the overall token count tractable for high-resolution inputs while preserving sufficient spatial granularity. The projector is randomly initialized and trained from scratch. Language Model Decoder We initialize the decoder from pretrained Qwen3 [16]. The decoder produces single, linearized representation of the page that preserves reading order while emitting structured tokens for non-text elements (e.g., ![image](image_N.png)). To simplify the interface between modalities, we remove the image-break and image-end tokens and condition the decoder on single contiguous block of visual tokens (after spatial merging), followed by text tokens. This yields compact end-to-end VLM with consistent generation format across datasets. Initialization By initializing from strong pretrained components, LightOnOCR inherits robust visual representations and multilingual language modeling capabilities, enabling effective transfer to OCR with reduced training cost. 2 Figure 1: Model architecture. 2.2 LightOnOCR-1 The first version of LightOnOCR [13], established the core architecture described above. It was developed using supervised training on the PDF Association (PDFA) dataset [17], using transcriptions generated by Qwen2-VL-72B-Instruct [18] as teacher supervision. Training used maximum longest-edge resolution of 1024 pixels. 2.3 LightOnOCRLightOnOCR-2 keeps the same architecture but significantly updates both the data and the training recipe. We scale the pretraining mixture from 17M to 43M pages (Section 3), with stronger coverage of scanned documents, scientific PDFs, and European languages with an emphasis on French. Supervision quality is improved by leveraging stronger teacher, Qwen3-VL-235B-A22B-Instruct [16], and by improving conversion and normalization so that targets better preserve layout and clean LATEX formatting. We also increase the maximum longest-edge resolution from 1024 to 1540 pixels, improving legibility for small text and dense mathematical notation while retaining dynamic resizing for diverse page sizes. Beyond transcription, we train an image-localization variant that predicts bounding boxes for embedded images by extending the output format with normalized coordinates; to avoid OCR regressions, we introduce coordinate supervision during pretraining via resume strategy and then refine localization with RLVR using IoU-based objectives (Section 4.2). Finally, we apply lightweight weight-space techniques, checkpoint averaging and task-arithmetic merging, to combine complementary gains across runs and to control trade-offs between OCR quality and bbox accuracy."
        },
        {
            "title": "3 Datasets and Preprocessing",
            "content": "3.1 Dataset Scale and Composition Both LightOnOCR versions are trained on large-scale document OCR corpora built primarily through distillation: strong visionlanguage teacher produces naturally ordered transcriptions (Markdown with LATEX spans) from rendered PDF pages. Compared to LightOnOCR-1B, LightOnOCR-2-1B scales up the data mixture and improves supervision quality by upgrading the teacher model from Qwen2-VL-72B-Instruct [18] to Qwen3-VL-235B-A22B-Instruct, yielding more faithful mathematical notation and fewer formatting artifacts. The LightOnOCR-2-1B mixture combines teacher-annotated document pages from multiple permissibly usable sources, including scanned material for robustness, as well as auxiliary data to broaden layout coverage. In addition to full pages, we include document-region crops (paragraphs, headers, abstracts, and general snippets) annotated with GPT-4o [19] to expose the model to varied formats, and we add explicit blank-page examples to enforce consistent target for empty inputs and mitigate looping or hallucination behaviors. To better cover scientific documents, we also incorporate TEX-derived supervision obtained by compiling raw arXiv sources with an nvpdftex [20] pipeline (Section 3.3), and we complement the mix with publicly available OCR datasets for additional diversity [9]. We release the PDFA-derived annotated subset under license terms that match the underlying PDFA source data as lightonai/LightOnOCR-mix-0126. During large-scale annotation, we observed that the teacher occasionally emitted figure bounding box coordinates even when not explicitly prompted to do so. Manual inspection of sampled subset showed these boxes to be highly accurate. To keep the base OCR objective strictly focused on transcription, we removed coordinate traces from the main supervised training targets; however, we retained them as separate supervision signal for the bounding-box addition procedure described in Section 4.2.1. We also reformatted the coordinates to match our target output convention before training. We publicly release this extracted and normalized subset as lightonai/LightOnOCR-bbox-mix-0126. 3.2 Normalization Pipeline and data clean-up As described above, our training corpus is assembled from heterogeneous sources (PDFA, scans, open mixtures, arXiv renders) and multiple VLM teachers, which introduces systematic but superficial inconsistencies in the raw transcriptions: stray Markdown code fences, watermark text, variable image placeholders, templated this page is empty messages, and occasional format drift (e.g., LATEX environments or HTML fragments where Markdown is expected). While these artifacts rarely affect human readability, they significantly increase target entropy and hurt both deduplication and learning stability. We therefore apply unified normalization pipeline that maps all sources to single, canonical target format before mixing and training. Concretely, normalization is implemented as sequence of lightweight, mostly deterministic transforms applied prior to hashing and filtering. We (i) sanitize the text (e.g., remove spurious Markdown ticks/code blocks and harmonize whitespace), and apply source-specific cleanup when needed. Notably, we remove recurring watermark artifacts during preprocessing so they do not pollute the deduplication procedure. (ii) homogenize special cases such as full-page embedded images and blank pages by mapping them to fixed targets (respectively single standard image placeholder and the empty string), and (iii) perform loop and repetition filtering and deduplication on the normalized text. To retain control over the proportion of degenerate-but-common cases, we also separate dedicated pools of empty pages, which were then re-injected at chosen rate during training as mentioned in Section 3.1. Finally, we run LATEX conversion and validation pass to enforce formatting invariants across the full mixture: LATEX commands are restricted to math spans, headers and sections are converted to markdown, tables are standardized (we use HTML targets to reduce ambiguity), and math expressions are checked for KaTeX [21] compatibility. The conversion step emits structured metadata (success/partial/timeout, unresolved references, missing figure numbering, KaTeX compatibility), enabling simple, reproducible filtering rules when constructing the final training mixture. 3.3 nvpdftex data curation pipeline High-quality OCR supervision requires pairing rendered page images with faithful, layout-consistent transcriptions. In earlier iterations we relied on Nougat-style PDF parsing pipeline [7], but in practice we found it difficult to obtain sufficiently reliable (image, markup) pairs at scale for training: conversion errors, reading order issues, and imperfect alignment between rendered pages and extracted markup introduce noise that can hinder learning. For LightOnOCR-2, we revamped the arXiv extraction pipeline around nvpdftex, recently released NVIDIA toolchain that hooks directly into the pdfLATEX engine to produce pixel-aligned annotations without heuristic matching. Concretely, nvpdftex compiles TEX sources and emits (i) PNG rendering of each page, (ii) structured text targets (Markdown 4 or HTML) for each region, (iii) pixel-accurate bounding boxes with semantic classes (e.g. headers/footers, captions, tables, formulas, pictures), and (iv) page-level metadata (e.g. image dimensions) [20, 22]. We additionally use this pipeline to generate high-quality figure/image boxes for the arXiv subset used in our bbox RLVR experiments (Section 4.2.2); these arXiv bounding boxes are used for RL supervision rather than included in the main pretraining mixture. 3.4 LightOnOCR-bbox-bench: New Image Localization Benchmark LightOnOCR-bbox-bench While OCR benchmarks are widely used, there are no standardized benchmarks that specifically measure how well end-to-end vision-language models localize images within documents. To fill this gap, we introduce LightOnOCR-bbox-bench, released with this work and composed of two subsets: (i) manually reviewed subset derived from OlmOCR-Bench [8] pages (290 samples) and (ii) an automatically annotated arXiv subset generated with nvpdftex (565 samples) and filtered programmatically. We report F1 at IoU threshold 0.5, mean IoU, and count accuracy (exact match on the number of predicted boxes), and evaluate separately on the manual OlmOCR-derived subset and the automatic arXiv+nvpdftex subset."
        },
        {
            "title": "4 Training",
            "content": "4.1 Pretraining Training setup We train on large mixed corpus of OCR datasets described in Section 3, with filtering to remove missing references, numberless figures, KaTeX-incompatible samples, and long completions (capped at 3100 tokens). Document pages are rendered at 200 DPI during training (except for crops which were already available as images) and resized to maximum longest edge of 1540 pixels for computational efficiency. We apply moderate document augmentations such as bitmap corruption, erosion/dilation, small affine shears, shiftscalerotate, 90 rotations, and mild grid distortions, so that page receives at least one augmentation with probability 0.22 (and we use more aggressive regime for blank-page examples). We optimize next-token prediction objective but mask part of the loss: the loss is computed only on assistant tokens, excluding prompt tokens (system prompt and special tokens) as well as image tokens. Optimization uses AdamW with no weight decay, peak learning rate of 104, cosine learning rate schedule with 100-step warmup, global batch size of 384 samples, and maximum sequence length of 6144 tokens. Training is distributed with DDP over 96 NVIDIA H100 GPUs (80 GB) using bf16 precision and FlashAttention-2 [23]. 4.2 Reinforcement Learning with Verifiable Rewards We apply RLVR [14], where rewards are computed by automatic checks that can be evaluated deterministically on model outputs (e.g., binary unit tests on synthetic documents). This allows us to directly optimize for specific OCR failure modes without having to annotate extra data. from the pretraining checkpoints Training setup We start (LightOnOCR-2-1B-base for OCR and LightOnOCR-2-1B-bbox-base for localization) and train with GRPO [24] for one epoch with AdamW at learning rate 4 105 and KL regularization strength β = 0.01. We use group-scaled rewards with token-level importance sampling and sample multiple rollouts per prompt (28 for OCR and 14 for bbox). Training is implemented with the Hugging Face TRL library [25], and rollouts are generated with vLLM for increased efficiency. We apply two RLVR recipes: an OCR-focused variant that extends OlmOCR unit tests with additional rewards, and bbox-focused variant that optimizes IoU-based localization rewards. 4.2.1 OlmOCR-2 Style RLVR Recipe We build on the OlmOCR-2 RLVR recipe [9], where rewards are derived from synthetic OlmOCR-Benchstyle unit tests, and extend it with additional checks tailored to scientific documents. In addition to scoring completions by the fraction of tests they pass, we (i) penalize low-entropy repetition loops using compression-based heuristic and ensuring proper EOS termination, (ii) reward mathematical correctness by extracting math spans and verifying they render successfully with KaTeX, (iii) enforce clean math formatting by rejecting common artifacts such as HTML tags, misused Markdown italics for variables, and unbalanced LATEX delimiters or environments, and (iv) remove the frontmatter reward that encourages the model to output document metadata at the top of its response. Finally, for the headers/footers category, we change the absence reward to instead reward presence of headers, footers, and page numbers, encouraging high-fidelity extraction of all visible content. 5 4.2.2 Bounding Box RLVR Recipe To complement OCR with image localization, we extend the output for embedded images to ![image](image_N.png)x1,y1,x2,y2, where coordinates are normalized to [0, 1000]. Throughout pretraining, the model is already trained to emit the image placeholder ![image](image_N.png) whenever an image is present; bounding box learning therefore mainly adds where the image is, not whether it exists. We enable this capability by resuming pretraining from the base checkpoint and continuing on mixture stage that includes bbox-annotated pages, providing cold start for RL. This pretraining integration preserves OCR performance, enabling joint OCR and image localization. format We then further improve image localization with RLVR using an IoU-based reward The full reward definition is given in Appendix D. 4.2.3 Model Averaging and Merging We use checkpoint averaging by souping the last 5 checkpoints, which consistently outperforms any single checkpoint. This yields our strong supervised pretraining baseline LightOnOCR-2-1B-base. We then apply RLVR on top of this base to obtain our best OCR model LightOnOCR-2-1B. In parallel, we train bbox-capable variant by resuming supervised pretraining with coordinate annotations introduced during training, producing LightOnOCR-2-1B-bbox-base, and further refine it with bbox-focused RLVR to obtain LightOnOCR-2-1B-bbox. Finally, we apply task arithmetic merging, θmerge = θbase + α(θrl θbase), to form LightOnOCR-2-1B-ocr-soup by merging LightOnOCR-2-1B into the averaged base with α = 0.4. We then construct LightOnOCR-2-1B-bbox-soup by merging LightOnOCR-2-1B-ocr-soup into the bbox-specialized checkpoint LightOnOCR-2-1B-bbox (used as the base) with α = 0.1, yielding an explicit OCRbbox trade-off without additional training. Our training targets retain headers/footers, and in RLVR we flip OlmOCRs header/footer absence tests to reward presence of this text. We also applied the OCR RLVR recipe to LightOnOCR-1B-1025 to obtain LightOnOCR-1B-1025-GRPO for reference."
        },
        {
            "title": "5 Results",
            "content": "We evaluate LightOnOCR-2-1B on OlmOCR-Bench [8] as our primary OCR benchmark. We additionally report OmniDocBench v1.0 [26] results in Appendix C.2. For localization, we evaluate on LightOnOCR-bbox-bench (Section 3.4). Unless otherwise stated, we use single-pass evaluation without test-time heuristics (e.g., rotation sweeps or retries); inference details for LightOnOCR models are provided in Appendix B. 5.1 OlmOCR-Bench We report OlmOCR-Bench results without the headers/footers category as originally defined, since it rewards omitting visible content (e.g., titles and page numbers) whereas our objective is full-page transcription; systems that explicitly suppress these regions can score well without improving full-page extraction quality, and even empty outputs can achieve perfect score. We evaluate all models without test-time heuristics (e.g., retries or rotation correction) to reflect raw model behavior. Table 1 shows that LightOnOCR-2-1B achieves the highest overall score (83.2 0.9) among evaluated systems, outperforming substantially larger end-to-end models while using only 1B parameters and being end-to-end trainable. Improvements are broad across categories, with particularly strong scores on ArXiv, old scans math, and table-heavy documents, highlighting the benefit of higher-quality data, increased scientific coverage, and higher-resolution training. Comparing LightOnOCR-2-1B-base to LightOnOCR-2-1B isolates the effect of RLVR, which improves overall performance and reduces common generation failures such as repetition loops (Appendix C.4). Introducing bbox prediction (LightOnOCR-2-1B-bbox) yields small OCR quality drop relative to the base checkpoint while enabling localization; task-arithmetic merging partially recover OCR performance and provide an explicit OCRbbox trade-off (bbox-soup). As we optimize for the presence of headers/footers, under the original OlmOCR-Bench definition (rewarding suppression), our headers_footers score decreases; see Appendix C.1 for details. When applying the same recipe to our previous model, we observe similar gains on OCR performance, see LightOnOCR-1B-1025-GRPO in Table 1. 5. Image Bounding Box Detection Table 2 reports bounding-box detection on LightOnOCR-bbox-bench. The goal is to measure whether compact end-to-end OCR model can localize embedded images while retaining strong transcription quality. We compare against 6 Model End-to-end Size (B) ArXiv Old Scans Math Tables Old Scans Multi-column Long Tiny Text Base Overall Mistral OCR 3 API Gemini Flash 2 Qwen2.5-VL-8B olmOCR v0.3.0 MonkeyOCR-pro-3B dots.ocr DeepSeekOCR Chandra-9B olmOCR-2-8B MonkeyOCR-pro-1.2B MinerU2.5 PaddleOCR-VL LightOnOCR-1B-1025 LightOnOCR-1B-1025-GRPO LightOnOCR-2-1B-base LightOnOCR-2-1B-bbox-base LightOnOCR-2-1B-bbox LightOnOCR-2-1B-bbox-soup LightOnOCR-2-1B-ocr-soup LightOnOCR-2-1B - - - - 8 8 3 3 3 9 8 1.2 1.2 0.9 1 1 1 1 1 1 1 85.6 32.1 63.1 78.6 83.8 82.1 77.5 82.2 82.9 80.5 76.6 85.7 81.4 86.5 84.9 84.6 86.9 86.1 86.8 89.6 69.7 56.3 65.7 79.9 68.8 64.2 74.5 80.3 82.1 62.9 54.6 71.0 71.6 73. 80.3 78.6 74.7 77.9 81.2 85.6 85.5 61.4 67.3 72.9 74.7 88.3 77.3 88.0 84.3 71.1 84.9 84.1 76.4 74.5 86.7 84.7 88.6 88.2 89.0 89.0 43.5 27.8 38.6 43.9 36.1 40.9 33.1 50.4 48.3 32.9 33.7 37.8 35.2 32. 47.0 46.0 39.7 41.2 45.4 42.2 81.2 58.7 68.3 77.3 76.6 82.4 67.3 81.2 84.3 68.3 78.2 79.9 80.0 85.1 84.6 83.8 85.0 85.4 84.2 84.8 88.5 84.4 49.1 81.2 80.1 81.2 83.0 92.3 81.4 74.0 81.2 85.7 88.7 91. 89.1 88.0 86.4 87.3 90.3 91.4 99.7 79.1 1.0 94.0 59.2 1.1 98.3 64.3 1.2 98.9 76.1 1.1 95.3 73.6 1.0 99.5 76.9 1.0 99.3 73.1 1.0 99.9 81.7 0.9 99.7 80.4 1.1 92.6 68.9 1.1 83.5 70.4 1.0 98.5 77.5 1.0 99.5 76.1 1.1 99.7 77.7 1.0 99.8 81.8 0.9 99.8 80.8 0.9 99.8 80.2 0.9 99.7 80.8 0.9 99.7 82.4 0.9 99.6 83.2 0.9 Table 1: OlmOCR-Bench results (headers/footers category excluded; see Appendix C.1). Per-column best is highlighted in blue and second best in bold. Results are taken from the corresponding published works; we additionally evaluate DeepSeekOCR and the Mistral OCR 3 API since they do not report OlmOCR-Bench numbers. Inference details for our models are provided in Appendix B. Chandra-9B [12] under the same evaluation protocol and restrict evaluation to visual elements (figures/images), ignoring other layout categories. Table 2: Bounding box detection on LightOnOCR-bbox-bench. Model Chandra-9B LightOnOCR-2-1B-bbox LightOnOCR-2-1B-bbox-soup OlmOCR (290) arXiv (565) F1@0.5 IoU Count Acc. F1@0.5 IoU Count Acc. 0.75 0.78 0. 0.71 0.70 0.67 75.2 83.8 80.7 0.81 0.83 0. 0.77 0.77 0.76 81.8 85.0 85.1 Localization quality LightOnOCR-2-1B-bbox improves F1@0.5 and count accuracy over the 9B baseline on both subsets, while achieving comparable mean IoU. This indicates reliable detection of both the presence and number of figures, with accurate localization, despite the substantially smaller model size. 5.3 Efficiency Beyond accuracy, we measure throughput to characterize the practical speedquality trade-off of end-to-end OCR models. We measure inference efficiency by running the full OlmOCR-Bench evaluation (1,403 pages) end-to-end and reporting pages/sec as the total number of pages divided by the wall-clock time to complete the benchmark. We prefer pages/sec over tokens/sec, since tokenization and output lengths differ across models and formats, making tokens/sec less comparable. Each model was run using its official library and the inference parameters recommended by its respective authors to ensure fair comparison. Table 3 compares LightOnOCR-2-1B against the main end-to-end baselines; full comparison with additional systems is provided in Appendix C.3. Model Dtype Size (B) Throughput (pages/sec) LightOnOCR-2 olmOCR-2 Chandra 1 8 9 Table 3: Inference throughput on single NVIDIA H100 (80 GB). BF16 FP8 BF16 5.71 3.28 1.70 7 These results show that LightOnOCR-2 provides substantially higher throughput than larger end-to-end baselines, making it practical for high-volume document processing."
        },
        {
            "title": "6 Scope and Limitations",
            "content": "LightOnOCR-2-1B models are designed for printed document understanding and perform particularly well on: (i) scientific PDFs, including dense typography and accurate LATEX math transcription (e.g., strong arXiv and old scans math results in Table 1); (ii) scans of typed documents, including moderately degraded, noisy or rotated scans; (iii) European languages and Latin scripts, reflecting the distributional emphasis of the pretraining mixture; and (iv) layout-heavy pages such as multi-column documents and long-form tables, where faithful reading order and structure are critical. Despite strong overall robustness, we note two important limitations. First, multilingual performance outside of European / Latin-script languages is not fully supported: our training mix and normalization pipeline prioritize Latin-script documents, and some non-Latin scripts (e.g., CJK or Arabic) can exhibit degraded fidelity or inefficient tokenization compared to the in-scope languages (as shown in Appendix it is as expected all the more evident for pruned models). Second, handwritten text transcription remains inconsistent: while LightOnOCR-2-1B benefits from scan coverage, its supervision is primarily derived from printed or typeset sources, and cursive or unconstrained handwriting is not target use-case for the released checkpoints. We view these as promising directions for future work through targeted data collection and evaluation."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce LightOnOCR-2-1B 1B-parameter end-to-end OCR VLM setting new state-of-the-art on OlmoOCRBench. Building on top of the LightOnOCR architecture, setup and insight, we detailed the key factors behind its improved performance: substantially larger and cleaner pretraining mixture, stronger document normalization and conversion pipelines, and higher-resolution training with targeted augmentations. We further presented an imagelocalization variant trained to predict bounding boxes without degrading OCR by introducing coordinates during pretraining and refining them with RLVR. Finally, we showed that lightweight weight-space techniques, checkpoint averaging and task-arithmetic merging, provide practical gains and enable explicit control of the OCRbbox trade-off. We release model weights, datasets, and the LightOnOCR-bbox-bench benchmark to support reproducible research on high-fidelity document extraction and localization."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was granted access to the HPC resources of IDRIS under GENCI allocations AS011016449, A0181016214, and A0171015706, enabling us to use the Jean Zay supercomputer. All RL experiments were run on our in-house H200 nodes. We thank Stéphane Réquena and the IDRIS support team for their valuable help. We also thank the LightOn team for their support in making this release possible, with special thanks to Antoine Chaffin and Amélie Chatelain for their help throughout the release process."
        },
        {
            "title": "References",
            "content": "[1] Ray Smith. An overview of the tesseract ocr engine. In Proceedings of the International Conference on Document Analysis and Recognition (ICDAR), 2007. [2] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. arXiv preprint arXiv:1507.05717, 2015. [3] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei. Trocr: Transformer-based optical character recognition with pre-trained models. arXiv preprint arXiv:2109.10282, 2021. [4] PaddlePaddle. Paddleocr, 2025. Accessed 2026-01-17. [5] Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, and Haoshuang Wang. Pp-ocr: practical ultra lightweight ocr system. arXiv preprint arXiv:2009.09941, 2020. [6] anonymous or see arXiv page. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839, 2024. 8 [7] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents. In International Conference on Learning Representations, 2024. [8] Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443, 2025. [9] Jake Poznanski, Luca Soldaini, and Kyle Lo. olmocr 2: Unit test rewards for document ocr. 2025. [10] Yumeng Li, Guang Yang, Hao Liu, Bowen Wang, and Colin Zhang. dots.ocr: Multilingual document layout parsing in single vision-language model. arXiv preprint arXiv:2512.02498, 2025. [11] Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Handong Zheng, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. Paddleocr-vl: Boosting multilingual document parsing via 0.9b ultra-compact vision-language model. arXiv preprint arXiv:2510.14528, 2025. [12] Datalab. Chandra: Ocr model that handles complex tables, forms, handwriting with full layout. https: //github.com/datalab-to/chandra, 2025. GitHub repository, accessed 2026-01-16. [13] Said Taghadouini, Baptiste Aubertin, and Adrien Cavaillès. Lightonocr-1b: Making knowledge machinehttps://www.lighton.ai/lighton-blogs/making-knowledge-machine-readable, 2025. readable. LightOnOCR v1 blog post. [14] Nathan Lambert, Jacob Daniel Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Validad Miranda, Alisa Liu, Nouha Dziri, Xinxi Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Christopher Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hanna Hajishirzi. Tülu 3: Pushing frontiers in open language model post-training. ArXiv, abs/2411.15124, 2024. [15] Mistral AI. Mistral small 3.1. https://mistral.ai/news/mistral-small-3-1/, 2025. [16] Qwen Team. Qwen3 technical report, 2025. [17] Pixparse. Pdf association dataset. https://huggingface.co/datasets/pixparse/pdfa-eng-wds, 2025. Hugging Face Datasets. [18] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [19] OpenAI. Gpt-4o system card. https://cdn.openai.com/gpt-4o-system-card.pdf, August 2024. Accessed: 2026-01-14. [20] NVIDIA. nvtexlive: Fork of tex live with hooks to generate ocr annotations (nvpdftex toolchain). https: //github.com/NVIDIA/nvtexlive, 2025. [21] KaTeX Contributors. KaTeX: Fast math typesetting for the web. https://katex.org/, 2025. Version 0.16.27. [22] Ilia Karmanov et al. éclair: Extracting content and layout with integrated reading order for documents. 2025. [23] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. [24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [25] Hugging Face. TRL: Transformer reinforcement learning, 2026. Accessed: 2026-01-18. [26] Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu, Minghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang, Botian Shi, Zhongying Tu, and Conghui He. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations, 2024. [27] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In International Conference on Learning Representations (ICLR), 2023."
        },
        {
            "title": "A Vocabulary Pruning",
            "content": "The Qwen3 decoder uses 151,936-token multilingual vocabulary, much of which is unused for language-specific OCR. We investigate frequency-based vocabulary pruning for English/French documents, reducing to 51k, 32k, and 16k tokens while preserving tokenizer integrity through recursive sub-token frequency propagation. Table 4 summarizes the trade-offs. Pruning to 16k tokens reduces parameters by 13.8% with minimal OCR degradation on English benchmarks (75.4% vs 76.1% on OlmOCR-Bench). The 32k variant achieves the best speedaccuracy balance: 11.6% faster inference while retaining 96% of base performance. However, non-Latin scripts (Arabic, Chinese) experience 3 token count inflation as script-specific tokens are removed. These experiments were conducted on LightOnOCR-1; we release the pruned variants as LightOnOCR-0.9B-32k-10251 and LightOnOCR-0.9B-16k-10252. Vocab Params (M) OlmOCR Overall Speedup vs Base Tokens/page (EN/ZH) 151k (Base) 51k 32k 16k 1005.6 902.5 883.6 866. 76.1 67.7 73.1 75.4 +9.5% +11.6% +3.9% 475 / 950 485 / 2220 510 / 2750 575 / 3200 Table 4: Vocabulary pruning trade-offs. Smaller vocabularies reduce parameters and improve speed for Latin-script languages but degrade tokenization for Chinese (ZH) and other non-Latin scripts."
        },
        {
            "title": "B Inference Details",
            "content": "LightOnOCR models are evaluated with = 0.2, top_p = 0.9 and top_k = 0. For OlmOCR-Bench, we use three independent generations(3 repeats) per page; maximum resolution set to 1540, no test-time heuristics (retries/rotations) are used."
        },
        {
            "title": "C Additional Results",
            "content": "C.1 OlmOCR Headers/Footers OlmOCR-Bench includes headers_footers category whose unit tests reward absence of header and footer text. In contrast, our training objective is full-page transcription: throughout both pretraining and RLVR, our targets retain all visible content, including page numbers, running headers, titles, and footers. Moreover, in RLVR we explicitly flip the header/footer absence tests to reward presence of header and footer text. As result, when evaluated under the original OlmOCR-Bench scoring, models optimized for full-page extraction can obtain lower headers_footers scores. Table 5 reports this metric for all released checkpoints. Checkpoint OlmOCR headers_footers score LightOnOCR-2-1B LightOnOCR-2-1B-base LightOnOCR-2-1B-bbox LightOnOCR-2-1B-bbox-base LightOnOCR-2-1B-bbox-soup LightOnOCR-2-1B-ocr-soup 19.74 31.05 29.34 31.05 29.47 25. Table 5: OlmOCR-Bench headers_footers scores under the original benchmark definition (rewarding absence of header/footer text). Our models are trained for full-page transcription, so this metric is not aligned with our objective. C.2 OmniDocBench Results We report results on OmniDocBench v1.0 [26] in Table 6. Its language-split reporting provides complementary viewmost notably for reading order and structured layout fidelity. However, OmniDocBench relies heavily on 1https://huggingface.co/lightonai/LightOnOCR-0.9B-32k-1025 2https://huggingface.co/lightonai/LightOnOCR-0.9B-16k-1025 10 edit-distance-based metrics that are sensitive to formatting conventions, and the benchmark primarily targets English and Chinese documents. We therefore treat it as secondary evaluation signal. Model Size (B) OverallEdit TextEdit FormulaEdit FormulaCDM TableTEDS TableEdit Read OrderEdit Gemini2.0-flash Qwen2-VL-72B MonkeyOCR-pro-3B dots.ocr DeepSeek-OCR (Gundam-M) MonkeyOCR-pro-1.2B MinerU2.5 PaddleOCR-VL LightOnOCR-1B-1025 LightOnOCR-1B-1025-GRPO LightOnOCR-2-1B LightOnOCR-2-1B-bbox LightOnOCR-2-1B-base LightOnOCR-2-1B-bbox-base LightOnOCR-2-1B-ocr-soup - 72 3 3 3 1.2 1.2 0. 1 1 1 1 1 1 1 EN ZH EN ZH EN ZH EN ZH EN ZH EN ZH EN 77.6 82.2 0.193 0.206 0.092 0.191 0.264 0.091 0.139 0.389 0.584 0.387 0.408 0.119 0.252 0.327 0.096 0.218 0.404 0.487 0.139 0.111 0.100 0.138 0.206 0.067 0.107 0.246 0.421 0.099 0.092 0.040 0.125 0.160 0.032 0.066 0.329 0.416 0.123 0.157 0.049 0.087 0.242 0.377 0.056 0.08 0.147 0.146 0.221 0.068 0.118 0.272 0.452 76.663 63.282 81.342 85.504 0.149 0.134 0.093 0.089 0.083 0.045 0.111 0.174 0.050 0.074 0.258 0.473 0.093 0.062 0.045 0.105 0.126 0.041 0.062 0.241 0.316 78.9 76.4 87.5 89. 79.7 76.8 81.5 88.6 88.3 88.0 89.2 92.1 43.6 61.2 0.234 0.392 0.090 0.380 0.486 0.621 0.254 0.390 0.267 0.463 0.384 0.586 0.146 0.263 0.082 0.317 0.339 0.438 0.152 0.305 0.080 0.343 0.316 0.523 0.150 0.281 0.059 0.289 0.340 0.530 0.153 0.277 0.062 0.303 0.341 0.489 0.150 0.255 0.071 0.295 0.338 0. 70.1 65.3 85.7 82.1 82.9 82.0 84.3 63.7 59.6 82.7 78.0 80.5 80.9 83.1 0.262 0.307 0.098 0.305 0.316 0.061 0.112 0.132 0.050 0.144 0.171 0.068 0.140 0.144 0.060 0.146 0.145 0.064 0.126 0.124 0. ZH 0.128 0.193 0.185 0.067 0.085 0.179 0.068 0.063 0.259 0.195 0.167 0.181 0.161 0.173 0.164 Table 6: OmniDocBench v1.0 (EN/ZH) results. Per-column best within each size group is highlighted in blue. LightOnOCR-2 shows clear improvement over LightOnOCR-1, with LightOnOCR-2-1B ranking among the strongest models in its size class. C.3 Extended Efficiency Comparison Table 7 extends the efficiency comparison from Section 5.3 to include additional OCR systems. Each model was run using its official library and the inference parameters recommended by its respective authors. We prefer pages/sec over tokens/sec since tokenization and output lengths differ across models, making tokens/sec less comparable. Model Dtype Size (B) Throughput (pages/sec) Speedup vs slowest LightOnOCR-2 olmOCR-2 olmOCR-2 DeepSeek-OCR PaddleOCR-VL Chandra dots.ocr BF16 FP8 BF16 BF16 BF16 BF16 BF16 1 8 8 3 1 9 3 5.71 3.28 2.54 2.36 2.14 1.70 0.88 6.49 3.73 2.89 2.68 2.43 1.93 1.00 Table 7: Full inference throughput comparison on single NVIDIA H100 (80 GB). C.4 RLVR Effect on Repetition Loops To quantify repetition loops, we scan OlmOCR-Bench generations with low-entropy detector based on the ZLIB compression ratio (flagging outputs with ratio < 0.13). Table 8 summarizes the fraction of flagged generations, showing reduction after RLVR, consistent with our repetition-focused reward component (Section 4.2). Checkpoint % Loopy LightOnOCR-2-1B-base LightOnOCR-2-1B 1.14% 0.50% Table 8: Loopy generations on OlmOCR-Bench."
        },
        {
            "title": "D Bounding Box RLVR Reward",
            "content": "Let Igt and Ipred denote the sets of image IDs present in the ground-truth and predicted outputs for page, and let Bgt and Bpred be the corresponding boxes for image ID i. We compute the mean IoU over matched IDs and scale it by an 11 ID-overlap factor: Rbbox = (cid:32) 1 (cid:88) (cid:16)"
        },
        {
            "title": "Bpred\ni",
            "content": ", Bgt (cid:33) (cid:17) iI max(Igt, Ipred) , = Igt Ipred. (1) This formulation rewards accurate localization for correctly predicted image IDs while penalizing missing and hallucinated boxes through the overlap factor. D.1 Task-Arithmetic Model Merging Because OCR quality and localization accuracy can pull model behavior in different directions, we use weight-space merging to expose controllable trade-off between the two directions. We apply task-arithmetic merging [27] to combine an OCR-specialized checkpoint with bounding-box (bbox) specialized checkpoint while explicitly controlling their trade-off. Concretely, we interpolate in weight space as θ = θbbox + α(θocr θbbox) with α [0, 1], and evaluate the resulting models on both OCR and bbox localization metrics (Figure 2). The difference vector (θocr θbbox) can be interpreted as an OCR task vector added to the bbox model with strength α, yielding single merged checkpoint with no additional training cost and unchanged inference. As expected, OCR performance increases with α, but bbox detection degrades beyond α > 0.4 and collapses for α 0.6, suggesting that localization-specific parameters are progressively overwritten. In our experiments, the best balance occurs around α 0.1, which preserves strong bbox quality (IoU= 0.677) while improving OCR performance to 80.88%. Figure 2: Task-arithmetic interpolation between bbox-specialized and an OCR-specialized checkpoint."
        },
        {
            "title": "E Examples",
            "content": "We provide here some examples of transcriptions highlighting the capabilities or the OCR model family. Figure 3: Complex table. Left: Original image. Right: Rendered transcription. Generated with LightOnOCR-2-1B. 13 Figure 4: Example of bounding box generating models. Left: Original image. Right: Rendered transcription, with image crop corresponding to generated bounding box. Generated with LightOnOCR-2-1B-bbox. 14 Figure 5: LightOnOCR-2-1B models excel at transcription of scientific documents. Left: Original image. Right: Rendered transcription. Generated with LightOnOCR-2-1B. 15 Figure 6: Example of transcription outside of distribution. Left: Original image. Right: Rendered transcription. Generated with LightOnOCR-2-1B. 16 Figure 7: LightOnOCR-2-1B models were trained with large portion of scanned documents which translates to improved performance on those types of files compared to v1. Left: Original image. Right: Rendered transcription. Generated with LightOnOCR-2-1B. Source: August Lustig, Sämtliche Werke Zweiter Band (1909), p. 214215. Wikimedia Commons, ALustig_SämtlicheWerke_ZweiterBand_page214_215.pdf, Public Domain (PDM 1.0)"
        }
    ],
    "affiliations": [
        "LightOn"
    ]
}