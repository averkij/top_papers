{
    "paper_title": "SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning",
    "authors": [
        "Prabhat Pandey",
        "Rupak Vignesh Swaminathan",
        "K V Vijay Girish",
        "Arunasish Sen",
        "Jian Xie",
        "Grant P. Strimel",
        "Andreas Schwarz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SIFT (Speech Instruction Fine-Tuning), a 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing a diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, a benchmark dataset specifically designed to evaluate the instruction-following capabilities of speech-text LLMs."
        },
        {
            "title": "Start",
            "content": "SIFT-50M: Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning Prabhat Pandey*1 Rupak Vignesh Swaminathan*1 Vijay Girish*1 Arunasish Sen1 Jian Xie2 Grant P. Strimel1 Andreas Schwarz1 2Apple Inc. 1Amazon AGI Correspondence: {panprabh, swarupak, kvvijayg}@amazon.com"
        },
        {
            "title": "Abstract",
            "content": "We introduce SIFT (Speech Instruction FineTuning), 50M-example dataset designed for instruction fine-tuning and pre-training of speech-text large language models (LLMs). SIFT-50M is built from publicly available speech corpora, which collectively contain 14K hours of speech, and leverages LLMs along with off-the-shelf expert models. The dataset spans five languages, encompassing diverse range of speech understanding as well as controllable speech generation instructions. Using SIFT-50M, we train SIFT-LLM, which outperforms existing speech-text LLMs on instruction-following benchmarks while achieving competitive performance on foundational speech tasks. To support further research, we also introduce EvalSIFT, benchmark dataset specifically designed to evaluate the instructionfollowing capabilities of speech-text LLMs."
        },
        {
            "title": "Introduction",
            "content": "Recent years have witnessed significant advancements in integrating speech and audio capabilities into large language models (LLMs). common approach involves coupling an audio encoder with an LLM by projecting audio embeddings into the models input space (Tang et al.; Yang et al., 2024a; Gong et al., 2023b; Das et al., 2024). This integration preserves rich prosodic and acoustic information beyond textual transcriptions, enabling speechtext LLMs to perform broad range of speech understanding tasks. Another research direction focuses on extending LLM vocabulary to generate discrete audio tokens for speech synthesis (Rubenstein et al., 2023; Du et al., 2023). Speech-text LLMs are typically trained on existing task-specific speech corpora, which are primarily designed for automatic speech recognition (ASR). These datasets predominantly consist of paired audio and task-specific labels rather than natural language instructions, limiting their utility for instruction-following training. This lack of diverse, large-scale instruction datasets poses challenges in generalizing to broader speech understanding tasks. To address this gap, we introduce SIFT-50M1 (Speech Instruction Fine-Tuning), large-scale multilingual dataset covering five languages for instruction-based speech model training. SIFT-50M augments existing speech datasets with instruction-based question-answer (QA) pairs for speech understanding and includes approximately 5M examples for controllable speech generation. In summary, our main contributions are as follows: (1) We release SIFT-50M, 50M-example multilingual instruction dataset which, to the best of our knowledge, is the largest publicly available instruction dataset for speech understanding tasks. (2) We introduce EvalSIFT, benchmark dataset designed for systematically evaluating speech-text LLMs across multiple dimensions. (3) We develop SIFT-LLM, speech-text LLM trained on SIFT-50M, achieving state-of-the-art performance on instruction-following benchmarks compared to publicly available models of similar size. (4) We conduct controllable speech generation experiments using SIFT-50M, demonstrating the models ability to generate speech in instructed styles."
        },
        {
            "title": "2 Related Work",
            "content": "Speech-text LLMs: Several speech-text LLMs have been introduced recently. Early works focused on specific tasks such as ASR (Fathullah et al., 2024; Yu et al., 2024) and second-pass ASR rescoring (Li et al., 2023b). SALMONN (Tang et al.) integrates Whisper (Radford et al., 2023) and BEATs (Chen et al., 2023) encoders with pre-trained LLM using window-level Q-former *Equal contribution. Work done while at Amazon. 1The dataset can be accessed at https://huggingface. co/datasets/amazon-agi/SIFT-50M 5 2 0 2 2 ] . e [ 1 1 8 0 9 0 . 4 0 5 2 : r (Li et al., 2023a). Qwen-Audio (Chu et al., 2023) and Qwen2-Audio (Chu et al., 2024) are trained on over 30 speech, audio, and music tasks. WavLLM (Hu et al., 2024) incorporates two speech encoders, Whisper-large-v2 and WavLM-base (Chen et al., 2022), while LTU-AS (Gong et al., 2023b) integrates Whisper-large with Llama-7B (Touvron et al., 2023) via time and layerwise transformer (Gong et al., 2023a). These models process continuous audio representations, projecting encoded audio into the LLMs input space. To support both understanding and generation, AudioPaLM (Rubenstein et al., 2023) and LauraGPT (Du et al., 2023) extend LLM vocabulary with discrete audio tokens. AudioPaLM discretizes embeddings from USM (Zhang et al., 2023) and w2v-BERT (Chung et al., 2021) using kmeans clustering, extending LLM vocabulary with discrete audio tokens. LauraGPT adopts hybrid approach, utilizing conformer-based encoder for continuous representation of input speech while generating discrete EnCodec (Défossez et al., 2022) codes for output speech. Instruction Fine-Tuning Datasets: Most prior speech-text LLMs rely on task-specific datasets with instructions generated by LLMs. To improve task diversity, WavLLM (Hu et al., 2024) created 3K-hour speech question-answering dataset using GPT-4 based on transcriptions from Librispeech (Panayotov et al., 2015), the AMI Meeting Corpus (Carletta et al., 2005), Fisher (Cieri et al.), and Switchboard (Godfrey et al., 1992). Additionally, they developed multi-task dataset by merging independent training examples into single multitask instructions. Another relevant dataset is OpenASQA (Gong et al., 2023b), publicly available instruction finetuning dataset with 2.7M examples for speech and audio tasks. Its construction methodology, closely related to ours, leverages datasets such as IEMOCAP (Busso et al., 2008), LibriTTS (Zen et al., 2019), VoxCeleb2 (Chung et al., 2018), and MOSEI (Zadeh et al., 2018). OpenASQA augments existing annotations (e.g., transcription, emotion, gender) with metadata such as speed, pitch, and energy, and then uses this enriched metadata to prompt GPT-3.5-Turbo to generate QA pairs. In contrast, SIFT-50M is an order of magnitude larger than OpenASQA, covering broader range of speech dimensions (e.g., accent, age, gender, pitch, speaking rate, word alignment, room characteristics, noise level, distortion) while also supporting instructions in four additional languages beyond English. Controllable Speech Generation: Promptbased speech generation, enabling controllable synthesis of acoustic characteristics such as gender, speaking rate, pitch, and loudness, has become prominent area of investigation (Guo et al., 2023; Shimizu et al., 2024). (Kawamura et al., 2024) annotated the LibriTTS corpus (Zen et al., 2019) with textual prompts to augment these specific speech properties. While (Lyth and King, 2024) detailed method for annotating substantial 45K-hour corpus for high-fidelity speech generation, these prompts are not publicly available, limiting reproducibility and benchmarking. Conversely, SIFT50M offers speech generation instructions across five languages, enabling control over broader spectrum of parameters, including gender, accent, age, speaking rate, pitch, and intensity."
        },
        {
            "title": "3 SIFT Dataset Construction",
            "content": "To construct SIFT-50M, we utilize three publicly available multilingual speech data sources: Multilingual Librispeech (Pratap et al., 2020), Common Voice (Ardila et al., 2019), and VCTK (Yamagishi et al., 2019). These sources provide diverse range of speakers, speech styles, languages, and accents. We augment the existing metadata with additional features extracted from the speech data and generate natural language instructions from the enriched metadata with the help of an LLM. Finally, we employ feedback loop involving verification and iterative prompt refinement to improve the generation process."
        },
        {
            "title": "3.1 Metadata Extraction",
            "content": "We augment the source speech datasetswhich typically include content-level characteristics, with additional acoustic-level features such as pitch, speaking rate, intensity, duration, reverberation, and noise level. We extract pitch using PYYAPT (Kasi and Zahorian, 2002) and intensity (in decibels) using Parselmouth (Jadoul et al., 2018). Phonemes are derived from transcripts via grapheme-to-phoneme (G2P) model (Park, 2019), while noise, speech quality, and reverberation features are computed following the method described in (Lacombe, 2024). Content-level attributes (e.g., transcription, gender, age, and accent) are provided in the source datasets, although accent and age information is available only for random subset of utterances in the Common Voice and VCTK 2 Figure 1: Block diagram showing the stages of SIFT-50M dataset construction. For non-English data generation, we substitute the metadata mapping with the respective language and prompt the LLM to generate responses in that language. datasets. When gender label is missing, we employ gender classification model (Huh, 2024) to fill in the gap. We then convert numerical utterance-level statistics into categorical ranges for pitch, intensity, age, reverberation, noise level, and speech quality based on the distribution of each variable within dataset. For example, pitch is binned separately for male and female speakers due to their different typical ranges. Specifically, for male speaker, an average pitch metadata is mapped to slightly low pitch when the median pitch falls within 125 < fmedian < 138 Hz, while for female speaker the corresponding range is 166 < fmedian < 182 Hz. Age is similarly grouped into broad categories (e.g., twenties, thirties, . . . , nineties) to both de-identify and generalize the attribute. For detailed list of categorical values, please refer to Appendix Section B.1. To generate word-level metadata, we extract time alignments for each word using our in-house hybrid HMM-LSTM acoustic model (Sak, 2014) for forced alignment. The start and end times of each word are then used to derive word-level pitch and intensity statistics, which are subsequently converted to categorical values. For non-English languages (French, German, Spanish, and Italian), we map the metadata characteristics and their corresponding categorical values to the target language using phrase-level translation dictionary."
        },
        {
            "title": "3.2 Natural Language Instruction Generation",
            "content": "Our instructions are organized into three main categories: closed-ended, open-ended, and controllable generation. To generate instruction-response pairs, we feed categorical metadata to the LLM and prompt it to produce up to 10 QA pairs per speech utterance, as shown in Figure 1. For multilingual instruction generation, the LLM is prompted to generate instructions in the specified language using the corresponding mapped metadata. Table 1 provides examples of QA pairs for different categories in the SIFT-50M dataset. High-level statistics of SIFT-50M are presented in Table 2. Closed-ended Instructions: These instructions are subdivided into four sub-categories: (1) Acoustic-Level, (2) Content-Level, (3) Word-Align, and (4) Comparison types. The first three subcategories, generated using Mixtral 8x7B (Jiang et al., 2024), are based on the respective metadata types. As reported in LTU (Gong et al., 2023c), closed-ended instructions are crucial for guiding the model to understand speech and follow instructions without hallucinating. Unlike LTUs OpenASQA data, which relies on pre-processed questions and strict answer templates, our approach leverages careful prompt engineering to allow the 3 Closed-ended: Acoustic-Level Question: How would you characterize the speaking rate in this audio? Answer: The speaking rate in this audio is slightly slow. Closed-ended: Word-Align Question: What is the average pitch for the word bent and how does it compare to the average pitch of the surrounding words? Answer: The average pitch for the word bent is quite low. The previous word upon has similar average pitch, while the following word upon has slightly lower pitch. Overall, there is not significant change in pitch for these words. Closed-ended: Comparison Question: Which audio has speaker with Scottish accent? Answer: The second audio has speaker with Scottish accent, whereas the first audio has speaker with an English accent. Open-ended Question: Analyze the phonetic structure of the audio and discuss how the phonemes contribute to the overall perception of the speakers accent and age category. Answer: The phonetic structure, including phonemes like IH1, T, S, IY1, M, , AH0, W, EY1, S, T, AH1, V, T, AE1, L, AH0, N, T, S, reflects an Irish accent characterized by distinct vowel sounds and consonant clusters. The precise articulation and the youthful intonation patterns suggest the speaker is in their twenties. Controllable Generation Question: Please synthesize speech sample with very low pitch and quite monotone pitch variation, spoken quite slowly and at moderate volume. The speaker should be an adult woman in her fifties with United States English accent. Given the transcription text as: In my book of memory, time writes tales of moments past. Answer: <reference to original audio> Category Closed-Ended Acoustic-level Content-level Word-Align Comparison Open-Ended Controllable Generation Total #Samples (train/dev/EvalSIFT) 17.8M / 100K / 2.5K 14.5M / 80K / 2.5K 9.8M / 40K / 2.5K 3.6M / 100K / 2.5K 4.3M / 100K / 10K 5.6M / 50K / 10K 55.6M / 470K / 30K Table 2: High-level statistics of SIFT-50M dataset showing the distribution of categories pooled across the source speech datasets and languages. diverse questions, with answers inferred from the metadata rather than provided directly by it. This approach encourages the model to produce thoughtful and detailed responses. We employ Amazon Nova Pro to generate open-ended dataset. Controllable Generation: This category includes instructions for expressive speech synthesis from text with specific speech characteristics. These instructions are generated using Mixtral 8x7B based on the provided acoustic and content-level metadata. EvalSIFT: We also release benchmark, EvalSIFT, constructed in manner similar to SIFT-50M. For each of the five languages, we generate 2K examples per category (closed-ended, open-ended, and controllable generation), yielding total of 30K examples. Table 1: Representative examples from SIFT-50M for different categories of instruction types."
        },
        {
            "title": "3.3 Quality Assurance",
            "content": "model to construct diverse and generalizable QA pairs. In addition, language field is incorporated into the metadata (by sampling examples from the top-20 locales in the Common Voice dataset) to facilitate the generation of instructions for the acoustic-based language identification (LID) task. We introduce novel task in the form of Comparison instructions, where the LLM compares two audio files based on their speech characteristics. The LLM is provided with metadata from two distinct audio files and is prompted to generate comparative questions and free-form answers rather than binary responses. Due to the increased complexity of this task, we employ Amazon Nova Pro (Intelligence, 2024), which has demonstrated stronger performance on text benchmarks compared to Mixtral 8x7B. Open-ended Instructions: In this category, the LLM is prompted to generate more complex and During the metadata extraction stage, we reject any values that fall outside the feasible ranges for the respective speech characteristics, ensuring accurate mapping from numerical to categorical values. High-quality, well-crafted LLM-generated data is essential for efficient model fine-tuning and improving the models ability to follow unseen instructions. We employ two types of quality control over the LLM-generated datasets. First, the authors perform an exhaustive review of the LLM responses across all categories and languages. We establish feedback loop with iterative prompt refinement to ensure that the models responses are cogent, meet our expectations, and are free from hallucinations. Second, we conduct ablation studies on the generated data by evaluating performance on the SIFT-50M development sets, which allows us to identify and discard problematic data or even entire categories. For example, we found that instructions based on speech-text alignment that include numer4 ical values (e.g., What words occur between the 3 and 5 second mark?) significantly degraded model performance. Consequently, we retain only instructions with non-numerical values (i.e., instructions similar to the word-align category shown in Table 1). We release the rejected portion as bonus research partition to support future research directions."
        },
        {
            "title": "4 Experimental Setup",
            "content": "To evaluate the effectiveness of SIFT-50M, we train speech-text LLM, called SIFT-LLM, on our dataset. In this section, we detail the model architecture and training configuration."
        },
        {
            "title": "4.1 SIFT-LLM Architecture",
            "content": "Acoustic Encoders: We adopt hybrid speech representation approach: input speech is encoded as continuous embeddings, while output speech is generated as discrete tokens. Specifically, we use Whisper-medium (Radford et al., 2023) as the encoder to process input audio. To produce discretized speech tokens, we apply k-means clustering on HuBERT embeddings (Lakhotia et al., 2021), which primarily capture the semantic content of the speech. These discrete tokens are subsequently converted to audio using vocoder based on the Big-VGan architecture (Lee et al., 2023). Large Language Model: Our core LLM is Qwen2.5-7B-instruct (Yang et al., 2024a). We add linear layer on top of the Whisper-medium encoder to project the 1024-dimensional speech embeddings into the LLMs 3584-dimensional space. Additionally, we expand the LLMs vocabulary by including 2000 k-means clustered HuBERT speech tokens. We employ low-rank adaptation (Hu et al., 2021) to train the LLM parameters."
        },
        {
            "title": "4.2 Model Training",
            "content": "Continued Pre-training: To align the audio embedding space of the pre-trained Whisper encoder with the input representation of the Qwen2.5 LLM and to train the randomly initialized projection layer, we perform continued pre-training of SIFT-LLM. This phase involves training on mixture of speech understanding and generation tasksnamely, ASR, emotion recognition (ER), speaker-attributed ASR (SA-ASR), intent classification (IC), slot entity recognition (SER), speechto-text translation (S2TT), speech-to-speech translation (S2ST), and text-to-speech (TTS). We convert the IC task into multiple-choice question format by providing possible intent values in the instructions. All tasks are represented using natural language instructions. In addition to SIFT-50M, we also release the prompt templates used during this stage. During pre-training, the acoustic encoder remains frozen, and we train only the linear connector layer and the LoRA parameters of the LLM. We set the LoRA rank to 16, resulting in 8.7M learnable parameters, and train for total of 200K steps. The resulting pre-trained models are referred to as SIFT-LLM PT. For controllable generation experiments, we continue pre-training for an additional 200K steps, using LoRA rank of 128 and making the core LLMs embedding layer fully trainable. In this stage, the total number of learnable parameters is 590M, and we increase the weights of the TTS and S2ST datasets. Instruction Fine-tuning: Following continued pre-training, we perform an instruction finetuning stage using the SIFT-50M dataset. We conduct separate fine-tuning processes for speech understanding and controllable generation. For speech understanding, training is initialized from the 50Kth SIFT-LLM PT checkpoint and uses only the speech understanding instructions (closedand open-ended) from SIFT-50M. The number of trainable parameters remains the same as during pretraining for the first 200K steps. We refer to this model as SIFT-LLM. For controllable generation, training is initialized from the 400Kth SIFT-LLM PT checkpoint and focuses exclusively on controllable generation instructions, with the number of trainable parameters unchanged from the last 200K steps of pre-training. This model is referred to as SIFT-LLM GEN. For further details on training datasets, prompts, and setup, we refer the reader to Appendix D."
        },
        {
            "title": "5.1 Evaluation Setup",
            "content": "We evaluate SIFT-LLM by benchmarking it against publicly available speech-text LLMs of similar sizes, including SALMONN-7B and Qwen2Audio-7B-Instruct. For direct comparison between the SIFT-50M dataset and the OpenASQA dataset, we train speech-text LLM using the same pre-trained model as SIFT-LLM but finetuned on OpenASQA; we refer to this model as O-ASQA-LLM. Across all evaluations, we use Temperature= 0.1, Top P= 0.95, repetition 5 Model Closed-Ended Open-Ended Dynamic-Superb Tasks DS-1 EvalSIFT AB-Chat EvalSIFT Audio PL Semt. Degrd. Content Speaker SALMONN-7B Qwen2-Audio-Inst. O-ASQA-LLM SIFT-LLM (ours) 34.7 48.0 45.9 57.4 21.9 25.1 22.9 46.1 6.4 7.2 6.6 7.3 6.0 7.3 4.7 7.8 31.7 53.5 28.5 37.5 30.5 28.9 30.0 42. 47.5 40.3 38.6 51.3 30.0 43.9 45.9 63.6 45.2 70.6 72.3 75.6 31.9 43.6 40.7 47.7 Table 3: Evaluation results of speech-text LLMs on Dynamic-Superb (DS-1), AIR-Bench Chat (AB-Chat), and EvalSIFT (English). We report accuracy (in %) for closed-ended evaluations and LLM score (0 to 10) for open-ended evaluations. The adjacent table provides breakdown by task categories in Dynamic-Superb Audio, Paralinguistics (PL), Semantics (Semt.), Degradation (Degrd.), Content, and Speaker. Bold values indicate the best results, and underlined values indicate the second-best results. Task (Metric) ASR (WER ) ER (Acc. ) IC (Acc. ) SER (Acc. ) S2TT (BLEU ) Test Set SALMN QwA-Inst OASQAL SIFT-LLM SIFT-LLM PT 50k-ckp 200k-ckp LS-Clean LS-Other PS FLEURS-en FLEURS-5 MSP-test1 MSP-test2 SLURP SLURP de en fr en it en es en 2.5 5.7 22.2 9.2 23.7 38.5 28. 58.4 26.4 21.3 20.6 18.9 21.6 4.8 7.4 29.4 9.1 13.2 40.0 32.1 86.0 52.1 31.4 34.2 33.6 36. 3.8 8.1 19.4 11.3 23.1 38.6 40.4 68.0 48.3 28.9 29.8 29.4 33.1 3.5 7.5 26.0 8.1 11.4 53.6 50. 92.7 71.3 29.2 30.9 31.7 35.5 2.5 5.4 24.4 6.6 9.0 54.3 52.4 94.8 73.1 34.4 36.3 36.0 39. 2.3 5.0 24.9 6.4 8.2 54.3 52.7 96.0 72.7 35.2 37.0 36.8 40.4 Table 4: Evaluation results for SALMONN-7B (SALMN), Qwen2-Audio-Instruct (QwA-Inst), O-ASQA-LLM (OASQAL), and SIFT-LLM on foundational tasks, along with results for SIFT-LLM PT 50K and 200K checkpoints. Accuracies (Acc.) are reported as percentages. FLEURS-5 includes test sets for five languages: en, de, fr, it, and es. PS denotes the Peoples Speech dataset. S2TT results are reported on CoVoST2. Bold values indicate the best results, while underlined values represent the second-best results among instruction fine-tuned models. penalty of 1.1 (Keskar et al., 2019), and set the no_repeat_ngram size to 3 (Wolf, 2020). 5.2 Instruction-following Evaluation Benchmarks: Several benchmarks have been proposed for evaluating instruction-following speechtext LLMs. For example, Dynamic-Superb (Huang et al., 2024b) originally comprised 55 speech and audio tasks and was later expanded to 180 tasks in its Phase-2 release (Huang et al., 2024a). AIR-Bench (Yang et al., 2024b) is composed of two parts: Foundation, which covers 19 standard speech tasks, and Chat, which includes open-ended questions. We use Dynamic-Superb (DS-1)2 for closed-ended evaluations, and the Chat partition of AIR-Bench for open-ended evaluation, selecting only the speech subset (excluding sound and music). Additionally, we report metrics on both the closedand open-ended categories of EvalSIFT. SIFT-LLM and O-ASQA-LLM accept any number of text and audio segments in all possible permutations, whereas Qwen2-Audio-Instruct and SALMONN-7B accept only single audio input. To address this limitation during evaluation for comparison tasks in EvalSIFT or for tasks in Dynamic-Superb that involve multiple audio segments, we concatenate the segments with 1second silence between each. Metrics: While exact match accuracy is common metric for closed-ended or classification tasks, it becomes less reliable for speech-text LLMs, which tend to produce verbose, open-ended responses. Therefore, we adopt an LLM-as-a-judge strategy (Zheng et al., 2023) similar to that used in DS-1 and AIR-Bench, employing Claude-3.5Sonnet3 for evaluation. Using chain-of-thought 2Phase-2 data was not available at the time of writing. 3https://aws.amazon.com/bedrock/claude/ 6 reasoning prompt (Wei et al., 2022), we provide Claude-3.5-Sonnet with both the question and the reference answer. For closed-ended and classification benchmarks, we prompt it to classify responses as correct or incorrect, while for open-ended benchmarks, we ask it to score the responses on scale from 0 to 10 based on their alignment with the reference answer. We repeated LLM scoring multiple times and found that the width of 95% confidence interval for the mean accuracy (in %) was less than 0.1 and for the mean score was less than 0.01. Results: Table 3 summarizes the performance of various models on instruction-following benchmarks. SIFT-LLM achieves competitive results across all benchmarks and outperforms all other models. Notably, it performs better than O-ASQALLM on every benchmark, underscoring the importance of the large-scale and diverse SIFT-50M dataset. Furthermore, SIFT-LLM is second-best on the Audio task in the Dynamic-Superb benchmark, despite not being explicitly trained on audio tasks. We observed that the performance gap between SIFT-LLM and other models is less for Contentlevel instructions in EvalSIFT, but significantly larger for Acoustic-level, Word-Align, and Comparison type instructions, highlighting the value of data addressing these dimensions in the SIFT50M dataset. Finally, the model shows signs of reasoning abilities, as evidenced by the examples in Appendix H, where SIFT-LLM reasons through open-ended questions before arriving at its conclusions. Results on non-English languages: Since both Dynamic-Superb and AIR-Bench Chat benchmarks provide instructions exclusively in English, we use EvalSIFT for multilingual evaluation. The results are reported in Table 5. SIFT-LLM outperforms SALMONN-7B and Qwen2-Audio-Instruct on closed-ended evaluations across languages. However, Qwen2-Audio-Instruct remains competitive on open-ended evaluations, as observed with English data as well. Overall, absolute accuracy is lower on non-English languages compared to English. Detailed evaluation results are provided in Appendix F."
        },
        {
            "title": "5.3 Evaluation on Foundational Tasks",
            "content": "In addition to the instruction-following evaluation, we assess performance on standard speech understanding tasks using task-specific metrics: Word Error Rate (WER) for ASR, accuracy for ER, IC, and SER tasks, and BLEU for S2TT. Since model Model German French Italian Spanish SAL QwA SIFL 15.0 4.3 18.6 6.0 39.0 6.6 16.3 5.0 18.8 6.8 34.3 7.1 14.3 5.0 18.2 7.2 33.2 7.5 16.7 5.4 21.2 7.3 35.6 7.0 Table 5: Evaluation results for SALMONN-7B (SAL), Qwen2-Audio-Instruct (QwA), and SIFT-LLM (SIFL) on EvalSIFT for non-English languages. Accuracy (in %) for the closed-ended category and LLM score (010) for the open-ended category are separated by pipe (). Bold values indicate the best results. performance can be sensitive to the prompts used (Wang et al., 2024), we use the prompts recommended by the corresponding model wherever possible. For ER, IC, and SER tasks, we leverage Claude-3.5-Sonnet to classify responses as correct or incorrect, ensuring robustness against modelspecific output formats. Sequence generation tasks, such as ASR and S2TT pose additional challenges due to some models producing extra text; to address this, we use regular expressions and Claude-3.5Sonnet to parse model hypotheses. Furthermore, we apply Whisper text normalization4 before computing WER. The results, presented in Table 4, reveal that no single model excels across all tasks. While SALMONN-7B achieves the best performance on ASR benchmarks for English, Qwen2-AudioInstruct outperforms the others on S2TT, and SIFTLLM significantly outperforms on ER and spoken language understanding tasks. On the multilingual ASR task, SIFT-LLM and Qwen2-Audio-Instruct show significantly better results than SALMONN7B and O-ASQA-LLM. Compared to the pretrained model from which SIFT-LLM is initialized (the 50Kth checkpoint), SIFT-LLM shows slight decline in performance across tasksan observation consistent with findings for Qwen2-Audio and its instruction fine-tuned variant (Wang et al., 2024). Furthermore, performance differences between the 50K and 200K pre-trained checkpoints are minimal across all tasks."
        },
        {
            "title": "5.4 Evaluation on Controllable Generation",
            "content": "We provide baseline numbers on the controllable generation set of EvalSIFT for the SIFT-LLM GEN model. To assess controllability, we compare the acoustic metadata extracted from the generated speech with the controllability parameter specified 4https://github.com/openai/whisper/tree/main/ whisper/normalizers 7 Feature MAE () QWK () Setup Pitch variation Speaking rate Intensity 0.99 1.05e2 0.65 0.35e2 0.18 0.16e2 0.15 2.17e2 0.46 0.49e2 0.02 0.57e2 Table 6: Evaluation of SIFT-LLM GEN on the controllable generation set of EvalSIFT. We report MAE and QWK metrics by comparing speech characteristics from SIFT-LLM GENs generated audio with those specified in the instructions. The results are presented as mean standard deviation, computed over 10 random speaker combinations in the vocoder, each with 10 inference iterations at temperature = 0.8. in the instruction. We report two metrics: Mean Absolute Error (MAE) and Quadratic Weighted Kappa (QWK). The distance-based metrics are computed on values categorized into ordinal groupsfor example, speaking rate is scored on scale ranging from very slowly (0) to very fast (6). We focus on stationary characteristics such as pitch variation, speaking rate, and intensity to avoid bias from the reference speaker used during vocoder generation. We set the sampling temperature to 0.8 during the inference to balance diversity and coherence in generation. The results, presented in Table 6, show that the MAE value for each category is less than 1, indicating that, on average, predictions deviate from the true category by less than one step. It is important to note that the speech characteristics of the generated waveform are influenced both by the non-semantic information preserved in the speech codes (HuBERT codes, in this case) and the reference speaker signal used in the vocoder, if any. Additional details on the impact of the reference speaker are provided in Appendix F.3."
        },
        {
            "title": "5.5 Ablation Studies",
            "content": "For ablation studies, the default SIFT-LLM configuration employs LoRA rank of 8 and is trained exclusively on the English subset of SIFT-50M during instruction fine-tuning. The remainder of the setup is identical to that described in Section 4.2. Factors Influencing Instruction-Following: Table 7 summarizes the key factors impacting the models instruction-following abilities. Extended pre-training can cause the core LLM to lose some of its instruction-following capability. While eliminating pre-training leads to only moderate degradation on instruction-following benchmarks, it results in significant decline in performance on foundational tasks like ASR and S2TT. Increasing the LoRA rank from 8 to 16 yields mixed results, DS-1 EvalSIFT (Closed) EvalSIFT (Open) Default Init. from 200K ckpt No Pre-training LoRA rank=16 LoRA rank=32 No open-ended data No word-align data No comparison data 57.3 52.5 56.7 58.4 55.7 54.3 57.1 56. 45.4 42.3 43.7 45.1 43.9 42.2 41.5 34.3 8.0 7.7 7.9 8.0 7.8 6.1 8.0 7.3 Table 7: Evaluation results on DS-1 and EvalSIFT (English) for various training setups. The Default setup is initialized from the 50Kth PT checkpoint and instructionfinetuned using LoRA (rank = 8). Metrics reported are accuracy (in %) for DS-1 and closed-ended EvalSIFT, and LLM score (010) for the open-ended EvalSIFT. Figure 2: Effect of SIFT data volume used during instruction fine-tuning on SIFT-LLMs performance, as measured on DS-1, AIR-Bench Chat, and EvalSIFT. but raising it further to 32 results in degradation. Training exclusively on closed-ended datasets significantly impairs open-ended performance, and it also negatively affects accuracy on the closedended set. Additionally, omitting comparison data leads to marked drop in performance on closedended EvalSIFT, especially for comparison instructions, since comparison data is the only subset of EvalSIFT that involves multiple audio inputs. Effect of Data Volume: To assess the impact of data volume during instruction fine-tuning, we trained SIFT-LLM using varying fractions of the SIFT-50M dataset: half, one-quarter, and oneeighth of the total volume. The evaluation results, shown in Figure 2, indicate that even with just one-eighth of the data, the model achieves strong performance on open-ended benchmarks. However, increasing the data volume leads to further improvements on closed-ended instructions."
        },
        {
            "title": "6 Conclusions",
            "content": "In this work, we introduce SIFT-50M, large-scale multilingual dataset designed for instruction finetuning of speech-text LLMs. By integrating diverse set of instructions spanning various speech understanding tasks, SIFT-50M not only facilitates instruction-following but also helps models generalize better to unseen instructions. Our experimental results show that SIFT-LLM, our model trained on SIFT-50M, achieves strong performance on instruction-following benchmarks and competitive results on foundational speech understanding tasks. Moreover, we present EvalSIFT, benchmark dataset tailored for the systematic evaluation of speech-text LLMs for both speech understanding and controllable generation."
        },
        {
            "title": "Limitations",
            "content": "While SIFT-LLM demonstrates strong performance on instruction-following benchmarks, it does not achieve state-of-the-art results on foundational speech tasks. Future work could explore the tradeoff between enhancing instruction-following generalization and maintaining competitive performance on foundational tasks relative to task-specific models. Additionally, SIFT-LLM occasionally generates hallucinated responses when queried about content unrelated to the input audio. To mitigate this, we generated additional instruction data by prompting the LLM to produce questions unrelated to the audio along with appropriate answers. However, assigning higher weight to this auxiliary dataset during training resulted in reduced speech understanding capability. As LLMs continue to improve, the LLM-as-a-judge approach has become increasingly common for subjective evaluations. However, the reported metrics can vary depending on the LLM used. We observed that the variation was significantly low when running evaluations multiple times with the same LLM. Furthermore, speech-text LLMs have been found to be sensitive to the prompts used for evaluating foundational tasks such as ASR and S2TT. Although we made every effort to use the prompts recommended by each respective model, the necessary information was not available for some tasks or was provided only for the pre-trained variants."
        },
        {
            "title": "Ethics Statement",
            "content": "The proposed SIFT-50M dataset is constructed from three publicly available and widely used speech corpora: Common Voice, MLS, and VCTK. The SIFT-50M dataset is released under the CDLASharing-1.0 license. We acknowledge the potential risks associated with the capability of controllable speech generation, such as misuse for deepfakes, voice impersonation, or the spread of misinformation. Although this task involves inherent complexities, our dataset is curated to limit control to generic, broad parameters. SIFT-LLM only extracts, and we only use non-specific features that cannot be used for identification."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Leif Rädel, Hayoon Kim, Melanie McHale, Akshat Jaiswal, Subhadeep Maji, Aparna Khare, Anna Piunova, Mohammed Hethnawi, Harish Mallidi, Haolin Chen, Mateusz Łajszczak, Luyao Wang, Berke Toptas, Sunwoo Kim, Jing Liu, Yang Li, Ajay Srinivasamurthy, Nathan Susanj, Volker Leutnant, Sri Garimella, and Athanasis Mouchtaris for their support and valuable feedback."
        },
        {
            "title": "References",
            "content": "Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2019. Common voice: massivelyarXiv preprint multilingual arXiv:1912.06670. speech corpus. Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser. 2020. Slurp: spoken language understanding resource package. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 72527262. Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette Chang, Sungbok Lee, and Shrikanth Narayanan. 2008. Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation, 42:335359. Jean Carletta, Simone Ashby, Sebastien Bourban, Mike Flynn, Mael Guillemot, Thomas Hain, Jaroslav Kadlec, Vasilis Karaiskos, Wessel Kraaij, Melissa Kronenthal, et al. 2005. The ami meeting corpus: pre-announcement. In International workshop on machine learning for multimodal interaction, pages 2839. Springer. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022. Wavlm: Large-scale self-supervised pre-training for 9 full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518. Sanyuan Chen, Yu Wu, Chengyi Wang, Shujie Liu, Daniel Tompkins, Zhuo Chen, Wanxiang Che, Xiangzhan Yu, and Furu Wei. 2023. Beats: audio pretraining with acoustic tokenizers. In Proceedings of the 40th International Conference on Machine Learning, pages 51785193. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. 2024. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023. Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models. arXiv preprint arXiv:2311.07919. Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018. Voxceleb2: Deep speaker recognition. arXiv preprint arXiv:1806.05622. Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. 2021. W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 244250. IEEE. Christopher Cieri, David Miller, and Kevin Walker. The fisher corpus: resource for the next generations of speech-to-text. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2023. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798805. IEEE. Nilaksh Das, Saket Dingliwal, Srikanth Ronanki, Rohit Paturi, David Huang, Prashant Mathur, Jie Yuan, Dhanush Bekal, Xing Niu, Sai Muralidhar Jayanthi, et al. 2024. Speechverse: large-scale generalizable audio language model. arXiv preprint arXiv:2405.08295. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. 2022. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438. Zhihao Du, Jiaming Wang, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, et al. 2023. Lauragpt: Listen, attend, understand, and regenerate audio with gpt. arXiv preprint arXiv:2310.04673. Prompting large language models with speech recognition abilities. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1335113355. IEEE. Daniel Galvez, Greg Diamos, Juan Ciro, Juan Felipe Cerón, Keith Achorn, Anjali Gopi, David Kanter, Maximilian Lam, Mark Mazumder, and Vijay Janapa Reddi. 2021. The peoples speech: large-scale diverse english speech recognition dataset for commercial usage. arXiv preprint arXiv:2111.09344. John Godfrey, Edward Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corIn Acoustics, pus for research and development. speech, and signal processing, ieee international conference on, volume 1, pages 517520. IEEE Computer Society. Yuan Gong, Sameer Khurana, Leonid Karlinsky, and James Glass. 2023a. Whisper-at: Noise-robust automatic speech recognizers are also strong general audio event taggers. arXiv preprint arXiv:2307.03183. Yuan Gong, Alexander Liu, Hongyin Luo, Leonid Karlinsky, and James Glass. 2023b. Joint audio and speech understanding. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE. Yuan Gong, Hongyin Luo, Alexander Liu, Leonid Karlinsky, and James Glass. 2023c. Listen, think, and understand. arXiv preprint arXiv:2305.10790. Zhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao, and Xu Tan. 2023. Prompttts: Controllable text-tospeech with text descriptions. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Lingwei Meng, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, et al. 2024. Wavllm: Towards robust and adaptive speech large language model. arXiv preprint arXiv:2404.00656. Chien-yu Huang, Wei-Chih Chen, Shu-wen Yang, Andy Liu, Chen-An Li, Yu-Xiang Lin, Wei-Cheng Tseng, Anuj Diwan, Yi-Jen Shih, Jiatong Shi, et al. 2024a. Dynamic-superb phase-2: collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks. arXiv preprint arXiv:2411.05361. Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al. 2024. Chien-yu Huang, Ke-Han Lu, Shih-Heng Wang, ChiYuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, et al. 10 2024b. Dynamic-superb: Towards dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1213612140. IEEE. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Jaesung Huh. 2024. Voice-gender-classifier. Accessed: 2025-01-07. Amazon Artificial General Intelligence. 2024. The amazon nova family of models: Technical report and model card. Amazon Technical Reports. Yannick Jadoul, Bill Thompson, and Bart De Boer. 2018. Introducing parselmouth: python interface to praat. Journal of Phonetics, 71:115. Adam Janin, Don Baron, Jane Edwards, Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke, et al. 2003. The icsi meeting corpus. In 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP03)., volume 1, pages II. IEEE. Ye Jia, Michelle Tadmor Ramanovich, Quan Wang, and Heiga Zen. 2022. Cvss corpus and massively multilingual speech-to-speech translation. arXiv preprint arXiv:2201.03713. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088. Kavita Kasi and Stephen Zahorian. 2002. Yet another algorithm for pitch tracking. In 2002 ieee international conference on acoustics, speech, and signal processing, volume 1, pages I361. IEEE. Masaya Kawamura, Ryuichi Yamamoto, Yuma Shirahata, Takuya Hasumi, and Kentaro Tachibana. 2024. Libritts-p: corpus with speaking style and speaker identity prompts for text-to-speech and style captioning. arXiv preprint arXiv:2406.07969. Nitish Shirish Keskar, Bryan McCann, Lav Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858. Yoach Lacombe. 2024. Data-speech. Accessed: 202412-23. Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. 2021. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336 1354. Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. 2023. Bigvgan: universal neural vocoder with large-scale training. Preprint, arXiv:2206.04658. Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu. 2023b. Prompting large language models for zero-shot domain adaptation in speech recognition. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. IEEE. Reza Lotfian and Carlos Busso. 2017. Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordIEEE Transactions on Affective Computing, ings. 10(4):471483. Dan Lyth and Simon King. 2024. Natural language guidance of high-fidelity text-to-speech with synthetic annotations. arXiv preprint arXiv:2402.01912. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 52065210. IEEE. Jongseok Park, Kyubyong & Kim. 2019. g2pe. https: //github.com/Kyubyong/g2p. Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. 2020. Mls: largescale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Paul Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. 2023. Audiopalm: large language model that can speak and listen. arXiv preprint arXiv:2306.12925. Sak. 2014. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. INTERSPEECH. Reo Shimizu, Ryuichi Yamamoto, Masaya Kawamura, Yuma Shirahata, Hironori Doi, Tatsuya Komatsu, and Kentaro Tachibana. 2024. Prompttts++: Controlling speaker identity in prompt-based text-to-speech In ICASSP using natural language descriptions. 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1267212676. IEEE. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, MA Zejun, and Chao Zhang. Salmonn: Towards generic hearing abilities Multimodal language analysis in the wild: Cmumosei dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 22362246. Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. 2019. Libritts: corpus derived from librispeech for textto-speech. arXiv preprint arXiv:1904.02882. Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, et al. 2023. Google usm: Scaling automatic speech recognition beyond 100 languages. arXiv preprint arXiv:2303.01037. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. for large language models. In The Twelfth International Conference on Learning Representations. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, and Nancy Chen. 2024. Audiobench: universal benchmark for audio large language models. arXiv preprint arXiv:2406.16020. Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. 2021a. Voxpopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In ACL 2021-59th Annual Meeting of the Association for Computational Linguistics. Changhan Wang, Anne Wu, Jiatao Gu, and Juan Pino. 2021b. Covost 2 and massively multilingual speech translation. In Interspeech, pages 22472251. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Thomas Wolf. 2020. art natural language processing. arXiv:1910.03771. Transformers: State-of-thearXiv preprint Junichi Yamagishi, Christophe Veaux, and Kirsten MacDonald. 2019. Cstr vctk corpus: English multispeaker corpus for cstr voice cloning toolkit (version 0.92). An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, et al. 2024b. Airbench: Benchmarking large audio-language models via generative comprehension. arXiv preprint arXiv:2402.07729. Wenyi Yu, Changli Tang, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. 2024. Connecting speech encoder and large language model for asr. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1263712641. IEEE. AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018. 12 SIFT-50M Dataset Closed-ended: Acoustic-Level Question: Hay distorsión en el audio? Answer: Casi no hay ruido de distorsión en el audio. Closed-ended: Acoustic-Level Question: What is the transcript of the audio, and what emotions can be inferred from it? Answer: The transcript of the audio is girl smiles while interacting with medical personnel. The emotions that can be inferred from the transcript are happiness and cooperation. Closed-ended: Word-Align Question: Wie verändert sich die durchschnittliche Tonhöhe im Laufe des gesprochenen Satzes? Answer: Die durchschnittliche Tonhöhe des gesprochenen Satzes beginnt mit einer ziemlich tiefen Tonhöhe, sinkt dann zu einem sehr tiefen Ton und steigt dann allmählich zu einer mittleren Tonhöhe an. Am Ende des Satzes sinkt die Tonhöhe erneut auf eine sehr tiefe Tonhöhe ab. Closed-ended: Comparison Question: Qual è la differenza di tonalità tra due audio? Answer: Il primo audio ha una tonalità molto alta, mentre il secondo ha una tonalità abbastanza alta. Open-ended Question: Dans quelle mesure la réverbération et le niveau de distorsion perçus dans laudio affectent-ils la clarté de la transcription et la compréhension globale du message? Answer: La réverbération légèrement lointain et le niveau de distorsion bruyant rendent la transcription moins claire, compliquant la compréhension précise du message. Cela nécessite un effort supplémentaire de lauditeur pour interpréter le contenu, ce qui peut entraîner des malentendus ou des interprétations erronées. Controllable Generation Question: Synthetisieren Sie Sprache mit einer mäßig schnellen Geschwindigkeit, einer mittelmäßigen Lautstärke und einer sehr monotonen Tonhöhenvariation. Der Transkriptionstext lautet: Auf Kamelen ritten sie durchs Morgenland. Answer: <reference to original audio> Table 8: Representative multilingual examples from SIFT-50M for various instruction categories. Figure 3: Dataset distribution showing the multi-lingual nature of SIFT-50M and the different categories within each language. Figure 4: Distribution of the number of examples per language in the acoustic-based language ID (LID) task that is part of the closed-ended instructions."
        },
        {
            "title": "B Metadata Extraction",
            "content": "B.1 Categorical Values of Acoustic-Level Metadata Average Pitch Pitch Variation Speaking Rate Average Intensity Noise Level Reverberation Distortion Level Speech Quality Very slowly Very monotone Quite slowly Quite monotone Slightly monotone Slightly slowly Moderate intonation Moderate speed Very low pitch Quite low pitch Slightly low pitch Moderate pitch Slightly high pitch Slightly expressive Quite high pitch Very high pitch Quite expressive Very expressive Slightly fast Quite fast Very fast Softly Moderate volume Loudly Very distant-sounding Distant-sounding Slightly distant-sounding Noisy Very noisy Noisy Slightly noisy Balanced in clarity Slightly close-sounding Slightly clean Clean Very clean Very close-sounding Extremely noisy Very bad speech quality Very noisy Bad speech quality Slightly bad speech quality Moderate speech quality Slightly noisy Almost no noise Great speech quality Very clear Wonderful speech quality Table 9: Categorical values of acoustic-level metadata B.2 Sample Metadata We show an example of metadata from the VCTK dataset after mapping to categorical values. { acoustic_level: { average_pitch: slightly low pitch, pitch_variation: quite monotone, speaking_rate: moderate speed, average_intensity: moderate volume, noise_level: slightly noisy, reverberation: very close-sounding, distortion_level: almost no noise, speech_quality: moderate speech quality }, content_level: { phonemes: SHIY1 KAE1N SKUW1P DHIY1Z THIH1NGZ ..., age_category: twenties, accent: English, gender: Female, transcript: She can scoop these things into three red bags, and we will go meet her Wednesday at the train station., } } B.2.1 Sample Word-level Metadata [ { }, { }, { } ] word: nodded, position: 1, average_pitch: slightly low pitch, pitch_variation: very monotone, average_intensity: moderate volume word: his, position: 2, average_pitch: slightly low pitch, pitch_variation: very monotone, average_intensity: moderate volume word: head, position: 3, average_pitch: quite low pitch, pitch_variation: very monotone, average_intensity: moderate volume"
        },
        {
            "title": "C Natural Language Instruction Generation",
            "content": "C.1 LLM Hyperparameters We provide the LLM hyper-parameters used for generating instructions in Table 10. Mixtral 8x7B model was run on g5.48xlarge5 instance (8 A10 GPUs), while Amazon Nova Pro was used on AWS Bedrock6. Parameter Mixtral 8x7B Amazon Nova Pro Temperature Top Max. tokens 0.7 0.9 2048 0.7 - Table 10: LLM hyperparameters used during instruction generation C.2 Data Format Below is an example of how SIFT-50M data is organized in the ChatML7/Messages API8 format. { id: < instance_id >, messages: [ role: user, content: [ {text: Which audio has speaker with Scottish accent ?}, {audio_path: < audio_path_1 >} , {audio_path: < audio_path_2 >} ] role: assistant, content: [{text: The second audio has Scottish - accented speaker .}] { }, { } ] } 5https://aws.amazon.com/ec2/instance-types/g5/ 6https://aws.amazon.com/bedrock/ 7https://platform.openai.com/docs/api-reference/chat/create 8https://docs.anthropic.com/en/api/messages"
        },
        {
            "title": "D Training Setup",
            "content": "D.1 Pre-training Datasets Task Task Wt. Dataset #Hours Automatic Speech Recognition (ASR) 0. Emotion Recognition (ER) 0.05 Speaker-Attributed ASR (SA-ASR) Intent Classification (IC) Slot Entity Recognition (SER) Speech Translation (S2TT/ S2ST) 0. 0.05 0.20 Text to Speech (TTS) 0.30 Librispeech (Panayotov et al., 2015) MLS (Pratap et al., 2020) FLEURS (Conneau et al., 2023) Peoples Speech (Galvez et al., 2021) MSP-Podcast (Lotfian and Busso, 2017) AMI Meeting Corpus (Carletta et al., 2005) ICSI Meeting Corpus (Janin et al., 2003) Fisher (Cieri et al.) SLURP (Bastianelli et al., 2020) SLURP (Bastianelli et al., 2020) CoVoST (Wang et al., 2021b) CVSS (Jia et al., 2022) VoxPopuli (Wang et al., 2021a) FLEURS (Conneau et al., 2023) MLS (Pratap et al., 2020) FLEURS (Conneau et al., 2023) Peoples Speech (Galvez et al., 2021) 960 50K 1.4K 30K 237 100 70 2K 58 58 2.9K 1.1K 1.8K 1.4K 50K 1.4K 30K Table 11: Details of the datasets used during the pre-training stage and the weights assigned to different tasks. D.2 Training Hyper-parameters We provide the training hyper-parameters used for SIFT-LLM in Table 12. All models were trained on four p4d.24xlarge9 instances (32 A100 GPUs in total, each with 40GB of memory), with training for 200K steps taking approximately 3 days. Parameter PT: Stage 1 IFT: SIFT-LLM PT: Stage IFT: SIFT-LLM GEN Batch Size # Steps # Warmup Steps Max Learning Rate LoRA rank LoRA α # Trainable Params 256 200K 1000 1e-4 16 8 8.7M 128 200K 500 1e-4 16 8 8.7M 256 200K 500 1e-4 128 64 590M 128 200K 500 1e-4 128 64 590M Table 12: Training hyperparameters used during pre-training (PT) and instruction fine-tuning (IFT). D.3 Pre-Training Prompt Template Examples Below are examples of instruction templates used during pre-training: ASR: Transcribe what the speaker is saying. Decode this $language speech. ER: Get the most probable emotion in the utterance. Annotate the speech and obtain the most prominent emotion. 9https://aws.amazon.com/ec2/instance-types/p4/ 16 SA-ASR: Transcribe what each of the speakers says and assign speaker labels to transcribed segments. Format the output as text interspersed with speaker tags, where each utterance is followed by its speaker in JSON format (e.g., \"speaker\": \"spk_id\"). Use integer speaker ids starting with 0. IC: From this audio, what intent does the speaker have? Options include: joke, definition, takeaway_query, and social_query. SER: Can you identify the entity that fits the device_type slot? S2TT: Recognize and then translate this utterance to text in $target_language. Translate text from $source_language to $target_language for this audio. S2ST: Can you translate this recording to $target_language TTS: Can you convert the text to speech: $transcription?"
        },
        {
            "title": "E Evaluation Setup",
            "content": "E.1 Instruction-Following Benchmarks Test Set # Samples Languages Dynamic-Superb (Huang et al., 2024a) AIR-Bench Chat (Yang et al., 2024b) EvalSIFT (Closed-Ended) EvalSIFT (Open-Ended) EvalSIFT (Control. Generation) 10,400 793 10K (2K per lang) 10K (2K per lang) 10K (2K per lang) English English English, German, French, Italian, Spanish English, German, French, Italian, Spanish English, German, French, Italian, Spanish Table 13: Details of benchmarks used for instruction-following evaluation. E.2 Foundational Tasks Benchmarks Task ASR ER IC SER S2TT Test Set Librispeech Test-Clean (Panayotov et al., 2015) Librispeech Test-Other (Panayotov et al., 2015) Peoples Speech (Galvez et al., 2021) FLEURS (en, de, fr, it, es) (Conneau et al., 2023) MSP-test1 (Lotfian and Busso, 2017) MSP-test2 (Lotfian and Busso, 2017) SLURP (Bastianelli et al., 2020) SLURP (Bastianelli et al., 2020) CoVost2 (de en) (Wang et al., 2021b) CoVost2 (fr en) (Wang et al., 2021b) CoVost2 (it en) (Wang et al., 2021b) CoVost2 (es en) (Wang et al., 2021b) # Samples 2,620 2,560 34,898 350 * 5 = 1,750 30,647 14,815 13,078 11,585 13,509 14,760 8,945 13,221 Table 14: Details of benchmarks used for foundational tasks evaluation. 17 E.3 Evaluation Prompt Templates As model performance can be sensitive to the prompts used (Wang et al., 2024), we use the prompts recommended by the corresponding model wherever we can find for evaluation of SALMONN-7B10, Qwen2-Audio-Instruct11 and O-ASQA-LLM12. Task Model Prompt Template ASR ER S2TT SALMONN-7B Qwen2-Audio-Instruct [en] Recognize the speech and give me the transcription. [de] Hören Sie sich die Rede an und schreiben Sie ihren Inhalt auf. [fr] Écoutez le discours et écrivez son contenu. [it] Ascolta il discorso scrivi il suo contenuto. [es] Escuche el discurso escriba su contenido. [en/de/fr/it/es] Recognize the speech in $language: [en] Can you identify the spoken text? [de/fr/it/es] Can you identify the spoken text? Your output MUST be in $language. [en] Transcribe what the speaker says. [de/fr/it/es] Transcribe this $language speech. Describe the emotion of the speaker in one word. Qwen2-Audio-Instruct Recognize the emotion with keywords in English: SALMONN-7B O-ASQA-LLM SIFT-LLM O-ASQA-LLM SIFT-LLM SALMONN-7B Qwen2-Audio-Instruct O-ASQA-LLM SIFT-LLM Identify the most likely emotion in the following speech. Identify the most likely emotion in the following speech. Listen to the speech and translate it into $target_language. Translate the speech into $target_language: Translate the audio to $target_language, returning only the translated text. Translate the audio to $target_language, returning only the translated text. Table 15: Prompts used to evaluate different speech-text LLMs on foundational tasks."
        },
        {
            "title": "F Evaluation Results",
            "content": "F.1 EvalSIFT Closed-Ended Results Model Acoustic-Level Content-Level Word-Align Comparison SALMONN-7B Qwen2-Audio-Instruct O-ASQA-LLM SIFT-LLM 16.1 19.1 12.8 58.4 34.6 37.5 39.6 41. 9.7 16.8 16.7 28.6 27.3 27.3 22.7 55.5 Table 16: Breakdown of evaluation results, reported as accuracy (%) for different sub-categories of the closed-ended set in EvalSIFT (English). F.2 Dynamic-Superb Detailed Results Task SALMONN Qwen2-Audio-Instruct O-ASQA-LLM SIFT-LLM AccentClassification_AccentdbExtended DialogueEmotionClassification_DailyTalk EmotionRecognition_MultimodalEmotionlinesDataset HowFarAreYou_3DSpeaker SpoofDetection_ASVspoof2015 SpoofDetection_ASVspoof2017 StressDetection_MIRSD Paralinguistics BirdSoundDetection_Warblrb10k ChordClassification_AcousticGuitarAndPiano 13 16.5 32.32 23.12 61.5 65 30.49 74 7.5 14.5 46 38.5 31 20.1 27.64 24.5 28.89 76.88 40.2 17 57.29 51 28 14.07 23.62 18. 29.94 40.91 49.5 50.5 65.83 60.41 28.5 5.5 23 28.5 37.46 35.35 46.5 10https://github.com/bytedance/SALMONN/blob/main/prompts/test_prompt.json 11https://github.com/QwenLM/Qwen2-Audio/blob/main/eval_audio/EVALUATION.md 12https://github.com/YuanGongND/ltu/blob/main/README.md#for-ltu-as-openasqa 18 Task SALMONN-7B Qwen2-Audio-Inst. O-ASQA-LLM SIFT-LLM EnvironmentalSoundClassification_ESC50-Animals EnvironmentalSoundClassification_ESC50-ExteriorAndUrbanNoises EnvironmentalSoundClassification_ESC50HumanAndNonSpeechSounds EnvironmentalSoundClassification_ESC50InteriorAndDomesticSounds EnvironmentalSoundClassification_ESC50NaturalSoundscapesAndWaterSounds Audio DialogueActClassification_DailyTalk DialogueActPairing_DailyTalk SarcasmDetection_Mustard Semantics EnhancementDetection_LibriTTS-TestClean_WHAM NoiseDetection_LJSpeech_MUSAN-Gaussian NoiseDetection_LJSpeech_MUSAN-Music NoiseDetection_LJSpeech_MUSAN-Noise NoiseDetection_LJSpeech_MUSAN-Speech NoiseDetection_VCTK-MUSAN-Gaussian NoiseDetection_VCTK_MUSAN-Music NoiseDetection_VCTK_MUSAN-Noise NoiseDetection_VCTK_MUSAN-Speech NoiseSNRLevelPrediction_VCTK_MUSAN-Gaussian NoiseSNRLevelPrediction_VCTK_MUSAN-Music NoiseSNRLevelPrediction_VCTK_MUSAN-Noise NoiseSNRLevelPrediction_VCTK_MUSAN-Speech ReverberationDetection_LJSpeech_RirsNoises-LargeRoom ReverberationDetection_LJSpeech_RirsNoises-MediumRoom ReverberationDetection_LJSpeech_RirsNoises-SmallRoom ReverberationDetection_VCTK_RirsNoises-LargeRoom ReverberationDetection_VCTK_RirsNoises-MediumRoom ReverberationDetection_VCTK_RirsNoises-SmallRoom Degradation LanguageIdentification_VoxForge SpeechCommandRecognition_GoogleSpeechCommandsV1 SpeechDetection_LJSpeech SpeechDetection_LibriSpeech-TestClean SpeechDetection_LibriSpeech-TestOther SpeechTextMatching_LJSpeech SpeechTextMatching_LibriSpeech-TestClean SpeechTextMatching_LibriSpeech-TestOther SpokenTermDetection_LJSpeech SpokenTermDetection_LibriSpeech-TestClean SpokenTermDetection_LibriSpeech-TestOther Content MultiSpeakerDetection_LibriSpeech-TestClean MultiSpeakerDetection_VCTK SpeakerCounting_LibriTTS-TestClean SpeakerVerification_LibriSpeech-TestClean SpeakerVerification_VCTK Speaker 42 10 54.04 5.56 28.5 31.66 42.35 53.77 46. 47.54 28.93 49 49.75 50 43 57 47.72 45.5 36.5 25.13 26.74 29.29 23.5 9 8.54 3.5 12.5 15 10 30.03 31.5 18.88 57.87 45.73 46.5 51.26 52.26 52.53 51.27 47.21 41.75 45.16 13.2 39.39 9.18 44.72 52. 31.85 62.31 60.1 47.47 31.5 55.84 53.47 33.17 40.1 47.5 40. 52.26 49.5 46 49 46.5 54.5 54.5 58.29 55.5 22.8 16.67 18.18 28.21 47.5 48 48 46 46 46 43.86 89.95 67.19 40.7 53.77 48.5 80.2 86.36 83.33 81.87 73.22 71.2 70.57 44.95 45.41 24.37 51.5 51.5 43. 10.61 14 24.5 34.5 25.5 28.5 23.62 41.41 50.75 38.59 48.5 52.5 51.28 52.31 52.55 49 51.52 47.98 52.28 26.63 30.15 28.28 28.43 45.5 51.5 50.51 54.5 48.24 50. 45.9 50 70.56 67 49.49 50.75 88.72 83.94 84.9 87.05 81.44 81.35 72.29 47.24 47.74 21 41.71 45.73 40.68 42.71 36.36 43. 48.72 46.23 42.77 40.7 47.24 66 51.31 82.5 72.5 86.93 86.5 87.94 74 84.92 89 87.5 21.11 25.5 23 25.13 65.5 54.82 57.79 67.84 60 56 63. 88.5 64.32 62.28 49.95 45.73 78.28 72.96 76.77 97.97 97.85 97.3 75.63 63.13 73.5 17.17 46.73 38.19 47.74 F.3 Effect of Reference Speaker signal used in the Vocoder To validate the efficacy of discretized HuBERT tokens in preserving speech characteristics, we conducted an experiment in which we synthesized waveform from the HuBERT tokens extracted from an original audio waveform. We refer to the resulting audio as re-synthesized audio. In this experiment, we used randomly selected reference speaker signal in the vocoder. Using the same metrics as in Table 6, we compared the speech characteristics of the re-synthesized audio with those of the original audio. Ideally, the re-synthesized audio should match the characteristics of the original audio. However, the results, shown in Table 18, indicate that while speaking rate exhibited some similarity, other characteristics differed considerably. In further experiment, we assessed the effect of the reference speaker signal used in the vocoder. First, we selected original audio samples from the source speech corpus, whose metadata was used to construct given controllable generation instruction. We then compared two outputs: (1) the audio re-synthesized from HuBERT tokens extracted from the original audio, and (2) the audio generated by SIFT-LLM GEN for the corresponding instruction. For each example, the same reference speaker signal was used in the vocoder. The results, reported in Table 19, show that pitch variation and intensity exhibit much stronger correlation when the same reference speaker is used. We also observed that gender predictions 19 matched approximately 96% between the SIFT-LLM GEN generated audio and the re-synthesized audio, whereas the match was only around 50% when compared against the gender specified in the controllable instruction. We hypothesize this limitation is due to incomplete representation of speech in HuBERT codesi.e, they primarily capture semantic content and limited stationary aspects such as speaking rate, pitch, and intensity. Metric Pitch variation Speaking rate Intensity MAE () QWK () ρ () 0.99 0.11 0.12 0.24 0.72 0.79 0.32 0.06 0.09 Table 18: Evaluation results on controllable generation, comparing the speech characteristics of resynthesized audio and original audio. Metric Pitch variation Speaking rate Intensity MAE () QWK () ρ () 0.43 0.62 0. 0.95 0.23 0.48 0.06 0.10 0.15 Table 19: Evaluation results on controllable generation, comparing the speech characteristics of audio generated by SIFT-LLM GEN and resynthesized audio."
        },
        {
            "title": "G Prompt Templates",
            "content": "G.1 Prompts used for Data Generation Acoustic/ Content level Based on the following audio clip, generate $noques different types of questions and corresponding step-by-step answers. Questions should be about the audio (eg. asking about its properties). The question should be framed as if someone has uploaded an audio (and not its metadata) and asking question about it. It should be answered based on metadata values but answer should not mention about metadata being available or so. It should reflect as if it is answered after listening to the audio. The more complex and diverse the question, the better. Format each QA pair in single line as JSON dictionary (key for question, and for answer, wrapped with { and }). Do not include any other explanation. Metadata: $metadata Word-Align Based on the following audio clip, generate $noques different types of questions and corresponding step-by-step answers in $language language. The metadata is provided in $language language. Questions should be about the audio (eg. asking about its properties). The question should be framed as if someone has uploaded an audio (and not its metadata) and asking question about it. It should be answered based on metadata values but answer should not mention about metadata being available or so. It should reflect as if it is answered after listening to the audio. The more complex and diverse the question, the better. The metadata is list of word level characteristics like position and possible pitch and intensity categories. Questions can be framed on highlighting considerable changes in pitch and intensity at phrase level, each phrase is combination of consecutive words, determined by position info for each word. Format each QA pair in single line as JSON dictionary (key for question, and for answer, wrapped with { and }). Do not include any other explanation. Metadata: $metadata"
        },
        {
            "title": "Controllable Generation",
            "content": "You will be provided with metadata describing an audio sample of persons speech. Your task is to generate prompt in $language language that instructs model to synthesize speech based on description derived from the given metadata. Requirements: - The generated prompt must be grammatically correct, concise, and easy to understand. - It should not reference the transcription, which is provided separately. - Aim for complexity and diversity by selecting random speech characteristics to introduce variability. - Do not add details beyond what is provided in the metadata. Return only the generated prompt and do not include any other explanation. Metadata: $metadata G.2 Prompts used for Evaluation Closed-ended Evaluation Below is question and the correct/reference answer about an audio: { \"question\": \"$question\", \"answer\": \"$answer\" } Rate the following answer as correct or not. If the above answer is correct, \"score\" should be 1, else 0. Answer: $hyp Your output should be JSON dictionary with keys as \"reason\" and \"score\". Do not include preambles, or any additional content. Open-ended Evaluation Below is question and the correct/reference answer about an audio: { \"question\": \"$question\", \"answer\": \"$answer\" } Rate the following answer on scale of 0 to 10. Answer: \"$hyp\" Your output should be JSON dictionary with keys as \"reason\" and \"score\". Do not include preambles, or any additional content. Examples of SIFT-LLMs reasoning abilities Question 1: What might be the possible scenario in which this speech was delivered? (Audio: We want to continue to be on the periphery of the European infrastructure system, besides the sins of our people.) Reference Answer: One possible scenario in which this speech could have been delivered is during conference or meeting where the speaker is discussing their organizations stance on the European infrastructure system. Model Response: Given the formal tone, international accent, and specific vocabulary related to infrastructure and Europe, this speech could have been delivered in an academic or professional setting, possibly at conference or seminar. Question 2: What is the speakers concern in the speech? (Audio: How do strengthen it all in acceptable form that can be done in two or two and half hours?) Reference Answer: The speakers concern is to figure out way to achieve the task within the given time frame. Model Response: The speaker is concerned about the effort and time required to achieve certain form or appearance. Specifically, they are questioning how someone could accomplish this in two or two-and-a-half hours. 21 Question 3: Evaluate the effectiveness of the audio for use in professional setting and propose modifications to enhance its suitability. Reference Answer: The audio is ineffective for professional setting due to its poor speech quality and slow delivery. Modifications such as re-recording with better equipment, increasing speaking rate, and reducing background noise would enhance its suitability. Model Response: The audio is not suitable for professional setting due to its very bad speech quality, noisy background, and distant-sounding reverberation. To enhance its suitability, the speaker should record in quiet environment with minimal background noise. Using high-quality microphone and ensuring proper distance from the microphone can reduce reverberations. Additionally, post-production editing could be used to clean up the audio and improve clarity."
        }
    ],
    "affiliations": [
        "Amazon AGI",
        "Apple Inc."
    ]
}