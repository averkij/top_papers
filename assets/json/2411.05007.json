{
    "paper_title": "SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models",
    "authors": [
        "Muyang Li",
        "Yujun Lin",
        "Zhekai Zhang",
        "Tianle Cai",
        "Xiuyu Li",
        "Junxian Guo",
        "Enze Xie",
        "Chenlin Meng",
        "Jun-Yan Zhu",
        "Song Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, a new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ a high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, na\\\"{\\i}vely running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-$\\Sigma$, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5$\\times$, achieving 3.0$\\times$ speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090 GPU, paving the way for more interactive applications on PCs. Our quantization library and inference engine are open-sourced."
        },
        {
            "title": "Start",
            "content": "SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models SVDQUANT: ABSORBING OUTLIERS BY LOW-RANK COMPONENTS FOR 4-BIT DIFFUSION MODELS Muyang Li1 Yujun Lin1 Zhekai Zhang1 Tianle Cai4 Xiuyu Li5 Junxian Guo1,6 Enze Xie2 Chenlin Meng7 1MIT 5UC Berkeley 2NVIDIA 3CMU 4Princeton https://hanlab.mit.edu/projects/svdquant Song Han1,2 6SJTU 7Pika Labs Jun-Yan Zhu3 4 2 0 N 8 ] . [ 2 7 0 0 5 0 . 1 1 4 2 : r Figure 1: SVDQuant is post-training quantization technique for 4-bit weights and activations that well maintains visual fidelity. On 12B FLUX.1-dev, it achieves 3.6 memory reduction compared to the BF16 model. By eliminating CPU offloading, it offers 8.7 speedup over the 16-bit model when on 16GB laptop 4090 GPU, 3 faster than the NF4 W4A16 baseline. On PixArt-Σ, it demonstrates significantly superior visual quality over other W4A4 or even W4A8 baselines. E2E means the end-to-end latency including the text encoder and VAE decoder."
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion models have been proven highly effective at generating high-quality images. However, as these models grow larger, they require significantly more memory and suffer from higher latency, posing substantial challenges for deployment. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where conventional post-training quantization methods for large language models like smoothing become insufficient. To overcome this limitation, we propose SVDQuant, new 4-bit quantization paradigm. Different from smoothing which redistributes outliers between weights and activations, our approach absorbs these outliers using low-rank branch. We first consolidate the outliers by shifting them from activations to weights, then employ high-precision low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD). This process eases the quantization on both sides. However, naïvely running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine Nunchaku that fuses the kernels of Algorithm co-lead. System lead. Part of the work done during an internship at NVIDIA. 1 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without the need for re-quantization. Extensive experiments on SDXL, PixArt-Σ, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5, achieving 3.0 speedup over the 4-bit weight-only quantized baseline on the 16GB laptop 4090 GPU, paving the way for more interactive applications on PCs. Our quantization library* and inference engine are open-sourced."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have shown remarkable capabilities in generating high-quality images (Ho et al., 2020), with recent advances further enhancing user control over the generation process. Trained on vast data, these models can create stunning images from simple text prompts, unlocking diverse image editing and synthesis applications (Meng et al., 2022b; Ruiz et al., 2023; Zhang et al., 2023). To pursue higher image quality and more precise text-to-image alignment, researchers are increasingly scaling up diffusion models. As shown in Figure 2, Stable Diffusion (SD) (Rombach et al., 2022) 1.4 only has 800M parameters, while SDXL (Podell et al., 2024) scales this up to 2.6B parameters. AuraFlow v0.1 (fal.ai, 2024) extends this further to 6B parameters, with the latest model, FLUX.1 (Black-Forest-Labs, 2024), pushing the boundary to 12B parameters. Compared to large language models (LLMs), diffusion models are significantly more computationally intensive. Their computational costs increase more rapidly with model size, posing prohibitive memory and latency barrier for real-world model deployment, particularly for interactive use cases that demand low latency. Figure 2: Computation vs. parameters for LLMs and diffusion models. LLMs computation is measured with 512 context and 256 output tokens, and diffusion models computation is for single step. Dashed lines show trends. As Moores law slows down, hardware vendors are turning to lowprecision inference to sustain performance improvements. For instance, NVIDIAs Blackwell Tensor Cores introduce new 4-bit floating point (FP4) precision, doubling the performance compared to FP8 (NVIDIA, 2024). Therefore, using 4-bit inference to accelerate diffusion models is appealing. In the realm of LLMs, researchers have leveraged quantization to compress model sizes and boost inference speed (Dettmers et al., 2022; Xiao et al., 2023). However, unlike LLMswhere latency is primarily constrained by loading model weights, especially with small batch sizesdiffusion models are heavily computationally bound, even with single batch. As result, weight-only quantization cannot accelerate diffusion models. To achieve speedup, both weights and activations must be quantized to the same bit width; otherwise, the lower-precision side will be upcast during computation, negating potential performance enhancements. In this work, we focus on quantizing both the weights and activations of diffusion models to 4 bits. This challenging and aggressive scheme is often prone to severe quality degradation. Existing methods like smoothing (Xiao et al., 2023; Lin et al., 2024a), which attempt to transfer the outliers between the weights and activations, are less effective since both sides are highly vulnerable to outliers. To address this issue, we propose new general-purpose quantization paradigm, SVDQuant. Our core idea is to introduce low-cost branch to absorb outliers on both sides. To achieve this, as illustrated in Figure 3, we first aggregate the outliers by migrating them from activation to weight via smoothing. Then we apply Singular Value Decomposition (SVD) to the updated weight, ˆW , decomposing it into low-rank branch L1L2 and residual ˆW L1L2. The low-rank branch operates at 16 bits, allowing us to quantize only the residual to 4 bits, which has significantly reduced outliers and magnitude. However, naively running the low-rank branch separately incurs substantial memory access overhead, offsetting the speedup of 4-bit inference. To overcome this, we co-design specialized inference engine Nunchaku, which fuses the low-rank branch computation into the 4-bit *Quantization library: github.com/mit-han-lab/deepcompressor Inference Engine: github.com/mit-han-lab/nunchaku Computational cost is measured by number of Multiply-Accumulate operations (MACs). 1 MAC=2 FLOPs. 2 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Figure 3: Overview of SVDQuant. (a) Originally, both the activation and weight contain outliers, making 4-bit quantization challenging. (b) We migrate the outliers from the activation to weight, resulting in the updated activation ˆX and weight ˆW . While ˆX becomes easier to quantize, ˆW now becomes more difficult. (c) SVDQuant further decomposes ˆW into low-rank component L1L2 and residual ˆW L1L2 with SVD. Thus, the quantization difficulty is alleviated by the low-rank branch, which runs at 16-bit precision. quantization and computation kernels. This design enables us to achieve measured inference speedup even with additional branches. SVDQuant can quantize various text-to-image diffusion architectures, including both UNet (Ho et al., 2020; Ronneberger et al., 2015) and DiT (Peebles & Xie, 2023) backbones, into 4 bits, while maintaining visual quality. It supports both INT4 and FP4 data types, and integrates seamlessly with pre-trained low-rank adapters (LoRA) (Hsu et al., 2022) without requiring re-quantization. To our knowledge, we are the first to successfully apply 4-bit post-training quantization to both the weights and activations of diffusion models, and achieve measured speedup on NVIDIA GPUs. On the latest 12B FLUX.1, we largely preserve the image quality and reduce the memory footprint of the original BF16 model by 3.5 and deliver 3.0 speedup over the NF4 weight-only quantized baseline, measured on 16GB laptop-level RTX4090 GPU. See Figure 1 for visual examples."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Diffusion models. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have emerged as powerful class of generative models, known for their ability to generate high-quality samples by modeling the data distribution through an iterative denoising process. Recent advancements in textto-image diffusion models (Balaji et al., 2022; Rombach et al., 2022; Podell et al., 2024) have already revolutionized content generation. Researchers further shifted from convolution-based UNet architectures (Ronneberger et al., 2015; Ho et al., 2020) to transformers (e.g., DiT (Peebles & Xie, 2023) and U-ViT (Bao et al., 2023)) and scaled up the model size (Esser et al., 2024). However, diffusion models suffer from extremely slow inference speed due to their long denoising sequences and intense computation. To address this, various approaches have been proposed, including few-step samplers (Zhang & Chen, 2022; Zhang et al., 2022; Lu et al., 2022) or distilling fewer-step models from pre-trained ones (Salimans & Ho, 2021; Meng et al., 2022a; Song et al., 2023; Luo et al., 2023; Sauer et al., 2023; Yin et al., 2024b;a; Kang et al., 2024). Another line of works choose to optimize or accelerate computation via efficient architecture design (Li et al., 2023b; 2020; Cai et al., 2024; Liu et al., 2024a), quantization (Shang et al., 2023; Li et al., 2023a), sparse inference (Li et al., 2022; Ma et al., 2024b;a), and distributed inference (Li et al., 2024b; Wang et al., 2024c; Chen et al., 2024b). This work focuses on quantizing the diffusion models to 4 bits to reduce the computation complexity. Our method can also be applied to the few-step diffusion models to further reduce the latency (see Section 5.2). Quantization. Quantization has been recognized as an effective approach for LLMs to reduce the model size and accelerate inference (Dettmers et al., 2022; Frantar et al., 2023; Xiao et al., 2023; Lin et al., 2024b;a; Kim et al., 2024; Zhao et al., 2024d). For diffusion models, Q-Diffusion (Li et al., 2023a) and PTQ4DM (Shang et al., 2023) first achieved 8-bit quantization. Subsequent works refined these techniques with approaches like sensitivity analysis (Yang et al., 2023) and timestep-aware quantization (He et al., 2023; Huang et al., 2024; Liu et al., 2024b; Wang et al., 2024a). Some recent works extended the setting to text-to-image models (Tang et al., 2023; Zhao et al., 2024c), DiT backbones (Wu et al., 2024), quantization-aware training (He et al., 2024; Zheng et al., 2024; Wang et al., 2024b; Sui et al., 2024), video generation (Zhao et al., 2024b), and different data types (Liu & Zhang, 2024). Among these works, only MixDQ (Zhao et al., 2024c) and ViDiT-Q (Zhao et al., 2024b) implement low-bit inference engines and report measured 8-bit speedup on GPUs. In this work, we push the boundary further by quantizing diffusion models to 4 bits, supporting both the integer or floating-point data types, compatible with the UNet backbone (Ho et al., 2020) and recent 3 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models DiT architecture (Peebles & Xie, 2023). Our co-designed inference engine, Nunchaku, further ensures on-hardware speedup. Additionally, when applying LoRA to the model, existing methods require fusing the LoRA branch to the main branch and re-quantizing the model to avoid tremendous memoryaccess overhead in the LoRA branch. Nunchaku cuts off this overhead via kernel fusion, allowing the low-rank branch to run efficiently as separate branch, eliminating the need for re-quantization. Low-rank decomposition. Low-rank decomposition has gained significant attention in deep learning for enhancing computational and memory efficiency (Hu et al., 2022; Zhao et al., 2024a; Jaiswal et al., 2024). While directly applying this approach to model weights can reduce the compute and memory demands (Hsu et al., 2022; Yuan et al., 2023; Li et al., 2023c), it often leads to performance degradation. Instead, Yao et al. (2023) combined it with quantization for model compression, employing low-rank branch to compensate for the quantization error. Low-Rank Adaptation (LoRA) (Hu et al., 2022) introduces another important line of research by using low-rank matrices to adjust subset of pre-trained weights for efficient fine-tuning. This has sparked numerous advancements (Dettmers et al., 2023; Guo et al., 2024; Li et al., 2024c; He et al., 2024; Xu et al., 2024b), which combines quantized models with low-rank adapters to reduce memory usage during model fine-tuning. However, our work differs in two major aspects. Firstly, our goal is different, as we aim to accelerate model inference through quantization, while previous works focus on model compression or efficient fine-tuning. Thus, they primarily consider weight-only quantization, resulting in no speedup. Secondly, as shown in our experiments (Figure 6 and ablation study in Section 5.2), directly applying these methods not only degrades the image quality, but also introduces significant overhead. In contrast, our method yields much better performance due to our joint quantization of weights and activations and our inference engine Nunchaku minimizes the overhead by fusing the low-rank branch kernels into the low-bit computation ones."
        },
        {
            "title": "3 QUANTIZATION PRELIMINARY",
            "content": "Quantization is an effective approach to accelerate linear layers in networks. Given tensor X, the quantization process is defined as: QX = round (cid:19) (cid:18) sX , sX = max(X) qmax . (1) Here, QX is the low-bit representation of X, sX is the scaling factor, and qmax is the maximum quantized value. For signed b-bit integer quantization, qmax = 2b1 1. For 4-bit floating-point quantization with 1-bit mantissa and 2-bit exponent, qmax = 6. Thus, the dequantized tensor can be formulated as Q(X) = sX QX . For linear layer with input and weight , its computation can be approximated by XW Q(X)Q(W ) = sX sW QX QW . (2) The same approximation applies to convolutional layers. To speed up computation, modern arithmetic logic units require both QX and QW using the same bit width. Otherwise, the low-bit side needs to be upcast to match the higher bit width, negating the speed advantage. Following the notation in QServe (Lin et al., 2024b), we denote x-bit weight, y-bit activation as WxAy. INT and FP refer to the integer and floating-point data types, respectively. In this work, we focus on W4A4 quantization for acceleration, where outliers in both weights and activations place substantial obstacles. Traditional methods to suppress these outliers include quantization-aware training (QAT) (He et al., 2024) and rotation (Ashkboos et al., 2024; Liu et al., 2024c; Lin et al., 2024b). QAT requires massive computing resources, especially for tuning models with more than 10B parameters (e.g., FLUX.1). Rotation is inapplicable due to the usage of adaptive normalization layers (Peebles & Xie, 2023) in diffusion models. The runtime-generated normalization weights preclude the offline integration of the rotation matrix with the weights of projection layers. Consequently, online rotation of both activations and weights incurs significant runtime overhead."
        },
        {
            "title": "4 METHOD",
            "content": "In this section, we first formulate our problem and discuss where the quantization error comes from. Next, we present SVDQuant, new W4A4 quantization paradigm for diffusion models. Our key idea 4 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Figure 4: Example value distribution of inputs and weights in PixArt-Σ. λ is the smooth factor. Red indicates the outliers. Initially, both the input and weight contain significant outliers. After smoothing, the range of ˆX is reduced with much fewer outliers, while ˆW shows more outliers. Once the SVD low-rank branch L1L2 is subtracted, the residual has narrower range and is free from outliers. is to introduce an additional low-rank branch that can absorb quantization difficulties in both weights and activations. Finally, we provide co-designed inference engine Nunchaku with kernel fusion to minimize the overhead of the low-rank branches in the 4-bit model."
        },
        {
            "title": "4.1 PROBLEM FORMULATION",
            "content": "Consider linear layer with input Rbm and weight Rmn. The quantization error can be defined as E(X, ) = XW Q(X)Q(W )F , (3) where denotes Frobenius Norm. Proposition 4.1 (Error decomposition). The quantization error can be decomposed as follows: E(X, ) XF Q(W )F + Q(X)F (W + Q(W )F ). (4) See Appendix A.1 for the proof. From the proposition, we can see that the error is bounded by four elements the magnitude of the weight and input, F and XF , and their respective quantization errors, Q(W )F and Q(X)F . To minimize the overall quantization error, we aim to optimize these four terms."
        },
        {
            "title": "4.2 SVDQUANT: ABSORBING OUTLIERS VIA LOW-RANK BRANCH",
            "content": "Migrate outliers from activation to weight. Smoothing (Xiao et al., 2023; Lin et al., 2024a) is an effective approach for reducing outliers. We can smooth outliers in activations by scaling down the input and adjusting the weight matrix correspondingly using per-channel smoothing factor λ Rm. As shown in Figure 4(a)(c), the smoothed input ˆX = diag(λ)1 exhibits reduced magnitude and fewer outliers, resulting in lower input quantization error. However, in Figure 4(b)(d), the transformed weight ˆW = diag(λ) has significant increase in both magnitude and the presence of outliers, which in turn raises the weight quantization error. Consequently, the overall error reduction is limited. Absorb magnified weight outliers with low-rank branch. Our core insight is to introduce 16-bit low-rank branch and further migrate the weight quantization difficulty to this branch. Specifically, we decompose the transformed weight as ˆW = L1L2 + R, where L1 Rmr and L2 Rrn are two low-rank factors of rank r, and is the residual. Then XW can be approximated as XW = ˆX ˆW = ˆXL1L2 + ˆXR ˆXL1L2 (cid:124) (cid:123)(cid:122) (cid:125) 16-bit low-rank branch + Q( ˆX)Q(R) (cid:125) (cid:123)(cid:122) 4-bit residual (cid:124) . (5) Compared to direct 4-bit quantization, i.e., Q( ˆX)Q(W ), our method first computes the low-rank branch ˆXL1L2 in 16-bit precision, and then approximates the residual ˆXR with 4-bit quantization. Empirically, min(m, n), and is typically set to 16 or 32. As result, the additional parameters and computation for the low-rank branch are negligible, contributing only mr+nr to the overall costs. However, it still requires careful system design to eliminate redundant memory access, which we will discuss in Section 4.3. mn 5 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Figure 6: (a) Naïvely running low-rank branch with rank 32 will introduce 57% latency overhead due to extra read of 16-bit inputs in Down Projection and extra write of 16-bit outputs in Up Projection. Our Nunchaku engine optimizes this overhead with kernel fusion. (b) Down Projection and Quantize kernels use the same input, while Up Projection and 4-Bit Compute kernels share the same output. To reduce data movement overhead, we fuse the first two and the latter two kernels together. From Equation 5, the quantization error can be expressed as (cid:13) ˆX ˆW ( ˆXL1L2 + Q( ˆX)Q(R)) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)F = (cid:13) (cid:13) ˆXR Q( ˆX)Q(R) (cid:13) (cid:13) (cid:13)F (cid:13) = E( ˆX, R), (6) where = ˆW L1L2. According to Proposition 4.1, since ˆX is already free from outliers, we only need to focus on optimizing the magnitude of R, RF and its quantization error, Q(R)F . Proposition 4.2 (Quantization error bound). For any tensor and quantization method described in Equation 1 as Q(R) = sR QR. Assuming the element of follows normal distribution, we have [R Q(R)F ] (cid:112)log (size(R)) π qmax [RF ] , (7) where size(R) denotes the number of elements in R. (cid:13) ˆW L1L2 (cid:13) (cid:13) See Appendix A.2 for the proof. From this proposition, we obtain the intuition that the quantization error Q(R)F is bounded by the magnitude of the residual RF . Thus, our goal is to (cid:13) (cid:13) find the optimal L1L2 that minimizes RF = , (cid:13)F which can be solved by simple Singular Value Decomposition (SVD). Given the SVD of ˆW = ΣV , the optimal solution is L1 = Σ:,:r and L2 = V:r,:. Figure 5 illustrates the singular value distribution of the original weight , transformed weight ˆW and residual R. The singular values of the original weight are highly imbalanced. After smoothing, the singular value distribution of ˆW becomes even sharper, with only the first several values being significantly larger. By removing these dominant values, EckartYoungMirsky theorem suggests that the magnitude of the residual is dramatically reduced, as RF = (cid:113)(cid:80)min(m,n) Figure 5: First 64 singular values of , ˆW , and R. The first 32 singular values of ˆW exhibit steep drop, while the remaining values are much more gradual. = (cid:113) (cid:13) (cid:13) (cid:13)F (cid:13) ˆW (cid:13) (cid:13) (cid:80)min(m,n) σ2 , compared to the original , where σi is the i-th singular value of ˆW . Furthermore, σ2 magnitude empirical observations reveal that exhibits fewer outliers with substantially compressed value range compared to ˆW , as shown in Figure 4(d)(e). In practice, we can further reduce quantization errors by iteratively updating the low-rank branch through decomposing Q(R) and adjusting accordingly for several iterations, and then picking the result with the smallest error. i=r+ i="
        },
        {
            "title": "4.3 NUNCHAKU: FUSING LOW-RANK AND LOW-BIT BRANCH KERNELS",
            "content": "Although the low-rank branch introduces theoretically negligible computation, running it as separate branch would incur significant latency overheadapproximately 50% of the 4-bit branch latency, as shown in Figure 6(a). This is because, for small rank r, even though the computational cost is greatly reduced, the data sizes of input and output activations remain unchanged, shifting the https://en.wikipedia.org/wiki/Low-rank_approximation 6 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models bottleneck from computation to memory access. The situation deteriorates, especially when the activation cannot fit into the GPU L2 cache. For example, the up projection in the low-rank branch for QKV projection is much slower since its output exceeds the available L2 cache and results in the extra load and store operations to DRAM. Fortunately, we observe that the down projection L1 in the low-rank branch shares the same input as the quantization kernel in the low-bit branch, while the up projection L2 shares the same output as the 4-bit computation kernel, as illustrated in Figure 6(b). By fusing the down projection with the quantization kernel and the up projection with the 4-bit computation kernel, the low-rank branch can share the activations with the low-bit branch, eliminating the extra memory access and also halving the number of kernel calls. As result, the low-rank branch adds only 510% latency, making it nearly cost-free."
        },
        {
            "title": "5.1 SETUPS",
            "content": "Models. We benchmark our methods using the following text-to-image models, including both the UNet (Ronneberger et al., 2015; Ho et al., 2020) and DiT (Peebles & Xie, 2023) backbones: FLUX.1 (Black-Forest-Labs, 2024) is the SoTA open-sourced DiT-based diffusion model. It consists of 19 joint attention blocks (Esser et al., 2024) and 38 parallel attention blocks (Dehghani et al., 2023), totaling 12B parameters. We evaluate on both the 50-step guidance-distilled (FLUX.1dev) and 4-step timestep-distilled (FLUX.1-schnell) variants. PixArt-Σ (Chen et al., 2024a) is another DiT-based model. Instead of using joint attention, it stacks 28 attention blocks composed of self-attention, cross-attention, and feed-forward layers, amounting to 600M parameters. We evaluate it on the default 20-step setting. Stable Diffusion XL (SDXL) is widely-used UNet-based model with 2.6B parameters (Podell et al., 2024). It predicts noise with three resolution scales. The highest-resolution stage is processed entirely by ResBlocks (He et al., 2016), while the other two stages jointly use ResBlocks and attention layers. Like PixArt-Σ, SDXL employs cross-attention layers for text conditioning. We evaluate it in the 30-step setting, along with its 4-step distilled variant, SDXL-Turbo (Sauer et al., 2023). Datasets. Following previous works (Li et al., 2023a; Zhao et al., 2024c;b), we randomly sample the prompts in COCO Captions 2024 (Chen et al., 2015) for calibration. To assess the generalization capability of our method, we adopt two distinct prompt sets with varying styles for benchmarking: MJHQ-30K (Li et al., 2024a) consists of 30K samples from Midjourney with 10 common categories, 3K samples each. We uniformly select 5K prompts from this dataset to evaluate model performance on artistic image generation. Densely Captioned Images (DCI) (Urbanek et al., 2024) is dataset containing 8K images with detailed human-annotated captions, averaging over 1,000 words. For our experiments, we use the summarized version (sDCI), where captions are condensed to 77 tokens using large language models (LLMs) to accommodate diffusion models. Similarly, we randomly sample 5K prompts for efficient evaluation of realistic image generation. Baselines. We compare SVDQuant against the following post-training quantization (PTQ) methods: 4-bit NormalFloat (NF4) is data type for weight-only quantization (Dettmers et al., 2023). It assumes that weights follow normal distribution and is the information-theoretically optimal 4-bit representation. We use the community-quantized NF4 FLUX.1 models (Lllyasviel, 2024) as the baselines. ViDiT-Q (Zhao et al., 2024b) uses per-token quantization and smoothing (Xiao et al., 2023) to alleviate the outliers across different batches and tokens and achieves lossless 8-bit quantization on PixArt-Σ. MixDQ (Zhao et al., 2024c) identifies the outliers in the begin-of-sentence token of text embedding and protects them with 16-bit pre-computation. This method enables up to W4A8 quantization with negligible performance degradation on SDXL-Turbo. TensorRT contains an industry-level PTQ toolkit to quantize the diffusion models to 8 bits. It uses smoothing and only calibrates activations over selected timestep range with percentile scheme. 7 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Table 1: Quantitative quality comparisons across different models. IR means ImageReward. Our 8-bit results closely match the quality of the 16-bit models. Moreover, our 4-bit results outperform other 4-bit baselines, effectively preserving the visual quality of 16-bit models. Backbone Model Precision Method Quality Similarity Quality Similarity FID () IR () LPIPS () PSNR( ) FID () IR () LPIPS () PSNR () MJHQ sDCI FLUX.1 -dev (50 Steps) DiT FLUX.1 -schnell (4 Steps) PixArt-Σ (20 Steps) SDXL -Turbo (4 Steps) UNet BF16 INT W8A W4A16 INT W4A4 FP W4A4 BF16 INT W8A8 W4A16 INT W4A4 FP W4A4 FP16 Ours NF4 Ours Ours Ours NF4 Ours Ours INT W8A8 ViDiT-Q INT W8A8 Ours INT W4A8 ViDiT-Q INT W4A4 ViDiT-Q INT W4A4 FP W4A4 Ours Ours FP16 INT W8A8 MixDQ INT W8A8 Ours INT W4A8 MixDQ INT W4A4 MixDQ INT W4A4 FP W4A4 Ours Ours FP16 SDXL (30 Steps) INT W8A8 TensorRT INT W8A8 Ours INT W4A4 FP W4A4 Ours Ours 20. 20.4 20.6 19.9 21.0 19.2 19.2 18.9 18.4 19.9 16. 15.7 16.3 37.3 412 20.1 18.3 24.3 24.1 24.3 27.7 353 24.5 24.1 16. 20.2 16.6 20.7 19.0 0.953 0.948 0.910 0.932 0.933 0. 0.966 0.943 0.969 0.956 0.944 0.944 0.955 0.573 -2.27 0.898 0.946 0. 0.834 0.845 0.708 -2.26 0.816 0.822 0.729 0.591 0.718 0.609 0.607 0.089 0.272 0.254 0.247 0.120 0.257 0.292 0.279 0.137 0.109 0.611 0.854 0.394 0.326 0.147 0.100 0.402 0.685 0.265 0.250 0.247 0.119 0.298 0.294 27.0 19.5 20.1 20.2 22.9 18.2 17.5 17.5 22.5 23.7 12.0 6.44 16.2 17.4 21.7 24.0 15.7 11.0 17.9 18.5 22.0 26.4 20.6 21.0 24. 24.7 24.9 24.7 25.7 20.8 20.7 20.7 20.1 21.5 24. 23.5 24.2 40.6 425 25.1 23.7 24.7 25.0 24.8 25.9 373 25.7 24.7 22. 25.4 22.4 26.3 25.4 1.02 1.02 0.986 0.992 0.995 0. 0.975 0.953 0.988 0.967 0.966 0.974 0.969 0.600 -2.28 0.922 0.978 0. 0.690 0.701 0.610 -2.28 0.667 0.699 0.573 0.453 0.574 0.494 0.480 0.106 0.292 0.273 0.267 0.133 0.263 0.299 0.278 0.163 0. 0.629 0.838 0.434 0.357 0.157 0.110 0.415 0.686 0.278 0.261 0.265 0. 0.314 0.312 24.9 18.2 18.8 18.7 21. 17.1 16.3 16.6 20.4 21.8 11.2 6.70 14.9 16.1 21.6 23.7 15.7 11.3 17.8 18. 21.7 25.9 20.4 20.7 Metrics. Following previous work (Li et al., 2022; 2024b), we mainly benchmark image quality and similarity to the results produced by the original 16-bit models. For the image quality assessment, we use Fréchet Inception Distance (FID, lower is better) to measure the distribution distance between the generated images and the ground-truth images (Heusel et al., 2017). Besides, we employ Image Reward (higher is better) to approximate the human rating of the generated images (Xu et al., 2024a). We use LPIPS (lower is better) to measure the perceptual similarity (Zhang et al., 2018) and Peak Signal Noise Ratio (PSNR, higher is better) to measure the numerical similarity of the images from the 16-bit models. Please refer to our Appendix B.1 for more metrics (CLIP IQA (Wang et al., 2023), CLIP Score (Hessel et al., 2021) and SSIM). Implementation details. For the 8-bit setting, we use per-token dynamic activation quantization and per-channel weight quantization with low-rank branch of rank 16. For the 4-bit setting, we adopt pergroup symmetric quantization for both activations and weights, along with low-rank branch of rank 32. INT4 quantization uses group size of 64 with 16-bit scales. FP4 quantization uses group size of 32 with FP8 scales (Rouhani et al., 2023). For FLUX.1 models, the inputs of linear layers in adaptive normalization are kept in 16 bits (i.e., W4A16). For other models, key and value projections in the cross-attention are retained at 16 bits since their latency only covers less than 5% of total runtime."
        },
        {
            "title": "5.2 RESULTS",
            "content": "Quality results. We report the quantitative quality results in Table 1 across various models and precision levels, and show some corresponding 4-bit qualitative comparisons in Figure 7. Among https://en.wikipedia.org/wiki/Structural_similarity_index_measure 8 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Figure 7: Qualitative visual results on MJHQ. Image Reward is calculated over the entire dataset. On FLUX.1 models, our 4-bit models outperform the NF4 W4A16 baselines, demonstrating superior text alignment and closer similarity to the 16-bit models. For instance, NF4 misinterprets dinosaur style, generating real dinosaur. On PixArt-Σ and SDXL-Turbo, our 4-bit results demonstrate noticeably better visual quality than ViDiT-Qs and MixDQs W4A8 results. all models, our 8-bit results can perfectly mirror the 16-bit results, achieving PSNR higher than 21, beating all other 8-bit baselines. On FLUX.1-dev, our INT8 PSNR even reaches 27 on MJHQ. For 4-bit quantization, on FLUX.1, our SVDQuant surpasses the NF4 W4A16 baseline regarding Image Reward. On the schnell variant, our Image Reward even exceeds that of the original BF16 model, suggesting stronger human preference. On PixArt-Σ, while our INT4 Image Reward shows slight degradation, our FP4 model achieves an even higher score than the FP16 model. This is likely due to PixArt-Σs small model size (600M parameters), which is already highly compact and benefits from smaller group size. Remarkably, both our INT4 and FP4 results consistently outperform ViDiT-Qs W4A8 results by large margin across all metrics. For UNet-based models, on SDXL-Turbo, our 4-bit models significantly outperform MixDQs W4A8 results, and our FID scores are on par with the FP16 models, indicating no loss in performance. On SDXL, both our INT4 and FP4 results achieve comparable quality to TensorRTs W8A8 performance, which represents the 8-bit SoTA. As shown in Figure 15 in the Appendix, our visual quality only shows minor degradation. Our FP16 PixArt-Σ model is slightly different from ViDiTs, though both offer the same quality. For fair comparisons, ViDiT-Qs similarity results are calculated using their FP16 results. 9 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Figure 8: SVDQuant reduces the model size of the 12B FLUX.1 by 3.6. Additionally, our engine, Nunchaku, further cuts memory usage of the 16-bit model by 3.5 and delivers 3.0 speedups over the NF4 W4A16 baseline on both the desktop and laptop NVIDIA RTX 4090 GPUs. Remarkably, on laptop 4090, it achieves in total 10.1 speedup by eliminating CPU offloading. Figure 9: Our INT4 model seamlessly integrates with off-the-shelf LoRAs without requiring requantization. When applying LoRAs, it matches the image quality of the original 16-bit FLUX.1-dev. See Appendix for the text prompts. Memory save & speedup. In Figure 8, we report measured model size, memory savings, and speedup for FLUX.1. Our INT4 quantization reduces the original transformer size from 22.2 GiB to 6.1 GiB, including 0.3 GiB overhead due to the low-rank branch, resulting in an overall 3.6 reduction. Since both weights and activations are quantized, compared to the NF4 weight-only-quantized variant, our inference engine Nunchaku even saves more memory footprint, and offers 3.0 speedup on both desktopand laptop-level NVIDIA RTX 4090 GPUs. Notably, while the original BF16 model requires per-layer CPU offloading on the 16GB laptop 4090, our INT4 model fits entirely in GPU memory, resulting in 10.1 speedup by avoiding offloading. We anticipate even greater speedups for FP4-quantized models on NVIDIAs next-generation Blackwell GPUs, as they inherently support microscaling for group quantization without the need for specialized GPU kernels. Integrate with LoRA. Previous quantization methods require fusing the LoRA branches and requantizing the model when integrating LoRAs. In contrast, our Nunchaku eliminates redundant memory access, allowing adding separate LoRA branch. In practice, we can fuse the LoRA branch into our low-rank branch by slightly increasing the rank, further enhancing efficiency. In Figure 9, we exhibit some visual examples of applying LoRAs of five different styles (Realism, Ghibsky Illustration, Anime, Children Sketch, and Yarn Art) to our INT4 FLUX.1-dev model. Our INT4 model successfully adapts to each style while preserving the image quality of the 16-bit version. For more visual examples, see Appendix B.2. Ablation study. In Figure 10, we present several ablation studies of SVDQuant on PixArt-Σ. First, both SVD-only and naïve quantization perform poorly in the 4-bit setting, resulting in severe degradation of image quality. While applying smoothing to the quantization slightly improves image quality compared to naïve quantization, the overall results remain unsatisfactory. LoRC (Yao et al., 2023) introduces low-rank branch to compensate for quantization errors, but this approach is suboptimal. Quantization errors exhibit smooth singular value distribution. Consequently, low-rank compensation fails to effectively mitigate these errors, as discussed in Section 4.2. In contrast, we first decompose the weights and quantize only the residual. As demonstrated in Figure 5, the first several singular values are significantly larger than the rest, allowing us to shift them to the low-rank branch, which 10 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Figure 10: Ablation study of SVDQuant on PixArt-Σ. The rank of the low-rank branch is 64. Image Reward is measured over 1K samples from MJHQ. Our results significantly outperform the others, achieving the highest image quality by wide margin. Figure 11: Increasing the rank of the low-rank branch in SVDQuant can enhance image quality, but it also leads to higher parameter and latency overhead. effectively reduces weight magnitude. Finally, smoothing consolidate the outliers, further enabling the low-rank branch to absorb outliers from the activations and substantially improving image quality. Trade-off of increasing rank. Figure 11 presents the results of different rank in SVDQuant on PixArt-Σ. Increasing the rank from 16 to 64 significantly enhances image quality but increases parameter and latency overhead. In our experiments, we select rank of 32, which offers decent quality with minor overhead."
        },
        {
            "title": "6 CONCLUSION & DISCUSSION",
            "content": "In this work, we introduce novel 4-bit post-training quantization paradigm, SVDQuant, for diffusion models. It adopts low-rank branch to absorb the outliers in both the weights and activations, easing the process of quantization. Our inference engine Nunchaku further fuses the low-rank and low-bit branch kernels, reducing memory usage and cutting off redundant data movement overhead. Extensive experiments demonstrate that SVDQuant preserves image quality. Nunchaku further achieves 3.5 reduction in memory usage over the original 16-bit model and 3.0 speedup over the weight-only quantization on an NVIDIA RTX-4090 laptop. This advancement enables the efficient deployment of largescale diffusion models on edge devices, unlocking broader potential for interactive AI applications. Limitations. In this work, we do not report the speedups for our FP4 models. This is because we have no access to Blackwell GPUs, which natively support the precision and microscaling for group quantization. On Blackwell hardware, we anticipate greater speedups compared to our INT4 results on 4090 GPUs."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank MIT-IBM Watson AI Lab, MIT and Amazon Science Hub, MIT AI Hardware Program, National Science Foundation, Packard Foundation, Dell, LG, Hyundai, and Samsung for supporting this research. We thank NVIDIA for donating the DGX server. 11 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models"
        },
        {
            "title": "REFERENCES",
            "content": "Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. 4 Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 3 Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. 3 Black-Forest-Labs. Flux.1, 2024. URL https://blackforestlabs.ai/. 2, 7 Han Cai, Muyang Li, Qinsheng Zhang, Ming-Yu Liu, and Song Han. Condition-aware neural network for controlled image generation. In CVPR, 2024. 3 Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024a. 7 Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 7 Zigeng Chen, Xinyin Ma, Gongfan Fang, Zhenxiong Tan, and Xinchao Wang. Asyncdiff: Parallelizing diffusion models by asynchronous denoising. arXiv preprint arXiv:2406.06911, 2024b. 3 Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In ICML. PMLR, 2023. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. NeurIPS, 2022. 2, 3 Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of quantized LLMs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 4, 7 Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 3, 7 fal.ai. Auraflow v0.1, 2024. URL https://blog.fal.ai/auraflow/. 2 Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. ICLR, 2023. 3 Han Guo, Philip Greengard, Eric Xing, and Yoon Kim. Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model finetuning. ICLR, 2024. 4 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 7 Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate post-training quantization for diffusion models. NeurIPS, 2023. 3 Yefei He, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Efficientdm: Efficient quantizationaware fine-tuning of low-bit diffusion models. In ICLR, 2024. 3, 4 Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: referencefree evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 8, 18 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. 8 Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2, 3, 7 Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language model compression with weighted low-rank factorization. In The Tenth International Conference on Learning Representations, 2022. 3, 4 Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, 2022. Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, and Xianglong Liu. Tfmq-dm: Temporal feature maintenance quantization for diffusion models. In CVPR, 2024. 3 Ajay Jaiswal, Lu Yin, Zhenyu Zhang, Shiwei Liu, Jiawei Zhao, Yuandong Tian, and Zhangyang Wang. From galore to welore: How low-rank weights non-uniformly emerge from low-rank gradients. arXiv preprint arXiv: 2407.11239, 2024. 4 Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. arXiv preprint arXiv:2405.05967, 2024. 3 Sehoon Kim, Coleman Richard Charles Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, and Kurt Keutzer. SqueezeLLM: Dense-and-sparse quantization. In ICML, 2024. 3 Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024a. Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, and Song Han. Gan compression: Efficient architectures for interactive conditional gans. In CVPR, 2020. 3 Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, and Jun-Yan Zhu. Efficient spatially sparse inference for conditional gans and diffusion models. In NeurIPS, 2022. 3, 8 Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In CVPR, 2024b. 3, 8 Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In ICCV, 2023a. 3, 7 Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. NeurIPS, 2023b. 3 Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao. LoSparse: Structured compression of large language models based on low-rank and sparse approximation. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pp. 2033620350. PMLR, 2023c. 4 Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language models. In The Twelfth International Conference on Learning Representations, 2024c. 4 Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. In MLSys, 2024a. 2, 3, 5 Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024b. 3, 13 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024a. 3 Wenxuan Liu and Saiqian Zhang. Hq-dit: Efficient diffusion transformer with fp4 hybrid quantization. arXiv preprint arXiv:2405.19751, 2024. Xuewen Liu, Zhikai Li, Junrui Xiao, and Qingyi Gu. Enhanced distribution alignment for post-training quantization of diffusion models. arXiv preprint arXiv:2401.04585, 2024b. 3 Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquantllm quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024c. 4 Lllyasviel. [major update] bitsandbytes guidelines and flux webui-forge discussion #981, 2024. stable-diffusion-webui-forge/discussions/981. lllyasviel stable-diffusionURL https://github.com/lllyasviel/ Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. 3 Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv: 2310.04378, 2023. 3 Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. arXiv preprint arXiv:2406.01733, 2024a. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In CVPR, 2024b. 3 Pascal Massart. Concentration inequalities and model selection: Ecole dEté de Probabilités de Saint-Flour XXXIII-2003. Springer, 2007. 17 Chenlin Meng, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. arXiv preprint arXiv:2210.03142, 2022a. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022b. 2 NVIDIA. Nvidia blackwell architecture technical brief, 2024. URL https://resources. nvidia.com/en-us-blackwell-architecture. 2 William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 3, 4, 7 Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 2, 3, Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 18 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3 Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234241. Springer, 2015. 3, 7 Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. 8 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 2 Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2021. 3 Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. 3, Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In CVPR, 2023. 3 Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 3 Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In ICML, 2023. Yang Sui, Yanyu Li, Anil Kag, Yerlan Idelbayev, Junli Cao, Ju Hu, Dhritiman Sagar, Bo Yuan, Sergey Tulyakov, and Jian Ren. Bitsfusion: 1.99 bits weight quantization of diffusion model. arXiv preprint arXiv:2406.04333, 2024. 3 Siao Tang, Xin Wang, Hong Chen, Chaoyu Guan, Zewen Wu, Yansong Tang, and Wenwu Zhu. Post-training quantization with progressive calibration and activation relaxing for text-to-image diffusion models. arXiv preprint arXiv:2311.06322, 2023. 3 Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana RomeroSoriano. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. In CVPR, 2024. 7 Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, and Jiwen Lu. Towards accurate post-training quantization for diffusion models. In CVPR, 2024a. 3 Haoxuan Wang, Yuzhang Shang, Zhihang Yuan, Junyi Wu, and Yan Yan. Quest: Low-bit diffusion model quantization via efficient selective finetuning. arXiv preprint arXiv:2402.03666, 2024b. Jiannan Wang, Jiarui Fang, Aoyu Li, and PengCheng Yang. Pipefusion: Displaced patch pipeline parallelism for inference of diffusion transformer models. arXiv preprint arXiv:2405.14430, 2024c. 3 Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In AAAI, 2023. 8, 18 Junyi Wu, Haoxuan Wang, Yuzhang Shang, Mubarak Shah, and Yan Yan. Ptq4dit: Post-training quantization for diffusion transformers. arXiv preprint arXiv:2405.16005, 2024. 3 Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In ICML, 2023. 2, 3, 5, Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. NeurIPS, 2024a. 8 Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models. In The Twelfth International Conference on Learning Representations, 2024b. 4 Yuewei Yang, Xiaoliang Dai, Jialiang Wang, Peizhao Zhang, and Hongbo Zhang. Efficient quantization strategies for latent diffusion models. arXiv preprint arXiv:2312.05431, 2023. 3 Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation. arXiv preprint arXiv:2303.08302, 2023. 4, 10 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024a. 3 Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024b. 3 Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activationaware singular value decomposition for compressing large language models. arXiv preprint arXiv: 2312.05821, 2023. 4 Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In ICLR, 2022. 3 Qinsheng Zhang, Molei Tao, and Yongxin Chen. gddim: Generalized denoising diffusion implicit models. In ICLR, 2022. 3 Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. GaLore: Memory-efficient LLM training by gradient low-rank projection. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pp. 6112161143. PMLR, 2024a. 4 Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, et al. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation. arXiv preprint arXiv:2406.02540, 2024b. 3, 7 Tianchen Zhao, Xuefei Ning, Tongcheng Fang, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan, Guohao Dai, and Yu Wang. Mixdq: Memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization. arXiv preprint arXiv:2405.17873, 2024c. 3, 7 Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. MLSys, 2024d. 3 Xingyu Zheng, Haotong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao, Jinyang Guo, and Xianglong Liu. Binarydm: Towards accurate binarization of diffusion model. arXiv preprint arXiv:2404.05662, 2024. 3 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models"
        },
        {
            "title": "A MISSING PROOFS",
            "content": "A.1 PROOF OF PROPOSITION 4.1 Proof. XW Q(X)Q(W )F = XW XQ(W ) + XQ(W ) Q(X)Q(W )F X(W Q(W ))F + (X Q(X))Q(W )F XF Q(W )F + Q(X)F Q(W )F XF Q(W )F + Q(X)F (W Q(W ))F XF Q(W )F + Q(X)F (W + Q(W )F ). A.2 PROOF OF PROPOSITION 4.2 Proof. So, = (cid:13) (cid:13) (cid:13) (cid:13) Q(R)F = sR QRF sR sR =sR round sR (cid:13) (cid:13) (cid:13) (cid:13) sR round (cid:19)(cid:13) (cid:13) (cid:13) (cid:13)F . (cid:18) sR (cid:19)(cid:13) (cid:13) (cid:13) (cid:13)F (cid:18) sR [R Q(R)F ] [sR] (cid:112) size(R) (cid:112)size(R) qmax (cid:112)size(R) qmax = [max(R)] σ(cid:112)2 log (size(R)), (8) where σ is the std deviation of the normal distribution. Equation 8 comes from the maximal inequality of Gaussian variables (Lemma 2.3 in Massart (2007)). On the other hand, [RF ] (cid:115)(cid:88) =E x2 xR (cid:35) (cid:34) (cid:80) xR (cid:112)size(R) (cid:114) =σ 2size(R) π , (9) (10) where Equation 9 comes from Cauchy-Schwartz inequality and Equation 10 comes from the expectation of half-normal distribution. 17 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models [R Q(R)F ] (cid:112)size(R) qmax (cid:112)log (size(R)) π qmax σ(cid:112)2 log (size(R)) [RF ] . Together, we have"
        },
        {
            "title": "B ADDITIONAL RESULTS",
            "content": "B.1 QUALITY RESULTS We report extra quantitative quality results with additional metrics in Table 2. Specifically, CLIP IQA (Wang et al., 2023) and CLIP Score (Hessel et al., 2021) assesses the image quality and textimage alignment with CLIP (Radford et al., 2021), respectively. Structural Similarity Index Measure (SSIM) is used to measure the luminance, contrast, and structure similarity of images produced by our 4-bit model against the original 16-bit model. We also visualize more qualitative comparsions in Figures 12, 13, 14, 15 and 16. Table 2: Additional quantitative quality comparisons across different models. C.IQA means CLIP IQA, and C.SCR means CLIP Score. Backbone Model Precision Method Quality Similarity Quality Similarity C.IQA () C.SCR () SSIM( ) C.IQA () C.SCR () SSIM () MJHQ sDCI FLUX.1 -dev (50 Steps) DiT FLUX.1 -schnell (4 Steps) PixArt-Σ (20 Steps) SDXL -Turbo (4 Steps)"
        },
        {
            "title": "UNet",
            "content": "BF16 INT W8A8 W4A16 INT W4A4 FP W4A4 BF16 INT W8A8 W4A16 INT W4A4 FP W4A FP16 Ours NF4 Ours Ours Ours NF4 Ours Ours INT W8A8 ViDiT-Q INT W8A8 Ours INT W4A8 ViDiT-Q INT W4A4 ViDiT-Q INT W4A4 FP W4A"
        },
        {
            "title": "Ours\nOurs",
            "content": "FP16 INT W8A8 MixDQ INT W8A"
        },
        {
            "title": "Ours",
            "content": "INT W4A8 MixDQ INT W4A4 MixDQ INT W4A4 FP W4A"
        },
        {
            "title": "Ours\nOurs",
            "content": "FP16 SDXL (30 Steps) INT W8A8 TensorRT INT W8A"
        },
        {
            "title": "Ours",
            "content": "INT W4A4 FP W4A"
        },
        {
            "title": "Ours\nOurs",
            "content": "0.952 0.953 0.947 0.950 0.950 0.938 0.938 0.941 0.939 0. 0.944 0.948 0.947 0.912 0.185 0.927 0.935 0.926 0.922 0.925 0.893 0.556 0.916 0. 0.907 0.905 0.912 0.916 0.919 26.0 26.0 25.8 25.8 25. 26.6 26.6 26.6 26.5 26.5 26.8 26.7 26.8 25.7 13.3 26.6 26. 26.5 26.5 26.5 25.9 13.1 26.5 26.4 27.2 26.7 27.0 26.5 26. 0.748 0.748 0.773 0.780 0.844 0.713 0.693 0. 0.815 0.849 0.356 0.077 0.602 0.652 0.763 0.821 0.512 0.289 0.630 0. 0.733 0.843 0.630 0.640 0.955 0.955 0.951 0.953 0. 0.932 0.932 0.933 0.932 0.933 0.966 0.966 0.967 0.917 0.176 0.952 0. 0.913 0.907 0.912 0.895 0.548 0.894 0.901 0.911 0.901 0.910 0.894 0. 25.4 25.4 25.4 25.3 25.3 26.2 26.2 26.2 26.2 26. 26.1 26.1 26.0 25.4 13.3 26.1 26.1 26.5 26.5 26.5 26.1 11.9 26.8 26. 26.5 26.1 26.3 26.8 26.7 0.697 0.697 0.721 0. 0.811 0.674 0.647 0.667 0.756 0.800 0.295 0.080 0.519 0. 0.750 0.808 0.493 0.296 0.610 0.620 0.697 0.814 0.610 0. 18 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Figure 12: Qualitative visual results of FLUX.1-dev on MJHQ. Figure 13: Qualitative visual results of FLUX.1-schnell on MJHQ. 19 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Figure 14: Qualitative visual results of PixArt-Σ on MJHQ. Figure 15: Qualitative visual results of SDXL on MJHQ. 20 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Figure 16: Qualitative visual results of SDXL-Turbo on MJHQ. SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models B.2 LORA RESULTS In Figure 17, we showcase more visual results of applying the aforementioned five communitycontributed LoRAs of different styles (Realism, Ghibsky Illustration, Anime, Children Sketch, and Yarn Art) to our INT4 quantized models. Figure 17: Additional LoRA results on FLUX.1-dev. When applying LoRAs, our INT4 model matches the image quality of the original BF16 model. See Appendix for the detailed used text prompts. 22 SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models"
        },
        {
            "title": "C TEXT PROMPTS",
            "content": "Below we provide the text prompts we use in Figure 9 (from left to right). surrounded by quaint cottages man in armor with beard and sword GHIBSKY style, fisherman casting line into peaceful village lake (cid:44) girl, neck tuft, white hair, sheep horns, blue eyes, nm22 style sketched style, squirrel wearing glasses and reading tiny book under (cid:44) panda playing in the snow, yarn art style an oak tree The text prompts we use in Figure 17 are (in the rasterizing order): burning building male secret agent in tuxedo, holding gun, standing in front of (cid:44) handsome man in suit, 25 years old, cool, futuristic knight in shining armor, standing in front of castle under siege knight fighting fire-breathing dragon in front of medieval castle, (cid:44) male wizard with long white beard casting lightning spell in the (cid:44) young woman with long flowing hair, standing on mountain peak at dawn, (cid:44) overlooking misty valley middle of storm flames and smoke distant city lights soft glow of lanterns lighting up the path surrounded by towering pine trees and rocky cliffs GHIBSKY style, cat on windowsill gazing out at starry night sky and (cid:44) GHIBSKY style, quiet garden at twilight, with blooming flowers and the (cid:44) GHIBSKY style, serene mountain lake with crystal-clear water, (cid:44) GHIBSKY style, an enchanted forest at night, with glowing mushrooms and (cid:44) GHIBSKY style, peaceful beach town with colorful houses lining the (cid:44) GHIBSKY style, cozy living room with view of snow-covered forest, the fireplace crackling and blanket draped over comfy chair (cid:44) shore and calm ocean stretching out into the horizon fireflies lighting up the underbrush dog wearing wizard hat, nm22 anime style girl looking at the stars, nm22 anime style fish swimming in pond, nm22 style giraffe with long scarf, nm22 style bird sitting on branch, nm22 minimalist style girl wearing flower crown, nm22 style flying overhead fields and forests below flowers with gentle breeze blowing sketched style, garden full of colorful butterflies and blooming (cid:44) sketched style, beach scene with kids building sandcastles and seagulls (cid:44) sketched style, hot air balloon drifting peacefully over patchwork of (cid:44) sketched style, sunny meadow with girl in flowy dress chasing (cid:44) sketched style, little boy dressed as pirate, steering toy ship on (cid:44) sketched style, small boat floating on peaceful lake, surrounded by (cid:44) trees and mountains small stream butterflies hot air balloon flying over mountains, yarn art style cat chasing butterfly, yarn art style squirrel collecting acorns, yarn art style wizard casting spell, yarn art style jellyfish floating in the ocean, yarn art style sea turtle swimming through coral reef, yarn art style"
        }
    ],
    "affiliations": [
        "MIT",
        "UC Berkeley",
        "NVIDIA",
        "CMU",
        "Princeton",
        "SJTU",
        "Pika Labs"
    ]
}