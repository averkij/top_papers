{
    "paper_title": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion",
    "authors": [
        "Ralf Römer",
        "Yi Zhang",
        "Angela P. Schoellig"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "To teach robots complex manipulation tasks, it is now a common practice to fine-tune a pre-trained vision-language-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, a general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning a new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at https://tum-lsy.github.io/clare."
        },
        {
            "title": "Start",
            "content": "CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion Ralf Romer,1 Yi Zhang,1 Angela P. Schoellig1,2 6 2 0 2 J 4 1 ] . [ 1 2 1 5 9 0 . 1 0 6 2 : r Abstract To teach robots complex manipulation tasks, it is now common practice to fine-tune pre-trained visionlanguage-action model (VLA) on task-specific data. However, since this recipe updates existing representations, it is unsuitable for long-term operation in the real world, where robots must continually adapt to new tasks and environments while retaining the knowledge they have already acquired. Existing continual learning methods for robotics commonly require storing previous data (exemplars), struggle with long task sequences, or rely on task identifiers for deployment. To address these limitations, we propose CLARE, general, parameter-efficient framework for exemplar-free continual learning with VLAs. CLARE introduces lightweight modular adapters into selected feedforward layers and autonomously expands the model only where necessary when learning new task, guided by layer-wise feature similarity. During deployment, an autoencoder-based routing mechanism dynamically activates the most relevant adapters without requiring task labels. Through extensive experiments on the LIBERO benchmark, we show that CLARE achieves high performance on new tasks without catastrophic forgetting of earlier tasks, significantly outperforming even exemplar-based methods. Code and data are available at the project website: tum-lsy.github.io/clare. I. INTRODUCTION Robots deployed in homes, hospitals, or warehouses must operate for long periods while facing ever-changing conditions and task demands. household robot may encounter newly purchased appliance, or an assistive robot may meet patients with unfamiliar mobility profiles. In such settings, robots must continually acquire new skills without sacrificing previously acquired capabilities. This long-term adaptability, known as continual or lifelong learning [1], remains an open challenge in robotics despite decades of research [2][4]. Recent advances in vision-language-action models (VLAs) have demonstrated impressive performance on complex, long-horizon tasks by integrating perception, language understanding, and action generation within unified model [5] [8]. Pre-training on diverse data sources, including internetscale vision-language data and robot demonstrations [9], provides VLAs with broad priors that enable certain degree of generalization [6]. However, state-of-the-art VLAs still cannot adapt reliably to unseen tasks without fine-tuning on task-specific data [6][8]. In continual learning setting, where new tasks and environments emerge over time, naive approach would be to iteratively fine-tune VLA Equal contribution. 1 Technical University of Munich, Germany; TUM School of Computation, Information and Technology, Learning Systems and Robotics Lab; Munich Institute of Robotics and Machine Intelligence (MIRMI). 2 Robotics Institute Germany. Corresponding email: {ralf.roemer@tum.de} Fig. 1: Starting from pretrained vision-language-action model (VLA), CLARE autonomously and continually expands selected feedforward network (FFN) layers with new lightweight adapters. During inference, the most relevant adapters are selected based on feature similarity, captured by learned autoencoder discriminators. By freezing existing parameters and fine-tuning only the new ones at each stage, we can acquire new task-specific knowledge without catastrophic forgetting of previously learned skills. on new datasets. However, updating the parameters shared across modalities without regard for previously learned representations leads to significant degradation of both semantic grounding and policy performance on old tasks, common issue known as catastrophic forgetting [10]. Therefore, current VLAs are not inherently capable of continual learning, where new tasks and environments emerge over time. Experience replay [11][13] can mitigate forgetting; however, it has major practical drawbacks in robotics. Past data may be unavailable due to storage or privacy constraints in lifelong learning setting, selecting representative samples for replay is challenging, and maintaining and accessing large replay buffer increases both computational and memory overhead. Hence, there is strong need for exemplar-free continual learning methods for VLAs that can preserve old knowledge while acquiring new skills. Modular and expandable architectures [14], [15] represent promising direction for scalable continual learning as they allocate new capacity for each task instead of overwriting shared representations. However, existing approaches in this direction typically require oracle task identifiers, which are generally not available when robots operate autonomously in open-world settings. Moreover, the application of these methods to robotics has so far been mostly limited to multitask learning, where all tasks and data are available in advance [14], [16]. To close this gap, we introduce Continual Learning via Adapter Routing and Expansion (CLARE), general framework that enables VLAs to continually incorporate new taskspecific knowledge without exemplars, task labels, or predefined expansion rules. CLARE injects lightweight adapters into subset of feedforward layers and expands only when the feature statistics indicate substantial novelty, as visualized in Figure 1. At deployment, an autoencoder-based routing mechanism dynamically selects among the adapters, enabling autonomous task-agnostic inference. This design maintains balance between stability and plasticity by preserving pre-trained representations and adding capacity as needed, enabling VLAs to learn new tasks with minimal parameter growth. In summary, our main contributions are: lightweight, modular adapter framework enabling VLAs to acquire new skills without overwriting prior representations. An autonomous routing mechanism that activates the most suitable adapters during inference using feature similarity without task identifiers. dynamic expansion strategy that increases parameter count by only about 2% per task in our experiments. Extensive simulation experiments on the LIBERO benchmark demonstrating that CLARE significantly outperforms continual learning baselines. We focus on relatively small models in this work to evaluate the validity of CLARE with limited computational resources. However, our ideas can be straightforwardly extended to large-scale VLAs in the future. II. RELATED WORK 1) Vision-Language-Action Models: Building upon the success of scaling laws in vision and language models [17], [18], the robotics community has recently begun to collect massive multimodal datasets [9] and exploit generative modeling architectures [19], [20] to equip robots with broad semantic priors and task generalization abilities [5][8]. These VLAs are usually based on vision-language models [21] and pre-trained on robot demonstrations [9] via imitation learning, enabling end-to-end mapping from highdimensional multimodal observations to robot actions. Despite the growing scale of training data and model capacity, the ability of VLAs to generalize zero-shot to unseen tasks and environments remains very limited [6], [22]. VLAs often overfit to their pre-training domains, as real-world robot data is much more expensive and scarce than webscale vision and text datasets [3]. Therefore, to achieve high performance on specific task, it has become the standard approach to fine-tune pre-trained VLA on curated, highquality demonstrations [5][8]. However, in settings where tasks arrive sequentially and old data may be unavailable, this naive fine-tuning recipe is inadequate since it overwrites previously learned task knowledge, leading to catastrophic forgetting [23]. 2) Continual Learning: Acquiring new skills from stream of data without catastrophic forgetting of previously learned capabilities or losing plasticity is hard problem in deep learning [4]. widely used approach is experience replay (ER) [11][13], which retains subset of past examples and mixes them together with new data to preserve existing representations during training. Since storing exemplars may be infeasible, regularization-based methods [24], [25] constrain parameter updates for weights deemed important to past task. For example, Elastic Weight Consolidation (EWC) [24] uses Fisher-information penalty to protect critical parameters during subsequent training. Related to this idea, PackNet [26] prunes less relevant parameters from the previously learned task and re-trains them for the new incoming data. However, methods like EWC or PackNet struggle with long task sequences as they are restricted by fixed set of initial parameters. To overcome capacity bottlenecks and avoid catastrophic forgetting, architectural methods [14], [15], [27] inject new parameters or modules for novel tasks. By keeping the original model frozen and leveraging techniques such as lowrank adaptation (LoRA) [28], these methods can store new task-specific knowledge in memory-efficient way. 3) Continual Imitation Learning in Robotics: Data scarcity and safety concerns make continual learning for robotics particularly challenging [2], [3]. LOTUS [29], hierarchical method, constructs an ever-growing library of skill policies [30] and uses meta-policy to select the correct skill during deployment. However, training the metapolicy requires ER of previous data, leading to high storage requirements. Sparse Diffusion Policy (SDP) [16] introduces task-specific expert modules to diffusion policies but requires oracle task identifiers during deployment to manually dispatch the input to the correct expert. Hence, SDP cannot operate fully autonomously in continual learning scenarios. Another recent work [31] fine-tunes pre-trained base VLA checkpoint on each new task, and employs task scheduler to select from model library for deployment. However, this method is not memory-efficient and does not enable knowledge sharing between tasks, limiting its practicality for long-term deployment in the real world. III. PROBLEM SETUP We consider robotic system with state st and action at at timestep t. We focus on the common problem of task-incremental learning, in which the robot must sequentially learn new tasks {Tn}N n=1 during its operational lifetime, with the total number of tasks, , unknown. Each task Tn = (ρn 0 , ln) is characterized by an initial distribution of the state of the robot and the environment ρn 0 and natural language instruction ln describing the desired outcome. , . . . , Nc We assume the availability of base policy π0 = πθ0 with model parameters θ0 that has been pre-trained on largescale internet and robot data. The policy takes as input an observation ot = (I 1 , qt, l) consisting of camera images nc , nc = 1, . . . , Nc, proprioceptive state qt and language instruction l, and generates an action chunk (sequence) At = (at, . . . , at+H1) π0( ot). The first actions in At are applied to the robot, and the policy generates new action chunk at timestep + in receding horizon manner [32], [33]. Pre-training has provided the base policy with general visual, language, and action representations, but it cannot solve new tasks zeroshot. The robot should be able to learn new task Tn at stage while retaining the general knowledge from pre-training and without forgetting how to solve previous tasks T1, . . . , Tn1. Specifically, given an expert demonstrat ), ln}T tion dataset Dn = {(on t=1 of observation-action pairs for task Tn, we aim to train new policy πn = πθn with parameters θn. Due to the aforementioned reasons, we consider exemplar-free continual learning, i.e., data from earlier stages D1, . . . , Dn1 is not available. Hence, only the previous model parameters θn1 and the new data Dn can be used to adapt the policy to the new task. , an IV. METHODOLOGY In this section, we describe our proposed method, CLARE, for continual learning without forgetting for pre-trained generative policies. The training and inference strategies are summarized in Algorithms 1 and 2, respectively. A. Base Policy Demonstration datasets for imitation learning typically involve high-dimensional, multimodal distributions [34]. To capture these distributions, we train the policy using flow matching [20] during both pre-training and continual learning. Hereby, the policy at stage learns vector field vθn that transports action chunk samples from simple base distribution (e.g., Gaussian) to the target distribution. We adopt the standard conditional flow matching loss L(θn) = Es,(A1,o),A0 (cid:2)(cid:13) (cid:13)vθn (As, o, s) (A1 A0)(cid:13) (cid:13)2 (cid:3), (1) where U([0, 1]), (A1, o) Dn, A0 (0, I), and As = (1 s)A0 + sA1. After training, we can generate new action chunks At = A1 πn( ot) by Euler integration of the learned vector field, starting from Gaussian noise A0 (0, I), via As+δs = As + δsvθn (As, ot, s), with = 1/δs integration steps. Nearly all state-of-theart VLAs are transformer-based [5][8]. However, since our method is agnostic to the specific model architecture, we keep the following presentation of our methodology general. Algorithm 1 Continual learning for VLAs with CLARE. Require: Pretrained base VLA policy with parameters θ0, set of expandable layers E, expansion threshold γ. 1: for all layers ℓ do Set Aℓ = , kℓ = 0. 2: Set Dℓ = . 3: 4: for all tasks Tn {T1, T2, . . . } do 5: Set θn θn1. Collect demonstration data Dn. for all layers ℓ do Initialize adapter modules Initialize discriminator modules Continual learn. Dynamic Expansion ℓ Dℓ do for all discriminators Dj Compute z-score zj Expand Dℓ Dℓ (cid:8)Dn Update model parameters: θn (θn, Dn if = 1 or zj ℓ via (7). ℓ ℓ ). ℓ > γ for all = 1, . . . , 1 then (cid:9). New discriminator Set kℓ kℓ + 1. Expand Aℓ Aℓ (cid:8)Akℓ Update model parameters: θn (θn, Akℓ Link new discriminator Bℓ(Dn (cid:9). New adapter ℓ ). ℓ ) = Akℓ ℓ . ℓ else"
        },
        {
            "title": "Link Dn",
            "content": "ℓ to an existing adapter via (8). if > 1 and no layers ℓ were expanded then Expand the shallowest layer ℓ1 E. Train Akℓ Train Dn ℓ of all layers ℓ from Dn via (1). ℓ of all layers ℓ from Dn via (5). 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: Algorithm 2 Autonomous routing during deployment. Require: Adapters Aℓ, discriminators Dℓ, learned linking Bℓ : Dℓ Aℓ, input feature xℓ."
        },
        {
            "title": "Compute the reconstruction error ej",
            "content": "1: for all discriminators Dj 2: 3: Select the most relevant adapter 4: Sum the outputs of original FFN and adapter via (3). ℓ Aℓ via (6). ℓ(xℓ) via (4). ℓ Dℓ do B. Modularized Adapters The policy must continually acquire new task-specific knowledge, but it should leverage the general representations to new tasks in parameterfrom pre-training to adapt efficient way. To achieve this, we draw inspiration from the mixture-of-experts (MoE) approach in large language models (LLMs) [35], [36], which combines the outputs of specialized sub-networks during inference. However, while their number is fixed in MoE, our setup requires continually injecting parameters into the model to learn new tasks, and we aim to do this in memory-efficient way. Multiple studies have shown that large fraction of factual associations and high-level knowledge in transformerbased LLMs is stored inside mid-layer feedforward (FFN) modules [37], [38]. Motivated by this insight, we define set of ne expandable FFN layers = {ℓ1, . . . , ℓne } for continual learning. At each stage n, maximum of one adapter module is added as side branch per expandable layer according to the dynamic expansion strategy detailed in Section IVD. We employ lightweight encoder-decoder structure with ReLU activation functions for the adapters. Denoting the input feature of an expandable layer ℓ by xℓ Rdℓ, the output of the i-th adapter in that layer is given by (cid:1), (2) Ai ℓ,i xℓ ℓ = (cid:8)A1 ℓ(xℓ) = up ℓ,i Rdℓr, down ℓ,iReLU(cid:0)W down where up ℓ,i Rrdℓ, and dℓ. We denote the set of adapters in layer ℓ at stage (cid:9) with kℓ n. To maintain distinct by An representations for each task, we train only the newly added adapters on the data Dn and freeze the rest of the model. During inference, routing mechanism described in Section IVC activates one adapter ℓ Aℓ per layer, and its output is added to that of the original pre-trained layer FFNpre ℓ () as ℓ , . . . , Akℓ ℓ FFNℓ(xℓ) = FFNpre ℓ (xℓ) + ℓ (xℓ). (3) Adding new adapters as parallel side branches to the model is beneficial as it preserves the original network structure and does not change the input and output of existing layers and adapters. C. Autonomous Routing During deployment, routing mechanism needs to determine which adapter ℓ Aℓ to activate in each layer ℓ E. This selection should be autonomous and based solely on the current observation, i.e., without requiring task labels, since these are typically not provided in open, realworld scenarios. Unlike fixed-size routing in MoE, our setup requires selecting from continually increasing set of adapters. We achieve this by designing an expandable and lightweight routing mechanism that selects, for each expandable layer ℓ E, the adapter most relevant to the current situation, as shown in Figure 2. We pair each layer with an expanding set of autoencoder discriminators Dℓ = {D1 ℓ , . . . }, all of which receive the same features xℓ as input, and attach new discriminator Dn ℓ at each stage n. Every discriminator Dj ℓ , = 1, . . . , n, is linked to one corresponding adapter Ai ℓ ) through surjective mapping Bℓ : Dℓ Aℓ, as explained in Section IV-D. We use the reconstruction errors of the discriminators ℓ = Bℓ(Dj ℓ , D2 ej ℓ(xℓ) = (cid:13) (cid:13)xℓ Dj (cid:13) ℓ (xℓ) (cid:13) (cid:13) (cid:13) , = 1, . . . , n, (4) to determine the most relevant adapter. By training the discriminators added at stage with the loss (cid:105) ej ℓ(xℓ) ℓ ) = ExℓDn Lrecon(Dn (5) (cid:104) , we ensure that the discriminators have lower reconstruction error when the input features belong to the training distribution of their corresponding adapter. During inference, we activate the most relevant adapter that is linked to the discriminator with the smallest reconstruction error (4) via the routing mechanism ℓ (xℓ) = Bℓ (cid:1), (cid:0)Dj ej = arg min ℓ(xℓ). j{1,...,n} ℓ where (6a) (6b) Fig. 2: CLARE sequentially adds adapters and discriminators as side branches to selected feedforward network layers of pretrained VLA. Top: During inference, our routing mechanism activates only the most relevant adapter that is linked to the discriminator with the lowest reconstruction error for the input feature. Bottom: During the dynamic expansion phase, if all z-scores exceed threshold γ, new adapter and discriminator are added to the corresponding layer. If at least one z-score value is smaller than γ, we only add discriminator and link it to the most relevant adapter. The routing strategy for layer ℓ is summarized in Algorithm 2. The distribution of features xℓ depends on the adapters in the shallower layers and, therefore, changes when training those adapters. To ensure stable training of the discriminators, we adopt two-stage training strategy. First, we train the new adapters jointly using the flow matching loss (1). Then, we freeze all parameters except for the newly added discriminators and train them using the reconstruction loss (5). D. Dynamic Expansion To effectively capture task-specific knowledge without catastrophic forgetting in the context of exemplar-free continual learning, certain expansion of the model is necessary for each new task. straightforward approach would be to add new adapters to all expandable layers. However, this limits knowledge sharing between the tasks and leads to an excessive linear increase in the number of adapter parameters. Therefore, we only expand layer ℓ at stage if the features of the new task Tn deviate substantially from all previous tasks. Since the discriminators are trained on different data, comparing their reconstruction losses requires normalization. To this end, we maintain the running mean µj ℓ and standard deviation σj ℓ of the reconstruction loss for each Hyperparameter Adapters Discriminators # Parameters (FFN) # Parameters (Proj.) Learning rate Learning rate schedule Batch size Training steps Expansion threshold γ 0.26M 3.2M 1 104 cosine 32 20,000 2.5 0.33M 1.4M 5 104 constant 32 2,000 - TABLE I: Model and training hyperparameters. The injected modules are much smaller than the base models, which have about 200M parameters. discriminator Dj ℓ and calculate the normalized z-scores ℓ(xℓ) µj ej σj ℓ 1 Dn (cid:88) . ℓ zj ℓ (xℓ) = (7) ℓ xℓDn ℓ , . . . , Dn1 If all discriminators D1 have z-score (7) larger than threshold γ, the features xℓ of the new task Tn in layer ℓ are out-of-distribution with respect to all previously learned tasks. In this case, we expand the layer by new adapter Akℓ to ℓ it, i.e., Bℓ(Dn ℓ . On the contrary, if at least one discriminator Dj ℓ yields z-score smaller than the specified threshold γ, the layer ℓ is deemed not to require expansion. This dynamic expansion strategy, illustrated in Figure 2, results in memory-efficient, sub-linear increase in the number of adapter parameters. and link the new discriminator Dn ℓ ℓ ) = Akℓ Even if layer is not expanded, we still need to attach new discriminator to it. To explain why this is necessary, we consider the following scenario: Assume that at stage n, new adapter Ai is added to layer ℓ2, but the shallower ℓ2 layer ℓ1 is not expanded. Then, the routing mechanism activates only adapters from earlier stages in layer ℓ1 during . However, new adapter Aj training of Ai could be added ℓ2 ℓ1 to layer ℓ1 at the next stage n+1. In this case, when revisiting task n, the router might activate the new adapter Aj instead ℓ1 of an earlier one. As consequence, the input features xℓ2 to layer ℓ2 when performing task Tn are different from those seen during training of Ai . This distribution shift in ℓ2 feature space can lead to unpredictable behavior and task failure [39], [40]. To avoid this problem and ensure consistent routing behavior, we attach an auxiliary discriminator if layer is not expanded, as visualized in Figure 2. The auxiliary discriminator is linked to the same adapter as the existing discriminator with the smallest reconstruction loss (5), i.e., ℓ = Bℓ(Dj ℓ ), ExℓDn ℓ ) = Ai = arg min (cid:105) ej ℓ(xℓ) Bℓ(Dn where (8b) (8a) (cid:104) . j{1,...,n1} Fig. 3: Architecture of our pretrained diffusion transformer (DiT) base policy. We investigate two variants of observation encoding and generative modeling: DiT-EncDec employs self-attention transformer encoder and denoising diffusion objective, while DiT-Dec performs linear projection of the concatenated input tokens and uses flow matching objective. The potential locations for inserting CLARE adapters are shown as dashed blocks. Our experiments indicate that adding adapters in the encoder module yields the best performance. layers typically exhibit stronger distribution shift between tasks than deeper layers. Thus, if no layer is deemed to require expansion, we still add an adapter to the shallowest layer ℓ1 to capture the peculiarities of new task. At the first stage, we expand all layers ℓ by default. In summary, our dynamic expansion mechanism ensures that CLARE adds only small, task-dependent number of parameters without compromising performance when revisiting previous tasks. V. EVALUATION We conduct extensive simulation experiments with the primary goal of answering the following research questions: Q1: How well can CLARE learn new tasks, and is the performance on previous tasks affected? Q2: Can our autonomous dynamic expansion strategy reuse relevant skills from previous tasks? Q3: Which layers are best suited for expansion? Intuitively, since adapter (8a) was trained on features most similar to task Tn, we consider its learned representations to be transferable to Tn. We note that with our dynamic expansion strategy, an adapter can potentially be activated by more than one discriminator, as shown in Figures 1 and 2. We found that introducing at least some new parameters for each new task is essential for the policy to acquire and retain novel skills. In addition, we observed that shallower A. Experimental Setup 1) Tasks: We conduct our experiments using the LIBERO benchmark [41], which is designed specifically for continual learning. Hereby, Franka robotic manipulator with parallel yaw gripper needs to perform tasks in kitchen environment, and 50 human expert demonstrations are available per task. We pre-train the policy on 90 short-horizon tasks from LIBERO-90 and evaluate continual learning performance on 10 sequentially arriving long-horizon tasks from LIBERO10, which require the robot to understand language instructions and execute different types of motion, such as pickand-place, opening drawer, or turning knob. 2) Policy: The observations contain RGB images from wrist-mounted and third-person camera, the robots endeffector pose and gripper state, and language command. The policy generates action chunks of length = 16 and applies = 8 actions to the robot at control frequency of 20 Hz before replanning. We adopt diffusion transformer (DiT) [42] architecture with adaptive layer normalization (adaLN) [43] conditioning for our pre-trained base policy, as visualized in Figure 3. We leverage pretrained DINOv2 [44] and CLIP [45] models as vision and text encoders, respectively, and keep their parameters frozen during continual learning. The visual and language features, as well as the proprioceptive state, are first projected into tokens of the same dimension through linear layers before they are fed into the transformer backbone. We consider two different DiT variants, which are illustrated in Figure 3. DiTEncDec has an encoder-decoder backbone [42], and adapters can be added to all transformer layers in the encoder and the decoder. DiT-Dec uses decoder-only backbone [42], and adapters can be added to the linear projection layer of the encoder and the transformer layers in the decoder. To evaluate CLARE for different generative modeling techniques, we train DiT-Dec with the flow matching loss (1) and DiTEncDec with denoising diffusion objective [19], [32]. Our training hyperparameters are provided in Table I. Both VLA variants have approximately 200M base parameters. 3) Metrics: We use three metrics to measure performance [41], [46]: Area under the success rate curve (AUC), forward transfer (FWT), and negative backward transfer (NBT). Denoting the success rate on task after learning the first tasks as rnm, the metrics are defined as AUC = FWT ="
        },
        {
            "title": "1\nN",
            "content": "n=1 (cid:88) n=1 NBT ="
        },
        {
            "title": "1\nN − 1",
            "content": "(cid:32) (cid:88)"
        },
        {
            "title": "1\nN − n + 1",
            "content": "N (cid:88) m=n (cid:33) rnm , rnn, (cid:32) 1 (cid:88) n="
        },
        {
            "title": "1\nN − n",
            "content": "N (cid:88) m=n+1 (cid:0)rnn rnm (cid:1) (cid:33) . Intuitively, AUC measures overall performance on new and old tasks, FWT quantifies the ability to learn new tasks, and NBT measures the degree of forgetting (lower being better). All numerical results are given in percentage points. 4) Evaluation Protocol: After each continual learning stage n, we evaluate the policy on all previously learned tasks T1, . . . , Tn using 100 rollouts per task. The evaluations are conducted over 50 distinct initial configurations of relevant objects, with each configuration used twice. We average all results across three random seeds. 5) Baselines: We compare our method with five baselines for continual learning without oracle task identifiers: Backbone Expand. layers AUC FWT NBT DiT-EncDec DiT-Dec Encoder Decoder Enc. & Dec. Lin. projection Decoder 65.382.68 28.992.20 66.600.29 75.111.31 41.752.42 66.532.18 30.874.25 65.770.41 75.031.42 45.473.77 1.701.20 2.953.41 1.500. 1.850.38 7.021.65 TABLE II: Ablation study for the choice of expandable layers. Adding adapters to the observation encoder module yields the best results. Sequential Fully Fine-tuning (SeqFFT) [41], [47] treats all model parameters as trainable and sequentially fine-tunes the whole policy for each new task. Sequential Low-Rank Adaptation (SeqLoRA) [28] adds task-specific LoRA adapters to selected layers. After training the adapters on the new data, they are merged back into the base model weights. We add adapters to all linear layers within the attention, feedforward, and adaLN modules, and, for DiT-Dec, additionally into the linear projection layer. PackNet [26] freezes the most important 25% of model weights, and the remaining free weights are used to learn the next task. This process is repeated iteratively, with each new task having fewer free weights available. Experience Replay (ER) [12] is popular method that stores previous data. For each new task, the model is trained on mix of 50% previous and 50% new data. LOTUS [29] is hierarchical method that constructs skill library, which is updated and expanded when learning new tasks. learned meta-policy flexibly composes these skills during deployment. Note that while CLARE is designed not to rely on previous data, as it may be unavailable, we allow ER and LOTUS to access previous data in our experiments to match the setup in prior works. This privileged information gives these two baselines an inherent advantage. B. Results We first examine key design choice of CLARE; the set of layers to expand E. To this end, we set γ = 0, such that new adapter is added to each expandable layer to learn new task. The results provided in Table II demonstrate that adding adapters only to the encoder part of the model vastly outperforms expanding the decoder, achieving 30 to 40% higher absolute AUC and FWT for both backbones. Notably, expanding all transformer layers for DiT-EncDec does not improve performance compared to expanding only the encoder. These results demonstrate that the policys encoder module, whether implemented as transformer or linear projection, is well-suited to store task-specific knowledge in continual this approach in all subsequent experiments. learning with VLAs. Hence, we adopt summary of our baseline comparison is provided in Table III. Our method achieves the highest overall performance, as measured by AUC, with both backbones, outperforming the best baseline, ER, by approximately 11% and 15% in absolute terms, respectively. Compared to SeqFFT and ER, Fig. 4: Success rate curves of CLARE and five baselines on the LIBERO-Long benchmark. The solid lines represent the average success rates across three random seeds, and the shaded regions indicate the standard deviations. The results demonstrate that our method achieves higher overall success rate and more effectively mitigates catastrophic forgetting during continual learning compared to the baselines, despite ER and LOTUS using previous data. Backbone Method AUC FWT NBT DiT-EncDec DiT-Dec SeqFFT SeqLoRA PackNet [26] ER [12] CLARE (ours) SeqFFT SeqLoRA PackNet [26] ER [12] CLARE (ours) 21.000.49 16.260.72 20.910.39 55.871.47 66.711.29 22.370.27 21.371.03 4.840.24 60.540.21 75.111.31 71.131.97 55.000.54 73.771.31 67.671.65 66.071.32 76.130.97 73.101.77 37.201.04 76.600.94 75.031. 70.332.21 53.080.21 73.741.42 15.790.48 -0.800.76 74.701.05 71.641.60 41.341.16 22.741.82 1.850.38 LOTUS [29] 52.931.57 58.120.24 -7.162. TABLE III: Baseline comparison. CLARE achieves the highest overall performance, as measured by AUC, and demonstrates strong capabilities to acquire new skills without forgetting. which fine-tune the full model, CLARE achieves comparable FWT, indicating that it can store new task-specific knowledge in much smaller number of learnable parameters. Moreover, our method achieves very low NBT close to zero, demonstrating that it can avoid forgetting without relying on exemplar data or oracle task identifiers. While their this requires adding new parameters to the model, relative increase with respect to the base policy is only 1.7% and 2.3%, respectively, on average per task. To better understand the continual learning behavior, we provide the task success rates of DiT-Dec for all stages in Figure 4. These results further demonstrate that CLARE does not exhibit catastrophic forgetting in contrast to most baselines, while also achieving higher overall success rate across most tasks. Interestingly, for LOTUS, performance on some tasks improves during later stages, which is also indicated by the negative NBT value reported in Table III. We attribute this behavior to the iterative use of ER. We also evaluate the impact of the expansion threshold γ, Fig. 5: Ablation study for the dynamic expansion threshold γ. Increasing γ significantly reduces the number of adapters added to the model but slightly reduces the capability to learn new tasks, as shown by the small decrease in AUC and FWT. In contrast, NBT remains at around zero, indicating that the model does not exhibit catastrophic forgetting. using DiT-EncDec with expandable encoder layers, and provide the results in Figure 5. Increasing γ from 0 to 20 reduces the number of new adapters added to the model from 60 to 16, i.e., almost by factor of 4. At the same time, we observe moderate decrease in AUC (from 65% to 57%) and FWT (from 67% to 57%). Intuitively, compressing new knowledge into fewer model parameters reduces the robots ability to learn novel tasks. However, we also note that AUC remains higher than for ER, the best-performing baseline, and NBT stays close to zero, indicating that the policy does not exhibit forgetting even when only small number of adapters is added per task. Comparing the results in Table II and Figure 5, we find that the choice of expandable layers has significantly larger impact than the expansion threshold value. In summary, adapters should be added to the encoder part, and γ can be chosen according to the importance of high task performance compared to low memory requirements. VI. CONCLUSIONS requiring neither stored exemplars nor Our proposed framework, CLARE, enables pre-trained VLAs to continually learn new tasks without catastrophic forgetting, task identifiers. By combining lightweight adapters, featuresimilarity-driven expansion strategy, and an autoencoderbased routing module, CLARE increases model capacity only when needed while maintaining previously learned representations. Our experiments demonstrate that our method is parameter-efficient and able to achieve and retain high task performance, significantly outperforming even strong baselines that have access to previous data. The modular design of CLARE is also compatible with emerging largescale VLAs [6], [7], and we consider evaluation on such models and on hardware to be natural steps for further work."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "Ralf Romer gratefully acknowledges the support of the research group ConVeY funded by the German Research Foundation under grant GRK 2428. This work has been partially supported by the German Federal Ministry of Research, Technology and Space (BMFTR) under the Robotics Institute Germany (RIG)."
        },
        {
            "title": "REFERENCES",
            "content": "[1] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, Continual lifelong learning with neural networks: review, Neural Networks, pp. 5471, 2019. [2] S. Thrun and T. M. Mitchell, Lifelong robot learning, Robotics and Autonomous systems, pp. 2546, 1995. [3] A. Billard et al., roadmap for AI in robotics, Nature Machine Intelligence, 2025. [4] S. Dohare, J. F. Hernandez-Garcia, Q. Lan, P. Rahman, A. R. Mahmood, and R. S. Sutton, Loss of plasticity in deep continual learning, Nature, pp. 768774, 2024. [5] M. J. Kim et al., OpenVLA: An open-source vision-language-action model, in Conference on Robot Learning, 2025, pp. 26792713. [6] P. Intelligence et al., π0.5: vision-language-action model with openworld generalization, arXiv preprint arXiv:2504.16054, 2025. [7] M. Reuss, H. Zhou, M. Ruhle, O. E. Yagmurlu, F. Otto, and R. Lioutikov, Flower: Democratizing generalist robot policies with efficient vision-language-action flow policies, in Conference on Robot Learning (CoRL), 2025. [8] M. Shukor et al., SmolVLA: vision-language-action model for affordable and efficient robotics, arXiv preprint arXiv:2506.01844, 2025. [9] A. ONeill et al., Open X-embodiment: Robotic learning datasets and RT-X models, in International Conference on Robotics and Automation (ICRA), 2024, pp. 68926903. [10] R. M. French, Catastrophic forgetting in connectionist networks, Trends in Cognitive Sciences, pp. 128135, 1999. [11] D. Lopez-Paz and M. Ranzato, Gradient episodic memory for continual learning, Advances in Neural Information Processing Systems (NeurIPS), 2017. [12] A. Chaudhry et al., On tiny episodic memories in continual learning, arXiv preprint arXiv:1902.10486, 2019. [13] A. Xie and C. Finn, Lifelong robotic reinforcement learning by retaining experiences, in Conference on Lifelong Learning Agents. PMLR, 2022, pp. 838855. [14] Z. Liu et al., Tail: Task-specific adapters for imitation learning with large pretrained models, arXiv preprint arXiv:2310.05905, 2023. [15] H. Wang, H. Lu, L. Yao, and D. Gong, Self-expansion of pretrained models with mixture of adapters for continual learning, in Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), 2025, pp. 10 08710 098. [16] Y. Wang et al., Sparse diffusion policy: sparse, reusable, and flexible policy for robot learning, in Conference on Robot Learning (CoRL), 2024. [17] J. Kaplan et al., Scaling laws for neural language models, arXiv preprint arXiv:2001.08361, 2020. [18] J. Achiam et al., GPT-4 Technical report, arXiv preprint arXiv:2303.08774, 2023. [19] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in Neural Information Processing Systems, pp. 68406851, 2020. [20] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, Flow matching for generative modeling, in International Conference on Learning Representations (ICLR), 2023. [21] X. Chen et al., Pali-3 vision language models: Smaller, faster, stronger, arXiv preprint arXiv:2310.09199, 2023. [22] F. Lin, Y. Hu, P. Sheng, C. Wen, J. You, and Y. Gao, Data scaling laws in imitation learning for robotic manipulation, in International Conference on Learning Representations (ICLR), 2025. [23] D.-W. Zhou et al., Learning without forgetting for vision-language models, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [24] J. Kirkpatrick et al., Overcoming catastrophic forgetting in neural networks, Proceedings of the National Academy of Sciences, pp. 35213526, 2017. [25] F. Zenke, B. Poole, and S. Ganguli, Continual learning through synaptic intelligence, in International Conference on Machine Learning (ICML), 2017, pp. 39873995. [26] A. Mallya and S. Lazebnik, Packnet: Adding multiple tasks to single network by iterative pruning, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 77657773. [27] A. A. Rusu et al., Progressive neural networks, arXiv preprint arXiv:1606.04671, 2016. [28] E. J. Hu et al., LoRA: Low-rank adaptation of large language models. in International Conference on Learning Representations (ICLR), 2022. [29] W. Wan, Y. Zhu, R. Shah, and Y. Zhu, LOTUS: Continual imitation learning for robot manipulation through unsupervised skill discovery, in IEEE International Conference on Robotics and Automation (ICRA), 2024, pp. 537544. [30] Y. Zhu, P. Stone, and Y. Zhu, Bottom-up skill discovery from unsegmented demonstrations for long-horizon robot manipulation, IEEE Robotics and Automation Letters, pp. 41264133, 2022. [31] L. Xie, Y. Li, and H. Zhuang, Analytic task scheduler: Recursive least squares based method for continual learning in embodied foundation models, arXiv preprint arXiv:2506.09623, 2025. [32] C. Chi et al., Diffusion policy: Visuomotor policy learning via action diffusion, in Robotics: Science and Systems (RSS), 2023. [33] T. Zhao, V. Kumar, S. Levine, and C. Finn, Learning fine-grained bimanual manipulation with low-cost hardware, in Robotics: Science and Systems (RSS), 2023. [34] J. Urain et al., Deep generative models in robotics: survey on learning from multimodal demonstrations, arXiv preprint arXiv:2408.04380, 2024. [35] N. Shazeer et al., Outrageously large neural networks: The sparselygated mixture-of-experts layer, in International Conference on Learning Representations (ICLR), 2017. [36] D. Dai et al., DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models, in Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024, pp. 12801297. [37] K. Meng, D. Bau, A. Andonian, and Y. Belinkov, Locating and editing factual associations in gpt, Advances in Neural Information Processing Systems, pp. 17 35917 372, 2022. [38] M. Geva, R. Schuster, J. Berant, and O. Levy, Transformer feedforward layers are key-value memories, in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2021, pp. 54845495. [39] Q. Gu et al., Safe: Multitask failure detection for vision-languageaction models, Advances in Neural Information Processing Systems (NeurIPS), 2025. [40] R. Romer, A. Kobras, L. Worbis, and A. P. Schoellig, Failure prediction at runtime for generative robot policies, Advances in Neural Information Processing Systems (NeurIPS), 2025. [41] B. Liu et al., LIBERO: Benchmarking knowledge transfer for lifelong robot learning, Advances in Neural Information Processing Systems (NeurIPS), pp. 44 77644 791, 2023. [42] S. Dasari, O. Mees, S. Zhao, M. K. Srirama, and S. Levine, The ingredients for robotic diffusion transformers, in Proceedings of the International Conference on Robotics and Automation (ICRA), 2025. [43] W. Peebles and S. Xie, Scalable diffusion models with transformers, in Proceedings of the IEEE/CVF international Conference on computer vision, 2023, pp. 41954205. [44] M. Oquab et al., DINOv2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023. [45] A. Radford et al., Learning transferable visual models from natural language supervision, in International Conference on machine learning. PMLR, 2021, pp. 87488763. [46] N. Dıaz-Rodrıguez et al., Dont forget, there is more than forgetting: new metrics for continual learning, in Continual Learning Workshop at NeurIPS 2018, 2018, pp. 17. [47] L. Ouyang et al., Training language models to follow instructions with human feedback, Advances in Neural Information Processing Systems, pp. 27 73027 744, 2022."
        }
    ],
    "affiliations": [
        "Technical University of Munich"
    ]
}