{
    "paper_title": "Learning to Configure Agentic AI Systems",
    "authors": [
        "Aditya Taparia",
        "Som Sagar",
        "Ransalu Senanayake"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from a large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as a query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns a light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is a powerful alternative to \"one size fits all\" designs."
        },
        {
            "title": "Start",
            "content": "Aditya Taparia * 1 Som Sagar * 1 Ransalu Senanayake 1 6 2 0 2 2 1 ] . [ 1 4 7 5 1 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Configuring LLM-based agent systems involves choosing workflows, tools, token budgets, and prompts from large combinatorial design space, and is typically handled today by fixed large templates or hand-tuned heuristics. This leads to brittle behavior and unnecessary compute, since the same cumbersome configuration is often applied to both easy and hard input queries. We formulate agent configuration as query-wise decision problem and introduce ARC (Agentic Resource & Configuration learner), which learns light-weight hierarchical policy using reinforcement learning to dynamically tailor these configurations. Across multiple benchmarks spanning reasoning and tool-augmented question answering, the learned policy consistently outperforms strong hand-designed and other baselines, achieving up to 25% higher task accuracy while also reducing token and runtime costs. These results demonstrate that learning per-query agent configurations is powerful alternative to one size fits all designs. Codebase: Github 1. Introduction Large Language Models (LLMs) have evolved from simple single LLM predictors to the backbone of complex multiagent systems capable of iterative planning, tool usage, and multi-step reasoning. In this new paradigm, the performance of an agentic system is governed not only by the underlying LLM but also by the architecture that wraps it: workflows (e.g. voters, verifiers, optimizers), tool availability, information routing, and context management. Current approaches to agentic architecture design largely rely on static heuristics or kitchen sink strategies that flood the context window with extensive history and retrieved evidence. However, this is suboptimal for two reasons. First, *Equal contribution 1School of Computing and Augmented Intelligence, Arizona State University, Tempe, United States of America. Correspondence to: Aditya Taparia <ataparia@asu.edu>, Som Sagar <ssagar6@asu.edu>. Figure 1. (a) Shows how our method learns to configure optimal configuration across thousands of possibilities for the given input. (b) Shows improvement by our method over multiple datasets. (These results are for Qwen 2.5 7B Instruct model.) performance degrades in long contexts due to the lost-inthe-middle phenomenon, where models fail to attend to relevant information (Liu et al., 2024; Hong et al., 2025). Second, static systems are inefficient; they fail to adapt their computational footprint to task difficulty. For example, an agentic system designed for multi-hop reasoning might trigger expensive web-search tools and iterative verification loops even for basic arithmetic query. This one-sizefits-all approach leads to wasted compute and unnecessary latency by applying the same heavy resources to trivial and complex inputs. Ideally, an agent system should adapt its configuration to each query at hand. While intuitive, learning such dynamic configurations is difficult because the design space is combinatorial. For instance, consider system that must answer questions of varying difficulty: Should it invoke single-shot Chain-of-Thought? Deploy parallel voters to 1 Learning to Configure Agentic AI Systems cross-check answers? Allocate generous token budget, or conserve compute with minimal response? Even simple 3-agent system with 5 workflow patterns, 3 tools (each independently enabled) for the first two agents, and 3 budget tiers per agent yields 5 82 33 8,600 structural configurations and adding prompt selection quickly pushes this beyond 105 configurations, rendering brute-force search intractable and manual tuning impractical. To address this, we cast agent configuration as query-wise decision-making problem. To tackle the large search space, we formulate this as reinforcement learning (RL) setup, where lightweight controller on top of frozen agent system jointly optimizes workflows and context management while trading off task accuracy against computational cost. Our contributions are as follows: 1. We introduce ARC, hierarchical RL framework where high-level policy selects workflows and tools while low-level policy composes prompt instructions. Importantly, this policy learning operates over lightweight networks, without updating the backbone LLM, making the method computationally feasible. 2. We propose hybrid training pipeline that combines masked RL with SFT on elite trajectories to to stabilize learning under sparse rewards. 3. We demonstrate through extensive experiments across reasoning and tool-use benchmarks that our method achieves significant gains over static, flat RL-policies and configuration optimization baselines (see Figure 1). 2. Related Work We situate our work at the intersection of three research threads: LLM-based agents, prompt and workflow optimization, and hierarchical reinforcement learning. Large Language Agents LLM-based agents extend language models with iterative decision-making, tool use, and multi-step interaction (Yao et al., 2022; Schick et al., 2023). Recent frameworks provide abstractions for tool calling and multi-agent coordination (Chase, 2022; Wu et al., 2024), while evaluation suites stress realistic long-horizon behavior across general agent benchmarks and web-interaction settings (Liu et al., 2023; Zhou et al., 2023). Tool-centric datasets further broaden the space of available actions and APIs (Qin et al., 2023), and agents have been instantiated in software engineering interfaces where actions include editing code and navigating repositories (Yang et al., 2024b). Despite this progress, most systems rely on manually specified or heuristically tuned workflows with fixed sequencing of reasoning, retrieval, and tool calls. In contrast, our work treats agent configuration as learnable decision problem, enabling query-wise adaptive workflow and budget selection under resource constraints. Prompt and Workflow Optimization Early work on adapting LLM behavior focused on the input prompt optimization. Automated frameworks such as OPRO (Yang et al., 2023), DSPy (Khattab et al., 2023), and GEPA (Agrawal et al., 2025) treat the prompt as an optimization variable, iteratively searching for improved instructions. More recently, the optimization target has shifted from static prompts to agentic workflows and context management. Methods like LLMLingua (Jiang et al., 2023) and LongLLMLingua (Jiang et al., 2024) compress input prompts to fit context windows, while cost-aware frameworks have evolved from model routing (Chen et al., 2023) to token-budget-aware reasoning (Han et al., 2025). Our work unifies these directions by learning hierarchical policy that jointly optimizes workflow structure and context budget end-to-end. Hierarchical Planning and Control While LLM agents are typically viewed through the lens of prompt engineering, our work connects to the rich literature on Hierarchical Reinforcement Learning (HRL). Classic frameworks such as the Options framework (Sutton et al., 1999) and Feudal RL (Dayan & Hinton, 1993) decompose complex tasks into high-level goals and low-level actions, enabling more efficient exploration in large state-action spaces. Recent work has applied HRL to language model fine-tuning and multistep reasoning (Pang et al., 2024), though typically within fixed architectural constraints. We adapt this paradigm by treating workflow selection as the high-level option and the selection and ordering of prompt components as the low-level sub-policy, allowing structured search over the combinatorial configuration space that flat RL approaches cannot efficiently navigate. 3. Methodology We aim to learn policy π (ARC) that adapts the agent configuration to each input query, balancing correctness against computational cost. Formally, let = {(qi, ai)}N i=1 be dataset of queries qi and ground truth answers ai. For each query q, π selects configuration = (ω, t, b, p), where ω is workflow pattern, encodes the enabled tools, denotes token-budget tiers, and specifies prompt instructions for the agents. Executing the configured system on produces agentic response ˆa = π(q; c), correctness signal I, and cost statistics (e.g., token usage, number of steps)."
        },
        {
            "title": "We define a utility for configuration c on query q as",
            "content": "U (q, c) = I[ˆa = a] λ Ccost(c), (1) where Ccost(c) aggregates over computational overhead (e.g., token usage and runtime) and λ 0 controls the accuracy efficiency trade-off. Our goal is to learn policy π that (cid:2)U (cid:0)q, π(q)(cid:1)(cid:3) . maximizes the expected utility maxπ EqD In practice, we optimize shaped reward that decomposes into interpretable terms (See Section 3.2). To make this 2 Learning to Configure Agentic AI Systems Figure 2. Training pipeline. The structure policy selects workflows, tools, and budgets while the prompt policy composes instructions. During RL training, episodes are stored in memory buffer. After RL converges, high-reward episodes are filtered and used for supervised fine-tuning (SFT), which consolidates successful strategies and improves consistency. optimization tractable, we decompose π into two levels in manner that is particularly well-suited to practical implementation with LLMs: Structure policy πstruct that decides what to deploy (workflow, tools, and budgets). Prompt policy πprompt that decides how to instruct each individual agent. Crucially, this hierarchy replaces single joint decision with sequential structural and prompt decisions, which often improves optimization and sample efficiency in structured settings. We therefore report an effective reduction in search complexity, rather than universal worst-case change from O(AstructAprompt) to O(Astruct+Aprompt). Action masking (see Section 3.2) further prunes invalid branches during training. 3.1. Hierarchical Policy Architecture We model the configuration search as short episodic Markov decision process (MDP) = (S, A, P, r, γ) in which each episode corresponds to configuring and executing the agent system for single query q. Here, tors for multi-step reasoning and tool use: sq = [ϕ(q); fq]. We implement ϕ() using MetaCLIP-H/14 (Xu et al., 2024), which we found to provide robust task-agnostic representations for structural decisions (see Appendix D). The state sq remains fixed throughout the configuration episode, and all decisions condition on this representation. Action Space (A): The action space is hierarchical. The high-level action astruct selects the workflow and resources. The low-level actions aprompt sequentially select system prompts for each individual agent. Transition Dynamics (P ): The configuration episode unfolds through an ordered sequence of decision stages: the agent first selects the workflow and resources, followed by sequence of prompt selection actions. After the configuration phase, the environment transitions to terminal state where the configured workflow executes, and reward is observed. Reward Function (R): scalar signal is received at the end of the episode, combining task correctness with penalties for step count and token usage (Section 3.2). . Discount Factor (γ): We use γ 1 as the task is episodic with delayed sparse rewards. State Space (S): For each query q, we construct an embedding sq by concatenating semantic representation ϕ(q) with feature vector fq encoding query length, word count, numerical density, and binary indica3.1.1. STRUCTURE POLICY The structure policy πstruct is learned high-level policy that makes single joint decision per episode to establish the agents architectural blueprint based on the query state sq. 3 Learning to Configure Agentic AI Systems Action Space. Outputs of πstruct is composite action astruct = (ω, t, b), where ω denotes the selected workflow, the set of enabled tools, and the allocated budget tiers for each agent. For instance, in our experiments, we consider 9 workflow patterns, 4 tools per agent (yielding 24 possible tool subsets), and 3 budget tiers per agent. Workflows are drawn from agentic patterns (Anthropic, 2024), spanning sequential (Chain-of-Thought), parallel (Voting), direct, and iterative (Evaluator-Optimizer) topologies. 3.1.2. PROMPT POLICY The prompt policy πprompt operates as sequential decisionmaking process, conditioned on the output of the πstruct . Its role is to operationalize the high-level architectural decision by defining the instructions for each agent. Action Space. The prompt policy πprompt operates over compositional prompt space Aprompt = {STOP}, where denotes library of semantic instruction components and STOP is termination action indicating completion of prompt composition. These prompts represent semantic instruction fragments (e.g., Decompose the problem, Verify intermediate steps). To ensure high-quality instructions, the prompt space is constructed by augmenting base strategies with dataset-specific instructions generated via meta-prompting. Our ablations (See Section E) demonstrate that instruction generation using GPT-5.2 outperforms static hand-crafted prompts and other large models. 3.2. Training Procedure We optimize both policies end-to-end using Proximal Policy Optimization (PPO) (Schulman et al., 2017) as seen in Figure 2. Each policy maintains separate value network for advantage estimation, and we apply per-batch advantage normalization to stabilize learning across the heterogeneous reward landscape. The clipped surrogate objective prevents destructive updates: LPPO(θ) = (cid:104) min (cid:16) ρt ˆAt, clip(ρt, 1ϵ, 1+ϵ) ˆAt (cid:17)(cid:105) , (2) where ρt = πθ(atst)/πθold (atst) and ˆAt is the estimated advantage. We add entropy regularization to encourage exploration in the early stages of training. Reward Design. Designing rewards for agentic systems is non-trivial: optimizing correctness alone leads to overprovisioning (allocating maximum resources), while pure efficiency metrics sacrifice accuracy. We decompose the reward into three interpretable terms: ntokens Tmax (cid:125) (cid:123)(cid:122) efficiency penalties where I[correct] is the binary correctness indicator, nsteps counts reasoning iterations, ntokens is total token conα I[correct] (cid:123)(cid:122) (cid:125) (cid:124) task success + η Rtool (cid:124) (cid:123)(cid:122) (cid:125) tool shaping βs nsteps βt (3) (cid:124) , Algorithm 1 Training Pipeline (HRL SFT) Require: Dataset D, policies πstruct, πprompt, buffer = 1: Initialize policies and value networks Vstruct, Vprompt 2: for = 1 to TRL do 3: 4: for each do sample astruct Encode sq = [ϕ(q); fq]; πstruct(sq) Apply mask based on workflow ω Compose prompts via πprompt execute workflow = (ω, t, b, p) Compute reward r; store (sq, astruct, p, r) in end for Compute advantages ˆAt; normalize per batch for = 1 to do Update θ via PPO clipped objective with value loss and entropy regularization 5: 6: 7: 8: 9: 10: 11: 12: end for 13: 14: end for 15: Extract elite: Delite = {(si, i ) : correcti ri τ } {Top 30%} 16: for = 1 to ESFT do 17: 18: end for 19: return πSFT struct, πSFT prompt Update θ via SFT: max E(s,a)Delite[log πθ(as)] sumption normalized by the maximum budget Tmax, and α, βs, βt, η are weighting coefficients. The first term incentivizes correct answers; the second penalizes computational overhead (both latency via steps and cost via tokens); the third shapes tool utilization. The tool shaping term Rtool addresses key challenge: the structure policy allocates tools, but the downstream LLM decides whether to invoke them. Naively rewarding tool allocation creates mismatch tools may be provisioned but never used. We design an asymmetric reward: Rtool = (cid:40) +δ1nused + δ2I[correct] nused > 0 δ3nalloc nused = 0, nalloc > 0 (4) where nused is the number of tools invoked during execution, nalloc is the number allocated by the structure policy, and δ1, δ2, δ3 > 0 are shaping coefficients. When tools are used (nused > 0), we reward each invocation (δ1) with bonus if the answer is correct (δ2), reinforcing tool-assisted success. When tools are allocated but unused (nalloc > 0, nused = 0), we penalize proportionally (δ3), discouraging wasteful provisioning. This asymmetry teaches the policy to allocate tools precisely when the LLM will invoke them. Action Masking. The action space of the structure policy πstruct (Astruct = 9 162 33 = 62,208) contains structurally invalid configurations. For example, when the selected workflow is ω = DIRECT, the workflow employs 4 Learning to Configure Agentic AI Systems single reasoning agent, yet Astruct permits allocation of tools and budgets to an additional agent dimension. To avoid such wasteful exploration of such infeasible configurations, we employ action masking to prune invalid action combinations, reducing Astruct to 41,904 valid configurations (a 32.6% reduction, Figure 3). Figure 3. reduces sequence within the RL policy. Action masking the effective actionDuring action sampling, once πstruct selects workflow ω, we apply conditional mask that excludes incompatible action choices by setting their logits to prior to the softmax operation. Formally, for action dimension with raw logits zi, we apply binary mask mi(ω) {0, 1}Ai conditioned on the selected workflow, yielding masked logits zi = zi + log(cid:0)mi(ω)(cid:1), where log(0) = ensuring the masked actions receive zero probability mass after normalization. When computing log probabilities during policy updates, The same mask is applied to the policys logits to constrain the πθ(a s) to the identical set of valid actions used during sampling. This guarantees that πθ(a s) normalizes only over feasible configurations, ensuring that the probability ratio ρt in Eq. 2 is computed with respect to consistent, valid action distributions. 3.3. Post-Training Refinement While RL effectively explores the combinatorial design space, the high variance gradient estimates can leave the final policy with residual stochasticity. To address this, we introduce Supervised Fine-Tuning (SFT) refinement phase that runs after RL training completes. Critically, SFT is computationally inexpensive: it fine-tunes only the lightweight policy networks (not the LLM) via standard supervised learning on subset of the RL buffer, avoiding the expensive LLM rollouts needed during RL training. Concretely, SFT distills the most successful configurations encountered during RL by training the policy to imitate highquality decisions from its own experience. We therefore filter the RL experience buffer to obtain elite trajectories that are both correct and exceed reward threshold: formal performance guarantees (Theorem 3.2). 3.4. Theoretical Guarantees The SFT phase provides formal guarantees on the final policy. We first establish that MLE recovers the empirical distribution, then derive performance bounds. Lemma 3.1 (MLE Convergence). Let ˆpelite(as) = n(s, a)/n(s) denote the empirical distribution over Delite. Under sufficient model capacity, the objective in Eq. 6 is minimized when πθ = ˆpelite. Proof sketch. The MLE objective is equivalent to minimizing DKL(ˆpeliteπθ), which achieves its minimum of zero when πθ = ˆpelite. Theorem 3.2 (Policy Concentration). Let Aelite(s) = {a : (s, a) Delite}. Under Lemma 3.1, the refined policy πSFT satisfies: 1. (Support restriction) πSFT(as) > 0 = Aelite(s). 2. (Reward guarantee) EaπSFT[r(s, a)] τ . Proof sketch. (Support restriction) By Lemma 3.1, πSFT = ˆpelite. Since ˆpelite(as) = 0 for / Aelite(s), the support is restricted. (Reward guarantee) By Eq. 5, (s, a) Delite r(s, a) τ . Combined with support restriction: = EaπSFT[r(s, a)] = (cid:88) aAelite(s) πSFT(as) r(s, a) τ. RL training optimizes the policy toward high-reward configurations; the elite buffer Delite captures the top-performing trajectories from this trained policy. Theorem 3.2 then guarantees that SFT concentrates πSFT on these successful configurations, establishing performance floor at threshold τ . We validate this empirically in Section 4.5, where SFT improves average reward by 5 35% across datasets. 4. Experiments Our experiments investigate three key aspects of the proposed framework ARC: (RQ1) Does the learned configuration outperform fixed architectures? (RQ2) Does adaptive allocation improve efficiency? (RQ3) Can these learned configuration be transferred across task and parameters? Delite = {(si, ) : correcti r(si, ) τ }, (5) 4.1. Experimental Setup where τ is set to retain the top performing episodes (empirically, the top 30% by reward). We fine-tune both the structure and prompt policies separately via maximum likelihood estimation on the elite demonstrations: E(s,a)Delite [log πθ(as)] . (6) max θ This distillation process stabilizes the policy by restricting it to configurations that succeeded during training, providing 5 Benchmarks. We evaluate on five benchmarks spanning across two primary capability axes: Reasoning Capability, which includes GSM8k (Cobbe et al., 2021), DROP (Dua et al., 2019), and MedQA (Jin et al., 2021); and Tool-Use Capability, comprising HotPotQA (Yang et al., 2018) and GAIA (Mialon et al., 2023). We employ standard training split for policy learning and evaluate on the test (or validation) set. For GAIA, we partitioned the test set, using Learning to Configure Agentic AI Systems Table 1. Performance comparison across reasoning and tool-use benchmarks. We compare our approach (ARC and ARC w/o SFT) against base models with tools, search-based methods (Grid/Greedy Search), optimization frameworks (AutoGen, DSPy, GEPA, LLM as policy (LAP)), and flat PPO policy baselines (RL Bandits, RL Episodes). Our method achieves the best results on most tasks. Bold and underline indicates best and second best performance over each task. Reasoning Capability () Tool-Use Capability () Method Model GSM8k DROP MedQA Avg / HotpotQA GAIA Avg / Base Model w/ Tools * Qwen 2.5 7B Gemini 2.5 FL Grid Search Qwen 2.5 7B Greedy Search Qwen 2.5 7B AutoGen DsPy GEPA LAP LAP - FS RL Bandits RL Episodes ARC w/o SFT ARC Qwen 2.5 7B Qwen 2.5 7B Qwen 2.5 7B Qwen 2.5 7B Qwen 2.5 7B Qwen 2.5 7B Gemini 2.5 FL Qwen 2.5 7B Gemini 2.5 FL Qwen 2.5 7B Gemini 2.5 FL Qwen 2.5 7B Gemini 2.5 FL 37.8% 62.3% 74.0% 78.2% 74.8% 80.4% 83.6% 38.3% 46.1% 80.0% 74.7% 85.2% 71.0% 36.4% 45.9% 54.3% 57.5% 58.0% 57.9% 39.3% 38.4% 44.9% 54.3% 54.5% 54.3% 59.0% 49.1% 10.0% 53.1% 54.9% 41.1% 39.4% 60.5% (+19.4) 63.5% (+22.4) 67.8% (+26.7) 70.5% 65.4% (+24.3) 57.9% 87.1% 70.0% (+28.9) 41.9% (+0.8) 49.0% 46.8% (+5.7) 49.4% 56.9% 58.8% 57.3% 60.9% 63.7% (+22.6) 62.7% (+23.3) 65.6% (+24.5) 63.6% (+24.2) 69.4% (+28.3) 58.4% 62.3% 87.6% 69.9% (+30.4) 85.5% 62.8% 61.3% 72.4% (+31.3) 88.6% 63.9% 64.6% 88.5% 65.6% 64.7% 72.9% (+33.5) 10.1% 18.8% 27.9% 28.6% 34.1% 28.2% 27.4% 10.2% 10.5% 27.3% 33.4% 27.8% 30.5% 33.7% 33.8% 34.1% 35.7% 2.0% 3.0% 1.0% 3.0% 1.0% 3.0% 4.0% 0.0% 1.0% 2.0% 3.0% 2.0% 1.0% 6.1% 10.9% 14.45% (+8.35) 15.8% (+9.7) 17.55% (+11.45) 15.6% (+9.5) 15.7% (+9.6) 5.1% (1.0) 5.75% (0.35) 14.65% (+8.55) 18.2% (+7.3) 14.9% (+8.8) 15.75% (+4.85) 19.35% (+13.25) 18.4% (+7.5) 5.0% 3.0% 6.0% 20.05% (+13.95) 4.0% 19.85% (+8.95) *Base model constrained to match the maximum per-query budget available to learned policies. the first 65 samples for training and the rest for evaluation. More details are provided in Appendix F. Baselines. We compare our framework and its non SFTrefined variant against several categories of baselines: (1) Base models with tools, utilizing off-the-shelf LLMs evaluated under max-token constraints and restricted to noniterative tool calls; (2) Search-based methods, employing Grid and Greedy search to establish upper bounds on fixed strategies; (3) Workflow/Prompt optimization frameworks, including AutoGen (Wu et al., 2024), DSPy (Khattab et al., 2023), GEPA (Agrawal et al., 2025), and LLM as Policy (LAP); and (4) RL baselines, which treat configuration as either bandit problem (RL Bandits) or sequential decision process (RL Episodes) optimized via PPO. Implementation. We provide modular framework supporting arbitrary tool registries, custom workflows, and nagent topologies. Our default experimental setup uses 9 workflow patterns (details in Appendix A), 4 tools, and 3 agents. We evaluate both Qwen 2.5 7B Instruct (Yang et al., 2024a) and Gemini 2.5 Flash Lite (Comanici et al., 2025), focusing on Qwen in the main text. Full training details and additional results are included in Appendix F. 4.2. RQ1: Does Learning Configuration Improve Performance? Table 1 summarizes the performance across the reasoning and tool-use benchmarks. We observe consistent improvements with ARC over baseline methods. Reasoning Tasks On GSM8k, ARC achieves 88.6% accuracy with Qwen 2.5 7B Instruct, surpassing GEPA (83.6%), RL Episodes (85.2%). On DROP, ARC reaches 63.9% with Qwen and 65.6% with Gemini, outperforming base models by 27.5% and 19.7% respectively. Interestingly, on MedQA, baselines like GEPA (87.1%) outperform ARC (58.4%) despite ARCs 15.5% improvement over the base model. We attribute this gap to prompt content; GEPA employs domain-specific system prompt containing 1,100 tokens of medical reasoning heuristics, while our prompt library consists of general-purpose reasoning prompts averaging 50 tokens. This suggests that for knowledge-intensive domains, the semantic content of prompts can dominate structural choices, complementary axis that our current atom library does not target. Notably, ARC still surpasses RL and search baselines, confirming the value of hierarchical decision-making; combining structural optimization with domain-specific prompt remains promising future work. Tool-Use Tasks On HotpotQA, ARC achieves 34.1% with Qwen, matching AutoGen (34.1%) and beating standard RL approaches by 6.7%. GAIA, challenging multi-modal tool-use benchmark, sees ARC reach 6.0% and 4.0% with Qwen and Gemini, improving over base model by 4.0% and 1.0% respectively. These gains demonstrate that learned configurations effectively allocate tools and budgets for complex retrieval tasks. Model Agnostic As shown in Table 1, the consistent gains across Qwen 2.5 (open-weights) and Gemini 2.5 (propri6 Learning to Configure Agentic AI Systems Table 2. Workflow diversity across datasets. We report the number of unique workflows (UW), entropy, and Gini coefficient for different configuration strategies. Higher entropy and lower Gini indicate more diverse and balanced exploration of the workflows. Dataset Method UW () Entropy () Gini Table 3. In-domain policy transfer across datasets for Qwen 2.5 7B Instruct model. ST SN : accuracy on training dataset transferring to zero-shot accuracy on new dataset; Dsim: dataset similarity measured via embedding cosine distance. Policies show moderate transfer to reasoning tasks, with performance degradation of 6 7%, while transfer for tool-use tasks is more weaker. HotpotQA GSM8k MedQA Grid Search Greedy Search ARC (w/o SFT) Grid Search Greedy Search ARC (w/o SFT) Grid Search Greedy Search ARC (w/o SFT) 5 2 9 3 2 9 3 2 0.48 0.24 2.13 0.98 0.24 1.90 1.00 0.24 2.95 0.74 0.46 0.58 0.45 0.46 0.67 0.44 0.46 0. Capability Train New ST SN Dsim Reasoning Tool-Use GSM8k DROP GSM8k MedQA DROP GSM8k 88.6 63.0 88.6 57.0 63.9 76.0 34.1 2.0 HotpotQA GAIA GAIA HotpotQA 6.0 19.0 HotpotQA MedQA 34.1 57.0 0.79 0.81 0.79 0.93 0.93 0.76 Figure 4. Accuracy Vs. Cost trade-off on GSM8K. Each point shows average test accuracy versus inference cost for method. The dashed curve denotes the Pareto frontier, representing nondominated methods that achieve the best possible accuracy for given cost. Red points correspond to our ARC variants, which lie on or define the Pareto frontier, indicating superior accuracycost efficiency compared to existing baselines. etary) validates the generalizability of our framework. These results demonstrate that learning query-adaptive configurations via ARC consistently outperforms both fixed architectures and optimization strategies. 4.3. RQ2: Does Adaptive Allocation Improve Efficiency? key advantage of learned configurations is adaptive resource allocation: our policy can deploy expensive workflows (e.g., multi-agent verification, iterative refinement) when necessary, while using lightweight strategies for simpler queries. We quantify this in terms of token consumption and workflow complexity. Figure 4 shows the accuracycost trade-off across methods. Cost is computed as token-based API cost per episode using OpenRouter rates for Qwen2.5 7B Instruct. The dashed curve denotes the Pareto frontier, computed via pairwise dominance. Fixed and search-based baselines lie along this frontier, reflecting the expected trade-off between accuracy and token expense. In contrast, ARC occupies Pareto-optimal regions that strictly improve upon prior methFigure 5. Scaling trends of model accuracy with capacity. Accuracy as function of model size for the Qwen 2.5 family (7B, 32B, 72B) across four benchmarks. Performance improves consistently with scale, with gains varying by task complexity. ods, achieving high accuracy at lower cost. This indicates that instance-specific adaptation yields more efficient accuracycost trade-offs than uniform resource allocation or naive search strategies. Consistent with this observation, Table 2 shows that ARC explores diverse set of workflows. 4.4. RQ3: Can Policies Transfer Across Tasks and Model Capacity? We investigate the generalization capabilities of our learned configurations across two dimensions: distinct task domains (to test structural adaptability) and varying model sizes (to test scalability). Task Specificity Shapes Cross-Domain Transfer. Table 3 reports zero-shot transfer of policies trained on source dataset (ST ) and evaluated on target subset dataset of (SN ). For reasoning tasks, transfer preserves most in-domain performance: ARC trained on GSM8k achieves 63.0% on DROP (vs. 63.9% in-domain) and 57.0% on MedQA (vs. 64.7%), indicating only moderate degradation. In contrast, tool-use transfer depends strongly on tool overlap. ARC trained on HotpotQA transfer reasonably to MedQA (57.0% vs. 64.7%), where both rely on web retrieval, but perform poorly on GAIA (2.0% vs. 6.0%), which requires fundamentally different multimodal tools. This suggests that policy transfer is governed more by shared workflow and tool structure than by semantic similarity alone. Learning to Configure Agentic AI Systems Table 4. Comparison of alternative training objectives across models. PPO with shaped rewards outperforms GRPO, while SFT provides better generalization than DPO. Method Model GSM8k () GAIA () Avg () i - p PPO GRPO SFT DPO Qwen 2.5 7B Gemini 2.5 FL Qwen 2.5 7B Gemini 2.5 FL Qwen 2.5 7B Gemini 2.5 FL Qwen 2.5 7B Gemini 2.5 FL 87.6% 85.5% 82.3% 88.9% 88.6% 88.5% 85.7% 80.1% 5.0% 3.0% 3.0% 2.0% 6.0% 4.0% 46.3% 44.2% 42.6% 45.4% 47.3% 46.2% 4.0% 44.85% 41.1% 2.0% Scaling with Model Capacity. In contrast to task transfer, we observe zero-shot generalization across model scales. We trained the ARC using the Qwen 2.5 7B backbone and evaluated it directly on the 32B and 72B variants of the same family. As shown in Figure 5, performance improves with model capacity across tasks (Avg. Kendalls τ = 1.0 and Pearsons = 0.94). This indicates that the structural priors learned on smaller models are largely invariant to scale. 4.5. Ablation Studies Why SFT refinement? Comparing ARC and ARC w/o SFT rows in Table 1, the SFT phase consistently yields 13% accuracy gains. Beyond accuracy, SFT improves average episode reward by 535% across all datasets and models, validating Theorem 3.2: distillation to elite trajectories raises the performance floor. This refinement distills successful strategies from the RL buffer, reducing output variance and improving consistency. Alternative training objectives. We compared our PPObased training against GRPO (group relative policy optimization), popular policy optimization method. PPO with shaped rewards outperformed GRPO: GRPO struggled with the sparse reward signal from correctness alone. We did not compare against value-based methods due to the highdimensional action space and the need for efficient exploration. For post-training refinement, we evaluated DPO (direct preference optimization) as an alternative to SFT. Although pairwise comparisons can be constructed from the RL buffer for DPO, this approach exhibits overfitting on the training set, leading to degraded generalization performance compared to SFT. Details are in Table 4 and Appendix H. Search-based alternatives. Table 1 includes grid and greedy search baselines. Grid search achieves 74.0% on GSM8k after evaluating 50 configurations, substantially below ARC (88.5%). Greedy search attains 78.2%, still falls well short of the RL baselines, confirming that naive exploration of the configuration space is insufficient. These results underscore the necessity of learning-based approaches Figure 6. Error distribution across benchmarks. Reasoning tasks exhibit primarily reasoning errors, while tool-use tasks are dominated by knowledge gap errors. Policy configuration errors remain minimal (<10%) across all datasets. for tractably navigating the combinatorial design space. 4.6. Error Analysis We categorize errors across all benchmarks into four types: (1) policy configuration errors, where the learned policy selects suboptimal workflows or tools; (2) reasoning errors, where the LLM applies incorrect logic; (3) knowledge gap, where the LLM fails to retrieve correct information or hallucinates; and (4) execution errors, including arithmetic mistakes and answer extraction failures. (see Appendix for detection criteria) Figure 6 reveals distinct error profiles of ARC. On reasoning tasks (GSM8k), 77% of errors are reasoning failures with only 9% from policy misconfiguration. On tool-use tasks (HotpotQA, GAIA), knowledge gap errors dominate (8498%). Critically, policy configuration errors remain below 10% across all benchmarks, indicating the learned policy effectively adapts to query requirements. Remaining errors stem from inherent LLM limitations. 5. Conclusion We introduced ARC, hierarchical reinforcement learning framework for query-adaptive configuration of LLM-based agents. ARC decomposes the combinatorial design space of workflows, tools, budgets, and prompts into tractable two-level policy, enabling efficient decision-making over architectural choices. By combining masked reinforcement learning with supervised fine-tuning refinement, ARC effectively explores this space and focuses learning on highreward configurations. Across multiple reasoning and tooluse benchmarks, ARC consistently outperforms fixed architectures, prompt optimization methods, and flat reinforcement learning policy baselines, achieving up to 16% higher accuracy while minimizing token usage. These results highlight the value of hierarchical structure for improving both performance and resource efficiency in LLM agents. More broadly, this work shows that adaptive architectural decisionmaking is practical and effective direction for scaling LLM-based systems, opening avenues for more flexible and compute-efficient agent design at scale and in practice. Learning to Configure Agentic AI Systems 6. Impact Statement This paper introduces novel framework for dynamically configuring LLM-based agent systems to improve efficiency and adaptability. By enabling query-specific workflow and tool selection without retraining the backbone model, the approach reduces unnecessary computation, latency, and deployment costs. We do not foresee any direct societal harm from this work."
        },
        {
            "title": "References",
            "content": "Agrawal, L. A., Tan, S., Soylu, D., Ziems, N., Khare, R., Opsahl-Ong, K., Singhvi, A., Shandilya, H., Ryan, M. J., Jiang, M., et al. Gepa: Reflective prompt evolution can outperform reinforcement learning. arXiv preprint arXiv:2507.19457, 2025. Anthropic. Building effective agents. //www.anthropic.com/research/ building-effective-agents, 2024. Accessed: 2025-01-14. https: Chase, H. Langchain, 2022. URL https:// github.com/langchain-ai/langchain. Accessed: 2025-01-12. Chen, L., Zaharia, M., and Zou, J. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176, 2023. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Dayan, P. and Hinton, G. E. Feudal reinforcement learning. In Advances in Neural Information Processing Systems, volume 5. Morgan-Kaufmann, 1993. Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. Han, T., Wang, Z., Fang, C., Zhao, S., Ma, S., and Chen, Z. Token-budget-aware llm reasoning. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 2484224855, 2025. 9 Hong, K., Troynikov, A., and Huber, J. Context rot: How increasing input tokens impacts llm performance. Technical report, Chroma, July 2025. URL https: //research.trychroma.com/context-rot. Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023. Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16581677, 2024. Jin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and Szolovits, P. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. Khattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., Santhanam, K., Vardhamanan, S., Haq, S., Sharma, A., Joshi, T. T., Moazam, H., et al. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714, 2023. Koukounas, A., Mastrapas, G., Gunther, M., Wang, B., Martens, S., Mohr, I., Sturua, S., Akram, M. K., Martınez, J. F., Ognawala, S., Guzman, S., Werk, M., Wang, N., and Xiao, H. Jina clip: Your clip model is also your text retriever, 2024. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., Ding, H., Men, K., Yang, K., et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. Ni, J., Abrego, G. H., Constant, N., Ma, J., Hall, K., Cer, D., and Yang, Y. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. In Findings of the association for computational linguistics: ACL 2022, pp. 18641874, 2022. Pang, R. Y., Yuan, W., Cho, K., He, H., Sukhbaatar, S., and Weston, J. Iterative reasoning preference optimization. In arXiv preprint arXiv:2404.19733, 2024. Learning to Configure Agentic AI Systems Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K., and Press, O. Swe-agent: Agentcomputer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024b. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 23692380, 2018. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. Schick, T., Dwivedi-Yu, J., Dess`ı, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. In arXiv preprint arXiv:1707.06347, 2017. Singh, A., Hu, R., Goswami, V., Couairon, G., Galuba, W., Rohrbach, M., and Kiela, D. Flava: foundational language and vision alignment model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1563815650, 2022. Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mpnet: Masked and permuted pre-training for language understanding. Advances in neural information processing systems, 33:1685716867, 2020. Sutton, R. S., Precup, D., and Singh, S. Between mdps and semi-mdps: framework for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1-2): 181211, 1999. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., Jiang, L., Zhang, X., Zhang, S., Liu, J., et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on Language Modeling, 2024. Xu, H., Xie, S., Tan, P.-Y., Huang, P.-Y., Russell, B., and In International Zhang, X. Demystifying clip data. Conference on Learning Representations, 2024. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024a. Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and Chen, X. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2023. 10 A. Agentic Workflows Learning to Configure Agentic AI Systems Figure 7. Overview of the nine agentic workflows: Direct (0), Reason+Ans (1), Reason+Verify+Ans (2), Routing (3), Parallel-Sectioning (4), Parallel-Voting (5), Orchestrator-Workers (6), Evaluator-Optimizer (7), and Autonomous-Agent (8). Each workflow defines distinct pattern of LLM calls and agent interactions. Our framework supports nine agentic workflows, ranging from single-call baselines to multi-agent orchestration patterns. Each workflow specifies computation graph over LLM calls, with configurable tool access and token budgets per agent role. Below we summarize all workflows concisely. Workflow 0: Direct. single LLM call produces the final answer directly. Tools may optionally be used. LLM Calls: 1, Agent2 Tools: Not used. Workflow 1: Reason + Answer. The Reasoner generates intermediate reasoning, which the Answerer converts into final response. LLM Calls: 2, Agent2 Tools: Not used. Workflow 2: Reason + Verify + Answer.An explicit verification step critiques the reasoning before answer synthesis. LLM Calls: 3, Agent2 Tools: Verifier. Workflow 3: Routing.A router dispatches the query to one of two specialized reasoners with different tool configurations. LLM Calls: 3, Agent2 Tools: Conditional (Reasoner2). Workflow 4: Parallel Sectioning.The question is decomposed into independent subtasks solved in parallel and then aggregated. LLM Calls: 4, Agent2 Tools: Worker1. Workflow 5: Parallel Voting.Multiple independent attempts are aggregated via majority voting. LLM Calls: 4, Agent2 Tools: Not used. Workflow 6: Orchestrator-Workers.The Orchestrator delegates subtasks to workers, separating planning from execution. LLM Calls: 4, Agent2 Tools: Workers. Workflow 7: Evaluator-Optimizer.An iterative generateevaluaterefine loop runs until convergence or maximum number of iterations. LLM Calls: 4-7, Agent2 Tools: Evaluator. 11 Learning to Configure Agentic AI Systems Workflow 8: Autonomous Agent.The agent iteratively reasons and invokes tools without an explicit evaluator. LLM Calls: 4, Agent2 Tools: Iterations 2+. B. Computational Resources This section summarizes the computational resources required to reproduce our experiments (training, evaluation, and plotting). We support two execution modes: local (running an open-weight LLM on your own GPU) and API (calling an external provider such as OpenRouter1). The codebase works on Python 3.10+ installation and CUDA-capable machine when using local models. For local (HuggingFace) inference and training, it is sufficient to have single modern GPU for 7B-class models such as Qwen 2.5 7B-Instruct, together with at least 64 GB of CPU RAM to store datasets and logs. Models in the 1.5B3B range can typically be trained and evaluated on commodity GPUs with 24 GB of VRAM, which is adequate for ablations and scaling experiments. For larger models (14B parameters and above), practical training and evaluation usually require either multiple GPUs with 40 GB VRAM each or aggressive quantization; in such cases, it is often preferable to access hosted checkpoints via an API. Under these conditions, reproducing the GSM8K and DROP results with 7B-class model is feasible on single A100/V100/4090-class GPU with 40 GB VRAM (or 24 GB with 4-bit quantization) and 64 GB CPU RAM. In API mode no accelerator is needed; the limiting factors are latency, concurrency limits, and monetary budget. Our evaluation scripts support parallel workers so that, when the provider permits concurrent requests, hundreds to thousands of evaluation episodes can be processed in few hours from single machine. C. Proofs of Theoretical Guarantees This appendix provides complete proofs for the theoretical results in Section 3.4. C.1. Notation We use the following notation throughout: B: experience buffer collected during RL training Delite B: elite subset defined in Eq. 5 n(s, a): count of (s, a) pairs in Delite n(s) = (cid:80) Aelite(s) = {a : n(s, a) > 0}: actions observed for state in elite data n(s, a): total count for state C.2. Proof of Lemma 3.1 (MLE Convergence) Lemma C.1 (MLE Convergence, Restated). Let ˆpelite(as) = n(s, a)/n(s) be the empirical distribution over Delite. Under sufficient model capacity, the MLE objective (Eq. 6) is minimized when πθ = ˆpelite. Proof. The MLE objective is: L(θ) = E(s,a)Delite [log πθ(as)] = (cid:88) s,a ˆpelite(as) log πθ(as). This can be rewritten as: L(θ) = = (cid:88) s,a (cid:88) s,a ˆpelite(as) log πθ(as) ˆpelite(as) log πθ(as) ˆpelite(as) (cid:88) s,a ˆpelite(as) log ˆpelite(as) = DKL(ˆpeliteπθ) + H(ˆpelite), 1https://openrouter.ai/ 12 (7) (8) (9) (10) Learning to Configure Agentic AI Systems where DKL denotes the Kullback-Leibler divergence and H(ˆpelite) is the entropy of the empirical distribution. Since H(ˆpelite) is constant with respect to θ, minimizing L(θ) is equivalent to minimizing DKL(ˆpeliteπθ). By Gibbs inequality, DKL(pq) 0 for any distributions p, q, with equality if and only if = almost everywhere. Therefore: min θ L(θ) = H(ˆpelite), achieved when πθ = ˆpelite. (11) Under sufficient model capacity (i.e., the policy class {πθ : θ Θ} contains ˆpelite), this global minimum is attainable. C.3. Proof of Theorem 3.2 (Policy Concentration) Theorem C.2 (Policy Concentration, Restated). Let Aelite(s) = {a : (s, a) Delite}. Under Lemma 3.1, the refined policy πSFT satisfies: 1. (Support restriction) πSFT(as) > 0 = Aelite(s). 2. (Reward guarantee) EaπSFT[r(s, a)] τ . Proof. Part 1 (Support Restriction): By Lemma 3.1, under sufficient model capacity and optimization, the SFT policy converges to the empirical distribution: By definition of the empirical distribution: πSFT(s) = ˆpelite(s). ˆpelite(as) = n(s, a) n(s) > 0 n(s, a) > 0 Aelite(s). Therefore: πSFT(as) > 0 = ˆpelite(as) > 0 = Aelite(s). Part 2 (Reward Guarantee): By the definition of the elite dataset (Eq. 5): Delite = {(si, ) : correcti r(si, ) τ }. This implies that every action in Aelite(s) has reward at least τ : Aelite(s) = r(s, a) τ. Now we compute the expected reward under πSFT: EaπSFT[r(s, a)] = (cid:88) aA πSFT(as) r(s, a) (cid:88) aAelite(s) (cid:88) = πSFT(as) r(s, a) (by Part 1, zero mass outside Aelite) πSFT(as) τ (since r(s, a) τ for Aelite(s)) aAelite(s) (cid:88) = τ aAelite(s) πSFT(as) = τ 1 (probability sums to 1) = τ. (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) C.3.1. DISCUSSION: WHY THESE GUARANTEES MATTER Learning to Configure Agentic AI Systems Support Restriction ensures that the refined policy only proposes configurations that were successful during training. This prevents the policy from inventing novel, untested configurations at deployment time, which could lead to unpredictable behavior. Reward Guarantee provides performance floor. Since all elite actions achieve reward at least τ , and πSFT only samples from this set, the expected performance is guaranteed to be at least τ . In our experiments, τ corresponds to the 70th percentile of rewards in the RL buffer, meaning the SFT policy is guaranteed to perform at least as well as the top 30% of RL trajectories in expectation. These theoretical properties complement the empirical findings in Section 4.5, where we observe that adding the SFT phase consistently improves performance and reduces variance across runs. D. Ablation: Identifying the Best Embedding for State Representation The structure policy πstruct conditions on state representation sq = [ϕ(q); fq], where ϕ(q) is learned embedding and fq contains hand-crafted features (Section 3.1). The choice of embedding model ϕ() is critical: if the representation fails to capture task-relevant semantics, the policy cannot learn to differentiate queries that require different configurations. We conduct comprehensive ablation across 19 embedding models spanning text-only encoders (Sentence-T5 (Ni et al., 2022), E5 (Wang et al., 2022), MiniLM, MPNet (Song et al., 2020)), vision-language models (CLIP, MetaCLIP, SigLIP (Zhai et al., 2023), FLAVA (Singh et al., 2022)), and hybrid approaches (Jina-CLIP (Koukounas et al., 2024)). For each model, we evaluate embeddings in two modes: native (using the models original dimensionality) and projected (linearly projecting to fixed 768-dimensional space for fair comparison). We design four evaluation tasks that directly measure properties relevant to RL policy learning: Clustering Quality (ARI). We evaluate how well embeddings group semantically similar questions using Adjusted Rand Index (ARI). RL policies rely on embeddings that place similar problems close together so that similar inputs lead to consistent decisions. We cluster embeddings using k-means and compare against ground-truth labels (dataset tool type). ARI ranges from 1 to 1, where 1.0 indicates perfect alignment with ground truth. Classification Accuracy (Cls. Acc). We test whether embeddings distinguish problem types and tool requirements, which is essential for the policy to select correct workflows and tools. We train linear classifiers for two tasks: (1) dataset classification predicting the source benchmark (GSM8k, HotpotQA, AIME, MedQA), and (2) tool requirement classification predicting the required tool (calculator, web search, python, none). The reported score is the average cross-validation accuracy across both tasks. Figure 8. Overall score vs. runtime vs. multimodality with the Pareto frontier under three-dimensional dominance criterion (runtime , score , multimodality ). Circles denote text-only models and triangles denote multimodal models. The selected multimodal model (MetaCLIP-H14) is highlighted. Complexity Ranking (Complexity). We assess whether embeddings capture problem difficulty, which is crucial for budget allocation decisions. We define complexity score using heuristics (question length, answer length, difficulty-indicating keywords) and train Ridge regressor to predict complexity from embeddings. We report the Spearman correlation between predicted and true complexity rankings, which measures monotonic relationship quality. Higher values indicate better difficulty ordering. Decision Prediction (Decision). This task most directly simulates the RL objective. We train multi-head MLP to simultaneously predict: (1) workflow ID, (2) required tool, and (3) compute budget tier (Low/Mid/High). The Decision Score is the average accuracy across all three prediction heads, weighted by combined accuracy metric that requires all three predictions to be correct simultaneously. Overall Score. We compute weighted average across all four metrics: Overall = 0.15 ARI + 0.25 Cls. Acc + 0.25 14 Learning to Configure Agentic AI Systems Complexity + 0.35 Decision The weights reflect the relative importance of each property for downstream RL performance, with Decision Prediction receiving the highest weight as it most closely matches the policys objective. Table 5. Evaluation of embedding models for downstream RL tasks. The highlighted sentence-t5-base (text-only) and MetaCLIP-H14 (multimodal) are the final candidates, with their performanceefficiency trade-off shown as Pareto frontier in Fig. 8. Embedder sentence-t5-base (768D) Mode native sentence-t5-base (768D) projected MetaCLIP-H14 (1024D) native MetaCLIP-2-Worldwide-L14 (768D) native MetaCLIP-2-Worldwide-L14 (768D) projected Jina-CLIP-v2 SigLIP-Large SigLIP-Large (1024D) CLIP-Base-Patch16 (512D) CLIP-Large (768D) projected projected native native native SigLIP-Base projected all-mpnet-base-v2 (768D) native CLIP-Large siglip-base (768D) flava-full flava-full (768D) all-MiniLM-L12-v2 (384D) all-MiniLM-L6-v2 e5-base (768D) projected native projected native native projected native ARI Cls. Acc Complexity Decision Overall Time (s) 0.5603 0. 0.4867 0.1017 0.4074 0.0525 0.3470 0.3272 0.3736 0.0415 0.3790 0. 0.3469 0.0412 0.3370 0.0036 0.3406 0.0067 0.3264 0.0447 0.3356 0.0181 0.3293 0. 0.3025 0.0183 0.3526 0.0294 0.3362 0.0086 0.3304 0.0195 0.3250 0.0028 0.3225 0. 0.8733 0.0048 0.8635 0.0084 0.8514 0.0043 0.8553 0.8470 0.8476 0. 0.8048 0.0132 0.8081 0.0132 0.8454 0.0094 0.8416 0.0082 0.8251 0.0102 0.8426 0. 0.8366 0.0113 0.8284 0.0121 0.8476 0.0086 0.8509 0.0090 0.8432 0.0034 0.8399 0. 0.8432 0.0054 0.9261 0.0099 0.9189 0.0109 0.8944 0.0112 0.9131 0. 0.8833 0.0124 0.8857 0.0088 0.8911 0.0095 0.8894 0.0141 0.8811 0.0116 0.8797 0. 0.8894 0.0076 0.8744 0.0111 0.8863 0.0087 0.8898 0.0047 0.8923 0.0081 0.8766 0. 0.8624 0.0113 0.8705 0.0133 0.7221 0.0070 0.7138 0.0018 0.7198 0.0139 0. 0.7245 0.6798 0.0027 0.7135 0.0030 0.7264 0.0037 0.6872 0.0121 0.6955 0. 0.7187 0.0035 0.6784 0.0071 0.7029 0.0051 0.7262 0.0128 0.6513 0.0041 0.6513 0. 0.6497 0.0115 0.6670 0.0148 0.6412 0.0056 0.7704 0.0305 0.7457 0.0229 0.7183 0. 0.7151 0.7025 0.6961 0.0129 0.6958 0.0057 0.6931 0.0090 0.6898 0. 0.6897 0.0050 0.6875 0.0115 0.6865 0.0097 0.6858 0.0064 0.6858 0.0084 0.6853 0. 0.6827 0.0031 0.6750 0.0118 0.6735 0.0071 0.6694 0.0042 38.39 1.24 37.56 0. 44.04 1.07 43.91 49.20 236.38 2.23 47.45 1.58 39.74 1. 28.03 0.87 33.74 5.19 27.17 1.10 42.37 0.92 30.08 0.87 27.54 1. 31.11 0.01 31.64 0.97 34.55 1.24 26.56 1.26 36.48 1.05 E. Ablation: Identifying the Best Prompt Generator The prompt policy πprompt selects instructions from predefined library of prompts, semantic instruction fragments such as Decompose the problem or Verify intermediate steps (Section 3.1.2). To ensure high-quality instructions, we augment base templates with dataset-specific prompts generated via meta-prompting. This ablation identifies the optimal LLM for generating these prompts. We generate atoms for 5 datasets (GSM8k, HotpotQA, GAIA, MedQA, AIME) using 12 models spanning OpenAI (GPT-4o, GPT-4 Turbo, GPT-5.2), Anthropic (Claude 3.5 Haiku/Sonnet), Meta (Llama 3.1 8B/70B), Mistral (Large), Google (Gemini 15 Learning to Configure Agentic AI Systems Table 6. Ablation study comparing LLM models for generating prompt atoms. Each row is model-dataset combination, ranked by combined score (40% diversity + 60% quality). Diversity metrics: Uniqueness (1 mean pairwise cosine similarity), Coverage (fraction of 6 strategy types), Semantic (silhouette score from k-means). Quality metrics: Coherence and Clarity (GPT-5 rated 110), Specificity (distance from base atoms). Model GPT-5.2 GPT-5.2 GPT-5.2 GPT-5.2 GPT-5.2 Llama 3.1 70B Llama 3.1 8B GPT-4o Claude 3.5 Haiku Claude 3.5 Haiku Llama 3.1 8B GPT-4 Turbo Llama 3.1 70B Qwen 2.5 7B Qwen 2.5 7B Size Dataset Unique Diversity Cover Sem. Coher. Quality Spec. Clar. Score Closed Closed Closed Closed Closed 70B 8B Closed Closed Closed 8B Closed 70B 7B 7B gsm8k medqa gaia hotpotqa aime25 gaia gaia medqa hotpotqa aime25 hotpotqa medqa hotpotqa gaia hotpotqa 0.18 0.24 0.19 0.17 0.15 0.18 0.24 0.19 0.17 0.15 0.23 0.19 0.16 0.25 0.22 0.83 0.67 0.83 0.83 0. 0.83 0.67 0.67 0.67 0.67 0.67 0.67 0.67 0.50 0.67 0.08 0.19 0.10 0.05 0.18 0.29 0.30 0.30 0.25 0.19 0.27 0.29 0.25 0.28 0.25 8.76 8.94 8.56 8.88 8.81 5.50 5.53 5.50 5.50 5.71 5.50 5.50 5.50 5.50 5.50 0.24 0.26 0.25 0.27 0. 0.21 0.22 0.19 0.25 0.25 0.22 0.20 0.23 0.21 0.23 5.50 5.50 5.50 5.50 5.50 5.50 5.50 5.76 5.50 5.71 5.50 5.50 5.50 5.50 5.50 0.48 0.49 0.48 0.48 0.49 0.44 0.42 0.42 0.41 0.41 0.41 0.40 0.40 0.40 0."
        },
        {
            "title": "2.5 Pro), and Qwen (2.5 7B/72B). Each model generates atoms for three agent roles: reasoner, verifier, and answerer.",
            "content": "We evaluate generated atoms on two axes: Diversity (40% weight): Uniqueness (1 mean pairwise cosine similarity), strategy coverage (ratio of covered reasoning strategies), and semantic diversity (silhouette score from k-means clustering). Quality (60% weight): Coherence (GPT-5 rating 110), specificity (cosine distance from base atoms), and clarity (GPT-5 rating 110). The combined score is 0.4 Diversity + 0.6 Quality. Table 6 shows results aggregated by model. GPT-5.2 achieves the highest combined score (0.487 average) across all datasets, driven by superior quality metrics (coherence 8.79, clarity 5.50). Notably, GPT-5.2 also achieves the highest strategy coverage (0.80), indicating diverse reasoning approaches. Based on these results, we use GPT-5.2-generated atoms in our prompt library. F. Training Details PPO Training. We use the following hyperparameters: Learning rate: 3 104 (structure), 5 105 (prompt) Batch size: 32 episodes PPO clip epsilon: ϵ = 0.2 Discount factor: γ = 0.95 Entropy coefficient: βent = 0.05 Value loss coefficient: 0.5 Gradient clipping: max norm 0.5 PPO epochs per batch: = 4 Total training episodes: 4,000 per dataset minimum Reward shaping coefficients: Task success weight: α = 5.0 Step penalty: βs = 0. 16 Learning to Configure Agentic AI Systems Token penalty: βt = 0.03 Tool shaping: δ1 = 0.1, δ2 = 0.2, δ3 = 0.3 SFT Post-training: Structure LR: 1 104, Prompt LR: 5 106 Entropy regularization: 0.01 Reward threshold τ : 4.0 (top 30% of episodes) Epochs: 3 For the SFT refinement phase: Learning rate: 1 104 Elite threshold: τ set to retain top 30% by reward Number of SFT epochs: ESFT = 10 Batch size: 32 Figure 9. Training dynamics of ARC across datasets. Left: cumulative reward over episodes, showing steady improvement as the policy discovers higher-value configurations on GSM8K, DROP, MedQA, HotpotQA, and GAIA. Middle: rolling mean standard deviation of per-episode reward, indicating reduced variance and stabilization over time. Right: running validation accuracy, demonstrating that reward gains translate into improved task performance with dataset-specific convergence levels. Figure 9 summarizes training behavior across datasets. Cumulative and smoothed rewards increase steadily, while running accuracy converges to stable plateaus, indicating that the learned structure and prompt policies make consistent progress rather than overfitting to early episodes. Figure 11 illustrates how the structure policys workflow distribution evolves over the course of training. Early on, the agent explores broad mix of patterns, but as learning proceeds the distribution sharpens and different datasets either converge to distinct dominant workflows or maintain small mixture of high-value patterns, reflecting task-dependent preferences (e.g., more verification-heavy patterns on GSM8K versus coordination-heavy patterns on GAIA). This behavior confirms that ARC is not merely memorizing single best architecture, but actively specializing (and, when needed, mixing) structural choices to match the demands of each domain. Figure 10 tracks the running average number of tools invoked per episode during training. Across datasets, the policy initially explores wider range of tool configurations and then converges to stable, task-specific usage levels, indicating that ARC learns when tools are actually helpful Figure 10. Tool usage during training. Running average number of tools used per episode for each dataset. ARC quickly learns sparse tool usage and gradually adjusts invocation patterns, with different steady-state levels reflecting task-specific reliance on tools. 17 Learning to Configure Agentic AI Systems Figure 11. Evolution of workflow selection during training. Stacked area plots show, for GSM8K, HotpotQA, and GAIA, the fraction of episodes assigned to each workflow as training progresses. The structure policy quickly prunes suboptimal patterns and concentrates mass on small set of task-appropriate workflows (e.g., EvaluatorOptimizer on GSM8K, OrchestratorWorkers on HotpotQA). rather than indiscriminately calling them. Notably, tool-centric benchmarks such as HotpotQA and GAIA converge to higher usage than primarily textual reasoning tasks like GSM8K, suggesting that the learned structure policy adapts its reliance on tools to the demands of each domain. G. Additional Analysis Figure 12. Accuracy by workflow and dataset. Each bar shows the average accuracy of fixed workflow on given benchmark. Performance varies substantially across workflows and tasks no single workflow is uniformly optimalhighlighting the importance of learning query-adaptive configurations rather than relying on fixed architecture. Figure 12 breaks down accuracy by workflow and dataset, revealing how different structural choices contribute to performance. Within each benchmark, accuracy varies substantially across workflows, with multi-step patterns (e.g., ReasonAns or ReasonVerifyAns) typically outperforming simple Direct execution, and more complex coordination patterns (such as routing or parallel voting) being beneficial only on some tasks. No single workflow dominates across all datasets, underscoring that the optimal configuration is highly task-dependent and motivating the need for learned policy that can adaptively select workflows rather than relying on fixed template. Figure 13 visualizes the reward distribution for each workflow across datasets. We observe that workflows favored by the learned policy (e.g., ReasonAns and ReasonVerifyAns) concentrate mass at higher rewards with lower variance, whereas rarely selected patterns exhibit broader, lower-reward distributions, indicating that the policy systematically avoids structurally inefficient configurations. H. Alternative Training Objectives We compared PPO against two alternative RL algorithms: GRPO (Group Relative Policy Optimization): variant of PPO that uses group-based advantage estimation to reduce Learning to Configure Agentic AI Systems Figure 13. Reward distribution by workflow. Violin plots show the distribution of per-episode rewards for each workflow across datasets. Higher-performing workflows exhibit both higher central reward and tighter spread, illustrating that certain structural patterns not only achieve better returns but also yield more stable behavior during training. variance. On GSM8K, GRPO achieved 81.2% accuracy after 2,000 episodes, compared to PPOs 85.7%. GRPO struggled with the sparse binary reward signal from task correctness, as group normalization dampened learning signals. We used the following hyperparameters for GRPO: Learning rate: 3 104 Batch size: 64 episodes Clip epsilon: ϵ = 0.2 Discount factor: γ = 0.99 Entropy coefficient: 0.05 KL coefficient: 0.0 (no KL regularization) Update epochs per batch: 4 Max gradient norm: 0.5 Advantage computation: Ai = Ri σR+108 (group-relative) DPO (Direct Preference Optimization): preference-based method requiring pairwise configuration comparisons. We sampled pairs of configurations and labeled preferences based on reward differences. DPO achieved 79.8% accuracy but required 3 more environment interactions to collect pairwise data. Additionally, the preference labeling was noisy when configurations had similar rewards, leading to unstable training. DPO hyperparameters: Learning rate (structure policy): 1 104 Learning rate (prompt policy): 1 105 Batch size: 16 DPO temperature: β = 0.05 Entropy coefficient: 0.05 Training epochs: 3 Max gradient norm: 0.5 Preference pair thresholds: correct episodes with reward 4.0, incorrect episodes with reward 2.0 I. Error Categorization Methodology We automatically classify errors into four categories based on heuristic analysis of the episode data. Each incorrect prediction is analyzed as follows: I.1. Policy Configuration Errors Learning to Configure Agentic AI Systems We detect policy misconfiguration by checking if the selected workflow, tools, and token budget are appropriate for the query: Workflow mismatch: Simple workflows (Direct) assigned to multi-step problems (detected via question length >100 words or presence of multiple sub-questions). Tool mismatch: Calculator missing when query contains arithmetic keywords (calculate, sum, total); web search missing when query asks about real-world facts. Budget under-allocation: Low token budget (<256) assigned to complex queries (word count >50 or contains explain, step-by-step). I.2. Reasoning Errors We detect reasoning failures by comparing the prediction structure against ground truth: Wrong operation: Prediction uses addition when ground truth uses subtraction (detected via operation keywords: add/plus vs subtract/minus/remaining). Missing steps: Ground truth contains 3 reasoning steps but prediction is single textbf. Comprehension failure: Query contains critical constraints (remaining, left, after) that are absent from prediction. I.3. Knowledge Gap Errors We detect knowledge/retrieval failures for tool-use tasks: Retrieval failure: Prediction contains phrases like could not find, no information available, cannot determine while using retrieval tools (web search, code execution). Factual error: Uses retrieval tools but provides confident answer that differs from ground truth without explicit not found indicators (hallucination). I.4. Execution Errors We detect execution failures where the approach is correct but output is wrong: Arithmetic error: Ground truth contains intermediate calculations (e.g., 5*3=15) and prediction produces different numeric answer. Answer extraction error: Correct answer appears in prediction text but different value is extracted as the final answer. Priority. Errors are assigned to the first matching category in order: policy configuration answer extraction reasoning arithmetic knowledge gap unclassified. This ensures policy failures are surfaced first, as they are most actionable for our framework. I.5. Failure Case Examples We provide concrete examples illustrating each error category: Policy Configuration Error: Query: Find such that log2(x) + log2(x 7) = 3 Selected: Direct workflow, Low budget Issue: Multi-step algebra requires Reason+Verify workflow with high budget. Reasoning Error: 20 Learning to Configure Agentic AI Systems Query: John has 5 apples. He gives 2 to Mary. How many does John have left? Prediction: John has 5 + 2 = 7 apples. Issue: Used addition instead of subtraction despite gives and left keywords. Knowledge Gap Error: Query: Who directed the 2014 film Big Stone Gap? Prediction: Based on the search results, cannot find information about the director. Ground truth: Adriana Trigiani Issue: Retrieval failed to find available information. Execution Error: Query: What is 1 Prediction: 1 Ground truth: 30 Issue: Correct formula, wrong arithmetic. 2 5 12? 2 5 12 ="
        }
    ],
    "affiliations": [
        "School of Computing and Augmented Intelligence, Arizona State University, Tempe, United States of America"
    ]
}