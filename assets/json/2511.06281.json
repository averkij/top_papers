{
    "paper_title": "VideoSSR: Video Self-Supervised Reinforcement Learning",
    "authors": [
        "Zefeng He",
        "Xiaoye Qu",
        "Yafu Li",
        "Siyuan Huang",
        "Daizong Liu",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially advanced the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, the rapid progress of MLLMs is outpacing the complexity of existing video datasets, while the manual annotation of new, high-quality data remains prohibitively expensive. This work investigates a pivotal question: Can the rich, intrinsic information within videos be harnessed to self-generate high-quality, verifiable training data? To investigate this, we introduce three self-supervised pretext tasks: Anomaly Grounding, Object Counting, and Temporal Jigsaw. We construct the Video Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty, revealing that current state-of-the-art MLLMs struggle significantly on these tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset and propose VideoSSR, a novel video self-supervised reinforcement learning framework for RLVR. Extensive experiments across 17 benchmarks, spanning four major video domains (General Video QA, Long Video QA, Temporal Grounding, and Complex Reasoning), demonstrate that VideoSSR consistently enhances model performance, yielding an average improvement of over 5\\%. These results establish VideoSSR as a potent foundational framework for developing more advanced video understanding in MLLMs. The code is available at https://github.com/lcqysl/VideoSSR."
        },
        {
            "title": "Start",
            "content": "VideoSSR: Video Self-Supervised Reinforcement Learning Zefeng He1,2 Xiaoye Qu1* Yafu Li3 Siyuan Huang4 Daizong Liu5 Yu Cheng3* 1Shanghai Artificial Intelligence Laboratory, 2Nanjing Univerisity 3The Chinese University of Hong Kong 4Shanghai Jiao Tong University, 5Wuhan University 5 2 0 2 ] . [ 1 1 8 2 6 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially advanced the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, the rapid progress of MLLMs is outpacing the complexity of existing video datasets, while the manual annotation of new, high-quality data remains prohibitively expensive. This work investigates pivotal question: Can the rich, intrinsic information within videos be harnessed to self-generate high-quality, verifiable training data? To investigate this, we introduce three self-supervised pretext tasks: Anomaly Grounding, Object Counting, and Temporal Jigsaw. We construct the Video Intrinsic Understanding Benchmark (VIUBench) to validate their difficulty, revealing that current state-of-the-art MLLMs struggle significantly on these tasks. Building upon these pretext tasks, we develop the VideoSSR-30K dataset and propose VideoSSR, novel video self-supervised reinforcement learning framework for RLVR. Extensive experiments across 17 benchmarks, spanning four major video domains (General Video QA, Long Video QA, Temporal Grounding, and Complex Reasoning), demonstrate that VideoSSR consistently enhances model performance, yielding an average improvement of over 5%. These results establish VideoSSR as potent foundational framework for developing more advanced video understanding in MLLMs. The code is available at https://github.com/lcqysl/VideoSSR. 1. Introduction In past years, Multimodal Large Language Models (MLLMs) have achieved remarkable progress in the field of video understanding [2, 3, 10, 34, 35, 39, 45, 51]. Benefiting from recent Reinforcement Learning with Verifiable Reward (RLVR) [12, 17, 27, 43, 44, 65, 68], the performance of MLLMs has been further improved. cornerstone of the RLVR approach is the availability of video *Corresponding authors Figure 1. Distribution of answer correctness on ReWatch and LongVideoReason. Across both models and datasets, vast majority of questions yield bimodal outcome, resulting in either zero or eight correct answers. This zero variance issue is notably more pronounced for the more powerful Qwen3-VL model. datasets with verifiable answers. To obtain the verifiable answers, existing datasets, such as LongVideoReason [9] and ReWatch [65], utilize multi-agent collaboration to construct high-quality datasets with verifiable answers. Although current datasets have effectively enhanced the performance of models like Qwen2.5-VL [3], significant limitations arise when applying them to more powerful models, such as the recent Qwen3-VL [39]. First, for highly capable models, many questions in existing datasets lack sufficient complexity to serve as effective training challenges. To illustrate this, we generate eight independent responses per question using Qwen2.5-VL [3] and Qwen3VL [39]. As shown in Figure 1, vast majority of questions yield perfect score where all eight responses are correct, indicating they are insufficiently challenging. Second, the multi-agent annotation process introduces systemic biases and artifacts, which create flawed or spurious reward signals for RLVR, particularly when the annotator models are less capable than the target models. This is evidenced by another large portion of questions where all generated responses are incorrect, suggesting either intractable difficulty or biased ground truths. The resulting bimodal distribution of scores, with most questions exhibiting zero variance, offers an ineffective learning signal for GRPO [17, 43] training in RLVR. Consequently, training advanced models on 1 Figure 2. Performance comparison on four video tasks. Input frames for VideoSSR and Qwen3-VL-8B do not exceed 64. such data yields marginal gains or even performance degradation (Section 4.2). Compounding these issues is the prohibitive cost of manual annotation for video. This predicament, however, points to compelling alternative: can the rich, intrinsic information within videos be harnessed to construct highquality, verifiable questions for RLVR? Inspired by traditional video self-supervised learning [13, 32, 33, 58], we first design three self-supervised pretext tasks with parametrically scalable difficulty, including Anomaly Grounding, Object Counting, and Temporal Jigsaw, to generate verifiable questions. To validate the difficulty of these tasks, we construct Video Intrinsic Understanding Bench (VIUBench) and found that questions targeting the intrinsic properties of the video itself remain profoundly challenging, even for leading closed-source models like GPT-5 [35]. Building on this insight, we introduce VideoSSR, new Video Self-Supervised Reinforcement learning framework to enhance the video understanding of MLLM. We construct the VideoSSR-30K dataset using the aforementioned pretext tasks, which is entirely independent of human or MLLM annotations. This dataset is subsequently utilized to train our model with GRPO. To overcome the challenge of sparse reward signals arising from the inherent difficulty of these tasks, we design corresponding smooth reward functions for each pretext task to ensure efficient and stable RLVR training. To validate the generalization capability of VideoSSR, we conduct extensive experiments on 17 benchmarks spanning four main video tasks: General Video QA, Long Video QA, Temporal Grounding, and Complex Reasoning. The results show that our proposed VideoSSR achieves consistent performance improvements across all benchmarks and under three different input frame settings, demonstrating an average gain of over 5%. In summary, our main contributions are fourfold: 2 We generate verifiable training data for RLVR that harnesses intrinsic video signals. This self-supervised paradigm circumvents the prohibitive costs and inherent biases of prevailing multi-agent and manual annotation, thereby addressing critical bottleneck in scaling MLLMs for video understanding. We introduce three self-supervised pretext tasks with parametrically scalable difficulty. Moreover, we construct VIUBench benchmark from these tasks, which reveals profound limitations in state-of-the-art MLLMs for intrinsic video understanding. We introduce VideoSSR, self-supervised reinforcement learning framework for video RLVR training and construct VideoSSR-30K dataset. To facilitate efficient and stable RLVR training, in VideoSSR, we further design three tailored smooth reward functions. Extensive experiments across 17 benchmarks demonstrate the superior generalization capability of VideoSSR, consistently achieving average performance improvements exceeding 5% and establishing it as foundational approach for advancing video understanding. 2. Related Works 2.1. Reinforcement Learning for MLLMs of reasoning capabilities"
        },
        {
            "title": "Learning with Verifiable",
            "content": "Reward Reinforcement [17, 43] has been shown to significantly (RLVR) the language enhance that has been rapidly extended success models, into MLLMs [8, 9, 11, 19, 27, 38, 60]. stance, Video-R1 [12] leverages existing Video QA datasets [37, 54, 57, 62, 69] to bolster performance on Video QA tasks. Time-R1 [52] utilizes datasets with precise timestamp annotations to improve Temporal Grounding. SpaceR [36] automatically generates verifiable questions from the geometric and semantic ground truths of 3D"
        },
        {
            "title": "For",
            "content": "Figure 3. An overview of our three self-supervised pretext tasks. (a) Anomaly Grounding: temporal segment is perturbed (e.g., via rotation), and the task is to identify the start and end timestamps of this anomaly. (b) Object Counting: Procedurally generated shapes are overlaid onto selected frames, and the task is to count the total number of each shape type. (c) Temporal Jigsaw: The video is divided into clips which are then shuffled. The task is to predict the original temporal order of the segments. scenes [4, 61], enhancing the models spatial reasoning abilities. ReWatch-R1 [65] leverages multi-agent collaboration to construct high-quality reasoning datasets, thereby advancing its capabilities in complex reasoning. Despite these diverse data sourcing strategies, several fundamental limitations persist. The reliance on external annotations often introduces significant bias. Meanwhile, many approaches often specialize in enhancing single capability, which can limit their broader generalization. 2.2. Self-supervised learning for Video Self-supervised learning [13, 24, 28, 29, 32, 33, 42, 49, 58] for video aims to learn effective spatio-temporal representations from unlabeled video data. The core principle involves designing pretext tasks that capitalize on the inherent properties of video. For instance, early works leverage tasks such as video jigsaw puzzles [1, 23, 32, 47, 58] to learn representations. Similarly, recent research [56, 64] has employed the jigsaw puzzle task to facilitate the reinforcement learning of MLLMs. While training MLLMs with the video jigsaw task has been shown to enhance performance on tasks requiring temporal-centric understanding [56], the self-supervised paradigm has not been fully explored for video understanding. In this work, we move beyond single task and investigate richer suite of pretext tasks to cultivate more comprehensive generalization in MLLMs. 3. Method in Considering there is rich information in the video, this paper, we explore leveraging the intrinsic information within the video itself to construct high-quality questions with scalable difficulty. To investigate it, we begin by designing novel pretext tasks. 3.1. Pretext Tasks In this section, we introduce three pretext tasks, including Anomaly Grounding, Object Counting, and Temporal Jigsaw. These tasks share common design philosophy, namely, they can generate verifiable question-answer pairs directly from raw videos, independent of any human or model-generated annotations. Furthermore, the difficulty of these pairs can be parametrically controlled. The overall process for these three tasks is illustrated in Figure 3. 3.1.1. Anomaly Grounding This task assesses the models ability to localize temporal segments that violate natural video dynamics. Let video be represented as sequence of frames = {f1, f2, . . . , fT }, with total duration of seconds. We first randomly select temporal interval [ts, te] [0, D], where ts and te are the start and end timestamps, respectively. This interval corresponds to contiguous segment of 3 frames = {fi timestamp(fi) [ts, te]}. Next, we apply perturbation function to this segment to create perturbed version, = P(S). The function is sampled from set of predefined transformations targeting different core capabilities: Fine-grained Perception: e.g., swapping the red and blue color channels for every frame in S. Spatial Perception: e.g., rotating every frame in by 180 degrees. Temporal Perception: e.g., randomly shuffling the frame order within S. The final video is constructed by replacing the original segment with its perturbed counterpart S. The model is then provided with the modified video and is tasked to identify the anomalous interval by predicting its start and end timestamps, (ts, te). 3.1.2. Object Counting This task targets the models fine-grained perception and counting abilities. We define set of primitive geometric shapes = {c1, c2, . . . , cK}, such as circles, rectangles, and triangles, which can be procedurally generated. For given video , we randomly select subset of frames Fsub . For each frame fi Fsub, we synthesize set of objects Oi, where each object Oi is an instance of shape class from with randomized attributes (size, color, rotation, position). These modified frames, denoted as , are then used to create the final video by replacing their original counterparts. The ground truth is vector of counts = [N1, N2, . . . , NK], where each element Nk is the total number of occurrences of shape ck: (cid:88) Nk = fiFsub {o Oi type(o) = ck} (1) Given , the model is required to output the counts for each shape category. 3.1.3. Temporal Jigsaw This task is designed to evaluate the models temporal perception, specifically its understanding of temporal coherence and event ordering. We partition the video into contiguous, non-overlapping segments of equal duration, = [S1, S2, . . . , Sn]. We then generate random permutation π of the indices {1, 2, . . . , n}. new video is created by reordering the segments according to this permutation: = [Sπ(1), Sπ(2), . . . , Sπ(n)] (2) The model is presented with the shuffled video and is tasked with restoring the original temporal order. To achieve this, it must predict sequence of indices that correctly reorders the shuffled segments. This target sequence is the inverse of the permutation π that was used for shuffling. The answer is therefore the sequence defined by π1: Answer = (π1(1), π1(2), . . . , π1(n)) (3) 3.2. Video Intrinsic Understanding Benchmark After defining the above three tasks, critical question arises: are these pretext tasks sufficiently challenging for state-of-the-art MLLMs? To investigate this, we construct the Video Intrinsic Understanding Bench (VIUBench), which systematically evaluates models ability to comprehend intrinsic video properties across three core axes: Fine-grained Perception, Spatial Perception, and Temporal Perception. The benchmark is composed of 2700 questionanswer pairs generated from our three pretext tasks. The proportional distribution of data across these tasks is illustrated in the left panel of Figure 4. Figure 4. Task distribution in VIUBench and VideoSSR-30K. The left panel illustrates the proportional data distribution across our three pretext tasks and their subtypes for VIUBench. The right panel shows the corresponding composition of VideoSSR-30K. Anomaly Grounding. For the Anomaly Grounding task, we select five representative perturbation types from larger pool of 14 (detailed in Appendix B.1). The selected types include: (1) swapping the red and blue color channels, (2) rotation by 180 degrees, (3) zooming out, (4) horizontal mirroring, and (5) shuffling the intra-segment frame order. We compute the Mean Intersection over Union (mIoU) between the predicted and ground-truth temporal intervals as the performance score. Object Counting. For the Object Counting task, we use three primitive shapes (circles, rectangles, and triangles) and configure two difficulty levels: Easy: Objects are overlaid onto maximum of three frames, with no more than three instances of any single shape type appearing in any given frame. Hard: The constraints are increased to maximum of four frames and up to four instances per shape per frame. score of 1 is awarded for specific shape type if the predicted count is exactly equal to the ground-truth count, and 4 Table 1. Performance comparison on VIUBench. The benchmark assesses three core abilities (Fine-Grained Perception, Spatial Perception, and Temporal Perception) via our three pretext tasks (Object Counting, Anomaly Grounding, and Temporal Jigsaw). For both open source and closed source models, the top result is shown in bold, and the second-best is underlined. Ability Task Type Fine-Grained Perception Spatial Perception Temporal Perception Average Object Counting Anomaly Grounding Temporal Jigsaw Counting Grounding Jigsaw Overall Easy Hard Channel Rotate ZoomOut Mirror Shuffle Easy Hard Random Guess Random Guess 11.1 6. 25.9 25.4 25.4 25.2 25.2 0. 0.0 8.7 25.4 0.1 16.1 GPT-5 [35] Gemini-2.5-Pro [10] Gemini-2.5-Flash [10] Seed1.5-VL [16] Qwen2.5-VL-7B-Instruct [3] VideoJigsaw-7B [56] Qwen3-VL-8B-Instruct [39] Qwen3-VL-32B-Instruct [39] Qwen3-VL-235B-A22B-Instruct [39] GLM-4.5V [20] InternVL-3.5-8B [51] InternVL-3.5-38B [51] VideoSSR-8B (Ours) 88.4 80.8 35.7 72.3 11.3 12.1 13.8 20.1 23.8 59.1 15.2 28.0 29.0 70.3 61.3 22.4 52. 5.3 5.4 7.7 13.0 14.8 45.4 9.6 15.9 24.6 82.6 84.6 75.8 79.0 6.4 1.5 50.1 66.1 68.9 66.4 25.9 47.0 88.7 Closed Source Models 56.5 51.0 28.5 19.4 48.9 55.7 30.2 31.4 Open Source Models 8.1 1.2 21.4 17.6 32.9 21.2 6.1 9.6 94.4 5.2 1.1 13.6 29.4 28.3 29.8 9.8 18. 67.8 81.8 82.3 73.7 70.7 13.7 4.5 53.4 63.0 67.8 61.0 42.4 54.5 89.0 34.1 52.1 28.6 24.1 7.0 2.0 14.1 18.9 26.8 15.3 3.1 12. 41.0 39.0 25.3 8.3 20.7 0.7 20.3 1.3 1.3 7.7 11.0 0.0 0.0 24.3 27.0 17.7 4.0 9.3 0.0 5.0 0.0 0.0 3.3 3.3 0.0 0. 8.0 79.4 71.1 29.1 62.6 8.3 8.8 10.7 16.6 19.3 52.3 12.4 21.9 26.8 60.8 65.1 47.4 44.9 8.1 2.1 30.5 39.0 45.0 38.7 17.5 28. 76.2 33.0 21.5 6.2 15.0 0.3 12.7 0.7 0.7 5.5 7.2 0.0 0.0 16.2 58.7 56.7 34.1 42.2 6.4 5.9 19.5 25.5 30.5 34.7 12.5 20. 51.9 0 otherwise. The final task score is the average of these binary scores across all shape types. Temporal Jigsaw. The Temporal Jigsaw task is configured with two difficulty settings based on the number of segments the video is partitioned into: Easy: The video is partitioned into 6 segments. Hard: The video is partitioned into 8 segments. score is awarded only if the predicted sequence of segments is identical to the ground-truth permutation. As shown in Table 1, we evaluate suite of powerful MLLMs on VIUBench. Our findings reveal that this benchmark poses significant challenge even for the most advanced models. Notably, even strong closed-source model like GPT-5 [35] only achieves modest average score of 58.7. The performance of open-source models is even more limited. For instance, Qwen3-VL-8B attains an average score of just 19.5. These results underscore critical insight that understanding and reasoning about intrinsic video properties, such as fine-grained details and temporal coherence, remains substantial bottleneck for current MLLMs. This highlights the effectiveness of VIUBench in exposing the limitations of existing models and validates its role as challenging benchmark for future research. More importantly, our experiments with VIUBench reveal key advantage of these pretext tasks: the difficulty of the generated questions can be easily scaled by adjusting simple parameters. For instance, in the Object Counting task, switching from the Easy to the Hard configuration caused the score of GPT-5 to drop sharply from 88.4 to 70.3. similar trend is observed in the Temporal Jigsaw task. By increasing the number of video segments from six to eight, the models score plummeted from 39.0 to 27.0. Even for Video Jigsaw [56], model specifically trained on jigsaw tasks, its performance decreases significantly from 20.3 to 5.0 under the same conditions. To sum up, all these findings demonstrate that our method can dynamically generate tasks that challenge powerful MLLMs. The ability to parametrically control task difficulty ensures that VIUBench can remain relevant and challenging benchmark for evaluating the continuous advancements of future models. 3.3. Video Self-Supervised Reinforcement Learning Motivated by the insights from our proposed VIUBench, leverages Video we introduce novel framework that Self-Supervised Reinforcement learning (VideoSSR) to enhance the generalization of MLLMs. To perform reinforcement learning, we first construct the VideoSSR-30K dataset. Specifically, this dataset consists of the aforementioned three pretext tasks. The proportional distribution of VideoSSR-30K dataset is detailed in the right panel of Figure 4. For training, we employ RLVR using GRPO [17, 43]. We do not use recent variants of GRPO [22, 63, 71], as our primary focus is on the data itself. Our reward function is based solely on answer correctness. While these tasks are designed to be difficult, this very characteristic poses problem for RLVR training: using strict reward function often results in sparse rewards, leading to inefficient and unstable training. To address this challenge, we design specific smooth reward function for each task to provide denser and more informative learning signal. Table 2. Performance comparison on General Video QA and Long Video QA tasks."
        },
        {
            "title": "Model",
            "content": "Frames MVBench TempCompass AoTBench VinoGround Video-MME LVBench LongVideoBench CGBench"
        },
        {
            "title": "Long Video QA",
            "content": "GPT-4o [34] Gemini-1.5-Pro [45] Qwen3-VL-8B-Instruct [39] VideoSSR-8B (Ours) - - 32 48 64 32 48 64.6 60.5 66.9 67.6 67."
        },
        {
            "title": "Closed Source Models",
            "content": "63.4 58.3 54 35."
        },
        {
            "title": "Open Source Models",
            "content": "59.8 60.2 60.7 45.2 45.0 45.0 73.8 67.1 74.9 74.8 74.8 71.9 75.0 64.1 66.0 66. 30.8 33.1 39.6 41.5 43.0 62.0 58.6 58.6 60.0 61.3 45.2 37.2 40.1 41.7 42. 68.6(+1.7) 68.8(+1.2) 68.9(+1.1) 75.7(+0.8) 75.8(+1.0) 75.7(+0.9) 60.5(+0.7) 61.7(+1.5) 61.8(+1.1) 55.6(+10.4) 55.6(+10.6) 55.6(+10.6) 65.2(+1.1) 66.7(+0.7) 67.6(+1.2) 41.5(+1.9) 42.9(+1.4) 44.0(+1.0) 59.6(+1.0) 61.1(+1.1) 61.5(+0.2) 40.2(+0.1) 42.5(+0.8) 43.4(+0.8) Anomaly Grounding. For the temporal grounding of anomalies, the Mean Intersection over Union (mIoU) naturally serves as smooth reward signal. It provides score between 0 and 1 that reflects the degree of overlap between the predicted and ground-truth temporal segments. Let Tpred and Tgt be the predicted and ground-truth intervals, respectively. The reward Rground is simply as below: This error is then normalized by the maximum possible error, Emax, which occurs for reversed sequence. The final reward is given by: Rjigsaw ="
        },
        {
            "title": "Ejigsaw\nEmax",
            "content": "(8) Rground = IoU(Tpred, Tgt) = Tpred Tgt Tpred Tgt (4) 4. Experiments Object Counting. For the counting task, our reward function provides dense signal based on the average relative error across all shape categories. For each category k, we first compute score Rcount,k that is inversely proportional to the relative error. Let yk be the ground-truth count and ˆyk be the predicted count for category k. The score for single category is: Rcount,k = max 0, 1 (cid:18) (cid:19) ˆyk yk yk + ε (5) Here, the absolute error is normalized by the magnitude of the ground-truth value, and small constant ε (e.g., 109) ensures numerical stability. The final reward for the entire task, Rcount, is the average of these scores over all shape categories: Rcount ="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 Rcount,k (6) Temporal Jigsaw. For the jigsaw puzzle, our reward function measures the structural correctness of the predicted sequence. We compute penalty based on the cumulative displacement of elements from their correct positions. Let Pgt be the ground-truth permutation and ˆP be the predicted permutation. Let pos(v, ) denote the position of an element in sequence . The total displacement error Ejigsaw is defined as: Ejigsaw = (cid:88) k=1 pos(k, ˆP ) pos(k, Pgt) (7) 6 Implementation Details. In this paper, our VideoSSR model is built upon the Qwen3-VL-8B-Instruct [39]. We perform RLVR on our newly constructed VideoSSR-30K dataset for one epoch. Key hyperparameters for training include learning rate of 1 106, global batch size of 64, and rollout number (N ) of 8 for generation, KL divergence penalty with coefficient of 1 103. MAX FRAMES is configured to 48, and MAX PIXELS is set to 256 256 for efficient training. The entire training process is conducted on 8 H200 GPUs and takes approximately 16 hours. To ensure fair and reproducible comparison, both Qwen3-VL and VideoSSR are evaluated under identical conditions: FPS is set to 2, with MAX FRAMES configured to {32, 48, 64}. MAX PIXELS is set to 512 512. Greedy decoding is used to ensure reproducibility. Chain of thought [53] is not utilized to mitigate hallucination [31] and ensure correct output formatting, therefore enhancing performance. Benchmarks and Baselines. To comprehensively evaluate the generalization capability of VideoSSR, we conduct experiments on 16 distinct benchmarks spanning four major video task categories: General Video QA: MVBench [26], TempCompass [30], AoTBench [59], and VinoGround [66]. Long Video QA: Video-MME [14], LVBench [50], LongVideoBench [55], and CGBench [6]. Temporal Grounding: QVHighlights [25], ActivityNet [5], CharadesSTA [15], and TACoS [40]. Table 3. Performance comparison on Temporal Grounding and Complex Reasoning tasks."
        },
        {
            "title": "TACoS",
            "content": "VideoMMMU Video-TT VCRBench CVBench"
        },
        {
            "title": "Complex Reasoning",
            "content": "GPT-4o [34] Gemini-1.5-Pro [45] Qwen3-VL-8B-Instruct [39] VideoSSR-8B (Ours) - - 32 48 64 32 48 - - 43.7 46.4 48."
        },
        {
            "title": "Closed Source Models",
            "content": "35.7 -"
        },
        {
            "title": "Open Source Models",
            "content": "50.3 50.0 49.2 - - 36.5 38.4 39.8 - - 22.4 25.9 28.1 61.2 53. 58.2 58.5 58.8 45.2 38.2 41.8 43.0 44.0 29.0 48.2 7.4 7.4 8.8 69.2 - 61.8 61.5 61.6 59.6(+15.9) 61.1(+14.7) 62.6(+14.0) 42.1(+5.6) 43.0(+4.6) 43.7(+3.9) 52.1(+1.8) 51.1(+1.1) 49.9(+0.7) 23.1(+0.7) 27.7(+1.8) 30.6(+2.5) 59.9(+1.7) 60.0(+1.5) 60.9(+2.1) 44.2(+2.4) 44.9(+1.9) 45.8(+1.8) 10.7(+3.3) 15.3(+7.9) 17.8(+9.0) 63.5(+1.7) 63.8(+2.3) 63.3(+1.7) Complex Reasoning: VideoMMMU [21], VideoTT [70], VCRBench [41], and CVBench [72]. Temporal Jigsaw task. It also obtains consistent improvements on other video reasoning benchmarks. For the Temporal Grounding tasks, we report the Mean Intersection over Union (mIoU) as the primary evaluation metric. Further details regarding each benchmark and full breakdown of the results can be found in Appendix A.2.3. For our primary baseline, we select Qwen3-VL-8BInstruct, as it represents the state-of-the-art among opensource models. To further contextualize the performance of our method, we also provide comparative analysis against two formidable proprietary models: GPT-4o [34] and Gemini-1.5-Pro [45]. 4.1. Main Results General Video QA As shown in the left half of Table 2, VideoSSR achieves substantial improvements on temporally related benchmarks such as VinoGround [66], even surpassing closed source models. It also obtains improvements on more general benchmarks, for instance on MVBench [26], achieving score of 68.9 and similarly outperforming the closed source models. Long Video QA As shown in the right half of Table 2, VideoSSR also achieves consistent improvements on four mainstream benchmarks. Because we primarily conduct training and evaluation with low number of frames, gap remains compared to closed-source models on such long video understanding tasks, which is direction for future research. Temporal Grounding As shown in the left side of Table 3, benefiting from the Anomaly Grounding task, VideoSSR achieves remarkable zero-shot improvements on multiple mainstream temporal grounding benchmarks, especially on QVHighlights [25] and ActivityNet [5], with gains of +15.9 and +5.6, respectively. Complex Reasoning As shown in the right side of Table 3, VideoSSR achieves large improvement of +9.0 on VCRBench [41], benchmark that is highly correlated with our In summary, we validate the generalization capability of VideoSSR on the 16 aforementioned benchmarks. Notably, VideoSSR achieves consistent performance improvements across four major video tasks under three different frame settings. Under the 48 frame setting, VideoSSR obtains an average improvement of 5.1% across all 17 benchmarks (including VIUBench), comprehensively demonstrating the effectiveness of VideoSSR. 4.2. Ablation Study Analysis on three pretext tasks. First, we individually validate the effectiveness of the three pretext tasks, the as shown in Table 4. Benefiting from its design, Anomaly Grounding task leads to significant performance increase on CharadesSTA. Similarly, the Temporal Jigsaw task brings substantial boost to VCRBench. Notably, all three tasks individually improve performance on VideoMME, confirming their contribution to enhancing general video understanding capabilities. Moreover, Table 4 also shows the impact of the smooth reward function on the results. We observe that the model trained with strict matching reward function performs closer to the baseline. This is because strict reward function often leads to sparse reward signals, which are more likely to result in zero advantage in GRPO. Consequently, the training becomes inefficient, leading to smaller update magnitudes. Furthermore, this approach introduces training instability. For instance, training the anomaly grounding task with strict reward function even degrades performance on CharadesSTA. To further investigate the benefits of task diversity, we conducted comparative analysis between single task training and our mixed task VideoSSR-30K framework, controlling for the data scale at 30k samples for both settings. As illustrated in Figure 5, we observe that simply scaling up the data for single task yields diminishing returns and even 7 Table 4. Ablation study of the three pretext tasks and their corresponding smooth reward functions. G, C, and represent Anomaly Grounding, Object Counting, and Temporal Jigsaw, respectively. indicates the component is used for training. R@0.5 denotes recall at an IoU threshold of 0.5. Step denotes step accuracy for VCRBench. The best and second best results are shown in bold and underlined. the powerful Qwen3-VL on LongVideoReason, dataset annotated by less capable MLLM, can even lead to performance degradation, which further demonstrate the importance of our self-supervised paradigm. Table 5. Ablation study on different training datasets. None indicates the baseline Qwen-VL3."
        },
        {
            "title": "Grounding",
            "content": "Reasoning CharadesSTA VCRBench Step mIoU R@0.5 Acc"
        },
        {
            "title": "Training Config",
            "content": "Fine-tuning Data"
        },
        {
            "title": "Size",
            "content": "58.4 7.4 25."
        },
        {
            "title": "None",
            "content": "Video-MME CharadesSTA VCRBench All Step Long mIoU R@0.5 Acc Baseline Model 64.1 25.9 58.4 54.3 50. 7.4 Fine-tuned Models"
        },
        {
            "title": "Training Config\nPretext Tasks Reward\nSmooth\nJ\nG C",
            "content": "Understanding Video-MME Long All Baseline Model 64.1 54.3 Models on Subtasks 54.8 64.7 55.9 64.8 54.7 64.7 56.2 64.9 55.0 64.3 64.8 55.8 Models on All Tasks 55.4 64.8 57.1 65.2 50.3 47.5 53.8 51.5 51.4 51.3 51. 51.3 52.1 52.2 63.8 59.9 60.1 59.1 59.0 59.3 60.6 5.8 4.1 6.3 5.5 13.4 15.9 10.7 10.7 24.9 22.8 25.4 24.8 32.7 35. 30.4 32.3 degrades performance. This finding suggests that designing diverse and rich set of pretext tasks, rather than focusing on single one, is more promising direction for enhancing model capabilities. LongVideoReason [9] ReWatch [65] VideoSSR-30K 63.6 32k 27k 64.7 30K 65.2 53.3 56.7 57.1 51.7 51.6 52. 59.4 59.2 60.6 7.1 2.7 10.7 26.1 22.2 32.3 Finally, we explored the optimal selection of subtasks for the Anomaly Grounding task. We investigate 14 distinct perturbation types and report their corresponding accuracies on Video-MME, as illustrated in Figure 6. Further details are provided in Appendix B.1. Based on these results, we select four perturbations that offer substantial improvements and create uniform mixture to construct our final training set. Furthermore, we found that perturbations targeting temporal properties, such as simulating fast forward effect by sampling denser frame sequences, does not appear to yield benefits and even introduced negative side effects. This may be because the base model, Qwen3-VL, relies on textual timestamps for its temporal awareness. Deliberately creating visual anomalies in this domain might confuse the model rather than enhance its learning. Figure 5. Comparison of single task and mixed task training at the 30k data scale. The results demonstrates that task diversity is more effective for improving performance than simply scaling up the data for single pretext task."
        },
        {
            "title": "We also compare our method against",
            "content": "training with LongVideoReason [9] or ReWatch [65]. We utilize only the multiple-choice subsets from each dataset. The specific training procedures are: For LongVideoReason, the model is trained for 500 steps with batch size of 64. For ReWatch, we use composite subset of questions from its Video-R1 [12] and VideoEspresso [18] portions and train the model for one full epoch. The results are presented in Table 5. Notably, the model trained with VideoSSR-30K surpasses the performance of models trained on annotated datasets of comparable scale. Furthermore, we observe critical limitation: fine-tuning Figure 6. Ablation study of the 14 perturbation subtypes for Anomaly Grounding. Accuracy is reported on Video-MME. 5. Conclusion In this paper, we introduce VideoSSR, novel selfsupervised reinforcement learning framework designed to address the critical limitations of existing video datasets for training MLLMs. By designing the three pretext tasks of Anomaly Grounding, Object Counting, and Temporal Jigsaw, we construct the challenging VIUBench and the VideoSSR-30K dataset without reliance on manual or MLLM annotations. Our extensive experiments demonstrate that VideoSSR leads to consistent and significant performance gains across 17 diverse benchmarks, including 8 four main video tasks, achieving an average improvement of over 5%. Our work highlights self-supervision as powerful method for generating scalable, low-cost, and highquality training data. Crucially, the parametric control over task difficulty ensures the long-term relevance of our framework for benchmarking increasingly capable MLLMs. Our approach moves beyond the limitations of static, annotated datasets, enabling the development of models that learn directly from the intrinsic structure of video."
        },
        {
            "title": "References",
            "content": "[1] Unaiza Ahsan, Rishi Madhok, and Irfan Essa. Video jigsaw: Unsupervised learning of spatiotemporal context for video In 2019 IEEE Winter Conference on action recognition. Applications of Computer Vision (WACV), pages 179189. IEEE, 2019. 3 [2] Lei Bai, Zhongrui Cai, Yuhang Cao, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, et al. Intern-s1: scientific multimodal foundation model. arXiv preprint arXiv:2508.15763, 2025. 1 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin, et al. Qwen2.5-VL Technical Report, 2025. 1, 5 [4] Ellis Brown, Arijit Ray, Ranjay Krishna, Ross Girshick, Rob Fergus, and Saining Xie. SIMS-V: Simulated instructionarXiv preprint tuning for spatial video understanding. arXiv:2511.04668, 2025. 3 [5] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In Proceedbenchmark for human activity understanding. ings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 6, 7 [6] Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. Cgbench: Clue-grounded question answering benchmark for long video understanding, 2024. [7] Xinlong Chen, Yuanxing Zhang, Yushuo Guan, Bohan Zeng, Yang Shi, Sihan Yang, Pengfei Wan, Qiang Liu, Liang Wang, and Tieniu Tan. Versavid-r1: versatile video understanding and reasoning model from question answering to captioning tasks. arXiv preprint arXiv:2506.09079, 2025. 13 [8] Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Junhao Cheng, Ying Shan, and Xihui Liu. Grpo-care: Consistency-aware arXiv reinforcement learning for multimodal reasoning. preprint arXiv:2506.16141, 2025. 2 [9] Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, et al. Scaling rl to long videos. arXiv preprint arXiv:2507.07966, 2025. 1, 2, 8 [10] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 1, 5 [11] Lu Dong, Haiyu Zhang, Han Lin, Ziang Yan, Xiangyu Zeng, Hongjie Zhang, Yifei Huang, Yi Wang, Zhen-Hua Ling, Limin Wang, et al. Videotg-r1: Boosting video temporal grounding via curriculum reinforcement learnarXiv preprint ing on reflected boundary annotations. arXiv:2510.23397, 2025. [12] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 1, 2, 8 [13] Basura Fernando, Hakan Bilen, Efstratios Gavves, and Stephen Gould. Self-supervised video representation learnIn Proceedings of the ing with odd-one-out networks. IEEE conference on computer vision and pattern recognition, pages 36363645, 2017. 2, 3 [14] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 6 [15] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. 6 [16] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. 5 [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, 2, [18] Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, and Si Liu. Videoespresso: large-scale chain-of-thought dataset for fine-grained video reasoning via core frame selection. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2618126191, 2025. 8 [19] Zefeng He, Xiaoye Qu, Yafu Li, Siyuan Huang, Daizong Liu, and Yu Cheng. Framethinker: Learning to think with long videos via multi-turn frame spotlighting. arXiv preprint arXiv:2509.24304, 2025. 2 [20] Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507, 2025. 5 [21] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. 7 9 [22] Siyuan Huang, Xiaoye Qu, Yafu Li, Yun Luo, Zefeng He, Daizong Liu, and Yu Cheng. Spotlight on token perception for multimodal reinforcement learning. arXiv preprint arXiv:2510.09285, 2025. [23] Yuqi Huo, Mingyu Ding, Haoyu Lu, Zhiwu Lu, Tao Xiang, Ji-Rong Wen, Ziyuan Huang, Jianwen Jiang, Shiwei Zhang, Mingqian Tang, et al. Self-supervised video representation learning with constrained spatiotemporal jigsaw. 2021. 3 [24] Longlong Jing and Yingli Tian. Self-supervised visual feaIEEE ture learning with deep neural networks: survey. transactions on pattern analysis and machine intelligence, 43(11):40374058, 2020. 3 [25] Jie Lei, Tamara Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. Advances in Neural Information Processing Systems, 34: 1184611858, 2021. 6, 7 [26] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 6, 7 [27] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal arXiv preprint perception via reinforcement fine-tuning. arXiv:2504.06958, 2025. 1, 2 [28] Daizong Liu, Xiaoye Qu, Yinzhen Wang, Xing Di, Kai Zou, Yu Cheng, Zichuan Xu, and Pan Zhou. Unsupervised temporal video grounding with deep semantic clustering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 16831691, 2022. 3 [29] Daizong Liu, Xiang Fang, Xiaoye Qu, Jianfeng Dong, He Yan, Yang Yang, Pan Zhou, and Yu Cheng. Unsupervised domain adaptative temporal sentence localization with mutual information maximization. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 35673575, 2024. 3 [30] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. [31] Mi Luo, Zihui Xue, Alex Dimakis, and Kristen Grauman. When thinking drifts: Evidential grounding for robust video reasoning. arXiv preprint arXiv:2510.06077, 2025. 6, 12 [32] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of In Eurovisual representations by solving jigsaw puzzles. pean conference on computer vision, pages 6984. Springer, 2016. 2, 3 [33] Mehdi Noroozi, Hamed Pirsiavash, and Paolo Favaro. Representation learning by learning to count. In Proceedings of the IEEE international conference on computer vision, pages 58985906, 2017. 2, 3 [34] OpenAI. Hello GPT-4o. OpenAI Blog, 2024. Accessed: 2024-05-14. 1, 6, 7 [35] OpenAI. Chatgpt, 2025. 1, 2, 5 [36] Kun Ouyang, Yuanxin Liu, Haoning Wu, Yi Liu, Hao Zhou, Jie Zhou, Fandong Meng, and Xu Sun. Spacer: Reinforcing mllms in video spatial reasoning. arXiv preprint arXiv:2504.01805, 2025. [37] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:4274842761, 2023. 2 [38] Xiaoye Qu, Pengwei Tang, Zhikang Zou, Yu Cheng, Jianfeng Dong, Pan Zhou, and Zichuan Xu. Fine-grained iterative attention network for temporal language localization in videos. In Proceedings of the 28th ACM International Conference on Multimedia, pages 42804288, 2020. 2 [39] Qwen. Qwen3-vl, 2025. 1, 5, 6, 7, 13 [40] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics, 1:2536, 2013. 6 [41] Pritam Sarkar and Ali Etemad. Vcrbench: Exploring longform causal reasoning capabilities of large video language models. arXiv preprint arXiv:2505.08455, 2025. 7 [42] Madeline Schiappa, Yogesh Rawat, and Mubarak Shah. Self-supervised learning for videos: survey. ACM Computing Surveys, 55(13s):137, 2023. 3 [43] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 1, 2, 5 [44] Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, et al. Video-lmm post-training: deep dive into video reasoning with large multimodal models. arXiv preprint arXiv:2510.05034, 2025. [45] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 6, 7 [46] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 13 [47] Guodong Wang, Yunhong Wang, Jie Qin, Dongming Zhang, Xiuguo Bao, and Di Huang. Video anomaly detection by In Eusolving decoupled spatio-temporal jigsaw puzzles. ropean Conference on Computer Vision, pages 494511. Springer, 2022. 3 [48] Haonan Wang, Hongfu Liu, Xiangyan Liu, Chao Du, Kenji Kawaguchi, Ye Wang, and Tianyu Pang. Fostering video reasoning via next-event prediction. arXiv preprint arXiv:2505.22457, 2025. 13 [49] Jiangliu Wang, Jianbo Jiao, and Yun-Hui Liu. Selfsupervised video representation learning by pace prediction. In European conference on computer vision, pages 504521. Springer, 2020. 3, 13 [50] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao 10 [63] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 5 [64] Yu Zeng, Wenxuan Huang, Shiting Huang, Xikun Bao, Yukun Qi, Yiming Zhao, Qiuchen Wang, Lin Chen, Zehui Chen, Huaian Chen, et al. Agentic jigsaw interaction learning for enhancing visual perception and reasoning in visionlanguage models. arXiv preprint arXiv:2510.01304, 2025. 3 [65] Congzhi Zhang, Zhibin Wang, Yinchao Ma, Jiawei Peng, Yihan Wang, Qiang Zhou, Jun Song, and Bo Zheng. Rewatchr1: Boosting complex video reasoning in large visionarXiv language models through agentic data synthesis. preprint arXiv:2509.23652, 2025. 1, 3, 8 [66] Jianrui Zhang, Mu Cai, and Yong Jae Lee. Vinoground: Scrutinizing lmms over dense temporal reasoning with short videos. arXiv preprint arXiv:2410.02763, 2024. 6, 7 [67] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models, 2024. 12 [68] Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025. [69] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2, 12 [70] Yuanhan Zhang, Yunice Chew, Yuhao Dong, Aria Leo, Bo Hu, and Ziwei Liu. Towards video thinking test: holistic benchmark for advanced video reasoning and understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2062620636, 2025. 7 [71] Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. 5 [72] Nannan Zhu, Yonghao Dong, Teng Wang, Xueqian Li, Shengjun Deng, Yijia Wang, Zheng Hong, Tiantian Geng, Guo Niu, Hanyan Huang, et al. Cvbench: Evaluating crossvideo synergies for complex multimodal understanding and reasoning. arXiv preprint arXiv:2508.19542, 2025. 7 Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 6 [51] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 1, 5 [52] Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, et al. Time-r1: Post-training large vision language model for temporal video grounding. arXiv preprint arXiv:2503.13377, 2025. [53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 6, 12 [54] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua Tenenbaum, and Chuang Gan. Star: benchmark for situated reasoning in real-world videos. arXiv preprint arXiv:2405.09711, 2024. 2 [55] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 6 [56] Penghao Wu, Yushan Zhang, Haiwen Diao, Bo Li, Lewei Lu, and Ziwei Liu. Visual jigsaw post-training improves mllms. arXiv preprint arXiv:2509.25190, 2025. 3, 5 [57] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 2 [58] Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, and Yueting Zhuang. Self-supervised spatiotemporal learning via video clip order prediction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1033410343, 2019. 2, [59] Zihui Xue, Mi Luo, and Kristen Grauman. Seeing the arrow of time in large multimodal models. arXiv preprint arXiv:2506.03340, 2025. 6 [60] Ziang Yan, Xinhao Li, Yinan He, Zhengrong Yue, Xiangyu Zeng, Yali Wang, Yu Qiao, Limin Wang, and Yi Wang. Videochat-r1. 5: Visual test-time scaling to reinforce multimodal reasoning by iterative perception. arXiv preprint arXiv:2509.21100, 2025. 2 [61] Shusheng Yang, Jihan Yang, Pinzhi Huang, Ellis Brown, Zihao Yang, Yue Yu, Shengbang Tong, Zihan Zheng, Yifan Xu, Muhan Wang, Danhao Lu, Rob Fergus, Yann LeCun, Li FeiFei, and Saining Xie. Cambrian-s: Towards spatial supersensing in video. arXiv preprint arXiv:2511.04670, 2025. 3, 13 [62] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442, 2019."
        },
        {
            "title": "Supplementary Materials",
            "content": "A. Implementation Details A.1. Training Details We utilize Llava-Video [69] as the primary video source for constructing both VideoSSR-30K dataset and VIUBench. During training, we did not employ chain of thought [53] for VideoSSR or any models in the ablation studies. This decision aligns with our focus on enhancing fundamental perceptual abilities, namely, Fine-Grained, Spatial, and Temporal Perception, rather than complex reasoning. This approach also yields greater training efficiency and reduces the potential for model hallucination [31]. A.2. Evaluation Details for VideoSSR A.2.1. Prompts For Video QA tasks, we prompt the model to generate direct answer. The specific prompt template utilized for these tasks is illustrated in Figure 7. For Temporal Grounding tasks, our prompt format is based on the one utilized in the lmms eval library [67], as depicted in Figure 8. While we observed that CharadesSTA seems to be particularly sensitive to prompt phrasing, we nonetheless applied this unified prompt across all benchmarks to ensure fair and consistent evaluation. For other specialized benchmarks, such as VCRBench, we adhere to the official prompts. Figure 7. Prompt template for Video QA tasks Figure 8. Prompt template for Temporal Grounding tasks A.2.2. Benchmarks We adhered to specific evaluation protocols for several benchmarks to ensure fair and accurate assessment. 12 VinoGround: We report the text score, which offers greater discriminative power between models. Video-MME & LongVideoBench: For both benchmarks, evaluations are conducted without the use of subtitles. For LongVideoBench, we specifically test on its validation set. CGBench: Our evaluation is performed on its 3k subset. Temporal Grounding: For benehmarks in this category, the model is required to predict single most likely temporal interval. Results for QVHighlights and ActivityNet are reported on their validation sets. VideoMMMU & Video-TT: We report results on the multiple choice subset to facilitate answer extraction and comparison. CVBench: Our evaluation uses configurations of 32, 48, and 64 frames for each video, resulting in significantly larger total number of frames processed per query. A.2.3. Detailed Results For Temporal Grounding tasks, we provide more detailed breakdown of the results, as detailed in Table 6 and Table 7. A.3. Evaluation Details for VIUBench All evaluations on VIUBench utilized fixed input of 48 frames with maximum resolution of 512 512 pixels. B. Details of Pretext Tasks B.1. Anomaly Grounding Figure 9 illustrates the prompt template used for the Anomaly Grounding task. Table 8 provides the comprehensive list and definitions for all 14 perturbation subtypes designed for this task. The text in the Description column of the table is what replaces the {description} placeholder in the prompt for each respective subtype. Notably, for perturbations targeting Temporal Perception (specifically Slow and Fast), we provided an expanded and highly detailed description within the prompt. This special note, as detailed at the bottom of Table 8, explicitly instructed the model to disregard the evenly spaced frame timestamps and instead rely solely on visual motion cues. Despite this explicit guidance, the models performance on these tasks remained notably poor, as shown in Figure 6. We hypothesize that this is because the base model, Qwen3VL, has strong inherent bias towards relying on textual timestamp information when it is available. Forcing the model to overcome this bias and learn true visual motion Table 6. More results on QVHighlights and ActivityNet."
        },
        {
            "title": "Frames",
            "content": "mIoU R@0.3 R@0.5 R@0.7 mIoU R@0. R@0.5 R@0.7 Qwen3-VL-8B-Instruct [39] VideoSSR-8B (Ours) 32 48 64 32 48 43.7 46.4 48.6 62.3 64.4 64.5 42.5 46.5 48.6 24.2 30.3 33.9 36.5 38.4 39.8 52.3 54.3 55. 34.5 36.4 38.6 18.3 21.0 23.0 59.6(+15.9) 61.1(+14.7) 62.6(+14.0) 83.3(+21.0) 83.5(+19.1) 83.7(+19.2) 66.0(+23.5) 66.9(+20.4) 68.0(+19.4) 43.4(+19.2) 48.3(+18.0) 49.7(+15.8) 42.1(+5.6) 43.0(+4.6) 43.7(+3.9) 63.0(+10.7) 63.2(+8.9) 63.3(+7.7) 41.4(+6.9) 42.3(+5.9) 42.7(+4.1) 21.5(+3.2) 22.7(+1.7) 24.2(+1.2) Table 7. More results on CharadesSTA and Tacos."
        },
        {
            "title": "Frames",
            "content": "mIoU R@0.3 R@0.5 R@0.7 mIoU R@0. R@0.5 R@0.7 Qwen3-VL-8B-Instruct [39] VideoSSR-8B (Ours) 32 48 64 32 48 50.3 50.0 49.2 76.5 76.6 77.1 58.1 56.1 54.2 27.9 26.9 25.5 22.4 25.9 28.1 34.7 39.0 42. 19.2 24.0 26.6 7.1 10.7 12.3 52.1(+1.8) 51.1(+1.1) 49.9(+0.7) 78.2(+1.7) 79.0(+2.4) 78.7(+1.6) 60.6(+2.5) 59.9(+3.8) 57.6(+3.4) 30.8(+2.9) 27.5(+0.6) 24.2(-1.3) 23.1(+0.7) 27.7(+1.8) 30.6(+2.5) 34.1(-0.6) 40.0(+1.0) 43.8(+1.8) 19.8(+0.6) 24.7(+0.7) 28.1(+1.5) 7.4(+0.3) 12.3(+1.6) 14.4(+2.1) perception appears to be significant challenge, even with detailed and explicit prompting. Figure 9. Prompt template for Anomaly Grounding. Figures 12 through 15 illustrate several concrete examples of the Anomaly Grounding task, corresponding to four different perturbation types. For clarity, only subset of key frames from each video is displayed. The models objective is to predict the temporal range of the introduced anomaly based on the visual evidence. B.2. Object Counting Figure 10 illustrates the prompt template used for the Object Counting task. Concrete visual examples of this task are provided in Figure 16 and Figure 17. B.3. Temporal Jigsaw Figure 11 shows the prompt template for the Temporal Jigsaw task. Figure 18 provides concrete visual example of the shuffled video sequence that is presented to the model. For clearer understanding of the task and to provide direct comparison, the corresponding original video with the clips in their correct temporal order is also shown in Figure 19. Figure 10. Prompt template for Object Counting. B.4. Exploration of Alternative Pretext Tasks In addition to the three pretext tasks detailed in the main paper, we also investigated other self-supervised learning paradigms. Our exploration included generative modeling approaches, such as masked [7, 46] frame reconstruction and autoregressive [48, 61] next frame prediction. Furthermore, we experimented with task focused on direct temporal speed prediction [49]. However, our preliminary experiments indicated that these alternative tasks did not yield significant or consistent performance improvements on our downstream evaluation benchmarks. This suggests that while these methods are powerful, their objectives may not be as directly aligned with cultivating the high level perceptual and reasoning skills targeted by our final task selection. The discovery of an even broader range of effective self-supervised 13 Table 8. Definitions of the 14 Perturbation Subtypes for Anomaly Grounding. For temporal perception tasks, an additional detailed note (marked with *) was provided to guide the model."
        },
        {
            "title": "Category Perturbation Type Description",
            "content": "Fine-Grained Perception"
        },
        {
            "title": "Saturation\nNoise\nBlur\nGrayscale\nInvert\nChannel Swap",
            "content": "the colors in the video become oversaturated and unnaturally vibrant. Gaussian noise is added to the video. the video becomes blurry or out of focus. the video becomes black and white. the colors in the video are inverted. the red and blue color channels in the video are swapped."
        },
        {
            "title": "Zoom In\nRotate\nZoom Out\nMirror",
            "content": "the video is zoomed in. the video is rotated 180 degrees. the video is zoomed out. The video is mirrored horizontally."
        },
        {
            "title": "Shuffle",
            "content": "the video slows down, this means the action unfolds at an unusually slow pace, making movements appear prolonged.* the video speeds up, this means the segment plays at high speed, compressing the action and making movements appear jerky or rushed.* the video appears to freeze and stutter on few frames, this means instead of playing smoothly, the video repeatedly freezes on single frame before jumping to the next. the frames are shuffled, this means the order of events is scrambled, making the action appear illogical and chaotic. *Special Note for Slow/Fast perturbations: To ensure fair challenge, even if the videos actual speed changes (e.g., slow motion or fast forward), the timestamps for each frame have been intentionally kept evenly spaced. This creates the illusion of constant playback speed. Therefore, you should not rely on the timestamps when judging the speed. Instead, your judgment must be based solely on the visual content. You should analyze the motion within the video itself by observing how much or how little the scene changes between consecutive frames to determine the true playback speed. C. Limitations and Future Work While our work demonstrates the significant potential of VideoSSR, we also recognize several limitations that present clear opportunities for future research. First, our experiments were primarily conducted using low number of input frames for both training and evaluation. This decision was driven by considerations of computational efficiency, allowing for both rapid iteration on more pretext tasks and comprehensive coverage of evaluation benchmarks. However, this approach may limit the models scalability to long videos. key direction for future work is to scale the VideoSSR framework to handle higher frame rates and longer video inputs. This will be crucial for enhancing the models capabilities on complex, long-form content where dense temporal information is paramount. Second, our framework relies on only three pretext tasks. While effective, this approach overlooks both the potential of broader range of self-supervised objectives and the posFigure 11. Prompt template for Temporal Jigsaw. tasks for enhancing MLLMs remains promising direction for future work. 14 sible synergies that could be unlocked with more sophisticated mixing strategies. Future work could therefore explore richer suite of pretext tasks and investigate advanced mixing techniques like curriculum learning or adaptive task weighting to further enhance model generalization. 15 Figure 12. An example of Channel Swap. The ground truth is 6.9s9.2s. Figure 13. An example of Rotate. The ground truth is 5.1s11.7s. Figure 14. An example of ZoomOut. The ground truth is 5.4s6.9s. 16 Figure 15. An example of Mirror. The ground truth is 14.1s22.6s. 17 Figure 16. An example of Object Counting. The ground truth (circles, squares, and triangles) is 3,2,3. Figure 17. An example of Object Counting. The ground truth (circles, squares, and triangles) is 1,3,1. 18 Figure 18. An example of Temporal Jigsaw. The ground truth is 452316. The corresponding unshuffled video is shown in Figure 19. 19 Figure 19. The original video corresponding to the example in Figure 18."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "Wuhan University"
    ]
}