{
    "paper_title": "RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame Animated Sticker Generation",
    "authors": [
        "Zhiqiang Yuan",
        "Ting Zhang",
        "Ying Deng",
        "Jiapei Zhang",
        "Yeshuang Zhu",
        "Zexi Jia",
        "Jie Zhou",
        "Jinchao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, great progress has been made in video generation technology, attracting the widespread attention of scholars. To apply this technology to downstream applications under resource-constrained conditions, researchers usually fine-tune the pre-trained models based on parameter-efficient tuning methods such as Adapter or Lora. Although these methods can transfer the knowledge from the source domain to the target domain, fewer training parameters lead to poor fitting ability, and the knowledge from the source domain may lead to the inference process deviating from the target domain. In this paper, we argue that under constrained resources, training a smaller video generation model from scratch using only million-level samples can outperform parameter-efficient tuning on larger models in downstream applications: the core lies in the effective utilization of data and curriculum strategy. Take animated sticker generation (ASG) as a case study, we first construct a discrete frame generation network for stickers with low frame rates, ensuring that its parameters meet the requirements of model training under constrained resources. In order to provide data support for models trained from scratch, we come up with a dual-mask based data utilization strategy, which manages to improve the availability and expand the diversity of limited data. To facilitate convergence under dual-mask situation, we propose a difficulty-adaptive curriculum learning method, which decomposes the sample entropy into static and adaptive components so as to obtain samples from easy to difficult. The experiment demonstrates that our resource-efficient dual-mask training framework is quantitatively and qualitatively superior to efficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the feasibility of our method on downstream tasks under constrained resources. Code will be available."
        },
        {
            "title": "Start",
            "content": ":Resource-efficient Dual-mask Training Framework for Multi-frame Animated Sticker Generation"
        },
        {
            "title": "Zhiqiang Yuan",
            "content": ", 1, Ting Zhang ,1, Ying Deng1, Jiapei Zhang1, Yeshuang Zhu1, Zexi Jia1, Jie Zhou1, Jinchao Zhang Pattern Recognition Center, WeChat AI, Tencent1 , 1 5 2 0 2 M 2 2 ]"
        },
        {
            "title": "M\nM",
            "content": ". [ 1 5 3 7 7 1 . 3 0 5 2 : r Figure 1: Exemplary results of animated sticker generation using our proposed approach, RDTF. Our method performs well on different ASG tasks: (Top) Text&image to GIF, (Middle) Prediction, (Bottom) Interpolation. Gray boxes indicate text or visual guidance. Click here to see dynamic results."
        },
        {
            "title": "Abstract",
            "content": "Recently, great progress has been made in video generation technology, attracting the widespread attention of scholars. To apply this technology to downstream applications under resource-constrained conditions, researchers usually fine-tune the pre-trained models based on parameter-efficient tuning methods such as Adapter or Lora. Although these methods can transfer the knowledge from the source domain to the target domain, fewer training parameters lead to poor fitting ability, and the knowledge from the source domain may lead to the inference process deviating from the target domain. In this paper, we argue that under constrained resources, training smaller video generation model from scratch using only million-level samples can outperform parameter-efficient tuning on larger models in downstream applications: the core lies in the effective utilization of data and curriculum strategy. Take animated sticker generation (ASG) as an example, we first construct discrete frame generation network for stickers with low frame rates, ensuring that its parameters meet the requirements of model training under constrained resources. In order to provide data support for models trained from scratch, we come up with dual-mask based data utilization strategy, which manages to improve the availability and expand the diversity of limited data. To facilitate convergence under dual-mask situation, we propose difficulty-adaptive curriculum learning method, which decomposes the sample entropy into static and adaptive components so as to obtain samples from easy to difficult. The experiment demonstrates that our resource-efficient dualmask training framework is quantitatively and qualitatively superior to efficient-parameter tuning methods such as I2V-Adapter and SimDA, verifying the feasibility of our method on downstream tasks under constrained resources. Code will be released at anonymous link https://mails9523.github.io."
        },
        {
            "title": "Introduction",
            "content": "Video generation has made remarkable strides with the success of recent pretrain diffusion models like Imagen(Saharia et al. 2022), Stable Diffusion (SD) XL(Podell et al. 2023), I2VGen-XL(Zhang et al. 2023). Users can write text prompt or use image guidance to generate videos with the powerful pretrained diffusion models. And in some scenarios, researchers often use small amount of data to fine-tune pre-trained models, thus to apply them to downstream applications. Although the pre-training model has brought convenience to users, there are still some problems when the training resources are constrained. For example, on V100 with only 32G display memory, it is challenging to perform full-parameter fine-tuning of I2VGen-XL(Zhang et al. 2023) with 1.8B parameters on specific applications. To address this issue, some works (Guo et al. 2024b; Xing et al. 2024; Guo et al. 2024a) have attempted to fine-tune parameters under constrained resources based on technologies such as Adapter(Houlsby et al. 2019) or Lora(Hu et al. 2021), etc. These parameter-efficient tuning methods can be severed for downstream tasks with only fine-tune small number of parameters, which have attracted extensive attention from researchers in the field of video generation. The video generation method based on parameterefficient tuning can effectively utilize computational resources, however, the shortcomings of the method are also obvious: a. For methods such as Adapter and Lora, only few parameters are involved in the gradient propagation, which may lead to poor fitting ability compared with full-parameter fine-tuning. b. Since the model after parameter-efficient tuning inherits the domain knowledge from the previous pre-training process, the source domain knowledge may cause the inference process to deviate from the target domain. Due to the above issues, the current parameter-efficient tuning methods may not be satisfactory in downstream video generation applications. In this paper, we argue that under constrained computing resources, training smaller video generation model from scratch using only million-level samples can outperform parameter-efficient tuning on larger models in downstream applications: the core lies in the effective utilization of data and curriculum strategy. Effective use of data can expose more patterns to the model, while the curriculum strategy plans the learning path of the model. The combination of the data utilization and curriculum strategies enables the small model to learn robust generative patterns from scratch, thus avoiding the problems caused by parameter-efficient tuning on the large model. We take animated sticker generation (ASG), an application of video generation in the social communication, as an case study to verify the above statement. Firstly, based on the differences between animated stickers and natural videos, we come up with noise prediction model with spatial-temporal interaction layer, and make it satisfy the model training under resource-constrained situations. Secondly, to effectively utilize limited data, we leverage clustering algorithm to increase the information density of the data and put forward dual-mask training method to expose more patterns to the model. Thirdly, we propose difficulty-adaptive curriculum strategy for dual-mask video generation, which decompose the sample entropy into static and adaptive components to maintain the entropy increase of samples, thereby improving the smooth convergence of the model under resource-constrained conditions through curriculum learning. Experiments qualitatively and quantitatively show that our resource-efficient dual-mask training framework (RDTF) significantly outperforms parameter-efficient tuning methods under resource constraints such as SimDA(Xing et al. 2024) and I2VAdapter(Guo et al. 2024a), proving the feasibility of RDTF on downstream applications. The contributions of our work are as follows: We verify that under constrained resources, training smaller video generation model from scratch using only million-level samples can outperform parameter-efficient tuning on larger models in downstream applications. resource-efficient dual-mask training framework is proposed for video generation, which achieves promising results in downstream applications by the effective utilize of data and curriculum strategy. We show generation method for animated stickers, which achieves State-of-The-Art performance compared with SimDA, I2V-Adapter and Customize-A-Video. Related Work Video Generation Model. Most of the earlier video generation methods (Vondrick, Pirsiavash, and Torralba 2016; Saito, Matsumoto, and Saito 2017; Tulyakov et al. 2018; Brooks et al. 2022; Skorokhodov, Tulyakov, and Elhoseiny 2022) are based on adversarial networks (Creswell et al. 2018), which achieve well temporal consistency by allowing the generator to directly learn the joint distribution of video frames. With the development of self-attention mechanism (Vaswani et al. 2017), researchers manage to utilize transformers to model video generation tasks and achieve acceptable computing efficiency and scalability (Yan et al. 2022; Ge et al. 2022; Wu, Yoon, and Ahn 2021; Yan et al. 2021; Hong et al. 2022). In recent years, due to the key advances of Diffusion Probabilistic Models (DPMs) in generation tasks, researchers have attempted to use DPMs for video generation (Ho et al. 2022; Yang, Srivastava, and Mandt 2022; Ni et al. 2023). Most studies optimize this task from DPMs architecture (Luo et al. 2023; Ge et al. 2023; Blattmann et al. 2023) and controllability (Wu et al. 2023; Wang et al. 2023; He et al. 2023), thereby achieving well generation performance. These approaches typically integrate temporal modules with 2D U-Net architectures to adapt to the dynamic nature of video content, which usually requires redesign and full training of volumetric U-Net models. After that, video generation models based on the DiT (Peebles and Xie 2023) architecture have been continuously proposed, and the model capabilities have been improved based on scaling capabilities (Liu et al. 2024; Zheng et al. 2024). Although the above methods bring usable model weights to this task, it is still difficult for users to fine-tune them when training resources are constrained. Figure 2: An overview of resource-efficient dual-mask training framework. We propose discrete frame generation network to model the discreteness between animated sticker frames. Furthermore, the dual masks, i.e., condition mask and loss mask, are designed to improve the availability and expand the diversity of limited data. The difficulty-adaptive curriculum learning is applied to facilitate convergence. Parameter-efficient Tuning for Video Generation. To enable users to better fine-tune open source weights, some works (Guo et al. 2024b; Xing et al. 2024; Guo et al. 2024a) have attempted to achieve parameter tuning with limited resources based on techniques such as Adapter(Houlsby et al. 2019; Pan et al. 2022) or Lora(Hu et al. 2021; Choi et al. 2024), etc. AnimateDiff(Guo et al. 2024b) presented MotionLoRA, which can make the pre-trained motion module adapt to the new motion mode with low training and data collection costs. I2V-Adapter(Guo et al. 2024a) retained the structural integrity of T2I model and its inherent motion module, and effectively links the input conditions to the self-attention mechanism of the model by using the lightweight adapter module. SimDA(Xing et al. 2024) proposed simple diffusion adapter, which only fine-tuned 24M of the 1.1B parameters of T2I model, and made it adapt to video generation applications in parameter-efficient way. Although the video generation method based on the parameter-efficient tuning can effectively save training resources, the fitting ability may be poorer compared to the full parameter finetuning because the gradient propagation involves only few parameters. Curriculum Learning. Drawing inspiration from human learning patterns, curriculum learning (Bengio et al. 2009; Hacohen and Weinshall 2019) is structured approach for model training that begins with simpler tasks (Pentina, Sharmanska, and Lampert 2015) or examples (Bengio et al. 2009) and progressively increases in difficulty. As highlighted by (Bengio et al. 2009), the formulation of curriculum learning can be seen as continuum method (Allgower and Georg 2003), initiating from smoother objective and gradually evolving into less smooth version until it aligns with the original objective function. Within the context of diffusion model literature, curriculum learning has been employed to systematically arrange the sequence of training data types, leveraging prior knowledge about the specific generation tasks being targeted(Tang et al. 2024; Kim et al. 2024). Moreover, consistency models (Song et al. 2023; Song and Dhariwal 2023) adopt curriculum-based approach to discretize noise levels, gradually enhancing the discretization steps of noise levels during the training process. In this paper, we come up with difficulty-adaptive curriculum learning method to promote model convergence under the dual-mask training situation. Animated Sticker Generation. Stickers, as common communication medium on social platforms, have great influence on the dialogue experience (Zhang et al. 2025). Knn-Diffusion(Sheynin et al. 2022) has explored the generation of static stickers and achieved the generation of out-of-distribution stickers by using large-scale retrieval methods. Text-to-Sticker(Sinha et al. 2023) first fine-tuned Emu(Dai et al. 2023) using millions of sticker images collected with weak supervision to induce diversity, and then proposed style tailoring fine-tuning method, which improves the visual quality of sticker generation by jointly fitting content and style distribution. To generate multi-frame stickers, Animated Stickers (Yan et al. 2024) used twostage fine-tuning process: first using weak in-domain data and then utilizing human-machine loop strategy, which can effectively improve the motion quality. In this paper, animated sticker generation is used as an application of video generation to validate the effectiveness of the proposed method."
        },
        {
            "title": "Method",
            "content": "Definitions Assuming paired dataset with samples (x, y) ptrain(x, y), where denotes the video clip with frames and = {xii = 1, 2, ..., N}, is the corresponding caption. DPMs (Song et al. 2020) aims to sample from the distribution p(xi) by denoising and converting samples from Gaussian distribution into target distribution iteratively (Sohl-Dickstein et al. 2015; Song et al. 2020), which can be divided into diffusion process and reverse process. In step during diffusion process, noised image xi = αtxi + (cid:112)(1 αt)ϵ, ϵ (0, Id), where ϵ is Gaussian noise and αt controls the noise ratio at step t. In the reverse process, learnable network Gθ1 t, t) aims to predict the noise and recover the clean image from the noised input xi t. After training, the model starts from pure Gaussian noise xT (0, Id) and samples clean image by iteratively running for steps. is generated by xi (xi Conditional video diffusion model (Batzolis et al. 2021) regards the caption and image as guidance information to generate the paired video clip x. In this case, the model samples from the conditional distribution of p(xy, z), and the learnable network ([xt, y, z], t) parameterized by θ2 utilize the xt, Gθ2 and as input. During training, the loss function can be modeled as: Lt = Gθ2 αtx + (cid:112) (1) where Lt is the loss function and ϵt is the noise at step t. 1 αtϵt, αt) ϵt (y, Discrete Frame Generation Network Due to the lower frame rate and abstract semantics of animated stickers, to better model the animated sticker generation task, we come up with discrete frame generation network (DFGN) as shown in Figure. 2. U-Net architecture (Ronneberger, Fischer, and Brox 2015; Luo et al. 2023) is chosen to build the model, because our experiments show that training DiT-based architecture from scratch (Peebles and Xie 2023; Zheng et al. 2024) does not have well generalization performance in the case of million-level data size. To enhance the models ability of processing discrete frames, we use Spatial Temporal Interaction (STI) layer to model temporal features, which further divides temporal modeling into semantic interaction and detail maintenance. a. Semantic interaction. To selectively focus on regions in other frames when predicting the current frame, we interact the region features in all frames during temporal modeling. Specifically, for the feature RFHWd that needs to be sent to the temporal layer, where F, H, W, represent frame, height, width, and dimension respectively, we first downsample it by γ γ γ times to reduce the interaction complexity and name it as hγ RF γ d. Next, we unfold its temporal and γ RF γ spatial dimensions RF γ d, and utilize self-attention to achieve interaction between different frames. After that, we use upsampling with the same magnification of γ and tensor transformation to restore the transformed hγ back to the original size. b. Detail maintenance. Although semantic interaction can consider global features when making next-frame predictions, the downside of such an interaction process is the lost of image details. To preserve details, we use convolutions with kernel size 1 1 to reduce the attenuation of fine-grained features, and is the kernel size of frame. After extracting the features with different patterns, we use linear layer to get the weights of the two branches, so as to dynamically fuse the semantic and detailed features in the different depth layers after normalization. Image and text guidance are used simultaneously in DFGN. For image guidance, the guided frames are first downsampled to the same size as the VAE encoder output, then concatenated with the noisy images and input to U-Net. Note that we didnt follow the works (Zhang et al. 2023; Voleti et al. 2024) to inject the visual semantics into U-Net through cross-attention, because we found that this would interfere the text controllability in the presence of the condition mask. For the text guidance, we randomly replace the tokens with the symbol # with probability to achieve generalization. After that, CLIP and T5 text encoder extract the text features, which are mapped to the same space through linear layers and fed into the model as text embedding. Dual-mask based Data Utilization Strategy In this subsection, we propose dual-mask based data utilization strategy, which enhances the diversity of the generated models by fully utilizing each frame of the limited data. As shown in Figure. 2, we first design condition mask and switch between different training tasks by masking out appropriate image and text guidance. The condition mask allows us to perform the following tasks simultaneously: interpolation (IPT), pre & post prediction (PDT), text-based & image-based generation (GRT). It should be noted that the condition mask is similar to MCVD(Voleti, Jolicoeur-Martineau, and Pal 2022). However, this paper regards it as one of data utilization strategies, and the implementation details also vary greatly. For more details about condition mask, please refer to Appendix A.1. Condition mask attempts to utilize limited data, but augmenting limited data is also an important direction to improve the generalization of the model. As shown in Figure. 3, the frame numbers of animated stickers data often presents long-tail distribution, i.e., more short frames and fewer long frames. If we follow the traditional method of extracting frames at equal interthis H(PDT,N) and H(,N1) though we can intuitively get the rough judgment of H(GRT,N) H(IPT,N) H(,N) due to gradually information missing, where (TASK, N) denotes masked frame length in task is N, it is still difficult to determine route so as to obtain monotonically increasing information entropy. For come up with difficulty-adaptive curriculum learning strategy, which provides stable increase of entropy during training thus to facilitate convergence. reason, we sample First, for Ht, considering the additivity of entropy, we split it into two independent terms: static component Ht and adaptive comHt. The static ponent component Ht aims to ensure that the sample entropy globally follows monotonically increasing trend, let: IPT Figure 5: The masked frame length and task type during training are independent of each other, which is difficult to determine route so as to obtain entropy increase samples stably. (2) PDT GRT t1 t1 , PDT t1 , GRT IPT where denotes the static probability of task in step t. In this case, the static probability of the interpolation task continues to decay, while the static probability of the prediction and generation tasks with larger information entropy in the target continues to increase, making the overall information entropy in an increasing state. Although the static component can maintain the increase of entropy globally, it cannot maintain the strict increase of entropy in the local area due to the independence of masked frame length and task type. Ht For this reason, we design an adaptive component to handle the change of local entropy, aiming to obtain the difficulty of the current sample by statistically analyzing the historical loss. Specifically, for the loss Lc(t) in step t, we can get the difference Ls(t) between the current loss and the historical loss: (cid:90) Ls(t) = Lc(t) Lc(τ) (3) 0 Ls(t) and the next-sample difficulty can be regarded as dynamic system. In order to make system output Ls(t) more stable, we use feedback loop to obtain the joint perplexity of the next-sample: = KpLs(t) + Ki (cid:90) 0 Ls(τ)dτ + Kd dt Ls(t) (4) where Kp, Ki, and Kd are constants. Next, we divide generative tasks into different levels according to the Figure 3: Frame distribution in collected sticker dataset, which follows the long-tail distribution, i.e., more short frames and fewer long frames. Figure 4: Frame extraction algorithm based on feature clustering. During training, data are clustered into clusters randomly to increase the information density. vals to generate data, less data will be generated and most frames will be lost. To improve the information density, as shown in Figure. 4, we use the feature clustering algorithm to cluster the data into random clusters during training, so as to expose more data patterns to the model. Next, we design loss mask and attempt to calculate the loss based only on the first frames. The above strategy allows us to effectively utilize short frame data, and improve the information density of long frame data due to the randomness of k. The dual-mask, i.e., condition mask and loss mask, improve the availability and expand the diversity of limited data, so as to provide data support for models trained from scratch. Difficulty-adaptive Curriculum Learning In addition to data support, learning strategies are also essential for model training. Curriculum learning(Bengio et al. 2009), inspired by human learning patterns, is method of training models in structured sequence, starting with easier tasks or examples and gradually increasing the difficulty. In curriculum learning, by gradually increasing the entropy of the sample, i.e., Ht > Ht1, the learning difficulty of the model can be gradually increased, where is the training step. However, in our training framework, as shown in Figure. 5, the masked frame length and task type, which is controlled by condition mask and loss mask, are independent of each other. Aldifference between loss mask and condition mask, and use the output to let the model dynamically select the task level thus to obtain the adaptive probability of different tasks IPT,N , PDT,N . The adapt tive component brings certain damping to the system difficulty, which can produce more stable growth of sample entropy compared to the static component alone. , and GRT,N After obtaining the static probability and adaptive probability, we balance them to select the condition mask and loss mask, thus to select more suitable samples in training. Difficulty-adaptive curriculum learning strategy splits information entropy into static and adaptive components, enabling the sample difficulty can be adjusted according to the current loss situation, so as to facilitate convergence. For more algorithm details, please refer to Appendix A.1."
        },
        {
            "title": "Experiments",
            "content": "three parameter-efficient Experiment Details Models & Training. For comparison methods, we select tuning methods Customize-A-Video(Ren et al. 2024), I2V-Adapter(Guo et al. 2024a), and SimDA(Xing et al. 2024), due to their well results and reproducibility. The three comparison methods in implementation follow the structural design in the original paper. We use I2VGen-XL(Zhang et al. 2023) as the initial weights of these models and fine-tune them on the sticker domain. During inference, we fix the frame number to 8 and the output size to 256 256. The number of DDIM (Song, Meng, and Ermon 2020) sampling steps is 25. For more network configuration of RDTF, please refer to Appendix A.2. Dataset & Metrics. We collect 1.8 animated stickers as training data and used 500 samples as the testset to evaluate each model(Yuan et al. 2024). FVD (Unterthiner et al. 2018) and CLIP similarity (Wu et al. 2021) are utilized to assess the distribution consistency and semantic correlation of our models. All the texts in the testset are considered as candidates for CLIP similarity calculation. Moreover, we use video quality assessment (VQA) (Wu et al. 2022) to evaluate the video quality based on contextual relations. Quantitative & Qualitative Results Table 1 shows the quantitative comparison of different methods on I&TV task. Our method achieves the best in VQA and FVD metrics, which verify the detail preservation and semantic enrichment of RDTF. Regarding the CLIP indicator, our method falls slightly behind Customize-A-Video, but the difference is minor. The sub-optimal CLIP results could be attributed to significant discrepancies between the generated frames, causing lack of cross-modal similarity. In addition, Table 2 shows the evaluation results on interpolation and pre/post prediction task, where I2VAdapter and SimDA are retrained for each task. Compared with other methods, RDTF still leads on image quality and diversity in these two tasks. It is worth noting that our method, which only uses the same weights for different tasks, can support the guidance of multiple conditions, and is ahead of other parameterefficient fine-tuning methods in different cases. For parameters and time consumption comparison of different methods, please see Appendix A.3. Indicator Customize-A-Video FVD VQA CLIP 451.83 0.479 0.377 I2V-Adapter SimDA Ours 442.18 0.502 0.376 456.24 0.476 0.367 448.36 0.462 0.372 Table 1: Quantitative comparison of different methods on I&TV task under constrained resources. Bold and underline indicate the best and the second-best, respectively. Method I2V-Adapter SimDA Ours FVD VQA 0.517 213.71 0.509 198.49 0.536 192.62 Method I2V-Adapter SimDA Ours FVD VQA 0.526 79.34 0.521 74.68 0.539 67.92 Table 2: Quantitative comparison of different methods for (left) interpolation and (right) prediction task under constrained resources. Bold and underline indicate the best and the second-best, respectively. Figure. 1 shows examples of using RDTF for generation, prediction, and interpolation task, which demonstrates that our method performs well on different tasks, reflecting the ability to model smooth motion. Figure. 6 shows two sets of visualization results sampled by different methods on I&TV task. The motion obtained by RDTF method is smoother and sharper than other methods. Due to the dual mask strategy, RDTF can also obtain results based on different condition types. Figure. 7 shows two sets of comparisons between SimDA and our method on interpolation and prediction tasks. In the interpolation task, our results are smoother and have fewer defects than SimDA. In the prediction task, RDTF produces more diverse results, which is consistent with the FVD results. Ablative Study Module Effectiveness. The ablation results of different modules are shown in Table 3. Compared with DFGN which uses 3D convolution as the temporal layer, using STI layer brings greater gains to the network in terms of FVD and VQA indicators. After adding the dual-mask based data utilization (DDU) strategy, due to the diversity of data and the increase of information density, it has brought positive benefits to the model. DCL produces easy-to-difficult samples during training, which leads to better convergence with the help of curriculum learning. Learning Strategy. As shown in Table 4, to verify the effectiveness of the proposed difficulty-adaptive curriculum learning strategy, we use the RDTF framework without curriculum learning and the linear Figure 6: Visual comparison for animated sticker generation on I&TV task between Customize-A-Video, I2V-Adapter, SimDA and RDTF. Compared with other methods, the motion obtained by RDTF method is smoother and clearer than others. Click here to see dynamic results. Method (I&TV) DFGN(3D-Conv) DFGN DFGN (w/ DDU) DFGN (w/ DDU & w/ DCL) FVD VQA 0.397 492.31 0.435 478.47 0.476 459.21 0.502 442.18 Table 3: Module ablation results. DFGN(3D-Conv) represents DFGN using 3D convolution as the temporal layer. DDU stands for dual-mask based data utilization strategy and DCL stands for difficulty-adaptive curriculum learning. Method (I&TV) RDTF (w/o CL) RDTF (w/ LCL) RDTF (w/ DCL) FVD VQA 0.476 459.21 0.484 451.65 0.502 442. Table 4: (Left) Quantitative comparison when using different curriculum learning strategies. W/o CL, LCL, and DCL denote without curriculum learning, monotonous curriculum learning, and difficultyadaptive curriculum learning, respectively. (Right) Demonstration of task weight changes in linear curriculum learning. Compared with the LCL strategy, the DCL strategy makes adaptive adjustments based on historical losses when calculating sample entropy, thereby achieving more stable convergence. Perceptual User Study During the user study, we utilize the comparison methods to create animated stickers. Afterwards, participants are requested to rank the stickers individually on text alignment, interesting, and motion smoothness. To measure preference, we use the Average User Figure 7: Visual comparison for interpolation and prediction tasks between SimDA and RDTF. Gray boxes indicate visual guidance. curriculum learning (LCL) strategy for comparison. Among them, LCL attempts to gradually reduce the probability of the interpolation task and increase the probability of the prediction and generation tasks, thereby obtaining globally increasing sample entropy. Compared with RDTF without curriculum learning, RDTF with LCL strategy has improved in both FVD and VQA, indicating that learning from easy to difficult is beneficial to the model. Compared with RDTF using LCL strategy, RDTF using DCL strategy achieves better results. The reason is that DCL strategy can produce stable increasing sample entropy thus to improve model convergence. Figure. 8(left) shows the loss comparison between RDTF and SimDA during training. Compared with RDTF, the loss of SimDA decreases slowly after 150k steps due to fewer training parameters, which reflects that RDTF can better fit the target domain distribution in downstream applications. Figure. 8(right) shows the loss comparison when using linear and difficultyadaptive curriculum learning strategy to train RDTF. Figure 8: (Left) Loss comparison during training between RDTF and SimDA. (Right) Loss comparison when using linear (LCL) and difficulty-adaptive (DCL) curriculum learning strategy to train RDTF. Method (I&TV) Customize-A-Video I2V-Adapter SimDA RDTF Text. 2.10 1.90 1.97 2.03 Interest. Motion. 1.87 1.73 2.06 2.34 1.94 1.71 2.03 2. Table 5: User study results. higher score indicates superior performance. Bold and underline indicate the best and the second-best, respectively. Ranking (AUR) to evaluate the performance of methods, where higher score indicates better performance. Noted that relevant prompts and images are given for reference to ensure text alignment evaluation. Compared with other methods, RDTF is slightly inferior to Customize-A-Video in terms of text alignment, which may come from the high dispersion of RDTF results. In terms of user interest and motion smoothness, our model is ahead of other models, indicating that RDTF can better fit the distribution of animated stickers and obtain smoother results. However, the ASG task still faces major challenges, and we discuss this and possible future work in Appendix A.4. Strengths and Limitations This paper verifies that in downstream applications, e.g., animated sticker generation, the effective utilization of data and curriculum strategy can enable smallscale video generation models to outperform large models optimized by parameter-efficient tuning methods under resource constraints. The above conclusions enlighten us that under the limited resources, we can design model suitable for specific application and achieve better results from scratch only through data utilization and effective learning strategies, which can greatly reduce the reliance on pretrained models. However, since this solution requires training from scratch, the training time and the required video memory is larger than that of efficient parameter tuning, which can be regarded as trade-off between video memory and time consumption. Conclusion We present resource-efficient dual-mask training framework for animated sticker generation. The dualmask based data usage strategy is proposed to enhance the availability and diversity of data, and the difficulty-adaptive curriculum learning is designed to facilitate convergence by obtaining samples of progressively increasing difficulty. Compared to other tuning methods under constrained resources such as I2V-Adapter, SimDA, etc., RDTF quantitatively and qualitatively achieves SoTA results on the ASG task."
        },
        {
            "title": "References",
            "content": "Allgower, E. L.; and Georg, K. 2003. numerical continuation methods. SIAM. Introduction to Batzolis, G.; Stanczuk, J.; Sch onlieb, C.-B.; and Etimage generation mann, C. 2021. Conditional with score-based diffusion models. arXiv preprint arXiv:2111.13606. Bengio, Y.; Louradour, J.; Collobert, R.; and Weston, J. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, 4148. Blattmann, A.; Rombach, R.; Ling, H.; Dockhorn, T.; Kim, S. W.; Fidler, S.; and Kreis, K. 2023. Align your latents: High-resolution video synthesis In Proceedings of with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2256322575. Brooks, T.; Hellsten, J.; Aittala, M.; Wang, T.-C.; Aila, T.; Lehtinen, J.; Liu, M.-Y.; Efros, A.; and Karras, T. 2022. Generating long videos of dynamic scenes. Advances in Neural Information Processing Systems, 35: 31769 31781. Choi, J. Y.; Park, J. R.; Park, I.; Cho, J.; No, A.; and Ryu, E. K. 2024. Simple Drop-in LoRA Conditioning on Attention Layers Will Improve Your Diffusion Model. arXiv preprint arXiv:2405.03958. Creswell, A.; White, T.; Dumoulin, V.; Arulkumaran, K.; Sengupta, B.; and Bharath, A. A. 2018. Generative adversarial networks: An overview. IEEE signal processing magazine, 35(1): 5365. Dai, X.; Hou, J.; Ma, C.-Y.; Tsai, S.; Wang, J.; Wang, R.; Zhang, P.; Vandenhende, S.; Wang, X.; Dubey, A.; et al. 2023. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807. Ge, S.; Hayes, T.; Yang, H.; Yin, X.; Pang, G.; Jacobs, D.; Huang, J.-B.; and Parikh, D. 2022. Long video generation with time-agnostic vqgan and time-sensitive In European Conference on Computer transformer. Vision, 102118. Springer. Ge, S.; Nah, S.; Liu, G.; Poon, T.; Tao, A.; Catanzaro, B.; Jacobs, D.; Huang, J.-B.; Liu, M.-Y.; and Balaji, Y. 2023. Preserve your own correlation: noise prior for video In Proceedings of the IEEE/CVF diffusion models. International Conference on Computer Vision, 22930 22941. Guo, X.; Zheng, M.; Hou, L.; Gao, Y.; Deng, Y.; Wan, P.; Zhang, D.; Liu, Y.; Hu, W.; Zha, Z.; et al. 2024a. I2VAdapter: General Image-to-Video Adapter for Diffusion Models. In ACM SIGGRAPH 2024 Conference Papers, 112. Guo, Y.; Yang, C.; Rao, A.; Liang, Z.; Wang, Y.; Qiao, Y.; Agrawala, M.; Lin, D.; and Dai, B. 2024b. AnimateDiff: Animate Your Personalized Text-toImage Diffusion Models without Specific Tuning. In The Twelfth International Conference on Learning Representations. Hacohen, G.; and Weinshall, D. 2019. On the power of curriculum learning in training deep networks. In International conference on machine learning, 2535 2544. PMLR. He, Y.; Xia, M.; Chen, H.; Cun, X.; Gong, Y.; Xing, J.; Zhang, Y.; Wang, X.; Weng, C.; Shan, Y.; et al. 2023. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940. Ho, J.; Chan, W.; Saharia, C.; Whang, J.; Gao, R.; Gritsenko, A.; Kingma, D. P.; Poole, B.; Norouzi, M.; Fleet, D. J.; et al. 2022. Imagen video: High definition video arXiv preprint generation with diffusion models. arXiv:2210.02303. Hong, W.; Ding, M.; Zheng, W.; Liu, X.; and Tang, J. 2022. Cogvideo: Large-scale pretraining for textto-video generation via transformers. arXiv preprint arXiv:2205.15868. Houlsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.; De Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly, S. 2019. Parameter-efficient transfer learning for NLP. In International conference on machine learning, 27902799. PMLR. Hu, E. J.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; and Wang, L. 2021. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. Hu, M.; Xia, P.; Wang, L.; Yan, S.; Tang, F.; Xu, Z.; Luo, Y.; Song, K.; Leitner, J.; Cheng, X.; et al. 2024. Ophnet: large-scale video benchmark for ophthalmic surgical workflow understanding. In European Conference on Computer Vision, 481500. Springer. Kim, J.-Y.; Go, H.; Kwon, S.; and Kim, H.-G. 2024. Denoising Task Difficulty-based Curriculum for Training Diffusion Models. arXiv preprint arXiv:2403.10348. Liu, Y.; Zhang, K.; Li, Y.; Yan, Z.; Gao, C.; Chen, R.; Yuan, Z.; Huang, Y.; Sun, H.; Gao, J.; et al. 2024. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177. Luo, Z.; Chen, D.; Zhang, Y.; Huang, Y.; Wang, L.; Shen, Y.; Zhao, D.; Zhou, J.; and Tan, T. 2023. VideoFusion: Decomposed Diffusion Models for HighIn Proceedings of the Quality Video Generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1020910218. Ni, H.; Shi, C.; Li, K.; Huang, S. X.; and Min, M. R. 2023. Conditional Image-to-Video Generation with In Proceedings of Latent Flow Diffusion Models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1844418455. Pan, J.; Lin, Z.; Zhu, X.; Shao, J.; and Li, H. 2022. Stadapter: Parameter-efficient image-to-video transfer learning. Advances in Neural Information Processing Systems, 35: 2646226477. Peebles, W.; and Xie, S. 2023. Scalable diffusion models In Proceedings of the IEEE/CVF with transformers. International Conference on Computer Vision, 4195 4205. Pentina, A.; Sharmanska, V.; and Lampert, C. H. 2015. Curriculum learning of multiple tasks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 54925500. Podell, D.; English, Z.; Lacey, K.; Blattmann, A.; Dockhorn, T.; uller, J.; Penna, J.; and Rombach, Sdxl: Improving latent diffusion models R. 2023. for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. Ren, Y.; Zhou, Y.; Yang, J.; Shi, J.; Liu, D.; Liu, F.; Kwon, M.; and Shrivastava, A. 2024. Customize-a-video: Oneshot motion customization of text-to-video diffusion models. arXiv preprint arXiv:2402.14780. Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, 234241. Springer. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan, B.; Salimans, T.; et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35: 3647936494. Saito, M.; Matsumoto, E.; and Saito, S. 2017. Temporal generative adversarial nets with singular value clipping. In Proceedings of the IEEE international conference on computer vision, 28302839. Sheynin, S.; Ashual, O.; Polyak, A.; Singer, U.; Gafni, O.; Nachmani, E.; and Taigman, Y. 2022. Knn-diffusion: Image generation via large-scale retrieval. arXiv preprint arXiv:2204.02849. Sinha, A.; Sun, B.; Kalia, A.; Casanova, A.; Blanchard, E.; Yan, D.; Zhang, W.; Nelli, T.; Chen, J.; Shah, H.; et al. 2023. Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human Expression. arXiv preprint arXiv:2311.10794. I.; Tulyakov, S.; and Elhoseiny, M. Skorokhodov, Stylegan-v: continuous video generator 2022. image quality and perks of stylewith the price, gan2. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 36263636. DearXiv preprint Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and Ganguli, S. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, 22562265. PMLR. Song, J.; Meng, C.; and Ermon, S. 2020. noising diffusion implicit models. arXiv:2010.02502. Song, Y.; and Dhariwal, P. 2023. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189. Song, Y.; Dhariwal, P.; Chen, M.; and Sutskever, I. 2023. Consistency models. Song, Y.; Sohl-Dickstein, J.; Kingma, D. P.; Kumar, A.; Ermon, S.; and Poole, B. 2020. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456. Tang, Z.; Yang, Z.; Zhu, C.; Zeng, M.; and Bansal, M. 2024. Any-to-any generation via composable diffusion. Advances in Neural Information Processing Systems, 36. Tulyakov, S.; Liu, M.-Y.; Yang, X.; and Kautz, J. 2018. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, 15261535. Unterthiner, T.; Van Steenkiste, S.; Kurach, K.; Marinier, R.; Michalski, M.; and Gelly, S. 2018. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. Advances in neural information processing systems, 30. Voleti, V.; Jolicoeur-Martineau, A.; and Pal, C. 2022. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. Advances in information processing systems, 35: 23371 neural 23385. Voleti, V.; Yao, C.-H.; Boss, M.; Letts, A.; Pankratz, D.; Tochilkin, D.; Laforte, C.; Rombach, R.; and Jampani, V. 2024. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. arXiv preprint arXiv:2403.12008. Vondrick, C.; Pirsiavash, H.; and Torralba, A. 2016. Generating videos with scene dynamics. Advances in neural information processing systems, 29. Wang, X.; Yuan, H.; Zhang, S.; Chen, D.; Wang, J.; Zhang, Y.; Shen, Y.; Zhao, D.; and Zhou, J. 2023. VideoComposer: Compositional Video Synthesis with Motion Controllability. arXiv preprint arXiv:2306.02018. Wen, Y.; Zhao, Y.; Liu, Y.; Jia, F.; Wang, Y.; Luo, C.; Zhang, C.; Wang, T.; Sun, X.; and Zhang, X. 2024. Panacea: Panoramic and controllable video generaIn Proceedings of tion for autonomous driving. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 69026912."
        },
        {
            "title": "In Proceedings of",
            "content": "Wu, C.; Huang, L.; Zhang, Q.; Li, B.; Ji, L.; Yang, F.; Sapiro, G.; and Duan, N. 2021. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806. Wu, H.; Chen, C.; Hou, J.; Liao, L.; Wang, A.; Sun, W.; Yan, Q.; and Lin, W. 2022. Fast-vqa: Efficient end-toend video quality assessment with fragment sampling. In European Conference on Computer Vision, 538554. Springer. Wu, J. Z.; Ge, Y.; Wang, X.; Lei, S. W.; Gu, Y.; Shi, Y.; Hsu, W.; Shan, Y.; Qie, X.; and Shou, M. Z. 2023. Tunea-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 76237633. Wu, Y.-F.; Yoon, J.; and Ahn, S. 2021. Generative video transformer: Can objects be the words? In International Conference on Machine Learning, 11307 11318. PMLR. Xing, Z.; Dai, Q.; Hu, H.; Wu, Z.; and Jiang, Y.- Simda: Simple diffusion adapter for efG. 2024. ficient video generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 78277839. Yan, D.; Zhang, W.; Zhang, L.; Kalia, A.; Wang, D.; Ramchandani, A.; Liu, M.; Pumarola, A.; Schoenfeld, E.; Blanchard, E.; et al. 2024. Animated Stickers: Bringing Stickers to Life with Video Diffusion. arXiv preprint arXiv:2402.06088. Yan, W.; Okumura, R.; James, S.; and Abbeel, P. 2022. Patch-based Object-centric Transformers for Efficient Video Generation. arXiv preprint arXiv:2206.04003. Yan, W.; Zhang, Y.; Abbeel, P.; and Srinivas, A. 2021. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157. Yang, R.; Srivastava, P.; and Mandt, S. 2022. Diffusion probabilistic modeling for video generation. arXiv preprint arXiv:2203.09481. Yuan, Z.; Zhang, J.; Deng, Y.; Zhu, Y.; Zhou, J.; and Zhang, J. 2024. VSD2M: Large-scale Vision-language Sticker Dataset for Multi-frame Animated Sticker Generation. arXiv preprint arXiv:2412.08259. Zhang, S.; Wang, J.; Zhang, Y.; Zhao, K.; Yuan, H.; Qin, Z.; Wang, X.; Zhao, D.; and Zhou, J. 2023. I2vgenxl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145. Zhang, T.; Yuan, Z.; Zhu, Y.; Zhou, J.; and Zhang, J. 2025. ILDiff: Generate Transparent Animated Stickers by Implicit Layout Distillation. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 15. IEEE. Zheng, Z.; Peng, X.; Yang, T.; Shen, C.; Li, S.; Liu, H.; Zhou, Y.; Li, T.; and You, Y. 2024. Open-Sora: Democratizing Efficient Video Production for All. Appendix.1 Details of Difficulty-adaptive Curriculum Learning Algorithm 1: Difficulty-adaptive Curriculum Learning Require: Static component function fs, Adaptive difficulty map mad, Dual-mask function mdual Require: Constants Kp, Ki, Kd, Data Require: Target network (θ), Training step Te, Current loss Lc Ensure: Target network 1: for 1 to Te do 2: , GRT , PDT fs(t) Calculation of static components IPT Calculation of adaptive components Ls(t) Lc(t) (cid:82) KpLs(t) + Ki , PDT,N IPT,N Training on masked sample Xt mdual(X Train on Xt 0 Lc(τ)dτ (cid:82) 0 Ls(τ)dτ + Kd mad( ) , (P, )) , GRT,N 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: end for 13: return dt Ls(t) Algorithm 1 shows the specific implementation of the proposed difficulty-adaptive curriculum learning. We first calculate the probability of samples on the masked frame length and condition task for the static component and the adaptive component. Next, the probabilities are used to mask the extracted samples for training the model. The loss and the mask degree of the sample in the iterative process are regarded as loop, and more stable growth of sample entropy is achieved by adaptively balancing the loop. Hyperparameter Spatial Layers Architecture LDM Latent shape Input channels Output channels Layers per block Head channels Condition Embedding dimension Text sequence length STI Layers Architecture Convolution kernel size Convolution layers Num attention layers Num attention heads Attention head dim Dual-mask based Data Utilization Strategy range of Difficulty-adaptive Curriculum Learning Kp Ki Kd Training Parameterization Learnable para. # train steps Learning rate Batch size per GPU # GPUs GPU-type Diffusion Setup Diffusion steps Noise schedule β0 βT Sampling Parameters Sampler Steps η Text guidance scale Img guidance scale RDTF 8 32 32 4 8 4 2 64 1024 77 3,1,1 2 1 8 32 3-8 5.0 3.0 1. ϵ Full 50K 1e-5 1 16 A100-40GB 1000 Linear 0.00085 0.0120 DDIM 50 1.0 7.5 7.5 Appendix.2 Implementation Details Table 6: Hyperparameters for our RDTF. All architecture parameter details, diffusion process details, as well as training hyper-parameters are provided in Table. 6, which should be mostly selfexplanatory. Table 7 shows the condition mask used in this paper. In the implementation of the comparative experiment, we use I2VGen-XL(Zhang et al. 2023) as the basic model and add different Adapters to reproduce these method, which enables these fine-tune methods to have well initial weights. For example, when implementing the SimDA method(Xing et al. 2024), we refer to the Temporal Adapter and Spatial Adapter mentioned in the paper, add them to each U-Net Block in I2VGen-XL, and set the newly added modules to be learnable. Appendix.3 Resource Consumption Comparison Table 8 shows the comparison of model parameters and inference time. In this experiment, the inference time refers to the duration between tensor passing through U-Net. The experiments are performed on single V100 graphics card with no load to sample 8frame GIFs in 256 size, and each set of experiments is conducted hundreds of times to obtain average results. Compared with other models, our method has the smallest model size and the shortest inference time. Task Text Guidance Visual Condition mask Interpolation Prediction Generation (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) [0 1 0 1 0 1 0 1] [1 0 1 0 1 0 1 0] [1 0 0 1 0 0 1 0] [1 1 1 1 1 0 0 0] [0 0 1 1 1 1 1 1] [1 0 0 0 0 0 0 0] [0 0 1 0 0 0 0 0] [0 0 1 0 0 0 0 0] [1 0 0 0 0 0 0 0] ... ... ... Table 7: Different conditional mask definitions. Param. (B) Infer Time (s) Customize-A-Video (Ren et al. 2024) 1.19 0.42 I2V-Adapter (Guo et al. 2024a) 1.21 0.42 SimDA (Xing et al. 2024) 1.32 0.43 Ours 0.96 0. Table 8: Parameters and inference costs comparison of different methods, which is evaluated on inputs of shape 1 8 256 256 with single V100. Appendix.4 Generalization Performance of the Proposed Methods This paper aims to explore how to use less data to finetune the model, thereby achieving better model performance. Given that animated sticker is subfield of video, this paper takes it as research task, designs the specific model architecture for it, and adopts dualmask based data utilization strategy and difficultyadaptive curriculum learning to promote the models learning. We found that such method is beneficial for the animated sticker generation task, so the paper title specifically emphasizes for Multi-frame Animated Sticker Generation. Indicator Customize-A-Video FVD VQA 296.2 0.584 I2V-Adapter 304.7 0. SimDA Ours 273.1 0.612 291.7 0.596 Table 9: Comparison of different model test results on OphNet2024 dataset (Hu et al. 2024) in medical video generation task. Indicator Customize-A-Video FVD VQA 320.8 0. I2V-Adapter 311.9 0.347 SimDA Ours 300.4 0.364 305.6 0.360 Table 10: Comparison of different model test results on unScene dataset (Wen et al. 2024) in autonomous driving video generation task. To verify the generalization of the method, we also verified the advantages of this method in other subtasks as shown in Table 9 and Table 10. We followed the experimental setup in Section III.A of the paper and selected medical video generation and autonomous driving video generation as two subtasks. It should be noted that in these tasks, we only used the dualmask based data utilization strategy and difficultyadaptive curriculum learning, and did not use the designed discrete frame generation network for animated sticker. a. Medical video generation. We selected the OphNet2024 dataset (Hu et al. 2024) as the experimental data, split the data into frames of 8 as one clip, and obtained total of 1.1 million training data and 2k validation data. Since the data has no text annotation, we only report the FVD and VQA metrics on the validation set. In medical image generation, compared to other methods, our method achieved the smallest FVD, verifying that the method can generate videos that are most consistent with the validation set distribution. Our method also achieved the highest VQA metric, proving that the video quality generated by our method is the highest. b. Autonomous driving video generation. We also conducted experiments on autonomous driving video generation using the nuScenes dataset(Wen et al. 2024), and the experimental settings are consistent with the above. This dataset is smaller, and we used it to produce about 120k 8-frame video training data, and also collected 1k validation samples. As shown in Table 10, our method still achieved the best results on the FVD and VQA metrics. However, it should be noted that SimDA is basically close to our method, which indicates that when the data volume is small, our method may face challenges. Appendix.5 Challenges and Future Works for Animated Sticker Generation As novel task that urgently needs to be explored, ASG task plays an important role in user interaction and chat communities. However, compared with video generation task in general fields, ASG task present some unique challenges: Sticker usually covers large number of scenes (i.e. cartoon and real scenes), while for specific scene, they contain action, characters, etc. that are often interesting and rare for generic scenes. On the other hand, stickers always contain large amount of optical characters to highlight the theme, and the distribution of these texts is messy and difficult to model. In this scenario, the generative model may need to adapt to scaling law by employing more parameters to handle such distribution. For animated stickers, the diversity and abstraction of content makes it difficult to divide the actions at fine-grained level, which makes the model lacks perception of motion during learning. In addition, stickers are difficult to describe. For example, dog in one sticker is greatly different from another, and the text cannot be fully used to better distinguish. How to control the fine-grained subject and motion in the sticker is one of the urgent issues that need to be studied. Under the above challenges, there are also some corresponding opportunities in ASG field: Collecting larger-scale data supervised methods and using selfsupervised or to obtain pre-trained models for ASG task is an important milestone. By learning the common features in stickers, the pre-trained model can quickly iterate on downstream tasks, thus greatly promoting the development of this field. The cartoon stickers usually consist of simple lines and color blocks, with much less texture than the nature scene. While for lines and blocks, the two distributions are extremely different in the frequency domain, and it may be necessary to disassemble and reconstruct them separately. The creation of artificial stickers is often based on the process of sketching-coloring, and how to model ASG task based on this process is also promising direction. Whether the sticker generation can be broken down into sketching and coloring, thereby reducing the modeling difficulty and improving the sample quality, is an interesting approach that needs to be explored. Similar to natural scenes, how to achieve detailed control in the generation process is also an indispensable part in intelligent creation. Characterization of subjects and modeling of action will inevitably become one of the bottlenecks in generating high-quality stickers."
        }
    ],
    "affiliations": [
        "Pattern Recognition Center, WeChat AI, Tencent"
    ]
}