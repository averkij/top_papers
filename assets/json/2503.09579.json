{
    "paper_title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
    "authors": [
        "Yingfa Chen",
        "Yutong Wu",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building effective and efficient Transformer-based large language models (LLMs) has recently become a research focus, requiring maximizing model language capabilities and minimizing training and deployment costs. Existing efforts have primarily described complex relationships among model performance, parameter size, and data size, as well as searched for the optimal compute allocation to train LLMs. However, they overlook the impacts of context length and attention head configuration (the number of query and key-value heads in grouped-query attention) on training and inference. In this paper, we systematically compare models with different parameter sizes, context lengths, and attention head configurations in terms of model performance, computational cost, and memory cost. Then, we extend the existing scaling methods, which are based solely on parameter size and training compute, to guide the construction of cost-optimal LLMs during both training and inference. Our quantitative scaling studies show that, when processing sufficiently long sequences, a larger model with fewer attention heads can achieve a lower loss while incurring lower computational and memory costs. Our findings provide valuable insights for developing practical LLMs, especially in long-context processing scenarios. We will publicly release our code and data."
        },
        {
            "title": "Start",
            "content": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs Yingfa Chen1*, Yutong Wu2*, Xu Han1, Zhiyuan Liu1, Maosong Sun1 1NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China 2SIST, University of Science and Technology Beijing, Beijing, China chenyingfa1999@gmail.com, wuyutong_yuna@163.com"
        },
        {
            "title": "Abstract",
            "content": "Building effective and efficient Transformerbased large language models (LLMs) has recently become research focus, requiring maximizing model language capabilities and minimizing training and deployment costs. Existing efforts have primarily described complex relationships among model performance, parameter size, and data size, as well as searched for the optimal compute allocation to train LLMs. However, they overlook the impacts of context length and attention head configuration (the number of query and key-value heads in grouped-query attention) on training and inference. In this paper, we systematically compare models with different parameter sizes, context lengths, and attention head configurations in terms of model performance, computational cost, and memory cost. Then, we extend the existing scaling methods, which are based solely on parameter size and training compute, to guide the construction of cost-optimal LLMs during both training and inference. Our quantitative scaling studies show that, when processing sufficiently long sequences, larger model with fewer attention heads can achieve lower loss while incurring lower computational and memory costs. Our findings provide valuable insights for developing practical LLMs, especially in long-context processing scenarios. We will publicly release our code and data.1 5 2 0 2 2 1 ] . [ 1 9 7 5 9 0 . 3 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "It is well established that increasing the size of large language models (LLMs) can improve their language modeling qualities (Hestness et al., 2017; Kaplan et al., 2020). Minimizing model size while maintaining quality is crucial for efficiency and cost-effectiveness (Hoffmann et al., 2022; Hu et al., 1The code is available at: https://www.github.com/ THUNLP/cost-optimal-gqa Equal contributions 1 2024; Abdin et al., 2024). However, the vast majority of LLMs are Transformer-based (Vaswani et al., 2017; Grattafiori et al., 2024), and the cost of running such architectures does not solely depend on the model size. large portion of the FLOPs (floating-point operations) does not involve model parameters, and large portion of the memory usage during inference is spent storing neural activation instead of model weights. Specifically, during inference, the key and value vectors (KVs) of attention blocks must be cached, incurring memory cost that scales linearly with the context length. Also, attention blocks include the computation of pair-wise attention score and the weighted summation of value vectors, which results in FLOPs count per-token scaling linearly with the context length. Many studies have aimed to reduce these costs, including KV cache compression (Li et al., 2024a), prompt compression (Pan et al., 2024; Li et al., 2024b), sparse attention (Lou et al., 2024; Ge et al., 2024; Jiang et al., 2024), etc. One of the most widely used techniques for reducing these costs is Grouped Query Attention (GQA) (Ainslie et al., 2023), in which attention heads are split into groups and the heads in each group share the same KV vectors2. Current implementations of GQA have two critical limitations. (1) Existing models unnecessarily restrict the total number of head dimensions to be equal to the models hidden dimension. This restriction renders the GQA technique unable to reduce the number of FLOPs. (2) When deciding on the number of attention heads and groups, current models do not take into account the influence of context length on the computational and memory costs, resulting in suboptimal head configuration. In this paper, we aim to minimize both the computational and memory costs of LLMs while max2In this paper, we refer to the number of attention heads as query heads and the number of groups as KV heads. imizing their language modeling abilities. We first decouple the attention head number from the models hidden dimension, making it an independent hyperparameter and allowing us to more flexibly adjust the amount of compute allocated to the attention operator. Secondly, we modify existing scaling law formulas (Kaplan et al., 2020) to account for the influence of context lengths and attention head configuration. Specifically, we model the language modeling quality as function of the compute and memory costs, which better reflects the cost of running LLMs. Furthermore, we show that loss is power-plus-constant function of the number of attention heads, allowing us to predict the loss of head configurations before training. We systematically evaluate head configurations across model sizes and context lengths and show that at certain desired costs and/or inference context lengths, commonly used head configurations may be sub-optimal. For instance, for Llama-3.2-1B, which supports 128K, if we expect that the context length will be roughly 128K tokens most of the time, then we can reduce the training and inference FLOPs and memory required to serve the model by almost 50% with the same loss, by using fewer number of query and KV heads and increasing the model size. The contribution of this paper can be summarized by the following points: Decoupling the number of attention heads from the hidden dimension. Most existing models are structured such that the total number of dimensions of all attention heads in layer equals the models hidden dimensions. We provide the first systematic study of the efficiency-performance tradeoff and show that it can be suboptimal in many cases. Accounting for context lengths in scaling laws. We improve the estimation of computational costs in existing works in scaling laws by accounting for the influence of context lengths and the attention head configuration. Describing the relationship between loss and attention head configurations. We empirically establish the relationship between language modeling loss and attention head configurations, and how this relationship changes across model sizes and context lengths. Searching for computeand memory-optimal head configurations. By training series of LMs of different scales and head configurations, we find that commonly used configurations of GQA are far from computeand memory-optimal. For instance, we find that at 128K context length, the optimal head configuration can achieve the same loss as Llama-3 with roughly 50% of its FLOPs and memory."
        },
        {
            "title": "2 Related Works",
            "content": "In this paper, we completely decouple the number of attention heads from the model hidden dimension and explore how to build an efficient LLM based on GQA, orienting long context scenarios. More details of LLMs can be found in LLM-related surveys (Zhao et al., 2023; Lu et al., 2024). Grouped-Query Attention The original Transformer model employs multi-head attention (MHA) (Vaswani et al., 2017), where each head projects the input into query, key, and value vectors, and then computes weighted sum of the value vectors based on query-key similarities as its output. To further improve decoding efficiency, especially improving memory efficiency, multi-query attention (MQA) (Shazeer, 2019) has been proposed to share the weights of all key and value (KV) projections, significantly reducing KV cache size and memory bandwidth requirements during autoregressive decoding. Grouped-query attention (GQA) (Ainslie et al., 2023) further generalizes the concept of MQA by partitioning queries into groups where each group shares common KV projection. Formally, MHA represents special case of GQA with independent KV projections per query head, while MQA corresponds to the extreme where all queries share one common KV projection. Recent attention methods based on low-rank factorization, such as MLA (DeepSeek-AI et al., 2024), can also be viewed as variants of GQA. It can be said that most of the current popular LLMs (Groeneveld et al., 2024; Biderman et al., 2023; Hu et al., 2024; Grattafiori et al., 2024; Qwen et al., 2025) are built based on GQA. Efficient Long-Context Attention The attention mechanism has become significant bottleneck in long-context scenarios due to its huge computational costs and memory overhead caused by the KV cache. To address these challenges, various methods such as sparse attention (Lou et al., 2024; Ge et al., 2024; Jiang et al., 2024), prompt compression (Pan et al., 2024; Xiao et al., 2024), and KV cache compression (Liu et al., 2024; Hooper et al., 2024; Zhang et al., 2024; Yao et al., 2024; 2 Cai et al., 2024) have been proposed. While these methods can be viewed as further efficiency optimizations over GQA, they often introduce performance degradation compared to the original GQA implementation. This work focuses on identifying cost-optimal GQA architectures for long-context scenarios through precise characterization of parameter sizes, context lengths, and attention head configurations in terms of their impacts on model performance, computational cost, and memory cost. The efficient long-context attention methods described above remain orthogonal to our GQA architecture search and can be subsequently applied as complementary optimizations to the cost-optimal GQA structures. For more details on efficient longcontext attention methods, please refer to the surveys (Yuan et al., 2024; Shi et al., 2024). Scaling Laws for LLMs Recent studies on scaling laws for LLMs (Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022) have established that model loss follows log-linear relationship concerning parameter size and training data size. These studies conveniently approximate the FLOPs per training token as 2N , where represents the parameter size, and then directly link loss to compute. While this law aids in optimizing compute allocation between model parameters and training tokens, we identify their three critical limitations: (1) As we pointed out in the introduction, in long-context scenarios, the computational load of the attention mechanism cannot be ignored, so the approximation for FLOPs adopted by these laws is not reasonable. (2) These laws prioritize the optimal allocation of compute during training, ignoring the critical inference phase. Although Sardana et al. (2023) supplement scaling laws by accounting for total inference FLOPs, their FLOPs estimation has the same approximation error as previous works. (3) These laws consider more compute allocation, yet the huge storage overhead generated by the KV cache is also bottleneck during inference, especially in long-context inference scenarios. Therefore, both computing and memory need to be considered when building cost-optimal LLMs, which is the focus of this paper."
        },
        {
            "title": "3 Optimizing Computational and\nMemory Costs in GQA-based\nTransformers",
            "content": "In this section, we first briefly introduce GQAbased Transformers and decouple the number of attention heads from the hidden dimension. We then describe key model configurations and their impact on computational and memory costs during training and inference. Finally, we provide more accurate formula for the computational and memory costs of Transformer-based LLMs that explicitly consider context length and can guide the design of cost-optimal LLMs. 3.1 GQA-based Transformers In this paper, we mainly study decoder-only LLMs based on the GQA architecture. Specifically, they are stacked by Transformer layers, each of which consists of an attention block and feedforward layer (FFN) block. For each Transformer layer, let xi, yi Rd denote the i-th input and output states of the layer, where is the models hidden dimension. For the attention block of the layer, xi is first projected into query qi = xiWq Rdh, key ki = xiWk Rdh, value vi = xiWv Rdh, where dh is the head dimension, then the attention output is given as xatt,i = softmax (cid:19) (cid:18) qiK dh ViW Rd, (1) = [k = [v 1 where Wq, Wk, Wv, Wo Rddh are learnable projection matrices. ] and 1 ] are the KV cache for the current layer, where denotes the concatenation along the sequence dimension. For the MHA architecture, each attention block consists of nh heads computed in parallel, and their outputs are summed as the final output. For the GQA architecture, every nh/nkv query heads share the same KV projection matrices and KV cache, where nkv is the number of KV heads. The FFN layer takes the output of the attention layer as input and calculates the output of the entire Transformer layer, which is given as yi = σ (cid:16) xatt,iW up (cid:17) Wdown Rd, (2) where Wup Rddff, Wdown Rdffd are learnable projection matrices and σ() is pre-defined activation function. Moreover, we denote the vocabulary size with . This paper assumes that when scaling up model, we always keep dh and constant, d/L = 128, and dff = 4d. These are common practices in current LLMs (Grattafiori et al., 2024; Groeneveld et al., 2024; Biderman et al., 2023; Qwen et al., 2025). Table 1 lists the number of parameters for each component in the 3 Component Parameters FLOPs Emb. in. ATT. proj. ATT. comp. FFN Emb. out. dV 2Lddh(nh + nkv) 0 2Lddff 0 0 4Lddh(nh + nkv) 4LT nhdh 4Lddff 2dV Table 1: Statistics of the parameters and FLOPs (per token for single forward pass) of the main components in Transformer-based LLM. Emb. in. and Emb. out. represent the input and output embedding layers, respectively, sharing the same embedding weights. ATT. proj. and ATT. comp. represent the projection and computation processes of all attention blocks, respectively. FFN indicates all FFN blocks. Some modules with very small number of parameters and FLOPs are left out. Transformer model and the FLOPs associated with parameters. Appendix reports the complete list of the notations used in Table 1. 3.2 Decoupling the Number of Attention Heads from the Hidden Dimension Most of the current LLMs adopt the GQA architecture, in which nh = nkv, and they all keep nh dh = d. We argue that this is an unnecessary restriction since decoupling nh from allows more flexible and fine-grained adjustment of the allocation of FLOPs among different components in the Transformer model. Specifically, by reducing (by reducing and d), but increasing nh, we can allocate more compute to attention blocks. Similarly, we can allocate more compute to other components by increasing and decreasing nh. In this paper, we focus on finding the optimal allocation of memory usage and compute usage. The memory usage needs to consider model parameters and KV cache. The compute usage needs to consider parametric and non-parametric operators. We do this allocation primarily by tweaking nh and nkv independently of the hidden dimension d. 3.3 Inference Costs of GQA-based Transformers The inference computational cost. Cinfer(T ) is the number of FLOPs used to process one token within the context with tokens. This is roughly given as Cinfer(T ) = Cparam + Catt(T ) , = 2N + 4T Ldhnh (cid:125) (cid:124) (cid:123)(cid:122) Attention (3) (4) 4 where Cparam denotes the parametric FLOPs, the number of FLOPs involving model parameters. Catt(T ) denotes the non-parametric FLOPs, which is the number of FLOPs used to compute the attention softmax process. The inference memory cost. (T ) is defined as the memory required to process one token within the context with tokens. Ignoring the necessary system overhead, we need to store the model parameters and the KV cache, which is roughly given as Minfer(T ) = +Nkv(T ) = +2T Ldhnkv (cid:125) (cid:124) (cid:123)(cid:122) KV cache , (5) where denotes the number of model parameters and Nkv(T ) denotes the number of values in the KV cache for the context with tokens. From here on, we refer to Nkv(T ) and Catt(T ) as non-parametric costs, as well as refer to and Cparam as parametric costs, without explicitly distinguishing memory costs and computational costs. 3.4 Training Costs of GQA-based Transformers The training computational cost. In addition to inference costs, different head configurations also result in different training costs, because the number of training FLOPs, Ctrain, is function of Cinfer. Following Kaplan et al. (2020), we estimate the FLOPs of the backward pass as double the FLOPs of the forward pass. Let Dtrain denote the number of training tokens, Ti denotes the number of tokens preceding the i-th training token in the training corpora, then the training FLOPs are: Ctrain 3DtrainCinfer (cid:0)T (cid:1) ), = 6Dtrain(N + 2LT dhnh (cid:125) (cid:124) (cid:123)(cid:122) Attention (6) (7) where is the average value of {Tii = 1, , Dtrain}. When all examples in the training corpora are set to the constant length Ttrain, during training, we have = Ttrain/2. However, in practice, when training long context LLMs, it is more common to use short contexts for most of the time, and only use long contexts consisting of small number of tokens to adapt the model to the target context length. Hence, the non-parametric FLOPs may only make up small portion of the training FLOPs, making the cost largely independent of the GQA configuration. Consequently, our Phase Type Parametric Non-Parametric Inference FLOPs Inference Mem. Training FLOPs Training Mem. 2N 6DtrainN 4N 4T Ldhnh 2T Ldhnkv 12DtrainT Ldhnh dL Table 2: The parametric and non-parametric costs of GQA-based Transformer models during inference and training. paper considers training costs, but focuses more on optimizing inference costs. The training memory cost. There is no KV cache during training, so we only need to store model parameters, activations, gradients, and optimizer states. Assuming we use the widely-used Adam (Kingma and Ba, 2015) optimizer without offloading any storage to the CPU, the memory cost is roughly:3 Mtrain(T ) 4N + dL (cid:124)(cid:123)(cid:122)(cid:125) Activations . (8) While it is important to lower the cost of cache activations when is large, we do not have free hyperparameter to adjust this cost (like nh for computational costs and nkv for memory costs). To reduce the size of activations, we have to modify and/or L, which either drastically changes the model size or its aspect ratio. Either of such changes leads to major consequences that are beyond the scope of this paper. Regarding the 4N part of training memory cost, it is only dependent on the model size so it suffices to minimize the model size, which is already addressed in many existing works (Kaplan et al., 2020; Grattafiori et al., 2024; Sardana et al., 2023). 3.5 Searching for Cost-Optimal GQA Configurations for Long-Context Processing The formulas for the memory and computational costs (Eq. (3), (5), and (7)) provide valuable insights for optimizing the allocation of computational and memory resources. Table 2 summarizes the different parametric and non-parametric costs. Implication 1: Allocation of FLOPs and memory can be controlled with , nh, and nkv. They indicate that the non-parametric costs can managed 3When using FlashAttention (Dao, 2024) to avoid storing attention scores, attention blocks only require constant memory cost. 4. Hence, we can adjust by adjusting nkv and nh the allocation of resources between the parametric and non-parametric costs, by jointly tweaking (controlled by L) and either nh or nkv. Implication 2: The optimal resource allocation is function of . The non-parametric costs scale linearly with while the parametric costs are independent of . This means that when processing long contexts, the non-parametric costs become large and become bottleneck. In this case, the efficiency gain of reducing nh and nkv becomes large. On the other hand, when is small, the parametric costs dominate the overall costs, thus, we get little efficiency gain from reducing nh and nkv. Therefore, to optimize the inference costs, we should determine the value of nh and nkv based on the expected context length during inference. Implication 3: Optimized model can be both faster and stronger. Since the non-parametric costs depend on , at certain values of , it is possible to optimize the allocation of FLOPs and memory. The optimized model has lower loss, costs less compute and memory during inference, and uses fewer training FLOPs."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Model configurations. The models in our experiments are roughly the same architecture as Llama-3 (Grattafiori et al., 2024), which is one of the most widely-used LLMs at the moment5. When scaling the model, we always keep the ratio d/L as 128. For more details, see Appendix C. Data configurations. We use SlimPajama (Soboleva et al., 2023) in our experiments. It is deduplicated version of the RedPajama (Weber et al., 2024) corpus and contains 627B tokens. In most of our experiments, we use 10B training tokens, which are randomly sampled from the data. By default, we always ensure that each batch has 512K tokens. For more details, see Appendix E. 4.2 Overall Results First, we present the main results and takeaways. Then, we empirically demonstrate that the loss of different head configurations and context lengths is predictable. Further, we show that the results 4We emphasize that changing the values of nh or nkv will also change the model size , but this change is typically minimal compared to the size of other components. 5According to download counts at https://www. huggingface.co 5 Figure 1: The main results. It shows LM loss as function of memory and computational costs during inference when the context length is 128K tokens, assuming we use BF16 for both model parameters and KV cache. = (nh, nkv) denotes the number of attention heads and KV heads. The x-axis is on the log scale. The result shows that the widely used GQA configuration (H = (d/dh, 8)) is suboptimal. Figure 2: Loss as function of inference costs with context length of 128K. Assuming we use BF16 for both parameters and the KV cache. = (nh, nkv) denotes the attention head configuration. nh and nkv affects memory and computational cost differently. are generalizable to most training data and longcontext adaptation by post-training. 4.2.1 Loss VS. Inference Costs In this section, we compare the loss-cost tradeoffs of different head configurations, denoted with = (nh, nkv). Concretely, for each H, we train series of LMs of different sizes, from = 4, 6, 8, , 16, keeping = 128L (at = 16, it is the configuration of Llama-3.2-1B), using training length of 8K. Figure 1 shows the loss as function of memory and FLOPs usage during inference with context length of 128K tokens. To save space, we report the result of other context lengths in Appendix G. Takeaway 1. We discover that loss does not have simple relationship (e.g., power-plus-constant function) with either memory or computational costs. However, it is still possible to predict the loss by fitting loss as function of , then rescale the fitted curves along the x-axis to account for the non-parametric costs. The fitted curves are highly accurate, with R2 values over 0.999. Takeaway 2. The commonly used configuration = (d/dh, 8) is highly suboptimal at this context length. For instance, Llama-3.2-1B6 uses this head configuration and supports 128K context length. At that context length, using = (8, 1) and increasing the model size would achieve the same loss while reducing 48.4% and 49.6% inference memory and FLOPs usage, respectively. Alternatively, using = (8, 1) can achieve loss that is 0.117 lower than Llama-3.2-1B with the same training compute budget. Looking at memory alone, the curves of GQA and = (8, 1) will intersect at around = 48, which corresponds to 20B model size. Thus, for models smaller than 20B, it is better to use = (8, 1), in terms of both memory and loss. Also, at that size, = (8, 1) would still be advantageous in terms of FLOPs. This configuration is also advantageous at more modest context lengths 6https://huggingface.co/meta-llama/Llama-3. 2-1B-Instruct 6 Figure 3: Loss as function of memory and computational costs, aligned by training cost (FLOPs) at 128K tokens. Models with the same number of layers are trained with the same amount of training compute. such as 32K, although the reduction in memory and FLOPs is also comparatively smaller (Appendix reports the concrete results). 4.2. Influence of Query and KV Heads Figure 2 shows the loss as function of memory and computational cost. As we decrease nkv from 8 to 1, the memory cost decreases considerably while the computational cost stays largely unchanged. Conversely, changing nh adjusts the computational cost, but the memory cost is largely unchanged. Therefore, practitioners need to make tradeoff between memory and compute cost, likely based on the specifications of the machine to deploy the model. Interestingly, the change in loss brought by changing nh is considerably greater than the change in loss brought by changing nkv. This means that reducing the number of query heads the models capabilities more than the number of KV heads. As possible explanation, we hypothesize that having more query heads provides more expressivity to the attention layers because it permits aggregating information from multiple tokens with one layer while having more KVs primarily provides more values to represent information and retrieval patterns, which is less important. From Figure 2, by extrapolating the scaling law curves, the curves for = (8, 8) and = (8, 1) intersect at roughly = 48, which is roughly model size of 20B. This means that for all models smaller than 20B, it is better to use = (8, 1) than = (8, 8). Appendix G.1 shows the result with other training lengths. Figure 4: The relationship between LM loss and the number of attention heads, fitted with power-plusconstant function. Figure 5: The relationship between LM loss and the number of attention heads, fitted with power-plusconstant function. The model has = 12, = 1536. 4.2.3 Aligning Training Costs In the previous sections, each model was trained on the same amount of data. This favors configurations that spend more FLOPs per token. Instead, we can allow more compute-efficient configurations to use more data to align the training costs of different configurations. Figure 3 reports the result when we always train with = 128K7. We find that using fewer heads is even more advantageous because of the extra training data, producing model with the same loss but with 88% and 83% reduces memory and FLOPs usage. 4.3 The Scaling Laws of Attention Heads In this section, we show that one can predict the loss for certain head configuration using experiments with smaller number of heads. Specifically, we find thatfor the first timethe relationship between loss and the number of attention head closely resembles power-plus-constant function: ℓ(nh) = anb + where ℓ is the LM loss, and a, b, are coefficients. Figure 4 shows that this relationship is 7As noted in Section 3.4, LMs are usually trained with short contexts most of the time, so this result may not apply. 7 Figure 6: The relationship between loss and nh when nkv is constant, for two model sizes. observed with different model sizes. The concrete functions for the curves are: ℓ = 0.579n0.124 ℓ = 0.398n0.177 ℓ = 0.301n0. + 2.473 (L = 12) + 2.583 (L = 14) + 2.622 (L = 16) However, although the fitted curve is very accurate up to 128 heads, it will likely fall off eventually before the intersection point at approximately 8K heads. This is because larger model will likely have lower loss, so the curves should not intersect (but they will converge to the same value because attention projection layers will make up the vast majority of model weights). Fortunately, virtually all existing LMs have fewer than 200 attention heads, so this law is practically highly accurate, with R2 values over 0.999. Similarly, Figure 5 shows that this trend is consistent across different context lengths. The fitted curves are ℓ = 0.578n0.124 ℓ = 0.726n0.092 ℓ = 0.575n0. + 2.473 (T = 1K) + 2.244 (T = 2K) + 2.317 (T = 8K) The R2 of these fits are also over 0.999. Once again, these curves will likely fall off eventually but are highly accurate in the typical range. From these results, we conclude that this powerplus-constant scaling law between loss and the number of heads is exhibited independent of model size and context length. One important implication of this result is that increasing the number of heads to improve model quality gives diminishing returns. This means that beyond certain point, the loss reduction brought by further increasing the number of heads is not worth the cost increase. 8 Figure 7: Loss as function of the context length when using different numbers of heads, for two model sizes. 4.3.1 Constant Number of KV Heads Some LMs, e.g., Llama-3 (Grattafiori et al., 2024), keep the number of KV heads constant when scaling up the model. Therefore, we also investigate the relationship between LM loss and nh when nkv is constant. Figure 6 shows this relationship with different values of nkv and two model sizes. We discover that the relationship is no longer powerplus-constant, but is better fit with logarithmic function: ℓ(nh) = log(nh) + where a, are coefficients. This function will also fall off when nh is sufficiently large because the loss cannot be negative, but when using the typical values for nh, this curve is fairly good fit with R2 values over 0.99. 4.4 The Influence of Context Length In this section, we investigate the influence of context length on the relationship between loss and nh = nkv. Specifically, we train the same model with different and nh from scratch and find that the curvature is roughly the same independent of nh. Figure 7 shows how the relationship between loss and training length is influenced by the values of nh. The main conclusion from this result is that for sufficiently long training contexts, has minimal impact on the loss difference between different values of nh. Thus, it is possible to extrapolate the scaling laws of attention heads (discussed in 4.3) to longer context lengths. 4.5 The Generalization for Post-Training As discussed in Section 3.4, LLMs are typically trained on shorter sequences in practice, followed by adaptation to longer contexts using smaller amount of data tailored to the target context length. As mentioned, there are many techniques for improving the efficiency of the attention layer, although those have enjoyed less adoption. When using these techniques, the computational and memory costs may be considerably different, and some of our conclusions may not apply. Despite so, our work is still valuable improvement over existing implementations of GQA."
        },
        {
            "title": "6 Conclusion",
            "content": "To optimize the allocation of FLOPs and memory between parametric and non-parametric components, we first decouple the number of attention heads from the models hidden dimensions, enabling more flexible distribution of FLOPs and memory. Next, we refine the estimation of computational and memory costs in existing approaches by incorporating context length. Our findings reveal that typical configurations of GQA are significantly suboptimal for certain context lengths. Through detailed analysis, we offer valuable insights for improving the allocation of resources by adjusting the model size and the number of query and KV heads. As the demand for longer context lengths during inference continues to grow, our work marks critical advancement toward efficient long-context LLMs."
        },
        {
            "title": "Limitations",
            "content": "This paper focused on jointly optimizing the cost and performance of attention-based language models. We mainly focused on the memory and computational costs and neglected wall clock time. This is because wall clock time is hardware-dependent, which unnecessarily couples the analysis with the ever-changing hardware. Moreover, we have focused on adjusting the number of attention heads and the model scale, and have largely kept most components of the Transformer architecture unchanged. Many novel architectures dedicated to long contexts have been proposed, and optimizing the costs for these architectures may involve entirely new hyperparameters. Nonetheless, the idea of optimizing costs by adjusting the distribution of parametric and nonparametric costs may be applicable to other architectures as well. Figure 8: The loss curves of model with 2K context length adapted to 64K through post-training compared to model trained with 64K from scratch. To ensure the validity of our conclusions in such training scenarios, we adapted checkpoint initially trained with 2K context length to 64K context length through continual pretraining. This adapted model was then compared to model trained from scratch with 64K context length. As illustrated in Figure 8, the adapted model rapidly achieves lower loss compared to the 2K baseline and gradually converges toward the performance of the model trained from scratch with 64K context length. This indicates that, with sufficient posttraining, the loss of the adapted model approaches that of model trained entirely from scratch. Consequently, our findings regarding inference costs and the relationship between loss and head configuration remain applicable to post-training scenarios."
        },
        {
            "title": "5 Discussions",
            "content": "Do the FLOPs and Memory Usage Reflect Actual Throughput? In practice, FLOPs and memory usage may not reflect actual throughput. However, throughput is highly dependent on the downstream hardware, so if the goal is to optimize the loss and throughput tradeoffs, we should measure the actual speed and try to model loss as function of speed. This has two problems. (1) The conclusions are not generalizable to new hardware, and since new hardware is constantly being developed, minimizing FLOPs and memory usage may be more beneficial in the long term. (2) There is no simple formula for throughput, which makes it hard to model the relationship between loss and throughput. Due to these two problems, we have focused on minimizing FLOPs and memory usage. What About Other Efficient Attention? This paper primarily adjusts the allocation of compute and memory usage by tweaking the model size (controlled with and d) and head configuration (nh, nkv) in GQA, which is rather simple method."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In The 2023 Conference on Empirical Methods in Natural Language Processing. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Preprint, Layer normalization. Hinton. 2016. arXiv:1607.06450. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. 2023. Pythia: suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research, pages 23972430. Ruisi Cai, Yuandong Tian, Zhangyang Wang, and Beidi Chen. 2024. Lococo: Dropping in convolutions for long context compression. Proceedings of ICML. Tri Dao. 2024. Flashattention-2: Faster attention with In The better parallelism and work partitioning. Twelfth International Conference on Learning Representations. DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, Hao Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, Tao Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, 10 Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, and Xiaowen Sun. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. CoRR. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2024. Model tells you what to discard: Adaptive kv cache compression for llms. Proceedings of ICLR. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, and Others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah Smith, and Hannaneh Hajishirzi. 2024. OLMo: Accelerating the science of language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1578915809. Dan Hendrycks and Kevin Gimpel. 2016. GausarXiv preprint sian error linear units (gelus). arXiv:1606.08415. Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory F. Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. Deep learning scaling is predictable, empirically. CoRR. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén Simonyan, Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre. 2022. An empirical analysis of compute-optimal large language model training. In Advances in Neural Information Processing Systems, pages 3001630030. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. 2024. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv:2401.18079. Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai li, Zhiyuan Liu, and Maosong Sun. 2024. MiniCPM: Unveiling the potential of small language models with scalable training strategies. In First Conference on Language Modeling. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. 2024. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Proceedings of ICML. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. Preprint, arXiv:2001.08361. Diederik P. Kingma and Jimmy Ba. 2015. Adam: In 3rd Intermethod for stochastic optimization. national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, and Lei Chen. 2024a. survey on large language model acceleration based on kv cache management. arXiv preprint arXiv:2412.19442. Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier. 2024b. Prompt compression for large language models: survey. Preprint, arXiv:2410.12388. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. 2024. Kivi: tuning-free asymmetric 2bit quantization for kv cache. Proceedings of ICML. Chao Lou, Zixia Jia, Zilong Zheng, and Kewei Tu. 2024. Sparser is faster and less is more: Efficient sparse attention for long-range transformers. arXiv preprint arXiv:2406.16747. Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas Lane, and Mengwei Xu. 2024. Small language models: Survey, measurements, and insights. arXiv:2409.15790. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. 2024. LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. In Findings of the Association for Computational Linguistics: ACL 2024, pages 963981. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Alec Radford and Karthik Narasimhan. 2018. Improving language understanding by generative pretraining. Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. 2023. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. arXiv preprint arXiv:2401.00448. Noam Shazeer. 2019. transformer decoding: One write-head is all you need. Preprint, arXiv:1911.02150. Fast Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, and Hai Zhao. 2024. Keep the cost down: review on methods to optimize llms kv-cache consumption. Proceedings of COLM. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomput., 568(C). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Maurice Weber, Daniel Fu, Quentin Gregory Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Re, Irina Rish, and Ce Zhang. 2024. Redpajama: an open dataset for training large language models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024. Efficient streaming language models with attention sinks. Proceedings of ICLR. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. 2020. On layer 11 In normalization in the transformer architecture. Proceedings of the 37th International Conference on Machine Learning, Proceedings of Machine Learning Research, pages 1052410533. Yao Yao, Zuchao Li, and Hai Zhao. 2024. Sirllm: Streaming infinite retentive llm. Proceedings of ACL. Jiayi Yuan, Hongyi Liu, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, et al. 2024. Kv cache compression, but what must we give in return? comprehensive benchmark of long context capable approaches. Proceedings of EMNLP. Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. Advances in Neural Information Processing Systems. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. 2024. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Proceedings of NeurIPS. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv:2303.18223."
        },
        {
            "title": "A Notations",
            "content": "For completeness, we provide list of notations we used in the paper, reported in Table 3. Variable Meaning Context length dh dff nh nkv Model hyperparameters Vocabulary size Models hidden dimension # layers Head size Intermediate size in FFN The activation function in FFN # attention heads # KV heads (or # groups in GQA) Costs Number of model parameters. Cinfer(T ) Minfer(T ) Ctrain(T ) Mtrain(T ) The computational cost (in FLOPs) per forward pass with context length of tokens. The memory usage (in bytes) of serving the model with context length of tokens. The computational cost (in FLOPs) used to train the model with context length of tokens. The memory usage (in bytes) of training the model with context length of tokens. Table 3: List of notations used in the paper."
        },
        {
            "title": "B More Discussions",
            "content": "What If The Context Length Varies? The formulas for computational and memory costs (Eq. 5, 3, and 7) are affine functions of , so the expected inference costs and expected training computational cost are: E(M (T )) = (E(T )) E(Cfw(T )) = Cfw(E(T )) E(Ctrain(T )) = Ctrain(E(Ttrain)) Hence, it suffices to compare the costs with the expected context length. We also emphasize that there are many applications where the context is very long. These include retrieval-augmented systems, chatbots with streaming dialogue, etc."
        },
        {
            "title": "C Model Configurations",
            "content": "Table 4 shows the configurations of the vanilla models with MHA in our experiments. In general, we ensure that d/L = 128, dh = 64, dff = 4d, and nhdh = when scaling the model size, which is adopted from common hyperparameters found in existing LLMs such as GPT (Radford and Narasimhan, 2018) and Llama (Grattafiori et al., 2024). Moreover, we use the GPT-2 tokenizer, which has vocabulary size of 50,304, and we tie the input and output embeddings. Compared to the vanilla GPT model (Radford and Narasimhan, 2018), we make the following changes: We use RoPE (Su et al., 2024) with θ value of 500,000, which is widely used in current LMs (Grattafiori et al., 2024). The model uses ordinary FFNs (one up projection followed by one down projection) instead of gated FFNs to keep the model architecture as simple as possible. Our preliminary experiments found that using gated activation units in FFNs resulted in very small change in language modeling loss. Moreover, we also use SILU activation function instead of GELU in the original GPT model (Hendrycks and Gimpel, 2016). We use pre-norm (Xiong et al., 2020) and use RMSNorm (Zhang and Sennrich, 2019) instead of LayerNorm (Ba et al., 2016), which is more common in current LLMs. The epsilon in RMSNorm is 106."
        },
        {
            "title": "Model Size",
            "content": "Figure 9 and 10 show the FLOPs and memory breakdown of different components as function of model size. One can see that changes in the model size and/or context length can influence the allocation of FLOPs and memory between different components in the model. For instance, when the context has 128K tokens, the vast majority of FLOPs is spent computing the attention scores and value summation (σ(qKV )), and the vast majority of memory is spent caching KVs. With 1B model parameters, roughly 90% of memory will be spent storing the KV cache, and only 10% will be used to store the model parameters (assuming the KVs and model parameters use the same precision). More Result: Loss VS. Inference Costs Here, we provide the results for the relationship between loss and inference costs, on other context lengths. The results are shown in Figure 12, 13, and 17. We can see that for shorter context lengths, the gain of reducing nh or nkv is relatively small, but the commonly used GQA (nkv = 8) configuration is still suboptimal at 32K context length. At = 16 (the configuration of Llama-3.2-1B), GQA uses more FLOPs and memory than = (8, 1). For longer context lengths such as 512K, we can achieve the same loss with less than 10% of the original memory usage by using fewer KV heads. G.1 Influence of Query and KV Heads Here, we report the results of the configurations in Section 4.2.2 when using different training lengths: 8K, 32K, and 512K. Similar to the previous section, larger training length means that the advantage of using fewer heads is greater. # params 1 197K 2 1.6M 4 13M 42M 6 100M 8 197M 10 340M 12 540M 14 805M 16 128 256 512 768 1,024 1,280 1,536 1,792 2,048 nh 2 4 8 12 16 20 24 28 dh 64 64 64 64 64 64 64 64 64 dff 512 1,024 2,048 3,072 4,096 5,120 6,144 7,168 8,192 Table 4: The configurations of the vanilla models with MHA in our experiments. # params refers to the number of non-embedding parameters. Our model has no bias terms or dropout, which is also common practice and can slightly increase the training efficiency."
        },
        {
            "title": "D Training Configurations",
            "content": "Here, we provide the default training configurations we used during the experiments. Optimizer: We use the widely-used AdamW optimizer (Kingma and Ba, 2015), with β1 = 0.9, β2 = 0.95, and weight decay of 0.1. We only apply weight decay to linear layers, which excludes the re-scaling factor in RMSNorm. We also use gradient clipping value of 1.0. Learning rate scheduler: We use WSD LR scheduler (Hu et al., 2024), with maximum LR of 5 104, 10% warmup steps steps and 20% decay steps. Warmup starts from 0 and increases linearly to the maximum LR. The decay stage uses cosine annealing scheme, where the minimum LR is 10% of the maximum LR. Batch size: 512K tokens. All training experiments were run on A800 GPUs, mostly with 8 GPUs."
        },
        {
            "title": "E Data Processing",
            "content": "In most of our experiments, we used SlimPajama (Soboleva et al., 2023). We append an EOS token to each document in the corpus before chunking the documents into the specified training length. If the last chunk is shorter than the specified training length, it will be discarded. 13 Figure 9: The proportion of FLOPs allocated to different components in Transformer LM, with multi-head attention and RoPE. As the context lengths increase, most FLOPs are spent on the non-parametric computation of the attention operator σ(QK )V , where σ is the row-wise softmax function. Figure 10: The proportion of memory allocated to different components in Transformer LM, with multi-head attention and RoPE. As the context lengths increase, most of the memory usage is spent on storing the KV cache. Figure 11: Loss as function of memory and computational costs during inference with context length of 2K tokens. Figure 12: Loss as function of memory and computational costs during inference with context length of 8K tokens. Figure 13: Loss as function of memory and computational costs during inference with context length of 32K tokens. Figure 14: Loss as function of memory and computational costs during inference with context length of 512K tokens. 15 Figure 15: Loss as function of memory and computational costs during inference with context length of 8K tokens. Figure 16: Loss as function of memory and computational costs during inference with context length of 32K tokens. Figure 17: Loss as function of memory and computational costs during inference with context length of 512K tokens."
        }
    ],
    "affiliations": [
        "NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China",
        "SIST, University of Science and Technology Beijing, Beijing, China"
    ]
}