{
    "paper_title": "MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems",
    "authors": [
        "Jihao Zhao",
        "Zhiyuan Ji",
        "Simin Niu",
        "Hanyu Wang",
        "Feiyu Xiong",
        "Zhiyu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The traditional RAG paradigm, which typically engages in the comprehension of relevant text chunks in response to received queries, inherently restricts both the depth of knowledge internalization and reasoning capabilities. To address this limitation, our research transforms the text processing in RAG from passive chunking to proactive understanding, defining this process as document memory extraction with the objective of simulating human cognitive processes during reading. Building upon this, we propose the Mixtures of scenario-aware document Memories (MoM) framework, engineered to efficiently handle documents from multiple domains and train small language models (SLMs) to acquire the ability to proactively explore and construct document memories. The MoM initially instructs large language models (LLMs) to simulate domain experts in generating document logical outlines, thereby directing structured chunking and core content extraction. It employs a multi-path sampling and multi-perspective evaluation mechanism, specifically designing comprehensive metrics that represent chunk clarity and extraction completeness to select the optimal document memories. Additionally, to infuse deeper human-like reading abilities during the training of SLMs, we incorporate a reverse reasoning strategy, which deduces refined expert thinking paths from high-quality outcomes. Finally, leveraging diverse forms of content generated by MoM, we develop a three-layer document memory retrieval mechanism, which is grounded in our theoretical proof from the perspective of probabilistic modeling. Extensive experimental results across three distinct domains demonstrate that the MoM framework not only resolves text chunking challenges in existing RAG systems, providing LLMs with semantically complete document memories, but also paves the way for SLMs to achieve human-centric intelligent text processing."
        },
        {
            "title": "Start",
            "content": "MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems Jihao Zhao1,2,3, Zhiyuan Ji1,2,3, Simin Niu1,2,3, Hanyu Wang1,2,3, Feiyu Xiong2,3, Zhiyu Li2,3, 1School of Information, Renmin University of China, Beijing, China, 2MemTensor (Shanghai) Technology Co., Ltd., 3Institute for Advanced Algorithms Research, Shanghai"
        },
        {
            "title": "Abstract",
            "content": "The traditional retrieval-augmented generation (RAG) paradigm, which typically engages in the comprehension of relevant text chunks in response to received queries, inherently restricts both the depth of knowledge internalization and reasoning capabilities. To address this limitation, our research transforms the text processing in RAG from passive chunking to proactive understanding, defining this process as document memory extraction with the objective of simulating human cognitive processes during reading. Building upon this, we propose the Mixtures of scenario-aware document Memories (MoM) framework, engineered to efficiently handle documents from multiple domains and train small language models (SLMs) to acquire the ability to proactively explore and construct document memories. The MoM initially instructs large language models (LLMs) to simulate domain experts in generating document logical outlines, thereby directing structured chunking and core content extraction. It employs multi-path sampling and multi-perspective evaluation mechanism, specifically designing comprehensive metrics that represent chunk clarity and extraction completeness to select the optimal document memories. Additionally, to infuse deeper human-like reading abilities during the training of SLMs, we incorporate reverse reasoning strategy, which deduces refined expert thinking paths from high-quality outcomes. Finally, leveraging diverse forms of content generated by MoM, we develop three-layer document memory retrieval mechanism, which is grounded in our theoretical proof from the perspective of probabilistic modeling: compared to the strategy of fusing embeddings prior to retrieval, independently retrieving memories from each layer and subsequently fusing them can more effectively reduce information loss. Extensive experimental results across three distinct domains demonstrate that the MoM framework not only resolves text chunking challenges in existing RAG systems, providing LLMs with semantically complete document memories, but also paves the way for SLMs to achieve human-centric intelligent text processing. Correspondence: Team Leader at lizy@memtensor.cn Author Legend: Corresponding author Code: https://github.com/MemTensor/MoM 5 2 0 2 6 1 ] . [ 1 2 5 2 4 1 . 0 1 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Retrieval-Augmented Generation (RAG), as technical paradigm that combines information retrieval with generative models, effectively mitigates inherent limitations of large language models (LLMs), such as insufficient data freshness [10], hallucinations [3, 14], and the lack of domain-specific knowledge [12]. As the core architecture for knowledge-intensive tasks [17], its effectiveness is fundamentally influenced by the optimization boundaries of the retrieval mechanism. Research has shown that the quality of the retrieved text segments directly determines the upper limit of the performance of RAG systems [15, 20]. Optimal segmentation of documents into semantically complete and coherent segments not only enhances the generation accuracy of LLMs but also significantly improves the systems processing efficiency while reducing computational resource consumption [23, 29]. However, profound cognitive gap still persists in current RAG practices. Most of these methods reduce document processing to mechanistic preprocessing step [8, 16]. This passive approach of segmenting first and then understanding is contrary to the cognitive process of human experts [6, 21]. When studying complex document, human experts actively construct mental model: they first grasp the macro-level logical structure, then identify key arguments, and ultimately form structured memory that is interconnected and hierarchical [22, 24]. To bridge this gap, we advocate for shift in the text processing approach of RAG from passive text chunking to active memory extraction. This study aims to address the core issues arising from the traditional RAG paradigm: How can we enable the model to actively transform an unstructured text stream into semantically complete and logically coherent structured knowledge, namely, document memory, in manner similar to domain experts? And how can we efficiently imbue small language models (SLMs) with this deep understanding capability? To achieve this goal, we propose the Mixtures of scenario-aware document Memories (MoM) framework. Initially, to establish macro-level understanding of the document, we instruct LLMs to assume the role of domain-specific expert. LLMs conduct an in-depth analysis and generate well-structured document outline. This outline not only serves as an index of the content but also lays the foundation for subsequent processing steps. Secondly, guided by the logical outline, we initiate multi-path memory extraction and evaluation. To ensure the quality of the final extraction, we design two unique metrics for comprehensive evaluation and automatic selection. This approach ensures the domain adaptability of the framework, enabling it to accurately grasp the core knowledge structure and key points of different professional documents. Thirdly, with the aim of transferring this advanced cognitive capability from LLMs to SLMs, we employ reverse engineering to construct logically rigorous Chain reasoning of Memory extraction (CoM), which can assist SLMs in thinking. Finally, to fully leverage the multi-level content produced by MoM, we develop three-layer document memory retrieval algorithm consisting of the logical outline, core content, and the original text. Meanwhile, our theoretical analysis demonstrates that this strategy can more effectively avoid information loss, thereby achieving precise retrieval of the target knowledge. We summarize contributions of this work as follows: We propose active memory extraction as an alternative to passive text chunking. By achieving global understanding of domain-specific documents, we construct structured document memory. Additionally, leveraging reverse reasoning techniques, we enable SLMs to autonomously perform this complex task. We develop three-layer document memory retrieval mechanism and provide theoretical proof from the perspective of probabilistic modeling. Compared to the traditional strategy of fusing information before retrieval, independently retrieving from different memory layers and then fusing the results can more effectively reduce information loss, thereby achieving more precise knowledge localization. To validate the effectiveness of the MoM framework, we conduct experiments and analyses on three datasets from different domains. By obtaining data through multiple channels, we construct 40K training samples and train multiple MemReaders. The results indicate that MoM can adaptively process texts with different structures and domains, generate high-quality document memory, and also demonstrate the feasibility of achieving human-centered high-quality text processing through SLMs."
        },
        {
            "title": "2 Related Works",
            "content": "Text Chunking in RAG. As critical prerequisite for RAG, text chunking profoundly influences the ultimate performance of the system. Mainstream RAG systems and development libraries (such as LlamaIndex and LangChain) commonly adopt traditional chunking strategies, which includes fixed-size chunking, recursive chunking, or segmentation based on grammatical boundaries like sentences and paragraphs [8, 9, 11, 16]. These methods are inherently context-independent and completely overlook the deep-seated semantic coherence and logical structure of the content. To overcome the aforementioned drawbacks, the research community has begun exploring semantic chunking approaches, such as merging text based on the similarity of sentence embedding vectors [28] or decomposing text into atomic factual units like propositions [2]. Although these methods outperform traditional strategies in preserving local semantics, they generally follow bottom-up construction logic. They focus on the relationships between adjacent text units but lack macroscopic understanding of the documents overall architecture. Consequently, even though they can generate locally coherent segments, when these segments are combined, they may still deviate from the documents overall theme or chapter logic, resulting in logically incomplete or biased knowledge chunks. Even attempts to use LLMs for iterative segmentation still essentially seek breakpoints locally, failing to fundamentally solve this issue while incurring substantial computational costs [7]. Memory of RAG. To overcome the limitations of LLMs finite context windows and endow them with capabilities for continuous learning and long-term interaction, constructing memory systems has emerged as pivotal research direction in the development of RAG. However, through systematic examination of existing memory frameworks, we find that the current research focus significantly leans towards managing short-term and long-term memory in dialogue scenarios. Represented by systems such as Mem0 [4], LangMem 1, MemoryScope [13], and MemoBase 2, considerable complexity has been developed in conversational memory management, exemplified by Mem0s four-stage memory updating and conflict detection, MemoryScopes importance scoring and temporal decay mechanisms, and graph-structured reasoning in Mem0g [4]. In contrast, memory construction for documents remains at relatively nascent stage, with relatively simplistic processing approaches. MemGPT [18] employs paging mechanism to process information segment by segment, while Zep 3 constructs document collection for vector-based retrieval, which essentially still adheres to the traditional RAG paradigm of chunking first and understanding later, rather than constructing holistic, structured memory for documents. Even MemoRAG [19], which focuses on documents, primarily relies on pointers to navigate and associate between pre-segmented text fragments. This emphasis on conversations over documents reveals research gap: the lack of an advanced mechanism for proactively constructing structured, semantically coherent memories for documents within the field. Our proposed scenario-aware document memory extraction framework can serve as text processing module for these systems, facilitating the development of the entire field."
        },
        {
            "title": "3.1 Document Memory",
            "content": "The core objective of the MoM framework is to simulate the process by which domain experts deeply read and digest documents, transforming unstructured raw text into structured, multi-level, and easily retrievable knowledge, which we refer to as document memory, denoted as Mdoc. The entire framework can be viewed as process of learning mapping function fMoM : Mdoc, which encompasses three key stages: memory extraction, CoM construction, and model training."
        },
        {
            "title": "3.1.1 Task Definition",
            "content": "Document memory is not merely simple segmentation of the original text but rather structured knowledge system that has undergone deep understanding, refinement, and reconstruction. Its core characteristics are 1https://github.com/langchain-ai/langmem 2https://github.com/memodb-io/memobase 3https://github.com/getzep/zep 3 Figure 1 Overview of the entire process of our MoM framework: Logical outline generation, multi-path memory extraction and evaluation, reverse CoM construction, as well as three-layer retrieval mechanism. domain-specificity, structural organization, and completeness. Formally, we define the document memory corresponding to document as triplet: Mdoc = {O, C, A}, where: (Outline): Represents the macro-logical structure of the document. It is an ordered set composed of core topics, = {o1, o2, . . . , on}, providing high-level view and indexing framework for document information organization. (Core Content): Represents core viewpoints of the document. It is highly condensed set of knowledge points extracted from the content corresponding to each outline node oi, = {c1, c2, . . . , cn}. (Atomic Chunks): Represents the structured, fine-grained content segmentation of the original document guided by O. = {a1, a2, . . . , an} exhibit stronger semantic cohesion compared to traditional segmentation methods."
        },
        {
            "title": "3.1.2 Scenario-aware Document Memory Extraction",
            "content": "To generate high-quality document memory, we design an extraction process that incorporates expert simulation, multi-path sampling, and multi-dimensional evaluation. We initially leverage highly capable LLM, designated as the guiding model MG, to emulate the expertise of specialists within particular domains. Through the crafting of scenario-aware prompts, we steer MG to perform comprehensive analysis of the input document D, resulting in the generation of its logical outline O. Following this, with serving as structural framework, we instruct MG to further distill and refine the core content as well as the corresponding atomic chunks for each individual outline. Considering the randomness and limitations of single generation, we introduce multi-path sampling strategy. By adjusting the decoding parameters of MG, we generate candidate document memory sets 4 doc, M(2) {M(1) design two key quantitative evaluation metrics: doc, . . . , M(N ) doc } for the same document D. To select the optimal solution from these candidates, we Atomic Chunks Clarity. This metric aims to evaluate the semantic rationality of the segmentation among atomic chunks A. An ideal segmentation should ensure high semantic cohesion within each chunk and clear semantic boundaries between chunks. We leverage language model Meval to assess the marginal probability of the semantic boundary existing between any two consecutive chunks ai and ai+1. The clarity score is defined as follows: Sclarity(Mdoc) = 1 1 n1 (cid:88) PMeval(bi,i+1ai, ai+1) i=1 where is the total number of atomic chunks, and bi,i+1 denotes the event that semantic boundary exists between chunks ai and ai+1. higher score indicates clearer and more logical overall chunking structure of the document. Core Content Completeness. This metric is used to evaluate the effectiveness and conciseness of the core content in covering the information of the original document D. It is achieved by calculating the conditional perplexity of generating the entire chunks given C, and introducing penalty term for the length of the core content. Its formal definition is as follows: Scomp(Mdoc) = 1 (cid:88) i="
        },
        {
            "title": "1\nP P L(ai|ci) · log(|ci|)",
            "content": "whereci is the total number of tokens in the core content. higher score signifies that, on average, each core content provides strong, concise support for its respective original chunk. Optimal Document Memory Selection. We rank all candidate memories {M(i) doc} in descending order based on Sclarity and Scomp, respectively, resulting in two independent ranking lists. Subsequently, we draw on the reciprocal rank fusion algorithm [5] to calculate comprehensive score for each candidate. For any candidate M(i) doc , its comprehensive ranking score SRRF is defined as: SRRF(M(i) doc) = + rank(i) clarity + 1 + rank(i) comp and rank(i) where rank(i) in the ranking lists for clarity and completeness, respectively, and is smoothing constant (typically set to 60). All candidates will be finally ranked according to their SRRF scores. are the positions of M(i) doc clarity comp"
        },
        {
            "title": "3.1.3 Reverse Construction of CoM",
            "content": "To enable SLMs to master such complex knowledge construction capabilities, merely providing supervised data consisting of inputs and outputs Mdoc is insufficient. In order to instill deeper human-like reading abilities, we introduce the reverse construction strategy of CoM. Specifically, we once again leverage the guiding model MG, providing it with the original document and the optimal document memory Mdoc, and generating the reasoning path through specific prompts. This reasoning path constitutes high-quality CoM data and becomes an essential component for training SLMs."
        },
        {
            "title": "3.1.4 MemReader",
            "content": "Based on the aforementioned process, we construct approximately 40K high-quality training samples by employing DeepSeek-R1 to act as MG. Each sample is composed of triplet (D, P, Mdoc). Our objective is to transform SLM into MemReader, enabling it to directly generate reasoning paths and document memories from raw documents. For training sample, let the input be (the document and related instructions), and the target output sequence be (the concatenation of and Mdoc). The loss function is defined as: τ (cid:88) LF(θ) = log (oto<t, s; θ) 1 τ t=1 where ot represents the t-th token in the target sequence o, o<t denotes the prefix of the target sequence up to position 1, is the input context, θ represents the learnable parameters of the SLM, and τ denotes the length of the target output sequence o."
        },
        {
            "title": "3.2 Three-layer Document Memory Retrieval\nBased on the document memory Mdoc = {O, C, A} generated within the MoM framework, we construct a\nthree-layer document memory retrieval mechanism, corresponding respectively to the global summary O, the\ncore content C, and the original chunks A. This design is not merely based on empirical evidence; rather, it is\nalso theoretically substantiated by the underpinnings of our proposed probabilistic model.",
            "content": "We regard the user query as random vector sampled from mixed distribution. Specifically, the query space is composed of two distinct types of queries: global queries Qabs, which are designed to represent and seek information related to the global summary O, and local queries Qquery, which aim to represent and retrieve details pertaining to the core content C, alongside enabling access to the original chunks for more granular information when necessary. We assume that these two types of queries follow different Gaussian distributions in the embedding space: global query qabs (µabs, Σabs) and local query qquery (µquery, Σquery), where µ is the mean vector and Σ is the covariance matrix. Hypothesis 1 (Semantic Divergence Hypothesis). We assume that the semantic centers of global queries and local queries are significantly separated in the embedding space. Formally, it is expressed as: µabs µquery2 > 0 This implies that the directions of µabs and µquery are significantly different, that is, their inner-product µT absµquery = cos(θ) < 1. This hypothesis is rooted in the fundamental duality of human information-seeking behavior and linguistic expression. The users query intentions can typically be clearly classified into macro-level comprehension and micro-level exploration, and these two types of vectors form two distinct clusters in the high-dimensional embedding space. Therefore, for the hierarchical multi-vector (HMV), its vectors Vabs and Vquery serve as unbiased estimates of the corresponding semantic centers µabs and µquery, which can be expressed as E[Vabs] = µabs and E[Vquery] = µquery. In contrast, the single-vector fusion (SVF) produces fused vector Vf used that constitutes biased, compromise estimate, thereby diluting the representational purity of either semantic mode: E[Vf used] = µf used = (1 w)µabs + wµquery, (0, 1) Theorem 1. For user queries, the HMV outperforms the SVF in terms of expected similarity. Specifically, we need to prove the following two points: For global query qabs, we have E[qT For local query qquery, we have E[qT absVabs] > E[qT queryVquery] > E[qT queryVf used]. absVf used]. Since we do not need to consider the vector length and only involve the direction representation, all vectors are normalized to unit vectors for subsequent calculations. Proof. First, we analyze the expected similarity of the HMV method. According to the law of total expectation and the linearity of expectation, we can obtain: E[qT absVabs] = E[E[qT absVabsqabs]] = E[qT absE[Vabs]] = E[qT absµabs] = E[qabs]T µabs = µT absµabs = 1 Next, we analyze the expected similarity of the SVF method: 6 E[qT absVf used] = E[qT absE[Vf used]] = E[qT absµf used] = E[qT abs((1 w)µabs + wµquery)] = (1 w)E[qT = (1 w) 1 + cos(θ) absµabs] + wE[qT absµquery] = (1 w)(µT absµabs) + w(µT absµquery) where cos(θ) = µT absµquery. According to the Hypothesis 1, µabs and µquery point in different directions, so θ > 0 and cos(θ) < 1. Since (0, 1), we have cos(θ) < w. Therefore, (1w)+w cos(θ) < (1w)+w = 1. That is, E[qT absVfused]. The second point can be proved in the same way. absVabs] > E[qT Further, we not only demonstrate that, on average, the representation method of the HMV outperforms the SVF but also offer more robust probabilistic guarantee through the introduction of probability bounds. Theorem 2. For any small positive deviation threshold ϵ > 0, the probability that the deviation of the retrieval result of the HMV strategy from the ideal case is greater than ϵ is lower than that of the SVF strategy: (qT (qT absVabs < 1 ϵ) < (qT queryVquery < 1 ϵ) < (qT absVf used < 1 ϵ) queryVf used < 1 ϵ) In summary, the HMV method not only exhibits superior performance in terms of expectation but also maintains consistently high similarity score with extremely low probability of obtaining low similarity score. In contrast, the SVF model has much higher probability of achieving similarly low similarity score. Detailed analysis and proof are presented in Appendix 6.1. Remark 1. The core insights of Theorem 1 and Theorem 2 lie in the fact that semantic fusion is costly compromise. The SVF strategy creates semantic average by compressing globally concepts and local details into single vector. This fused vector occupies compromised position in the embedding space and fails to perfectly represent the original intentions of either side. As result, when user query explicitly targets global or local information, its similarity to this compromised vector is necessarily lower than its similarity to dedicated vector. Our theory rigorously explains this intuition mathematically: preserving the independence of information levels is not only beneficial but also probabilistically superior as retrieval method because it fundamentally minimizes the loss of key features due to forced information fusion. Hence, the three-layer document memory retrieval mechanism can maximize the preservation of document information at different granularities, thereby providing more accurate and comprehensive context for subsequent generation tasks."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets and Metrics. The experiment primarily select CRUD [16] in the news domain, OmniEval [27] in the financial domain, and MultiFieldQA_zh [1] across multiple domains as the evaluation benchmarks for domain document question answering. Among them, CRUD focuses on long-answer generation; OmniEval provides manually annotated data across 5 task types and 16 financial topics, enabling comprehensive assessment of the retrieval and generation quality of RAG systems in the vertical domain; multifieldqa_zh is derived from the LongBench benchmark. The evaluation metrics uniformly adopt the BLEU series, ROUGE-L, and METEOR, which respectively measure n-gram overlap, longest common subsequence, and the matching degree of synonyms and syntactic variations. Baselines. We primarily compare MoM with six representative methods spanning from rule-based to semantic-based and then to LLM-driven approaches. Original chunking 4 merely divides long texts into fixed-length chunks while disregarding sentence boundaries. The Llama_index method [25] maintains sentence boundaries while ensuring that the number of tokens in each chunk is close to preset threshold. Similarity chunking [28] utilizes sentence embedding models to partition texts according to semantic similarity, effectively grouping highly relevant sentences together. LumberChunker [7], for the first time, introduces LLMs into 4https://github.com/run-llama/llama_index 7 Table 1 Main experimental results are presented in three domain QA datasets. B-1, B-Avg, ROU., and MET. represent BLEU-1, BLEU-AVG, ROUGE-L, and METEOR, respectively. The best result is in bold, and the second best result is underlined. Chunking Methods CRUD OmniEval MultiFieldQA B-1 B-Avg ROU. MET. BB-Avg ROU. MET. B-1 B-Avg ROU. MET. Original 0.5022 0. 0.5654 0.7324 0.1906 0.1006 0.2254 0. 0.1707 0.0684 0.2315 0.3650 Llama_index 0. 0.4114 0.5896 0.7449 0.1969 0.1065 0. 0.4040 0.1732 0.0765 0.2363 0.3726 Semantic Chunking 0.5188 0.3985 0.5823 0.7434 0.1913 0. 0.2240 0.3821 0.1609 0.0576 0.2191 0. LumberChunker 0.5061 0.3910 0.5701 0.7399 0. 0.1092 0.2375 0.4085 0.1841 0.0838 0. 0.3809 MoC MetaChunker 0.5456 0.4225 0.6031 0. 0.2042 0.1128 0.2457 0.4141 0.1707 0. 0.2255 0.3512 Qwen2.5-14B 0.5329 0.4127 0. 0.7502 0.2048 0.1140 0.2473 0.4160 0. 0.0890 0.2497 0.3827 Qwen3-14B 0.5382 0. 0.5953 0.7531 0.1907 0.1040 0.2329 0. 0.1800 0.0873 0.2412 0.3759 MemReader-1.5B 0. 0.4314 0.6106 0.7611 0.1908 0.1023 0. 0.4083 0.1774 0.0771 0.2424 0.3900 MemReader-3B 0.5539 0.4325 0.6098 0.7600 0.1936 0. 0.2394 0.4171 0.1897 0.0840 0.2559 0. MemReader-7B 0.5565 0.4372 0.6152 0.7669 0. 0.1123 0.2500 0.4229 0.2000 0.0837 0. 0.4043 the segmentation process. By using prompts, it instructs the model to judge whether there is topic shift segment by segment and dynamically outputs the optimal segmentation points. MoC [30] adopts framework combining small router and meta-chunkers to balance precision and efficiency, representing novel paradigm for parameter-efficient chunking. Implementation Details. In our approach, the construction of core training data leverages the DeepSeek-R1 5. To stimulate the diversity of content generated by the model, we set temperature = 0.7 and top_p = 0.8. For the MemReader implementation, we select Qwen2.5-1.5B 6, Qwen2.5-3B 7 and Qwen2.5-7B 8 as base models for training. During model evaluation, we primarily utilize Qwen2.5-7B and Qwen2.5-14B 9. All language models used in experiments are of the instruction version and are loaded with float16 precision to optimize computational efficiency. To enable retrieval-based QA, we construct vector database using Milvus and choose bge-base-zh-v1.5 10 as the embedding model. We set top_k= 8 to retrieve the most relevant contextual information. The hardware configuration is divided according to task requirements: model training and text processing are carried out on the NVIDIA A800 80G, while evaluation is completed on the MetaX C500 64G."
        },
        {
            "title": "4.2 Main Results",
            "content": "To comprehensively evaluate the effectiveness of the MoM framework, we conduct extensive experiments on three QA datasets from distinct professional domains, as detailed in Table 1. The experimental results demonstrate that in the CRUD benchmark, our proposed MemReader exhibits significant advantages, achieving the best performance across all four evaluation metrics. Notably, even the smaller-scale MemReader-3B and MemReader-1.5B outperform all baseline methods, highlighting the efficiency and effectiveness of our framework. To further assess the generalization capability of MemReader, we also analyze its performance on two additional, more challenging datasets: OmniEval and MultiFieldQA. In the OmniEval dataset, all methods encounter difficulties, primarily due to the substantial amount of discrete information present in the financial dataset, which differs significantly from the training documents we utilized. Nevertheless, MemReader-7B still performs comparatively well on three of the metrics. In the MultiFieldQA dataset, MemReader-7B once again secures the best performance, while MemReader-3B achieves the second-best scores on the ROUGE-L 5https://huggingface.co/deepseek-ai/DeepSeek-R1 6https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct 7https://huggingface.co/Qwen/Qwen2.5-3B-Instruct 8https://huggingface.co/Qwen/Qwen2.5-7B-Instruct 9https://huggingface.co/Qwen/Qwen2.5-14B-Instruct 10https://huggingface.co/BAAI/bge-base-zh-v1.5 8 Table 2 Correlation analysis of atomic chunks clarity with ROUGE-L under different LLMs. Metric Chunking Method ROUGE-L Atomic Chunks Clarity Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B"
        },
        {
            "title": "Original",
            "content": "Llama_index"
        },
        {
            "title": "Semantic Chunking",
            "content": "Qwen2.5-14B Qwen2.5-72B 0.4213 0.4326 0.4131 0. 0."
        },
        {
            "title": "Pearson Correlation Coefficient",
            "content": "-0.0071 0.2710 0.1945 0.3782 0.3484 0. 0.0085 0.1919 0.1447 0.3651 0.3355 0. -0.0510 0.3531 0.2380 0.4996 0.4908 0. Table 3 Quantitative evaluation of informational support from retrieved memories for answers."
        },
        {
            "title": "Informational Support",
            "content": "Chunking Method Qwen2.5-7B Qwen2.5-14B Qwen2-7B"
        },
        {
            "title": "Original",
            "content": "Llama_index"
        },
        {
            "title": "MoC MetaChunker",
            "content": "Qwen2.5-14B Qwen3-14B MemReader-3B 3.304 3.343 3. 3.337 3.628 3.429 3.264 3.341 3. 3.695 3.369 3.671 3.498 3.348 2. 2.935 3.024 2.906 3.151 2.955 2. 3.149 3.247 2.795 and METEOR metrics, which measure semantic similarity. This indicates that our method has advantages in accurately recalling factual information and generating semantically similar answers. Based on comprehensive experimental results, we find that the MoM framework exhibits exceptional performance in handling plain-text QA tasks across different domains, proving its ability to enhance the upper limit of RAG system performance through proactive memory extraction and retrieval."
        },
        {
            "title": "4.3 Exploration of Evaluation Metrics for Memory Extraction",
            "content": "During the process of memory extraction, central challenge lies in objectively evaluating the quality of the generated memory fragments. Although traditional metrics can measure information recall to certain extent, they are often based on retrieval QA and overlook the semantic rationality of memories themselves, thus failing to provide direct scores for rapid assessment. To address this issue, we propose atomic chunks clarity and core content completeness, both of which can directly score the memory extraction results within the MoM framework. Given the complex relationship between the former and chunk quality, we conduct the further investigation. As shown in the last row of Table 2, under three evaluation models, the correlation coefficients between our metric and ROUGE-L reach 0.7044, 0.7585, and 0.7248, respectively, all indicating strong positive correlations. Based on this, we employ Qwen2.5-7B as the base model for metric computation when constructing the document memory dataset. Additionally, we observe that the scores for the original method are notably low, even negative in some cases, suggesting that the original paragraph segmentation is often semantically ambiguous. In contrast, chunking methods processed by algorithms achieve higher positive scores, demonstrating that these methods indeed create more semantically independent units. We also conduct tests using larger model, Qwen2.5-72B, to ensure that atomic chunks clarity can be applied to larger-scale models."
        },
        {
            "title": "4.4 How Retrieved Content Supports Answers",
            "content": "In the evaluation framework of RAG systems, mainstream methods typically focus on the similarity between the finally generated response and the answer. However, this end-to-end evaluation fails to clearly attribute the systems performance bottleneck to either the retrieval module or the generation module. To overcome this challenge, we design an experiment to directly quantify the informational support provided by the retrieved content for the answer. Instead of examining what the system ultimately generates, we evaluate whether the retrieved context itself contains sufficient information to derive the correct answer. Given query q, the RAG system retrieves and returns set = {c1, c2, . . . , ck} consisting of memories. Meanwhile, the query corresponds to reference answer = (a1, a2, . . . , am). We define the informational support score as: Ssupport(AC) = 1 (cid:88) i=1 log (aia1, . . . , ai1, C) smaller Ssupport value indicates higher likelihood of the correct answer being inferred from the retrieved memories, signifying stronger support. We conduct the test on the MultiFieldQA dataset, with the results presented in Table 3. Our proposed MoM method demonstrate superior performance across all evaluation models, which proves that the memories extracted and organized by MemReader-3B can provide more information for downstream tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper aims to bridge the cognitive gap in the current RAG paradigm, namely, how to transition from passive text chunking to proactive document understanding and memory construction that simulates human experts. To address this challenge, we design and implement an innovative MoM framework, which successfully elevates document processing from superficial operations to deep cognition through holistic solution. It begins by constructing cognitive blueprint through the generation of logical outline that simulates domain experts, then utilizes multi-path extraction and evaluation algorithm to ensure the completeness and accuracy of memory content, and finally employs reverse reasoning strategies CoM to impart this complex cognitive ability to SLMs. Experimental results demonstrate that SLMs empowered by MoM exhibit superior understanding and organizational capabilities when processing multi-domain documents. Meanwhile, we propose and validate three-layer document memory retrieval mechanism based on probabilistic modeling theory. This mechanism not only makes full use of the multi-level memories generated by MoM in engineering but also theoretically proves its superiority in reducing information loss and enhancing retrieval accuracy. Therefore, the MoM framework not only provides an effective technical pathway for optimizing existing RAG systems but also opens up new avenues for exploring how to construct SLMs that are closer to human thinking patterns and possess greater autonomous cognitive abilities."
        },
        {
            "title": "References",
            "content": "[1] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [2] Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Dong Yu, and Hongming Zhang. Dense retrieval: What retrieval granularity should we use? arXiv preprint arXiv:2312.06648, 2023. [3] Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, and Yanghua Xiao. Hallucination detection: Robustly discerning reliable answers in large language models. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 245255, 2023. [4] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building productionready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. [5] Gordon Cormack, Charles LA Clarke, and Stefan Buettcher. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 758759, 2009. 10 [6] Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin Yang, and Anton Tsitsulin. Dont forget to connect! improving rag with graph-based reranking. arXiv preprint arXiv:2405.18414, 2024. [7] André Duarte, João Marques, Miguel Graça, Miguel Freire, Lei Li, and Arlindo Oliveira. Lumberchunker: Long-form narrative document segmentation. arXiv preprint arXiv:2406.17526, 2024. [8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. [9] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 39293938. PMLR, 2020. [10] Hangfeng He, Hongming Zhang, and Dan Roth. Rethinking with retrieval: Faithful large language model inference. arXiv preprint arXiv:2301.00303, 2022. [11] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. [12] Xianzhi Li, Samuel Chan, Xiaodan Zhu, Yulong Pei, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. Are chatgpt and gpt-4 general-purpose solvers for financial text analytics? study on several typical tasks. arXiv preprint arXiv:2305.05862, 2023. [13] Zouying Cao Li Yu, Jiaji Deng. Reme: Memory management framework for agents, 2025. URL https: //github.com/modelscope/ReMe. [14] Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Feiyu Xiong, Internal consistency and self-feedback in large language models: survey. arXiv preprint and Zhiyu Li. arXiv:2407.14507, 2024. [15] Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert, and Gonzalo Iglesias. Li-rage: Late interaction retrieval augmented generation with explicit signals for open-domain table question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 15571566, 2023. [16] Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, and Enhong Chen. Crud-rag: comprehensive chinese benchmark for retrieval-augmented generation of large language models. arXiv preprint arXiv:2401.17043, 2024. [17] Agada Joseph Oche, Ademola Glory Folashade, Tirthankar Ghosal, and Arpan Biswas. systematic review of key retrieval-augmented generation (rag) systems: Progress, gaps, and future directions. arXiv preprint arXiv:2507.18910, 2025. [18] Charles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. Memgpt: Towards llms as operating systems. 2023. [19] Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, and Tiejun Huang. Memorag: Boosting long context processing with global memory-enhanced retrieval augmentation. In Proceedings of the ACM on Web Conference 2025, pages 23662377, 2025. [20] Renyi Qu, Ruixuan Tu, and Forrest Bao. Is semantic chunking worth the computational cost? arXiv preprint arXiv:2410.13070, 2024. [21] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. Agentic retrieval-augmented generation: survey on agentic rag. arXiv preprint arXiv:2501.09136, 2025. [22] Eleanor Spens and Neil Burgess. generative model of memory construction and consolidation. Nature human behaviour, 8(3):526543, 2024. [23] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. Dragin: Dynamic retrieval augmented generation based on the real-time information needs of large language models. arXiv preprint arXiv:2403.10081, 2024. [24] Zhisheng Tang and Mayank Kejriwal. Humanlike cognitive patterns as emergent phenomena in large language models. arXiv preprint arXiv:2412.15501, 2024. 11 [25] Oguzhan Topsakal and Tahir Cetin Akinci. Creating large language model applications utilizing langchain: primer on developing llm apps fast. In International conference on applied engineering and natural sciences, volume 1, pages 10501056, 2023. [26] Liangdong Wang, Bo-Wen Zhang, Chengwei Wu, Hanyu Zhao, Xiaofeng Shi, Shuhao Gu, Jijie Li, Quanyue Ma, TengFei Pan, and Guang Liu. Cci3. 0-hq: large-scale chinese dataset of high quality designed for pre-training large language models. arXiv preprint arXiv:2410.18505, 2024. [27] Shuting Wang, Jiejun Tan, Zhicheng Dou, and Ji-Rong Wen. Omnieval: An omnidirectional and automatic rag evaluation benchmark in financial domain. arXiv preprint arXiv:2412.13018, 2024. [28] Shitao Xiao, Zheng Liu, Peitian Zhang, and Muennighof. C-pack: packaged resources to advance general chinese embedding. 2023. arXiv preprint arXiv:2309.07597, 2023. [29] Shicheng Xu, Liang Pang, Huawei Shen, and Xueqi Cheng. Berm: Training the balanced and extractable representation for matching to improve generalization ability of dense retrieval. arXiv preprint arXiv:2305.11052, 2023. [30] Jihao Zhao, Zhiyuan Ji, Zhaoxin Fan, Hanyu Wang, Simin Niu, Bo Tang, Feiyu Xiong, and Zhiyu Li. Moc: Mixtures of text chunking learners for retrieval-augmented generation system. arXiv preprint arXiv:2503.09600, 2025."
        },
        {
            "title": "6.1 Proof of Theorem 2\nProof. The random variables of our concern are the similarity scores between the global query qabs and Vabs or\nVfused under the HMV and SVF method, respectively. Let XHMV = qT\nabsVfused. According\nto the derivation from Theorem 1, we have E[XHMV] = 1 and E[XSVF] = (1−w)+w·cos(θ) = 1−w(1−cos(θ)).\nBoth the query qabs and the vector Vabs are modeled as Gaussian distributions, and their inner product is\nabsVabs).\nalso a random variable following a Gaussian distribution. We define its variance as σ2\nSimilarly, σ2\nOur goal is to set an upper bound on the probability that the value of XHMV is less than 1 − ϵ. Applying\nHoeffding’s inequality, we obtain:",
            "content": "absVabs and XSVF = qT HMV = Var(qT SVF = Var(qT absVfused). (XHMV 1 ϵ) exp (cid:18) (cid:19) ϵ2 2σ2 HMV Let = w(1 cos(θ)), then the expectation of SVF can be written as E[XSVF] = µS = 1 C. Case 1: Small Deviation (0 < ϵ C). We standardize XSVF: (XSVF < 1 ϵ) = (cid:18) XSVF µS σSVF < 1 ϵ µS σSVF (cid:19) Let Φ() denote the cumulative distribution function of the standard normal distribution. Then, we obtain: (XSVF < 1 ϵ) = Φ (cid:18) 1 ϵ µS σSVF (cid:19) = Φ (cid:19) (cid:18) ϵ σSVF In this case, ϵ 0. Therefore, (XSVF < 1 ϵ) Φ(0) = 0.5. Since ϵ is positive constant, (XHMV 1 ϵ) remains low-probability event constrained by an exponentially decaying term. Thus, negligible tail probability is clearly smaller than significant probability of at least 0.5. Case 2: Large Deviation (ϵ > C). This case describes more extreme requirement. We can view both events as the extent to which the random variables deviate from their respective means. The distance between XHMV and its mean 1 is at least ϵ: (XHMV < 1 ϵ) = (XHMV E[XHMV] < ϵ) The distance between XSVF and its mean 1 is at least ϵ C: (XSVF < 1 ϵ) = (XSVF < µS (ϵ C)) = (XSVF E[XSVF] < (ϵ C)) Since ϵ > C, we know that ϵ > ϵ > 0. Therefore, in practical scenarios within specialized domain, the probability of large deviation ϵ occurring in the specialized method HMV is much smaller than the probability of small deviation occurring in the general method SVF. On the other hand, formally, the probability upper bound of XHMV decays as exp(ϵ2), while the probability of XSVF decays as exp((ϵ C)2). Since ϵ2 > (ϵ C)2, the decay rate of XHMV is faster, and its probability value is smaller."
        },
        {
            "title": "6.2 Main Experimental Details",
            "content": "In our experiments, we employ total of five baseline methods, with their specific configurations detailed as follows: (a) Rule-based Chunking Methods 13 Original: This method divides long texts into segments of fixed length, such as two hundred Chinese characters or words, without considering sentence boundaries. Llama_index [25]: This method considers both sentence completeness and token counts during segmentation. It prioritizes maintaining sentence boundaries while ensuring that the number of tokens in each chunk are close to preset threshold. We use the SimpleNodeParser function from Llama_index, adjusting the chunk_size parameter to control segment length. (b) Dynamic Chunking Methods Similarity Chunking [28]: Utilizes pre-trained sentence embedding models to calculate the cosine similarity between sentences. By setting similarity threshold, sentences with lower similarity are selected as segmentation points, ensuring that sentences within each chunk are highly semantically related. This method employs the SemanticSplitterNodeParser from Llama_index. The size of text chunks is controlled by adjusting the similarity threshold. LumberChunker [7]: Leverages the reasoning capabilities of LLMs to predict suitable segmentation points within the text. We utilize Qwen2.5 models with 14B parameters, set to full precision. MoC MetaChunker [30]: MoC trains lightweight chunker model to automatically learn how to partition long texts into semantically coherent chunks without relying on fixed lengths or predefined rules. Compared to traditional heuristic methods, MetaChunker demonstrates stronger cross-task generalization capabilities, particularly in downstream tasks such as RAG, serving as representative strong baseline approach."
        },
        {
            "title": "6.3 Collection and Refinement of Training Data",
            "content": "To construct high-quality training set for document memory extraction, we first extract raw texts from the pre-trained corpus CCI3-HQ [26]. CCI3-HQ itself comprises 500GB of high-quality web pages and book content, encompassing approximately 100B tokens, and is accompanied by quality scores. From this corpus, we select 30K documents spanning multiple domains, including news, social media, literature, academic papers, educational and scientific popularization, legal regulations, healthcare, and more. Concurrently, we construct training and test sets from the open-source CRUD dataset. Specifically, since CRUD provides evidence context snippets corresponding to each QA pair, along with the original news repository, we can retrieve the original news articles containing these context snippets through sentence matching. Taking two-hop QA as an example, CRUD provides two news snippets, namely news1 and news2, which are essential for answering the question. We then save the matched original news articles, matched_news1 and matched_news2, that contain news1 and news2, respectively. Finally, from repository of 80K original news articles, we recall 10K news articles that contain the context snippets as the initial texts for evaluation. From the remaining documents, we randomly select 10K data samples for training. After obtaining 40K multi-domain mixed documents, we employ the MoM framework for high-quality memory extraction and CoM construction, providing reliable foundation for subsequent supervised fine-tuning of SLMs and evaluation of document memory capabilities. 14 Table 4 Prompt for guiding the training and inference of MemReader in the MoM framework. Scenario-Aware Document Memory Extraction This is document memory extraction task, and you are an expert in memory extraction. Firstly, carefully analyze the content provided below and generate logical, self-consistent, and complete reasoning process to solve this problem. Then, utilizing the holistic understanding of the document, create memory extraction outline, and based on this outline, generate corresponding number of scenario memories for the given document. The generation of the memory extraction outline should be approached from the perspective of domain expert, leveraging the global information of the original document. Each entry in the outline should represent the role of the corresponding text chunk in the scenario memory and its summary content. When extracting memories from the document according to the generated outline, each scenario memory should consist of two parts: 1. text chunk with complete logical expression, segmented from the document according to logical and semantic structures. Requirements: Avoid overly short text chunks and achieve good balance between identifying content transitions and chunk length. Each output text chunk should be composed of the first and last few characters of the chunk, with the intermediate content replaced by [MASK]\". 2. description of the core content within the corresponding text chunk. The overall output format is as follows: <think>"
        },
        {
            "title": "Reasoning process",
            "content": "</think> <outline>"
        },
        {
            "title": "Memory extraction outline for the document",
            "content": "</outline> <scenario> <chunk> First few characters of text chunk 1 [MASK] Last few characters of text chunk 1 </chunk> Description of the core content in text chunk 1 </scenario> ....... If you understand, reply directly with the content in the specified format, using line breaks to distinguish between different scenario memories. Do not output any other explanatory content, and do not enclose your reply in quotation marks or other delimiters. Document content: <document>Document content</document>"
        }
    ],
    "affiliations": [
        "Institute for Advanced Algorithms Research, Shanghai",
        "MemTensor (Shanghai) Technology Co., Ltd.",
        "School of Information, Renmin University of China, Beijing, China"
    ]
}