{
    "paper_title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance",
    "authors": [
        "Yujie Wei",
        "Shiwei Zhang",
        "Hangjie Yuan",
        "Yujin Han",
        "Zhekai Chen",
        "Jiayu Wang",
        "Difan Zou",
        "Xihui Liu",
        "Yingya Zhang",
        "Yu Liu",
        "Hongming Shan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 1 1 7 4 2 . 0 1 5 2 : r ROUTING MATTERS IN MOE: SCALING DIFFUSION TRANSFORMERS WITH EXPLICIT ROUTING GUIDANCE Yujie Wei1, Shiwei Zhang2, Hangjie Yuan3, Yujin Han4, Zhekai Chen4,5, Jiayu Wang2, Difan Zou4, Xihui Liu4,5, Yingya Zhang2, Yu Liu2, Hongming Shan1 1Fudan University 2Tongyi Lab, Alibaba Group 4The University of Hong Kong 5MMLab 3Zhejiang University"
        },
        {
            "title": "ABSTRACT",
            "content": "Mixture-of-Experts (MoE) has emerged as powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to first partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and second refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models (Ho et al., 2020) have made substantial advances for visual synthesis (Rombach et al., 2022b; Yang et al., 2024; Wei et al., 2024a; Wan et al., 2025). Driven by the growing demand for higher fidelity and quality, research has focused on scaling up diffusion models (Esser et al., 2024b) and propelled an architectural shift from U-Net (Ronneberger et al., 2015) backbones to the now-prevalent Diffusion Transformers (DiTs) (Peebles & Xie, 2023). Despite the proven effectiveness of DiT-based models (Esser et al., 2024a), their dense activation of all parameters irrespective of task or input incurs substantial computational overhead, thereby hindering further scalability. To scale toward larger and more capable models, the large language model (LLM) community has widely adopted the Mixture-of-Experts (MoE) (Jacobs et al., 1991; Shazeer et al., 2017) paradigm, which expands model capacity while maintaining computational efficiency. Conceptually, an MoE layer dispatches input tokens to specialized expert sub-networks via router and returns weighted sum of the selected experts outputs. Despite MoEs profound success in language modeling (Jiang et al., 2024; Dai et al., 2024), recent efforts to integrate it into DiT models have not yielded the significant gains observed in LLMs. Specifically, DiT-MoE (Fei et al., 2024), which employs token-to-expert routing, often underperforms dense counterparts despite activating the same number of parameters. In contrast, EC-DiT (Sun et al., 2024), which assigns each expert fixed quota of tokens, delivers only marginal gains even with extended training. More recently, Project Leader Corresponding Author 1 Figure 1: (a) We randomly sample 1k intermediate-layer tokens from 110 ImageNet classes for 10-cluster k-means clustering (differentiated by color). With class names/labels as inputs, LLM tokens form compact, well-separated clusters with high semantic density, whereas visual tokens are diffuse. This disparity is quantified by the ratio of interto intra-class distance (19.283 0.748). (b) We measure inter-expert diversity using singular value decomposition on each MoE layers expert weight matrices and computing the mean similarity of the subspaces spanned by their top-k left singular vectors (Hu et al., 2021). Incorporating routing guidance (Ours) enhances expert diversity. DiffMoE (Shi et al., 2025), which introduces global token-distribution routing scheme, still reports relatively limited improvements. This pronounced gap between MoEs transformative impact in LLMs and its modest returns in DiT models motivates fundamental question: What are the underlying factors that impede the effectiveness of MoE in DiT models? To answer this question, we examine how linguistic and visual inputs differ in models and highlight the following two distinctive properties of visual inputs. 1) High Spatial Redundancy. Unlike discrete text tokens, which are semantically dense with salient inter-token differences, visual tokens (i.e., image patches) are continuous, spatially coupled, and substantially redundant (Fig. 1(a)). The high correlation between patches often leads experts to learn homogeneous features. 2) Functional Heterogeneity. The practice of classifier-free guidance (Ho & Salimans, 2022) in diffusion models inherently introduces two functionally distinct input types: conditional and unconditional. naive MoE treats them uniformly with undifferentiated routing, ignoring their different roles. These properties collectively impede effective expert diversity and specialization (Fig. 1(b)). Motivated by these observations, we revisit the foundational principle of MoE design: expert specialization, in which each expert acquires focused and non-overlapping knowledge (Dai et al., 2024; Cai et al., 2025). We decompose this objective into two criteria: Intra-Expert Coherence, which ensures that an expert consistently processes similar patterns, maintaining stable functional role; and Inter-Expert Diversity, which encourages different experts to specialize in distinct tasks to achieve functional differentiation. In language modeling, the semantic density and separability of discrete text tokens provide potent inductive bias that naturally fosters expert specialization, satisfying both criteria. In contrast, for visual inputs, the combination of intrinsic redundancy and extrinsic functional heterogeneity makes expert specialization non-trivial. Therefore, in this paper, we move beyond implicit expert allocation, and introduce explicit routing guidance designs to promote both intra-expert coherence and inter-expert diversity. To this end, we present ProMoE, Mixture-of-Experts framework featuring two-step router with explicit routing guidance to promote expert specialization. Specifically, this guidance provides two distinct routing signals: the tokens functional role and its semantic content. Guided by these signals, the router implements two steps: conditional routing and prototypical routing. First, conditional routing addresses functional heterogeneity by partitioning visual tokens into unconditional and conditional sets. Unconditional image tokens, derived from image patches under null conditioning (e.g., empty labels or texts), are processed by dedicated unconditional experts. In contrast, conditional image tokens, obtained from patches under specific conditioning, are dispatched to standard MoE experts. This hard routing mechanism enforces functional segregation, fostering specialization across unconditional and standard experts. Second, prototypical routing further assigns conditional image tokens using set of learnable prototypes, each associated with specific expert, by computing cosine similarity between token embeddings and the prototypes in latent space. While prototypical routing is flexible and effective, it still relies on implicit learning from token semantics. Fortunately, its similarity-based allocation in latent space provides natural mechanism for injecting explicit semantic routing guidance. We validate the importance of semantic guidance in systematic experiments (Sec. 4.2), where both explicit (classification-based) and implicit (clusteringbased) guidance yield clear improvements. Building on this, we propose routing contrastive loss that explicitly enhances the prototypical routing process by assigning semantically similar tokens to the same expert while preserving distinct token distributions across experts. Compared with alternative guidance strategies, the proposed contrastive loss requires no manual labels and is more robust, promoting intra-expert coherence and inter-expert diversity in vision MoE. Extensive experimental results demonstrate ProMoEs superior performance and effective scalability on both Flow Matching and DDPM paradigms. Notably, ProMoE achieves significant gains over dense models despite using fewer activated parameters, and surpasses state-of-the-art methods that have 1.7 more total parameters than ours. In summary, our contributions are fourfold: 1) By analyzing differences between language and visual tokens, we present ProMoE, an MoE framework with explicit routing guidance for DiT models. 2) We design two-step router, where conditional routing first partitions image tokens by functional roles, and prototypical routing then refines assignments using learnable prototypes based on semantic content. 3) We propose routing contrastive loss that enhances prototypical routing, explicitly enforcing intra-expert coherence and inter-expert diversity. 4) Extensive experiments demonstrate that ProMoE outperforms dense models and state-of-the-art MoE methods across diverse settings."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Diffusion Models. Diffusion models (Ho et al., 2020; Nichol & Dhariwal, 2021) have made remarkable progress in visual synthesis. Early work (Rombach et al., 2022a; Podell et al., 2023; Wei et al., 2024b) primarily use U-Net (Ronneberger et al., 2015) architectures trained with the DDPM objective (Ho et al., 2020; Song et al., 2020). Recent models (Chen et al., 2023; Ma et al., 2024; Hatamizadeh et al., 2024; Chu et al., 2024; Esser et al., 2024a; Wei et al., 2025) have shifted to Diffusion Transformers (DiT) (Peebles & Xie, 2023), offering superior scalability and generative quality, and are trained with the more effective Rectified Flow (RF) (Liu et al., 2022), flow-matching formulation (Lipman et al., 2022) that constructs straight-line path between data and noise distributions. In this work, we adopt standard DiT backbone and train with both DDPM and RF objectives, demonstrating the effectiveness of our approach across different training paradigms. Mixture of Experts. Mixture-of-Experts (MoE) (Jacobs et al., 1991; Shazeer et al., 2017; Lepikhin et al., 2020) are designed to expand model capacity while minimizing computational overhead by sparsely activating sub-networks for distinct inputs. Inspired by MoE successes in LLMs (Dai et al., 2024; Liu et al., 2024; Li et al., 2025; Muennighoff et al., 2024), recent work has integrated MoE to scale diffusion models to improve generative quality (Riquelme et al., 2021). Early MoE applications in U-Net-based diffusion models (Lee et al., 2024; Balaji et al., 2022; Feng et al., 2023; Xue et al., 2023; Park et al., 2023; 2024; Zhao et al., 2024) often assign experts by diffusion timestep ranges, showing strong scaling potential. However, adapting MoE to DiT architecture (Shen et al., 2025; Sehwag et al., 2025; Cheng et al., 2025) faces several limitations. Token-choice routing methods (e.g., DiT-MoE (Fei et al., 2024)) suffer poor expert specialization due to imbalanced token assignments, whereas expert-choice methods (e.g., EC-DiT (Sun et al., 2024)) that fix token quotas per expert yield only marginal gains. More recently, DiffMoE (Shi et al., 2025) and Expert Race (Yuan et al., 2025) explore batch-level global token selection and mutual experttoken routing, yet still rely on implicit expert learning and struggle with limited expert specialization due to the redundancy and functional heterogeneity of visual tokens. In contrast, we analyze languagevision token differences and introduce explicit routing guidance to the MoE router based on the tokens functional role and its semantic content. We further enhance the routing process through the proposed routing contrastive loss, promoting intra-expert coherence and inter-expert diversity."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Diffusion Models. Diffusion models are generative models that learn data distributions by reversing forward noising process. The continuous-time forward process can be formulated as xt = αtx0 + σtϵ, with (0, 1) and ϵ (0, I). αt and σt are monotonically decreasing and increasing functions of t, respectively. For the reverse process, denoising network Fθ is trained to predict the target at each timestep t, conditioned on (e.g., class labels or text prompts): (cid:104) (cid:105) = Ex0,c,ϵ,t Fθ (xt, c, t)2 2 , (1) 3 Figure 2: Overview of ProMoE architecture. The input tokens are split by conditional routing into unconditional and conditional subsets. Unconditional image tokens are processed by unconditional experts. Conditional image tokens are assigned by prototypical routing with learnable prototypes. The routing contrastive learning explicitly enhances semantic guidance in prototypical routing. where the training target can be the Gaussian noise ϵ for DDPM models (Ho et al., 2020), or the vector field (ϵ x0) for Rectified Flow models (Liu et al., 2022). Mixture of Experts. Mixture-of-Experts (MoE) is an architectural paradigm that scales model capacity while preserving computational efficiency by selectively activating subset of experts sub-networks. standard MoE layer comprises NE experts and trainable router R. Each expert Ei is implemented as feed-forward network (FFN). Given input RBLD, where is the batch size, is the token length, is the hidden dimension, the router maps the input to tokenexpert affinity scores RBLNE via an activation function A: = A(R(X)). (2) At each forward pass, the router activates the top-K highest-scoring experts and dispatches the input to them. The final output is the weighted sum of the activated experts outputs with gating function: = (cid:26)S, if TopK(S) 0, Otherwise , MoE(X) = NE(cid:88) i= Gi Ei(X), (3) where RBLNE is the final gating tensor. There are two common routing paradigms in MoE: Token-Choice (TC) and Expert-Choice (EC). In TC, each token independently selects its top-K experts; in EC, each expert selects fixed number of top-K tokens."
        },
        {
            "title": "4 PROMOE",
            "content": "In this section, we present ProMoE, an MoE framework for DiTs that integrates two-step router with explicit routing guidance. The overall pipeline is depicted in Fig. 2. We first detail the two-step router in Sec. 4.1. We then validate the importance of semantic routing guidance in visual MoEs in Sec. 4.2 and further propose routing contrastive learning to enhance semantic guidance in Sec. 4.3."
        },
        {
            "title": "4.1 TWO-STEP ROUTER",
            "content": "The ProMoE router operates in two steps: conditional routing based on the tokens functional role, followed by fine-grained prototypical routing based on token semantics. 4 Conditional Routing. Unlike LLMs, diffusion models typically employ classifier-free guidance (CFG) (Ho & Salimans, 2022) at inference to enhance sample quality. Specifically, CFG steers the generation process by combining the models conditional and unconditional noise predictions. This paradigm naturally defines two functionally heterogeneous tokens: 1) unconditional image tokens, derived from image patches under null conditioning (e.g., empty labels or texts); and 2) conditional image tokens, obtained from patches under specific conditioning (e.g., class labels or texts). To handle different token types, the first step of the ProMoE router employs hard routing based on input conditioning. Specifically, unconditional image tokens are deterministically assigned to Nu unconditional experts, each implemented as feed-forward network (FFN), analogous to standard experts. Conversely, conditional image tokens are passed to the second step for fine-grained routing among standard MoE experts. This explicit partitioning encourages experts to learn the functional disparity between token types, facilitating the specialization of unconditional and standard experts. Prototypical Routing. The second step of our ProMoE router is to dispatch conditional image tokens for fine-grained expert allocation. Concretely, we introduce novel prototypical routing mechanism where the routing weights are parameterized by set of learnable prototypes RNE D, as illustrated in Fig. 2. Each prototype pi corresponds to an expert Ei and is trained to represent the shared characteristics of cluster of semantically similar tokens. Compared with standard MoE token assignment, which computes pre-activation scores RBLNE via linear layer, we assign tokens using cosine similarity, which is more effective and naturally suited for measuring semantic similarity in latent space between tokens and prototypes: Zi,j = [R(X)]i,j = α xip xipj , (4) where xi and pj are the i-th token in and the j-th prototype in P. α is scaling factor. Then, the activation function transforms the pre-activation scores into tokenexpert affinity scores S. Instead of softmax, which is computationally expensive and sensitive to sequence length, we opt for simple monotonic function that preserves relative rankings. We evaluate both sigmoid and identity functions, finding that the identity A(Z) = performs best in practice, as shown in Tab. 7. We argue that the identity activation enables direct top-K selection and provides stable training, thus improving performance. Consequently, we adopt identity activation as = Z. Finally, each conditional image token is routed to the top-K experts with gating scores G, as in Eq. (3). Forward Process. Besides unconditional and standard experts, we also incorporate Ns shared experts that process all tokens to learn shared knowledge (Dai et al., 2024; Cheng et al., 2025). For each token, the output of our MoE block is defined as the sum of the shared experts output and selective output determined by the token type (conditional or unconditional): MoE(x) = Ns(cid:88) ES (x) + i=1 (cid:124) (cid:123)(cid:122) Shared (cid:125) NE(cid:88) j=1 Nu(cid:88) k=1 Gj Ej(x) if Xc EU (x) if Xu , (5) where ES are conditional and unconditional image token sets, respectively, and Xc Xu = X. are the shared, standard, and unconditional experts, respectively. Xc and Xu , Ej, and EU To maintain constant number of activated parameters, MoE models often employ fine-grained expert segmentation (Dai et al., 2024), where the inner hidden dimension of each expert is divided by the number of activated experts. In our most settings, each forward pass of our model activates exactly two experts: the single shared expert and one expert selected from the combined pool of standard and unconditional experts. Therefore, to match the computational cost of dense model, we divide the hidden dimension of each experts intermediate layer by factor of two."
        },
        {
            "title": "4.2 SEMANTIC ROUTING GUIDANCE",
            "content": "Due to the inherent high spatial redundancy of visual tokens, naive MoE router fails to sufficiently distinguish tokens for effective routing, leading experts to learn homogeneous features. Consequently, additional semantic routing guidance is required to promote intra-expert coherence and 5 inter-expert diversity. To validate this, we conduct experiments by augmenting the MoE router with two guidance types: 1) Explicit Routing Guidance and 2) Implicit Routing Guidance. Explicit Routing Guidance. We design routing classification loss that uses class labels to explicitly guide token assignment. Specifically, we manually partition the 1K ImageNet classes into Nc superclasses based on coarse labels in (Feng & Patras, 2023), and allocate one expert per superclass. Since labels are sample-level, we instantiate the router as classifier C: we average-pool the input over the token length dimension to obtain X, feed into to produce sampleexpert affinity scores RBNc, and assign the expert with highest score. During training, we supervise the routing process with cross-entropy loss Lcls = CE(S, c), where is the superclass label. Implicit Routing Guidance. We replace the standard MoE router with k-means clustering, assigning all tokens in cluster to single expert. Unlike the routing classification loss that provides explicit supervision, this design offers implicit guidance by measuring token similarity, encouraging semantically similar tokens to be co-assigned. Concretely, we initialize NE cluster centroids by randomly sampling tokens. At each forward pass, we compute each tokens distances to all centroids to obtain distance-based tokenexpert affinity scores. Each token is then assigned to its nearest centroid and thus routed to the corresponding expert. During training, centroids are updated iteratively by replacing each with the mean of their currently assigned tokens. Table 1: Comparison results under Rectified Flow on ImageNet (256256) after 500K training steps, evaluated with CFG=1.5. Results for both routing guidance are reported in Tab. 1, with all MoE models having the same activated parameters and comparable total parameters to ensure fairness. On the base model size, DiT-MoE (Fei et al., 2024) and DiffMoE (Shi et al., 2025) yield limited performance improvements. In contrast, adding either explicit or implicit guidance produces substan165.45 tial gains. Notably, for both guidance strategies, 159.77 we disable the load-balancing loss to isolate its routing effects; despite its importance for TC routing, guidance alone still markedly improves performance. These findings highlight the pivotal role of semantic routing guidance in vision MoEs. Dense-DiT-B-Flow DiT-MoE-B-Flow DiffMoE-B-Flow Classification-based Routing K-Means-based Routing 131.13 131.66 137.46 Model (500K) 9.02 8.94 8.22 FID50K 5.91 6.24 IS"
        },
        {
            "title": "4.3 ENHANCING SEMANTIC ROUTING GUIDANCE VIA ROUTING CONTRASTIVE LEARNING",
            "content": "While the routing guidance strategies in Sec. 4.2 are effective, they suffer from key limitations: 1) The classification-based routing loss is defined at the sample level, restricting token-level flexibility and requiring costly manual annotations, hindering generalization. 2) Clustering-based routing supports only top-1 assignment, and struggles to scale to top-K, as methods like k-means rely on disjoint clusters, making multi-centroid assignment difficult. Moreover, k-means is sensitive to the number of clusters and the cluster initialization (Arthur & Vassilvitskii, 2006), reducing robustness. To address these limitations, we propose the Routing Contrastive Loss (RCL), as illustrated in Fig. 2, to explicitly enhance semantic guidance in prototypical routing. Given mini-batch of conditional image tokens, RCL encourages semantically similar tokens to be routed to the same expert and pushes dissimilar tokens toward different experts, prompting expert specialization in MoE. Concretely, for each prototype pi associated with expert Ei, tokens assigned to pi form the positive set, representing cluster of semantically similar tokens, while tokens dispatched to other prototypes constitute the negative sets, comprising multiple clusters with semantics different from pi. Next, RCL pulls each prototype pi toward the centroid of its positive token set to enforce intraexpert coherence, while pushing it away from the centroids of negative sets to encourage interexpert diversity. Let Xi denote the tokens assigned to expert Ei in mini-batch, its centroid mi (cid:80) Xi. The RCL loss is then computed over the is computed as the token mean: mi = 1 Xi prototypes of Na experts that are assigned tokens in an online manner: LRCL ="
        },
        {
            "title": "1\nNa",
            "content": "Na(cid:88) i=1 log exp(sim(pi, mi)/τ ) j=1 exp(sim(pi, mj)/τ ) (cid:80)Na , (6) where sim(a, b) = ab ab denotes cosine similarity, and τ is temperature hyperparameter. Furthermore, we empirically find that the push-away operation in RCL acts as load-balancing regu6 Table 2: Model configurations of ProMoE with different model sizes, aligning with DiT (Peebles & Xie, 2023). E14A1S1U1 denotes that total of 14 experts are used, with 1 expert activated for each token, 1 expert shared by all tokens, and 1 unconditional expert for unconditional image tokens. Model Config #Activated Params. #Total Params. #Experts #Blocks #Hidden dim. #Head ProMoE-S ProMoE-B ProMoE-L ProMoE-XL 33M 130M 458M 675M 75M 300M 1.063B 1.568B E14A1S1U1 E14A1S1U1 E14A1S1U1 E14A1S1U1 12 12 24 28 384 768 1024 1152 6 12 16 16 Table 3: Quantitative comparison with Dense DiTs under Rectified Flow on ImageNet (256256) after 500K training steps, evaluated with CFG scales of 1.0 and 1.5. Model (500K) # Activated Params. # Total Params. cfg=1.0 cfg=1. FID50K IS FID50K IS Dense-DiT-B-Flow ProMoE-B-Flow Dense-DiT-L-Flow ProMoE-L-Flow Dense-DiT-XL-Flow ProMoE-XL-Flow 130M 130M 458M 458M 675M 675M 130M 300M 458M 1.063B 675M 1.568B 30.61 24.44 15.44 11.61 13.38 9.44 49.89 60.38 84.20 100. 91.57 114.94 9.02 6.39 3.56 2.79 3.23 2.59 131.13 154.21 209.03 244. 227.05 265.62 larizer based on token semantics, and is more effective than traditional load-balancing loss (Shazeer et al., 2017) (see Appendix E.1). The final training loss of ProMoE is the combination of Eqs. (1) and (6), weighting LRCL by factor λRCL."
        },
        {
            "title": "5.1 EXPERIMENTAL SETUP",
            "content": "Baseline and model configurations. We compare against Dense-DiT (Peebles & Xie, 2023) and MoE baselines, including DiT-MoE (Fei et al., 2024), EC-DiT (Sun et al., 2024), and DiffMoE (Shi et al., 2025). For fair comparison, all MoE models are evaluated with equivalent activated parameters to the dense model and comparable total parameters, training with both DDPM (Ho et al., 2020) and Rectified Flow (Esser et al., 2024a) objectives. We scale ProMoE across four sizes (S/B/L/XL) to align with established DiT benchmarks, as shown in Tab. 2. Models are named as: [Model]- [Size]-[Training Type], with an additional expert configuration. For instance, expert configuration E14A1S1U1 denotes 14 total experts (E14), top-1 activation (A1) over 12 standard experts, 1 shared expert (S1), and 1 unconditional expert (U1). More details are provided in Appendix A. Implementation details. We conduct experiments on class-conditional image generation using the ImageNet (Deng et al., 2009) dataset, which contains 1,281,167 training images across 1,000 classes. Following (Peebles & Xie, 2023), we train all models with the AdamW optimizer with learning rate of 1e-4. The batch size is 256, and weight decay is 0. We use horizontal flips as the only data augmentation, and pretrained VAE from Stable Diffusion (Rombach et al., 2022b) to encode and decode images. We also maintain an exponential moving average (EMA) of model parameters during training with decay rate of 0.9999, and all reported results use the EMA mode. Evaluation metrics. We evaluate image generation quality of all methods using Frechet Inception Distance (FID) (Heusel et al., 2017; Dhariwal & Nichol, 2021), calculated over 50K generated samples with 250 DDPM or Flow Matching Euler sampling steps. We also report Inception Score (IS) (Salimans et al., 2016) to measure the diversity of generated images."
        },
        {
            "title": "5.2 MAIN RESULTS",
            "content": "Comparison with Dense DiT. The results in Fig. 3(a) and Tabs. 3 and 4 draw three conclusions: 1) ProMoE consistently surpasses dense counterparts at equivalent activated parameters across all sizes, objectives, and CFG settings, demonstrating strong effectiveness, scalability, and generalization. 2) Gains are more pronounced under Rectified Flow, the current dominant training paradigm, highlighting ProMoEs ability to scale modern diffusion models. Compared to dense models, with7 Table 4: Quantitative comparison with Dense DiTs under DDPM on ImageNet (256256) after 500K training steps, evaluated with CFG scales of 1.0 and 1.5. Model (500K) # Activated Params. # Total Params. cfg=1.0 cfg=1.5 FID50K IS FID50K IS Dense-DiT-B-DDPM ProMoE-B-DDPM Dense-DiT-L-DDPM ProMoE-L-DDPM Dense-DiT-XL-DDPM ProMoE-XL-DDPM 130M 130M 458M 458M 675M 675M 130M 300M 458M 1.063B 675M 1.568B 41.19 40. 20.81 18.75 17.67 15.87 35.94 37.84 65.51 73.07 74.05 81.90 18.61 17. 6.29 5.12 5.07 4.11 78.71 82.65 148.38 168.91 165.81 187.86 Figure 3: Comparisons and scaling results across diverse settings. Figure 4: Samples generated by ProMoE-XL-Flow after 2M iterations with cfg=4.0. out CFG, ProMoE-L-Flow reduces FID by 24.8% and increases IS by 19.7%; at the largest scale, ProMoE-XL-Flow reduces FID by 29.4%. With CFG=1.5, ProMoE-B-Flow reduces FID by 29.2%, while ProMoE-XL-Flow reduces FID by 19.8%. 3) ProMoE is notably parameter-efficient; it uses fewer activated parameters yet outperforms dense models with more. Specifically, ProMoE-L-Flow achieves FID 11.61/2.79 at CFG 1.0/1.5, versus 13.38/3.23 for Dense-DiT-XL-Flow. Comparison with MoE SOTAs. The results in Fig. 3(a), Tab. 5 and Appendix Tab. 9 show that ProMoE outperforms all baselines across both objectives at equivalent activated parameters, with and without CFG. Without CFG, ProMoE-L-Flow reduces FID by 19.7% and increases IS by 15.2% relative to DiffMoE-L-Flow; with CFG=1.5, it reduces FID by 20.5% and increases IS by 14.8%. Notably, ProMoE-L-Flow (1.063B params) surpasses the larger DiffMoE-L-Flow with 16 experts (1.846B params), despite fewer total parameters, underscoring the effectiveness of our method. Visualization Results. Fig. 4 shows the samples generated by ProMoE-XL-Flow on ImageNet (256256) after 2M training steps with CFG=4.0; see Appendix C.3 for more analyses and results. Comparison of training losses. Fig. 5 shows training loss curves for our method, the dense model, and MoE baselines. ProMoE attains lower loss and faster convergence, even at the largest scale with extended training (up to 1.2M steps)."
        },
        {
            "title": "5.3 SCALING BEHAVIOR",
            "content": "Figure 5: Training loss curve comparisons. Scaling the model size. As shown in Fig. 3(b), ProMoE exhibits consistent performance improvements over Dense-DiT when scaling from base (B) to large (L) to XL, with 130M, 458M, and 675M activated parameters, respectively, thereby validating the scalability of our method. 8 Table 5: Quantitative comparison with MoE baselines under Rectified Flow on ImageNet (256256) after 500K training steps, evaluated with CFG scales of 1.0 and 1.5. Activated Params. Total Params. Model (500K) #Experts cfg=1.5 cfg=1.0 # # FID50K IS FID50K IS E8A1S0N0 DiT-MoE-L-Flow E8A1S0N0 EC-DiT-L-Flow DiffMoE-L-Flow E8A1S0N0 DiffMoE-L-Flow E16A1S0N0 E14A1S1N1 ProMoE-L-Flow 458M 458M 458M 458M 458M 1.163B 1.163B 1.095B 1.846B 1.063B 16.57 15.58 14.46 13.55 11.61 80.25 84.11 87.55 92.33 100.82 4.10 3.65 3.51 3.30 2.79 199.05 209.06 212.78 222.40 244.21 Table 6: Ablation study of each component on ImageNet (256256) after 500K training steps, trained with Rectified Flow and evaluated with CFG scales of 1.0 and 1.5. Model (500K) cfg=1.0 cfg=1.5 FID50K IS FID50K IS Dense-DiT-B-Flow + Prototypical Routing + Routing Contrastive Learning + Conditional Routing 30.61 27.93 24.97 24.44 49.89 53.35 58.59 60.38 9.02 7.92 6.75 6.39 131.13 140.86 150.15 154.21 Table 7: Ablation of activation functions, trained with Rectified Flow on ImageNet (256256) for 500K steps. Activation (500K)"
        },
        {
            "title": "Softmax\nSigmoid\nIdentity",
            "content": "cfg=1.0 cfg=1.5 FID 25.74 25.49 24.44 IS FID IS 58.04 58.51 60.38 6.92 6.63 6.39 149.11 150.94 154.21 Table 8: Ablation of conditional routing in K-Means-based Routing, trained with Rectified Flow on ImageNet (256256) for 500K steps. cfg=1.5 cfg=1. K-Means-based Routing (500K) w/o Cond. w/ Cond. FID 30.12 25.61 IS FID IS 50.47 59.76 8.75 6.24 133.14 159.77 Scaling the number of experts. Fig. 3(c) shows monotonic gains as the expert number increases from 4 to 16, with 1 shared and 1 unconditional expert per setup. For fair comparison, we maintain comparable total parameters over MoE baselines and use 14 experts across settings."
        },
        {
            "title": "5.4 ABLATION STUDIES",
            "content": "Ablation on each component. The results in Tab. 6 show that using prototypical routing alone improves performance and already surpasses DiT-MoE-B-Flow and DiffMoE-B-Flow (see Tab. 1). Adding routing contrastive learning yields substantial gains, reducing FID by 10.6% and increasing IS by 9.8%, highlighting the importance of semantic routing guidance. Incorporating conditional routing further lowers FID and raises IS. These results validate the effectiveness of each component. Ablation on score activation function. Since our prototypical routing computes similarities in latent space, choosing an appropriate activation to map similarities into routing scores is crucial. As shown in Tab. 7, the identity mapping yields the best performance, sigmoid is second-best, and softmax performs worst. Consequently, we adopt the identity function as the score activation. Ablation on conditional routing. We emphasize that the proposed conditional routing is general, method-agnostic component that can benefit other routing schemes. We ablate it within the K-Meansbased Routing method in Sec. 4.2. As shown in Tab. 8, removing conditional routing significantly degrades performance, underscoring its importance."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we present ProMoE, Mixture-of-Experts framework featuring two-step router with explicit routing guidance to promote expert specialization. We analyze differences between language and vision tokens: discrete text tokens are semantically dense, whereas visual tokens exhibit high spatial redundancy and functional heterogeneity, hindering the effectiveness of MoE in DiT models. To address this, we introduce routing guidance based on the tokens functional role and semantic content, yielding two-step router comprising conditional routing and prototypical routing. 9 Furthermore, we propose routing contrastive loss that enhances semantic guidance in prototypical routing, explicitly promoting intra-expert coherence and inter-expert diversity. Extensive experiments demonstrate that ProMoE outperforms dense DiT and existing MoE SOTAs, even with fewer activated or total parameters, providing robust solution for applying MoE to DiT models. Limitations. While we follow standard evaluation protocols and report FID50K and IS, these metrics may not fully capture fine-grained perceptual quality or semantic faithfulness. Moreover, we validate ProMoE only on image generation; extending it to multiple modalities remains an open and meaningful direction that we leave for future work."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "Our method achieves substantial improvements on the ImageNet benchmark over dense DiT and state-of-the-art MoE methods, providing an effective solution for scaling DiT with MoE. Nonetheless, it inherits common risks of generative models, such as the potential to create fake data. Robust image forgery detection may help mitigate these concerns. In addition, we adhere to ethical guidelines in all experiments."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We make the following efforts to ensure the reproducibility of ProMoE: (1) All experiments are (2) Our code and trained model conducted on the publicly available ImageNet-1K benchmark. weights will be made publicly available. (3) We provide implementation details in Sec. 5.1 and Appendix A."
        },
        {
            "title": "REFERENCES",
            "content": "David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. Technical report, Stanford, 2006. Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. survey on mixture of experts in large language models. IEEE Transactions on Knowledge and Data Engineering, 2025. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Kun Cheng, Xiao He, Lei Yu, Zhijun Tu, Mingrui Zhu, Nannan Wang, Xinbo Gao, and Jie Hu. Diff-moe: Diffusion transformer with time-aware and space-adaptive experts. In Forty-second International Conference on Machine Learning, 2025. Xiangxiang Chu, Jianlin Su, Bo Zhang, and Chunhua Shen. Visionllama: unified llama backbone for vision tasks. In European Conference on Computer Vision, pp. 118. Springer, 2024. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 10 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024a. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024b. Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformers to 16 billion parameters. arXiv preprint arXiv:2407.11633, 2024. Chen Feng and Ioannis Patras. Maskcon: Masked contrastive learning for coarse-labelled dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1991319922, 2023. Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1013510145, 2023. Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision In European Conference on Computer Vision, pp. 3755. transformers for image generation. Springer, 2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, arXiv preprint and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Yunsung Lee, JinYoung Kim, Hyojun Go, Myeongho Jeong, Shinhyeok Oh, and Seungtaek Choi. In Proceedings of the AAAI Conference on Multi-architecture multi-expert diffusion models. Artificial Intelligence, volume 38, pp. 1342713436, 2024. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025. 11 Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pp. 2340. Springer, 2024. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, et al. Olmoe: Open mixture-of-experts language models. arXiv preprint arXiv:2409.02060, 2024. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, and Changick Kim. Denoising task routing for diffusion models. arXiv preprint arXiv:2310.07138, 2023. Byeongjun Park, Hyojun Go, Jin-Young Kim, Sangmin Woo, Seokil Ham, and Changick Kim. Switch diffusion transformer: Synergizing denoising tasks with sparse mixture-of-experts. In European Conference on Computer Vision, pp. 461477. Springer, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022a. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022b. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234241. Springer, 2015. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, and Lingjuan Lyu. Stretching each dollar: Diffusion training from scratch on micro-budget. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2859628608, 2025. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. 12 Ying Shen, Zhiyang Xu, Jiuhai Chen, Shizhe Diao, Jiaxin Zhang, Yuguang Yao, Joy Rimchala, Ismini Lourentzou, and Lifu Huang. Latte-flow: Layerwise timestep-expert flow-based transformer. arXiv preprint arXiv:2506.06952, 2025. Minglei Shi, Ziyang Yuan, Haotian Yang, Xintao Wang, Mingwu Zheng, Xin Tao, Wenliang Zhao, Wenzhao Zheng, Jie Zhou, Jiwen Lu, et al. Diffmoe: Dynamic token selection for scalable diffusion transformers. arXiv preprint arXiv:2503.14487, 2025. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Haotian Sun, Tao Lei, Bowen Zhang, Yanghao Li, Haoshuo Huang, Ruoming Pang, Bo Dai, and Nan Du. Ec-dit: Scaling diffusion transformers with adaptive expert-choice routing. arXiv preprint arXiv:2410.02098, 2024. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and Hongming Shan. Dreamvideo: Composing your dream videos with customized subject and motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 65376549, 2024a. Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, et al. Dreamvideo-2: Zero-shot subject-driven video customization with precise motion control. arXiv preprint arXiv:2410.13830, 2024b. Yujie Wei, Shiwei Zhang, Hangjie Yuan, Biao Gong, Longxiang Tang, Xiang Wang, Haonan Qiu, Hengjia Li, Shuai Tan, Yingya Zhang, et al. Dreamrelation: Relation-centric video customization. arXiv preprint arXiv:2503.07602, 2025. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. Advances in Neural Information Processing Systems, 36:4169341706, 2023. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Yike Yuan, Ziyu Wang, Zihao Huang, Defa Zhu, Xun Zhou, Jingyi Yu, and Qiyang Min. Expert race: flexible routing strategy for scaling diffusion transformer with mixture of experts. arXiv preprint arXiv:2503.16057, 2025. Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Yibing Song, Gao Huang, Fan Wang, and Yang You. Dynamic diffusion transformer. arXiv preprint arXiv:2410.03456, 2024."
        },
        {
            "title": "A EXPERIMENTAL SETUP",
            "content": "Baselines. We compare against open-source state-of-the-art DiT-based MoE methods, with activated parameters equivalent to the dense model and comparable total parameter counts. We implement DiT-MoE (Fei et al., 2024), EC-DiT (Sun et al., 2024), and DiffMoE (Shi et al., 2025) following their original papers and referring to the open-source repository1. All methods are trained with the same training settings, including learning rate, batch size, and data augmentation. Implementation details. We train ProMoE with two objectives: the standard DDPM objective (Ho et al., 2020), and Rectified Flow with Logit-Normal sampling from SD3 (Esser et al., 2024a) to align better with modern DiT training paradigms (e.g., Sana (Xie et al., 2024)). Besides the hyperparameters in Sec. 5.1, we provide additional details as follows. In prototypical routing, α in Eq. (4) is set to 1. For routing contrastive learning, the temperature is set to 0.07, and the loss weight λRCL is 1 unless stated otherwise. Evaluation metrics. We follow the standard DiT evaluation protocol (Peebles & Xie, 2023), computing FID and IS on 50,000 generated images2 at classifier-free guidance scales of 1.0 and 1.5."
        },
        {
            "title": "B IMPLEMENTATION ALGORITHMS",
            "content": "The implementation algorithm of ProMoE is provided in Algorithm 1. In the algorithm, input class labels are used solely to distinguish conditional image tokens from unconditional image tokens. During inference, if classifier-free guidance (CFG) is disabled, all tokens are routed to the standard experts and the shared expert; the unconditional expert is not used. If CFG is enabled, class labels are replaced with batch-level binary mask indicating which samples receive conditioning (i.e., treated as conditional image tokens). No routing contrastive loss is computed during inference."
        },
        {
            "title": "C MORE RESULTS",
            "content": "C.1 MORE T-SNE VISUALIZATIONS OF LANGUAGE AND VISUAL TOKENS To further validate the findings on differences between language and visual tokens in Sec. 1, we extend the visualization results in Fig. 1(a). Figs. 10 and 11 present t-SNE visualizations of token embeddings from DiT-XL/2 (Peebles & Xie, 2023) and Llama-3 8B (Dubey et al., 2024) across different layers; for DiT-XL/2, we also visualize tokens at different diffusion timesteps. To facilitate comparison, we cluster token embeddings into 10 groups using k-means. For model inputs, we randomly sample 110 ImageNet classes, feed the corresponding class labels to DiT-XL/2 and the class names to Llama-3 8B, and randomly select 1K intermediate-layer tokens for visualization. The results in Figs. 10 and 11 further confirm that language tokens are semantically dense with high inter-token differences, whereas visual tokens exhibit high spatial redundancy. C.2 T-SNE VISUALIZATIONS OF TOKEN ASSIGNMENTS To assess the impact of visual-token redundancy on MoE expert selection, as indicated by Fig. 1(a) and Figs. 10 and 11, we visualize intermediate-layer token assignments of ProMoE-L-Flow and DiTMoE-L-Flow at 500K training steps without classifier-free guidance, as shown in Fig. 6. Following Sec. C.1, we randomly sample 110 ImageNet classes, feed the corresponding class labels to both ProMoE and DiT-MoE, and randomly select 2,560 tokens from an intermediate-layer MoE block to visualize the expert selection of each token. Compared with token-choice MoE methods such as DiT-MoE, our approach assigns experts according to token semantics, producing well-formed clusters in the token-embedding space: semantically similar tokens form compact clusters and are routed to the same expert, whereas clusters assigned to different experts are clearly separated. These 1https://github.com/KwaiVGI/DiffMoE 2https://github.com/openai/guided-diffusion/tree/main/evaluations 14 results further corroborate the importance of explicit routing guidance for visual MoE, and our method achieves effective intra-expert coherence and inter-expert diversity. Figure 6: t-SNE visualization results of ProMoE and DiT-MoE on expert allocation (token assignment). Each color corresponds to single expert. C.3 MORE VISUALIZATION RESULTS We provide additional generation results in Fig. 12. Our method produces high-quality images across both simple and challenging categories. C.4 MORE COMPARISON RESULTS We provide additional comparisons with dense models and MoE SOTAs. Besides the FID results in Fig. 3(a), we also report Inception Score comparisons, as shown in Fig. 7. In addition, we present quantitative comparisons with MoE SOTAs under the DDPM objective in Tab. 9. Across both training objectives and CFG settings, our method consistently outperforms the dense model and existing MoE SOTAs, demonstrating its effectiveness. Table 9: Quantitative comparison with MoE baselines under DDPM on ImageNet (256256) after 500K training steps, and evaluated at CFG scales of 1.0 and 1.5. Model (500K) #Experts # Activated Params. # Total Params. cfg=1.0 cfg=1.5 FID50K IS FID50K IS DiT-MoE-L-DDPM E8A1S0N0 E8A1S0N0 EC-DiT-L-DDPM DiffMoE-L-DDPM E8A1S0N0 ProMoE-L-DDPM E14A1S1N1 458M 458M 458M 458M 1.163B 1.163B 1.095B 1.063B 23.12 20.76 19.45 18. 60.08 64.77 70.93 73.07 7.55 6.48 5.47 5.12 133.63 146.49 158.30 168.91 Figure 7: Comparison with dense model and MoE SOTAs on Inception Score. 15 C.5 COMPARISON WITH DENSE DIT ON MORE TRAINING STEPS We provide comparison results between our ProMoE and Dense DiT on more training steps in Tab. 10. We observe that ProMoE-L-Flow at 500K steps surpasses Dense-DiT-L-Flow at 1M steps on FID, and ProMoE-XL-Flow at 500K steps surpasses Dense-DiT-XL-Flow at 1M steps on both FID and IS. With longer training, ProMoE-L-Flow at 1M steps outperforms Dense-DiT-L-Flow at 2M steps and Dense-DiT-XL-Flow at 1M steps. These findings are consistent with those in Sec. 5.2, demonstrating faster convergence and scalability of our method. Table 10: Quantitative comparison with Dense DiTs under Rectified Flow on ImageNet (256256) after more training steps, evaluated with CFG scales of 1.0 and 1.5. Model (Training Steps) Dense-DiT-L-Flow (1M) ProMoE-L-Flow (500K) ProMoE-L-Flow (1M) Dense-DiT-L-Flow (2M) ProMoE-L-Flow (2M) Dense-DiT-XL-Flow (1M) ProMoE-XL-Flow (500K) ProMoE-XL-Flow (1M) # Activated Params. # Total Params. cfg=1.0 cfg=1. FID50K IS FID50K IS 458M 458M 458M 458M 458M 675M 675M 675M 458M 1.063B 1.063B 458M 1.063B 675M 1.568B 1.568B 12.21 11.61 9.88 10.55 9. 10.67 9.44 8.34 100.97 100.82 118.91 112.55 125.88 107.68 114.94 128.58 2.97 2.79 2.75 2.81 2. 2.82 2.59 2.53 245.63 244.21 278.22 266.24 290.61 260.61 265.62 292.38 C."
        },
        {
            "title": "INCREASING THE NUMBER OF ACTIVATED EXPERTS",
            "content": "As discussed in Sec. 4.2, classificationand clustering-based routing inherently do not support top-k assignment, permitting only top-1. In contrast, ProMoE is more flexible and scalable, and supports top-k assignment. To validate this, we increase the number of activated standard experts from 1 to 3, which raises the activated parameters while keeping the total parameter count unchanged. As shown in Tab. 11, this increase yields improved performance, confirming the effectiveness and scalability of our method. Table 11: Results of increasing the number of activated standard experts on ImageNet (256256) after 500K steps, trained with Rectified Flow and evaluated at CFG scales 1.0 and 1.5. Model (500K) #Experts # Activated Params. # Total Params. cfg=1.0 cfg=1.5 FID50K IS FID50K IS ProMoE-L-Flow E14A1S1N1 ProMoE-L-Flow E14A3S1N1 458M 558M 1.063B 1.063B 11.61 11. 100.82 103.78 2.79 2.72 244.21 246."
        },
        {
            "title": "D MORE RESULTS ON SCALING BEHAVIOR",
            "content": "D.1 SCALING MODEL SIZE Fig. 3(b) shows FID results for model size scaling at CFG=1.0. We additionally report FID results at CFG=1.5 and Inception Score at CFG=1.0 and 1.5, as shown in Fig. 8. ProMoE consistently outperforms its dense counterparts, and ProMoE-L-Flow surpasses Dense-XL-Flow in terms of FID and Inception Score at both CFG=1.0 and 1.5, despite using fewer activated parameters. These observations are consistent with those in Sec. 5.2. D.2 SCALING THE NUMBER OF EXPERTS We report Inception Score for scaling the number of experts at CFG=1.0 and 1.5, and FID at CFG=1.5, as shown in Fig. 9. Performance of ProMoE improves as the number of experts increases, demonstrating the scalability of our approach. 16 Figure 8: More scaling results on model size. Figure 9: More scaling results on the number of experts."
        },
        {
            "title": "E MORE ABLATION STUDIES",
            "content": "E.1 ABLATION ON LOAD-BALANCING LOSS As discussed in Sec. 4.3, the push-away term in our routing contrastive learning (RCL) serves role similar to load balancing. We verify this with an ablation in Tab. 12. Adding conventional load-balancing loss on top of our method slightly degrades performance. We attribute this to RCLs explicit semantic guidance: it leverages token semantics to maintain diverse expert assignments, whereas load balancing loss only regularizes token counts and ignores assignment quality and semantics, thereby interfering with RCL. These results indicate that the semantic routing guidance from RCL is more effective than traditional load-balancing losses. Table 12: Ablation study of using load-balancing loss under Rectified Flow on ImageNet (256256) after 500K training steps. Model (500K) cfg=1.0 cfg=1. FID50K IS FID50K IS w/ load-balancing loss w/o load-balancing loss 24.98 24. 59.04 60.38 6.53 6.39 151.37 154.21 E.2 ABLATION ON LOSS WEIGHT OF ROUTING CONTRASTIVE LEARNING We vary the loss weight of routing contrastive learning (RCL) and report the results in Tab. 13. We observe that RCL is insensitive to the loss weight, as increasing it from 1 to 10 yields only marginal gains. Therefore, we use default weight of 1 for all experiments, except for ProMoE-B-DDPM, which uses weight of 10 based on this ablation study. USAGE OF LARGE LANGUAGE MODELS (LLMS) In accordance with the ICLR 2026 policy, we report our use of large language model (LLM) in preparing this manuscript. The LLMs role was strictly confined to language polishing, such as correcting grammar, refining wording, and improving readability. All scientific contributions, including the ideation, methodology, experimental design, and final conclusions, are entirely our own. The LLM was used solely as writing-enhancement tool and did not contribute to the scientific aspects of the work. We have reviewed the manuscript and take full responsibility for its content. 17 Table 13: Ablation study of λRCL in ProMoE-B-DDPM on ImageNet (256256) after 500K training steps. λRCL (500K) cfg=1.0 cfg=1. FID50K IS FID50K IS 1 2 5 10 40.48 40.37 40.33 40. 36.77 37.46 37.08 37.84 18.34 18.01 18.03 17.90 80.07 81.88 81.1 82.65 c.shape[0] Initialize final output reshape(Xc, (1, D)) Get mask of unconditional image tokens Get mask of conditional image tokens Get unconditional image tokens Get conditional image tokens Algorithm 1 ProMoE Layer (Training) Input: RBLD (input sequence), ZB (batch labels) Variables: NE (number of standard experts), (number of activated standard experts), RNE (Learnable prototypes for routing), (List of standard expert FFNs), EU (Unconditional expert FFN), ES (Shared expert FFN), λRCL (coef of Routing contrastive loss), τ (temperature) 1: Initialize: zeros like(X) 2: /*** Step 1. Functional Routing ***/ 3: Mu (c == empty conditioning) 4: Mc Mu 5: Xu X[expand(Mu)] 6: Xc X[expand(Mc)] 7: /*** Step 2. Unconditional Image Tokens Processing ***/ 8: OU EU(Xu) 9: O[Mu] OU 10: if any(Mc) then 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: end if 31: /*** Step 6. Shared Expert Processing ***/ 32: + ES(X) 33: Return: O, aux loss /*** Step 3. Prototypical Routing ***/ nc RncNE L2 Normalize(X Identity(Z) RncK, indices ZncK TopK(S, K) /*** Step 4. Conditional Image Tokens Processing ***/ zeros like(X c) for 0 to NE 1 do end for OC reshape(O O[Mc] O[Mc] + OC /*** Step 5. Routing Contrastive Learning ***/ aux loss λRCL LRCL(Xc, indices, P, τ ) Flatten conditional image tokens Get number of conditional image tokens Get pre-activation scores Get tokenexpert affinity scores Get gating tensor and indices Gi sum(G[mi] (indices[mi] == i), dim = 1) end if mi (indices == i).any(dim = 1) if any(mi) then C[mi] + Gi.unsqueeze(1) Ei(X Final gating scores Update final output Mask of tokens routed to expert c) L2 Normalize(P) C[mi] C, Xc.shape) c[mi]) 18 Figure 10: More t-SNE visualization results of Llama-3 8B on different layers. Figure 11: More t-SNE visualization results of DiT-XL/2 on different layers and diffusion timesteps. Figure 12: More samples generated by ProMoE-XL-Flow after 2M iterations with cfg=4.0."
        }
    ],
    "affiliations": [
        "Fudan University",
        "MMLab",
        "The University of Hong Kong",
        "Tongyi Lab, Alibaba Group",
        "Zhejiang University"
    ]
}