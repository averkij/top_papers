{
    "paper_title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
    "authors": [
        "Junchen Liu",
        "Sven Elflein",
        "Or Litany",
        "Zan Gojcic",
        "Ruilong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity."
        },
        {
            "title": "Start",
            "content": "Test-Time Training with KV Binding Is Secretly Linear Attention Junchen Liu * 1 2 3 Sven Elflein 1 2 3 Or Litany 1 4 Zan Gojcic 1 Ruilong Li * 1 6 2 0 2 4 2 ] . [ 1 4 0 2 1 2 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as form of online meta-learning that memorizes keyvalue mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that broad class of TTT architectures can be expressed as form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides systematic reduction of diverse TTT variants to standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity. 1. Introduction Test-Time Training (TTT) has emerged as powerful paradigm for dynamic model adaptation. Initially introduced to address distribution shift by updating model parameters on unlabeled test inputs, TTT has since evolved into distinct architectural primitive (Sun et al., 2020). Recent work has increasingly framed TTT as an alternative to standard softmax attention in transformers, offering favorable properties like linear-time compute and constant memory usage during autoregressive inference (Zhang et al., 2025; Behrouz et al., 2024). Among the various TTT formulations, this paper focuses on TTT with KV binding (Sun et al., 2025; Zhang et al., 2025; Han et al., 2025; Behrouz et al., 2024), which optimizes self-supervised key-value association objective in the inner loop, as opposed to end-to-end methods that backpropagate from the final task loss (Tandon et al., 2025). The prevailing interpretation of TTT casts it as 1NVIDIA 2University of Toronto 3Vector Institute 4Technion. Correspondence to: Ruilong Li <ruilongl@nvidia.com>. Preprint. February 25, 2026. 1 form of online meta-learning or memorization (Sun et al., 2025; Finn et al., 2017; Metz et al., 2018), where the inner loop dynamically constructs temporary key-value (KV) map by optimizing neural network (e.g., an MLP) on previously observed tokens. The subsequent inference step is viewed as querying this stored knowledge. This perspective has led to increased complexity in recent architectural designs, motivating the use of sophisticated optimizers, normalization schemes, and deep inner-loop networks (Zhang et al., 2025; Han et al., 2025; Behrouz et al., 2024; 2025a; Dalal et al., 2025), all explicitly designed to improve the fidelity of this memorization. The Memorization Paradox. Yet, this interpretation of TTT as test-time memorization can be directly contradicted by empirical evidence. If TTT would truly function by explicitly learning and retrieving keyvalue associations, its behavior would conform to basic principles of memory formation and optimization dynamics. Contrary to this expectation, we identify systematic anomalies that directly contradict this hypothesis: Distributional Asymmetry. Unlike standard attention, in which queries and keys share the same semantic space, converged TTT models exhibit significant distributional mismatch between queries and keys. Replacing queries with keys. Replacing queries with keys for TTT models has negligible effect on the task performance, suggesting that queries do not play functional retrieval role as in standard attention. Optimization vs. Performance. Counterintuitively, improvements in the inner loop, which can be interpreted as stronger memorization, do not guarantee better downstream performance. The Gradient Ascent Anomaly. Most strikingly, we find that replacing inner-loop gradient descent with gradient ascent always preserves, and in some cases even improves, task performance. These observations collectively challenge the prevailing view of TTT as an online meta-learning or key-value memorization mechanism. TTT is Secretly Linear Attention. Motivated by these observations, and by prior work showing that TTT reduces to linear attention in the restricted case of single linear innerTest-Time Training with KV Binding Is Secretly Linear Attention loop layer with zero initialization (Sun et al., 2025), we revisit the mathematical formulation of TTT. We show analytically that even TTT variants with complex fast-weight parameterizations (including multi-layer MLPs and momentum) can be equivalently rewritten as form of learned linear attention operator (Katharopoulos et al., 2020). Under this unified view, the inner loop does not perform meta learning in the conventional sense. Instead, it induces structured, history-dependent mixing of query, key, and value vectors. This perspective resolves the empirical paradoxes identified above: gradient ascent preserves performance because sign inversions are absorbed into the learned value projection, and distributional symmetry between queries and keys is unnecessary because the mechanism operates as feature mixer rather than similaritybased retrieval system. Practical Implications. Unmasking TTT as Linear Attention is not just theoretical exercise; it unlocks significant practical benefits. By adopting this perspective, we: Simplify. We show that many components introduced in prior TTT architectures, such as weight normalization and momentum, are often redundant. Parallelize. We derive fully parallel form of TTT that achieves up to 4.0 inference throughput on attention calculation while maintaining performance. Generalize. We provide systematic reduction of diverse TTT variants to common linear attention form, suggesting that TTT is best understood as flexible learned linear attention mechanism with enhanced representational capacity which opens up an increased design space. 2. Related Work 2.1. Linear Attention Recurrent Neural Networks (RNNs) have recently gained renewed interest as efficient alternatives to standard Transformers (Vaswani et al., 2017). The introduction of linear attention (Katharopoulos et al., 2020) has accelerated advances in RNN-based architectures, leading to the incorporation of various mechanisms such as token-dependent decay factors (Gu et al., 2021; Smith et al., 2023; Orvieto et al., 2023; Peng et al., 2023; Sun et al., 2023) and, more recently, data-dependent decay (Qin et al., 2023; 2024; Peng et al., 2024; Gu & Dao, 2024; Dao & Gu, 2024; Zhang et al., 2024; Yang et al., 2024a). The data-dependent decay factor, termed the selective mechanism in Mamba (Gu & Dao, 2024; Dao & Gu, 2024), has been highlighted as crucial for strong incontext learning performance. DeltaNet (Schlag et al., 2021) conditions the update rule on both the current token and state for improved retrieval, and chunk-parallelization (Yang et al., 2024b) has enabled its efficient deployment in many recent architectures (Yang et al., 2024a; Peng et al., 2025a; Behrouz et al., 2024; Zhong et al., 2025; Team et al., 2025; Peng et al., 2025b; Lei et al., 2025; Liu et al., 2024). Notably, DeltaNet and its variants are equivalent to TTT with single linear layer and MSE loss (Yang et al., 2024a). 2.2. Test-Time Training Test-time training (TTT) broadly refers to methods that continue to update model parameters during inference. This concept was first introduced to address train-test distribution shift (Sun et al., 2020; Gandelsman et al., 2022), where models adapt to test data by optimizing self-supervised objective at inference time. Subsequently, TTT has been explored for improving inference-time performance in specific applications such as 3D reconstruction (Chen et al., 2024; 2025; Yuan et al., 2025). More recently, TTT has been developed as sequence modeling architecture with linear complexity, serving as an alternative to softmax attention in transformers (Sun et al., 2025; Wang et al., 2025; Zhang et al., 2025; Han et al., 2025; Dalal et al., 2025; Behrouz et al., 2024). When used as sequence modeling layer, TTT has two main variants: (1) methods that use key-value binding loss (e.g., dot-product or MSE loss) as the inner-loop objective (Sun et al., 2025; Zhang et al., 2025; Han et al., 2025; Behrouz et al., 2024), referred to as TTT-KVB in prior work (Tandon et al., 2025), and (2) methods that perform end-to-end backpropagation through the inner loop from the final task loss (e.g., cross-entropy in language modeling), referred to as TTT-E2E (Tandon et al., 2025). This paper focuses on the first variant. TTT has demonstrated effectiveness across diverse tasks, including language modeling (Sun et al., 2025; Wang et al., 2025; Zhang et al., 2025), video generation (Dalal et al., 2025; Zhang et al., 2025), novel view synthesis (Zhang et al., 2025), and image classification (Han et al., 2025). In this paradigm, part of the model parameters (known as fast weights (Hinton & Plaut, 1987)) are updated online at test time using self-supervised key-value association loss, with the goal of memorizing history associations. Since this nested optimization is performed in-context, TTT is also referred to as in-context meta-learning (Finn et al., 2017; Metz et al., 2018) and is tightly related to the concept of fast weight programming (Schlag et al., 2021) and test-time scaling (Muennighoff et al., 2025; Snell et al., 2024). The design space of TTT is rich and has been actively explored. LaCT (Zhang et al., 2025) improves hardware utilization through large chunk sizes. Based on the key-value memorization perspective, other works have explored advanced test-time optimizers (Behrouz et al., 2024; Zhang et al., 2025; Karami et al., 2025) and alternative regression targets (Han et al., 2025; Behrouz et al., 2025b). Notably, 2 Test-Time Training with KV Binding Is Secretly Linear Attention 4. Empirical Contradictions to Memorization As we show in this section, the empirical behavior of TTT models consistently violates the properties implied by the storage-and-retrieval interpretation. 4.1. Better Inner Loss Leads to Worse Performance Under memorization-based interpretation, inner-loop loss serves as natural proxy for memorization quality: lower loss indicates more accurate encoding of keyvalue mappings and should therefore improve task performance. To test this property, we vary the number of inner-loop gradient steps at inference time, which usually yields lower inner-loop loss in pretrained TTT models (Zhang et al., 2025; Han et al., 2025), without changing the architecture, training data, or learned parameters. Figure 1 shows that despite improved inner-loop fitting, downstream performance degrades consistently as the number of inner-loop steps increases. This inverse relationship holds across both LLMs and novel view synthesis (NVS) tasks. Such behavior directly contradicts the memorizationbased interpretation of TTT, under which more accurate keyvalue fitting should be beneficial, or at least not harmful for the downstream performance. Instead, these results indicate that the inner loop affects model computation in fundamentally different manner, inconsistent with conventional notions of test-time memory. 4.2. TTT with Gradient Ascent The degradation in downstream performance with improved inner-loop fitting raises more fundamental question: is gradient-descent-based memorization in the inner loop necessary at all? To test this, we replace gradient descent in the inner loop with gradient ascent, effectively flipping the sign of all fast-weight gradients. Under memorization-based interpretation, this change should have strong negative effect, as gradient ascent explicitly worsens the fit to the keyvalue regression objective. Contrary to this expectation, across all evaluated models and tasks, TTT with gradient ascent performs comparably to, and in some cases even slightly better than, standard gradient descent  (Table 1)  . This holds despite the fact that gradient ascent consistently increases the inner-loop loss. Notably, in methods such as LaCT (Zhang et al., 2025), where the inner-loop loss is Frobenius inner product, flipping the gradient is equivalent to negating the loss itself. The fact that TTT remains effective even under such an inverted objective provides additional evidence that TTT does not rely on memorizing keyvalue mapping. Figure 1. Inner-Loop Optimization vs. Performance. Increasing inner-loop iterations improves inner-loop loss but degrades task performance, contradicting the memorization-based interpretation of TTT. Experiments are based on LaCT (Zhang et al., 2025). prior work has shown that when the inner loop consists of single linear layer, TTT can be reinterpreted as linear attention operator (Sun et al., 2025). In this work, we demonstrate that this interpretation extends to general TTT architectures with complex multi-layer MLPs as inner loops, and show that this perspective enables practical benefits including simplified formulations and efficient parallel implementations. 3. Preliminary We provide brief overview of the TTT mechanism that this paper focuses on. In TTT, each sequence modeling layer maintains set of fast weights fθ (typically lightweight MLP) that is updated during both training and inference. Given an input sequence, tokens are first projected into keys K, values , and queries (similar to standard attention). The core idea is to perform online gradient descent on fθ using self-supervised key-value binding objective: for each token, the key serves as input and the value serves as the regression target, i.e., = fθ(k) v2 (or dotproduct loss variant). After updating θ with this objective, the query is passed through the updated function fθ to produce the output. This mechanism is commonly interpreted as storage-andretrieval system (Sun et al., 2025; Finn et al., 2017): the inner-loop optimization memorizes key-value associations into fθ, which are later retrieved by querying the learned function. Under this view, architectural capacity, optimizer selection, and the number of inner-loop steps are all motivated by achieving more faithful memorization of keyvalue associations. This variant of test-time training, which optimizes keyvalue binding objective in the inner loop, is referred to as TTT-KVB in prior work (Tandon et al., 2025). Among the various TTT formulations (Sun et al., 2025; Gandelsman et al., 2022; Sun et al., 2020; Tandon et al., 2025), this paper exclusively focuses on this variant. 3 Test-Time Training with KV Binding Is Secretly Linear Attention Table 1. Observations Contradicting the Storage-and-Retrieval Interpretation of TTT. Replacing gradient descent with ascent breaks the storage interpretation, and replacing queries with keys breaks the retrieval interpretation, yet task performance remains mostly unchanged. Experiments are base on LaCT (Zhang et al., 2025) and ViTTT (Han et al., 2025). Model Perplexity (LaCT-LLM) PSNR (LaCT-NVS) Top-1 Acc (ViTTT) Baseline Gradient Ascent Replace with 16.43 16.19 16.18 25.94 25.85 25. 79.34 79.61 79.18 Figure 2. Distributional Asymmetry Between and K. tSNE visualizations of (Q, K) and (V, O) features in pretrained LaCT (Zhang et al., 2025) model on the NVS task, showing that the TTT inner loop is evaluated out of distribution and thus does not perform reliable retrieval. 4.3. Distributional Asymmetry Between and As we mentioned above, TTT is commonly interpreted as storage-and-retrieval mechanism. In the previous subsections, we showed that the storage aspect of this interpretation is empirically inconsistent with model behavior. We now examine whether the retrieval aspect provides plausible explanation. During the TTT inner-loop update, the parametric function (e.g., MLP) is optimized using keys as inputs and values as regression targets. After this update, queries are feed into the same function to produce outputs O. For such retrieval process to be effective, the distribution of must be close to that of the used during optimization. Otherwise, the parametric function is evaluated out-of-distribution, and its outputs are not expected to reliably lie on the manifold of . In other words, meaningful retrieval requires substantial distributional overlap between and K. To test this assumption, we analyze the distributions of and in pretrained LaCT (Zhang et al., 2025) model on the NVS task. For each layer, we collect and vectors across all tokens and visualize their distributions using t-SNE (Maaten & Hinton, 2008). As shown in Figure 2, 4 we observe pronounced and consistent mismatch between Q/K and /O across layers, indicating that the parametric function optimized on keys, is systematically evaluated on out-of-distribution inputs when applied to queries, at both training and inference time. Under such conditions, the resulting outputs cannot be interpreted as reliable retrieval of stored keyvalue information, and this mismatch persists regardless of how accurately the inner loop fits the keyvalue regression objective. 4.4. Replacing with The distributional asymmetry between queries and keys again raises more direct question: is the query representation necessary at all for TTT to function? To test this, we perform simple experiment in which we replace the query with the key when computing the In standard attention mechanisms, such TTT output. substitution would be highly disruptive. Because attention weights depend on querykey similarity, replacing with typically leads to degenerate behavior dominated by selfsimilarity and substantial drop in performance. Contrary to the expectation, we observe little to no degradation in TTTs performance. For both LaCT and ViTTT formulations performance remains comparable  (Table 1)  . This insensitivity to the query representation contradicts retrieval-based interpretation of TTT. If the inner loop relied on query-based access to stored information, replacing with should significantly alter model behavior. Instead, the observed invariance indicates that does not function as meaningful query into memorized key-value map. Summary. Taken together, these observations challenge the interpretation of test-time training as storage-and-retrieval mechanism. Downstream behavior is largely insensitive to both the quality and direction of inner-loop optimization, and to the presence of meaningful query signal. These failures suggest that in practice TTT does not operate as test-time memorization. 5. TTT is secretly Linear Attention If TTT is not functioning as memorization mechanism, how should its behavior be understood? An important clue comes from prior work, which shows that in the restricted setting of single linear inner-loop layer with zero initialization, TTT is exactly equivalent to linear attention (Sun et al., 2025). More generally, despite their apparent complexity, existing TTT variants share the defining computational properties of linear attention: linear-time computation and constant-size state with respect to sequence length. Motivated by this connection, we re-examine TTT by explicitly unrolling the inner-loop updates. We show analytically Test-Time Training with KV Binding Is Secretly Linear Attention that TTT induces linear attentionlike operator in general form, even when the inner loop consists of multi-layer non-linear mappings (Section 5.1). This perspective naturally explains our empirical observations that contradict the storage-and-retrieval interpretation (Section 5.2). Finally, we rewrite two representative variants of TTT, LaCT (Zhang et al., 2025) and ViTTT (Han et al., 2025), in their linear attention form (Sections 5.3 and 5.4). Detailed derivations are deferred to the appendix. 5.1. General Form Theorem 5.1 (Linearization of Inner-Loop Updates). Consider TTT model whose inner-loop function has linear, bias-free final layer, (x) = ϕ(x; Θ) W, where ϕ(x; Θ) RDh denotes the hidden representation of the inner-loop function with parameters Θ, and RDhDout is the weight matrix of the final layer. Suppose that at step t, the inner loop performs one step of gradient descent on an objective with learning rate η, using key input k, updating all trainable parameters, (Wt+1, Θt+1) = (Wt, Θt) η(Wt,Θt)L(ft(k)), where ϕt() ϕ(; Θt) and ft() ϕt()Wt. Then, for any query q, the output after the update can be written as = ϕt+1(q) (cid:0)Wt + ϕt(k)gt(k)(cid:1) , gt(k) η ft(k) . This expression is linear attention operator of the form = ˆq (cid:16) (cid:17) S0 + ˆkˆv , where ˆq = ϕt+1(q), ˆk = ϕt(k), ˆv = gt(k), S0 = Wt. The complete proof is provided in Appendix B. Theorem 5.2 (Unrolling Inner-Loop Updates). Given sequence of querykey pairs {(q0, k0), (q1, k1), . . . , (qt, kt)}, suppose the TTT model performs one gradient descent step per input in sequence. By repeated application of Theorem 5.1, the parameters after processing token are (Wt+1, Θt+1) = (W0, Θ0) η (cid:88) i=0 (Wi,Θi)L(fi(ki)) . Evaluating the TTT model on query qt yields ot = ϕt+1(qt) Wt+1 (cid:32) = ϕt+1(qt) W0 + (cid:33) ϕi(ki)gi(ki) , (cid:88) i=0 where gi(ki) is defined as in Theorem 5.1. This corresponds to the extended linear attention form on sequential inputs. (cid:32) ot = ˆqt S0 + (cid:33) ˆk ˆvi . (cid:88) i=0 The complete proof is provided in Appendix C. Next, we extend Theorem 5.2 to the case where the innerloop employs gradient descent with momentum. Theorem 5.3 (Gradient Descent with Momentum). Given the momentum-augmented gradient accumulator defined as (Wt, Θt) = (W,Θ)L(ft(kt)) + αt (Wt1, Θt1), where αt denotes the (possibly token-dependent) momentum factor at step t. The model parameters are then updated according to (Wt+1, Θt+1) = (Wt, Θt) η (Wt, Θt). Define the cumulative momentum coefficient as βj (cid:40)(cid:81)j s=i+1 αs 1 if < j, if = j. Unrolling this recurrence and evaluating the TTT model on query qt yields ot = ϕt+1(qt) Wt+1 (cid:32) = ϕt+1(qt) W0 + (cid:33) ϕi(ki)mi(ki) , (cid:88) i= which induces linear-attentionlike form identical to that of Theorem 5.2, with the effective value vector being momentum-weighted sum: ˆvi = mi(ki) gi(ki) (cid:88) j=i βj . The complete proof is provided in Appendix D. 5.2. Explanation of TTT Empirical Behaviors Having shown that TTT can be rewritten as linear attention operator (Theorem 5.1 - 5.3), we can now revisit the empirical behaviors that appeared contradictory under the memorization-based interpretation. The linear-attention perspective provides unified and mechanistic explanation. More Inner-Loop Steps. According to Theorem 5.1, the inner loop does not perform storage of keyvalue information, but instead defines learnable mapping that transforms the original inputs into the effective query, key, and value 5 Test-Time Training with KV Binding Is Secretly Linear Attention representations. This mapping depends on inner-loop hyperparameters, including the number of optimization steps. Increasing the number of inner-loop iterations at inference time therefore induces an attention operator different to the one used during training, naturally leading to degraded performance due to traintest mismatch rather than improved memorization. Gradient Ascent in the Inner Loop. Under the same formulation, replacing gradient descent with gradient ascent simply flips the sign of the effective value vector gt. Since this sign is absorbed into the learned attention operator and the mapping itself is optimized under the downstream objective, the model adapts to this change. This explains why TTT remains effective under gradient ascent, despite the absence of meaningful memorization objective. Distributional Asymmetry Between and K. The linearattention view clarifies why similarity between and is not required. In TTT, and influence different components of the attention operator: determines the effective query via ϕt+1(q), while determines the effective key and value via ϕt(k) and gt(k), respectively. They are therefore intermediate features rather than symmetric querykey representations, making distributional mismatch expected rather than pathological. Replacing with K. Finally, replacing with does not collapse the attention mechanism because the effective query and key remain distinct: ϕt+1(k) versus ϕt(k). Since ϕ is learnable and evaluated at different parameter states, the model can map the same input to different representations, preserving attention functionality. Summary. Viewed through the lens of linear attention, the inner loop of TTT parameterizes structured linear-form attention operator rather than performing test-time storage and retrieval. Under this perspective, the observed empirical behaviors follow naturally from representation learning and traintest consistency considerations. 5.3. Example: LaCT as Linear Attention We now show how representative instantiation of TTT formula, LaCT (Zhang et al., 2025), can be rewritten in the form of linear attention. LaCT adopts bias-free SwiGLU MLP (Shazeer, 2020) as its inner-loop mapping, parameterized by three learnable weight matrices W0, W2 RDhDk and W1 RDhDv : (x) = (cid:0)silu(xW0) (xW2)(cid:1)W1. The inner-loop objective is defined via the Frobenius inner product L(f (k), v) = (k), v. token learning rate ηt, momentum αt, and gradient orthogonalization M() inspired by Muon (Jordan et al., 2024): Wi,t+1 = Wi,t ηt M(Wi,t), Wi,t = Wi,tL(ft(kt), vt), for {0, 1, 2}. Following Theorem 5.3, the inner-loop model at step can be written as ft(x) = ϕt(x) W1,t, ϕt(x) = silu(xW0,t) (xW2,t)."
        },
        {
            "title": "Evaluating the updated model on query qt yields",
            "content": "ot = ft+1(qt) = ϕt+1(qt) W1,t+1 (cid:32) = ϕt+1(qt) W1,0 + M(cid:0)ϕi(ki)mi (cid:1) (cid:33) , (1) (cid:88) i= where mi(ki) vi ηi (cid:88) j=i βj . This expression reveals that the inner loop of LaCT is effectively linear attentionlike operator, with ϕi(ki) and mi(ki) playing the roles of keys and values, and ϕt+1(qt) acting as the query vector. Note that LaCT also applies weight normalization after each update, which we omit here for simplicity. See Appendix for more details. 5.4. Example: ViTTT as Linear Attention We next show that another instantiation of test-time training, ViTTT (Han et al., 2025), can likewise be rewritten in the form of linear attention. ViTTT employs fast weights consisting of two independent components: (i) simplified gated linear unit (GLU), and (ii) depthwise convolution layer. These components are updated independently in the inner loop. We show that each admits linear-attention interpretation, implying that ViTTT as whole falls within the same framework. GLU component. The GLU is defined as (x) = silu(xW0) (xW1), where W0 and W1 are fast weights updated via gradient descent. As in LaCT, the inner-loop loss is defined using Frobenius inner product. Following derivation analogous to previous sections (see Appendix F), evaluating the updated GLU on query qt yields At each token, LaCT performs gradient descent with perϕ(x) = silu(xW0), 6 Test-Time Training with KV Binding Is Secretly Linear Attention and-retrieval interpretation are in fact redundant or optional. This perspective enables principled ablation path that progressively simplifies complex TTT variants such as LaCT and ViTTT into standard linear attention. Below, we outline this reduction trajectory and analyze the role of each component. Step 1. Update only the last-layer parameters. As shown in Theorem 5.1, the effective query and key vectors are ϕt+1(q) and ϕt(k), where ϕt() ϕ(; Θt). Updating Θt within the inner loop makes ϕt dynamic kernel that is difficult to unroll analytically. If the inner loop instead updates only the final-layer parameter, Θ remains fixed during TTT, and ϕ() ϕ(; Θ) becomes static function with learnable parameters. From the linear-attention perspective, ϕ() then acts as learnable kernel function with effective queries and keys given by ϕ(q) and ϕ(k). Step 2. Remove weight normalization. LaCT applies weight normalization to all learnable parameters Θt, Wt after each inner-loop update. After Step 1, normalization on Θt is no-op since Θt is fixed. Normalizing the finallayer parameter Wt, which corresponds to the state in the linear attention view, is therefore equivalent to normalizing the state St (see Appendix E.4). As such normalization is uncommon in linear attention literature, we remove it for ablation. Notably, after this step the TTT formulation becomes fully parallelizable, as discussed in Section 6.2. Step 3. Multi-layer MLP single linear layer. Several TTT variants (Han et al., 2025; Behrouz et al., 2024) employ deeper inner-loop MLPs, but report inconsistent empirical gains. From linear attention perspective, increasing MLP depth simply induces more complex kernel function ϕ() over queries and keys. When and already have sufficient representational capacity, this added complexity is unlikely to help. We therefore reduce the multi-layer MLP to single linear layer, effectively removing ϕ() altogether. This exposes the true queries and keys as ˆq = and ˆk = k, bringing the formulation closer to basic linear attention. Step 4. Remove per-token learning rates. Many TTT methods (Behrouz et al., 2024; Zhang et al., 2025) introduce per-token learnable learning rate ηt. As shown in Section 5.3, with Frobenius dot product as inner loss this can be absorbed into the learnable vt, indicating its functionally redundant. Consistent with our finding, ViTTT empirically finds that constant learning rate of 1.0 suffices. Step 5. Remove momentum in SGD. As shown in Theorem 5.3, adding momentum to the inner-loop SGD update, as done in LaCT and related methods (Behrouz et al., 2024), only alters the effective value ˆv, from the instantaneous gradient gt(k) to momentum-weighted sum of past gradients. From the linear attention perspective, this corresponds to remixing historical keyvalue contributions Figure 3. Perplexity Metric for Ablation om LaCT-LLM. Evaluated on 2.5B tokens from the Book-3 dataset. ot = ϕt+1(qt) (cid:16) (cid:0)W1 + qt (vt ϕt(kt))(cid:1)(cid:17) . This expression takes the form of linear attention, with ϕt(kt) acting as multiplicative gate on values and ϕt+1(qt) gating the final output. Depthwise convolution component. ViTTT additionally includes 3 3 depthwise convolution layer with fast weights, updated in the inner loop. As convolution is effectively sliding window linear layer, this TTT component is equivalent to sliding window linear attention. formal derivation is provided in Appendix G. Discussion. Since both fast-weight components in ViTTT admit linear-attention formulations, their combination also induces linear-attentionlike operator. This establishes ViTTT as another concrete instance of test-time training whose behavior is more naturally understood through the lens of linear attention. 6. Practical Implications Unmasking TTT as linear attention is not merely theoretical exercise but yields concrete practical benefits. This perspective reveals systematic trajectory for reducing complex TTT formulations into linear attention, along which we observe that several commonly adopted design choices, such as per-token learnable learning rates, weight normalization, are not essential to final performance. By clarifying which components are truly valuable and which are unnecessary, this view enables substantial simplification of TTT formulations. Moreover, recognizing TTT as linear attention makes explicit that the seemingly recurrent inner-loop updates admit parallel formulation, leading to significant efficiency gains in both training and inference. All experiments in this section use the official implementations of LaCT (Zhang et al., 2025) for the LLM and NVS tasks, and ViTTT (Han et al., 2025) for the image recognition task. Detailed experimental settings are provided in Appendix A. 6.1. Reduce TTT to Linear Attention Viewing TTT through the lens of linear attention reveals that several design choices that were justified under storage7 Test-Time Training with KV Binding Is Secretly Linear Attention Table 2. Ablation Trajectory Reducing TTT to Standard Linear Attention. By progressively simplifying complex TTT formulations into standard linear attention (Variant 6), we quantify the contribution of each design component in TTT. Variant 1 achieves the best performance across all three tasks. Variants 26 admit parallel implementations. We additionally report the inference throughput of each variants TTT layer on the LLM task. denotes ablations that do not apply, in which case performance matches the preceding variant. * indicates that ViTTT does not use gradient orthogonalization, so we ablate gradient normalization instead. Alias Description of TTT Inner-loop Designs Perplexity (LaCT-LLM) PSNR (LaCT-NVS) Top-1 Acc (ViTTT) Tokens Per Sec (Recurrent Impl.) Tokens Per Sec (Parallel Impl.) LaCT (Zhang et al., 2025) / ViTTT (Han et al., 2025) Baseline Variant 1 Baseline w/ only update the last layer parameters Variant 2 Variant 1 w/ remove weight normalization Variant 3 Variant 2 w/ multi-layer MLP singel linear layer Variant 4 Variant 3 w/ remove per-token learnable lr Variant 5 Variant 4 w/ remove momentum in SGD Variant 6 Variant 5 w/ remove gradient orthogonalization 16.43 15.93 16.31 16.23 16.12 15.97 16.80 25.94 25.97 25.93 25.71 25.70 25.70 25.73 79.34% 79.63% 79.63% 79.39% 79.39% 79.39% 79.54%* 4.30M 10.60M 11.02M 12.95M 13.31M 14.40M 89.67M N/A N/A 30.18M 49.69M 53.99M 57.28M 124.6M into single value vector. Since both keys and values are already learnable, this additional mixing is unlikely to provide meaningful benefit, thus we remove momentum for ablation. Notably, for both LaCT and ViTTT, gt(k) = and removing momentum recovers ˆv = v, exposing the true value vector. Step 6. Remove gradient orthogonalization. As discussed in Section 5.3, LaCT optionally applies gradient orthogonalization M(W ). Under the linear attention reformulation, this corresponds to applying an operator to the state update M(ˆkv). We remove this operation for ablation. After this final step, Both LaCT and ViTTT reduce (cid:1) . exactly to standard linear attention: = q(cid:0)W + (cid:80) vi Results. We progressively apply the above ablations to LaCT on the LLM and NVS tasks, and to ViTTT on the image recognition task, with results summarized in Table 2. Surprisingly, restricting the inner loop to update only the final MLP layer consistently yields the best overall performance across tasks, suggesting that many of the more complex design choices are unnecessary, or even detrimental. Most other components contribute only marginally to performance, with two notable exceptions: deeper MLPs are beneficial for the NVS task, while gradient orthogonalization improves performance on the LLM task. Overall, reducing the full TTT formulation to basic linear attention operator (Variant 6) results in only minor performance degradation (+0.4 perplexity on LLM and 0.2 dB on NVS). For the LLM task, Table 2 reports perplexity at 32k sequence length, with results across other lengths shown in Figure 3. 6.2. Parallel Form of TTT Existing TTT variants are typically implemented in recurrent manner, reflecting their original storage-and-retrieval interpretation. However, as we have shown that TTT can alternatively be viewed as form of linear attention, natural question arises: can TTT admit parallel formulation that enables more efficient implementation? We show that under certain conditions, corresponding to Variants 26 in our ablation, such parallel form indeed exists. The key insight is that when weight normalization is re8 Figure 4. Training loss vs. wall-clock time on LaCT-LLM. We compare the original LaCT-TTT with both parallel and recurrent form of Variant 2. The parallel form achieves 1.19 end-to-end speedup while maintaining comparable convergence. moved and only the final-layer parameters are updated, the state update becomes associative. In this setting, the kernel function ϕt() ϕ(; Θt) is static and independent of sequence history, allowing the recurrence in Theorem 5.3 to be computed via parallel prefix scan rather than sequential token-by-token updates. We implement this parallel formulation for LaCT on the LLM task. As shown in Table 2, switching from the recurrent to the parallel implementation improves the inference throughput of the TTT layer by up to 4.0 (measured in tokens per second, single batch). Combined with the simplifications from Step 1 and Step 2, this yields 1.19 end-to-end training speedup without degrading model quality, as shown in Figure 4. We provide the full parallel formulation and proof of equivalence to the sequential recurrence in Appendix H. In Appendix I, we further show that introducing weight normalization or dynamic kernel functions breaks associativity, thereby preventing parallelization. 7. Conclusion In this work, we challenge the prevailing view of TTT as mechanism for test-time memorization of keyvalue mappings. Through systematic empirical analysis, we identify several anomaliesincluding gradient ascent behavior, distributional asymmetry between queries and keys, and the Test-Time Training with KV Binding Is Secretly Linear Attention lack of correlation between inner-loop convergence and downstream performancethat are fundamentally incompatible with memorization-based interpretation. We provide an alternative explanation by showing that TTT, even with complex inner loops involving multi-layer MLPs and momentum-based optimizers, can be analytically rewritten as form of linear attention operator. Under this view, the inner loop does not perform meta learning in the conventional sense, but instead parameterizes structured mixing of queries, keys, and values. Viewing TTT through the lens of linear attention not only resolves the observed paradoxes, but also enables principled simplifications, parallel implementations with improved efficiency, and unified framework for understanding TTT variants. Our analysis applies to most current TTT formulations but is limited to settings where the inner-loop final layer is linear and bias-free. Extending these insights to nonlinear final layers, and exploring deeper connections between TTT and modern linear attention mechanisms in both directions, remain important avenues for future work."
        },
        {
            "title": "References",
            "content": "Behrouz, A., Zhong, P., and Mirrokni, V. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. Behrouz, A., Li, Z., Kacham, P., Daliri, M., Deng, Y., Zhong, P., Razaviyayn, M., and Mirrokni, V. Atlas: Learning to optimally memorize the context at test time. arXiv preprint arXiv:2505.23735, 2025a. Behrouz, A., Razaviyayn, M., Zhong, P., and Mirrokni, V. Its all connected: journey through test-time memorization, attentional bias, retention, and online optimization. arXiv preprint arXiv:2504.13173, 2025b. Chen, X., Chen, Y., Xiu, Y., Geiger, A., and Chen, A. Ttt3r: 3d reconstruction as test-time training. arXiv preprint arXiv:2509.26645, 2025. Chen, Y., Wang, J., Yang, Z., Manivasagam, S., and Urtasun, R. G3r: Gradient guided generalizable reconstruction. In European Conference on Computer Vision, pp. 305323. Springer, 2024. Dalal, K., Koceja, D., Xu, J., Zhao, Y., Han, S., Cheung, K. C., Kautz, J., Choi, Y., Sun, Y., and Wang, X. Oneminute video generation with test-time training. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1770217711, 2025. Dao, T. and Gu, A. Transformers are ssms: generalized models and efficient algorithms through structured state space duality. In Proceedings of the 41st International Conference on Machine Learning, pp. 1004110071, 2024. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Finn, C., Abbeel, P., and Levine, S. Model-agnostic metalearning for fast adaptation of deep networks. In International conference on machine learning, pp. 11261135. PMLR, 2017. Gandelsman, Y., Sun, Y., Chen, X., and Efros, A. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 35:2937429385, 2022. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. In First conference on language modeling, 2024. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Han, D., Li, Y., Li, T., Cao, Z., Wang, Z., Song, J., Cheng, Y., Zheng, B., and Huang, G. Vit3: Unlocking test-time training in vision. arXiv preprint arXiv:2512.01643, 2025. Hinton, G. E. and Plaut, D. C. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pp. 177186, 1987. Jordan, K., Jin, Y., Boza, V., You, J., Cesista, F., Newhouse, L., and Bernstein, J. Muon: An optimizer for hidden layers in neural networks, 2024. URL https: //kellerjordan.github.io/posts/muon/. Karami, M., Pascanu, R., and Mirrokni, V. Lattice: Learning to efficiently compress the memory. arXiv preprint arXiv:2504.05646, 2025. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. Lei, J., Zhang, D., and Poria, S. Error-free linear attention is free lunch: Exact solution from continuous-time dynamics. arXiv preprint arXiv:2512.12602, 2025. Liu, B., Wang, R., Wu, L., Feng, Y., Stone, P., and Liu, Q. Longhorn: State space models are amortized online learners. arXiv preprint arXiv:2407.14207, 2024. 9 Test-Time Training with KV Binding Is Secretly Linear Attention Maaten, L. v. d. and Hinton, G. Visualizing data using t-sne. Journal of machine learning research, 9(Nov): 25792605, 2008. Metz, L., Maheswaranathan, N., Cheung, B., and SohlDickstein, J. Meta-learning update rules for unsupervised representation learning. arXiv preprint arXiv:1804.00222, 2018. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. B. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 20286 20332, 2025. Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre, C., Pascanu, R., and De, S. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pp. 2667026698. PMLR, 2023. Penedo, G., Kydlıˇcek, H., Lozhkov, A., Mitchell, M., Raffel, C. A., Von Werra, L., Wolf, T., et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37: 3081130849, 2024. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman, S., Cao, H., Cheng, X., Chung, M., Derczynski, L., et al. Rwkv: Reinventing rnns for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1404814077, 2023. Peng, B., Goldstein, D., Anthony, Q., Albalak, A., Alcaide, E., Biderman, S., Cheah, E., Du, X., Ferdinan, T., Hou, H., et al. Eagle and finch: Rwkv with matrixvalued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. Peng, B., Zhang, R., Goldstein, D., Alcaide, E., Du, X., Hou, H., Lin, J., Liu, J., Lu, J., Merrill, W., et al. Rwkv-7 goose with expressive dynamic state evolution. arXiv preprint arXiv:2503.14456, 2025a. Peng, L., Chattopadhyay, A., Zancato, L., Nunez, E., Xia, W., and Soatto, S. Gated kalmanet: fading memory layer through test-time ridge regression. arXiv preprint arXiv:2511.21016, 2025b. Qin, Z., Yang, S., and Zhong, Y. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36:33202 33221, 2023. Qin, Z., Yang, S., Sun, W., Shen, X., Li, D., Sun, W., and Zhong, Y. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers. In International conference on machine learning, pp. 93559366. PMLR, 2021. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Smith, J. T., Warrington, A., and Linderman, S. W. Simplified state space layers for sequence modeling. In International Conference on Learning Representations (ICLR), 2023. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., and Hardt, M. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pp. 92299248. PMLR, 2020. Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., and Wei, F. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Sun, Y., Li, X., Dalal, K., Xu, J., Vikram, A., Zhang, G., Dubois, Y., Chen, X., Wang, X., Koyejo, S., et al. Learning to (learn at test time): Rnns with expressive hidden states. In International Conference on Machine Learning, 2025. Tandon, A., Dalal, K., Li, X., Koceja, D., Rød, M., Buchanan, S., Wang, X., Leskovec, J., Koyejo, S., Hashimoto, T., et al. End-to-end test-time training for long context. arXiv preprint arXiv:2512.23675, 2025. Team, K., Zhang, Y., Lin, Z., Yao, X., Hu, J., Meng, F., Liu, C., Men, X., Yang, S., Li, Z., et al. Kimi linear: An expressive, efficient attention architecture. arXiv preprint arXiv:2510.26692, 2025. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wang, K. A., Shi, J., and Fox, E. B. Test-time regression: unifying framework for designing sequence models with associative memory. arXiv preprint arXiv:2501.12352, 2025. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. In Proceedings of the 41st International Conference on Machine Learning, pp. 5650156523, 2024a. 10 Test-Time Training with KV Binding Is Secretly Linear Attention Yang, S., Wang, B., Zhang, Y., Shen, Y., and Kim, Y. Parallelizing linear transformers with the delta rule over sequence length. Advances in neural information processing systems, 37:115491115522, 2024b. Yuan, Y., Shen, Q., Wang, S., Yang, X., and Wang, X. Test3r: Learning to reconstruct 3d at test time. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Zhang, T., Bi, S., Hong, Y., Zhang, K., Luan, F., Yang, S., Sunkavalli, K., Freeman, W. T., and Tan, H. Test-time training done right. arXiv preprint arXiv:2505.23884, 2025. Zhang, Y. and Yang, S. Flame: Flash language modeling made easy, January 2025. URL https://github. com/fla-org/flame. Zhang, Y., Yang, S., Zhu, R.-J., Zhang, Y., Cui, L., Wang, Y., Wang, B., Shi, F., Wang, B., Bi, W., et al. Gated slot attention for efficient linear-time sequence modeling. Advances in Neural Information Processing Systems, 37: 116870116898, 2024. Zhong, S., Xu, M., Ao, T., and Shi, G. Understanding transformer from the perspective of associative memory. arXiv preprint arXiv:2505.19488, 2025. Zhou, T., Tucker, R., Flynn, J., Fyffe, G., and Snavely, N. Stereo magnification: learning view synthesis using multiplane images. ACM Transactions on Graphics (TOG), 37(4):112, 2018. 11 Test-Time Training with KV Binding Is Secretly Linear Attention A. Experiment Setup Here we outline the three tasks used to evaluate the TTT framework, along with the experimental setup. Language Modeling We use the 760M parameter LaCT-LLM (Zhang et al., 2025) as our baseline model. All models are trained on 100B tokens sampled from the FineWeb-Edu dataset (Penedo et al., 2024) with batch size of 4 per GPU across 8 NVIDIA A100 GPUs for 20K iterations, which takes approximately 56 hours to complete. All other hyperparameters follow the original (Zhang et al., 2025) configuration. For evaluation, we report perplexity on 2.5B tokens from the Book-3 dataset (Gao et al., 2020). All implementations are based on the Flame (Zhang & Yang, 2025) codebase. Novel View Synthesis We use 12-layer 768 hidden dimension LaCT-NVS (Zhang et al., 2025) model as baseline, totaling 114M parameters. We train our model on the RealEstate10K (Zhou et al., 2018) dataset with batch size of 128 per GPU across 4 NVIDIA A100 GPUs for 20K iterations, which takes approximately 38 hours to complete. We use 2 input views and 6 target views for training, and 2 input views and 3 target views for evaluation. All images are resized to 128 128 resolution. We train using only the MSE loss and evaluate using the Peak Signal-to-Noise Ratio (PSNR) metric. All other hyperparameters follow the original LaCT-NVS (Zhang et al., 2025) configuration. Image Classification We use ViTTT-B (Han et al., 2025) as our baseline model, totaling 90M parameters. Follow (Han et al., 2025), we train our model on the ImageNet-1K (Deng et al., 2009) dataset with batch size of 256 per GPU across 2 NVIDIA H100 GPUs for 60 epochs, which takes approximately 16 hours to complete. We evaluate on the ImageNet-1K (Deng et al., 2009) validation set using the top-1 accuracy. All other hyperparameters follow the original (Han et al., 2025) configuration. B. Proof of Theorem 5.1 Proof. We explicitly unroll the single-step inner-loop update. Using the shorthand notation ϕt() ϕ(; Θt) and ft() ϕt()Wt, the model output for the key input at step is By the chain rule, the gradient of the last layer with respect to the loss L(ft(k)) is ft(k) = ϕt(k) Wt. WtL = ϕt(k) ft(k) . Applying one step of gradient descent with learning rate η yields Wt+1 = Wt ηϕt(k) ft(k) = Wt + ϕt(k)gt(k) where we define ft(k) The kernel function parameters Θ are also updated with gradient descent, yielding the updated function ϕt+1() = ϕ(; Θt+1). gt(k) η RDout. Evaluating the updated model on query gives = ϕt+1(q) Wt+1 = ϕt+1(q) (cid:0)Wt + ϕt(k)gt(k)(cid:1) This expression is linear attention operator of the form = ˆq (cid:16) (cid:17) S0 + ˆkˆv , where ˆq = ϕt+1(q), ˆk = ϕt(k), ˆv = gt(k), S0 = Wt. 12 Test-Time Training with KV Binding Is Secretly Linear Attention C. Proof of Theorem 5. Proof. We prove by induction on the number of tokens processed. Base case. Starting from initial parameters (W0, Θ0), processing token 0 with key k0 applies one gradient descent step. By Theorem 5.1: W1 = W0 + ϕ0(k0)g0(k0), where g0(k0) = η f0(k0) , and Θ1 is updated accordingly. This matches the claimed form with = 0. Inductive step. Assume that after processing tokens 0, . . . , 1, we have Wt = W0 + t1 (cid:88) i=0 ϕi(ki)gi(ki). Processing token with key kt applies another gradient descent step. By Theorem 5.1: Wt+1 = Wt + ϕt(kt)gt(kt). Substituting the inductive hypothesis: Wt+1 = W0 + t1 (cid:88) i=0 ϕi(ki)gi(ki) + ϕt(kt)gt(kt) = W0 + (cid:88) i=0 ϕi(ki)gi(ki). Output computation. Evaluating the model on query qt with the updated parameters gives: ot = ϕt+1(qt) Wt+1 (cid:32) = ϕt+1(qt) W0 + (cid:33) ϕi(ki)gi(ki) (cid:88) i= This is the extended linear attention form: (cid:32) ot = ˆqt S0 + (cid:33) ˆk ˆvi , (cid:88) i=0 where ˆqt = ϕt+1(qt), ˆki = ϕi(ki), ˆvi = gi(ki), and S0 = W0. D. Proof of Theorem 5.3 Proof. We prove by induction, extending the approach of Theorem 5.2 to momentum updates. Base case. At = 0, the momentum accumulator is initialized as W1 = 0, so W0 = L(f0(k0)) = ϕ0(k0) f0(k0) . Applying the update rule W1 = W0 ηW0 gives where g0(k0) = η f0(k0) . This matches the claimed form with m0(k0) = g0(k0). W1 = W0 + ϕ0(k0)g0(k0), 13 Test-Time Training with KV Binding Is Secretly Linear Attention Inductive step. Define the cumulative momentum coefficient from step to step as: βj (cid:40)(cid:81)j s=i+1 αs 1 if < j, if = j. Assume that after processing tokens 0, . . . , 1, the momentum accumulator satisfies Wt1 = t1 (cid:88) i=0 βt1 ϕi(ki) fi(ki) . At step t, the momentum update gives Wt = L(ft(kt)) + αtWt1 = ϕt(kt) ft(kt) + αt t1 (cid:88) i=0 βt1 ϕi(ki) fi(ki) . For terms [0, 1], we have αt βt1 βt = 1. Thus: = αt (cid:81)t1 s=i+1 αs = (cid:81)t s=i+1 αs = βt . For the = term, the coefficient is Wt = (cid:88) i=0 ϕi(ki) βt fi(ki) . Weight accumulation. From Wj+1 = Wj ηWj, we have Wt+1 = W0 η (cid:80)t for Wj and exchanging the order of summation: j=0 Wj. Substituting the closed form Wt+1 = W0 η (cid:88) (cid:88) j=0 i=0 ϕi(ki) βj fi(ki) = W0 + ϕi(ki) η fi(ki) (cid:88) j=i βj . (cid:88) i=0 (cid:124) (cid:123)(cid:122) mi(ki) (cid:125) The momentum-weighted effective value is thus mi(ki) = gi(ki) (cid:88) j=i βj . Output computation. Evaluating the model on query qt with the updated parameters: ot = ϕt+1(qt) Wt+1 (cid:32) = ϕt+1(qt) W0 + (cid:33) ϕi(ki)mi(ki) . (cid:88) i=0 This is the linear attention form with ˆqt = ϕt+1(qt), ˆki = ϕi(ki), ˆvi = mi(ki), and S0 = W0. E. Derivation: LaCT as Linear Attention We provide the full derivation showing how LaCT (Zhang et al., 2025) can be written in the form of linear attention, following the framework of Theorem 5.3. 14 Test-Time Training with KV Binding Is Secretly Linear Attention E.1. LaCT Architecture and Update Rule LaCT adopts bias-free SwiGLU MLP as its inner-loop mapping: (x) = (cid:0)silu(xW0) (xW2)(cid:1)W1, where W0, W2 RDhDk and W1 RDhDv . The inner-loop objective uses the Frobenius inner product: L(f (k), v) = (k), v. At each token t, LaCT performs gradient descent with per-token learning rate ηt, momentum αt, and Muon-style (Jordan et al., 2024) gradient orthogonalization M(): Wi,t = Wi,tL(ft(kt), vt) + αtWi,t1, Wi,t+1 = Wi,t ηt M(Wi,t), for {0, 1, 2}. E.2. Gradient Computation The upstream gradient with respect to the output is is: ft(kt) = vt. Using the chain rule, the gradient for the final layer W1 = ϕt(kt) ft(kt) = ϕt(kt)vt, where ϕt(x) = silu(xW0,t) (xW2,t) is the kernel function. E.3. Linear Attention Form with Muon Following the same induction as Theorem 5.3, but with the Muon orthogonalization M() applied after each gradient accumulation, the weight update becomes: W1,t+1 = W1,0 (cid:88) j=0 ηj M(W1,j). Using the cumulative momentum coefficient βt defined in Theorem 5.3, the momentum accumulator for W1 is: W1,t = ϕt(kt)vt + αtW1,t1 = (cid:88) i=0 ϕi(ki)vi. βt Substituting into the weight update and exchanging the order of summation (as in the proof of Theorem 5.3): W1,t+1 = W1,0 + (cid:88) i=0 M(cid:0)ϕi(ki)mi (cid:1) , where the momentum-weighted effective value is: Evaluating on query qt: mi ηi vi (cid:88) j=i βj . ot = ϕt+1(qt) W1,t+1 (cid:32) = ϕt+1(qt) W1,0 + M(cid:0)ϕi(ki)mi (cid:1) (cid:33) . (cid:88) i=0 This is linear attentionlike form where ϕi(ki) and mi play the roles of keys and values, and ϕt+1(qt) acts as the query. The Muon orthogonalization M() is applied element-wise to each key-value outer product before accumulation. 15 Test-Time Training with KV Binding Is Secretly Linear Attention E.4. Effect of Weight Normalization LaCT additionally applies weight normalization to W1 after each update: W1,t+1 = Norm(W1,t ηt M(W1,t)) , where Norm() applies channel-wise ℓ2 normalization. Weight normalization does not break the linear attention perspective. Recall that in the linear attention formulation, the state matrix St = W1,t accumulates key-value outer products. With weight normalization, we simply normalize this state after each update: St+1 = Norm(cid:0)St + ϕt(kt)mt (cid:1) . The output remains linear function of the (normalized) state: ot = ϕt+1(qt) St+1. Thus, LaCT with weight normalization is still linear attention mechanismthe query linearly reads from state that accumulates key-value information. However, the normalization prevents expressing the state as simple sum over history. Unlike the unnormalized case where St+1 = S0 + (cid:80)t i=0 ϕi(ki)mi, the nested normalization creates sequential dependency: St+1 = Norm(cid:0)Norm(cid:0)St1 + ϕt1(kt1)mt1 (cid:1) + ϕt(kt)mt (cid:1) . This has implications for parallelization, which we discuss in Appendix I. F. Derivation: ViTTT GLU as Linear Attention We show that the simplified GLU used in ViTTT (Han et al., 2025) can be written into linear attention form with element-wise multiplication. F.1. Architecture and Loss In ViTTT, the GLU is defined as: where W0,t, W1,t RDhDh are both square matrices. We define the kernel function as ft(x) = silu(xW0,t) (xW1,t), ϕt(x) silu(xW0,t). Unlike the general form where the kernel function output is followed by matrix multiplication with final layer, here the GLU uses element-wise multiplication between the nonlinear gating ϕt(x) and the linear projection xW1,t. The inner-loop loss uses the Frobenius inner product: L(ft(kt), vt) = ft(kt), vt. F.2. Gradient Computation The upstream gradient with respect to the GLU output is ft(kt) = vt. Applying the chain rule, the gradient of W1,t is: W1,tL = t (cid:18) ft(kt) (cid:19) ϕt(kt) = (vt ϕt(kt)). 16 Test-Time Training with KV Binding Is Secretly Linear Attention F.3. Linear Attention Form After one step of gradient descent with learning rate η: W1,t+1 = W1,t ηW1,t = W1,t + η (vt ϕt(kt)). Similarly, W0,t is updated to W0,t+1, yielding the updated kernel function ϕt+1(). Evaluating the model on query qt gives: ot = ft+1(qt) = ϕt+1(qt) (qtW1,t+1) = ϕt+1(qt) (cid:0)qtW1,t + η (qtk (cid:16) = ϕt+1(qt) (cid:0)W1,t + qt (vt ϕt(kt))(cid:1)(cid:17) . )(vt ϕt(kt))(cid:1) This is linear attention form where: The second term computes scalar attention weight qt, kt that modulates the value vector vt ϕt(kt) acts as multiplicative gate on the values ϕt+1(qt) gates the final output The state matrix St = W1,t accumulates outer products of keys and gated values, consistent with the general linear attention framework. G. Derivation: ViTTT Depthwise Convolution as Linear Attention We show that the 3 3 depthwise convolution layer in ViTTT (Han et al., 2025) can be formulated as form of spatially-local linear attention. G.1. Architecture and Loss Let K, RCHW be the spatial key and value tensors, and Wt RC133 be the depthwise convolution weights at step t. The forward pass computes: where Conv33 denotes depthwise convolution with 3 3 kernel. The inner-loop loss uses the Frobenius inner product: ft(K) = Conv33(K; Wt), L(ft(K), ) = ft(K), = (cid:88) c,i,j [ft(K)]c,i,j Vc,i,j. G.2. Gradient Computation The upstream gradient is ft(K) = . For depthwise convolution, the gradient with respect to the weight Wt can be written as: WtL = ft(K) = V, where denotes cross-correlation. Specifically, for each channel and offset (δy, δx) {1, 0, 1}2: [WtL]c,δy,δx = (cid:88) i,j 17 Kc,i+δy,j+δx Vc,i,j. Test-Time Training with KV Binding Is Secretly Linear Attention G.3. Linear Attention Form After one step of gradient descent with learning rate η: [Wt+1]c,δy,δx = [Wt]c,δy,δx + η (cid:88) i,j Kc,i+δy,j+δx Vc,i,j. When evaluating on query RCHW , the output at position (i, j) is: Oc,i,j = = (cid:88) δy,δx (cid:88) δy,δx (cid:124) [Wt+1]c,δy,δx Qc,i+δy,j+δx [Wt]c,δy,δx Qc,i+δy,j+δx (cid:123)(cid:122) [Conv(Q;Wt)]c,i,j (cid:16) (cid:88) (cid:125) (cid:88) + η Qc,i+δy,j+δx Kc,i+δy,j+δx i,j δy,δx (cid:124) (cid:123)(cid:122) spatial attention weight This is linear attention form where: The first term corresponds to the initial state S0 = Wt Vc,i,j. (cid:17) (cid:125) The attention weight between query position (i, j) and key position (i, j) is the sum of element-wise products over the 3 3 neighborhood offsets This spatially-local attention allows each output position to attend to all key-value positions, weighted by the overlap of their local 3 3 neighborhoods Conceptually, since convolution is effectively sliding-window linear layer, this TTT component is equivalent to sliding-window linear attention mechanism. H. Parallel Form of TTT We present the parallel formulation for TTT when only W1 is dynamic (while W0 and W2 are static) and weight normalization is omitted. Under these conditions, the kernel function becomes static: ϕ(x) = silu(xW0) (xW2), and the state update is associative, enabling parallel computation. H.1. Parallel Formulation The parallel formulation operates on sequence of chunks, each of size L. Let Q, R(N L)Dk and R(N L)Dv denote the concatenated query, key, and value inputs across all chunks. Define the batched kernel function: Φ(X) = silu(XW0) (XW2) R(N L)Dh. To express chunk-wise operations, we introduce: Block-diagonal matrix R(N L)(N L): = diag(IL, . . . , IL) with identity blocks Learning rate vector η R(N L)1: per-token learning rates Scaled values = η 18 Test-Time Training with KV Binding Is Secretly Linear Attention Momentum matrix RN : Aij = (cid:81)i s=j+1 αs for j, else 0 Causal mask {0, 1}N : Mij = 1 if The output can be computed in parallel as: = Φ(Q) W1,0 + (cid:0)(Φ(Q) Φ(K)) (A M)L(cid:1) V, where ()L denotes the Kronecker product with 1LL to expand the mask to (N L) (N L). H.2. Proof of Equivalence We prove that this parallel formulation is equivalent to the sequential recurrence. Proof. Consider the sequential formulation where at each chunk t, the weight update is: Wt = W1L(ft(Kt)) + αtWt1, W1,t+1 = W1,t ηtWt. Since W1L = Φ(Kt) ft(Kt) and using the Frobenius inner product loss, we have: Wt = Φ(Kt)Vt + αtWt1. Step 1: Unroll the momentum recurrence. Expanding Wt: Wt = (cid:88) (cid:32) (cid:89) i=0 s=i+1 (cid:33) αs Φ(Ki)Vi. Step 2: Unroll the weight recurrence. From W1,t+1 = W1,0 (cid:80)t j=0 ηjWj, substituting and rearranging: W1,t+1 = W1,0 + (cid:88) i=0 Φ(Ki)Vi ηi (cid:88) (cid:32) (cid:89) (cid:33) αs . j=i (cid:124) s=i+1 (cid:123)(cid:122) ct (cid:125) Step 3: Compute the sequential output. The output at chunk is: Ot = Φ(Qt) W1,t+1 = Φ(Qt) W1,0 + (cid:88) i=0 Φ(Qt) Φ(Ki)Vi ct i. Step 4: Match with parallel formulation. The mask (A M)L at block (t, i) has value (cid:80)t The parallel form computes: j=i (cid:81)j s=i+1 αs for t. [O]p = Φ(Qt)p W1,0 + (cid:88) i=0 Φ(Qt)p Φ(Ki)Vi ct i, which exactly matches the sequential output. I. Non-Reducible Case Analysis In Section 6.2, we showed that TTT becomes reducible (and thus parallelizable) when only W1 is dynamic while W0 and W2 are static. Here, we analyze two cases that break reducibility: (1) updating the kernel function parameters Θ = {W0, W2}, and (2) applying weight normalization. 19 Test-Time Training with KV Binding Is Secretly Linear Attention I.1. Case 1: Dynamic Kernel Function (Updating W0 and W2) When W0 and W2 are also updated, the kernel function ϕt(x) = silu(xW0,t) (xW2,t) becomes history-dependent. Consider the gradient update for W0 (the analysis for W2 is analogous): W0,tL = (cid:16) ft(Kt) 1,t (KtW2,t) silu(KtW0,t) (cid:17) . The weight update becomes: W0,t+1 = W0,t ηtW0,tL. Expanding recursively, the kernel function at step depends on W0,t: ϕt(Kt) = silu(KtW0,t) (KtW2,t). Substituting the recursive expression for W0,t, the kernel function involves nested nonlinearities: ϕt(Kt) = silu (cid:16) (cid:0)W0,t1 ηt1W0,t1 L(cid:1)(cid:17) Kt ( ), where W0,t1 itself contains silu(Kt1W0,t1). The nested silu and silu functions create non-linear dependency chain: computing ϕt requires W0,t, which depends on silu(Kt1W0,t1), which in turn depends on W0,t1, and so on. This nested structure prevents expressing the output as simple weighted sum over history, breaking the associativity required for parallel prefix scan. I.2. Case 2: Weight Normalization Even when only W1 is dynamic, applying weight normalization (as in LaCT, see Appendix E) introduces non-reducibility for the parallel formulation. As discussed in Appendix E, weight normalization does not break the linear attention interpretationit simply normalizes the state St after each token. However, it does prevent the parallel computation enabled by the sum form. The key issue is that normalization is not associative: Norm(A + B) = Norm(A) + Norm(B). (2) The parallel formulation in Section 6.2 relies on expressing the state as St+1 = S0 + (cid:80)t computed via associative parallel prefix scan. With weight normalization, the state becomes: i=0 ϕ(Ki)mi, which can be St+1 = Norm (cid:16) Norm(St1 + ϕ(Kt1)mt1) + ϕ(Kt)mt (cid:17) . This nested structure creates strict sequential dependency: computing St+1 requires the fully normalized St, which requires St1, and so on. The associativity required for parallel prefix scan is broken, forcing token-by-token sequential computation even though the underlying mechanism remains linear attention."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Technion",
        "University of Toronto",
        "Vector Institute"
    ]
}