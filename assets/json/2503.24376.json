{
    "paper_title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1",
    "authors": [
        "Yi Chen",
        "Yuying Ge",
        "Rui Wang",
        "Yixiao Ge",
        "Lu Qiu",
        "Ying Shan",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, a benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through a three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with a large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL with supervised fine-tuning (SFT), demonstrating RL's data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 6 7 3 4 2 . 3 0 5 2 : r Technical Report (In Progress) Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1 Yi Chen1,2, Yuying Ge2, Rui Wang3, Yixiao Ge2, Lu Qiu1,2, Ying Shan2, Xihui Liu1 1The University of Hong Kong, 2ARC Lab, Tencent PCG, 3The Chinese University of Hong Kong https://github.com/TencentARC/SEED-Bench-R1 Figure 1: (a) Overview of SEED-Bench-R1 (SBR), which systematically evaluates post-training methods for MLLMs in video understanding. SBR features three-level evaluation hierarchy: indistribution, cross-environment, and cross-environment-task scenarios, equipped with training data containing easily verifiable ground-truth answers, to assess generalization across different levels. These tasks necessitate both perception and reasoning to tackle complex real-world challenges. (b) Notably, an MLLM trained using reinforcement learning via GRPO outperforms both the base model and supervised fine-tuning (SFT) model, particularly in out-of-distribution scenarios (e.g., levels 23). Additionally, this RL-trained model exhibits strong generalization capabilities across general video understanding benchmarks (e.g., LongVideoBench)."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in Chain of Thought (COT) generation have significantly improved the reasoning capabilities of Large Language Models (LLMs), with reinforcement learning (RL) emerging as an effective post-training approach. Multimodal Large Language Models (MLLMs) inherit this reasoning potential but remain underexplored in tasks requiring both perception and logical reasoning. To address this, we introduce SEED-Bench-R1, benchmark designed to systematically evaluate post-training methods for MLLMs in video understanding. It includes intricate real-world videos and complex everyday planning tasks in the format of multiple-choice questions, requiring sophisticated perception and reasoning. SEED-Bench-R1 assesses generalization through three-level hierarchy: in-distribution, cross-environment, and cross-environment-task scenarios, equipped with large-scale training dataset with easily verifiable ground-truth answers. Using Qwen2-VL-Instruct-7B as base model, we compare RL with supervised fine-tuning (SFT), demonstrating RLs data efficiency and superior performance on both in-distribution and out-of-distribution tasks, even outperforming Corresponding Authors. 1 Technical Report (In Progress) SFT on general video understanding benchmarks like LongVideoBench. Our detailed analysis reveals that RL enhances visual perception but often produces less logically coherent reasoning chains. We identify key limitations such as inconsistent reasoning and overlooked visual cues, and suggest future improvements in base model reasoning, reward modeling, and RL robustness against noisy signals."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in the reasoning capabilities of Large Language Models (LLMs) (Guo et al., 2025; ope, 2024; Team et al., 2025) have been driven by progress in long Chain of Thought (COT) generation. Among the various approaches to enhancing long COT, reinforcement learning (RL) (Shao et al., 2024; Ouyang et al., 2022; Yeo et al., 2025) has emerged as particularly effective post-training method. RL enables LLMs to refine their COT through self-evolution with verifiable rewards, resulting in models that excel at solving complex problems and demonstrate strong generalization in out-of-distribution (OOD) tasks. Multimodal Large Language Models (MLLMs), which are built upon LLMs, inherit their reasoning potential while incorporating additional modules for processing multimodal inputs. This has led to growing interest in whether RL can similarly enhance multimodal understanding in MLLMs (Zhang et al., 2025; Liu et al., 2025; Meng et al., 2025). However, existing research primarily focuses on image-based tasks, emphasizing either perception (e.g., detection and grounding) or logical inference (e.g., multimodal math problem solving). We argue that an ideal testbed for studying the impact of post-training methods on multimodal understanding should balance both perception and logical inference, ensuring models can integrate these capabilities to derive accurate answers. Additionally, the generalization capability of MLLMs must be rigorously evaluated to assess the robustness of post-training methods. To address this, we introduce SEED-Bench-R1, benchmark designed for systematic evaluation of the effectiveness of post-training methods on video understanding. As shown in Table 1, SEEDBench-R1 is built on our prior works, reusing the training and validation data from our EgoPlanBench (Chen et al., 2023), as well as the test data from our EgoPlan-Bench2 (Chen et al., 2023). Generally, the benchmark leverages realistic egocentric videos (Damen et al., 2022; Grauman et al., 2022) capturing everyday human activities, requiring models to understand open-form task goals, track long-horizon visual progress, perceive complex environmental observations, and reason about next actions using world knowledge. Specifically, the validation data from EgoPlan-Bench are used for Level-1 (in-distribution) and Level-2 (OOD, cross-environment) evaluation, while the test data from EgoPlan-Bench2 cover more general domains and are used for Level-3 (OOD, crossenvironment-task) evaluation. As shown in Figure 1, SEED-Bench-R1 features: 1) intricate real-world visual inputs necessitating nuanced perception, 2) diverse questions requiring logical inference with common sense, 3) rigorously partitioned validation sets to assess in-distribution (L1) and OOD (L2/L3) generalization capabilities, and 4) large-scale, automatically constructed training questions with easily verifiable ground-truth answers (the ground-truth answer comes from the actual next action occurring right after the current observation in the original uncropped video). We evaluate representative post-training methods on SEED-Bench-R1 using Qwen2-VL-Instruct7B (Wang et al., 2024) as the base model, comparing RL (specifically GRPO (Shao et al., 2024)) with supervised fine-tuning (SFT). Our experiments reveal that RL is highly data-efficient, significantly improving performance on both in-distribution (L1) and OOD (L2/L3) questions, even with simple outcome-based rewards. RLs superiority over SFT is particularly pronounced in OOD scenarios and extends to general video understanding benchmarks like LongVideoBench (Wu et al., 2024). Our analysis further explores how RL influences COT generation and its impact on visual perception and logical inference. We find that RL enhances the models ability to attend to visual content more effectively, particularly in OOD scenarios. RL teaches the model to dynamically query visual inputs with COT tokens rather than memorizing superficial reasoning patterns, thus achieving better generalization performance. However, limitations remain, such as the models occasional neglect of key visual cues due to limited perception granularity and the lack of logical coherence in generated reasoning chains without process supervision, which hinder transparency and performance. 2 Technical Report (In Progress) Split # Samples Domain Cross-Environment Cross-Task Video Source Benchmark Source Train Val-L1 Val-L2 Val-L3 50,269 2,432 923 1,321 Daily life Daily life Daily life Hobbies, Daily life, Recreation, Work - - Epic-Kitchens Epic-Kitchens Ego4D Ego4D EgoPlan-Bench EgoPlan-Bench EgoPlan-Bench EgoPlan-Bench2 Table 1: Data statistics of SEED-Bench-R1, which consists of training set and hierarchical threelevel validation set for in-distribution, cross-environment, and cross-environment-task evaluations. Figure 2: Example questions from the three-level evaluation hierarchy in SEED-Bench-R1s validation set, including in-distribution, cross-environment, and cross-environment-task scenarios. We conclude by outlining promising directions for future work, including eliciting stronger base reasoning abilities before RL, improving reward modeling to balance visual perception and logical inference, and enhancing RLs robustness to noisy reward signals. These steps are crucial for scaling RL to larger datasets and achieving more reliable alignment in multimodal understanding. In summary, our contributions are as below: We introduce SEED-Bench-R1, video understanding benchmark designed to evaluate post-training methods, featuring real-world egocentric videos, questions balancing perception and logical inference, and large-scale training dataset with rigorous validation splits divided into three-level hierarchy for generalization assessment. Using SEED-Bench-R1, we conduct systematic study and demonstrate that reinforcement learning (RL)specifically GRPOoutperforms supervised fine-tuning (SFT) in data efficiency and generalization, notably in OOD scenarios, even with simple outcome-based rewards. Our analysis reveals that RL enhances video understanding by improving visual attention and encouraging dynamic querying of visual inputs via Chain of Thought (COT), though challenges remain in perception granularity and logical coherence. 3 Technical Report (In Progress) We identify future directions, such as strengthening the base models reasoning capability pre-RL, refining reward design, and improving robustness to noise, to advance RL for multimodal alignment."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Recent advancements in Large Language Models (LLMs), such as OpenAIs o1 (ope, 2024) and DeepSeeks R1 (Guo et al., 2025), have demonstrated that reinforcement learning (RL) can significantly enhance reasoning abilities by enabling models to autonomously refine their Chain-ofThought (CoT) processes without explicit supervision. Inspired by these successes, researchers have begun adapting RL-based approaches to Multimodal Large Language Models (MLLMs), achieving notable improvements in vision tasks (Huang et al., 2025; Zhao et al., 2025; Liu et al., 2025; Zhang et al., 2025). However, existing work primarily focuses on image-based tasksranging from perception-heavy problems like image classification (Lin et al., 2014) and object detection (Gupta et al., 2019) to reasoning-intensive tasks like visual math problem-solving (Sun et al., 2024; Lu et al.; Zhang et al., 2024; Zou et al.)while video understanding remains underexplored. Preliminary efforts (Wang & Peng, 2025; Zhao et al., 2025) have applied RL to MLLMs for video benchmarks (Wu et al., 2024; Liu et al., 2022; Jiang et al., 2020), but these evaluations often rely on narrow task formulations (e.g., recognizing videos dominant emotion (Liu et al., 2022; Jiang et al., 2020)) or suffer from limited training data (Wu et al., 2024), hindering systematic analysis. While additional benchmarks exist to assess MLLMs across various dimensions (Li et al., 2024; Liu et al., 2024; Fang et al., 2024; Fu et al., 2024; Song et al., 2024; Chandrasegaran et al., 2024; Zhou et al., 2024; Chen et al., 2024; Xiao et al., 2024; Cheng et al., 2025), none fully address the critical requirements for advancing video understanding. Specifically, there is currently no comprehensive benchmark that fulfills three key criteria: (1) Large-scale training resources to support robust post-training, (2) Structured validation sets to assess different levels of generalizationessential for rigorously evaluating post-training methods like RL, (3) Diverse tasks that balance perception and reasoning under real-world conditions aligned with human beings. To bridge this gap, we introduce SEED-Bench-R1, video understanding benchmark designed to require balanced visual perception and logical reasoning across diverse real-world scenarios. Unlike prior work, our benchmark includes large-scale training dataset and carefully partitioned validation set with three generalization tiers, enabling comprehensive evaluation of MLLM capabilities and training methodologies."
        },
        {
            "title": "3 SEED-BENCH-R1",
            "content": "As shown in Figure 1, SEED-Bench-R1 is benchmark designed for systematically studying the impact of post-training methods for MLLMs on video understanding. Built upon our previous works, EgoPlan-Bench (Chen et al., 2023) and EgoPlan-Bench2 (Qiu et al., 2024), SEED-Bench-R1 features 1) intricated visual inputs from the real world, 2) diverse questions requiring logical inference with common sense to solve practical tasks, 3) rigorous partition of validation sets to assess the robustness and generalization abilities of MLLMs across different levels, and 4) large-scale automatically constructed training questions with easily verifiable ground-truth answers. As shown in Figure 2, the visual inputs and questions of SEED-Bench-R1 are grounded from realistic egocentric videos (Damen et al., 2022; Grauman et al., 2022) capturing everyday human activities. To answer questions in SEED-Bench-R1 correctly, the model must be capable of understanding open-form task goals, tracking long-horizon task progress, perceiving real-time environment state from an egocentric view, and utilizing inherent world knowledge to reason about the next action plan. The ground-truth answer comes from the actual next action occurring right after the current observation in the original uncropped video, with the negative options sampled from the same video. This challenging setting of candidate options demands deep understanding of the environment state from dynamic visual input and world knowledge, such as action order dependency, rather than just the semantic meanings of task goals and actions, to discern the correct action plan. Moreover, the derivation of golden answers is traceable and easy to verify. 4 Technical Report (In Progress) As listed in Table 1, we provide both training and validation datasets to benefit community research. The training dataset is automatically constructed using Epic-Kitchens (Damen et al., 2022) videos recording daily life household tasks in kitchen environments. The validation dataset has undergone strict human verification to ensure correctness and is divided into three levels. The Level-1 (L1) questions are created using the same video source as the training data, representing in-distribution evaluation scenarios where the visual environments and task goals share overlaps with the training data. The Level-2 (L2) questions cover similar task goals as L1, but the visual observations are recorded in unseen kitchen environments by new participators from the Ego4D (Grauman et al., 2022) team. The Level-3 (L3) validation subset utilizes the full set of Ego4D videos beyond the kitchen-specific subset. It contains general-domain questions spanning hobbies, recreation, and work, in addition to daily life. The visual inputs come from variety of in-door and out-door environments, posing greater challenges for testing the models generalization abilities."
        },
        {
            "title": "4 METHOD",
            "content": "We begin with Qwen2-VL-Instruct-7B (Wang et al., 2024) as the base model to investigate whether reinforcement learning (RL) can effectively enhance the video understanding performance of Multimodal Large Language Models (MLLMs) on SEED-Bench-R1. For this exploration, we adopt GRPO (Shao et al., 2024) as representative RL algorithm and compare it with traditional posttraining methods such as supervised fine-tuning (SFT). Both RL and SFT utilize 6k out of the 50k training samples from SEED-Bench-R1 for preliminary study. To improve training efficiency, we limit the maximum number of sampled frames per input video to 16, with frame resolution of 252 252. Besides, we explicitly append the frame indicating the current observation as an additional image input. In the case of SFT, the training data is augmented with chain-of-thought (COT) reasoning processes for supervision. These COT annotations are distilled from stronger models, specifically Qwen2.5VL-Instruct-7B and 72B, using rejection sampling. In contrast, RL eliminates the need for explicit COT annotations and instead relies on rule-based outcome supervision rewards. Similar to DeepSeek-R1 (Guo et al., 2025), we employ prompt template as following: {Question Here} Output the thinking process in <think> </think> and final answer in <answer> </answer> tags, i.e., <think> reasoning process here </think><answer> answer here </answer> to guide the model to generate COT content before providing the final answer. This structured output facilitates easier extraction of answers for evaluation. 4.1 SUPERVISED FINE-TUNING (SFT) SFT is simple post-training method, which refines LLMs by aligning their outputs with desired behaviors using human-curated data. The objective of SFT is to optimize the following loss function: JSF (θ) = [q, Psf t(Q, O)] 1 o (cid:88) t= log πθ (ot q, o<t) Here, the expectation is taken over input-output pairs (q, o) sampled from supervised dataset Psf t(Q, O). The goal is to maximize the average log-likelihood of the model generating the correct token ot at each position t, conditioned on the input and the preceding tokens o<t. This process fine-tunes the model parameters θ to minimize the discrepancy between the models predictions and the ground truth, thereby improving task-specific performance. 4.2 OUTCOME SUPERVISION RL WITH GROUP RELATIVE POLICY OPTIMIZATION (GRPO) GRPO is an RL framework initially developed to enhance the mathematical reasoning capabilities of LLMs. Unlike traditional PPO (Ouyang et al., 2022), GRPO optimizes memory usage by eliminating the need for additional value function approximation. Instead, it simplifies the process by sampling 5 Technical Report (In Progress) multiple candidate responses for the same question and assessing their relative quality based on verifiable rewards. Specifically, for given question sampled from Psf t(Q), GRPO generates group of distinct responses {o1, o2, ..., oG} using the policy model πθold , rather than relying on predefined responses from Psf t(Q, O). The policy is then optimized by maximizing the following objective: JGRP O(θ) = (cid:104) Psf t(Q), {oi}G (cid:26) min (cid:20) πθ (oi,t q, oi,<t) πθold (oi,t q, oi,<t) ˆAi,t, clip (cid:88) i= 1 oi oi (cid:88) t=1 i=1 πθold (O q) (cid:105) 1 (cid:18) πθ (oi,t q, oi,<t) πθold (oi,t q, oi,<t) , 1 ε, 1 + ε (cid:19) (cid:21) ˆAi,t βDKL [πθπref ] (cid:27) Here, ϵ and β are hyperparameters, DKL(πθπref) represents the KL divergence between the trained policy πθ and the reference policy πref, and ˆAi,t is the per-token advantage calculated using outcome supervision based on relative rewards within the group. Specifically, for each response oi to question q, reward ri is assigned based on rules by checking whether the answer extracted from oi matches the ground-truth answer (e.g., ri = 1 if correct, otherwise 0). The rewards are normalized by computing their mean and standard deviation. Outcome supervision directly sets the advantages ˆAi,t of all tokens in the response oi to the normalized reward: ˆAi,t = (cid:101)ri = ri mean({r1, ..., rG}) std({r1, ..., rG}) ."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 OVERALL PERFORMANCE Table 2 summarizes the performance of the MLLM trained using different methods on SEED-BenchR1. Notably, RL demonstrates remarkable data efficiency in improving the MLLMs performance on both in-distribution (L1) and OOD (L2 and L3) questions, despite relying solely on simple outcome-based reward signal without specialized COT annotations. RLs superiority over SFT is particularly promising in OOD scenarios and even extends to distinct video understanding benchmark (i.e., LongVideoBench (Wu et al., 2024) as shown in Table 3). Model Vanilla SFT RL (GRPO) L1 (In-Distribution) L2 (OOD, Cross-Environment) L3 (OOD, Cross-Task, Cross-Environment) Daily Life Daily Life Daily Life Hobbies Recreation Work Overall 34.79 43.79 46.01 34.02 44.10 50.16 31.21 38.27 48. 32.30 41.02 45.08 33.33 32.24 43.72 30.69 38.61 41.34 31.57 38.15 44.89 Table 2: Performance comparison on SEED-Bench-R1s hierarchical validation set. The model trained with GRPO using SEED-Bench-R1s training set demonstrates significant performance improvements across both in-distribution (L1) and out-of-distribution (L2 and L3) scenarios. Model Duration Group (unit: second) Question Category (8,15] (15,60] (180,600] (900,3600] L1 (Perception) L2 (Relation) Vanilla SFT RL (GRPO) 42.33 44.97 54.50 44.77 54.65 53.49 33.98 36.65 42. 25.35 36.35 37.23 34.24 44.80 48.48 31.74 35.81 38.90 Overall 32.90 40.00 43.40 Table 3: Performance comparison on LongVideoBenchs validation set. The model trained with GRPO using SEED-Bench-R1s training set achieves higher performance gains on this general video understanding benchmark, which includes diverse themes. 6 Technical Report (In Progress) Figure 3: The variation curves of completion length and accuracy reward w.r.t. RL training steps. While the reward value generally increases during RL, the completion length of the MLLM does not show significant increase. Figure 4: Comparison of model responses to Level-1 question from SEED-Bench-R1. The visual input includes 16 sampled frames from video (showing task progress) and final observation image. Attention maps (output-to-visual tokens) are shown for each model: the base (Qwen2-VL7B), GRPO fine-tuned, and SFT fine-tuned versions. The base and SFT models exhibited illusory perceptions (red text), while the GRPO model attended more accurately to visual regionse.g., correctly identifying cream cheese in the pot (green box) and suggesting the next step (discarding the empty yogurt container). The SFT models attention was ineffective (red box), and the base models attention was dispersed, impairing judgment. 7 Technical Report (In Progress) Figure 5: Comparison of model responses to Level-2 (out-of-distribution, cross-environment) question from SEED-Bench-R1. The GRPO fine-tuned model demonstrates more accurate attention to hand movement (highlighted in the green box). Interestingly, while the GRPO fine-tuned model produces similar incorrect reasoning steps as the SFT-trained model (red text), it ultimately outputs the correct answer by disregarding the flawed semantic reasoning. This suggests that GRPO, with its outcome-supervised reward signal, primarily enhances visual perception but may compromise the logical coherence and semantic accuracy of the models reasoning process. 5.2 DETAILED ANALYSIS We analyze how RL influences the MLLMs chain-of-thought (COT) generation process and its impact on final answers, focusing on visual perception and logical inference. Limited Emergent Reasoning Abilities. The MLLMs performance improves without significant emergent reasoning abilities. As shown in Figure 3, although the reward value generally climbs during RL, the completion length of the MLLM does not increase notably. This may stem from the base models capacity. By comparing the outputs from the base model (Qwen2-VL-7B) in Figure 4 and Figure 5, we find that the answer accuracy does not consistently improve with longer completions pre-RL. Extended reasoning chains are not always helpful and sometimes can even introduce noise, hindering decision-making. Another reason may be due to the training data quality and the outcomebased reward signal. The training data provided by SEED-Bench-R1 are automatically constructed 8 Technical Report (In Progress) Figure 6: Comparison of model responses to Level-3 (out-of-distribution, cross-environment-task) question from SEED-Bench-R1. The GRPO-tuned model attends effectively to the entire video, while the SFT-tuned model ignores the middle segments. SFT also favors superficial reasoning (e.g., templated phrases like The person has already... and is holding...), often mismatching visual observations, which is also evidenced in Figure 4. This suggests GRPO trains models to search visual space adaptively, whereas SFT encourages memorized reasoning patterns. without manually verifying the uniqueness of the golden answers. Consequently, the occasional data noise makes the model confused and learn to earn outcome-based rewards by skipping the logical inference process, which may be unreasonable for deducing the correct but noisy golden answers during training. Enhanced Visual Perception. Post-RL, the MLLMs COT content aids perception more than reasoning, as evidenced by the case study in Figure 4 to 6. Compared with the counterpart trained with SFT, the MLLM trained with RL may lack logical coherence in the generated COT content. However, by analyzing the attention map from the generated COT tokens to the visual inputs, we find that the COT tokens for the model trained with RL act more like dynamic queries, attending to the visual content more comprehensively and more accurately than those for the model trained with SFT, particularly in OOD scenarios. We speculate that RL teaches the model to search more effectively in the visual space with the additional COT tokens, whereas SFT often forces memorization of 9 Technical Report (In Progress) Figure 7: Failure Case 1. The model post-trained using GRPO-based RL lacks commonsense reasoningit fails to recognize that the tap must be turned on before washing the mango. Figure 8: Failure Case 2. The GRPO-based RL post-trained model overlooks key visual cuesit fails to detect the removal of leek ends due to the limited frame sampling rate and low frame resolution. Additionally, the final answer exhibits semantic inconsistency with the reasoning steps. reasoning patterns, leading to superficial COT content generation without sufficient attention to the visual inputs. This may explain RLs superior generalization. Limitations of Outcome Supervision RL with GRPO. Further, we identify the limitations of our current practice with RL through additional studies on failure cases in Figure 7 to 8. On the one hand, despite improved visual perception after RL, the model may still overlook key visual cues. One of the reasons may lie in constraints like frame sampling rate and image resolution, which are limited to reduce context length for training efficiency. Additionally, captions reflecting perception results typically appear only at the beginning of COT content. Ideally, the model should interleave captioning and logical inference to enable backtracking key visual areas and self-correction. On the other hand, the MLLM post-trained with RL using outcome supervision rewards often produces inconsistent or illogical reasoning chains. While the model may derive correct answers despite flawed COT, this undermines decision-making transparency. Moreover, limited reasoning ability also constrains the upper bound of model performance, since it is essential for the model to combine 10 Technical Report (In Progress) the internal world knowledge with visual perception results to logically infer the correct answer for more challenging questions. Future Directions. Based on our experimental findings, we propose several promising directions for future research: Pre-RL Reasoning Elicitation: Before RL, the base MLLM should exhibit trend of generating improved answers as its chain-of-thought (COT) reasoning becomes more sophisticated. This capability enables self-evolution during RL, where higher-quality COT leads to greater rewards from outcome-based signals. Future work could investigate efficient data curation methods to collect high-quality COT demonstrations that showcase advanced reasoning skillssuch as problem decomposition, reflection, and self-correction. Fine-tuning the model on such data as cold start could streamline and enhance subsequent RL training. Reward Modeling and RL Algorithms: Regularizing the model to balance visual perception and logical inference is crucial. Process-based rewards could explicitly supervise reasoning rationality, preventing shortcuts. Additionally, we have currently only used small proportion of training data from SEED-Bench-R1. Enhancing RL algorithms robustness against noisy reward signals is vital for scaling with larger, less clean datasets and enabling weak-to-strong alignment. Moreover, improving the efficiency of algorithms to allow for long context of visual inputs also plays an important role in large-scale training."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce SEED-Bench-R1, benchmark for systematically evaluating posttraining methods in Multimodal Large Language Models (MLLMs), with focus on video understanding tasks that demand both nuanced perception and robust logical reasoning. By leveraging hierarchically structured validation splits, SEED-Bench-R1 provides rigorous testbed to assess in-distribution and out-of-distribution generalization. Our experiments with reinforcement learning (RL), specifically GRPO, demonstrated its superiority over supervised fine-tuning (SFT) in data efficiency and performance across all generalization levels, even extending to general video benchmarks like LongVideoBench. By refining reward designs, incorporating process supervision, and scaling to larger datasets, future efforts can further bridge the gap between perceptual and reasoning prowess in MLLMs, advancing their applicability in open-ended video understanding tasks."
        },
        {
            "title": "REFERENCES",
            "content": "Learning 2024. learning-to-reason-with-llms/. reason with LLMs, to URL https://openai.com/index/ Keshigeyan Chandrasegaran, Agrim Gupta, Lea Hadzic, Taran Kota, Jimming He, Cristobal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour videolanguage understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2024. Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, Yen-Chun Chen, and Frank Wang. Rextime: benchmark suite for reasoning-across-time in videos. Advances in Neural Information Processing Systems, 37:2866228673, 2024. Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplan-bench: Benchmarking multimodal large language models for human-level planning. arXiv preprint arXiv:2312.06722, 2023. Zixu Cheng, Jian Hu, Ziquan Liu, Chenyang Si, Wei Li, and Shaogang Gong. V-star: Benchmarking video-llms on video spatio-temporal reasoning. arXiv preprint arXiv:2503.11495, 2025. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision (IJCV), 130:3355, 2022. URL https://doi.org/10. 1007/s11263-021-01531-2. Technical Report (In Progress) Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:8909889124, 2024. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1899519012, 2022. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 53565364, 2019. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia, Cheng Lu, and Jiateng Liu. Dfew: large-scale database for recognizing dynamic facial expressions in the wild. In Proceedings of the 28th ACM international conference on multimedia, pp. 28812889, 2020. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Computer Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pp. 740755. Springer, 2014. Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? In Findings of the Association for Computational Linguistics ACL 2024, pp. 87318772, 2024. Yuanyuan Liu, Wei Dai, Chuanxu Feng, Wenbin Wang, Guanghao Yin, Jiabei Zeng, and Shiguang Shan. Mafw: large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild. In Proceedings of the 30th ACM international conference on multimedia, pp. 2432, 2022. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. 12 Technical Report (In Progress) Lu Qiu, Yuying Ge, Yi Chen, Yixiao Ge, Ying Shan, and Xihui Liu. Egoplan-bench2: benchmark for multimodal large language model planning in real-world scenarios. arXiv preprint arXiv:2412.04447, 2024. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1822118232, 2024. Kai Sun, Yushi Bai, Ji Qi, Lei Hou, and Juanzi Li. Mm-math: Advancing multimodal math evaluation with process evaluation and fine-grained classification. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 13581375, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Xiaodong Wang and Peixi Peng. Open-r1-video. Wang-Xiaodong1899/Open-R1-Video, 2025. https://github.com/ Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. Junbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1320413214, 2024. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chainof-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024. Jiaxing Zhao, Xihan Wei, and Liefeng Bo. R1-omni: Explainable omni-multimodal emotion recognition with reinforcing learning. arXiv preprint arXiv:2503.05379, 2025. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. In The Thirteenth International Conference on Learning Representations."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong"
    ]
}