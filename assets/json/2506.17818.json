{
    "paper_title": "CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning",
    "authors": [
        "Angelos-Nikolaos Kanatas",
        "Charilaos Papaioannou",
        "Alexandros Potamianos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in music foundation models have improved audio representation learning, yet their effectiveness across diverse musical traditions remains limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation model developed to enhance cross-cultural music representation learning and understanding. To achieve this, we propose a two-stage continual pre-training strategy that integrates learning rate re-warming and re-decaying, enabling stable adaptation even with limited computational resources. Training on a 650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music traditions, results in an average improvement of 4.9% in ROC-AUC and AP across diverse non-Western music auto-tagging tasks, surpassing prior state-of-the-art, with minimal forgetting on Western-centric benchmarks. We further investigate task arithmetic, an alternative approach to multi-cultural adaptation that merges single-culture adapted models in the weight space. Task arithmetic performs on par with our multi-culturally trained model on non-Western auto-tagging tasks and shows no regression on Western datasets. Cross-cultural evaluation reveals that single-culture models transfer with varying effectiveness across musical traditions, whereas the multi-culturally adapted model achieves the best overall performance. To support research on world music representation learning, we publicly release CultureMERT-95M and CultureMERT-TA-95M, fostering the development of more culturally aware music foundation models."
        },
        {
            "title": "Start",
            "content": "CULTUREMERT: CONTINUAL PRE-TRAINING FOR CROSS-CULTURAL MUSIC REPRESENTATION LEARNING Angelos-Nikolaos Kanatas1,2 Charilaos Papaioannou1,3,4 1 School of ECE, National Technical University of Athens, Greece 2 Institute for Language and Speech Processing, Athena Research Center, Greece 3 Centre for Digital Music, Queen Mary University of London, UK 4 Archimedes, Athena Research Center, Greece el19169@mail.ntua.gr, cpapaioan@mail.ntua.gr, potam@central.ntua.gr Alexandros Potamianos1,4 5 2 0 2 J 1 2 ] . [ 1 8 1 8 7 1 . 6 0 5 2 : r ABSTRACT Recent advances in music foundation models have improved audio representation learning, yet their effectiveness across diverse musical traditions remains limited. We introduce CultureMERT-95M, multi-culturally adapted foundation model developed to enhance crosscultural music representation learning and understanding. To achieve this, we propose two-stage continual pretraining strategy that integrates learning rate re-warming and re-decaying, enabling stable adaptation even with limited computational resources. Training on 650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music traditions, results in an average improvement of 4.9% in ROC-AUC and AP across diverse non-Western music auto-tagging tasks, surpassing prior state-of-the-art, with minimal forgetting on Western-centric benchmarks. We further investigate task arithmetic, an alternative approach to multi-cultural adaptation that merges singleculture adapted models in the weight space. Task arithmetic performs on par with our multi-culturally trained model on non-Western auto-tagging tasks and shows no regression on Western datasets. Cross-cultural evaluation reveals that single-culture models transfer with varying effectiveness across musical traditions, whereas the multiculturally adapted model achieves the best overall performance. To support research on world music representation learning, we publicly release CultureMERT-95M 1 and CultureMERT-TA-95M 2 , fostering the development of more culturally aware music foundation models. 1. INTRODUCTION Foundation models have recently emerged in the music domain [15], offering powerful general-purpose represen1 https://huggingface.co/ntua-slp/CultureMERT-95M 2 https://huggingface.co/ntua-slp/CultureMERT-TA-95M A.-N. Kanatas, C. Papaioannou, and A. Potamianos. Licensed under Creative Commons Attribution 4.0 International License (CC BY 4.0). Attribution: A.-N. Kanatas, C. Papaioannou, and A. Potamianos, CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning, in Proc. of the 26th Int. Society for Music Information Retrieval Conf., Daejeon, South Korea, 2025. tations learned from large-scale audio data. These models capture broad musical characteristics and have demonstrated state-of-the-art performance across range of music understanding tasks, reducing the need for task-specific training. By leveraging self-supervised learning (SSL) on large amounts of unlabelled music data, foundation models address data scarcity, reduce annotation costs, and improve generalization in music information retrieval (MIR) [4]. Despite these advances, most existing foundation models for music have been trained primarily on Westerncentric datasets, limiting their ability to represent diverse musical styles [6, 7]. Many musical traditions, including Turkish, Indian, and Greek traditional music, feature unique melodic structures, modal or tonal systems, and rhythmic patterns that are not adequately captured by these models [810]. Failing to model such culture-specific stylistic elements not only narrows the applicability of music foundation models, for example, in region-specific recommendation systems [11] or cultural heritage preservation, but also overlooks rich, culturally specific knowledge crucial for advancing MIR research [4]. Accordingly, there is an urgent need to develop more inclusive and culturally aware computational models [12,13], capable of generalizing beyond Western-centric traditions and adapting effectively to diverse underrepresented musical cultures. One promising avenue for addressing these challenges is continual pre-training (CPT), which has emerged as an effective and increasingly popular approach in large language models (LLMs) [1422] and multimodal learning [23]. By enabling models to incrementally adapt to new domains, tasks, or languages, CPT avoids the need for full re-training, which is often impractical and computationally expensive [14, 18, 19, 22, 24]. Notably, it has been shown to match, or even surpass, training from scratch in some cases [20, 21], while also converging faster [25] and mitigating catastrophic forgetting [26]. CPT has also gained traction in the audio domain, with recent work demonstrating its effectiveness in adapting pre-trained speech models to both highand low-resource languages [24, 2730]. Additionally, model merging [3134] has proven to be simple yet effective technique for adapting pre-trained models across multiple domains by combining domainspecific parameters in weight space, without requiring additional training [35] or access to the original training data [36]. notable method within this paradigm is task arithmetic (TA) [37], which constructs task vectors by computing the difference between the parameters of an adapted model and its pre-trained counterpart, thereby encoding domain-specific knowledge. These task vectors can then be integrated into the pre-trained model via algebraic operations in Euclidean space to create unified model from multiple independently adapted models. While both continual pre-training and task arithmetic have been widely explored in other domains, their application to MIR remains largely unexplored. We bridge this gap by leveraging these techniques to adapt the MERT-v1-95M music foundation model [1], originally trained on 1K hours of predominantly Western music [1, 38], to diverse musical cultures from the Eastern Mediterranean and the Indian subcontinent, while preserving performance on \"Western\"-centric benchmarks. We summarize our main contributions as follows: 1. To the best of our knowledge, this is the first study to explore continual pre-training and task arithmetic for cross-cultural adaptation in MIR, demonstrating their effectiveness in music audio representation learning. 2. We propose two-stage CPT strategy that stabilizes training, mitigates catastrophic forgetting, and facilitates effective adaptation under constrained computational resources. 3. Our multi-cultural model, CultureMERT, outperforms the original MERT-v1 by an average of 4.9% across ROC-AUC and AP on culturally diverse nonWestern music tagging tasks, while exhibiting minimal forgetting on Western benchmarks. 4. Our culturally adapted models surpass previous state-of-the-art results across all evaluated nonWestern music tagging tasks. 5. We analyze cross-cultural transferability, showing that single-culture adaptations exhibit varying degrees of transfer across cultural domains. To support reproducibility and further research in crosscultural music representation learning, we publicly release CultureMERT-95M, along with the task arithmetic variant, CultureMERT-TA-95M. 2. DATASETS For our experiments, we use diverse set of music datasets spanning both Western and non-Western traditions. Specifically, we adopt the MagnaTagATune (MTAT) [39] and FMA-medium [40] datasets to represent \"Western\" 3 music. For \"non-Western\" traditions, we incorporate the Lyra corpus [41], featuring Greek traditional and folk music, along with three collections from the CompMusic Corpora 4 [42]: Turkish-makam [43, 44], which, together with 3 We use the term Western to refer to music styles predominantly rooted in Western cultures, including pop, rock, and Western classical. 4 https://compmusic.upf.edu/corpora Lyra, represent music of the Eastern Mediterranean; and Hindustani and Carnatic music [45], representing North and South Indian classical traditions, respectively. We assess our models on both Western and nonWestern music tagging tasks for cross-cultural evaluation, using standard multi-label classification metrics, including the area under the receiver operating characteristic curve (ROC-AUC) and average precision (AP). Following [46, 47], we utilize the top-k tags relevant to each dataset: 50 tags for MTAT (spanning genre, instruments, and mood), 20 hierarchical genre tags for FMA-medium, 30 tags for Turkish-makam (covering makam, usul, and instruments), 20 tags for Hindustani and Carnatic (primarily reflecting raga, tala, instruments, and forms), and 30 tags for Lyra (related to genre, place, and instruments). All audio is resampled to 24 kHz, and we adopt the same data splits as [46]. To prepare our data for continual pre-training, we extract 30-second segments from each training split of the non-Western datasets. Given the varying dataset sizes, we balance the pre-training duration across cultures to ensure proportional representation by extracting 200 hours each from the Turkish-makam, Carnatic, and Hindustani datasets, and 50 hours from Lyra due to its smaller size. Additionally, we combine these subsets to construct unified 650-hour dataset integrating all four traditions for multi-cultural continual pre-training. 3. METHOD The overall framework of our approach is illustrated in Figure 1, which depicts the two-stage continual pre-training strategy for CultureMERT. In this section, we first review the architecture and pre-training objective of MERT, and then present our CPT strategy for cultural adaptation. Finally, we investigate task arithmetic, an alternative approach to multi-cultural adaptation that merges culturally specialized models in weight space to construct unified multi-cultural model, CultureMERT-TA. 3.1 MERT Pre-Training Objective Our continual pre-training objective follows the selfsupervised masked language modeling (MLM) objective of MERTRVQ-VAE, where two teacher models provide the pseudo-labels: (i) an acoustic teacher, the EnCodec model [48], which discretizes audio into tokens from = 8 residual vector quantization (RVQ) codebooks, each containing = 1024 codewords, and (ii) musical teacher, based on constant-Q transform (CQT) spectrogram reconstruction, encoding pitch and harmonic structure. MERT-v1-95M follows the HuBERT architecture [49], comprising CNN-based feature extractor that encodes raw 24 kHz waveforms into 75 Hz frame-level representations, followed by 12-layer Transformer encoder, producing 768-dimensional contextual embeddings. During training, subset of frame embeddings is masked, and the model is optimized using multi-task learning (MTL) objective, combining masked acoustic token prediction and spectrogram reconstruction. The overall training objective is: = αLRVQ + LCQT, (1) where the acoustic MLM loss LRVQ encourages the model to predict masked RVQ-VAE tokens from codebooks, using noise-contrastive estimation (NCE) loss: LRVQ = (cid:88) (cid:88) k=1 tM log pθ(ct,kx t), (2) with denoting the set of masked time frames, ct,k the ground-truth discrete codeword from the k-th codebook at time frame extracted via the EnCodec tokenizer, and pθ the models predicted token distribution: pθ(cx t) = exp(sim(T (ot), ec)/τ ) c=1 exp(sim(T (ot), ec)/τ ) (cid:80)C . (3) Here, is the masked input feature, ot is the models output representation, (ot) projects it to the codeword embedding space, ec is the embedding of codeword Ck, where {1, . . . , K}, sim(, ) denotes cosine similarity, and τ = 0.1 is temperature scaling parameter. The CQT reconstruction loss LCQT minimizes the mean squared error (MSE) between the models predicted ˆzCQT,t and ground-truth zCQT,t frame-level CQT features: LCQT = (cid:88) tM zCQT,t ˆzCQT,t2 2 . (4) 3.2 Two-Stage Continual Pre-Training Strategy To adapt the MERT foundation model to diverse musical traditions, we employ continual pre-training, which extends the training of pre-trained model on new data, aiming to adapt it to shifted domain or task while retaining prior knowledge, without re-training from scratch. this involves continually pre-training the In our case, MERT-v1-95M model, using the same pre-training objective, on culturally diverse data that introduce significant distribution shift, as it was initially trained on predominantly Western music [1,38]. Given this shift, naively continuing to train the model, i.e., adapting all parameters at once without resetting the learning rate, can lead to catastrophic forgetting [50] and poor adaptation [14], as confirmed by our preliminary experiments (see Table 1). To address this, we propose two-stage strategy that stabilizes training through: (i) learning rate re-warming and redecaying [14, 19, 21, 23, 51], and (ii) staged adaptation. Staged Adaptation In our preliminary experiments, we observed an initial performance drop during CPT, followed by slow recovery phase, phenomenon known as the stability gap [23, 52, 53]. This instability arises due to the abrupt adaptation of model parameters to substantially shifted data distribution, which can temporarily degrade previously learned representations before stabilizing. To mitigate this, rather than full-parameter adaptation on the entire dataset in single epoch, which induces large plasticity gradient for long period [53], we split training into two stages to reduce instability and ensure smoother adaptation, as illustrated in Figure 1: Figure 1: Two-Stage Continual Pre-Training Strategy for CultureMERT. In Stage 1, subset of parameters is trained on 100h of multi-cultural data with 20% Western music for stabilization. In Stage 2, all parameters are unfrozen and trained on the full 650h dataset. Learning rate re-warming and re-decaying is applied in both stages. Stage 1 Stabilization Phase: We first train on smaller data subset [52], updating only the CNN-based feature extractor and the codeword embedding layer while keeping the Transformer encoder frozen. To reduce the distribution gap and mitigate forgetting [19, 25, 28], we incorporate fraction of Music4All data [54], which is primarily of Western origin, into the pre-training mix, accounting for 20% of the total training data (Western replay). Stage 2 Full Adaptation: We unfreeze the Transformer encoder and continue training on the full dataset. CPT Strategy Western Replay Turkish-makam MTAT MERT-v1 (Baseline) Single-stage Single-stage (no re-warm) - Two-stage (Ours) Two-stage (Ours) Stage 1 Both stages 83. 83.8 83.0 89.6 88.6 89.6 86.0 87.5 89.2 89.4 Table 1: CPT Strategy Comparison. ROC-AUC scores on Turkish-makam and MTAT datasets. Two-stage CPT outperforms single-stage adaptation, with Western replay limited to Stage 1 yielding the best trade-off between cultural adaptation and knowledge retention. This two-stage approach is particularly motivated specifically the batch by computational constraints, size mismatch between pre-training and adaptation. MERT-v1-95M was originally trained with batch sizes of 1.5 hours per step, whereas we use significantly smaller effective batch size of 160 seconds per step due to memory limitations. Training with this reduced batch size directly on the entire dataset with full-parameter adaptation resulted in unstable training and frequent crashes, degrading performance on both Western and non-Western benchmarks. By structuring adaptation in two stages, we strike to balance plasticity (adaptation to non-Western traditions) and stability (retaining knowledge on Western datasets), challenge known as the stability-plasticity dilemma [55, 56]. Intuitively, the initial stabilization phase allows lower-level acoustic representations, captured by the CNN-based feature extractor and the codeword embeddings, to adapt first and calibrate to the shifted distribution before updating high-level Transformer representations. Learning Rate Re-Warming To further improve adaptation stability, we apply learning rate re-warming and re-decaying in both stages. Prior work has shown that resetting the learning rate schedule, i.e., re-warming the model, during continual pre-training is crucial for preventing poor convergence and mitigating catastrophic forgetting [14, 19, 21, 23, 51]. In Stage 1, we adopt moderately aggressive warm-up and decay schedule to encourage early adaptation of low-level representations. In Stage 2, less aggressive schedule balances plasticity and stability during full-model training, reducing also training instabilities. Following this two-stage CPT strategy, we develop (i) multitwo types of culturally adapted models: culturally adapted model, CultureMERT, trained on culturally diverse mix spanning all four non-Western musical traditions; and (ii) single-culture adapted models, each continually pre-trained on data from single tradition, resulting in MakamMERT, HindustaniMERT, CarnaticMERT, and LyraMERT. 3.3 Task Arithmetic for Cross-Cultural Adaptation As an alternative to continual pre-training on multi-cultural data, we explore task arithmetic [37], model merging method that combines culturally specialized models in weight space to construct unified multi-cultural model. Task arithmetic operates by algebraically merging model parameters through task vector addition and negation. In our setting, we obtain task vectors by computing the element-wise difference between the parameters of the single-culture continually pre-trained models and those of the MERT-v1 model. Formally, given the pre-trained base model with parameters θpre and continually pre-trained model θi adapted to cultural dataset Di, the task vector for culture is given by τi = θi θpre, capturing the parameter shift induced by culture-specific adaptation. For multi-cultural adaptation, we construct unified model θmerged by merging single-culture adapted models via task arithmetic, summing their respective task vectors τi with corresponding scaling factors λi: θmerged = θpre + (cid:88) i=1 λiτi, (5) where λi are scalar hyperparameters that control the contribution of each task vector. Prior work typically uses single scaling factor λ for all task vectors, i.e., λi = λ, i. In the special case where λ = 1/N , Equation 5 simplifies to weight averaging [31, 33, 34], in which the adapted models are merged by directly averaging their parameters. 4. EXPERIMENTS 4.1 Implementation Details In all continual pre-training setups, we initialize our models from the publicly available MERT-v1-95M 5 pretrained checkpoint. Training was conducted using the FAIRSEQ 6 framework on single NVIDIA GeForce GTX TITAN GPU with 12 GB of memory. All models were trained with half-precision (FP16), using 5-second audio segments as input context, randomly cropped from the extracted 30-second pre-training audio data. The weight of the acoustic loss in the pre-training objective is set to α = 10.0. The EnCodec neural audio codec (NAC) model [48], which tokenizes audio into discrete codewords, remains frozen during continual pre-training, as in [1]. To enhance representation robustness, we apply in-batch noise mixture augmentation with mixup probability of 0.5, and use pre-layer normalization (Pre-LN) [57] for training stability, following [1]. Other training settings mirror those of the MERT-v1-95M setup. 4.2 Probing-Based Evaluation Following [1, 2, 58], we adopt probing-based evaluation rather than fine-tuning, keeping the pre-trained models frozen as deep feature extractors while training only shallow multilayer perceptron (MLP) with single 512dimensional hidden layer for sequence-level tasks. Our evaluation follows the MARBLE protocol [59] under constrained settings, and we apply it to both Western and nonWestern music tagging tasks for cross-cultural evaluation. To process long-duration audio files, we segment them into 30-second chunks using sliding window approach and aggregate the chunk-level predictions by averaging to obtain the final prediction for the entire audio file. For Turkish-makam, Hindustani, and Carnatic tasks, we apply maximum duration cut as in [46] to ensure comparability with prior state-of-the-art results. 4.3 Continual Pre-Training Settings Multi-Cultural CPT In Stage 1, training runs for 2,250 steps with 10% linear warm-up period, using 100 hours of the dataset. Optimization follows AdamW [61] with β1 = 0.9, β2 = 0.999, and ϵ = 1e5. Training employs an effective batch size of 32 recordings (160 seconds), with gradient accumulation over 8 steps. The maximum learning rate is set to ηmax = 5e4, followed by cosine decay to minimum of ηmin = 5e5. Gradient clipping is applied with norm of 1.0 to prevent exploding gradients. In Stage 2, training extends to 14,625 steps with 1% warmup period, using the full 650-hour dataset. Optimization follows AdamW with β1 = 0.9, β2 = 0.95, and ϵ = 1e5, maintaining the same batch size as Stage 1. The learning rate decays from maximum value of ηmax = 5e5 to ηmin = 5e6. Gradient clipping remains at 1.0. Single-Culture CPT In Stage 1, we train on 60 hours for total of 1,350 training steps. In Stage 2, we expand training 5 https://huggingface.co/m-a-p/MERT-v1-95M 6 https://github.com/facebookresearch/fairseq Dataset Metrics Turkish-makam Hindustani Carnatic Lyra FMA-medium MagnaTagATune Avg. ROC AP ROC AP ROC AP ROC AP ROC AP ROC AP MERT-v1 83.20.08 53.30.12 82.40.04 52.90.19 74.90.05 39.70.15 85.70.10 56.50.18 90.70.04 48.10.11 89.60.07 35.90.15 66.1 88.70.11 58.80.22 84.50.16 57.80.18 77.60.14 42.70.16 84.60.12 53.20.17 90.30.12 47.10.16 89.00.07 35.60.12 67.5 MakamMERT 88.40.06 58.40.16 87.00.06 60.20.14 78.80.13 44.00.17 85.40.11 55.80.16 90.20.10 46.70.09 89.20.10 35.30.11 68.3 CarnaticMERT HindustaniMERT 88.30.12 58.20.16 87.40.11 60.30.16 77.00.12 42.70.16 84.20.13 52.00.15 90.20.13 46.10.10 89.10.09 35.80.13 67.6 86.70.07 56.80.13 85.90.08 57.40.13 76.40.09 40.10.13 85.00.11 53.50.14 90.00.08 46.00.16 88.90.05 35.10.14 66.8 LyraMERT CultureMERT 89.60.09 60.60.21 88.20.20 63.50.24 79.20.18 43.10.22 86.90.10 56.70.20 90.70.09 48.10.13 89.40.09 35.90.16 69.3 CultureMERT-TA 89.00.12 61.00.18 87.50.10 59.30.13 79.10.11 43.30.13 87.30.08 57.30.19 90.80.06 49.10.15 89.60.10 36.40.14 69.1 (Previous) SOTA 87.7 [46] 57.7 [46] 86.5 [46] 63.1 [46] 77.0 [46] 43.9 [46] 85.4 [46] 54.3 [46] 92.4 [46] 53.7 [46] 92.7 [60] 41.4 [58] - Table 2: Evaluation Results (ROC-AUC and AP) of Pre-Trained and Culturally Adapted MERT Models on Diverse Music Auto-Tagging Tasks. We report averages across five random seeds with standard deviations as subscripts. The \"Avg.\" column represents the average performance across all datasets and evaluation metrics for each model. The results highlight the impact of multi-cultural CPT and model merging via task arithmetic on cross-cultural adaptation and transfer. AUC and AP), demonstrating the efficacy of our approach. We further observe that single-culture adapted models tend to perform best on their respective in-domain tasks for well-resourced traditions, reaffirming the effectiveness of CPT for domain-specific adaptation [18]. However, even low-resource adaptation, as in the case of LyraMERT trained on just 50 hours, leads to noticeable gains across other non-Western tasks, indicating that even limited cultural exposure can significantly boost cross-cultural generalization. Moreover, task arithmetic performs comparably to CultureMERT on non-Western tasks and even surpasses it on Western benchmarks and Lyra, demonstrating that weight-space merging of culturally specialized models can serve as an effective, training-free alternative to multi-cultural CPTprovided such models are available. Interestingly, it also outperforms the unadapted base model by 0.4% on average across Western tasks. Notably, only the multi-cultural models, CultureMERT and CultureMERT-TA, outperform MERT-v1 on Lyra, where the latter already serves as strong baseline. This further underscores the effectiveness of multi-cultural adaptation, particularly in low-resource and transfer settings. Finally, CultureMERT and CultureMERT-TA surpass previous state-of-the-art (SOTA) results on all nonWestern music tagging tasks, with the best task arithmetic variant obtained using λ = 0.2 (see Figure 4). 5.1 Cross-Cultural Transfer As illustrated in Figure 2, continual pre-training on one musical tradition can benefit others to varying degrees, revealing asymmetries in cross-cultural transfer effectiveness. For instance, we observe strong transfer between Turkish-makam and Carnatic music, with models adapted to either tradition generalizing well to the other. This aligns with their shared theoretical foundations as modal frameworks that emphasize microtonality and improvisation, serving similar roles in their respective cultures [62]. Additionally, the strong performance of the Carnaticadapted model on the Hindustani domain reinforces the musical proximity between these traditions, particularly in Figure 2: Cross-Cultural Transferability. Relative ROC-AUC performance across datasets, highlighting key trends in cross-cultural transfer. CultureMERT generalizes well to non-Western datasets, while task arithmetic performs on par in these settings and even surpasses both the pre-trained and multi-culturally adapted models on Western benchmarks (FMA-medium, MTAT) and Lyra. to the full 200-hour dataset for 4,500 steps. We employ the same optimizers, batch size, and learning rate schedules as in the multi-cultural CPT. For Lyra, due to its smaller size (50 hours), we train on 20 hours in Stage 1 (450 steps) and then on the full dataset in Stage 2 (1,125 steps). 5. RESULTS AND DISCUSSION As shown in Table 2, CultureMERT, adapted via multicultural continual pre-training, consistently outperforms the original MERT-v1 model across all non-Western tasks and evaluation metrics, achieving an average improvement of 4.9%. It also surpasses the single-culture adapted models on average, suggesting that incorporating culturally diverse data during CPT benefits all non-Western traditions by improving the quality of representations for each individual culture, thereby enhancing generalization. Notably, CultureMERT achieves this with minimal forgetting on Western benchmarks (0.05% average drop across ROCtheir shared use of raga (melodic mode) and tala (rhythmic framework) [10]. Interestingly, the model adapted to Carnatic music appears to be the most consistently transferable among single-culture adaptations, achieving strong results not only within Indian classical traditions but also generalizing well to Turkish-makam and Lyra. 5.2 Token-Level Culture Similarity To further examine cross-cultural similarities in our data, we analyze token overlap across musical traditions using both the Jensen-Shannon divergence (JSD) and cosine distance between token distributions extracted from the EnCodec model [48], which serves as our audio tokenizer. Lower values in both metrics indicate greater similarity. Our analysis, as shown in Figure 3, reveals strong tokenlevel similarity among non-Western traditions, particularly between Hindustani and Carnatic music. In contrast, Western datasets (MTAT, FMA-medium) are highly similar to each other but notably dissimilar from non-Western traditions. Greek traditional music (Lyra), while distinct, aligns more closely with non-Western traditions than Western ones. Interestingly, these findings correlate with our results on cross-cultural transfer (Section 5.1), suggesting that token-level similarity metrics can serve as predictors of positive cross-cultural transfer. This insight has practical implications: such similarity metrics can guide the selection and refinement of pre-training data mixtures during CPT, or inform the adjustment of arithmetic operations when merging models via task arithmetic. Similar approaches for quantifying language similarity and predicting positive cross-lingual transfer, based on the similarity of extracted linguistic or acoustic tokens, have been explored in both the text [17, 63] and speech domains [29]. Figure 3: Token Similarity Across Cultures. Pairwise similarity between acoustic token distributions extracted from the EnCodec NAC model [48]. Similarity scores are averaged across 8 codebooks, each containing 1024 discrete codewords (acoustic pseudo-tokens). 5.3 Task Arithmetic Scaling Factor key consideration in task arithmetic is the choice of the scaling factor λ, which controls the balance between task vectors. Prior work [64, 65] has shown that suboptimal values can significantly degrade performance in multi-task model merging. We systematically evaluate different values of shared scaling factor λ {0.1, 0.2, 0.25, 0.3, 0.5, 0.75, 1.0}, applied uniformly across all task vectors, including the special case of weight averaging (λ = 0.25). We similarly observe consistent ill-suited values, such as λ = 1.0, result in poor trend: performance across all benchmarks, as shown in Figure 4. Figure 4: Effect of Scaling Factor λ on Task Arithmetic Performance. The ROC-AUC scores across six diverse music tagging tasks demonstrate how varying λ impacts task arithmetic when merging the four non-Western singleculture adapted models. 6. CONCLUSIONS In this paper, we introduce CultureMERT-95M, multiculturally adapted music foundation model developed via continual pre-training on diverse non-Western musical traditions. We propose two-stage CPT strategy that incorporates learning rate re-warming and staged adaptation for stable training. Cross-cultural evaluation demonstrates that CultureMERT-95M consistently outperforms the base MERT-v1-95M model on non-Western music tagging tasks, surpassing prior state-of-the-art methods while preserving performance on Western benchmarks. Additionally, we investigate task arithmetic, which offers strong alternative to multi-cultural CPT by effectively merging culturally specialized models in weight space. While our results are promising, several limitations remain. The frozen EnCodec tokenizer used in the MERT architecture may be suboptimal for encoding culturally diverse musical languages, as it was pre-trained on Western music. Future directions include scaling to additional musical cultures, exploring alternative architectures, extending evaluation beyond sequence-level classification tasks, conducting fine-grained ablation studies, and investigating whether the proposed two-stage CPT strategy remains necessary under less constrained computational budgets. 7. ETHICS STATEMENT 7.1 Cultural Framing and Interpretive Scope We acknowledge the limitations of framing music within \"Western\" versus \"non-Western\" dichotomy. While such terminology is commonly used in computational research for convenience, it risks oversimplifying the rich diversity of global musical traditions. Furthermore, this work does not aim to establish or analyze cross-cultural similarities from an ethnomusicological perspective. Our analysis of cross-cultural transferability should be considered in light of potential limitations in the representativeness and coverage of the corpora used. 7.2 Responsible Use Careful consideration is advised before deploying these models in real-world contexts, as they may still reflect cultural and dataset-specific biases. Some of the datasets used in this work are not publicly available and were obtained under research-use agreements. The released models should not be used in commercial or generative applications without explicit attention to cultural representation, appropriate licensing, and the consent of the relevant communities or dataset curators. 8. ACKNOWLEDGMENTS We would like to thank the reviewers and Georgios Paraskevopoulos for their valuable and constructive feedback, which helped us improve this work. We also gratefully acknowledge the Music Technology Group (MTG) at Universitat Pompeu Fabra for providing access to datasets used in this study. This work has been partially supported by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program. 9. REFERENCES [1] Y. Li, R. Yuan, G. Zhang, Y. Ma, X. Chen, H. Yin, C. Xiao, C. Lin, A. Ragni, E. Benetos et al., MERT: acoustic music understanding model with large-scale self-supervised training, in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [2] M. Won, Y. Hung, and D. Le, foundation model for music informatics, in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024. IEEE, 2024, pp. 12261230. [3] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and I. Sutskever, Jukebox: generative model for music, CoRR, vol. abs/2005.00341, 2020. [4] Y. Ma, A. Øland, A. Ragni, B. M. D. Sette, C. Saitis, C. Donahue, C. Lin, C. Plachouras, E. Benetos, E. Quinton et al., Foundation models for music: survey, CoRR, vol. abs/2408.14340, 2024. [5] W. Li, Y. Cai, Z. Wu, W. Zhang, Y. Chen, R. Qi, M. Dong, P. Chen, X. Dong, F. Shi et al., survey of foundation models for music understanding, CoRR, vol. abs/2409.09601, 2024. [6] E. Gómez, P. Herrera, and F. Gómez-Martin, Computational Ethnomusicology: perspectives and challenges, Journal of New Music Research, vol. 42, no. 2, June 2013, pp. 111112. [7] A. Mehta, S. Chauhan, A. Djanibekov, A. Kulkarni, G. Xia, and M. Choudhury, Music for all: Exploring multicultural representations in music generation models, CoRR, vol. abs/2502.07328, 2025. [8] T. Lidy, C. N. S. Jr., O. Cornelis, F. Gouyon, A. Rauber, C. A. A. Kaestner, and A. L. Koerich, On the suitability of state-of-the-art music information retrieval methods for analyzing, categorizing and accessing nonwestern and ethnic music collections, Signal Process., vol. 90, no. 4, 2010, pp. 10321048. [9] G. Plaja-Roglans, T. Nuttall, L. Pearson, X. Serra, and M. Miron, Repertoire-specific vocal pitch data generation for improved melodic analysis of carnatic music, Trans. Int. Soc. Music. Inf. Retr., vol. 6, no. 1, 2023, pp. 1326. [10] G. K. Koduri, M. Miron, J. Serrà, and X. Serra, Computational approaches for the understanding of melody in carnatic music, in Proceedings of the 12th International Society for Music Information Retrieval Conference, ISMIR 2011, Miami, Florida, USA, October 2428, 2011, A. Klapuri and C. Leider, Eds. University of Miami, 2011, pp. 263268. [11] A. Ferraro, G. Ferreira, F. Diaz, and G. Born, Measuring commonality in recommendation of cultural content to strengthen cultural citizenship, Trans. Recomm. Syst., vol. 2, no. 1, 2024, pp. 10:110:32. [12] C. C. Liu, I. Gurevych, and A. Korhonen, Culturally aware and adapted NLP: taxonomy and survey of the state of the art, CoRR, vol. abs/2406.03930, 2024. [13] A. Holzapfel, B. L. Sturm, and M. Coeckelbergh, Ethical dimensions of music information retrieval technology, Trans. Int. Soc. Music. Inf. Retr., vol. 1, no. 1, 2018, pp. 4455. [14] A. Ibrahim, B. Thérien, K. Gupta, M. L. Richter, Q. G. Anthony, E. Belilovsky, T. Lesort, and I. Rish, Simple and scalable strategies to continually pre-train large language models, Trans. Mach. Learn. Res., vol. 2024, 2024. [15] D. M. Alves, J. Pombal, N. M. Guerreiro, P. H. Martins, J. Alves, M. A. Farajian, B. Peters, R. Rei, P. Fernandes, S. Agrawal et al., Tower: An open multilingual large language model for translation-related tasks, CoRR, vol. abs/2402.17733, 2024. [16] L. Voukoutis, D. Roussis, G. Paraskevopoulos, S. Sofianopoulos, P. Prokopidis, V. Papavasileiou, A. Katsamanis, S. Piperidis, and V. Katsouros, Meltemi: The first open large language model for greek, CoRR, vol. abs/2407.20743, 2024. [17] E. Gogoulou, T. Lesort, M. Boman, and J. Nivre, Continual learning under language shift, in Text, Speech, and Dialogue - 27th International Conference, TSD 2024, Brno, Czech Republic, September 9-13, 2024, Proceedings, Part I, ser. Lecture Notes in Computer Science, E. Nöth, A. Horák, and P. Sojka, Eds., vol. 15048. Springer, 2024, pp. 7184. [18] S. Gururangan, A. Marasovic, S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, and N. A. Smith, Dont stop pretraining: Adapt language models to domains and tasks, in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds. Association for Computational Linguistics, 2020, pp. 83428360. [19] J. Parmar, S. Satheesh, M. Patwary, M. Shoeybi, and B. Catanzaro, Reuse, dont retrain: recipe for continued pretraining of language models, CoRR, vol. abs/2407.07263, 2024. [20] K. Fujii, T. Nakamura, M. Loem, H. Iida, M. Ohi, K. Hattori, H. Shota, S. Mizuki, R. Yokota, and N. Okazaki, Continual pre-training for cross-lingual LLM adaptation: Enhancing japanese language capabilities, CoRR, vol. abs/2404.17790, 2024. [21] K. Gupta, B. Thérien, A. Ibrahim, M. L. Richter, Q. Anthony, E. Belilovsky, I. Rish, and T. Lesort, Continual pre-training of large language models: CoRR, vol. How to (re)warm your model? abs/2308.04014, 2023. [22] H. Shi, Z. Xu, H. Wang, W. Qin, W. Wang, Y. Wang, large lanand H. Wang, Continual guage models: comprehensive survey, CoRR, vol. abs/2404.16789, 2024. learning of [23] V. Udandarao, K. Roth, S. Dziadzio, A. Prabhu, M. Cherti, O. Vinyals, O. J. Hénaff, S. Albanie, Z. Akata, and M. Bethge, practitioners guide to real-world continual multimodal pretraining, in Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, Eds., 2024. [24] K. Nowakowski, M. Ptaszynski, K. Murasaki, and J. Nieuwazny, Adapting multilingual speech representation model for new, underresourced language through multilingual fine-tuning and continued pretraining, Inf. Process. Manag., vol. 60, no. 2, 2023, p. 103148. [25] W. Zheng, W. Pan, X. Xu, L. Qin, L. Yue, and M. Zhou, Breaking language barriers: Cross-lingual continual pre-training at scale, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Y. Al-Onaizan, M. Bansal, and Y. Chen, Association for Computational Linguistics, Eds. 2024, pp. 77257738. [26] A. Cossu, A. Carta, L. C. Passaro, V. Lomonaco, T. Tuytelaars, and D. Bacciu, Continual pre-training mitigates forgetting in language and vision, Neural Networks, vol. 179, 2024, p. 106492. [27] M. DeHaven and J. Billa, Improving low-resource speech recognition with pretrained speech models: Continued pretraining vs. semi-supervised training, CoRR, vol. abs/2207.00659, 2022. [28] H. Zhu, G. Cheng, J. Wang, W. Hou, P. Zhang, and Y. Yan, Boosting cross-domain speech recognition with self-supervision, IEEE ACM Trans. Audio Speech Lang. Process., vol. 32, 2024, pp. 471485. [29] N. San, G. Paraskevopoulos, A. Arora, X. He, P. Kaur, O. Adams, and D. Jurafsky, Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens, in Proceedings of the 6th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, SIGTYPE 2024, St. Julians, Malta, March 22, 2024, M. Hahn, A. Sorokin, R. Kumar, A. Scherbakov, Y. Otmakhova, J. Yang, O. Serikov, P. Rani, E. M. Ponti, S. Muradoglu et al., Eds. Association for Computational Linguistics, 2024, pp. 100112. [30] A. A. Attia, D. Demszky, T. Ògúnrèmí, J. Liu, and C. Y. Espy-Wilson, Cpt-boosted wav2vec2.0: Towards noise robust speech recognition for classroom environments, CoRR, vol. abs/2409.14494, 2024. [31] M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. G. Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith et al., Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, in International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, ser. Proceedings of Machine Learning Research, K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvári, G. Niu, and S. Sabato, Eds., vol. 162. PMLR, 2022, pp. 23 965 23 998. [32] E. Yang, L. Shen, G. Guo, X. Wang, X. Cao, J. Zhang, and D. Tao, Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities, CoRR, vol. abs/2408.07666, 2024. [33] J. Choi, D. Kim, C. Lee, and S. Hong, Revisiting weight averaging for model merging, CoRR, vol. abs/2412.12153, 2024. [34] G. Ilharco, M. Wortsman, S. Y. Gadre, S. Song, H. Hajishirzi, S. Kornblith, A. Farhadi, and L. Schmidt, Patching open-vocabulary models by interpolating weights, in Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., 2022. [35] G. Stoica, D. Bolya, J. Bjorner, P. Ramesh, T. Hearn, and J. Hoffman, Zipit! merging models from different tasks without training, in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [36] X. Jin, X. Ren, D. Preotiuc-Pietro, and P. Cheng, Dataless knowledge fusion by merging weights of language models, in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [37] G. Ilharco, M. T. Ribeiro, M. Wortsman, L. Schmidt, H. Hajishirzi, and A. Farhadi, Editing models with task arithmetic, in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [38] D. Li, Y. Ma, W. Wei, Q. Kong, Y. Wu, M. Che, F. Xia, E. Benetos, and W. Li, Mertech: Instrument playing technique detection using self-supervised pretrained model with multi-task finetuning, in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024. IEEE, 2024, pp. 521525. [39] E. Law, K. West, M. I. Mandel, M. Bay, and J. S. Downie, Evaluation of algorithms using games: The case of music tagging, in Proceedings of the 10th International Society for Music Information Retrieval Conference, ISMIR 2009, Kobe International Conference Center, Kobe, Japan, October 26-30, 2009, K. Hirata, G. Tzanetakis, and K. Yoshii, Eds. International Society for Music Information Retrieval, 2009, pp. 387392. [40] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, FMA: dataset for music analysis, in Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, S. J. Cunningham, Z. Duan, X. Hu, and D. Turnbull, Eds., 2017, pp. 316323. [41] C. Papaioannou, I. Valiantzas, T. Giannakopoulos, M. A. Kaliakatsos-Papakostas, and A. Potamianos, dataset for greek traditional and folk music: Lyra, in Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022, Bengaluru, India, December 4-8, 2022, P. Rao, H. A. Murthy, A. Srinivasamurthy, R. M. Bittner, R. C. Repetto, M. Goto, X. Serra, and M. Miron, Eds., 2022, pp. 377383. [42] X. Serra, Creating research corpora for the computational study of music: the case of the compmusic project, in AES International Conference on Semantic Audio 2014, London, UK, January 27-29, 2014, C. Dittmar, G. Fazekas, and S. Ewert, Eds. Audio Engineering Society, 2014. [43] B. Uyar, H. S. Atli, S. Sentürk, B. Bozkurt, and X. Serra, corpus for computational research of turkish makam music, in Proceedings of the 1st International Workshop on Digital Libraries for Musicology, DLfM@JCDL 2014, London, United Kingdom, September 12, 2014, B. Fields and K. R. Page, Eds. ACM, 2014, pp. 17. [44] S. Sentürk, Computational analysis of audio recordings and music scores for the description and discovery of ottoman-turkish makam music, Ph.D. dissertation, Pompeu Fabra University, Spain, 2017. [45] A. Srinivasamurthy, G. K. Koduri, S. Gulati, V. Ishwar, and X. Serra, Corpora for music information research in indian art music, in Music Technology meets Philosophy - From Digital Echos to Virtual Ethos: Joint Proceedings of the 40th International Computer Music Conference, ICMC 2014, and the 11th Sound and Music Computing Conference, SMC 2014, Athens, Greece, September 14-20, 2014. Michigan Publishing, 2014. [46] C. Papaioannou, E. Benetos, and A. Potamianos, From west to east: Who can understand the music of the others better? in Proceedings of the 24th International Society for Music Information Retrieval Conference, ISMIR 2023, Milan, Italy, November 5-9, 2023, A. Sarti, F. Antonacci, M. Sandler, P. Bestagini, S. Dixon, B. Liang, G. Richard, and J. Pauwels, Eds., 2023, pp. 311318. [47] C. Papaioannou, E. Benetos, and A. Potamianos, LCProtonets: Multi-label few-shot learning for world music audio tagging, IEEE Open Journal of Signal Processing, vol. 6, 2025, pp. 138146. [48] A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, High fidelity neural audio compression, Trans. Mach. Learn. Res., vol. 2023, 2023. [49] W. Hsu, B. Bolte, Y. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, Hubert: Self-supervised speech representation learning by masked prediction of hidden units, IEEE ACM Trans. Audio Speech Lang. Process., vol. 29, 2021, pp. 34513460. [50] J. Kirkpatrick, R. Pascanu, N. C. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska et al., Overcoming catastrophic forgetting in neural networks, CoRR, vol. abs/1612.00796, 2016. [60] Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y. Li, and D. P. W. Ellis, Mulan: joint embedding of music audio and natural language, in Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022, Bengaluru, India, December 4-8, 2022, P. Rao, H. A. Murthy, A. Srinivasamurthy, R. M. Bittner, R. C. Repetto, M. Goto, X. Serra, and M. Miron, Eds., 2022, pp. 559566. [61] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. [62] D. Karomat, 12 maqam system and its similarity with indian ragas (according to the indian manuscripts), Indian Musicological Society. Journal of the Indian Musicological Society, vol. 36, 2006, p. 62. [63] V. Blaschke, M. Fedzechkina, and M. ter Hoeve, Analyzing the effect of linguistic similarity on crosslingual transfer: Tasks and experimental setups matter, CoRR, vol. abs/2501.14491, 2025. [64] E. Yang, Z. Wang, L. Shen, S. Liu, G. Guo, X. Wang, and D. Tao, Adamerging: Adaptive model merging for multi-task learning, in The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [65] M. Parovic, I. Vulic, and A. Korhonen, Investigating the potential of task arithmetic for cross-lingual transfer, in Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 - Volume 2: Short Papers, St. Julians, Malta, March 17-22, 2024, Y. Graham and M. Purver, Eds. Association for Computational Linguistics, 2024, pp. 124137. [51] S. Hu, Y. Tu, X. Han, C. He, G. Cui, X. Long, Z. Zheng, Y. Fang, Y. Huang, W. Zhao et al., Minicpm: Unveiling the potential of small language models with scalable training strategies, CoRR, vol. abs/2404.06395, 2024. [52] Y. Guo, J. Fu, H. Zhang, D. Zhao, and Y. Shen, Efficient continual pre-training by mitigating the stability gap, CoRR, vol. abs/2406.14833, 2024. [53] M. D. Lange, G. M. van de Ven, and T. Tuytelaars, Continual evaluation for lifelong learning: Identifying the stability gap, in The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [54] I. A. P. Santana, F. Pinhelli, J. Donini, L. G. Catharin, R. B. Mangolin, Y. M. Gomes da Costa, V. D. Feltrim, and M. A. Domingues, Music4all: new music database and its applications, in 2020 International Conference on Systems, Signals and Image Processing, IWSSIP 2020, Niterói, Brazil, July 1-3, 2020. IEEE, 2020, pp. 399404. [55] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, Continual lifelong learning with neural networks: review, Neural Networks, vol. 113, 2019, pp. 5471. [56] D. Kim and B. Han, On the stability-plasticity dilemma of class-incremental learning, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023. IEEE, 2023, pp. 20 19620 204. [57] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu, On layer normalization in the transformer architecture, in Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, ser. Proceedings of Machine Learning Research, vol. 119. PMLR, 2020, pp. 10 52410 533. [58] R. Castellon, C. Donahue, and P. Liang, Codified audio language modeling learns useful representations for music information retrieval, in Proceedings of the 22nd International Society for Music Information Retrieval Conference, ISMIR 2021, Online, November 7-12, 2021, J. H. Lee, A. Lerch, Z. Duan, J. Nam, P. Rao, P. van Kranenburg, and A. Srinivasamurthy, Eds., 2021, pp. 8896. [59] R. Yuan, Y. Ma, Y. Li, G. Zhang, X. Chen, H. Yin, L. Zhuo, Y. Liu, J. Huang, Z. Tian et al., MARBLE: music audio representation benchmark for universal evaluation, in Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, Eds., 2023."
        }
    ],
    "affiliations": [
        "Archimedes, Athena Research Center, Greece",
        "Centre for Digital Music, Queen Mary University of London, UK",
        "Institute for Language and Speech Processing, Athena Research Center, Greece",
        "School of ECE, National Technical University of Athens, Greece"
    ]
}