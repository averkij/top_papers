{
    "paper_title": "Qwen3 Technical Report",
    "authors": [
        "An Yang",
        "Anfeng Li",
        "Baosong Yang",
        "Beichen Zhang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chang Gao",
        "Chengen Huang",
        "Chenxu Lv",
        "Chujie Zheng",
        "Dayiheng Liu",
        "Fan Zhou",
        "Fei Huang",
        "Feng Hu",
        "Hao Ge",
        "Haoran Wei",
        "Huan Lin",
        "Jialong Tang",
        "Jian Yang",
        "Jianhong Tu",
        "Jianwei Zhang",
        "Jianxin Yang",
        "Jiaxi Yang",
        "Jing Zhou",
        "Jingren Zhou",
        "Junyang Lin",
        "Kai Dang",
        "Keqin Bao",
        "Kexin Yang",
        "Le Yu",
        "Lianghao Deng",
        "Mei Li",
        "Mingfeng Xue",
        "Mingze Li",
        "Pei Zhang",
        "Peng Wang",
        "Qin Zhu",
        "Rui Men",
        "Ruize Gao",
        "Shixuan Liu",
        "Shuang Luo",
        "Tianhao Li",
        "Tianyi Tang",
        "Wenbiao Yin",
        "Xingzhang Ren",
        "Xinyu Wang",
        "Xinyu Zhang",
        "Xuancheng Ren",
        "Yang Fan",
        "Yang Su",
        "Yichang Zhang",
        "Yinger Zhang",
        "Yu Wan",
        "Yuqiong Liu",
        "Zekun Wang",
        "Zeyu Cui",
        "Zhenru Zhang",
        "Zhipeng Zhou",
        "Zihan Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0."
        },
        {
            "title": "Start",
            "content": "2025-05-15 Qwen3 Technical Report"
        },
        {
            "title": "Qwen Team",
            "content": "https://huggingface.co/Qwen https://modelscope.cn/organization/qwen https://github.com/QwenLM/Qwen"
        },
        {
            "title": "Abstract",
            "content": "In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into unified framework. This eliminates the need to switch between different modelssuch as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ32B)and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0. 5 2 0 2 4 1 ] . [ 1 8 8 3 9 0 . 5 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "The pursuit of artificial general intelligence (AGI) or artificial super intelligence (ASI) has long been goal for humanity. Recent advancements in large foundation models, e.g., GPT-4o (OpenAI, 2024), Claude 3.7 (Anthropic, 2025), Gemini 2.5 (DeepMind, 2025), DeepSeek-V3 (Liu et al., 2024a), Llama-4 (Meta-AI, 2025), and Qwen2.5 (Yang et al., 2024b), have demonstrated significant progress toward this objective. These models are trained on vast datasets spanning trillions of tokens across diverse domains and tasks, effectively distilling human knowledge and capabilities into their parameters. Furthermore, recent developments in reasoning models, optimized through reinforcement learning, highlight the potential for foundation models to enhance inference-time scaling and achieve higher levels of intelligence, e.g., o3 (OpenAI, 2025), DeepSeek-R1 (Guo et al., 2025). While most state-of-the-art models remain proprietary, the rapid growth of open-source communities has substantially reduced the performance gap between open-weight and closed-source models. Notably, an increasing number of top-tier models (Meta-AI, 2025; Liu et al., 2024a; Guo et al., 2025; Yang et al., 2024b) are now being released as open-source, fostering broader research and innovation in artificial intelligence. In this work, we introduce Qwen3, the latest series in our foundation model family, Qwen. Qwen3 is collection of open-weight large language models (LLMs) that achieve state-of-the-art performance across wide variety of tasks and domains. We release both dense and Mixture-of-Experts (MoE) models, with the number of parameters ranging from 0.6 billion to 235 billion, to meet the needs of different downstream applications. Notably, the flagship model, Qwen3-235B-A22B, is an MoE model with total of 235 billion parameters and 22 billion activated ones per token. This design ensures both high performance and efficient inference. Qwen3 introduces several key advancements to enhance its functionality and usability. First, it integrates two distinct operating modes, thinking mode and non-thinking mode, into single model. This allows users to switch between these modes without alternating between different models, e.g., switching from Qwen2.5 to QwQ (Qwen Team, 2024). This flexibility ensures that developers and users can adapt the models behavior to suit specific tasks efficiently. Additionally, Qwen3 incorporates thinking budgets, providing users with fine-grained control over the level of reasoning effort applied by the model during task execution. This capability is crucial to the optimization of computational resources and performance, tailoring the models thinking behavior to meet varying complexity in real-world applications. Furthermore, Qwen3 has been pre-trained on 36 trillion tokens covering up to 119 languages and dialects, effectively enhancing its multilingual capabilities. This broadened language support amplifies its potential for deployment in global use cases and international applications. These advancements together establish Qwen3 as cutting-edge open-source large language model family, capable of effectively addressing complex tasks across various domains and languages. The pre-training process for Qwen3 utilizes large-scale dataset consisting of approximately 36 trillion tokens, curated to ensure linguistic and domain diversity. To efficiently expand the training data, we employ multi-modal approach: Qwen2.5-VL (Bai et al., 2025) is finetuned to extract text from extensive PDF documents. We also generate synthetic data using domain-specific models: Qwen2.5-Math (Yang et al., 2024c) for mathematical content and Qwen2.5-Coder (Hui et al., 2024) for code-related data. The pre-training process follows three-stage strategy. In the first stage, the model is trained on about 30 trillion tokens to build strong foundation of general knowledge. In the second stage, it is further trained on knowledge-intensive data to enhance reasoning abilities in areas like science, technology, engineering, and mathematics (STEM) and coding. Finally, in the third stage, the model is trained on long-context data to increase its maximum context length from 4,096 to 32,768 tokens. To better align foundation models with human preferences and downstream applications, we employ multi-stage post-training approach that empowers both thinking (reasoning) and non-thinking modes. In the first two stages, we focus on developing strong reasoning abilities through long chain-of-thought (CoT) cold-start finetuning and reinforcement learning focusing on mathematics and coding tasks. In the final two stages, we combine data with and without reasoning paths into unified dataset for further fine-tuning, enabling the model to handle both types of input effectively, and we then apply generaldomain reinforcement learning to improve performance across wide range of downstream tasks. For smaller models, we use strong-to-weak distillation, leveraging both off-policy and on-policy knowledge transfer from larger models to enhance their capabilities. Distillation from advanced teacher models significantly outperforms reinforcement learning in performance and training efficiency. We evaluate both pre-trained and post-trained versions of our models across comprehensive set of benchmarks spanning multiple tasks and domains. Experimental results show that our base pre-trained models achieve state-of-the-art performance. The post-trained models, whether in thinking or nonthinking mode, perform competitively against leading proprietary models and large mixture-of-experts (MoE) models such as o1, o3-mini, and DeepSeek-V3. Notably, our models excel in coding, mathematics, and agent-related tasks. For example, the flagship model Qwen3-235B-A22B achieves 85.7 on AIME 2 and 81.5 on AIME25 (AIME, 2025), 70.7 on LiveCodeBench v5 (Jain et al., 2024), 2,056 on CodeForces, and 70.8 on BFCL v3 (Yan et al., 2024). In addition, other models in the Qwen3 series also show strong performance relative to their size. Furthermore, we observe that increasing the thinking budget for thinking tokens leads to consistent improvement in the models performance across various tasks. In the following sections, we describe the design of the model architecture, provide details on its training procedures, present the experimental results of pre-trained and post-trained models, and finally, conclude this technical report by summarizing the key findings and outlining potential directions for future research."
        },
        {
            "title": "2 Architecture",
            "content": "The Qwen3 series includes 6 dense models, namely Qwen3-0.6B, Qwen3-1.7B, Qwen3-4B, Qwen3-8B, Qwen3-14B, and Qwen3-32B, and 2 MoE models, Qwen3-30B-A3B and Qwen3-235B-A22B. The flagship model, Qwen3-235B-A22B, has total of 235B parameters with 22B activated ones. Below, we elaborate on the architecture of the Qwen3 models. The architecture of the Qwen3 dense models is similar to Qwen2.5 (Yang et al., 2024b), including using Grouped Query Attention (GQA, Ainslie et al., 2023), SwiGLU (Dauphin et al., 2017), Rotary Positional Embeddings (RoPE, Su et al., 2024), and RMSNorm (Jiang et al., 2023) with pre-normalization. Besides, we remove QKV-bias used in Qwen2 (Yang et al., 2024a) and introduce QK-Norm (Dehghani et al., 2023) to the attention mechanism to ensure stable training for Qwen3. Key information on model architecture is provided in Table 1. The Qwen3 MoE models share the same fundamental architecture as the Qwen3 dense models. Key information on model architecture is provided in Table 2. We follow Qwen2.5-MoE (Yang et al., 2024b) and implement fine-grained expert segmentation (Dai et al., 2024). The Qwen3 MoE models have 128 total experts with 8 activated experts per token. Unlike Qwen2.5-MoE, the Qwen3-MoE design excludes shared experts. Furthermore, we adopt the global-batch load balancing loss (Qiu et al., 2025) to encourage expert specialization. These architectural and training innovations have yielded substantial improvements in model performance across downstream tasks. Qwen3 models utilize Qwens tokenizer (Bai et al., 2023), which implements byte-level byte-pair encoding (BBPE, Brown et al., 2020; Wang et al., 2020; Sennrich et al., 2016) with vocabulary size of 151,669. Table 1: Model architecture of Qwen3 dense models. Models Layers Heads (Q / KV) Tie Embedding Context Length Qwen3-0.6B Qwen3-1.7B Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B 28 28 36 36 40 64 16 / 8 16 / 8 32 / 8 32 / 8 40 / 8 64 / 8 Yes Yes Yes No No No 32K 32K 128K 128K 128K 128K Table 2: Model architecture of Qwen3 MoE models. Models Layers Heads (Q / KV) # Experts (Total / Activated) Context Length Qwen3-30B-A3B Qwen3-235B-A22B 48 94 32 / 4 64 / 4 128 / 8 128 / 8 128K 128K"
        },
        {
            "title": "3 Pre-training",
            "content": "In this section, we describe the construction of our pretraining data, the details of our pretraining approach, and present experimental results from evaluating the base models on standard benchmarks."
        },
        {
            "title": "3.1 Pre-training Data",
            "content": "Compared with Qwen2.5 (Yang et al., 2024b), we have significantly expanded the scale and diversity of our training data. Specifically, we collected twice as many pre-training tokenscovering three times more languages. All Qwen3 models are trained on large and diverse dataset consisting of 119 languages and dialects, with total of 36 trillion tokens. This dataset includes high-quality content in various 3 domains such as coding, STEM (Science, Technology, Engineering, and Mathematics), reasoning tasks, books, multilingual texts, and synthetic data. To further expand the pre-training data corpus, we first employ the Qwen2.5-VL model (Bai et al., 2025) to perform text recognition on large volume of PDF-like documents. The recognized text is then refined using the Qwen2.5 model (Yang et al., 2024b), which helps improve its quality. Through this two-step process, we are able to obtain an additional set of high-quality text tokens, amounting to trillions in total. Besides, we employ Qwen2.5 (Yang et al., 2024b), Qwen2.5-Math (Yang et al., 2024c), and Qwen2.5-Coder (Hui et al., 2024) models to synthesize trillions of text tokens in different formats, including textbooks, question-answering, instructions, and code snippets, covering dozens of domains. Finally, we further expand the pre-training corpus by incorporating additional multilingual data and introducing more languages. Compared to the pre-training data used in Qwen2.5, the number of supported languages has been significantly increased from 29 to 119, enhancing the models linguistic coverage and cross-lingual capabilities. We have developed multilingual data annotation system designed to enhance both the quality and diversity of training data. This system has been applied to our large-scale pre-training datasets, annotating over 30 trillion tokens across multiple dimensions such as educational value, fields, domains, and safety. These detailed annotations support more effective data filtering and combination. Unlike previous studies (Xie et al., 2023; Fan et al., 2023; Liu et al., 2024b) that optimize the data mixture at the data source or domain level, our method optimizes the data mixture at the instance-level through extensive ablation experiments on small proxy models with the fine-grained data labels."
        },
        {
            "title": "3.2 Pre-training Stage",
            "content": "The Qwen3 models are pre-trained through three-stage process: (1) General Stage (S1): At the first pre-training stage, all Qwen3 models are trained on over 30 trillion tokens using sequence length of 4,096 tokens. At this stage, the models have been fully pre-trained on language proficiency and general world knowledge, with training data covering 119 languages and dialects. (2) Reasoning Stage (S2): To further improve the reasoning ability, we optimize the pre-training corpus of this stage by increasing the proportion of STEM, coding, reasoning, and synthetic data. The models are further pre-trained with about 5T higher-quality tokens at sequence length of 4,096 tokens. We also accelerate the learning rate decay during this stage. (3) Long Context Stage: In the final pre-training stage, we collect high-quality long context corpora to extend the context length of Qwen3 models. All models are pre-trained on hundreds of billions of tokens with sequence length of 32,768 tokens. The long context corpus includes 75% of text between 16,384 to 32,768 tokens in length, and 25% of text between 4,096 to 16,384 in length. Following Qwen2.5 (Yang et al., 2024b), we increase the base frequency of RoPE from 10,000 to 1,000,000 using the ABF technique (Xiong et al., 2023). Meanwhile, we introduce YARN (Peng et al., 2023) and Dual Chunk Attention (DCA, An et al., 2024) to achieve four-fold increase in sequence length capacity during inference. Similar to Qwen2.5 (Yang et al., 2024b), we develop scaling laws for optimal hyper-parameters (e.g., learning rate scheduler, and batch size) predictions based on three pre-training stages mentioned above. Through extensive experiments, we systematically study the relationship between model architecture, training data, training stage, and optimal training hyper-parameters. Finally, we set the predicted optimal learning rate and batch size strategy for each dense or MoE model."
        },
        {
            "title": "3.3 Pre-training Evaluation",
            "content": "We conduct comprehensive evaluations of the base language models of the Qwen3 series. The evaluation of base models mainly focuses on their performance in general knowledge, reasoning, mathematics, scientific knowledge, coding, and multilingual capabilities. The evaluation datasets for pre-trained base models include 15 benchmarks: General Tasks: MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang et al., 2024) (5shot, CoT), MMLU-redux (Gema et al., 2024) (5-shot), BBH (Suzgun et al., 2023) (3-shot, CoT), SuperGPQA (Du et al., 2025)(5-shot, CoT). Math & STEM Tasks: GPQA (Rein et al., 2023) (5-shot, CoT), GSM8K (Cobbe et al., 2021) (4-shot, CoT), MATH (Hendrycks et al., 2021b) (4-shot, CoT). 4 Coding Tasks: EvalPlus (Liu et al., 2023a) (0-shot) (Average of HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), Humaneval+, MBPP+) (Liu et al., 2023a), MultiPL-E (Cassano et al., 2023) (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript), MBPP-3shot (Austin et al., 2021), CRUX-O of CRUXEval (1-shot) (Gu et al., 2024). Multilingual Tasks: MGSM (Shi et al., 2023) (8-shot, CoT), MMMLU (OpenAI, 2024) (5-shot), INCLUDE (Romanou et al., 2024) (5-shot). For the base model baselines, we compare the Qwen3 series base models with the Qwen2.5 base models (Yang et al., 2024b) and other leading open-source base models, including DeepSeek-V3 Base (Liu et al., 2024a), Gemma-3 (Team et al., 2025), Llama-3 (Dubey et al., 2024), and Llama-4 (Meta-AI, 2025) series base models, in terms of scale of parameters. All models are evaluated using the same evaluation pipeline and the widely-used evaluation settings to ensure fair comparison. Summary of Evaluation Results Based on the overall evaluation results, we highlight some key conclusions of Qwen3 base models. (1) Compared with the previously open-source SOTA dense and MoE base models (such as DeepSeekV3 Base, Llama-4-Maverick Base, and Qwen2.5-72B-Base), Qwen3-235B-A22B-Base outperforms these models in most tasks with significantly fewer total parameters or activated parameters. (2) For the Qwen3 MoE base models, our experimental results indicate that: (a) Using the same pre-training data, Qwen3 MoE base models can achieve similar performance to Qwen3 dense base models with only 1/5 activated parameters. (b) Due to the improvements of the Qwen3 MoE architecture, the scale-up of the training tokens, and more advanced training strategies, the Qwen3 MoE base models can outperform the Qwen2.5 MoE base models with less than 1/2 activated parameters and fewer total parameters. (c) Even with 1/10 of the activated parameters of the Qwen2.5 dense base model, the Qwen3 MoE base model can achieve comparable performance, which brings us significant advantages in inference and training costs. (3) The overall performance of the Qwen3 dense base models is comparable to the Qwen2.5 base models at higher parameter scales. For example, Qwen3-1.7B/4B/8B/14B/32B-Base achieve comparable performance to Qwen2.5-3B/7B/14B/32B/72B-Base, respectively. Especially in STEM, coding, and reasoning benchmarks, the performance of Qwen3 dense base models even surpasses Qwen2.5 base models at higher parameter scales. The detailed results are as follows. Qwen3-235B-A22B-Base We compare Qwen3-235B-A22B-Base to our previous similar-sized MoE Qwen2.5-Plus-Base (Yang et al., 2024b) and other leading open-source base models: Llama-4-Maverick (Meta-AI, 2025), Qwen2.5-72B-Base (Yang et al., 2024b), DeepSeek-V3 Base (Liu et al., 2024a). From the results in Table 3, the Qwen3-235B-A22B-Base model attains the highest performance scores across most of the evaluated benchmarks. We further compare Qwen3-235B-A22B-Base with other baselines separately for the detailed analysis. (1) Compared with the recently open-source model Llama-4-Maverick-Base, which has about twice the number of parameters, Qwen3-235B-A22B-Base still performs better on most benchmarks. (2) Compared with the previously state-of-the-art open-source model DeepSeek-V3-Base, Qwen3235B-A22B-Base outperforms DeepSeek-V3-Base on 14 out of 15 evaluation benchmarks with only about 1/3 the total number of parameters and 2/3 activated parameters, demonstrating the powerful and cost-effectiveness of our models. (3) Compared with our previous MoE Qwen2.5-Plus of similar size, Qwen3-235B-A22B-Base significantly outperforms it with fewer parameters and activated parameters, which shows the remarkable advantages of Qwen3 in pre-training data, training strategy, and model architecture. (4) Compared with our previous flagship open-source dense model Qwen2.5-72B-Base, Qwen3235B-A22B-Base surpasses the latter in all benchmarks and uses fewer than 1/3 of the activated parameters. Meanwhile, due to the advantage of the model architecture, the inference costs and training costs on each trillion tokens of Qwen3-235B-A22B-Base are much cheaper than those of Qwen2.5-72B-Base. Qwen3-32B-Base Qwen3-32B-Base is our largest dense model among the Qwen3 series. We compare it to the baselines of similar sizes, including Gemma-3-27B (Team et al., 2025) and Qwen2.5-32B (Yang et al., 2024b). In addition, we introduce two strong baselines: the recently open-source MoE model Llama4-Scout, which has three times the parameters of Qwen3-32B-Base but half the activated parameters; 5 Table 3: Comparison among Qwen3-235B-A22B-Base and other representative strong open-source baselines. The highest, the second-best scores are shown in bold and underlined, respectively. Qwen2.5-72B Qwen2.5-Plus Llama-4-Maverick DeepSeek-V3 Qwen3-235B-A22B Architecture # Total Params # Activated Params MMLU MMLU-Redux MMLU-Pro SuperGPQA BBH GPQA GSM8K MATH EvalPlus MultiPL-E MBPP CRUX-O MGSM MMMLU INCLUDE Base Dense 72B 72B 86.06 83.91 58.07 36.20 86.30 45.88 91.50 62.12 65.93 58.70 76.00 66.20 82.40 84.40 69.05 Base MoE 271B 37B 85.02 82.69 63.52 37.18 85.60 Base MoE 402B 17B General Tasks 85.16 84.05 63.91 40.85 83. Math & STEM Tasks 41.92 91.89 62.78 61.43 62.16 74.60 68.50 43.94 87.72 63.32 Coding Tasks 68.38 57.28 75.40 77. Multilingual Tasks 82.21 83.49 66.97 79.69 83.09 73.47 Base MoE 671B 37B 87.19 86.14 59.84 41.53 86. 41.92 87.57 62.62 63.75 62.26 74.20 76.60 82.68 85.88 75.17 Base MoE 235B 22B 87.81 87.40 68.18 44.06 88. 47.47 94.39 71.84 77.60 65.94 81.40 79.00 83.53 86.70 73.46 Table 4: Comparison among Qwen3-32B-Base and other strong open-source baselines. The highest and second-best scores are shown in bold and underlined, respectively. Qwen2.5-32B Qwen2.5-72B Gemma-3-27B Llama-4-Scout Qwen3-32B Base Base Base Base Base Architecture # Total Params # Activated Params Dense 32B 32B Dense 72B 72B MMLU MMLU-Redux MMLU-Pro SuperGPQA BBH GPQA GSM8K MATH EvalPlus MultiPL-E MBPP CRUX-O MGSM MMMLU INCLUDE 83.32 81.97 55.10 33.55 84.48 47.97 92.87 57. 66.25 58.30 73.60 67.80 78.12 82.40 64.35 Dense 27B 27B 78.69 76.53 52.88 29.87 79.95 General Tasks 86.06 83.91 58.07 36.20 86. Math & STEM Tasks 45.88 91.50 62.12 Coding Tasks 65.93 58.70 76.00 66.20 Multilingual Tasks 82.40 84.40 69. 26.26 81.20 51.78 55.78 45.03 68.40 60.00 73.74 77.62 68.94 6 MoE 109B 17B 78.27 71.09 56.13 26.51 82. 40.40 85.37 51.66 59.90 47.38 68.60 61.90 79.93 74.83 68.09 Dense 32B 32B 83.61 83.41 65.54 39.78 87.38 49.49 93.40 61. 72.05 67.06 78.20 72.50 83.06 83.83 67.87 Table 5: Comparison among Qwen3-14B-Base, Qwen3-30B-A3B-Base, and other strong open-source baselines. The highest and second-best scores are shown in bold and underlined, respectively. Gemma-3-12B Qwen2.5-14B Qwen2.5-32B Qwen2.5-Turbo Qwen3-14B Qwen3-30B-A3B Architecture # Total Params # Activated Params MMLU MMLU-Redux MMLU-Pro SuperGPQA BBH GPQA GSM8K MATH EvalPlus MultiPL-E MBPP CRUX-O MGSM MMMLU INCLUDE Base Dense 12B 12B 73.87 70.70 44.91 24.61 74. 31.31 78.01 44.43 52.65 43.03 60.60 52.00 64.35 72.50 63.34 Base Dense 14B 14B Base Dense 32B 32B General Tasks 79.66 76.64 51.16 30.68 78.18 32.83 90.22 55.64 60.70 54.79 69.00 61.10 74.68 78.34 60. 83.32 81.97 55.10 33.55 84.48 Math & STEM Tasks 47.97 92.87 57.70 Coding Tasks 66.25 58.30 73.60 67.80 Multilingual Tasks 78.12 82.40 64.35 Base MoE 42B 6B 79.50 77.11 55.60 31.19 76.10 41.41 88.32 55.60 61.23 53.24 67.60 60. 70.45 79.76 59.25 Base Dense 14B 14B 81.05 79.88 61.03 34.27 81.07 39.90 92.49 62.02 72.23 61.69 73.40 68. 79.20 79.69 64.55 Base MoE 30B 3B 81.38 81.17 61.49 35.72 81.54 43.94 91.81 59.04 71.45 66.53 74.40 67. 79.11 81.46 67.00 Table 6: Comparison among Qwen8B-Base and other strong open-source baselines. The highest and second-best scores are shown in bold and underlined, respectively. Llama-3-8B Qwen2.5-7B Qwen2.5-14B Qwen3-8B Architecture # Total Params # Activated Params MMLU MMLU-Redux MMLU-Pro SuperGPQA BBH GPQA GSM8K MATH EvalPlus MultiPL-E MBPP CRUX-O MGSM MMMLU IINCLUDE Base Dense 14B 14B 79.66 76.64 51.16 30.68 78.18 32.83 90.22 55. 60.70 54.79 69.00 61.10 74.68 78.34 60.26 Base Dense 8B 8B 66.60 61.59 35.36 20.54 57.70 Base Dense 7B 7B General Tasks 74.16 71.06 45.00 26.34 70.40 Math & STEM Tasks 25.80 55.30 20.50 44.13 31.45 48.40 36. 38.92 59.65 44.94 36.36 85.36 49.80 Coding Tasks 62.18 50.73 63.40 48.50 Multilingual Tasks 63.60 71.34 53. 7 Base Dense 8B 8B 76.89 76.17 56.73 31.64 78.40 44.44 89.84 60.80 67.65 58.75 69.80 62. 76.02 75.72 59.40 Table 7: Comparison among Qwen3-4B-Base and other strong open-source baselines. The highest and second-best scores are shown in bold and underlined, respectively. Gemma-3-4B Qwen2.5-3B Qwen2.5-7B Qwen3-4B Architecture # Total Params # Activated Params MMLU MMLU-Redux MMLU-Pro SuperGPQA BBH GPQA GSM8K MATH EvalPlus MultiPL-E MBPP CRUX-O MGSM MMMLU INCLUDE Base Dense 4B 4B 59.51 56.91 29.23 17.68 51.70 Base Dense 3B 3B General Tasks 65.62 63.68 34.61 20.31 56.30 Math & STEM Tasks 24.24 43.97 26.10 43.23 28.06 46.40 34. 26.26 79.08 42.64 Coding Tasks 46.28 39.65 54.60 36.50 Multilingual Tasks 33.11 59.62 49.06 47.53 65.55 45. Base Dense 7B 7B 74.16 71.06 45.00 26.34 70.40 36.36 85.36 49.80 62.18 50.73 63.40 48.50 63.60 71.34 53. Base Dense 4B 4B 72.99 72.79 50.58 28.43 72.59 36.87 87.79 54.10 63.53 53.13 67.00 55.00 67.74 71.42 56. Table 8: Comparison among Qwen3-1.7B-Base, Qwen3-0.6B-Base, and other strong open-source baselines. The highest and second-best scores are shown in bold and underlined, respectively. Qwen2.5-0.5B Qwen3-0.6B Gemma-3-1B Qwen2.5-1.5B Qwen3-1.7B Base Base Base Base Base Architecture # Total Params # Activated Params Dense 0.5B 0.5B Dense 0.6B 0.6B MMLU MMLU-Redux MMLU-Pro SuperGPQA BBH GPQA GSM8K MATH EvalPlus MultiPL-E MBPP CRUX-O MGSM MMMLU INCLUDE 47.50 45.10 15.69 11.30 20.30 24.75 41.62 19.48 31.85 18.70 29.80 12.10 12.07 31.53 24.74 Dense 1.5B 1.5B Dense 1.7B 1.7B 60.90 58.46 28.53 17.64 45.10 24.24 68.54 35.00 44.80 33.10 43.60 29.60 32.82 60.27 39.55 62.63 61.66 36.76 20.92 54. 28.28 75.44 43.50 52.70 42.71 55.40 36.40 50.71 63.27 45.57 Dense 1B 1B 26.26 25.99 9.72 7.19 28.13 General Tasks 52.81 51.26 24.74 15.03 41.47 Math & STEM Tasks 24.75 2.20 3.66 8.98 5.15 9.20 3.80 1.74 26.57 25.62 26.77 59.59 32. Coding Tasks 36.23 24.58 36.60 27.00 Multilingual Tasks 30.99 50.16 34.26 8 and our previous flagship open-source dense model Qwen2.5-72B-Base, which has more than twice the number of parameters compared to Qwen3-32B-Base. The results are shown in Table 4, which support three key conclusions: (1) Compared with the similar-sized models, Qwen3-32B-Base outperforms Qwen2.5-32B-Base and Gemma-3-27B Base on most benchmarks. Notably, Qwen3-32B-Base achieves 65.54 on MMLUPro and 39.78 on SuperGPQA, significantly outperforming its predecessor Qwen2.5-32B-Base. In addition, Qwen3-32B-Base achieves significantly higher encoding benchmark scores than all baseline models. (2) Surprisingly, we find that Qwen3-32B-Base achieves competitive results compared to Qwen2.572B-Base. Although Qwen3-32B-Base has less than half the number of parameters of Qwen2.572B-Base, it outperforms Qwen2.5-72B-Base in 10 of the 15 evaluation benchmarks. On coding, mathematics, and reasoning benchmarks, Qwen3-32B-Base has remarkable advantages. (3) Compared to Llama-4-Scout-Base, Qwen3-32B-Base significantly outperforms it on all 15 benchmarks, with only one-third of the number of parameters of Llama-4-Scout-Base, but twice the number of activated parameters. Qwen3-14B-Base & Qwen3-30B-A3B-Base The evaluation of the Qwen3-14B-Base and Qwen3-30BA3B-Base is compared against baselines of similar sizes, including Gemma-3-12B Base, Qwen2.5-14B Base. Similarly, we also introduce two strong baselines: (1) Qwen2.5-Turbo (Yang et al., 2024b), which has 42B parameters and 6B activated parameters. Note that its activated parameters are twice those of Qwen3-30B-A3B-Base. (2) Qwen2.5-32B-Base, which has 11 times the activated parameters of Qwen330B-A3B and more than twice that of Qwen3-14B. The results are shown in Table 5, where we can draw the following conclusions. (1) Compared with the similar-sized models, Qwen3-14B-Base significantly performs better than Qwen2.5-14B-Base and Gemma-3-12B-Base on all 15 benchmarks. (2) Similarly, Qwen3-14B-Base also achieves very competitive results compared to Qwen2.5-32B-Base with less than half of the parameters. (3) With only 1/5 activated non-embedding parameters, Qwen3-30B-A3B significantly outperforms Qwen2.5-14B-Base on all tasks, and achieves comparable performance to Qwen3-14B-Base and Qwen2.5-32B-Base, which brings us significant advantages in inference and training costs. Qwen3-8B / 4B / 1.7B / 0.6B-Base For edge-side models, we take similar-sized Qwen2.5, Llama-3, and Gemma-3 base models as the baselines. The results can be seen in Table 6, Table 7, and Table 8. All Qwen3 8B / 4B / 1.7B / 0.6B-Base models continue to maintain strong performance across nearly all benchmarks. Notably, Qwen3-8B / 4B / 1.7B-Base models even outperform larger size Qwen2.5-14B / 7B / 3B Base models on over half of the benchmarks, especially on STEM-related and coding benchmarks, reflecting the significant improvement of the Qwen3 models."
        },
        {
            "title": "4 Post-training",
            "content": "Figure 1: Post-training pipeline of the Qwen3 series models. 9 The post-training pipeline of Qwen3 is strategically designed with two core objectives: (1) Thinking Control: This involves the integration of two distinct modes, namely the non-thinking and thinking modes, providing users with the flexibility to choose whether the model should engage in reasoning or not, and to control the depth of thinking by specifying token budget for the thinking process. (2) Strong-to-Weak Distillation: This aims to streamline and optimize the post-training process for lightweight models. By leveraging the knowledge from large-scale models, we substantially reduce both the computational costs and the development efforts required for building smallerscale models. As illustrated in Figure 1, the flagship models in the Qwen3 series follow sophisticated four-stage training process. The first two stages focus on developing the models thinking abilities. The next two stages aim to integrate strong non-thinking functionalities into the models. Preliminary experiments suggest that directly distilling the output logits from teacher models into lightweight student models can effectively enhance their performance while maintaining fine-grained control over their reasoning processes. This approach eliminates the necessity of performing an exhaustive four-stage training process individually for every small-scale model. It leads to better immediate performance, as indicated by higher Pass@1 scores, and also improves the models ability of exploration, as reflected in improved Pass@64 results. In addition, it achieves these gains with much greater training efficiency, requiring only 1/10 of the GPU hours compared to the four-stage training method. In the following sections, we present the four-stage training process and provide detailed explanation of the Strong-to-Weak Distillation approach."
        },
        {
            "title": "4.1 Long-CoT Cold Start",
            "content": "We begin by curating comprehensive dataset that spans wide range of categories, including math, code, logical reasoning, and general STEM problems. Each problem in the dataset is paired with verified reference answers or code-based test cases. This dataset serves as the foundation for the cold start phase of long Chain-of-Thought (long-CoT) training. The dataset construction involves rigorous two-phase filtering process: query filtering and response filtering. In the query filtering phase, we use Qwen2.5-72B-Instruct to identify and remove queries that are not easily verifiable. This includes queries containing multiple sub-questions or those asking for general text generation. Furthermore, we exclude queries that Qwen2.5-72B-Instruct can answer correctly without using CoT reasoning. This helps prevent the model from relying on superficial guessing and ensures that only complex problems requiring deeper reasoning are included. Additionally, we annotate each querys domain using Qwen2.5-72B-Instruct to maintain balanced domain representation across the dataset. After reserving validation query set, we generate candidate responses for each remaining query using QwQ-32B (Qwen Team, 2025). When QwQ-32B consistently fails to generate correct solutions, human annotators manually assess the accuracy of the responses. For queries with positive Pass@N, further stringent filtering criteria are applied to remove responses that (1) yield incorrect final answers, (2) contain substantial repetition, (3) clearly indicate guesswork without adequate reasoning, (4) exhibit inconsistencies between the thinking and summary contents, (5) involve inappropriate language mixing or stylistic shifts, or (6) are suspected of being overly similar to potential validation set items. Subsequently, carefully selected subset of the refined dataset is used for the initial cold-start training of the reasoning patterns. The objective at this stage is to instill foundational reasoning patterns in the model without overly emphasizing immediate reasoning performance. This approach ensures that the models potential is not limited, allowing for greater flexibility and improvement during the subsequent reinforcement learning (RL) phase. To achieve this objective effectively, it is preferable to minimize both the number of training samples and the training steps during this preparatory phase."
        },
        {
            "title": "4.2 Reasoning RL",
            "content": "The query-verifier pairs used in the Reasoning RL stage must satisfy the following four criteria: (1) They were not used during the cold-start phase. (2) They are learnable for the cold-start model. (3) They are as challenging as possible. (4) They cover broad range of sub-domains. We ultimately collect total of 3,995 query-verifier pairs, and employed GRPO (Shao et al., 2024) to update the model parameters. We observe that using large batch size and high number of rollouts per query, along with off-policy training to improve sample efficiency, is beneficial to the training process. We have also addressed how to balance exploration and exploitation by controlling the models entropy to increase steadily or remain 10 Table 9: Examples of SFT data for thinking and non-thinking modes during the thinking mode fusion stage. For the thinking mode, the /think flag can be omitted since it represents the default behavior. This feature has been implemented in the chat template1 supported by the Hugging Faces tokenizer, where the thinking mode can be disabled using an additional parameter enable thinking=False. Thinking Mode Non-Thinking Mode <im start>user {query} /think<im end> <im start>assistant <think> {thinking content} </think> <im start>user {query} /no think<im end> <im start>assistant <think> </think> {response}<im end> {response}<im end> stable, which is crucial for maintaining stable training. As result, we achieve consistent improvements in both training reward and validation performance over the course of single RL run, without any manual intervention on hyperparameters. For instance, the AIME24 score of the Qwen3-235B-A22B model increases from 70.1 to 85.1 over total of 170 RL training steps."
        },
        {
            "title": "4.3 Thinking Mode Fusion",
            "content": "The goal of the Thinking Mode Fusion stage is to integrate the non-thinking capabilities into the previously developed thinking model. This approach allows developers to manage and control reasoning behaviors, while also reducing the cost and complexity of deploying separate models for thinking and non-thinking tasks. To achieve this, we conduct continual supervised fine-tuning (SFT) on the Reasoning RL model and design chat template to fuse the two modes. Moreover, we find that models capable of handling both modes proficiently perform consistently well under different thinking budgets. Construction of SFT data. The SFT dataset combines both the thinking and non-thinking data. To ensure that the performance of the Stage 2 model is not compromised by the additional SFT, the thinking data is generated via rejection sampling on Stage 1 queries using the Stage 2 model itself. The non-thinking data, on the other hand, is carefully curated to cover diverse range of tasks, including coding, mathematics, instruction-following, multilingual tasks, creative writing, question answering, and role-playing. Additionally, we employ automatically generated checklists for assessing the response quality of non-thinking data. To enhance the performance on tasks with low-resource languages, we particularly increase the proportion of translation tasks. Chat Template Design. To better integrate the two modes and enable users to dynamically switch the models thinking process, we design chat templates for Qwen3, as shown in Table 9. Specifically, for samples in thinking mode and non-thinking mode, we introduce /think and /no think flags in the user query or system message, respectively. This allows the model to follow the users input and select the appropriate thinking mode accordingly. For non-thinking mode samples, we retain an empty thinking block in the assistants response. This design ensures internal format consistency within the model and allows developers to prevent the model from engaging in thinking behavior by concatenating an empty think block in the chat template. By default, the model operates in thinking mode; therefore, we add some thinking mode training samples where the user queries do not include /think flags. For more complex multi-turn dialogs, we randomly insert multiple /think and /no think flags into users queries, with the model response adhering to the last flag encountered. Thinking Budget. An additional advantage of Thinking Mode Fusion is that, once the model learns to respond in both non-thinking and thinking modes, it naturally develops the ability to handle intermediate casesgenerating responses based on incomplete thinking. This capability lays the foundation for implementing budget control over the models thinking process. Specifically, when the length of the models thinking reaches user-defined threshold, we manually halt the thinking process and insert the stop-thinking instruction: Considering the limited time by the user, have to give the solution based on the thinking directly now.n</think>.nn. After this instruction is inserted, the model proceeds to generate final response based on its accumulated reasoning up to that point. It is worth noting that this ability is not explicitly trained but emerges naturally as result of applying Thinking Mode Fusion."
        },
        {
            "title": "4.4 General RL",
            "content": "The General RL stage aims to broadly enhance the models capabilities and stability across diverse scenarios. To facilitate this, we have established sophisticated reward system covering over 20 distinct tasks, each with customized scoring criteria. These tasks specifically target enhancements in the following core capabilities: Instruction Following: This capability ensures that models accurately interpret and follow user instructions, including requirements related to content, format, length, and the use of structured output, delivering responses that align with user expectations. Format Following: In addition to explicit instructions, we expect the model to adhere to specific formatting conventions. For instance, it should respond appropriately to the /think and / no think flags by switching between thinking and non-thinking modes, and consistently use designated tokens (e.g., <think> and </think>) to separate the thinking and response parts in the final output. Preference Alignment: For open-ended queries, preference alignment focuses on improving the models helpfulness, engagement, and style, ultimately delivering more natural and satisfying user experience. Agent Ability: This involves training the model to correctly invoke tools via designated interfaces. During the RL rollout, the model is allowed to perform complete multi-turn interaction cycles with real environment execution feedback, thereby improving its performance and stability in long-horizon decision-making tasks. Abilities for Specialized Scenarios: In more specialized scenarios, we design tasks tailored to the specific context. For example, in Retrieval-Augmented Generation (RAG) tasks, we incorporate reward signals to guide the model toward generating accurate and contextually appropriate responses, thereby minimizing the risk of hallucination. To provide feedback for the aforementioned tasks, we utilized three distinct types of rewards: (1) Rule-based Reward: The rule-based reward has been widely used in the reasoning RL stage, and is also useful for general tasks such as instruction following (Lambert et al., 2024) and format adherence. Well-designed rule-based rewards can assess the correctness of model outputs with high precision, preventing issues like reward hacking. (2) Model-based Reward with Reference Answer: In this approach, we provide reference answer for each query and prompt Qwen2.5-72B-Instruct to score the models response based on this reference. This method allows for more flexible handling of diverse tasks without requiring strict formatting, avoiding false negatives that can occur with purely rule-based rewards. (3) Model-based Reward without Reference Answer: Leveraging human preference data, we train reward model to assign scalar scores to model responses. This approach, which does not depend on reference answer, can handle broader range of queries while effectively enhancing the models engagement and helpfulness."
        },
        {
            "title": "4.5 Strong-to-Weak Distillation",
            "content": "The Strong-to-Weak Distillation pipeline is specifically designed to optimize lightweight models, encompassing 5 dense models (Qwen3-0.6B, 1.7B, 4B, 8B, and 14B) and one MoE model (Qwen3-30B-A3B). This approach enhances model performance while effectively imparting robust mode-switching capabilities. The distillation process is divided into two primary phases: (1) Off-policy Distillation: At this initial phase, we combine the outputs of teacher models generated with both /think and /no think modes for response distillation. This helps lightweight student models develop basic reasoning skills and the ability to switch between different modes of thinking, laying solid foundation for the next on-policy training phase. (2) On-policy Distillation: In this phase, the student model generates on-policy sequences for fine-tuning. Specifically, prompts are sampled, and the student model produces responses in either /think or /no think mode. The student model is then fine-tuned by aligning its logits with those of teacher model (Qwen3-32B or Qwen3-235B-A22B) to minimize the KL divergence."
        },
        {
            "title": "4.6 Post-training Evaluation",
            "content": "To comprehensively evaluate the quality of instruction-tuned models, we adopted automatic benchmarks to assess model performance under both thinking and non-thinking modes. These benchmarks are 12 Table 10: Multilingual benchmarks and the included languages. The languages are identified in IETF language tags."
        },
        {
            "title": "Benchmark",
            "content": "# Langs Languages Multi-IF INCLUDE MMMLU MT-AIME"
        },
        {
            "title": "PolyMath\nMLogiQA",
            "content": "8 44 14 55 18 10 en, es, fr, hi, it, pt, ru, zh ar, az, be, bg, bn, de, el, es, et, eu, fa, fi, fr, he, hi, hr, hu, hy, id, it, ja, ka, kk, ko, lt, mk, ml, ms, ne, nl, pl, pt, ru, sq, sr, ta, te, tl, tr, uk, ur, uz, vi, zh ar, bn, de, en, es, fr, hi, id, it, ja, ko, pt, sw, zh af, ar, bg, bn, ca, cs, cy, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id, it, ja, kn, ko, lt, lv, mk, ml, mr, ne, nl, no, pa, pl, pt, ro, ru, sk, sl, so, sq, sv, sw, ta, te, th, tl, tr, uk, ur, vi, zh-Hans, zh-Hant ar, bn, de, en, es, fr, id, it, ja, ko, ms, pt, ru, sw, te, th, vi, zh ar, en, es, fr, ja, ko, pt, th, vi, zh categorized into several dimensions: General Tasks: We utilize benchmarks including MMLU-Redux (Gema et al., 2024), GPQADiamond (Rein et al., 2023), C-Eval (Huang et al., 2023), and LiveBench (2024-11-25) (White et al., 2024). For GPQA-Diamond, we sample 10 times for each query and report the averaged accuracy. Alignment Tasks: To evaluate how well the model aligns with human preferences, we employ suite of specialized benchmarks. For instruction-following performance, we report the strictprompt accuracy of IFEval (Zhou et al., 2023). To assess alignment with human preferences on general topics, we utilize Arena-Hard (Li et al., 2024) and AlignBench v1.1 (Liu et al., 2023b). For writing tasks, we rely on Creative Writing V3 (Paech, 2024) and WritingBench (Wu et al., 2025) to evaluate the models proficiency and creativity. Math & Text Reasoning: For evaluating mathematical and logical reasoning skills, we employ high-level math benchmarks including MATH-500 (Lightman et al., 2023), AIME24 and AIME25 (AIME, 2025), and text reasoning tasks including ZebraLogic (Lin et al., 2025) and AutoLogi (Zhu et al., 2025). For AIME problems, each years questions include Part and Part II, totaling 30 questions. For each question, we sample 64 times and take the average accuracy as the final score. Agent & Coding: To test the models proficiency in coding and agent-based tasks, we use BFCL v3 (Yan et al., 2024), LiveCodeBench (v5, 2024.10-2025.02) (Jain et al., 2024), and Codeforces Ratings from CodeElo (Quan et al., 2025). For BFCL, all Qwen3 models are evaluated using the FC format, and yarn was used to deploy the models to context length of 64k for Multi-Turn evaluation. Some baselines are derived from the BFCL leaderboard, taking the higher scores between FC and Prompt formats. For models not reported on the leaderboard, the Prompt formats are evaluated. For LiveCodeBench, for the non-thinking mode, we use the officially recommended prompt, while for the thinking mode, we adjust the prompt template to allow the model to think more freely, by removing the restriction You will not return anything except for the program. To evaluate the performance gap between models and competitive programming experts, we use CodeForces to calculate Elo ratings. In our benchmark, each problem is solved by generating up to eight independent reasoning attempts. Multilingual Tasks: For multilingual capabilities, we evaluate four kinds of tasks: instruction following, knowledge, mathematics, and logical reasoning. Instruction following is assessed using Multi-IF (He et al., 2024), which focuses on 8 key languages. Knowledge assessment consisted of two types: regional knowledge evaluated through INCLUDE (Romanou et al., 2024), covering 44 languages, and general knowledge assessed with MMMLU (OpenAI, 2024) across 14 languages, excluding the unoptimized Yoruba language; for these two benchmarks, we sample only 10% of the original data to improve evaluation efficiency. The mathematics task employ MT-AIME2024 (Son et al., 2025), encompassing 55 languages, and PolyMath (Wang et al., 2025), which includes 18 languages. Logical reasoning is evaluated using MlogiQA, covering 10 languages, sourced from Zhang et al. (2024). For all Qwen3 models in the thinking mode, we utilize sampling temperature of 0.6, top-p value of 0.95, and top-k value of 20. Additionally, for Creative Writing v3 and WritingBench, we apply presence penalty of 1.5 to encourage the generation of more diverse content. For Qwen3 models in the non-thinking mode, we configure the sampling hyperparameters with temperature = 0.7, top-p = 0.8, top-k = 20, and presence penalty = 1.5. For both the thinking and non-thinking modes, we set the max output length to 32,768 tokens, except AIME24 and AIME25 where we extend this length to 38,912 tokens to provide sufficient thinking space. 13 Table 11: Comparison among Qwen3-235B-A22B (Thinking) and other reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. OpenAI-o1 DeepSeek-R1 Grok-3-Beta (Think) Gemini2.5-Pro Qwen3-235B-A22B Architecture # Activated Params # Total Params MMLU-Redux GPQA-Diamond C-Eval LiveBench 2024-11-25 IFEval strict prompt Arena-Hard AlignBench v1.1 Creative Writing v3 WritingBench MATH-500 AIME24 AIME25 ZebraLogic AutoLogi - - - 92.8 78.0 85.5 75.7 92.6 92.1 8.86 81.7 7.69 96.4 74.3 79.2 81.0 79.8 MoE 37B 671B 92.9 71.5 91.8 71.6 83.3 92.3 8.76 85.5 7.71 97.3 79.8 70.0 78.7 86.1 BFCL v3 LiveCodeBench v5 CodeForces (Rating / Percentile) 1891 / 96.7% 2029 / 98.1% 56.9 64.3 67.8 63. Multi-IF INCLUDE MMMLU 14 languages MT-AIME2024 PolyMath MLogiQA 48.8 84.6 88.4 67.4 38.9 75.5 67.7 82.7 86.4 73.5 47.1 73.8 General Tasks Alignment Tasks Math & Text Reasoning Agent & Coding Multilingual Tasks - - - - 80.2 - - - - - - - 83.9 77.3 - - - 70.6 - - - - - - - - - - 93.7 84.0 82.9 82.4 89.5 96.4 9.03 86.0 8.09 98.8 92.0 86.7 87.4 85. MoE 22B 235B 92.7 71.1 89.6 77.1 83.4 95.6 8.94 84.6 8.03 98.0 85.7 81.5 80.3 89.0 62.9 70.4 2001 / 97.9% 70.8 70.7 2056 / 98.2% 77.8 85.1 86.9 76.9 52.2 75.6 71.9 78.7 84.3 80.8 54.7 77.1 Table 12: Comparison among Qwen3-235B-A22B (Non-thinking) and other non-reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. GPT-4o -2024-11-20 DeepSeek-V3 Qwen2.5-72B -Instruct LLaMA-4 -Maverick Qwen3-235B-A22B General Tasks Alignment Tasks Math & Text Reasoning Agent & Coding Multilingual Tasks Architecture # Activated Params # Total Params MMLU-Redux GPQA-Diamond C-Eval LiveBench 2024-11-25 IFEval strict prompt Arena-Hard AlignBench v1.1 Creative Writing v3 WritingBench MATH-500 AIME24 AIME25 ZebraLogic AutoLogi - - - 87.0 46.0 75.5 52.2 86.5 85.3 8.42 81.1 7.11 77.2 11.1 7.6 27.4 65.9 MoE 37B 671B 89.1 59.1 86.5 60.5 86.1 85.5 8.64 74.0 6. 90.2 39.2 28.8 42.1 76.1 Dense 72B 72B 86.8 49.0 84.7 51.4 84.1 81.2 7.89 61.8 7.06 83.6 18.9 15.0 26.6 66.1 MoE 17B 402B 91.8 69.8 83.5 59.5 86.7 82.7 7.97 61.3 5.46 90.6 38.5 15.9 40.0 75.2 BFCL v3 LiveCodeBench v5 CodeForces (Rating / Percentile) 864 / 35.4% 1134 / 54.1% 859 / 35.0% 712 / 24.3% 57.6 33.1 63.4 30. 52.9 37.2 72.5 32.7 Multi-IF INCLUDE MMMLU 14 languages MT-AIME2024 PolyMath MLogiQA 65.6 78.8 80.3 9.2 13.7 57.4 65.3 69.6 76.9 12.7 16.9 59.3 75.5 80.9 82.5 27.0 26.1 59. 55.6 76.7 81.1 20.9 20.4 58.9 14 MoE 22B 235B 89.2 62.9 86.1 62.5 83.2 96.1 8.91 80.4 7.70 91.2 40.1 24.7 37.7 83. 68.0 35.3 1387 / 75.7% 70.2 75.6 79.8 32.4 27.0 67.6 Summary of Evaluation Results From the evaluation results, we summarize several key conclusions of the finalized Qwen3 models as follows: (1) Our flagship model, Qwen3-235B-A22B, demonstrates the state-of-the-art overall performance among open-source models in both the thinking and non-thinking modes, surpassing strong baselines such as DeepSeek-R1 and DeepSeek-V3. Qwen3-235B-A22B is also highly competitive to closed-source leading models, such as OpenAI-o1, Gemini2.5-Pro, and GPT-4o, showcasing its profound reasoning capabilities and comprehensive general abilities. (2) Our flagship dense model, Qwen3-32B, outperforms our previous strongest reasoning model, QwQ-32B, in most of the benchmarks, and performs comparably to the closed-source OpenAI-o3mini, indicating its compelling reasoning capabilities. Qwen3-32B is also remarkably performant in the non-thinking mode and surpasses our previous flagship non-reasoning dense model, Qwen2.5-72B-Instruct. (3) Our lightweight models, including Qwen3-30B-A3B, Qwen3-14B, and other smaller dense ones, possess consistently superior performance to the open-source models with close or larger amount of parameters, proving the success of our Strong-to-Weak Distillation approach. The detailed results are as follows. Qwen3-235B-A22B For our flagship model Qwen3-235B-A22B, we compare it with the leading reasoning and non-reasoning models. For the thinking mode, we take OpenAI-o1 (OpenAI, 2024), DeepSeek-R1 (Guo et al., 2025), Grok-3-Beta (Think) (xAI, 2025), and Gemini2.5-Pro (DeepMind, 2025) as the reasoning baselines. For the non-thinking mode, we take GPT-4o-2024-11-20 (OpenAI, 2024), DeepSeek-V3 (Liu et al., 2024a), Qwen2.5-72B-Instruct (Yang et al., 2024b), and LLaMA-4-Maverick (Meta-AI, 2025) as the non-reasoning baselines. We present the evaluation results in Table 11 and 12. (1) From Table 11, with only 60% activated and 35% total parameters, Qwen3-235B-A22B (Thinking) outperforms DeepSeek-R1 on 17/23 the benchmarks, particularly on the reasoning-demanded tasks (e.g., mathematics, agent, and coding), demonstrating the state-of-the-art reasoning capabilities of Qwen3-235B-A22B among open-source models. Moreover, Qwen3-235B-A22B (Thinking) is also highly competitive to the closed-source OpenAI-o1, Grok-3-Beta (Think), and Gemini2.5Pro, substantially narrowing the gap in the reasoning capabilities between open-source and close-source models. (2) From Table 12, Qwen3-235B-A22B (Non-thinking) exceeds the other leading open-source models, including DeepSeek-V3, LLaMA-4-Maverick, and our previous flagship model Qwen2.5-72BInstruct, and also surpasses the closed-source GPT-4o-2024-11-20 in 18/23 the benchmarks, indicating its inherent strong capabilities even when not enhanced with the deliberate thinking process. Qwen3-32B For our flagship dense model, Qwen3-32B, we take DeepSeek-R1-Distill-Llama-70B, OpenAIo3-mini (medium), and our previous strongest reasoning model, QwQ-32B (Qwen Team, 2025), as the baselines in the thinking mode. We also take GPT-4o-mini-2024-07-18, LLaMA-4-Scout, and our previous flagship model, Qwen2.5-72B-Instruct, as the baselines in the non-thinking mode. We present the evaluation results in Table 13 and 14. (1) From Table 13, Qwen3-32B (Thinking) outperforms QwQ-32B on 17/23 the benchmarks, making it the new state-of-the-art reasoning model at the sweet size of 32B. Moreover, Qwen3-32B (Thinking) also competes with the closed-source OpenAI-o3-mini (medium) with better alignment and multilingual performance. (2) From Table 14, Qwen3-32B (Non-thinking) exhibits superior performance to all the baselines on almost all the benchmarks. Particularly, Qwen3-32B (Non-thinking) performs on par with Qwen2.5-72B-Instruct on the general tasks with significant advantages on the alignment, multilingual, and reasoning-related tasks, again proving the fundamental improvements of Qwen3 over our previous Qwen2.5 series models. Qwen3-30B-A3B & Qwen3-14B For Qwen3-30B-A3B and Qwen3-14B, we compare them with DeepSeekR1-Distill-Qwen-32B and QwQ-32B in the thinking mode, and Phi-4 (Abdin et al., 2024), Gemma-3-27B-IT (Team et al., 2025), and Qwen2.5-32B-Instruct in the non-thinking mode, respectively. We present the evaluation results in Table 15 and 16. (1) From Table 15, Qwen3-30B-A3B and Qwen3-14B (Thinking) are both highly competitive to QwQ-32B, especially on the reasoning-related benchmarks. It is noteworthy that Qwen3-30BA3B achieves comparable performance to QwQ-32B with smaller model size and less than 15 Table 13: Comparison among Qwen3-32B (Thinking) and other reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. DeepSeek-R1 -Distill-Llama-70B QwQ-32B OpenAI-o3-mini (medium) Qwen3-32B Architecture # Activated Params # Total Params MMLU-Redux GPQA-Diamond C-Eval LiveBench 2024-11-25 IFEval strict prompt Arena-Hard AlignBench v1.1 Creative Writing v3 WritingBench General Tasks Alignment Tasks Math & Text Reasoning MATH-500 AIME24 AIME25 ZebraLogic AutoLogi Dense 70B 70B 89.3 65.2 71.8 54. 79.3 60.6 6.74 62.1 6.08 94.5 70.0 56.3 71.3 83.5 Dense 32B 32B 90.0 65.6 88.4 72.0 83.9 89.5 8.70 82.4 7.86 98.0 79.5 69.5 76.8 88. - - - 90.0 76.8 75.1 70.0 91.5 89.0 8.38 74.8 7.52 98.0 79.6 74.8 88.9 86.3 Dense 32B 32B 90.9 68.4 87.3 74. 85.0 93.8 8.72 81.0 7.90 97.2 81.4 72.9 88.8 87.3 Agent & Coding BFCL v3 LiveCodeBench v5 CodeForces (Rating / Percentile) 49.3 54.5 1633 / 91.4% 66.4 62.7 1982 / 97.7% 64.6 66.3 2036 / 98.1% 70.3 65.7 1977 / 97.7% Multilingual Tasks Multi-IF INCLUDE MMMLU 14 languages MT-AIME2024 PolyMath MLogiQA 57.6 62.1 69.6 29.3 29.4 60.3 68.3 69.7 80.9 68.0 45.9 75. 48.4 73.1 79.3 73.9 38.6 71.1 73.0 73.7 80.6 75.0 47.4 76.3 Table 14: Comparison among Qwen3-32B (Non-thinking) and other non-reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. Architecture # Activated Params # Total Params MMLU-Redux GPQA-Diamond C-Eval LiveBench 2024-11-25 IFEval strict prompt Arena-Hard AlignBench v1.1 Creative Writing v3 WritingBench General Tasks Alignment Tasks Math & Text Reasoning MATH-500 AIME24 AIME25 ZebraLogic AutoLogi Agent & Coding BFCL v3 LiveCodeBench v5 CodeForces (Rating / Percentile) Multilingual Tasks Multi-IF INCLUDE MMMLU 14 languages MT-AIME2024 PolyMath MLogiQA GPT-4o-mini -2024-07-18 LLaMA-4 -Scout Qwen2.5-72B -Instruct Qwen3-32B - - - 81.5 40.2 66.3 41.3 80.4 74.9 7.81 70.3 5.98 78.2 8.1 8.8 20.1 52.6 64.0 27.9 MoE 17B 109B 86.3 57.2 78.2 47.6 84.7 70.5 7.49 55.0 5.49 82.6 28.6 10.0 24.2 56.8 45.4 29.8 1113 / 52.6% 981 / 43.7% Dense 72B 72B Dense 32B 32B 86.8 49.0 84.7 51.4 84.1 81.2 7.89 61.8 7.06 83.6 18.9 15.0 26.6 66.1 85.7 54.6 83.3 59.8 83.2 92.8 8.58 78.3 7. 88.6 31.0 20.2 29.2 78.5 63.4 30.7 859 / 35.0% 63.0 31.3 1353 / 71.0% 64.2 74.1 77.5 19.1 20.9 53.9 65.3 69.6 76.9 12.7 16.9 59.3 70.7 70.9 76.5 24.1 22.5 62. 62.4 66.0 72.1 6.0 12.0 42.6 16 Table 15: Comparison among Qwen3-30B-A3B / Qwen3-14B (Thinking) and other reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. DeepSeek-R1 -Distill-Qwen-32B QwQ-32B Qwen3-14B Qwen3-30B-A3B Dense 32B 32B Dense 32B 32B Dense 14B 14B Architecture # Activated Params # Total Params MMLU-Redux GPQA-Diamond C-Eval LiveBench 2024-11-25 IFEval strict prompt Arena-Hard AlignBench v1.1 Creative Writing v3 WritingBench General Tasks Alignment Tasks Math & Text Reasoning MATH-500 AIME24 AIME25 ZebraLogic AutoLogi 88.2 62.1 82.2 45.6 72.5 60.8 7.25 55.0 6. 94.3 72.6 49.6 69.6 74.6 Agent & Coding BFCL v3 LiveCodeBench v5 CodeForces (Rating / Percentile) 53.5 54.5 1691 / 93.4% Multilingual Tasks Multi-IF INCLUDE MMMLU 14 languages MT-AIME2024 PolyMath MLogiQA 31.3 68.0 78.6 44.6 35.1 63.3 90.0 65.6 88.4 72.0 83.9 89.5 8.70 82.4 7.86 98.0 79.5 69.5 76.8 88.1 66.4 62.7 88.6 64.0 86.2 71. 85.4 91.7 8.56 80.3 7.80 96.8 79.3 70.4 88.5 89.2 70.4 63.5 1982 / 97.7% 1766 / 95.3% 68.3 69.7 80.9 68.0 45.9 75.5 74.8 71.7 77.9 73.3 45.8 71. MoE 3B 30B 89.5 65.8 86.6 74.3 86.5 91.0 8.70 79.1 7.70 98.0 80.4 70.9 89.5 88.7 69.1 62.6 1974 / 97.7% 72.2 71.9 78.4 73.9 46.1 70. Table 16: Comparison among Qwen3-30B-A3B / Qwen3-14B (Non-thinking) and other non-reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. General Tasks Alignment Tasks Math & Text Reasoning Agent & Coding Multilingual Tasks Architecture # Activated Params # Total Params MMLU-Redux GPQA-Diamond C-Eval LiveBench 2024-11-25 IFEval strict prompt Arena-Hard AlignBench v1.1 Creative Writing v3 WritingBench MATH-500 AIME24 AIME25 ZebraLogic AutoLogi Phi-4 Dense 14B 14B 85.3 56.1 66.9 41.6 62.1 75.4 7.61 51.2 5.73 80.8 22.9 17.3 32.3 66.2 Gemma-3 -27B-IT Qwen2.5-32B -Instruct Qwen3-14B Qwen3-30B-A3B Dense 27B 27B Dense 32B 32B Dense 14B 14B 82.6 42.4 66.6 49.2 80.6 86.8 7.80 82.0 7.22 90.0 32.6 24.0 24.6 64. 83.9 49.5 80.6 50.0 79.5 74.5 7.71 54.6 5.90 84.6 18.8 12.8 26.1 65.5 82.0 54.8 81.0 59.6 84.8 86.3 8.52 73.1 7.24 90.0 31.7 23.3 33.0 82. MoE 3B 30B 84.1 54.8 82.9 59.4 83.7 88.0 8.55 68.1 7.22 89.8 32.8 21.6 33.2 81.5 BFCL v3 LiveCodeBench v5 CodeForces (Rating / Percentile) 1280 / 65.3% 1063 / 49.3% 903 / 38.2% 1200 / 58.6% 1267 / 64.1% 61.5 29. 59.1 26.9 47.0 25.2 58.6 29.8 62.8 26.4 Multi-IF INCLUDE MMMLU 14 languages MT-AIME2024 PolyMath MLogiQA 49.5 65.3 74.7 13.1 17.4 53. 69.8 71.4 76.1 23.0 20.3 58.5 17 63.2 67.5 74.2 15.3 18.3 58.0 72.9 67.8 72.6 23.2 22.0 58.9 70.8 67.8 73.8 24.6 23.3 53.3 Table 17: Comparison among Qwen3-8B / Qwen3-4B (Thinking) and other reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. DeepSeek-R1 -Distill-Qwen-14B DeepSeek-R1 -Distill-Qwen-32B Qwen3-4B Qwen3-8B Dense 32B 32B Dense 4B 4B Dense 8B 8B Architecture # Activated Params # Total Params MMLU-Redux GPQA-Diamond C-Eval LiveBench 2024-11-25 IFEval strict prompt Arena-Hard AlignBench v1.1 Creative Writing v3 WritingBench General Tasks Alignment Tasks Math & Text Reasoning MATH-500 AIME24 AIME25 ZebraLogic AutoLogi Dense 14B 14B 84.1 59.1 78.1 52.3 72.6 48.0 7.43 54.2 6.03 93.9 69.7 44.5 59.1 78. Agent & Coding BFCL v3 LiveCodeBench v5 CodeForces (Rating / Percentile) 49.5 45.5 1574 / 89.1% 53.5 54.5 1691 / 93.4% Multilingual Tasks Multi-IF INCLUDE MMMLU 14 languages MT-AIME2024 PolyMath MLogiQA 29.8 59.7 73.8 33.7 28.6 53.6 31.3 68.0 78.6 44.6 35.1 63.3 88.2 62.1 82.2 45.6 72.5 60.8 7.25 55.0 6.13 94.3 72.6 49.6 69.6 74.6 83.7 55.9 77.5 63. 81.9 76.6 8.30 61.1 7.35 97.0 73.8 65.6 81.0 87.9 65.9 54.2 87.5 62.0 83.4 67.1 85.0 85.8 8.46 75.0 7.59 97.4 76.0 67.3 84.8 89. 68.1 57.5 1671 / 92.8% 1785 / 95.6% 66.3 61.8 69.8 60.7 40.0 65.9 71.2 67.8 74.4 65.4 42.7 69.0 Table 18: Comparison among Qwen3-8B / Qwen3-4B (Non-thinking) and other non-reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. Architecture # Activated Params # Total Params MMLU-Redux GPQA-Diamond C-Eval LiveBench 2024-11-25 IFEval strict prompt Arena-Hard AlignBench v1.1 Creative Writing v3 WritingBench General Tasks Alignment Tasks Math & Text Reasoning MATH-500 AIME24 AIME25 ZebraLogic AutoLogi Agent & Coding BFCL v3 LiveCodeBench v5 CodeForces (Rating / Percentile) Multilingual Tasks Multi-IF INCLUDE MMMLU 14 languages MT-AIME2024 PolyMath MLogiQA LLaMA-3.1-8B -Instruct Gemma-3 -12B-IT Qwen2.5-7B -Instruct Qwen2.5-14B -Instruct Qwen3-4B Qwen3-8B Dense 8B 8B Dense 12B 12B Dense 7B 7B Dense 14B 14B Dense 4B 4B Dense 8B 8B 61.7 32.8 52.0 26.0 75.0 30.1 6.01 52.8 4.57 54.8 6.3 2.7 12.8 30. 49.6 10.8 77.8 40.9 61.1 43.7 80.2 82.6 7.77 79.9 7.05 85.6 22.4 18.8 17.8 58.9 50.6 25.7 75.4 36.4 76.2 34. 71.2 52.0 7.27 49.8 5.82 77.6 9.1 12.1 12.0 42.9 55.8 14.4 80.0 45.5 78.0 42.2 81.0 68.3 7.67 55.8 5.93 83.4 15.2 13.6 19.7 57. 58.7 21.9 77.3 41.7 72.2 48.4 81.2 66.2 8.10 53.6 6.85 84.8 25.0 19.1 35.2 76.3 57.6 21.3 79.5 39.3 77.9 53. 83.0 79.6 8.38 64.5 7.15 87.4 29.1 20.9 26.7 76.5 60.2 22.8 473 / 14.9% 462 / 14.7% 191 / 0.0% 904 / 38.3% 842 / 33.7% 1110 / 52.4% 52.1 34.0 44.4 0.4 5.8 41.9 65.6 65.3 70.0 16.7 17.6 54. 18 47.7 53.6 61.4 5.5 11.9 49.5 55.5 63.5 70.3 8.5 15.0 51.3 61.3 53.8 61.7 13.9 16.6 49.9 69.2 62.5 66.9 16.6 18.8 51.4 Table 19: Comparison among Qwen3-1.7B / Qwen3-0.6B (Thinking) and other reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. Architecture # Activated Params # Total Params MMLU-Redux GPQA-Diamond C-Eval LiveBench 2024-11-25 IFEval strict prompt Arena-Hard AlignBench v1.1 Creative Writing v3 WritingBench General Tasks Alignment Tasks Math & Text Reasoning MATH-500 AIME24 AIME25 ZebraLogic AutoLogi Agent & Coding BFCL v3 LiveCodeBench v5 Multilingual Tasks Multi-IF INCLUDE MMMLU 14 languages MT-AIME2024 PolyMath MLogiQA DeepSeek-R1 -Distill-Qwen-1.5B DeepSeek-R1 -Distill-Llama-8B Qwen3-0.6B Qwen3-1.7B Dense 1.5B 1.5B Dense 8B 8B Dense 0.6B 0.6B Dense 1.7B 1.7B 45.4 33.8 27.1 24.9 39.9 4.5 5.00 16.4 4.03 83.9 28.9 22.8 4.9 19.1 14.0 13.2 13.3 21.9 27.3 12.4 14.5 29.0 66.4 49.0 50.4 40. 59.0 17.6 6.24 51.1 5.42 89.1 50.4 27.8 37.1 63.4 21.5 42.5 27.0 34.5 40.1 13.2 10.8 32.8 55.6 27.9 50.4 30.3 59.2 8.5 6.10 30.6 5. 77.6 10.7 15.1 30.3 61.6 46.4 12.3 36.1 35.9 43.1 7.8 11.4 40.9 73.9 40.1 68.1 51.1 72.5 43.1 7.60 48.0 7.02 93.4 48.3 36.8 63.2 83. 56.6 33.2 51.2 51.8 59.1 36.1 25.2 56.0 Table 20: Comparison among Qwen3-1.7B / Qwen3-0.6B (Non-thinking) and other non-reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. Gemma-3 -1B-IT Phi-4-mini Qwen2.5-1.5B -Instruct Qwen2.5-3B -Instruct Qwen3-0.6B Qwen3-1.7B Architecture # Activated Params # Total Params Dense 1.0B 1.0B Dense 3.8B 3.8B Dense 1.5B 1.5B Dense 3.1B 3.1B Dense 0.6B 0.6B Dense 1.7B 1.7B General Tasks Alignment Tasks MMLU-Redux GPQA-Diamond C-Eval LiveBench 2024-11IFEval strict prompt Arena-Hard AlignBench v1.1 Creative Writing v3 WritingBench Math & Text Reasoning MATH-500 AIME24 AIME25 ZebraLogic AutoLogi Agent & Coding BFCL v3 LiveCodeBench v5 Multilingual Tasks Multi-IF INCLUDE MMMLU 14 languages MT-AIME2024 PolyMath MLogiQA 33.3 19.2 28.5 14.4 54.5 17.8 5.3 52.8 5.18 46.4 0.9 0.8 1.9 16.4 16.3 1.8 32.8 32.7 32.5 0.2 3.5 31. 50.7 29.8 53.3 18.0 42.5 9.0 5.60 31.5 4.67 55.0 0.9 0.4 3.4 22.5 47.8 5.3 20.2 33.1 40.4 0.7 5.0 40.9 67.9 25.2 40.0 25. 68.6 32.8 6.00 10.3 4.05 67.6 8.1 5.3 2.7 28.8 31.3 10.4 40.5 43.8 51.4 0.9 6.7 39.5 19 64.4 30.3 68.2 23. 58.2 23.7 6.49 42.8 5.55 67.2 6.7 4.2 4.8 29.9 50.4 9.2 32.3 43.8 51.8 1.6 7.3 39.5 44.6 22.9 42.6 21.8 54.5 6.5 5.60 28.4 5. 55.2 3.4 2.6 4.2 37.4 44.1 3.6 33.3 34.4 37.1 1.5 4.6 37.3 64.4 28.6 61.0 35.6 68.2 36.9 7.20 43.6 6.54 73.0 13.4 9.8 12.8 59. 52.2 11.6 44.7 42.6 48.3 4.9 10.3 41.1 1/10 activated parameters, demonstrating the effectiveness of our Strong-to-Weak Distillation approach in endowing lightweight models with profound reasoning capabilities. (2) From Table 16, Qwen3-30B-A3B and Qwen3-14B (Non-thinking) surpass the non-reasoning baselines in most of the benchmarks. They exceed our previous Qwen2.5-32B-Instruct model with significantly fewer activated and total parameters, allowing for more efficient and costeffective performance. Qwen3-8B / 4B / 1.7B / 0.6B For Qwen3-8B and Qwen3-4B, we compare them with DeepSeek-R1-DistillQwen-14B and DeepSeek-R1-Distill-Qwen-32B in the thinking mode, and LLaMA-3.1-8B-Instruct (Dubey et al., 2024), Gemma-3-12B-IT (Team et al., 2025), Qwen2.5-7B-Instruct, and Qwen2.5-14B-Instruct in the non-thinking mode, respectively. For Qwen3-1.7B and Qwen3-0.6B, we compare them with DeepSeekR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Llama-8B in the thinking mode, and Gemma-3-1B-IT, Phi-4-mini, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct in the non-thinking mode, respectively. We present the evaluation results of Qwen3-8B and Qwen3-4B in Table 17 and 18 and those of Qwen3-1.7B and Qwen3-0.6B in Table 19 and 20, respectively. Overall, these edge-side models exhibit impressive performance and outperform baselines even with more parameters, including our previous Qwen2.5 models, in either the thinking or the non-thinking mode. These results, once again, demonstrate the efficacy of our Strong-to-Weak Distillation approach, making it possible for us to build the lightweight Qwen3 models with remarkably reduced costs and efforts."
        },
        {
            "title": "4.7 Discussion",
            "content": "The Effectiveness of Thinking Budget To verify that Qwen3 can enhance its intelligence level by leveraging an increased thinking budget, we adjust the allocated thinking budget on four benchmarks across Mathematics, Coding, and STEM domains. The resulting scaling curves are presented in Figure 2, Qwen3 demonstrates scalable and smooth performance improvements correlated to the allocated thinking budget. Moreover, we observe that if we further extend the output length beyond 32K, the models performance is expected to improve further in the future. We leave this exploration as future work. Figure 2: Performance of Qwen3-235B-A22B with respect to the thinking budget. The Effectiveness and Efficiency of On-Policy Distillation We evaluate the effectiveness and efficiency of on-policy distillation by comparing the performance and computational costmeasured in GPU hoursafter undergoing distillation versus direct reinforcement learning, both starting from the same off-policy distilled 8B checkpoint. For simplicity, we focus solely on math and code-related queries in 20 this comparison. The results, summarized in Table 21, show that distillation achieves significantly better performance than reinforcement learning while requiring approximately only 1/10 of the GPU hours. Furthermore, distillation from teacher logits enables the student model to expand its exploration space and enhance its reasoning potential, as evidenced by the improved pass@64 scores on the AIME24 and AIME25 benchmarks after distillation, compared to the initial checkpoint. In contrast, reinforcement learning does not lead to any improvement in pass@64 scores. These observations highlight the advantages of leveraging stronger teacher model in guiding student model learning. Table 21: Comparison of reinforcement learning and on-policy distillation on Qwen3-8B. Numbers in parentheses indicate pass@64 scores. Method AIME24 AIME25 MATH500 LiveCodeBench v5 MMLU -Redux GPQA -Diamond GPU Hours Off-policy Distillation 55.0 (90.0) 42.8 (83.3) + Reinforcement Learning 67.6 (90.0) 55.5 (83.3) 74.4 (93.3) 65.5 (86.7) + On-policy Distillation 92.4 94.8 97.0 42.0 52.9 60.3 86.4 86.9 88.3 55.6 61.3 63.3 - 17,920 1, The Effects of Thinking Mode Fusion and General RL To evaluate the effectiveness of Thinking Mode Fusion and General Reinforcement Learning (RL) during the post-training, we conduct evaluations on various stages of the Qwen-32B model. In addition to the datasets mentioned earlier, we introduce several in-house benchmarks to monitor other capabilities. These benchmarks include: CounterFactQA: Contains counterfactual questions where the model needs to identify that the questions are not factual and avoid generating hallucinatory answers. LengthCtrl: Includes creative writing tasks with length requirements; the final score is based on the difference between the generated content length and the target length. ThinkFollow: Involves multi-turn dialogues with randomly inserted /think and /no think flags to test whether the model can correctly switch thinking modes based on user queries. ToolUse: Evaluates the stability of the model in single-turn, multi-turn, and multi-step tool calling processes. The score includes accuracy in intent recognition, format accuracy, and parameter accuracy during the tool calling process. Table 22: Performance of Qwen3-32B after Reasoning RL (Stage 2), Thinking Mode Fusion (Stage 3), and General RL (Stage 4). Benchmarks with * are in-house datasets. General Tasks Instruction & Format Following Benchmark LiveBench 2024-11-25 Arena-Hard CounterFactQA* IFEval strict prompt Multi-IF LengthCtrl* ThinkFollow* Agent BFCL v3 ToolUse* Knowledge & STEM MMLU-Redux GPQA-Diamond Math & Coding AIME24 LiveCodeBench Stage 2 Reasoning RL Stage 3 Thinking Mode Fusion Stage 4 General RL Thinking Thinking Non-Thinking Thinking Non-Thinking 68.6 86.8 50.4 73.0 61.4 62.6 - 69.0 63.3 91.4 68. 83.8 68.4 70.9+2.3 89.4+2.6 61.3+10.9 78.4+5.4 64.6+3.2 70.6+8.0 68.4-0.6 70.4+7.1 91.0-0.4 69.0+0.2 81.9-1.9 67.2-1. 88.7 57.1 88.5 64.3 78.4 65.2 84.9 61.5 73.2 86.7 50.4 28.5 31. 74.9+4.0 93.8+4.4 68.1+6.8 85.0+6.6 73.0+8.4 73.5+2.9 59.8+2.8 92.8+4.3 66.4+2.1 83.2+4.8 70.7+5.5 87.3+2.4 98.9+10.2 70.3+1.9 85.5+15. 63.0+1.5 86.5+13.3 90.9-0.1 68.4-0.6 81.4-0.5 65.7-1.5 85.7-1.0 54.6+4.3 31.0+2.5 31.3+0.2 The results are shown in Table 22, where we can draw the following conclusions: (1) Stage 3 integrates the non-thinking mode into the model, which already possesses thinking capabilities after the first two stages of training. The ThinkFollow benchmark score of 88.7 indicates that the model has developed an initial ability to switch between modes, though it still occasionally makes errors. Stage 3 also enhances the models general and instruction-following capabilities in thinking mode, with CounterFactQA improving by 10.9 points and LengthCtrl by 8.0 points. 21 (2) Stage 4 further strengthens the models general, instruction-following, and agent capabilities in both thinking and non-thinking modes. Notably, the ThinkFollow score improves to 98.9, ensuring accurate mode switching. (3) For Knowledge, STEM, Math, and Coding tasks, Thinking Mode Fusion and General RL do not bring significant improvements. In contrast, for challenging tasks like AIME24 and LiveCodeBench, the performance in thinking mode actually decreases after these two training stages. We conjecture this degradation is due to the model being trained on broader range of general tasks, which may compromise its specialized capabilities in handling complex problems. During the development of Qwen3, we choose to accept this performance trade-off to enhance the models overall versatility."
        },
        {
            "title": "5 Conclusion",
            "content": "In this technical report, we introduce Qwen3, the latest version of the Qwen series. Qwen3 features both thinking mode and non-thinking mode, allowing users to dynamically manage the number of tokens used for complex thinking tasks. The model was pre-trained on an extensive dataset containing 36 trillion tokens, enabling it to understand and generate text in 119 languages and dialects. Through series of comprehensive evaluations, Qwen3 has shown strong performance across range of standard benchmarks for both pre-trained and post-trained models, including tasks related to code generation, mathematics, reasoning, and agents. In the near future, our research will focus on several key areas. We will continue to scale up pretraining by using data that is both higher in quality and more diverse in content. At the same time, we will work on improving model architecture and training methods for the purposes of effective compression, scaling to extremely long contexts, etc. In addition, we plan to increase computational resources for reinforcement learning, with particular emphasis on agent-based RL systems that learn from environmental feedback. This will allow us to build agents capable of tackling complex tasks that require inference time scaling."
        },
        {
            "title": "6 Authors",
            "content": "Core Contributors: An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu Contributors: Bei Chen, Biao Sun, Bin Luo, Bin Zhang, Binghai Wang, Bowen Ping, Boyi Deng, Chang Si, Chaojie Yang, Chen Cheng, Chenfei Wu, Chengpeng Li, Chengyuan Li, Fan Hong, Guobin Zhao, Hang Zhang, Hangrui Hu, Hanyu Zhao, Hao Lin, Hao Xiang, Haoyan Huang, Hongkun Hao, Humen Zhong, Jialin Wang, Jiandong Jiang, Jianqiang Wan, Jianyuan Zeng, Jiawei Chen, Jie Zhang, Jin Xu, Jinkai Wang, Jinyang Zhang, Jinzheng He, Jun Tang, Kai Zhang, Ke Yi, Keming Lu, Keqin Chen, Langshi Chen, Le Jiang, Lei Zhang, Linjuan Wu, Man Yuan, Mingkun Yang, Minmin Sun, Mouxiang Chen, Na Ni, Nuo Chen, Peng Liu, Peng Wang, Peng Zhu, Pengcheng Zhang, Pengfei Wang, Qiaoyu Tang, Qing Fu, Qiuyue Wang, Rong Zhang, Rui Hu, Runji Lin, Shen Huang, Shuai Bai, Shutong Jiang, Sibo Song, Siqi Zhang, Song Chen, Tao He, Ting He, Tingfeng Hui, Wei Ding, Wei Liao, Wei Lin, Wei Zhang, Weijia Xu, Wenbin Ge, Wenmeng Zhou, Wenyuan Yu, Xianyan Jia, Xianzhong Shi, Xiaodong Deng, Xiaoming Huang, Xiaoyuan Li, Ximing Zhou, Xinyao Niu, Xipin Wei, Xuejing Liu, Yang Liu, Yang Yao, Yang Zhang, Yanpeng Li, Yantao Liu, Yidan Zhang, Yikai Zhu, Yiming Wang, Yiwen Hu, Yong Jiang, Yong Li, Yongan Yue, Yu Guan, Yuanzhi Zhu, Yunfei Chu, Yunlong Feng, Yuxin Zhou, Yuxuan Cai, Zeyao Ma, Zhaohai Li, Zheng Li, Zhengyang Tang, Zheren Fu, Zhi Li, Zhibo Yang, Zhifang Guo, Zhipeng Zhang, Zhiying Xu, Zhiyu Yin, Zhongshen Zeng, Zile Qiao, Ziye Meng, Zongmeng Zhang"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Additional Evaluation Results A.1.1 Long-Context Ability Table 23: Performance of Qwen3 Models on the RULER benchmark. Model Avg. 4K Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B Qwen3-30B-A3B Qwen3-235B-A22B Qwen3-4B Qwen3-8B Qwen3-14B Qwen3-32B Qwen3-30B-A3B Qwen3-235B-A22B 85.4 91.4 92.9 95.1 85.2 89.1 94.6 93.7 91.6 95.0 83.5 84.4 90.1 91.0 86.6 92. 96.7 97.7 96.9 97.7 95.1 96.3 98.0 98.4 96.5 97.7 92.7 94.7 95.4 94.7 94.1 95.1 RULER 8K 95.1 96.8 97.1 97. 93.6 96.0 97.8 96.0 97.0 97.2 88.7 94.4 93.6 93.7 92.7 94.8 16K 32K 64K 128K 93.7 95.9 95.5 97.7 91.0 91.8 96.4 96.2 95.3 96.4 86.5 86.1 89.8 91.6 89.0 93. 89.4 93.4 95.5 96.5 87.8 91.2 96.1 94.4 92.4 95.1 83.2 80.8 91.9 92.5 86.6 92.3 82.3 86.7 90.3 93.0 77.8 82.1 94.0 91.8 89.1 93.3 83.0 78.3 90.6 90.0 82.1 92. 55.1 78.1 82.0 88.4 66.0 77.4 85.1 85.6 79.2 90.6 67.2 72.0 79.0 83.5 75.0 86.0 Non-thinking Mode Thinking Mode For evaluating long-context processing capabilities, we report the results on the RULER benchmark (Hsieh et al., 2024) in Table 23. To enable length extrapolation, we utilize YARN (Peng et al., 2023) with scaling factor=4. In thinking mode, we set the thinking budget to 8192 tokens to mitigate overly verbose reasoning on the extremely long inputs. The results show that: 1. In non-thinking mode, Qwen3 outperforms Qwen2.5 models of similar size in long-context processing tasks. 2. In thinking mode, the models performance slightly degrades. We hypothesize that the thinking content does not provide significant benefits for these retrieval tasks, which do not rely on reasoning and may instead interfere with the retrieval process. We are committed to enhancing the long-context capability in the thinking mode in future versions. A.1.2 Multilingual Ability Table 24-35 presents the detailed benchmark scores across various languages, including Spanish, French, Portuguese, Italian, Arabic, Japanese, Korean, Indonesian, Russian, Vietnamese, German, and Thai. The results of these tables demonstrate that the Qwen3 series models achieve competitive performance across all evaluated benchmarks, showcasing their strong multilingual capabilities. To evaluate the performance of Qwen3 across broader range of languages, we utilize Belebele (Bandarkar et al., 2023), benchmark for natural language understanding. We conduct evaluations on 80 supported languages from the benchmark, excluding 42 unoptimized languages, as shown in Table 36 (organized by language family). The performance comparison between Qwen3 and other baseline models on the Belebele benchmark is presented in Table 37. The results show that Qwen3 achieves comparable performance to similarly-sized Gemma models while outperforming Qwen2.5 significantly. 23 Table 24: Benchmark scores for language: Spanish (es). The highest and second-best scores are shown in bold and underlined, respectively. Model Multi-IF MLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 80.1 70.0 74.2 74.7 74.9 76.2 74.1 69.1 56.0 39.2 67.5 73.5 66.7 71.7 72.1 72.1 76.2 73.1 65.8 47.9 35.5 70.0 75.0 76.2 68.8 71.2 67.5 70.0 68.8 55.0 42. 52.5 57.5 61.3 66.2 65.0 53.8 63.7 50.0 50.0 43.8 37.5 96.4 81.8 89.1 90.9 80.0 83.6 78.2 72.7 72.7 54.5 89.1 89.1 80.0 83.6 83.6 85.5 78.2 80.0 60.0 50.9 43.6 88.7 84.5 86.7 82.8 81.9 81.1 79.2 75.7 64.5 48.8 80.6 77.7 80.1 83.7 80.4 78.3 77.4 73.7 68.3 54.3 39.5 90.0 76.7 86.7 76.7 76.7 73.3 70.0 66.7 46.7 13. 10.0 30.0 20.0 33.3 26.7 33.3 40.0 16.7 13.3 10.0 3.3 54.4 52.2 57.3 51.8 48.5 50.3 43.7 42.3 30.2 14.3 15.5 22.4 18.8 29.5 24.7 25.0 25.0 21.3 17.3 11.6 5.8 79.9 73.4 78.4 74.3 72.2 72.0 69.2 65.9 54.2 35.4 52.5 58.4 54.5 61.3 58.8 58.0 60.1 52.5 45.8 36.4 27.5 Table 25: Benchmark scores for language: French (fr). The highest and second-best scores are shown in bold and underlined, respectively. Model Multi-IF MLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 80.5 72.4 77.3 76.7 75.2 77.6 73.8 71.3 52.6 36.1 67.8 73.9 72.1 73.2 75.8 75.6 78.4 71.9 64.2 46.1 32.8 73.8 78.8 78.8 81.2 67.5 71.2 66.2 63.7 56.2 48.8 56.2 57.5 55.0 65.0 60.0 52.5 63.7 52.5 47.5 43.8 35.0 85.7 76.2 85.7 76.2 83.3 73.8 85.7 71.4 54.8 47.6 85.7 73.8 81.0 88.1 73.8 69.0 73.8 71.4 61.9 64.3 38. 88.3 84.0 86.6 82.1 81.0 80.4 77.9 74.5 64.8 48.4 81.8 78.3 80.2 81.1 79.5 77.9 75.1 71.7 67.6 53.2 39.4 80.0 80.0 86.7 83.3 76.7 73.3 70.0 66.7 60.0 6.7 10.0 23.3 26.7 36.7 30.0 26.7 33.3 20.0 20.0 3.3 6.7 52.8 49.4 57.4 47.1 46.9 44.2 45.3 40.2 28.7 14.0 15.3 21.5 15.7 28.1 23.0 27.3 24.4 21.4 19.2 11.6 4. 76.8 73.5 78.8 74.4 71.8 70.1 69.8 64.6 52.8 33.6 52.8 54.7 55.1 62.0 57.0 54.8 58.1 51.5 46.7 37.0 26.1 24 Table 26: Benchmark scores for language: Portuguese (pt). The highest and second-best scores are shown in bold and underlined, respectively. Model Multi-IF MLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 80.5 70.5 73.6 74.1 76.1 77.3 73.9 70.6 55.6 38.7 66.8 72.9 68.8 72.5 71.1 72.3 75.5 71.9 66.1 49.5 36. 73.8 70.0 78.8 76.2 71.2 68.8 67.5 62.5 60.0 33.8 57.5 55.0 55.0 67.5 61.3 47.5 58.8 56.2 50.0 33.8 37.5 83.9 80.4 78.6 76.8 71.4 75.0 75.0 71.4 53.6 42.9 78.6 75.0 71.4 82.1 73.2 67.9 75.0 71.4 73.2 39.3 42.9 88.9 84.0 86.2 82.6 81.0 81.6 78.6 75.1 64.6 47.5 80.7 77.1 82.2 83.5 80.6 77.8 76.5 72.9 66.7 52.9 37. 73.3 80.0 86.7 80.0 76.7 83.3 56.7 73.3 46.7 10.0 10.0 33.3 23.3 33.3 30.0 26.7 26.7 20.0 10.0 6.7 3.3 52.2 48.7 58.3 52.4 49.3 46.7 44.8 44.2 28.2 12.7 15.0 20.9 11.3 28.3 23.9 24.0 25.8 19.7 18.1 12.8 5.7 75.4 72.3 77.0 73.7 71.0 72.1 66.1 66.2 51.4 30.9 51.4 55.7 52.0 61.2 56.7 52.7 56.4 52.0 47.4 32.5 27. Table 27: Benchmark scores for language: Italian (it). The highest and second-best scores are shown in bold and underlined, respectively. Model Multi-IF INCLUDE MMMLU MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 80.9 71.2 73.7 76.6 75.9 79.0 74.6 69.8 54.6 37.8 67.6 74.6 67.2 72.9 71.4 73.9 75.8 72.1 63.0 46.1 35.1 87.2 84.9 85.7 81.6 81.9 80.2 77.5 74.4 64.2 45.9 80.7 78.4 80.7 82.6 79.5 77.7 75.7 72.9 67.8 53.4 39.0 90.0 76.7 80.0 80.0 80.0 70.0 76.7 76.7 53.3 6. 13.3 23.3 16.7 33.3 30.0 33.3 26.7 13.3 23.3 6.7 0.0 54.1 49.3 57.4 49.7 48.1 47.0 46.1 44.5 29.6 13.3 15.2 20.5 16.7 28.6 23.0 24.8 27.6 23.8 19.3 11.9 4.5 82.4 75.7 78.6 75.8 76.1 74.1 72.8 69.8 55.2 29.8 55.0 57.5 55.2 62.0 59.3 59.4 59.0 53.5 50.3 37.8 24.4 100.0 96.4 96.4 90.9 94.5 94.5 89.1 83.6 74.5 45. 98.2 90.9 94.5 92.7 92.7 87.3 89.1 85.5 78.2 70.9 43.6 25 Table 28: Benchmark scores for language: Arabic (ar). The highest and second-best scores are shown in bold and underlined, respectively. Model MLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 75.0 75.0 80.0 66.2 66.2 71.2 65.0 62.5 55.0 40.0 51.2 56.2 56.2 66.2 55.0 48.8 52.5 45.0 52.5 31.2 40.0 89.3 67.9 71.4 73.2 66.1 67.9 67.9 55.4 44.6 41. 78.6 62.5 66.1 67.9 69.6 64.3 60.7 58.9 42.9 37.5 39.3 87.8 81.8 83.6 80.1 77.2 77.4 74.4 67.7 53.2 38.9 80.9 74.4 77.2 79.5 75.7 71.6 69.5 64.6 56.7 43.6 35.4 76.7 80.0 76.7 86.7 83.3 83.3 76.7 66.7 36.7 10.0 13.3 26.7 6.7 40.0 23.3 30.0 23.3 13.3 13.3 3.3 0.0 52.6 41.3 53.7 47.0 47.3 46.6 44.9 41.2 25.8 11. 12.9 22.8 14.7 28.2 25.4 22.6 23.5 16.4 15.3 9.4 3.8 76.3 69.2 73.1 70.6 68.0 69.3 65.8 58.7 43.1 28.3 47.4 48.5 44.2 56.4 49.8 47.5 45.9 39.6 36.1 25.0 23.7 Table 29: Benchmark scores for language: Japanese (ja). The highest and second-best scores are shown in bold and underlined, respectively. Model MLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 72.5 73.8 75.0 70.0 66.2 68.8 71.2 63.7 53.8 47.5 60.0 66.2 55.0 67.5 58.8 51.2 55.0 47.5 46.2 40.0 37. 83.8 82.3 84.8 80.2 79.9 79.4 74.9 72.5 61.8 45.1 81.9 76.5 77.7 80.9 78.0 74.9 73.8 69.9 64.8 46.3 37.9 83.3 53.3 73.3 76.7 73.3 66.7 73.3 53.3 36.7 13.3 10.0 20.0 16.7 26.7 20.0 30.0 33.3 20.0 13.3 3.3 3.3 55.4 39.9 52.7 47.7 47.4 45.7 44.7 40.7 28.5 14.5 12.5 17.3 17.7 26.9 20.5 20.6 19.8 18.5 15.1 11.6 3. 73.9 67.1 76.0 73.0 71.0 69.8 70.1 62.1 51.1 33.5 51.3 53.3 52.2 58.8 53.9 51.8 53.2 47.7 43.2 34.0 23.9 74.5 86.3 94.1 90.2 88.2 88.2 86.3 80.4 74.5 47.1 92.2 86.3 94.1 92.2 92.2 82.4 84.3 82.4 76.5 68.6 37.3 26 Table 30: Benchmark scores for language: Korean (ko). The highest and second-best scores are shown in bold and underlined, respectively. Model MLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 75.0 76.2 71.2 71.2 68.8 67.5 60.0 66.2 53.8 33.8 63.7 58.8 58.8 63.7 60.0 52.5 52.5 52.5 46.2 48.8 40.0 88.0 72.0 80.0 74.0 72.0 74.0 80.0 74.0 66.0 52.0 80.0 76.0 68.0 76.0 74.0 72.0 68.0 76.0 74.0 58.0 52.0 85.9 81.8 84.7 79.2 78.6 79.6 74.7 68.8 57.8 41.5 80.5 75.9 76.7 79.8 77.2 72.5 73.3 66.5 59.9 46.0 36. 76.7 60.0 80.0 80.0 76.7 76.7 76.7 70.0 43.3 13.3 13.3 20.0 6.7 33.3 26.7 16.7 20.0 23.3 13.3 6.7 0.0 50.0 40.0 55.7 48.5 46.6 46.0 42.3 40.6 25.2 11.8 12.9 18.3 17.7 27.9 21.2 20.7 18.7 16.3 16.6 9.0 5.5 75.1 66.0 74.3 70.6 68.5 68.8 66.7 63.9 49.2 30.5 50.1 49.8 45.6 56.1 51.8 46.9 46.5 46.9 42.0 33.7 26. Table 31: Benchmark scores for language: Indonesian (id). The highest and second-best scores are shown in bold and underlined, respectively. Model INCLUDE MMMLU MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 80.0 76.4 80.0 80.0 81.8 78.2 72.7 70.9 63.6 36.4 80.0 76.4 74.5 81.8 81.8 70.9 70.9 78.2 67.3 52.7 52.7 86.3 83.7 87.2 82.0 80.4 79.6 77.7 72.3 61.2 46.6 81.1 75.9 78.8 81.9 77.2 76.4 74.1 69.6 66.5 49.0 40.0 83.3 73.3 80.0 76.7 80.0 70.0 70.0 66.7 36.7 10. 10.0 13.3 10.0 33.3 23.3 30.0 26.7 20.0 13.3 3.3 3.3 51.3 47.3 53.5 45.6 44.9 45.3 43.8 41.2 26.8 12.6 14.7 22.6 16.6 27.5 24.3 25.9 24.6 21.6 19.0 10.8 5.1 75.2 70.2 75.2 71.1 71.8 68.3 66.0 62.8 47.1 26.4 46.4 47.0 45.0 56.1 51.6 50.8 49.1 47.4 41.5 29.0 25.3 Table 32: Benchmark scores for language: Russian (ru). The highest and second-best scores are shown in bold and underlined, respectively. Model Multi-IF INCLUDE MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 68.1 61.2 62.2 62.5 60.7 63.6 62.9 52.8 37.8 26.4 52.0 57.3 54.1 56.7 58.6 58.0 60.3 59.3 46.1 34.8 25.5 80.4 73.2 80.4 73.2 76.8 80.4 69.6 69.6 46.4 46.4 80.4 71.4 67.9 75.0 71.4 73.2 71.4 58.9 58.9 41.1 46.4 70.0 76.7 80.0 63.3 73.3 66.7 63.3 56.7 20.0 3. 20.0 23.3 20.0 40.0 30.0 30.0 26.7 20.0 13.3 3.3 0.0 52.3 43.6 53.1 46.5 45.4 46.4 37.7 36.6 22.8 7.0 13.7 21.6 13.3 26.1 23.3 21.1 24.2 22.8 17.8 13.2 5.8 67.7 63.7 68.9 61.4 64.0 64.3 58.4 53.9 31.8 20.8 41.5 43.4 38.8 49.4 45.8 45.6 45.6 40.2 34.0 23.1 19.4 Table 33: Benchmark scores for language: Vietnamese (vi). The highest and second-best scores are shown in bold and underlined, respectively. Model MLogiQA INCLUDE MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 72.5 71.2 75.0 67.5 68.8 72.5 65.0 68.8 52.5 33.8 57.5 52.5 61.3 70.0 60.0 52.5 63.7 48.8 48.8 36.2 30.0 89.1 69.1 87.3 81.8 78.2 72.7 72.7 63.6 61.8 38.2 81.8 74.5 72.7 83.6 81.8 81.8 67.3 65.5 65.5 60.0 36.4 70.0 70.0 83.3 83.3 76.7 73.3 73.3 60.0 30.0 6.7 10.0 33.3 26.7 36.7 23.3 20.0 20.0 20.0 20.0 3.3 3. 52.1 49.2 55.1 44.0 46.1 45.8 42.9 42.2 26.9 9.8 13.0 20.6 18.6 27.1 21.8 24.7 21.6 19.1 19.0 10.9 3.9 70.9 64.9 75.2 69.2 67.4 66.1 63.5 58.6 42.8 22.1 40.6 45.2 44.8 54.4 46.7 44.8 43.2 38.4 38.3 27.6 18.4 28 Table 34: Benchmark scores for language: German (de). The highest and second-best scores are shown in bold and underlined, respectively. Model INCLUDE MMMLU MT-AIME24 PolyMath Average Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 50.0 57.1 71.4 64.3 64.3 57.1 64.3 57.1 64.3 57.1 57.1 57.1 64.3 71.4 57.1 57.1 57.1 64.3 64.3 42.9 42.9 85.6 83.8 86.0 81.9 81.9 80.9 78.1 74.0 63.4 47.6 80.4 76.1 79.9 81.7 77.2 77.7 76.0 70.8 66.0 53.2 37.8 86.7 76.7 83.3 86.7 80.0 70.0 66.7 73.3 36.7 10.0 10.0 26.7 16.7 40.0 30.0 23.3 30.0 20.0 26.7 10.0 3. 53.8 51.0 55.4 48.1 46.6 48.1 43.6 43.1 26.8 13.7 13.5 20.2 19.3 25.9 21.9 25.2 24.5 19.9 16.4 10.6 5.7 69.0 67.2 74.0 70.2 68.2 64.0 63.2 61.9 47.8 32.1 40.2 45.0 45.0 54.8 46.6 45.8 46.9 43.8 43.4 29.2 22.4 Table 35: Benchmark scores for language: Thai (th). The highest and second-best scores are shown in bold and underlined, respectively. Model MLogiQA MT-AIME24 PolyMath Average 80.0 60.0 86.7 76.7 80.0 76.7 70.0 60.0 33.3 13.3 10.0 16.7 6.7 23.3 13.3 30.0 23.3 10.0 13.3 6.7 0.0 50.7 41.3 53.6 46.9 45.2 44.4 41.3 39.4 23.7 11.4 11.9 19.0 17.4 27.6 22.2 22.3 22.1 17.2 16.1 9.5 3.6 68.2 58.8 71.4 65.8 63.0 62.0 60.0 53.1 35.3 19. 24.8 28.6 27.6 37.4 32.3 34.1 31.0 23.2 24.4 19.6 13.7 Thinking Mode Non-thinking Mode Gemini2.5-Pro QwQ-32B Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B GPT-4o-2024-1120 Gemma-3-27b-IT Qwen2.5-72B-Instruct Qwen3-235B-A22B Qwen3-32B Qwen3-30B-A3B Qwen3-14B Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-0.6B 73.8 75.0 73.8 73.8 63.7 65.0 68.8 60.0 48.8 33. 52.5 50.0 58.8 61.3 61.3 50.0 47.5 42.5 43.8 42.5 37.5 29 Table 36: Language families and language codes supported by Qwen3 in Belebele Benchmark Language family # Langs Language code (ISO 639-3 ISO 15924) Indo-European Sino-Tibetan Afro-Asiatic Austronesian Dravidian Turkic Tai-Kadai Uralic Austroasiatic Other 3 8 7 4 4 2 3 2 7 por Latn, deu Latn, tgk Cyrl, ces Latn, nob Latn, dan Latn, snd Arab, spa Latn, isl Latn, slv Latn, eng Latn, ory Orya, hrv Latn, ell Grek, ukr Cyrl, pan Guru, srp Cyrl, npi Deva, mkd Cyrl, guj Gujr, nld Latn, swe Latn, hin Deva, rus Cyrl, asm Beng, cat Latn, als Latn, sin Sinh, urd Arab, mar Deva, lit Latn, slk Latn, ita Latn, pol Latn, bul Cyrl, afr Latn, ron Latn, fra Latn, ben Beng, hye Armn zho Hans, mya Mymr, zho Hant heb Hebr, apc Arab, acm Arab, ary Arab, ars Arab, arb Arab, mlt Latn, erz Arab ilo Latn, ceb Latn, tgl Latn, sun Latn, jav Latn, war Latn, ind Latn mal Mlym, kan Knda, tel Telu, tam Taml kaz Cyrl, azj Latn, tur Latn, uzn Latn tha Thai, lao Laoo fin Latn, hun Latn, est Latn vie Latn, khm Khmr eus Latn, kor Hang, hat Latn, swh Latn, kea Latn, jpn Jpan, kat Geor Table 37: Comparison of Belebele Benchmark performance between Qwen3 and other baseline models. Scores are highlighted with the highest in bold and the second-best underlined. Model Gemma-3-27B-IT Qwen2.5-32B-Instruct QwQ-32B Qwen3-32B (Thinking) Qwen3-32B (Non-thinking) Gemma-3-12B-IT Qwen2.5-14B-Instruct Qwen3-14B (Thinking) Qwen3-14B (Non-thinking) Gemma-3-4B-IT Qwen2.5-3B-Instruct Qwen3-4B (Thinking) Qwen3-4B (Non-thinking) Gemma-3-1B-IT Qwen2.5-1.5B-Instruct Qwen3-1.7B (Thinking) Qwen3-1.7B (Non-thinking) IndoEuropean SinoTibetan AfroAsiatic Austronesian Dravidian Turkic TaiKadai Uralic Austroasiatic Other 83.5 67.8 69.3 84.5 84.0 79.0 66.2 81.0 78.0 64.8 36.9 74.3 65. 28.8 28.6 52.8 43.3 86.8 80.8 80.3 89.3 85.0 82.8 74.2 83.8 81.8 64.0 45.1 76.3 64.0 27.3 29.7 57.8 48.0 81.0 74.5 77.0 83.5 85. 77.5 72.2 83.5 80.5 61.5 49.8 68.5 60.5 28.0 39.4 53.5 46.0 91.0 87.0 88.0 91.3 88.7 89.0 83.9 91.0 87.7 70.7 50.6 83.0 74. 32.7 33.8 70.3 54.3 86.5 79.0 83.0 88.0 88.0 83.0 77.9 82.5 81.5 71.0 56.8 74.5 74.0 33.0 42.0 63.5 54.0 87.0 72.6 74.0 83.1 81. 81.6 70.4 81.7 77.0 62.6 48.4 67.9 61.0 30.9 36.0 53.4 43.9 89.2 85.5 86.1 90.7 89.1 85.8 82.7 88.6 87.4 71.8 58.0 82.2 76. 36.5 41.5 69.7 58.8 86.3 82.3 83.7 89.7 88.0 83.3 78.9 87.3 82.7 72.0 62.3 77.7 77.0 36.0 43.0 66.0 62.7 85.9 80.4 81.9 84.8 82. 83.4 80.4 82.4 80.1 63.5 57.2 74.1 65.6 30.0 39.6 59.4 50.8 84.1 70.6 71.3 86.7 83.7 79.3 69.1 82.4 80.7 61.7 47.9 73.0 65. 29.1 34.8 58.6 53."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. AIME. AIME problems and solutions, 2025. URL https://artofproblemsolving.com/wiki/index.p hp/AIME Problems and Solutions. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr on, and Sumit Sanghai. GQA: Training generalized multi-query Transformer models from multi-head checkpoints. In EMNLP, pp. 48954901. Association for Computational Linguistics, 2023. Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong. Training-free long-context scaling of large language models. CoRR, abs/2402.17463, 2024. Anthropic. Claude 3.7 Sonnet, 2025. URL https://www.anthropic.com/news/claude-3-7-sonnet. Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. CoRR, abs/2309.16609, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The Belebele benchmark: parallel reading comprehension dataset in 122 language variants. CoRR, abs/2308.16884, 2023. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. MultiPL-E: scalable and polyglot approach to benchmarking neural code generation. IEEE Trans. Software Eng., 49(7):36753691, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. CoRR, abs/2401.06066, 2024. Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In ICML, volume 70 of Proceedings of Machine Learning Research, pp. 933941. PMLR, 2017. Google DeepMind. Gemini 2.5, 2025. URL https://blog.google/technology/google-deepmind/gemi ni-model-thinking-updates-march-2025/. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. In ICML, volume 202 of Proceedings of Machine Learning Research, pp. 74807512. PMLR, 2023. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. SuperGPQA: Scaling LLM evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The Llama 3 herd of models. CoRR, abs/2407.21783, 2024. Simin Fan, Matteo Pagliardini, and Martin Jaggi. DoGE: Domain reweighting with generalization estimation. arXiv preprint arXiv:2310.15393, 2023. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we done with MMLU? CoRR, abs/2406.04127, 2024. Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang. CRUXEval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, et al. Multi-IF: Benchmarking LLMs on multi-turn and multilingual instructions following. arXiv preprint arXiv:2410.15553, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS Datasets and Benchmarks, 2021b. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. RULER: Whats the real context size of your long-context language models? CoRR, abs/2404.06654, 2024. 32 Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-Eval: multilevel multi-discipline chinese evaluation suite for foundation models. In NeurIPS, 2023. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2.5-Coder technical report. CoRR, abs/2409.12186, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and efficient pre-LN Transformers. CoRR, abs/2305.14858, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. ulu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and BenchBuilder pipeline. CoRR, abs/2406.11939, 2024. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. CoRR, abs/2305.20050, 2023. Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. ZebraLogic: On the scaling limits of LLMs for logical reasoning. CoRR, abs/2502.01100, 2025. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. DeepSeek-V3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation. In NeurIPS, 2023a. Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. RegMix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024b. Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. AlignBench: Benchmarking Chinese alignment of large language models. CoRR, abs/2311.18743, 2023b. Meta-AI. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation, 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. OpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. OpenAI. Multilingual massive multitask language understanding, 2024. URL https://huggingface.co /datasets/openai/MMMLU. OpenAI. Learning to reason with LLMs, 2024. URL https://openai.com/index/learning-to-reaso n-with-llms/. OpenAI. Introducing openai o3 and o4-mini, 2025. URL https://openai.com/index/introducing-o 3-and-o4-mini/. Samuel J. Paech. Creative writing v3, 2024. URL https://eqbench.com/creative writing.html. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension of large language models. CoRR, abs/2309.00071, 2023. Zihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Demons in the detail: On implementing load balancing loss for training specialized mixture-of-expert models. CoRR, abs/2501.11873, 2025. 33 Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, and Junyang Lin. CodeElo: Benchmarking competition-level code generation of LLMs with human-comparable Elo ratings. CoRR, abs/2501.01257, 2025. Qwen Team. QwQ: Reflect deeply on the boundaries of the unknown, November 2024. URL https: //qwenlm.github.io/blog/qwq-32b-preview/. Qwen Team. QwQ-32B: Embracing the power of reinforcement learning, March 2025. URL https: //qwenlm.github.io/blog/qwq-32b/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level Google-proof Q&A benchmark. CoRR, abs/2311.12022, 2023. Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, orje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzeminski, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani Moakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara Hooker, and Antoine Bosselut. INCLUDE: evaluating multilingual language understanding with regional knowledge. CoRR, abs/2411.19799, 2024. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In ACL (1). The Association for Computer Linguistics, 2016. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In ICLR. OpenReview.net, 2023. Guijin Son, Jiwoo Hong, Hyunwoo Ko, and James Thorne. Linguistic generalizability of test-time scaling in mathematical reasoning. CoRR, abs/2502.17407, 2025. Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced Transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. In ACL (Findings), pp. 1300313051. Association for Computational Linguistics, 2023. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level subwords. In AAAI, pp. 91549160. AAAI Press, 2020. Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, and Jingren Zhou. PolyMath: Evaluating mathematical reasoning in multilingual contexts, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-Pro: more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid ShwartzZiv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. LiveBench: challenging, contamination-free LLM benchmark. CoRR, abs/2406.19314, 2024. Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, and Fei Huang. WritingBench: comprehensive benchmark for generative writing. CoRR, abs/2503.05244, 2025. xAI. Grok 3 beta the age of reasoning agents, 2025. URL https://x.ai/news/grok-3. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36:6979869818, 2023. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8 ber keley function calling leaderboard.html, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024a. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024c. Yidan Zhang, Boyi Deng, Yu Wan, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, and Jingren Zhou. P-MMEval: parallel multilingual multitask benchmark for consistent evaluation of LLMs. CoRR, abs/2411.09116, 2024. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911, 2023. Qin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang, and Junyang Lin. AutoLogi: Automated generation of logic puzzles for evaluating reasoning abilities of large language models. CoRR, abs/2502.16906, 2025."
        }
    ],
    "affiliations": []
}