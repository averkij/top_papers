{
    "paper_title": "Global and Local Entailment Learning for Natural World Imagery",
    "authors": [
        "Srikumar Sastry",
        "Aayush Dhakal",
        "Eric Xing",
        "Subash Khanal",
        "Nathan Jacobs"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning the hierarchical structure of data in vision-language models is a significant challenge. Previous works have attempted to address this challenge by employing entailment learning. However, these approaches fail to model the transitive nature of entailment explicitly, which establishes the relationship between order and semantics within a representation space. In this work, we introduce Radial Cross-Modal Embeddings (RCME), a framework that enables the explicit modeling of transitivity-enforced entailment. Our proposed framework optimizes for the partial order of concepts within vision-language models. By leveraging our framework, we develop a hierarchical vision-language foundation model capable of representing the hierarchy in the Tree of Life. Our experiments on hierarchical species classification and hierarchical retrieval tasks demonstrate the enhanced performance of our models compared to the existing state-of-the-art models. Our code and models are open-sourced at https://vishu26.github.io/RCME/index.html."
        },
        {
            "title": "Start",
            "content": "Srikumar Sastry, Aayush Dhakal, Eric Xing, Subash Khanal, Nathan Jacobs Washington University in St. Louis {s.sastry, a.dhakal, e.xing, k.subash, jacobsn}@wustl.edu 5 2 0 2 6 2 ] . [ 1 6 7 4 1 2 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Learning the hierarchical structure of data in visionlanguage models is significant challenge. Previous works have attempted to address this challenge by employing entailment learning. However, these approaches fail to model the transitive nature of entailment explicitly, which establishes the relationship between order and semantics within representation space. In this work, we introduce Radial Cross-Modal Embeddings (RCME), framework that enables the explicit modeling of transitivity-enforced entailment. Our proposed framework optimizes for the partial order of concepts within vision-language models. By leveraging our framework, we develop hierarchical visionlanguage foundation model capable of representing the hierarchy in the Tree of Life. Our experiments on hierarchical species classification and hierarchical retrieval tasks demonstrate the enhanced performance of our models compared to the existing state-of-the-art models. Our code and models are open-sourced at https://vishu26. github.io/RCME/index.html. 1. Introduction Computer Vision has become increasingly valuable in understanding the natural world, thanks to the rise of open citizen science platforms and the abundance of consumer data. These tools, complemented by domain experts, have been instrumental in addressing pressing challenges at scale such as automatic species identification [39, 42], animal behavior understanding [6, 25] and visual geolocalization [16, 43]. Nevertheless, the complex and ever-changing nature of our world poses significant challenge in constructing models that can generalize and adapt to novel data. BioCLIP [36] and BioTroveCLIP [45] successfully attempted to build vision-language foundation model for the Tree of Life. Recently, TaxaBind [34] extended BioCLIPs capabilities to handle additional modalities such as audio and satellite imagery. However, these models fail to fully leverage the hierarchical nature of the label space. This limited capability of these models limits them to reason at the Figure 1. Conceptual overview of our method focusing on preserving the global order of concepts in vision-language models according to their distance from an entailment root. Our method aims to enforce transitivity in entailment. most granular level of the hierarchy (i.e. species) using fixed database of taxonomic labels. Consequently, this restriction prevents the models from accurately representing the actual taxonomic system and the evolution of species in the Tree of Life. We argue that learning hierarchical representations for the Tree of Life is crucial. significant portion of species on Earth remain undescribed [29], and labeling specimens up to the species rank is expensive and requires adequate expertise for biologists [15, 32, 37]. Furthermore, the taxonomic classification system and labels are subject to change over time due to mislabeling or the discovery of new species [36]. Hierarchical representations can allow for reasoning about such species at any rank and can eventually be used for grouping and routing specimens to biologists with appropriate expertise [29]. It can also help understand the evolution of certain species in the Tree of Life. For end users with arbitrary expertise, hierarchical representations facilitate classification at any taxonomic rank. Finally, in the paper, we empirically demonstrate the benefits of such structured representations for classification and retrieval tasks. popular technique for learning hierarchical representations for vision-language models is entailment learning, 1 which aims to learn concentric cones of embedding subregions. In the past, studies on entailment learning relied on explicitly defining aperture angles that defined the structure of these cones [11, 14, 28]. However, these approaches can be limiting since the optimality of the cone structures can vary from application to application. Recently, Alper et al. [1] introduced radial embeddings, an approach to finetune existing vision-language models that eliminate the dependence of the objective on entailment cones. However, their method fails to enforce the partial order of concepts in their hierarchical embedding space. In Figure 1, we illustrate transitivity in entailment, imposing partial ordering of concepts in the embedding space [14]. For instance, if Mammalia is entailed by Chordata and Carnivora is entailed by Mammalia, then Chordata entails Carnivora. Ideally, this phenomenon should hold for all possible sub-hierarchies in the data. Transitivity is an important property for representation space as it controls the distance between concepts based on their semantic granularity. For instance, finegrained concepts are projected farther from coarse-grained concepts. To this end, we propose novel framework called Radial Cross-Modal Embeddings (RCME) which enables the learning of hierarchical representations by imposing partial order constraints while eliminating the need to define the structure of the cones. Using our framework, we propose hierarchical vision-language foundation model for the Tree of Life, outperforming existing state-of-the-art models in hierarchical classification tasks. Notably, our framework is general enough to be adapted for any other domain. Our contributions are as follows: 1. We propose an objective function to optimize for transitivity in textual entailment within vision-language models. We address the issue of Alper et al. [1], which overlooks partial order in textual entailment. 2. We propose Radial Cross-Modal Embeddings (RCME), framework that solves for transitivity-enforced entailment and cross-modal alignment in vision-language models. 3. Experiments show our models outperform the stateof-the-art in hierarchical classification, hierarchical retrieval, and image-to-image retrieval tasks. 2. Related Works 2.1. Representation learning Contrastive learning has enabled training large-scale visionlanguage models [19, 21, 30] which have generated significant advancement in diverse tasks like image classification and few-shot learning. Recent lines of work have focused on improving these generalist models by either achieving fine-grained alignment [8, 22, 46] or enhancing intra-modal representations [9, 24]. However, such methods are still not ideal for specific domains where there are structures of representations imposed by semantics. Hierarchical representation learning has gained traction, particularly in tasks requiring structured knowledge representation, such as natural language inference (NLI) [12, 20, 38] and knowledge graph embeddings [2, 5]. One of the key challenges in hierarchical representation learning is preserving the partial ordering of concepts in the embedding space while maintaining generalizability across different domains. 2.2. Computer vision for ecology The intersection of computer vision and ecology has led to significant advances in tasks like fine-grained species classification [15, 36], animal detection using camera traps [3, 35], and animal behavior recognition [6, 25]. Large-scale datasets [23, 36, 41, 45] and citizen science platforms like iNaturalist [39] have enabled the training of deep learning models to solve these tasks. Multimodal representation learning frameworks [7, 10, 17, 33] with wildlife observations and satellite images has shown benefits in solving ecological tasks like species distribution modeling. Recently, vision-language foundation models for the Tree of Life such as BioCLIP [36] and BioTroveCLIP [45] have shown excellent capabilities in zero-shot species identification. TaxaBind [34] extended such vision-language models by incorporating additional modalities such as audio and satellite imagery. However, all such models are limited to fixed taxonomic labels and struggle with classification at arbitrary taxonomic ranks. They lack structured hierarchical representations implied by the hierarchical nature of the Tree of Life. 2.3. Entailment learning Traditional hierarchical learning approaches often rely on hyperbolic embeddings [4, 26, 27] to enforce hierarchical relationships. Entailment learning is particularly useful for structuring embeddings in semantic order [40]. Early works on entailment learning explicitly defined cone structures using aperture angles to capture hierarchical dependencies [11, 14, 28, 44, 47]. However, these approaches are often restrictive as the optimal structure of the cones can vary across datasets and applications. Recent works, such as Radial Embeddings [1] and ATMG [31], attempt to relax these constraints by learning hierarchical embeddings without predefined cone structures in the radial and hyperbolic geometry respectively. While these methods improves adaptability, it does not explicitly enforce partial ordering, leading to suboptimal performance in hierarchical retrieval tasks. 2 Figure 2. Transitivity in Entailment. In an ideal transitivity-imposed entailment, textual embeddings satisfy partial order conditions. 3. Preliminaries We begin with the arguments on the conditions for entailment as proposed in Esteva et al. [13] and Ganea et al. [14]. For our purposes, we consider the tree of life hierarchy and develop the logic for entailment with respect to it. Let R{j=0,1...N } represent the sets of the domain of textual embeddings at each hierarchical rank. As increases, the semantic granularity in textual embedding increases. We consider R0 as the set which contains the entailment root embedding, T0. Let Rj, denote the textual embedding for the ith species belonging to some rank in the hierarchy. We use j1 Rj1 to denote the immediate ancestor of j+1 Rj+1 represent the child of . Let i . To define optimal radial cones, we first show the following. j+ ST Let ST Lemma 1. In transitivity-enforced entailment, finegrained concepts are progressively projected away from the entailment root and into smaller subregion when moving down in the hierarchy. and ψ(T ) denote cone and its half aperture angle respectively defined at with respect to T0. The transitivity property states that if j+1 ST , then ST [14]. The direct consequence of this result is ) ψ(T on the aperture angles of nested cones: ψ(T j+1). In other words, if , then the cone at j+1 is completely enclosed by the cone at . This means that the distance of the embeddings from the root increases when one goes down in the hierarchy (see Equation 3). Hence, combining the above-stated results, finegrained concepts (textual embeddings lower in the hierarchy) are contained within smaller cones than coarse-grained concepts. We provide mathematical proof in the appendix. Lemma 1 is natural property to have in textual entailment because it establishes direct relationship between the semantic granularity of textual embeddings and their distance from the entailment root. j+1 is entailed by Ganea et al. [14] defined the distance between two emin the entailment configuration as the T0) and (T ) conj . In the Radial/Euclidean geometry, and beddings exterior angle (Ξ) between (T sidering the cone at Ξ is defined as follows: Ξ(T , ) = arccos (cid:32) (T j T0), (T T0.T i ) (cid:33) (1) where , is the inner product between the embeddings. Likewise, the similarity measure can be defined as: S(T , ) = cos(Ξ(T , )) (2) Furthermore, they defined the half aperture angle of any cone as monotonically decreasing function with respect to its distance from the entailment root: ψ(T ) arcsin(1/r(T , T0)) (3) where is distance function with range [ϵ, 1]. Esteva et al. [13] proposed entailment configurations to adhere to the transitivity property that defines the partial order of concepts. Inspired from their formulation we devise the transitivity property mathematically as follows: S(T j1, j+1) S(T j1, ).S(T j , j+1) j, (4) where [0, 1]. This constraint establishes the relationship between text embeddings and their higher-level ancestors. In Figure 2, we show two different scenarios in the entailment configuration. Figure 2c) shows perfect entailment configuration that satisfies transitivity constraints. In Figure 2d) we show configuration where transitivity is violated. Additionally, for entailment configuration where transitivity holds, Ganea et al. [14] showed that Ξ(T ) π/2 is true for any given parent and its child. This means that the cosine similarity between j+1) ψ(T , 3 LLE(i, j, k) = Ξ(T , j+1) Ξ(T , j+1) (5) LGLE(i, k; α) = the two embeddings under transitivity constraints is always non-negative with respect to an entailment root. Alper et al. [1] proposed vision-text radial embeddings by minimizing Ξ for positive pairs while maximizing it for negative pairs. They perform text-only fine-tuning while keeping the vision encoder frozen. The main contribution was eliminating the dependence of the objective on aperture angle (Equation 3) and formulating the entailment problem on normalized embeddings. However, this resulted in the objective ignoring the transitivity constraint (equation 4) and providing no guarantee that Lemma 1 holds. Their objective optimizes for the local entailment but does not necessarily solve for the partial order of concepts. Their objective is defined as follows: j+1 is negative example for where . We argue that Alper et al. [1] method only optimizes for the local entailment objective, i.e. entailment with respect to the immediate ancestor. 4. Method In this section, we introduce our proposed objective function, which seeks to optimize for transitivity without the requirement of defining an expression for the aperture angles. In addition, we describe our hard negative mining technique for improved performance. 4.1. Global Entailment Learning We begin by coining the terms local and global entailment. We say local entailment is enforced when j+1 is completely entailed by up to reasonable degree, for all possible values of and j. Global entailment is enforced when Equation 4 holds for all possible sub-hierarchies in addition to local entailment. Mathematically, if S(T j+1) = γ ( ) = δ ( 0), then S(T 0) and S(T j+1) γ.δ. This ensures that Lemma 1 is satisfied. We enforce this objective using margin-based loss as follows: , j1, j1, LGE(i, j; α) = max(0, Ξ(T , arccos(S(T j1, j+1) j+1).S(T j1, )) + α) (6) where α is the expected margin by which the angles should differ. We set it to the maximum possible value of π/2. We clip the values of between [0, 1] for practical implementation. This loss is only calculated for consecutive positive triplets in given hierarchy. To enable global and local entailment learning, we combine the global and local objective functions. The loss is iteratively computed for each rank given positive and negative examples. The final loss is combination of Equa4 Figure 3. Hard Negative Examples. For the local entailment objective, we propose to sample negatives by matching all previous ranks of the positive examples. We recursively sample negatives for each rank separately. tions 5 and 6: 1 1 (cid:88) LGE(i, p; α) p=1 (cid:124) (cid:123)(cid:122) Global Entailment (cid:125) + 1 1 (cid:88) p=0 (cid:124) LLE(i, p, K[p]) (cid:123)(cid:122) Local Entailment (cid:125) (7) where represents set of negative examples for each rank of the hierarchy, indexed by p. 4.2. Radial Cross-Modal Embeddings In addition to our proposed global entailment objective, we also propose to add cross-modal alignment loss term to fine-tune the vision encoder along with the text encoder. In [1], they added prior preservation loss on the text encoder to preserve the original image-text embedding space. Instead, we propose to add cross-modal alignment term to simultaneously fine-tune the vision and text encoders. This is given by: LCMA(i) = log ,I eT ,Im + eT m=1 eT (cid:80)B ,I (8) where is an image embedding of the same species as represented by . Note that the sum in the denominator is over batch of negative samples. We only compute the objective for the most granular level of hierarchy which is the species level. The final loss is now given by combining Equations 7 and 8: LRCME(i, k; α) = LGLE(i, k; α) + βLCMA(i) (9) 4.3. Hard Negative Mining We propose hard negative mining technique to sample negative examples required for the local entailment objective. Recall from equation 5 that the negative example must belong to the same rank as the positive example. To create hard negative example for given rank, we propose to sample labels that exactly match the taxonomic labels for all previous ranks of given positive. In other words, we randomly sample sibling of the parent for given positive example. We then randomly sample child of this sibling to create the final negative example. This is done recursively to create negative examples required at each rank. Figure 3 illustrates our hard negative sampling approach. This approach encourages the model to learn fine-grained differences between species of the same ancestry. 5. Experiments In this section, we present the details of our implementation and the baselines used for comparison. We conduct four experiments to evaluate the effectiveness of the models on: 1) hierarchical retrieval and ordering of taxonomic labels; 2) zero-shot classification at each taxonomic rank; 3) intramodal image-to-image retrieval at each taxonomic rank; 4) UMAP visualizations of textual embeddings. 5.1. Experimental Setup Implementation Details. We train two variants of our model. One model is trained using OpenCLIPs initialization and the other model is fine-tuned starting from the BioCLIPs checkpoint (denoted using FT). Both models are based on OpenCLIPs ViT-B/16 architecture. Both models are trained on the TreeofLife-10M dataset using 2 NVIDIA H100 GPUs. We use the word Eukarya as the entailment root for the Tree of Life. Please refer to the appendix for additional details on the implementation. Baselines. We compare our models against various vision-language baseline models, including CLIP [30], OpenCLIP [18], BioTroveCLIP [45], BioCLIP [36], TaxaBind [34], Radial Embeddings [1], MERU [11] and ATMG [31]. Each of these models is based on the ViT-B/16 architecture. Since CLIP and OpenCLIP are not specifically trained for the task, we use the common names of species for image classification at the species rank [36]. To ensure fair comparison, we fine-tuned Radial Embeddings, MERU and ATMG on the TreeofLife-10M dataset, starting from BioCLIPs checkpoint. Additionally, we do hard negative mining at each rank for fine-tuning. Evaluation Datasets. We evaluate our models using the iNaturalist-2021 [39] and BioCLIP-Rare datasets [36]. The iNaturalist-2021 dataset comprises 10,000 unique species of animals, plants, and fungi. It includes held-out test set with total of 100,000 images. The BioCLIP-Rare dataset features 400 rare species of animals categorized under the IUCN Red List. Each species in the dataset is represented by 30 images for evaluation. 5.2. Results Ordering of taxonomic labels. We evaluate the effectiveness of our learned vision-language representations on hiModel Kendalls τd Precision Recall F1 CLIP [30] OpenCLIP [18] BioTroveCLIP [45] BioCLIP [36] TaxaBind [34] Radial Emb. [1] MERU [11] ATMG [31] RCMEFT (ours) RCME (ours) 0.737 0.825 0.566 0.012 0.012 0.521 0.403 0.571 0.963 0.993 0.047 0.149 0.122 0.115 0. 0.147 0.356 0.343 0.386 0.458 0.054 0.190 0.173 0.153 0.155 0.196 0.133 0.130 0.405 0. 0.050 0.167 0.143 0.131 0.133 0.168 0.193 0.189 0.395 0.508 Table 1. Hierarchical Retrieval Metrics. We evaluate the ability of different models to encode the partial order of taxonomies in the Tree of Life. Additionally, we evaluate the models on the standard task of hierarchical image-text retrieval. erarchical retrieval tasks as defined in Alper et al. [1] and Desai et al. [11]. Firstly, we check whether the taxonomic labels are correctly ordered according to their distance from the entailment root using Kendalls Tau (τd). Secondly, we calculate image-to-text hierarchical retrieval metric relative to each taxonomic label. To ensure fair evaluation, each unique species in the dataset is represented by single image, which is selected at random from the test set. More details about the task setup present in the appendix. Table 1 presents the performance of the models on the iNaturalist-2021 dataset. Our learned representations exhibit excellent ordering of the taxonomic labels in the embedding space. This means embeddings corresponding to the species rank are projected farthest from the root, while those corresponding to the kingdom rank are projected closer to the root. Notably, radial embeddings perform worse than CLIP and OpenCLIP in the ordering task. Both our models show significant improvement, with the model trained using CLIPs checkpoint showing more substantial performance improvement. We see minimum absolute gain of +0.168 in correlation as compared to the baseline models. Furthermore, our method is able to outperform the rest of the methods in the hierarchical image-to-text retrieval task. We see minimum absolute gain of +0.102 and +0.376 precision and recall respectively. Overall, these experiments demonstrate that our proposed objective function successfully imparts partial order to the embedding space. Zero-shot classification. We perform image classification by using taxonomic labels at each rank of the Tree of Life. The classification of each rank is performed independently to assess the ability of the models in the zero-shot setting. Table 2 and 3 show the performance of different models in this task on iNaturalist-2021 and BioCLIP-Rare datasets 5 Model Kingdom Phylum Class Family Order Genus Species Average CLIP [30] OpenCLIP [18] BioTroveCLIP [45] BioCLIP [36] TaxaBind [34] Radial Emb. [1] MERU [11] ATMG [31] RCMEFT (ours) RCME (ours) 79.60 66. 37.43 36.96 40.45 45.84 95.82 99.12 86.18 88.18 37.45 18.42 21.81 32.02 32.22 35.34 94.84 86. 68.01 84.81 17.97 15.45 19.92 19.97 19.68 22.23 63.12 73.03 38.27 55.22 17.76 07. 10.61 24.31 24.38 24.96 27.27 51.83 38.16 46.74 05.77 02.60 12.91 31.43 30.80 32.86 3.12 33. 38.27 41.82 04.90 04.42 59.56 61.04 62.38 61.07 1.30 49.59 64.31 67.41 52.11 58. 68.00 68.24 70.08 68.23 0.73 39.52 70.81 73.52 30.79 24.99 32.89 39.13 40.00 41.50 40.89 61. 57.71 65.09 Table 2. Zero-shot classification performance on iNaturalist-2021 dataset at various levels of the taxonomy. Model Phylum Class Family Order Genus Species Average CLIP [30] OpenCLIP [18] BioTroveCLIP [45] BioCLIP [36] TaxaBind [34] Radial Emb. [1] MERU [11] ATMG [31] RCMEFT (ours) RCME (ours) 77.97 20.35 41.69 43.55 46. 43.77 80.66 82.20 62.07 79.60 42.54 33.56 37.86 61.08 60.59 63.03 58.99 80.31 62.25 77. 24.35 19.14 22.85 53.25 53.95 53.96 25.92 72.48 63.64 68.41 11.75 04.42 17.76 45.32 46. 45.75 08.12 53.03 47.64 50.10 14.18 10.54 31.16 53.38 55.09 53.43 05.13 45.02 55.33 56. 30.41 30.22 27.82 34.52 35.84 34.85 04.53 35.32 36.79 41.62 33.53 19.71 29.84 48.51 49. 49.13 30.56 61.39 54.62 62.64 Table 3. Zero-shot classification performance on BioCLIP-Rare dataset at various levels of the taxonomy. Note that this dataset primarily contains Animals. respectively. In both datasets, our model is able to exhibit excellent classification performance at each taxonomic rank. When averaging performance over each taxonomic rank, our model exhibits gains of +5.17% and +2.03% on iNaturalist-2021 and BioCLIP-Rare respectively. For radial emb., MERU and ATMG, we see gain in performance at higher ranks of the taxonomy, while dip in performance at fine ranks such as genus and species. This empirically demonstrates the usefulness of global entailment learning in preserving performance at fine ranks of the hierarchy. Interestingly, each of the models show lower performance for ranks class, family, and order than for ranks genus and species in the iNaturalist dataset. This is because of the lower performance of the models for plants as compared to animals in these classes. Notably, this behavior is not seen in Table 3 since the dataset only contains animals. We suspect this happens as plants usually exhibit convergent traits and are usually mislabeled combination of factors including morphological variability, hybridization, genetic complexity, and evolving methodologies usually complicates plant taxonomy. We show the kingdom-wise performance of our model on this dataset in the appendix to further analyze this behavior. This demonstrates the importance of learning hierarchical representations to detect such behavior and improve the taxonomic classification system. Image-to-image retrieval. In this experiment, we explore whether our method enhances intra-modal representations. We conduct image-to-image retrieval at each taxonomic rank. Given an image of particular species and its corresponding taxonomic label at rank, our objective is to retrieve images of species with the same taxonomic label at the given rank. For example, given an image of an elephant with the label Mammalia, our goal is to retrieve images of Mammalia using solely the image of the given elephant. Table 4 presents the results of this experiment on the iNaturalist-2021 dataset. We compute the recall metric 6 Model Kingdom Phylum Class Family Order Genus Species Average CLIP [30] OpenCLIP [18] BioTroveCLIP [45] BioCLIP [36] TaxaBind [34] Radial Emb. [1] MERU [11] ATMG [31] RCMEFT (ours) RCME (ours) 95.09 96.29 98.60 98.50 98.66 98.50 98.20 99. 98.72 99.65 91.38 93.33 98.07 97.60 98.07 97.60 96.02 98.45 98.08 99.04 80.04 85. 95.31 94.94 95.71 94.94 85.04 96.88 95.76 97.11 49.29 59.85 85.06 84.98 85.64 84.98 58.81 89. 86.95 90.61 29.50 41.85 78.89 78.63 78.90 78.63 41.96 82.30 80.43 85.38 14.64 26. 68.97 67.22 68.84 67.22 26.56 70.09 70.27 75.09 10.60 16.78 55.27 51.62 54.47 51.62 16.40 52. 57.10 61.27 52.94 59.95 82.88 81.92 82.90 81.92 60.43 84.08 83.91 86.88 Table 4. Image-to-Image Retrieval. We evaluate the effectiveness of the intra-modal image representations learned by the models on the task of image-to-image retrieval at each taxonomic rank. Radial Emb. performs identically to BioCLIP since the vision encoder is not fine-tuned during its training. Figure 4. UMAP Visualization of Textual Embeddings. We visualize the textual embeddings using 2-D UMAP to show our model learns to preserve the partial order of taxonomic labels based on their distance from the entailment root. (R@1) for this task. Our method outperforms all other models under consideration. We see gain of +3.33% when performance is averaged over all taxonomic ranks. Notably, the radial embedding methods performance is limited to the frozen image encoder model used, as it does not fine-tune the image encoder. There is noticeable gap in the performance of the models at the species rank as compared to our model showing that our model is effective at extracting fine-grained visual features. CLIP and OpenCLIP exhibit poor performance, particularly in the genus and species categories, indicating that these models struggle to extract finegrained visual features. This suggests the potential of our model in future applications such as fine-grained retrieval augmented generation. UMAP visualization of textual embeddings. In Figure 4, we present 2-D Uniform Manifold Approximation and Projection (UMAP) visualization of textual embeddings obtained from BioCLIP, Radial Emb., and RCME. For given species, we plot the embedding for all the ancestors in the taxonomic hierarchy and their siblings. For instance, if we are given the specie Cardinalis cardinalis, we begin with kingdom label Animalia and plot all the Phyla that belong to the kingdom Animalia. This is repeated for all the subsequent ranks. From the plot, we anticipate two properties: 1) siblings share similar embeddings; 2) the embeddings are ordered in coarse-to-fine manner (from kingdom to species), based on their distance from the entailment root. From the figure, it is evident that our model has successfully preserved the partial order of taxonomic labels based on their distance from the entailment root. For instance, the embeddings corresponding to the species rank are projected farthest away from the entailment root. However, BioCLIP and Radial Emb. are unable to effectively enforce transitivity. We provide additional UMAPs in the appendix. 7 LLE LGE Lprior LCM Kingdom Phylum Class Family Order Genus Species Average 36.96 45.84 47.34 85.13 88.18 32.02 35.34 38.14 81.11 84.81 19.97 22.23 25.78 53.21 55. 24.31 24.96 26.88 44.33 46.74 31.43 32.86 35.34 39.82 41.82 61.04 61.07 62.67 65.90 67.41 68.24 68.23 69.43 71.28 73.52 39.13 41.50 43.65 62.97 65.09 Table 5. Loss Ablation. We evaluate the performance of our model when trained with various combinations of the loss terms proposed in our objective function. RCME iNaturalist-2021 BioCLIP-Rare w/o negative mining with negative mining 62.22 65.09 59.12 62.64 Table 6. Benefits of hard negative mining. We evaluate the effectiveness of our hard negative mining approach for hierarchical representation learning of Tree of Life. 5.3. Ablations , , where We conduct an ablation study to analyze the effect of each loss component on the performance of our models. For the losses not using our cross-modal alignment term (LCM A), we include the Lprior from Alper et al. [1] to preserve the original vision-language alignment. It is given as: Lprior = is an embedding from frozen pretrained text encoder. For losses using Lprior, we perform fine-tuning starting from BioCLIPs checkpoint. Table 5 presents the performance of these losses for the image classification task on the iNaturalist-2021 dataset. Notably, incorporating our proposed global entailment objective function significantly enhances models performance compared to using only the local entailment objective. We notice gain of +51.73% in performance when adding our global entailment objective function to Alper et al.s [1] objective function. Furthermore, our cross-modal alignment term outperforms the prior preservation loss. We notice minimum gain of +3.36% when replacing the prior preservation loss with our cross-modal alignment loss to train the vision and text encoder simultaneously. We additionally investigate whether our proposed hard negative mining approach outperforms the random sampling approach. We evaluate the models trained using our negative mining and random sampling approaches on the iNaturalist-2021 and BioCLIP-Rare datasets. Table 6 presents the comparison. We notice that we get performance improvement of +4.61% and +5.62% on both datasets respectively. See appendix for additional ablations. Model CLIPB CLIPB (MERU) CLIPB (ATMG) CLIPB (HyCoCLIP) CLIPB (Radial Emb.) CLIPB (RCME) CLIPL CLIPL (Radial Emb.) CLIPL (RCME) Kendalls τd Precision Recall F1 0.883 0.855 0.981 0.892 0.988 0.991 0.881 0.973 0. 0.335 0.122 0.134 0.124 0.155 0.162 0.151 0.145 0.158 0.142 0.401 0.422 0.451 0.441 0.467 0.343 0.415 0.452 0.199 0.187 0.203 0.194 0.229 0.241 0.209 0.215 0. Table 7. Hierarchical retrieval metrics on HierarCaps dataset. Our objective function results in improved ordering and image-to-text retrieval performance. 5.4. Generalization to HierarCaps To demonstrate the generalizability of our proposed objective function across other domains, we conducted experiments on the HierarCaps dataset [1]. This dataset comprises subset of images from the Conceptual Captions (CC) dataset, each accompanied by captions at four different levels of granularity. We fine-tune the ViT-B/16 and ViT-L/14 variants of the CLIP model on this dataset using our proposed objective function in equation 9 without hard negative mining. Once trained, we compute hierarchical retrieval metrics on the held-out test set of HierarCaps. As evident from Table 7, our model outperforms radial embeddings in both ordering and hierarchical image-to-text retrieval tasks. These results demonstrate the successful application of our objective function in other application domains, enabling the imposition of partial ordering along with entailment in an embedding space. 6. Conclusion In this work, we presented Radial Cross-Modal Embeddings (RCME), framework for learning transitivityenforced entailment in vision-language models. We proposed novel objective function to enable global learning of entailment, which aids in preserving the partial order of concepts. Our framework not only improves cross-modal representations but also intra-modal representations. By leveraging our framework, we proposed hierarchical foundation model for the Tree of Life, outperforming the stateof-the-art. We showed how hierarchical representations can improve the taxonomic classification of species and reveal unusual patterns in the taxonomic classification system, especially in plants. Our future works will focus on using the learned hierarchical representations to understand and comprehend species evolution, identify distinctive anomalies within the Tree of Life, and devise strategies to enhance the taxonomic classification system. 7. Acknowledgements and This research used the TGI RAILs advanced comsupported pute by resource which (award OACthe National Institute. 2232860) Taylor Geospatial Science the Foundation data and is"
        },
        {
            "title": "References",
            "content": "[1] Morris Alper and Hadar Averbuch-Elor. Emergent visualsemantic hierarchies in image-text representations. European Conference on Computer Vision, 2024. 2, 4, 5, 6, 7, 8 [2] Ivana Balazevic, Carl Allen, and Timothy Hospedales. Multi-relational poincare graph embeddings. Advances in Neural Information Processing Systems, 32, 2019. 2 [3] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European conference on computer vision (ECCV), pages 456473, 2018. 2 [4] Ines Chami, Zhitao Ying, Christopher Re, and Jure Leskovec. Hyperbolic graph convolutional neural networks. Advances in neural information processing systems, 32, 2019. 2 [5] Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, and Christopher Re. Low-dimensional hyarXiv preprint perbolic knowledge graph embeddings. arXiv:2005.00545, 2020. 2 [6] Jun Chen, Ming Hu, Darren Coker, Michael Berumen, Blair Costelloe, Sara Beery, Anna Rohrbach, and Mohamed Elhoseiny. Mammalnet: large-scale video benchmark for In Promammal recognition and behavior understanding. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1305213061, 2023. 1, 2 [7] Theresa Chen and Yao-Yi Chiang. Mitree: Multi-input transformer ecoregion encoder for species distribution modelling. In Proceedings of the 7th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery, pages 110120, 2024. 2 [8] Yuxiao Chen, Jianbo Yuan, Yu Tian, Shijie Geng, Xinyu Li, Ding Zhou, Dimitris Metaxas, and Hongxia Yang. Revisiting multimodal representation in contrastive learning: from patch and token embeddings to finite discrete tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1509515104, 2023. 9 [9] Sanghyuk Chun, Wonjae Kim, Song Park, and Sangdoo Yun. Probabilistic language-image pre-training. arXiv preprint arXiv:2410.18857, 2024. 2 [10] Rangel Daroya, Elijah Cole, Oisin Mac Aodha, Grant Van Horn, and Subhransu Maji. Wildsat: Learning satellite image representations from wildlife observations. arXiv preprint arXiv:2412.14428, 2024. 2 [11] Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and Shanmukha Ramakrishna Vedantam. HyperIn International Conferbolic image-text representations. ence on Machine Learning, pages 76947731. PMLR, 2023. 2, 5, 6, 7 [12] Bhuwan Dhingra, Christopher Shallue, Mohammad Norouzi, Andrew Dai, and George Dahl. Embedding text in hyperbolic spaces. In Proceedings of the Twelfth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-12), pages 5969, New Orleans, Louisiana, USA, 2018. Association for Computational Linguistics. 2 [13] Francesc Esteva, Lluıs Godo, Ricardo Rodrıguez, and Thomas Vetterlein. Logics for approximate and strong entailments. Fuzzy Sets and Systems, 197:5970, 2012. 3 [14] Octavian Ganea, Gary Becigneul, and Thomas Hofmann. Hyperbolic entailment cones for learning hierarchical embeddings. In International conference on machine learning, pages 16461655. PMLR, 2018. 2, 3, [15] ZeMing Gong, Austin Wang, Xiaoliang Huo, Joakim Bruslund Haurum, Scott C. Lowe, Graham W. Taylor, and Angel Chang. CLIBD: Bridging vision and genomics for biodiversity monitoring at scale. In The Thirteenth International Conference on Learning Representations, 2025. 1, 2 [16] Lukas Haas, Michal Skreta, Silas Alberti, and Chelsea Finn. In Proceedings of Pigeon: Predicting image geolocations. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1289312902, 2024. 1 [17] Andy Huynh, Lauren Gillespie, Jael Lopez-Saucedo, Claire Tang, Rohan Sikand, and Moises Exposito-Alonso. Contrastive ground-level image and remote sensing pretraining improves representation learning for natural world In European Conference on Computer Vision, imagery. pages 173190. Springer, 2024. 2 [18] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. 5, 6, 7, 2 [19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. conference on machine learning, pages 49044916. PMLR, 2021. 2 [20] Matt Le, Stephen Roller, Laetitia Papaxanthos, Douwe Kiela, and Maximilian Nickel. Inferring concept hierarchies from text corpora via hyperbolic embeddings. arXiv preprint arXiv:1902.00913, 2019. [21] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34:96949705, 2021. 2 [22] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang Tang. Fine-grained semantically aligned vision-language pre-training. Advances in neural information processing systems, 35:72907303, 2022. 2 [23] M. Maruf, Arka Daw, Kazi Sajeed Mehrab, Harish Babu Manogaran, Abhilash Neog, Medha Sawhney, Mridul Khurana, James P. Balhoff, Yasin Bakis, Bahadir Altintas, Matthew J. Thompson, Elizabeth G. Campolongo, Josef C. Uyeda, Hilmar Lapp, Henry L. Bart, Paula M. Mabee, Yu Su, Wei-Lun Chao, Charles Stewart, Tanya Berger-Wolf, Wasila Dahdul, and Anuj Karpatne. Vlm4bio: benchmark dataset to evaluate pretrained vision-language models for trait discovery from biological images, 2024. 2 [24] Marco Mistretta, Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Andrew Bagdanov. Cross the gap: Exposing the intra-modal misalignment in clip via modality inversion. arXiv preprint arXiv:2502.04263, 2025. 2 [25] Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni, Si Yong Yeo, and Jun Liu. Animal kingdom: large and diverse dataset for animal behavior understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1902319034, 2022. 1, 2 [26] Maximillian Nickel and Douwe Kiela. Poincare embeddings for learning hierarchical representations. Advances in neural information processing systems, 30, 2017. 2 [27] Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. In International conference on machine learning, pages 3779 3788. PMLR, 2018. [28] Avik Pal, Max van Spengler, Guido Maria DAmely di Melendugno, Alessandro Flaborea, Fabio Galasso, and Pascal Mettes. Compositional entailment learning for hyperbolic vision-language models. The Thirteenth International Conference on Learning Representations, 2025. 2 [29] Laura Pollock, Justin Kitzes, Sara Beery, Kaitlyn Gaynor, Marta Jarzyna, Oisin Mac Aodha, Bernd Meyer, David Rolnick, Graham Taylor, Devis Tuia, et al. Harnessing artificial intelligence to fill global shortfalls in biodiversity knowledge. Nature Reviews Biodiversity, pages 117, 2025. 1 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 5, 6, 7 [31] Sameera Ramasinghe, Violetta Shevchenko, Gil Avraham, and Ajanthan Thalaiyasingam. Accept the modality gap: An exploration in the hyperbolic space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2726327272, 2024. 2, 5, 6, 7 Pier-Luigi Buttigieg, Olivier De Clerck, Claudia Delgado, Daniel Distel, et al. Accelerating ocean species discovery and laying the foundations for the future of marine biodiversity research and monitoring. Frontiers in Marine Science, 10:1224471, 2023. 1 [33] Srikumar Sastry, Subash Khanal, Aayush Dhakal, Di Huang, and Nathan Jacobs. Birdsat: Cross-view contrastive masked autoencoders for bird species classification and mapping. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 71367145, 2024. 2 [34] Srikumar Sastry, Subash Khanal, Aayush Dhakal, Adeel Ahmad, and Nathan Jacobs. Taxabind: unified embedding space for ecological applications. In Winter Conference on Applications of Computer Vision. IEEE/CVF, 2025. 1, 2, 5, 6, [35] Fanny Simoes, Charles Bouveyron, and Frederic Precioso. Deepwild: Wildlife identification, localisation and estimation on camera trap videos using deep learning. Ecological Informatics, 75:102095, 2023. 2 [36] Samuel Stevens, Jiaman Wu, Matthew Thompson, Elizabeth Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila Dahdul, Charles Stewart, Tanya BergerWolf, et al. Bioclip: vision foundation model for the tree of life. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1941219424, 2024. 1, 2, 5, 6, 7 [37] Jong-Chyi Su and Subhransu Maji. Semi-supervised learning with taxonomic labels. arXiv preprint arXiv:2111.11595, 2021. 1 [38] Alexandru Tifrea, Gary Becigneul, and Octavian-Eugen Ganea. Poincare glove: Hyperbolic word embeddings. arXiv preprint arXiv:1810.06546, 2018. 2 [39] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 87698778, 2018. 1, 2, 5 [40] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-embeddings of images and language. arXiv preprint arXiv:1511.06361, 2015. [41] Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara Beery, and Grant Van Horn. Inquire: natural world textto-image retrieval benchmark, 2024. 2 [42] Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate Jones, Oisin Mac Aodha, Sara Beery, and Grant Van Horn. Inquire: natural world text-to-image retrieval benchmark. Advances in Neural Information Processing Systems, 37:126500126514, 2025. 1 [43] Vicente Vivanco Cepeda, Gaurav Kumar Nayak, and Mubarak Shah. Geoclip: Clip-inspired alignment between locations and images for effective worldwide geolocalization. Advances in Neural Information Processing Systems, 36, 2023. 1 [32] Alex David Rogers, Hannah Appiah-Madson, Jeff Ardron, Nicholas Bax, Punyasloke Bhadury, Angelika Brandt, [44] Ziwei Wang, Sameera Ramasinghe, Chenchen Xu, Julien Monteil, Loris Bazzani, and Thalaiyasingam Ajanthan. Learning visual hierarchies with hyperbolic embeddings. arXiv preprint arXiv:2411.17490, 2024. 2 [45] Chih-Hsuan Yang, Benjamin Feuer, Talukder Jubery, Zi Deng, Andre Nakkab, Md Zahid Hasan, Shivani Chiranjeevi, Kelly Marshall, Nirmal Baishnab, Asheesh Singh, et al. Biotrove: large curated image dataset enabling ai for biodiversity. Advances in Neural Information Processing Systems, 37:102101102120, 2025. 1, 2, 5, 6, 7 [46] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783, 2021. 2 [47] Tao Yu, Toni JB Liu, Albert Tseng, and Christopher De Sa. Shadow cones: generalized framework for partial order embeddings. arXiv preprint arXiv:2305.15215, 2023."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Proof for Lemma 1 Lemma 1 states that finer-grained concepts are progressively projected: 1) away from the entailment root and 2) into smaller subregions in transitivity-enforced entailment. We begin with the definition of distance in an entailment configuration: Case 2: Euclidean Geometry In Euclidean geometry, the entailment root is considered to be the origin (a vector of zeros). This means T0 = 0. Textual embeddings in this geometry are unnormalized and can have arbitrary norms. The distance of textual embeddings in this geometry is simply the L-2 norm. Using equation 6, we get the following expressions: Ξ(T , ) = arccos (cid:32) (T j T0), (T T0.T i ) (cid:33) (10) where , is an inner product between the embeddings. The distance between two textual embeddings are computed with respect to the entailment root. In an entailment configuration with transitivity, the following property is satisfied [14] between parent and its child: Ξ(T , j+1) ψ(T ) π/2 (11) This means that Ξ(T , j+1) [0, π/2]. It follows: , j+1 j+1. cos θ j , T j j j T j+1 (20) (21) (22) In Euclidean geometry, the norms of the embeddings increase with increasing ranks. In both geometries, we can conclude that the distance of textual embeddings monotonically increase with increasing ranks. This leads to the following expression for the distance of an embedding from the root: r(T j+1, T0) r(T , T0) (23) (24) (cid:32) 0 arccos (cid:33) (T j T0), (T T0.T T0), (T T0.T j+1 ) j+1 j+1 ) j+1 (T (cid:32) 0 π (12) r(T , T0) = (i, j; T0) (cid:33) 1 (13) where is monotonically increasing function with respect to the rank and is the distance function. Now let, the aperture angle of cone defined at each textual embedding have the following expression (as done in [14]): Simplifying the above equation, we get the following expressions: 0 (T 0 j T0), (T j+1 + j , j+1 , T0 j ) (14) j+1, T0 j , (15) Case 1: Radial Geometry In radial geometry, all textual embeddings lie on unit hypersphere. As result, the inner product between any two embeddings can never exceed the value of 1. As result, we get the following expressions: , j+1 1 + 1 + T j+1, T0 j+1, T0 j+1, T0 j , T0 j , T0 1 , T0 0 j+1, T0 T , T0 (16) (17) (18) (19) As can be seen from equation 19 , the cosine similarity bej and T0 is always greater than that of tween j+1 and T0. This means the distance of j+1 and T0 is always greater than that of and T0. 1 ψ(T ) arcsin(1/r(T , T0)) (25) The above expression establishes the relation between the aperture angle of cone defined at some textual embedding and its semantic granularity. From the expression, it is evident that the aperture angle monotonically decreases with increasing which defines its semantic granularity. Hence, we can conclude that fine-grained concepts are progressively projected into smaller subregions. The proof is complete. B. Implementation Details All our models are based on the ViT-B/16 architecture and use the OpenCLIP implementation in PyTorch. For training, we use learning rate of 1e7 with OneCycleLR scheduler and the Adam optimizer. We use batch size of 32 and accumulate gradient batches of 2. We use 2 NVIDIA H100 GPUs with the Distributed Data Parallel training strategy. We train for single epoch. We found training for larger number of epochs hindered the performance of the model Kingdom # Samples Phylum Class Family Order Genus Species Average Fungi Plantae Animalia 3410 42710 53880 68.09 92.17 84.73 38.24 37.01 72.86 31.40 15.82 73. 23.78 30.35 55.68 63.84 66.34 68.25 73.05 74.45 70.41 49.73 52.69 70.89 Table 8. Zero-shot classification performance for each distinct kingdom class present in the iNaturalist-2021 dataset. UMAP visualization. We show additional UMAP visualizations of textual embeddings from the models in Figure 5. D. Additional Ablations In Table 9, we show the performance of our global objective function with varying margins (see equation 6 in the main paper). We see that our objective functions performance improves with increasing margins. α π/2 π/4 π/8 0 Kendalls τd"
        },
        {
            "title": "Precision Recall",
            "content": "F1 0.991 0.990 0.990 0.990 0.162 0.152 0.154 0.151 0.467 0.467 0.470 0.454 0.241 0.229 0.232 0.226 Table 9. Hierarchical retrieval metrics on HierarCaps dataset with varying margins (α) in our global entailment objective. Additionally, we assess our models performance in the ordering task by varying the number of retrieval steps in the embedding space. Tables 10 and 11 present the results. Reducing retrieval steps improves precision, but negatively affects recall. The ordering performance remains consistent, as expected. especially in the fine-grained taxonomic ranks like genus and species. We fixed the value of β to 0.1 and 1.0 for the model trained from BioCLIPs [36] and OpenCLIPs [18] checkpoints respectively. For our global entailment objective, we set the margin α to π/2. C. Experimental Setup Below we describe the details of the experiments done in the main paper. Ordering of taxonomic labels. We use the same setup as Alper et al. [1]. We sample 50 equally spaced points from the entailment root to the closest textual embedding to given query image in the embedding space. At each point, we retrieve textual embedding from database which is closest to the given image embedding. We define radius equivalent to the distance between the points for retrieving relevant embeddings at each level. We compute the Kendalls Correlation Coefficient (τd) to evaluate the quality of ordinal association among the retrieved embeddings. Similarly, we compute precision and recall metric relative to the seven ranks of ground-truth taxonomic labels. Zero-shot image classification. For each evaluation dataset, we first create database of unique textual embeddings for each rank of the taxonomy. For given rank, we compute the top-1 recall/accuracy metric on image to text retrieval task. Unlike the ordering task, we compute the accuracy metric for each taxonomic rank independently. From the experiments, we notice that the performance of the models does not decrease monotonically with increasing ranks of the taxonomy. In Table 8, we present kingdom-wise performance of our model. We notice that classification performance of plants especially in the family and order ranks is abnormally low. We believe this is due to highly similar traits and mislabeling of plant species in these ranks. Note that in this experiment, we create independent database of textual embeddings for each kingdom. Image-to-image retrieval. In this experiment, we retrieve images of species with given taxonomic label at given rank using query image. For an evaluation dataset, we precompute the embeddings for each of the images. Subsequently, we retrieve images by calculating the cosine similarity between the query image embedding and the precomputed image embeddings. We compute the recall metric (R@1). 2 Steps Kendalls τd"
        },
        {
            "title": "Precision Recall",
            "content": "F1 10 20 30 40 50 0.993 0.993 0.993 0.993 0.993 0.527 0.491 0.493 0.455 0.458 0.472 0.552 0.618 0.568 0.572 0.498 0.520 0.548 0.505 0. Table 10. Hierarchical retrieval metrics on iNaturalist-2021 dataset with varying number of retrieval steps. Steps Kendalls τd"
        },
        {
            "title": "Precision Recall",
            "content": "F1 10 20 30 40 50 0.991 0.991 0.991 0.991 0.991 0.224 0.190 0.174 0.165 0.162 0.344 0.419 0.450 0.465 0.467 0.271 0.261 0.251 0.244 0. Table 11. Hierarchical retrieval metrics on HierarCaps dataset with varying number of retrieval steps. Figure 5. UMAP Visualization of Textual Embeddings. The visualizations show our model has successfully imparted partial order in the textual embeddings."
        }
    ],
    "affiliations": [
        "Washington University in St. Louis"
    ]
}