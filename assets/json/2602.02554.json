{
    "paper_title": "BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation",
    "authors": [
        "Jingwen Xu",
        "Yiyang Lu",
        "Zisu Huang",
        "Changze Lv",
        "Xiaohua Wang",
        "Shizheng Li",
        "Zhibo Xu",
        "Zhengkang Guo",
        "Zhengyuan Wang",
        "Muzhao Tian",
        "Xuanjing Huang",
        "Xiaoqing Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, a self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs a back-translation strategy: a documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the model's performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with a 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity."
        },
        {
            "title": "Start",
            "content": "BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation Jingwen Xu * 1 Yiyang Lu * 1 Zisu Huang 1 Changze Lv 1 Xiaohua Wang 1 Shizheng Li 1 Zhibo Xu 1 Zhengkang Guo 1 Zhengyuan Wang 1 Muzhao Tian 1 Xuanjing Huang 1 Xiaoqing Zheng 1 6 2 0 2 0 3 ] . [ 1 4 5 5 2 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Training LLMs for code-related tasks typically depends on high-quality code-documentation pairs, which are costly to curate and often scarce for niche programming languages. We introduce BatCoder, self-supervised reinforcement learning framework designed to jointly optimize code generation and documentation production. BatCoder employs back-translation strategy: documentation is first generated from code, and then the generated documentation is used to reconstruct the original code. The semantic similarity between the original and reconstructed code serves as an implicit reward, enabling reinforcement learning to improve the models performance both in generating code from documentation and vice versa. This approach allows models to be trained using only code, substantially increasing the available training examples. Evaluated on HumanEval and MBPP with 7B model, BatCoder achieved 83.5% and 81.0% pass@1, outperforming strong open-source baselines. Moreover, the framework demonstrates consistent scaling with respect to both training corpus size and model capacity. 1. Introduction The interaction of natural language and programming languages has been core focus in software engineering research. Key directions include generating executable code from natural language specifications (Haiduc & Marcus, 2008; Gulwani, 2011; Chen et al., 2021) and producing natural language documentation from source code (McBurney, 2015; Jiang et al., 2017; Iyer et al., 2016). These bidirectional transformations bridge human intent and machine1College of Computer Science and Artificial gence, Fudan University, Shanghai, China. dence Yiyang Huang <xqzheng@fudan.edu.cn>. IntelliCorresponJingwen Xu <xujw24@m.fudan.edu.cn>, Xuanjing Zheng Lu <xjhuang@fudan.edu.cn>, <yylu24@m.fudan.edu.cn>, Xiaoqing to: Preprint. February 4, 2026. 1 executable logic, enabling rapid prototyping and code synthesis (Austin et al., 2021; Hendrycks et al., 2021; Zhuo et al., 2024), while facilitating automated commenting and legacy code maintenance (Stolee et al., 2014; Wan et al., 2018). Overall, they enhance developer productivity and software maintainability across the development lifecycle. Code-related tasks have been studied for over decade. Early approaches relied on information retrieval techniques (Ye et al., 2016) as well as statistical models (Iyer et al., 2016; Wan et al., 2018). Recent advances in LLMs have substantially improved bidirectional generation between natural language and programming code. Foundation models such as GPT-5 (OpenAI, 2025), Qwen3 (Team, 2025), Gemini 2.5 (DeepMind, 2025), and Claude 3.7 (Anthropic, 2025) demonstrate strong code understanding and generation capabilities. In parallel, code-specialized models, including CodeLLaMA (Roziere et al., 2023), DeepSeek-Coder V2 (Zhu et al., 2024), and Qwen3-Coder (Team, 2025), have emerged and achieved strong performance across wide range of code-related benchmarks (Elnaggar et al., 2021; Chen et al., 2021; Austin et al., 2021; Zhuo et al., 2024). However, training LLMs for code-documentation alignment and transformation tasks, such as code description and documentation generation, typically relies on large collections of high-quality code-documentation pairs. Despite the abundance of raw source code in public repositories such as GitHub, such paired supervision remains limited and uneven in quality, which in turn constrains the scalability and generalization of models on these tasks. To address this bottleneck, prior work has explored various data augmentation strategies. For example, WizardCoder (Luo et al., 2023) iteratively evolves instructions to construct diverse training pairs, while Magicoder (Wei et al., 2024b) synthesizes coding problems from unlabeled code snippets. Gao et al. (2024) employs self-supervised pseudo-labeling on unlabeled code. Although effective, these approaches typically require stronger external models to synthesize documentation or pseudo-labels, preventing the target model from leveraging the same mechanism for self-improvement. Moreover, once such code-documentation pairs are constructed, model optimization is usually carried out using conventional suBatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation pervised fine-tuning (SFT) or reinforcement learning (RL) paradigms, where the generated documentation is treated as fixed supervision rather than being explicitly evaluated or optimized with respect to the training objective. To overcome these constraints, we revisit the problem from self-supervised perspective and explore whether meaningful training signals can be derived directly from unlabeled code via model generation and feedback. key observation is that well-formed documentation should preserve sufficient information to enable faithful reconstruction of the code, and effective code generation should adhere to the requirements outlined in the documentation. Consequently, the reconstructed code is expected to be similar with the original code. This relationship provides natural foundation for learning code-documentation transformations without relying on explicit paired data. This observation naturally motivates back-translation learning paradigm, in which documentation is generated from code and then used to regenerate code, with the similarity between the original and reconstructed code serving as an implicit supervisory signal. Building on this back-translation strategy, we propose BatCoder, self-contained framework that learns code description and code generation jointly from unlabeled code snippets. Given code snippet, the model first generates natural language document, which is then used to reconstruct the original code. The similarity between the original and reconstructed code provides unified training signal that serves two complementary purposes: assessing the quality of the generated documentation and guiding the code generation process. Recent advances in code similarity metrics, such as CSSG (Xu et al., 2026), make this design practically feasible. These similarity-based signals are incorporated as rewards within reinforcement learning algorithm, enabling the joint optimization of both generation stages without relying on external documentation or stronger teacher models. Our main contributions are summarized as follows: We propose, self-supervised back-translation framework, named BatCoder, which enables bidirectional generation learning between code and documentation without relying on externally curated paired data, alleviating the scarcity of code-documentation supervision. We empirically demonstrate that BatCoder exhibits favorable scaling behavior with respect to both model capacity and training data size, showing that reconstruction-based self-supervision provides increasingly effective learning signals as scale increases. Through extensive experiments across multiple programming languages, we show that BatCoder consistently improves performance on both code generation and documentation production tasks, with particularly strong gains in low-resource languages. 2. Related Work LLMs for Code. Modern LLMs, having absorbed massive collections of natural language and code during training, exhibit impressive capabilities in various coding tasks, such as code generation (Chen et al., 2021; Zhuo et al., 2024; Jain et al., 2024), code summarization (Lu et al., 2021; Sun et al., 2025; Su et al., 2025) and program repair (Xia et al., 2023; Jiang et al., 2023; Jimenez et al., 2024). Specialized models, such as CodeT5+ (Wang et al., 2023), CodeLlama (Roziere et al., 2023), DeepSeek-Coder (Guo et al., 2024), StarCoder2 (Lozhkov et al., 2024) and Qwen2.5-Coder (Hui et al., 2024), undergo dedicated pre-training or fine-tuning on large-scale code corpora to build robust code generation and understanding capabilities. Data Augmentation for Code-Related Tasks. Despite abundant raw code data, high-quality code-text pairs for model training remain scarce, motivating data augmentation techniques to construct task-specific supervision. WizardCoder (Luo et al., 2023) iteratively evolves code-text pairs by increasing instruction complexity, incorporating constraints, and introducing adversarial elements to improve data quality. Magicoder (Wei et al., 2024b) synthesizes coding problems from unlabeled code snippets by composing code fragments and leveraging strong LLM to ensure task coherence. Similarly exploiting unlabeled code, Gao et al. (2024) generates pseudo-labels using pre-trained models and filters low-quality instances via normalized edit distance. Closely related to our work, UniCoder (Sun et al., 2024) also seeks to align source code with its semantic meaning by introducing structured intermediate representations, such as rule-constrained pseudo-code or documentation as latent bridge between code understanding and generation. However, the quality of these intermediate representations is primarily ensured through hand-crafted rules or external stronger model judgments. Another closely related work is SelfCodeAlign (Wei et al., 2024a), which also explores leveraging only the base model and unlabeled code snippets. Specifically, it extracts coding concepts from seed functions, generates instructionresponse pairs via in-context learning, and validates the responses through self-generated tests and sandbox execution. However, their approach mainly serves as data augmentation strategy, as the model is trained using supervised fine-tuning on these execution-validated pairs. In contrast, our method employs self-contained bidirectional transformation between code and documentation. Rather than relying on external supervision, we evaluate the quality of generated documentation by measuring how faithfully the original code can be reconstructed through back-translation process. This back-translation similarity then serves as the primary learning signal for both directions, 2 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation Figure 1. BatCoder training pipeline via self-supervised back-translation. Given an unlabeled code snippet c, the model first generates multiple documentation candidates in Stage 1 (code-to-documentation). After extracting content and filtering for structural validity, each valid candidate is used to sample single reconstructed code in Stage 2 (documentation-to-code). Rewards consist of code similarity and document format compliance, allocated differently to the two stages. Both directions are jointly optimized via Reinforce++ algorithm. enabling joint optimization without reliance on rule-based heuristics or stronger external models. Reinforcement Learning in Code Generation. Reinforcement learning enhances code generation by leveraging execution feedback to optimize model performance on various programming tasks. CodeRL (Le et al., 2022) integrates pre-trained models with actor-critic structure, employing unit test rewards to bolster functional correctness. Similarly, PPOCoder (Shojaee et al., 2023) advances this paradigm via Proximal Policy Optimization (PPO), incorporating rewards from compiler feedback, syntactic AST similarity, semantic DFG matching, and KL-divergence regularization to align generations with target codes. Ye et al. (2025) enhances code generation by applying process supervision, providing denser step-wise rewards to mitigate sparse outcome signals. Complementing these, advancements in math and code reasoning(Liu et al., 2025) synergize supervised fine-tuning with reinforcement learning, leveraging feedback from dual domains to enhance logical capabilities. Our framework extends these approaches by deriving rewards from code reconstruction similarity within back-translation process, eliminating the need for target code and facilitating reward propagation to the code-todocumentation stage, alongside an evaluation-train separation mechanism that alleviates memory overheads in conventional RL setups. 3. Methods In this section, we present the formulation and training procedure of BatCoder. We first define the problem setting and learning objectives, followed by the sampling strategy used during training, the reward design based on reconstruction similarity, and the reinforcement learning algorithm for parameter optimization. BatCoder treats code description and code generation as back-translation process, where documentation is generated from code and subsequently used for code reconstruction. Prior studies have shown that reconstruction-based similarity provides an effective signal for assessing code-documentation alignment (Allamanis et al., 2024; Sharma, 2024). Building on this insight, we use the similarity between the original code and its reconstruction as the core learning signal to jointly optimize both transformation stages. Figure 1 presents an overview of the BatCoder training process. 3.1. Problem Formulation Given code snippet C, where denotes the space of code, the BatCoder framework trains the model in two sequential stages. The transformation from code to documentation is referred to as Stage 1, in which the model generates descriptive document = fθ(c) D, where denotes the space of natural language documentation. The inverse transformation from documentation back to code is referred to as Stage 2, where the model reconstructs code snippet = gθ(d) C. The composition of Stage 1 and Stage 2, formalized as gθ fθ : C, defines reconstruction objective that implicitly regularizes both code generation and documentation synthesis through structural faithfulness. The model parameters θ are optimized to maximize the expected reward over 3 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation the two-stage transformation: where K. J(θ) = Ecp(c) (cid:2)R(c, d, c)(cid:3) , (1) 3.3. Reward Design where evaluates syntactic and semantic similarity among c, d, and c, and p(c) denotes the empirical distribution of the training code corpus. Based on the two-stage formulation, BatCoder is trained by constructing complete back-translation trajectories from code to documentation and back to reconstructed code, and jointly optimizing both stages using reinforcement learning. Each trajectory starts from an unlabeled code snippet and yields learning signals through reconstruction similarity, allowing the model to improve both documentation generation and code reconstruction without external supervision. We next describe how training trajectories are sampled, followed by the reward formulation and the reinforcement learning algorithm used for parameter optimization. 3.2. Sampling Strategy During training, BatCoder adopts an asymmetric sampling strategy for the two stages. In Stage 1, given an input code snippet, we sample documentation candidates to account for the inherent diversity of natural language descriptions. In Stage 2, each selected documentation instance is used to generate only single reconstructed code sample. The prompts used in Stage 1 are provided in Appendix A. This asymmetric design treats Stage 1 and Stage 2 as single continuous rollout. For each training code snippet, we generate complete trajectories from code to documentation and back to reconstructed code. Each trajectory provides rewards for both stages, ensuring that model updates reflect the dual objectives of producing valid documentation and improving code reconstruction quality. Sampling single reconstruction per documentation instance in Stage 2 maintains balanced number of trajectories for both stages and additionally reduces computational and memory overhead. Before entering Stage 2, the documentation generated in Stage 1 is filtered and rewritten according to the predefined format and validity constraints. The filtering and rewriting criteria are described in Appendix B. Formally, the asymmetric sampling strategy produces two sets of trajectories. In Stage 1, for each input code snippet c, we sample set of documentation trajectories Tcode2doc(c) = {(c, dk)}K k=1, (2) Each documentation instance dk is then subjected to the filtering and rewriting process. For documentation instances that satisfy the validity constraints, second-stage trajectory is constructed in Stage 2 as Tdoc2code(c) = {(dm, m)}M m=1, (3) 4 To enable self-supervised optimization in the absence of paired supervision, we design rewards that reflect both codelevel similarity and documentation quality. Since codelevel similarity can only be evaluated after reconstructing code from documentation, we first define the reward for Stage 2, and then derive the reward for Stage 1 based on reconstruction outcomes and documentation validity. Documentation-to-Code Reward. For Stage 2, the primary learning signal comes from the similarity between the original code and its reconstruction generated from the documentation. Let S(, ) denote code-level similarity function that measures the similarity between two code snippets. This function is abstract and can be instantiated using different structural or semantic comparison metrics. In our experiments, we instantiate using CSSG (Xu et al., 2026), code similarity metric based on an improved program dependence graph (PDG). CSSG produces scores in the range [0, 1], where higher values indicate stronger semantic and structural similarity between code snippets. For each trajectory (dm, m) Tdoc2code(c) generated in Stage 2, we define documentation-to-code similarity reward by comparing the reconstructed code with the original input code c: doc2code = R(m) R(m) sim,doc2code = S(c, m). (4) No additional validity constraints are imposed at this stage, as the reconstruction directly reflects whether the documentation preserves the essential program semantics. Code-to-Documentation Reward. For each trajectory (c, dk) Tcode2doc(c) generated in Stage 1, we compute similarity-based reward by comparing the original code with its reconstruction: sim,code2doc = R(k) R(k) sim,doc2code = S(c, k). (5) In addition to reconstruction similarity, we incorporate documentation validity reward R(k) doc . Beyond reflecting the structural correctness and well-formedness of the generated documentation, this reward is designed to stabilize the documentation generation process during training by encouraging consistent adherence to the required documentation format and reducing variance in early-stage optimization. The auxiliary validity reward is defined as follows: R(k) doc = 0, 0.5, 1, incapable of code generation, contains redundant content, perfectly fits the required format. (6) BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation Algorithm 1 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation Input: Unlabeled code corpus C, model parameters θ, replay buffer B, iterations , documentation samples Initialize θ, for = 1 to do Sample code snippet Sample documentation candidates {d(1), . . . , d(K)} πθ(d c) for = 1 to do Apply filtering and rewriting to d(k), and compute documentation validity reward R(k) doc if d(k) fails validity constraints then code2doc Set R(k) Store (c, d(k), R(k) continue code2doc) in end if Sample reconstructed code c(k) πθ(c d(k)) Compute similarity reward: sim = S(c, c(k)) R(k) Set rewards: R(k) R(k) doc sim R(k) code2doc = R(k) doc2code = R(k) sim Store (c, d(k), R(k) code2doc) in Store (d(k), c(k), R(k) doc2code) in end for Sample minibatches from Update θ using policy gradients for both directions end for Output: Optimized model parameters θ The final reward for Stage 1 is defined at the trajectory level: code2doc = R(k) R(k) sim,code2doc R(k) doc. (7) where R(k) struction is available. sim,code2doc is only computed when valid reconIn practice, the documentation generated in Stage 1 is first subjected to the filtering and rewriting process described in Section 3.2. If documentation instance dk fails to satisfy the predefined validity constraints, it is excluded from the subsequent documentation-to-code stage and no reconstruction is performed. Such trajectories terminate at (c, dk) and do not contribute to the Stage 2 optimization. Since no valid reconstruction is available in this case, the code-todocumentation reward is explicitly defined as R(k) code2doc = 0. 3.4. Reinforcement Learning Algorithm To optimize BatCoder, we adopt Reinforce++ (Hu, 2025), policy gradient algorithm that supports on-policy updates for improved sample efficiency. Training proceeds over trajectories generated by the bidirectional code-documentation back-translation process described in Section 3.2. The reward for the Stage 2, denoted as Rdoc2code, and the reward for the Stage 1, denoted as Rcode2doc, are computed as de5 fined in Section 3.3. Samples generated during training are stored in fixed-size replay buffer B, including codeto-documentation pairs (c, dk) with their corresponding rewards R(k) m) with rewards R(m) code2doc, and documentation-to-code pairs (dm, doc2code. Model parameters θ are updated by sampling minibatches from B. To stabilize training, rewards are normalized using statistics computed over the buffer, yielding the following token-level advantage estimates: A(k) code2doc = R(k) code2doc µcode2doc σcode2doc A(m) doc2code = R(m) doc2code µdoc2code σdoc2code , , (8) (9) where µcode2doc, µdoc2code and σcode2doc, σdoc2code denote the mean and standard deviation of the corresponding rewards maintained in B. The training objective jointly optimizes both generation directions. Specifically, the loss function is defined as follows: L(θ) = ET (cid:2)A(k) (cid:2)A(m) doc2code log πθ(c code2doc log πθ(dk c)(cid:3) dm)(cid:3) ET + β KL(πθ πref), (10) where the KL regularization term constrains updates with respect to reference policy πref. This on-policy optimization strategy enables efficient reuse of past trajectories and promotes stable learning across both stages of the cycle. The overall procedure is summarized in Algorithm 1. 4. Experimental Setup 4.1. Base Models and Training Data Qwen2.5-3B-Instruct and Qwen2.5-7B-Instruct (Team, 2024) were used as the base models. These models are widely adopted in recent code-related studies and allow us to evaluate the robustness of our approach across different model scales. All experiments were initialized from the corresponding pretrained checkpoints released on the Hugging Face platform. For training, we used the code data from the Code-Text task in the CodeXGLUE benchmark suite1 (Husain et al., 2019; Lu et al., 2021). This dataset contains code samples from multiple programming languages, including Python, Ruby, and Go, making it well suited for training on unified corpus and evaluating generalization across languages. Although the original dataset includes accompanying text fields, these texts primarily consist of function-level docstrings rather 1https://huggingface.co/datasets/google/ code_x_glue_ct_code_to_text BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation Table 1. Pass@1 (%) results on HumanEval(+) and MBPP(+). Results for Qwen2.5-Instruct and BatCoder are obtained using the bigcode-evaluation-harness framework, while all other baseline results are retrieved from the EvalPlus Leaderboard. BatCoder consistently improves upon its base model and surpasses open-source baselines with comparable or even larger parameter scales. Scale Model Unknown GPT-4o O1 Mini CodeT5+ StarCoder2 CodeLlama-Instruct WizardCoder-Python Magicoder-S-DS DeepSeek-Coder-Instruct Qwen2.5-Instruct BatCoder (Ours) StarCoder2 DeepSeek-Coder-Instruct Qwen2.5-Instruct BatCoder (Ours) >6B 3B Size Unknown Unknown 16B 15B 34B 34B 6.7B 33B 7B 7B 3B 1.3B 3B 3B Benchmark Open-Source HumanEval (+) MBPP (+) 92.7 (87.2) 96.3 (89.0) 31.7 (26.8) 46.3 (37.8) 51.8 (43.9) 73.2 (64.6) 76.8 (71.3) 81.1 (75.0) 81.7 (73.2) 83.5 (76.8) 31.7 (27.4) 65.9 (60.4) 73.8 (68.3) 76.2 (71.3) 87.6 (72.2) 93.1 (78.8) 56.6 (47.1) 78.0 (65.1) 69.3 (56.3) 75.1 (63.2) 79.4 (69.0) 80.4 (70.1) 78.6 (68.0) 81.0 (69.3) 57.4 (47.4) 65.3 (54.8) 73.0 (62.2) 75.9 (66.4) than the structured documentation targeted in our setting. Consequently, we discarded the text component and used only the code snippets during training. 4.2. Baselines We compare BatCoder against diverse set of representative baselines, including widely adopted code-oriented LLMs that are pre-trained or fine-tuned on large-scale code corpora. The open-source baselines include CodeT5+ (Wang et al., 2023), CodeLlama (Roziere et al., 2023), WizardCoder (Luo et al., 2023), Magicoder (Wei et al., 2024b), StarCoder2 (Lozhkov et al., 2024), DeepSeek-CoderInstruct (Guo et al., 2024), and Qwen2.5-Instruct (Hui et al., 2024). In addition, we report results from two closed-source models, GPT-4o and O1 Mini, which currently represent the state-of-the-art on the EvalPlus leaderboard2 (Liu et al., 2023) and serve as upper-bound references. For each model family, we evaluate variants whose parameter scales or reported performance levels are closest to those of BatCoder, enabling fair and meaningful comparison across comparable settings. All results are consistently drawn from the EvalPlus leaderboard. 4.3. Benchmarks and Evaluation Protocol We evaluate model performance on HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), two widely adopted benchmarks for assessing code generation quality. HumanEval and MBPP consists of Python programming problems with unit tests for execution-based evaluation. To more 2https://evalplus.github.io/leaderboard. html thoroughly assess model performance, we further report results on HumanEval+ and MBPP+, the rigorized versions from EvalPlus (Liu et al., 2023) that augment the original benchmarks with 80 and 35 more test cases. In addition to Python-centric evaluation, we further examine model performance on programming languages with comparatively smaller training corpora, where the scarcity of paired code-documentation data is more pronounced. To this end, we include MultiPL-E (Cassano et al., 2022) for evaluation and focus on Ruby and Go, which are also covered in the CodeXGLUE training data. MultiPL-E extends HumanEval and MBPP by providing translated versions of these benchmarks in multiple programming languages. We used bigcode-evaluation-harness framework (Ben Allal et al., 2022) for all the evaluations, which standardizes test execution and pass@1 computation across benchmarks. We employ greedy decoding (i.e., temperature = 0) with maximum generation length of 1024 tokens. 4.4. Training Settings The 3B and 7B BatCoder models are trained with Reinforce++, implemented on modified version of the verl (Sheng et al., 2024) framework (v0.5.0.dev). Experiments are conducted on two NVIDIA A100 GPUs. We set the number of documentation samples to = 8, with maximum response length of 1500 tokens. The training batch size is 64, the actor learning rate is 1 106, and the mini-batch size is 32. Although standard Reinforce++ incorporates KL regularization to prevent excessive policy deviation, we do not apply an explicit KL constraint in practice, as it was observed to overly restrict policy updates in 6 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation our experimental setting. This can equivalently be viewed as setting the KL coefficient hyperparameter β to 0. We additionally adopt dynamic sampling strategy (Yu et al., 2025), where minibatches with zero rewards for all trajectories are discarded and excluded from parameter updates. For ablation experiments with SFT, we use the default configuration, with batch size of 16, learning rate of 1105, and maximum sequence length of 2048. 5. Results and Discussion 5.1. Python Documentation-to-Code Generation Table 1 summarizes the main results on HumanEval, HumanEval+, MBPP and MBPP+. At the 7B scale, BatCoder achieves consistent gains over the base Qwen2.5-Instruct model, with pass@1 improving from 81.7 to 83.5 on HumanEval and from 73.2 to 76.8 on HumanEval+. The same trend holds on the MBPP benchmarks, increasing from 78.6 to 81.0 on MBPP and from 68.0 to 69.3 on MBPP+. Notably, the 7B BatCoder model outperforms the substantially larger DeepSeek-Coder-Instruct (33B) on HumanEval, HumanEval+, and MBPP, demonstrating the effectiveness of the proposed self-supervised training signal beyond merely increasing model size. Similar trends are observed at smaller scales. At the 3B scale, BatCoder consistently outperforms the corresponding Qwen2.5-Instruct baseline, achieving pass@1 gains of +2.4 on HumanEval, +3.0 on HumanEval+, +1.1 on MBPP, and +4.2 on MBPP+. The presence of improvements at both 3B and 7B scales indicates that BatCoder generalizes across different model capacities, rather than being tailored to specific parameter regime. 5.2. Multilingual Code Generation Table 2. Pass@1 (%) results on MultiPL-E for low-resource programming languages, including Ruby and Go. We compare the base Qwen2.5-Instruct models with BatCoder at different model scales, observing consistent and substantial improvements across both programming languages and model sizes. Model Size Ruby Qwen2.5-Instruct BatCoder (Ours) Qwen2.5-Instruct BatCoder (Ours) 7B 7B 3B 3B 3.1 13. 0.0 10.6 Go 34.4 39.0 33.8 37.7 Table 3. Pass@1 (%) ablation results on Ruby from MultiPL-E. We compare SFT, partial variants of BatCoder, and the full framework, with the full model achieving the best performance. Training Setting Pass@1 Base Model SFT BatCoder (w/o Stage 1) BatCoder 0.0 6.2 1.9 10.6 ining whether BatCoder can not only strengthen existing code capabilities but also elicit non-trivial performance in previously underrepresented languages. Table 2 reports the evaluation results. BatCoder consistently outperforms the corresponding Qwen2.5-Instruct baselines across both Ruby and Go, with markedly larger gains on Ruby. In particular, at the 3B scale, the base model attains pass@1 of 0.0 on Ruby, indicating complete failure to solve any test instances, whereas BatCoder raises performance to 10.6. This substantial jump from zero to non-trivial accuracy highlights the effectiveness of the proposed framework in extremely low-resource regimes. similar but less extreme pattern is observed at the 7B scale on Ruby, where BatCoder improves pass@1 from 3.1 to 13.0, yielding an absolute gain of nearly 10 points. On Go, where the base models already exhibit moderate performance, BatCoder still delivers consistent improvements at both model scales, increasing pass@1 from 33.8 to 37.7 for the 3B model and from 34.4 to 39.0 for the 7B model. Taken together, these results demonstrate that BatCoder generalizes effectively across model scales and is particularly well suited to low-resource programming languages. By leveraging code-level similarity as self-supervised signal, the framework enables meaningful performance improvements in settings where curated code-documentation pairs are scarce or unavailable, suggesting promising applicability to broader range of low-resource code domains. 5.3. Ablation Study We conduct ablation experiments on Ruby from MultiPL-E to better understand the contribution of different components in BatCoder, with particular focus on the role of the Stage 1 and its associated reward. The base model used in these experiments is Qwen2.5-3B-Instruct. We further conducted experiments on MultiPL-E for Ruby and Go, two languages for which high-quality codedocumentation resources are comparatively scarce. Compared to Python, obtaining paired data for these languages is more challenging under conventional training paradigms, and the low baseline scores suggest that the base models may have limited exposure to these languages during training. This setting provides meaningful testbed for examEffect of the Code-to-Documentation Stage We examine the impact of optimizing documentation generation by disabling parameter updates associated with Stage 1, while preserving Stage 2 optimization based on code-level similarity. In this setting, the model still produces documentation, but it is no longer guided by an explicit reinforcement signal and does not contribute to parameter updates. As reported 7 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation (a) Stage 1 mean reward curve (b) Stage 2 mean reward curve (c) pass@1 results of MultiPL-Ruby Figure 2. Training dynamics of BatCoder. All curves show consistent upward trend, indicating that the reward signals are aligned with the training objectives and correlate with improved model performance. in Table 3, this variant leads to only marginal improvement over the base model, with pass@1 increasing from 0.0 to 1.9, and remains far below the performance achieved by the complete BatCoder. This observation highlights the importance of directly optimizing documentation generation. When documentation is not encouraged to preserve sufficient information for faithful code reconstruction, the resulting self-supervised signal becomes substantially weaker, limiting the effectiveness of subsequent code generation optimization. To illustrate the differences between documentation generated before and after BatCoder training, we provide case study in Appendix C. Comparison with Supervised Fine-Tuning. We further compare BatCoder against SFT baseline to assess whether the observed gains can be attributed solely to exposing the model to additional synthetic data. In this setting, documentation is generated by the base model, with the same filtering and rewriting procedures applied as in BatCoder, and resampling performed until valid documents are obtained. The model is then fine-tuned using these documents as inputs and the original code as targets, following conventional SFT paradigm. While SFT improves performance over the base model, it remains notably inferior to the complete BatCoder. This suggests that training on model-generated pseudo pairs is insufficient. Instead, explicitly evaluating and reinforcing documentation quality through code-level similarity provides more effective learning signal. 5.4. Training Dynamics and Reward Analysis To analyze the training dynamics induced by the proposed reinforcement learning formulation, we track the evolution of the mean rewards for Stage 1 and Stage 2 throughout training, together with the downstream Pass@1 performance on MultiPL-Ruby. All curves are obtained from the training trajectories of BatCoder-3B on the Ruby subset. Figure 2 summarizes these dynamics across training checkpoints. As shown in Figures 2a and 2b, the mean rewards for both stages increase steadily as training progresses, exhibiting smooth upward trends. Figure 2c reports the corresponding Pass@1 scores on MultiPL-Ruby, which also improve consistently over training steps and follow similar progression pattern to the reward curves. Overall, the consistent alignment between reward trajectories and downstream evaluation performance suggests that the proposed reward design provides stable and meaningful optimization signal. The bidirectional reinforcement learning objective effectively guides both documentation generation and code reconstruction, resulting in tangible improvements in code generation quality. Although our analysis is conducted at fixed training scale, the observed training dynamics indicate that the framework has the potential to further benefit from larger training data. 6. Conclusion and Future Work We present BatCoder, self-supervised framework that jointly learns code generation and documentation synthesis through back-translation paradigm. By enforcing consistency between code and documentation under bidirectional transformations, BatCoder enables effective optimization directly from unlabeled code, without requiring paired supervision or external teacher models. Experimental results show that BatCoder consistently improves code generation performance across multiple benchmarks, outperforming supervised and synthetic-data-based baselines at comparable model scales. Notably, the proposed framework is particularly effective in low-resource programming languages, such as Ruby and Go, where curated codedocumentation pairs are scarce. These findings indicate that back-translation similarity provides robust learning signal in data-constrained settings, facilitating meaningful improvements from unlabeled code alone. Several directions are promising for future work. One direction is to incorporate more diverse reward signals and evaluate them under alternative reinforcement learning algorithms to assess the robustness of similarity-based rewards. Another avenue is to further investigate scaling behavior 8 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation along multiple dimensions, including larger training corpora, increased model capacity, and alternative architectural choices, to better understand the regimes in which BatCoder is most effective. In addition, the proposed framework may be extended to related tasks such as code completion or code translation. The limitations are discussed in Appendix D."
        },
        {
            "title": "Accessibility",
            "content": "We have made efforts to ensure that this submission is as accessible as possible to broad audience, including readers with disabilities or sensory and neurological differences."
        },
        {
            "title": "Software and Data",
            "content": "We have shown all the prompts in the Appendix, and we will release the core code of our approach on Github upon acceptance."
        },
        {
            "title": "Impact Statement",
            "content": "This work focuses on self-supervised learning for code generation and documentation synthesis via back-translation learning process. We demonstrate consistent improvements on standard benchmarks, low-resource programming languages, and across varying data and model scales. We do not think our work will negatively impact ethical aspects or future societal consequences."
        },
        {
            "title": "References",
            "content": "Allamanis, M., Panthaplackel, S., and Yin, P. Unsupervised evaluation of code llms with round-trip correctness. In Proceedings of the 41st International Conference on Machine Learning, pp. 10501066, 2024. Anthropic. Claude 3.7 Sonnet, 2025. URL https://www. anthropic.com/news/claude-3-7-sonnet. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Ben Allal, L., Muennighoff, N., Kumar Umapathi, L., Lipkin, B., and von Werra, L. framework for code generation models. https://github.com/bigcode-project/ bigcode-evaluation-harness, 2022. evaluation of the Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. DeepMind, G. Gemini 2.5, 2025. URL https://blog. google/technology/google-deepmind/ gemini-model-thinking-updates-march-2025/. Elnaggar, A., Ding, W., Jones, L., Gibbs, T., Feher, T., Angerer, C., Severini, S., Matthes, F., and Rost, B. Codetrans: Towards cracking the language of silicons code through self-supervised deep learning and high performance computing. arXiv preprint arXiv:2104.02443, 2021. Gao, S., Mao, W., Gao, C., Li, L., Hu, X., Xia, X., and Lyu, M. R. Learning in the wild: Towards leveraging unlabeled data for effectively tuning pre-trained code models. In 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE), pp. 969981. IEEE, 2024. Gulwani, S. Automating string processing in spreadsheets using input-output examples. ACM Sigplan Notices, 46 (1):317330, 2011. Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., Chen, G., Bi, X., Wu, Y., Li, Y., et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Haiduc, S. and Marcus, A. On the use of domain terms in source code. In 2008 16th IEEE International Conference on Program Comprehension, pp. 113122. IEEE, 2008. Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. Hu, J. Reinforce++: simple and efficient approach arXiv preprint for aligning large language models. arXiv:2501.03262, 2025. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Husain, H., Wu, H.-H., Gazit, T., Allamanis, M., and Brockschmidt, M. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019. Cassano, F., Gouwar, J., Nguyen, D., Nguyen, S., PhippsCostin, L., Pinckney, D., Yee, M.-H., Zi, Y., Anderson, C. J., Feldman, M. Q., et al. Multipl-e: scalable and extensible approach to benchmarking neural code generation. arXiv preprint arXiv:2208.08227, 2022. Iyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L. Summarizing source code using neural attention model. In 54th Annual Meeting of the Association for Computational Linguistics 2016, pp. 20732083. Association for Computational Linguistics, 2016. 9 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation Jain, N., Gu, A., Li, W.-D., Yan, F., Zhang, T., Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., et al. Codexglue: machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664, 2021. Jiang, N., Liu, K., Lutellier, T., and Tan, L. Impact of code language models on automated program repair. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 14301442. IEEE, 2023. Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568, 2023. Jiang, S., Armaly, A., and McMillan, C. Automatically generating commit messages from diffs using neural machine translation. In 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 135146. IEEE, 2017. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. Swe-bench: Can language models resolve real-world github issues? In 12th International Conference on Learning Representations, ICLR 2024, 2024. Le, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. H. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35: 2131421328, 2022. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:21558 21572, 2023. Liu, Z., Yang, Z., Chen, Y., Lee, C., Shoeybi, M., Catanzaro, B., and Ping, W. Acereason-nemotron 1.1: Advancing math and code reasoning through sft and rl synergy. arXiv preprint arXiv:2506.13284, 2025. Lozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., Liu, T., Tian, M., Kocetkov, D., Zucker, A., Belkada, Y., Wang, Z., Liu, Q., Abulkhanov, D., Paul, I., Li, Z., Li, W.-D., Risdal, M. L., Li, J., Zhu, J., Zhuo, T. Y., Zheltonozhskii, E., Dade, N. O. O., Yu, W., Krauss, L., Jain, N., Su, Y., He, X., Dey, M., Abati, E., Chai, Y., Muennighoff, N., Tang, X., Oblokulov, M., Akiki, C., Marone, M., Mou, C., Mishra, M., Gu, A., Hui, B., Dao, T., Zebaze, A. R., Dehaene, O., Patry, N., Xu, C., McAuley, J. J., Hu, H., Scholak, T., Paquet, S., Robinson, J., Anderson, C. J., Chapados, N., Patwary, M., Tajbakhsh, N., Jernite, Y., Ferrandis, C. M., Zhang, L., Hughes, S., Wolf, T., Guha, A., von Werra, L., and de Vries, H. Starcoder 2 and the stack v2: The next generation. ArXiv, abs/2402.19173, 2024. URL https://api.semanticscholar. org/CorpusID:268063676. McBurney, P. W. Automatic documentation generation via source code summarization. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, volume 2, pp. 903906. IEEE, 2015. OpenAI. Gpt-5 system card, 2025. URL https://cdn. openai.com/gpt-5-system-card.pdf. Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Sharma, A. Patched rtc: evaluating llms for diverse software development tasks. arXiv preprint arXiv:2407.16557, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Shojaee, P., Jain, A., Tipirneni, S., and Reddy, C. K. Execution-based code generation using deep reinforcement learning. arXiv preprint arXiv:2301.13816, 2023. Stolee, K. T., Elbaum, S., and Dobos, D. Solving the search for source code. ACM Transactions on Software Engineering and Methodology (TOSEM), 23(3):145, 2014. Su, C.-Y., Bansal, A., Huang, Y., Li, T. J.-J., and McMillan, C. Context-aware code summary generation. Journal of Systems and Software, pp. 112580, 2025. Sun, T., Chai, L., Yang, J., Yin, Y., Guo, H., Liu, J., Wang, B., Yang, L., and Li, Z. Unicoder: Scaling code large language model via universal code. arXiv preprint arXiv:2406.16441, 2024. Sun, W., Miao, Y., Li, Y., Zhang, H., Fang, C., Liu, Y., Deng, G., Liu, Y., and Chen, Z. Source code summarization in the era of large language models. In 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE), pp. 18821894. IEEE, 2025. Team, Q. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/ blog/qwen2.5/. 10 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation Zhuo, T. Y., Zebaze, A., Suppattarachai, N., von Werra, L., de Vries, H., Liu, Q., and Muennighoff, N. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024. Team, Q. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Wan, Y., Zhao, Z., Yang, M., Xu, G., Ying, H., Wu, J., and Yu, P. S. Improving automatic source code summarization via deep reinforcement learning. In Proceedings of the 33rd ACM/IEEE international conference on automated software engineering, pp. 397407, 2018. Wang, Y., Le, H., Gotmare, A., Bui, N., Li, J., and Hoi, S. Codet5+: Open code large language models for code understanding and generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 10691088, 2023. Wei, Y., Cassano, F., Liu, J., Ding, Y., Jain, N., Mueller, Z., de Vries, H., Von Werra, L., Guha, A., and Zhang, L. Selfcodealign: Self-alignment for code generation. Advances in Neural Information Processing Systems, 37: 6278762874, 2024a. Wei, Y., Wang, Z., Liu, J., Ding, Y., and Zhang, L. Magicoder: empowering code generation with oss-instruct. In Proceedings of the 41st International Conference on Machine Learning, pp. 5263252657, 2024b. Xia, C. S., Wei, Y., and Zhang, L. Automated program repair in the era of large pre-trained language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pp. 14821494. IEEE Computer Society, 2023. Xu, J., Lu, Y., Lv, C., Huang, Z., Guo, Z., Wang, Z., Tian, M., Huang, X., and Zheng, X. Cssg: Measuring code similarity with semantic graphs. arXiv preprint arXiv:2601.04085, 2026. Ye, X., Shen, H., Ma, X., Bunescu, R., and Liu, C. From word embeddings to document similarities for improved In Proinformation retrieval in software engineering. ceedings of the 38th international conference on software engineering, pp. 404415, 2016. Ye, Y., Zhang, T., Jiang, W., and Huang, H. Processsupervised reinforcement learning for code generation. arXiv preprint arXiv:2502.01715, 2025. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Zhu, Q., Guo, D., Shao, Z., Yang, D., Wang, P., Xu, R., Wu, Y., Li, Y., Gao, H., Ma, S., et al. Deepseek-coderv2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. 11 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation A. Prompt Templates This appendix presents the prompt templates used for documentation sampling in Stage 1. The prompts are designed to elicit structured and well-formed documentation that can be reliably used for subsequent reconstruction in Stage 2. We adopt one-shot prompting strategy, providing single format-aligned example to encourage the model to consistently follow the desired documentation structure. A.1. Python Documentation Generation Prompt Please analyze the code provided at the end and reverse-engineer it to create python code generation instruction. The output must be wrapped in <doc> and </doc> tags and include: 1. Any necessary library imports. 2. The Python function definition line with type annotations. 3. Indented by 4 spaces, docstring starting with \"\"\" that includes description of what the code does. 4. Indented by 4 spaces, one or several illustrative input/output examples (using >>> syntax) inside the docstring. 5. Indented by 4 spaces, the closing \"\"\" of the docstring. Here is an example of the expected structure (pay attention to the format, not the content): <doc> import json def test_func(arg1: str, arg2: int) -> bool: \"\"\" test_func implements the functionality of ... >>> test_func(example, 1) True \"\"\" </doc> Now, please generate the python code problem statement within <doc> and </doc >: {origin_lan} {code} A.2. Ruby Documentation Generation Prompt Please analyze the code provided at the end and reverse-engineer it to create ruby code generation instruction. The output must be wrapped in <doc> and </doc> tags and include: 1. Any necessary require statements. 2. descriptive problem statement comment explaining what the code does. 3. One or several illustrative input/output comment line (using >>> syntax). 4. The Ruby function definition line matching the logic (needs to complete generating the leading spaces of the next line). Here is an example of the expected structure (pay attention to the format, not the content): 12 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation <doc> require json # test_func implements the functionality of ... # >>> test_func(arg1, arg2) # expected_result def test_func(arg1, arg2) </doc> Now, please generate the ruby code problem statement within <doc> and </doc>: {origin_lan} {code} A.3. Go Documentation Generation Prompt Please analyze the code provided at the end and reverse-engineer it to create go code generation instruction. The output must be wrapped in <doc> and </doc> tags and include: 1. The package declaration and relevant imports. 2. descriptive problem statement comment explaining what the code does. 3. One or several illustrative input/output comment line (using >>> syntax). 4. The Go function definition line matching the logic (needs to complete generating the leading tab of the next line). Here is an example of the expected structure (pay attention to the format, not the content): <doc> package main import \"fmt\" // test_func implements the functionality of ... // >>> test_func(arg1, arg2) // expected_result func test_func(arg1 Type, arg2 Type) Type { </doc> Now, please generate the go code problem statement within <doc> and </doc>: {origin_lan} {code} B. Filtering and Rewriting This appendix describes the filtering and rewriting procedure applied to the documentation generated in Stage 1 before it is passed to Stage 2. The purpose of this process is to retain only structurally valid and semantically informative documentation, ensuring that reconstruction-based rewards are computed on reliable inputs under the back-translation setting. B.1. Documentation Structure Constraints To enforce consistent documentation format, we apply set of structural constraints using regular-expression-based matching. These constraints are aligned with the prompt specification and the one-shot example provided in Appendix A, ensuring consistency between the expected documentation format and the filtering criteria. 13 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation documentation instance is considered valid if it contains: (i) one or more natural language description lines summarizing the functionality, (ii) at least one illustrative input-output example, and (iii) function or procedure definition corresponding to the described behavior. These constraints are designed to be language-agnostic and capture common documentation patterns across programming languages. B.2. Filtering and Rewriting Procedure The filtering procedure consists of three steps. First, only the content enclosed within designated documentation tags (e.g., <doc> and </doc>) is extracted from the raw model output. Generations without valid tags are discarded. Second, the extracted content is matched against the structural constraints described above. If valid match is found, the documentation is truncated at the end of the matched span, removing any trailing or extraneous content beyond the required structure. This rewriting step is applied only when additional text appears after the end of the structurally valid documentation. In such cases, the instance is marked as containing redundant content, and only the prefix that satisfies the structural constraints is retained. In addition to producing cleaned documentation instance, the filtering process records whether the generated documentation terminates exactly at the matched boundary, which serves as an auxiliary indicator of structural completeness. Only documentation that passes this filtering and rewriting procedure is used as input to Stage 2. C. Case Study: Documentation Quality Comparison To qualitatively analyze the differences before and after BatCoder training, we present case study comparing the generated documentation for the same code snippet after training with the two approaches. The goal of this analysis is to examine whether BatCoder leads to higher-quality and more informative documentation, even though no explicit reward is imposed directly on documentation content. The following Ruby helper function is used as the input code for documentation generation: def sign_in(user = create(:user)) visit new_user_session_path within(#new_user) do fill_in user_email, with: user.email fill_in user_password, with: user.password end click_button Log in end This function implements an end-to-end user login procedure in Rails system test, including navigation to the login page, form filling, and submission. Base Model Generated Documentation. The documentation generated by the base model is shown below. require capybara require capybara/dsl require support/database_cleaner # sign_in generates the login process for user. # >>> sign_in(user: create(:user)) # This will log in user with the specified email and password. def sign_in(user = create(:user)) <end> BatCoder Generated Documentation. In contrast, the documentation produced by BatCoder is as follows: require rails_helper # The sign_in method is used to simulate user signing in to the application. 14 BatCoder: Self-Supervised Bidirectional Code-Documentation Learning via Back-Translation # It visits the login path, fills in the email and password for the given user, # and submits the form by clicking the log in button. # >>> sign_in(create(:user)) # visits the new_user_session_path, fills in email and password, and clicks the Log in button def sign_in(user = create(:user)) <end> Analysis. Under the filtering and rewriting mechanisms introduced in Appendix B, both outputs satisfy the required documentation format and are therefore considered valid. However, their quality differs substantially. The base model produces only coarse and generic description of the function, largely restating that it performs login operation without capturing the procedural structure or key execution steps. In contrast, BatCoder generates more detailed and functionally grounded description that explicitly reflects the control flow of the code, including page navigation, form interaction, and submission behavior. Although BatCoder does not impose an explicit reward on documentation content itself, higher-quality documentation improves the fidelity of the subsequent code reconstruction stage. More precise descriptions enable more accurate backtranslation, which in turn yields higher code-level similarity rewards. This indirect supervision effectively guides the model toward generating more informative and semantically aligned documentation, demonstrating the advantage of the proposed bidirectional reinforcement learning framework over the unaligned base model. D. Limitations While BatCoder demonstrates strong empirical performance across multiple benchmarks and programming languages, we acknowledge several limitations that point toward promising avenues for future work. First, we used the same set of hyperparameters for both 3B and 7B model training without extensive tuning. While this simplifies the experimental setup, our results already demonstrate the effectiveness of BatCoder, suggesting even stronger performance is achievable with tailored hyperparameter optimization. Second, the reward signals in Stage 1 and Stage 2 rely solely on code similarity and documentation formatting. We did not explore alternative similarity metrics, their weighting schemes, or additional reward sources. Nevertheless, the consistent gains across benchmarks indicate substantial headroom for improvement through richer reward design."
        }
    ],
    "affiliations": [
        "Fudan University"
    ]
}