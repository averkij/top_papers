{
    "paper_title": "RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems",
    "authors": [
        "Jingru Lin",
        "Chen Zhang",
        "Stephen Y. Liu",
        "Haizhou Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) mitigates key limitations of Large Language Models (LLMs)-such as factual errors, outdated knowledge, and hallucinations-by dynamically retrieving external information. Recent work extends this paradigm through agentic RAG systems, where LLMs act as agents to iteratively plan, retrieve, and reason over complex queries. However, these systems still struggle with challenging multi-hop questions, and their intermediate reasoning capabilities remain underexplored. To address this, we propose RAGCap-Bench, a capability-oriented benchmark for fine-grained evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs from state-of-the-art systems to identify common tasks and the core capabilities required for their execution, then construct a taxonomy of typical LLM errors to design targeted evaluation questions. Experiments show that \"slow-thinking\" models with stronger RAGCap performance achieve better end-to-end results, underscoring the benchmark's validity and the importance of enhancing these intermediate capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 0 1 9 3 1 . 0 1 5 2 : r RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems Jingru Lin1 Chen Zhang Stephen Y. Liu Haizhou Li1,2 1National University of Singapore, Singapore 2The Chinese University of Hong Kong, Shenzhen, China {jingrulin}@u.nus.edu"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) mitigates key limitations of Large Language Models (LLMs)such as factual errors, outdated knowledge, and hallucinationsby dynamically retrieving external information. Recent work extends this paradigm through agentic RAG systems, where LLMs act as agents to iteratively plan, retrieve, and reason over complex queries. However, these systems still struggle with challenging multi-hop questions, and their intermediate reasoning capabilities remain underexplored. To address this, we propose RAGCap-Bench, capability-oriented benchmark for fine-grained evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs from state-of-the-art systems to identify common tasks and the core capabilities required for their execution, then construct taxonomy of typical LLM errors to design targeted evaluation questions. Experiments show that slow-thinking models with stronger RAGCap performance achieve better end-to-end results, underscoring the benchmarks validity and the importance of enhancing these intermediate capabilities."
        },
        {
            "title": "Introduction",
            "content": "Although large language models (LLMs) demonstrate impressive performance across diverse domains (Grattafiori et al., 2024; Hadi et al., 2023; Xi et al., 2024), their reliance on internal knowledge often leads to factual errors, outdated knowledge, and hallucinations, reducing their reliability on complex and dynamic queries (McKenna et al., 2023; Ji et al., 2023; Huang et al., 2025). These problems are alleviated with Retrieval Augmented Generation (RAG) which equips the LLMs with an external knowledge base (Lewis et al., 2020; Izacard et al., 2023; Gao et al., 2023; Jiang et al., 2023). However, such traditional RAG system still suffers Equal Contribution 1 from problems such as limited access to real-time information, shallow reasoning and context integration etc. (Singh et al., 2025). Recent progress in LLM-based agentic RAG systems presents more promising approach to mitigate these issues and improve task handling capabilities (Li et al., 2025c; Wu et al., 2025; Jin et al., 2025b; Zheng et al., 2025). An agentic RAG system enhances the RAG pipeline by granting the LLM agency, where LLMs interact with open web environments to dynamically retrieve and filter information, reason logically with contexts, and plan adaptively and systematically to answer complex query. Given the recent rapid advancement of agentic RAG systems and their growing complexity (Asai et al., 2024; Yao et al., 2023; Li et al., 2025b; Feng et al., 2025; Zheng et al., 2025; Pan et al., 2025; Rawat et al., 2025; Jin et al., 2025a; Song et al., 2025; Li et al., 2025a), it is becoming crucial to move beyond end-to-end QA-based evaluations (Krishna et al., 2025; Rein et al., 2023; Zhou et al., 2025; Xi et al., 2025) and assess these systems at finer-grained level. Such granular evaluation requires deeper understanding of the intermediate processes that drive overall performance. An agentic RAG system typically engages in multiple rounds of planning, retrieval, and reasoning. Like solving complex math problem, errors in intermediate steps can propagate and degrade the final outcome. To spotlight these critical intermediate tasks and the core model capabilities they require, we introduce RAGCap-Bench, finegrained benchmark for component-level evaluation of common tasks within agentic RAG systems. We identify the common tasks by analyzing several recent open-source agentic RAG systems (Li et al., 2025c; Jin et al., 2025b; Li et al., 2025a) and design four task types, including Planning, Evidence Extraction, Grounded Reasoning and Noise Robustness. The evaluation questions for each task type are derived from taxonomy of typical erBenchmark Language Open Web. Process 2WikiMultiHopQA (2020) MuSiQue (Trivedi et al., 2022) BrowseComp(Zhou et al., 2025) BrowseComp-ZH(Zhou et al., 2025) InfoDeepSeek(Xi et al., 2025) RAGCap-Bench (ours) En En En Zh En&Zh En&Zh Table 1: Comparisons between different benchmarks. rors made by LLMs during intermediate task execution and are presented in Multiple-Choice Question (MCQ) format. We evaluate the performance of both fast-thinking and slow-thinking LLMs on these MCQs and demonstrate that performance on RAGCap-Bench reliably correlates with their end-to-end performance in complex agentic RAG workflows. Additionally, we show that LLM performance on RAGCap-Bench is also indicative of their ability to evaluate intermediate task outputs accurately. Compared to other benchmarks, as shown in Table 1, RAGCap-Bench offers finegrained process-level evaluation, with realistic and diverse information from open web environments. Our key contributions are summarised as follows: RAGCap-Bench is the first comprehensive evaluation benchmark focused on core intermediate tasks common to most agentic RAG workflows, moving beyond end-to-end assessments that provide guidance on which specific model capabilities need improvement. Our experiments show strong correlations between the performance of LLMs on RAGCapBench and the performance on downstream tasks. We also demonstrate that LLMs performing well on RAGCap-Bench exhibit strong capabilities in evaluating intermediate task outputs, and the intermediate evaluation is indicative of final end-to-end performance."
        },
        {
            "title": "2 Problem Formulation",
            "content": "Figure 2 shows the general pipeline of the agentic RAG system. Given user query Q, the LLM first designs its question-answering plan π0 = Plan(q) about how to tackle the question. In the planning task, LLMs are required to break down the question into sub-queries, extract the questions intents, or brainstorm ideas etc., depending on the nature of the query. With the initial plan, the LLM will start to solve the problem, 2 Figure 1: Left: Proportion of samples in each MCQ type. Right: Average number of options in each MCQ type. which requires iterative live searching, reasoning and dynamic planning up to steps. At each step t, the agent retrieves set of top-K evidence Et = {e1, e2, ..., ek} from the open web using search engine. It then analyses the evidence set and identifies useful evidences from the set. Based on the evidences, the LLM starts to reason and produces reasoning chain Rt = {r1, r2, ..., rm} where is the number of reasoning steps. With current evidence and reasoning, the LLM will then refine its plan πt+1 = Plan(qE <t, R<t), in which it determines whether current information is enough to answer the question and makes strategic plan for the next step. If the agent thinks the current evidence and reasoning are enough, it will stop searching and proceed to generate the answer ˆy. Depending on the specific system design, some systems incrementally gather relevant evidence through iterative search and analysis, and then perform final reasoning step over the aggregated evidence set (as indicated by the blue dotted box in Figure 2) before generating the final answer ˆy. The complexity of the agentic RAG system makes it hard to locate the actual problem. For example, extracting wrong evidence could lead to incorrect reasoning, which propagates to later processes. In RAGCap-Bench, we aim to propose capability-oriented evaluations systematically assess on the agentic systems at the process-level. Formally, our goal is to evaluate how well the foundation LLMs strategizes its question-answering plan πt, extract useful relevant evidence from Et and reason Rt faithfully with t. Due to the dynamic nature of the open web environments, the evaluation of the noise robustness of LLMs toward potentially low-quality, misleading and less credible sources of information is also important. In the next section, we will introduce the construction of this benchmark in detail."
        },
        {
            "title": "3 RAGCap-Bench",
            "content": "Our benchmark comprises 255 curated questions. To construct the dataset, we draw queries from multiple open-source deep research benchmarks (Zhou et al., 2025; Xi et al., 2025; Krishna et al., 2025; Chen et al., 2025) spanning diverse domains, including entertainment, sports, arts, technology, and medicine. Since the open-source datasets consist solely of end-to-end query-answer pairs, we utilize various agentic RAG systems to process the queries and collect execution logs. From these logs, we extract the relevant intermediate information, which is then used to generate task-specific MCQs. Figure 1 shows the number of samples and the average number of options in RAGCap-Bench by task type. More details about the open-source datasets and agentic systems can be found in Appendix A."
        },
        {
            "title": "3.1 MCQ Format",
            "content": "Motivated by prior works (Ross et al., 2025; Hendrycks et al., 2020; Zellers et al., 2019), we frame RAGCap-Bench questions as MCQs. Each MCQ consists of query, some intermediate outputs from the agentic RAG systems, and an instruction to ask the LLMs to choose the correct/incorrect options from all options given. The general formats of the MCQs are shown in the bottom of Figure 2. Examples of MCQs are shown in Appendix E."
        },
        {
            "title": "3.2 Dataset Construction",
            "content": "There are four MCQ task types: Planning, Evidence Extraction, Grounded Reasoning and Noise Robustness. The two main MCQs generation strategies include 1) Vanilla Generation; 2) Error-Guided Generation. The former involves directly extracting and reformatting relevant information from execution logs, framing it as MCQs to evaluate the corresponding model capabilities. The latter involves dedicated error-guided prompts to instruct the LLMs to generate high-quality and challenging MCQs. We mainly use GPT-4o (Hurst et al., 2024), Qwen-Plus (Yang et al., 2025) and DeepSeeekV3 (Liu et al., 2024) for our dataset construction. Below shows how the MCQs of each type are constructed: Planning: As shown in Figure 2, planning capability is essential at two key stages within agentic RAG systems. The first occurs when the LLM receives the initial query and begins interpreting it. Based on its understanding of the users intent, the model formulates strategic plan to guide the question-answering process. The second stage arises after the LLM has retrieved search results and conducted some reasoning. At this point, the model must assess what aspects of the query have already been addressed, identify remaining gaps, and dynamically adjust its plan based on the newly acquired information. Depending on the nature of the question and the current information the LLM has gathered, the LLMs might need convergent and divergent planning abilities at any stage. The converging planning ability refers to the ability to narrow down the search space, advancing the reasoning process efficiently. In contrast, divergent planning entails broadening the search space to enhance the diversity and coverage of retrieved information, ensuring that important aspects are not overlooked. In Appendix B, we show the typical use cases of convergent and divergent planning abilities for different types of queries. To curate MCQs for assessing both convergent and divergent planning abilities, we gather relevant thinking trajectories (CoT from the slow-thinking agent) for the two stages of question-answering planning, as mentioned above, from various agentic RAG systems. To ensure the data quality, we only keep the trajectories that lead to the correct final answer. Among all the systems analyzed, WebThinker (Li et al., 2025c) and HiRA (Jin et al., 2025b) are particularly specialized in highlevel question-answering planning. Leveraging this, we prompt LLMs to generate MCQs based on their planning traces. Additionally, since humans excel at both convergent and divergent thinking when solving complex problems, we also collect samples from open-source datasets where humanannotated stepwise problem-solving strategies are available (Chen et al., 2025; Du et al., 2025), and instruct the LLMs to generate MCQs based on these strategies. Appendix provides the detailed prompts used by LLMs to generate MCQs designed to assess the convergent and divergent planning abilities. The above-mentioned high-quality thinking trajectories from both agentic RAG systems and humans make up the correct options in the MCQs. For wrong options, we instruct the LLMs to generate them. Specifically, we employ Error-Guided Generation, which includes the identified common mistakes made by agentic RAG systems during the planning process. The common mistakes made are listed in Table 2. Evidence Extraction: Vanilla Generation is 3 Figure 2: General agentic RAG pipeline and data construction processes for RAGCap-Bench. We take queries from open-source question-answering (QA) datasets. We then run different deep research systems and collect the intermediate outputs. Two main strategies, Vanilla Generation and Error-Guided Generation are deployed to generate the MCQs based on queries and the intermediate outputs. The generated MCQs are filtered to ensure the quality and experts are recruited to provide the answers. adopted for this task. Specifically, we collect the search queries and retrieved web pages in the execution trajectories of different agentic RAG systems and each web page is formatted into an MCQ option, as shown in Figure 2. The MCQs typically include more than four options, and the model being evaluated is tasked with selecting the least relevant evidence to answer the user query. Appendix presents the prompts for the MCQs generation. Grounded Reasoning: We employ both ErrorGuided Generation and Vanilla Generation strategies here. For Error-Guided Generation, we use intermediate outputs that lead to correct final answer. The query, evidence set, the associated reasoning steps from the intermediate outputs and the common mistakes for grounded reasoning as listed in Table 2 are all included in the generation prompts. The LLMs are then instructed to generate MCQs to introduce errors into some statements in the reasoning steps. To facilitate more realistic critique of the actual agentic RAG systems, we also make use of reasoning steps that lead to wrong answers. For these reasoning steps, they already contain errors. Therefore, we only employ Vanilla Generation, which prompts the LLMs to extract statements from the reasoning steps and maximise the diversity of the extracted statements. The prompts for generation using both strategies are shown in Appendix D. Finally, an instruction is added to MCQs asking for the selection of the incorrect statements, enabling the evaluation of the models ability to identify erroneous reasoning. Noise Robustness: Real-time search results may include noise, such as low-quality, less credible, or entirely irrelevant information. During inspection of the execution logs of different agentic RAG systems, we find that they are prone to forcing their reasoning based on the retrieved evidence even though they may contain lots of noise. In this task, we mainly assess whether the models are robust against noisy search results. In the MCQs, we provide the model to evaluate search results that contain no or very limited useful information, and prompt the model with two options: answer the question or admit that the question cannot be answered with the given search results. In this type of question, we deem an LLM that admits the questions are not answerable as noise-robust one. We term the capability as noise-abstain. We also assess whether the LLMs can identify less credible sources. While it is still possible to extract information from less credible sources, it is hard to guarantee the quality of the information. This capability is especially important when answering queries from high-stakes areas such as medicine, laws, academic and policies etc. Furthermore, this capability prioritises more credible sources of information, which might be useful 4 Question Type Targeted Capability Common Mistakes The capability to interpret and break down the problem, implement an optimal/strategic problem-answering plan, and dynamically refine its plan based on current information. Poor question interpretation Omission of necessary constraints Poor planning logic Poor dynamic planning Limited scope and depth The capability to identify useful evidence from large amount of retrieved documents. Shallow keyword matching Fail to recognise implicit connections Planning Evidence Extraction Grounded Reasoning The capability to reason with grounding and generate well-supported statement. Noise Robustness The capability to detect low-quality, less-reliable information and the capability to abstain. Hallucinated support Contradiction with retrieved information Failed to identify implicit reasoning gap Forced/Incorrect reasoning based on noisy information Unable to detect low-quality, misleading, and less credible sources Table 2: Question types, the targeted capabilities, and common mistakes made by LLMs. when multiple sources present conflicting information. Vanilla MCQs Generation is employed to format the MCQs, which ask the LLMs to identify less credible sources from all the MCQ options. We term this capability as noise-reliability."
        },
        {
            "title": "3.3 Dataset Post Processing",
            "content": "Despite employing Error-Guided Generation, LLMs may still generate trivial cases and suffer from issues such as hallucination and poor instruction following. To ensure the quality of the benchmark questions, we apply both difficulty filtering and format filtering. Difficulty filtering removes trivial cases by prompting multiple models to answer the MCQs, and discarding those where all LLMs provide the same answers. Format filtering removes MCQs with poor formatting or ambiguity using combination of rule-based heuristics and manual verification. For more precise evaluations, all MCQs are annotated by human experts equipped with advanced deep research tools. The ground-truth answers to the MCQs is determined by the majority vote among the annotators."
        },
        {
            "title": "3.4 Evaluation Metrics",
            "content": "We adopt both Exact Match (EM) and instancewise macro F1 score as the evaluation metrics. Planning questions contain two sub-categories: converging and diverging. Noise Robustness also contains two sub-categories: abstain and reliability. The EM and F1 scores are first averaged within the group. The overall score is the average of four scores from each question type."
        },
        {
            "title": "4.1 Setups",
            "content": "We evaluate range of fast-thinking LLMs, including Qwen2.5-72B-Instruct (Team, 2024), QwenPlus w/o think (Yang et al., 2025), DeepSeekV3 (Liu et al., 2024), GPT-4o (Hurst et al., 2024) and Gemini-1.5-Pro (Team et al., 2024), and slowthinking LLMs, including Qwen3-8B (Yang et al., 2025), Qwen3-32B w/ think (Yang et al., 2025), QwQ-32B (Yang et al., 2025), Qwen3-235B-A22B w/ think (Yang et al., 2025), DeepSeek-R1 (Guo et al., 2025), O1-mini (Jaech et al., 2024) and Gemini-2.5-Flash (Comanici et al., 2025)."
        },
        {
            "title": "4.2 Benchmark Results",
            "content": "We evaluate the performance of fast-thinking and slow-thinking models across the different task types using both bare and informative prompts. All prompts are shown in Appendix H. Bare prompts refer to those with no error examples included. The LLMs are given the query, MCQs, and simple instructions about the output format. Informative prompts additionally include the few-shot examples about the common mistakes listed in Table 2. The latter shows improved and more robust performance. Table 3 shows the results for using informative prompts. The comparison with bare prompts is shown in Section 4.3. Planning: We present the scores for convergent and divergent planning ability separately in Table 3. The EMc and F1c denote the scores for convergent planning ability. As for divergent planning ability, we only compute EMd. For both planning 5 Model Planning F1c EMd EMc Evidence Extraction Grounded Reasoning EM EM F1 F1 Noise Robustness F1r EMa EMr Overall EM F1 fast-thinking models Qwen2.5-72B-Instruct Qwen-Plus w/o think Deepseek-v3 GPT-4o Gemini-1.5-Pro 27.45 54.88 76.00 31.88 49.02 74.10 84.00 27.54 49.02 74.97 80.00 15.94 35.29 59.48 72.00 34.78 49.02 75.82 76.00 18.84 79.47 74.44 75.07 78.27 75.37 35.85 35.85 41.51 50.94 37.74 slow-thinking models Qwen3-8B Qwen3-32B w/ think QwQ-32B 45.10 67.52 72.00 18.84 43.14 67.45 68.00 30.43 33.33 65.56 80.00 34.78 Qwen3-235B-A22B w/ think 47.06 70.92 80.00 39.13 52.94 74.38 84.00 36.23 33.33 70.10 64.00 30.40 52.94 72.75 80.00 31. DeepSeek-R1 O1-mini Gemini-2.5-Flash 74.10 62.97 79.05 73.39 81.34 77.05 78.33 49.06 49.06 54.72 56.60 52.83 43.40 47.17 77.98 80.82 81.19 81.89 80.49 83.54 83.95 85.19 88.70 85.89 78.87 83.48 64.86 10.00 77.18 39.22 72.37 64.86 25.00 74.45 43.71 75.95 78.38 25.00 81.01 43.41 78.06 97.30 10.00 65.96 48.26 71.40 54.05 10.00 73.36 37.78 76. 56.76 50.00 83.25 44.96 76.26 59.46 40.00 82.15 46.20 74.13 62.16 35.00 84.32 48.68 78.53 54.05 40.00 82.68 51.57 79.94 70.27 35.00 80.92 52.54 80.63 62.16 20.00 76.14 40.89 75.52 75.68 25.00 75.40 48.97 77.48 Table 3: Performance of different fastand slow-thinking LLMs using informative prompts. EMc and F1c denote the scores for converging ability. EMd denotes the EM score for diverging ability. EMa denotes the EM score for noise-abstain. EMr and F1r denote the scores for noise-reliability. The scores are presented as percentages for clarity. abilities, high EM score demonstrates that the LLM is capable of identifying the optimal solution path to the final answer among all the non-optimal paths. In the agentic RAG systems, implementing good plan not only leads to final answer but also means higher efficiency. For example, with concise problem-solving steps, the systems can avoid redundant search that leads to more useless evidence or repetitively checking on known information. On the other hand, high F1 score indicates that both optimal and non-optimal paths might be chosen. In agentic RAG systems, where iterative reasoning is performed, it is still possible for the system to reach the final solution even if the system takes redundant steps. From Table 3, we find that Gemini and DeepSeek series generally exhibit consistent ability to identify optimal paths among all fast-thinking and slow-thinking models. DeepSeek-V3 and Gemini-1.5-Pro both obtain the highest EMc score of 49.02% among fast-thinking models. DeepSeek-R1 and Gemini-2.5-Flash obtain the highest EMc score of 52.94% among slowthinking models. In terms of F1c, Gemini-1.5-Pro leads with score of 75.82%, closely followed by DeepSeek-v3 with score of 74.97% among fast-thinking models. DeepSeek-R1 obtains an F1c score of 74.38%, followed by Gemini-2.5-Flash, with an F1c score of 72.75% among slow-thinking models. Among the Qwen series, the planning ability seems inconsistent. While Qwen-Plus w/o think achieves the highest EMc and EMd, Qwen3235B-A22B w/ think, despite the large size, does not perform as good. Evidence Extraction: high EM score in evidence extraction shows the LLMs ability to identify the relevant evidence and filter useless evidence precisely. In the agentic RAG system, clean set of evidence provides strong foundation for faithful and accurate reasoning. From Table 3, we see that all fast-thinking and slow-thinking models show low EM scores of less than 40% on this question type, which shows that the LLMs struggle to process information from dynamic open-web environments. The best EM scores achieved are 34.78% by GPT-4o among the fast-thinking models, and 39.13% by Qwen3-235B-A22B w/ think among the slow-thinking models. This weakness can be attributed to the mismatch between pretraining distributions and noisy, unstructured web data, and the limitations in aggregating evidence across sources. However, in the context of agentic RAG systems, it is common for multiple sources to provide overlapping information. In such cases, the omission of some repetitive evidence has trivial impact on the reasoning. Through analysis on the performance of LLMs on this question type, we summarise three common failure cases: a) omission of repetitive evidence; b) omission of key information; c) inclusion of irrelevant information. While a) is trivial mistake, the effects of b) and c) are dependent on the systems planning and noise robustness capabilities. We will elaborate on these interdependencies later in this section. In this sense, F1, which provides partial correctness, captures whether the systems are able to extract at least some relevant information to carry on with their reasoning process. From Table 3, we can see the LLMs are generally able to achieve F1 scores of more than 70%. Among all models, DeepSeek-R1 achieves the best F1 score of 81.34% among all models. Grounded Reasoning: While an F1 score reflects partial correctness in reasoning statements, it does not reflect deeper reasoning failures. mistake made by LLMs in any intermediate reasoning stage can be propagated through subsequent stages of the agentic RAG systems. Conversely, even perfect extraction can be undermined if the models reasoning process is flawed. Therefore, the EM score serves as stronger indicator for flawless reasoning grounded in given evidence. Despite high F1 scores (generally more than 80%) achieved by many LLMs, the EM scores are much lower. Among fast-thinking models, the best EM score of 50.94% is obtained by GPT-4o. Among slow-thinking models, the best EM score is 56.60%, achieved by Qwen3-235B-A22B. Noise Robustness: For noise-abstain, only EM score, denoted as EMa is computed. Our results show that most models are noise robust. When provided with irrelevant information, most LLMs admit that the question is not answerable. On the other hand, most LLMs do not recognise the credibility and reliability of the sources. Most models in Table 3 have an EMr of less than 50% despite high F1r scores. Models such as Qwen2.5-72B-Instruct, GPT-4o and Gemini-1.5-Pro obtain an EMr as low as 10.00%. This suggests concerning trend: many models exhibit tendency to trust retrieved information indiscriminately, regardless of the sources credibility. closer examination of the models reasoning outputs reveals that many models prioritise the content of retrieved web pages over their reliability. For example, the LLMs tend to accept the sources as long as they appear informative, even if they think the sources might come from potentially less credible authority. While this tendency may be relatively safe when information across sources is consistent, it becomes problematic in the presence of conflicting evidence. In such cases, typical approach that humans would involve checking the origin of the sources before drawing the conclusions. Among all models, Qwen3-8B performs surprisingly well in detecting the less credible sources. However, one should note that the Qwen3-8B is also weak model in evidence extraction, which partially explains its good performance Figure 3: RAGCap-Bench overall EM scores for different fast-thinking (left) and slow-thinking models (right), with informative (orange) and bare (blue) prompts. on EMr due to its disengagement with the content. Although we define four capabilities of LLMs in an agentic RAG system, it is important to recognize that these capabilities do not operate in isolation. Instead, they interact synergistically, influencing one another and working together as an integrated process to answer the query effectively. For example, for evidence extraction and noise robustness, the negative impacts of missing out key information and selecting irrelevant information are possibly alleviated if the LLMs are noise robust enough. When conflicts occur between different sources of information, noise-robust LLM that recognises more reliable sources of information can guide the selection of evidence. For planning and noise robustness, concise strategy can lead to fewer irrelevant search results, which relieves the burden of handling the noisy search results."
        },
        {
            "title": "4.3 With vs. Without Error-Guided Prompts",
            "content": "The detailed results for bare prompts (without errorguided exemplars) are presented in Appendix F. Here, we only show comparison between the overall EM scores for the two types of prompts. From Figure 3, it is clear that the use of informative prompts, guided by our error identification in Table 2, leads to improved performance in both fastand slow-thinking LLMs. This observation underscores central challenge in building agentic RAG systems: even slow-thinking LLMs struggle to interact with dynamic and noisy web information without structured guidance. This explains why building robust agentic RAG systems usually requires intensive prompt engineering or other posttraining techniques (Asai et al., 2024; Zhang et al., 2025)."
        },
        {
            "title": "4.4 Correlation with Downstream",
            "content": "A good benchmark for evaluating the intermediate tasks should faithfully reflect their effective7 Evidence Extraction Direct Scoring RAGCAP-Bench Qwen3-8B Qwen3-32B Qwen3-235B Correlation EM F1 0.210 0.113 0.528 18.84 30.43 39.13 74.10 62.97 73.39 Grounded Reasoning Direct Scoring RAGCAP-Bench Qwen3-8B Qwen3-32B Qwen3-235B Correlation EM F1 0.291 0.316 0.338 49.06 49.06 56.60 83.54 83.95 88. Table 4: The second column reports the point-biserial correlation coefficient between the evaluator scores and the downstream performance. All correlation scores are statistically significant (<0.05). The third and fourth columns show corresponding EM and F1 scores on RAGCap-Bench. overall scores are averaged across all the steps in the reasoning trajectory. The evaluation prompts are provided in Appendix G. Due to lack of ground-truth human scores, we approximate the direct evaluation performance using the point-biserial correlation coefficient (Tate, 1954) between the LLM overall scores and the downstream outcomes of the 500 samples, measured as binary output (i.e., whether the final answer is correct or incorrect). We choose three LLMs of varying sizes: Qwen3-8B, Qwen3-32B, and Qwen3-235B. Table 4 presents the comparisons. Notably, the correlation values of the LLMs show consistent trend with their respective performance for the evidence extraction and grounded-reasoning categories on RAGCap-Bench. Qwen3-235Bs evaluator scores have the highest correlation with the results, and it also obtains the best EM score in RAGCap-Bench. Qwen3-8B and Qwen3-32B both have lower EM scores on evidence extraction and grounded reasoning, and hence, they also demonstrate weaker correlation with the downstream task results. Qwen3-8B has slightly stronger correlation than Qwen3-32B on Evidence Extraction, which might be explained by the higher F1 scores in evidence extraction. As discussed in Section 4.2, both EM and F1 scores are significant indicators of the evidence extraction capability."
        },
        {
            "title": "5 Related Work",
            "content": "Agentic RAG Systems: While RAG equips the LLMs with access to external knowledge bases, mitigating issues such as factual errors and halFigure 4: Correlation of performance on RAGCap-Bench with performance on InfoDeepSeek and BrowseComp-Zh, for Qwen3-8B, Qwen3-32B and Qwen3-235B. ness with the end-to-end performance. Therefore, we compare the results of different models on RAGCap-Bench to the downstream questionanswering (QA) tasks. We use three LLMs of varying sizes, including Qwen3-8B, Qwen3-32B, and Qwen3-235B-A22B, and two representative agentic RAG pipelines, including WebThinker (ReACTbased) and HiRA (Multi-Agent-based), yielding total of six configurations. For downstream datasets, we use the QA pairs from BrowseCompZh and InfoDeepSeek. We limit each inference to 10 Google Search API calls. Figure 4 shows the correlation between the performance of these LLMs on RAGCap-Bench and the downstream performance when the LLMs act as the agents in respective pipelines. positive correlation is observed from the figure, showcasing that RAGCap-Bench can reliably serve as an efficient alternative to the more costly and time-consuming end-to-end QA evaluation. Furthermore, the performance of Qwen3-8B and Qwen3-32B, while the former is much smaller in size, are close in RAGCap-Bench, with the overall EM scores of 44.96% and 46.20% respectively. This is also reflected in their downstream performance. From Figure 4, we can see that the performance of Qwen3-8B and Qwen3-32B is close on both datasets for the HiRA workflow, as well as on InfoDeepSeek for the WebThinker workflow."
        },
        {
            "title": "4.5 LLMs as Evaluator",
            "content": "In this section, we examine the relationship between RAGCAP-Bench and direct evaluation of intermediate outputs within the reasoning trajectory. We randomly sample 500 inference trajectories from WebThinker experiments described in Section 4.4 and apply the LLM-as-a-judge approach, prompting different LLMs to score grounded reasoning (thought-action) and evidence extraction (observation-extraction) on 110 scale for each thought-action-observation-extraction step. The 8 it still struggles to solve complex lucinations, tasks (Singh et al., 2025). Agentic RAG has enhanced the capabilities of the traditional RAG systems. It no longer treats the LLM merely as passive text generator, but as an active agent capable of adaptive planning, dynamic information seeking, and iterative reasoning (Asai et al., 2024; Yao et al., 2023; Li et al., 2025b; Feng et al., 2025; Zheng et al., 2025; Pan et al., 2025). Notably, this emerging paradigm has seen increasing adoption in practical applications, including OpenAI Deep Research (OpenAI, 2025), Gemini Deep Research (Gemini, 2025), Perplexity Deep Research (Perplexity Team, 2025) etc., all of which leverage LLMs as autonomous agents. RAG Benchmark: With the rise of agentic RAG systems, comprehensive and systematic benchmarking becomes increasingly important for uncovering potential weaknesses and steering the development of more effective and capable systems. Some benchmarks are proposed for this purpose (Xi et al., 2025; Zhou et al., 2025; Wei et al., 2025). However, most of these efforts focus on question-answering (QA) tasks that evaluate systems ability to answer challenging, multi-hop questions. Although they are useful indicators of end-task performance, they offer limited insight into the intermediate planning, retrieving and reasoning tasks executed by the systems."
        },
        {
            "title": "6 Conclusion",
            "content": "This work introduces RAGCap-Bench, capabilityoriented benchmark designed for fine-grained, component-wise evaluation for the agentic RAG systems. RAGCap-Bench addresses the critical gap in existing benchmarks, which lack evaluation on the intermediate processes of the systems. Experimental results demonstrate that the RAGCapBench scores are correlated with the downstream task performance, highlighting its practical relevance. In addition, we conduct exploratory experiments to show the potential of using LLMs to assess intermediate outputs of the agentic RAG systems. This paves the way for future research into the integration of LLMs as means of improving the agentic RAG systems."
        },
        {
            "title": "References",
            "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations. Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, Chen Sun, Han Hou, Hui Yang, James Pan, Jianan Lou, Jiayi Mao, Jizheng Liu, Jinpeng Li, Kangyi Liu, and 14 others. 2025. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. Preprint, arXiv:2506.13651. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. 2025. Deepresearch bench: comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763. Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi Song, and Hao Wang. 2025. Airrag: Activating intrinsic reasoning for retrieval augmented genarXiv preprint eration using tree-based search. arXiv:2501.10053. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1). Gemini. 2025. Gemini deep research. https://gemini.google/overview/ deep-research/. Accessed: 2025-08-02. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, and 1 others. 2023. survey on large language models: Applications, challenges, limitations, and practical usage. Authorea Preprints. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. ArXiv, abs/2009.03300. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multihop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, Barcelona, Spain (Online). International Committee on Computational Linguistics. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and 1 others. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, 43(2):155. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research, 24(251):143. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM computing surveys, 55(12):138. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 79697992, Singapore. Association for Computational Linguistics. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025a. Search-R1: Training llms to reason and leverage search engines with reinforcement learning. Preprint, arXiv:2503.09516. Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, and Zhicheng Dou. 2025b. Decoupled planning and execution: hierarchical reasoning framework for deep search. arXiv preprint arXiv:2507.02652. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. 2025. Fact, fetch, and reason: unified evaluation of retrievalaugmented generation. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 47454759, Albuquerque, New Mexico. Association for Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeIn Advances in Neural Inforintensive nlp tasks. mation Processing Systems, volume 33, pages 9459 9474. Curran Associates, Inc. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, and 1 others. 2025a. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025b. Search-o1: Agentic search-enhanced large reasoning models. Preprint, arXiv:2501.05366. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. 2025c. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Hosseini, Mark Johnson, and Mark Steedman. 2023. Sources of hallucination by large language models on inference tasks. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 27582774, Singapore. Association for Computational Linguistics. 2025. reOpenAI. https://openai.com/index/ search. introducing-deep-research/. Accessed: 2025-08-02. Introducing deep Melissa Pan, Mert Cemri, Lakshya Agrawal, Shuyi Yang, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Kannan Ramchandran, Dan Klein, and 1 others. 2025. Why do multiagent systems fail? In ICLR 2025 Workshop on Building Trust in Language Models and Applications. Perplexity Team. deep 2025. research. plexity www.perplexity.ai/hub/blog/ introducing-perplexity-deep-research. Accessed: 2025-08-02. Introducing perhttps:// Mrinal Rawat, Ambuje Gupta, Rushil Goomer, Alessandro Di Bari, Neha Gupta, and Roberto Pieraccini. 10 Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, and Yong Yu. 2024. Towards openworld recommendation with knowledge augmentation from large language models. In Proceedings of the 18th ACM Conference on Recommender Systems, pages 1222. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800. Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, and 1 others. 2025. Process vs. outcome reward: Which is better for agentic rag reinforcement learning. arXiv preprint arXiv:2505.14069. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, and 1 others. 2025. Browsecomp-zh: Benchmarking web browsing abilarXiv ity of large language models in chinese. preprint arXiv:2504.19314. 2025. Pre-act: Multi-step planning and reasoning improves acting in llm agents. arXiv preprint arXiv:2505.09970. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. Preprint, arXiv:2311.12022. Hayley Ross, Ameya Sunil Mahabaleshwarkar, and Yoshi Suhara. 2025. When2call: When (not) to call tools. arXiv preprint arXiv:2504.18851. Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. 2025. Agentic retrieval-augmented generation: survey on agentic rag. arXiv preprint arXiv:2501.09136. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. Preprint, arXiv:2503.05592. Robert Tate. 1954. Correlation between discrete and continuous variable. point-biserial correlation. The Annals of mathematical statistics, 25(3):603607. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, and 1 others. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Qwen Team. 2024. Qwen2.5: party of foundation models. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, and 1 others. 2025. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648. Yunjia Xi, Jianghao Lin, Menghui Zhu, Yongzhao Xiao, Zhuoying Ou, Jiaqi Liu, Tong Wan, Bo Chen, Weiwen Liu, Yasheng Wang, and 1 others. 2025. Infodeepseek: Benchmarking agentic information seeking for retrieval-augmented generation. arXiv preprint arXiv:2505.15872. 11 Dataset glaveai/RAG-v1 InfoDeepSeek (Xi et al., 2025) BrowseComp-Zh (Zhou et al., 2025) Frames (Krishna et al., 2025) XBench (Chen et al., 2025) Deep Research Bench (Du et al., 2025) Language Number of Samples En Zh Zh En Zh En&Zh 100 245 289 230 100 Table 5: Datasets used for the construction of RAGCapBench. Note that the original Frames contain 824 samples, we subsample 230 to run the agentic RAG systems on these 230 samples only. In addition, the original Glaveai/RAG-v1 contains 51.4k rows, we select the first 100 rows with longest list of retrieved documents."
        },
        {
            "title": "A Datasets",
            "content": "All open-source datasets used for constructing RAGCap-Bench are listed in Table 5. The agentic RAG systems used include WebThinker (Li et al., 2025c), WebSailor (Li et al., 2025a), WebDancer (Wu et al., 2025) and HiRA (Jin et al., 2025b). We run these systems on InfoDeepSeek, BrowseComp-Zh and Frames to obtain the intermediate outputs. Glaveai/RAG-v11 dataset includes retrieved documents as part of its content. The remaining datasets provide human-annotated stepwise problem-solving strategies, which are used to construct planning questions."
        },
        {
            "title": "B Convergent and Divergent Capabilities",
            "content": "Figure 5 illustrates the roles of convergent and divergent planning capabilities for different types of queries. Convergent planning capability is required to gradually narrow down the search space in order to reach final deterministic answer. Divergent planning capability is required to expand user query to explore multiple perspectives, interpretations and possibilities, so that the final answer is well-rounded and considers diverse viewpoints."
        },
        {
            "title": "C Examples of Errors",
            "content": "Figure 6 shows examples of the actual errors collected from agentic RAG pipelines. The errors are highlighted in red. Figure 5: Convergent (top) and divergent (bottom) planning capabilities required for different user queries. can be adapted as needed for each specific pipeline and dataset. Figure 7 and Figure 8 show the prompts for generating MCQs using Error-Guided Generation. For most questions using Vanilla Generation, except for grounded reasoning, LLM-based generation is not needed. Figure 9 show the prompt for generating grounded reasoning questions using Vanilla Generation with LLM. Other questions generated with Vanilla Generation follow the formats in Figure 2. Here we only show an example of evidence extraction in Figure 10."
        },
        {
            "title": "E Examples of MCQs",
            "content": "Figure 11 and Figure 12 are MCQ examples from RAGCap-Becnh. The full dataset can be assessed at https://github.com/ jingru-lin/RAGCap-Bench."
        },
        {
            "title": "F Results with Bare Prompt",
            "content": "Since different datasets and pipelines generate intermediate outputs with varying structures and formats, the prompts must be customised accordingly. Here, we provide some examples of our generation prompts to illustrate the core idea. These prompts 1https://huggingface.co/datasets/glaiveai/RAG-v1 Table 6 shows the evaluation results on RAGCapBench using bare prompts. G. Evaluation Prompts Figure 13 provides the prompts used for evaluating the intermediate outputs from WebThinker."
        },
        {
            "title": "H Bare and Informative Prompts",
            "content": "Figure 14 and Figure 16 are bare and informative prompts for grounded reasoning. Figure 15 and Figure 17 are bare and informative prompts for noise-reliability. All prompts for evaluation are available in the prompts folder at https://anonymous.4open.science/ r/RAGCap-Bench-5C02/README.md. 13 Figure 6: Error examples from agentic RAG pipelines. 14 Figure 7: Example prompt for generating planning questions using Error-Guided Generation. Figure 8: Example prompt for generating grounded reasoning questions using Error-Guided Generation. 16 Figure 9: Example prompt for generating grounded reasoning questions using Vanilla Generation. Figure 10: Vanilla Generation for evidence extraction. 17 Model Planning F1c EMd EMc Evidence Extraction Grounded Reasoning EM EM F1 Qwen2.5-72B-Instruct Qwen-Plus w/o think Deepseek-v3 GPT-4o Gemini-1.5-Pro Qwen3-8B Qwen3-32B w/ think QwQ-32B Qwen3-235B-A22B w/ think DeepSeek-R1 O1-mini Gemini-2.5-Flash 31.37 27.45 43.14 35.29 52.94 37.25 39.22 19.61 49.02 45.10 21.57 50.98 64.31 67.26 69.93 56.99 74. 63.20 67.84 58.21 72.29 70.33 60.20 74.79 72.00 84.00 80.00 64.00 80.00 72.00 84.00 80.00 80.00 84.00 64.00 80.00 fast-thinking models 28.99 24.64 20.29 30.43 14.49 79.56 77.40 71.93 79.65 75. 39.62 30.19 24.53 50.94 41.51 slow-thinking models 23.19 28.99 28.99 42.03 31.88 23.19 31.88 74.85 57.29 76.70 75.15 79.11 72.27 77.68 39.62 37.74 49.06 52.83 56.60 41.51 39.62 79.98 76.55 69.56 81.29 80. 83.01 84.06 83.24 86.27 88.73 76.72 79.83 Noise Robustness F1r EMr EMa Overall EM 62.16 54.05 34.14 54.05 37.84 37.84 35.14 51.35 56.76 56.76 54.05 78.38 10.00 15.00 15.00 10.00 5.00 15.00 10.00 20.00 5.00 35.00 15.00 15.00 68.05 65.67 61.93 54.69 73.19 75.51 74.92 72.43 55.37 80.03 64.59 72. 39.09 36.27 32.86 40.76 35.97 35.96 37.72 40.88 46.56 49.73 35.50 45.02 72.98 71.72 68.34 68.16 75.98 74.14 71.02 72.64 72.27 79.55 68.45 76.17 Table 6: Performance of different fastand slow-thinking LLMs using bare prompts. EMc and F1c denote the scores for converging ability. EMd denotes the EM score for diverging ability. EMa denotes the EM score for noise-abstain. EMr and F1r denote the scores for noise-reliability. The scores are presented as percentages for clarity. Figure 11: An example from RAGCap-Bench. 18 Figure 12: An example from RAGCap-Bench. 19 Figure 13: Prompt for evaluation on the intermediate outputs from WebThinker. 20 Figure 14: Bare prompt for grounded reasoning. Figure 15: Bare prompt for noise robustness (reliability). 21 Figure 16: Informative prompt for grounded reasoning. 22 Figure 17: Informative prompt for noise robustness (reliability)."
        }
    ],
    "affiliations": [
        "National University of Singapore, Singapore",
        "The Chinese University of Hong Kong, Shenzhen, China"
    ]
}