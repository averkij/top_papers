{
    "paper_title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space",
    "authors": [
        "Lin Li",
        "Zehuan Huang",
        "Haoran Feng",
        "Gengxiong Zhuang",
        "Rui Chen",
        "Chunchao Guo",
        "Lu Sheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 7 4 2 9 1 . 8 0 5 2 : r VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space Lin Li2* Zehuan Huang1* Haoran Feng3 Gengxiong Zhuang1 Rui Chen1 Chunchao Guo4 Lu Sheng1 (cid:12) 1Beihang University 2Renmin University of China 3Tsinghua University 4Tencent Hunyuan Project page: https://huanngzh.github.io/VoxHammer-Page/ Figure 1. High-quality 3D assets edited by our method using text prompts. Our method uses training-free approach to perform percise and coherent 3D local editing, transforming multiple 3D assets in the scene (left) into high-quality results (right). The bottom row shows detailed comparison of each 3D asset before and after editing, as well as the conditioning texts."
        },
        {
            "title": "Abstract",
            "content": "3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, novel training-free approach that performs precise and coherent editing in 3D latent space. Given 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. SubEqual contribution Project lead Corresponding author sequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. 1. Introduction In recent years, the rapid advancement of generative AI has greatly facilitated the creation of 3D assets [32, 47, 53, 62, 90, 97], providing powerful production tools for industries such as gaming, robotics, and VR. Among these, 3D local editing [2, 6, 41, 71, 72, 106] is crucial task that enables partial modification of existing or AI-generated 3D assets while keeping other regions unchanged. It presents challenges in maintaining consistency in preserved regions and ensuring overall coherence in the edited model. Existing 3D editing methods can be broadly categorized into two pipelines. One approach [6, 13, 15, 51, 71, 106] employs Score Distillation Sampling (SDS) [62] to optimize 3D representations so that it aligns with input prompts, but per-scene editing typically takes minutes or even hours. Another approach [1, 5, 20, 26, 42, 63] attempts to edit multi-view images [34, 73] rendered from the 3D model and then reconstruct the 3D model from the modified views. These techniques achieve higher efficiency through feedforward process. However, editing in 2D space instead of 3D space usually introduces position bias in the 3D reconstruction stage, making accurate local editing difficult. In addition, inconsistencies among edited multi-view images [2, 4, 20] lead to artifacts in the edited 3D model, compromising quality and coherence [42, 104]. Recently, advanced 3D generative models [43, 47, 48, 86, 90, 96, 97], trained in native 3D space, can generate high-fidelity 3D content from text or image prompts. These models exhibit significant advantages in 3D consistency and quality, inspiring us to edit 3D assets in native 3D space. However, fine-tuning these models for editing is constrained by critical data bottleneck: large-scale paired datasets for 3D local editing are exceptionally difficult to acquire. Therefore, our research focuses on unleashing the potential of pretrained 3D generative models for precise and coherent 3D editing, eliminating the need for additional training. We propose VoxHammer, training-free framework for precise and coherent 3D editing. Our method is based on pretrained structured 3D latent diffusion model [90], and introduce two-stage process: precise 3D inversion, and denoising and editing based on the inverted latents. Given 3D model (mesh, NeRF [58], or Gaussian Splat [38]), VoxHammer first predicts its inversion trajectory of 3D diffusion process and caches its inverted latents and key-value tokens at each timestep. We demonstrate that the inversion can reconstruct the given models 3D geometry and texture with high precision. In the subsequent denoising and editing phase, we denoise the edited region, and replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, our approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. This is achieved without training the base model and only through feature replacement at inference, allowing high-quality 3D local editing at minimal cost. The lack of labeled editing regions in existing datasets makes it challenging to objectively evaluate consistency in preserved areas. To address this, we constructed Edit3DBench, human-annotated dataset comprising hundreds of samples with carefully labeled 3D editing regions. Quantitative and qualitative experiments on Edit3D-Bench show that VoxHammer significantly outperforms existing methods in terms of both editing accuracy and overall quality. Our training-free method also holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. Our main contributions are summarized as follows: We propose training-free, native 3D local editing framework that leverages pretrained 3D generative model for highly precise and coherent editing. We introduce precise 3D inversion and denoising-based editing using inverted latents, where we replace inverted latents and key-value tokens in the 3D latent space to ensure consistent reconstruction of preserved regions and coherent editing. We built Edit3D-Bench, benchmark to thoroughly validate the superiority of VoxHammer in both editing accuracy and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. 2. Related Work 3D generative models. Recent advances in diffusion models [31, 74] and the availability of high-quality 3D datasets [11, 12] have significantly accelerated the development of 3D generative models [8, 9, 14, 25, 27, 28, 32, 35, 43, 45, 50, 5255, 57, 67, 75, 77, 8084, 86, 89, 91, 95, 97, 101, 103]. Some methods [33, 34, 54, 55, 64, 75, 77, 81, 83, 91] generate 3D models by first synthesizing multi-view images and then reconstructing 3D from these views. But inconsistent multi-view synthesis may lower the quality of the final 3D model. series of methods [10, 16, 43, 44, 46, 47, 49, 76, 8587, 90, 97, 102, 103] train native 3D generative models that comprise of variational autoencoder [39] and diffusion transformer (DiT) [61] for denoising in latent space. These approaches unify 3D generation with high fidelity and consistency, laying the foundation for downstream inversion and editing. 3D editing. Early 3D editing methods [6, 13, 15, 51, 71, 100, 106] employ Score Distillation Sampling (SDS) [62] to optimize 3D representation to align with the input prompts, but the per-scene editing requires minutes or even hours. Figure 2. Pipeline. Given an input 3D model, user-specified editing region, and text prompt, the off-the-shelf models [3, 40] are used to inpaint the rendered view from the 3D model. Subsequently, our VoxHammer, training-free framework based on structured 3D diffusion models [90], performs native 3D editing conditioned on the input 3D and the edited image. Subsequent works [1, 2, 4, 5, 20, 26, 42, 63, 104] attempt to edit multi-view images rendered from the 3D model and reconstruct 3D from the modified views. But the lack of consistency across edited images frequently leads to degraded reconstruction quality. In contrast, our method operates directly in the native 3D space, enabling precise and coherent editing that integrates seamlessly with the overall structure. Image generation and editing. Diffusion models [21, 31, 40, 66, 68, 70, 74] synthesize images through gradual denoising process starting from standard noise. To enable image editing with pretrained image diffusion models, several methods [18, 23, 24, 37, 59, 69, 78, 105] employ inversion techniques, which map real images into the diffusion models denoising trajectory for precise controllable manipulation. Inspired by these approaches, we explore inversion within native 3D generative models [90] and further propose native 3D editing framework. Our method achieves training-free precise and coherent 3D editing through 3D inversion and novel contextual feature replacement. 3. Methodology We propose VoxHammer, training-free framework for 3D local editing, aimed at achieving precise and globally coherent modifications on 3D models. As shown in Fig. 2, given an input 3D model (mesh, NeRF [58], or 3DGS [38]), an editing region and text prompt, our framework first renders view from the 3D model and utilizes advanced image diffusion models [3, 40] to produce an inpainted image. Subsequently, VoxHammer performs native 3D editing conditioned on the input 3D model and the edited image. The illustration of VoxHammer is shown in Fig. 3. We first invert the 3D asset to noise and cache the inverted latents and key-value tokens at each timestep, which are then masked and used to guide the denoising process. We first introduce our base model [90] and inversion design [78] in Sec. 3.1. Then, Sec. 3.2 explores the 3D inversion and validates its consistency in geometry and texture reconstruction. Based on these findings, Sec. 3.3 presents our training-free 3D local editing method through inversion and re-editing. 3.1. Preliminary Structured 3D diffusion models. VoxHammer is based on structured 3D latent diffusion models [88, 90], which are generative models that operate in sparse voxel-based latent space for high-quality and scalable 3D generation. They represent 3D asset (mesh, NeRF [58], or 3DGS [38]) as structured latents (SLAT), i.e., set of local latent vectors {(zi, pi)}L i=1 anchored to active voxels pi that intersect the object surface, where each zi RC encodes fine-scale geometry and appearance. During inference, the model first samples noise in the voxel-based latent space, followed by two-stage denoising process. In the first stage, referred to as the structure (ST) stage, the diffusion model predicts voxel occupancy over 643 grid to obtain sparse structures, where each location corresponds to surface-intersecting voxel in In the second stage, called the sparsethe coarse space. latent (SLAT) stage, the structured latents are denoised to produce fine-grained geometry and texture, thereby enhancing the visual fidelity of the 3D output. Rectified Flow Inversion. Several methods [37, 59, 69, 78] explore flow inversion for downstream editing tasks. The challenges associated with achieving accurate inversion stem from the accumulation of numerical errors during ODE integration, which lead to noticeable deviations in the reconstructed samples. To address this, RF-Solver [78] introduces training-free, plug-and-play sampler that improves inversion fidelity by analytically approximating the ODE solution using high-order Taylor expansion. This formulation significantly reduces integration errors and enables more faithful reconstructions. Given state xss 0 , it integrate the rectified-flow ODE from data to noise using second-order Taylor-improved Euler scheme: xt = xt + fθ(xt, t) + 1 2 2 tfθ(xt, t) tfθ(xt, t) fθ(xt/2, /2) fθ(xt, t) / (1) (2) where xt represents the state of the data at the current time t, and xt is the predicted state for the next denoising step, Figure 3. Architecture of VoxHammer. Our framework adopts TRELLIS [90] as the base model, which predicts sparse structures at the first structure (ST) stage and denoise fine-grained structured latents at the second sparse-latent (SLAT) stage. VoxHammer performs inversion prediction in both the ST and SLAT stages, which map the textured 3D asset to its terminal noise, with latents and key/value tensors cached at each timestep. Subsequently, VoxHammer denoises from the inverted noise, and replace the features of the preserved regions with the corresponding cached latents and key-value tokens, thereby achieving precise and coherent editing in native 3D space. which is time interval away. The function fθ(xt, t) denotes the noise-prediction network, and tfθ(xt, t) is the second equation approximates using finite difference scheme. The Taylor-improved update yields local truncat ) and global error O(2 tion error O(3 ), which it found crucial for high-fidelity reconstructions of fine structures. Inspired by it, we explore inversion within native 3D generative models [90] and further propose our precise and coherent 3D editing framework through our novel contextual feature replacement. 3.2. 3D Inversion We introduce an inversion prediction strategy within the structured 3D generation pipeline [90] to map the textured 3D asset to its terminal noise. The inversion proceeds in both the structure (ST) stage and the sparse-latent (SLAT) stage, with latents and key/value (K/V) tensors cached at each time step for later reuse during editing. The inherent invertibility of the underlying Flux model enables this inversion strategy. In the forward pass, the pipeline generates assets by following predefined discrete time schedule, denoted as 0 = s0 < s1 < < sT = 1. To perform the inversion, we reverse the execution of this schedule by traversing the trajectory from timestep sT back to s0. This reversed traversal deterministically traces the generation path backward, allowing us to map final 3D asset to its corresponding source noise. During the ST stage, the K, tensors from all attention layers are stored into dictionary KV st indexed by latent time, block order, positional encoding, layer ID, and attention type. In the SLAT stage, we first extract the preserved set Ωkeep from the decoded output of the ST stage by removing the edit voxels, normalize the features, and run the same Taylor-improved inversion scheme. During this process, K/V tensors are cached into KV slat. Throughout both stages, we apply classifier-free guidance (CFG) [30] only in late-time interval [0.5, 1.0]: fcfg = (1 + ω) fθ(cond) ω fθ(neg) (3) and revert to fθ(cond) otherwise, thereby stabilizing early inversion steps and improving fidelity. In practice, we keep ω fixed and activate guidance only in the late interval, which preserves the invertibility of early steps while providing sufficient semantic sharpness for the features in Ωkeep. During the next editing stage(Sec. 3.3), the denoising features of the preserved regions are directly overwritten by the inverted source features, ensuring geometric and textural fidelity in unedited regions. 3.3. 3D Editing Based on the inversion strategy, we further introduce training-free local editing strategy, where the model denoises from the inverted noise, and performs latent replacement and key-value replacement on the unedited regions. Both operations are guided by 3D edit masks, to achieve precise preservation in unedited regions. Latent replacement. latent replacement is performed using binary edit mask In the structure (ST) stage, Table 1. Quantitative comparison on our Edit3D-Bench. We compute Chamfer Distance (CD.), masked PSNR, SSIM, LPIPS of unedited region to evaluate 3D consistency, FID, FVD to evaluate overall 3D quality, and DINO-I and CLIP-T to assess condition alignment. Method Vox-E [71] MVEdit [5] Tailor3D [63] Instant3DiT [2] TRELLIS [90] Ours (full) w/o Attn KV w/ Noise Re-init Unedited Region Preservation Overall 3D Quality Condition Alignment CD. PSNR (M) SSIM (M) LPIPS (M) FID / 0.017 0.043 0.016 0. 0.012 0.015 0.014 13.84 26.12 20.94 27.70 23.64 41.68 35.71 36.31 0.827 0.945 0.861 0.957 0. 0.994 0.986 0.989 0.316 0.070 0.148 0.067 0.131 0.027 0.042 0.038 87.41 58.53 110.52 45.93 38. 23.05 27.68 25.71 FVD 3000.3 946.5 3812.1 450.1 757.2 187.8 361.8 259. DINO-I CLIP-T 0.721 0.911 0.704 0.903 0.911 0.947 0.938 0.945 0.274 0.281 0.258 0.260 0. 0.287 0.285 0.287 ss {0, 1}HW D. At each denoising step t, the latent is blended with the inverted source latent ˆzss : ss zss zss + (1 ss) ˆzss (4) To mitigate visible seams at mask boundaries, ss can be replaced with soft mask (cid:102)M ss [0, 1]HW obtained by dilation and Gaussian falloff. In the sparse-latent (SLAT) stage, features at the unedited coordinate set Ωkeep are replaced with the inverted source latent at each denoising step: zslat Ωkeep : [u] ˆzslat [u] (5) Similar to the ST stage, coordinates near the boundaries can be weighted to ensure smooth transitions, effectively applying soft-mask-like effect. Key-value replacement. Beyond latent replacement, feature-level consistency is enforced by our proposed keyvalue replacement in the attention mechanism. In the ST stage, selfattention use binary masks self to indicate edited tokens. During editing, K/V tensors in unedited regions are replaced by their cached counterparts: Knew + (1 ) Kcache Vnew + (1 ) Vcache (6) (7) Optional attention masks can be supplied to attention calculation to block mixing between edited and preserved tokens, which is especially helpful when the edited region is small but semantically strong. Similar to the solution used in inversion, the model also adopts strategies of rescaled time scheduling and late-time CFG [30] in the denoise phase. All the above modifications are implemented by dynamically adjusting forward functions at inference time, without retraining or weight updates. 4. Experiments 4.1. Setup Implementation details. Our method is built on TRELLIS [90] and executed on single NVIDIA A100 GPU. We set the sampling steps to 25 for both the inversion and denoising phases, and set the classifier-free guidance (CFG) scales of both stages to 5.0 to balance reconstruction fidelity and edit creativity. Baselines. We adopt five competitive 3D editing methods as baselines. Vox-E [71] perform per-scene optimization on voxel representation with the guidance of image diffusion models. MVEdit [5], Tailor3D [63] and Instant3dit [2] achieves customized 3D asset editing through multi-view editing. TRELLIS [90] provides native 3D editing method based on RePaint [56], which guides the denoising process by sampling from the known regions. Evaluation dataset. We conduct evaluation on Edit3DBench, new benchmark we curated for systematic 3D editing assessment. It consists of 100 high-quality 3D models, with 50 carefully selected from Google Scanned Objects (GSO) [19] and 50 from PartObjaverse-Tiny [92]. For each model, we provide 3 distinct editing prompts that cover wide range of modifications. Each prompt is accompanied by complete set of annotated 3D assets, including the 2D renderings of the original object, 2D mask of the edit region, 2D edited image generated by FLUX.1 Fill [3], illustrating the intended target edit, and 3D mask specifying the precise editing region in 3D space. This well-structured dataset serves as rigorous benchmark for assessing the accuracy, robustness, and fidelity of 3D editing methods. Evaluation metrics. We use three aspects of metrics to comprehensively evaluate performance. First, for unedited region preservation, we assess the fidelity of preserved regions by computing Chamfer Distance (CD) [22] for geometry consistency, as well as masked PSNR, SSIM [79], and LPIPS [99] on rendered multi-view images for texture. Second, for overall 3D quality, we evaluate holistic performance by computing FID [29] on rendered images and conducting user study to capture human perceptual preferences. Finally, for the alignment with the input prompts, we evaluate the alignment of the edited 3D assets with the Figure 4. Qualitative comparisons on Edit3D-Bench. Our method achieves best performance on precision of editing and overall quality. edited image using DINO-I [60], and its alignment with the text prompt using CLIP-T [65]. 4.2. Main Results Quantitative comparison. As shown in Tab. 1, our method significantly outperforms all baselines across nearly all metrics. For unedited region preservation, our approach achieves the best scores in Chamfer Distance, PSNR, SSIM, and LPIPS, demonstrating its superior ability to maintain the original geometry and texture. This is because our method operates directly in the native 3D space and leverages latents and key-value replacement to explicitly enIn contrast, multi-view based methods force consistency. like MVEdit [5] and Instant3DiT [2], which rely on lifting multi-view edits to 3D space, usually introduce multi-view inconsistency and spatial bias, and struggle with maintaining 3D consistency. TRELLIS [90] adapts Repaint [56] to achieve native 3D editing, but lacks inversion and key-value replacement to introduce the context of the reserved region, thus showing poor performance in 3D consistency. For overall 3D quality and condition alignment, our method further demonstrates superiority by achieving the lowest FID and the highest DINO-I and CLIP-T scores. These results collectively indicate that our editing operations yield coherent and accurate outcomes in high-fidelity 3D models. Qualitative comparison. The qualitative results in Fig. 4 highlight the superiority of our method. Our approach consistently generates edits that are both locally accurate and geometrically coherent, while preserving unedited regions with high fidelity. In contrast, the baselines exhibit Figure 5. Ablation studies. Results demonstrate the effectiveness of key-value replacement in attention mechanism and latent replacement. Table 2. User preference study. Our method achieves consistently higher preference than Instant3DiT and TRELLIS in text alignment and overall 3D quality."
        },
        {
            "title": "Method",
            "content": "Text Alignment Overall 3D Quality Instant3DiT [2] TRELLIS [90] Ours 4.7% 25.0% 70.3% 1.6% 17.2% 81.2% Table 3. Analysis of two-stage inversion. We report Chamfer Distance (CD.) and PSNR, SSIM, LPIPS of rendered views to analyze the reconstruction consistency. Inversion Stage CD. PSNR SSIM LPIPS ST stage ST + SLAT stage 0.0094 0. 37.68 39.70 0.936 0.987 0.067 0.012 various artifacts. Some methods suffer from poor reconstruction quality, leading to blurry outputs and distortions in preserved regions, as observed in Vox-E [71] and Tailor3D [63]. Others are overly conservative, retaining most of the original content with only minimal edits, as is the case for MVEdit [5]. Instant3DiT [2], by contrast, often fails to generate results that align with text prompts, resulting in misplaced or misrepresented modifications. In particular, the native TRELLIS [90] editing method, lacking inversion and KV-cache mechanisms, fails to effectively constrain the original 3D structure, often resulting in positional shifts and inconsistencies in the preserved regions. Our method effecFigure 6. The impact of inversion stages on reconstruction. ST stage inversion lacks detailed consistency, while inversion on both stages achieves fine-grained geometry and texture reconstruction. tively avoids these issues, demonstrating the robustness of our native 3D, inversion-based editing framework. User study. To evaluate the perceptual quality of our editing results, we conducted user study with 30 participants. For each editing task, participants were presented with the input 3D models and the edit prompts, paired with the edited 3D assets by VoxHammer alongside two strong baseline methods TRELLIS [90] and Instant3DiT [2]. They Figure 7. More applications. VoxHammer easily generalizes to part-aware 3D object, scene, and NeRF [58] or 3DGS [38] editing. We show the input models in the top row and the edited results in the bottom two rows. were then asked to select the result that best aligned with the text prompt and exhibited the highest overall quality. The study results demonstrate clear human preference for our method, confirming its superior performance and robustness across multiple evaluation aspects. 4.3. Ablation Study We conduct series of ablation studies to analyze the reconstruction performance of two-stage inversion, and the contributions of the key components of our editing strategies, particularly the feature replacement strategy and the inversion-based initialization. Analysis of two-stage inversion. We conducted analysis of doing inversion (1) only in ST stage, and (2) in both ST and SLAT stage, which is our full setting. As shown in Tab. 3 and Fig. 6, we found that the initial ST stage inversion provides reasonable reconstruction of coarse geometry, but lacks detailed geometry and appearance consistency. After incorporating the second SLAT stage, which handles high-resolution geometry and fine-grained texture details, the reconstruction quality improves significantly. It demonstrates that our two-stage inversion process faithfully restores the source 3D model with high fidelity, offering robust basis for the subsequent editing stage. Assessment of editing strategies. We compare our full setting against four variants: (1) w/o Attn KV, where we disable key-value replacement for attention mechanism; and (2) w/ Noise Re-init, where we initiate the denoising process from randomly sampled Gaussian noise instead of the inverted noise of the source 3D asset. The quantitative results are presented in Tab. 1, and qualitative comparison is shown in Fig. 5. Missing key-value replacement leads to significant degradation in preservation quality, as the edit concept leaks into unedited regions. Re-initializing noise leads to loss of positional information, causing unexpected alterations in preserved areas. In contrast, our full setting effectively maintains the consistency of the preserved regions and achieves overall coherent editing, which validates that our inversion and key-value replacement is essential for achieving high-fidelity local 3D editing. 4.4. More Applications Part-aware object editing. VoxHammer enables flexible editing of part-aware generated 3D assets [7, 17, 49, 50, 76, 92, 93, 98, 102], where the pre-segmented structure offers 3D masks for us. We report visualization results in Fig. 7. Compositional 3D scene editing. VoxHammer further extends to compositional 3D scene editing [36, 94]. As shown in Fig. 7, it supports fine-grained local modifications while preserving the integrity of the surrounding scene. NeRF or 3DGS editing. Benefiting from the versatility of the base model, VoxHammer also generalizes NeRF [58] or 3DGS [38] editing, as visualized in Fig. 7. 5. Conclusion We presented VoxHammer, training-free framework for precise and coherent 3D local editing. By leveraging accurate 3D inversion and feature replacement in the latent space of pretrained structured 3D diffusion model, VoxHammer preserves unedited regions with high fidelity while seamlessly integrating edits. To enable objective evaluation, we introduced Edit3D-Bench, human-annotated benchmark for 3D local editing. Comprehensive experiments demonstrate that our method outperforms prior approaches in both consistency and quality. Beyond editing, VoxHammer also enables the synthesis of paired data, laying foundation for future in-context 3D generation."
        },
        {
            "title": "References",
            "content": "[1] Roi Bar-On, Dana Cohen-Bar, and Daniel Cohen-Or. Editp23: 3d editing via propagation of image prompts to multi-view, 2025. 2, 3 [2] Amir Barda, Matheus Gadelha, Vladimir G. Kim, Noam Aigerman, Amit H. Bermano, and Thibault Groueix. Instant3dit: Multiview inpainting for fast editing of 3d objects, 2024. 2, 3, 5, 6, 7, 1 [3] Black Forest Labs. FLUX.1 Tools: Introducing Fill, Depth, Canny, and Redux. https://bfl.ai/blog/24-1121-tools, 2024. Accessed: 2025-08-15. 3, 5 [4] Chenjie Cao, Chaohui Yu, Fan Wang, Xiangyang Xue, and Yanwei Fu. Mvinpainter: Learning multi-view consistent inpainting to bridge 2d and 3d editing, 2024. 2, 3 [5] Hansheng Chen, Ruoxi Shi, Yulin Liu, Bokui Shen, Jiayuan Gu, Gordon Wetzstein, Hao Su, and Leonidas Guibas. Generic 3d diffusion adapter using controlled multi-view editing, 2024. 2, 3, 5, 6, 7, 1 [6] Minghao Chen, Junyu Xie, Iro Laina, and Andrea Vedaldi. Instruction-guided latent 3d editing in secShap-editor: onds, 2023. 2 [7] Minghao Chen, Jianyuan Wang, Roman Shapovalov, Tom Monnier, Hyunyoung Jung, Dilin Wang, Rakesh Ranjan, Iro Laina, and Andrea Vedaldi. Autopartgen: AutogresarXiv preprint sive 3d part generation and discovery. arXiv:2507.13346, 2025. 8 [8] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Yanru Wang, Zhibin Wang, Chi Zhang, Jingyi Yu, Gang Yu, Bin Fu, and Tao Chen. Meshxl: Neural coordinate field for generative 3d foundation models, 2024. 2 [9] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, Guosheng Lin, and Chi Zhang. Meshanything: Artistcreated mesh generation with autoregressive transformers, 2024. 2 [10] Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, and Guosheng Lin. Ultra3d: Efficient and highfidelity 3d generation with part attention, 2025. 2 [11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. [12] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. 2 [13] Nam Anh Dinh, Itai Lang, Hyunwoo Kim, Oded Stein, and Rana Hanocka. Geometry in style: 3d stylization via surface normal deformation, 2025. 2 [14] Junting Dong, Qi Fang, Zehuan Huang, Xudong Xu, Jingbo Wang, Sida Peng, and Bo Dai. Tela: Text to layer-wise 3d clothed human generation. In European Conference on Computer Vision, pages 1936. Springer, 2025. 2 [15] Shaocong Dong, Lihe Ding, Zhanpeng Huang, Zibin Wang, Tianfan Xue, and Dan Xu. Interactive3d: Create what you want by interactive 3d generation, 2024. 2 [16] Shaocong Dong, Lihe Ding, Xiao Chen, Yaokun Li, Yuxin Wang, Yucheng Wang, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, Zibin Wang, Tianfan Xue, and Dan Xu. From one to more: Contextual part latents for 3d generation, 2025. 2 [17] Shaocong Dong, Lihe Ding, Xiao Chen, Yaokun Li, Yuxin Wang, Yucheng Wang, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, et al. From one to more: Contextual part latents for 3d generation. arXiv preprint arXiv:2507.08772, 2025. 8 [18] Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han. Prompt tuning inversion for text-driven image editing using diffusion models, 2023. [19] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B. McHugh, and Vincent Vanhoucke. Google scanned objects: high-quality dataset of 3d scanned household items, 2022. 5 [20] Ziya Erkoc, Can Gumeli, Chaoyang Wang, Matthias Nießner, Angela Dai, Peter Wonka, Hsin-Ying Lee, and Peiye Zhuang. Preditor3d: Fast and precise 3d shape editing, 2024. 2, 3 [21] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 3 [22] Haoqiang Fan, Hao Su, and Leonidas Guibas. point set generation network for 3d object reconstruction from single image, 2016. 5 [23] Haoran Feng, Zehuan Huang, Lin Li, Hairong Lv, and Lu Sheng. Personalize anything for free with diffusion transformer, 2025. 3 [24] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. [25] Daoyi Gao, Yawar Siddiqui, Lei Li, and Angela Dai. Meshart: Generating articulated meshes with structure-guided transformers, 2025. 2 [26] Will Gao, Dilin Wang, Yuchen Fan, Aljaz Bozic, Tuur Stuyck, Zhengqin Li, Zhao Dong, Rakesh Ranjan, and Nikolaos Sarafianos. 3d mesh editing using masked lrms, 2024. 2, 3 [27] Zekun Hao, David W. Romero, Tsung-Yi Lin, and Ming-Yu Liu. Meshtron: High-fidelity, artist-like 3d mesh generation at scale, 2024. 2 [28] Zexin He, Tengfei Wang, Xin Huang, Xingang Pan, and Ziwei Liu. Neural lightrig: Unlocking accurate object normal and material estimation with multi-light diffusion, 2024. 2 [29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. 5 [30] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 4, 5 [31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [32] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. [33] Xiufeng Huang, Ka Chun Cheung, Runmin Cong, Simon See, and Renjie Wan. Stereo-gs: Multi-view stereo vision model for generalizable 3d gaussian splatting reconstruction, 2025. 2 [34] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. arXiv preprint arXiv:2412.03632, 2024. 2 [35] Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding Liang, Yu Qiao, Bo Dai, et al. Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 97849794, 2024. 2 [36] Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, and Lu Sheng. Midi: Multi-instance diffusion for single image to 3d scene generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2364623657, 2025. 8 [37] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and In Proceedings of the IEEE/CVF Confermanipulations. ence on Computer Vision and Pattern Recognition, pages 1246912478, 2024. 3 [38] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3, 8 [39] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2 [40] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 3 [41] Haoxuan Li, Ziya Erkoc, Lei Li, Daniele Sirigatti, Vladyslav Rozov, Angela Dai, and Matthias Nießner. Meshpad: Interactive sketch-conditioned artist-reminiscent mesh generation and editing, 2025. 2 [42] Peng Li, Suizhi Ma, Jialiang Chen, Yuan Liu, Congyi Zhang, Wei Xue, Wenhan Luo, Alla Sheffer, Wenping Wang, and Yike Guo. Cmd: Controllable multiview diffusion for 3d editing and progressive generation, 2025. 2, 3 [43] Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. [44] Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman3d: High-fidelity mesh generation with 3d native generation and interactive geometry refiner, 2025. 2 [45] Weiyu Li, Xuanyang Zhang, Zheng Sun, Di Qi, Hao Li, Wei Cheng, Weiwei Cai, Shihao Wu, Jiarui Liu, Zihao Wang, Xiao Chen, Feipeng Tian, Jianxiong Pan, Zeming Li, Gang Yu, Xiangyu Zhang, Daxin Jiang, and Ping Tan. Step1x3d: Towards high-fidelity and controllable generation of textured 3d assets, 2025. 2 [46] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, and Yan-Pei Cao. Triposg: Highfidelity 3d shape synthesis using large-scale rectified flow models, 2025. 2 [47] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape synthesis using large-scale rectified flow models. arXiv preprint arXiv:2502.06608, 2025. 2 [48] Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, and Bihan Wen. Sparc3d: Sparse representation and construction for high-resolution 3d shapes modeling. arXiv preprint arXiv:2505.14521, 2025. [49] Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compositional latent diffusion transformers, 2025. 2, 8 [50] Anran Liu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Zhiyang Dou, Hao-Xiang Guo, Ping Luo, and Wenping Wang. Part123: part-aware 3d reconstruction from single-view In ACM SIGGRAPH 2024 Conference Papers, image. pages 112, 2024. 2, 8 [51] Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, and Yueqi Duan. Make-your-3d: Fast and consistent subject-driven 3d content generation, 2024. 2 [52] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10072 10083, 2024. 2 [53] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36, 2024. 2 [54] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. [55] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99709980, 2024. 2 [56] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models, 2022. 5, 6 [57] Quan Meng, Lei Li, Matthias Nießner, and Angela Dai. Lt3sd: Latent trees for 3d scene diffusion. arXiv preprint arXiv:2409.08215, 2024. 2 [58] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2, 3, 8 [59] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 3 [60] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. [61] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195 4205, 2023. 2 [62] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion, 2022. 2 [63] Zhangyang Qi, Yunhan Yang, Mengchen Zhang, Long Xing, Xiaoyang Wu, Tong Wu, Dahua Lin, Xihui Liu, Jiaqi Wang, and Hengshuang Zhao. Tailor3d: Customized 3d assets editing and generation with dual-side images, 2024. 2, 3, 5, 7, 1 [64] Yansong Qu, Shaohui Dai, Xinyang Li, Yuze Wang, You Shen, Liujuan Cao, and Rongrong Ji. Deocc-1-to-3: 3d de-occlusion from single image via self-supervised multiview diffusion, 2025. 2 [65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 6 [66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey text-conditional arXiv preprint Chu, and Mark Chen. image generation with clip latents. arXiv:2204.06125, 1(2):3, 2022. 3 Hierarchical [67] Barbara Roessle, Norman Muller, Lorenzo Porzi, Samuel Rota Bulø, Peter Kontschieder, Angela Dai, and Matthias Nießner. L3dg: Latent 3d gaussian diffusion. arXiv preprint arXiv:2410.13530, 2024. 2 [68] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022. 3 [69] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. arXiv preprint arXiv:2410.10792, 2024. 3 [70] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 3 [71] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. Vox-e: Text-guided voxel editing of 3d objects, 2023. 2, 5, 7, 1 [72] Etai Sella, Noam Atia, Ron Mokady, and Hadar AverbuchElor. Blended point cloud diffusion for localized textguided shape editing. arXiv preprint arXiv:2507.15399, 2025. [73] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 2 [74] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, 3 and Stefano Ermon. arXiv preprint [75] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multiview gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118. Springer, 2025. 2 [76] Jiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang Zeng, Ming-Yu Liu, and Tsung-Yi Lin. Efficient part-level 3d object generation via dual volume packing, 2025. 2, [77] Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2025. 2 [78] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. arXiv Taming rectified flow for inversion and editing. preprint arXiv:2411.04746, 2024. 3 [79] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4): 600612, 2004. 5 [80] Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui Zeng. Llama-mesh: Unifying 3d mesh generation with language models, 2024. 2 [81] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model. arXiv preprint arXiv:2403.05034, 2024. 2 [82] Si-Tong Wei, Rui-Huan Wang, Chuan-Zhi Zhou, Baoquan Chen, and Peng-Shuai Wang. Octgpt: Octree-based multiscale autoregressive models for 3d shape generation, 2025. neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. 2 [97] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2 [98] Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, and Jingyi Yu. Bang: Dividing 3d assets via generative exploded dynamics. ACM Transactions on Graphics, 44(4):121, 2025. 8 [99] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. 5 [100] Yunzhi Zhang, Zizhang Li, Matt Zhou, Shangzhe Wu, and Jiajun Wu. The scene language: Representing scenes with programs, words, and embeddings, 2025. 2 [101] Ruowen Zhao, Junliang Ye, Zhengyi Wang, Guangce Liu, Yiwen Chen, Yikai Wang, and Jun Zhu. Deepmesh: Autoregressive artist-mesh creation with reinforcement learning, 2025. [102] Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, and Ying Shan. Assembler: Scalable 3d part assembly via anchor point diffusion, 2025. 2, 8 [103] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, 36, 2024. 2 [104] Yang Zheng, Mengqi Huang, Nan Chen, and Zhendong Mao. Pro3d-editor : progressive-views perspective for consistent and precise 3d editing, 2025. 2, 3 [105] Tianrui Zhu, Shiyi Zhang, Jiawei Shao, and Yansong Tang. Kv-edit: Training-free image editing for precise background preservation, 2025. 3 [106] Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, and Ying Shan. Tip-editor: An accurate 3d editor following both text-prompts and image-prompts, 2024. 2 [83] Hao Wen, Zehuan Huang, Yaohui Wang, Xinyuan Chen, Yu Qiao, and Lu Sheng. Ouroboros3d: Image-to-3d generation via 3d-aware recursive diffusion. arXiv preprint arXiv:2406.03184, 2024. [84] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. arXiv preprint arXiv:2405.20343, 2024. 2 [85] Ruiqi Wu, Xinjie Wang, Liu Liu, Chunle Guo, Jiaxiong Qiu, Chongyi Li, Lichao Huang, Zhizhong Su, and MingMing Cheng. Dipo: Dual-state images controlled articulated object generation powered by diverse data, 2025. 2 [86] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. 2 [87] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Xun Cao, Philip Torr, and Yao Yao. Direct3d-s2: Gigascale 3d generation made easy with spatial sparse attention, 2025. 2 [88] Tianhao Wu, Chuanxia Zheng, Frank Guan, Andrea Vedaldi, and Tat-Jen Cham. Amodal3r: Amodal 3d reconstruction from occluded 2d images, 2025. 3 [89] Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, et al. Blockfusion: Expandable 3d scene generation using latent tri-plane extrapolation. ACM Transactions on Graphics (TOG), 43(4):117, 2024. [90] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation, 2025. 2, 3, 4, 5, 6, 7, 1 [91] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2 [92] Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Y. Lam, Yan-Pei Cao, and Xihui Liu. Sampart3d: Segment any part in 3d objects, 2024. 5, 8 [93] Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, YanPei Cao, and Xihui Liu. Omnipart: Part-aware 3d generation with semantic decoupling and structural cohesion. arXiv preprint arXiv:2507.06165, 2025. 8 [94] Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Lan Xu, Wei Yang, Jiayuan Gu, and Jingyi Yu. Cast: Component-aligned 3d scene reconstruction from an rgb image. ACM Transactions on Graphics (TOG), 44(4): 119, 2025. 8 [95] Junliang Ye, Zhengyi Wang, Ruowen Zhao, Shenghao Xie, and Jun Zhu. Shapellm-omni: native multimodal llm for 3d generation and understanding, 2025. [96] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Text-Condition 3D Editing Benefiting from the versatility of TRELLIS [90], our framework also supports text-condition 3D editing by injecting textual conditions into the inversion and denoising stages of the base model for masked assets, as illustrated in Fig. 9. Leveraging this capability, we evaluate VoxHammer on textcondition 3D editing tasks, where it achieves competitive performance in preserving unedited regions and maintaining overall 3D quality, as presented in Fig. 8. However, condition alignment is not always reliable, as the model may deviate from textual instructions. As shown in Tab. 5, this underscores the need to further enhance the fidelity of text-conditioned guidance. 7. Explanation of Evaluation Metrics In terms of evaluating unedited region preservation, Chamfer Distance assesses the geometry consistency, while masked PSNR, SSIM and LPIPS of rendered multi-view images evaluate the consistency of structures and appearance. In terms of evaluating editing quality, FID assesses the overall visual similarity between the edited results and the original object. FVD evaluates the temporal continuity and stability across multi-view images. In terms of edit controllability, the text-asset alignment score from CLIP-T measures the similarity between the editing results and the editing text, while DINO-I measures the similarity between the editing results and the original object. Since our task focuses on 3D local editing, DINO-I can reflect the accuracy of the edits to some extent. Overall, these metrics provide comprehensive quantitative evaluation of unedited region preservation, overall editing quality, and editing accuracy from different perspectives, collectively reflecting the overall performance of the 3D editing method. 8. More Results More results of image-condition 3D editing are shown in Fig. 10, which demonstrates the ability to achieve precise and coherent 3D editing. 9. Limitation Although VoxHammer preserves unedited regions and maintains overall 3D quality, several limitations remain. First, textual alignment is not yet optimal, partly due to the scarcity of large-scale captioned 3D datasets, making text condition less robust than image-based guidance. Second, editing fidelity is bounded by the resolution of the TRELLIS [90] backbone, limiting precision for highresolution assets. Finally, our pipeline comprises of 3D encoding, inversion, denoising and decoding. Due to the time-consuming rendering phase in the 3D encoding stage (> 1 min), VoxHammer takes about 2 minutes to edit one 3D asset, indicating room for efficiency improvements toward interactive use. Table 4. Runtime comparison across different methods. Method Vox-E [71] MVEdit [5] Tailor3D [63] Instant3DiT [2] Ours Runtime 32 min 242 83 20 133 Figure 8. Visualization results of text-condition 3D editing. Table 5. Quantitative comparison on text-condition and image-condition3D editing. Unedited Region Preservation Overall 3D Quality Condition Alignment Method CD. PSNR (M) SSIM (M) LPIPS (M) Text-condition 3D editing Image-condition 3D editing 0.010 0.012 38.61 41.68 0.992 0.994 0.024 0.027 FID 25.93 23.05 FVD 150.4 187.8 CLIP-T 0.277 0.287 Figure 9. Pipeline of text-condition (left) and image-condition (right) 3D editing. Figure 10. More visualization results of image-condition 3D editing."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Renmin University of China",
        "Tencent Hunyuan",
        "Tsinghua University"
    ]
}