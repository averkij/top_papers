{
    "paper_title": "JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization",
    "authors": [
        "Kai Liu",
        "Wei Li",
        "Lai Chen",
        "Shengqiong Wu",
        "Yanhao Zheng",
        "Jiayi Ji",
        "Fan Zhou",
        "Rongxin Jiang",
        "Jiebo Luo",
        "Hao Fei",
        "Tat-Seng Chua"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces JavisDiT, a novel Joint Audio-Video Diffusion Transformer designed for synchronized audio-video generation (JAVG). Built upon the powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to generate high-quality audio and video content simultaneously from open-ended user prompts. To ensure optimal synchronization, we introduce a fine-grained spatio-temporal alignment mechanism through a Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. This module extracts both global and fine-grained spatio-temporal priors, guiding the synchronization between the visual and auditory components. Furthermore, we propose a new benchmark, JavisBench, consisting of 10,140 high-quality text-captioned sounding videos spanning diverse scenes and complex real-world scenarios. Further, we specifically devise a robust metric for evaluating the synchronization between generated audio-video pairs in real-world complex content. Experimental results demonstrate that JavisDiT significantly outperforms existing methods by ensuring both high-quality generation and precise synchronization, setting a new standard for JAVG tasks. Our code, model, and dataset will be made publicly available at https://javisdit.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 7 7 3 3 2 . 3 0 5 2 : r a"
        },
        {
            "title": "JavisDiT",
            "content": ": Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization Kai Liu1,2*, Wei Li3, Lai Chen1, Shengqiong Wu2, Yanhao Zheng1, Jiayi Ji2, Fan Zhou1, Rongxin Jiang1, Jiebo Luo4, Hao Fei2, Tat-Seng Chua2 1 Zhejiang University, 2 National University of Singapore, 3 University of Science and Technology of China, 4 University of Rochester"
        },
        {
            "title": "Abstract",
            "content": "This paper introduces JavisDiT, novel Joint AudioVideo Diffusion Transformer designed for synchronized audio-video generation (JAVG). Built upon the powerful Diffusion Transformer (DiT) architecture, JavisDiT is able to generate high-quality audio and video content simultaneously from open-ended user prompts. To ensure optimal synchronization, we introduce fine-grained spatiotemporal alignment mechanism through Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) Estimator. This module extracts both global and fine-grained spatiotemporal priors, guiding the synchronization between the visual and auditory components. Furthermore, we propose new benchmark, JavisBench, consisting of 10,140 high-quality text-captioned sounding videos spanning diverse scenes and complex real-world scenarios. Further, we specifically devise robust metric for evaluating the synchronization between generated audio-video pairs in realworld complex content. Experimental results demonstrate that JavisDiT significantly outperforms existing methods by ensuring both high-quality generation and precise synchronization, setting new standard for JAVG tasks. Our code, model, and dataset will be made publicly available at https://javisdit.github.io/. 1. Introduction In the field of AI-generated content (AIGC), the generation of multimodal content, such as images, videos, and audio, has recently garnered unprecedented attention [27, 36, 41, 59], where diffusion-based models [13, 29, 34] have demonstrated remarkable performance. While earlier works primarily focused on generating single-modal content, there is now growing interest in simultaneously generating multiple modalities [9, 54]. Notably, synchronized audio *Work done during Kai Lius visiting period at NExT++ Lab, National University of Singapore. Email: kail@zju.edu.cn. Corresponding author. Email: haofei37@nus.edu.sg. Figure 1. Given the input text prompt, JAVG system generates spatial-temporally synchronized sounding video. The sounds align perfectly with the temporal progression of the actions. and video generation [42, 48, 56] has emerged as crucial area of study. Audio and video are inherently interconnected in most real-world scenarios, making their joint generation highly valuable for applications, such as movie production and short video creation. Current approaches to synchronized audio-video generation can be broadly categorized into two types. The first involves asynchronous pipelines, where audio is generated first and then used to synthesize video [18, 60, 62], or vice versa [6, 55, 63]. The second type involves end-to-end Joint Audio-Video Generation (namely JAVG) [42, 48, 56], which has attracted more research attention by avoiding cascaded noise accumulation. Overall, it is strongly and widely believed that achieving high-quality JAVG requires two equally critical criteria: (1) ensuring highquality generation of audio and video, and (2) maintaining perfect synchronization between two modalities. On one hand, for high-quality content generation of audio and video, the backbone architectures of both modules Figure 2. On the left, we present the overall DiT-based sounding video generation architecture of our JavisDiT System, consisting of video generation branch, audio generation branch, and the MM-BiCrossAttn module. On the right, we illustrate the detailed structural design of Spatio-temporal Self-attention (ST Self-Attn), Fine-grained Spatio-temporal Cross-attention (Fine-grained ST-CrossAttn), and Multi-Modality Bidirectional Cross-attention (MM-BiCrossAttn). need to be as strong as possible. Benefiting from the superior generative capabilities of diffusion models, most current JAVG research [42, 48, 56] adopts diffusion backbone architectures. Recently, the Diffusion Transformer (DiT) [34] architecture has demonstrated remarkably enhanced performance for vision and even audio tasks, also offering advantages in efficiency and scalability for concurrent works [26, 48, 51, 64]. In particular, AV-DiT [51] and MMLDM [48] inherit an image DiT [8, 34] to generate video and audios, where the fine-grained spatio-temporal modeling capability can be however limited. Uniform [64] and SyncFlow [26] take the enhanced STDiT3 [65] blocks, but they either simply concatenate video/audio tokens or utilize mono-directional video-to-audio adapter, lacking sufficient mutual information exchange between two channels. On the other hand, in terms of synchronization between the audio and video channels in JAVG, extensive investigations have been made to explore more effective alignment strategies. However, most of these works still fall prey to insufficient modeling of synchronization. For example, some emphasize coarse-grained temporal alignment by implementing straightforward parameter sharing [42, 51, 64] or temporal modulating [26, 51] between the audio and video channels. Others focus on coarse-grained semantic alignment, such as Seeing-Hearing [56], which carries out simple audiovideo embedding alignment, and MM-LDM [48] performing straightforward representation alignment in the latent space of the diffusion model. These approaches fail to consider fine-grained spatio-temporal alignment, which is essential for realistic synchronized audio-video generation. Specifically, in realistic-sounding video, all visual and auditory contents should be dictated by spatio-temporal characteristics of various objects, i.e., a) spatially distinct visual content (e.g., where events occur and how objects move), and b) temporally corresponding auditory attributes, such as timbre and synchronized duration. We illustrate this in Fig. 1: dog and robot are playing on the ground, while an alien appears and starts to talk. In this scene, the robots mechanical whirs and the dogs squeaks and barks keep consistent, and the aliens talking emerges later and continues to the end. Overall, the visual content and audio stay synchronized both temporally and spatially. To address the above dual challenges, this paper proposes novel niche-targeting JAVG system, called JavisDiT (cf. Fig. 2). First, JavisDiT adopts the DiT architecture as the backbone, where the audio and video channels share AV-DiT blocks to enable high-quality audio-video generation. Within JavisDiT, we design three infrastructural blocks: SpatioTemporal Self-Attention, Coarse-Grained Cross-Attention, and Fine-Grained Spatio-Temporal Self-Attention CrossAttention. To implement the fine-grained spatio-temporal alignment idea, we design Hierarchical Spatial-Temporal Synchronized Prior (HiST-Sypo) estimation module. The HiST-Sypo Estimator hierarchically extracts the following from the input conditional prompt: global coarse-grained spatio-temporal priors, such as the semantic framework of the overall sounding video; fine-grained spatio-temporal priors, such as the distinct visual content triggered by various sounds and their corresponding temporal priors. These global and fine-grained priors serve as spatio-temporal features, injected into different AV-DiT blocks to guide the spatial semantic and temporal synchronization of both audio and video. Building on JavisDiT, we further design contrastive learning-based [3] HiST-Sypo estimation strategy to learn robust spatial-temporal synchronized prior knowledge from large-scale data of sounding videos. Further, we observe that the existing JAVG benchmarks, e.g., Landscape [21] and AIST++ [22] suffer from certain limitations, including overly simplistic audio-video content and limited scene diversity [31]. This creates an evident gap compared to the complex audio-video content encountered in real-world applications, i.e., models [42, 48] trained there largely suffer from out-of-domain issue, thereby partially hindering the development and practical applicability on open environment. To bridge these gaps, we introduce new JAVG benchmark, called JavisBench. After rigorous manual inspection, we collect total of 10,140 high-quality text-captioned sounding videos, which span over 5 dimensions with 19 scene categories. Notably, more than 50% of these videos feature highly complex and challenging scenarios, making the dataset more representative of diverse real-world applications. Meanwhile, we find that existing JAVG evaluation method can also be limited by failing to adequately assess the JAVG systems on complex sounding videos. Thus, we devise novel metric (JavisScore) based on temporal-aware semantic alignment mechanism. Extensive experiments on both existing JAVG benchmarks and our JavisBench dataset demonstrate that JavisDiT significantly outperforms state-of-the-art methods in generating high-quality sounding videos. Our system is especially effective in handling complex-scene videos, thanks to the HiST-Sypo estimation mechanism. In summary, our main contributions are as follows: We introduce JavisDiT, novel DiT-based JAVG system with hierarchical spatio-temporal prior estimation mechanism to achieve synchronized audio-video alignment in both spatial and temporal dimensions. We contribute JavisBench, new large-scale JAVG benchmark dataset with challenging scenarios, along with robust metrics to evaluate audio-video synchronization, offering comprehensive baseline for future research. Empirically, our system achieves state-of-the-art performance across both existing closed-set and our open-world benchmarks, setting new standard for JAVG. 2. Preliminaries Task Definition and Formulation. Joint Audio-Video Generation (JAVG) requires diffusion models Gθ to simultaneously generate videos and corresponding audios given text input s. The forward process at timestamp t, either with DDPM [13] or FlowMatching [24], is formulated as: (vt1, at1) = Gθ(vt, at, s, t) . (1) In general practice, RTv(HW )3 denotes video with Tv frames and 3 RGB channels, where H, are the height and width of each video frame. On the other hand, RTaM 1 encodes audios with the image-like MelSpectrogram with Ta temporal frames and 1 channel, where is the frequency bins of the audio spectrogram. Spatio-Temporal Alignment. For an ideal model G, the generated video and audio should maintain coarse-grained semantic alignment with the input text s, while further achieving fine-grained spatio-temporal alignment between video and audio a, which are formally defined as: Spatial Alignment means the occurrence of specific event in region of the video frame (at the dimension in v), which corresponds to the appearance of matching frequency components in the audio (at the dimension in a). Temporal Alignment means the onset or termination of an event at specific frame or timestamp in the video (at the Tv dimension in v), which must coincide with the corresponding start or stop of the response in the audio (at the Ta dimension in a). This work focuses on the joint generation of high-quality and spatio-temporally aligned video-audio pairs. 3. The Proposed JavisDiT"
        },
        {
            "title": "System",
            "content": "3.1. JavisDiT Model Architecture Fig. 2 illustrates the overall architecture. Spatio-temporal attention is employed for effective cross-modal alignment while ensuring high-quality generation for videos and audios. Each branch uses ST-SelfAttn for intra-modal aggregation, incorporates coarse text semantics via CrossAttn, integrates fine-grained spatio-temporal priors through ST-CrossAttn, and enhances video-audio fusion with Bi-CrossAttn. Here we introduce the core components displayed in Fig. 2 (b): Spatio-Temporal Self-Attention. Given that both videos and audios possess spatial and temporal attributes [42], we employ cascaded spatio-temporal self-attention mechanism for intra-modal information aggregation. As Fig. 2(b) illustrates, MHSA is applied sequentially along the spatial ((H ) for v, for a) and temporal (Tv for v, Ta for a) dimensions. This efficiently achieves fine-grained spatiotemporal modeling with reduced computational cost. Spatio-Temporal Cross-Attention. For text prompt s, we estimate spatial and temporal priors with Ns and Nt learnable c-dimensional tokens by our ST-Prior Estimator (details are presented in Sec. 3.2). As shown in Fig. 2(b), spatial and temporal priors guide cross-attention in both branches along the spatial and temporal dimensions, enabling unified, Fine-Grained ST-Prior Estimation. As shown in Fig. 3, for given text input s, we utilize the 77 hidden states from ImageBinds [11] text encoder, and use Ns = 32 spatial tokens ps and Nt = 32 temporal tokens pt to query 4-layer transformer to extract spatio-temporal information. Since input text prompts usually may not specify an events happening time and location, it may almost occur and cease in arbitrary location and timing, and the same text should correspondingly yield different ST-Priors (ps, pt). To capture this variability, our ST-Prior Estimator outputs the mean and variance of Gaussian distribution, from which plausible (ps, pt) is sampled: (ps, pt) Pϕ(s; ϵ), where ϵ is Gaussian noise. Furthermore, we carefully devised contrastive learning approach to learn robust STPrior Estimator, which involves series of negative sample (asynchronous video-audio pairs) construction strategies and specifically-designed loss functions. Implementation details are provided in the Appendix C.2. 3.3. Multi-Stage Training Strategy Our JavisDiT focuses on two core objectives: achieving highquality single-modal generation and ensuring fine-grained spatio-temporal alignment between generated videos and audios. To do this, we leverage the pretrained weights of OpenSora [65] for the video branch, and adopt three-stage training strategy for robust joint video-audio generation: 1) Audio Pretraining. We initialize the audio branch with OpenSoras video branch weights and train it on largescale audio dataset (0.8M samples) to ensure superior single-modal generation quality. 2) ST-Prior Training. We train the ST-Prior Estimator Pϕ using 0.6M synchronous text-video-audio triplets and synthesized asynchronous negative samples. This stage can run concurrently with the first stage. 3) Joint Generation. We freeze video and audio branches self-attention blocks and ST-Prior Estimator, training only the ST-CrossAttn and Bi-CrossAttn modules with 0.6M samples to enable synchronized video-audio generation. In addition, dynamic temporal masking [65] is equipped to enable flexible adaptation of JavisDiT to x-conditional tasks (v2a/a2v generation, image animation, video-audio extension, etc.). Details are provided in Appendix F. 4. Challenging JavisBench Benchmark strong generative model must ensure diverse video content, audio types, and fine-grained spatio-temporal synchronization. However, current JAVG benchmarks might lack diversity and robustness for comprehensive evaluation (as indicated in Appendix D.1). To combat this shortcoming, we propose more challenging benchmark (Sec. 4.1), and also robust metric to assess spatio-temporal alignment (Sec. 4.2). Figure 3. The framework of Spatio-temporal Prior Estimator with 4-layer transformer encoder-decoder (referring to the purple region). Contrastive learning is utilized to optimize the estimator. fine-grained conditioning for video-audio synchronization. Cross-Modality Bidirectional-Attention. After aligning video and audio with ST-Prior, we incorporate bidirectional attention module [28] to enable direct cross-modal interactions. As Fig. 2(b) illustrates, after computing the attention matrix between qv and ka, we first multiply with va to obtain the audio-to-video cross-attention. Similarly, multiplying AT (the transpose of A) with vv yields the videoto-audio cross-attention. This mechanism enhances crossmodal information aggregation to facilitate high-quality joint audio-video generation. 3.2. Hierarchical Spatio-Temporal Prior Unlike previous works, our JavisDiT derives two hierarchical conditions (priors) from texts: global semantic prior for coarse event suggestion (what) and fine-grained spatiotemporal prior (ST-Prior) to specify event timing and location (when and where), enabling precise synchronization between generated video and audio. Coarse-Grained Spatio-Temporal Prior. Since the default semantic embeddings from T5 encoder [37] are strong enough to coarsely describe the overall sounding event, we simply reuse T5 embeddings as our coarse-grained spatiotemporal prior (or, semantic prior). Fine-Grained Spatio-Temporal Prior. Text inputs typically provide coarse-grained descriptions of events, e.g., car starts its engine and leaves the screen. To enable finegrained conditioning, we estimate spatio-temporal priors: spatial prior specifies where events occur (e.g., the car is in the top-left corner of screen), and temporal prior defines when they start and end (e.g., sound starts at 2s, exits at 7s, fades at 9s). Instead of generating explicit prompts from LLMs [27], we efficiently estimate spatio-temporal priors as latent token conditions to guide the diffusion process. Table 1. Comparison with existing benchmarks. TAVGBench [31]s evaluation set is not currently released (Unknown). Dataset #Sample Scenarios Categories OpenDomain AIST++ Landscape TAVGBench Ours 20 1 (dancing) 100 1 (natural) 3,000 Unknown 1 (music) 1 (ambient) Indiscriminate 10,140 Diversified Hierarchical 4.1. Data Construction Taxonomy. To assess the capabilities of joint video-audio generation models, we design five evaluation dimensions from coarse to fine: Event Scenario depicts the primary scenario where the audio-visual event happens, such as nature or industry. Video Style describes the visual style of given videos, such as camera shooting or 2D animations. Sound Type: the sounding type of given audios, such as sound effects or music. Spatial Composition defines the sounding subjects appearing in videos and audios, divided by single or multiple sounding subjects that exist. Temporal Composition describes events onsets and terminations in v-a pairs, distinguished by single or multiple sounding sources that sequentially or concurrently occur. Using GPT-4 [1], we develop hierarchical categorization system with 5 dimensions and 19 categories  (Fig. 4)  , and the detailed definitions are presented in Appendix D.2. Data Curation. There are two data sources: (1) test sets of existing datasets (e.g., Landscape/AIST++ and FAVDBench [46]), and (2) YouTube videos uploaded between June and December 2024 to prevent data leakage [31]. To collect YouTube videos, we prompt GPT-4 to generate categoryspecific keywords using our defined taxonomy, enabling efficient and targeted video collection while avoiding noisy data. After strict manual legal and ethical verification (see Appendix A), the above process yielded around 30K sounding video candidates. With several filtering tools to ensure quality and diversity, we use the advanced Qwen-family models [5, 52, 58] to generate captions and categorize videoaudio pairs into desired taxonomy, and finally curate 10,140 diverse and high-quality video-audio pairs for JavisBench10K. More details about data construction are presented in Appendix D.3. We also randomly select 1,000 samples to form the version of JavisBench-mini for efficient evaluation. Benchmark Statistics. Tab. 1 highlights JavisBenchs contributions: (1) offering more diverse data compared to AIST++ [22] and Landscape [21], and (2) providing detailed taxonomy for comprehensive evaluations, surpassing TAVGBench [31]. As Fig. 4 shows, JavisBench covers diverse event scenarios, visual styles, and audio types, ensuring balanced distribution for greater diversity. We also include under-represented scenarios like 2D/3D animations (25%) and industrial events (13%) for more realistic benchmark. Figure 4. Category distribution of our benchmark. 75% of samples feature multiple sounding subjects, 28% involve sequential events, and 57% include simultaneous events, presenting challenges for generative models. 4.2. JavisScore: More Robust JAVG Metric Motivation. AV-Align [60] is widely-adopted JAVG metric, which uses video optical-flow estimation to match the onset detected in audios to measure video-audio synchronization. However, AV-Align may struggle with complex scenarios (i.e., with multiple sounding events or subtle visual movements) and produce misleading results (see Appendix D.4). Therefore, we propose more robust evaluation metric, namely JavisScore, to measure visual-audio synchronization in diverse, real-world contexts. Implementation and Verification. Technically, we chunk each video-audio pair into several clips with 2-seconds window size and 1.5-seconds overlap, use ImageBind [11] to compute the audio-visual synchronization for each segment, and average the scores as the final metric: SJavis = 1 (cid:88) i= σ (Ev(Vi), Ea(Ai)) , (2) where Ev and Ea are vision and audio encoders, Ai and Vi are audio and video segments in i-th window, and σ is specifically designed synchronization measure for videoaudio segments. Motivated by Mao et al. [31], we calculate similarities between all frames and the audio within each segment, and select the 40% least synchronized frames to obtain the score of the current window. Appendix D.4 presents more details and discussions, where we also build humanannotated evaluation dataset with 3,000 samples to verify the effectiveness of our JavisScore against previous metrics. 5. Experiments 5.1. Setups and Implementations This part presents the main experimental settings, with detailed configurations provided in Appendix C.1. Evaluation Datasets. We primarily use the proposed JavisBench as the evaluation dataset to enable comprehensive Table 2. Main results on proposed JavisBench. Our method comprehensively outperforms or gets on par with the currently available audio/video generation models. All models generate 4-second sounding videos at 240P/24fps with 16kHz audio. Method - T2A+A2V TempoTkn [60] TPoS [18] - T2V+V2A ReWaS [19] See&Hear [56] FoleyC [63] - T2AV MM-Diff [42] Ours AV-Quality Text-Consistency AV-Consistency AV-Synchrony FVD KVD FAD TV-IB TA-IB CLIP CLAP AV-IB CAVP AVHScore JavisScore 539.8 839. 7.2 4.7 - - - - - - - - 9.4 7.6 9.1 2311.9 203. 12.2 1.4 27.5 6.9 0.080 0.151 0.085 0.103 - - 0.198 0. - - - - - 0.109 0.072 0.173 0.032 0.197 - - - 0.173 0. 0.236 0.248 0.299 0.048 0.320 0.137 0.142 0.111 0.164 0.204 0.119 0.201 0.787 0. 0.794 0.798 0.800 0.783 0.801 0.122 0.129 0.104 0.143 0.186 0.109 0.183 0.103 0. 0.079 0.112 0.151 0.070 0.158 Table 3. Experimental results on previous datasets. Numbers are borrowed from their released papers. Method Landscape AIST++ FVD KVD FAD FVD KVD FAD 332.1 MM-Diff [42] 326.2 See&Hear [56] AV-DiT [51] 172.7 MM-LDM [48] 105.0 94.2 Ours 26.6 9.2 15.4 8.3 7.8 9.9 12.7 11.2 9.1 8.5 219.6 - 68.8 105.0 86.7 49.1 - 21.0 27.9 19. 12.3 - 10.2 10.2 9.6 during inference. 5.2. Main Results and Observations Our JavisDiT Achieves Superior Single-Modal Quality and Video-Audio Synchrony. As shown in Tab. 2, our carefully designed spatiotemporal DiT architecture demonstrates exceptional unimodal generation quality, achieving significantly superior results compared to the UNet-based architecture (e.g., TempoToken [60]) and the naive DiT architecture (e.g., MM-Diffusion [42]), with remarkable FVD (203.2) and FAD (6.9) scores. Meanwhile, from the perspective of global semantic alignment, including text-consistency and video-audio consistency, our model also achieves state-ofthe-art performance, as evidenced by its TA-IB score of 0.151 and CLIP similarity score of 0.325. Fig. 6 showcases some representative JAVG examples. Notably, in terms of audiovideo synchrony, our model outperforms various cascaded and joint audio-video generation approaches, achieving JavisScore of 0.158, surpassing the state-of-the-art cascaded method FoleyCrafter [63]. To ensure rigorous model comparison, we follow the standard settings from prior works [42, 48, 56] and train our model for 300 epochs on two closed-set datasets, including Landscape [21] and AIST++ [22]. As demonstrated in Tab. 3, our method consistently achieves state-of-the-art performance, with FVD of 94.2 on Landscape and FAD of 9.6 on AIST++. These results further highlight the superiority of our meticulously designed DiT architecture and hierarchical spatial-temporal prior estimator. Current Models Fail to Simulate Complex Scenarios. Figure 5. Video-audio synchrony with JavisBenchs taxonomy. Current SOTA models still suffer from challenging scenarios and extensive model comparisons. Additionally, we also evaluate on the AIST++ [22] and landscape [21] datasets for consistency with prior works. Metrics. For JavisBench, we employ multiple metrics across four dimensions: (1) single-modal generation quality, (2) semantic consistency between conditional texts and generated videos/audios, (3) semantic consistency and (4) spatiotemporal synchrony between videos and audios. For the AIST++/Landscape, we follow prior works [42, 48] and report FVD, KVD, and FAD. Compared Methods. To benchmark the audio-video generation performance on JavisBench, we reproduce and compare series of baselines: (1) for cascaded T2A+A2V methods [18, 60], we use AudioLDM2 [27] for the prepositive T2A task; (2) for cascaded T2V+V2A methods [19, 56, 63], OpenSora [65] is adopted to generate videos at first; and (3) for JAVG models, we currently compare with MMDiffusion [42] since others [26, 48, 51, 64] are not currently open-sourced. For AIST++ and Landscape, we directly adopt the results reported in prior works for comparison. Implementation Details. We collect 780K audio-text pairs from multiple sources for audio pertaining by 13 epochs, with 610K video-audio-text triplets from MMTrail [4] and TAVGBench [31] to train ST-Prior Estimator for 1 epoch and JavisDiT for 2 epochs. The learning rate is 1e-5 for ST-Prior Estimator and 1e-4 for DiT. We follow Zheng et al. [65] to leverage bucketing strategy to support dynamic video resolution (from 144P to 1080P) and audio duration (from 2s to 16s) in batches, largely improving application flexibility Table 4. Ablation on module components of our JavisDiT. The specifically designed DiT architecture and ST-priors jointly contribute to single-modal quality and va-synchrony. ST-SA ST-CA Bi-CA AV-Quality AV-Cons AV-Sync 9.371 7.293 6.127 6.581 6.012 1.140 1.155 1.191 1.157 1.201 0.133 0.144 0.167 0.149 0.169 Table 5. Ablation on token number and utilization of ST-priors. We provide more experimental results in Fig. A8 and Tab. A6. Ns Nt Utilize AV-Quality AV-Cons AV-Sync 0 1 16 32 32 32 - 0 CrossAttn 1 CrossAttn 16 CrossAttn 32 32 Addition 32 Modulate 6.581 6.909 6.322 6.012 6.267 6. 1.157 1.188 1.200 1.201 1.183 1.191 0.149 0.163 0.166 0.169 0.159 0.161 Fig. 5 presents the performance of two representative methods (FoleyCrafter[63] and our JavisDiT) across JavisBench categories, showing that existing modelsincluding oursstruggle with AV synchrony in complex scenarios. When the sounding video contains only single sounding object (e.g., person playing the violin alone), JavisScore is generally higher than in multi-object cases (e.g., street performance with multiple musicians), as the latter requires identifying the correct visual-audio correspondence. Similarly, videos with multiple simultaneous events (e.g., dog barking while car horn sounds) yield lower JavisScore than single-event cases (e.g., person clapping), due to the increased challenge of modeling event timing and interactions. Appendix D.5 and Fig. A7 thoroughly highlight the limitations of current JAVG in handling real-world complexity. 5.3. In-depth Analyses and Discussions To efficiently evaluate our proposed method, we perform the 3rd stage (JAVG) training using subset of 60K entries from our entire training data, and test the models on JavisBenchmini (with 1,000 randomly selected samples from JavisBench). For simplicity, we report three normalized scores for clear comparison from three dimensions: AV-Quality: SAVQ = 0.01 SFVD + SKFD + 0.1 SFAD AV-Consistency: SAVC = SAV-IB + SCAVP + SAVHScore AV-Synchrony: SAVS = SJavisScore Well-designed DiT Backbone is Superior. In Tab. 4, we first build vanilla baseline to replace the spatio-temporal self-attention (ST-SA) backbone by UNet [27] for the audio branch, and the generation receives pool AV-Quality at 9.371. After replacing the UNet backbone with ST-SA modules [65], SAVQ immediately improves to 7.293, demonstrating the efficacy of the ST-DiT architecture. Then, we respectively incorporate the spatio-temporal cross-attention (ST-CA) with our ST-Prior and the bidirectional crossattention (Bi-CA) modules for video-audio information sharing. Accordingly, ST-CA brings significant enhancement on AV-Consistency (1.191 vs. 1.155) and AV-Synchrony (0.167 vs. 0.144), more than the simple Bi-CA module (1.157 vs. 1.155 for SAVC and 0.149 vs. 0.144 for SAVS). This verifies our motivation that simple channel-sharing mechanism in AV-DiT [51] cannot effectively build audio-video synchronization, which can be achieved by our proposed fine-grained ST-Prior guidance instead. After bridging STSA, ST-CA, and Bi-CA modules, our JavisDiT reaches the best performance for both single-modal quality (SAVQ = 6.012) and video-audio synchronization (SAVC = 1.201 and SAVS = 0.169), demonstrating the superiority of the welldesigned DiT backbone. Spatio-Temporal Prior is Effective and Generic. In Tab. 5, we take preliminary step to investigate the number of spatiotemporal priors and the way to utilize ST-Priors for better video-audio synchronization. We first take the model without ST-CA modules (the 4th row in Tab. 4) as our baseline (the 1st row in Tab. 5), and add ST-CA modules by gradually increasing the prior number from 1 to 32. According to the results, both single-modal quality (SAVQ) and video-audio synchrony (SAVC and SAVS) consistently increase as the prior number gains. Then, we try to utilize the 32 spatial-temporal priors for addition (adding to video/audio latent representations like conditional embedding) and modulation (mapping priors to scales and biases to modulate video/audio representations). Although the final performance is inferior to the utilization of cross-attention, ST-priors still considerably enhance all metrics (e.g., 0.159/0.161 vs. 0.149 for SAVS). Besides, we further experiment and evaluate the influence of prior numbers and dimensions in Fig. A8, and verify the optimization objectives during ST-priors estimation in Tab. A6. All the empirical results verify the effectiveness and versatility of our estimated spatio-temporal priors. How ST-Prior Ensures Synchronized Video&Audio? Fig. 7 illustrates the synchronization mechanism for our ST-priors to guide the video-audio generation process. In particular, we visualize the spatial-temporal cross-attention map from the last block in JavisDiT at the last sampling step on both video and audio branches. The qualitative results in Fig. 7 show the spatial priors successfully help JavisDiT focus on the subject that would produce the sound (in this case, it is the bubbles rather than the diver that can make the sound), and the temporal priors bring nearly uniform attention scores along the timeline (as the bubbling sound continues from beginning to end). The cross-attention mechanism for ST-priors is well-learned for synchronized video-audio generation. 6. Related Work In the field of AIGC, multimodal generation is an important topic, especially text-conditioned multimodal generation, Figure 6. JavisDiT precisely captures the visual and auditory clues from text inputs to generate faithful sounding-videos with high-quality spatio-temporal alignments. Colored texts are spatial-temporal objects (underlined) and actions. More cases are shown in Appendix E.4. ral alignment, and has thus garnered significant attention [12, 17, 42]. For example, Ruan et al. [42] propose diffusion block that enables interaction between the two modalities, synchronizing denoising processes. Xing et al. [56] propose using ImageBind to align the semantic representations of the two modalities. Sun et al. [48] introduce hierarchical VAE, mapping audio and video to shared semantic space while reducing their dimensionality. AV-DiT [51] employs single DiT model to generate both video and audio modalities simultaneously, improving efficiency. Uniform [64] also takes single DiT for JAVG, by simply concatenating the video and audio latent tokens during the diffusion process, without any explicit synchronization guidance. SyncFlow [26] utilizes STDiT [65] blocks to enhance the video generation quality, with temporal adapter to guide the audio generation process. Unfortunately, current JAVG methods either lack strong backbone for audio and video generation or insufficiently model audio-video synchronization, resulting in suboptimal generation quality. To address these issues, we propose constructing novel DiT-based JAVG system, further enhanced by modeling hierarchical spatial-temporal synchronized prior features. Moreover, this paper aims to further advance JAVG by providing more comprehensive, robust, and challenging benchmark. 7. Conclusion This paper presents JavisDiT, novel Joint Audio-Video Diffusion Transformer that simultaneously generates highquality audio and video content with precise synchronization. We introduce the HiST-Sypo Estimator, fine-grained spatio-temporal alignment module that extracts global and fine-grained priors to guide the synchronization between audio and video. We also propose the JavisBench dataset, comprising 10,140 high-quality text-captioned sounding videos with diverse scenes and real-world complexity, addressing Figure 7. Visualization of cross-attention maps by spatiotemporal priors. Spatial priors successfully capture the sounding subjects (bubbles in this case), and temporal priors accurately cover the whole timeline for the continuous sounding event. such as text-to-image [38, 41, 43], text-to-video [14, 53, 61], and text-to-audio [16, 25, 27] generation, and so on. Among these tasks, the generation of sounding videos [26, 42, 48, 51, 56] has been gaining increasing attention, primarily due to its alignment with real-world application scenarios. To generate synchronized audio and video, early works decomposed the task into two cascaded subtasks. a) generating video from text, followed by adding audio to the video in second step [6, 15, 19, 40, 63]; or b) generating audio first, followed by synchronizing video generation with the audio [18, 60, 62]. As an instance, MovieGen [36] achieves movie-grade generation quality using this cascaded approach. From methodological perspective, the community focus has shifted from UNet-based models [27, 41] to DiT-based methods [36, 65] to achieve more state-of-the-art performance. Another paradigm considers the limitations of the pipeline approach, such as error propagation and reliability issues, and instead focuses on synchronized audio-video joint generation through end-to-end methods, also known as Joint Audio-Video Generation (JAVG). This paradigm emphasizes improving generation quality by modeling audio-video synchronization, including semantic consistency and tempolimitations in current benchmarks. In addition, we introduce temporal-aware semantic alignment mechanism to better evaluate JAVG systems on complex content. Experimental results show that JavisDiT outperforms existing approaches in both content generation and synchronization, establishing new benchmark for JAVG tasks. Potential limitation and future work are discussed in Appendix B."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 5, 16 [2] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 721725. IEEE, 2020. 13 [3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning In International conference on of visual representations. machine learning, pages 15971607. PMLR, 2020. 3 [4] Xiaowei Chi, Yatian Wang, Aosong Cheng, Pengjun Fang, Zeyue Tian, Yingqing He, Zhaoyang Liu, Xingqun Qi, Jiahao Pan, Rongyu Zhang, et al. Mmtrail: multimodal trailer video dataset with language and music descriptions. arXiv preprint arXiv:2407.20962, 2024. 6, 13 [5] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. 5, 19 [6] Marco Comunit`a, Riccardo Gramaccioni, Emilian Postolache, Emanuele Rodol`a, Danilo Comminiello, and Joshua Reiss. Syncfusion: Multimodal onset-synchronized video-toaudio foley synthesis. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 936940. IEEE, 2024. 1, [7] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 736740. IEEE, 2020. 13 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2 [9] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024. 1 [10] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776780. IEEE, 2017. 13 [11] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1518015190, 2023. 4, 5, 13, 19, 20 [12] Akio Hayakawa, Masato Ishii, Takashi Shibuya, and Yuki Mitsufuji. Discriminator-guided cooperative diffusion for joint audio and video generation. arXiv preprint arXiv:2405.17842, 2024. 8 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, [14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Imagen video: Mohammad Norouzi, David Fleet, et al. High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 8 [15] Yuchen Hu, Yu Gu, Chenxing Li, Rilin Chen, and Dong Yu. Video-to-audio generation with fine-grained temporal semantics. arXiv preprint arXiv:2409.14709, 2024. 8 [16] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. In International Conference on Machine Learning, pages 1391613932. PMLR, 2023. 8 [17] Masato Ishii, Akio Hayakawa, Takashi Shibuya, and Yuki Mitsufuji. simple but strong baseline for sounding video generation: Effective adaptation of audio and video diffusion models for joint generation. arXiv preprint arXiv:2409.17550, 2024. 8, 15 [18] Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo, Wonmin Byeon, Sangpil Kim, and Jinkyu Kim. The power of sound (tpos): Audio reactive video generation with stable In Proceedings of the IEEE/CVF International diffusion. Conference on Computer Vision, pages 78227832, 2023. 1, 6, 8 [19] Yujin Jeong, Yunji Kim, Sanghyuk Chun, and Jiyoung Lee. Read, watch and scream! sound generation from text and video. arXiv preprint arXiv:2407.05551, 2024. 6, 8 [20] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119132, 2019. 13, [21] Seung Hyun Lee, Gyeongrok Oh, Wonmin Byeon, Chanyoung Kim, Won Jeong Ryoo, Sang Ho Yoon, Hyunjun Cho, Jihyun Bae, Jinkyu Kim, and Sangpil Kim. Sound-guided semantic video generation. In European Conference on Computer Vision, pages 3450. Springer, 2022. 3, 5, 6, 13, 15, 17 [22] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1340113412, 2021. 3, 5, 6, 13, 15, 17 [23] Minghui Liao, Zhisheng Zou, Zhaoyi Wan, Cong Yao, and Xiang Bai. Real-time scene text detection with differentiable binarization and adaptive scale fusion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 19 [24] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations (ICLR), 2023. 3 [25] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. 8 [26] Haohe Liu, Gael Le Lan, Xinhao Mei, Zhaoheng Ni, Anurag Kumar, Varun Nagaraja, Wenwu Wang, Mark Plumbley, Yangyang Shi, and Vikas Chandra. Syncflow: Toward temporally aligned joint audio-video generation from text. arXiv preprint arXiv:2412.15220, 2024. 2, 6, [27] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. 1, 4, 6, 7, 8, 13, 14, 15, 20, 23 [28] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2025. 4 [29] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations (ICLR), 2023. 1, 13 [30] Xubo Liu, Qiuqiang Kong, Yan Zhao, Haohe Liu, Yi Yuan, Yuzhuo Liu, Rui Xia, Yuxuan Wang, Mark Plumbley, and Wenwu Wang. Separate anything you describe. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. 15 [31] Yuxin Mao, Xuyang Shen, Jing Zhang, Zhen Qin, Jinxing Zhou, Mochu Xiang, Yiran Zhong, and Yuchao Dai. Tavgbench: Benchmarking text to audible-video generation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 66076616, 2024. 3, 5, 6, 13, 15, 16, 17, 19 [32] Irene Martın-Morato and Annamaria Mesaros. What is the ground truth? reliability of multi-annotator data for audio tagging. In 2021 29th European Signal Processing Conference (EUSIPCO), pages 7680. IEEE, 2021. [33] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark Plumbley, Yuexian Zou, and Wenwu Wang. Wavcaps: chatgpt-assisted weaklylabelled audio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. 13 [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 2 [35] Karol Piczak. Esc: Dataset for environmental sound clasIn Proceedings of the 23rd ACM international sification. conference on Multimedia, pages 10151018, 2015. 13 [36] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 1, 8, 13, 26 [37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 4 [38] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2): 3, 2022. [39] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 15 [40] Yong Ren, Chenxing Li, Manjie Xu, Wei Liang, Yu Gu, Rilin Chen, and Dong Yu. Sta-v2a: Video-to-audio generation with semantic and temporal alignment. arXiv preprint arXiv:2409.08601, 2024. 8 [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 8 [42] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. 1, 2, 3, 6, 8, 13 [43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 8 [44] Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. dataset and taxonomy for urban sound research. In Proceedings of the 22nd ACM international conference on Multimedia, pages 10411044, 2014. 13 [45] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. [46] Xuyang Shen, Dong Li, Jinxing Zhou, Zhen Qin, Bowen He, Xiaodong Han, Aixuan Li, Yuchao Dai, Lingpeng Kong, [59] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1 [60] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz, and Yossi Adi. Diverse and aligned audio-to-video generation via text-to-video model adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 66396647, 2024. 1, 5, 6, 8, 15, 20 [61] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 8 [62] Lin Zhang, Shentong Mo, Yijing Zhang, and Pedro Morgado. Audio-synchronized visual animation. In European Conference on Computer Vision, pages 118. Springer, 2025. 1, 8 [63] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024. 1, 6, 7, [64] Lei Zhao, Linfeng Feng, Dongxu Ge, Fangqiu Yi, Chi Zhang, Xiao-Lei Zhang, and Xuelong Li. Uniform: unified diffusion transformer for audio-video generation. arXiv preprint arXiv:2502.03897, 2025. 2, 6, 8 [65] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 2, 4, 6, 7, 8, 12, 13, 14, 17, 24, 26 [66] Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. Audio-visual segmentation. In European Conference on Computer Vision, 2022. 17 Meng Wang, et al. Fine-grained audible video description. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1058510596, 2023. 5, 17 [47] Bob Sturm. The gtzan dataset: Its contents, its faults, their effects on evaluation, and its future use. arXiv preprint arXiv:1306.1461, 2013. 13 [48] Mingzhen Sun, Weining Wang, Yanyuan Qiao, Jiahui Sun, Zihan Qin, Longteng Guo, Xinxin Zhu, and Jing Liu. Mmldm: Multi-modal latent diffusion model for sounding video generation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1085310861, 2024. 1, 2, 3, 6, 8, [49] Yi Tay, Mostafa Dehghani, Vinh Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, et al. Ul2: Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022. 24 [50] Ao Wang, Hui Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Repvit: Revisiting mobile cnn from vit perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1590915920, 2024. 15 [51] Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, and Yapeng Tian. Av-dit: Efficient audio-visual diffusion transformer for joint audio and video generation. arXiv preprint arXiv:2406.07686, 2024. 2, 6, 7, 8 [52] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 5, 19 [53] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, pages 120, 2024. 8 [54] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Forty-first International Conference on Machine Learning. 1 [55] Zhifeng Xie, Shengye Yu, Qile He, and Mengtian Li. Sonicvisionlm: Playing sound with vision language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2686626875, 2024. 1 [56] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visual-audio generation with diffusion latent aligners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71517161, 2024. 1, 2, 6, [57] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 19 [58] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 5,"
        },
        {
            "title": "JavisDiT",
            "content": ": Joint Audio-Video Diffusion Transformer with Hierarchical Spatio-Temporal Prior Synchronization"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Ethical Statement In this work, we construct our JavisBench from publicly available academic datasets as well as YouTube videos. To ensure ethical compliance, we strictly adhere to YouTubes Terms1 of Service and licensing policies. Specifically: 1. Privacy Protection: We have taken measures to remove any personally identifiable or privacy-sensitive information from the collected data. No private, confidential, or user-specific metadata has been retained. 2. Copyright Compliance: All data collection respects the original content licenses. We only utilize publicly accessible videos that are either explicitly licensed for research use or fall under fair-use considerations. No copyrighted content is redistributed or modified in violation of licensing terms. 3. Responsible Data Release: Any potential dataset release will fully comply with YouTubes data policies and ethical guidelines. We will first release the self-curated caption data to support the evaluation of all metrics except the AV-Quality terms in Tab. 2, and then ensure that shared sounding-videos are either appropriately anonymized or restricted in accordance with relevant regulations and platform policies. By implementing these measures, we strive to maintain high ethical standards in data collection, use, and dissemination. B. Potential Limitation and Future Work Despite the strong performance in joint audio-video generation, our JavisDiT has several limitations that present opportunities for future research: 1. Scalability of Training Data: Our model was trained on 0.6M text-video-audio triplets, which, while substantial, is still limited compared to the scale of some large visionlanguage models (e.g., OpenSora [65] takes over 60M data to build foundation mode for video generation). Expanding the dataset with more diverse and higher-quality real-world audio-video samples could further enhance the models ability to generalize across different domains and fine-grained synchronization patterns. 2. Synchronization Evaluation Metrics: While we introduce JavisScore for evaluating audio-video synchronization, its current accuracy of 75% suggests room for improvement. More robust synchronization metricspotentially incorporating perceptual alignment as1https://www.youtube.com/t/terms sessments or human-in-the-loop evaluationcould further refine synchronization quality measurement in JAVG research. 3. Efficiency and Computation Overhead: JavisDiT employs Diffusion Transformer for high-quality generation, but diffusion-based models tend to be computationally intensive. While our approach achieves state-ofthe-art results, generation speed and efficiency remain challenges (e.g., generating 2-second sounding-video at 720P/24fps/16kHz takes 6 minutes on one H100 GPU). Exploring accelerated sampling strategies or hardware optimization could improve efficiency. 4. Benchmarking Across Resolutions and Durations: Our evaluations primarily focus on fixed resolution (240P) and duration (4s) setting in JavisBench. However, realworld applications may require generation at higher resolutions (e.g., 720P, 1080P) and longer durations. Conducting benchmark tests across multiple settings would provide more comprehensive understanding of current models strengths and limitations, benefiting the research community with standardized evaluations. By addressing these limitations, future iterations of JavisDiT could further enhance scalability, synchronization accuracy, efficiency, and adaptability, pushing the boundaries of joint audio-video generation. C. Implementation Details C.1. JavisDiT Model Configuration Model Architecture. As Fig. 2 illustrates, JavisDiT consists of two branches for video and audio generation, each comprising = 28 DiT blocks. Within each DiT block, the latent video and audio representations are processed through several modules, where all attention modules utilize 16 attention heads with hidden size of 1152. The intermediate dimension of the FFN is 4 the hidden size. Each attention and FFN module is preceded by LayerNorm layer. In practical implementation, every attention module is followed by an FFN module to further enhance feature modeling. The video/audio latent is sequentially processed with: SpatialSelfAttn CrossAttn Spatial-CrossAttn Bi-CorssAttn FFN Temporal-SelfAttn CrossAttn Temporal-CrossAttn Bi-CorssAttn FFN for = 28 times, resulting in our JavisDiT with 3.14B parameters in total. Training Strategy. As Sec. 3.3 states, we carefully design three-stage training strategy to achieve high-quality single-modal generation and ensure fine-grained spatioTable A1. Detailed settings for three-stage training. Setting Stage-1 Stage-2 Stage-3 trainable params learning rate warm-up steps weight decay ema decay dropout rate training samples batch size epoch training objective rectified flow contrastive rectified flow GPU days (H100) 29.3M 1e-5 1000 0.0 - 0.0 611K dynamic 1.11B 1e-4 1000 0.0 0.99 0.0 788K dynamic 13 923.8M 1e-4 1000 0.0 0.99 0.0 611K dynamic 2 256 16 8 Figure A1. Stage1: audio pretraining. Parameters are initialized from the video branch. temporal synchrony on generated videos and audios. In particular, the video branch of JavisDiT (including the STSelfAttn module, the Coarse-Graind CrossAttn module, and the FFN module) is initialized from OpenSora [65] and frozen during the whole training stages. We use the weights of the video branch to initialize the audio branch for better convergence (see Fig. A1). The detailed configuration for the three-stage training strategy is displayed in Tab. A1. Training Data Curation. For audio-pretraining (stage1), the audio-caption pairs come from various public datasets, including AudioSet [10], AudioCaps [20], VGGSound [2], WavCaps [33], Clotho [7], ESC50 [35], GTZAN [47], MACS [32], and UrbanSound8K [44]. They contribute to total of 788K training entries. For the ST-Prior estimator and the final JavisDiT training (stage-2&3), the data source comes from two newly-proposed sounding video datasets: MMTrail [4] and TAVGBench [31]. MMTrail provides slightly more caption annotations (2M) than TAVGBench (1.7M) but has lower quality. Due to the resource limit, we collect part of the original sounding-videos from YouTube and filtered out around 80% human-talking videos with the FunASR 2 tool. We finally collect 136K high-resolution video-audio-caption triplets from MMTrail and 475K from TAVGBench, resulting in 611K data for training in total. The data construction for ST-Prior estimators contrastive learning is discussed in Appendix C.2.4. Training Objective and Inference. The ST-Prior Estimator is trained with contrastive learning objectives, which will be detailed in Appendix C.2. For the DiT model, we use rectified flow [29] as the denoising scheduler for better performance [36, 65]. The inference sample step is 30, with classifier-free guidance at 7.0. The video and audio latent 2https://github.com/modelscope/FunASR are concurrently sampled at each inference step. The video encoder-decoder comes from OpenSora [65], and the audio encoder-decoder comes from AudioLDM2 [27]. Both of them are frozen during training. Detailed Evaluation Setup. In Sec. 5 in our manuscript, we conduct comprehensive evaluation across multiple video and/or audio generation models on two settings: (1) our proposed JavisBench benchmark and (2) previously used AIST++ [22] and Landscape [21] datasets. For our JavisBench, generative models are required to generate videos and 240p and 24fps, with audios at 16kHz sample rate. The duration is 4 seconds. For AIST++ [22], we follow the literature [42, 48] to generate 2-second clips at the visual resolution of 256 256, with 240p on landscape [21]. C.2. ST-Prior Estimator Configuration C.2.1. Overview Since ImageBind [11] has built consistent semantic space across multiple modalities, we use ImageBinds text encoder to extract the potential spatio-temporal prior from the input text caption. Formally, for given input text s, we leverage the 77 hidden states from the last layer of ImageBinds text encoder, and learn Ns spatial tokens ps and Nt temporal tokens pt from 4-layer transformer block P. Notably, since the same event may occur at different locations and timestamps in different video-audio generations, the same text can also produce different ST-Priors (ps, pt). We adopt sampling strategy akin to VAE, where our ST-Prior Estimator outputs the mean and variance of Gaussian distribution, from which we sample plausible (ps, pt) to describe specific spatio-temporal events. This can be formalized as: (ps, pt) = Pϕ(s; ϵ), where ϵ refers to the normalized Gaussian distribution. As Fig. 3 states, the ST-Prior Estimator Pϕ is trained with contrastive learning. Specifically, we take the text prompts and estimated prior (ps, pt) as an anchor, and treat the corresponding synchronous video-audio (v, a) pairs from training datasets as positive samples. Then, we either synFigure A2. Architecture of VA-Fuser to encode spatio-temporal embeddings. thesize an asynchronous video or audio a, and take the original or to form negative pairs. Formally, we also extract Ns/Nt positive or negative spatial/temporal embeddings from synchronous )=Eψ(v, a), or asynchronous video-audio pairs: (e )=Eψ(v, a) or Eψ(v, a). Appendix C.2.2 displays more details. , , e+ (e+ The training objective is to make the priors (ps, pt) closer to the synchronous embeddings (e+ ) while pushing them further from the asynchronous (e ), thereby equipping the priors (ps, pt) with fine-grained conditioning capabilities. Several contrastive losses with the Kullback-Leibler (KL) divergence loss are utilized to optimize the spatial and temporal priors (more details refer to Appendix C.2.3): , e+ , Lprior = Lcontrast(ps, e+ Lcontrast(pt, e+ , , The details for negative sample construction are provided ) + Lkl(ps)+ ) + Lkl(pt) . (A1) in Appendix C.2.4. C.2.2. Spatial-Temporal VA-Encoding For given video clip with the corresponding audio, we first use OpenSoras [65] and AudioLDM2s [27] VAE-encoders to respectively map the raw data to the feature space, and derive lightweight VA-Fuser module to extract the spatiotemporal embeddings from the video-audio pair. As shown in Fig. A2, the raw video RT (HW )C will be encoded and reshaped to video embedding = Ev(V ) RTvSvDv , where the downsampling rate for (T ,H,W ) is (4, 8, 8), and the output channel = 4. Similarly, the raw audio will be first transformed to the MelSpectrogram RRM 1, and then encoded to an audio embedding = Ea(A) RTaSaDa . We use linear layer to unify video and audio channels into the hidden size = 512. Then, we align the video and audio embeddings with the pre-defined spatial-temporal token numbers via linear interpolation. Specifically, we will obtain video-spatial embedding vs RTvSD, video-temporal embedding vt RT SvD, audio-spatial embedding as RTaSD, and audio-temporal embedding at RT SaD. Subsequently, we apply bidirectional attention (see Fig. 2(d)) for the spatial va-embedding-pair (vs, as) and the temporal va-embedding-pair (vt, at). After averaging the temporal dimension (Tv, Ta) and merging the video-audio embeddings, we use 2-layer MLP module to project to the desired video-audio spatial embedding es RSD. With similar operations, we can obtain the video-audio temporal embedding et RT D. As defined in Sec. 2, the extracted spatial embedding es should determine the occurrence of specific event in region of the video frame, with the corresponding appearance of matching frequency components in the audio spectrogram. On the other hand, the extracted temporal embedding et should identify the onset or termination of an event at specific frame or timestamp in the video, with the corresponding start or stop of the response in the audio. We achieve these goals by dual contrastive learning objective. C.2.3. Contrastive Training Losses Our core objective is to align the spatial/temporal priors (ps, pt) (extracted from text) more closely with the positive spatial/temporal embeddings (e+ ) derived from synchronized video-audio pairs, while pushing them away from the embeddings (e ) of negative samples (asynchronous video-audio pairs). This ensures that the priors (ps, pt) provide the fine-grained spatio-temporal condition required for synchronized video-audio generation. , , e+ To achieve this, we designed four contrastive loss functions. For simplicity, we omit the subscripts of s, in this part, as we apply the same loss functions on both spatial and temporal priors/embeddings. loss: 1. Token-level hinge Ltoken(p, e+, e) 1.0 sim(p, e+) + 1.0 + sim(p, e), sim refers to the cosine similarity. = where 2. Auxiliary discriminative loss: Ldisc(p, e+, e) = LBCE(Dθ(p, e+), 1)+LBCE(Dθ(p, e+), 0), where Dθ is 1-layer attention module parameterized with θ that use the [CLS] token to gather the information in and e. 3. VA-embedding discrepancy loss: Lvad(e+, e) = 1.0 + sim(e+, e), which aims to enlarge the discrepancy of positive va-embedding e+ and negative e. 4. L2-regularization loss: Lreg(p, e+) = e+2, which directly pushes the prior towards the positive (synchronous) va-embedding e+. Overall, the contrastive learning loss is combined with: Lcontrast(p, e+, e) =Ltoken(p, e+, e)+ Ldisc(p, e+, e)+ Lvad(e+, e)+ Lreg(p, e+) . (A2) Specifically, we take the text prompts and estimated prior (ps, pt) as an anchor, and treat the corresponding synchronous video-audio (v, a) pairs from training datasets as positive samples. Then, we either synthesize an asynchronous video or audio a, and take the original or to form negative pairs. It can be formulated , as (e+ )=Eψ(v, a) or Eψ(v, a). )=Eψ(v, a), (e The VA-Encoder Eψ is detailed in Appendix C.2.2, and the negative sample construction is introduced in Appendix C.2.4. We also discuss the efficacy of each loss function in Appendix E.3 , e+ C.2.4. Negative Sample Construction Given synchronized video-audio pair from the training set, we design two approaches to synthesize easy and hard asynchronous videos/audios for optimization. (1) For easy negatives, we take AudioLDM2 [27] to generate arbitrary audios from the text without referencing the corresponding video, which naturally results in asynchronous video-audio pairs. (2) For hard negatives, various augmentation strategies are employed to add, remove, or modify elements in either the original video or audio from GT-pairs, creating more finegrained spatio-temporally asynchronous pairs. As detailed below, these methods further enhance the discriminative power of the ST-Prior, and Fig. A3 and Fig. A4 showcase some representative augmentation examples. Video Spatial Augmentation Random Masking (Fig. A3(A)). Videos are divided into 6 6 grids, with mask ratio uniformly sampled from (0.2, 0.8)). proportion of the grids is randomly masked. Adding Subject Trajectories (Fig. A3(B)). The trajectories from the SA-V dataset [39] are overlaid to simulate new sound-producing objects. Trajectories are preprocessed using RepViT-SAM [50] for interpolation from 6fps to 24fps, retaining objects with an average size > 32 32 pixels. During augmentation, random trajectory is selected from the pool and added to the video. Video Temporal Augmentation Video Temporal Shifting (Fig. A3(C)). The video is cyclically rearranged from random starting point to create temporal misalignment. Video Pausing (Fig. A3(D)). random frame is duplicated for at least 0.5 seconds at chosen point, pausing the video and disrupting temporal synchronization. Audio Spatial Augmentation All audio augmentations (except temporal shifting) focus on sound-producing intervals, avoiding silent or noisy segments. We use QwenPlusAPI3 to extract potential soundproducing objects from audio descriptions, and utilize AudioSep [30] to separate sound sources and labels intervals using RMS. Separation results are precomputed and stored as metadata, guiding augmentations on target audios. Audio Source Addition (Fig. A4(A)). Additional audio from other dataset sources is mixed with the original one, causing spatial content misalignment between audio and video. Audio Source Removal (Fig. A4(B)). separated sound source from the original audio is randomly removed to induce more spatial misalignments. Audio Volume Adjustment (Fig. A4(C)). The target audios volume is modified using randomly selected transformation (e.g., cosine, sine, linear increase/decrease) to adjust its amplitude dynamically. Audio Temporal Augmentation Audio Temporal Shifting (Fig. A4(D)). Audio is shifted by extracting new interval from the original, creating temporal misalignment. Silent Segment Insertion (Fig. A4(E)). Silent segments are inserted into the target interval, disrupting synchronization with the video timeline in subsequent frames. Repeated Segment Insertion (Fig. A4(F)). Target segments are repeated after random transformation interval, making another type of timeline misalignment. Audio Speed Adjustment (Fig. A4(G)). Playback speed within the target interval is altered to 0.5 or 2, causing another type of timeline desynchronization. D. More Details on JavisBench D.1. Motivation strong generative model must produce diverse video content and audio types while ensuring fine-grained spatiotemporal synchronization. However, we have noticed that current benchmarks and metrics lack sufficient diversity and robustness for comprehensive evaluation. For instance, the commonly used AIST++ [22] and Landscape [21] benchmarks focus only on limited scenarios (human dancing for AIST++ and natural scenarios for landscape) with merely 20100 test samples, and the video-audio synchronization metric AV-Align [60] cannot produce accurate evaluation on complicated scenarios, e.g., with multiple sounding events [17, 31]. 3https://help.aliyun.com/zh/model-studio/developer-reference/useqwen-by-calling-api Figure A3. Video augmentation for spatial and temporal negative samples. On the other hand, although TAVGBench [31] provides 3,000 test samples (but not released yet), it lacks detailed analysis of different scenarios in practical audio-video generations. We are therefore motivated to propose more challenging benchmark to evaluate generative models from various dimensions on larger data scale in Sec. 4.1, and also introduce robust metric to quantify the fine-grained spatio-temporal alignment between generated video and audio pairs in Sec. 4.2. Here we supplement more details of the benchmark construction and metric verification. D.2. Taxonomy To comprehensively evaluate the capabilities of joint videoaudio generation models, we designed five evaluation dimensions from different aspects: (1) Event Scenario, (2) Video Style, (3) Sound Type, (4) Spatial Subjects, and (5) Temporal Composition. Leveraging GPT-4 [1], we enumerated 3-5 primary categories under each aspect, contributing to hierarchical categorization system with 5 dimensions and 19 categories. Tab. A2 presents the detailed definitions and clarifications. Figure A4. Audio augmentation for spatial and temporal negative samples. D.3. Data Curation D.3.1. Collection We collected data from two sources to construct the benchmark. First, we incorporate existing datasets test sets, including those in the JAVG domain like Landscape [21] and AIST++ [22] (despite their limited coverage) and others from sounding-video understanding tasks such as FAVDBench [46] and AVSBench [66]. Second, we expanded the benchmark by crawling videos uploaded to YouTube between June 2024 and December 2024 to prevent data leakage in previous methods training scope [31]. Using the previously defined categorization system, we prompt GPT4 to generate potential keywords for specific categories, enabling targeted video collection and avoiding indiscriminate gathering of homogeneous data, which significantly improves curation efficiency. This stage yields around 30K sounding video candidates. D.3.2. Quality-based Pre-Filtering Following OpenSora [65], we utilize series of filtering tools to improve data quality, including: 1. Scene Cutting. Given the extended duration of some YouTube videos (up to several hours), we employed PySceneDetect4 for scene detection and segmented the videos at identified scene transitions. Each clip was con4https://github.com/Breakthrough/PySceneDetect Table A2. Clarification of the category taxonomy of our JavisBench. Aspect Category Description and Examples Event Scenario Visual Style Sound Type Spatial Composition"
        },
        {
            "title": "Camera Shooting",
            "content": "2D-Animate 3D-Animate Ambient Sounds Biological Sounds Mechanical Sounds Musical Sounds Speech Sounds Single Subject Multiple Subject Off-screen Sound Single Event Temporal Composition Sequential Events Scenes dominated by natural environments with minimal human interference, such as forests, oceans, and mountains. Outdoor spaces shaped by human activity, including cities, villages, streets, and parks. Indoor environments where daily human activities occur, like houses, schools, and shopping malls. Work-oriented spaces related to industrial or energy activities, such as factories, construction sites, and mines. Imaginative or abstract settings, including virtual worlds, sci-fi cities, and artistic installations. Filmed with handheld, fixed, or drone cameras, including slow-motion footage. Styles like hand-drawn animation, flat animation, cartoon styles, or watercolor illustrations. Photorealistic styles, sci-fi/magical effects, CG (Computer Graphics), or steampunk aesthetics. Sounds that occur naturally in the environment, including both natural and human-influenced surroundings. This category includes sounds like wind, rain, water flow, animal sounds, human activity (e.g., traffic, construction), and urban noise. Sounds produced by living creatures (e.g. animals, birds). This includes vocalizations such as barking, chirping, growling, as well as non-vocal human sounds like heartbeat, and other physical noises. Sounds generated by man-made machines, devices, or mechanical processes. This includes the noise of engines, motors, appliances, and any mechanical or electronic noise. This category also includes malfunction sounds (e.g., malfunctioning machinery or alarms). Sounds related to music or musical performance, including both humangenerated and instrument-generated sounds and melodies. This category covers singing, instrumental performances, as well as background music used in various media formats. Sounds generated from human speech, whether in conversation, dialogue, public speeches, debates, interviews, or monologues. This category specifically covers linguistic communication in various contexts, whether formal, informal, or contentious. There is only one primary object or source producing sound in the scene. There are multiple primary objects that (or potentially can) make sounds in the scene. The source of the sound is not visible in the scene but logically exists (e.g., car engine outside the camera view). The audio contains only one event, with no overlapping sounds. For example, single dog barking without background noise. There are multiple events occurring sequentially, with no overlap. For example, the applause begins after the music performance ends. Simultaneous Events Multiple audio sources are present simultaneously, such as person speaking while music plays in the background. Table A3. Data source composition before and after series of filtering strategies of our JavisBench YouTube FAVDBench AVSBench Landscape AIST++ Total Before Filtering After Filtering 30,107 8,507 1,000 804 680 100 100 20 20 32,031 10,140 strained to length of 260 seconds, resulting in approximately 230K sounding video clips. 2. Aesthetic Filtering. We filter out videos with aesthetic scores [45] lower than 4.5, remaining in 70K clips. 3. Optical-flow Filtering. We use UniMatch [57] to estimate the motion quality of the given videos, and remove the (static) videos whose score is lower than 0.1. This produces 46K candidates. 4. OCR Filtering. We use DBNet [23] to detect and filter out the videos containing more than 5 text regions, resulting in smaller 30K scope. 5. Speech Filtering. As the Internet-available videos contain too much human speech (including talking and voiceover), we use FunASR5 to detect and remove the speech videos. In this step, there are still around 20% speech videos remaining, due to the non-perfect speech detection by FunASR. We keep this part of videos in our final benchmark to ensure the diversity of video-audio sources. This step leads to 22.3K sounding video clips. D.3.3. Annotation Since most of the collected data lacks captions, we designed generic pipeline to generate captions for the video-audio pairs, and then categorize them into corresponding classes from different aspects, as illustrated in Appendix D.2. First, we use Qwen2-VL-72B [52] to generate detailed captions for videos. Metadata from the data source (e.g., object labels or YouTube keywords) is included in the context to enhance caption quality. Next, we use Qwen2-Audio-7B [5] to caption the audio in detail. We do not incorporate the previously generated video captions as contextual input, as it produces even more hallucinations with the input bias (i.e., wrongly identifying sound that will happen in corresponding visual scenarios but actually does not exist in the current audio). Then, we employ Qwen2.5-72B-Instruct [58] to merge video and audio captions into unified text prompt, serving as the textual condition for the JAVG task. During this process, Qwen2.5-72B is prompted to reason on the video-audio captions and identify apparent logical mistakes (e.g., there is dog in the video but Qwen2-Audio gives sounds from car engine) or missing captions (e.g., unknown sound source). Finally, we query Qwen2.5-72B-Instruct again to classify the data points based on the video, audio, and generated captions, assigning each entry to the appropriate 5https://github.com/modelscope/FunASR category in the classification system. We do not prompt multimodal LLMs to do this categorization because the generative models to evaluate will only receive the text caption as inputs. After removing the logical conflict captions and classification results that fail to parse, we obtain 19.4K sounding video clips with detailed captions and hierarchical categorization results. D.3.4. Content-based Post-Filtering To build diversified and balanced benchmark to evaluate joint audio-video generation, we conduct another postfiltering based on the categorization results obtained above. In particular, we further remove videos that only contain background music and speech voice. By doing so, nearly half of videos are filtered out, since large part of YouTube videos rely on music and voice acting to attract viewers. After human checking, we obtained 10,140 samples in our JavisBench with diverse data sources and fine-grained category annotations, setting new standard to facilitate comprehensive evaluation for future joint audio-video generation (JAVG) methods. The category statistics can be found in Fig. 4, and Tab. A3 displays the data sources before and after our filtering strategies. D.4. Extended Specification on JavisScore Evaluation Suite is chunked into several D.4.1. Implementation Details Given generated video-audio pair (V , A), we use ImageBind [11] to estimate audio-visual synchrony with following steps: 1. Temporal Segmentation via Sliding Windows. (V , A) segments with 2-seconds window size and 1.5-seconds overlap (C = {(V1, A1), (V2, A2), (VW , AW )}). The 2second window ensures compatibility with the audio encoders minimum processing length [11], preventing suboptimal feature extraction. The 1.5-second overlap enhances continuity and robustness, allowing each frame to be evaluated within multiple temporal contexts. This mitigates artifacts caused by abrupt segmentation boundaries. 2. Frame-wise Audio-Visual Similarity Computation. Inspired by Mao et al. [31], we calculate the cosine similarity between each frames (F = {Vi,1, Vi,2, Vi,w}) and the whole audio clip Ai by using ImageBinds vision and audio encoders (Ev, Ea). Frame-wise similarity captures fine-grained temporal dynamics, ensuring transient events (e.g., lip movements, object interactions) are accurately modeled. 3. Segment-wise Synchronization Estimation. Instead of averaging all similarity scores, we select the 40% least synchronized frames (with lower similarities) and compute their mean to obtain the synchronization score for each window. By focusing on the least synchronized frames, the metric becomes more sensitive to local resynchronization, as simple mean can be biased by considerable proportion (e.g., 70%) of synchronized frames. 4. Global Synchronization Estimation. The final JavisScore is computed by averaging the window-level scores across all segments, balancing local variations while maintaining sensitivity to desynchronization patterns. Due to window overlap, each video frame is evaluated multiple times, reducing the influence of outliers and providing more stable final score. These steps can be formulated as: SJavis = 1 (cid:88) i= σ(Vi, Ai), σ(Vi, Ai) = 1 (cid:88) j=1 top-k min {cos (Ev(Vi,j), Ea(Ai))} . (A3) This approach effectively differentiates synchronized and desynchronized video-audio pairs. D.4.2. Verification on Metric Quality This paper provides quantitative comparison between the accuracy of video-audio synchrony metrics. We constructed validation dataset consisting of 3,000 audio-video pairs to evaluate the effectiveness of our proposed metric, JavisScore. The validation set is initialized by 1,000 positive (synchronous) video-audio pairs from our JavisBench dataset, and we construct around 2,000 negative pairs from three different sources: 1. The first 1,000 negative pairs are generated through online augmentation/transformation on the positive pairs. Given synchronized video-audio pair (V +, A+), we reuse the augmentation strategies in Appendix C.2.4 to generate asynchronous video-audio pairs, i.e., (V +, A) or (V , A+). The four augmentation types (video-spatial, video-temporal, audio-spatial, audio-temporal) produce 250 negative samples, resulting in the first 1,000 negative pairs. 2. The second 500 negative pairs come from separate generations. In particular, we randomly split the 1,000 positive samples into two parts, and select the first 500 samples to prompt AudioLDM2 [27] to generate audios conditioned solely on text captions, without accessing the corresponding videos. This naturally leads to 500 asynchronous video-audio pairs. 3. The third 500 negative video-audio pairs are generated Table A4. Comparison of VA-Synchrony Metrics Metric AUROC Accuracy Random-Guess AV-Align JavisScore 0.5000 0.5296 0. 0.5000 0.5254 0.7514 by preliminary JAVG model conditioned on the other 500 text prompts from positive samples. These generated pairs were then manually annotated to determine whether they were synchronized or not. We finally obtained 411 hard negative samples with 89 positive video-audio pairs. In the evaluation dataset, each positive sample is related to 2 negative video-audio pairs. One is constructed by online augmentation on the positive entry (the first negative source), and the other is generated from text captions of the positive data point (the second or third negative source). Then, we compare our proposed JavisScore with the previously-used AV-Align [60] metric on the 3,000 evaluation samples, and the result is displayed in Tab. A4. We calculate the AV-Align scores and our JavisScore on all the 3000 samples, and compute the AUROC value for the binary classification (ideally, VA-synchrony metric should produce higher scores on positive video-audio pairs), which shows JavisScore achieves approximately 0.13 higher AUROC than AV-Align. Then, as each positive sample is related to 2 negative video-audio pairs, we can calculate the prediction accuracy of 1000 2 = 2000 paired positive-negative samples. It is viewed as correct when metric assigns higher score on the positive sample than the corresponding negative video-audio pair. According to Tab. A4, our metric significantly surpasses AV-Align by 23% accuracy, demonstrating the efficacy of video-audio synchrony measurement. Notably, AV-Align nearly performs as random-guess (with near 0.5 AUROC and Accuracy). This is because AV-Align simply uses optical flow to capture the video dynamics, and match with audio dynamics estimated by onset detection results. If there is concurrent pulse at the video and audio timelines, AV-Align will produce high synchrony score. However, in real-world scenarios, the video optical flow and audio onset detection cannot precisely capture the start and end of the visual-audio event. As exemplified in Fig. A5, when person in the video speaks, the movements of their mouth are subtle and often too minimal to be effectively captured by optical flow. Moreover, in environments with strong background noise (e.g., TV programs), the onset of human talking also becomes difficult to detect. The scores generated by AV-Align are unreliable in such complex scenarios. In contrast, our proposed JavisScore leverages the high-dimensional semantic space of ImageBind [11] to compute synchronization at the second-level, robustly distinguishing between synchronized and unsynchronized cases. It is worth noting, however, that our metric does not achieve 100% accuracy. Developing more precise evaluation metFigure A5. Complex Scenario with multi-source sounds at time. Figure A6. Parameter sensitivity evaluation of our JavisScore. Our method presents stable and robust video-audio synchrony estimate at various settings. We finally choose (2-second window size, 1.5-second overlap, topmin-40%) due to the relatively better performance. ric remains significant challenge in the JAVG domain, and we hope this work will inspire further advancements in this critical area. Besides, we also conducted an ablation study to evaluate the parameter sensitivity of our JavisScore, including the sliding window size, overlap length, and score computation strategy. As suggested by Fig. A6, the optimal parameters were identified as sliding window size of 2, an overlap length of 1.5, and selecting the top 40% minimum strategy. However, other settings do not significantly reduce the performance of our metric. The worst accuracy, for example, still achieves around 74% and outperforms AV-Align of 52.5% by large margin. The results further demonstrate the robustness of our JavisScore metric. D.5. In-depth Analysis of Evaluation Results Fig. A7 provides more fine-grained, multi-dimensional, and comprehensive evaluation of our model on JavisBench, offering an in-depth analysis of the current SOTA models limitations in real-world joint audio-video generation tasks. Based on these results, we summarize the following key insights: Insufficient unimodal modeling capability in rare scenarios: (1) The FVD in Event Scenario (1st row, 1st column) indicates poor video quality in industrial and virtual scenes, suggesting weaker modeling capability in these domains. (2) The FVD in Visual Style (1st row, 2nd column) shows significant disparity between generated and real videos in 3D-animate scenes, potentially due to insufficient training data. (3) The FAD in Sound Type (r3c3) suggests weaker audio modeling in ambient, biological, and mechanical categories, likely because these categories are too broad. This observation aligns with Event Scenario (r3c1), where natural and industrial scenes also exhibit high FAD values. Unimodal quality does not directly correlate with text consistency: (1) The TV-IB in Visual Style (r4c2) reveals that 2D/3D-animate scenes exhibit poor text-following capability, despite their unimodal quality. (2) The CLAPScore in Sound Type (r7c3) suggests weak audio-text alignment in ambient and mechanical scenes, potentially due to lack of corresponding audio-text pairs in the Figure A7. Metric distribution on all JavisBenchs taxonomy. Table A5. Evaluation on audio generation. After sufficient training iterations at stage 1, our JavisDiT presents moderate audio generation performance on in-domain test set AudioCaps [20] and comparable quality on out-of-domain test set JavisBench-mini. Method AudioCaps (InD) JavisBench-mini (OoD) FAD TA-IB CLAP FAD TA-IB CLAP AudioLDM2 [27] JavisDiT-audio-ep13 JavisDiT-audio-ep 2.01 5.88 5.19 0.205 0.145 0.164 0.487 0.319 0.356 4.62 5.11 4.68 0.198 0.165 0.194 0.428 0.401 0. first-stage (Audio Branch) training data. Poor AV-synchronization in challenging scenarios: (1) The JavisScore in Event Scenario, Visual Style, and Sound Type (r10c1-3) show that scenarios with poor unimodal quality also suffer from weak audio-video synchronization, particularly in virtual environments, 2D/3Danimate styles, and musical sound types. (2) The JavisScore in Spatial and Temporal Composition (r10c4-5) indicate significantly lower AV synchronization performance for complex events (e.g., sequential/simultaneous events, multiple sound sources, or off-screen sounds) compared to simpler scenes (e.g., single events with single sound source). In conclusion, current SOTA models still struggle with rare and complex scenarios, both in terms of audio-video generation quality and synchronization performance. The JAVG community still faces significant challenges in bridging the gap between research models and real-world applications. E. Additional Experiments E.1. Evaluation on Audio Generation Quality Tab. A5 presents the evaluation of our models performance in the first training stage, concerning both unimodal textto-audio generation quality and text-following capability. Specifically, we employ two datasets for evaluation: AudioCaps test set [20]: Filtered by AudioLDM2 [27], this dataset consists of 964 samples, each 810 seconds long. Models are required to generate 10-second audio clips. JavisBench-mini: subset containing 1,000 samples from JavisBench ranging from 2 to 10 seconds in length, which is also utilized in ablation studies in Sec. 5.3). Models are required to generate 4-second audio clips. On these datasets, we first analyze the performance progression of our model across different training stages and then compare it against the current state-of-the-art (SOTA) model, AudioLDM2 [27]. As shown in Tab. A5, with increased training iterations (from epoch 13 to epoch 55), our model demonstrates improvements in both audio generation quality (e.g., FAD decreases from 5.88 to 5.19 on AudioCaps) and text consistency (e.g., TA-IB increases from 0.165 to 0.194, while CLAPScore improves from 0.401 to 0.432 on JavisBenchmini). Furthermore, our model achieves comparable audio generation performance to AudioLDM2 on JavisBench-mini, further validating the effectiveness of our DiT-based architecture. Notably, our model performs worse than AudioLDM2 on AudioCaps, which can be attributed to two primary factors: (1) Training strategy: AudioLDM2 is explicitly trained for 10-second audio generation, whereas our model supports variable-length audio generation. Consequently, our model currently lacks sufficient training data and iterative steps specifically for 10-second audio generation. (2) Potential data leakage: The training data of AudioLDM2 includes the training set of AudioCaps, leading to smaller in-domain test gap, which increases the risk of overfitting. In contrast, JavisBench is collected from more diverse real-world YouTube audio sources, and its domain-specific soundscapes are not exposed to either AudioLDM2 or our model during training. Thus, performance on the out-of-domain JavisBench-mini provides more realistic reflection of models usability in real-world scenarios. In future iterations, we will continue enhancing the unimodal audio generation quality, as this is crucial prerequisite for achieving precise video-audio synchronization. E.2. Investigation of ST-Priors Extraction and Representation In Fig. A8, we investigate the representation capability of ST-priors from three aspects. Firstly, since the latent representations of video (v) and audio (a) contain different numbers of spatial and temporal tokens (e.g., for 240P, 24fps, 16kHz, 4-second sounding video, R40030Cv while R1664Ca ), we also experimented with various token allocation strategies for the spatial and temporal priors. These included ratios such as 1:2 (n16x32), 1:1 (n32x32), and 2:1 (n32x16). As shown in Fig. A8, the 1:1 ratio achieved the best performance. This could be attributed to the inherent disparity in the latent shapes: video latent contains significantly more spatial tokens than temporal tokens, while audio latent stays in the opposite situation. Thus, balanced 1:1 prior token ratio helps minimize bias. Next, using the 1:1 ratio, we explored different configurations of prior numbers and dimensions. The results in Fig. A8 indicate that increasing the prior number and diFigure A8. Further ablation on ST-Prior hyper-parameters, including (1) spatial-temporal token ratio, (2) token number, and (3) embedding dimension. Our default setting of n32x32-d128 is good trade-off between performance and training cost. Table A6. Ablation of ST-Prior loss functions. All training objectives jointly contribute to video-audio synchronization. Ltoken Ldisc Lvad Lreg AV-Consistency AV-Synchrony IB-AV CAVP AVHScore AV-Align JavisScore 0.190 0.193 0.196 0.202 0.211 0.799 0.799 0.800 0.800 0. 0.167 0.170 0.174 0.179 0.190 0.097 0.102 0.096 0.107 0.122 0.149 0.151 0.156 0.159 0.169 mension consistently improves performance, demonstrating the scalability of our approach. In this work, we chose the n32x32+d128 configuration as it offers good trade-off between performance and training cost, considering the diminishing returns of further scaling. E.3. Training Losses for ST-Priors Estimation In Appendix C.2.3, we introduce four loss functions to train our ST-Prior Estimator: (1) Token-level hinge loss: Ltoken, (2) Auxiliary discriminative loss: Ldisc, (3) VA-embedding discrepancy loss: Lvad, and (4) L2-regularization loss: Lreg. This section presents detailed ablation study on the efficacy of utilized loss functions. According to Tab. A6, Ldisc brings slight improvement in addition to the original Ltoken (e.g., 0.193 vs. 0.190 for IB-AV), as they share the same optimization purpose pushing the text prior anchor to the positive video-audio samples while pushing away from negative samples and differ only in the specific gradient back-propagation mechanism. On the other hand, Lvad considerably enhances the synchrony of generated video-audio pairs (e.g., 0.156 vs. 0.151 on JavisScore), thanks to its ability to maximize the divergence between positive and negative video-audio samples themselves. However, Lreg offers even greater benefits due to its smoother regularization, facilitating the convergence of the text prior to the positive embeddings. Moreover, the combination of all loss functions achieves the best performance (e.g., 0.169 of JavisScore), as they collaboratively address the same goal from different perspectives: embedding video-audio synchrony into the text prior and ensuring it captures the semantic meaning of synchronization. This comprehensively validates both our motivation and the effectiveness of our methodology. E.4. More Generation Examples Fig. A9 presents realistic audio-visual pairs generated by our JavisDiT across diverse scenarios, including industrial, outdoor, and natural environments. These results encompass wide range of visual styles, such as camera-captured footage, 2D/3D animations, as well as various audio types, including mechanical, musical, ambient, and biological sounds. Our model effectively maintains video-audio synchronization across cases involving single or multiple sounding subjects, as well as single, sequential, or simultaneous sounding events. All multimedia resources are available in the supplementary materials. F. Extension to X-conditional Generation F.1. Masked Training for Joint Video-Audio Generation Built on diffusion models with transformers (DiT), our JavisDiT can be easily extended to support various conditional video-audio generation tasks, as shown in Fig. A10. Inspired by UL2 [49] and OpenSora [65], we propose dynamic masking strategy to support video and audio conditioning, Figure A9. Extensive JAVG cases on diverse event scenarios, visual styles, audio types, sounding subjects, and temporal compositions. Our JavisDiT achieves high-quality and text-consistency for single-modality generation and keeps good video-audio synchronization. Figure A10. Masking strategies for X-conditional generation. The DiT architecture allows feasible conditional video-audio generation by replacing the noisy latent representation with reference videos and/or audios with specific strategies during training and inference. audio differs from the ground truth, it retains high quality, textual consistency, and precise video-audio synchronization. AI2V/I2AV: When given the first frame of an image (I2V), our model can animate the image, generating videos that resemble the ground truth (GT) more closely while being distinct from those in A2V. In cases where both audio and the initial image are provided (AI2V), the generated video aligns more closely with the ground truth. Conversely, when only an image is given (I2AV), the generated audio resembles the V2A results. AV-Ext: When provided with short clip (e.g., 2 seconds) of audio and video, our model effectively understands the preceding content and generates semantically and contextually consistent continuations. This capability is particularly beneficial for generating long videos [36, 65]. By employing stepwise extension strategy, the model mitigates challenges such as memory overflow and difficulties in ensuring continuity when directly generating longer videos (e.g., 1 minute). Thanks to our proposed dynamic masking strategy, in addition to standard JAVG tasks, our model also supports various modality-conditional generation tasks, opening up new possibilities for wide range of applications within the research community. from the basic (I) text-to-audio-video (t2av) generation to (II) audio-to-video (a2v) generation, (III) video-to-audio (v2a) generation, (IV) audio-image-to-video (ai2v) generation, (V) image-to-audio-video (i2av) generation, and (VI) audio-video-extension (av ext) generation. Note that the text condition still works in all kinds of conditional generations, which provides both coarse-grained global semantic embeddings and fine-grained spatio-temporal priors on targeted visual-sounding events. In particular, we unmask the specific video and frames to be conditioned on for X-conditional generation. During both model training and inference, unmasked frames will have timestep 0, while others remain the same (t): Unmasking all video/audio frames for v2a/a2v generation. Unmasking the first frame of the video (and all audio frames) for i2av (and ia2v) generation. Unmasking preceding part of video and audio frames for va-extension. We randomly apply one of the last five (from (II) to (IV)) masking strategy during training with total probability of 30%, and perform the original text-to-audio-video generation task on the remaining 70% samples. F.2. Application to X-conditional Generation Fig. A11 illustrates list of representative x-conditional cases: A2V/V2A: In the audio-to-video (A2V) mode, due to the inherent randomness of the diffusion process, the model generates diverse outputs while maintaining spatial consistency (e.g., man in blue playing blues guitar piece) and temporal synchronization (e.g., the sound is produced precisely when the fingers strum the strings). Similarly, in the video-to-audio (V2A) mode, although the generated Figure A11. Zero-shot X-conditional generation cases. Our model ensures diversified audio-video generation while accurately generating based on various conditions. (1) For A2V/V2A tasks, our models generation differs from ground-truth video/audio but maintains spatial consistency and temporal synchronization (similar finger movements and approximate start/end timestamps). (2) For AI2V/I2AV tasks, we nearly recover the ground-truth video in AI2V while keep similar audio results between I2AV and V2A. (3) For AV-Ext, our model produces consistent continues but differs from both GT-AV and I2AV results."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of Rochester",
        "University of Science and Technology of China",
        "Zhejiang University"
    ]
}