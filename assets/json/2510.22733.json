{
    "paper_title": "$\\text{E}^2\\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker",
    "authors": [
        "Qi Liu",
        "Yanzhao Zhang",
        "Mingxin Li",
        "Dingkun Long",
        "Pengjun Xie",
        "Jiaxin Mao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework $\\text{E}^2\\text{Rank}$, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, $\\textrm{E}^2\\text{Rank}$ achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy."
        },
        {
            "title": "Start",
            "content": "Preprint. E2RANK: YOUR TEXT EMBEDDING CAN ALSO BE AN EFFECTIVE AND EFFICIENT LISTWISE RERANKER Qi Liu1,2, Yanzhao Zhang2, Mingxin Li2, Dingkun Long2, Pengjun Xie2, Jiaxin Mao1 1Renmin University of China 2Alibaba Group qiliu6777@gmail.com, maojiaxin@gmail.com https://Alibaba-NLP.github.io/E2Rank"
        },
        {
            "title": "ABSTRACT",
            "content": "Text embedding models serve as fundamental component in real-world search applications. By mapping queries and documents into shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose simple yet effective unified framework E2RANK, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, E2RANK achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy. 5 2 0 2 6 2 ] . [ 1 3 3 7 2 2 . 0 1 5 2 : r Figure 1: (a) Overview of E2RANK. (b) Average reranking performance on the BEIR benchmark, E2RANK outperforms other baselines. (c) Reranking latency per query on the Covid dataset. Corresponding Author 1 Preprint."
        },
        {
            "title": "INTRODUCTION",
            "content": "Text embedding and reranking are fundamental components in numerous natural language processing (NLP) and information retrieval (IR) applications, including web search, question answering, retrieval-augmented generation, and beyond (Karpukhin et al., 2020; Zhao et al., 2024). In general, most production IR systems adopt two-stage architecture: lightweight embedding retriever retrieves small candidate set, which is then reranked by more powerful reranking model (Matveeva et al., 2006). In the first stage, text embedding offers efficient similarity search abilities by mapping queries and documents into shared low-dimensional vector space, enabling real-time and web-scale applications (Karpukhin et al., 2020). The advent of large language models (LLMs) has further improved the retrieval performance of these embedding models (Zhu et al., 2023; Ma et al., 2023; BehnamGhader et al., 2024). However, performance gap persists between embedding-based retrievers and state-of-the-art rerankers, particularly those using LLMs (Zhu et al., 2023). Specifically, listwise methods like RankGPT (Sun et al., 2023) can model fine-grained interactions within the entire candidate set and capture both query-document and document-document relationships, leading to rankings that better reflect human judgment and achieving state-of-the-art results across various benchmarks (Sun et al., 2023; Pradeep et al., 2023). Despite their effectiveness, LLM-based listwise rerankers incur high computational costs and inference latency, limiting their deployment in real-time environments. The need to encode all candidates in single pass introduces substantial prefilling delays, while autoregressive decoding further slows the process (Liu et al., 2025b). Therefore, some recent works tried to improve the efficiency of listwise rerankers by compressing the input documents (Liu et al., 2025b) and leveraging LLMs output logits or attention patterns to avoid expensive auto-regressive generation (Reddy et al., 2024; Chen et al., 2024b; Zhang et al., 2025b). Among the above works, an important observation is that the auto-regressive generation paradigm adopted by RankGPT (Sun et al., 2023) is not necessary for ranking, while the interaction between query and documents in the context is critical for ranking effectiveness (Chen et al., 2024b). Additionally, Liu et al. (2025b) shows that incorporating document embeddings in the ranking process is also helpful. Based on these, we naturally raise the following question: What if incorporating the interaction signals in embedding models for reranking? Intuitively, this question can be addressed from two complementary perspectives. From the standpoint of dense retrieval, the listwise prompt integrating both the document and the query can be viewed as form of pseudo relevance feedback (PRF) (Xu & Croft, 1996) query in traditional IR, which can enhance the quality of query embeddings (Yu et al., 2021). Conversely, from the perspective of listwise reranking, the rich contextual information encoded in the listwise prompt enables the use of simple cosine similarity in place of autoregressive decoding. In essence, the listwise prompt can be transformed into single PRF-enhanced query embedding, allowing reranking to be efficiently performed via cosine similarity against precomputed document embeddings. This leads to unified scoring mechanism and unified model that seamlessly bridges retrieval and reranking. We then introduce E2RANK (Efficient Embedding-based Ranking or Embedding-to-Rank) and propose two-stage process to train the unified model, shown in Figure 1. First, we train an embedding model via contrastive learning, then continue to train it under multi-task learning framework that jointly optimizes contrastive and ranking objectives. Specifically, we use the listwise prompt as pseudo query and adopt the RankNet loss (Burges et al., 2005) for optimization. This multitask approach encourages the embedding space to capture both query-document relevance and full interactions. At inference time of reranking, we only compute cosine similarity between document embeddings and the optimized query representation derived from the listwise prompt. This unified design offers advantages for both efficiency and effectiveness. First, by operating in the embedding space instead of generation, it eliminates the computational overhead of LLM-based rerankers, enabling low-latency inference suitable for large-scale applications. Second, the full interaction between query and documents and richer training signals substantially enhances reranking quality. We evaluate E2RANK on popular reranking and embedding benchmarks. Experimental results demonstrate that our model achieves state-of-the-art reranking performance on BEIR (Thakur et al., 2021) and exhibits strong performance on reasoning-intensive benchmark BRIGHT (Su et al., Preprint. 2025), while notably improving inference efficiency. Additionally, trained solely on public data, our model preserves competitive embedding performance on MTEB (Muennighoff et al., 2022), demonstrating the effectiveness of unifying retrieval and reranking. Our contributions are summarized as follows: We reinterpret the listwise prompt as PRF query and propose unified framework, E2RANK, for both retrieval and reranking. We propose two-stage training process to optimize the unified model for both retrieval and listwise reranking tasks. Extensive experiments show that E2RANK achieves state-of-the-art reranking performance, with significantly lower latency than existing LLM-based rerankers, while maintaining competitive retrieval performance on MTEB."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Large Language Model For Document Reranking Large language models (LLMs) like GPT4 (OpenAI, 2024) and Qwen-3 (Yang et al., 2025) have significantly advanced information retrieval, achieving state-of-the-art performance in document ranking tasks across multiple benchmarks (Sun et al., 2023; Zhu et al., 2023; Chen et al., 2024c). Existing methods generally fall into three prompting paradigms: pointwise, pairwise, and listwise. Pointwise methods evaluate each query-document pair independently, offering efficiency but lacking cross-document comparisons (Liang et al., 2022; Sachan et al., 2022; Zhang et al., 2023a; Liu et al., 2024b). Pairwise methods compare document pairs for given query to determine relative relevance (Qin et al., 2023). Listwise methods instead consider the entire candidate set simultaneously and generate ranking list based on global relevance signals (Sun et al., 2023; Pradeep et al., 2023; Liu et al., 2024a). Recent studies further improve listwise reranking by refining prompting strategies or the method of outputting the ranking list (Reddy et al., 2024; Liu et al., 2025b; Chen et al., 2024b; Zhang et al., 2025b). Text Embedding Models Text embeddings map queries and documents into shared semantic space and serve as foundation component in modern search systems. Based on pre-trained language models such as BERT (Devlin et al., 2018) and T5 (Raffel et al., 2020), they significantly improved retrieval performance over traditional methods (Karpukhin et al., 2020; Ni et al., 2021; Zhao et al., 2024), and approaches like GTE (Li et al., 2023b), E5 (Wang et al., 2022), and BGE (Xiao et al., 2023) further boosted quality via large-scale contrastive learning. More recently, LLMs have emerged as powerful backbones due to their strong semantic understanding and generalization capabilities. Representative methods include LLM2Vec (BehnamGhader et al., 2024), E5-Mistral (Wang et al., 2023), NV-Embed (Lee et al., 2025), and Qwen3-Embedding (Zhang et al., 2025c), which explore architectural modifications, training data construction, or advanced training strategies. Instruction following and in-context learning abilities of text embeddings are also studied (Su et al., 2022; Li et al., 2024a). Additionally, GritLM (Muennighoff et al., 2024) unified the embedding model and generative model through multi-task learning. Compared to previous work, our work unifies the embedding and listwise reranking ability, which share similar objective, in single embedding model, considering both effectiveness and efficiency. Pseudo Relevance Feedback for Dense Retrieval Pseudo Relevance Feedback (PRF) is an important concept in classic IR. Specifically, it is an automatic query expansion technique widely used in classic IR (Xu & Croft, 1996; Manning, 2008). After an initial retrieval, the system assumes that the top-K retrieved documents are relevant, extracts informative terms from these documents, and uses them to expand the original query for second round of retrieval. Recent studies show the effectiveness of incorporating PRF in dense retrievers. ANCE-PRF (Yu et al., 2021) consumed the query and the top retrieved documents to learn better query encoder, but is less robust for strong models (Li et al., 2022; 2023a). Other works leveraged PRF in rerankers, but were limited in pointwise cross-encoders and needed to generate keywords for query expansion (Li et al., 2024b; Weller et al., 2024). Compared to previous work, we first interpret and systematically study PRF in the framework of LLM-based listwise reranking instead of merely retrieval and without additional query-augmented techniques, and also demonstrate its effectiveness in this context through training under ranking objective. 3 Preprint."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "We first review embedding-based retrieval and LLM-based listwise reranking, then present our key insight: listwise prompts can be treated as pseudo relevance feedback queries, then the cosine similarity of embeddings could be unified ranking function, leading to unified model E2RANK. Finally, we detail the training of E2RANK."
        },
        {
            "title": "3.1 PRELIMINARY",
            "content": "For LLM-based decoder-only text embedding model and any document d, we append the special end-of-sequence token [EOS] at the end of the input sequence, and the hidden state at the position of [EOS] from the final decoder layer is taken as the sequence embedding: ed = (d, [EOS])[1]. Further, given query q, we append the instruction in front of the query to ensure its instructionfollowing abilities (Su et al., 2022) and obtain the embedding eq = (I, q, [EOS])[1]. The relevance between the query and the document is measured by the cosine similarity between their corresponding embeddings, denoted as s(q, d) = cos (eq, ed). While the embedding model learning encodes the semantic information of single document in the embedding space, it has not been optimized to capture nuanced differences between multiple documents. In contrast, LLM-based listwise rerankers (e.g., RankGPT (Sun et al., 2023)) use listwise prompt that includes the query and the entire candidate set, formulated as ˆq = (I, d1, ..., dk, q), where {di}k i=1 is candidate documents set. The model is then asked to output text form permutation (e.g., [2] > [1] > [3]...) of the documents in decreasing order of relevance. While effective, this approach requires auto-regressive decoding or full-sequence encoding over long inputs, leading to high computational cost and latency. Moreover, the decoding process is inherently sequential and difficult to parallelize. Meanwhile, some work proposed that the auto-regressive decoding may not be necessary for listwise reranker; however, the listwise prompt containing the interaction between query and documents in the context is the most important (Chen et al., 2024b; Zhang et al., 2025b)."
        },
        {
            "title": "3.2 LISTWISE PROMPTS AS PSEUDO RELEVANCE FEEDBACK QUERY",
            "content": "Inspired by these observations, we propose to reinterpret the listwise prompt as pseudo-relevance feedback (PRF) query. Therefore, we can formulate the listwise reranking and retrieval in unified framework. Specifically, instead of generating ranking list auto-regressively, we start from an embedding model and use the cosine similarity of embeddings as unified ranking function for both retrieval and reranking. Formally, for the listwise prompt, we obtain its embedding eˆq = (I, d1, ..., dk, q)[1], and compute s(ˆq, di) = cos (eˆq, edi) as the score for reranking. The instructions we use is similar to Given query and some relevant documents, rerank the documents, detailed in the Appendix C. It should be noted that, different from text embedding, we apply chat templates for listwise prompt. (1) This design allows us to exploit listwise information for effectiveness without sacrificing efficiency at inference time. First, the listwise prompt provides the model with additional contextual PRF signals, allowing it to refine the query representation by implicitly leveraging document-document and query-document relationships. Second, both retrieval and reranking reduce to simple cosine similarity computations in the shared embedding space, and the document embeddings can be reused. Finally, PRF-based design enables feeding only partial candidates in LLM inputs for the full ranking, for example, including only top-20 documents in the PRF query to rerank top-100, which can further improve the efficiency."
        },
        {
            "title": "3.3 TRAINING THE UNIFIED EMBEDDING AND LISTWISE RERANKING MODEL",
            "content": "We propose training E2RANK in two stages: first, training an embedding model, then endowing it with listwise reranking capacity. Stage We start from training an LLM-based decoder-only text embedding model. In the training process, we employ standard contrastive learning to align relevant querydocument pairs while pushing apart irrelevant ones. Specifically, for training query qi, there is one positive document 4 Preprint. d+ and set of negative documents D. Given batch of instances, we minimize the InfoNCE loss (Izacard et al., 2021): LInfoNCE ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 log e(s(qi,d+ )/τ ) e(s(qi,d+ )/τ ) + (cid:80) dj e(s(qi,dj )/τ ) , (2) where τ is temperature hyperparameter, which is set to 0.03 during training. This embedding training stage ensures that the base embedding model learns strong semantic representations suitable for large-scale retrieval. Stage II To incorporate the listwise reranking capabilities into the embedding model, we continue training the model using multi-task learning framework. Basically, we include the contrastive learning with InfoNCE loss to maintain the embedding capacity of the model and new learningto-rank loss function, RankNet (Burges et al., 2005) loss, which is pairwise loss that measures the correctness of relative orders, for listwise ranking ability. The RankNet loss is defined as follows: LRankNet ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (cid:88) (cid:88) i=1 dj dkD 1rj <rk log(1 + e(s(qi,dj )/τ s(qi,dk)/τ )), (3) where is the same set of documents as used in contrastive learning (including both positive and negative). τ is set to 0.1 in RankNet loss to scale the similarity score. rj is the rank of document dj among D, and the smaller the rank, the more relevant. For example, rj = 2 means dj ranks second among documents. Following (Sun et al., 2023; Pradeep et al., 2023), we can leverage powerful LLM to generate the full ranking permutation and obtain set of pairwise relative relevance orders. The final training objective of stage II combines retrieval and reranking losses: = LInfoNCE + λLRankNet, (4) where λ is hyperparameter that balances the two tasks, which is set to 2.0 based on our prior experiments."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Base LLMs We conduct our main experiments with open-weight, instruction-tuned LLMs from the Qwen3 family (Yang et al., 2025) across different sizes, including 0.6B, 4B, and 8B. Training Datasets At Stage I, we use the public portion of the E5 training dataset (Wang et al., 2023) with roughly 1.5 million samples, curated by Springer et al. (2025). For the second stage training, we use some of the retrieval datasets from the above mixture, as well as 2 additional public Chinese retrieval datasets from BGE-M3 training dataset (Chen et al., 2024a). We further sample instances from these datasets and construct hard negatives for each query, resulting in about 87k training samples each with 1 query, 1 positive, and 15 negatives. We also leverage Qwen3-32B for labeling the ranking permutation. For more details about the datasets, please refer to Appendix B. Implementation Details We train the embedding model with full parameters for 1 epoch with batch size of 512, using learning rate of 5e-6. At the second stage, we continue to train the model for 700 steps with batch size of 128, and the number of negatives is 15. We provide other hyperparameters in Appendix C."
        },
        {
            "title": "4.2 RERANKING PERFORMANCE",
            "content": "Datasets Following Sun et al. (2023), we use TREC DL dataset (Craswell et al., 2020) and subset of BEIR (Thakur et al., 2021) for evaluation of general reranking ability. Specifically, we conduct evaluations on 8 datasets of BEIR that contain relatively small number of queries, including TREC Covid, NFCorpus, Touch2020, DBPedia, SciFact, Signal1M, TREC News, and Robust04. Since the rise of reasoning-intensive ranking for complex retrieval-augmented tasks like DeepResearch, we also evaluate E2RANK on BRIGHT (Su et al., 2025). We use BM25 as the first-stage retriever for TREC DL and BEIR and use ReasonIR (Shao et al., 2025) with GPT4 reason-query for BRIGHT. For all benchmarks, we rerank the top-100 candidate documents and use NDCG@10 as the metric. 5 Preprint. Table 1: Performance comparison on TREC DL and BEIR benchmarks across LLMs. We bold the best performance for each task with each base LLM. Model BM RankQwen3-0.6B E2RANK-0.6B RankQwen3-4B E2RANK-4B RankQwen3-8B E2RANK-8B DL19 DL20 Coivd NFCorpus Touche DBPedia SciFact Signal News Robust Avg. 50. 47.96 59.47 69.11 70.84 +1.73 72.36 70.44 -1.92 73.15 72.95 -0.20 67.74 70.15 +2. 69.83 70.64 +0.81 70.75 71.16 +0.41 78.35 79.17 +0.82 83.91 83.30 -0.61 85.37 84.09 -1.28 30. 36.41 38.60 +2.19 39.88 39.20 -0.68 40.05 39.08 -0.97 44.22 37.54 41.91 +4.37 32.66 43.16 +10. 31.73 42.06 +10.33 31.80 39.19 41.96 +2.77 43.91 42.95 -0.96 45.44 43.44 -2.00 67. 71.01 73.43 +2.42 76.37 77.19 +0.82 78.96 77.49 -1.47 33.05 39.52 40. 43.43 30.96 35.26 +4.30 32.15 34.48 +2.33 32.48 34.01 +1.53 44.43 52.75 +8.32 50.81 52.71 +1. 52.36 54.25 +1.89 46.31 53.67 +7.36 59.36 60.16 +0.80 60.72 60.34 -0.38 48.03 52.09 +4.06 52.38 54.14 +1. 53.39 54.35 +0.96 Baselines In order to achieve fair and direct comparison, we used the same base LLM to compare RankGPT-like listwise rerank with E2RANK, and finetune Qwen3 on the training data provided by Pradeep et al. (2023), denoted as RankQwen3. More training details will be provided in Appendix C. For RankQwen3, we use sliding window strategy of window size 20 and step 10; while for our model, we only feed the top-20 documents to the listwise prompt and use its embedding to rerank the top-100. We believe that this direct comparison between E2RANK and RankQwen3 without the influence of base LLMs can provide richer insights. For reference, we also report other baseline results on TREC DL and BEIR, including cross-encoders monoBERT (Nogueira et al., 2019), monoT5 (Nogueira et al., 2020), and RankT5 (Zhuang et al., 2023), as well as listwise LLM-based rerankers ListT5 (Yoon et al., 2024), RankZephyr (Pradeep et al., 2023), and RankGPT (Sun et al., 2023). As for the baselines of BRIGHT, we compare E2RANK with reasoning rerankers with parameters less than 14B, including Rank-R1 (Zhuang et al., 2025), Rank1 (Weller et al., 2025), JudgeRank (Niu et al., 2024), Rearank (Zhang et al., 2025a), ERank (Cai et al., 2025), and ReasonRank (Liu et al., 2025c). Note that only RankGPT and JudgeRank are zero-shot; others are all fine-tuned, and most reasoning rerankers are trained with RL. E2RANK consistently outperforms RankQwen3. We present the direct comparison with RankQwen3 on general reranking tasks in Table 1. Our proposed E2RANK demonstrates clear and consistent advantage over the directly comparable RankQwen3 baseline across all model sizes on general reranking tasks, especially for the 0.6B model with an average gain of +4.06 NDCG@10, while E2RANK-4B and E2RANK-8B show smaller but stable improvements on average. As model size grows, both RankQwen3 and E2RANK improve over BM25, but E2RANK-8B achieves the best overall performance. Table 2: Performance comparison across broader baselines. The best result of each benchmark is bolded, and the second best is underlined."
        },
        {
            "title": "Model",
            "content": "BM25 DL19 DL20 BEIR Avg. 50.58 47. 43.43 Fine-tuned Pointwise Reranker monoBERT (340M) monoT5 (3B) RankT5 (3B) 70.50 71.83 72.50 67.28 68.89 70.40 47.16 51.36 52. Fine-tuned Listwise Reranker ListT5 (3B) RankZephyr 71.80 73.39 69.10 70.02 53.00 51.15 Zero-shot Listwise Reranker RankQwen3 (14B) RankGPT-4o RankGPT-4o-mini E2RANK achieves competitive rerank accuracy across other strong baselines. Table 2 presents broader comparisons on the TREC DL and BEIR benchmarks, and our models compete effectively with diverse array of state-of-the-art rerankers. Compared to fine-tuned pointwise rerankers such as monoBERT and monoT5, our approach achieves significantly higher average scores, and even surpasses strong listwise baselines like RankZephyr and ListT5 on BEIR benchmarks. Notably, while RankGPT-4o remains the strongest zero-shot model, our fine-tuned 8B model secures the top performance on the DL20 dataset (71.16) and achieves the highest overall BEIR average (54.35), surpassing even much larger zero-shot models like RankGPT-4o and establishing our approach as powerful and efficient alternative to existing fine-tuned and zero-shot methods. E2RANK-0.6B E2RANK-4B E2RANK-8B 70.84 70.44 72.65 52.09 54.14 54.35 70.15 70.64 71. 74.19 74.78 72.36 53.67 53.09 51.16 69.10 69.52 67."
        },
        {
            "title": "Ours",
            "content": "6 Preprint. Table 3: Performance comparison on BRIGHT benchmarks across LLMs. We bold the best performance for each task and underline the second best. Model ReasonIR RankT5 (3B) RankZephyr Rank-R1 (7B) Rank-R1 (14B) Rank1 (7B) Rearank (7B) JudgeRank (8B) ERank (4B) ERank (14B) ReasonRank (7B) RankQwen3-0.6B E2RANK-0.6B RankQwen3-4B E2RANK-4B RankQwen3-8B E2RANK-8B StackExchange Coding Theorem-based Bio. Econ. Earth. Psy. Rob. Stack. Sus. Pony. LC. AoPS TheoQ. ThoT. 43.5 11.4 19.9 39.3 27.4 44.1 35.3 37.1 42.1 46.6 35.1 44.7 44.1 47.0 47. 49.5 49.2 32.8 22.1 17.4 28.1 38.7 33.5 29.8 27.2 42.5 42.5 47.8 38.7 46.5 44.2 46. 44.2 47.2 43.0 10.9 12.4 23.9 23.1 21.8 25.5 19.2 26.3 25.2 31.2 28.4 31.0 25.2 31. 30.4 32.3 38.9 13.6 34.9 30.0 44.5 30.0 35.7 28.6 36.4 37.3 56.7 40.4 40.8 44.7 43. 44.9 44.7 21.1 11.4 24.7 17.3 37.1 15.0 19.1 11.6 20.8 19.6 47.8 20.5 26.1 24.1 26. 24.9 28.2 30.6 11.4 13.4 18.1 27.8 22.1 20.1 19.9 27.3 30.2 32.5 26.1 30.6 29.7 31. 26.1 32.9 27.3 16.0 22.3 33.2 36.8 28.5 32.9 22.5 33.2 34.6 40.9 28.5 30.6 41.1 34. 39.6 38.4 31.6 27.5 29.3 18.6 21.3 11.8 29.9 10.2 31.7 31.9 23.2 19.9 11.7 22.6 8. 18.8 10.6 19.6 38.1 32.4 15.0 19.2 21.7 20.2 10.2 21.8 25.6 25.0 29.1 38.5 22.0 38. 20.8 36.2 7.3 9.2 6.1 4.2 8.8 1.2 6.2 3.6 10.9 10.5 7.7 6.8 8.0 9.0 8. 7.6 8.2 36.7 18.3 29.0 25.4 31.7 26.2 36.7 22.9 32.8 32.4 39.5 35.8 35.9 38.2 39. 39.0 38.2 34.1 9.5 30.1 35.7 39.5 36.2 38.3 29.4 40.6 45.0 41.8 30.5 28.0 36.0 31. 37.9 33.4 Avg. 30.5 16.6 22.6 24.1 29.7 24.3 27.5 20.2 30.5 31.8 35.7 29.1 31. 32.0 32.4 32.0 33.4 Efficiency Analysis. We conduct the efficiency analysis on the Covid dataset using single NVIDIA A100 80G GPU. The Covid dataset contains 50 test queries, and the average length of documents tokenized by Qwen3 tokenizer is approximately 350. We implement the evaluation code using vLLM (Kwon et al., 2023), highly-efficient LLM inference infrastructure. As shown in Figure 1 (b), E2RANK significantly reduces inference latency across all model sizes compared to RankQwen3, achieving up to about 5 speedup at 8B while maintaining superior ranking performance. Even E2RANK-8B model is faster than RankQwen3-0.6B. Since RankQwen3 uses sliding window strategy, it cant use the batch inference techniques for inference, while full ranking is less effective. In contrast, E2RANK inherits the advantages of the embedding model, supports batch inference, and can encode document embeddings offline, further reducing online reranking latency. The detailed results of reranking latency are listed in Appendix D, Table 12 and 13. E2RANK demonstrates strong performance on the BRIGHT benchmark. On the challenging BRIGHT benchmark, E2RANK delivers robust performance, as shown in Table 3. Without any RL or reasoning process, E2RANK-8B attains highly competitive average score of 33.4, surpassing RankQwen3 and most reasoning rerankers and only underperforming ReasonRank trained on synthetic reasoning data, validating the strong generalization capabilities."
        },
        {
            "title": "4.3 EMBEDDING ABILITY",
            "content": "Benchmark and Baselines We evaluate E2RANK on the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2022). Specifically, we mainly evaluate its English v1 version, collection of 56 datasets covering seven types of embedding tasks: classification, clustering, pairwise classification, reranking, retrieval, sentence similarity (STS), and summarization. We also leverage its English v2 version for quick evaluation and ablation studies, which is smaller and cleaner with 41 tasks. We compare our models with recent advanced open source text embedding models that are trained on public datasets, including Instructor-xl (Su et al., 2022), BGE-large-en-v1.5 (Xiao et al., 2023), GritLM (Muennighoff et al., 2024), E5 (Wang et al., 2023), EchoEmbedding (Springer et al., 2025), and LLM2Vec (BehnamGhader et al., 2024) Results Table 4 presents the performance of E2RANK on the MTEB(Eng, v1) benchmark. When leveraging only the public dataset, E2RANK demonstrates strong embedding capabilities, while E2RANK-8B shows slight performance advantages on average compared to previous advanced models. Notably, compared with the variant with only contrastive learning, distilling from richer ranking signals will bring consistent and significant enhancements in retrieval tasks ( 1.58 for E2RANK8B), demonstrating the effectiveness of the ranking objective. Noticed that here we focus on general and pure embedding ability, so we do not use the listwise prompt for reranking tasks. 7 Preprint. Table 4: Performance comparison on MTEB. Note that some baselines are trained with non-public data, and we only report the version trained on public data, marked using *. The best results for each subtask are highlighted in bold, and the second-best results are underlined. Categories # of datasets Instructor-xl BGElarge-en-v1.5 GritLMMistral-7b-v1* E5Mistral-7b-v1* EchoMistral-7b-v1 LLM2VecMistral-7B LLM2VecMeta-LLaMA-3-8B E2RANK-0.6B (w/ only Stage I) E2RANK-0.6B E2RANK-4B (w/ only Stage I) E2RANK-4B E2RANK-8B (w/ only Stage I) E2RANK-8B Retr. 15 49.26 54.29 53.10 52.78 55.52 55.99 56. 48.07 51.74 54.36 55.33 55.31 56.89 Rerank. 4 Clust. 11 PairClass. Class. 12 57.29 60.03 61.30 60.38 58.14 58.42 59.68 56.16 55.97 59.30 59.10 55.73 59.58 44.74 46.08 48.90 47.78 46.32 45.54 46. 42.38 40.85 44.62 44.27 45.84 44.75 86.62 87.12 86.90 88.47 87.34 87.99 87.80 82.47 83.93 84.36 87. 85.23 86.96 73.12 75.97 77.00 76.80 77.43 76.63 75.92 72.05 73.66 76.11 77.08 75.69 76.81 STS 83.06 83.11 82.80 83.77 82.56 84.09 83.58 80.90 81.41 82.31 84.03 83.23 84.52 Summ. 1 32.32 31.61 29.40 31.90 30.73 29.96 30. 29.84 30.90 29.33 30.06 29.66 30.23 Avg. 56 61.79 64.23 64.70 64.56 64.68 64.80 65.01 60.05 61. 63.61 64.47 64.26 65.03 Table 5: End-to-end ranking performance. Table 6: Ablation on different training strategies. DL20 BEIR BRIGHT DL20 BEIR BRIGHT MTEB(v2) E2RANK-0.6B Retrieval + Rerank E2RANK-4B E2RANK-8B Retrieval + Rerank Retrieval + Rerank 66.77 74. 74.00 76.88 75.83 78.02 47.60 50.66 52.11 54.12 53.39 55.08 18.37 22. 27.84 32.15 25.09 31.00 E2RANK-0.6B w/o Stage w/o InfoNCE in Stage II w/ only Stage w/o RankNet in Stage II 70. 52.09 69.32 69.11 63.55 66.50 51.33 52.17 46.31 49.24 w/o Listwise in Stage II 66.29 49.93 30.96 30.66 29.99 15.30 22.40 22. 63.41 60.61 61.92 62.40 63.31 63."
        },
        {
            "title": "4.4 UNIFIED AND END-TO-END RETRIEVAL AND RERANKING",
            "content": "We also perform end-to-end ranking to evaluate if the single E2RANK model could be unified model in the search paradigm. Specifically, we use E2RANK first to retrieve the top-100 candidate documents and then use it to rerank these documents further. The results in Table 5 indicate that using single E2RANK model for both retrieval and reranking leads to consistent improvements across different model scales and datasets. Notably, as the model size increases from 0.6B to 8B parameters, we observe progressive gains in end-to-end ranking performance on all benchmarks. Additionally, reranking consistently enhances the initial retrieval performance, with the E2RANK-8B achieving the best performance of 55.08 nDCG@10 on BEIR after reranking. These results demonstrate the viability of using single unified model for both stages of the search pipeline, thereby reducing system complexity and latency while maintaining strong performance."
        },
        {
            "title": "4.5 ABLATION STUDY",
            "content": "We evaluate the effectiveness of different training strategies and conduct ablation studies using the Qwen3-0.6B model on TREC DL20, BEIR, BRIGHT, and MTEB(eng, v2). The reranking settings and metrics are the same as in Section 4.2. The results shown in Table 6 indicate that the full training strategy achieves the best or highly competitive performance across all datasets, demonstrating the effectiveness of the integrated design. For the last three lines, we use query-only embedding instead of listwise prompt for evaluation since they are not trained on it. The first-stage contrastive learning is crucial for foundational query-document alignment and embedding ability. Its removal causes consistent performance degradation, especially on MTEB. This confirms that initial large-scale contrastive learning provides an essential foundation for subsequent ranking tasks. The RankNet loss is the most critical element for effective ranking. Removing the RankNet loss causes the most severe performance collapse, particularly on BEIR and BRIGHT. This underscores that the pairwise ranking objective is indispensable for learning complex relevance ordering patterns. 8 Preprint. Figure 2: Trend of NDCG@10 changes with the number of input documents in listwise prompt. Figure 3: Score distribution of using listwise (with 20 documents) and non-listwise prompts. The listwise prompts with documents as PRF contribute meaningfully to ranking effectiveness. If retaining the ranknet loss but removing the listwise prompt, the ranking performance will still be greatly affected (last line). This indicates that the reranking ability is mainly from the listwise prompt with PRF signals, but not the richer training labels."
        },
        {
            "title": "4.6 ANALYSIS",
            "content": "In order to understand the reranking behaviors of E2RANK, we conduct further analysis. Influence of number of input documents in the listwise prompts. Figure 2 shows that when the number of input documents is small (less than 20), incorporating more documents into the listwise prompt consistently improves ranking performance. This trend can be interpreted as additional documents enriching the query with pseudo-relevance signals, allowing the model to capture the fine-grained relevance. Notably, the gains plateau after around 20 documents, indicating that the marginal benefit of adding more feedback signals diminishes once the prompt already captures sufficient relevance context, and may even bring negative benefits on different datasets. Similarity score distribution. Figure 3 further analyzes how this pseudo relevance feedback affects the ranking behavior by comparing similarity score distributions between listwise and non-listwise settings. Specifically, we sort the reranking scores of 100 documents from high to low, and take the average of all queries for the rank position. We can see that the listwise prompts yield consistently higher similarity scores for top-ranked documents while maintaining steeper decline for lower-ranked ones, suggesting sharper discrimination between relevant and irrelevant documents. In contrast, the query-only setting produces flatter score distribution. This demonstrates that listwise prompts with PRF enhance E2RANK ability to allocate higher scores to truly relevant documents. Influence of different first-stage retrievers. We evaluate the E2RANKs reranking ability under different first-stage retrievers, and detail the results in Appendix D, Table 21. Across all retrievers, E2RANK consistently improves the performance, demonstrating its generalization ability and robustness while adapting to varying initial retrieval qualities as reranker. Additionally, this also indicates that better search results as better PRF can lead to better ranking performance."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we propose E2RANK, unified framework that enables single text embedding model to perform both efficient retrieval and high-quality listwise reranking, by reformulating the listwise reranking prompt as pseudo relevance feedback query. Extensive experiments demonstrate that E2RANK can be an independent reranker and achieve state-of-the-art reranking performance on BEIR and strong results on BRIGHT, while significantly reducing inference latency compared to existing RankGPT-like listwise rerankers. Moreover, E2RANK maintains competitive embedding capabilities on the MTEB benchmark. Our work highlights the potential of single embedding models to serve as unified retrieval-reranking engines, offering practical, efficient, and accurate alternative to complex multi-stage ranking systems. 9 Preprint."
        },
        {
            "title": "ETHICS AND REPRODUCIBILITY STATEMENT",
            "content": "This study does not raise concerns related to discrimination, bias, or fairness. To ensure reproducibility, we provide detailed descriptions of the experimental setup in Section 4.1 and additional implementation details in Appendix C. All data used in our experiments are obtained from previously released and widely adopted datasets. with details in Appendix B. All open source libraries and resources used in this study are also fully specified. We also provide the complete source code for reproduction directly in the supplementary material."
        },
        {
            "title": "REFERENCES",
            "content": "Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. In First Conference on Language Modeling, 2024. Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning, pp. 8996, 2005. Yuzheng Cai, Yanzhao Zhang, Dingkun Long, Mingxin Li, Pengjun Xie, and Weiguo Zheng. Erank: Fusing supervised fine-tuning and reinforcement learning for effective and efficient text reranking. arXiv preprint arXiv:2509.00520, 2025. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. arXiv preprint arXiv:2402.03216, 2024a. Shijie Chen, Bernal Jimenez Gutierrez, and Yu Su. Attention in large language models yields efficient zero-shot re-rankers. arXiv preprint arXiv:2410.02642, 2024b. Yiqun Chen, Qi Liu, Yi Zhang, Weiwei Sun, Daiting Shi, Jiaxin Mao, and Dawei Yin. Tourrank: Utilizing large language models for documents ranking with tournament-inspired strategy. arXiv preprint arXiv:2406.11678, 2024c. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen Voorhees. Overview of the trec 2019 deep learning track. arXiv preprint arXiv:2003.07820, 2020. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stephane Clinchant. From distillation to hard negative sampling: Making sparse neural ir models more effective. In Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval, pp. 23532359, 2022. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 67696781, 2020. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. In The Thirteenth International Conference on Learning Representations, 2025. 10 Preprint. Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao, Defu Lian, and Zheng Liu. Making text embedders few-shot learners. arXiv preprint arXiv:2409.15700, 2024a. Hang Li, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy Lin, and Guido Zuccon. Improving query representations for dense retrieval with pseudo relevance feedback: reproducibility study. In European Conference on Information Retrieval, pp. 599612. Springer, 2022. Hang Li, Ahmed Mourad, Shengyao Zhuang, Bevan Koopman, and Guido Zuccon. Pseudo relevance feedback with deep language models and dense retrievers: Successes and pitfalls. ACM Transactions on Information Systems, 41(3):140, 2023a. Minghan Li, Honglei Zhuang, Kai Hui, Zhen Qin, Jimmy Lin, Rolf Jagerman, Xuanhui Wang, and Michael Bendersky. Can query expansion improve generalization of strong cross-encoder In Proceedings of the 47th International ACM SIGIR Conference on Research and rankers? Development in Information Retrieval, SIGIR 24, pp. 23212326, New York, NY, USA, 2024b. Association for Computing Machinery. ISBN 9798400704314. doi: 10.1145/3626772.3657979. URL https://doi.org/10.1145/3626772.3657979. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281, 2023b. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. Qi Liu, Haozhe Duan, Yiqun Chen, Quanfeng Lu, Weiwei Sun, and Jiaxin Mao. Llm4ranking: An easy-to-use framework of utilizing large language models for document reranking. arXiv preprint arXiv:2504.07439, 2025a. Qi Liu, Bo Wang, Nan Wang, and Jiaxin Mao. Leveraging passage embeddings for efficient listwise reranking with large language models. In Proceedings of the ACM on Web Conference 2025, pp. 42744283, 2025b. Wenhan Liu, Xinyu Ma, Yutao Zhu, Ziliang Zhao, Shuaiqiang Wang, Dawei Yin, and Zhicheng Dou. Sliding windows are not the end: Exploring full ranking with long-context large language models. arXiv preprint arXiv:2412.14574, 2024a. Wenhan Liu, Yutao Zhu, and Zhicheng Dou. Demorank: Selecting effective demonstrations for large language models in ranking task. arXiv preprint arXiv:2406.16332, 2024b. Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, and Zhicheng Dou. arXiv preprint Reasonrank: Empowering passage ranking with strong reasoning ability. arXiv:2508.07050, 2025c. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. arXiv preprint arXiv:2310.08319, 2023. Christopher Manning. Introduction to information retrieval. Syngress Publishing,, 2008. Irina Matveeva, Chris Burges, Timo Burkard, Andy Laucius, and Leon Wong. High accuracy retrieval with multiple nested ranker. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 437444, 2006. Niklas Muennighoff, Nouamane Tazi, Loıc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. Generative representational instruction tuning. arXiv preprint arXiv:2402.09906, 2024. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, et al. Large dual encoders are generalizable retrievers. arXiv preprint arXiv:2112.07899, 2021. 11 Preprint. Tong Niu, Shafiq Joty, Ye Liu, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Judgerank: Leveraging large language models for reasoning-intensive reranking. arXiv preprint arXiv:2411.00142, 2024. Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. Multi-stage document ranking with bert. arXiv preprint arXiv:1910.14424, 2019. Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. Document ranking with pretrained sequence-to-sequence model. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 708718, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.63. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2024. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. Rankzephyr: Effective and robust zeroshot listwise reranking is breeze! arXiv preprint arXiv:2312.02724, 2023. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, et al. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563, 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Revanth Gangi Reddy, JaeHyeok Doo, Yifei Xu, Md Arafat Sultan, Deevya Swain, Avirup Sil, and Heng Ji. First: Faster improved listwise reranking with single token decoding. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 86428652, 2024. Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. Improving passage retrieval with zero-shot question generation. arXiv preprint arXiv:2204.07496, 2022. Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, et al. Reasonir: Training retrievers for reasoning tasks. arXiv preprint arXiv:2504.20595, 2025. Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. RepIn The Thirteenth International Conference on etition improves language model embeddings. Learning Representations, 2025. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741, 2022. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Liu Haisu, Quan Shi, Zachary Siegel, Michael Tang, et al. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. In The Thirteenth International Conference on Learning Representations, 2025. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. Is chatgpt good at search? investigating large language models as re-ranking agents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1491814937, 2023. Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. Beir: heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. Preprint. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023. Orion Weller, Kyle Lo, David Wadden, Dawn Lawrie, Benjamin Van Durme, Arman Cohan, and Luca Soldaini. When do generative query and document expansions fail? comprehensive study across methods, retrievers, and datasets. In Findings of the Association for Computational Linguistics: EACL 2024, pp. 19872003, 2024. Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn Lawrie, and Benjamin Van Durme. Rank1: Test-time compute for reranking in information retrieval. arXiv preprint arXiv:2502.18418, 2025. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. C-pack: Packaged resources to advance general chinese embedding. arXiv preprint arXiv:2309.07597, 2023. Jinxi Xu and Bruce Croft. Query expansion using local and global document analysis. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pp. 411, 1996. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Soyoung Yoon, Eunbi Lee, Jiyeon Kim, Yireun Kim, Hyeongu Yun, and Seung-won Hwang. Listt5: Listwise reranking with fusion-in-decoder improves zero-shot retrieval. arXiv preprint arXiv:2402.15838, 2024. HongChien Yu, Chenyan Xiong, and Jamie Callan. Improving query representations for dense retrieval with pseudo relevance feedback. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, CIKM 21, pp. 35923596, New York, NY, USA, ISBN 9781450384469. doi: 10.1145/3459637. 2021. Association for Computing Machinery. 3482124. URL https://doi.org/10.1145/3459637.3482124. Le Zhang, Bo Wang, Xipeng Qiu, Siva Reddy, and Aishwarya Agrawal. Rearank: Reasoning reranking agent via reinforcement learning. arXiv preprint arXiv:2505.20046, 2025a. Longhui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. Rankinggpt: Empowering large language models in text ranking with progressive enhancement. arXiv preprint arXiv:2311.16720, 2023a. Wuwei Zhang, Fangcong Yin, Howard Yen, Danqi Chen, and Xi Ye. Query-focused retrieval heads improve long-context reasoning and re-ranking. arXiv preprint arXiv:2506.09944, 2025b. Xinyu Zhang, Sebastian Hofstatter, Patrick Lewis, Raphael Tang, and Jimmy Lin. Rank-withoutgpt: Building gpt-independent listwise rerankers on open-source large language models. arXiv preprint arXiv:2312.02969, 2023b. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025c. Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. Dense text retrieval based on pretrained language models: survey. ACM Transactions on Information Systems, 42(4):160, 2024. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. Large language models for information retrieval: survey. arXiv preprint arXiv:2308.07107, 2023. Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. Rankt5: Fine-tuning t5 for text ranking with ranking losses. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 23082313, 2023. Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon. Rank-r1: Enhancing reasoning in llm-based document rerankers via reinforcement learning. arXiv preprint arXiv:2503.06034, 2025. 13 Preprint."
        },
        {
            "title": "A THE USE OF LARGE LANGUAGE MODELS",
            "content": "We only used large language models (LLMs) as auxiliary tools for grammar checking, language polishing, and logo generation. All outputs were carefully reviewed by the authors, who take full responsibility for the final manuscript."
        },
        {
            "title": "B TRAINING DATASET DETAILS",
            "content": "Dataset composition We mainly leverage the public portion of the E5 dataset (Wang et al., 2023). Specifically, for the training at Stage I, we use the sampled version with around 1.5 million samples in total, which is constructed by Springer et al. (2025) and is also used by LLM2Vec (BehnamGhader et al., 2024). The mixture consists of ELI5, HotpotQA, FEVER, MIRACL, MS MARCO passage ranking and document ranking, NQ, NLI, SQuAD, TriviaQA, Quora Duplicate Questions, Mr.TyDi, DuReader, and T2Ranking. Each query in the datasets has only one positive and one negative. As for the training at Stage II, since we need more negatives to meet the training objective, the E5 dataset cannot fully meet our requirements. Therefore, we used the dataset from BGE-M3 (Chen et al., 2024a), where each query contains multiple negatives. Specifically, we mainly used the retrieval dataset from the intersection of the E5 dataset and BGE-M3 dataset, including HotpotQA, MIRACL, MSMARCO passage, NQ, TriviaQA, DuReader, and T2Ranking. In addition, we have added two widely used Chinese retrieval datasets, cMedQAv2 and MMarco Chinese, which are included in the BGE-M3 dataset. Due to the length division of the BGE-M3 dataset, we only used the parts with document lengths less than 500. Meanwhile, we filtered queries containing fewer than 15 negative examples and further downsampled the dataset. In the end, we obtained mixed dataset containing approximately 157k samples, with each instance containing one query, one negative, and fifteen negatives. Figure 4: Dataset distribution for training. Producing full ranking labels using Qwen3-32B We leverage Qwen3-32B (disabled thinking mode) (Yang et al., 2025) to generate the full ranking labels for the Stage II training data. The process is similar to RankZephyrs (Pradeep et al., 2023). Specifically, we use the instruction in Table 8 to have the model generate ranking list in text form, and then parse the text. Then, we filter the results with the wrong output formations, which is only very small portion of the entire dataset. The instruction used for each dataset is adapted from BehnamGhader et al. (2024), which can be found in Table 7. Interestingly, we calculate the accuracy of model annotation, which refers to the frequency at which the model places the golden positive in the dataset at the top of its ranking. The results are shown in Figure 5. We can see that the LLMs judgment and actual annotation of the most relevant Preprint. Table 7: Instructions used for each of the E5 datasets in Stage I. Dataset NLI DuReader ELI5 FEVER HotpotQA MIRACL MrTyDi MSMARCO Passage MSMARCO Document NQ QuoraDuplicates SQuAD T2Ranking TriviaQA Instruction(s) Given premise, retrieve hypothesis that is entailed by the premise Retrieve semantically similar text Given Chinese search query, retrieve web passages that answer the question Provided user question, retrieve the highest voted answers on Reddit ELI5 forum Given claim, retrieve documents that support or refute the claim Given multi-hop question, retrieve documents that can help answer the question Given question, retrieve Wikipedia passages that answer the question Given question, retrieve Wikipedia passages that answer the question Given web search query, retrieve relevant passages that answer the query Given web search query, retrieve relevant documents that answer the query Given question, retrieve Wikipedia passages that answer the question Given question, retrieve questions that are semantically equivalent to the given question Find questions that have the same meaning as the input question Retrieve Wikipedia passages that answer the question Given Chinese search query, retrieve web passages that answer the question Retrieve Wikipedia passages that answer the question documents are not always consistent. Especially in the MS MARCO dataset, the consistency rate only barely exceeds half. Previous work discussed and compared using the golden label and using reranker for labeling, but they didnt leverage LLMs (Zhang et al., 2023b). Since the construction of the dataset is not the focus of this article, we will not discuss this discovery in detail and will leave higher-quality dataset construction schemes for future work. Table 8: Instruction for generating full ranking labels. <im start>user will provide you with {N} passages, each indicated by Rank the passages based on numerical identifier []. their relevance to the search query: {query}. Documents: [1] {document 1} [2] {document 2} ... [N] {document N} Search Query: Rank the {N} passages above based on their relevance to the search query. All the passages should be included and listed using identifiers, in descending order of relevance. The output format should be [] > [] > ..., e.g., [4] > [2] > ..., Only respond with the ranking results, do not say anything else or explain. <im start>assistant <think>nn</think>nn {query} <im end>"
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "In this section, we provide detailed introduction to our training settings. Stage training All models are trained with full parameters, DeepSpeed Zero3, brain floating point (BF16) quantization, and gradient checkpointing to optimize GPU memory consumption. We train on 8 NVIDIA A100 80G GPUs with an effective batch size of 512 for 1 epoch using maximum sequence length of 512 tokens. We use learning rate of 2 105 and linear learning rate warm-up for the first 300 steps. Stage II training For the training data, it is important to note that we do not use the full datasets introduced in Appendix for training. Instead, for each dataset, we only sample at most 10,000 instances, leading to around 87k training instances actually. 15 Preprint. Figure 5: Accuracy for labeling the datasets. Table 9: Instructions in listwise prompts used for each of the datasets in Stage II."
        },
        {
            "title": "Dataset",
            "content": "Instruction(s) DuReader HotpotQA MIRACL MSMARCO Passage NQ T2Ranking TriviaQA Given Chinese search query and some relevant documents, rerank the documents that answer the query Given multi-hop question and some relevant documents, rerank the documents that answer the question Given question and some relevant Wikipedia documents, rerank the documents that answer the question Given web search query and some relevant documents, rerank the documents that answer the query Given question, retrieve Wikipedia passages that answer the question Given Chinese search query and some relevant documents, rerank the documents that answer the query Given question and some relevant Wikipedia documents, rerank the documents that answer the question The instructions for the listwise prompt are listed in Table 9. We train all models on 8 NVIDIA A100 80G GPUs with an effective batch size of 128 for 1 epoch (each instance contains multiple documents). We use DeepSpeed Zero3, BF16, and gradient checkpointing to optimize GPU memory consumption. For documents, we use maximum length of 1024. We also use in-batch negatives. We use learning rate initialized at 5 106 with linear scheduler and warmup ratio of 0.03. Training RankQwen3 We fine-tune the Qwen3 model on the GPT-4 labeled listwise ranking dataset provided by Pradeep et al. (2023). The dataset contains 40k samples, and we train the model for 1 epoch with batch size of 16 per device, leading to an effective batch size of 64. For different sizes, we adjust the gradient accumulation steps to fit the batch size. We use DeepSpeed and BF16 mixed precision for acceleration. The learning rate is initialized at 5 106 with linear scheduler and warmup ratio of 0.03. The training is performed on 4 NVIDIA A100 80G GPUs. We use LLM4Ranking Framework (Liu et al., 2025a) for training and evaluation. Evaluation details We use the following instruction for the evaluation of all reranking tasks: <im start>user Given web search query and some relevant documents, rerank the documents that answer the query: Documents: [1] {document 1} [2] {document 2} ... [N] {document N} Search Query: <im end> <im start>assistant <think>nn</think>nn {query} 16 Preprint. Table 10: Instructions used for evaluation on the MTEB benchmark. STS* refers to all the STS tasks. Task Name Instruction AmazonCounterfactualClassif. AmazonPolarityClassification AmazonReviewsClassification Banking77Classification EmotionClassification ImdbClassification MassiveIntentClassification MassiveScenarioClassification MTOPDomainClassification MTOPIntentClassification ToxicConversationsClassif. TweetSentimentClassification ArxivClusteringP2P ArxivClusteringS2S BiorxivClusteringP2P BiorxivClusteringS2S MedrxivClusteringP2P MedrxivClusteringS2S RedditClustering RedditClusteringP2P StackExchangeClustering StackExchangeClusteringP2P TwentyNewsgroupsClustering SprintDuplicateQuestions TwitterSemEval2015 TwitterURLCorpus AskUbuntuDupQuestions MindSmallReranking SciDocsRR StackOverflowDupQuestions ArguAna ClimateFEVER CQADupstackRetrieval DBPedia FEVER FiQA2018 HotpotQA MSMARCO NFCorpus NQ QuoraRetrieval SCIDOCS SciFact Touche2020 TRECCOVID STS* SummEval Classify given Amazon customer review text as either counterfactual or not-counterfactual Classify Amazon reviews into positive or negative sentiment Classify the given Amazon review into its appropriate rating category Given online banking query, find the corresponding intents Classify the emotion expressed in the given Twitter message into one of the six emotions: anger, fear, joy, love, sadness, and surprise Classify the sentiment expressed in the given movie review text from the IMDB dataset Given user utterance as query, find the user intents Given user utterance as query, find the user scenarios Classify the intent domain of the given utterance in task-oriented conversation Classify the intent of the given utterance in task-oriented conversation Classify the given comments as either toxic or not toxic Classify the sentiment of given tweet as either positive, negative, or neutral Identify the main and secondary category of Arxiv papers based on the titles and abstracts Identify the main and secondary category of Arxiv papers based on the titles Identify the main category of Biorxiv papers based on the titles and abstracts Identify the main category of Biorxiv papers based on the titles Identify the main category of Medrxiv papers based on the titles and abstracts Identify the main category of Medrxiv papers based on the titles Identify the topic or theme of Reddit posts based on the titles Identify the topic or theme of Reddit posts based on the titles and posts Identify the topic or theme of StackExchange posts based on the titles Identify the topic or theme of StackExchange posts based on the given paragraphs Identify the topic or theme of the given news articles Retrieve duplicate questions from Sprint forum Retrieve tweets that are semantically similar to the given tweet Retrieve tweets that are semantically similar to the given tweet Retrieve duplicate questions from AskUbuntu forum Retrieve relevant news articles based on user browsing history Given title of scientific paper, retrieve the titles of other relevant papers Retrieve duplicate questions from StackOverflow forum Given claim, find documents that refute the claim Given claim about climate change, retrieve documents that support or refute the claim Given question, retrieve detailed question descriptions from Stackexchange that are duplicates to the given question Given query, retrieve relevant entity descriptions from DBPedia Given claim, retrieve documents that support or refute the claim Given financial question, retrieve user replies that best answer the question Given multi-hop question, retrieve documents that can help answer the question Given web search query, retrieve relevant passages that answer the query Given question, retrieve relevant documents that best answer the question Given question, retrieve Wikipedia passages that answer the question Given question, retrieve questions that are semantically equivalent to the given question Given scientific paper title, retrieve paper abstracts that are cited by the given paper Given scientific claim, retrieve documents that support or refute the claim Given question, retrieve detailed and persuasive arguments that answer the question Given query on COVID-19, retrieve documents that answer the query Retrieve semantically similar text. Given news summary, retrieve other semantically similar summaries In fact, based on our experiments, different instructions have very small impact on performance, at least not statistically significant. So this will not affect the experimental results of the paper. Instructions used for evaluation of MTEB When evaluating MTEB, we use the same instructions as Zhang et al. (2025c). The list of instructions for each task is listed in Table 10."
        },
        {
            "title": "D ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "Full comparison on BEIR with baselines We present all detailed results of baselines in Table 11, which is an extended version of Table 2. We report the results of reasoning-intensive rerankers, however, not all of them perform well on these general reranking tasks. In addition, we use the same training dataset with E2RANK to train cross-encoder style pointwise reranker, using the same RankNet loss. We believe that the reason why them do not perform so well is due to insufficient training data. This comparison between the pointwise model and E2RANK also demonstrates the effectiveness of listwise reranking. 17 Preprint. Table 11: Full results on BEIR. For reasoning rerankers, the results are borrowed from Liu et al. (2025c) and only contain 7 datasets, excluding Touche2020. BM25 59. 30.75 44.22 31.80 67.89 33.05 39. 40.70 43.31 43.43 Covid NFCorpus Touche DBPedia SciFact Signal News Robust Avg. (7) Avg. (8) Previous Fine-tuned Pointwise Reranker MonoBERT (340M) MonoT5 (3B) RankT5 (3B) 70.01 79.80 81.70 Previous Fine-tuned Listwise Reranker ListT5 (3B) RankVicuna RankZephyr Zero-shot Listwise Reranker RankGPT-4o RankGPT-4o-mini RankQwen3-14B RankQwen3-32B Reasoning Reranker Rank-R1 (7B) Rank-R1 (14B) Rank1 (7B) Rearank (7B) ReasonRank (7B) 84.70 79.50 83.20 83.41 80.03 84.45 83. 83.71 84.63 79.04 81.28 82.01 36.88 37.30 37.40 37.70 32.50 37.60 39.67 38.73 38.94 39.22 38.94 38.58 37.52 35.20 39.60 31.75 32.20 31. 33.80 33.30 32.40 32.26 30.91 38.30 37.13 - - - - - Fine-tuned Listwise Reranker based on Qwen3 RankQwen3-0.6B RankQwen3-4B RankQwen3-8B 78.35 83.91 85. 36.41 39.88 40.05 37.54 32.66 31.73 Pointwise reranker finetund by RankNet loss based on Qwen3 Qwen3-0.6B (Pointwise) Qwen3-4B (Pointwise) Qwen3-8B (Pointwise) 84.01 80.40 81.02 Ours E2RANK-0.6B E2RANK-4B E2RANK-8B 79.17 83.30 84.09 33.13 31.58 28.36 38.60 39.20 39.08 36.89 29.92 34.05 41.91 43.16 42. 41.87 48.30 49.50 53.20 45.00 44.50 45.56 44.54 44.52 45.00 42.27 44.05 35.79 45.23 46.03 39.19 43.91 45.44 33.07 40.84 40. 41.96 42.95 43.44 71.36 58.50 58.30 57.80 47.00 74.90 77.41 73.14 78.64 78.22 72.16 75.96 73.32 75.02 75.55 71.01 76.37 78. 70.27 72.09 70.55 73.43 77.19 77.49 31.44 76.30 77.10 44.62 32.50 38.80 49.35 44.80 45.00 77.00 68.80 31. 33.60 32.90 52.50 46.20 44.50 54.30 34.20 33.64 33.58 32.12 33.08 32.95 25.41 36.00 31.36 51.92 50.91 51.24 51.08 50.60 49.20 47.67 51.88 50. 60.25 57.41 59.66 60.74 54.46 56.91 57.11 57.49 55.40 30.96 32.15 32.48 44.43 50.81 52.36 46.31 59.36 60.72 27.28 25.98 26. 37.53 47.56 43.91 45.58 56.60 52.27 35.26 34.48 34.01 52.75 52.71 54.25 53.67 60.16 60.34 49.36 53.93 55. 55.74 50.03 54.07 56.06 54.06 55.86 55.69 53.60 54.61 50.84 54.59 54.35 49.52 55.20 56.48 47.27 50.72 48.91 53.55 55.71 56. 47.16 51.21 52.46 53.00 47.94 51.36 53.09 51.16 53.67 53.37 - - - - - 48.03 52.38 53.39 45.97 48.12 47. 52.09 54.14 54.35 Detailed efficiency analysis We listed the detailed latency results in Table 12 and Table 13. For E2RANK, we calculate the latency of encoding documents separately from other latencies, because if we use E2RANK as the retrieval model at the same time, the embedding of the document can be reused to avoid duplicate encoding. Table 12: Reranking latency per query (s) for E2RANK on the Covid Dataset. Table 13: Reranking latency per query (s) for RankQwen3 on the Covid Dataset."
        },
        {
            "title": "Overall Latency",
            "content": "E2RANK-0.6B E2RANK-4B E2RANK-8B 0.50 1.74 2.76 0.13 0.43 0.64 0.63 2.17 3.40 RankQwen3-0.6B RankQwen3-4B RankQwen3-8B 4.58 11.25 16. Full results on MTEB We present the detailed results on MTEB (eng, v1) benchmark of our models in Table 14. We also evaluate the models on MTEB (eng, v2) benchmark, and the results are shown in Table 15. Full results for end-to-end retrieval We present the detailed results of Table 5 in Table 16 and Table 17. Full results for the ablation studies of training We present the detailed results of Table 6 on Table 18, Table 19, and Table 20. 18 Preprint. Table 14: Detailed Results on MTEB(eng, v1) Benchmark. Task Qwen3-0.6B Qwen3-4B Qwen3-8B Stage Stage II Stage Stage II Stage Stage II AmazonCounterfactualClassification ArXivHierarchicalClusteringP2P ArXivHierarchicalClusteringS2S ArguAna AskUbuntuDupQuestions BIOSSES Banking77Classification BiorxivClusteringP2P.v2 CQADupstackGamingRetrieval CQADupstackUnixRetrieval ClimateFEVERHardNegatives FEVERHardNegatives FiQA2018 HotpotQAHardNegatives ImdbClassification MTOPDomainClassification MassiveIntentClassification MassiveScenarioClassification MedrxivClusteringP2P.v2 MedrxivClusteringS2S.v2 MindSmallReranking SCIDOCS SICK-R STS12 STS13 STS14 STS15 STS17 STS22.v2 STSBenchmark SprintDuplicateQuestions StackExchangeClustering.v2 StackExchangeClusteringP2P.v2 SummEvalSummarization.v2 TRECCOVID Touche2020Retrieval.v3 ToxicConversationsClassification TweetSentimentExtractionClassification TwentyNewsgroupsClustering.v2 TwitterSemEval2015 TwitterURLCorpus Average 79.72 57.38 55.44 52.88 62.21 84.68 79.96 38.50 56.19 41.53 26.37 88.02 38.12 53.52 76.66 92.60 72.90 74.58 33.82 32.61 30.67 16.87 79.69 76.75 84.07 78.48 85.99 89.92 60.30 84.39 91.15 56.38 38.91 31.55 70.48 53.79 64.42 66.04 44.40 70.68 85.59 62.40 82.21 58.55 54.00 50.56 62.92 85.22 80.88 39.28 55.35 39.31 30.80 85.68 40.84 68.42 82.57 93.62 72.48 74.71 35.17 31.19 29.85 17.85 79.89 74.12 84.19 78.98 86.25 90.09 65.60 84.75 93.49 53.12 38.95 31.66 81.03 58.46 64.99 66.23 38.29 72.13 86.16 63.41 83.52 57.86 56.09 52.13 66.45 86.13 83.12 40.11 61.47 49.86 37.85 92.26 50.76 61.72 86.57 94.09 76.36 78.96 34.00 32.19 32.04 20.14 81.92 77.48 83.47 79.76 87.41 91.63 62.89 86.82 90.64 55.68 40.34 33.53 81.41 52.39 69.56 64.86 42.70 75.93 86.51 65. 82.63 56.72 55.37 51.28 66.92 87.93 83.68 40.21 61.95 50.24 27.07 88.85 49.97 73.20 89.97 95.71 76.41 79.54 34.96 32.58 31.09 20.77 82.24 76.03 87.07 82.37 88.96 92.59 67.69 88.73 95.63 52.04 41.36 35.08 81.84 57.51 69.32 65.38 44.06 78.47 87.31 66.12 81.90 57.82 56.99 55.25 66.31 86.46 84.46 39.06 61.99 51.07 39.99 92.86 52.95 64.11 86.10 94.11 76.77 78.04 34.65 32.06 32.52 20.47 82.21 78.88 85.00 81.62 88.46 91.58 64.77 87.06 92.45 56.96 40.82 34.62 78.53 52.37 68.68 63.72 47.42 76.49 86.74 65.96 81.84 58.50 54.26 54.41 66.80 88.40 85.04 39.31 62.18 50.51 31.90 88.91 52.27 75.11 89.39 95.70 77.08 79.24 35.44 33.37 31.54 22.32 82.80 77.65 87.48 83.05 89.45 92.09 68.44 88.69 95.07 52.71 41.72 35.07 82.28 56.61 69.59 63.96 42.84 78.35 87.46 66. 19 Preprint. Table 15: Detailed Results on MTEB(eng, v2) Benchmark. Task Qwen3-0.6B Qwen3-4B Qwen3-8B Stage Stage II Stage Stage II Stage Stage II AmazonCounterfactualClassification ArXivHierarchicalClusteringP2P ArXivHierarchicalClusteringS2S ArguAna AskUbuntuDupQuestions BIOSSES Banking77Classification BiorxivClusteringP2P.v2 CQADupstackGamingRetrieval CQADupstackUnixRetrieval ClimateFEVERHardNegatives FEVERHardNegatives FiQA2018 HotpotQAHardNegatives ImdbClassification MTOPDomainClassification MassiveIntentClassification MassiveScenarioClassification MedrxivClusteringP2P.v2 MedrxivClusteringS2S.v2 MindSmallReranking SCIDOCS SICK-R STS12 STS13 STS14 STS15 STS17 STS22.v2 STSBenchmark SprintDuplicateQuestions StackExchangeClustering.v2 StackExchangeClusteringP2P.v2 SummEvalSummarization.v2 TRECCOVID Touche2020Retrieval.v3 ToxicConversationsClassification TweetSentimentExtractionClassification TwentyNewsgroupsClustering.v2 TwitterSemEval2015 TwitterURLCorpus Average 79.72 57.38 55.44 52.88 62.21 84.68 79.96 38.50 56.19 41.53 26.37 88.02 38.12 53.52 76.66 92.60 72.90 74.58 33.82 32.61 30.67 16.87 79.69 76.75 84.07 78.48 85.99 89.92 60.30 84.39 91.15 56.38 38.91 31.55 70.48 53.79 64.42 66.04 44.40 70.68 85.59 62.40 82.21 58.55 54.00 50.56 62.92 85.22 80.88 39.28 55.35 39.31 30.80 85.68 40.84 68.42 82.57 93.62 72.48 74.71 35.17 31.19 29.85 17.85 79.89 74.12 84.19 78.98 86.25 90.09 65.60 84.75 93.49 53.12 38.95 31.66 81.03 58.46 64.99 66.23 38.29 72.13 86. 63.41 83.52 57.86 56.09 52.13 66.45 86.13 83.12 40.11 61.47 49.86 37.85 92.26 50.76 61.72 86.57 94.09 76.36 78.96 34.00 32.19 32.04 20.14 81.92 77.48 83.47 79.76 87.41 91.63 62.89 86.82 90.64 55.68 40.34 33.53 81.41 52.39 69.56 64.86 42.70 75.93 86.51 65.33 82.63 56.72 55.37 51.28 66.92 87.93 83.68 40.21 61.95 50.24 27.07 88.85 49.97 73.20 89.97 95.71 76.41 79.54 34.96 32.58 31.09 20.77 82.24 76.03 87.07 82.37 88.96 92.59 67.69 88.73 95.63 52.04 41.36 35.08 81.84 57.51 69.32 65.38 44.06 78.47 87.31 66.12 81.90 57.82 56.99 55.25 66.31 86.46 84.46 39.06 61.99 51.07 39.99 92.86 52.95 64.11 86.10 94.11 76.77 78.04 34.65 32.06 32.52 20.47 82.21 78.88 85.00 81.62 88.46 91.58 64.77 87.06 92.45 56.96 40.82 34.62 78.53 52.37 68.68 63.72 47.42 76.49 86. 65.96 81.84 58.50 54.26 54.41 66.80 88.40 85.04 39.31 62.18 50.51 31.90 88.91 52.27 75.11 89.39 95.70 77.08 79.24 35.44 33.37 31.54 22.32 82.80 77.65 87.48 83.05 89.45 92.09 68.44 88.69 95.07 52.71 41.72 35.07 82.28 56.61 69.59 63.96 42.84 78.35 87.46 66.56 20 Preprint. Table 16: Full end-to-end ranking performance on BEIR. Coivd NFCorpus Touche DBPedia SciFact Signal News Robust Avg. E2RANK-0.6b E2RANK-4b E2RANK-8b Retrieval + Rerank Retrieval + Rerank Retrieval + Rerank 81.03 83.33 81.84 84.42 82.29 86. 33.80 37.62 38.64 41.39 40.08 42.33 29.96 30.87 27.95 33.19 27.95 34. 41.36 43.68 47.75 47.74 48.75 48.20 71.12 72.95 78.94 78.48 80.91 78. 27.97 27.94 27.90 27.10 28.13 26.31 42.85 50.03 49.56 52.85 53.46 53. 52.71 58.89 64.29 67.81 65.55 69.58 47.60 50.66 52.11 54.12 53.39 55. Table 17: Full end-to-end ranking performance on BRIGHT. StackExchange Coding Theorem-based Bio. Econ. Earth. Psy. Rob. Stack. Sus. Pony. LC. AoPS TheoQ. ThoT. E2RANK-0.6b Retrieval + Rerank E2RANK-4b E2RANK-8b Retrieval + Rerank Retrieval + Rerank 19.9 27.1 35.4 43.6 28.6 39.9 29.8 37.4 42.6 49.8 36.6 46. 17.5 23.1 23.7 29.2 22.3 28.9 20.7 31.0 34.4 43.8 30.9 41. 17.3 22.4 24.5 29.6 22.2 28.3 15.4 19.3 22.2 32.1 21.7 29. 12.3 20.0 22.4 31.0 19.8 34.4 4.2 3.7 7.2 4.6 7.3 6. 38.9 38.8 43.0 40.4 37.9 37.4 9.1 8.9 11.3 10.6 10.3 9. 13.7 17.9 33.5 36.2 30.2 33.6 21.7 21.5 34.1 34.9 33.3 36. Avg. 18.4 22.6 27.8 32.2 25.1 31.0 Results of using different first-stage retrieval models We evaluate the reranking performance of E2RANK on TREC DL19 and DL20 using different first-stage retrieval models, including popular dense embedding models Contriver (Izacard et al., 2021), BGE-base (Xiao et al., 2023), and Qwen3-Embedding-0.6B (Zhang et al., 2025c), as well as an effective neural sparse retrieval model SPLADE++ED Formal et al. (2022). The full results are shown in Table 21. Additionally, we also report the reranking results on BRIGHT, using BM25 and the original query for first-stage retrieval, as presented in Table 22. 21 Preprint. Table 18: Full results of ablation study on BEIR. Coivd NFCorpus Touche DBPedia SciFact Signal News Robust Avg. BM25 E2RANK-0.6B w/o Stage w/o InfoNCE in Stage II w/ only Stage w/o RankNet in Stage II w/o Listwise in Stage II 59. 79.17 79.22 79.48 77.04 80.85 81.47 30.75 38. 38.13 39.02 35.83 36.27 36.79 44.22 41.91 40.98 40. 25.35 32.67 33.33 31.80 41.96 40.99 42.27 40.58 40. 41.41 67.89 73.43 73.18 74.27 70.08 71.95 72. 33.05 39.52 40.70 43.43 35.26 33.35 34. 31.91 31.69 31.93 52.75 51.74 52.44 43.95 47.55 48. 53.67 53.01 54.59 45.72 51.98 53.24 52.09 51.33 52. 46.31 49.24 49.93 Table 19: Full results of ablation study on BRIGHT. StackExchange Coding Theorem-based Bio. Econ. Earth. Psy. Rob. Stack. Sus. Pony. LC. AoPS TheoQ. ThoT. BM25 E2RANK-0.6B w/o Stage w/o InfoNCE in Stage II w/ only Stage w/o RankNet in Stage II w/o Listwise in Stage II 18. 27.9 44.1 44.4 42.2 11.1 25.1 25.1 46. 46.9 45.0 16.7 36.2 36.5 16.4 31.0 29.9 27. 13.0 22.0 22.1 13.4 10.9 40.8 26. 40.7 41.4 13.3 26.2 25.8 25.5 14.8 19.8 27.0 20. 16.3 30.6 26.5 29.9 11.0 20.1 20.4 16. 30.6 32.0 29.0 10.3 16.2 16.9 4.3 11. 13.8 10.4 10.0 8.0 8.4 24.7 38.5 37.5 36. 37.4 40.2 40.1 6.5 8.0 8.1 7.1 9.0 9. 10.1 2.1 35.9 31.5 36.1 14.4 20.2 20. 7.3 28.0 30.8 29.1 22.7 25.0 25.0 Avg. 13.7 31.0 30.7 30.0 15.3 22.4 22.7 Table 20: Detailed Results of ablation study on MTEB(eng, v2) Benchmark."
        },
        {
            "title": "Task",
            "content": "E2RANK-0.6B w/o Stage w/o InfoNCE in Stage II w/ only Stage w/o RankNet in Stage II w/o Listwise in Stage II AmazonCounterfactualClassification ArXivHierarchicalClusteringP2P ArXivHierarchicalClusteringS2S ArguAna AskUbuntuDupQuestions BIOSSES Banking77Classification BiorxivClusteringP2P.v2 CQADupstackGamingRetrieval CQADupstackUnixRetrieval ClimateFEVERHardNegatives FEVERHardNegatives FiQA2018 HotpotQAHardNegatives ImdbClassification MTOPDomainClassification MassiveIntentClassification MassiveScenarioClassification MedrxivClusteringP2P.v2 MedrxivClusteringS2S.v2 MindSmallReranking SCIDOCS SICK-R STS12 STS13 STS14 STS15 STS17 STS22.v2 STSBenchmark SprintDuplicateQuestions StackExchangeClustering.v2 StackExchangeClusteringP2P.v2 SummEvalSummarization.v2 TRECCOVID Touche2020Retrieval.v3 ToxicConversationsClassification TweetSentimentExtractionClassification TwentyNewsgroupsClustering.v2 TwitterSemEval2015 TwitterURLCorpus"
        },
        {
            "title": "Average",
            "content": "82.21 58.55 54.00 50.56 62.92 85.22 80.88 39.28 55.35 39.31 30.80 85.68 40.84 68.42 82.57 93.62 72.48 74.71 35.17 31.19 29.85 17.85 79.89 74.12 84.19 78.98 86.25 90.09 65.60 84.75 93.49 53.12 38.95 31.66 81.03 58.46 64.99 66.23 38.29 72.13 86.16 63.41 80.76 57.09 55.77 50.85 61.21 85.13 81.1 38.27 55.36 39.27 28.03 63.58 34.74 59.29 82.01 93.67 71.78 74.9 34.47 32.17 30.22 18.13 80.63 75.15 84.93 79.15 86.57 90.42 65.53 85.46 92.67 52.16 38.94 31.63 67.55 51.36 65.41 66.38 41.35 69.93 85.61 61.92 70.34 57.87 54.23 51.46 61.94 85.35 79.99 38.63 53.32 38.3 28.91 76.43 36.79 63.33 73.02 92.75 69.5 72.92 35.21 30.82 29.95 17.07 70.59 63.39 80.41 74.29 82.54 83.99 65.08 79.05 94.73 53.99 39.1 28.75 78.78 56.42 61.35 62.33 41.01 65.49 85.71 60. 22 79.72 57.38 55.44 52.88 62.21 84.68 79.96 38.50 56.19 41.53 26.37 88.02 38.12 53.52 76.66 92.60 72.90 74.58 33.82 32.61 30.67 16.87 79.69 76.75 84.07 78.48 85.99 89.92 60.30 84.39 91.15 56.38 38.91 31.55 70.48 53.79 64.42 66.04 44.40 70.68 85.59 62.40 81.6 58.52 54.74 49.04 62.72 85.54 80.82 38.82 56.33 40.42 30.53 86.17 40.88 67.8 80.73 93.61 72.35 74.57 34.98 31.18 30.17 17.63 79.81 74.28 83.83 78.86 86.41 90.01 62.9 84.58 93.78 52.82 39.19 31.12 81.11 59.77 64.52 66.08 39.43 72.08 86.23 63.31 81.18 58.13 54.66 49.59 62.98 85.64 81.23 40.17 57.02 40.6 30.91 85.35 40.91 69.22 80.91 93.84 72.28 74.82 36.07 32.04 30.15 18.09 79.9 74.28 84.83 79.2 86.68 90.17 63.78 84.88 93.82 54.59 39.77 31.02 82.01 58.44 64.91 65.81 42.63 71.41 86. 63.66 Preprint. Table 21: Reranking results using different first-stage retrievers. BGE-base Contriver DL DL20 DL19 DL20 SPLADE++ED DL20 DL19 Qwen3E-0.6B DL20 DL19 First-stage Retrieval 70.22 66.21 62.02 63.42 73.08 71. 68.05 66.69 RankQwen3-0.6B E2RANK-0.6B RankQwen3-4B E2RANK-4B RankQwen3-8B E2RANK-8B 72.60 74. 72.71 75.46 73.73 74.15 72.51 73.97 76.31 74.90 75.68 76.40 68.63 71. 70.89 72.71 72.62 73.77 71.78 74.52 76.06 76.01 75.94 75.04 75.82 76. 75.56 75.74 74.61 77.37 74.34 77.82 74.78 79.25 75.81 80.08 74.00 74. 72.42 74.92 73.96 74.97 72.65 73.42 73.29 74.88 75.26 75.24 Table 22: Reranking results on BRIGHT. We use BM25 as the first-stage retriever and use original queries to obtain the top-100 candidates. The baseline results are mainly borrowed from Cai et al. (2025). RankQwen3-14B (32B) are zero-shot, others are all fine-tuned."
        },
        {
            "title": "Coding",
            "content": "Theorem-based Avg. Bio. Econ. Earth. Psy. Rob. Stack. Sus. Pony. LC. AoPS TheoQ. ThoT. BM25 18.2 27.9 16. 13.4 10.9 16.3 16.1 4.3 24. 6.5 2.1 7.3 13.7 Non-reasoning Listwise Reranker RankZephyr RankQwen3-0.6B RankQwen3-4B RankQwen3-8B RankQwen3-14B RankQwen3-32B 21.9 21.2 28.8 29.7 30.7 31.9 23.7 32.3 37.4 40.2 41.3 45.5 Reasoning-Intensive Reranker Rank-R1-7B Rank1-7B Rearank-7B JudgeRank-8B ERank-4B ERank-14B"
        },
        {
            "title": "Ours",
            "content": "E2RANK-0.6B E2RANK-4B E2RANK-8B 26.0 31.6 23.4 28.7 30.4 31.2 27.1 27.8 28.7 28.5 34.4 27.4 32.2 42.5 43.6 41.7 45.6 45.2 14.4 17.4 19.2 21.0 23.4 23. 17.2 18.0 18.5 20.9 21.5 25.8 20.7 23.9 24.4 10.3 20.8 31.4 31.0 30.1 33.2 24.2 23.5 24.2 24.6 27.7 27.8 7.6 14.7 20.5 23.3 24.7 25.6 19.1 16.7 17.4 16.5 22.4 23. 24.3 27.6 27.2 19.8 21.4 22.9 16.6 18.8 26.8 27.0 27.4 30.8 24.2 22.9 25.1 20.6 24.0 24.6 19.6 24.3 25.6 6.5 6.0 10.0 10.1 7.5 7. 4.3 20.1 8.0 11.7 31.6 29.8 4.8 5.0 6.6 24.7 26.6 22.5 16.9 30.0 29.7 19.8 9.4 27.0 7.1 14.6 16.8 32.7 34.8 32.5 6.8 6.4 6.3 6.5 8.9 10. 4.3 4.5 7.4 4.7 11.0 8.6 10.7 12.3 11.8 2.0 4.5 11.5 12.0 12.0 11.7 10.9 9.4 9.5 8.4 12.1 10.5 8.6 9.3 9.3 7.3 8.7 10.8 11.6 11.7 13. 8.3 9.9 7.9 10.0 11.4 11.9 9.9 10.8 10.7 13.0 16.0 20.6 21.1 22.4 23.8 16.4 18.3 17.7 17.0 22.7 23.1 20.2 22.3 22.5 13.7 14.9 21.8 23.2 21.1 22. 10.4 18.6 16.3 18.3 22.9 23.9 22.1 25.0 25."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Renmin University of China"
    ]
}