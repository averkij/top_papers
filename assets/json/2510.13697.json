{
    "paper_title": "On Pretraining for Project-Level Code Completion",
    "authors": [
        "Maksim Sapronov",
        "Evgeniy Glukhov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Repository-level pretraining is commonly used to enable large language models for code to leverage codebase-wide context. This enhances their ability to generate accurate and context-aware code completions. In this work, we investigate how different repository-processing strategies affect in-context learning in OpenCoder, a 1.5B-parameter model. We extend its context window from 4,096 to 16,384 tokens by training on additional 1B tokens of curated repository-level data. Despite relying on a smaller dataset than competing models (which often use hundreds of billions of tokens), our model achieves comparable performance on the Long Code Arena benchmark. We find that various repository-processing techniques yield similarly strong results, with the primary gain coming from adapting to a new rotary positional embedding (RoPE) scaling parameter. Finally, we show that a simpler file-level training approach at the original sequence length remains highly effective, opening up repository-level code completion research to settings with more constrained data and compute resources."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 7 9 6 3 1 . 0 1 5 2 : r Published as conference paper at ICLR 2025 ON PRETRAINING FOR PROJECT-LEVEL CODE COMPLETION Maksim Sapronov, Evgeniy Glukhov JetBrains Research {name.last name}@jetbrains.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Repository-level pretraining is commonly used to enable large language models for code to leverage codebase-wide context. This enhances their ability to generate accurate and context-aware code completions. In this work, we investigate how different repository-processing strategies affect in-context learning in OpenCoder, 1.5B-parameter model. We extend its context window from 4,096 to 16,384 tokens by training on additional 1B tokens of curated repository-level data. Despite relying on smaller dataset than competing models (which often use hundreds of billions of tokens), our model achieves comparable performance on the Long Code Arena benchmark. We find that various repository-processing techniques yield similarly strong results, with the primary gain coming from adapting to new rotary positional embedding (RoPE) scaling parameter. Finally, we show that simpler file-level training approach at the original sequence length remains highly effective, opening up repository-level code completion research to settings with more constrained data and compute resources."
        },
        {
            "title": "INTRODUCTION AND MOTIVATION",
            "content": "Large Language Models (LLMs) trained on source code, commonly known as Code LLMs, have demonstrated impressive capabilities on variety of code-related tasks (Lu et al., 2021; Jimenez et al., 2023; Hou et al., 2024; Jiang et al., 2024). Traditionally, these models are pretrained on individual files, effectively capturing local context but often missing broader, project-level information. To address this limitation, several recent works have incorporated repository-level pretraining phase, i.e., stage of pretraining during which the model gets training examples from entire repositories to learn context spanning multiple files, shared dependencies, and cohesive development patterns. For example, models such as DeepSeek Coder, Starcoder 2, Qwen2.5 Coder and CodeGemma (Guo et al., 2024; Lozhkov et al., 2024; Hui et al., 2024; CodeGemma Team et al., 2024) incorporate repository-level pretraining to extend their context windows and capture cross-file relationships. Beyond repository-level pretraining, other techniques have been investigated (Guo et al., 2023; Zhang et al., 2023; 2024). While repository-level pretraining enhances long-context capabilities, it also introduces significant challenges. Firstly, it requires huge amounts of data, e.g., Qwen2.5 Coders repository-level pretraining leverages approximately 300B tokens of repository data. Secondly, long sequences can strain computational resources due to the quadratic complexity of traditional transformer architecture. Recent advances in efficient attention mechanisms (for example, Flash Attention and Ring Attention (Dao, 2023; Liu et al., 2023a)) have enabled training with context lengths typically in the tens of thousands of tokens, and even millions of tokens for smaller models. However, effective utilization of repository-level information remains challenging both for training and inference (Ding et al., 2022; Liu et al., 2023b; Ding et al., 2024; Pei et al., 2023). In this work, we focus on single line repository-level code completion and study context extension pretraining for various repository preprocessing approaches. Following the Long Code Arena benchmark (Bogomolov et al., 2024) terminology, we evaluate the impact of different context composers, i.e., processors that transform repository files into context strings. Our approach builds on the OpenCoder base model (Huang et al., 2024), originally configured with 4,096 (4K) context window, by training on repository-level input sequences of up to 16,384 (16K) tokens. This exten1 Published as conference paper at ICLR 2025 sion results in significantly improved performance on 16K token sequences compared to the initial configuration. We assess our methods using the Project-Level Code Completion task from the Long Code Arena benchmark, which effectively estimates models ability to handle cross-file dependencies in realistic settings. By isolating the impact of repository-level pretraining and comparing different context composer strategies, our study provides practical insights for enhancing long-context code completion performance. The main contributions of this paper are: 1. We boost the project-level code completion performance of OpenCoder 1.5B to state-ofthe-art levels using only 1B tokens of training data; 2. Our experiments reveal that the choice of context composer during pretraining has only marginal impact on final model quality, with performance scores ranging from 45.2 to 48.8 (out of 100) on the chosen metric."
        },
        {
            "title": "2 EXPERIMENT DESIGN",
            "content": "We start this section with description of the data sourcing and preparation steps, then explain our training setup, and finally we detail our evaluation strategy. In addition, we discuss the role of context composers repository processing functions, and their distinct modes for the training and evaluation phases."
        },
        {
            "title": "2.1 TRAINING DATA",
            "content": "To collect the training data, we follow the approach from the Long Code Arena benchmark, see Bogomolov et al. (2024) for more details. Starting with open-source GitHub repositories in Python and then traverse the Git commit history for each repository to extract repository data. Filtering process is described in B.1 The repository data for each commit consists of two elements: (1) repository snapshot context source with contents of all code and text files before the commit; (2) completion files list of files to perform completion on with contents of all .py files added in that commit. The resulting raw repositories dataset contains 1,640 repositories, 160,801 commits, and 361,052 completion files. The total number of characters in completion files is 1.7B, and in repository snapshot files 4.8T. To get context string from the repository data, we apply context composer to repository snapshot. context composer is repository processor that (1) sorts subset of files (or file chunks) from the repository snapshot by relevance (based on specified criteria), (2) retrieves the most relevant ones that fit within the context window, and (3) concatenates them into single string with the most relevant file at the end. For each context composer in the list provided in Appendix A.1, we prepare the composed dataset from the raw repositories dataset which consists of two columns: (1) completion file one file from the completion files; (2) composed context string with the result of the context composer. Of the various context composers used in our experiments, we highlight the following two for clarity and conciseness. 1. File-level Produces an empty context. 2. Path Distance .py The context is built solely from .py files, sorted in descending order by their path distance from the completion file. For files with the same distance, secondary sort uses the Intersection over Union (IoU) score of their matching lines. Note that rows from the raw repositories dataset can produce multiple rows of the composed dataset with one row for each completion file. 2 Published as conference paper at ICLR"
        },
        {
            "title": "2.2 TRAINING",
            "content": "For each context composer, we pretrain OpenCoder 1.5B model (Huang et al., 2024) on the corresponding composed dataset with context window size of 16,384 tokens. In our training mode for the context composer, we aim to include as many files as possible in the context string. To achieve this, we truncate both the context string and the completion file, ensuring that the context-to-completion token ratio is at least 3 : 1. For more details, see Appendix C.2. To extend the model context window size, we change RoPEs base frequency Î¸ from 10,000 to 500,000 following the ABF approach (Xiong et al., 2023); our focus is on this method, although alternative approaches exist (Chen et al., 2023; Peng et al., 2023; Zhong et al., 2024; Liu et al., 2023c). To evaluate models after training on approximately 1 billion tokens, and in accordance with our training hyperparameters (see Appendix C.1), we save the models weights at the 512th optimization step, referring to this saved state as checkpoint."
        },
        {
            "title": "2.3 EVALUATION",
            "content": "Our evaluation setup is based on the large context dataset, which is part of the Project-level code completion task from the Long Code Arena dataset (LCA-large) (Bogomolov et al., 2024). The task is to write the next line of code based on the file prefix and the repository snapshot, with the evaluation metric being Exact Match (percentage of correct answers). Additionally, each line has one of six categories that corresponds to various scenarios of project cross-file dependencies. We use categories infile and inproject, i.e., completion line that contains an API declared in the completion file or in repository snapshot files. These two categories indicate in-context learning capabilities the best out of six, since they contain more project-specific information. We evaluate each checkpoint on infile and inproject categories for two different context composers in the evaluation mode: (1) FL-4K: File-Level composer with maximum sequence length 4K tokens, and (2) PD-16K: Path Distance .py composer with maximum sequence length 16K tokens. Moreover, we calculate RCB: repository-context boost, i.e., the difference between scores for the PD-16K and FL-4K composers."
        },
        {
            "title": "3 RESULTS",
            "content": "In the following subsections, we present our main results: first, we achieve state-of-the-art quality on LCA-large with much less extensive repository-level pretraining; second, we demonstrate the impact of the context composer choice on the result of repository-level pretraining. Additionally, we provide more detailed comparative study of repository-level pretraining in the Appendix."
        },
        {
            "title": "3.1 BENCHMARKING AGAINST STATE-OF-THE-ART",
            "content": "To estimate the effectiveness of our trained models, we compare them to DeepSeek Coder 1.3B, OpenCoder 1.5B with no repository-level pretraining, and Qwen2.5-Coder 0.5B and 1.5B. These models serve as strong baselines, representing state-of-the-art performance in similar parameter ranges. Results are shown in Table 1. We started with OpenCoder model which is pretty good on file-level code completion among the similar size models and got significant gain by file-level pretraining on just 1B tokens 1. This approach serves as guideline for scenarios with limited data and low GPU resources, since we do not need repositories, and do not actually need long context for training. We can even achieve Qwen2.5-Coder performance level with 1B tokens of curated repository-level data. 3."
        },
        {
            "title": "IMPACT OF CONTEXT COMPOSER CHOICE",
            "content": "Findings in the previous subsection leave an open question if there is even better composer for repository-level pretraining. To answer this question, we evaluate all studied composers and present 1While the model had access to 1B tokens, it was actually used just 72M tokens for training. 3 Published as conference paper at ICLR 2025 Table 1: Comparison of existing models on LCA-large for the line categories: inproject and infile. FL-4K and PD-16K report Exact Match scores for File-level and Path Distance .py evaluation composers. RCB represents the repository-context boost score. inproject infile"
        },
        {
            "title": "Model",
            "content": "FL-4K PD-16K RCB FL-4K PD-16K RCB Qwen2.5-Coder 0.5B DeepSeek Coder 1.3B OpenCoder 1.5B Qwen2.5-Coder 1.5B Ours (OpenCoder 1.5B) File-level pretr. Path Distance .py pretr. 22. 25.1 26.4 27.2 25.9 26.2 44.2 42.3 0.0 48.5 45.2 48.8 +21. +17.2 26.4 +21.3 +19.3 +22.6 27.5 30.3 32.6 34.3 33.0 33.1 43. 43.8 0.0 49.7 44.6 47.6 +15.7 +13.5 32.6 +15.4 +11.6 +14.5 Table 2: Results of evaluating checkpoints after repository-level pretraining. Evaluation dataset is LCA-large for the line categories: inproject and infile. FL-4K and PD-16K report Exact Match scores for File-level and Path Distance .py evaluation composers."
        },
        {
            "title": "Pretraining\nComposer",
            "content": "inproject infile FL-4K PD-16K FL-4K PD-16K Base model (no training) File-level Path Distance .py Other Pretraining Composers 26.4 25.9 26.2 25.5 26.5 0.0 45.2 48.8 46.8 48.7 32.6 33.0 33.1 32.3 33.3 0.0 44.6 47.6 45.6 47.8 condensed results for in Table 2 and extended results in Table 3. Our experiments demonstrate the performance variations across repository level pretrainings with different context composers. We observe that file-level composer pretraining results in +19.3 repository-context boost, with other pretraining strategies getting repository-context boost within +20.3 to +22.9 range. Combining with comparable values of the Exact Match, we validate that adapting to the longer context window, i.e., new RoPEs base frequency, rather than the specific sequence composition, is the primary factor in repository-level pretraining, with context composers contributing only marginally for suggested approaches."
        },
        {
            "title": "4 CONCLUSION",
            "content": "In this paper, we address the challenge of project-level code completion by evaluating pretraining for code LLM on various data extracted from repository. Our extensive experiments demonstrate that even relatively small training dataset and simple context composer (e.g., file-level or path distance) is enough to get model comparable to the latest state-of-the-art code LLMs. This insight reduces the complexity of repository-level pretraining, which effectively minimizes the technical complexities and encourages to broadly research the topic. Although our findings are promising, they have certain limitations. Our experiments are limited to the OpenCoder model, and it remains unclear whether they generalize to other LLMs. key direction for future work is to apply our approach on broader range of Code LLMs. However, recent Code LLMs were released after the repository-level pretraining stage, which may introduce inconsistencies in evaluation."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We appreciate the contributions of Alexander Bezzubov, Egor Bogomolov, Timofey Bryksin and Yaroslav Golubev from JetBrains, whose guidance greatly enhanced this research. 4 Published as conference paper at ICLR"
        },
        {
            "title": "REFERENCES",
            "content": "Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, and Timofey Bryksin. Long code arena: set of benchmarks for long-context code models. arXiv preprint arXiv:2406.11612, 2024. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea Hu, Christopher Choquette-Choo, Jingyue Shen, Joe Kelley, et al. CodeGemma: open code models based on gemma. arXiv preprint arXiv:2406.11409, 2024. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. CoCoMIC: Code completion by jointly modeling in-file and cross-file context. arXiv preprint arXiv:2212.10007, 2022. Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al. CrossCodeEval: diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information Processing Systems, 36, 2024. Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian McAuley. LongCoder: long-range pretrained language model for code completion. arXiv preprint arXiv:2306.14893, 2023. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Wu, YK Li, et al. DeepSeek-Coder: When the large language model meets programming the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: systematic literature review. ACM Transactions on Software Engineering and Methodology, 33(8):179, 2024. Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, Yang, JH Liu, Chenchen Zhang, Linzheng Chai, et al. OpenCoder: The open cookbook for top-tier code large language models. arXiv preprint arXiv:2411.04905, 2024. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2.5-Coder technical report. arXiv preprint arXiv:2409.12186, 2024. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik arXiv Narasimhan. SWE-bench: Can language models resolve real-world GitHub issues? preprint arXiv:2310.06770, 2023. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context. arXiv preprint arXiv:2310.01889, 2023a. Tianyang Liu, Canwen Xu, and Julian McAuley. RepoBench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023b. Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209, 2023c. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. StarCoder 2 and The Stack v2: The Next Generation. arXiv preprint arXiv:2402.19173, 2024. 5 Published as conference paper at ICLR 2025 Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. CodeXGLUE: machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664, 2021. Hengzhi Pei, Jinman Zhao, Leonard Lausen, Sheng Zha, and George Karypis. Better context makes better code language models: case study on function call argument completion. arXiv preprint arXiv:2306.00381, 2023. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code Llama: open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. RepoCoder: repository-level code completion through iterative retrieval and generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 24712484, 2023. Kechi Zhang, Ge Li, Huangzhao Zhang, and Zhi Jin. HiRoPE: Length extrapolation for code models using hierarchical position. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1361513627, 2024. Meizhi Zhong, Chen Zhang, Yikun Lei, Xikai Liu, Yan Gao, Yao Hu, Kehai Chen, and Min Zhang. Understanding the RoPE extensions of long-context LLMs: An attention perspective. arXiv preprint arXiv:2406.13282, 2024. Published as conference paper at ICLR"
        },
        {
            "title": "A CONTEXT COMPOSERS",
            "content": "A.1 COMPLETE LIST All composers follow two standard preprocessing steps, filtering out empty files and normalizing all line separators to Line Feed (LF). With these shared characteristics, the full list of context composers ensures comprehensive coverage for research exploration. 1. File-level Produces an empty context. 2. Path Distance .py Constructs the context using only files with the .py extension. The selected files are sorted in descending order based on their path distance from the completion file. If multiple files share the same path distance, secondary sorting step is applied using the Intersection over Union (IoU) metric, computed over lines shared with the completion file. The IoU metric is calculated on lines with leading and trailing whitespace characters removed, considering only those lines that are at least five characters long after the whitespace removal. 3. Lines IoU .py Similar to the Path Distance .py method but does not apply the primary sorting step based on path distance. Instead, files are directly ranked using the IoU metric. 4. Code Chunks Removes all docstrings, comments, and import statements from the context produced by Path Distance .py. 5. Half-memory .py Starts with the context produced by Path Distance .py. Each line is independently removed with dropout probability of 0.5, maintaining the overall saturation of the context window. 6. Declarations .py Builds upon Path Distance .py by filtering out all non-declarative elements, retaining only function and class declarations. 7. Text Chunks .py Uses Path Distance .py as the base method. All code is removed from the context, leaving docstrings and comments only. 8. Text files Constructs the context using files with the extensions .json, .yaml, .yml, .sh, .md, .txt, and .rst. The selected files are grouped in ascending order of relevance: [.json], [.yaml, .yml], [.sh], [.md, .txt, .rst]. Within each group, secondary sorting step is performed in descending order based on path distance from the completion file. 9. Random files Constructs the context by randomly ordering all files from the repository snapshot. 10. Random .py Selects only files with the .py extension and orders them randomly. 11. Mixed context2 The context for each data point is constructed by randomly selecting one of the following composers: File-level, Path Distance .py, Half-memory .py, Declarations .py, Text files, Random files, or Duplication. Furthermore, we propose four additional context composers, which are omitted from the results discussion as they do not reflect realistic scenarios. 1. Random tokens Constructs the context using randomly sampled sequence of nonspecial tokens, each selected independently and with equal probability. 2. Duplication Constructs the context by concatenating the content of the completion file repeatedly until the maximum context window size is reached. 3. Leak Starts with the context produced by Path Distance .py. The completion file is randomly split into five segments at newline characters, which then disjointedly replace context lines at random positions, approximately preserving the original token count. 4. Masked Leak Starts with the context produced by Path Distance .py. The completion file is divided into segments, each consisting of five lines with one overlapping line at the beginning and one at the end. These segments independently and disjointedly replace context lines at random positions. Additionally, each token in the context has 0.15 probability of being replaced with different non-special token. 2Duplication composer is disabled in evaluation mode 7 Published as conference paper at ICLR 2025 For each composer we also consider two modifications: reversed we retrieve files that fit into the context window with the composer and reverse their order, so the most relevant one is in the beginning of the context string; irrelevant we reverse the order of files obtained from the composer and therefore retrieve most irrelevant files. A."
        },
        {
            "title": "INPUT FORMATTING",
            "content": "All composers employ uniform strategy for input formatting. The files processed by composer undergo predefined formatting pattern 1, which uses special token from the OpenCoders vocabulary. Subsequently, the processed files are concatenated into single string, referred to as the composed context. <file sep># {file name}n{file content} Figure 1: File Representation similar transformation is applied independently to the completion file."
        },
        {
            "title": "B TRAINING DATASET",
            "content": "B.1 FILTERING To avoid training on test data, we exclude repositories used in the Long Code Arenas Project-level code completion task. In addition, to ensure data relevance and quality, we apply the following filtering criteria. First, all commits made prior to 2010 are excluded. Second, completion files with lengths outside the closed interval [800, 25000] characters are removed. Third, to eliminate redundancy, simple deduplication strategy is employed on completion files based on the file name and the name of the repository to which they belong. Finally, up to 1000 of the most recently updated unique completion files are selected from each repository. The remaining repository snapshot is retained without additional processing."
        },
        {
            "title": "C TRAINING DETAILS",
            "content": "C.1 HYPERPARAMETERS The optimization process was conducted using the AdamW optimizer with Î²1 = 0.9, Î²2 = 0.999, and weight decay of 0.01. batch size of 128 was employed, with micro-batch size of 1 to accommodate hardware constraints. To ensure stable training, gradient clipping was applied with maximum gradient Euclidean norm of 2. The learning rate was managed using cosine decay scheduler with linear warm-up phase, where the maximum learning rate was set to 5 105. The warm-up phase lasted for 256 iterations, after which the learning rate followed cosine decay schedule for 3244 additional iterations, reaching minimum value of 5 108. C.2 TRAINING MODE OF CONTEXT COMPOSERS For training, we obtain an input sequence from each row of the composed dataset by independently tokenizing the context string and the completion file. This process ensures that the completion sequence does not exceed 4,096 tokens and that the total length of the concatenated input remains within 16,384 tokens. To enforce these constraints, we apply truncation from the left for the context and from the right for the completion. Given that most composed contexts exhibit high token saturation, we maintain context-to-completion token ratio exceeding 3 : 1. 8 Published as conference paper at ICLR"
        },
        {
            "title": "D EVALUATION DETAILS",
            "content": "D.1 EVALUATION MODE OF CONTEXT COMPOSERS For evaluation, we obtain an input sequence for each row of the composed dataset by tokenizing the concatenation of the context string and the completion file. We then apply truncation from the left. Compared to the training mode (see Appendix C.2), we do not fix the maximum sequence length. Instead, we treat it as parameter that can be adjusted based on the evaluation requirements. For example, in Appendix F, we demonstrate the dependency between checkpoint quality and maximum sequence length. We use the following four evaluation composers in our tables: FL-4K: File-Level composer with maximum sequence length 4K tokens. We use it to estimate how hard the task is without any repository-context, and as reference point for calculating gains. PD-4K: Path Distance composer with maximum sequence length 4K tokens. We use it to estimate models in-context learning capabilities with initial input sequence length (4K tokens). PD-16K: Path Distance composer with maximum sequence length 16K tokens. We use it to estimate models in-context learning capabilities with new input sequence length (16K tokens), and this is the main composer to compare repository-level pretraining with different context composers. Or-16K: original pretraining composer in evaluation mode with maximum sequence length 16K tokens. We use it to identify the most promising composer overall. This composer applies only to our checkpoints."
        },
        {
            "title": "E COMPREHENSIVE COMPILATION OF EVALUATION RESULTS",
            "content": "We present results of our experiments in Table 3. They can be used as baselines for further research. We additionally include results for the base model with RoPEs base frequencies (Î¸) being 10,000 and 500,000, results for pretraining with file-level composer for the same values of Î¸. These results demonstrate that RoPE adjustments impact model quality, and that the model with initial base frequency performs on zero-level for long contexts even after finetuning. When using FL-4K composer, the model successfully recovers its quality after RoPE adjustments, suggesting that file-level data alone is sufficient to restore performance. The initial model shows strong in-context learning capabilities for the PD-4K composer, with it outperforming file-level inference. This advantage persists after repository-level pretraining, indicating that training on the collected data effectively retains models ability to utilize relevant context for shorter context size. For the PD-16K composer, the initial model, without RoPE adaptation, fails completely, but RoPE scaling alone improves Exact Match scores. Further pretraining yields gains of +19 for file-level training and +22 for the best composer in inproject category, with all final scores being slightly higher than file-level pretraining performance. This suggests that adapting to the longer context window, rather than the specific sequence composition, is the primary factor in repository-level code completion, with context composers contributing only marginally for suggested approaches (+3 points for inproject category). Overall, our findings emphasize that RoPE adaptation is the dominant factor in long-context performance gains, while sequence composition plays secondary role. Future work should explore more effective retrieval-based strategies to maximize repository-level context utilization."
        },
        {
            "title": "F PERFORMANCE SCALING BEYOND TRAINING CONTEXT WINDOW",
            "content": "The repository-level pretraining with File-level composer and Path Distance composer for maximum sequence lengths of 4K and 16K. However, pretrained checkpoints extrapolate beyond these lengths up to 16K and 32K as snown on Figure 2. 9 Published as conference paper at ICLR 2025 Table 3: Results of evaluating all checkpoints after repository-level pretraining on all evaluation composers. Evaluation dataset is LCA-large for the line categories: inproject and infile. Highlighted column is the main column for in-context learning capabilities comparison."
        },
        {
            "title": "Pretraining\nComposer",
            "content": "Base model (no training) inproject infile FL-4K PD-4K PD-16K Or-16K FL-4K PD-4K PD-16K Or-16K Î¸ = 10,000 Î¸ = 500, File-level 4K Î¸ = 10,000 Î¸ = 500,000 Path Distance .py reversed irrelevant Lines IoU .py reversed irrelevant Code Chunks .py reversed irrelevant Half-memory .py reversed irrelevant Declarations .py reversed irrelevant Text Chunks .py reversed irrelevant Text files reversed irrelevant Random files Random .py Mixed context Random tokens Duplication Leak reversed irrelevant Masked Leak 26.4 13.5 26.2 25.9 26.2 26.1 25. 25.7 26.1 25.8 25.9 26.1 25.8 25.7 25.7 25.8 25.9 25.7 26.2 26.1 26.0 25.9 25.9 26.0 26. 26.2 25.9 26.2 26.0 19.6 24.9 24.3 24. 25.2 36.6 16.6 36.4 36.1 37.0 36.9 36.5 36.3 36.8 36.4 36.5 36.5 36. 36.0 36.2 36.0 36.5 36.5 36.3 36.6 36.1 36.4 36.2 36.5 36.5 37.0 36. 36.7 36.2 28.8 34.8 34.7 34.6 35.4 32.6 15. 32.7 33.0 33.1 32.9 32.5 33.2 33.2 32.7 32.8 32.8 32.3 32.9 32.9 32.4 32.6 32.7 32. 33.0 32.9 32.7 33.0 33.2 32.7 32.8 32.8 32.6 32. 24.5 30.8 30.8 31.3 31.6 38.2 12.9 38.1 38.1 38.7 38.8 38. 38.4 38.9 38.4 38.2 38.3 37.9 38.4 38.2 37.7 38.1 38.1 38.4 38.5 38.5 38.5 38.6 38.4 38. 38.3 38.1 38.2 37.9 27.0 35.5 35.3 35. 36.9 0.0 4.5 0.0 44.6 47.6 47.5 46.7 47.7 47.4 46.6 47.5 47.4 46. 46.6 46.5 46.5 46.1 45.7 45.6 46.9 46.2 46.2 46.4 46.2 46.3 47.3 47. 47.5 45.1 28.1 43.6 42.8 43.2 45.0 32.7 33.0 48.8 44.0 33.4 50.1 44.6 33.4 47.9 43.0 33.2 38.7 36.9 33.0 34.4 34.2 33. 33.2 33.5 33.8 33.5 33.1 33.7 34.2 35.3 36.4 33. 95.0 81.6 81.0 79.7 63.5 26.2 25.9 48.8 43.2 26. 51.8 43.5 26.7 47.8 41.3 26.9 38.6 35.0 27.5 28.2 28.1 28.2 26.9 26.8 26.8 26.9 26.7 27. 29.8 31.9 31.0 26.0 96.7 82.9 83.8 82. 65.5 0.0 9.8 0.0 45.2 48.8 48.3 47.9 48.7 48.4 47.5 47.9 47.8 47. 47.4 47.3 47.0 46.8 46.9 47.2 47.5 47.4 47.2 47.1 46.9 47.1 48.1 48. 48.5 44.5 34.7 46.1 45.6 45.6 46.4 Published as conference paper at ICLR 2025 Performance of OpenCoder 1.5B in the inproject Category a a c M x 50 40 30 10 0 50 40 30 10 0 File-level Path Distance .py 1K 2K 4K 8K 16K 33K 66K 131K"
        },
        {
            "title": "Context Size",
            "content": "Performance of OpenCoder 1.5B in the infile Category File-level Path Distance .py 1K 2K 4K 8K 16K 33K 66K 131K"
        },
        {
            "title": "Context Size",
            "content": "Figure 2: Performance comparison of File-level and Path Distance .py approaches across different context sizes for OpenCoder 1.5B model. The plots show the Exact Match accuracy for both inproject (top) and infile (bottom) categories. The dashed vertical lines represent the context length used during repository-level pretraining. Similar behavior was observed in Roziere et al. (2023) and it needs additional research."
        },
        {
            "title": "G MASKED LOSS AND FULL LOSS RESULTS",
            "content": "Some context composers create out-of-distribution sequences (e.g., Declarations .py). We avoid distribution shift by masking the loss, i.e., use only gradients from completion file tokens for training. In case of any composer that includes unprocessed code files in the context, we lose tokens for 11 Published as conference paper at ICLR 2025 training. However, the results for each composer in Table 4 are comparable for masked loss and full loss pretraining, with the only exception being the Duplication composer. Table 4: Comparison of checkpoints pretrained with masked loss and full loss. Pretraining Composer Path Distance .py Masked loss Full loss Path Distance .py, reversed Masked loss Full loss Path Distance .py, irrelevant Masked loss Full loss Lines IoU .py Masked loss Full loss Lines IoU .py, reversed Masked loss Full loss Lines IoU .py, irrelevant Masked loss Full loss Code Chunks .py Masked loss Full loss Code Chunks .py, reversed Masked loss Full loss Code Chunks .py, irrelevant Masked loss Full loss Random .py Masked loss Full loss Duplication Masked loss Full loss inproject infile FL-4K PD-4K PD-16K Or-16K FL-4K PD-4K PD-16K Or-16K 48.8 48.4 43.2 43.1 26.7 26. 51.8 51.2 43.5 43.6 26.7 26.6 47.8 47.7 41.3 41.5 26.9 26. 31.9 32.2 96.7 97.3 33.1 33.1 32.9 33.1 32.5 33.0 33.2 32. 33.2 33.1 32.7 33.3 32.8 33.2 32.8 33.3 32.3 33.3 32.8 33. 24.5 32.8 38.7 38.6 38.8 38.6 38.1 38.4 38.4 38.8 38.9 38. 38.4 38.7 38.2 38.6 38.3 38.8 37.9 38.7 38.1 38.5 27.0 38. 47.6 47.8 47.5 47.2 46.7 46.8 47.7 47.5 47.4 47.4 46.6 47. 47.5 47.8 47.4 47.8 46.3 47.1 47.0 47.6 28.1 44.4 47.6 47. 44.0 44.1 33.4 33.6 50.1 49.6 44.6 44.8 33.4 33.3 47.9 47. 43.0 43.4 33.2 33.3 35.3 36.0 95.0 96.4 26.2 26.3 26.1 26. 25.8 25.5 25.7 25.9 26.1 26.0 25.8 26.2 25.9 26.5 26.1 26. 25.8 25.7 25.9 26.2 19.6 25.5 37.0 36.5 36.9 36.8 36.5 36. 36.3 36.6 36.8 36.8 36.4 36.8 36.5 36.8 36.5 37.0 36.5 36. 36.8 36.8 28.8 35.9 48.8 48.4 48.3 48.4 47.9 48.0 48.7 48. 48.4 48.3 47.5 48.2 47.9 48.7 47.8 48.7 47.7 48.1 48.4 48. 34.7 46."
        }
    ],
    "affiliations": [
        "JetBrains Research"
    ]
}